Journal of Articial Intelligence Research 15 (2001) 163-187

Submitted 10/00; published 09/01

An Analysis of Reduced Error Pruning
elomaa@cs.helsinki.fi
matti.kaariainen@cs.helsinki.fi

Tapio Elomaa
Matti Kriinen
Department of Computer Science
P. O. Box 26 (Teollisuuskatu 23)
FIN-00014 University of Helsinki, Finland

Abstract

Top-down induction of decision trees has been observed to suer from the inadequate
functioning of the pruning phase. In particular, it is known that the size of the resulting
tree grows linearly with the sample size, even though the accuracy of the tree does not
improve. Reduced Error Pruning is an algorithm that has been used as a representative
technique in attempts to explain the problems of decision tree learning.
In this paper we present analyses of Reduced Error Pruning in three dierent settings.
First we study the basic algorithmic properties of the method, properties that hold independent of the input decision tree and pruning examples. Then we examine a situation that
intuitively should lead to the subtree under consideration to be replaced by a leaf node,
one in which the class label and attribute values of the pruning examples are independent
of each other. This analysis is conducted under two dierent assumptions. The general
analysis shows that the pruning probability of a node tting pure noise is bounded by a
function that decreases exponentially as the size of the tree grows. In a specic analysis
we assume that the examples are distributed uniformly to the tree. This assumption lets
us approximate the number of subtrees that are pruned because they do not receive any
pruning examples.
This paper claries the dierent variants of the Reduced Error Pruning algorithm,
brings new insight to its algorithmic properties, analyses the algorithm with less imposed
assumptions than before, and includes the previously overlooked empty subtrees to the
analysis.
1. Introduction
Decision tree learning is usually a two-phase process (Breiman, Friedman, Olshen, & Stone,
1984; Quinlan, 1993).

training examples

First a tree reecting the given sample as faithfully as possible is

constructed. If no noise prevails, the accuracy of the tree is perfect on the

that were used to build the tree. In practice, however, the data tends to be noisy, which
may introduce contradicting examples to the training set.

overtted

necessarily be obtained even on the training set.
is

Hence, 100% accuracy cannot

In any case, the resulting decision tree

to the sample; in addition to the general trends of the data, it encodes the

pruned

peculiarities and particularities of the training data, which makes it a poor predictor of the
class label of future instances. In the second phase of induction, the decision tree is

in order to reduce its dependency on the training data. Pruning aims at removing from the
tree those parts that are likely to only be due to the chance properties of the training set.
The problems of the two-phased top-down induction of decision trees are well-known
and have been extensively reported (Catlett, 1991; Oates & Jensen, 1997, 1998). The size

c 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiElomaa & Kriinen
of the tree grows linearly with the size of the training set, even though after a while no
accuracy is gained through the increased tree complexity. Obviously, pruning is intended to
ght this eect. Another defect is observed when the data contains no relevant attributes;
i.e., when the class labels of the examples are independent of their attribute values. Clearly,
a single-node tree predicting the majority label of the examples should result in this case,
since no help can be obtained by querying the attribute values. In practice, though, often
large decision trees are built from such data.
Many alternative pruning schemes exist (Mingers, 1989a; Esposito, Malerba, & Semeraro, 1997; Frank, 2000).

pruning examples

They dier, e.g., on whether a single pruned tree or a series of

pruned trees is produced, whether a separate set of

is used, which aspects

(classication error and tree complexity) are taken into account in pruning decisions, how
these aspects are determined, and whether a single scan through the tree suces or whether
iterative processing is required.

The basic pruning operation that is applied to the tree

is the replacement of an internal node together with the subtree rooted at it with a leaf.
Also more elaborated tree restructuring operations are used by some pruning techniques

majority leaf
pruning of a tree

(Quinlan, 1987, 1993). In this paper, the only pruning operation that is considered is the
replacement of a subtree by the
examples reaching it. Hence, a

, i.e., a leaf labeled by the majority class of the
is a subtree of the original tree with just

zero, one, or more internal nodes changed into leaves.

Reduced Error Pruning

(subsequently

rep for short) was introduced by Quinlan (1987) in

the context of decision tree learning. It has subsequently been adapted to rule set learning as
well (Pagallo & Haussler, 1990; Cohen, 1993).
In practical decision tree pruning

overprunes

rep

rep is one of the simplest pruning strategies.

is seldom used, because it has the disadvantage of

requiring a separate set of examples for pruning. Moreover, it is considered too aggressive a
pruning strategy that

the decision tree, deleting relevant parts from it (Quinlan,

1987; Esposito et al., 1997). The need for a pruning set is often considered harmful because
of the scarceness of the data. However, in the data mining context the examples are often
abundant and setting a part of them aside for pruning purposes presents no problem.
Despite its shortcomings

rep

is a baseline method to which the performance of other

pruning algorithms is compared (Mingers, 1989a; Esposito, Malerba, & Semeraro, 1993;
Esposito et al., 1997).

It presents a good starting point for understanding the strengths

and weaknesses of the two-phased decision tree learning and oers insight to decision tree
pruning.

rep has the advantage of producing the smallest pruning among those that are the

most accurate with respect to the pruning set. Recently, Oates and Jensen (1999) analyzed

rep in an attempt to explain why and when decision tree pruning fails to control the growth

of the tree, even though the data do not warrant the increased size. We approach the same
subject, but try to avoid restricting the analysis with unnecessary assumptions.

We also

consider an explanation for the unwarranted growth of the size of the decision tree.

rep in three dierent settings. First, we explore the basic algorep, which apply regardless of the distribution of examples presented

In this paper we analyze
rithmic properties of

to the learning algorithm. Second, we study, in a probabilistic setting, the situation in which
the attribute values are independent of the classication of an example. Even though this
pure noise tting situation is not expected to arise when the whole pruning set is considered,
it is encountered at lower levels of the tree, when all relevant attributes have already been
exhausted. We further assume that all subtrees receive at least one pruning example, so that

164

fiAn Analysis of Reduced Error Pruning
none of them can be directly pruned due to not receiving any examples. The class value is
also assigned at random to the pruning examples. In our third analysis it is assumed that
each pruning example has an equal chance to end up in any one of the subtrees of the tree
being pruned. This rather theoretical setting lets us take into account those subtrees that
do not receive any examples. They have been left without attention in earlier analyses.
The rest of this paper is organized as follows. The next section discusses the dierent
versions of the

rep

algorithm and xes the one that is analyzed subsequently. In Section

3 we review earlier analyses of

rep.

Basic algorithmic properties of

Section 4. Then, in Section 5, we carry out a probabilistic analysis of
any assumptions about the distribution of examples.

rep are examined in
rep, without making

We derive a bound for the pruning

probability of a tree which depends exponentially on the relation of the number of pruning
examples and the size of the tree. Section 6 presents an analysis, which assumes that the
pruning examples distribute uniformly to the subtrees of the tree. This assumption lets us
sharpen the preceding analysis on certain aspects. However, the bounds of Section 5 hold
with certainty, while those of Section 6 are approximate results. Further related research is
briey reviewed in Section 7 and, nally, in Section 8 we present the concluding remarks of
this study.

2. Reduced Error Pruning Algorithm

rep

was never introduced algorithmically by Quinlan (1987), which is a source of much

confusion. Even though

rep

is considered and appears to be a very simple, almost trivial,

algorithm for pruning, there are many dierent algorithms that go under the same name.
No consensus exists whether

rep is a bottom-up algorithm

or an iterative method. Neither

is it obvious whether the training set or pruning set is used to decide the labels of the leaves
that result from pruning.

2.1 High-Level Control
Quinlan's (1987, p. 225226) original description of

rep does not clearly specify the pruning

algorithm and leaves room for interpretation. It includes, e.g., the following characterizations.
For every non-leaf subtree

S

of

T

we examine the change in misclassications

over the test set that would occur if

S

were replaced by the best possible leaf.

If the new tree would give an equal or fewer number of errors and
subtree with the same property,

S is replaced by the leaf.

S contains no

The process continues

until any further replacements would increase the number of errors over the test
set.
[...] the nal tree is the most accurate subtree of the original tree with respect
to the test set and the smallest tree with that accuracy.
Quinlan (1987, p. 227) also later continues to give the following description.
This method [pessimistic pruning] has two advantages. It is much faster than
either of the preceding methods [cost-complexity and reduced error pruning]
since each subtree is examined at most once.

165

fiElomaa & Kriinen
On one hand this description requires the nodes to be processed in a bottom-up manner,
since subtrees must be checked for the same property before pruning a node but, on the
other hand, the last quotation would indicate

rep

rep

to be an iterative method.

We take

to have the following single-scan bottom-up control strategy like in most other studies

(Oates & Jensen, 1997, 1998, 1999; Esposito et al., 1993, 1997; Kearns & Mansour, 1998).
Nodes are pruned in a single bottom-up sweep through the decision tree, pruning each node is considered as it is encountered.

The nodes are processed in

postorder.
By this order of node processing, any tree that is a candidate for pruning itself cannot
contain a subtree that could still be pruned without increasing the tree's error.
Due to the ambiguity of
1989a; Mitchell, 1997).

rep's denition, a dierent version of rep also lives on (Mingers,

It is probably due to Mingers' (1989) interpretation of Quinlan's

ambiguous denition.
Nodes are pruned iteratively, always choosing the node whose removal most
increases the decision tree accuracy over the pruning set. The process continues
until further pruning is harmful.
However, this algorithm appears to be incorrect. Esposito et al. (1993, 1997) have shown
that a tree produced by this algorithm does not meet the objective of being the most accurate
subtree with respect to the pruning set.

Moreover, this algorithm overlooks the explicit

requirement of checking whether a subtree would lead to reduction of the classication
error.
Other iterative algorithms could be induced from Quinlan's original description. However, if the explicit requirement of checking whether a subtree could be pruned before pruning a supertree is obeyed, then these versions of

rep

will all reduce to the more ecient

bottom-up algorithm.

2.2 Leaf Labeling
Another source of confusion in Quinlan's (1987) description of

rep

is that it is not clearly

specied how to choose the labels for the leaves that are introduced to the tree through

training

pruning. Oates and Jensen (1999) interpreted that the intended algorithm would label the
new leaves according to the majority class of the

pruning

examples, but themselves analyzed

a version of the algorithm where the new leaves obtain as their labels the majority of the
examples. Oates and Jensen motivated their choice by the empirical observation

that in practice there is very little dierence between choosing the leaf labels in either way.
However, choosing the labels of pruned leaves according to the majority of pruning examples
will set such leaves into a dierent status than the original leaves, which have as their label
the majority class of training examples.

Example

Figure 1 shows a decision tree that will be pruned into a single leaf if the

training examples are used to label pruned leaves. A negative leaf replaces the root of the
tree and makes two mistakes on the pruning examples, while the original tree makes three
mistakes.

With this tree we can illustrate an important dierence in using training and

166

fiAn Analysis of Reduced Error Pruning





+



+



+

0/1

0/1

2/0

2/0

Figure 1: A (part of a) decision tree. The labels inside the nodes denote the majority classes
of training examples arriving to these nodes. For leaves the numbers of pruning
examples from the two classes are also given.

x=y

means that

x negative

and

y

positive instances reach the leaf.

pruning examples to label pruned leaves. Using training examples and proceeding bottomup, observe that neither subtree is pruned, since the left one replaced with a negative leaf
would make two mistakes instead of the original one mistake. Similarly, the right subtree
replaced with a positive leaf would result in an increased number of classication errors.
Nevertheless, the root node  even though its subtrees have not been pruned  can still be
pruned.
When pruning examples are used to label pruned leaves, a node with two non-trivial
subtrees cannot be pruned unless both its subtrees are collapsed into leaves.

The next

section will prove this. In the tree of Figure 1 both subtrees would be collapsed into zeroerror leaves. However, in this case the root node will not be pruned.

A further possibility for labeling the leaf nodes would be to take both training and
pruning examples into account in deciding the label of a pruned leaf.

Depending on the

relation of the numbers of training and pruning examples this strategy resembles one or the
other of the above-described approaches. Usually the training examples are more numerous
than the pruning examples, and will thus dominate. In practice it is impossible to discern
this labeling strategy from that of using the majority of training examples.

2.3 Empty Subtrees
Since

rep

uses dierent sets of examples to construct and to prune a decision tree, it is

possible that some parts of the tree do not receive any examples in the pruning phase. Such
parts of the decision tree, naturally, can be replaced with a single leaf without changing
the number of classication errors that the tree makes on the pruning examples. In other
words, subtrees that do not obtain any pruning examples are always pruned. Quinlan (1987)
already noted that the parts of the original tree that correspond to rarer special cases, which
are not represented in the pruning set, may be excised.

167

fiElomaa & Kriinen
DecisionTree REP( DecisionTree T, ExampleArray S )
{ for ( i = 0 to S.length-1 ) classify( T, S[i] );
return prune( T ); }
void classify( DecisionTree T, Example e )
{ T.total++; if ( e.label == 1 ) T.pos++;
// update node counters
if ( !leaf(T) )
if ( T.test(e) == 0 ) classify( T.left, e );
else classify( T.right, e ); }
int prune( DecisionTree T ) // Output classification error after pruning T
{ if ( leaf(T) )
if ( T.label == 1 ) return T.total - T.pos;
else return T.pos;
else
{ error = prune( T.left ) + prune( T.right );
if ( error < min( T.pos, T.total - T.pos ) )
return error;
else
{ replace T with a leaf;
if ( T.pos > T.total - T.pos )
{ T.label = 1; return T.total - T.pos; }
else
{ T.label = 0; return T.pos; } } } }
Table 1: The

rep

algorithm. The algorithm rst classies the pruning examples in a top-

down pass using method
tree using method

prune.

classify

and then during a bottom-up pass prunes the

Intuitively, it is not clear which is the best-founded strategy for handling

empty subtrees

,

those that do not receive any examples. On one hand they obtain support from the training
set, which usually is more numerous than the pruning set but, on the other hand, the fact
that no pruning example corresponds to these parts of the tree would justify drawing the
conclusion that these parts of the decision tree were built by chance properties of the training
data. In

rep,

consistently with preferring smaller prunings also otherwise, the latter view

is adopted.
The problem of empty subtrees is connected to the problem of
learning algorithms (Holte, Acker, & Porter, 1989).
number of the training examples.

small disjuncts

in machine

A small disjunct covers only a small

Collectively the small disjuncts are responsible for a

small number of classication decisions, but they accumulate most of the error of the whole
concept. Nevertheless, small disjuncts cannot be eliminated altogether, without adversely
aecting other disjuncts in the concept.

168

fiAn Analysis of Reduced Error Pruning
2.4 The Analyzed Pruning Algorithm
Let us briey reiterate the details of the

rep algorithm that is analyzed subsequently.

As al-

ready stated, the control strategy of the algorithm is the single-sweep bottom-up processing.
First, a top-down traversal drives the pruning examples through the tree to the appropriate
leaves. The counters of the nodes en route are updated. Second, during a bottom-up traversal the pruning operations indicated by the classication errors are executed.

The errors

can be determined on the basis of the node counter values. In the bottom-up traversal each
node is visited only once. The pruned leaves are labeled by the majority of the pruning set
(see Table 1).

3. Previous Work
Pruning of decision trees has recently received a lot of analytical attention; existing pruning
methods have been analyzed (Esposito et al., 1993, 1997; Oates & Jensen, 1997, 1998,
1999) and new analytically-founded pruning techniques have been developed (Helmbold &
Schapire, 1997; Pereira & Singer, 1999; Mansour, 1997; Kearns & Mansour, 1998).

Also

many empirical comparisons of pruning have appeared (Mingers, 1989a; Malerba, Esposito,
& Semeraro, 1996; Frank, 2000). In this section we review earlier work that concerns the

rep

algorithm. Further related research is considered in Section 7.

Esposito et al. (1993) viewed the
search process in the state space.

rep

algorithm, among other pruning methods, as a

In addition to noting that the iterative version of

rep

cannot produce the optimal result required by Quinlan (1987), they also observed that even
though
the tree

rep is a linear-time algorithm in the size of the tree, with respect to the height of
rep requires exponential time in the worst case. In their subsequent comparative

analysis Esposito et al. (1997) sketched a proof for Quinlan's (1987) claim that the pruning
produced by

rep

tree.
The bias of

is the smallest among the most accurate prunings of the given decision

rep was briey examined by Oates and Jensen (1997, 1998).

They observed

rL , of the best majority leaf that could replace a subtree T only depends on
(the class distribution of ) the examples that reach the root N of T . In other words, the tree
structure above T and N decides the error rL . Let rT denote the error of the subtree T at
the moment when the pruning sweep reaches N ; i.e., when some pruning may already have
taken place in T . All pruning operations performed in T have led either rT to decrease from
the initial situation or to stay unchanged. In any case, pruning that has taken place in T
potentially decreases rT , but does not aect rL . Hence, the probability that rT < rL  i.e.,
that T will not be pruned  increases through pruning in T . This error propagation bias
that the error,

is inherent to

rep.

Oates and Jensen (1997, 1998) conjecture that the larger the original

tree and the smaller the pruning set, the larger this eect, because a large tree provides
more pruning opportunities and the high variance of a small pruning set oers more random
chances for

rL  rT .

Subsequently we study some of these eects exactly.

In a follow-up study Oates and Jensen (1999) used

rep

as a vehicle for explaining the

problems that have been observed in the pruning phase of top-down induction of decision
trees. They analyzed

rep in a situation in which

the decision node under consideration ts

noise  i.e., when the class of the examples is independent of the value of the attribute tested
in the node at hand  and built a statistical model of

169

rep

in this situation. It indicates,

fiElomaa & Kriinen
consistently with their earlier considerations, that even though the probability of pruning a
node that ts noise prior to pruning beneath it is close to 1, pruning that occurs beneath the
node reduces its pruning probability close to 0. In particular, this model shows that if even
one descendant of node

N

at depth

d is not pruned, then N

will not be pruned (assuming

d +1). The consequence of this result is that increasing depth
d leads to an exponential decrease of the node's pruning probability.
there are no leaves until depth

The rst part of Oates and Jensen's (1999) analysis is easy to comprehend, but its significance is uncertain, because this situation does not rise in any bottom-up pruning strategy.
The statistical model is based on the assumption that the number,

n, of pruning instances

that pass through the node under consideration is large, in which case  independence assumptions prevailing  the errors committed by the node can be approximated by the normal
distribution. The expected error of the original tree is the mean of the distribution, while, if
pruned to a leaf, the tree would misclassify a proportion of the

n examples that corresponds

to that of the minority class. Oates and Jensen show that the latter number is always less
than the mean of the standard distribution of errors. Hence, the probability of pruning is
over 0.5 and approaches 1 as

n grows.

In the second part of the analysis, in considering the pruning probability of a node

N

after pruning has taken place beneath it, Oates and Jensen assume that the proportion of
positive examples in any descendant of
assuming further that

N

N

at depth

d is the same as in N .

In this setting,

d also have a
d are pruned, they are

has a positive majority, all its descendants at level

positive majority. It directly follows that if all descendants at level

all replaced by a positive leaf. Hence, the function represented by this pruning is identically
positive. The majority leaf that would replace
smaller than the above pruning. Therefore,

N

also represents the same function and is

rep will choose the single leaf pruning.

On the

N at depth d are not pruned, then the
N , in which these subtrees are maintained and all other nodes

other hand, if one or more of the descendants of
pruning of the tree rooted at
at level

d are

pruned into positive leaves, is more accurate than the majority leaf. In this

case the tree will not be pruned.
Oates and Jensen (1999) also assume that starting from any node at level

d the proba-

bility of routing an example to a positive leaf is the same. In the following analyses we try
to rid all unnecessary assumptions; the same results can be obtained without any knowledge
of the example distribution.

4. Basic Properties of

rep

Before going to the detailed probabilistic analysis of the
of its basic algorithmic properties.

rep

algorithm, we examine some

Throughout this paper we review the binary case for

simplicity. The results, however, also apply with many-valued attributes and several classes.
Now that the processing control of the

rep

algorithm has been settled, we can actually

prove Quinlan's (1987) claim of the optimality of the pruning produced by

rep.

Observe

that the following result holds true independent of the leaf labeling strategy.

Theorem 1 Applying rep with a set of pruning examples, S , to a decision tree T produces
T 0 a pruning of T such that it is the smallest of those prunings of T that have minimal
error with respect to the example set S.
170

fiAn Analysis of Reduced Error Pruning
Proof We prove the claim by induction over the size of the tree. Observe that a decision
T is a full binary tree, which has 2L(T ) 1 nodes, where L(T ) is the number of leaves

tree

in the tree.

Base case

.

L(T ) = 1,

If

then the original tree

T

consists of a single leaf node.

T

is

the only possible pruning of itself. Thus, it is, trivially, also the smallest among the most
accurate prunings of

T.

Inductive hypothesis
Inductive step L(T ) = k

. The claim holds when

.

. Let

N

right subtree, respectively. Subtrees
the pruning decision for

N

L(T ) < k.

T1

the left and the

must have strictly less than

When

be the root of the tree and

T0

and

T1

prunings of these trees.

T00

and

T10 ,

k leaves.

By the inductive hypothesis,

are the smallest possible among the most accurate

(i): Accuracy
N

and

is taken, then  by the bottom-up recursive control strategy of

rep  T0 and T1 have already been processed by the algorithm.
the subtrees after pruning,

T0

. The pruning decision for the node

N consists of choosing whether to collapse

and the tree rooted at it into a majority leaf, or whether to maintain the whole

tree. If both alternatives make the same number of errors, then

N

is collapsed and the

original accuracy with respect to the pruning set is retained. Otherwise, by the

rep

algorithm, the pruning decision is based on which of the resulting trees would make

S . Hence, whichever choice is made, the
T 0 will make the smaller number of errors with respect to S .
00
Let us now assume that a pruning T of T makes even less errors with respect to S
0
00
00
00
than T . Then T must consist of the root N and two subtrees T0 and T1 , because the
0
00
majority leaf cannot be more accurate than T . Since T is a more accurate pruning of
T than T 0 , it must be that either T000 is a more accurate pruning of T0 than T00 or T100 is
0
a more accurate pruning of T1 than T1 . By the inductive hypothesis both possibilities
0
are false. Therefore, T is the most accurate pruning of T .

less errors with respect to the pruning set
resulting tree

(ii): Size
0
T

. To see that the chosen alternative is also as small as possible, rst assume that

consists of a single leaf. Such a tree is the smallest pruning of

claim follows.

T00 and T10 .

0
Otherwise, T consists of the root node N

T , and in this case the

and the two pruned subtrees

Since this tree was not collapsed, the tree must be more accurate than the

tree consisting of a single majority leaf. Now assume that there exists a pruning

T

T 0 , but smaller. Because the majority leaf is less accurate
0



than T , T must consist of the root node N and two subtrees T0 and T1 . Then, either
T0 is a smaller pruning of T0 than T00 , but as accurate, or T1 is a smaller pruning of
T1 than T10 , but as accurate. Both cases contradict the inductive hypothesis. Hence,
T 0 is the smallest among the most accurate prunings of T .
of

T

that is as accurate as

Thus, in any case, the claim follows for

T.

2

We consider next the situation in an internal node of the tree, when the bottom-up
pruning sweep reaches the node.

From now on we are committed to leaf labeling by the

majority of the pruning examples.

171

fiElomaa & Kriinen

An internal node, which prior to pruning had no leaves as its children, will not
be pruned by rep if it has a non-trivial subtree when the bottom-up pruning sweep reaches
it.
Theorem 2
Proof

For an internal node

N

we have two possible cases in which it has non-trivial

subtrees; either both its subtrees are non-trivial (non-leaf ) or one of them is trivial. Let us
review these cases.
Let

rT

denote the error of (sub)tree

T

with respect to the part of the pruning set that

T . By rL we denote the misclassication rate of the majority leaf L that
T , if T was chosen to be pruned.

reaches the root of
would replace

Case I: Let the two subtrees of

T , T0 and T1 , be non-trivial.

Hence, both of them have been

rT0 < rL0 and rT1 < rL1 ,
T0 and T1 , respectively,
if pruned. Because rT = rT0 + rT1 , it must be that rT < rL0 + rL1 .
If T0 and T1 have the same the majority class, then it is also the majority class of T .
Then rL = rL0 + rL1 , where L is the majority leaf corresponding to T . Otherwise,
rL  rL0 + rL1 . In any case, rL  rL0 + rL1 . Combining this with the fact that
rT < rL0 + rL1 means that rT < rL . Hence, T is not pruned.

retained when the pruning sweep has passed them. Thus,

L0

where

Case II: Let

T

and

L1

are the majority leaves that would replace

have one trivial subtree, which was produced by pruning, and one non-

T0 is non-trivial and L1 is
T1 in the pruning process. Then, rT0 < rL0 . Hence,
we have that rT = rT0 + rL1 < rL0 + rL1 .
In the same way as in the Case I, we can deduce that rL  rL0 + rL1 . Therefore,
rT < rL and T will be retained in the pruned tree.

trivial subtree. We assume, without loss of generality, that
a majority leaf which has replaced

T

cannot be pruned in either case, and the pruning process can be stopped on the branch

T

containing
If node

unless an original leaf appears along the path from the root to

N

2

T.

has an original leaf, then it may be pruned even if the other subtree of

is non-trivial. Also when

N

N

has two trivial subtrees, it may be pruned. Whether pruning

takes place depends on the class distribution of examples reaching

N

and its subtrees.

In the analysis of Oates and Jensen (1999) it was shown that the prerequisite for pruning
a node

N

from the tree is that all its descendants at depth

d

have been pruned.

depth just above the rst (original) leaf in the subtree rooted at
result to this situation, we can corroborate their nding that
more of its descendants at depth

d are retained.

N

N.

d

is the

If we apply the above

will not be pruned if one or

Applying Theorem 2 recursively gives the

result.

A tree T rooted at node N will be retained by rep if one or more of the
descendants of N at depth d are not pruned.
Corollary 3

To avoid the analysis being restricted by the leaf globally closest to the root, we need to

fringe

be able to consider the set of leaves closest to the root on all branches of the tree. Let us
dene that the

of a decision tree contains any node that prior to pruning had a leaf

172

fiAn Analysis of Reduced Error Pruning

Figure 2: The fringe (black and gray nodes), interior (white nodes), and the safe nodes
(black ones) of a decision tree. The triangles denote non-trivial subtrees.

as its child. Furthermore, any node that is in a subtree rooted at a node belonging to the

interior

Safe nodes

fringe of the tree is also in the fringe. Those nodes not belonging to the fringe make up the
of the tree.

themselves belong to the fringe of the tree, but have their

parent in the interior of the tree (see Figure 2). Because the fringe of a decision tree is closed
downwards, the safe nodes of a tree correspond to the leaves of some pruning of it. Observe
also that along the path from the root to a safe node there are no leaves. Therefore, if the
pruning process ever reaches a safe node, Theorem 2 applies on the corresponding branch
from there on.
If the decision tree under consideration will be pruned into a single majority leaf, safe
nodes also need to be turned into leaves at some point, not necessarily simultaneously. If
the pruning sweep continues to the safe nodes, from then on the question whether a node is
pruned is settled solely on the basis of whether all nodes on the path to the root have the
same majority class. The pruning of the whole tree can be characterized as below.
Let

T

be the tree to be pruned and

S the set of pruning examples, jS j = n.

We assume,

without loss of generality, that at least half of the pruning examples are positive. Let
the proportion of positive examples in

S ; p  0:5.

If

T

p be

was to be replaced by a majority

leaf, that leaf would have a positive class label. Under these assumptions we can prove the
following.

A tree T will be pruned into a single leaf if and only if
 all subtrees rooted at the safe nodes of T are pruned and
 at least as many positive as negative pruning examples reach each safe node in T .

Theorem 4

173

fiElomaa & Kriinen
Proof

To begin we show that the two conditions are necessary for the pruning of

we show that if the former condition is not fullled, then
leaf. Second, we prove that neither will
latter not.
hold, then

T

T

T.

First,

cannot be pruned into a single

be pruned if the former condition holds, but the

Third, we show the suciency of the conditions; i.e., prove that if they both

T

will be pruned into a single leaf.

T

(i): Let us rst assume that in

there is a safe node

the denition of a safe node, the parent
Therefore, by Theorem 2,
neither will the root of

T

P

P

of

N

N

such that it will not be pruned. By

originally had no leaves as its children.

will not be pruned.

It is easy to see, inductively, that

be pruned.

(ii): Let us then assume that all subtrees rooted at safe nodes get pruned and that there are
one or more safe nodes in

T

into which more negative than positive pruning examples

fall. Observe that all safe nodes cannot be such. Let us now consider the pruning of

T

in which the leaves are situated in place of the safe nodes; the leaves receive the same
examples as the original safe nodes.

Because safe nodes are internal nodes, in

rep

the corresponding pruned leaves are labeled by the majority of the pruning examples.
In particular, the safe nodes that receive more negative than positive examples are
replaced by negative leaves. All other leaves are labeled positive. This pruning of the
original tree is more accurate than the majority leaf. Hence, by Theorem 1,
not prune

T

rep

will

into a single-leaf tree.

(iii): Let us now assume that all subtrees rooted at the safe nodes of

T

are pruned and that

at least as many positive as negative pruning examples reach each safe node. Then
all interior nodes must also have a majority of positive pruning examples. Otherwise,

T that has more negative than positive examples. Thus,
N has a majority of negative examples. Carrying the
induction all the way to the safe nodes shows that no such node N can exist in T .
Hence, all interior prunings of T represent the same function (identically positive) and
all of them have the same error with respect to S . The majority leaf is the unique,

there is an interior node

N

in

at least one of the children of

smallest of these prunings and will, by Theorem 1, be chosen.

2
5. A Probabilistic Analysis of

rep

Let us now turn our attention to the question of what the prerequisites for pruning a decision
tree

T

into a single majority leaf are. Since, by Theorem 1,

rep

produces a pruning of

T

which is the most accurate with respect to the pruning set and such that it is as small as
possible, to show that

T

does not reduce to a single leaf it suces to nd its pruning that

has a better prediction accuracy on the pruning examples than the majority leaf has.
In the following the class of an example is assumed to be independent of its attribute
values. Obviously, if in a decision tree there is a node where this assumption holds for the
examples arriving to it, we would like the pruning algorithm to turn it into a majority leaf.
We do not make any assumptions about the decision tree. However, similar to the analysis
of Oates and Jensen (1999), for the obtained bounds to be tight, the shortest path from the
root of the tree to a leaf should not be too short.

174

fiAn Analysis of Reduced Error Pruning
5.1 Probability Theoretical Preliminaries
Let us recall some basic probabilistic concepts and results that are used subsequently. We
denote the probability of an event

and p

X
X  B (n; p), if

(integer-valued) random variable
, denoted by

E

by

PrfE g

EE .

binomially distributed with parameters n
and its expectation by

is said to be

A discrete

!

n k
Prf X = k g =
p (1 p)n k ; k = 0; 1; : : : ; n:
k
If

X  B (n; p), then its expected
p value or mean is EX =  = np, variance varX = np(1 p),
 = np(1 p).

indicator variable

and standard deviation
An

1. An indicator variable
If

A1 ; : : : ; An

is is a discrete random variable that takes on only the values 0 and

I

is used to denote the occurrence or non-occurrence of an event.

Pn

are independent events with

X=

IA

PrfAi g = p and IA1 ; : : : ; IA

n

are the respective

i=1
Bernoulli
p
density function fX : IN ! [0; 1]
X
fX (x) = Prf X = x g
cumulative
distribution
function
F
:
IN
!
[0; 1]
X
P
indicator variables, then

IA

i

is called a

i

is binomially distributed with parameters

random variable with parameter

The

for a discrete random variable

.

n and p.

.

The

is dened as

FX (y) = Prf X  y g = xy fX (x).
Let X  B (n; p) be a random variable with mean  = np and standard
p
 = np(1 p). The normalized random variable corresponding to X is

for

X

is

dened as

X

Xe =





deviation

:

By the central limit theorem we can approximate the cumulative distribution function

e
of X

by the

normal Gaussian distribution
or

n

FXe (y) = Pr Xe  y

o

 (y):

 is the cumulative distribution function of the bell curve density function e
Respectively, we can apply the
able

X

normal approximation

FX (y) = Prf X  y g = FXe

x2 =2 =

FXe

p

2 .

to the corresponding random vari-

y 
y 


:



5.2 Bounding the Pruning Probability of a Tree
Now, the pruning set is considered to be a sample from a distribution in which the class
attribute is independent of the other attributes.

We assume that the class attribute is

(p) distribution; i.e., the class is positive with probability
p and negative with probability 1 p. We assume that p > 0:5.
distributed according to Bernoulli

In the following we will analyze the situation in which the subtrees rooted at safe nodes
have already been pruned into leaves. We bound the pruning probability of the tree starting
from this initial conguration.

Since the bottom-up pruning may already have come to

a halt before that situation, the following results actually give too high a probability for
pruning. Hence, the following upper bounds are not as tight as possible.

175

fiElomaa & Kriinen
We consider pruning a decision tree by

rep

as a trial whose result is decided by the set

all

of pruning examples. By Theorem 4 we can approximate the probability that a tree will
be pruned into a majority leaf by approximating the probability that

safe nodes get a

positive majority or a negative majority. The latter alternative is not very probable under
the assumption

p > :5.

It is safe to assume that it never happens.

We can consider sampling the pruning examples in two phases. First the attribute values
are assigned.

This decides the leaf into which the example falls.

In the second phase we

independently assign the class label for the example.

Z (T ) = fz1 ; : : : ; zk g and let the number of examples in
the pruning set S be jS j = n. The number of pruning examples falling to a safe node zi is
Pk
denoted by ni ;
i=1 ni = n. For the time being we assume that ni > 0 for all i. The number
of positive examples falling to safe node zi is the sum of independent Bernoulli variables
and, thus, it is binomially distributed with parameters ni and p. Respectively, the number
of negative pruning examples in safe node zi is Xi  B (ni ; 1
p). The probability that there
is a majority of negative examples in safe node zi is Prf Xi > ni =2 g. We can bound this
Let the safe nodes of tree

T

be

probability from below by using the following inequality (Slud, 1977).

Lemma 5 (Slud's inequality)
for m(1 q)  h  mq,

Let X  B(m; q) be a random variable with q  1=2. Then
!

h mq
:
Prf X  h g  1  p
mq(1 q)
p > :5 and the random variable corresponding to the number of negative examples
zi is Xi  B (ni; 1 p), the rst condition of Slud's inequality holds. Furthermore,
to see that condition m(1
q)  h  mq holds in safe node zi substitute h = ni =2, m = ni ,
and q = 1
p to obtain ni p  ni=2  ni(1 p). Thus,
Since

in safe node



Pr Xi >
As

ni 

2

!

!

=2 ni(1 p)
= 1  p(p 1=2)ni :
 1  nip
ni p(1 p)
ni p(1 p)

(1)

ni , the number of pruning instances reaching safe node zi , grows, then the standard

normal distribution term in the above bound also grows. Hence, the bound on the probability
that the majority of the pruning examples reaching

zi

is negative is the smaller the more

pruning examples reach it. The probability of a negative majority also reduces through the
growing probability of positive class for an example,

p.

These both are also reected in the

pruning probabilities of the whole tree.
We can now roughly approximate the probability that
majority leaf as follows. By Theorem 4,
node in
are

T

T

T

will be pruned into a single

will be pruned into a leaf if and only if each safe

receives a majority of positive examples. Because

T

has

k

safe nodes and there

n pruning examples, then according to the pigeon-hole principle at least half of the safe
r = 2n=k examples. Each safe node zi with ni  r examples has, by

nodes receive at most

Inequality 1, a negative majority at least with probability

!

1  p(p 1=2)r :
rp(1 p)
176

fiAn Analysis of Reduced Error Pruning
Observe that Inequality 1 also holds when

ni < r, becausepthe cumulative

distribution

 is an increasing function. The argument ni (p 1=2)= ni p(1 p) canpbe rewritten
p
as
ni cp , where cp ispa positive constant depending on the value of p. Since ( ni cp ) grows
as ni grows, 1
( ni cp ) grows with decreasing ni . Hence, the lower bound of Inequality
1 also applies for values 0 < ni < r .

function

Thus, the probability that the half of the safe nodes that receive at most

r

examples

have a positive majority is at most

 p(p 1=2)r
rp(1 p)

!!k=2

:

(2)

This is an upper bound for the probability that the whole tree

T

will be pruned into a single

leaf. The only distribution assumption that was made to reach the result is that

p > :5.

order to obtain tighter bounds, one has to make assumptions about the shape of the tree
and the distribution of examples.
The bound of Equation 2 depends on the size of the decision tree (reected by

n

p

In

T

k), the

number ( ) and the class distribution ( ) of the pruning examples. Keeping other parameters
constant and letting

k

grow reduces the pruning probability exponentially. If the number

of pruning examples grows in the same proportion so that

r = 2n=k

stays constant, the

pruning probability still falls exponentially. Class distribution of the pruning examples also
aects the pruning probability which is the smaller, the closer

p is to value .5.

5.3 Implications of the Analysis
It has been empirically observed that the size of the decision tree grows linearly with the
training set size, even when the trees are pruned (Catlett, 1991; Oates & Jensen, 1997,
1998). The above analysis gives us a possibility to explain this behavior. However, let us
rst prove that when there is no correlation between the attribute values and the class label
of an example, the size of the tree that perfectly ts the training data depends linearly on
the size of the sample.
Our setting is as simple as can be. We only have one real-valued attribute
attribute

y, whose value is independent

of that of

x.

As before,

y

x and the class

has two possible values,

0 and 1. The tree is built using binary splits of a numerical value range; i.e., propositions
of type 

x < r

are assigned to the internal leaves of the tree.

In this analysis duplicate

instances occur with probability 0.

Let the training examples (x; y) be drawn from a distribution, where x is uniformly distributed in the range [0; 1) and y obtains value 1, independent of x, with probability
p, and value 0 with probability 1 p. Then the expected size of the decision tree that ts the
data is linear in the size of the sample.
Theorem 6

S = h(x1 ; y1 ); : : : ; (xt ; yt )i be a sample of the above described distribution. We
xi 6= xj , when i 6= j , because the probability of the complement event is 0.
Let us, further, assume that the examples of S have been indexed so that x1 < x2 < : : : < xt .
Let Ai be the indicator variable for the event that instances i and i + 1 have dierent class
labels; i.e., yi 6= yi+1 , 1  i  t 1. Then EAi = Prf Ai = 1 g = p(1 p)+(1 p)p = 2p(1 p),
Proof

Let

may assume that

177

fiElomaa & Kriinen
yi = 1 has probability p, at the same time the event yi+1 = 0 has
1 p, and vice versa. Now the number of class alternations is A = Pti=11 Ai and

because when the event
probability

its expectation is

EA =
Let

T

t 1
X
i=1

EAi =

t 1
X
i=1

2p(1 p) = 2p(1 p)

t 1
X
i=1

1 = 2(t 1)p(1 p):

be a decision tree that has been grown on the sample

continued until the training error is 0. Each leaf in

[a; b) in [0; 1).

If

yi 6= yy+1 ,

then

xi

and

xi+1

T

S.

(3)

The growing has been

corresponds to a half open interval

must fall into dierent leaves of

T,

because

T . Thus, the upper boundary b of
xi falls in must have a value less than xi+1 .

otherwise one or the other example is falsely classied by
the interval corresponding to the leaf into which

Repetitively applying this observation when scanning through the examples from left to

T must at least have one leaf for x1 and one leaf for each class alternation;
A + 1 leaves in total. By using Equation 3 we see that the expected number of leaves

right, we see that
i.e.,
in

T

is

EA + 1 = 2(t 1)p(1 p) + 1:
In particular, this is linear in the size of the sample S ; jS j = t.

2

The above theorem only concerns zero training error trees built in the rst phase of
decision tree induction. The empirical observations of Catlett (1991) and Oates and Jensen
(1997, 1998), however, concern decision trees that have been pruned in the second phase of
induction. We come back to the topic of pruned trees shortly.
Consider how

rep is used in practice.

There is some amount of (classied) data available

from the application domain. Let there be a total of

t examples available. Some part ff of
1 ff of it is reserved as the

the data is used for tree growing and the remaining portion
separate pruning set;

0 < ff < 1.

Quite a common practice is to use two thirds of the data

for growing and one third for pruning or nine tenths for growing and one tenth for pruning
when (ten-fold) cross-validation is used. In the decision tree construction phase the tree is
tted to the

fft examples as perfectly as possible.

If we hypothesize that the previous result

holds for noisy real-world data sets, which by empirical evidence would appear to be the
case, and that the number of safe nodes also grows linearly with the number of leaves, then
the tree grown will contain

t

safe nodes, where

 > 0. Since the pruning set size also is
r = 2n=k stays constant in this setting.

a linear fraction of the training set size, the ratio

Hence, by Equation 2, the growing data set size forces the pruning probability to zero, even
quite fast, because the reduction in the probability is exponential.

5.4 Limitations of the Analysis
Empty subtrees, which do not receive any pruning examples, were left without attention
above; we assumed that

ni > 0

for each

i.

Empty subtrees, however, decisively aect the

analysis; they are automatically pruned away. Unfortunately, one cannot derive a non-trivial
upper bound for the number of empty subtrees. In the worst case all pruning examples are
routed to the same safe node, which leaves

k 1 empty safe nodes to the tree.

Subsequently

we review the case where the examples are distributed uniformly to the safe nodes. Then
better approximations can be obtained.

178

fiAn Analysis of Reduced Error Pruning
Even though we assume that each pruning example is positive with a higher probability
than .5, there are no guarantees that the majority of all examples is positive.

However,

the probability that the majority of all examples changes is very small, even negligible, by
Cherno 's inequality (Cherno, 1952; Hagerup & Rb, 1990) when the number of pruning

n, is high and p is not extremely close to one half.
Prf X  h g, but above we used it to bound
the probability Prf X > h g. Some continuity correction could be used to compensate this.
examples,

Slud's inequality bounds the probability

In practice, the inexactness does not make any dierence.
Even though it would appear that the number of safe nodes increases in the same proportion as that of leaves when the size of the training set grows, we have not proved this
result. Theorem 6 essentially uses leaf nodes, and does not lend itself to modication, where
safe nodes could be substituted in place of leaves.
The relation between the number of safe nodes and leaves in a decision tree depends on
the shape of the tree. Hence, the splitting criterion that was used in tree growing decisively
aects this relation. Some splitting criteria aim at keeping the produced split as balanced as
possible, while others aim at separating small class coherent subsets from the data (Quinlan,
1986; Mingers, 1989b). For example, the common entropy-based criteria have a bias that
favors balanced splits (Breiman, 1996). Using a balanced splitting criterion would seem to
imply that the number of safe nodes in a tree depends linearly on the number of leaves in
the tree.

In that case the above reasoning would explain the empirically observed linear

growth of pruned decision trees.

6. Pruning Probability Under Uniform Distribution
We now assume that all
of the

k

n

pruning examples have an equal probability to end up in each

safe nodes; i.e., a pruning example falls to the safe node

zi

with probability

1=k.

Contrary to the normal uniform distribution assumption analysis, for our analysis this is not
the best case. Here the best distribution of examples into safe nodes would have one pruning
example in each of the safe nodes except one, into which all remaining pruning instances
would gather.

Nevertheless, the uniformity lets us sharpen the general approximation by

using standard techniques.

n=k. Let us calculate
cn=k examples, where c is an
for the event safe node zi receives at

The expected number of examples falling into any safe node is
the expected number of those safe nodes that receive at most

QPi be the indicator
k Q is the number of those safe nodes that receive less
i=1 i
Pk
than
By the linearity of expectation EQ =
i=1 EQi = kEQ1 , in which
the last equality follows from the fact that the Qi -s are identically distributed.
Let Y1 be the number of examples reaching safe node z1 . Because each of the n examples reaches z1 with probability 1=k independent of the other examples, Y1 is binomially
distributed with parameters n and 1=k . Clearly EQ1 = Prf Y1  cn=k g. We can approxiarbitrary positive constant. Let
most

cn=k examples.
cn=k examples.

Then

Q=

mate the last probability by the normal approximation, from which we obtain

!
!


cn=k
n=k
(
c
1)
n=k
cn
  pn  1=k  (1 1=k) =  pn=k(1 1=k) :
Pr Y1 
k
179

fiElomaa & Kriinen
Hence, by the above observation,

!

(c 1)n=k :
EQ = kEQ1  k p
n=k(1 1=k)
T

(4)

We now use Approximation 4 to determine the probability that the whole decision tree
will be pruned into a single leaf. Let

denote by

T

P

be a random variable that represent the number

cn=k examples and at least one example. If we
R the number of empty safe nodes, we have P = Q R. Hence, EP = E(Q R) =

of those safe nodes in

that receive at most

EQ ER.

The following result (Kamath, Motwani, Palem, & Spirakis, 1994; Motwani & Raghavan,
1995) lets us approximate the number of empty safe nodes when

Theorem 7

bins. Then

n  k.

Let Z be the number of empty bins when m balls are thrown randomly into h


 = EZ = h 1

and for  > 0,

Prf jZ

1 m  he

h

j   g  2 exp

m=h

!

2 (h 1=2)
:
h2 2

By this result the expected number of empty safe nodes is approximately
number is small when

ke

n=k ;

this

k is relatively small compared to n.
EQ (Equation 4) and using the pre-

Substituting the above obtained approximation for
vious result, we get

(c 1)n=k
EP = EQ ER  k  p
n=k(1 1=k)

!

e

n=k

!

:

Applying Slud's inequality we can, as before, bound from above the probability that
the majority class does not change in a safe node that receives
Since there are

P

cn=k

pruning examples.

such safe nodes and the class distribution of examples within them is

independent, the event majority class does not change in any safe node that receives at
least one and at most

cn=k examples

has the upper bound

 p(p :5)r
rp(1 p)
where

r = cn=k.

Replacing

P

!!P

;

(5)

with its expected value in this equation we have an approxi-

mation for the pruning probability. This approximation is valid if
from its expected value. We consider the deviation of

P

P

does not deviate a lot

from its expected value below.

The above upper bound for the pruning probability is similar to the upper bound that
was obtained without any assumptions about the distribution of the examples. However, the
earlier constant 2 has been replaced by a new, controllable parameter
are now explicitly taken into account. If

c, and empty subtrees

c is chosen suitably, this upper bound is more strict

than the one obtained in the general case.

180

fiAn Analysis of Reduced Error Pruning

Upper bound for the pruning probability
0.5
0.25

1

0.5

0

0.9
0.8
0.7

0.5
1

Figure 3: The eect of parameters

p

0.6

1.5

c

0.5

p and c on the upper bound of the pruning probability

of a tree with 100 safe nodes when 500 pruning examples are used. The curves
depicting the 0.25 and 0.5 upper bounds are also shown.

6.1 An Illustration of the Upper Bound
Figure 3 plots the upper bound of the pruning probability of a tree with 100 safe nodes when
500 pruning examples are used. The value of the parameter

c varies from 0 to 2 and p varies

from 0.5 to 1. We can observe that the surface corresponding to the upper bound stays very
close to 0 when the class distribution is not too skewed and when the parameter

c does not

have a very small value. When the probability of an example having a positive class label

c approaches 0, the upper bound climbs very steeply. At least
parameter c this is due to the inexactness of the approximation on the

hits value 0.75 or the value of
on the part of the
extreme values.
When the probability

p

that an example has a positive class approaches 1, the error

committed by a single positive leaf falls to 0. Hence, the accuracy of a non-trivial pruning
has to be better, the closer

p is to 1 for it to beat the majority leaf.

Intuitively, the probability

that such a pruning exists  i.e., that the root node is not pruned  should drop to zero as

p increases.

The bound reects this intuition.

When the value of parameter

c falls close to 0, the safe nodes that are taken into account

in the upper bound only receive very few pruning examples. The number of such nodes is

181

fiElomaa & Kriinen
small. On the other hand, when

c

is increased, the number of nodes under consideration

grows together with the upper limit on the number of examples reaching each single one of
them. Thus, both small and large values of
the value of

c is somewhere

c

yield loose bounds. In the strictest bounds

in the middle, in our example around values 1.01.5. In the

bound of Equation 5 the argument of the cumulative distribution function
zero when the value of

c is very small,

 tends towards

but at the same time the exponent decreases. The

 approaches 1/2, when its argument goes to zero. On the other hand, when c has
a large value,  approaches value 1 and the exponent P also increases.
value of

6.2 On the Exactness of the Approximation
Above we used the expected value of P in the analysis; EP = EQ
ER. We now probe into
the deviation of P from its expected value. The deviation of R is directly available from
Theorem 7:

For

!

2 (k 1=2)
:
k2 E2 R

Prf jR ERj   g  2 exp

Q we do not have a similar result yet.

Lipschitz condition

In this section we provide one.

Let us rst recapitulate the denition of the

.

f : D1      Dm ! IR be a real-valued function with m arguments from
f is said to satisfy the Lipchitz condition if for any
x1 2 D1 ; : : : ; xm 2 Dm , any i 2 f1; : : : ; mg, and any yi 2 Di ,
Denition

Let

possibly distinct domains. The function

jf (x1 ; : : : ; xi 1 ; xi ; xi+1; : : : ; xm ) f (x1; : : : ; xi 1 ; yi; xi+1 ; : : : ; xm )j  1:

Hence, a function satises the Lipschitz condition if an arbitrary change in the value of
any one argument does not change the value of the function more than 1.

martingales

The following result (McDiarmid, 1989) holds for functions satisfying the Lipschitz condition. More general results of the same kind can be obtained using
(Motwani & Raghavan, 1995)).

(see e.g.,

Theorem 8 (McDiarmid) Let X1 ; : : : ; Xm be independent random variables taking values
in a set V . Let f : V m ! IR be such that, for i = 1; : : : ; m:

sup

x1 ;:::;xm ;yi 2V

jf (x1; : : : ; xi 1; xi ; xi+1; : : : ; xm ) f (x1; : : : ; xi 1 ; yi; xi+1 ; : : : ; xm )j  ci :

Then for  > 0,
Prf jf (X1 ; : : : ; Xm ) Ef (X1 ; : : : ; Xm )j   g  2 exp

2
P2

m c2
i=1 i

!

:

Wi , i = 1; : : : ; n, be a random variable such that Wi = j if the i-th example is
directed to the safe node zj . By the uniform distribution assumption Wi -s are independent.
They have their values within the set f1; : : : ; k g. Let us dene the function f so that
f (w1 ; : : : ; wn ) is the number of those safe nodes that receive at most r = cn=k examples,
when the i-th example is directed to the safe node zw . That is,
Let

i

f (w1 ; : : : ; wn ) = jf i 2 f 1; : : : ; k g j jSi j  r gj;
182

fiAn Analysis of Reduced Error Pruning
where

Si is the set of those examples that are directed to safe node zi ;
Si = f h 2 f 1; : : : ; n g j wh = i g:
Q = f (W1 ; : : : ; Wn ).

Hence,

Moving any one example from one safe node to another (chang-

ing the value of any one argument
dition
of

f

Pn

wi ), can change one more safe node zi

jSij  r, one less safe node to fulll it, or both at the same time.

to fulll the conThus, the value

changes by at most 1. Hence, the function fullls the Lipschitz condition. Therefore,

we can apply McDiarmid's inequality to it by substituting

c2 = n:

i=1 i

ci = 1 and observing that then

2
Prf jf (W1 ; : : : ; Wn ) Ef (W1 ; : : : ; Wn )j   g  2e 2 =n ;
or equally

2
Prf jQ EQj   g  2e 2 =n :

Unfortunately, this concentration bound is not very tight. Nevertheless, combining the
concentration bounds for

Q and R we have for P

the following deviation from its expected

value.

Since jP
EP j = jQ R E(Q R)j = jQ EQ + ER Rj  jQ EQj + jR ERj,
jQ R E(Q R)j   implies that jQ EQj  =2 or jR ERj  =2. Thus,
Prf jP EP j   g = Prf jQ R E(Q R)j   g




 Pr jQ EQj  2 + Pr jR ERj  2

!

2
2n + 2 exp

 2 exp

!

2 (k 1=2)
:
4(k2 E2 R)

7. Related Work
Traditional pruning algorithms  like cost-complexity pruning (Breiman et al., 1984), pessimistic pruning (Quinlan, 1987), minimum error pruning (Niblett & Bratko, 1986; Cestnik
& Bratko, 1991), critical value pruning (Mingers, 1989a), and error-based pruning (Quinlan,
1993)  have already been covered extensively in earlier work (Mingers, 1989a; Esposito
et al., 1997; Frank, 2000). Thus we will not touch on these methods any further. Instead,
we review some of the more recent work on pruning.

rep

produces an optimal pruning of the given decision tree with respect to the pruning

set. Other approaches for producing optimal prunings have also been presented (Breiman
et al., 1984; Bohanec & Bratko, 1994; Oliver & Hand, 1995; Almuallim, 1996). However,
often optimality is measured over the training set. Then it is only possible to maintain the
initial accuracy, assuming that no noise is present. Neither is it usually possible to reduce
the size of the decision tree without sacricing the classication accuracy. For example, in
the work of Bohanec and Bratko (1994) it was studied how to eciently nd the optimal
pruning in the sense that the output decision tree is the smallest pruning which satises
a given accuracy requirement. A somewhat improved algorithm for the same problem was
presented subsequently by Almuallim (1996).

183

fiElomaa & Kriinen
The high level control of Kearns and Mansour's (1998) pruning algorithm is the same

cost-complexity

bottom-up sweep as in

rep.

However, the pruning criterion in their method is a kind of a

condition (Breiman et al., 1984) that takes both the observed classication

error and (sub)tree complexity into account.

Moreover, their pruning scheme does not

pessimistic

require the pruning set to be separate from the training set. Both Mansour's (1997) and
Kearns and Mansour's (1998) algorithms are
of a (sub)tree by its training error.

: they try to bound the true error

Since the training error is by nature optimistic, the

pruning criterion has to compensate it by being pessimistic about the error approximation.
Consider yet another variant of

rep, one which

is otherwise similar to the one analyzed

above, with the exception that the original leaves are not put to a special status, but can
be relabeled by the majority of the pruning examples just like internal nodes. This version
of

rep

produces the optimal pruning with respect to which the performance of Kearns and

Mansour's (1998) algorithm is measured. Their pessimistic pruning produces a decision tree
that is smaller than that produced by

rep.

Kearns and Mansour (1998) are able to prove that their algorithm has a strong performance guarantee. The generalization error of the produced pruning is bounded by that of
the best pruning of the given tree plus a complexity penalty.
local in the same sense as those of

rep

The pruning decisions are

and only the basic pruning operation of replacing a

subtree with a leaf is used in this pruning algorithm.

8. Conclusion
In this paper the

rep

algorithm has been analyzed in three dierent settings.

First, we

rep alone, without assuming anything about the input
this setting it is possible to prove that rep fullls its

studied the algorithmic properties of
decision tree nor pruning set.

In

intended task and produces an optimal pruning of the given tree. The algorithm proceeds
to prune the nodes of a branch as long as both subtrees of an internal node are pruned and
stops immediately if even one subtree is kept. Moreover, it prunes an interior node only if
all its descendants at level

d

have been pruned. Furthermore,

rep

either halts before the

safe nodes are reached or prunes the whole tree only in case all safe nodes have the same
majority class.
In the second setting the tree under consideration was assumed to t noise; i.e., it
was assumed that the class label of the pruning examples is independent of their attribute
values. In this setting the pruning probability of the tree could be bound by an equation
that depends exponentially on the size of the tree and linearly on the number and class
distribution of the pruning examples. Thus, our analysis corroborates the main nding of
Oates and Jensen (1999) that

rep fails to control the growth of a decision tree in the extreme

case that the tree ts pure noise. Moreover, our analysis opened a possibility to initially
explain why the learned decision tree grows linearly with an increasing data set. Our bound
on the pruning probability of a tree is based on bounding the probability that all safe nodes
have the same majority class. Surprisingly, essentially the same property, whose probability
we try to bound close to 0, is assumed to hold with probability 1 in the analysis of Oates
and Jensen (1999).
In

rep

it may happen that no pruning examples are directed to a given subtree. Such

subtrees have not been taken into account in earlier analyses.

184

In our nal analysis we

fiAn Analysis of Reduced Error Pruning
included empty subtrees in the equation for a tree's pruning probability.

Taking empty

subtrees into account gives a more realistic bound for the pruning probability of a tree.
Unfortunately, one cannot draw very denite general conclusions on the two-phased topdown induction of decision trees on the basis of analyses on the
bias is quite unique among pruning algorithms.

The fact that

rep algorithm, because its
rep does not penalize the

size of a tree, but only rests on the classication error on the pruning examples makes the
method sensitive to small changes in the class distribution of the pruning set. Other decision
tree pruning algorithms also have their individual characteristics. Therefore, unied analysis
of decision tree pruning may be impossible.
The version of

rep,

in which one is allowed to relabel original leaves, as well, is used

as the performance objective in Kearns and Mansour's (1998) pruning algorithm.

Thus,

the performance of pruning algorithms that use both error and size penalty is related to
those that use only error estimation. In the version of

rep

used by Kearns and Mansour

our analysis based on safe nodes applies with leaves in place of safe nodes. Hence for this
algorithm the derived bounds are stricter.
We leave the detailed analysis of other important pruning algorithms as future work.
Only through such investigation is it possible to disclose the dierences and similarities of
pruning algorithms.

Empirical examination has not managed to reveal clear performance

dierences between the methods.

Also, the relationship of the number of safe nodes and

leaves of a tree ought to be examined analytically and empirically. In particular, one should
study whether the number of safe nodes does increase linearly with a growing training set,
as conjectured in this paper. Deeper understanding of existing pruning algorithms may help
to overcome the problems associated with the pruning phase of decision tree learning.

References

Intelligence 83

Almuallim, H. (1996). An ecient algorithm for optimal pruning of decision trees.
,

Learning 15

, 347362.

Bohanec, M., & Bratko, I. (1994). Trading accuracy for simplicity in decision trees.
,

(3), 223250.

Regression Trees

Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984).
. Wadsworth, Pacic Grove, CA.

tional Joint Conference on Articial Intelligence

Machine

Classication and

Machine Learning 24
Proceedings of the Twelfth Interna-

Breiman, L. (1996). Some properties of splitting criteria.
Catlett, J. (1991). Overpruning large decision trees. In

Articial

,

(1), 4147.

, pp. 764769, San Mateo, CA. Morgan

Kaufmann.

Machine LearningEWSL-91: Proceedings of the Fifth European Working
Lecture Notes in Articial Intelligence

Cestnik, B., & Bratko, I. (1991). On estimating probabilities in tree pruning. In Kodrato,

Session

Y. (Ed.),

, Vol. 482 of

, pp. 138150, Berlin, Hei-

delberg, New York. Springer-Verlag.

Annals of Mathematical Statistics 23

Cherno, H. (1952). A measure of asymptotic eciency for tests of a hypothesis based on
the sum of observations.

,

185

(4), 493507.

fiElomaa & Kriinen

Proceedings of the Thirteenth International Joint Conference on Articial

Cohen, W. W. (1993).

Intelligence

systems. In

Ecient pruning methods for separate-and-conquer rule learning

, pp. 988994, San Mateo, CA. Morgan Kaufmann.

Machine Learning: ECML-93, Proceedings of
Lecture Notes in Articial Intelligence

Esposito, F., Malerba, D., & Semeraro, G. (1993).

the Sixth European Conference

the state space. In Brazdil, P. B. (Ed.),

Decision tree pruning as a search in

, Vol. 667 of

, pp.

165184, Berlin, Heidelberg, New York. Springer-Verlag.

IEEE Transactions on Pattern Analysis and Machine Intelligence 19
Pruning Decision Trees and Lists
Information Processing
Letters 33
Machine Learning 27
Proceedings of the Eleventh International Joint Conference on Articial
Intelligence
Proceedings of the Thirty-Fifth Annual
IEEE Symposium on Foundations of Computer Science

Esposito, F., Malerba, D., & Semeraro, G. (1997). A comparative analysis of methods for
pruning decision trees.
,

(5), 476491.

Frank, E. (2000).

. Ph.D. thesis, University of Waikato,

Department of Computer Science, Hamilton, New Zealand.

Hagerup, T., & Rb, C. (1990). A guided tour of Cherno bounds.
,

(6), 305308.

Helmbold, D. P., & Schapire, R. E. (1997). Predicting nearly as well as the best pruning of
a decision tree.

,

(1), 5168.

Holte, R. C., Acker, L., & Porter, B. (1989).

Concept learning and the problem of small

disjuncts. In

, pp. 813818, San Mateo, CA. Morgan Kaufmann.

Kamath, A., Motwani, R., Palem, K., & Spirakis, P. (1994).

Tail bounds for occupancy

and the satisability threshold conjecture. In

, pp. 592603, Los Alamitos,

CA. IEEE Press.

Proceedings of the Fifteenth Inter-

Kearns, M., & Mansour, Y. (1998). A fast, bottom-up decision tree pruning algorithm with

national Conference on Machine Learning

near-optimal generalization. In Shavlik, J. (Ed.),

, pp. 269277, San Francisco, CA. Morgan

Kaufmann.

Learning from

Malerba, D., Esposito, F., & Semeraro, G. (1996). A further comparison of simplication

Data: AI and Statistics V
Proceedings of the Fourteenth International Conference on Machine Learning

methods for decision-tree induction. In Fisher, D., & Lenz, H.-J. (Eds.),

, pp. 365374, Berlin, Heidelberg, New York. Springer-Verlag.

Mansour, Y. (1997). Pessimistic decision tree pruning based on tree size. In Fisher, D. H.
(Ed.),

,

pp. 195201, San Francisco, CA. Morgan Kaufmann.

Surveys in Combinatorics: Invited Papers of the 12th British Combinatorial Conference
Machine Learning 4
Machine Learning 3
Machine Learning

McDiarmid, C. J. H. (1989). On the method of bounded dierences. In Siemons, J. (Ed.),
, pp. 148188, Cambridge, U.K. Cambridge University Press.

Mingers, J. (1989a). An empirical comparison of pruning methods for decision tree induction.
,

(2), 227243.

Mingers, J. (1989b). An empirical comparison of selection measures for decision-tree induction.

Mitchell, T. M. (1997).

,

(4), 319342.

. McGraw-Hill, New York.

186

fiAn Analysis of Reduced Error Pruning
Motwani, R., & Raghavan, P. (1995).
New York.

Randomized Algorithms

. Cambridge University Press,

Research and Development in Expert Systems III

Niblett, T., & Bratko, I. (1986). Learning decision rules in noisy domains. In Bramer, M. A.
(Ed.),

, pp. 2534, Cambridge, UK.

Cambridge University Press.

Proceedings of the Fourteenth International Conference on

Oates, T., & Jensen, D. (1997). The eects of training set size on decision tree complexity.

Machine Learning

In Fisher, D. H. (Ed.),

, pp. 254261, San Francisco, CA. Morgan Kaufmann.

Oates, T., & Jensen, D. (1998). Large datasets lead to overly complex models: An expla-

Proceedings of the Fourth International Conference on Knowledge Discovery and Data
Mining
Proceedings of the Sixteenth National Conference on Articial Intelligence
nation and a solution.

In Agrawal, R., Stolorz, P., & Piatetsky-Shapiro, G. (Eds.),

, pp. 294298, Menlo Park, CA. AAAI Press.

Oates, T., & Jensen, D. (1999).

Toward a theoretical understanding of why and when

decision tree pruning algorithms fail. In

, pp. 372378, Menlo Park, CA/Cambridge, MA. AAAI

Press/MIT Press.

Proceedings of the Twelfth International Conference on Machine
Machine

Oliver, J. J., & Hand, D. J. (1995). On pruning and averaging decision trees. In Prieditis, A.,

Learning
Learning 5

& Russell, S. (Eds.),

, pp. 430437, San Francisco, CA. Morgan Kaufmann.

Pagallo, G., & Haussler, D. (1990). Boolean feature discovery in empirical learning.
,

(1), 7199.

Machine Learning 36

Pereira, F., & Singer, Y. (1999). An ecient extension to mixture techniques for prediction
and decision trees.

,

(3), 183199.

Machine Learning 1
International Journal of Man-Machine
C4.5: Programs for Machine Learning
The Annals of Probability

Quinlan, J. R. (1986). Induction of decision trees.

Studies 27

Quinlan, J. R. (1987).
,

,

, 81106.

Simplifying decision trees.

(3), 221248.

Quinlan, J. R. (1993).

.

Morgan Kaufmann, San

Slud, E. V. (1977). Distribution inequalities for the binomial law.

,

Mateo, CA.

5

(3), 404412.

187

fiJournal of Artificial Intelligence Research 15 (2001) 351-381

Submitted 9/00; published 11/01

Experiments with Infinite-Horizon, Policy-Gradient Estimation
Jonathan Baxter

JBAXTER @ WHIZBANG . COM

WhizBang! Labs.
4616 Henry Street Pittsburgh, PA 15213

Peter L. Bartlett

BARTLETT @ BARNHILLTECHNOLOGIES . COM

BIOwulf Technologies.
2030 Addison Street, Suite 102, Berkeley,CA 94704

Lex Weaver

L EX .W EAVER @ ANU . EDU . AU

Department of Computer Science
Australian National University , Canberra 0200, Australia

Abstract
In this paper, we present algorithms that perform gradient ascent of the average reward in a partially observable Markov decision process (POMDP). These algorithms are based on GPOMDP,
an algorithm introduced in a companion paper (Baxter & Bartlett, 2001), which computes biased
estimates of the performance gradient in POMDPs. The algorithms chief advantages are that it
uses only one free parameter fi 2 [0; 1), which has a natural interpretation in terms of bias-variance
trade-off, it requires no knowledge of the underlying state, and it can be applied to infinite state,
control and observation spaces. We show how the gradient estimates produced by GPOMDP can
be used to perform gradient ascent, both with a traditional stochastic-gradient algorithm, and with
an algorithm based on conjugate-gradients that utilizes gradient information to bracket maxima in
line searches. Experimental results are presented illustrating both the theoretical results of Baxter
and Bartlett (2001) on a toy problem, and practical aspects of the algorithms on a number of more
realistic problems.

1. Introduction
Function approximation is necessary to avoid the curse of dimensionality associated with largescale dynamic programming and reinforcement learning problems. The dominant paradigm is to
use the function to approximate the state (or state and action) values. Most algorithms then seek to
minimize some form of error between the approximate value function and the true value function,
usually by simulation (Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1996). While there have been
a multitude of empirical successes for this approach (for example, Samuel, 1959; Tesauro, 1992,
1994; Baxter, Tridgell, & Weaver, 2000; Zhang & Dietterich, 1995; Singh & Bertsekas, 1997), there
are only weak theoretical guarantees on the performance of the policy generated by the approximate
value function. In particular, there is no guarantee that the policy will improve as the approximate
value function is trained; in fact performance can degrade even when the function class contains an
approximate value function whose corresponding greedy policy is optimal (see Baxter & Bartlett,
2001, Appendix A, for a simple two-state example).
An alternative technique that has received increased attention recently is the policy-gradient
approach in which the parameters of a stochastic policy are adjusted in the direction of the gradient
of some performance criterion (typically either expected discounted reward or average reward). The

c 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBAXTER

ET AL .

key problem is how to compute the performance gradient under conditions of partial observability
when an explicit model of the system is not available.
This question has been addressed in a large body of previous work (Barto, Sutton, & Anderson,
1983; Williams, 1992; Glynn, 1986; Cao & Chen, 1997; Cao & Wan, 1998; Fu & Hu, 1994;
Singh, Jaakkola, & Jordan, 1994, 1995; Marbach & Tsitsiklis, 1998; Marbach, 1998; Baird &
Moore, 1999; Rubinstein & Melamed, 1998; Kimura, Yamamura, & Kobayashi, 1995; Kimura,
Miyazaki, & Kobayashi, 1997). See the introduction of (Baxter & Bartlett, 2001) for a discussion
of the history of policy-gradient approaches. Most existing algorithms rely on the existence of an
identifiable recurrent state in order to make their updates to the gradient estimate, and the variance
of the algorithms is governed by the recurrence time to that state. In cases where the recurrence time
is too large (for instance because the state space is large), or in situations of partial observability
where such a state cannot be reliably identified, we need to seek alternatives that do not require
access to such a state.
Motivated by these considerations, Baxter and Bartlett (2001, 2000) introduced and analysed
GPOMDPan algorithm for generating a biased estimate of the gradient of the average reward in
general Partially Observable Markov Decision Processes (POMDPs) controlled by parameterized
stochastic policies. The chief advantages of GPOMDP are that it requires only a single sample path
of the underlying Markov chain, it uses only one free parameter fi 2 ; , which has a natural
interpretation in terms of bias-variance trade-off, and it requires no knowledge of the underlying
state.
More specifically, suppose  2 R K are the parameters controlling the POMDP. For example, 
could be the parameters of an approximate neural-network value-function that generates a stochastic
policy by some form of randomized look-ahead, or  could be the parameters of an approximate Q
function used to stochastically select controls1 . Let   denote the average reward of the POMDP
with parameter setting  . GPOMDP computes an approximation rfi   to r  based on a single
continuous sample path of the underlying Markov chain. The accuracy of the approximation is
controlled by the parameter fi 2 ; , and one can show that

[0 1)

()

[0 1)

()

()

r() = filim
r ():
!1 fi

The trade-off preventing choosing fi arbitrarily close to 1 is that the variance of GPOMDPs estimates of rfi   scale as =
fi 2 . However, on the bright side, it can also be shown that the bias
of rfi  (measured by krfi  
r  k) is proportional to  fi where  is a suitable mixing
time of the Markov chain underlying the POMDP (Bartlett & Baxter, 2000a). Thus for rapidly
mixing POMDPs (for which  is small), estimates of the performance gradient with acceptable
bias and variance can be obtained.
Provided rfi   is a sufficiently accurate approximation to r  in fact, rfi   need only
be within  of r  small adjustments to the parameters  in the direction rfi   will guarantee improvement in the average reward   . In this case, gradient-based optimization algorithms
using rfi   as their gradient estimate will be guaranteed to improve the average reward   on
each step. Except in the case of table-lookup, most value-function based approaches to reinforcement learning cannot make this guarantee.
In this paper we present a conjugate-gradient ascent algorithm that uses the estimates of rfi  
provided by GPOMDP. Critical to the successful operation of the algorithm is a novel line search

()

()

90
()

1 (1 )
() ()

()
()

(1

)

()

()

()
()

()

()

1. Stochastic policies are not strictly necessary in our framework, but the policy must be differentiable in the sense
that  () exists.

r

352

fiP OLICY-G RADIENT E STIMATION

subroutine that brackets maxima by relying solely upon gradient estimates. This largely avoids
problems associated with finding the maximum using noisy value estimates. Since the parameters
are only updated after accumulating sufficiently accurate estimates of the gradient direction, we refer
to this approach as the off-line algorithm. This approach essentially allows us to take a stochastic
gradient optimization problem and treat it as a non-stochastic optimization problem, thus enabling
the use of a large body of accumulated heuristics and algorithmic improvements associated with
such methods. We also present a more traditional, on-line stochastic gradient ascent algorithm
based on GPOMDP that updates the parameters at every time step. This algorithm is essentially the
algorithm proposed in (Kimura et al., 1997).
The off-line and on-line algorithms are applied to a variety of problems, beginning with a simple
3-state Markov decision process (MDP) controlled by a linear function for which the true gradient
can be exactly computed. We show rapid convergence of the gradient estimates rfi   to the true
gradient, in this case over a large range of values of fi . With this simple system we are able to
illustrate vividly the bias/variance tradeoff associated with the selection of fi . We then compare the
performance of the off-line and on-line approaches applied to finding a good policy for the MDP.
The off-line algorithm reliably finds a near-optimal policy in less than 100 iterations of the Markov
chain, an order of magnitude faster than the on-line approach. This can be attributed to the more
aggressive exploitation of the gradient information by the off-line method.
Next we demonstrate the effectiveness of the off-line algorithm in training a neural network
controller to control a puck in a two-dimensional world. The task in this case is to reliably
navigate the puck from any starting configuration to an arbitrary target location in the minimum
time, while only applying discrete forces in the x and y directions. Although the on-line algorithm
was tried for this problem, convergence was considerably slower and we were not able to reliably
find a good local optimum.
In the third experiment, we use the off-line algorithm to train a controller for the call admission
queueing problem treated in (Marbach, 1998). In this case near-optimal solutions are found within
about 2000 iterations of the underlying queue, 1-2 orders of magnitude faster than the experiments
reported in (Marbach, 1998) with on-line (stochastic-gradient) algorithms.
In the fourth and final experiment, the off-line algorithm was used to reliably train a switched
neural-network controller for a two-dimensional variation on the classical mountain-car task (Sutton & Barto, 1998, Example 8.2).
The rest of this paper is organized as follows. In Section 2 we introduce POMDPs controlled by
stochastic policies, and the assumptions needed for our algorithms to apply. GPOMDP is described
in Section 3. In Section 4 we describe the off-line and on-line gradient-ascent algorithms, including
the gradient-based line-search subroutine. Experimental results are presented in Section 5.

()

2.

POMDPs Controlled by Stochastic Policies

A partially observable, Markov decision process (POMDP) consists of a state space S , observation
space Y and a control space U . For each state i 2 S there is a deterministic reward r i . Although
the results in Baxter and Bartlett (2001) only guarantee convergence of GPOMDP in the case of
finite S (but continuous U and Y ), the algorithm can be applied regardless of the nature of S so we
do not restrict the cardinality of S , U or Y .
Consider first the case of discrete S , U and Y . Each control u 2 U determines a stochastic
matrix P u
pij u giving the transition probability from state i to state j (i; j 2 S ). For each

()

( ) = [ ( )]

353

fiBAXTER

ET AL .

state i 2 S , an observation Y 2 Y is generated independently according to a probability distribution
 i over observations in Y . We denote the probability that Y y by y i . A randomized policy
is simply a function  mapping observations into probability distributions over the controls U . That
is, for each observation y 2 Y ,  y is a distribution over the controls in U . Denote the probability
under  of control u given observation y by u y .
For continuous S ; Y and U , pij u becomes a kernel kij u giving the probability density of
transitions from i to j ,  i becomes a probability density function on Y with y i the density at y ,
and  y becomes a probability density function on U with u y the density at u.
To each randomized policy  there corresponds a Markov chain in which state transitions are
generated by first selecting an observation Y in state i according to the distribution  i , then selecting a control U according to the distribution  Y , and finally generating a transition to state j
according to the probability pij U .
At present we are only dealing with a fixed POMDP. To parameterize the POMDP we parameterize the policies, so that  now becomes a function  ; y of a set of parameters  2 R K ,
as well as of the observation y . The Markov chain corresponding to  has state transition matrix
P 
pij  given by
pij  EY  (i) EU (;Y ) pij U :
(1)

()

=

()

()
()

()

()

()

()
()

()

()

( )

( )

( )

( ) = [ ( )]

( )=

( )

Note that the policies  are purely reactive or memoryless in that their choice of action is based only
upon the current observation. All the experiments described in the present paper use purely reactive
policies. Aberdeen and Baxter (2001) have extended GPOMDP and the techniques of the present
paper to controllers with internal state.
The following technical assumptions are required for the operation of GPOMDP.
Assumption 1. The derivatives,

exist, and the ratios

@u (; y)
;
@k

( ) fififi

fi
fi @u ; y
fi
fi
@

k

fi

u (; y)
are uniformly bounded by B < 1, for all u 2 U , y 2 Y ,  2 R K

and k

= 1; : : : ; K .

The second part of this assumption is needed because the ratio appears in the GPOMDP algorithm. It allows zero-probability actions u ; y
only if ru ; y is also zero, in which case
we set =
. See Section 5 for examples of policies satisfying this requirement.

( )=0

0 0=0

Assumption 2. The magnitudes of the rewards,
states i.

( )

jr(i)j, are uniformly bounded by R < 1 for all

For deterministic rewards, his condition only represents a restriction in infinite state spaces.
However, all the results in the present paper apply to bounded stochastic rewards, in which case r i
is the expectation of the reward in state i.

()

()

Assumption 3. Each P  ;  2 R K , has a unique stationary distribution 
satisfying the balance equations:

()P () = ():
354

() = [1 (; : : : ; n()],

fiP OLICY-G RADIENT E STIMATION

Assumption 3 ensures that, for all parameters  , the Markov chain forms a single recurrent class.
Since any finite-state Markov chain always ends up in a recurrent class, and it is the properties of
this class that determine the long-term average reward, this assumption is mainly for convenience
so that we do not have to include the recurrence class as a quantifier in our theorems. Observe
that episodic problems, such as the minimization of time to a goal state, may be modeled in a way
that satisfies Assumption 3 by simply resetting the agent upon reaching the goal state back to some
initial starting distribution over states. Examples are described in Section 5.
The average reward   is simply the expected reward under the stationary distribution   :

()

()

() =

()

n
X
i=1

i ()r(i):

(2)

Because of Assumption 3,   is also equal to the expected long-term average of the reward received when starting from any state i:
fi

!

1 TX1 r(X )fififi X0 = i
() = lim E
t
fi
T !1
T
t=0

:

Here the expectation is over sequences of states X0 ; : : : ; XT 1 with state transitions generated by
P  (note that the expectation is independent of the starting state i).

()

3. The GPOMDP Algorithm
(Algorithm 1) is an algorithm for computing a biased estimate
average reward r  . T satisfies

GPOMDP

()

T of the gradient of the

lim T = rfi ();

T !1

where rfi 

() (fi 2 [0; 1)) is an approximation to r() satisfying
r() = filim
r ();
!1 fi

(Baxter & Bartlett, 2001, Theorems 2, 5). Note that GPOMDP relies only upon a single sample path
from the POMDP. Also, it does not require knowledge of the transition probability matrix P , nor of
the observation process  ; it only requires knowledge of the randomized policy , in particular the
ability to compute the gradient of the probability of the chosen control divided by the probability of
the chosen control.
We cannot set fi arbitrarily close to in GPOMDP, since the variance of the estimate is proportional to =
fi 2 . However, on the bright side, it can also be shown that the bias of rfi 
(measured by krfi   r  k) is proportional to 
fi where  is a suitable mixing time of the
Markov chain underlying the POMDP (Bartlett & Baxter, 2000a). Under Assumption 3, regardless
of the initial starting state, the distribution over states converges to the stationary distribution  
when the agent is following policy  ;  . Standard Markov chain theory shows that the rate of
convergence to   is exponential, and loosely speaking, the mixing time  is the time constant in
the exponential decay.

1 (1

)
()

()

1

()

(1 )

()
()

( )

355

fiBAXTER

ET AL .

(fi; T; ) ! RK

Algorithm 1 GPOMDP
1: Given:

 fi 2 [0; 1).
 T > 0.
 Parameters  2 RK .
 Randomized policy (; ) satisfying Assumption 1.
 POMDP with rewards satisfying Assumption 2, and which when controlled by (; )
generates stochastic matrices P


2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

() satisfying Assumption 3.

Arbitrary (unknown) starting state X0 .

=0
=0

 =0
1



Set z0
and 0
(z0 ; 0 2 RK ).
for t
to T
do
Observe Yt (generated according to the observation distribution  Xt )
Generate control Ut according to  ; Yt
Observe r Xt+1 (where the next state Xt+1 is generated according to pXt Xt+1

( )
rUt (; Yt )
Set zt+1 = fizt +
Ut (; Yt )
Set t+1 = t + r (Xt+1 )zt+1
end for
T T =T
return T

(

)

( )

(Ut )).

Thus fi has a natural interpretation in terms of a bias/variance trade-off: small values of fi
give lower variance in the estimates T , but higher bias in that the expectation of T may be far
from r  , whereas values of fi close to yield small bias but correspondingly larger variance.
Fortunately, for problems which mix rapidly (small  ), fi can be small and still yield reasonable
bias. This bias/variance trade-off is vividly illustrated in the experiments of Section 5; see (Bartlett
& Baxter, 2000a) for a more detailed theoretical discussion of the bias/variance question.

()





1

4. Stochastic Gradient Ascent Algorithms
This section introduces two approaches to exploiting the gradient estimates produced by GPOMDP:

1. an off-line approach based on traditional conjugate-gradient optimization techniques but employing a novel line-search mechanism to cope with the noise in GPOMDPs estimates, and
2. an on-line stochastic optimization approach that uses the core update in GPOMDP (r
to update the parameters  on every iteration of the POMDP.
356

(Xt )zt )

fiP OLICY-G RADIENT E STIMATION

4.1 Off-line optimization of the average reward


()
()

()

biased and noisy estimates T of the gradient of the average reward r  for
controlled by parameterized stochastic policies. A straightforward algorithm for finding
local maxima of   would be to compute T  at the current parameter settings  , and then
modify  by 
  T  . Provided T  is close enough to the true gradient direction r  ,
and provided the step-sizes  are suitably decreasing, standard stochastic optimization theory tells us
that this technique will converge to a local maximum of   . However, given that each computation
of T  requires many iterations of the POMDP to guarantee suitably accurate gradient estimates
(that is, in general T needs to be large), we would like to more aggressively exploit the information
contained in T  than by simply adjusting the parameters  by a small amount in the direction
T  .
There are two techniques for making better use of gradient information that are widely used in
non-stochastic optimization: better choice of the search direction and better choice of step size. Better search directions can be found by employing conjugate-gradient directions rather than the pure
gradient direction. Better step sizes are usually obtained by performing some kind of line-search to
find a local maximum in the search direction, or through the use of second order methods. Since
line-search techniques tend to be more robust to departures from quadraticity in the optimization
surface, we will only consider those here (however, see Baxter & Bartlett, 2001, Section 7.3, for a
discussion of how second-order derivatives may be computed with a GPOMDP-like algorithm).
CONJPOMDP, described in Algorithm 2, is a version of the Polak-Ribiere conjugate-gradient
algorithm (see, e.g. Fine, 1999, Section 5.5.2) that is designed to operate using only noisy (and
possibly) biased estimates of the gradient of the objective function (for example, the estimates T
provided by GPOMDP). The argument GRAD to CONJPOMDP computes the gradient estimate.
The novel feature of CONJPOMDP is GSEARCH, a linesearch subroutine that uses only gradient information to find the local maximum in the search direction. The use of gradient information ensures GSEARCH is robust to noise in the performance estimates. Both CONJPOMDP and
GSEARCH can be applied to any stochastic optimization problem for which noisy (and possibly)
biased gradient estimates are available.
The argument s0 to CONJPOMDP provides an initial step-size for GSEARCH. The argument 
provides a stopping condition; when kGRAD  k2 falls below , CONJPOMDP terminates.
GPOMDP generates
POMDPs

()
+()

()

()

()

()

()



()

4.2 The GSEARCH algorithm
The key to the successful operation of CONJPOMDP is the linesearch algorithm GSEARCH (Algorithm 3). GSEARCH uses only gradient information to bracket the maximum in the direction   ,
and then quadratic interpolation to jump to the maximum.
We found the use of gradients to bracket the maximum far more robust than the use of function
values. To illustrate why this is so, in Figure 1 we have plotted a stylized view of the average reward
  along some search direction  (labeled f  in the figure), and its gradient in that direction
r    (labeled grad(f )). There are two ways we could search in the direction  to bracket
the maximum of   in that direction (at in this case), one using function values and the other
using gradient estimates:

()
()

()

0

( )

( )

1. Find three points 1 ; 2 ; 3 , all lying in the direction   from  , such that  1 <  2 and
 3 <  2 . Assuming no overshooting, we then know the maximum must lie between 1

( )

( )

357

fiBAXTER

Algorithm 2
1: Given:



(

CONJPOMDP GRAD

GRAD

ET AL .

; ; s0 ; )

: RK ! RK : a (possibly noisy and biased) estimate of the gradient of the objec-

tive function to be maximized.





2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

Starting parameters  2 R K (set to maximum on return).
Initial step size s0

> 0.

Gradient resolution .

g = h = GRAD()
while kg k2   do

(
; ; h; s0 ; )
 = GRAD()
 = ( g)  =kgk2
h =  + h
if h   < 0 then
h=
end if
g=
GSEARCH GRAD

end while

and 3 and we can use the three points and quadratic interpolation to estimate the location of
the maximum.

( )

0

( )

0

2. Find two points 1 and 2 such that r 1    > and r 2    < , and again use
quadratic interpolation (which corresponds to linear interpolation of the gradients) to estimate
the location of the maximum.
Both of these approaches will be equally satisfactory provided there is no noise in either the function
estimates   , or the gradient estimates r  . However, when estimates of   or r  are
available only through simulation, they will necessarily be noisy and the situation will look more
like Figure 2. In this case the use of gradients to bracket the maximum becomes more desirable,
because the line-search technique based on value estimates could choose any of the peaks in the
plot of f noise as the location of the maximum, which occur nearly uniformly along the x-axis,
whereas the second technique based on gradients would choose any of the zero-crossings of the
noisy gradient plot, which are far closer to the true maximum2 . This is illustrated in Figure 3.
Another view of this phenomenon is that regardless of the variance of our estimates of   , the
variance of
 1  2 approaches (the maximum possible) as 1 approaches 2 . Thus,
to reliably bracket the maximum using noisy estimates of   we need to be able to reduce the
variance of the estimates when 1 and 2 are close. In our case this means running the simulation

()

()

()

()

+

sign[ ( )

( )]

()

1

()

2. There is an implicit assumption in our argument that the noise processes in the gradient and value estimates are of
approximately the same magnitude. If the variance of the value estimates is considerably smaller than the variance of
the gradient estimates then we would expect bracketing with values to be superior. In all our experiments we found
gradient bracketing to be superior.

358

fiP OLICY-G RADIENT E STIMATION

2

f
grad(f)

1.5
1
0.5
0
-0.5
-1
-1.5
-2
-2.5
-1

-0.5

Figure 1: Stylized plot of the average reward 
 .

0

0.5

1

() and the gradient r()   in a search direction

2.5

f + noise
grad(f) + noise

2
1.5
1
0.5
0
-0.5
-1
-1.5
-2
-2.5
-1

-0.5

0

0.5

1

Figure 2: Plot as in Figure 1 but with estimation noise added to both the function and gradient
curves.

from which the estimates are derived for longer and longer periods of time. In contrast, the variance
of
r 1   (and r 2  ) is independent of the distance between 1 and 2 , and in
particular does not grow as the two points approach one another.
One disadvantage to using gradient estimates to bracket is that it is not possible to detect extreme
overshooting of the maximum. However, this can be avoided by using value estimates as a sanity

sign ( )

sign ( )

359

fiBAXTER

ET AL .

1

0.5

0

-0.5
f
grad(f)

-1
-1

-0.5

0

0.5

1

Figure 3: Plot of the possible maximum locations that would be found by a line-search algorithm
based on value estimates (f ), and one based on gradient estimates (grad(f )), for the curves
in Figure 2. The zero-crossings in each case are the possible locations. Note that the
gradient-based approach more accurately localizes the maximum.

check to determine if the value has dropped dramatically, and suitably adjusting the search if this
occurs.
In Algorithm 3, lines 525 bracket the maximum by finding a parameter setting 
0
s  such that GRAD    > , and a second parameter setting + 0 s+  such that

GRAD +   < . The reason for  rather than in these expressions is to provide some robustness
against errors in the estimates GRAD  . It also prevents the algorithm stepping to 1 if there is
no local maximum in the direction   . Note that we use the same  as used in CONJPOMDP to
determine when to terminate due to small gradient (line 4 in CONJPOMDP).
Provided that the signs of the gradients at the bracketing points  and + show that the maximum of the quadratic defined by these points lies between them, line 27 will jump to the maximum.
Otherwise the algorithm simply jumps to the midpoint between  and + .

( )

( )

()

= +

0

4.3 On-line optimization of the average reward:

= +

OLPOMDP

combined with GSEARCH operates by iteratively choosing uphill directions and
then searching for a local maximum in the chosen direction. If the GRAD argument to CONJPOMDP
is GPOMDP, the optimization will involve many iterations of the underlying POMDP between parameter updates.
In traditional stochastic optimization one typically uses algorithms that update the parameters
at every iteration, rather than accumulating gradient estimates over many iterations. Algorithm 4,
OLPOMDP, presents an adaptation of GPOMDP to this form. See Bartlett and Baxter (2000b) for a
proof that OLPOMDP converges to the vicinity of a local maximum of   . Note that OLPOMDP
is very similar to the algorithms proposed in Kimura et al. (1995, 1997).

CONJPOMDP

()

360

fiP OLICY-G RADIENT E STIMATION

Algorithm 3
1: Given:



(

GSEARCH GRAD

GRAD

; 0 ;  ; s0 ; )

: RK ! RK : a (possibly noisy and biased) estimate of the gradient of the objec-

tive function.






2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

2 RK (set to maximum on return).
Search direction   2 RK with GRAD(0 )    > 0.
Starting parameters 0
Initial step size s0

> 0.

= 0.

Inner product resolution  >

s = s0
 = 0 + s

 = GRAD()
if     < 0 then

Step back to bracket the maximum:
repeat

s+ = s
p+ =   
s = s=2
 = 0 + s

 = GRAD()
   > 
s =s
p =   
until

else
Step forward to bracket the maximum:
18:
repeat
16:
17:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:

s =s
p =   
s = 2s
 = 0 + s

 = GRAD()
until     < 
s+ = s
p+ =   
end if
if p > 0 and p+ < 0 then
s = s p ps++ sp
else
s = s +2 s+
end if
0 = 0 + s
361

fiBAXTER

ET AL .

(fi; T; 0 ) ! RK .

Algorithm 4 OLPOMDP
1: Given:

 fi 2 [0; 1).
 T > 0.
 Initial parameter values 0 2 RK .

	
 Randomized parameterized policies (; ):  2 RK satisfying Assumption 1.
 POMDP with rewards satisfying Assumption 2, and which when controlled by (; )

() satisfying Assumption 3.
 Step sizes t; t = 0; 1; : : : satisfying P t = 1 and P t2 < 1.
generates stochastic matrices P


2:
3:
4:
5:
6:
7:
8:

Arbitrary (unknown) starting state X0 .

=0
=0

Set z0
(z0 2 RK ).
for t
to T
do
Observe Yt (generated according to  Xt ).
Generate control Ut according to  ; Yt
Observe r Xt+1 (where the next state Xt+1 is generated according to pXt Xt+1

1

( )
rUt (; Yt )
Set zt+1 = fizt +
Ut (; Yt )
Set t+1 = t + t r (Xt+1 )zt+1

( )
( )

(Ut ).

end for
10: return T
9:

5. Experiments
In this section we present several sets of experimental results. Throughout this section, where we
refer to CONJPOMDP we mean CONJPOMDP with GPOMDP as its GRAD argument.
In the first set of experiments, we consider a system in which a controller is used to select
actions for a 3-state Markov Decision Process (MDP). For this system we are able to compute the
true gradient exactly using the matrix equation


r() = 0()rP () I P () + e0 () 1 r;

()
()

()



()

(3)

where P  is the transition matrix of the underlying Markov chain with the controllers parameters
set to  ,  0  is the stationary distribution corresponding to P  (written as a row vector), e 0 
is the square matrix in which each row is the stationary distribution, and r is the (column) vector of
rewards (see Baxter & Bartlett, 2001, Section 3, for a derivation of (3)). Hence we can compare the
estimates T generated by GPOMDP with the true gradient r  , both as a function of the number
of iterations T and as a function of the discount parameter fi . We also optimize the performance of
the controller using the on-line algorithm, OLPOMDP, and the off-line algorithm CONJPOMDP.
CONJPOMDP reliably converges to a near optimal policy with around 100 iterations of the MDP,
while the on-line method requires approximately 1000 iterations. This should be contrasted with
362

()

fiP OLICY-G RADIENT E STIMATION

Origin
State

A
A
B
B
C
C

Destination State Probabilities
Action

a1
a2
a1
a2
a1
a2

A

B

C

0.0
0.0
0.8
0.2
0.0
0.0

0.8
0.2
0.0
0.0
0.8
0.2

0.2
0.8
0.2
0.8
0.2
0.8

Table 1: Transition probabilities of the three-state MDP

r(A) = 0
r(B ) = 0
r(C ) = 1

12 2 (A) = 6
1 (A) = 18
18
12
6
1 (B ) = 18 2 (B ) = 18
1 (C ) = 185 2 (C ) = 185

Table 2: Three-state rewards and features.

TD(1)

training a linear value-function for this system using
(Sutton, 1988), which can be shown
to converge to a value function whose one-step lookahead policy is suboptimal (Weaver & Baxter,
1999).
In the second set of experiments, we consider a simple puck-world problem in which a small
puck must be navigated around a two-dimensional world by applying thrust in the x and y directions.
We train a 1-hidden-layer neural-network controller for the puck using CONJPOMDP. Again the
controller reliably converges to near optimality.
In the third set of experiments we use CONJPOMDP to optimize the admission thresholds for
the call-admission problem considered in (Marbach, 1998).
In the final set of experiments we use CONJPOMDP to train a switched neural-network controller for a two-dimensional variant of the mountain-car task (Sutton & Barto, 1998, Example
8.2).
In all the experiments we found that convergence of the line-searches was greatly improved if
all calls to the GPOMDP algorithm were seeded with the same random number sequence.
5.1 A three-state MDP
In this section we consider a three-state MDP, in each state of which there is a choice of two actions

a1 and a2 . Table 1 shows the transition probabilities as a function of the states and actions. Each
state x has an associated two-dimensional feature vector (x) = (1 (x); 2 (x)) and reward r (x)

which are detailed in Table 2. Clearly, the optimal policy is to always select the action that leads to
state C with the highest probability, which from Table 1 means always selecting action a2 .
This rather odd choice of feature vectors for the states ensures that a value function linear in
those features and trained using
while observing the optimal policywill implement a
suboptimal greedy one-step lookahead policy (see (Weaver & Baxter, 1999) for a proof). Thus, in

TD(1)

363

fiBAXTER

ET AL .

TD(1)

contrast to the gradient based approach, for this system,
training a linear value function is
guaranteed to produce a worse policy if it starts out observing the optimal policy.
5.1.1 T RAINING

A CONTROLLER

Our goal is to learn a stochastic controller for this system that implements an optimal (or nearoptimal) policy. Given a parameter vector 
1 ; 2 ; 3 ; 4 , we generate a policy as follows. For
any state x, let

=(

)

s1 (x) := 1 1 (x) + 2 2 (x)
s2 (x) := 3 1 (x) + 4 2 (x):
Then the probability of choosing action a1 in state x is given by

es1 (x)
a1 (x) = s1 (x) s2 (x) ;
e
+e
while the probability of choosing action a2 is given by
es2 (x)
a2 (x) = s1 (x) s2 (x) = 1 a1 (x):
e
+e
r (x)
The ratios aai(x) needed by Algorithms 1 and 4 are given by,
i
ra1 (x) = es2 (x) [ (x);  (x);  (x);  (x)]
(4)
2
1
2
a1 (x)
es1 (x) + es2 (x) 1
ra2 (x) = es1 (x) [  (x);  (x);  (x);  (x)]
(5)
1
2
1
2
a2 (x)
es1 (x) + es2 (x)
Since the second two components in r= are always the negative of the first two, this shows that
two of the parameters are redundant in this case: we could just as well have set 3 = 1 and
4 = 2 .
5.1.2 G RADIENT

ESTIMATES

= [1 1 1 1]
[0 1)



With a parameter vector3 of 
; ; ; , GPOMDP was used to generate estimates T of
rfi , for various values of T and fi 2 ; . To measure the progress of T towards the true gradient
r, r was calculated from (3) and then for each value of T the angle between T and r and
T rk were recorded. The angles and relative errors are plotted in Figures 4, 5
the relative error kkr
k
and 6.
The graphs illustrate a typical trade-off for the GPOMDP algorithm: small values of fi give
higher bias in the estimates, while larger values of fi give higher variance (the final bias is only
shown in Figure 6 for the norm deviation because it was too small to measure for the angular
deviation). The bias introduced by having fi < is very small for this system. In the worst case,
fi
: , the final gradient direction is indistinguishable from the true direction while the relative
 T k
deviation krkr
.
k is only :

= 00





1

7 7%

3. Other initial values of the parameter vector were chosen with similar results. Note that [1; 1; 1; 1] generates a
suboptimal policy.

364

fiP OLICY-G RADIENT E STIMATION

160

beta=0.0

140

140

120

120
Angle (degrees)

Angle (degrees)

160

100
80
60
40
20

beta=0.4

100
80
60
40
20

0

0

-20

-20
1

10

100

1000 10000 100000 1e+06 1e+07

1

10

100

Markov Chain Iterations (T)
160

beta=0.8

140

140

120

120
Angle (degrees)

Angle (degrees)

160

1000 10000 100000 1e+06 1e+07

Markov Chain Iterations (T)

100
80
60
40
20

beta=0.95

100
80
60
40
20

0

0

-20

-20
1

10

100 1000 10000 100000 1e+06 1e+07
Markov Chain Iterations (T)

1

10

100 1000 10000 100000 1e+06 1e+07
Markov Chain Iterations (T)



Figure 4: Angle between the true gradient r and the estimate T for the three-state Markov
chain, for various values of the discount parameter fi . T was generated by Algorithm 1.
Averaged over 500 independent runs. Note the higher variance at large T for the larger
values of fi . Error bars are one standard deviation.



5.1.3 T RAINING

VIA CONJUGATE - GRADIENT ASCENT

with GPOMDP as the GRAD argument was used to train the parameters of the
controller described in the previous section. Following the low bias observed in the experiments of
the previous section, the argument fi of GPOMDP was set to . After a small amount of experimentation, the arguments s0 and  of CONJPOMDP were set to
and :
respectively. None of
these values were critical, although the extremely large initial step-size (s0 ) did considerably reduce
the time required for the controller to converge to near-optimality.
We tested the performance of CONJPOMDP for a range of values of the argument T to
GPOMDP from to
. Since GSEARCH only uses GPOMDP to determine the sign of the inner
product of the gradient with the search direction, it does not need to run GPOMDP for as many
iterations as CONJPOMDP does. Thus, GSEARCH determined its own T parameter to GPOMDP
as follows. Initially, (somewhat arbitrarily) the value of T within GSEARCH was set to = the
value used in CONJPOMDP (or 1 if the value in CONJPOMDP was less than 10). GSEARCH then
called GPOMDP to obtain an estimate T of the gradient direction. If T    < (  being the
desired search direction) then T was doubled and GSEARCH was called again to generate a new
estimate T . This procedure was repeated until T    > , or T had been doubled four times. If

T   was still negative at the end of this process, GSEARCH searched for a local maximum in
the direction   , and the number of iterations T used by CONJPOMDP was doubled on the next
iteration (the conclusion being that the direction   was generated by overly noisy estimates from
GPOMDP).
CONJPOMDP

0
100

0 0001

1 4096

1 10











365

0

0

fiBAXTER

3

beta=0.0

2.5

Relative Norm Difference

Relative Norm Difference

3

ET AL .

2
1.5
1
0.5

beta=0.4

2.5
2
1.5
1
0.5

0

0
1

10

100

1000 10000 100000 1e+06 1e+07

1

10

Markov Chain Iterations (T)

1000 10000 100000 1e+06 1e+07

4.5

beta=0.8

beta=0.95

4

3

Relative Norm Difference

Relative Norm Difference

3.5

100

Markov Chain Iterations (T)

2.5
2
1.5
1
0.5

3.5
3
2.5
2
1.5
1
0.5

0

0
1

10

100 1000 10000 100000 1e+06 1e+07
Markov Chain Iterations (T)

1

10

100 1000 10000 100000 1e+06 1e+07
Markov Chain Iterations (T)

 T k
Figure 5: A plot of krkr
for the three-state Markov chain, for various values of the discount
k
parameter fi . T was generated by Algorithm 1. Averaged over 500 independent runs.
Note the higher variance at large T for the larger values of fi . Error bars are one standard
deviation.



Relative Norm Difference

10

beta=0.0
beta=0.40
beta=0.80
beta=0.95

1

0.1

0.01

0.001
1

10

100 1000 10000 100000 1e+06 1e+07
Markov Chain Iterations (T)

 T k
Figure 6: Graph showing the error in the estimate T (as measured by krkr
k ) for various values
of fi for the three-state Markov chain.
was
generated
by
Algorithm
1. Note the
T
decrease in the final bias as fi increases. Both axes are log scales.




366

fiP OLICY-G RADIENT E STIMATION

CONJGRAD Final Reward

0.8
0.7
0.6
0.5
0.4
0.3
0.2
1

10
100
1000
Markov Chain Iterations (T)

10000

Figure 7: Performance of the 3-state Markov chain controller trained by CONJPOMDP as a function of the total number of iterations of the Markov chain. The performance was computed exactly from the stationary distribution induced by the controller. The average
reward of the optimal policy is : . Averaged over 500 independent runs. The error bars
were computed by dividing the results into two separate bins depending on whether they
were above or below the mean, and then computing the standard deviation within each
bin.

08

()

Figure 7 shows the average reward   of the final controller produced by CONJPOMDP, as a
function of the total number of simulation steps of the underlying Markov chain. The plots represent
an average over
independent runs of CONJPOMDP. Note that : is the average reward of the
optimal policy. The parameters of the controller were (uniformly) randomly initialized in the range
: ; : before each call to CONJPOMDP. After each call to CONJPOMDP, the average reward
of the resulting controller was computed exactly by calculating the stationary distribution for the
controller. From Figure 7, optimality is reliably achieved using approximately 100 iterations of the
Markov chain.

500

08

[ 0 1 0 1]

5.1.4 T RAINING

ON - LINE WITH OLPOMDP

=

The controller was also trained on-line using Algorithm 4 (OLPOMDP) with fixed step-sizes t c
with c
: ; ; ; . Reducing step-sizes of the form t c=t were tried, but caused intolerably
slow convergence. Figure 8 shows the performance of the controller (measured exactly as in the
previous section) as a function of the total number of iterations of the Markov chain, for different
values of the step-size c. The graphs are averages over 100 runs, with the controllers weights
randomly initialized in the range
: ; : at the start of each run. From the figure, convergence
to optimal is about an order of magnitude slower than that achieved by CONJPOMDP, for the best
step-size of c
: . Step-sizes much greater that c
: failed to reliably converge to an optimal
policy.

= 0 1 1 10 100

=

[ 0 1 0 1]

=10

= 10 0

367

fiBAXTER

ET AL .

0.8

0.8
0.79
0.78
Average Reward

Average Reward

0.75
0.7
0.65
0.6

0.77
0.76
0.75
0.74
0.73
0.72

0.55

0.71

c=0.1

0.5
10

100

1000

c=1

0.7
10000

10

100

10000

Markov Chain Iterations

0.8

0.9

0.7

0.8
Average Reward

Average Reward

Markov Chain Iterations

1000

0.6
0.5
0.4
0.3

0.7
0.6
0.5
0.4
0.3

c=10

0.2
10

100
1000
Markov Chain Iterations

c=100

0.2
10000

10

100
1000
Markov Chain Iterations

10000

Figure 8: Performance of the 3-state Markov chain controller as a function of the number of iteration steps in the on-line algorithm, Algorithm 4, for fixed step sizes of : ; ; , and
.
Error bars were computed as in Figure 7.

0 1 1 10

100

5.2 Puck World
In this section, experiments are described in which CONJPOMDP and OLPOMDP were used to
train 1-hidden-layer neural-network controllers to navigate a small puck around a two-dimensional
world.
5.2.1 T HE W ORLD
The puck was a unit-radius, unit-mass disk constrained to move in the plane in a region 100 units
square. The puck had no internal dynamics (i.e rotation). Collisions with the regions boundaries
were inelastic with a (tunable) coefficient of restitution e (set to : for the experiments reported
here). The puck was controlled by applying a 5 unit force in either the positive or negative x
direction, and a 5 unit force in either the positive or negative y direction, giving four different
controls in total. The control could be changed every = of a second, and the simulator operated
at a granularity of =
of a second. The puck also had a retarding force due to air resistance of
:  speed2 . There was no friction between the puck and the ground.
The puck was given a reward at each decision point ( = of a second) equal to d where d
was the distance between the puck and some designated target point. To encourage the controller
to learn to navigate the puck to the target independently of the starting state, the puck state was
reset every 30 (simulated) seconds to a random location and random x and y velocities in the range
; , and at the same time the target position was set to a random location.
Note that the size of the state-space in this example is essentially infinite, being of the order of
PRECISION where PRECISION is the floating point precision of the machine (
bits). Thus, the

09

0 005

1 10

1 100

1 10

[ 10 10]
2

64

368

fiP OLICY-G RADIENT E STIMATION

time between visits to a recurrent state is likely to be large. Also, the puck cannot just maximize its
immediate reward because this leads to significant overshooting of the target locations.
5.2.2 T HE

CONTROLLER

A one-hidden-layer neural-network with six input nodes, eight hidden nodes and four output nodes
was used to generate a probabilistic policy in a similar manner to the controller in the three-state
Markov chain example of the previous section. Four of the inputs were set to the raw x and y
locations and velocities of the puck at the current time-step, the other two were the differences
between the pucks x and y location and the targets x and y location respectively. The location
inputs were scaled to lie between
and , while the velocity inputs were scaled so that a speed
of
units per second mapped to a value of . The hidden nodes computed a
squashing
function, while the output nodes were linear. Each hidden and output node had the usual additional
offset parameter. The four output nodes were exponentiated and then normalized as in the Markovchain example to produce a probability distribution over the four controls ( units thrust in the x
direction,  units thrust in the y direction). Controls were selected at random from this distribution.

1

10

1

1

tanh

5

5

5.2.3 C ONJUGATE

GRADIENT ASCENT

We trained the neural-network controller using CONJPOMDP with the gradient estimates generated
by GPOMDP. After some experimentation we chose fi
: and T
; ;
as the parameters CONJPOMDP supplied to GPOMDP. GSEARCH used the same value of fi and the scheme
discussed in Section 5.1.3 to determine the number of iterations with which to call GPOMDP.
Due to the saturating nature of the neural-network hidden nodes (and the exponentiated output
nodes), there was a tendency for the network weights to converge to local minima at infinity.
That is, the weights would grow very rapidly early on in the simulation, but towards a suboptimal
solution. Large weights tend to imply very small gradients and thus the network becomes stuck
at these suboptimal solutions. We have observed a similar behaviour when training neural networks
for pattern classification problems. To fix the problem, we subtracted a small quadratic penalty term
 kk2 from the performance estimates and hence also a small correction i from the gradient
calculation4 for i .
We used a decreasing schedule for the quadratic penalty weight  (arrived at through some
experimentation).  was initialized to : and then on every tenth iteration of CONJPOMDP, if the
performance had improved by less than 10% from the value ten iterations ago,  was reduced by a
factor of 10. This schedule solved nearly all the local minima problems, but at the expense of slower
convergence of the controller.
A plot of the average reward of the neural-network controller is shown in Figure 9, as a function
of the number of iterations of the POMDP. The graph is an average over 100 independent runs,
with the parameters initialized randomly in the range
: ; : at the start of each run. The four
bad runs shown in Figure 10 were omitted from the average because they gave misleadingly large
error bars.
Note that the optimal performance (within the neural-network controller class) seems to be
around
for this problem, due to the fact that the puck and target locations are reset every
simulated seconds and hence there is a fixed fraction of the time that the puck must be away from

= 0 95

= 1 000 000

2

05

[ 0 1 0 1]

8

30

4. When used as a technique for capacity control in pattern classification, this technique goes by the name weight
decay. Here we used it to condition the optimization problem.

369

fiBAXTER

ET AL .

-5
-10
Average Reward

-15
-20
-25
-30
-35
-40
-45
-50
-55
0

3e+07

6e+07
9e+07
Iterations

1.2e+08

1.5e+08

Figure 9: Performance of the neural-network puck controller as a function of the number of iterations of the puck world, when trained using CONJPOMDP. Performance estimates were
generated by simulating for ;
;
iterations. Averaged over 100 independent runs
(excluding the four bad runs in Figure 10).

1 000 000

the target. From Figure 9 we see that the final performance of the puck controller is close to optimal.
In only 4 of the 100 runs did CONJPOMDP get stuck in a suboptimal local minimum. Three of
those cases were caused by overshooting in GSEARCH (see Figure 10), which could be prevented
by adding extra checks to CONJPOMDP.
Figure 11 illustrates the behaviour of a typical trained controller. For the purpose of the illustration, only the target location and puck velocity were randomized every 30 seconds, not the puck
location.
5.3 Call Admission Control
In this section we report the results of experiments in which CONJPOMDP was applied to the task
of training a controller for the call admission problem treated by Marbach (1998, Chapter 7).
5.3.1 T HE P ROBLEM
The call admission control problem treated by Marbach (1998, Chapter 7) models the situation
in which a telecommunications provider wishes to sell bandwidth on a communications link to
customers in such a way as to maximize long-term average reward.
Specifically, the problem is a queuing problem. There are three different types of call, each
with its own call arrival rate ff , ff , ff , bandwidth demand b , b , b
and average
holding time h , h , h . The arrivals are Poisson distributed while the holding times are
exponentially distributed. The link has a maximum bandwidth of 10 units. When a call arrives and
there is sufficient available bandwidth, the service provider can choose to accept or reject the call
(if there is not enough available bandwidth the call is always rejected). Upon accepting a call of

(1) (2) (3)

(1) (2) (3)

370

(1) (2) (3)

fiAverage Reward

P OLICY-G RADIENT E STIMATION

5
0
-5
-10
-15
-20
-25
-30
-35
-40
-45
-50
-55
0

5e+07 1e+08 1.5e+08 2e+08 2.5e+08 3e+08 3.5e+08
Iterations

Figure 10: Plots of the performance of the neural-network puck controller for the four runs (out of
100) that converged to substantially suboptimal local minima.

target

Figure 11: Illustration of the behaviour of a typical trained puck controller.

type m, the service provider receives a reward of r
maximize the long-term average reward.

(m) units. The goal of the service provider is to

The parameters associated with each call type are listed in Table 3. With these settings, the
optimal policy (found by dynamic programming by Marbach (1998)) is to always accept calls of
type 2 and 3 (assuming sufficient available bandwidth) and to accept calls of type 1 if the available
371

fiBAXTER

ET AL .

Call Type
Bandwidth Demand
Arrival Rate
Average Holding Time
Reward

b
ff
h
r

1
1

2
1

3
1

1

2

4

1:8 1:6 1:4
0:6 0:5 0:4

Table 3: Parameters of the call admission control problem.

bandwidth is at least 3. This policy has an average reward of
policy has an average reward5 of : .

0 784

0:804, while the always accept

5.3.2 T HE C ONTROLLER

=(

)

The controller had three parameters 
1 ; 2 ; 3 , one for each type of call. Upon arrival of a call
of type m, the controller chooses to accept the call with probability
(

1
() = 1+exp(1:5(b

0

m ))

if b

+ b(m)  10,

otherwise,

where b is the currently used bandwidth. This is the class of controllers studied by Marbach (1998).
5.3.3 C ONJUGATE

GRADIENT ASCENT

was used to train the above controller, with GPOMDP generating the gradient estimates from a range of values of fi and T . The influence of fi on the performance of the trained
controllers was marginal, so we set fi
: which gave the lowest-variance estimates. We used
the same value of T for calls to GPOMDP within CONJPOMDP and within GSEARCH, and this
was varied between
and ;
. The controller was always started from the same parameter
setting 
; ; (as was done by Marbach (1998)). The value of this initial policy is : . The
graph of the average reward of the final controller produced by CONJPOMDP as a function of the
total number of iterations of the queue is shown in Figure 12. A performance of :
was reliably
achieved with less than
iterations of the queue.
Note that the optimal policy is not achievable with this controller class since it is incapable
of implementing any threshold policy other than the always accept and always reject policies.
Although not provably optimal, a parameter setting of 1  : and any suitably large values of 2
and 3 generates something close to the optimal policy within the controller class, with an average
reward of : . Figure 13 shows the probability of accepting a call of each type under this policy
(with 2 3
), as a function of the available bandwidth.
The controllers produced by CONJPOMDP with fi
: and sufficiently large T are essentially
always accept controllers with an average reward of : , within 2% of the optimum achievable
in the class. To produce policies even nearer to the optimal policy in performance, CONJPOMDP
must keep 1 close to its starting value of , and hence the gradient estimate T
1; 2; 3
CONJPOMDP

= 00

= (8 8 8)

10

10 000

0 691

0 784

2000

75

08
= = 15

=00
0 784

8

 = (   )

5. There is some discrepancy between our average rewards and those quoted by Marbach (1998). This is probably due
to a discrepancy in the way the state transitions are counted, which was not clear from the discussion in (Marbach,
1998).

372

fiP OLICY-G RADIENT E STIMATION

CONJGRAD Final Reward

0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5

class optimal
beta=0.0

0.45
1000

10000
Total Queue Iterations

100000

Figure 12: Performance of the call admission controller trained by CONJPOMDP as a function of
the total number of iterations of the queue. The performance was computed by simulating the controller for 100,000 iterations. The average reward of the globally optimal
policy is : , the average reward of the optimal policy within the class is : , and
the plateau performance of CONJPOMDP is : . The graphs are averages from 100
independent runs.

0 804

08

0 784

1

Acceptance Probability

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
call type 1
call types 2 or 3

0.1
0
1

2

3

4
5
6
7
Available Bandwidth

8

9

10

Figure 13: Probability of accepting a call of each type under the call admission policy with nearoptimal parameters 1
: ; 2
3
. Note that calls of type 2 and 3 are
essentially always accepted.

= 75

=

373

= 15

fiBAXTER

ET AL .

1

Normalized Delta

0.8
0.6
0.4
0.2
0
-0.2
Delta1
Delta2
Delta3

-0.4
-0.6
0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Beta



1

Figure 14: Plot of the three components of T for the call admission problem, as a function of the
discount parameter fi . The parameters were set at 
; ; . T was set to ; ; .
Note that 1 does not become negative (the correct sign) until fi  : .

= (8 8 8)



0 93

1 000 000

produced by GPOMDP must have a relatively small first component. Figure 14 shows a plot of
normalized T as a function of fi , for T
; ;
(sufficiently large to ensure low variance
in T ) and the starting parameter setting 
; ; . From the figure, 1 starts at a high value
which explains why CONJPOMDP produces always accept controllers for fi
: , and does not
become negative until fi  : , a value for which the variance in T even for moderately large T
is relatively high.
A plot of the performance of CONJPOMDP for fi
: and fi
: is shown in Figure
15. Approximately half of the remaining 2% in performance can be obtained by setting fi
:,
while for fi
: a sufficiently large choice for T gives most of the remaining performance. For
this problem, there is a huge difference between gaining 98% of optimal performance, which is
achieved for fi
: and less than 2000 iterations of the queue, and gaining 99% of the optimal
which requires fi
: and of the order of 500,000 queue iterations. A similar convergence rate
and final approximation error to the latter case were reported for the on-line algorithms by Marbach
(1998, Chapter 7).





= 1 000 000
= (8 8 8)



0 93



= 09

= 0 95
= 00
= 09

= 0 95

=00

= 09

5.4 Mountainous Puck World
The mountain-car task is a well-studied problem in the reinforcement learning literature (Sutton
& Barto, 1998, Example 8.2). As shown in Figure 16, the task is to drive a car to the top of a onedimensional hill. The car is not powerful enough to accelerate directly up the hill against gravity, so
any successful controller must learn to oscillate back and forth until it builds up enough speed to
crest the hill.
In this section we describe a variant of the mountain car problem based on the puck-world
example of Section 5.2. With reference to Figure 17, in our problem the task is to navigate a puck
374

fiP OLICY-G RADIENT E STIMATION

0.805

0.802
CONJGRAD Final Reward

CONJGRAD Final Reward

0.8
0.8
0.795
0.79
0.785
0.78
0.775
100000

class optimal
beta=0.90
1e+06
Total Queue Iterations

0.798
0.796
0.794
0.792
0.79
0.788
0.786
0.784
class optimal
beta=0.95

0.782
0.78
1e+07

0

1e+07
2e+07
3e+07
Total Queue Iterations

4e+07

Figure 15: Performance of the call admission controller trained by CONJPOMDP as a function
of the total number of iterations of the queue. The performance was calculated by
simulating the controller for 1,000,000 iterations. The graphs are averages from 100
independent runs.

Figure 16: The classical mountain-car task is to apply forward or reverse thrust to the car to get
it over the crest of the hill. The car starts at the bottom and does not have enough power
to drive directly up the hill.

out of a valley and onto a plateau at the northern end of the valley. As in the mountain-car task, the
puck does not have sufficient power to accelerate directly up the hill, and so has to learn to oscillate
in order to climb out of the valley. Once again we were able to reliably train near-optimal neuralnetwork controllers for this problem, using CONJPOMDP and GSEARCH, and with GPOMDP
generating the gradient estimates.
5.4.1 T HE W ORLD
The world dimensions, physics, puck dynamics and controls were identical to the flat puck world
described in Section 5.2, except that the puck was subject to a constant gravitational force of
units, the maximum allowed thrust was units (instead of ), and the height of the world varied as

3

5

375

10

fiBAXTER

ET AL .

Figure 17: In our variant of the mountain-car problem the task is to navigate a puck out of a valley
and onto the northern plateau. The puck starts at the bottom of the valley and does not
have enough power to drive directly up the hill.

follows:

8
<

height

3

15
(x; y) = :7:5 1 cos  ( y2 50) 
25

< 25 or y > 75
otherwise :
if y

With only units of thrust, a unit mass puck can not accelerate directly out of the valley.
Every 120 (simulated) seconds, the puck was initialized with zero velocity at the bottom of
the valley, with a random x location. The puck was given no reward while in the valley or on the
southern plateau, and a reward of
s2 while on the northern plateau, where s was the speed
of the puck. We found the speed penalty helped to improve the rate of convergence of the neural
network controller.

100

5.4.2 T HE

CONTROLLER

After some experimentation we found that a neural-network controller could be reliably trained to
navigate to the northern plateau, or to stay on the northern plateau once there, but it was difficult to
combine both in the same controller (this is not so surprising since the two tasks are quite distinct).
To overcome this problem, we trained a switched neural-network controller: the puck used one
controller when in the valley and on the southern plateau, and then switched to a second neuralnetwork controller while on the northern plateau. Both controllers were one-hidden-layer neuralnetworks with nine input nodes, five hidden nodes and four output nodes. The nine inputs were the
normalized (
; -valued) x, y and z puck locations, the normalized x, y and z locations relative
to center of the northern wall, and the x, y and z puck velocities. The four outputs were used to
generate a policy in the same fashion as the controller of Section 5.2.2.
An approach requiring less prior knowledge would be to have a third controller that stochastically selects the base neural network controller as a function of the pucks location. This master

[ 1 1]

376

fiP OLICY-G RADIENT E STIMATION

80

Average Reward

70
60
50
40
30
20
10
0
0

2e+07

4e+07
6e+07
Iterations

8e+07

1e+08

Figure 18: Performance of the neural-network puck controller as a function of the number of iterations of the mountainous puck world, when trained using CONJPOMDP. Performance
estimates were generated by simulating for ;
;
iterations. Averaged over 100
independent runs.

1 000 000

controller could itself be parameterized and have its parameters trained along with the base controllers.
5.4.3 C ONJUGATE

GRADIENT ASCENT

The switched neural-network controller was trained using the same scheme discussed in Section 5.2.3, except this time the discount factor fi was set to : .
A plot of the average reward of the neural-network controller is shown in Figure 18, as a function
of the number of iterations of the POMDP. The graph is an average over 100 independent runs, with
the neural-network controller parameters initialized randomly in the range
: ; : at the start of
each run. In this case no run failed to converge to near-optimal performance. From the figure we
can see that the pucks performance is nearly optimal after about 40 million total iterations of the
puck world. Although this figure may seem rather high, to put it in some perspective note that a
random neural-network controller takes about 10,000 iterations to reach the northern plateau from a
standing start at the base of the valley. Thus, 40 million iterations is equivalent to only about 4,000
trips to the top for a random controller.
Note that the puck converges to a final average performance around 75, which indicates it is
spending at least 75% of its time on the northern plateau. Observation of the pucks final behaviour
shows it behaves nearly optimally in terms of oscillating back and forth to get out of the valley.

0 98

[ 0 1 0 1]

5.5 Choosing fi and the Running Time of GPOMDP
One aspect of these experiments that required some measure of tuning is the choice of the fi parameter and running time T used by GPOMDP. Although these were selected by trial and error, we have
377

fiBAXTER

ET AL .

had some success recently with a scheme for automatically choosing these parameters as follows.
Before any training begins, GPOMDP is run for a large number of iterations whilst simultaneously
generating gradient estimates for a number of different choices of fi . This can be done from a single
simulation simply by maintaining a separate eligibility trace zt for each value of fi . Since the bias
reduces with increasing fi , the largest fi that gives a reasonably low-variance gradient estimate at the
end of the long run is selected as a reference fi (the variance is estimated by comparing gradient
estimates at reasonably well-separated intervals towards the end of the run). Furthermore, since
the variance of the gradient estimate decreases as fi decreases, all gradient estimates for values of fi
smaller than the reference fi will typically have smaller variance than that of the reference fi . Hence,
we can reliably compare the directions for smaller fi s with the direction given by the reference fi ,
and choose the smallest fi whose corresponding direction is sufficiently close to the reference fi
direction. We takesufficiently close to mean within    .
Note that this scheme only works if the original run is sufficiently long to get a low-variance
direction estimate at the right value of fi . If the right value of fi is too large then any fixed bound on
the run length can be made to fail, but this will be a problem for all algorithms that automatically
choose fi .
Once a suitable fi has been found, we can go back and find the point in the original long run
where the direction estimate corresponding to that value of fi settled down (again, we measure
the variance of the estimates by sampling at suitably large intervals, and choose a point where the
variance falls below some chosen value). This time is then used as the running time T for GPOMDP
when estimating the gradient direction. Finally, the running time used in GPOMDP when bracketing
the maximum in GSEARCH can also be automatically tuned by starting with an initial fixed running
time that is a fraction of T , and then continuing until the sign of the inner product of the estimates
produced by GPOMDP with the search direction settles down. With this technique, the sign
estimation time is usually considerably smaller than the gradient direction estimation time.
Another useful heuristic is to re-estimate fi and GPOMDPs running time T whenever the parameters  change by a large amount, since a large change in  can lead to significant changes in the
mixing time of the POMDP.

10 15

6. Conclusion
This paper showed how to use the performance gradient estimates generated by the GPOMDP algorithm (Baxter & Bartlett, 2001) to optimize the average reward of parameterized POMDPs. We
described both a traditional on-line stochastic gradient algorithm and an off-line approach that
relied on the use of GSEARCH, a robust line-search algorithm that uses gradient estimates, rather
than value estimates, to bracket the maximum. The off-line approach in particular was found to perform well on four quite distinct problems: optimizing a controller for a three-state MDP, optimizing
a neural-network controller for navigating a puck around a two-dimensional world, optimizing a
controller for a call admission problem, and optimizing a switched neural-network controller in a
variation of the classical mountain-car task. One reason for the superiority of the off-line approach
is that by searching for a local maximum at each step it makes much more aggressive use of the
gradient information than does the on-line algorithm.
For the three-state MDP and the call-admission problems we were able to provide graphic illustrations of how the bias and variance of the gradient estimates rfi  can be traded against one another
by varying fi between (low variance, high bias) and (high variance, low bias).

0

1

378

fiP OLICY-G RADIENT E STIMATION

Relatively little tuning was required to generate these results. In addition, the controllers operated on direct and simple representations of the state, in contrast to the more complex representations
usually required of value-function based approaches.
It is often the case that value-function methods converge much more rapidly than their policygradient counterparts. This is due to the fact that they enforce constraints on the value-function.
With this in mind an interesting avenue for further research is Actor-Critic algorithms (Barto et al.,
1983; Baird & Moore, 1999; Kimura & Kobayashi, 1998; Konda & Tsitsiklis, 2000; Sutton,
McAllester, Singh, & Mansour, 2000) in which one attempts to combine the fast convergence of
value-functions with the theoretical guarantees of policy-gradient approaches.
Despite the success of the off-line approach in the experiments described here, the on-line algorithm has advantages in other settings. In particular, when it is applied to multi-agent reinforcement
learning, both gradient computations and parameter updates can be performed for distinct agents
without any communication beyond the global distribution of the reward signal. This idea has led to
a parameter optimization procedure for spiking neural networks, and some successful preliminary
results with network routing (Bartlett & Baxter, 1999; Tao, Baxter, & Weaver, 2001).
Acknowledgements
This work was supported by the Australian Research Council, and benefited from the comments of
several anonymous referees. Most of this research was performed while the first and second authors were with the Research School of Information Sciences and Engineering, Australian National
University.

References
Aberdeen, D., & Baxter, J. (2001). Policy-gradient learning of controllers with internal state. Tech. rep.,
Australian National University.
Baird, L., & Moore, A. (1999). Gradient descent for general reinforcement learning. In Advances in Neural
Information Processing Systems 11. MIT Press.
Bartlett, P. L., & Baxter, J. (1999). Hebbian synaptic modifications in spiking neurons that learn. Tech.
rep., Research School of Information Sciences and Engineering, Australian National University.
http://csl.anu.edu.au/bartlett/papers/BartlettBaxter-Nov99.ps.gz.
Bartlett, P. L., & Baxter, J. (2000a). Estimation and approximation bounds for gradient-based reinforcement
learning. In Proceedings of the Thirteenth Annual Conference on Computational Learning Theory,
pp. 133141.
Bartlett, P. L., & Baxter, J. (2000b). Stochastic optimization of controlled partially observable markov decision processes. In Proceedings of the 39th IEEE Conference on Decision and Control (CDC00).
Barto, A. G., Sutton, R. S., & Anderson, C. W. (1983). Neuronlike adaptive elements that can solve difficult
learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, SMC-13, 834846.
Baxter, J., & Bartlett, P. L. (2000). Reinforcement learning in POMDPs via direct gradient ascent. In
Proceedings of the Seventeenth International Conference on Machine Learning.
Baxter, J., & Bartlett, P. L. (2001). Infinite-horizon policy-gradient estimation. Journal of Artificial Intelligence Research. To appear.
Baxter, J., Tridgell, A., & Weaver, L. (2000). Learning to play chess using temporal-differences. Machine
Learning, 40(3), 243263.
379

fiBAXTER

ET AL .

Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Cao, X.-R., & Chen, H.-F. (1997). Perturbation Realization, Potentials, and Sensitivity Analysis of Markov
Processes. IEEE Transactions on Automatic Control, 42, 13821393.
Cao, X.-R., & Wan, Y.-W. (1998). Algorithms for Sensitivity Analysis of Markov Chains Through Potentials
and Perturbation Realization. IEEE Transactions on Control Systems Technology, 6, 482492.
Fine, T. L. (1999). Feedforward Neural Network Methodology. Springer, New York.
Fu, M. C., & Hu, J. (1994). Smooth Perturbation Derivative Estimation for Markov Chains. Operations
Research Letters, 15, 241251.
Glynn, P. W. (1986). Stochastic approximation for monte-carlo optimization. In Proceedings of the 1986
Winter Simulation Conference, pp. 356365.
Kimura, H., & Kobayashi, S. (1998). An analysis of actor/critic algorithms using eligibility traces: Reinforcement learning with imperfect value functions. In Fifteenth International Conference on Machine
Learning, pp. 278286.
Kimura, H., Miyazaki, K., & Kobayashi, S. (1997). Reinforcement learning in POMDPs with function
approximation. In Fisher, D. H. (Ed.), Proceedings of the Fourteenth International Conference on
Machine Learning (ICML97), pp. 152160.
Kimura, H., Yamamura, M., & Kobayashi, S. (1995). Reinforcement learning by stochastic hill climbing
on discounted reward. In Proceedings of the Twelfth International Conference on Machine Learning
(ICML95), pp. 295303.
Konda, V. R., & Tsitsiklis, J. N. (2000). Actor-Critic Algorithms. In Neural Information Processing Systems
1999. MIT Press.
Marbach, P. (1998). Simulation-Based Methods for Markov Decision Processes. Ph.D. thesis, Laboratory for
Information and Decision Systems, MIT.
Marbach, P., & Tsitsiklis, J. N. (1998). Simulation-Based Optimization of Markov Reward Processes. Tech.
rep., MIT.
Rubinstein, R. Y., & Melamed, B. (1998). Modern Simulation and Modeling. Wiley, New York.
Samuel, A. L. (1959). Some Studies in Machine Learning Using the Game of Checkers. IBM Journal of
Research and Development, 3, 210229.
Singh, S. P., Jaakkola, T., & Jordan, M. I. (1994). Learning Without State-Estimation in Partially Observable
Markovian Decision Processes. In Proceedings of the Eleventh International Conference on Machine
Learning.
Singh, S., & Bertsekas, D. (1997). Reinforcement learning for dynamic channel allocation in cellular telephone systems. In Advances in Neural Information Processing Systems: Proceedings of the 1996
Conference, pp. 974980. MIT Press.
Singh, S., Jaakkola, T., & Jordan, M. (1995). Reinforcement learning with soft state aggregation. In Tesauro,
G., Touretzky, D., & Leen, T. (Eds.), Advances in Neural Information Processing Systems, Vol. 7. MIT
Press, Cambridge, MA.
Sutton, R. (1988). Learning to Predict by the Method of Temporal Differences. Machine Learning, 3, 944.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press, Cambridge MA.
ISBN 0-262-19398-1.
Sutton, R. S., McAllester, D., Singh, S., & Mansour, Y. (2000). Policy Gradient Methods for Reinforcement
Learning with Function Approximation. In Neural Information Processing Systems 1999. MIT Press.
380

fiP OLICY-G RADIENT E STIMATION

Tao, N., Baxter, J., & Weaver, L. (2001). A multi-agent, policy-gradient approach to network routing. Tech.
rep., Australian National University.
Tesauro, G. (1992). Practical Issues in Temporal Difference Learning. Machine Learning, 8, 257278.
Tesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves master-level play. Neural
Computation, 6, 215219.
Weaver, L., & Baxter, J. (1999). Reinforcement learning from state and temporal differences. Tech. rep.,
Australian National University.
Williams, R. J. (1992). Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement
Learning. Machine Learning, 8, 229256.
Zhang, W., & Dietterich, T. (1995). A reinforcement learning approach to job-shop scheduling. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, pp. 11141120.
Morgan Kaufmann.

381

fiJournal of Artificial Intelligence Research 15 (2001) 319-350

Submitted 9/00; published 11/01

Infinite-Horizon Policy-Gradient Estimation
Jonathan Baxter

JBAXTER @ WHIZBANG . COM

WhizBang! Labs.
4616 Henry Street Pittsburgh, PA 15213

Peter L. Bartlett

BARTLETT @ BARNHILLTECHNOLOGIES . COM

BIOwulf Technologies.
2030 Addison Street, Suite 102, Berkeley, CA 94704

Abstract
Gradient-based approaches to direct policy search in reinforcement learning have received
much recent attention as a means to solve problems of partial observability and to avoid some of
the problems associated with policy degradation in value-function methods. In this paper we introduce GPOMDP, a simulation-based algorithm for generating a biased estimate of the gradient of
the average reward in Partially Observable Markov Decision Processes (POMDPs) controlled by
parameterized stochastic policies. A similar algorithm was proposed by Kimura, Yamamura, and
Kobayashi (1995). The algorithms chief advantages are that it requires storage of only twice the
number of policy parameters, uses one free parameter fi 2 [0; 1) (which has a natural interpretation
in terms of bias-variance trade-off), and requires no knowledge of the underlying state. We prove
convergence of GPOMDP, and show how the correct choice of the parameter fi is related to the
mixing time of the controlled POMDP. We briefly describe extensions of GPOMDP to controlled
Markov chains, continuous state, observation and control spaces, multiple-agents, higher-order
derivatives, and a version for training stochastic policies with internal states. In a companion paper
(Baxter, Bartlett, & Weaver, 2001) we show how the gradient estimates generated by GPOMDP
can be used in both a traditional stochastic gradient algorithm and a conjugate-gradient procedure
to find local optima of the average reward.

1. Introduction
Dynamic Programming is the method of choice for solving problems of decision making under
uncertainty (Bertsekas, 1995). However, the application of Dynamic Programming becomes problematic in large or infinite state-spaces, in situations where the system dynamics are unknown, or
when the state is only partially observed. In such cases one looks for approximate techniques that
rely on simulation, rather than an explicit model, and parametric representations of either the valuefunction or the policy, rather than exact representations.
Simulation-based methods that rely on a parametric form of the value function tend to go by
the name Reinforcement Learning, and have been extensively studied in the Machine Learning
literature (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998). This approach has yielded some
remarkable empirical successes in a number of different domains, including learning to play checkers (Samuel, 1959), backgammon (Tesauro, 1992, 1994), and chess (Baxter, Tridgell, & Weaver,
2000), job-shop scheduling (Zhang & Dietterich, 1995) and dynamic channel allocation (Singh &
Bertsekas, 1997).
Despite this success, most algorithms for training approximate value functions suffer from the
same theoretical flaw: the performance of the greedy policy derived from the approximate valuefunction is not guaranteed to improve on each iteration, and in fact can be worse than the old policy

c 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiBAXTER & BARTLETT

by an amount equal to the maximum approximation error over all states. This can happen even when
the parametric class contains a value function whose corresponding greedy policy is optimal. We
illustrate this with a concrete and very simple example in Appendix A.
An alternative approach that circumvents this problemthe approach we pursue hereis to
consider a class of stochastic policies parameterized by  2 R K , compute the gradient with respect
to  of the average reward, and then improve the policy by adjusting the parameters in the gradient
direction. Note that the policy could be directly parameterized, or it could be generated indirectly
from a value function. In the latter case the value-function parameters are the parameters of the
policy, but instead of being adjusted to minimize error between the approximate and true value
function, the parameters are adjusted to directly improve the performance of the policy generated
by the value function.
These policy-gradient algorithms have a long history in Operations Research, Statistics, Control Theory, Discrete Event Systems and Machine Learning. Before describing the contribution of
the present paper, it seems appropriate to introduce some background material explaining this approach. Readers already familiar with this material may want to skip directly to section 1.2, where
the contributions of the present paper are described.
1.1 A Brief History of Policy-Gradient Algorithms
For large-scale problems or problems where the system dynamics are unknown, the performance
gradient will not be computable in closed form1 . Thus the challenging aspect of the policy-gradient
approach is to find an algorithm for estimating the gradient via simulation. Naively, the gradient
can be calculated numerically by adjusting each parameter in turn and estimating the effect on performance via simulation (the so-called crude Monte-Carlo technique), but that will be prohibitively
inefficient for most problems. Somewhat surprisingly, under mild regularity conditions, it turns out
that the full gradient can be estimated from a single simulation of the system. The technique is
called the score function or likelihood ratio method and appears to have been first proposed in the
sixties (Aleksandrov, Sysoyev, & Shemeneva, 1968; Rubinstein, 1969) for computing performance
gradients in i.i.d. (independently and identically distributed) processes.
Specifically, suppose r X is a performance function that depends on some random variable
X , and q ; x is the probability that X x, parameterized by  2 RK . Under mild regularity
conditions, the gradient with respect to  of the expected performance,

( )

( )

may be written

To see this, rewrite (1) as a sum

=

() = Er(X );

(1)

r() = Er(X ) rqq(;(;XX)) :

(2)

() =

X

r() =

X

r(x)q(; x);

x
differentiate (one source of the requirement of mild regularity conditions) to obtain
x

r(x)rq(; x);

1. See equation (17) for a closed-form expression for the performance gradient.

320

fiP OLICY-G RADIENT E STIMATION

rewrite as

X

r() =

r(x)

rq(; x) q(; x);

( )

q ; x
x
and observe that this formula is equivalent to (2).
If a simulator is available to generate samples X distributed according to q ; x , then any
sequence X1 ; X2 ; : : : ; XN generated i.i.d. according to q ; x gives an unbiased estimate,

( )

( )
N
X
r^ () = N1 r(Xi) rqq(;(;XX)i) ;
(3)
i
i
^ () ! r() with probability one. The quantity
of r ( ). By the law of large numbers, r
rq(; X )=q(; X ) is known as the likelihood ratio or score function in classical statistics. If
the performance function r (X ) also depends on  , then r (X )rq (; X )=q (; X ) is replaced by
rr(; X ) + r(; X )rq(; X )=q(; X ) in (2).
=1

1.1.1 U NBIASED E STIMATES
P ROCESSES

OF THE

P ERFORMANCE G RADIENT

FOR

R EGENERATIVE

Extensions of the likelihood-ratio method to regenerative processes (including Markov Decision
Processes or MDPs) were given by Glynn (1986, 1990), Glynn and LEcuyer (1995) and Reiman
and Weiss (1986, 1989), and independently for episodic Partially Observable Markov Decision
Processes (POMDPs) by Williams (1992), who introduced the REINFORCE algorithm2 . Here the
i.i.d. samples X of the previous section are sequences of states X0 ; : : : ; XT (of random length)
encountered between visits to some designated recurrent state i , or sequences of states from some
start state to a goal state. In this case rq ; X =q ; X can be written as a sum

(

) (

)

rq(; X ) = TX rpXtXt+1 () ;
1

q(; X )

()

t=0

pXt Xt+1 ()

(4)

where pXt Xt+1  is the transition probability from Xt to Xt+1 given parameters  . Equation (4)
admits a recursive computation over the course of a regenerative cycle of the form z0
2 RK ,
and after each state transition Xt ! Xt+1 ,

=0

zt+1 = zt +

( ) (
)

) (

)

rpXtXt+1 () ;
pXt Xt+1 ()

so that each term r X rq ; X =q ; X in the estimate (3) is of the form3
in addition, r X0 ; : : : ; XT can be recursively computed by

(

(5)

r(X0 ; : : : ; XT )zT . If,

r(X0 ; : : : ; Xt+1 ) = (r(X0 ; : : : ; Xt ); Xt+1 )
for some function , then the estimate r (X0 ; : : : ; XT )zT for each cycle can be computed using
storage of only K + 1 parameters (K for zt and 1 parameter to update the performance function
r). Hence, the entire estimate (3) can be computed with storage of only 2K + 1 real parameters, as
follows.

2. A thresholded version of these algorithms for neuron-like elements was described earlier in Barto, Sutton, and Anderson (1983).
3. The vector zT is known in reinforcement learning as an eligibility trace. This terminology is used in Barto et al.
(1983).

321

fiBAXTER & BARTLETT

Algorithm 1.1: Policy-Gradient Algorithm for Regenerative Processes.
1. Set j

= 0, r = 0, z = 0, and  = 0 (z ;  2 RK ).
0

0

0

2. For each state transition Xt




3. If j

0

! Xt

+1

0

:

If the episode is finished (that is, Xt+1
j +1
j rt zt ,
j j ,
zt+1 ,
rt+1 .

 = +
= +1
=0
=0
Otherwise, set

rp

= i), set



zt+1 = zt + pXXtXtXt+1() ;
t+1
rt+1 = (rt ; Xt+1 ).
( )

= N return N =N , otherwise goto 2.

Examples of recursive
performance functions include the sum of a scalar reward over a cycle,
P

r(X0 ; : : : ; XT ) = Tt=0 r(Xt ) where r(i) is a scalar reward associated with state i (this corresponds to  ( ) being the average reward multiplied by the expected recurrence time E [T ]); the

1
()

negative length of the cycle (which can be implemented by assigning a reward of
to each state,
and is used when the task is to mimimize time taken to get to a goal state, since
  in this case is
PT
t
just E T ); the discounted reward from the start state, r X0 ; : : : ; XT
t=0 ff r Xt , where
ff 2 ; is the discount factor, and so on.
As Williams (1992) pointed out, a further simplification is possible in the case that rT
r X0 ; : : : ; XT is a sum of scalar rewards r Xt ; t depending on the state and possibly the time
t since the starting state (such as r Xt ; t r Xt , or r Xt ; t fft r Xt as above). In that case,
the update from a single regenerative cycle may be written as

[ ]
[0 1)

(

(

)

(



=

TX1
t=0

pXt Xt+1 ()

" t
X

s=0

( )

=

( )
)= ( ) (

rpXtXt+1 ()

()

)=

)=

r(Xs ; s) +

T
X
s=t+1

( )

#

r(Xs ; s) :

(

)

Because changes in pXt Xt+1  have no influence on the rewards r Xs ; s associated with earlier
states (s  t), we should be able to drop the first term in the parentheses on the right-hand-side and
write
TX1
T
rpXtXt+1  X
r X ;s :
(6)
p
 s=t+1 s
t=0 Xt Xt+1

=

()
()

(

)

Although the proof is not entirely trivial, this intuition can indeed be shown to be correct.
Equation (6) allows an even simpler recursive formula for estimating the performance gradient. Set z0
, and introduce a new variable s
. As before, set zt+1
zt
0
rpXtXt+1  =pXtXt+1  and s s if Xt+1 6 i , or s
and zt+1
otherwise. But
now, on each iteration, set t+1 r Xt ; s zt
t . Then t =t is our estimate of r  . Since t
is updated on every iteration, this suggests that we can do away with t altogether and simply update  directly: t+1 t t r Xt ; s zt , where the t are suitable step-sizes4 . Proving convergence

()

P

= =0
()

= +1
=
 = ( ) +
= + ( )



=0
=0

=0



4. The usual requirements on t for convergence of a stochastic gradient algorithm are t > 0,
1 2 < .
t=0 t

1

322

=

()

P1t=0 t

=

+


1, and

fiP OLICY-G RADIENT E STIMATION

of such an algorithm is not as straightforward as normal stochastic gradient algorithms because the
updates r Xt zt are not in the gradient direction (in expectation), although the sum of these updates
over a regenerative cycle are. Marbach and Tsitsiklis (1998) provide the only convergence proof that
we know of, albeit for a slightly different update of the form t+1
t t r Xt ; s  t zt ,
where  t is a moving estimate of the expected performance, and is also updated on-line (this
update was first suggested in the context of POMDPs by Jaakkola et al. (1995)).
Marbach and Tsitsiklis (1998) also considered the case of  -dependent rewards (recall the discussion after (3)), as did Baird and Moore (1999) with their VAPS algorithm (Value And Policy
Search). This last paper contains an interesting insight: through suitable choices of the performance
function r X0 ; : : : ; XT ;  , one can combine policy-gradient search with approximate value function methods. The resulting algorithms can be viewed as actor-critic techniques in the spirit of Barto
et al. (1983); the policy is the actor and the value function is the critic. The primary motivation is
to reduce variance in the policy-gradient estimates. Experimental evidence for this phenomenon
has been presented by a number of authors, including Barto et al. (1983), Kimura and Kobayashi
(1998a), and Baird and Moore (1999). More recent work on this subject includes that of Sutton
et al. (2000) and Konda and Tsitsiklis (2000). We discuss the use of VAPS-style updates further in
Section 6.2.
So far we have not addressed the question of how the parameterized state-transition probabilities pXt Xt+1  arise. Of course, they could simply be generated by parameterizing the matrix of
transition probabilities directly. Alternatively, in the case of MDPs or POMDPs, state transitions
are typically generated by feeding an observation Yt that depends stochastically on the state Xt
into a parameterized stochastic policy, which selects a control Ut at random from a set of available controls (approximate value-function based approaches that generate controls stochastically
via some form of lookahead also fall into this category). The distribution over successor states
pXt Xt+1 Ut is then a fixed function of the control. If we denote the probability of control ut given
parameters  and observation yt by ut ; yt , then all of the above discussion carries through with
rpXtXt+1  =pXtXt+1  replaced by rUt ; Yt =Ut ; Yt . In that case, Algorithm 1.1 is precisely Williams REINFORCE algorithm.
Algorithm 1.1 and the variants above have been extended to cover multiple agents (Peshkin
et al., 2000), policies with internal state (Meuleau et al., 1999), and importance sampling methods
(Meuleau et al., 2000). We also refer the reader to the work of Rubinstein and Shapiro (1993)
and Rubinstein and Melamed (1998) for in-depth analysis of the application of the likelihood-ratio
method to Discrete-Event Systems (DES), in particular networks of queues. Also worth mentioning
is the large literature on Infinitesimal Perturbation Analysis (IPA), which seeks a similar goal of estimating performance gradients, but operates under more restrictive assumptions than the likelihoodratio approach; see, for example, Ho and Cao (1991).

( )

= + [(

^( )

(

) ^( )]

)

()

( )
()

(

()

1.1.2 B IASED E STIMATES

OF THE

)
(

)

(

)

P ERFORMANCE G RADIENT

All the algorithms described in the previous section rely on an identifiable recurrent state i , either
to update the gradient estimate, or in the case of the on-line algorithm, to zero the eligibility trace
z . This reliance on a recurrent state can be problematic for two main reasons:
1. The variance of the algorithms is related to the recurrence time between visits to i , which
will typically grow as the state space grows. Furthermore, the time between visits depends on
323

fiBAXTER & BARTLETT

the parameters of the policy, and states that are frequently visited for the initial value of the
parameters may become very rare as performance improves.
2. In situations of partial observability it may be difficult to estimate the underlying states, and
therefore to determine when the gradient estimate should be updated, or the eligibility trace
zeroed.
If the system is available only through simulation, it seems difficult (if not impossible) to obtain
unbiased estimates of the gradient direction without access to a recurrent state. Thus, to solve 1
and 2, we must look to biased estimates. Two principle techniques for introducing bias have been
proposed, both of which may be viewed as artificial truncations of the eligibility trace z . The first
method takes as a starting point the formula5 for the eligibility trace at time t:

zt =

t 1
X

rpXsXs+1 ()
pXs Xs+1 ()

s=0

and simply truncates it at some (fixed, not random) number of terms n looking backwards (Glynn,
1990; Rubinstein, 1991, 1992; Cao & Wan, 1998):

zt (n) :=

t 1
X
s=t n

rpXsXs+1 () :
pXs Xs+1 ()

(7)

(n) is then updated after each transition Xt ! Xt by
rp
() rpXt nXt n+1 () ;
zt (n) = zt (n) + XtXt+1
pXt Xt+1 ()
pXt n Xt n+1 ()
and in the case of state-based rewards r (Xt ), the estimated gradient direction after T steps is
T
^rn() := 1 X zt (n)r(Xt ):
T n+1 t n
The eligibility trace zt

+1

+1

(8)

(9)

=

Unless n exceeds the maximum recurrence time (which is infinite in an ergodic Markov chain),
rn  is a biased estimate of the gradient direction, although as n ! 1, the bias approaches zero.
However the variance of rn   diverges in the limit of large n. This illustrates a natural trade-off
in the selection of the parameter n: it should be large enough to ensure the bias is acceptable (the
expectation of rn   should at least be within  of the true gradient direction), but not so large
that the variance is prohibitive. Experimental results by Cao and Wan (1998) illustrate nicely this
bias/variance trade-off.
One potential difficulty with this method is that the likelihood ratios rpXs Xs+1  =pXs Xs+1 
must be remembered for the previous n time steps, requiring storage of Kn parameters. Thus,
to obtain small bias, the memory may have to grow without bound. An alternative approach that
requires a fixed amount of memory is to discount the eligibility trace, rather than truncating it:

^ ()

^ ()

^ ()

90

()

zt+1 (fi ) := fizt (fi ) +

rpXtXt+1 () ;
pXt Xt+1 ()

()

(10)

r

5. For ease of exposition, we have kept the expression for z in terms of the likelihood ratios pXs Xs+1 ()=pXs Xs+1 ()
which rely on the availability of the underlying state Xs . If Xs is not available, pXs Xs+1 ()=pXs Xs+1 () should
be replaced with Us (; Ys )=Us (; Ys ).

r

r

324

fiP OLICY-G RADIENT E STIMATION

( )=0

where z0 fi
and fi
after T steps is simply

2 [0; 1) is a discount factor.
r^fi () := T1

TX1
t=0

In this case the estimated gradient direction

r(Xt )zt (fi ):

(11)

( ) ()
(( ) )

This is precisely the estimate we analyze in the present paper. A similar estimate with r Xt zt fi
replaced by r Xt
b zt fi where b is a reward baseline was proposed by Kimura et al. (1995,
1997) and for continuous control by Kimura and Kobayashi (1998b). In fact the use of r Xt
b
in place of r Xt does not affect the expectation of the estimates of the algorithm (although judicious choice of the reward baseline b can reduce the variance of the estimates). While the algorithm
presented by Kimura et al. (1995) provides estimates of the expectation under the stationary distribution of the gradient of the discounted reward, we will show that these are in fact biased estimates
of the gradient of the expected discounted reward. This arises because the stationary distribution
itself depends on the parameters. A similar estimate to (11) was also proposed by Marbach and
Tsitsiklis (1998), but this time with r Xt zt fi replaced by r Xt
  zt fi , where   is an
estimate of the average reward, and with zt zeroed on visits to an identifiable recurrent state.
As a final note, observe that the eligibility traces zt fi and zt n defined by (10) and (8) are
simply filtered versions of the sequence rpXt Xt+1  =pXt Xt+1  , a first-order, infinite impulse
response filter in the case of zt fi and an n-th order, finite impulse response filter in the case of
zt n . This raises the question, not addressed in this paper, of whether there is an interesting theory
of optimal filtering for policy-gradient estimators.

(( )
( )

) ()

( ) ()

( ( ) ^( )) ( )

()

()

()

()

^( )

()
()

1.2 Our Contribution
We describe GPOMDP, a general algorithm based upon (11) for generating a biased estimate of the
performance gradient r  in general POMDPs controlled by parameterized stochastic policies.
Here   denotes the average reward of the policy with parameters  2 RK . GPOMDP does
not rely on access to an underlying recurrent state. Writing rfi   for the expectation of the estimate produced by GPOMDP, we show that
r  , and more quantitatively that
fi !1 rfi  
rfi   is close to the true gradient provided = fi exceeds the mixing time of the Markov chain
induced by the POMDP6 . As with the truncated estimate above, the trade-off preventing the setting
of fi arbitrarily close to is that the variance of the algorithms estimates increase as fi approaches
. We prove convergence with probability 1 of GPOMDP for both discrete and continuous observation and control spaces. We present algorithms for both general parameterized Markov chains and
POMDPs controlled by parameterized stochastic policies.
There are several extensions to GPOMDP that we have investigated since the first version of
this paper was written. We outline these developments briefly in Section 7.
In a companion paper we show how the gradient estimates produced by GPOMDP can be used
to perform gradient ascent on the average reward   (Baxter et al., 2001). We describe both
traditional stochastic gradient algorithms, and a conjugate-gradient algorithm that utilizes gradient
estimates in a novel way to perform line searches. Experimental results are presented illustrat-

()

()

()
lim
( )= ( )
1 (1 )

()

1

1

()

6. The mixing-time result in this paper applies only to Markov chains with distinct eigenvalues. Better estimates of the
bias and variance of GPOMDP may be found in Bartlett and Baxter (2001), for more general Markov chains than
those treated here, and for more refined notions of the mixing time. Roughly speaking, the variance of GPOMDP
grows with 1=(1 fi ), while the bias decreases as a function of 1=(1 fi ).

325

fiBAXTER & BARTLETT

ing both the theoretical results of the present paper on a toy problem, and practical aspects of the
algorithms on a number of more realistic problems.

2. The Reinforcement Learning Problem
We model reinforcement learning as a Markov decision process (MDP) with a finite state space
S f ; : : : ; ng, and a stochastic matrix7 P pij giving the probability of transition from state
i to state j . Each state i has an associated reward8 r i . The matrix P belongs to a parameterized
class of stochastic matrices, P
fP   2 RK g. Denote the Markov chain corresponding to
P  by M  . We assume that these Markov chains and rewards satisfy the following assumptions:

= 1

()

=[ ]

()

:= ( ):

()

()

Assumption 1. Each P  2 P has a unique stationary distribution 
satisfying the balance equations

() := [(; 1); : : : ; (; n)]0

0 ()P () = 0 ()
(throughout  0 denotes the transpose of  ).
Assumption 2. The magnitudes of the rewards, jr (i)j, are uniformly bounded by R <
states i.

(12)

1 for all

Assumption 1 ensures that the Markov chain forms a single recurrent class for all parameters  .
Since any finite-state Markov chain always ends up in a recurrent class, and it is the properties of
this class that determine the long-term average reward, this assumption is mainly for convenience
so that we do not have to include the recurrence class as a quantifier in our theorems. However,
when we consider gradient-ascent algorithms Baxter et al. (2001), this assumption becomes more
restrictive since it guarantees that the recurrence class cannot change as the parameters are adjusted.
Ordinarily, a discussion of MDPs would not be complete without some mention of the actions
available in each state and the space of policies available to the learner. In particular, the parameters
 would usually determine a policy (either directly or indirectly via a value function), which would
then determine the transition probabilities P  . However, for our purposes we do not care how
the dependence of P on  arises, just that it satisfies Assumption 1 (and some differentiability
assumptions that we shall meet in the next section). Note also that it is easy to extend this setup
to the case where the rewards also depend on the parameters  or on the transitions i ! j . It is
equally straightforward to extend our algorithms and results to these cases. See Section 6.1 for an
illustration.
The goal is to find a  2 R K maximizing the average reward:
fi
"
#
TX1
fi
fi

E
r Xt fi X0 i ;
fi
T !1  T
t=0
where E denotes the expectation over all sequences X0 ; X1 ; : : : ; with transitions generated according to P  . Under Assumption 1,   is independent of the starting state i and is equal to
n
X

 ; i r i 0  r;
(13)
i=1

()

1

( ) := lim

()

=

()

( )=

where r

( )

( ) ()= ( )

= [r(1); : : : ; r(n)]0 (Bertsekas, 1995).


P

7. A stochastic matrix P = [pij ] has pij 0 for all i; j and n
j =1 pij = 1 for all i.
8. All the results in the present paper apply to bounded stochastic rewards, in which case r(i) is the expectation of the
reward in state i.

326

fiP OLICY-G RADIENT E STIMATION

3. Computing the Gradient of the Average Reward

()

For general MDPs little will be known about the average reward   , hence finding its optimum
will be problematic. However, in this section we will see that under general assumptions the gradient
r  exists, and so local optimization of   is possible.
To ensure the existence of suitable gradients (and the boundedness of certain random variables),
we require that the parameterized class of stochastic matrices satisfies the following additional assumption.

()

()

Assumption 3. The derivatives,

rP () :=
2 RK . The ratios

exist for all 



@pij ()
@k

i;j =1:::n;k=1:::K

2 fifi @p () fifi 3
ij
fi @k fi
4
5

pij ()

are uniformly bounded by B



i;j =1:::n;k=1:::K

< 1 for all  2 RK .

The second part of this assumption allows zero-probability transitions pij

() = 0 only if

rpij () is also zero, in which case we set 0=0 = 0. One example is if i ! j is a forbidden
transition, so that pij ( ) = 0 for all  2 RK . Another example satisfying the assumption is
pij () =
where 

= [

11

; : : : ; 1n ; : : : ; nn ] 2 Rn2

are the parameters of P

@pij ()=@ij
pij ()
@pij ()=@kl
pij ()

Assuming for the moment that r
dependencies,

eij
ij ;
j =1 e

Pn

=1
=

pij ();

(), for then

and

pkl ():

() exists (this will be justified shortly), then, suppressing 
r = r0r;
(14)

since the reward r does not depend on  . Note that our convention for r in this paper is that it takes
precedence over all other operations, so rg  f 
rg  f  . Equations like (14) should be
regarded as shorthand notation for K equations of the form

( ) ( ) = [ ( )] ( )

@()
@k

where k

=





@(; 1)
@(; n)
;:::;
[r(1); : : : ; r(n)]0
@k
@k

= 1; : : : ; K . To compute r, first differentiate the balance equations (12) to obtain
r0P + 0 rP = r0;
327

fiBAXTER & BARTLETT

and hence

r0(I P ) = 0 rP:

(15)

The system of equations defined by (15) is under-constrained because I P is not invertible (the
balance equations show that I P has a left eigenvector with zero eigenvalue). However, let e
denote the n-dimensional column vector consisting of all s, so that e 0 is the n  n matrix with the
stationary distribution  0 in each row. Since r 0 e r  0 e
r
, we can rewrite (15) as

1

= ( ) = (1) = 0





r0 I (P e0) = 0rP:
To see that the inverse
Then we can write

[I (P
"

lim (I

e0 )]
A)

T !1

T
X
t=0

1

exists, let A be any matrix satisfying
#

= Tlim
!1

At

[

t=0

= I Tlim
!1
= I:

Thus,

(I

" T
X

A)

1

=

1
X
t=0

TX
+1

At

AT +1

t=1

limt!1 At = 0.

#

At

At :

] =

0

It is easy to prove by induction that P e 0 t
P t eP0 which
to as t ! 1 by
 tconverges
1
0
0 . Hence, we can write
Assumption 1. So I
P e
exists and is equal to 1
P
e
t=0

[

(

)]



r0 = 0rP I P + e0
and so9



r = 0rP I P + e0





;

(16)

r:

(17)

1

1

For MDPs with a sufficiently small number of states, (17) could be solved exactly to yield the precise
gradient direction. However, in general, if the state space is small enough that an exact solution of
(17) is possible, then it will be small enough to derive the optimal policy using policy iteration and
table-lookup, and there would be no point in pursuing a gradient based approach in the first place10 .
Thus, for problems of practical interest, (17) will be intractable and we will need to find some
other way of computing the gradient. One approximate technique for doing this is presented in the
next section.
9. The argument leading to (16) coupled with the fact that  () is the unique solution to (12) can be used to justify the
existence of  . Specifically, we can run through the same steps computing the value of  ( +  ) for small  and
show that the expression (16) for  is the unique matrix satisfying  ( +  ) =  () +   () + O(  2 ).
10. Equation (17) may still be useful for POMDPs, since in that case there is no tractable dynamic programming
algorithm.

r

r

r

328

kk

fiP OLICY-G RADIENT E STIMATION

4. Approximating the Gradient in Parameterized Markov Chains
In this section, we show that the gradient can be split into two components, one of which becomes
negligible as a discount factor fi approaches .
For all fi 2 ; , let Jfi 
Jfi ; ; : : : ; Jfi ; n denote the vector of expected discounted
rewards from each state i:

[0 1)

( ) = [ ( 1)

Jfi (; i) := E

1

"

( )]

1
X
t=0

fitr

fi
fi
Xt fifi X0
fi

( )

#

=i

:

(18)

Where the  dependence is obvious, we just write Jfi .

2 [0; 1),
r = (1 fi )r0 Jfi + fi0rP Jfi :

Proposition 1. For all  2 R K and fi

(19)

Proof. Observe that Jfi satisfies the Bellman equations:

Jfi = r + fiP Jfi :

(20)

(Bertsekas, 1995). Hence,

r = r0r
= r0 [Jfi fiP Jfi ]
= r0Jfi fi r0Jfi + fi0 rP Jfi
= (1 fi )r0 Jfi + fi0rP Jfi :

by (15)

We shall see in the next section that the second term in (19) can be estimated from a single sample path of the Markov chain. In fact, Theorem 1 in (Kimura et al., 1997) shows that the gradient
estimates of the algorithm presented in that paper converge to
fi 0 rJfi . By the Bellman equations (20), this is equal to
fi fi 0 rP Jfi 0 rJfi , which implies
fi 0 rJfi fi0 rP Jfi .
Thus the algorithm of Kimura et al. (1997) also estimates the second term in the expression for
r  given by (19). It is important to note that 0rJfi 6 r 0 Jfi the two quantities disagree
by the first term in (19). This arises because the the stationary distribution itself depends on the
parameters. Hence, the algorithm of Kimura et al. (1997) does not estimate the gradient of the expected discounted reward. In fact, the expected discounted reward is simply =
fi times the
average reward   (Singh et al., 1994, Fact 7), so the gradient of the expected discounted reward
is proportional to the gradient of the average reward.
The following theorem shows that the first term in (19) becomes negligible as fi approaches .
Notice that this is not immediate from Proposition 1, since Jfi can become arbitrarily large in the
limit fi ! .

(1 ) (

+

(1

)

= [

]

)

()

(1 )

=

1 (1

()

)

1

1

Theorem 2. For all  2 RK ,

r = filim
r ;
! fi

(21)

rfi  := 0 rP Jfi :

(22)

1

where

329

fiBAXTER & BARTLETT

Proof. Recalling equation (17) and the discussion preceeding it, we have 11

r = 0rP
But rP e

1 
X



e0 r:

Pt

t=0

(23)

= r(P e) = r(1) = 0 since P is a stochastic matrix, so (23) can be rewritten as
r = 0

"
1
X

#

rP P t r:

t=0

(24)

2 [0; 1] be a discount factor and consider the expression

Now let fi

f (fi ) := 0

= lim
( )=

()

"
1
X

t=0

#

rP (fiP )t r

(25)

( )=

Clearly r
rfi .
fi !1 f fi . To complete the proof we just need to show that f fi
t
t
t
t
0
Since fiP
fi P ! fi e ! , we can invoke the observation before (16) to write

0

1
X
t=0
P

In particular, 1
t=0
of (25) and write12

(fiP )t = [I

(fiP )t converges, so we can take rP back out of the sum in the right-hand-side
f (fi ) = 0 rP

But

P1

t=0


fitP t r

fiP ] 1 :

= Jfi . Thus f (fi ) = 0rP Jfi

"1
X

t=0

#

fitP t

r:

(26)

= rfi .

1

Theorem 2 shows that rfi  is a good approximation to the gradient as fi approaches , but it
turns out that values of fi very close to lead to large variance in the estimates of rfi  that we
describe in the next section. However, the following theorem shows that
fi need not be too
small, provided the transition probability matrix P  has distinct eigenvalues, and the Markov
chain has a short mixing time. From any initial state, the distribution over states of a Markov chain
converges to the stationary distribution, provided the assumption (Assumption 1) about the existence
and uniqueness of the stationary distribution is satisfied (see, for example, Lancaster & Tismenetsky,
1985, Theorem 15.8.1, p. 552). The spectral resolution theorem (Lancaster & Tismenetsky, 1985,
Theorem 9.5.1, p. 314) implies that the distribution converges to stationarity at an exponential rate,
and the time constant in this convergence rate (the mixing time) depends on the eigenvalues of
the transition probability matrix. The existence of a unique stationary distribution implies that the

1

1

()

11. Since e 0 r = e , (23) motivates a different kind of algorithm for estimating  based on differential rewards
(Marbach & Tsitsiklis, 1998).
12. We cannot back P out of the sum in the right-hand-side of (24) because 1
P t diverges (P t e 0 ). The reason
1 P P t converges is that P t becomes orthogonal to P in the limit tof=0large t. Thus, we can view 1 P t
t=0
t=0
as a sum of two orthogonal components: an infinite one in the direction e and a finite one in the direction e? . It
1
t
t
is the finite component that we need to estimate. Approximating 1
t=0 P with t=0 (fiP ) is a way of rendering
the e-component finite while hopefully not altering the e? -component too much. There should be other substitutions
that lead to better approximations (in this context, see the final paragraph in Section 1.1).

P

r

r

r

330

P

P

r

!

P

P

fiP OLICY-G RADIENT E STIMATION

1

1

largest magnitude eigenvalue is and has multiplicity , and the corresponding left eigenvector is
the stationary distribution. We sort the eigenvalues i in decreasing order of magnitude, so that
1 > j2 j >    > js j for some  s  n. It turns out that j2 j determines the mixing time
of the chain.
The following theorem shows that if
fi is small compared to
j2j, the gradient approximation described above is accurate. Since we will be using the estimate as a direction in which to
update the parameters, the theorem compares the directions of the gradient and its estimate. In this
theorem, 2 A denotes the spectral condition number of a nonsingular matrix A, which is defined
as the product of the spectral norms of the matrices A and A 1 ,

1=

2

1

1

( )

2 (A) = kAk2 kA

where

1

k;
2

kAk = x max
kAxk;
kxk
2

:

=1

and kxk denotes the Euclidean norm of the vector x.

()

Theorem 3. Suppose that the transition probability matrix P  satisfies Assumption 1 with stationary distribution  0
1 ; : : : ; n , and has n distinct eigenvalues. Let S
x1 x2    xn be
the matrix of right eigenvectors of P corresponding, in order, to the eigenvalues
1 > j2 j 
    jn j. Then the normalized inner product between r and fi rfi  satisfies

=(

)

=(
1=

)


 kr(p ; : : : ; p )k p
  fi rfi 
1 fi
n
1 rkr

 =S
r0r
(27)
k
krk
1 fi j j ;
where  = diag( ; : : : ; n ).
Notice that r 0 r is the expectation under the stationary distribution of r (X ) .
As well as the mixing time (via j j), the bound in the theorem depends on another parameter of
the Markov chain: the spectral condition number of  = S . If the Markov chain is reversible (which
2

1 2

2

1

2

1

2

2

1 2

implies that the eigenvectors x1 ; : : : ; xn are orthogonal), this is equal to the ratio of the maximum
to the minimum probability of states under the stationary distribution. However, the eigenvectors
do not need to be nearly orthogonal. In fact, the condition that the transition probability matrix
have n distinct eigenvalues is not necessary; without it, the condition number is replaced by a more
complicated expression involving spectral norms of matrices of the form P i I .

(

)



Proof. The existence of n distinct eigenvalues implies that P can be expressed as S S 1 , where
1 ; : : : ; n (Lancaster & Tismenetsky, 1985, Theorem 4.10.2, p 153). It follows that for
any polynomial f , we can write f P
Sf S 1 .
Now, Proposition 1 shows that r fi rfi  r 0
fi Jfi . But

 = diag(

)

( ) = ()
= (1 )
(1 fi )Jfi = (1 fi ) r + fiP r + fi P r +    
= (1 fi ) I + fiP + fi!P +    r
1
X
= (1 fi )S
fi t t S r
2

2

2

2

1

= (1

fi)

n
X
j =1

t=0

xj y 0

331

j

1
X
t=0

(fij )

!

t

r;

fiBAXTER & BARTLETT

=(
=

)

where S 1
y1 ; : : : ; yn 0 .
It is easy to verify that yi is the left eigenvector corresponding to i , and that we can choose
y1  and x1 e. Thus we can write

=

(1

fi )Jfi = (1 fi )e0 r +

n
X

xj yj0

(1

!

fi )(fij )t r

t=0
j =2


n
X
fi
r
xj yj0
fi
j
j =2

= (1

fi )e +

= (1

fi )e + SMS 1 r;


1
1



1
M = diag 0;
1

where

1
X

fi
1 fi :
;:::;
fi2
1 fin

It follows from this and Proposition 1 that

  fi rfi 
r  (r r0(1
1 rkr
=
1
k
krk
0
= r  r (1 fi )Jfi
2

2

fi )Jfi )

krk

r
  r0 (1 fi )e + SMS r
=
krk
0
 SMS r
= r  rkr
k 

r 0 SMS r 

krk ;
p 
0
Since r = r
0  = , we can apply the Cauchy2

1

2

1

2

1

by the Cauchy-Schwartz inequality.
Schwartz inequality again to obtain

1 2

  fi rfi 
1 rkr

k





r



p

0

 



 = SMS
1 2

1

krk

2



r

:

(28)

We use spectral norms to bound the second factor in the numerator. It is clear from the definition
that the spectral norm of a product of nonsingular matrices satisfies kAB k2  kAk2 kB k2 , and that
the spectral norm of a diagonal matrix is given by k
d1 ; : : : ; dn k2
i jdi j. It follows that




 = SMS
1 2

1

diag(
) = max
 

r =  = SMS  =  = r

 
 

  = S  S  =   = r kM k

p
   = S r0r 1 1 fi jfi j :
1 2

1

1 2

1

2

2

1 2

1 2

1 2

1 2

2

1 2

2

Combining with Equation (28) proves (27).
332

2

fiP OLICY-G RADIENT E STIMATION

5. Estimating the Gradient in Parameterized Markov Chains
Algorithm 1 introduces MCG (Markov Chain Gradient), an algorithm for estimating the approximate gradient rfi  from a single on-line sample path X0 ; X1 ; : : : from the Markov chain M  .
MCG requires only K reals to be stored, where K is the dimension of the parameter space: K
parameters for the eligibility trace zt , and K parameters for the gradient estimate t . Note that
after T time steps T is the average so far of r Xt zt ,

()

2





( )

TX
1
T =
zt r(Xt ):
1

T

t=0

Algorithm 1 The MCG (Markov Chain Gradient) algorithm
1: Given:




Parameter  2 R K .
Parameterized class of stochastic matrices P
3 and 1.

= fP ():  2 RK g satisfying Assumptions

 fi 2 [0; 1).
 Arbitrary starting state X .
 State sequence X ; X ; : : :
0

generated by M ( ) (i.e. the Markov chain with transition
()).
 Reward sequence r(X ); r(X ); : : : satisfying Assumption 2.
Set z = 0 and  = 0 (z ;  2 RK ).
for each state Xt visited do
rp
( )
zt = fizt + XtXt+1
pXt Xt+1 ()
t = t + t [r(Xt )zt t ]
probabilities P

0

1

0

2:
3:
4:
5:
6:

0

0

0

1

0

+1

+1

1
+1

+1

+1

+1

end for

Theorem 4. Under Assumptions 1, 2 and 3, the MCG algorithm starting from any initial state X0
will generate a sequence 0 ; 1 ; : : : ; t ; : : : satisfying

 



lim t = rfi 

t!1

=

w.p.1:

(29)

()

Proof. Let fXt g fX0 ; X1 ; : : : g denote the random process corresponding to M  . If X0  
then the entire process is stationary. The proof can easily be generalized to arbitrary initial distributions using the fact that under Assumption 1, fXt g is asymptotically stationary. When fXt g is
333

fiBAXTER & BARTLETT

stationary, we can write

0 rP Jfi =

X

=

X

=

X

i;j
i;j
i;j

(i)rpij ()Jfi (j )
(i)pij ()

rpij () J (j )
p () fi
ij

Pr(Xt = i)Pr(Xt = j jXt = i) rppij(()) E(J (t + 1)jXt = j );
+1

+1

ij

where the first probability is with respect to the stationary distribution and

J (t + 1) =

( ( + 1)

)= (

1
X
s=t+1

fis

t

1

(30)

J (t + 1) is the process

r(Xs ):

)

The fact that E J t
jXt+1 Jfi Xt+1 for all Xt+1 follows from the boundedness of the
magnitudes of the rewards (Assumption 2) and Lebesgues dominated convergence theorem. We
can rewrite Equation (30) as




X
rp ()
0 rP Jfi = E i (Xt )j (Xt+1 ) ij J (t + 1) ;

pij ()

i;j

where i

() denotes the indicator function for state i,
(
1 if Xt = i;
i (Xt ) :=
0 otherwise;

and the expectation is again with respect to the stationary distribution. When Xt is chosen according
to the stationary distribution, the process fXt g is ergodic. Since the process fZt g defined by

Zt := i (Xt )j (Xt+1 )

rpij () J (t + 1)
pij ()

is obtained by taking a fixed
of fXt g, fZt g is also stationary and ergodic (Breiman, 1966,
fi function
fi
fi rpij () fi
Proposition 6.31). Since fi pij () fi is bounded by Assumption 3, from the ergodic theorem we have
(almost surely):

0 rP Jfi

TX
1
rp ()
= Tlim
i (Xt )j (Xt ) ij J (t + 1)
!1 T t
pij ()
i;j
TX
rpXtXt+1 () J (t + 1)
1
= Tlim
!1 T t
pXtXt+1 ()
" T
TX
1
X
r
pXtXt+1 () X
1
= Tlim
fi s t r(Xs ) +
!1 T t
pXtXt+1 () s t
s T
X

1

+1

=0

1

=0

1

1

=0

= +1

334

= +1

#

fis t

1

r(Xs ) :

(31)

fiP OLICY-G RADIENT E STIMATION

Concentrating on the second term in the right-hand-side of (31), observe that:
fi
fi TX1
fi
fi
fiT

1

t=0

rpXtXt+1 ()
pXt Xt+1 ()

1
X

fis t

s=T +1
TX1 fifi

1
T

fi
fi

1

fi
fi
Xs fifi
fi

r(

)

pXt Xt+1 ()

t=0
1
BR TX1 X

 T

= BR
T

1
X

rpXtXt+1 () fififi

t=0 s=T +1
TX1 T t

fi

t=0

1

fis

fi

t

s=T +1

fis

t

1

jr(Xs)j

1

fi

1 fiT
= BRfi
T (1 fi )2
! 0 as T ! 1;



jrp j
where R and B are the bounds on the magnitudes of the rewards and pijij from Assumptions 2
and 3. Hence,
TX1
T
rpXtXt+1  X
0 rP Jfi
(32)
fi s t 1 r Xs :
T !1 T
p

X
X
t t+1
t=0
s=t+1
Unrolling the equation for T in the MCG algorithm shows it is equal to

()
()

= lim 1


T
1 TX rpXtXt+1 () X
T t pXt Xt+1 () s t
1

=0

hence

fis

( )

t

1

r(is );

= +1

T ! 0rP Jfi w.p.1 as required.

6. Estimating the Gradient in Partially Observable Markov Decision Processes

()

Algorithm 1 applies to any parameterized class of stochastic matrices P  for which we can compute the gradients rpij  . In this section we consider the special case of P  that arise from a
parameterized class of randomized policies controlling a partially observable Markov decision process (POMDP). The partially observable qualification means we assume that these policies have
access to an observation process that depends on the state, but in general they may not see the state.
Specifically, assume that there are N controls U
f ; : : : ; N g and M observations Y
f ; : : : ; M g. Each u 2 U determines a stochastic matrix P u which does not depend on the
parameters  . For each state i 2 S , an observation Y 2 Y is generated independently according to
a probability distribution  i over observations in Y . We denote the probability of observation y
by y i . A randomized policy is simply a function  mapping observations y 2 Y into probability
distributions over the controls U . That is, for each observation y ,  y is a distribution over the
controls in U . Denote the probability under  of control u given observation y by u y .
To each randomized policy   and observation distribution   there corresponds a Markov
chain in which state transitions are generated by first selecting an observation y in state i according

()

= 1

1

()

()

=

()

()

()
()

()

335

()

fiBAXTER & BARTLETT

()

()

to the distribution  i , then selecting a control u according to the distribution  y , and then generating a transition to state j according to the probability pij u . To parameterize these chains we
parameterize the policies, so that  now becomes a function  ; y of a set of parameters  2 R K as
well as the observation y . The Markov chain corresponding to  has state transition matrix pij 
given by

()
( )

[ ( )]

pij () = EY  (i) EU (;Y ) pij (U ):

Equation (33) implies

rpij () =

X

(33)

y (i)pij (u)ru (; y):

u;y

(34)

Algorithm 2 introduces the GPOMDP algorithm (for Gradient of a Partially Observable Markov
Decision Process), a modified form of Algorithm 1 in which updates of zt are based on Ut ; Yt ,
rather than pXt Xt+1  . Note that Algorithm 2 does not require knowledge of the transition probability matrix P , nor of the observation process  ; it only requires knowledge of the randomized
policy . GPOMDP is essentially the algorithm proposed by Kimura et al. (1997) without the
reward baseline.
The algorithm GPOMDP assumes that the policy  is a function only of the current observation.
It is immediate that the same algorithm works for any finite history of observations. In general, an
optimal policy needs to be a function of the entire observation history. GPOMDP can be extended
to apply to policies with internal state (Aberdeen & Baxter, 2001).

(

()

)

Algorithm 2 The GPOMDP algorithm.
1: Given:




Parameterized class of randomized policies



(; ) :  2 RK

	

satisfying Assumption 4.

Partially observable Markov decision process which when controlled by the randomized
policies  ;  corresponds to a parameterized class of Markov chains satisfying Assumption 1.

( )

 fi 2 [0; 1).
 Arbitrary (unknown) starting state X .
 Observation sequence Y ; Y ; : : : generated by the POMDP with controls U ; U ; : : :
0

0

1

0

(; Yt).

generated randomly according to 


2:
3:
4:
5:
6:

( ) ( )

Reward sequence r X0 ; r X1 ; : : : satisfying Assumption 2, where
(hidden) sequence of states of the Markov decision process.

=0

 =0 
= + (( ))
 = + [ ( )

Set z0
and 0
(z0 ; 0 2 RK ).
for each observation Yt , control Ut , and subsequent reward r
rUt ; Yt
zt+1 fizt
Ut ; Yt
1
t+1
t t+1 r Xt+1 zt+1
t
end for

]

336

(Xt ) do
+1

X0 ; X1 ; : : :

1

is the

fiP OLICY-G RADIENT E STIMATION

For convergence of Algorithm 2 we need to replace Assumption 3 with a similar bound on the
gradient of :
Assumption 4. The derivatives,

exist for all u 2 U , y

@u (; y)
@k

2 Y and  2 RK . The ratios
fi
2 fifi
@u (;y) fi 3
fi @
fi
k
4
5

u (; y)

are uniformly bounded by B

y=1:::M ;u=1:::N ;k=1:::K

< 1 for all  2 RK .

Theorem 5. Under Assumptions 1, 2 and 4, Algorithm 2 starting from any initial state
generate a sequence 0 ; 1 ; : : : ; t ; : : : satisfying

 



lim t = rfi 

w.p.1:

t!1

X0

will

(35)

Proof. The proof follows the same lines as the proof of Theorem 4. In this case,

0 rP Jfi =

=
=
=

X

i;j

(i)rpij ()Jfi (j )

X

i;j;y;u
X

i;j;y;u
X

i;j;y;u

(i)pij (u)y (i)ru (; y)Jfi (j ) from (34)
(i)pij (u)y (i)

ru (; y)  (; y)J (j );
fi
 (; y) u
u

EZt0;

where the expectation is with respect to the stationary distribution of fXt g, and the process fZt0 g is
defined by
ru ; y J t ;
Zt0 i Xt j Xt+1 u Ut y Yt
u ; y

:= ( ) (

) ( ) ( ) (( )) ( + 1)

where Ut is the control process and Yt is the observation process. The result follows from the same
arguments used in the proof of Theorem 4.
6.1 Control dependent rewards

There are many circumstances in which the rewards may themselves depend on the controls u. For
example, some controls may consume more energy than others and so we may wish to add a penalty
term to the reward function in order to conserve energy. The simplest way to deal with this is to
define for each state i the expected reward r i by

( )

r(i) = EY  (i) EU (;Y ) r(U; i);
337

(36)

fiBAXTER & BARTLETT



and then redefine Jfi in terms of r:

Jfi (; i) :=

lim E

"

N !1

N
X
t=0

fi
fi
Xt fifi X0
fi

( )

fitr

#

=i

;

(37)

X0 ; X1 ; : : : . The performance gradient then becomes
r = r0r + 0rr;

where the expectation is over all trajectories

which can be approximated by

rfi  = 0 rP Jfi + rr ;








due to the fact that Jfi satisfies the Bellman equations (20) with r replaced by r .
For GPOMDP to take account of the dependence of r on the controls, its fifth line should be
replaced by

1
t = t + t + 1 r(Ut
+1



r
Ut+1 (; Yt )
; Xt ) zt +
t :
 (; Y )


+1

+1

+1

+1

Ut+1
t+1
It is straightforward to extend the proofs of Theorems 2, 3 and 5 to this setting.

6.2 Parameter dependent rewards
It is possible to modify GPOMDP when the rewards themselves depend directly on  . In this case,
the fifth line of GPOMDP is replaced with

t = t + t +1 1 [r(; Xt )zt + rr(; Xt ) t] :
+1

+1

+1

(38)

+1

( )

Again, the convergence and approximation theorems will carry through, provided rr ; i is uniformly bounded. Parameter-dependent rewards have been considered by Glynn (1990), Marbach
and Tsitsiklis (1998), and Baird and Moore (1999). In particular, Baird and Moore (1999) showed
how suitable choices of r ; i lead to a combination of value and policy search, or VAPS. For
example, if J ; i is an approximate value-function, then setting13

( )

~( )

h
i
1
~
~
r(; Xt ; Xt ) =
2 r(Xt ) + ffJ (; Xt ) J (; Xt ) ;
where r (Xt ) is the usual reward and ff 2 [0; 1) is a discount factor, gives an update that seeks to
2

1

1

minimize the expected Bellman error
n
X
i=1

2

(; i) 4r(i) + ff

n
X
j =1

32

pij ()J~(; j ) J~(; i)5 :

(39)

~( )

This will have the effect of both minimizing the Bellman error in J ; i , and driving the system
(via the policy) to states with small Bellman error. The motivation behind such an approach can
be understood if one considers a J that has zero Bellman error for all states. In that case a greedy
policy derived from J will be optimal, and regardless of how the actual policy is parameterized, the
expectation of zt r ; Xt ; Xt 1 will be zero and so will be the gradient computed by GPOMDP.
This kind of update is known as an actor-critic algorithm (Barto et al., 1983), with the policy playing
the role of the actor, and the value function playing the role of the critic.

(

~

13. The use of rewards r(; Xt ; Xt
analysis.

~

)

1 ) that depend on the current and previous

338

state does not substantially alter the

fiP OLICY-G RADIENT E STIMATION

6.3 Extensions to infinite state, observation, and control spaces
The convergence proof for Algorithm 2 relied on finite state (S ), observation (Y ) and control (U )
spaces. However, it should be clear that with no modification Algorithm 2 can be applied immediately to POMDPs with countably or uncountably infinite S and Y , and countable U . All that
changes is that pij u becomes a kernel p x; x0 ; u and  i becomes a density on observations. In
addition, with the appropriate interpretation of r=, it can be applied to uncountable U . Specifically, if U is a subset of R N then  y;  will be a probability density function on U with u y; 
the density at u. If U and Y are subsets of Euclidean space (but S is a finite set), Theorem 5 can be
extended to show that the estimates produced by this algorithm converge almost surely to rfi  . In
fact, we can prove a more general result that implies both this case of densities on subsets of R N as
well as the finite case of Theorem 5. We allow U and Y to be general spaces satisfying the following
topological assumption. (For definitions see, for example, (Dudley, 1989).)

()

(

)

()

( )

( )

Assumption 5. The control space U has an associated topology that is separable, Hausdorff, and
first-countable. For the corresponding Borel  -algebra B generated by this topology, there is a
-finite measure  defined on the measurable space U ; B . We say that  is the reference measure
for U .
Similarly, the observation space Y has a topology, Borel  -algebra, and reference measure
satisfying the same conditions.

(

)

In the case of Theorem 5, where U and Y are finite, the associated reference measure is the
counting measure. For U
RN and Y RM , the reference measure is Lebesgue measure. We
assume that the distributions  i and  ; y are absolutely continuous with respect to the reference
measures, and the corresponding Radon-Nikodym derivatives (probability masses in the finite case,
densities in the Euclidean case) satisfy the following assumption.

=

()

=

( )

( )

Assumption 6. For every y 2 Y and  2 R K , the probability measure  ; y is absolutely continuous with respect to the reference measure for U . For every i 2 S , the probability measure  i is
absolutely continuous with respect to the reference measure for Y .
Let  be the reference measure for U . For all u 2 U , y 2 Y ,  2 R K , and k 2 f ; : : : ; K g, the
derivatives

@ d(; y)
(u)
@k d

fi
fi @ du (;y)
fi @k d

exist and the ratios

< 1.

1

fi

(u)fifi
du ;y
d (u)
(

are bounded by B

()

)

With these assumptions, we can replace  in Algorithm 2 with the Radon-Nikodym derivative
of  with respect to the reference measure on U . In this case, we have the following convergence
result. This generalizes Theorem 5, and also applies to densities  on a Euclidean space U .

Theorem 6. Suppose the control space U and the observation space Y satisfy Assumption 5 and let

 be the reference measure on the control space U . Consider Algorithm 2 with
rUt (; Yt)
Ut (; Yt )
339

fiBAXTER & BARTLETT

replaced by

r d d;Yt (Ut ) :
(

)

( )

d(;Yt )
Ut
d
Under Assumptions 1, 2 and 6, this algorithm, starting from any initial state
sequence 0 ; 1 ; : : : ; t ; : : : satisfying

 



lim t = rfi 

t!1

X0

will generate a

w.p.1:

Proof. See Appendix B

7. New Results
Since the first version of this paper, we have extended GPOMDP to several new settings, and also
proved some new properties of the algorithm. In this section we briefly outline these results.
7.1 Multiple Agents

Instead of a single agent generating actions according to (; y ), suppose we have multiple agents
i = 1; : : : ; na , each with their own parameter set i and distinct observation of the environment
yi , and that generate their own actions ui according to a policy ui (i ; yi ). If the agents all receive the same reward signal r (Xt ) (they may be cooperating to solve the same task, for example),

then GPOMDP can be applied to the collective POMDP obtained by concatenating
 1 the nobserva
1
na , u
tions, controls, and
parameters
into
single
vectors
y
y
;
:
:
:
;
y
u ; : : : ; u a , and


1 ; : : : ; na respectively. An easy calculation shows that the gradient estimate generated
by GPOMDP in the collective case is precisely the same as that obtained by applying GPOMDP
to

1
each agent independently, and then concatenating the results. That is,
; : : : ; na , where
i is the estimate produced by GPOMDP applied to agent i. This leads to an on-line algorithm
in which the agents adjust their parameters independently and without any explicit communication,
yet collectively the adjustments are maximizing the global average reward. For similar observations in the context of REINFORCE and VAPS, see Peshkin et al. (2000). This algorithm gives a
biologically plausible synaptic weight-update rule when applied to networks of spiking neurons in
which the neurons are regarded as independent agents (Bartlett & Baxter, 1999), and has shown
some promise in a network routing application (Tao, Baxter, & Weaver, 2001).

=

=

=

= 






7.2 Policies with internal states
So far we have only considered purely reactive or memoryless policies in which the chosen control
is a function of only the current observation. GPOMDP is easily extended to cover the case of
policies that depend on finite histories of observations Yt ; Yt 1 ; : : : ; Yt k , but in general, for optimal
control of POMDPs, the policy must be a function of the entire observation history. Fortunately, the
observation history may be summarized in the form of a belief state (the current distribution over
states), which is itself updated based only upon the current observation, and knowledge of which
is sufficient for optimal behaviour (Smallwood & Sondik, 1973; Sondik, 1978). An extension of
GPOMDP to policies with parameterized internal belief states is described by Aberdeen and Baxter
(2001), similar in spirit to the extension of VAPS and REINFORCE described by Meuleau et al.
(1999).
340

fiP OLICY-G RADIENT E STIMATION

7.3 Higher-Order Derivatives
can be generalized to compute estimates of second and higher-order derivatives of the
average reward (assuming they exist), still from a single sample Rpath of the underlying POMDP.
To see this for second-order derivatives, observe that if  
q ; x r x dx for some twicedifferentiable density q ; x and performance measure r x , then

GPOMDP

()= ( )( )
()
Z
r () = r(x) rq(q;(;x)x) q(; x) dx

( )

2

2

where r2 denotes the matrix of second derivatives (Hessian). It can be verified that

r q(; x) = r log q(; x) + [r log q(; x)]
q(; x)
2

2

2

(40)

log ( )

where the second term on the right-hand-side is the outer product between r
q ; x and itself
(that is, the matrix with entries @=@i
q ; x @=@j q ; x ). Taking x to be a sequence of
states X0 ; X1 ; : : : ; XT between visits to a recurrent state i in a parameterized Markov chain (recall
T 1p
Section 1.1.1), we have q ; X
t=0 Xt Xt+1  , which combined with (40) yields

log ( )
log ( )
)=
()

(

r q(; X ) = TX r pXtXt+1 ()
2

q(; X )

1

TX1 

2

pXt Xt+1 ()

t=0

rpXtXt+1 ()  +
p
()
2

Xt Xt+1

t=0

"T 1
X

rpXtXt+1 ()

#2

pXtXt+1 ()

t=0

(the squared terms in this expression are also outer products). From this expression we can derive
a GPOMDP-like algorithm for computing a biased estimate of the Hessian r2   , which involves
maintainingin addition to the usual eligibility trace zt a second matrix trace updated as follows:

()

Zt+1

= fiZt + rp pXtXt+1(())
2



Xt Xt+1

rpXtXt+1 ()  :
p
( )
2

Xt Xt+1

( ) +



After T time steps the algorithm returns the average so far of r Xt Zt zt2 where the second term
is again an outer product. Computation of higher-order derivatives could be used in second-order
gradient methods for optimization of policy parameters.
7.4 Bias and Variance Bounds

()

()

Theorem 3 provides a bound on the bias of rfi   relative to r  that applies when the underlying Markov chain has distinct eigenvalues. We have extended this result to arbitrary Markov chains
(Bartlett & Baxter, 2001). However, the extra generality comes at a price, since the latter bound involves the number of states in the chain, whereas Theorem 3 does not. The same paper also supplies
a proof that the variance of GPOMDP scales as =
fi 2 , providing a formal justification for the
interpretation of fi in terms of bias/variance trade-off.

1 (1

)

8. Conclusion
We have presented a general algorithm (MCG) for computing arbitrarily accurate approximations
to the gradient of the average reward in a parameterized Markov chain. When the chains transition
matrix has distinct eigenvalues, the accuracy of the approximation was shown to be controlled by the
341

fiBAXTER & BARTLETT

size of the subdominant eigenvalue j2 j. We showed how the algorithm could be modified to apply
to partially observable Markov decision processes controlled by parameterized stochastic policies,
with both discrete and continuous control, observation and state spaces (GPOMDP). For the finite
state case, we proved convergence with probability 1 of both algorithms.
We briefly described extensions to multi-agent problems, policies with internal state, estimating
higher-order derivatives, generalizations of the bias result to chains with non-distinct eigenvalues,
and a new variance result. There are many avenues for further research. Continuous time results
should follow as extensions of the results presented here. The MCG and GPOMDP algorithms can
be applied to countably or uncountably infinite state spaces; convergence results are also needed in
these cases.
In the companion paper (Baxter et al., 2001), we present experimental results showing rapid
convergence of the estimates generated by GPOMDP to the true gradient r . We give on-line
variants of the algorithms of the present paper, and also variants of gradient ascent that make use of
the estimates of rfi  . We present experimental results showing the effectiveness of these algorithms
in a variety of problems, including a three-state MDP, a nonlinear physical control problem, and a
call-admission problem.
Acknowledgements
This work was supported by the Australian Research Council, and benefited from the comments of
several anonymous referees. Most of this research was performed while the authors were with the
Research School of Information Sciences and Engineering, Australian National University.

Appendix A. A Simple Example of Policy Degradation in Value-Function Learning
Approximate value-function approaches to reinforcement work by minimizing some form of error
between the approximate value function and the true value function. It has long been known that this
may not necessarily lead to improved policy performance from the new value function. We include
this appendix because it illustrates that this phenomenon can occur in the simplest possible system,
a two-state MDP, and also provides some geometric intuition for why the phenomenon arises.
Consider the two-state Markov decision process (MDP) in Figure 1. There are two controls
u1 ; u2 with corresponding transition probability matrices

P (u1 ) =

1
3
1
3

2

2
3
2
3



; P (u2 ) =

2

3
2
3

23

1
3
1
3



;

so that u1 always takes the system to state with probability = , regardless of the starting state (and
therefore to state with probability = ), and u2 does the opposite. Since state has a reward of ,
while state has a reward of , the optimal policy is to always select action u1 . Under this policy
the stationary distribution on states is 1 ; 2
= ; = , while the infinite-horizon discounted
value of each state i
; with discount value ff 2 ; is

1

1

0

=1 2

13
[

Jff (i) = E

] = [1 3 2 3]
[0 1)

1
X

fft r

fi
fi
Xt fifi X0
fi

( )

=i

2

1

!

;

t=0
where the expectation is over all state sequences X0 ; X1 ; X2 ; : : : with state transitions generated according to P u1 . Solving Bellmans equations: Jff r ffP u1 Jff , where Jff
Jff ; Jff 0
2ff
2ff
and r
r ; r 0 yields Jff
and Jff
.
3(1 ff)
3(1 ff)

( )
= [ (1) (2)]

= + ( )
(2) = 1 +

(1) =

342

= [ (1) (2)]

fiP OLICY-G RADIENT E STIMATION

r(1) = 0

r(2) = 1

1

2

Figure 1: Two-state Markov Decsision Process

~

~( ) =

Now, suppose we are trying to learn an approximate value function J for this MDP, i.e. , J i
w i for each state i ; and some scalar feature  ( must have dimensionality to ensure that
J really is approximate). Here w 2 R is the parameter being learnt. For the greedy policy obtained
from J to be optimal, J must value state above state . For the purposes of this illustration choose

;
, so that for J
> J , w must be negative.
Temporal Difference learning (or
 ) is one of the most popular techniques for training
approximate value functions (Sutton & Barto, 1998). It has been shown that for linear functions,
converges to a parameter w minimizing the expected squared loss under the stationary
distribution (Tsitsikilis & Van-Roy, 1997):

()
=1 2
~
~
2
~(2) ~(1)
(1) = 2 (2) = 1
TD( )
TD(1)
~

w = argmin

w

1

1

2
X

i=1

i [w(i) Jff (i)]2 :

(41)

Substituting the previous expressions for 1 ; 2 ;  and Jff under the optimal policy and solving
3+ff
for w , yields w
. Hence w > for all values of ff 2 ; , which is the wrong
9(1 ff)
sign. So we have a situation where the optimal policy is implementable as a greedy policy based
on an approximate value function in the class (just choose any w < ), yet
observing the
optimal policy will converge to a value function whose corresponding greedy policy implements the
suboptimal policy.
A geometrical illustration of why this occurs is shown in Figure 2. In this figure, points on the
graph
represent
p
p the values of the states. The scales of the state 1 and state 2 axes are weighted by
 and  respectively. In this way, the squared euclidean distance on the graph between
two points J and J corresponds to the expectation under the stationary distribution of the squared
difference between values:

=

0

[0 1)

0

(1)

hp




TD(1)

(2)
~

(1)J (1);

p

(2)J (2)

i

hp

(1)J~(1); (2)J~(2)
p

i2





2

= E J (X ) J~(X )

:

For any value function in the shaded region, the corresponding greedy policy is optimal, since
those value functions rank state 2 above state 1. The bold line represents the set of all realizable
approximate value functions w ; w
. The solution to (41) is then the approximate value
function found by projecting the point corresponding to the true value function Jff ; Jff
onto
this line. This is illustrated in the figure for ff
= . The projection is suboptimal because weighted
mean-squared distance in value-function space does not take account of the policy boundary.

( (1)

(2))
=3 5

[( (1) (2)]

Appendix B. Proof of Theorem 6
The proof needs the following topological lemma. For definitions see, for example, (Dudley, 1989,
pp. 2425).
343

fiBAXTER & BARTLETT

3

2

1

0

1

111111111111111
000000000000000
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
[J (1), J (2)]
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
[w * (1), w * (2)]
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
Legend
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
Optimal Policy:
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
Approximate
000000000000000
111111111111111
000000000000000
111111111111111
Value Function:
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
1

0

1

2

3

11
00
00
11
00
11
00
11

4

5

Figure 2: Plot of value-function space for the two-state system. Note that the scale of each axis has
been weighted by the square root of the stationary probability of the corresponding state
under the optimal policy. The solution found by TD(1) is simply the projection of the true
value function onto the set of approximate value functions.

(

)

Lemma 7. Let X; T be a topological space that is Hausdorff, separable, and first-countable.
Let B be the Borel  -algebra generated by T . Then the measurable space X; B has a sequence
S1; S2 ; : : :  B of sets that satisfies the following conditions:

(

1. Each Si is a partition of X (that is, X
have empty intersection).

2. For all x 2 X , fxg 2 B and

1
\

)

= SfS : S 2 Sig and any two distinct elements of Si

fS 2 Si : x 2 S g = fxg:

i=1

=

Proof. Since X is separable, it has a countable dense subset S
fx1 ; x2 ; : : :g. Since X is firstcountable, each of these xi has aScountable neighbourhood base, Ni . Now, construct the partitions
Si using the countable set N 1
; ; : : :, define
i=1 Ni as follows. Let S0 X and, for i

=

=

=1 2

Si = fS \ Ni : S 2 Si g [ fS \ (X Ni) : S 2 Si g :
1

1

344

fiP OLICY-G RADIENT E STIMATION

Clearly, each Si is a measurable partition of X . Since X is Hausdorff, for each pair x; x0 of distinct
points from X , there is a pair of disjoint open sets A and A0 such that x 2 A and x0 2 A0 . Since S
is dense, there is a pair s; s0 from S with s 2 A and s0 2 A0 . Also, N contains neighbourhoods Ns
and Ns0 with Ns  A and Ns0  A0 . So Ns and Ns0 are disjoint. Thus, for sufficiently large i, x
and x0 fall in distinct elements of the partition Si . Since this is true for any pair x; x0 , it follows that

1
\

fS 2 Si : x 2 S g  fxg:

i=1

The reverse inclusion
is trivial. The measurability of all singletons fxg follows from the measuraS
bility of Sx
i fS 2 Si S \ fxg g and the fact that fxg X Sx .

:=

:

=

=

We shall use Lemma 7 together with the following result to show that we can approximate
expectations of certain random variables using a single sample path of the Markov chain.

(

)

Lemma 8. Let X; B be a measurable space satisfying the conditions of Lemma 7, and let S1 ; S2 ; : : :
be a suitable sequence of partitions as in that lemma. Let  be a probability measure defined on this
space. Let f be an absolutely integrable function on X . For an event S , define

f (S ) =
For each x 2 X and
almost all x in X ,

k

R

S f d :

(S )

= 1; 2; : : :, let Sk (x) be the unique element of Sk containing x. Then for
lim f (Sk (x)) = f (x):
k!1

Proof. Clearly, the signed finite measure  defined by

(E ) =

Z

E

fd

is absolutely continuous with respect to , and Equation (42) defines
derivative of  with respect to . This derivative can also be defined as

(42)

f

as the Radon-Nikodym

(Sk (x))
d
(x) = klim
:
!1 (Sk (x))
d

See, for example, (Shilov & Gurevich, 1966, Section 10.2). By the Radon-Nikodym Theorem (Dudley, 1989, Theorem 5.5.4, p. 134), these two expressions are equal a.e. ().
Proof. (Theorem 6.) From the definitions,

rfi  = 0 rP Jfi

=

n X
n
X
i=1 j =1

(i)rpij ()Jfi (j ):

(43)

For every y ,  is absolutely continuous with respect to the reference measure , hence for any i and
j we can write
Z Z
d(; y)
pij () =
pij (u)
(u) d(u) d (i)(y):
d
Y U
345

fiBAXTER & BARTLETT

Since  and  do not depend on
under the integral to obtain

rpij () =

 and d(; y)=d is absolutely integrable,

Z Z

Y U

pij (u) r

d(; y)
(u) d(u) d (i)(y):
d

To avoid cluttering the notation, we shall use  to denote the distribution
denote the distribution  i on Y . With this notation, we have

()

rpij () =

we can differentiate

Z Z

Y U

pij

(; y) on U , and 

to

r d
d d d:
d
d

Now, let  be the probability measure on Y  U generated by  and  . We can write (43) as

rfi  =

X

i;j

(i)Jfi (j )

Z

YU

pij

r d
d d:
d
d

Using the notation of Lemma 8, we define

pij (S ) =

R

S pij d ;

(S )

Z
d
1
r(S ) = (S ) rdd d;
S d

for a measurable set S

 Y  U . Notice that, for a given i, j , and S ,
pij (S ) = Pr(Xt+1 = j jXt = i; (y; u) 2 S )
fi

!

d fi
r
r(S ) = E dd fififi Xt = i; (Yt ; Ut ) 2 S :
d

Let S1 ; S2 ; : : : be a sequence of partitions of Y  U as in Lemma 7, and let Sk
element of Sk containing y; u . Using Lemma 8, we have

( )

Z

YU

pij

Z
r d
d d =

d
d

lim pij (Sk (y; u)) r (Sk (y; u)) d(y; u)

YU k!1

= klim
!1

(y; u) denote the

X Z

S 2Sk

S

pij (S ) r(S ) d;

346

fiP OLICY-G RADIENT E STIMATION

where we have used Assumption 6 and the Lebesgue dominated convergence theorem to interchange
the integral and the limit. Hence,

rfi  = klim
!1

= klim
!1

X X

i;j S 2Sk

X

i;j;S

(i)(S )pij (S )Jfi (j )r(S )

Pr(Xt = i)Pr((Yt ; Ut ) 2 S )Pr(Xt = j jXt = i; (Yt ; Ut ) 2 S )
+1

fi

E (J (t + 1)jXt

+1

= klim
!1

X

i;j;S

"

d fi
r
= j ) E dd fififi Xt = i; (Yt; Ut ) 2 S
d

!

#

d
E i(Xt )S (Yt; Ut )j (Xt )J (t + 1) rdd ;
+1

d

where probabilities and expectations are with respect to the stationary distribution  of Xt , and the
distributions on Yt ; Ut . Now, the random process inside the expectation is asymptotically stationary
and ergodic. From the ergodic theorem, we have (almost surely)
d
X TX
r
1
rfi  = klim
lim
i (Xt )S (Yt ; Ut )j (Xt )J (t + 1) dd :
!1 T !1 T
1

+1

i;j;S t=0

d

It is easy to see that the double limit also exists when the order is reversed, so
TX
d
X
r
1
rfi  = Tlim
lim i(Xt )S (Yt ; Ut )j (Xt )J (t + 1) dd
!1 T
k!1
1

1
= Tlim
!1 T

t=0
TX1
t=0

+1

i;j;S
d
(;Yt )
r d Ut
d(;Yt ) U
t
d

( ) J (t + 1):
( )

The same argument as in the proof of Theorem 4 shows that the tails of
when
fi
fi
fi r d(;Yt ) U fi
t fi
fi
d
fi
fi d(;Y
t)
fi
fi
U
t
d

d

J (t + 1) can be ignored

( )
( )
and jr (Xt )j are uniformly bounded. It follows that T !  0 rP Jfi w.p.1, as required.
References
Aberdeen, D., & Baxter, J. (2001). Policy-gradient learning of controllers with internal state. Tech.
rep., Australian National University.
Aleksandrov, V. M., Sysoyev, V. I., & Shemeneva, V. V. (1968). Stochastic optimaization. Engineering Cybernetics, 5, 1116.
Baird, L., & Moore, A. (1999). Gradient descent for general reinforcement learning. In Advances
in Neural Information Processing Systems 11. MIT Press.
347

fiBAXTER & BARTLETT

Bartlett, P. L., & Baxter, J. (1999). Hebbian synaptic modifications in spiking neurons that learn.
Tech. rep., Research School of Information Sciences and Engineering, Australian National
University. http://csl.anu.edu.au/bartlett/papers/BartlettBaxter-Nov99.ps.gz.
Bartlett, P. L., & Baxter, J. (2001). Estimation and approximation bounds for gradient-based reinforcement learning. Journal of Computer and Systems Sciences, 62. Invited Paper: Special
Issue on COLT 2000.
Barto, A. G., Sutton, R. S., & Anderson, C. W. (1983). Neuronlike adaptive elements that can solve
difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics,
SMC-13, 834846.
Baxter, J., Bartlett, P. L., & Weaver, L. (2001). Experiments with infinite-horizon, policy-gradient
estimation. Journal of Artificial Intelligence Research. To appear.
Baxter, J., Tridgell, A., & Weaver, L. (2000). Learning to play chess using temporal-differences.
Machine Learning, 40(3), 243263.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Bertsekas, D. P. (1995). Dynamic Programming and Optimal Control, Vol II. Athena Scientific.
Breiman, L. (1966). Probability. Addison-Wesley.
Cao, X.-R., & Wan, Y.-W. (1998). Algorithms for Sensitivity Analysis of Markov Chains Through
Potentials and Perturbation Realization. IEEE Transactions on Control Systems Technology,
6, 482492.
Dudley, R. M. (1989). Real Analysis and Probability. Wadsworth & Brooks/Cole, Belmont, California.
Glynn, P. W. (1986). Stochastic approximation for monte-carlo optimization. In Proceedings of the
1986 Winter Simulation Conference, pp. 356365.
Glynn, P. W. (1990). Likelihood ratio gradient estimation for stochastic systems. Communications
of the ACM, 33, 7584.
Glynn, P. W., & LEcuyer, P. (1995). Likelihood ratio gradient estimation for regenerative stochastic
recursions. Advances in Applied Probability, 27, 4 (1995), 27, 10191053.
Ho, Y.-C., & Cao, X.-R. (1991). Perturbation Analysis of Discrete Event Dynamic Systems. Kluwer
Academic, Boston.
Jaakkola, T., Singh, S. P., & Jordan, M. I. (1995). Reinforcement Learning Algorithm for Partially
Observable Markov Decision Problems. In Tesauro, G., Touretzky, D., & Leen, T. (Eds.),
Advances in Neural Information Processing Systems, Vol. 7. MIT Press, Cambridge, MA.
Kimura, H., & Kobayashi, S. (1998a). An analysis of actor/critic algorithms using eligibility traces:
Reinforcement learning with imperfect value functions. In Fifteenth International Conference
on Machine Learning, pp. 278286.
348

fiP OLICY-G RADIENT E STIMATION

Kimura, H., & Kobayashi, S. (1998b). Reinforcement learning for continuous action using stochastic gradient ascent. In Intelligent Autonomous Systems (IAS-5), pp. 288295.
Kimura, H., Miyazaki, K., & Kobayashi, S. (1997). Reinforcement learning in POMDPs with
function approximation. In Fisher, D. H. (Ed.), Proceedings of the Fourteenth International
Conference on Machine Learning (ICML97), pp. 152160.
Kimura, H., Yamamura, M., & Kobayashi, S. (1995). Reinforcement learning by stochastic hill
climbing on discounted reward. In Proceedings of the Twelfth International Conference on
Machine Learning (ICML95), pp. 295303.
Konda, V. R., & Tsitsiklis, J. N. (2000). Actor-Critic Algorithms. In Neural Information Processing
Systems 1999. MIT Press.
Lancaster, P., & Tismenetsky, M. (1985). The Theory of Matrices. Academic Press, San Diego, CA.
Marbach, P., & Tsitsiklis, J. N. (1998). Simulation-Based Optimization of Markov Reward Processes. Tech. rep., MIT.
Meuleau, N., Peshkin, L., Kaelbling, L. P., & Kim, K.-E. (2000). Off-policy policy search. Tech.
rep., MIT Artificical Intelligence Laboratory.
Meuleau, N., Peshkin, L., Kim, K.-E., & Kaelbling, L. P. (1999). Learning finite-state controllers for
partially observable environments. In Proceedings of the Fifteenth International Conference
on Uncertainty in Artificial Intelligence.
Peshkin, L., Kim, K.-E., Meuleau, N., & Kaelbling, L. P. (2000). Learning to cooperate via policy
search. In Proceedings of the Sixteenth International Conference on Uncertainty in Artificial
Intelligence.
Reiman, M. I., & Weiss, A. (1986). Sensitivity analysis via likelihood ratios. In Proceedings of the
1986 Winter Simulation Conference.
Reiman, M. I., & Weiss, A. (1989). Sensitivity analysis for simulations via likelihood ratios. Operations Research, 37.
Rubinstein, R. Y. (1969). Some Problems in Monte Carlo Optimization. Ph.D. thesis.
Rubinstein, R. Y. (1991). How to optimize complex stochastic systems from a single sample path
by the score function method. Annals of Operations Research, 27, 175211.
Rubinstein, R. Y. (1992). Decomposable score function estimators for sensitivity analysis and optimization of queueing networks. Annals of Operations Research, 39, 195229.
Rubinstein, R. Y., & Melamed, B. (1998). Modern Simulation and Modeling. Wiley, New York.
Rubinstein, R. Y., & Shapiro, A. (1993). Discrete Event Systems. Wiley, New York.
Samuel, A. L. (1959). Some Studies in Machine Learning Using the Game of Checkers. IBM
Journal of Research and Development, 3, 210229.
349

fiBAXTER & BARTLETT

Shilov, G. E., & Gurevich, B. L. (1966). Integral, Measure and Derivative: A Unified Approach.
Prentice-Hall, Englewood Cliffs, N.J.
Singh, S. P., Jaakkola, T., & Jordan, M. I. (1994). Learning Without State-Estimation in Partially
Observable Markovian Decision Processes. In Proceedings of the Eleventh International
Conference on Machine Learning.
Singh, S., & Bertsekas, D. (1997). Reinforcement learning for dynamic channel allocation in cellular telephone systems. In Advances in Neural Information Processing Systems: Proceedings
of the 1996 Conference, pp. 974980. MIT Press.
Smallwood, R. D., & Sondik, E. J. (1973). The optimal control of partially observable Markov
decision processes over a finite horizon. Operations Research, 21, 10711098.
Sondik, E. J. (1978). The optimal control of partially observable Markov decision processes over
the infinite horizon: Discounted costs. Operations Research, 26.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press,
Cambridge MA. ISBN 0-262-19398-1.
Sutton, R. S., McAllester, D., Singh, S., & Mansour, Y. (2000). Policy Gradient Methods for
Reinforcement Learning with Function Approximation. In Neural Information Processing
Systems 1999. MIT Press.
Tao, N., Baxter, J., & Weaver, L. (2001). A multi-agent, policy-gradient approach to network
routing. Tech. rep., Australian National University.
Tesauro, G. (1992). Practical Issues in Temporal Difference Learning. Machine Learning, 8, 257
278.
Tesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves master-level
play. Neural Computation, 6, 215219.
Tsitsikilis, J. N., & Van-Roy, B. (1997). An Analysis of Temporal Difference Learning with Function Approximation. IEEE Transactions on Automatic Control, 42(5), 674690.
Williams, R. J. (1992). Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. Machine Learning, 8, 229256.
Zhang, W., & Dietterich, T. (1995). A reinforcement learning approach to job-shop scheduling. In
Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, pp.
11141120. Morgan Kaufmann.

350

fiJournal of Artificial Intelligence Research 15 (2001) 207-261

Submitted 5/00; published 9/01

Planning by Rewriting
Jose Luis Ambite
Craig A. Knoblock

ambite@isi.edu
knoblock@isi.edu

Information Sciences Institute and Department of Computer Science,
University of Southern California,
4676 Admiralty Way, Marina del Rey, CA 90292, USA

Abstract
Domain-independent planning is a hard combinatorial problem. Taking into account
plan quality makes the task even more difficult. This article introduces Planning by Rewriting (PbR), a new paradigm for efficient high-quality domain-independent planning. PbR
exploits declarative plan-rewriting rules and efficient local search techniques to transform
an easy-to-generate, but possibly suboptimal, initial plan into a high-quality plan. In addition to addressing the issues of planning efficiency and plan quality, this framework offers
a new anytime planning algorithm. We have implemented this planner and applied it to
several existing domains. The experimental results show that the PbR approach provides
significant savings in planning effort while generating high-quality plans.

1. Introduction

Planning is the process of generating a network of actions, a plan, that achieves a desired
goal from an initial state of the world. Many problems of practical importance can be
cast as planning problems. Instead of crafting an individual planner to solve each specific
problem, a long line of research has focused on constructing domain-independent planning
algorithms. Domain-independent planning accepts as input, not only descriptions of the
initial state and the goal for each particular problem instance, but also a declarative domain
specification, that is, the set of actions that change the properties of the state. Domainindependent planning makes the development of planning algorithms more efficient, allows
for software and domain reuse, and facilitates the principled extension of the capabilities of
the planner. Unfortunately, domain-independent planning (like most planning problems)
is computationally hard (Bylander, 1994; Erol, Nau, & Subrahmanian, 1995; Backstrom
& Nebel, 1995). Given the complexity limitations, most of the previous work on domainindependent planning has focused on finding any solution plan without careful consideration
of plan quality. Usually very simple cost functions, such as the length of the plan, have
been used. However, for many practical problems plan quality is crucial. In this paper
we present a new planning paradigm, Planning by Rewriting (PbR), that addresses both
planning efficiency and plan quality while maintaining the benefits of domain independence.
The framework is fully implemented and we present empirical results in several planning
domains.
c
2001
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiAmbite & Knoblock

1.1 Solution Approach
Two observations guided the present work. The first one is that there are two sources of
complexity in planning:
 Satisfiability: the difficulty of finding any solution to the planning problem (regardless
of the quality of the solution).
 Optimization: the difficulty of finding the optimal solution under a given cost metric.
For a given domain, each of these facets may contribute differently to the complexity of
planning. In particular, there are many domains in which the satisfiability problem is
relatively easy and their complexity is dominated by the optimization problem. For example,
there may be many plans that would solve the problem, so that finding one is efficient
in practice, but the cost of each solution varies greatly, thus finding the optimal one is
computationally hard. We will refer to these domains as optimization domains. Some
optimization domains of great practical interest are query optimization and manufacturing
process planning.1
The second observation is that planning problems have a great deal of structure. Plans
are a type of graph with strong semantics, determined by both the general properties
of planning and each particular domain specification. This structure should and can be
exploited to improve the efficiency of the planning process.
Prompted by the previous observations, we developed a novel approach for efficient
planning in optimization domains: Planning by Rewriting (PbR). The framework works in
two phases:
1. Generate an initial solution plan. Recall that in optimization domains this is efficient.
However, the quality of this initial plan may be far from optimal.
2. Iteratively rewrite the current solution plan improving its quality using a set of declarative plan-rewriting rules, until either an acceptable solution is found or a resource
limit is reached.
As motivation, consider the optimization domains of distributed query processing and
manufacturing process planning.2 Distributed query processing (Yu & Chang, 1984) involves generating a plan that efficiently computes a user query from data that resides at
different nodes in a network. This query plan is composed of data retrieval actions at diverse
information sources and operations on this data (such as those of the relational algebra:
join, selection, etc). Some systems use a general-purpose planner to solve this problem
(Knoblock, 1996). In this domain it is easy to construct an initial plan (any parse of the
query suffices) and then transform it using a gradient-descent search to reduce its cost.
The plan transformations exploit the commutative and associative properties of the (relational algebra) operators, and facts such as that when a group of operators can be executed
together at a remote information source it is generally more efficient to do so. Figure 1
1. Interestingly, one of the most widely studied planning domains, the Blocks World, also has this property.
2. These domains are analyzed in Section 4. Graphical examples of the rewriting process appear in Figure 30
for query planning and in Figure 21 for manufacturing process planning. The reader may want to consult
those figures even if not all details can be explained at this point.

208

fiPlanning by Rewriting

shows some sample transformations. Simple-join-swap transforms two join trees according to the commutative and associative properties of the join operator. Remote-join-eval
executes a join of two subqueries at a remote source, if the source is able to do so.
Simple-Join-Swap:
retrieve(Q1, Source1) 1 [retrieve(Q2, Source2) 1 retrieve(Q3, Source3)] 
retrieve(Q2, Source2) 1 [retrieve(Q1, Source1) 1 retrieve(Q3, Source3)]
Remote-Join-Eval:
(retrieve(Q1, Source) 1 retrieve(Q2, Source))  capability(Source, join)
 retrieve(Q1 1 Q2, Source)
Figure 1: Transformations in Query Planning
In manufacturing, the problem is to find an economical plan of machining operations
that implement the desired features of a design. In a feature-based approach (Nau, Gupta,
& Regli, 1995), it is possible to enumerate the actions involved in building a piece by
analyzing its CAD model. It is more difficult to find an ordering of the operations and the
setups that optimize the machining cost. However, similar to query planning, it is possible
to incrementally transform a (possibly inefficient) initial plan. Often, the order of actions
does not affect the design goal, only the quality of the plan, thus many actions can commute.
Also, it is important to minimize the number of setups because fixing a piece on a machine
is a rather time consuming operation. Interestingly, such grouping of machining operations
on a setup is analogous to evaluating a subquery at a remote information source.
As suggested by these examples, there are many problems that combine the characteristics of traditional planning satisfiability with quality optimization. For these domains there
often exist natural transformations that may be used to efficiently obtain high-quality plans
by iterative rewriting. Planning by Rewriting provides a domain-independent framework
that allows plan transformations to be conveniently specified as declarative plan-rewriting
rules and facilitates the exploration of efficient (local) search techniques.
1.2 Advantages of Planning by Rewriting
There are several advantages to the planning style that PbR introduces. First, PbR is a
declarative domain-independent framework. This facilitates the specification of planning
domains, their evolution, and the principled extension of the planner with new capabilities. Moreover, the declarative rewriting rule language provides a natural and convenient
mechanism to specify complex plan transformations.
Second, PbR accepts sophisticated quality measures because it operates on complete
plans. Most previous planning approaches either have not addressed quality issues or have
very simple quality measures, such as the number of steps in the plan, because only partial
plans are available during the planning process. In general, a partial plan cannot offer
enough information to evaluate a complex cost metric and/or guide the planning search
effectively.
209

fiAmbite & Knoblock

Third, PbR can use local search methods that have been remarkably successful in scaling
to large problems (Aarts & Lenstra, 1997).3 By using local search techniques, high-quality
plans can be efficiently generated. Fourth, the search occurs in the space of solution plans,
which is generally much smaller than the space of partial plans explored by planners based
on refinement search.
Fifth, our framework yields an anytime planning algorithm (Dean & Boddy, 1988). The
planner always has a solution to offer at any point in its computation (modulo the initial
plan generation that needs to be fast). This is a clear advantage over traditional planning
approaches, which must run to completion before producing a solution. Thus, our system
allows the possibility of trading off planning effort and plan quality. For example, in query
planning the quality of a plan is its execution time and it may not make sense to keep
planning if the cost of the current plan is small enough, even if a cheaper one could be
found. Further discussion and concrete examples of these advantages are given throughout
the following sections.
1.3 Contributions
The main contribution of this paper is the development of Planning by Rewriting, a novel
domain-independent paradigm for efficient high-quality planning. First, we define a language of declarative plan-rewriting rules and present the algorithms for domain-independent
plan rewriting. The rewriting rules provide a natural and convenient mechanism to specify complex plan transformations. Our techniques for plan rewriting generalize traditional
graph rewriting. Graph rewriting rules need to specify in the rule consequent the complete
embedding of the replacement subplan. We introduce the novel class of partially-specified
plan-rewriting rules that relax that restriction. By taking advantage of the semantics of
planning, this embedding can be automatically computed. A single partially-specified rule
can concisely represent a great number of fully-specified rules. These rules are also easier
to write and understand than their fully-specified counterparts. Second, we adapt local
search techniques, such as gradient descent, to efficiently explore the space of plan rewritings and optimize plan quality. Finally, we demonstrate empirically the usefulness of the
PbR approach in several planning domains.
1.4 Outline
The remainder of this paper is structured as follows. Section 2 provides background on
planning, rewriting, and local search, some of the fields upon which PbR builds. Section 3
presents the basic framework of Planning by Rewriting as a domain-independent approach to
local search. This section describes in detail plan rewriting and our declarative rewriting rule
language. Section 4 describes several application domains and shows experimental results
comparing PbR with other planners. Section 5 reviews related work. Finally, Section 6
summarizes the contributions of the paper and discusses future work.
3. Although the space of rewritings can be explored by complete search methods, in the application domains
we have analyzed the search space is very large and our experience suggests that local search is more
appropriate. However, to what extent complete search methods are useful in a Planning by Rewriting
framework remains an open issue. In this paper we focus on local search.

210

fiPlanning by Rewriting

2. Preliminaries: Planning, Rewriting, and Local Search
The framework of Planning by Rewriting arises as the confluence of several areas of research, namely, artificial intelligence planning algorithms, graph rewriting, and local search
techniques. In this section we give some background on these areas and explain how they
relate to PbR.
2.1 AI Planning
We assume that the reader is familiar with classical AI planning, but in this section we will
highlight the main concepts and relate them to the PbR framework. Weld (1994, 1999) and
Russell & Norvig (1995) provide excellent introductions to AI planning.
PbR follows the classical AI planning representation of actions that transform a state.
The state is a set of ground propositions understood as a conjunctive formula. PbR, as most
AI planners, follows the Closed World Assumption, that is, if a proposition is not explicitly
mentioned in the state it is assumed to be false, similarly to the negation as failure semantics
of logic programming. The propositions of the state are modified, asserted or negated, by
the actions in the domain. The actions of a domain are specified by operator schemas.
An operator schema consists of two logical formulas: the precondition, which defines the
conditions under which the operator may be applied, and the postcondition, which specifies
the changes on the state effected by the operator. Propositions not mentioned in the
postcondition are assumed not to change during the application of the operator. This type
of representation was initially introduced in the STRIPS system (Fikes & Nilsson, 1971).
The language for the operators in PbR is the same as in Sage (Knoblock, 1995, 1994b),
which is an extension of UCPOP (Penberthy & Weld, 1992). The operator description
language in PbR accepts arbitrary function-free first-order formulas in the preconditions of
the operators, and conditional and universally quantified effects (but no disjunctive effects).
In addition, the operators can specify the resources they use. Sage and PbR address unit
non-consumable resources. These resources are fully acquired by an operator until the
completion of its action and then released to be reused.
Figure 2 shows a sample operator schema specification for a simple Blocks World
domain,4 in the representation accepted by PbR. This domain has two actions: stack,
which puts one block on top of another, and unstack, which places a block on the table.5
The state is described by two predicates: (on ?x ?y)6 denotes that a block ?x is on top of
another block ?y (or on the Table), and (clear ?x) denotes that a ?x block does not have
any other block on top of it.
An example of a more complex operator from a process manufacturing domain is shown
in Figure 3. This operator describes the behavior of a punch, which is a machine used to
make holes in parts. The punch operation requires that there is an available clamp at the
machine and that the orientation and width of the hole is appropriate for using the punch.
After executing the operation the part will have the desired hole but it will also have a
4. To illustrate the basic concepts in planning, we will use examples from a simple Blocks World domain.
The reader will find a real-world application of planning techniques, query planning, in Section 4.4.
5. (stack ?x ?y ?z) can be read as stack the block ?x on top of block ?y from ?z.
(unstack ?x ?y) can be read as lift block ?x from the top of block ?y and put it on the Table.
6. By convention, variables are preceded by a question mark symbol (?), as in ?x.

211

fiAmbite & Knoblock

(define (operator STACK)
:parameters (?X ?Y ?Z)
:precondition
(:and (on ?X ?Z) (clear ?X) (clear ?Y)
(:neq ?Y ?Z) (:neq ?X ?Z) (:neq ?X ?Y)
(:neq ?X Table) (:neq ?Y Table))
:effect (:and (on ?X ?Y) (:not (on ?X ?Z))
(clear ?Z) (:not (clear ?Y))))

(define (operator UNSTACK)
:parameters (?X ?Y)
:precondition
(:and (on ?X ?Y) (clear ?X) (:neq ?X ?Y)
(:neq ?X Table) (:neq ?Y Table))
:effect (:and (on ?X Table) (clear ?Y)
(:not (on ?X ?Y))))

Figure 2: Blocks World Operators
(define (operator PUNCH)
:parameters (?x ?width ?orientation)
:resources ((machine PUNCH) (is-object ?x))
:precondition (:and (is-object ?x)
(is-punchable ?x ?width ?orientation)
(has-clamp PUNCH))
:effect (:and (:forall (?surf) (:when (:neq ?surf ROUGH)
(:not (surface-condition ?x ?surf))))
(surface-condition ?x ROUGH)
(has-hole ?x ?width ?orientation)))

Figure 3: Manufacturing Operator
rough surface.7 Note the specification on the resources slot. Declaring (machine PUNCH)
as a resource enforces that no other operator can use the punch concurrently. Similarly,
declaring the part, (is-object ?x), as a resource means that only one operation at a time
can be performed on the object. Further examples of operator specifications appear in
Figures 18, 19, and 28.
A plan in PbR is represented by a graph, in the spirit of partial-order causal-link planners (POCL) such as UCPOP (Penberthy & Weld, 1992). The nodes are plan steps, that
is, instantiated domain operators. The edges specify a temporal ordering relation among
steps imposed by causal links and ordering constraints. A causal link is a record of how a
proposition is established in a plan. This record contains the proposition (sometimes also
called a condition), a producer step, and a consumer step. The producer is a step in the
plan that asserts the proposition, that is, the proposition is one of its effects. The consumer
is a step that needs that proposition, that is, the proposition is one of its preconditions. By
causality, the producer must precede the consumer.
The ordering constraints are needed to ensure that the plan is consistent. They arise
from resolving operator threats and resource conflicts. An operator threat occurs when a
step that negates the condition of a causal link can be ordered between the producer and the
consumer steps of the causal link. To prevent this situation, which makes the plan inconsistent, POCL planners order the threatening step either before the producer (demotion)
or after the consumer (promotion) by posting the appropriate ordering constraints. For the
7. This operator uses an idiom combining universal quantification and negated conditional effects to enforce
that the attribute surface-condition of a part is single-valued.

212

fiPlanning by Rewriting

unit non-consumable resources we considered, steps requiring the same resource have to be
sequentially ordered, and such a chain of ordering constraints will appear in the plan.
An example of a plan in the Blocks World using this graph representation is given in
Figure 4. This plan transforms an initial state consisting of two towers: C on A, A on the
Table, B on D, and D on the Table; to the final state consisting of one tower: A on B, B on C,
C on D, and D on the Table. The initial state is represented as step 0 with no preconditions
and all the propositions of the initial state as postconditions. Similarly, the goal state is
represented as a step goal with no postconditions and the goal formula as the precondition.
The plan achieves the goal by using two unstack steps to disassemble the two initial towers
and then using three stack steps to build the desired tower. The causal links are shown as
solid arrows and the ordering constraints as dashed arrows. The additional effects of a step
that are not used in causal links, sometimes called side effects, are shown after each step
pointed by thin dashed arrows. Negated propositions are preceded by . Note the need
for the ordering link between the steps 2, stack(B C Table), and 3, stack(A B Table).
If step 3 could be ordered concurrently or before step 2, it would negate the precondition
clear(B) of step 2, making the plan inconsistent. A similar situation occurs between steps
1 and 2 where another ordering link is introduced.

clear(B)

Causal Link
Ordering Constraint
Side Effect

on(A Table)
on(C A)

clear(A)

3 STACK(A B Table)

4 UNSTACK(C A)
on(C Table)

on(C A)
clear(C)
on(D Table)

on(A Table)
clear(B)

clear(C)

0
clear(B)

on(B Table)
on(A B)
clear(C)
2 STACK(B C Table)
on(B C)
on(C D)
1 STACK(C D Table)
GOAL
clear(D)
on(C Table)
clear(D)
A

on(B D)

B

on(B Table)
5 UNSTACK(B D)
on(B D)
clear(B)

C

B

C

A

D

D

clear(C)
Initial State

Goal State

Figure 4: Sample Plan in the Blocks World Domain

2.2 Rewriting
Plan rewriting in PbR is related to term and graph rewriting. Term rewriting originated
in the context of equational theories and reduction to normal forms as an effective way
to perform deduction (Avenhaus & Madlener, 1990; Baader & Nipkow, 1998). A rewrite
system is specified as a set of rules. Each rule corresponds to a preferred direction of an
equivalence theorem. The main issue in term rewriting systems is convergence, that is, if
two arbitrary terms can be rewritten in a finite number of steps into a unique normal form.
In PbR two plans are considered equivalent if they are solutions to the same problem,
213

fiAmbite & Knoblock

although they may differ on their cost or operators (that is, they are equivalent with
respect to satisfiability as introduced above). However, we are not interested in using
the rewriting rules to prove such equivalence. Instead, our framework uses the rewriting
rules to explore the space of solution plans.
Graph rewriting, akin to term rewriting, refers to the process of replacing a subgraph of
a given graph, when some conditions are satisfied, by another subgraph. Graph rewriting
has found broad applications, such as very high-level programming languages, database
data description and query languages, etc. Schurr (1997) presents a good survey. The
main drawback of general graph rewriting is its complexity. Because graph matching can
be reduced to (sub)graph isomorphism the problem is NP-complete. Nevertheless, under
some restrictions graph rewriting can be performed efficiently (Dorr, 1995).
Planning by Rewriting adapts general graph rewriting to the semantics of partial-order
planning with a STRIPS-like operator representation. A plan-rewriting rule in PbR specifies
the replacement, under certain conditions, of a subplan by another subplan. However, in
our formalism the rule does not need to specify the completely detailed embedding of the
consequent as in graph rewriting systems. The consistent embeddings of the rule consequent,
with the generation of edges if necessary, are automatically computed according to the
semantics of partial-order planning. Our algorithm ensures that the rewritten plans always
remain valid (Section 3.1.3). The plan-rewriting rules are intended to explore the space of
solution plans to reach high-quality plans.
2.3 Local Search in Combinatorial Optimization
PbR is inspired by the local search techniques used in combinatorial optimization. An
instance of a combinatorial optimization problem consists of a set of feasible solutions and a
cost function over the solutions. The problem consists in finding a solution with the optimal
cost among all feasible solutions. Generally the problems addressed are computationally
intractable, thus approximation algorithms have to be used. One class of approximation
algorithms that have been surprisingly successful in spite of their simplicity are local search
methods (Aarts & Lenstra, 1997; Papadimitriou & Steiglitz, 1982).
Local search is based on the concept of a neighborhood. A neighborhood of a solution
p is a set of solutions that are in some sense close to p, for example because they can be
easily computed from p or because they share a significant amount of structure with p.
The neighborhood generating function may, or may not, be able to generate the optimal
solution. When the neighborhood function can generate the global optima, starting from
any initial feasible point, it is called exact (Papadimitriou & Steiglitz, 1982, page 10).
Local search can be seen as a walk on a directed graph whose vertices are solutions
points and whose arcs connect neighboring points. The neighborhood generating function
determines the properties of this graph. In particular, if the graph is disconnected, then the
neighborhood is not exact since there exist feasible points that would lead to local optima
but not the global optima. In PbR the points are solution plans and the neighbors of a plan
are the plans generated by the application of a set of declarative plan rewriting rules.
The basic version of local search is iterative improvement. Iterative improvement starts
with an initial solution and searches a neighborhood of the solution for a lower cost solution. If such a solution is found, it replaces the current solution and the search continues.
214

fiPlanning by Rewriting

Otherwise, the algorithm returns a locally optimal solution. Figure 5(a) shows a graphical
depiction of basic iterative improvement. There are several variations of this basic algorithm. First improvement generates the neighborhood incrementally and selects the first
solution of better cost than the current one. Best improvement generates the complete
neighborhood and selects the best solution within this neighborhood.

Neighborhood

Local Optima

Local Optima

(a) Basic Iterative Improvement

(b) Variable-Depth Search

Figure 5: Local Search
Basic iterative improvement obtains local optima, not necessarily the global optimum.
One way to improve the quality of the solution is to restart the search from several initial points and choose the best of the local optima reached from them. More advanced
algorithms, such as variable-depth search, simulated annealing and tabu search, attempt to
minimize the probability of being stuck in a low-quality local optimum.
Variable-depth search is based on applying a sequence of steps as opposed to only one
step at each iteration. Moreover, the length of the sequence may change from iteration to
iteration. In this way the system overcomes small cost increases if eventually they lead to
strong cost reductions. Figure 5(b) shows a graphical depiction of variable-depth search.
Simulated annealing (Kirkpatrick, Gelatt, & Vecchi, 1983) selects the next point randomly. If a lower cost solution is chosen, it is selected. If a solution of a higher cost is
chosen, it is still selected with some probability. This probability is decreased as the algorithm progresses (analogously to the temperature in physical annealing). The function that
governs the behavior of the acceptance probability is called the cooling schedule. It can be
proven that simulated annealing converges asymptotically to the optimal solution. Unfortunately, such convergence requires exponential time. So, in practice, simulated annealing
is used with faster cooling schedules (not guaranteed to converge to the optimal) and thus
it behaves like an approximation algorithm.
Tabu search (Glover, 1989) can also accept cost-increasing neighbors. The next solution
is a randomly chosen legal neighbor even if its cost is worse than the current solution. A
neighbor is legal if it is not in a limited-size tabu list. The dynamically updated tabu list
prevents some solution points from being considered for some period of time. The intuition
is that if we decide to consider a solution of a higher cost at least it should lie in an
unexplored part of the space. This mechanism forces the exploration of the solution space
out of local minima.
Finally, we should stress that the appeal of local search relies on its simplicity and good
average-case behavior. As could be expected, there are a number of negative worst-case results. For example, in the traveling salesman problem it is known that exact neighborhoods,
215

fiAmbite & Knoblock

that do not depend on the problem instance, must have exponential size (Savage, Weiner,
& Bagchi, 1976). Moreover, an improving move in these neighborhoods cannot be found in
polynomial time unless P = NP (Papadimitriou & Steiglitz, 1977). Nevertheless, the best
approximation algorithm for the traveling salesman problem is a local search algorithm
(Johnson, 1990).

3. Planning by Rewriting as Local Search
Planning by Rewriting can be viewed as a domain-independent framework for local search.
PbR accepts arbitrary domain specifications, declarative plan-rewriting rules that generate
the neighborhood of a plan, and arbitrary (local) search methods. Therefore, assuming that
a given combinatorial problem can be encoded as a planning problem, PbR can take it as
input and experiment with different neighborhoods and search methods.
We will describe the main issues in Planning by Rewriting as an instantiation of the
local search idea typical of combinatorial optimization algorithms:
 Selection of an initial feasible point: In PbR this phase consists of efficiently generating
an initial solution plan.
 Generation of a local neighborhood : In PbR the neighborhood of a plan is the set of
plans obtained from the application of a set of declarative plan-rewriting rules.
 Cost function to minimize: This is the measure of plan quality that the planner is
optimizing. The plan quality function can range from a simple domain-independent
cost metric, such as the number of steps, to more complex domain-specific ones, such
as the query evaluation cost or the total manufacturing time for a set of parts.
 Selection of the next point: In PbR, this consists of deciding which solution plan to
consider next. This choice determines how the global space will be explored and has
a significant impact on the efficiency of planning. A variety of local search strategies
can be used in PbR, such as steepest descent, simulated annealing, etc. Which search
method yields the best results may be domain or problem specific.
In the following subsections we expand on these issues. First, we discuss the use of
declarative rewriting rules to generate a local neighborhood of a plan, which constitutes
the main contribution of this paper. We present the syntax and semantics of the rules, the
plan-rewriting algorithm, the formal properties and a complexity analysis of plan rewriting,
and a rule taxonomy. Second, we address the selection of the next plan and the associated
search techniques for plan optimization. Third, we discuss the measures of plan quality.
Finally, we describe some approaches for initial plan generation.
3.1 Local Neighborhood Generation: Plan-Rewriting Rules
The neighborhood of a solution plan is generated by the application of a set of declarative
plan-rewriting rules. These rules embody the domain-specific knowledge about what transformations of a solution plan are likely to result in higher-quality solutions. The application
of a given rule may produce one or several rewritten plans or fail to produce a plan, but
the rewritten plans are guaranteed to be valid solutions. First, we describe the syntax and
216

fiPlanning by Rewriting

semantics of the rules. Second, we introduce two approaches to rule specification. Third, we
present the rewriting algorithm, its formal properties, and the complexity of plan rewriting.
Finally, we present a taxonomy of plan-rewriting rules.
3.1.1 Plan-Rewriting Rules: Syntax and Semantics
First, we introduce the rule syntax and semantics through some examples. Then, we provide
a formal description. A plan-rewriting rule has three components: (1) the antecedent (:if
field) specifies a subplan to be matched; (2) the :replace field identifies the subplan that
is going to be removed, a subset of steps and links of the antecedent; (3) the :with field
specifies the replacement subplan. Figure 6 shows two rewriting rules for the Blocks World
domain introduced in Figure 2. Intuitively, the rule avoid-move-twice says that, whenever
possible, it is better to stack a block on top of another directly, rather than first moving
it to the table. This situation occurs in plans generated by the simple algorithm that first
puts all blocks on the table and then build the desired towers, such as the plan in Figure 4.
The rule avoid-undo says that the actions of moving a block to the table and back to its
original position cancel each other and both could be removed from a plan.
(define-rule :name avoid-move-twice
:if (:operators ((?n1 (unstack ?b1 ?b2))
(?n2 (stack ?b1 ?b3 Table)))
:links (?n1 (on ?b1 Table) ?n2)
:constraints ((possibly-adjacent ?n1 ?n2)
(:neq ?b2 ?b3)))
:replace (:operators (?n1 ?n2))
:with (:operators (?n3 (stack ?b1 ?b3 ?b2))))

(define-rule :name avoid-undo
:if (:operators
((?n1 (unstack ?b1 ?b2))
(?n2 (stack ?b1 ?b2 Table)))
:constraints
((possibly-adjacent ?n1 ?n2))
:replace (:operators (?n1 ?n2))
:with NIL))

Figure 6: Blocks World Rewriting Rules
A rule for the manufacturing domain of (Minton, 1988b) is shown in Figure 7. This
domain and additional rewriting rules are described in detail in Section 4.1. The rule states
that if a plan includes two consecutive punching operations in order to make holes in two
different objects, but another machine, a drill-press, is also available, the plan quality may
be improved by replacing one of the punch operations with the drill-press. In this domain
the plan quality is the (parallel) time to manufacture all parts. This rule helps to parallelize
the plan and thus improve the plan quality.
(define-rule :name punch-by-drill-press
:if (:operators ((?n1 (punch ?o1 ?width1 ?orientation1))
(?n2 (punch ?o2 ?width2 ?orientation2)))
:links (?n1 ?n2)
:constraints ((:neq ?o1 ?o2)
(possibly-adjacent ?n1 ?n2)))
:replace (:operators (?n1))
:with (:operators (?n3 (drill-press ?o1 ?width1 ?orientation1))))

Figure 7: Manufacturing Process Planning Rewriting Rule
217

fiAmbite & Knoblock

The plan-rewriting rule syntax is described by the BNF specification given in Figure 8.
This BNF generates rules that follow the template shown in Figure 9. Next, we describe
the semantics of the three components of a rule (:if, :replace, and :with fields) in detail.
<rule> ::= (define-rule :name <name>
:if (<graph-spec-with-constraints>)
:replace (<graph-spec>)
:with (<graph-spec>))
<graph-spec-with-constraints> ::= {<graph-spec>}
{:constraints (<constraints>)}
<graph-spec> ::= {:operators (<nodes>)}
{:links (<edges>)} | NIL
<nodes> ::= <node> | <node> <nodes>
<edges> ::= <edge> | <edge> <edges>
<constraints> ::= <constraint> | <constraint> <constraints>
<node> ::= (<node-var> {<node-predicate>} {:resource})
<edge> ::= (<node-var> <node-var>) |
(<node-var> <edge-predicate> <node-var>) |
(<node-var> :threat <node-var>)
<constraint> ::= <interpreted-predicate> |
(:neq <pred-var> <pred-var>)
<node-var>  <pred-var> = , {} = optional, | = alternative

Figure 8: BNF for the Rewriting Rules

(define-rule :name <rule-name>
:if (:operators ((<nv> <np> {:resource}) ...)
:links ((<nv> {<lp>|:threat} <nv>) ...)
:constraints (<ip> ...))
:replace (:operators (<nv> ...)
:links ((<nv> {<lp>|:threat} <nv>) ...))
:with (:operators ((<nv> <np> {:resource}) ...)
:links ((<nv> {<lp>} <nv>) ...)))
<nv> = node variable, <np> = node predicate, {} = optional
<lp> = causal link predicate, <ip> = interpreted predicate,

| = alternative

Figure 9: Rewriting Rule Template
The antecedent, the :if field, specifies a subplan to be matched against the current
plan. The graph structure of the subplan is defined in the :operators and :links fields.
The :operators field specifies the nodes (operators) of the graph and the :links field
specifies the edges (causal and ordering links). Finally, the :constraints field specifies a
set of constraints that the operators and links must satisfy.
The :operators field consists of a list of node variable and node predicate pairs. The
step number of those steps in the plan that match the given node predicate would be
correspondingly bound to the node variable. The node predicate can be interpreted in
two ways: as the step action, or as a resource used by the step. For example, the node
specification (?n2 (stack ?b1 ?b3 Table)) in the antecedent of avoid-move-twice in
Figure 6 shows a node predicate that denotes a step action. This node specification will
collect tuples, composed of step number ?n2 and blocks ?b1 and ?b3, obtained by matching
steps whose action is a stack of a block ?b1 that is on the Table and it is moved on top of
another block ?b3. This node specification applied to the plan in Figure 4 would result in
218

fiPlanning by Rewriting

three matches: (1 C D), (2 B C), and (3 A B), for the variables (?n2 ?b1 ?b3) respectively.
If the optional keyword :resource is present, the node predicate is interpreted as one of
the resources used by a plan step, as opposed to describing a step action. An example of a
rule that matches against the resources of an operator is given in Figure 10, where the node
specification (?n1 (machine ?x) :resource) will match all steps that use a resource of
type machine and collect pairs of step number ?n1 and machine object ?x.
(define-rule :name resource-swap
:if (:operators ((?n1 (machine ?x) :resource)
(?n2 (machine ?x) :resource))
:links ((?n1 :threat ?n2)))
:replace (:links (?n1 ?n2))
:with (:links (?n2 ?n1)))

Figure 10: Resource-Swap Rewriting Rule
The :links field consists of a list of link specifications. Our language admits link
specifications of three types. The first type is specified as a pair of node variables. For
example, (?n1 ?n2) in Figure 7. This specification matches any temporal ordering link in
the plan, regardless if it was imposed by causal links or by the resolution of threats.
The second type of link specification matches causal links. Causal links are specified
as triples composed of a producer step node variable, an edge predicate, and a consumer
step node variable. The semantics of a causal link is that the producer step asserts in its
effects the predicate, which in turn is needed in the preconditions of the consumer step. For
example, the link specification (?n1 (on ?b1 Table) ?n2) in Figure 6 matches steps ?n1
that put a block ?b1 on the Table and steps ?n2 that subsequently pick up this block. That
link specification applied to the plan in Figure 4 would result in the matches: (4 C 1) and
(5 B 2), for the variables (?n1 ?b1 ?n2).
The third type of link specification matches ordering links originating from the resolution
of threats (coming either from resource conflicts or from operator conflicts). These links
are selected by using the keyword :threat in the place of a condition. For example, the
resource-swap rule in Figure 10 uses the link specification (?n1 :threat ?n2) to ensure
that only steps that are ordered because they are involved in a threat situation are matched.
This helps to identify which are the critical steps that do not have any other reasons (i.e.
causal links) to be in such order, and therefore this rule may attempt to reorder them.
This is useful when the plan quality depends on the degree of parallelism in the plan as a
different ordering may help to parallelize the plan. Recall that threats can be solved either
by promotion or demotion, so the reverse ordering may also produce a valid plan, which is
often the case when the conflict is among resources as in the rule in Figure 10.
Interpreted predicates, built-in and user-defined, can be specified in the :constraints
field. These predicates are implemented programmatically as opposed to being obtained by
matching against components from the plan. The built-in predicates currently implemented
are inequality8 (:neq), comparison (< <= > >=), and arithmetic (+ - * /) predicates. The
user can also add arbitrary predicates and their corresponding programmatic implementa8. Equality is denoted by sharing variables in the rule specification.

219

fiAmbite & Knoblock

tions. The interpreted predicates may act as filters on the previous variables or introduce
new variables (and compute new values for them). For example, the user-defined predicate
possibly-adjacent in the rules in Figure 6 ensures that the steps are consecutive in some
linearization of the plan.9 For the plan in Figure 4 the extension of the possibly-adjacent
predicate is: (0 4), (0 5), (4 5), (5 4), (4 1), (5 1), (1 2), (2 3), and (3 Goal).
The user can easily add interpreted predicates by including a function definition that
implements the predicate. During rule matching our algorithm passes arguments and calls
such functions when appropriate. The current plan is passed as a default first argument to
the interpreted predicates in order to provide a context for the computation of the predicate
(but it can be ignored). Figure 11 show a skeleton for the (Lisp) implementation of the
possibly-adjacent and less-than interpreted predicates.
(defun possibly-adjacent (plan node1 node2)
(not (necessarily-not-adjacent
node1
node2
;; accesses the current plan
(plan-ordering plan)))

(defun less-than (plan n1 n2)
(declare (ignore plan))
(when (and (numberp n1) (numberp n2))
(if (< n1 n2)
(nil) ;; true
nil))) ;; false

Figure 11: Sample Implementation of Interpreted Predicates
The consequent is composed of the :replace and :with fields. The :replace field
specifies the subplan that is going to be removed from the plan, which is a subset of the
steps and links identified in the antecedent. If a step is removed, all the links that refer to
the step are also removed. The :with field specifies the replacement subplan. As we will
see in Sections 3.1.2 and 3.1.3, the replacement subplan does not need to be completely
specified. For example, the :with field of the avoid-move-twice rule of Figure 6 only
specifies the addition of a stack step but not how this step is embedded into the plan. The
links to the rest of the plan are automatically computed during the rewriting process.
3.1.2 Plan-Rewriting Rules: Full versus Partial Specification
PbR gives the user total flexibility in defining rewriting rules. In this section we describe two
approaches to guaranteeing that a rewriting rule specification preserves plan correctness,
that is, produces a valid rewritten plan when applied to a valid plan.
In the full-specification approach the rule specifies all steps and links involved in a
rewriting. The rule antecedent identifies all the anchoring points for the operators in the
consequent, so that the embedding of the replacement subplan is unambiguous and results
in a valid plan. The burden of proving the rule correct lies upon the user or an automated
rule defining procedure (cf. Section 6). These kind of rules are the ones typically used in
graph rewriting systems (Schurr, 1997).
In the partial-specification approach the rule defines the operators and links that constitute the gist of the plan transformation, but the rule does not prescribe the precise
9. The interpreted predicate possibly-adjacent makes the link expression in the antecedent of avoid-move-twice redundant. Unstack puts the block ?b1 on the table from where it is picked up by the
stack operator, thus the causal link (?n1 (on ?b1 Table) ?n2) is already implied by the :operators
and :constraints specification and could be removed from the rule specification.

220

fiPlanning by Rewriting

embedding of the replacement subplan. The burden of producing a valid plan lies upon the
system. PbR takes advantage of the semantics of domain-independent planning to accept
such a relaxed rule specification, fill in the details, and produce a valid rewritten plan.
Moreover, the user is free to specify rules that may not necessarily be able to compute
a rewriting for a plan that matches the antecedent because some necessary condition was
not checked in the antecedent. That is, a partially-specified rule may be overgeneral. This
may seem undesirable, but often a rule may cover more useful cases and be more naturally
specified in this form. The rule may only fail for rarely occurring plans, so that the effort in
defining and matching the complete specification may not be worthwhile. In any case, the
plan-rewriting algorithm ensures that the application of a rewriting rule either generates a
valid plan or fails to produce a plan (Theorem 1, Section 3.1.3).
As an example of these two approaches to rule specification, consider Figure 12 that
shows the avoid-move-twice-full rule, a fully-specified version of the avoid-move-twice
rule (of Figure 6, reprinted here for convenience). The avoid-move-twice-full rule is
more complex and less natural to specify than avoid-move-twice. But, more importantly,
avoid-move-twice-full is making more commitments than avoid-move-twice. In particular, avoid-move-twice-full fixes the producer of (clear ?b1) for ?n3 to be ?n4 when
?n7 is also known to be a valid candidate. In general, there are several alternative producers
for a precondition of the replacement subplan, and consequently many possible embeddings.
A different fully-specified rule is needed to capture each embedding. The number of rules
grows exponentially as all permutations of the embeddings are enumerated. However, by
using the partial-specification approach we can express a general plan transformation by a
single natural rule.
(define-rule :name avoid-move-twice-full
:if (:operators ((?n1 (unstack ?b1 ?b2))
(?n2 (stack ?b1 ?b3 Table)))
:links ((?n4 (clear ?b1) ?n1)
(?n5 (on ?b1 ?b2) ?n1)
(?n1 (clear ?b2) ?n6)
(?n1 (on ?b1 Table) ?n2)
(?n7 (clear ?b1) ?n2)
(?n8 (clear ?b3) ?n2)
(?n2 (on ?b1 ?b3) ?n9))
:constraints ((possibly-adjacent ?n1 ?n2)
(:neq ?b2 ?b3)))
:replace (:operators (?n1 ?n2))
:with (:operators ((?n3 (stack ?b1 ?b3 ?b2)))
:links ((?n4 (clear ?b1) ?n3)
(?n8 (clear ?b3) ?n3)
(?n5 (on ?b1 ?b2) ?n3)
(?n3 (on ?b1 ?b3) ?n9))))

(define-rule :name avoid-move-twice
:if (:operators
((?n1 (unstack ?b1 ?b2))
(?n2 (stack ?b1 ?b3 Table)))
:links (?n1 (on ?b1 Table) ?n2)
:constraints
((possibly-adjacent ?n1 ?n2)
(:neq ?b2 ?b3)))
:replace (:operators (?n1 ?n2))
:with (:operators
(?n3 (stack ?b1 ?b3 ?b2))))

Figure 12: Fully-specified versus Partially-specified Rewriting Rule
In summary, the main advantage of the full-specification rules is that the rewriting can
be performed more efficiently because the embedding of the consequent is already specified.
The disadvantages are that the number of rules to represent a generic plan transformation
may be very large and the resulting rules quite lengthy; both of these problems may decrease
221

fiAmbite & Knoblock

the performance of the match algorithm. Also, the rule specification is error prone if written
by the user. Conversely, the main advantage of the partial-specification rules is that a single
rule can represent a complex plan transformation naturally and concisely. The rule can
cover a large number of plan structures even if it may occasionally fail. Also, the partial
specification rules are much easier to specify and understand by the users of the system.
As we have seen, PbR provides a high degree of flexibility for defining plan-rewriting rules.
3.1.3 Plan-Rewriting Algorithm
In this section, first we describe the basic plan-rewriting algorithm in PbR. Second, we
prove this algorithm sound and discuss some formal properties of rewriting. Finally, we
discuss a family of algorithms for plan rewriting depending on parameters such as the
language for defining plan operators, the specification language for the rewriting rules, and
the requirements of the search method.
The plan-rewriting algorithm is shown in Figure 13. The algorithm takes two inputs:
a valid plan P , and a rewriting rule R = (qm , pr , pc ) (qm is the antecedent query, pr is
the replaced subplan, and pc is the replacement subplan). The output is a valid rewritten
plan P 0 . The matching of the antecedent of the rewriting rule (qm ) determines if the rule is
applicable and identifies the steps and links of interest (line 1). This matching can be seen
as subgraph isomorphism between the antecedent subplan and the current plan (with the
results then filtered by applying the :constraints). However, we take a different approach.
PbR implements rule matching as conjunctive query evaluation. Our implementation keeps
a relational representation of the steps and links in the current plan similar to the node
and link specifications of the rewriting rules. For example, the database for the plan in
Figure 4 contains one table for the unstack steps with schema (?n1 ?b1 ?b2) and tuples
(4 C A) and (5 B D), another table for the causal links involving the clear condition with
schema (?n1 ?n2 ?b) and tuples (0 1 C), (0 2 B), (0 2 C), (0 3 B), (0 4 C), (0 5 B), (4
3 A) and (5 1 D), and similar tables for the other operator and link types. The match
process consists of interpreting the rule antecedent as a conjunctive query with interpreted
predicates, and executing this query against the relational view of the plan structures. As a
running example, we will analyze the application of the avoid-move-twice rule of Figure 6
to the plan in Figure 4. Matching the rule antecedent identifies steps 1 and 4. More
precisely, considering the antecedent as a query, the result is the single tuple (4 C A 1 D)
for the variables (?n1 ?b1 ?b2 ?n2 ?b3).
After choosing a match i to work on (line 3), the algorithm instantiates the subplan
specified by the :replace field (pr ) according to such match (line 4) and removes the
instantiated subplan pir from the original plan P (line 5). All the edges incoming and
emanating from nodes of the replaced subplan are also removed. The effects that the
replaced plan pir was achieving for the remainder of the plan (P pir ), the UsefulEffects of pir ,
will now have to be achieved by the replacement subplan (or other steps of P  pir ). In order
to facilitate this process, the AddFlaws procedure records these effects as open conditions.10
10. POCL planners operate by keeping track and repairing flaws found in a partial plan. Open conditions, operator threats, and resource threats are collectively called flaws (Penberthy & Weld, 1992).
AddFlaws(F,P) adds the set of flaws F to the plan structure P .

222

fiPlanning by Rewriting

procedure RewritePlan
Input: a valid partial-order plan P
a rewriting rule R = (qm , pr , pc ), V ariables(pr )  V ariables(qm )
Output: a valid rewritten partial-order plan P 0 (or failure)
1.  := M atch(qm , P )
Match the rule antecedent qm (:if field) against P . The result is a set of substitutions
 = {..., i , ...} for variables in qm .
2. If  =  then return failure
3. Choose a match i  
4. pir := i pr
Instantiate the subplan to be removed pr (the :replace field) according to i .
5. Pri := AddFlaws(UsefulEffects(pir ), P  pir )
Remove the instantiated subplan pir from the plan P and add the UsefulEffects of pir
as open conditions. The resulting plan Pri is now incomplete.
6. pic := i pc
Instantiate the replacement subplan pc (the :with field) according to i .
7. Pci := AddF laws(P reconditions(pic )  F indT hreats(Pri  pic ), Pri  pic )
Add the instantiated replacement subplan pic to Pri . Find new threats and open
conditions and add them as flaws. Pci is potentially incomplete, having several flaws
that need to be resolved.
8. P 0 := rP OP (Pci )
Complete the plan using a partial-order causal-link planning algorithm (restricted to
do only step reuse, but no step addition) in order to resolve threats and open conditions.
rP OP returns failure if no valid plan can be found.
9. Return P 0
Figure 13: Plan-Rewriting Algorithm
The result is the partial plan Pri (line 5). Continuing with our example, Figure 14(a) shows
the plan resulting from removing steps 1 and 4 from the plan in Figure 4.
Finally, the algorithm embeds the instantiated replacement subplan pic into the remainder of the original plan (lines 6-9). If the rule is completely specified, the algorithm simply
adds the (already instantiated) replacement subplan to the plan, and no further work is
necessary. If the rule is partially specified, the algorithm computes the embeddings of the
replacement subplan into the remainder of the original plan in three stages. First, the
algorithm adds the instantiated steps and links of the replacement plan pic (line 6) into
the current partial plan Pri (line 7). Figure 14(b) shows the state of our example after
pic , the new stack step (6), has been incorporated into the plan. Note the open conditions
(clear A) and on(C D). Second, the FindThreats procedure computes the possible threats,
both operator threats and resource conflicts, occurring in the Pri  pic partial plan (line 7);
for example, the threat situation on the clear(C) proposition between step 6 and 2 in Figure 14(b). These threats and the preconditions of the replacement plan pic are recorded by
AddFlaws resulting in the partial plan Pci . Finally, the algorithm completes the plan using
rPOP, a partial-order causal-link planning procedure restricted to only reuse steps (i.e., no
223

fiAmbite & Knoblock

step addition) (line 8). rPOP allows us to support our expressive operator language and to
have the flexibility for computing one or all embeddings. If only one rewriting is needed,
rPOP stops at the first valid plan. Otherwise, it continues until exhausting all alternative ways of satisfying open preconditions and resolving conflicts, which produces all valid
rewritings. In our running example, only one embedding is possible and the resulting plan
is that of Figure 14(c), where the new stack step (6) produces (clear A) and on(C D),
its preconditions are satisfied, and the ordering (6 2) ensures that the plan is valid.
The rewriting algorithm in Figure 13 is sound in the sense that it produces a valid plan
if the input is a valid plan, or it outputs failure if the input plan cannot be rewritten using
the given rule. Since this elementary plan-rewriting step is sound, the sequence of rewritings
performed during PbRs optimization search is also sound.
Lemma 1 (Soundness of rPOP) Partial-order causal-link (POCL) planning without
step addition (rP OP ) is sound.
Proof: In POCL planning, a precondition of a step of a plan is achieved either by
inserting a new step snew or reusing a step sreuse already present in the current plan (the
steps having an effect that unifies with the precondition). Forbidding step addition decreases
the set of available steps that can be used to satisfy a precondition, but once a step is found
rPOP proceeds as general POCL. Since, the POCL completion of a partial-plan is sound
(Penberthy & Weld, 1992), rP OP is also sound. 2
Theorem 1 (Soundness of Plan Rewriting) RewritePlan (Figure 13) produces a
valid plan if the input P is a valid plan, or outputs failure if the input plan cannot be
rewritten using the given rewriting rule R = (qm , pr , pc ).
Proof: Assume plan P is a solution to a planning problem with goals G and initial
state I. In POCL planning, a plan is valid iff the preconditions of all steps are supported
by causal links (the goals G are the preconditions of the goal step, and the initial state
conditions I are the effects of the initial step), and no operator threatens any causal link
(McAllester & Rosenblitt, 1991; Penberthy & Weld, 1992).
If rule R does not match plan P , the algorithm trivially returns failure (line 2). Assuming
there is a match i , after removing from P the steps and links specified in pir (including
all links  causal and ordering  incoming and outgoing from steps of pir ), the only open
conditions that exist in the resulting plan Pri are those that pir was achieving (line 5).
Adding the instantiated replacement subplan pic introduces more open conditions in the
partial plan: the preconditions of the steps of pic (line 7). There are no other sources of
open conditions in the algorithm.
Since plan P is valid initially, the only (operator and/or resource) threats present in
plan Pci (line 7) are those caused by the removal of subplan pir (line 3) and the addition of
subplan pic (line 7). The threats may occur between any operators and causal links of Pri pic
regardless whether the operator or causal link was initially in Pri or in pic . The threats in
the combined plan Pri  pic can be effectively computed by finding the relative positions of
its steps and comparing each causal link against the steps that may be ordered between the
producer and the consumer of the condition in the causal link (FindThreats, line 7).
At this point, we have shown that we have a plan (Pci ) with all the flaws (threats and
open conditions) explicitly recorded (by AddFlaws in lines 5 and 7). Since rP OP is sound
(Lemma 1), we conclude that rP OP will complete Pci and output a valid plan P 0 , or output
failure if the flaws in the plan cannot be repaired. 2
224

fiPlanning by Rewriting

clear(B)

REMOVED SUBPLAN

on(A Table)
on(C A)

Causal Link
Ordering Constraint
Side Effect

clear(A)

4 UNSTACK(C A)
on(C A)
clear(C)
on(D Table)

on(A Table)
clear(B)

3 STACK(A B Table)
on(C Table)
2 STACK(B C Table)

clear(C)
1 STACK(C D Table)

0

on(B Table)
on(A B)
clear(C)
on(B C)
on(C D)
GOAL

clear(D)
clear(B)

on(C Table)

clear(D)

A

on(B D)

B

on(B Table)
5 UNSTACK(B D)
on(B D)
clear(B)

C

B

C

A

D

D

clear(C)
Initial State

Goal State

(a) Application of a Rewriting Rule: After Removing Subplan
clear(B)

Causal Link
Ordering Constraint
Open conditions

on(A Table)

clear(A)
clear(C)
on(D Table)
0

on(C A)
clear(B)

3 STACK(A B Table)
on(B Table)
on(A B)
clear(C)
on(B C)

2 STACK(B C Table)
clear(C)

clear(A)

on(C A)
clear(D)

on(B D)

on(A Table)
clear(B)

on(C D)

6 STACK(C D A)

on(C D)
clear(D)
on(C A)

GOAL
A

clear(D)

B

on(B Table)
5 UNSTACK(B D)
on(B D)

clear(B)

C

B

C

A

D

D

clear(C)
Initial State

Goal State

(b) Application of a Rewriting Rule: After Adding Replacement Subplan
on(A Table)
clear(B)

on(A Table)

3 STACK(A B Table)

clear(B)
clear(A)

on(D Table)
on(C A)

2 STACK(B C Table)

clear(C)

0

6 STACK(C D A)
clear(B)

on(B Table)
on(A B)
clear(C)
on(B C)
on(C D)
GOAL

clear(D)
on(C A)

clear(D)

Causal Link
Ordering Constraint
Side Effect

on(B D)
on(B Table)
5 UNSTACK(B D)
on(B D)
clear(B)

A
B

C

B

C

A

D

D

clear(C)
Initial State

Goal State

(c) Rewritten Plan
Figure 14: Plan Rewriting: Applying rule avoid-move-twice of Figure 6 to plan of Figure 4
225

fiAmbite & Knoblock

Corollary 1 (Soundness of PbR Search) The optimization search of PbR is sound.
Proof: By induction. Assume an initial valid plan and a single step rewriting search.
By Theorem 1, the output is either a valid rewritten plan or failure. If the output is failure,
the search is trivially sound. Assume there is a valid plan Pn1 after n  1 rewriting steps.
According to Theorem 1, applying a single rewriting rule to plan Pn1 produces a valid
plan Pn or failure. Thus, an arbitrary number of rewritings produces a valid plan (or no
plan), so PbRs search is sound. 2
Although RewritePlan is sound, it may certainly produce plans that do not have the
minimal number of steps when faced with arbitrary rules. For example, imagine that the
consequent of a rewriting rule specified two identical steps s1 and s2 (both having as only
effects e1 and e2) and that the only flaws in Pci were exactly the open conditions e1 and e2.
Then, a sound but non step-minimal plan would be using s1 to satisfy e1 and using s2 to
satisfy e2 (although each step by itself could satisfy both open conditions). PbR does not
discard this plan because we do not make any restriction on the types of acceptable cost
functions. If we had a cost function that took the robustness of the plan into account, a
plan with both steps may be desirable.
We cannot guarantee that PbRs optimization search is complete in the sense that the
optimal plan would be found. PbR uses local search and it is well known that, in general,
local search cannot be complete. Even if PbR exhaustively explores the space of plan
rewritings induced by a given initial plan and a set of rewriting rules, we still cannot prove
that all solution plans will be reached. This is a property of the initial plan generator, the
set of rewriting rules, and the semantics of the planning domain. The rewriting rules of PbR
play a similar role as traditional declarative search control where the completeness of the
search may be traded for efficiency. Perhaps using techniques for inferring invariants in a
planning domain (Gerevini & Schubert, 1998; Fox & Long, 1998; Rintanen, 2000) or proving
convergence of term and graph rewriting systems (Baader & Nipkow, 1998), conditions for
completeness of a plan-rewriting search in a given planning domain could be obtained.
The design of a plan-rewriting algorithm depends on several parameters: the language of
the operators, the language of the rewriting rules, the choice of full-specification or partialspecification rewriting rules, and the need for all rewritings or one rewriting as required by
the search method.
The language of the operators affects the way in which the initial and rewritten plans are
constructed. Our framework supports the expressive operator definition language described
in Section 2.1. We provide support for this language by using standard techniques for causal
link establishment and threat checking like those in Sage (Knoblock, 1995) and UCPOP
(Penberthy & Weld, 1992).
The language of the antecedents of the rewriting rules affects the efficiency of matching.
Our system implements the conjunctive query language that was described in Section 3.1.1.
However, our system could easily accommodate a more expressive query language for the
rule antecedent, such as a relationally complete language (i.e., conjunction, disjunction, and
safe negation) (Abiteboul, Hull, & Vianu, 1995), or a recursive language such as datalog
with stratified negation, without significantly increasing the computational complexity of
the approach in an important way, as we discuss in Section 3.1.4.
The choice of fully versus partially specified rewriting rules affects the way in which the
replacement plan is embedded into the current plan. If the rule is completely specified,
226

fiPlanning by Rewriting

the embedding is already specified in the rule consequent, and the replacement subplan
is simply added to the current plan. If the rule is partially specified, our algorithm can
compute all the valid embeddings.
The choice of one versus all rewritings affects both the antecedent matching and the
embedding of rule consequent. The rule matches can be computed either all at the same
time, as in bottom-up evaluation of logic databases, or one-at-a-time as in Prolog, depending on whether the search strategy requires one or all rewritings. If the rule is fully-specified
only one embedding per match is possible. But, if the rule is partially-specified multiple
embeddings may result from a single match. If the search strategy only requires one rewriting, it must also provide a mechanism for choosing which rule is applied, which match is
computed, and which embedding is generated (rPOP can stop at the first embedding or
compute all embeddings). Our implemented rewriting algorithm has a modular design to
support different combinations of these choices.
3.1.4 Complexity of Plan Rewriting
The complexity of plan rewriting in PbR originates from two sources: matching the rule
antecedent against the plan, and computing the embeddings of the replacement plan. In
order to analyze the complexity of matching plan-rewriting rules, we introduce the following
database-theoretic definitions of complexity (Abiteboul et al., 1995):
Data Complexity: complexity of evaluating a fixed query for variable database inputs.
Expression Complexity: complexity of evaluating, on a fixed database instance, the
queries specifiable in a given query language.
Data complexity measures the complexity with respect to the size of the database.
Expression complexity measures the complexity with respect to the size of the queries
(taken from a given language). In our case, the database is the steps and links of the plan
and the queries are the antecedents of the plan-rewriting rules.
Formally, the language of the rule antecedents described in Section 3.1.1 is conjunctive
queries with interpreted predicates. The worst-case combined data and expression complexity of conjunctive queries is exponential (Abiteboul et al., 1995). That is, if the size of the
query (rule antecedent) and the size of the database (plan) grow simultaneously, there is
little hope of matching efficiently. Fortunately, relationally-complete languages have a data
complexity contained in Logarithmic Space, which is, in turn, contained in Polynomial Time
(Abiteboul et al., 1995). Thus our conjunctive query language has at most this complexity.
This is a very encouraging result that shows that the cost of evaluating a fixed query grows
very slowly as the database size increases. For PbR this means that matching the antecedent
of the rules is not strongly affected by the size of the plans. Moreover, in our experience
useful rule antecedents are not very large and contain many constant labels (at least, the
node and edge predicate names) that help to reduce the size of the intermediate results
and improve the efficiency of matching. This result also indicates that we could extend the
language of the antecedent to be relationally complete without affecting significantly the
performance of the system.11 Another possible extension is to use datalog with stratified
negation, which also has polynomial time data complexity. Graph-theoretic properties of
11. Figure 32 in Section 6 proposes an example of a rule with a relationally-complete antecedent using an
appropriate syntax.

227

fiAmbite & Knoblock

our plans could be easily described in datalog. For example, the possibly-adjacent interpreted predicate of Figure 7 could be described declaratively as a datalog program instead of
a piece of code. In summary, rule match for moderately sized rules, even for quite expressive
languages and large plans, remains tractable and can be made efficient using production
match (Forgy, 1982) and query optimization techniques (Sellis, 1988).
The second source of complexity is computing the embeddings of the replacement plan
given in the consequent of a plan-rewriting rule. By the definition of full-specification rules,
the embedding is completely specified in the rule itself. Thus, it suffices simply to remove
the undesired subplan and directly add the replacement subplan. This is linear in the size
of the consequent.
For partial-specification rules, computing all the embeddings of the replacement subplan
can be exponential in the size of the plan in the worst case. However, this occurs only in
pathological cases. For example, consider the plan in Figure 15(a) in which we are going to
compute the embeddings of step x into the remainder of the plan in order to satisfy the open
precondition g0. Step x has no preconditions and has two effects b and g0. Each step in
the plan has proposition b as an effect. Therefore, the new step x conflicts with every step
in the plan (1 to n) and has to be ordered with respect to these steps. Unfortunately, there
are an exponential number of orderings. In effect, the orderings imposed by adding the step
x correspond to all the partitions of the set of steps (1 to n) into two sets: one ordered
before x and one after. Figure 15(b) shows one of the possible orderings. If the subplan we
were embedding contained several steps that contained similar conflicts the problem would
be compounded. Even deciding if a single embedding exists is NP-hard. For example, if
we add two additional effects a and g1 to operator x, there is no valid embedding. In
the worst case (solving first the flaws induced by the conflicts on proposition b) we have to
explore an exponential number of positions for step x in the plan, all of which end up in
failure. Nevertheless, given the quasi-decomposability of useful planning domains we expect
the number of conflicts to be relatively small. Also most of the useful rewriting rules specify
replacement subplans that are small compared with the plan they are embedding into. Our
experience indicates that plan rewriting with partial-specification rules can be performed
efficiently as shown by the results of Section 4.
b

b

1

1

g1

a

a

b

g1
b

2

2

g2
g0
b

a

0

g

0

g2

a
b

x

g

g0

x

g0

a

n

gn

gn

a

b

n

(a) Before embedding

b

(b) One possible embedding

Figure 15: Exponential Embeddings

228

fiPlanning by Rewriting

3.1.5 A Taxonomy of Plan-Rewriting Rules
In order to guide the user in defining plan-rewriting rules for a domain or to help in designing
algorithms that may automatically deduce the rules from the domain specification (see
Section 6), it is helpful to know what kinds of rules are useful. We have identified the
following general types of transformation rules:
Reorder: These are rules based on algebraic properties of the operators, such as commutative, associative and distributive laws. For example, the commutative rule that reorders
two operators that need the same resource in Figure 10, or the join-swap rule in Figure 29
that combines the commutative and associative properties of the relational algebra.
Collapse: These are rules that replace a subplan by a smaller subplan. For example, when
several operators can be replaced by one, as in the remote-join-eval rule in Figure 29.
This rule replaces two remote retrievals at the same information source and a local join
operation by a single remote join operation, when the remote source has the capability of
performing joins. An example of the application of this rule to a query plan is shown in
Figure 30. Other examples are the Blocks World rules in Figure 6 that replace an unstack
and a stack operators either by an equivalent single stack operator or the empty plan.
Expand: These are rules that replace a subplan by a bigger subplan. Although this may
appear counter-intuitive initially, it is easy to imagine a situation in which an expensive
operator can be replaced by a set of operators that are cheaper as a whole. An interesting
case is when some of these operators are already present in the plan and can be synergistically reused. We did not find this rule type in the domains analyzed so far, but Backstrom
(1994a) presents a framework in which adding actions improves the quality of the plans.
His quality metric is the plan execution time, similarly to the manufacturing domain of Section 4.1. Figure 16 shows an example of a planning domain where adding actions improves
quality (from Backstrom, 1994a). In this example, removing the link between Bm and C1
and inserting a new action A shortens significantly the time to execute the plan.
P

Rn1

R1

R0
A

C1

C1
R0
A
P

P

Q1

Qm

B1

Rn
Cn

P

Rn
Cn

P
Q1

Rn1

R1

R0
A

B1

Bm

Bm Qm

Qm1

Qm1

(a) Low Quality Plan

(b) High Quality Plan

Figure 16: Adding Actions Can Improve Quality
Parallelize: These are rules that replace a subplan with an equivalent alternative subplan
that requires fewer ordering constraints. A typical case is when there are redundant or alternative resources that the operators can use. For example, the rule punch-by-drill-press
in Figure 7. Another example is the rule that Figure 16 suggests that could be seen as a
combination of the expand and parallelize types.
229

fiAmbite & Knoblock

3.2 Selection of Next Plan: Search Strategies
Although the space of rewritings can be explored systematically, the Planning by Rewriting
framework is better suited to the local search techniques typical of combinatorial optimization algorithms. The characteristics of the planning domain, the initial plan generator, and
the rewriting rules determine which local search method performs best. First, we discuss
how the initial plan generator affects the choice of local search methods. Second, we consider the impact of the rewriting rules. Third, we discuss the role of domain knowledge in
the search process. Finally, we describe how several local search methods work in PbR.
An important difference between PbR and traditional combinatorial algorithms is the
generation of feasible solutions. Usually, in combinatorial optimization problems there exists
an effective procedure to generate all feasible solutions (e.g., the permutations of a schedule).
Thus, even if the local search graph is disconnected, by choosing an appropriate initial
solution generator (e.g., random) we could fall in a component of the graph that contains
the global optimum. In PbR we cannot assume such powerful initial plan generators. Even
in optimization domains, which have efficient initial plan generators, we may not have
guarantees on the coverage of the solution space they provide. Therefore, the optimal plan
may not be reachable by applying the rewriting rules when starting from the initial plans
available from the generator. Nevertheless, for many domains an initial plan generator that
provides a good sample of the solution space is sufficient for multiple-restart search methods
to escape from low-quality local minima and provide high-quality solutions.
The plan-rewriting rules define the neighborhood function, which may be exact (cf.
Section 2.3) or not. For example, in the query planning domain we can define a set of
rules that completely generate the space of solution plans (because of the properties of the
relational algebra). In other domains it may be hard to prove that we have an exact set of
rules. Both the limitations on initial plan generation and the plan-rewriting rules affect the
possibility of theoretically reaching the global optimum. This is not surprising since many
problems, regardless of whether they are cast as planning or in other formalisms, do not have
converging local search algorithms (e.g., Papadimitriou & Steiglitz, 1977). Nevertheless, in
practice, good local optima can still be obtained for many domains.
Many local search methods, such as first and best improvement, simulated annealing,
tabu search, or variable-depth search, can be applied straightforwardly to PbR. In our
experiments in Section 4 we have used first and best improvement, which have performed
well. Next, we describe some details of the application of these two methods in PbR. In
Section 6, we discuss our ideas for using variable-depth plan rewriting.
First improvement generates the rewritings incrementally and selects the first plan of
better cost than the current one. In order to implement this method efficiently we can use a
tuple-at-a-time evaluation of the rule antecedent, similarly to the behavior of Prolog. Then,
for that rule instantiation, generate one embedding, test the cost of the resulting plan, and
if it is not better that the current plan, repeat. We have the choice of generating another
embedding of the same rule instantiation, generate another instantiation of the same rule,
or generate a match for a different rule.
Best improvement generates the complete set of rewritten plans and selects the best.
This method requires computing all matches and all embeddings for each match. All the
matches can be obtained by evaluating the rule antecedent as a set-at-a-time database
230

fiPlanning by Rewriting

query. As we discussed in Section 3.1.4 such query evaluation can be quite efficient. In our
experience, computing the plan embeddings was usually more expensive than computing
the rule matches.
In Planning by Rewriting the choice of the initial plan generator, the rewriting rules,
and the search methods is intertwined. Once the initial plan generator is fixed, it determines
the shape of the plans that would have to be modified by the rewriting rules, then according
to this neighborhood, the most appropriate search mechanism can be chosen. PbR has a
modular design to facilitate experimentation with different initial plan generators, sets of
rewriting rules, and search strategies.
3.3 Plan Quality
In most practical planning domains the quality of the plans is crucial. This is one of the
motivations for the Planning by Rewriting approach. In PbR the user defines the measure
of plan quality most appropriate for the application domain. This quality metric could
range from a simple domain-independent cost metric, such as the number of steps, to more
complex domain-specific ones. For example, in the query planning domain the measure of
plan quality usually is an estimation of the query execution cost based on the size of the
database relations, the data manipulation operations involved in answering a query, and
the cost of network transfer. In a decentralized environment, the cost metric may involve
actual monetary costs if some of the information sources require payments. In the jobshop scheduling domain some simple cost functions are the schedule length (that is, the
parallel time to finish all pieces), or the sum of the times to finish each piece. A more
sophisticated manufacturing domain may include a variety of concerns such as the cost,
reliability, and precision of each operator/process, the costs of resources and materials used
by the operators, the utilization of the machines, etc. The reader will find more detailed
examples of quality metrics in these domains in Sections 4.1 and 4.4.
A significant advantage of PbR is that the complete plan is available to assess its quality.
In generative planners the complete plan is not available until the search for a solution is
completed, so usually only very simple plan quality metrics, such as the number of steps,
can be used. Some work does incorporate quality concerns into generative planners (Estlin
& Mooney, 1997; Borrajo & Veloso, 1997; Perez, 1996). These systems automatically
learn search control rules to improve both the efficiency of planning and the quality of the
resulting plans. In PbR the rewriting rules can be seen as post facto optimization search
control. As opposed to guiding the search of a generative planner towards high-quality
solutions based only on the information available in partial plans, PbR improves the quality
of complete solution plans without any restriction on the types of quality metrics. Moreover,
if the plan cost is not additive, a plan refinement strategy is impractical since it may need
to exhaustively explore the search space to find the optimal plan. An example of nonadditive cost function appears in the UNIX planning domain (Etzioni & Weld, 1994) where
a plan to transfer files between two machines may be cheaper if the files are compressed
initially (and uncompressed after arrival). That is, the plan that includes the compression
(and the necessary uncompression) operations is more cost effective, but a plan refinement
search would not naturally lead to it. By using complete plans, PbR can accurately assess
arbitrary measures of quality.
231

fiAmbite & Knoblock

3.4 Initial Plan Generation
Fast initial plan generation is domain-specific in nature. It requires the user to specify an
efficient mechanism to compute the initial solution plan. In general, generating an initial
plan may be as hard as generating the optimal plan. However, the crucial intuition behind
planning algorithms is that most practical problems are quasi-decomposable (Simon, 1969),
that is, that the interactions among parts of the problems are limited. If interactions in a
problem are pervasive, such as in the 8-puzzle, the operator-based representation and the algorithms of classical planning are of little use. They would behave as any other search based
problem solver. Fortunately, many practical problems are indeed quasi-decomposable. This
same intuition also suggests that finding initial plan generators for planning problems may
not be as hard as it appears, because the system can solve the subproblems independently,
and then combine them in the simplest way, for example, concatenating the solutions sequentially. Moreover, in many circumstances the problems may be easily transformed into a
state that minimizes the interactions and solving the problem from this state is much easier.
For example, in the Blocks World the state in which all blocks are on the table minimizes
the interactions. It is simple to design an algorithm that solves any Blocks World problem
passing through such intermediate state. Using these methods an initial plan generator may
produce suboptimal initial plans but at a reasonable planning cost.
These ideas for constructing initial plan generators can be embodied in two general ways,
which are both implemented in our system. The first one is to bootstrap on the results of
a general purpose planning algorithm with a strong search control bias. The second one is
to provide the user convenient high-level facilities in which to describe plan construction
algorithms programmatically.
3.4.1 Biased Generative Planners
There are a variety of ways in which to control the search of a generic planner. Some planners
accept search control rules, others accept heuristic functions, and some have built-in search
control. We present examples of these techniques.
A very general way of efficiently constructing plans is to use a domain-independent
generative planner that accepts search control rules. For example, Prodigy (Carbonell,
Knoblock, & Minton, 1991), UCPOP (Penberthy & Weld, 1992) and Sage (Knoblock, 1995)
are such planners. By setting the type of search and providing a strong bias by means of the
search control rules, the planner can quickly generate a valid, although possibly suboptimal,
initial plan. For example, in the manufacturing domain of (Minton, 1988a), analyzed in
detail in Section 4.1, depth-first search and a goal selection heuristic based on abstraction
hierarchies (Knoblock, 1994a) quickly generates a feasible plan, but often the quality of this
plan, which is defined as the time required to manufacture all objects, is suboptimal.
TLPlan (Bacchus & Kabanza, 1995, 2000) is an efficient forward-chaining planner that
uses search control expressed in temporal logic. Because in forward chaining the complete
state is available, much more refined domain control knowledge can be specified. The
preferred search strategy used by TLPlan is depth-first search, so although it finds plans
efficiently, the plans may be of low quality. Note that because it is a generative planner
that explores partial sequences of steps, it cannot use sophisticated quality measures.
232

fiPlanning by Rewriting

HSP (Bonet, Loerincs, & Geffner, 1997; Bonet & Geffner, 1999) is a forward search
planner that performs a variation of heuristic search applied to classical AI planning. The
built-in heuristic function is a relaxed version of the planning problem: it computes the
number of required steps to reach the goal disregarding negated effects in the operators.
Such metric can be computed efficiently. Despite its simplicity and that the heuristic is not
admissible, it scales surprisingly well for many domains. Because the plans are generated
according to the fixed heuristic function, the planner cannot incorporate a quality metric.
These types of planners are quite efficient in practice although they often produce suboptimal plans. They are excellent candidates to generate the initial plans that will be
subsequently optimized by PbR.
3.4.2 Facilitating Algorithmic Plan Construction
For many domains, simple domain-dependent approximation algorithms will provide good
initial plans. For example, in the query planning domain, the system can easily generate
initial query evaluation plans by randomly (or greedily) parsing the given query. In the
Blocks World it is also straightforward to generate a solution in linear time using the naive
algorithm: put all blocks on the table and build the desired towers from the bottom up.
This algorithm produces plans of length no worse than twice the optimal, which makes it
already a good approximation algorithm. However, the interest in the Blocks World has
traditionally been on optimal solutions, which is an NP-hard problem (Gupta & Nau, 1992).
Our system facilitates the creation of these initial plans by freeing the user from specifying the detailed graph structure of a plan. The user only needs to specify an algorithm
that produces a sequence of instantiated actions, that is, action names and the ground
parameters that each action takes.12 For example, the (user-defined) naive algorithm for
the Blocks World domain described above applied to the problem in Figure 4 produces
the sequence: unstack(C A), unstack(B D), stack(C D Table), stack(B C Table), and
stack(A B Table). Then, the system automatically converts this sequence of actions into
a fully detailed partial-order plan using the operator specification of the domain. The resulting plan conforms to the internal data structures that PbR uses. This process includes
creating nodes that are fully detailed operators with preconditions and effects, and adding
edges that represent all the necessary causal links and ordering constraints. In our Blocks
World example the resulting plan is that of Figure 4.
The algorithm that transforms the user-defined sequence of actions into a partial-order
plan is presented in Figure 17. The algorithm first constructs the causal structure of the plan
(lines 2 to 6) and then adds the necessary ordering links to avoid threats (lines 7 to 10).
The user only needs to specify action names and the corresponding instantiated action
parameters. Our algorithm consults the operator specification to find the preconditions
and effects, instantiate them, construct the causal links, and check for operator threats.
Operator threats are always resolved in favor of the ordering given by the user in the input
plan. The reason is that the input plan may be overconstrained by the total order, but it
is assumed valid. Therefore, by processing each step last to first, only the orderings that
indeed avoid threats are included in the partial-order plan.
12. The algorithm also accepts extra ordering constraints in addition to the sequence if they are available
from the initial plan generator.

233

fiAmbite & Knoblock

procedure TO2PO
Input: a valid total-order plan (a1 , ..., an )
Output: an equivalent partial-order plan
1. for i := n to 1
2.
for p  Preconditions(ai )
3.
choose k < i such that
4.
1. p  PositiveEffects(ak ) 
5.
2. 6  l such that k < l < i  p  NegativeEffects(al )
6.
add order ak  ai
7.
for p  NegativeEffects(ai )
8.
for j := (i  1) to 1
9.
if p  Preconditions(aj )
10.
then add order aj  ai
11. return ((a1 , ..., an ), )
Figure 17: Algorithm for Converting Total-order to Partial-order Plans
Our algorithm is an extension of the greedy algorithm presented by Veloso, Perez, & Carbonell (1990). Our algorithm explores non-deterministically all the producers of a proposition (line 3), as opposed to taking the latest producer in the sequence as in their algorithm.13
That is, if our algorithm is explored exhaustively, it produces all partially-ordered causal
structures consistent with the input sequence. Our generalization stems from the criticism
by Backstrom (1994b) to the algorithm by Veloso et al. (1990) and our desire of being able
to produce alternative initial plans.
The problem of transforming a sequence of steps into a least constrained plan is analyzed
by Backstrom (1994b) under several natural definitions of optimality. Under his definitions
of least-constrained plan and shortest parallel execution the problem is NP-hard. Backstrom
shows that Velosos algorithm, although polynomial, does not conform to any of these natural definitions. Because our algorithm is not greedy, it does not suffer from the drawbacks
pointed out by Backstrom. Moreover, for our purposes we do not need optimal initial plans.
The space of partial orders will be explored during the rewriting process.
Regardless of the method for producing initial plans, generators that provide multiple
plans are preferable. The different initial plans are used in conjunction with multiple restart
search techniques in order to escape from low-quality local minima.

4. Empirical Results
In this section we show the broad applicability of Planning by Rewriting by analyzing four
domains with different characteristics: a process manufacturing domain (Minton, 1988b),
a transportation logistics domain, the Blocks World domain that we used in the examples
throughout the paper, and a domain for distributed query planning.
13. To implement their algorithm it is enough to replace line 3 in Figure 17 with:
find max k < i such that

234

fiPlanning by Rewriting

4.1 Manufacturing Process Planning
The task in the manufacturing process planning domain is to find a plan to manufacture
a set of parts. We implemented a PbR translation of the domain specification in (Minton,
1988b). This domain contains a variety of machines, such as a lathe, punch, spray painter,
welder, etc, for a total of ten machining operations. The operator specification is shown in
Figures 18 and 19. The features of each part are described by a set of predicates, such as
temperature, painted, has-hole, etc. These features are changed by the operators. Other
predicates in the state, such as has-clamp, is-drillable, etc, are set in the initial state
of each problem.
As an example of the behavior of an operator, consider the polish operator in Figure 18.
It requires the part to manufacture to be cold and that the polisher has a clamp to secure
the part to the machine. The effect of applying this operator is to leave the surface of the
part polished. Some attributes of a part, such as surface-condition, are single-valued,
but others, like has-hole, are multivalued. Note how the drill-press and the punch
operators in Figure 18 do not prevent several has-hole conditions from being asserted on
the same part. Other interesting operators are weld and bolt. These operators join two
parts in a particular orientation to form a new part. No further operations can be performed
on the separate parts once they have been joined.
The measure of plan cost is the schedule length, the (parallel) time to manufacture all
parts. In this domain all of the machining operations are assumed to take unit time. The
machines and the objects (parts) are modeled as resources in order to enforce that only one
part can be placed on a machine at a time and that a machine can only operate on a single
part at a time (except bolt and weld which operate on two parts simultaneously).
We have already shown some of the types of rewriting rules for this domain in Figures 7
and 10. The set of rules that we used for our experiments is shown in Figure 20. The top
eight rules are quite straightforward once one becomes familiar with this domain. The two
top rules explore the space of alternative orderings originated by resource conflicts. The
machine-swap rule allows the system to explore the possible orderings of operations that
require the same machine. This rule finds two consecutive operations on the same machine
and swaps their order. Similarly, the rule object-swap allows the system to explore the
orderings of the operations on the same object. These two rules use the interpreted predicate
adjacent-in-critical-path to focus the attention on the steps that contribute to our cost
function. Adjacent-in-critical-path checks if two steps are consecutive along one of the
critical paths of a schedule. A critical path is a sequence of steps that take the longest
time to accomplish. In other words, a critical path is one of the sequences of steps that
determine the schedule length.
The next six rules exchange operators that are equivalent with respect to achieving
some effects. Rules IP-by-SP and SP-by-IP propose the exchange of immersion-paint
and spray-paint operators. By examining the operator definitions in Figure 19, it can be
readily noticed that both operators change the value of the painted predicate. Similarly,
PU-by-DP and DP-by-PU exchange drill-press and punch operators, which produce the
has-hole predicate. Finally, roll-by-lathe and lathe-by-roll exchange roll and lathe
operators as they both can make parts cylindrical. To focus the search on the most promising
235

fiAmbite & Knoblock

(define (operator POLISH)
:parameters (?x)
:resources ((machine POLISHER) (is-object ?x))
:precondition (:and (is-object ?x)
(temperature ?x COLD)
(has-clamp POLISHER))
:effect
(:and (:forall (?surf)
(:when (:neq ?surf POLISHED)
(:not (surface-condition ?x ?surf)))
(surface-condition ?x POLISHED)))

(define (operator GRIND)
:parameters (?x)
:resources ((machine GRINDER) (is-object ?x))
:precondition (is-object ?x)
:effect
(:and (:forall (?color)
(:not (painted ?x ?color)))
(:forall (?surf)
(:when (:neq ?surf SMOOTH)
(:not (surface-condition ?x ?surf))))
(surface-condition ?x SMOOTH)))

(define (operator LATHE)
:parameters (?x)
:resources ((machine LATHE) (is-object ?x))
:precondition (is-object ?x)
:effect
(:and (:forall (?color)
(:not (painted ?x ?color)))
(:forall (?shape)
(:when (:neq ?shape CYLINDRICAL)
(:not (shape ?x ?shape))))
(:forall (?surf)
(:when (:neq ?surf ROUGH)
(:not (surface-condition ?x ?surf))))
(surface-condition ?x ROUGH)
(shape ?x CYLINDRICAL)))

(define (operator ROLL)
:parameters (?x)
:resources ((machine ROLLER) (is-object ?x))
:precondition (is-object ?x)
:effect
(:and (:forall (?color)
(:not (painted ?x ?color)))
(:forall (?shape)
(:when (:neq ?shape CYLINDRICAL)
(:not (shape ?x ?shape))))
(:forall (?temp)
(:when (:neq ?temp HOT)
(:not (temperature ?x ?temp))))
(:forall (?surf)
(:not (surface-condition ?x ?surf)))
(:forall (?width ?orientation)
(:not (has-hole ?x ?width ?orientation)))
(temperature ?x HOT)
(shape ?x CYLINDRICAL)))

(define (operator DRILL-PRESS)
:parameters (?x ?width ?orientation)
:resources ((machine DRILL-PRESS)
(is-object ?x))
:precondition
(:and (is-object ?x)
(have-bit ?width)
(is-drillable ?x ?orientation))
:effect (has-hole ?x ?width ?orientation))

(define (operator PUNCH)
:parameters (?x ?width ?orientation)
:resources ((machine PUNCH) (is-object ?x))
:precondition
(:and (is-object ?x)
(has-clamp PUNCH)
(is-punchable ?x ?width ?orientation))
:effect
(:and (:forall (?surf)
(:when (:neq ?surf ROUGH)
(:not (surface-condition ?x ?surf))))
(surface-condition ?x ROUGH)
(has-hole ?x ?width ?orientation)))

Figure 18: Operators for Manufacturing Process Planning (I)

exchanges these rules only match operators in the critical path (by means of the interpreted
predicate in-critical-path).
The six bottom rules in Figure 20 are more sophisticated. The lathe+SP-by-SP rule
takes care of an undesirable effect of the simple depth-first search used by our initial plan
generator. In this domain, in order to spray paint a part, the part must have a regular shape.
Being cylindrical is a regular shape, therefore the initial planner may decide to make the
part cylindrical by lathing it in order to paint it! However, this may not be necessary as the
part may already have a regular shape (for example, it could be rectangular, which is also
a regular shape). Thus, the lathe+SP-by-SP substitutes the pair spray-paint and lathe
by a single spray-paint operation. The supporting regular-shapes interpreted predicate
236

fiPlanning by Rewriting

(define (operator IMMERSION-PAINT)
:parameters (?x ?color)
:resources ((machine IMMERSION-PAINTER)
(is-object ?x))
:precondition
(:and (is-object ?x)
(have-paint-for-immersion ?color))
:effect (painted ?x ?color))

(define (operator SPRAY-PAINT)
:parameters (?x ?color ?shape)
:resources ((machine SPRAY-PAINTER)
(is-object ?x))
:precondition (:and (is-object ?x)
(sprayable ?color)
(temperature ?x COLD)
(regular-shape ?shape)
(shape ?x ?shape)
(has-clamp SPRAY-PAINTER))
:effect (painted ?x ?color))

(define (operator BOLT)
(define (operator WELD)
:parameters (?x ?y ?new-obj ?orient ?width)
:parameters (?x ?y ?new-obj ?orient)
:resources ((machine BOLTER)
:resources ((machine WELDER)
(is-object ?x) (is-object ?y))
(is-object ?x) (is-object ?y))
:precondition
:precondition
(:and (is-object ?x) (is-object ?y)
(:and (is-object ?x) (is-object ?y)
(composite-object ?new-obj ?orient ?x ?y)
(composite-object ?new-obj ?orient ?x ?y)
(has-hole ?x ?width ?orient)
(can-be-welded ?x ?y ?orient))
(has-hole ?y ?width ?orient)
:effect (:and (temperature ?new-obj HOT)
(bolt-width ?width)
(joined ?x ?y ?orient)
(can-be-bolted ?x ?y ?orient))
(:not (is-object ?x))
:effect (:and (:not (is-object ?x))
(:not (is-object ?y))))
(:not (is-object ?y))
(joined ?x ?y ?orient)))

Figure 19: Operators for Manufacturing Process Planning (II)

just enumerates which are the regular shapes. These rules are partially specified and are
not guaranteed to always produce a rewriting. Nevertheless, they are often successful in
producing plans of lower cost.
The remaining rules explore bolting two parts using bolts of different size if fewer operations may be needed for the plan. We developed these rules by analyzing differences
in the quality of the optimal plans and the rewritten plans. For example, consider the
both-providers-diff-bolt rule. This rule states that if the parts to be bolted already
have compatible holes in them, it is better to reuse those operators that produced the
holes. The initial plan generator may have drilled (or punched) holes whose only purpose
was to bolt the parts. However, the goal of the problem may already require some holes to
be performed on the parts to be joined. Reusing the available holes produces a more economical plan. The rules has-hole-x-diff-bolt-add-PU, has-hole-x-diff-bolt-add-DP,
has-hole-y-diff-bolt-add-PU, and has-hole-y-diff-bolt-add-DP address the cases in
which only one of the holes can be reused, and thus an additional punch or drill-press
operation needs to be added.
As an illustration of the rewriting process in the manufacturing domain, consider Figure 21. The plan at the top of the figure is the result of a simple initial plan generator that
solves each part independently and concatenates the corresponding subplans. Although
such plan is generated efficiently, it is of poor quality. It requires six time-steps to manufacture all parts. The figure shows the application of two rewriting rules, machine-swap and
IP-by-SP, that improve the quality of this plan. The operators matched by the rule antecedent are shown in italics. The operators introduced in the rule consequent are shown in
bold. First, the machine-swap rule reorders the punching operations on parts A and B. This
237

fiAmbite & Knoblock

(define-rule :name machine-swap
:if (:operators ((?n1 (machine ?x) :resource)
(?n2 (machine ?x) :resource))
:links ((?n1 :threat ?n2))
:constraints
(adjacent-in-critical-path ?n1 ?n2))
:replace (:links (?n1 ?n2))
:with (:links (?n2 ?n1)))

(define-rule :name object-swap
:if (:operators ((?n1 (is-object ?x) :resource)
(?n2 (is-object ?x) :resource))
:links ((?n1 :threat ?n2))
:constraints
(adjacent-in-critical-path ?n1 ?n2))
:replace (:links (?n1 ?n2))
:with (:links (?n2 ?n1)))

(define-rule :name SP-by-IP
(define-rule :name IP-by-SP
:if (:operators (?n1 (spray-paint ?x ?c ?s))
:if (:operators (?n1 (immersion-paint ?x ?c))
:constraints ((in-critical-path ?n1)))
:constraints ((regular-shapes ?s)
:replace (:operators (?n1))
(in-critical-path ?n1)))
:with (:operators (?n2 (immersion-paint ?x ?c))))
:replace (:operators (?n1))
:with (:operators (?n2 (spray-paint ?x ?c ?s))))
(define-rule :name DP-by-PU
(define-rule :name PU-by-DP
:if (:operators ((?n1 (drill-press ?x ?w ?o)))
:if (:operators (?n1 (punch ?x ?w ?o))
:constraints ((in-critical-path ?n1)))
:constraints ((in-critical-path ?n1)))
:replace (:operators (?n1))
:replace (:operators (?n1))
:with (:operators (?n2 (punch ?x ?w ?o))))
:with (:operators (?n2 (drill-press ?x ?w ?o))))
(define-rule :name roll-by-lathe
:if (:operators ((?n1 (roll ?x)))
:constraints ((in-critical-path ?n1)))
:replace (:operators (?n1))
:with (:operators (?n2 (lathe ?x))))

(define-rule :name lathe-by-roll
:if (:operators ((?n1 (lathe ?x)))
:constraints ((in-critical-path ?n1)))
:replace (:operators (?n1))
:with (:operators (?n2 (roll ?x))))

(define-rule :name both-providers-diff-bolt
(define-rule :name lathe+SP-by-SP
:if (:operators ((?n3 (bolt ?x ?y ?z ?o ?w1)))
:if (:operators
:links ((?n1 (has-hole ?x ?w1 ?o) ?n3)
((?n1 (lathe ?x))
(?n2 (has-hole ?y ?w1 ?o) ?n3)
(?n2 (spray-paint ?x ?color ?shape1)))
(?n4 (has-hole ?x ?w2 ?o) ?n5)
:constraints ((regular-shapes ?shape2)))
(?n6 (has-hole ?y ?w2 ?o) ?n7))
:replace (:operators (?n1 ?n2))
:constraints ((:neq ?w1 ?w2)))
:with (:operators
((?n3 (spray-paint ?x ?color ?shape2))))) :replace (:operators (?n1 ?n2 ?n3))
:with (:operators ((?n8 (bolt ?x ?y ?z ?o ?w2)))
:links ((?n4 (has-hole ?x ?w2 ?o) ?n8)
(?n6 (has-hole ?y ?w2 ?o) ?n8))))
(define-rule :name has-hole-x-diff-bolt-add-DP
(define-rule :name has-hole-x-diff-bolt-add-PU
:if (:operators ((?n3 (bolt ?x ?y ?z ?o ?w1)))
:if (:operators ((?n3 (bolt ?x ?y ?z ?o ?w1)))
:links ((?n1 (has-hole ?x ?w1 ?o) ?n3)
:links ((?n1 (has-hole ?x ?w1 ?o) ?n3)
(?n2 (has-hole ?y ?w1 ?o) ?n3)
(?n2 (has-hole ?y ?w1 ?o) ?n3)
(?n4 (has-hole ?x ?w2 ?o) ?n5))
(?n4 (has-hole ?x ?w2 ?o) ?n5))
:constraints ((:neq ?w1 ?w2)))
:constraints ((:neq ?w1 ?w2)))
:replace (:operators (?n1 ?n2 ?n3))
:replace (:operators (?n1 ?n2 ?n3))
:with (:operators ((?n8 (bolt ?x ?y ?z ?o ?w2))
:with (:operators ((?n8 (bolt ?x ?y ?z ?o ?w2))
(?n6 (drill-press ?y ?w2 ?o)))
(?n6 (punch ?y ?w2 ?o)))
:links ((?n4 (has-hole ?x ?w2 ?o) ?n8)
:links ((?n4 (has-hole ?x ?w2 ?o) ?n8)
(?n6 (has-hole ?y ?w2 ?o) ?n8))))
(?n6 (has-hole ?y ?w2 ?o) ?n8))))
(define-rule :name has-hole-y-diff-bolt-add-DP
(define-rule :name has-hole-y-diff-bolt-add-PU
:if (:operators ((?n3 (bolt ?x ?y ?z ?o ?w1)))
:if (:operators ((?n3 (bolt ?x ?y ?z ?o ?w1)))
:links ((?n1 (has-hole ?x ?w1 ?o) ?n3)
:links ((?n1 (has-hole ?x ?w1 ?o) ?n3)
(?n2 (has-hole ?y ?w1 ?o) ?n3)
(?n2 (has-hole ?y ?w1 ?o) ?n3)
(?n6 (has-hole ?y ?w2 ?o) ?n7))
(?n6 (has-hole ?y ?w2 ?o) ?n7))
:constraints ((:neq ?w1 ?w2)))
:constraints ((:neq ?w1 ?w2)))
:replace (:operators (?n1 ?n2 ?n3))
:replace (:operators (?n1 ?n2 ?n3))
:with (:operators ((?n8 (bolt ?x ?y ?z ?o ?w2))
:with (:operators ((?n8 (bolt ?x ?y ?z ?o ?w2))
(?n4 (drill-press ?x ?w2 ?o)))
(?n4 (punch ?x ?w2 ?o)))
:links ((?n4 (has-hole ?x ?w2 ?o) ?n8)
:links ((?n4 (has-hole ?x ?w2 ?o) ?n8)
(?n6 (has-hole ?y ?w2 ?o) ?n8))))
(?n6 (has-hole ?y ?w2 ?o) ?n8))))

Figure 20: Rewriting Rules for Manufacturing Process Planning
238

fiPlanning by Rewriting

Lathe A

IPaint A Red

Punch A 2

Punch C 1

IPaint C Blue

Roll B

IPaint B Red

Reorder Parts on a Machine
Lathe A

IPaint A Red
Punch C 1

Cost: 6

Punch A 2

Cost: 4

IPaint C Blue

IPaint B Red

Roll B

Immersion-Paint => Spray-Paint
Lathe A
Roll B

IPaint A Red

Punch A 2

Punch C 1

IPaint C Blue

Cost: 3

Spray-Paint B Red

Figure 21: Rewriting in the Manufacturing Domain
breaks the long critical path that resulted from the simple concatenation of their respective
subplans. The schedule length improves from six to four time-steps. Still, the three parts
A, B, and C use the same painting operation (immersion-paint). As the immersion-painter
can only process one piece at a time, the three operations must be done serially. Fortunately, in our domain there is another painting operation: spray-paint. The IP-by-SP
rule takes advantage of this fact and substitutes an immersion-paint operation on part B
by a spray-paint operation. This further parallelizes the plan obtaining a schedule length
of three time-steps, which is the optimal for this plan.
We compare four planners (IPP, Initial, and two configurations of PbR):
IPP: This is one of the most efficient domain-independent planners (Koehler, Nebel, Hoffman, & Dimopoulos, 1997) in the planning competition held at the Fourth International
Conference on Artificial Intelligence Planning Systems (AIPS-98). IPP is an optimized reimplementation and extension of Graphplan (Blum & Furst, 1995, 1997). IPP produces
shortest parallel plans. For our manufacturing domain, this is exactly the schedule length,
the cost function that we are optimizing.
Initial: The initial plan generator uses a divide-and-conquer heuristic in order to generate
plans as fast as possible. First, it produces subplans for each part and for the joined goals
independently. These subplans are generated by Sage using a depth-first search without any
regard to plan cost. Then, it concatenates the subsequences of actions and merges them
using the facilities of Section 3.4.2.
PbR: We present results for two configurations of PbR, which we will refer to as PbR-100
and PbR-300. Both configurations use a first improvement gradient search strategy with
random walk on the cost plateaus. The rewriting rules used are those of Figure 20. For each
problem PbR starts its search from the plan generated by Initial. The two configurations
differ only on how many total plateau plans are allowed. PbR-100 allows considering up
to 100 plans that do not improve the cost without terminating the search. Similarly, PbR239

fiAmbite & Knoblock

300 allows 300 plateau plans. Note that the limit is across all plateaus encountered during
the search for a problem, not for each plateau.
We tested each of the four systems on 200 problems, for machining 10 parts, ranging
from 5 to 50 goals. The goals are distributed randomly over the 10 parts. So, for the 50goal problems, there is an average of 5 goals per part. The results are shown in Figure 22.
In these graphs each data point is the average of 20 problems for each given number of goals.
There were 10 provably unsolvable problems. Initial and PbR solved all 200 problems (or
proved them unsolvable). IPP solved 65 problems in total: all problems at 5 and 10 goals,
19 at 15 goals, and 6 at 20 goals. IPP could not solve any problem with more than 20 goals
under the 1000 CPU seconds time limit.
Figure 22(a) shows the average time on the solvable problems for each problem set for
the four planners. Figure 22(b) shows the average schedule length for the problems solved
by all the planners, that is, over the 65 problems solved by IPP up to 20 goals. The
fastest planner is Initial, but it produces plans with a cost of about twice the optimal. IPP
produces the optimal plans, but it cannot solve problems of more than 20 goals. The two
configurations of PbR scale much better than IPP solving all problems and producing good
quality plans. PbR-300 matches the optimal cost of the IPP plans, except in one problem
(the reason for the difference is interesting and we explain it below). The faster PbR-100
also stays very close to the optimal (less than 2.5% average cost difference).
Figure 22(c) shows the average schedule length for the problems solved by each of
the planners for the 50 goal range. The PbR configurations scale gracefully across this
range improving considerably the cost of the plans generated by Initial. The additional
exploration of PbR-300 allows it to improve the plans even further. The reason for the
difference between PbR and IPP at the 20-goal complexity level is because the cost results
for IPP are only for the 6 problems that it could solve, while the results for PbR and Initial
are the average of all of the 20 problems (as shown in Figure 22(b), PbR matches the cost
of these 6 optimal plans produced by IPP).
Figure 22(d) shows the average number of operators in the plans for the problems solved
by all three planners (up to 20 goals). Figure 22(e) shows the average number of operators
in the plans for the problems solved by each planner across the whole range of 50 problems.
The plans generated by Initial use about 2-3 additional operators. Both PbR and IPP
produce plans that require fewer steps. Interestingly, IPP sometimes produces plans that
use more operations than PbR. IPP produces the shortest parallel plan, but not the one
with the minimum number of steps. In particular, we observed that some of the IPP plans
suffer from the same problem as Initial. IPP would also lathe a part in order to paint
it, but as opposed to Initial it would only do so if it did not affect the optimal schedule
length. Surprisingly, adding such additional steps in this domain may improve the schedule
length, albeit in fairly rare situations. This was the case in the only problem in which IPP
produced a better schedule than PbR-300. We could have introduced a rewriting rule that
substituted an immersion-paint operator by both a lathe and spray-paint operators
for such cases. However, such rule is of very low utility (in the sense of Minton, 1988b).
It expands the rewriting search space, adds to the cost of match, and during the random
search provides some benefit very rarely.
240

fiPlanning by Rewriting

Average Planning Time (CPU Seconds)

1000
PbR-FI
Initial
IPP

100

10

1

0.1

0.01
0

10

20

30

40
50
60
70
Number of Blocks

80

90

100

(a) Average Planning Time
40

Average Plan Cost (Schedule Length)

Average Plan Cost (Schedule Length)

9
PbR-300
PbR-100
Initial
IPP

8
7
6
5
4
3
2

(b)

6

8

10
12
14
Number of Goals

16

18

30
25
20
15
10
5

20

5

Average Plan Cost
(Problems Solved by All)

(c)

24

10

15

20
25
30
35
Number of Goals

40

45

50

Average Plan Cost
(Problems Solved by Each)

60

22

PbR-300
PbR-100
Initial
IPP

20

Average Number of Plan Operators

Average Number of Plan Operators

PbR-300
PbR-100
Initial
IPP

0
4

18
16
14
12
10
8
6
4

55
50
45
40
35

PbR-300
PbR-100
Initial
IPP

30
25
20
15
10
5

4

(d)

35

6

8

10
12
14
Number of Goals

16

18

20

5

Number of Plan Operators
(Problems Solved by All)

(e)

10

15

20
25
30
35
Number of Goals

Number of Plan Operators
(Problems Solved by Each)

Figure 22: Experimental Results: Manufacturing Process Planning

241

40

45

50

fiAmbite & Knoblock

This experiment illustrates the flexibility of PbR in specifying complex rules for a planning domain. The results show the benefits of finding a suboptimal initial plan quickly and
then efficiently transforming it to improve its quality.
4.2 Logistics
The task in the logistics domain is to transport several packages from their initial location to
their desired destinations. We used a version of the logistics-strips planning domain of
the AIPS98 planning competition which we restricted to using only trucks but not planes.14
The domain is shown in Figure 23. A package is transported from one location to another
by loading it into a truck, driving the truck to the destination, and unloading the truck. A
truck can load any number of packages. The cost function is the (parallel) time to deliver
all packages (measured as the number of operators in the critical path of a plan).
(define (operator LOAD-TRUCK)
:parameters (?obj ?truck ?loc)
:precondition
(:and (obj ?obj) (truck ?truck) (location ?loc)
(at ?truck ?loc) (at ?obj ?loc))
:effect (:and (:not (at ?obj ?loc))
(in ?obj ?truck)))

(define (operator UNLOAD-TRUCK)
:parameters (?obj ?truck ?loc)
:precondition
(:and (obj ?obj) (truck ?truck) (location ?loc)
(at ?truck ?loc) (in ?obj ?truck))
:effect (:and (:not (in ?obj ?truck))
(at ?obj ?loc)))

(define (operator DRIVE-TRUCK)
:parameters (?truck ?loc-from ?loc-to ?city)
:precondition (:and (truck ?truck) (location ?loc-from) (location ?loc-to) (city ?city)
(at ?truck ?loc-from) (in-city ?loc-from ?city) (in-city ?loc-to ?city))
:effect (:and (:not (at ?truck ?loc-from)) (at ?truck ?loc-to)))

Figure 23: Operators for Logistics
We compare three planners on this domain:
IPP:

IPP (Koehler et al., 1997) produces optimal plans in this domain.

Initial: The initial plan generator picks a distinguished location and delivers packages
one by one starting and returning to the distinguished location. For example, assume that
truck t1 is at the distinguished location l1, and package p1 must be delivered from location
l2 to location l3. The plan would be: drive-truck(t1 l1 l2 c), load-truck(p1 t1 l2),
drive-truck(t1 l2 l3 c), unload-truck(p1 t1 l3), drive-truck(t1 l3 l1 c). The
initial plan generator would keep producing these circular trips for the remaining packages.
Although this algorithm is very efficient it produces plans of very low quality.
PbR: PbR starts from the plan produced by Initial and uses the plan rewriting rules shown
in Figure 24 to optimize plan quality. The loop rule states that driving to a location and
returning back immediately after is useless. The fact that the operators must be adjacent
is important because it implies that no intervening load or unload was performed. In
the same vein, the triangle rule states that it is better to drive directly between two
locations than through a third point if no other operation is performed at such point. The
14. In the logistics domain of AIPS98, the problems of moving packages by plane among different cities and
by truck among different locations in a city are isomorphic, so we focused on only one of them to better
analyze how the rewriting rules can be learned (Ambite, Knoblock, & Minton, 2000).

242

fiPlanning by Rewriting

load-earlier rule captures the situation in which a package is not loaded in the truck the
first time that the packages location is visited. This occurs when the initial planner was
concerned with a trip for another package. The unload-later rule captures the dual case.
PbR applies a first improvement search strategy with only one run (no restarts).
(define-rule :name loop
:if (:operators
((?n1 (drive-truck ?t ?l1 ?l2 ?c))
(?n2 (drive-truck ?t ?l2 ?l1 ?c)))
:links ((?n1 ?n2))
:constraints
((adjacent-in-critical-path ?n1 ?n2)))
:replace (:operators (?n1 ?n2))
:with NIL)

(define-rule :name triangle
:if (:operators
((?n1 (drive-truck ?t ?l1 ?l2 ?c))
(?n2 (drive-truck ?t ?l2 ?l3 ?c)))
:links ((?n1 ?n2))
:constraints
((adjacent-in-critical-path ?n1 ?n2)))
:replace (:operators (?n1 ?n2))
:with (:operators
((?n3 (drive-truck ?t ?l1 ?l3 ?c)))))

(define-rule :name unload-later
(define-rule :name load-earlier
:if (:operators
:if (:operators
((?n1 (drive-truck ?t ?l1 ?l2 ?c))
((?n1 (drive-truck ?t ?l1 ?l2 ?c))
(?n2 (unload-truck ?p ?t ?l2))
(?n2 (drive-truck ?t ?l3 ?l2 ?c))
(?n3 (drive-truck ?t ?l3 ?l2 ?c)))
(?n3 (load-truck ?p ?t ?l2)))
:links ((?n1 ?n2))
:links ((?n2 ?n3))
:constraints
:constraints
((adjacent-in-critical-path ?n1 ?n2)
((adjacent-in-critical-path ?n2 ?n3)
(before ?n2 ?n3)))
(before ?n1 ?n2)))
:replace (:operators (?n2))
:replace (:operators (?n3))
:with (:operators ((?n4 (unload-truck ?p ?t ?l2)))
:with (:operators ((?n4 (load-truck ?p ?t ?l2)))
:links ((?n3 ?n4))))
:links ((?n1 ?n4))))

Figure 24: Logistics Rewriting Rules

250

PbR
Initial
IPP

1000

PbR
Initial
IPP

200

Average Plan Cost

Average Planning Time (CPU Seconds)

10000

100
10
1

150

100

50

0.1
0.01

0
0

5

10

15 20 25 30 35
Number of Packages

40

45

50

0

(a) Average Planning Time

5

10

15

20
25
30
35
Number of Packages

40

45

50

(b) Average Plan Cost

Figure 25: Experimental Results: Logistics, Scaling the Number of Packages
We compared the performance of IPP, Initial, and PbR on a set of logistics problems
involving up to 50 packages. Each problem instance has the same number of packages,
locations, and goals. There was a single truck and a single city. The performance results
are shown in Figure 25. In these graphs each data point is the average of 20 problems for
each given number of packages. All the problems were satisfiable. IPP could only solve
243

fiAmbite & Knoblock

problems up to 7 packages (it also solved 10 out of 20 for 8 packages, and 1 out of 20
for 9 packages, but these are not shown in the figure). Figure 25(a) shows the average
planning time. Figure 25(b) shows the average cost for the 50 packages range. The results
are similar to the previous experiment. Initial is efficient but highly suboptimal. PbR is
able to considerably improve the cost of these plans and approach the optimal.
4.3 Blocks World
We implemented a classical Blocks World domain with the two operators in Figure 2. This
domain has two actions: stack that puts one block on top of another, and, unstack that
places a block on the table to start a new tower. Plan quality in this domain is simply
the number of steps. Optimal planning in this domain is NP-hard (Gupta & Nau, 1992).
However, it is trivial to generate a correct, but suboptimal, plan in linear time using the
naive algorithm: put all blocks on the table and build the desired towers from the bottom
up. We compare three planners on this domain:
IPP: In this experiment we used the GAM goal ordering heuristic (Koehler, 1998; Koehler
& Hoffmann, 2000) that had been tested in Blocks World problems with good scaling results.
Initial: This planner is a programmatic implementation of the naive algorithm using the
facilities introduced in Section 3.4.2.
PbR: This configuration of PbR starts from the plan produced by Initial and uses the
two plan-rewriting rules shown in Figure 6 to optimize plan quality. PbR applies a first
improvement strategy with only one run (no restarts).
We generated random Blocks World problems scaling the number of blocks. The problem
set consists of 25 random problems at 3, 6, 9, 12, 15, 20, 30, 40, 50, 60, 70, 80, 90, and 100
blocks for a total of 350 problems. The problems may have multiple towers in the initial
state and in the goal state.
Figure 26(a) shows the average planning time of the 25 problems for each block quantity.
IPP cannot solve problems with more than 20 blocks within the time limit of 1000 CPU
seconds. The problem solving behavior of IPP was interesting. IPP either solved a given
problem very fast or it timed out. For example, it was able to solve 11 out of the 25 20block problems under 100 seconds, but it timed out at 1000 seconds for the remaining 14
problems. This seems to be the typical behavior of complete search algorithms (Gomes,
Selman, & Kautz, 1998). The local search of PbR allows it to scale much better and solve
all the problems.
Figure 26(b) shows the average plan cost as the number of blocks increases. PbR
improves considerably the quality of the initial plans. The optimal quality is only known
for very small problems, where PbR approximates it, but does not achieve it (we ran Sage for
problems of less than 9 blocks). For larger plans we do not know the optimal cost. However,
Slaney & Thiebaux (1996) performed an extensive experimental analysis of Blocks World
planning using a domain like ours. In their comparison among different approximation
algorithms they found that our initial plan generator (unstack-stack) achieves empirically a
quality around 1.22 the optimal for the range of problem sizes we have analyzed (Figure 7 in
Slaney & Thiebaux, 1996). The value of our average initial plans divided by 1.22 suggests
244

fiPlanning by Rewriting

the quality of the optimal plans. The quality achieved by PbR is comparable with that value.
In fact it is slightly better which may be due to the relatively small number of problems
tested (25 per block size) or to skew in our random problem generator. Interestingly the
plans found by IPP are actually of low quality. This is due to the fact that IPP produces
shortest parallel plans. That means that the plans can be constructed in the fewest time
steps, but IPP may introduce more actions in each time step than are required.
In summary, the experiments in this and the previous sections show that across a variety
of domains PbR scales to large problems while still producing high-quality plans.
180

Average Plan Cost (Number of Operators)

Average Planning Time (CPU Seconds)

1000
PbR-FI
Initial
IPP

100

10

1

0.1

0.01
0

10

20

30

40
50
60
70
Number of Blocks

80

90

PbR-FI
Initial
IPP
Initial/1.22

160
140
120
100
80
60
40
20
0

100

0

(a) Average Planning Time

10

20

30

40 50 60 70
Number of Blocks

80

90

100

(b) Average Plan Cost

Figure 26: Experimental Results: Blocks World, Scaling the Number of Blocks

4.4 Query Planning
Query Planning is a problem of considerable practical importance. It is central to traditional
database and mediator systems. In this section we present some results in distributed query
planning to highlight the use of PbR in a domain with a complex cost function. A detailed
description of query planning, including a novel query processing algorithm for mediators
based on PbR, and a more extensive experimental analysis appear in (Ambite & Knoblock,
2000; Ambite, 1998).
Query planning involves generating a plan that efficiently computes a user query from
the relevant information sources. This plan is composed of data retrieval actions at distributed information sources and data manipulation operations, such as those of the relational algebra: join, selection, union, etc. The specification of the operators for query
planning and the encoding of information goals that we are using was first introduced by
Knoblock (1996). A sample information goal is shown in Figure 27. This goal asks to send
to the output device of the mediator all the names of airports in Tunisia. Two sample
operators are shown in Figure 28. The retrieve operator executes a query at a remote
information source and transports the data to the mediator, provided that the source is
in operation (source-available) and that the source is capable of processing the query
(source-acceptable-query). The join operator takes two subqueries, which are available
locally at the mediator, and combines them using some conditions to produce the joined
query.
245

fiAmbite & Knoblock

(available sims (retrieve (?ap_name)
(:and (airport ?aport)
(country-name ?aport "Tunisia")
(port-name ?aport ?ap_name))))

Figure 27: Sample Information Goal
(define (operator retrieve)
:parameters (?source ?query)
:resources ((processor ?source))
:precondition (:and (source-available ?source)
(source-acceptable-query ?query ?source))
:effect (available sims ?query))
(define (operator join)
:parameters (?join-conds ?query ?query-a ?query-b)
:precondition (:and (available sims ?query-a
(available sims ?query-b)
(join-query ?query ?join-conds ?query-a ?query-b))
:effect (available sims ?query))

Figure 28: Some Query Planning Operators
The quality of a distributed query plan is an estimation of its execution cost, which
is a function of the size of intermediate results, the cost of performing data manipulation
operations, and the transmission through the network of the intermediate results from the
remote sources to the mediator. Our system estimates the plan cost based on statistics
obtained from the source relations, such as the number of tuples in a relation, the number
of distinct values for each attribute, and the maximum and minimum values for numeric
attributes (Silberschatz, Korth, & Sudarshan, 1997, chapter 12). The sources accessed, and
the type and ordering of the data processing operations are critical to the plan cost.
The rewriting rules are derived from properties of the distributed environment and the
relational algebra.15 The first set of rules rely on the fact that, in a distributed environment,
it is generally more efficient to execute a group of operations together at a remote information source than to transmit the data over the network and execute the operations at the
local system. As an example consider the Remote-Join-Eval rule in Figure 29 (shown here
in the PbR syntax, it was shown algebraically in Figure 1). This rule specifies that if in
a plan there exist two retrieval operations at the same remote database whose results are
consequently joined and the remote source is capable of performing joins, the system can
rewrite the plan into one that contains a single retrieve operation that pushes the join to
the remote database.
The second class of rules are derived from the commutative, associative, and distributive
properties of the operators of the relational algebra. For example, the Join-Swap rule of
Figure 29 (cf. Figure 1) specifies that two consecutive joins operators can be reordered
and allows the planner to explore the space of join trees. Since in our query planning
15. In mediators, rules that address the resolution of the semantic heterogeneity are also necessary. See
(Ambite & Knoblock, 2000; Ambite, 1998) for details.

246

fiPlanning by Rewriting

(define-rule :name remote-join-eval
(define-rule :name join-swap
:if (:operators
:if (:operators
((?n1 (retrieve ?query1 ?source))
((?n1 (join ?q1 ?jc1 ?sq1a ?sq1b))
(?n2 (retrieve ?query2 ?source))
(?n2 (join ?q2 ?jc2 ?sq2a ?sq2b)))
(?n3 (join ?query ?jc ?query1 ?query2)))
:links (?n2 ?n1)
:constraints
:constraints
((capability ?source join)))
(join-swappable
:replace (:operators (?n1 ?n2 ?n3))
?q1 ?jc1 ?sq1a ?sq1b
;; in
:with (:operators
?q2 ?jc2 ?sq2a ?sq2b
;; in
((?n4 (retrieve ?query ?source))))
?q3 ?jc3 ?sq3a ?sq3b
;; out
?q4 ?jc4 ?sq4a ?sq4b))
;; out
:replace (:operators (?n1 ?n2))
:with (:operators
((?n3 (join ?q3 ?jc3 ?sq3a ?sq3b))
(?n4 (join ?q4 ?jc4 ?sq4a ?sq4b)))
:links (?n4 ?n3)))

Figure 29: Some Query Planning Rewriting Rules
domain queries are expressed as complex terms (Knoblock, 1996), the PbR rules use the
interpreted predicates in the :constraints field to manipulate such query expressions. For
example, the join-swappable predicate checks if the queries in the two join operators can
be exchanged and computes the new subqueries.
Figure 30 shows an example of the local search through the space of query plan rewritings in a simple distributed domain that describes a company. The figure shows alternative
query evaluation plans for a conjunctive query that asks for the names of employees, their
salaries, and the projects they are working on. The three relations requested in the query
(Employees, Payroll, and Project) are distributed among two databases (one at the companys headquarters  HQ-db  and another at a branch  Branch-db). Assume that the
leftmost plan is the initial plan. This plan first retrieves the Employee relation at the HQ-db
and the Project relation at the Branch-db, and then it joins these two tables on the employee name. Finally, the plan retrieves the Payroll relation from the HQ-db and joins it
on ssn with the result of the previous join. Although a valid plan, this initial plan is suboptimal. Applying the join-swap rule to this initial plan generates two rewritings. One of
them involves a cross-product, which is a very expensive operation, so the system, following a gradient descent search strategy, prefers the other plan. Now the system applies the
remote-join-eval rule and generates a new rewritten plan that evaluates the join between
the employee and project tables remotely at the headquarters database. This final plan is
of much better quality.
We compare the planning efficiency and plan quality of four query planners:
Sage: This is the original query planner (Knoblock, 1995, 1996) for the SIMS mediator,
which performs a best-first search with a heuristic commonly used in query optimization
that explores only the space of left join trees. Sage is a refinement planner (Kambhampati,
Knoblock, & Yang, 1995) that generates optimal left-tree query plans.
DP: This is our implementation of a dynamic-programming bottom-up enumeration of
query plans (Ono & Lohman, 1990) to find the optimal plan. Since in our distributed
domain subqueries can execute in parallel and the cost function reflects such preference,
247

fiAmbite & Knoblock

a(name sal proj) :- Emp(name ssn) ^ Payroll(ssn sal) ^ Projects(name proj)

HQ-db
Emp(name ssn)
Payroll(ssn sal)

name ssn

Branch-db
Project(name proj)

Ret Emp
@ HQ-db
Ret Payroll Ret Project
@ HQ-db @ Branch-db

ssn

name

Ret Payroll
@ HQ-db

Join
Swap

name

Remote
Join
Eval
name

Ret Emp Ret Project
@ HQ-db @ Branch-db

ssn

Ret Project
@ Branch-db

Ret Emp Ret Payroll
@ HQ-db @ HQ-db

Ret Project
@Branch-db
Ret (Emp
@HQ-db

Payroll)

Figure 30: Rewriting in Query Planning
our DP algorithm considers bushy join trees. However, to improve its planning time, DP
applies the heuristic of avoiding cross-products during join enumeration. Thus, in some rare
cases DP may not produce the optimal plan.
Initial: This is the initial plan generator for PbR. It generates query plans according to a
random depth-first search parse of the query. The only non-random choice is that it places
selections as soon as they can be executed. It is the fastest planner but may produce very
low quality plans.
PbR: We used the Remote-Join-Eval and Join-Swap rules defined in Figure 29. These
two rules are sufficient to optimize the queries in the test set. We tested two gradientdescent search strategies for PbR: first improvement with four random restarts (PbR-FI),
and steepest descent with three random restarts (PbR-SD).
In this experiment we compare the behavior of Sage, DP, Initial, PbR-FI, and PbR-SD
in a distributed query planning domain as the size of the queries increases. We generated a
synthetic domain for the SIMS mediator and defined a set of conjunctive queries involving
from 1 to 30 relations. The queries have one selection on an attribute of each table. Each
information source contains two relations and can perform remote operations. Therefore,
the optimal plans involve pushing operations to be evaluated remotely at the sources.
The results of this experiment are shown in Figure 31. Figure 31(a) shows the planning
time, in a logarithmic scale, for Sage, DP, Initial, PbR-FI, and PbR-SD as the query size
grows. The times for PbR include both the generation of all the random initial plans and
their rewriting. The times for Initial are the average of the initial plan construction across
all restarts of each query. Sage is able to solve queries involving up to 6 relations, but larger
248

fiPlanning by Rewriting

queries cannot be solved within its search limit of 200,000 partial-plan nodes. DP scales
better than Sage, but cannot solve queries of more than 9 relations in the 1000 second time
limit. Both configurations of PbR scale better than Sage and DP. The first-improvement
search strategy of PbR-FI is faster than the steepest descent of PbR-SD.
Figure 31(b) shows the cost of the query plans for the five planners. The cost for Initial
is the average of the initial plans across all the restarts of each query. The plan cost is an
estimate of the query execution cost. A logarithmic scale is used because of the increasingly
larger absolute values of the plan costs for our conjunctive chain queries and the very high
cost of the initial plans. PbR rewrites the very poor quality plans generated by Initial
into high-quality plans. Both PbR and DP produce better plans than Sage (in the range
tractable for Sage) for this experiment. This happens because they are searching the larger
space of bushy query trees and can take greater advantage of parallel execution plans. PbR
produces plans of quality comparable to DP for its tractable range and beyond that range
PbR scales gracefully. The two configurations of PbR produce plans of similar cost, though
PbR-FI needed less planning time than PbR-SD. PbR-SD generates all the plans in the local
neighborhood in order to select the cheapest one, but PbR-FI only generates a portion of
the neighborhood since it chooses the first plan of a cheaper cost, so PbR-FI is faster in
average. Figure 31 shows empirically that in this domain the locally optimal moves of
steepest descent do not translate in final solutions of a better cost than those produced by
the first-improvement strategy.

1000

1e+18
Sage
DP
Initial
PbR-FI
PbR-SD

100

1e+14
1e+12

10

Plan Cost

Planning Time (CPU seconds)

1e+16

1
Sage
DP
Initial
PbR-FI
PbR-SD

0.1

1e+10
1e+08
1e+06
10000
100

0.01

1
0

5

10

15
Query Size

20

25

30

0

(a) Planning Time

5

10

15
Query Size

20

25

30

(b) Plan Quality

Figure 31: Experimental Results: Distributed Query Planning

5. Related Work
In this section we review previous work related to the Planning by Rewriting framework.
First, we discuss work on the disciplines upon which PbR builds, namely, classical AI
planning, local search, and graph rewriting. Then, we discuss work related to our planrewriting algorithm.
249

fiAmbite & Knoblock

5.1 AI Planning
PbR is designed to find a balance among the requirements of planning efficiency, high quality
plans, flexibility, and extensibility. A great amount of work on AI Planning has focused on
improving its average-case efficiency given that the general cases are computationally hard
(Erol et al., 1995). One possibility is to incorporate domain knowledge in the form of search
control. A recent example is TLPlan (Bacchus & Kabanza, 1995, 2000), a forward-search
planner that has shown a remarkable scalability using control knowledge expressed in temporal logic. Some systems automatically learn search control for a given planning domain or
even specific problem instances. Minton (1988b) shows how to deduce search control rules
for a problem solver by applying explanation-based learning to problem-solving traces. He
also discusses the impact of the utility problem. The utility problem, simply stated, says
that the (computational) benefits of using the additional knowledge must outweigh the cost
of applying it. PbR plan-rewriting rules also are subject to the utility problem. The quality
improvement obtained by adding more rewriting rules to a PbR-based planner may not be
worth the performance degradation. Another approach to automatically generating search
control is by analyzing statically the operators (Etzioni, 1993) or inferring invariants in the
planning domain (Gerevini & Schubert, 1998; Fox & Long, 1998; Rintanen, 2000). Abstraction provides yet another form of search control. Knoblock (1994a) presents a system that
automatically learns abstraction hierarchies from a planning domain or a particular problem
instance in order to speed up planning. plan-rewriting rules can be learned with techniques
analogous to those used to learn search control. Ambite, Knoblock, & Minton (2000) present
an approach to automatically learn the plan-rewriting rules based on comparing initial and
optimal plans for example problems. Alternatively, analyzing the planning operators and
which combinations of operators are equivalent with respect to the achievement of some
goals can also lead to the automatic generation of the rewriting rules.
Local search algorithms have also been used to improve planning efficiency although
in a somewhat indirect way. Planning can be reduced to solving a series of propositional
satisfiability problems (Kautz & Selman, 1992). Thus, Kautz & Selman (1996) used an
efficient satisfiability testing algorithm based on local search to solve the SAT encodings
of a planning problem. Their approach proved more efficient than specialized planning
algorithms. We believe that the power of their approach stems from the use of local search.
PbR directly applies local search on the plan structures, as opposed to translating it first
to a larger propositional representation.
Although all these approaches do improve the efficiency of planning, they do not specifically address plan quality, or else they consider only very simple cost metrics (such as the
number of steps). Some systems learn search control that addresses both planning efficiency
and plan quality (Estlin & Mooney, 1997; Borrajo & Veloso, 1997; Perez, 1996). However,
from the reported experimental results, PbR appears to be more scalable. Moreover, PbR
provides an anytime algorithm while other approaches must run to completion.
5.2 Local Search
Local search has a long tradition in combinatorial optimization (Aarts & Lenstra, 1997;
Papadimitriou & Steiglitz, 1982). Local improvement ideas have found application in many
250

fiPlanning by Rewriting

domains. Some of the general work most relevant to PbR is on constraint satisfaction,
scheduling, satisfiability testing, and heuristic search.
In constraint satisfaction, local search techniques have been able to solve problems
orders of magnitude more complex than the respective complete (backtracking) approaches.
Minton et al. (Minton, Johnston, Philips, & Laird, 1990; Minton, 1992) developed a simple
repair heuristic, min-conflicts, that could solve large constraint satisfaction and scheduling
problems, such as the scheduling of operations in the Hubble Space Telescope. The minconflicts heuristic just selects the variable value assignment that minimizes the number of
constraints violated. This heuristic was used as the cost function of a gradient-descent
search and also in an informed backtracking search.
In satisfiability testing a similar method, GSAT, was introduced by Selman, Levesque,
& Mitchell (1992). GSAT solves hard satisfiability problems using local search where the
repairs consist in changing the truth value of a randomly chosen variable. The cost function
is the number of clauses satisfied by the current truth assignment. Their approach scales
much better than the corresponding complete method (the Davis-Putnam procedure).
In work on scheduling and rescheduling, Zweben, Daun, & Deale (1994) define a set
of general, but fixed, repair methods, and use simulated annealing to search the space of
schedules. Our plans are networks of actions as opposed to their metric-time totally-ordered
tasks. Also we can easily specify different rewriting rules (general or specific) to suit each
domain, as opposed to their fixed strategies.
Our work is inspired by these approaches but there are several differences. First, PbR
operates on complex graph structures (partial-order plans) as opposed to variable assignments. Second, our repairs are declaratively specified and may be changed for each problem
domain, as opposed to their general but fixed repair strategies. Third, PbR accepts arbitrary measures of quality, not just constraint violations as in min-conflicts, or number of
unsatisfied clauses as GSAT. Finally, PbR searches the space of valid solution plans, as
opposed to the space of variable assignments which may be internally inconsistent.
Iterative repair ideas have also been used in heuristic search. Ratner & Pohl (1986)
present a two-phase approach similar to PbR. In the first phase, they find an initial valid
sequence of operators using an approximation algorithm. In the second phase, they perform
local search starting from that initial sequence. The cost function is the plan length. The
local neighborhood is generated by identifying segments in the current solution sequence
and attempting to optimize them. The repair consists of a heuristic search with the initial
state being the beginning of the segment and the goal the end of the segment. If a shorter
path is found, the original sequence is replaced by the new shorter segment. A significant
difference with PbR is that they are doing a state-space search, while PbR is doing a planspace search. The least-committed partial-order nature of PbR allows it to optimize the
plans in ways that cannot be achieved by optimizing linear subsequences.
5.3 Graph Rewriting
PbR builds on ideas from graph rewriting (Schurr, 1997). The plan-rewriting rules in
PbR are an extension of traditional graph rewriting rules. By taking advantage of the
semantics of planning PbR introduces partially-specified plan-rewriting rules, where the
rules do not need to specify the completely detailed embedding of the consequent as in pure
251

fiAmbite & Knoblock

graph rewriting. Nevertheless, there are several techniques that can transfer from graph
rewriting into Planning by Rewriting, particularly for fully-specified rules. Dorr (1995)
defines an abstract machine for graph isomorphism and studies a set of conditions under
which traditional graph rewriting can be performed efficiently. Perhaps a similar abstract
machine for plan rewriting can be defined. The idea of rule programs also appears in this
field and has been implemented in the PROGRES system (Schurr, 1990, 1997).
5.4 Plan Rewriting
The work most closely related to our plan-rewriting algorithm is plan merging (Foulser, Li, &
Yang, 1992). Foulser et al. provide a formal analysis and algorithms for exploiting positive
interactions within a plan or across a set of plans. However, their work only considers the
case in which a set of operators can be replaced by one operator that provides the same
effects to the rest of the plan and consumes the same or fewer preconditions. Their focus is
on optimal and approximate algorithms for this type of operator merging. Plan rewriting
in PbR can be seen as a generalization of operator merging where a subplan can replace
another subplan. A difference is that PbR is not concerned with finding the optimal merge
(rewritten plan) in a single pass of an optimization algorithm as their approach does. In
PbR we are interested in generating possible plan rewritings during each rewriting phase,
not the optimal one. The optimization occurs as the (local) search progresses.
Case-based planning (e.g., Kambhampati, 1992; Veloso, 1994; Nebel & Koehler, 1995;
Hanks & Weld, 1995; Munoz-Avila, 1998) solves a problem by modifying a previous solution.
There are two phases in case-based planning. The first one identifies a plan from the library
that is most similar to the current problem. In the second phase this previous plan is adapted
to solve the new problem. PbR modifies a solution to the current problem, so there is no
need for a retrieval phase nor the associated similarity metrics. Plan rewriting in PbR can
be seen as a type of adaptation from a solution to a problem to an alternate solution for
the same problem. That is, a plan rewriting rule in PbR identifies a pair of subplans (the
replaced and replacement subplans) that may be interchangeable.
Veloso (1994) describes a general approach to case-based planning based on derivational
analogy. Her approach works in three steps. First, the retrieval phase selects a similar
plan from the library. Second, the parts of this plan irrelevant to the current problem
are removed. Finally, her system searches for a completion of this plan selecting as much
as possible the same decisions as the old plan did. In this sense the planning knowledge
encoded in the previous solution is transferred to the generation of the new solution plan.
The plan-rewriting algorithm for partially-specified rules of PbR can be seen as a strongly
constrained version of this approach. In PbR the subplan in the rule consequent fixes the
steps that can be added to repair the plan. We could use her technique of respecting
previous choice points when completing the plan as a way of ensuring that most of the
structure of the plan before and after the repair is maintained. This could be useful to
constrain the number of rewritten plans for large rewriting rules.
Nebel and Koehler (1995) present a computational analysis of case-based planning. In
this context they show that the worst-case complexity of plan modification is no better than
plan generation and point to the limitations of reuse methods. The related problem in the
PbR framework is the embedding of the replacement subplan for partially specified rules.
252

fiPlanning by Rewriting

As we explained in Section 3.1.4 there may be pathological cases in which the number of
embeddings is exponential in the size of the plan or deciding if the embedding exists is
NP-hard. However, often we are not interested in finding all rewritings, for example when
following a first improvement search strategy. In our experience the average case behavior
seems to be much better as was presented in Section 4.
Systematic algorithms for case-based planning (such as Hanks & Weld, 1995) invert the
decisions done in refinement planning to find a path between the solution to a similar old
problem and the new problem. The rewriting rules in PbR indicate how to transform a
solution into another solution plan based on domain knowledge, as opposed to the generic
inversion of the refinement operations. Plan rewriting in PbR is done in a very constrained
way instead of an open search up and down the space of partial plans. However, the rules in
PBR may search the space of rewritings non systematically. Such an effect is ameliorated
by using local search.

6. Discussion and Future Work
This paper has presented Planning by Rewriting, a new paradigm for efficient high-quality
domain-independent planning. PbR adapts graph rewriting and local search techniques
to the semantics of domain-independent partial-order planning. The basic idea of PbR
consists in transforming an easy-to-generate, but possibly suboptimal, initial plan into a
high-quality plan by applying declarative plan-rewriting rules in an iterative repair style.
There are several important advantages to the PbR planning approach. First, PbR is
a declarative domain-independent framework, which brings the benefits of reusability and
extensibility. Second, it addresses sophisticated plan quality measures, while most work in
domain-independent planning has not addressed quality or does it in very simple ways.
Third, PbR is scalable because it uses efficient local search methods. Finally, PbR is an
anytime planning algorithm that allows balancing planning effort and plan quality in order
to maximize the utility of the planning process.
Planning by Rewriting provides a domain-independent framework for local search. PbR
accepts declarative domain specifications in an expressive operator language, declarative
plan-rewriting rules to generate the neighborhood of a plan, complex quality metrics, interchangeable initial plan generators, and arbitrary (local) search methods.
Planning by Rewriting is well suited to mixed-initiative planning. In mixed-initiative
planning, the user and the planner interact in defining the plan. For example, the user can
specify which are the available or preferred actions at the moment, change the quality criteria of interest, etc. In fact, some domains can only be approached through mixed-initiative
planning. For example, when the quality metric is very expensive to evaluate, such as in
geometric analysis in manufacturing, the user must guide the planner towards good quality
plans in a way that a small number of plans are generated and evaluated. Another example
is when the plan quality metric is multi-objective or changes over time. Several characteristics of PbR support mixed-initiative planning. First, because PbR offers complete plans,
the user can easily understand the plan and perform complex quality assessment. Second,
the rewriting rule language is a convenient mechanism by which the user can propose modifications to the plans. Third, by selecting which rules to apply or their order of application
the user can guide the planner.
253

fiAmbite & Knoblock

Our framework achieves a balance between domain knowledge, expressed as plan-rewriting
rules, and general local-search techniques that have proved useful in many hard combinatorial problems. We expect that these ideas will push the frontier of solvable problems for
many practical domains in which high quality plans and anytime behavior are needed.
The planning style introduced by PbR opens several areas for future research. There
is great potential for applying machine learning techniques to PbR. An important issue is
the generation of the plan-rewriting rules. Conceptually, plan-rewriting rules arise from the
chosen plan equivalence relation. All valid plans that achieve the given goals in a finite
number of steps, i.e. all solution plans, are (satisfiability) equivalent. Each rule arises from
a theorem that states that two subplans are equivalent for the purposes of achieving some
goals, with the addition of some conditions that indicate in which context that rule can
be usefully applied. The plan-rewriting rules can be generated by automated procedures.
The methods can range from static analysis of the domain operators to analysis of sample
equivalent plans that achieve the same goals but at different costs. Note the similarity
with methods to automatically infer search control and domain invariants (Minton, 1988b;
Etzioni, 1993; Gerevini & Schubert, 1998; Fox & Long, 1998; Rintanen, 2000), and also the
need to deal with the utility problem. Ambite, Knoblock, & Minton (2000) present some
results on learning plan rewriting rules based on comparing initial and optimal plans for
sample problems.
Beyond learning the rewriting rules, we intend to develop a system that can automatically learn the optimal planner configuration for a given planning domain and problem
distribution in a manner analogous to Mintons Multi-TAC system (Minton, 1996). Our
system would perform a search in the configuration space of the PbR planner proposing
candidate sets of rewriting rules and different search methods. By testing each proposed
configuration against a training set of simple problems, the system would hill-climb in the
configuration space in order to arrive at the most useful rewriting rules and search strategies
for the given planning domain and distribution of problems.
There are many advanced techniques in the local search literature that can be adapted
and extended in our framework. In particular, the idea of variable-depth rewriting leads
naturally to the creation of rule programs, which specify how a set of rules are applied to
a plan. We have already seen how in query planning we could find transformations that
are better specified as a program of simple rewriting rules. For example, a sequence of
Join-Swap transformations may put two retrieve operators on the same database together
in the query tree and then Remote-Join-Eval would collapse the explicit join operator and
the two retrieves into a single retrieval of a remote join. Cherniack & Zdonik (1996, 1998)
present more complex examples of this sort of programs of rewriting rules in the context of
a query optimizer for object-oriented databases.
As we discussed in Sections 3.1.3 and 3.1.4 the language of the antecedent of the rewriting rules can be more expressive than conjunctive queries while still remaining computationally efficient. For example, Figure 32 shows a rule from the manufacturing domain of
Section 4.1 with a relationally-complete antecedent. This rule matches a subplan that contains a spray-paint operator, but does not contain either punch or drill-press operators
that create holes of diameter smaller than 1 millimeter. In such case, the rule replaces the
spray-paint operator by an immersion-paint operator. This rule would be useful in a
situation in which painting by immersion could clog small holes.
254

fiPlanning by Rewriting

(define-rule :name SP-by-IP-no-small-holes
:if (:and (:operator ?n1 (spray-paint ?x ?c ?s))
(:not (:and (:or (:operator ?n2 (punch ?x ?w ?o))
(:operator ?n3 (drill-press ?x ?w ?o)))
(:less ?w 1mm))))
:replace (:operators (?n1))
:with (:operator ?n4 (immersion-paint ?x ?c)))

Figure 32: Rule with a Relationally-Complete Antecedent
Another area for further research is the interplay of plan rewriting and plan execution.
Sometimes the best transformations for a plan may only be known after some portion of
the plan has been executed. This information obtained at run-time can guide the planner
to select the appropriate rewritings. For example, in query planning the plans may contain
information gathering actions (Ashish, Knoblock, & Levy, 1997) and depend on run-time
conditions. This yields a form of dynamic query optimization. Interleaved planning and
execution is also necessary in order to deal effectively with unexpected situations in the
environment such as database or network failures.
An open area of research is to relax our framework to accept incomplete plans during
the rewriting process. This expands the search space considerably and some of the benefits
of PbR, such as its anytime property, are lost. But for some domains the shortest path of
rewritings from the initial plan to the optimal may pass through incomplete or inconsistent
plans. This idea could be embodied as a planning style that combines the characteristics
of generative planning and Planning by Rewriting. This is reminiscent of the plan critics
approach (Sacerdoti, 1975; Sussman, 1975). The resulting plan-rewriting rules can be seen
as declarative specifications for plan critics. The plan refinements of both partial order
planning (Kambhampati et al., 1995) and Hierarchical Task Network Planning (Erol, Nau,
& Hendler, 1994) can be easily specified as plan-rewriting rules.
Applying PbR to other domains will surely provide new challenges and the possibility
of discovering and transferring general planning techniques from one domain to another.
We hope that the local-search methods used by PbR will help planning techniques to scale
to large practical problems and conversely that the domain-independent nature of PbR will
help in the analysis and principled extension of local search techniques.

Acknowledgments
This paper is an extended version of (Ambite & Knoblock, 1997).
The research reported here was supported in part by a Fulbright/Ministerio of Educacion
y Ciencia of Spain scholarship, in part by the Defense Advanced Research Projects Agency
(DARPA) and Air Force Research Laboratory, Air Force Materiel Command, USAF, under
agreement number F30602-00-1-0504, in part by the National Science Foundation under
grant number IRI-9313993, in part by the Rome Laboratory of the Air Force Systems Command and the Defense Advanced Research Projects Agency (DARPA) under contract numbers F30602-94-C-0210, F30602-97-2-0352, F30602-97-2-0238, F30602-98-2-0109, in part by
the United States Air Force under contract number F49620-98-1-0046, and in part by the
Integrated Media Systems Center, a National Science Foundation Engineering Research
255

fiAmbite & Knoblock

Center, Cooperative Agreement No. EEC-9529152. The U.S.Government is authorized to
reproduce and distribute reports for Governmental purposes notwithstanding any copyright
annotation thereon. The views and conclusions contained herein are those of the authors and
should not be interpreted as necessarily representing the official policies or endorsements,
either expressed or implied, of any of the above organizations or any person connected with
them.

References
Aarts, E., & Lenstra, J. K. (1997). Local Search in Combinatorial Optimization. John Wiley
and Sons, Chichester, England.
Abiteboul, S., Hull, R., & Vianu, V. (1995). Foundations of Databases. Addison-Wesley.
Ambite, J. L. (1998). Planning by Rewriting. Ph.D. thesis, University of Southern California.
Ambite, J. L., & Knoblock, C. A. (1997). Planning by rewriting: Efficiently generating
high-quality plans. In Proceedings of the Fourteenth National Conference on Artificial
Intelligence, pp. 706713 Providence, RI.
Ambite, J. L., & Knoblock, C. A. (2000). Flexible and scalable cost-based query planning
in mediators: A transformational approach. Artificial Intelligence, 118 (1-2), 115161.
Ambite, J. L., Knoblock, C. A., & Minton, S. (2000). Learning plan rewriting rules. In
Proceedings of the Fifth International Conference on Artificial Intelligence Planning
and Scheduling Systems Breckenridge, CO.
Ashish, N., Knoblock, C. A., & Levy, A. (1997). Information gathering plans with sensing
actions. In Steel, S., & Alami, R. (Eds.), Recent Advances in AI Planning: 4th
European Conference on Planning, ECP97. Springer-Verlag, New York.
Avenhaus, J., & Madlener, K. (1990). Term rewriting and equational reasoning. In Formal
Techniques in Artificial Intelligence, pp. 143. Elsevier, North Holland.
Baader, F., & Nipkow, T. (1998). Term Rewriting and All That. Cambridge University
Press.
Bacchus, F., & Kabanza, F. (1995). Using temporal logic to control search in a forward
chaining planner. In Proceedings of the 3rd European Workshop on Planning.
Bacchus, F., & Kabanza, F. (2000). Using temporal logics to express search control knowledge for planning. Artificial Intelligence, 116 (12), 123191.
Backstrom, C. (1994a). Executing parallel plans faster by adding actions. In Cohn, A. G.
(Ed.), Proceedings of the Eleventh European Conference on Artificial Intelligence, pp.
615619 Amsterdam, Netherlands. John Wiley and Sons.
Backstrom, C. (1994b). Finding least constrained plans and optimal parallel executions is
harder that we thought. In Backstrom, C., & Sandewell, E. (Eds.), Current Trends in
AI Planning: Proceedings of the 2nd European Workshop on Planning (EWSP-93),
pp. 4659 Vadstena, Sweeden. IOS Press (Amsterdam).
256

fiPlanning by Rewriting

Backstrom, C., & Nebel, B. (1995). Complexity results for SAS+ planning. Computational
Intelligence, 11 (4), 625655.
Blum, A. L., & Furst, M. L. (1995). Fast planning through planning graph analysis. In
Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence
Montreal, Canada.
Blum, A. L., & Furst, M. L. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90 (12), 281300.
Bonet, B., & Geffner, H. (1999). Planning as heuristic search: New results. In Proceedings
of the Fifth European Conference on Planning (ECP-99) Durham, UK.
Bonet, B., Loerincs, G., & Geffner, H. (1997). A robust and fast action selection mechanism for planning. In Proceedings of the Fourteenth National Conference on Artificial
Intelligence, pp. 714719 Providence, RI.
Borrajo, D., & Veloso, M. (1997). Lazy incremental learning of control knowledge for
efficiently obtaining quality plans. AI Review, 11, 371405.
Bylander, T. (1994). The computation complexity of propositional strips. Artificial Intelligence, 69 (1-2), 165204.
Carbonell, J. G., Knoblock, C. A., & Minton, S. (1991). PRODIGY: An integrated architecture for planning and learning. In VanLehn, K. (Ed.), Architectures for Intelligence,
pp. 241278. Lawrence Erlbaum, Hillsdale, NJ.
Cherniack, M., & Zdonik, S. B. (1996). Rule languages and internal algebras for rule-based
optimizers. SIGMOD Record (ACM Special Interest Group on Management of Data),
25 (2), 401412.
Cherniack, M., & Zdonik, S. B. (1998). Changing the rules: Transformations for rulebased optimizers. In Proceedings of the ACM SIGMOD International Conference on
Management of Data, pp. 6172 Seattle, WA.
Dean, T., & Boddy, M. (1988). An analysis of time-dependent planning. In Proceedings of
the Seventh National Conference on Artificial Intelligence, pp. 4954 Saint Paul, MN.
Dorr, H. (1995). Efficient graph rewriting and its implementation, Vol. 922 of Lecture Notes
in Computer Science. Springer-Verlag Inc., New York, NY, USA.
Erol, K., Nau, D., & Hendler, J. (1994). UMCP: A sound and complete planning procedure
for hierarchical task-network planning. In Proceedings of the Second International
Conference on Artificial Intelligence Planning Systems, pp. 249254 Chicago, IL.
Erol, K., Nau, D., & Subrahmanian, V. S. (1995). Decidability and undecidability results
for domain-independent planning. Artificial Intelligence, 76 (1-2), 7588.
Estlin, T. A., & Mooney, R. J. (1997). Learning to improve both efficiency and quality of
planning. In Proceedings of the Fifteenth International Joint Conference on Artificial
Intelligence, pp. 12271233 Nagoya, Japan.
257

fiAmbite & Knoblock

Etzioni, O. (1993). Acquiring search-control knowledge via static analysis. Artificial Intelligence, 62 (2), 255302.
Etzioni, O., & Weld, D. S. (1994). A softbot-based interface to the Internet. Communications of the ACM, 37 (7).
Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: A new approach to the application of
theorem proving to problem solving. Artificial Intelligence, 2 (3/4), 189208.
Forgy, C. L. (1982). Rete: A fast algorithm for the many pattern/many object pattern
match problem. Artificial Intelligence, 19, 1737.
Foulser, D. E., Li, M., & Yang, Q. (1992). Theory and algorithms for plan merging. Artificial
Intelligence, 57 (23), 143182.
Fox, M., & Long, D. (1998). The automatic inference of state invariants in TIM. Journal
of Artificicial Intelligence Research, 9, 367421.
Gerevini, A., & Schubert, L. (1998). Inferring state constraints for domain-independent
planning. In Proceedings of the Fifteenth National Conference on Artificial Intelligence, pp. 905912 Madison, WI.
Glover, F. (1989). Tabu searchPart I. ORSA Journal on Computing, 1 (3), 190206.
Gomes, C. P., Selman, B., & Kautz, H. (1998). Boosting combinatorial search through
randomization. In Proceedings of the Fifteenth National Conference on Artificial Intelligence Madison, WI.
Gupta, N., & Nau, D. S. (1992). On the complexity of blocks-world planning. Artificial
Intelligence, 56 (23), 223254.
Hanks, S., & Weld, D. S. (1995). A domain-independent algorithm for plan adaptation.
Journal of Artificicial Intelligence Research, 2, 319360.
Johnson, D. S. (1990). Local optimization and the traveling salesman problem. In Paterson,
M. S. (Ed.), Automata, Languages and Programming: Proc. of the 17th International
Colloquium, pp. 446461. Springer, New York.
Kambhampati, S. (1992). A validation-structure-based theory of plan modification and
reuse. Artificial Intelligence, 55 (2-3), 193258.
Kambhampati, S., Knoblock, C. A., & Yang, Q. (1995). Planning as refinement search:
A unified framework for evaluating the design tradeoffs in partial order planning.
Artificial Intelligence, 76 (1-2), 167238.
Kautz, H., & Selman, B. (1992). Planning as satisfiability. In Neumann, B. (Ed.), Proceedings of the 10th European Conference on Artificial Intelligence, pp. 359363 Vienna,
Austria. John Wiley & Sons.
258

fiPlanning by Rewriting

Kautz, H., & Selman, B. (1996). Pushing the envelope: Planning, propositional logic, and
stochastic search. In Proceedings of the Thirteenth National Conference on Artificial
Intelligence, pp. 11941201 Portland, OR.
Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization by simulated annealing.
Science, 220, 671680.
Knoblock, C. A. (1994a). Automatically generating abstractions for planning. Artificial
Intelligence, 68 (2), 243302.
Knoblock, C. A. (1994b). Generating parallel execution plans with a partial-order planner. In Proceedings of the Second International Conference on Artificial Intelligence
Planning Systems Chicago, IL.
Knoblock, C. A. (1995). Planning, executing, sensing, and replanning for information gathering. In Proceedings of the Fourteenth International Joint Conference on Artificial
Intelligence Montreal, Canada.
Knoblock, C. A. (1996). Building a planner for information gathering: A report from the
trenches. In Proceedings of the Third International Conference on Artificial Intelligence Planning Systems Edinburgh, Scotland.
Koehler, J. (1998). Solving complex planning tasks through extraction of subproblems. In
Simmons, R., Veloso, M., & Smith, S. (Eds.), Proceedings of the Fourth International
Conference on Artificial Intelligence Planning Systems, pp. 6269 Pittsburgh, PA.
Koehler, J., & Hoffmann, J. (2000). On reasonable and forced goal orderings and their use
in an agenda-driven planning algorithm. Journal of Artificial Intelligence Research,
12, 338386.
Koehler, J., Nebel, B., Hoffman, J., & Dimopoulos, Y. (1997). Extending planning graphs to
an ADL subset. In Steel, S., & Alami, R. (Eds.), Proceedings of the Fourth European
Conference on Planning (ECP-97): Recent Advances in AI Planning, Vol. 1348 of
LNAI, pp. 273285 Berlin. Springer.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proceedings of
the Ninth National Conference on Artificial Intelligence Anaheim, CA.
Minton, S. (1988a). Learning Effective Search Control Knowledge: An Explanation-Based
Approach. Ph.D. thesis, Computer Science Department, Carnegie Mellon University.
Minton, S. (1988b). Learning Search Control Knowledge: An Explanation-Based Approach.
Kluwer, Boston, MA.
Minton, S. (1992). Minimizing conflicts: A heuristic repair method for constraintsatisfaction and scheduling problems. Artificial Intelligence, 58 (1-3), 161205.
Minton, S. (1996). Automatically configuring constraint satisfaction programs: A case
study. Constraints, 1 (1), 743.
259

fiAmbite & Knoblock

Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1990). Solving large-scale constraint satisfaction and scheduling problems using a heuristic repair method. In Proceedings of the Eighth National Conference on Artificial Intelligence, pp. 1724 Boston,
MA.
Munoz-Avila, H. (1998). Integrating Twofold Case Retrieval and Complete Decision Replay
in CAPlan/CbC. Ph.D. thesis, University of Kaiserslautern.
Nau, D. S., Gupta, S. K., & Regli, W. C. (1995). AI planning versus manufacturingoperation planning: A case study. In Proceedings of the Fourteenth International
Joint Conference on Artificial Intelligence Montreal, Canada.
Nebel, B., & Koehler, J. (1995). Plan reuse versus plan generation: A theoretical and
empirical analysis. Artificial Intelligence, 76 ((1-2)), 427454.
Ono, K., & Lohman, G. M. (1990). Measuring the complexity of join enumeration in query
optimization. In McLeod, D., Sacks-Davis, R., & Schek, H.-J. (Eds.), 16th International Conference on Very Large Data Bases, pp. 314325 Brisbane, Queensland,
Australia. Morgan Kaufmann.
Papadimitriou, C. H., & Steiglitz, K. (1977). On the complexity of local search for the
traveling salesman problem. SIAM, 6 (1), 7683.
Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial Optimization: Algorithms and
Complexity. Prentice Hall, Englewood Cliffs, NJ.
Penberthy, J. S., & Weld, D. S. (1992). UCPOP: A sound, complete, partial order planner for
ADL. In Third International Conference on Principles of Knowledge Representation
and Reasoning, pp. 189197 Cambridge, MA.
Perez, M. A. (1996). Representing and learning quality-improving search control knowledge.
In Proceedings of the Thirteenth International Conference on Machine Learning Bari,
Italy.
Ratner, D., & Pohl, I. (1986). Joint and LPA*: Combination of approximation and search.
In Proceedings of the Fifth National Conference on Artificial Intelligence Philadelphia,
PA.
Rintanen, J. (2000). An iterative algorithm for synthesizing invariants. In Proceedings of
the Seventeenth National Conference on Artificial Intelligence Austin, TX.
Russell, S., & Norvig, P. (1995). Artificial Intelligence: A Modern Approach. Prentice Hall.
Sacerdoti, E. D. (1975). The nonlinear nature of plans. In Proceedings of the Fourth
International Joint Conference on Artificial Intelligence, pp. 206214 Tbilisi, Georgia,
USSR.
Savage, S., Weiner, P., & Bagchi, A. (1976). Neighborhood search algorithms for guaranteeing optimal traveling salesman tours must be inefficient. Journal of Computer and
System Sciences, 12 (1), 2535.
260

fiPlanning by Rewriting

Schurr, A. (1990). Introduction to PROGRES, an attribute graph grammar based specification language. In Nagl, M. (Ed.), Graph-Theoretic Concepts in Computer Science,
Vol. 411 of Lecture Notes in Computer Science, pp. 151165.
Schurr, A. (1997). Programmed graph replacement systems. In Rozenberg, G. (Ed.),
Handbook on Graph Grammars: Foundations, Vol. 1, pp. 479546. World Scientific,
Singapore.
Sellis, T. K. (1988). Multiple-query optimization. ACM Transactions on Database Systems,
13 (1), 2352.
Selman, B., Levesque, H., & Mitchell, D. (1992). A new method for solving hard satisfiability
problems. In Proceedings of the Tenth National Conference on Artificial Intelligence
(AAAI-92), pp. 440446 San Jose, California. AAAI Press.
Silberschatz, A., Korth, H. F., & Sudarshan, S. (1997). Database System Concepts (Third
edition). McGraw-Hill.
Simon, H. (1969). The sciences of the artificial. MIT Press.
Slaney, J., & Thiebaux, S. (1996). Linear time near-optimal planning in the blocks world. In
Proceedings of the Thirteenth National Conference on Artificial Intelligence and the
Eighth Innovative Applications of Artificial Intelligence Conference, pp. 12081214
Menlo Park. AAAI Press / MIT Press.
Sussman, G. J. (1975). A Computer Model of Skill Acquisition. American Elsevier, New
York.
Veloso, M. (1994). Planning and Learning by Analogical Reasoning. Springer Verlag.
Veloso, M. M., Perez, M. A., & Carbonell, J. G. (1990). Nonlinear planning with parallel
resource allocation. In Proceedings of the Workshop on Innovative Approaches to
Planning, Scheduling and Control, pp. 207212 San Diego, CA.
Weld, D. S. (1994). An introduction to least commitment planning. AI Magazine, 15 (4).
Weld, D. S. (1999). Recent advances in AI planning. AI Magazine, 20 (2).
Yu, C., & Chang, C. (1984). Distributed query processing. ACM Computing Surveys, 16 (4),
399433.
Zweben, M., Daun, B., & Deale, M. (1994). Scheduling and rescheduling with iterative
repair. In Intelligent Scheduling, pp. 241255. Morgan Kaufman, San Mateo, CA.

261

fiJournal of Artificial Intelligence Research 15 (2001) 391-454

Submitted 6/18; published 12/01

Parameter Learning of Logic Programs for
Symbolic-statistical Modeling

Taisuke Sato
Yoshitaka Kameya

sato@mi.cs.titech.ac.jp
kame@mi.cs.titech.ac.jp

Dept. of Computer Science, Graduate School of Information
Science and Engineering, Tokyo Institute of Technology
2-12-1 Ookayama Meguro-ku Tokyo Japan 152-8552

Abstract

We propose a logical/mathematical framework for statistical parameter learning of parameterized logic programs, i.e. definite clause programs containing probabilistic facts with
a parameterized distribution. It extends the traditional least Herbrand model semantics in
logic programming to distribution semantics , possible world semantics with a probability
distribution which is unconditionally applicable to arbitrary logic programs including ones
for HMMs, PCFGs and Bayesian networks.
We also propose a new EM algorithm, the graphical EM algorithm, that runs for a
class of parameterized logic programs representing sequential decision processes where each
decision is exclusive and independent. It runs on a new data structure called support graph s
describing the logical relationship between observations and their explanations, and learns
parameters by computing inside and outside probability generalized for logic programs.
The complexity analysis shows that when combined with OLDT search for all explanations for observations, the graphical EM algorithm, despite its generality, has the same
time complexity as existing EM algorithms, i.e. the Baum-Welch algorithm for HMMs, the
Inside-Outside algorithm for PCFGs, and the one for singly connected Bayesian networks
that have been developed independently in each research field. Learning experiments with
PCFGs using two corpora of moderate size indicate that the graphical EM algorithm can
significantly outperform the Inside-Outside algorithm.
1. Introduction

Parameter learning is common in various fields from neural networks to reinforcement learning to statistics. It is used to tune up systems for their best performance, be they classifiers
or statistical models. Unlike these numerical systems described by mathematical formulas however, symbolic systems, typically programs, do not seem amenable to any kind of
parameter learning. Actually there has been little literature on parameter learning of programs.
This paper is an attempt to incorporate parameter learning into computer programs.
The reason is twofold. Theoretically we wish to add the ability of learning to computer
programs, which the authors believe is a necessary step toward building intelligent systems.
Practically it broadens the class of probability distributions, beyond traditionally used numerical ones, which are available for modeling complex phenomena such as gene inheritance,
consumer behavior, natural language processing and so on.
c 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiSato & Kameya

The type of learning we consider here is statistical parameter learning applied to logic
programs.1 We assume that facts (unit clauses) in a program are probabilistically true and
have a parameterized distribution.2 Other clauses, non-unit definite clauses, are always
true as they encode laws such as \if one has a pair of blood type genes a and b, one's
blood type is AB". We call logic programs of this type a parameterized logic program and
use for statistical modeling in which ground atoms3 provable from the program represent
our observations such as \one's blood type is AB" and the parameters of the program are
inferred by performing ML (maximum likelihood) estimation on the observed atoms.
The probabilistic first-order framework sketched above is termed statistical abduction
(Sato & Kameya, 2000) as it is an amalgamation of statistical inference and abduction
where probabilistic facts play the role of abducible s, i.e. primitive hypotheses.4 Statistical
abduction is powerful in that it not only subsumes diverse symbolic-statistical frameworks
such as HMMs (hidden Markov models, Rabiner, 1989), PCFGs (probabilistic context free
grammars, Wetherell, 1980; Manning & Schutze, 1999) and (discrete) Bayesian networks
(Pearl, 1988; Castillo, Gutierrez, & Hadi, 1997) but gives us freedom of using arbitrarily
complex logic programs for modeling.5
The semantic basis for statistical abduction is distribution semantics introduced by Sato
(1995). It defines a parameterized distribution, actually a probability measure, over the set
of possible truth assignments to ground atoms and enables us to derive a new EM algorithm6
for ML estimation called the graphical EM algorithm (Kameya & Sato, 2000).
Parameter learning in statistical abduction is done in two phases, search and EM learning. Given a parameterized logic program and observations, the first phase searches for all
explanations for the observations. Redundancy in the first phase is eliminated by tabulating
partial explanations using OLDT search (Tamaki & Sato, 1986; Warren, 1992; Sagonas, T.,
& Warren, 1994; Ramakrishnan, Rao, Sagonas, Swift, & Warren, 1995; Shen, Yuan, You, &
Zhou, 2001). It returns a support graph which is a compact representation of the discovered
explanations. In the second phase, we run the graphical EM algorithm on the support graph
1. In this paper, logic programs mean definite clause programs. A definite clause program is a set of definite
clauses. A definite clause is a clause of the form A L1 ; : : : ; Ln (0  n) where A; L1 ; : : : ; Ln are atoms.
A is called the head, L1 ; : : : ; Ln the body. All variables are universally quantified. It reads if L1 and
1 1 1 and Ln hold, then A holds. In case of n = 0, the clause is called a unit clause. A general clause is
one whose body may contain negated atoms. A program including general clauses is sometimes called a
general program (Lloyd, 1984; Doets, 1994).
2. Throughout this paper, for familiarity and readability, we will somewhat loosely use \distribution" as a
synonym for \probability measure".
3. In logic programming, the adjective \ground" means no variables contained.
4. Abduction means inference to the best explanation for a set of observations. Logically, it is formalized as
a search for an explanation E such that E; KB ` G where G is an atom representing our observation, KB
a knowledge base and E a conjunction of atoms chosen from abducible s, i.e. a class of formulas allowed
as primitive hypotheses (Kakas, Kowalski, & Toni, 1992; Flach & Kakas, 2000). E must be consistent
with KB.
5. Existing symbolic-statistical modeling frameworks have restrictions and limitations of various types compared with arbitrary logic programs (see Section 7 for details). For example, Bayesian networks do not
allow recursion. HMMs and PCFGs, stochastic grammars, allow recursion but lack variables and data
structures. Recursive logic programs are allowed in Ngo and Haddawy's (1997) framework but they
assume domains are finite and function symbols seem prohibited.
6. \EM algorithm" stands for a class of iterative algorithms for ML estimation with incomplete data
(McLachlan & Krishnan, 1997).
392

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

and learn the parameters of the distribution associated with the program. Redundancy in
the second phase is removed by the introduction of inside and outside probability for logic
programs computed from the support graph.
The graphical EM algorithm has accomplished, when combined with OLDT search for
all explanations, the same time complexity as the specialized ones, e.g. the Baum-Welch
algorithm for HMMs (Rabiner, 1989) and the Inside-Outside algorithm for PCFGs (Baker,
1979), despite its generality. What is surprising is that, when we conducted learning experiments with PCFGs using real corpora, it outperformed the Inside-Outside algorithm by
orders of magnitudes in terms of time for one iteration to update parameters. These experimental results enhance the prospect for symbolic-statistical modeling by parameterized
logic programs of even more complex systems than stochastic grammars whose modeling
has been dicult simply because of the lack of an appropriate modeling tool and their sheer
complexities. The contributions of this paper therefore are
 distribution semantics for parameterized logic programs which unifies existing symbolicstatistical frameworks,
 the graphical EM algorithm (combined with tabulated search), a general yet ecient
EM algorithm that runs on support graphs and
 the prospect suggested by the learning experiments for modeling and learning complex
symbolic-statistical phenomena.
The rest of this paper is organized as follows. After preliminaries in Section 2, a probability space for parameterized logic programs is constructed in Section 3 as a mathematical
basis for the subsequent sections. We then propose a new EM algorithm, the graphical
EM algorithm, for parameterized logic programs in Section 4. Complexity analysis of the
graphical EM algorithm is presented in Section 5 for HMMs, PCFGs, pseudo PCSGs and
sc-BNs.7 Section 6 contains experimental results of parameter learning with PCFGs by the
graphical EM algorithm using real corpora that demonstrate the eciency of the graphical
EM algorithm. We state related work in Section 7, followed by conclusion in Section 8. The
reader is assumed to be familiar with the basics of logic programming (Lloyd, 1984; Doets,
1994), probability theory (Chow & Teicher, 1997), Bayesian networks (Pearl, 1988; Castillo
et al., 1997) and stochastic grammars (Rabiner, 1989; Manning & Schutze, 1999).
2. Preliminaries

Since our subject intersects logic programming and EM learning which are quite different
in nature, we separate preliminaries.

2.1 Logic Programming and OLDT

In logic programming, a program DB is a set of definite clauses8 and the execution is search
for an SLD refutation of a given goal G. The top-down interpreter recursively selects the
7. Pseudo PCSGs (probabilistic context sensitive grammars) are a context-sensitive extension of PCFGs
proposed by Charniak and Carroll (1994). sc-BN is a shorthand for a singly connected Bayesian network
(Pearl, 1988).
8. We do not deal with general logic programs in this paper.
393

fiSato & Kameya

next goal and unfolds it (Tamaki & Sato, 1984) into subgoals using a nondeterministically
chosen clause. The computed result by the SLD refutation, i.e. a solution, is an answer
substitution (variable binding)  such that DB ` G.9 Usually there is more than one
refutation for G, and the search space for all refutations is described by an SLD tree
which may be infinite depending on the program and the goal (Lloyd, 1984; Doets, 1994).
More often than not, applications require all solutions. In natural language processing
for instance, a parser must be able to find all possible parse trees for a given sentence as
every one of them is syntactically correct. Similarly in statistical abduction, we need to
examine all explanations to determine the most likely one. All solutions are obtained by
searching the entire SLD tree, and there is a choice of the search strategy. In Prolog, the
standard logic programming language, backtracking is used to search for all solutions in
conjunction with a fixed search order for goals (textually from left-to-right) and clauses
(textually top-to-bottom) due to the ease and simplicity of implementation.
The problem with backtracking is that it forgets everything until up to the previous
choice point, and hence it is quite likely to prove the same goal again and again, resulting in
exponential search time. One answer to avoid this problem is to store computed results and
reuse them whenever necessary. OLDT is such an instance of memoizing scheme (Tamaki
& Sato, 1986; Warren, 1992; Sagonas et al., 1994; Ramakrishnan et al., 1995; Shen et al.,
2001). Reuse of proved subgoals in OLDT search often drastically reduces search time
for all solutions, especially when refutations of the top goal include many common subrefutations. Take as an example a logic program coding an HMM. For a given string s,
there exist exponentially many transition paths that output s. OLDT search applied to
the program however only takes time linear in the length of s to find all of them unlike
exponential time by Prolog's backtracking search.
What does OLDT have to do with statistical abduction? From the viewpoint of statistical abduction, reuse of proved subgoals, or equivalently, structure sharing of sub-refutations
for the top-goal G brings about structure sharing of explanations for G, in addition to the
reduction of search time mentioned above, thereby producing a highly compact representation of all explanations for G.

2.2 EM Learning

Parameterized distributions such as the multinomial distribution and the normal distribution provide convenient modeling devices in statistics. Suppose a random sample x1; : : : ; xT
of size T on a random variable X drawn from a distribution P (X = x j ) parameterized
by unknown , is observed. The value of  is determined by ML estimation as the MLE
(maximum likelihood estimate) of , i.e. as the maximizer of the likelihood 1iT P (xi j ).
Things get much more dicult when data are incomplete. Think of a probabilistic
relationship between non-observable cause X and observable effect Y such as one between
diseases and symptoms in medicine and assume that Y does not uniquely determine the
cause X . Then Y is incomplete in the sense that Y does not carry enough information to
completely determine X . Let P (X = x; Y = y j ) be a parameterized joint distribution
over X and Y . Our task is to perform ML estimation on  under the condition that X is
Q

9. By a solution we ambiguously mean both the answer substitution  itself and the proved atom G, as
one gives the other.
394

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

non-observable while Y is observable. Let y1 ; : : : ; yT be a random sample of size T drawn
from the marginal distribution P (Y = y j ) = x P (X = x; Y = y j ). The MLE of  is
obtained by maximizing the likelihood 1iT P (yi j ) as a function of .
While mathematical formulation looks alike in both cases, the latter, ML estimation with
incomplete data, is far more complicated and direct maximization is practically impossible
in many cases. People therefore looked to indirect approaches to tackle the problem of
ML estimation with incomplete data to which the EM algorithm has been a standard
solution (Dempster, Laird, & Rubin, 1977; McLachlan & Krishnan, 1997). It is an iterative
algorithm applicable to a wide class of parameterized distributions including the multinomial
distribution and the normal distribution such that the MLE computation is replaced by the
iteration of two easier, more tractable steps. At n-th iteration, it first calculates the value
of Q function introduced below using current parameter value (n) (E-step)10 :
P

Q

Q( j  (n)) def
=

X

x

P (x j y; (n) ) ln P (x; y j  ):

(1)

Next, it maximizes Q( j (n)) as a function of  and updates (n) (M-step):
(n+1) = argmax Q( j (n) ):
(2)
Since the old value (n) and the updated value (n+1) do not necessarily coincide, the E-steps
and M-steps are iterated until convergence, during which the (log) likelihood is assured to
increase monotonically (McLachlan & Krishnan, 1997).
Although the EM algorithm merely performs local maximization, it is used in a variety
of settings due to its simplicity and relatively good performance. One must notice however
that the EM algorithm is just a class name, taking different form depending on distributions
and applications. The development of a concrete EM algorithm such as the Baum-Welch
algorithm for HMMs (Rabiner, 1989) and the Inside-Outside algorithm for PCFGs (Baker,
1979) requires individual effort for each case.
10. Q function is related to ML estimation as follows. We assume here only one data, y , is observed. From
Jensen's inequality (Chow & Teicher, 1997) and the concavity of ln function, it follows that
X

P (x j y; (n) ) ln P (x j y;  ) 0

x

X

P (x j y; (n) ) ln P (x j y;  (n) )  0

x

and hence that
Q( j (n) ) 0 Q((n) j (n) )
X
X
=
P (x j y;  (n) ) ln P (x j y; ) 0
P (x j y;  (n) ) ln P (x j y;  (n) ) + ln P (y j ) 0 ln P (y j (n) )
x

 ln P (y j ) 0 ln P (y j (n) ):

x

Consequently, we have
Q( j (n) )  Q((n) j (n) ) ) ln p(y j )  ln p(y j (n) ) ) p(y j )  p(y j (n) ):
395

fiSato & Kameya
3. Distribution Semantics

In this section, we introduce parameterized logic programs and define their declarative semantics. The basic idea is as follows. We start with a set F of probabilistic facts (atoms)
and a set R of non-unit definite clauses. Sampling from F determines a set F 0 of true
atoms, and the least Herbrand model of F 0 [ R determines the truth value of every atom
in DB = F [ R. Hence every atom can be considered as a random variable, taking on 1
(true) or 0 (false). In what follows, we formalize this process and construct the underlying
probability space for the denotation of DB.

3.1 Basic Distribution PF

Let DB = F [R be a definite clause program in a first-order language L with countably many
variables, function symbols and predicate symbols where F is a set of unit clauses (facts)
and R a set of non-unit clauses (rules). In the sequel, unless otherwise stated, we consider
for simplicity DB as the set of all ground instances of the clauses in DB, and assume that
F and R consist of countably infinite ground clauses (the finite case is similarly treated).
We then construct a probability space for DB in two steps. First we introduce a probability
space over the Herbrand interpretations11 of F i.e. the truth assignments to ground atoms
in F . Next we extend it to a probability space over the Herbrand interpretations of all
ground atoms in L by using the least model semantics (Lloyd, 1984; Doets, 1994).
Let A1 ; A2 ; : : : be a fixed enumeration of atoms in F . We regard an infinite vector ! =
hx1; x2; : : :i of 0s and 1s as a Herbrand interpretation of F in such a way that for i = 1; 2; : : :
Ai is true (resp. false) if and only if xi = 1 (resp. xi = 0). Under this isomorphism, the set
of all possible Herbrand interpretations of F coincides with the Cartesian product:
1
def

F = f0; 1gi:
Y

i=1

We construct a probability measure PF over the sample space 
F 12 from a collection of
finite joint distributions PF(n)(A1 = x1; : : : ; An = xn) (n = 1; 2; : : : ; xi 2 f0; 1g; 1  i  n)
such that
0  PF(n)(A1 = x1 ; : : : ; An = xn)  1
(n)
(3)
x ;:::;x PF (A1 = x1 ; : : : ; An = xn ) = 1
(
n+1)
(
n)
PF (A1 = x1 ; : : : ; An+1 = xn+1 ) = PF (A1 = x1 ; : : : ; An = xn ):
x
The last equation is called the compatibility condition. It can be proved (Chow & Teicher,
1997) from the compatibility condition that there exists a probability space (
F ; F ; PF )
where PF is a probability measure on F , the minimal  algebra containing open sets of 
F ,
such that for any n,
PF (A1 = x1 ; : : : ; An = xn ) = PF(n) (A1 = x1 ; : : : ; An = xn ):
8
>
>
<
>
>
:

P

P

1

n

n+1

11. A Herbrand interpretation interprets a function symbol uniquely as a function on ground terms and
assigns truth values to ground atoms. Since the interpretation of function symbols is common to all
Herbrand interpretations, given L, they have a one-to-one correspondence with truth assignments to
ground atoms in L. So we do not distinguish them.
12. We regard 
F as a topological space with the product topology such that each f0; 1g is equipped with
the discrete topology.
396

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

We call PF a basic distribution
.13
(
n)
The choice of PF is free as long as the compatibility condition is met. If we want all
interpretations to be equiprobable, we should set PF(n)(A1 = x1; : : : ; An = xn) = 1=2n for
every hx1; : : : ; xn i. The resulting PF is a uniform distribution over 
F just like the one
over the unit interval [0; 1]. If, on the other hand, we stipulate no interpretation except
!0 = hc1 ; c2; : : :i should be possible, we put, for each n,
PF(n) (A1 = x1 ; : : : ; An = xn ) = 10 ifo.w. 8i xi = ci (1  i  n)
Then PF places all probability mass on !0 and gives probability 0 to the rest.
Define a parameterized logic program as a definite clause program14 DB = F [ R where
F is a set of unit clauses, R is a set of non-unit clauses such that no clause head in R is
unifiable with a unit clause in F and a parameterized basic distribution PF is associated with
F . A parameterized PF is obtained from a collection of parameterized joint distributions
satisfying the compatibility condition. Generally, the more complex PF(n)'s are, the more
exible PF is, but at the cost of tractability. The choice of parameterized finite distributions
made by Sato (1995) was simple:
PF(2n) (ON 1 = x1; OFF 2 = x2 ; : : : ; OFF 2n = x2n j 1 ; : : : ; n)
n
= Pbs (ON 2i01 = x2i01; OFF 2i = x2i j i)
i=1
where
Pbs (ON 2i01 = x2i01 ; OFF 2i = x2i j i )
0 if x2i01 = x2i
=
i if x2i01 = 1; x2i = 0
(4)
1 0 i if x2i01 = 0; x2i = 1:
Pbs (ON 2i01 = x2i01 ; OFF 2i = x2i j i ) (1  i  n) represents a probabilistic binary switch,
i.e. a Bernoulli trial, using two exclusive atoms ON 2i01 and OFF 2i in such a way that either
one of them is true on each trial but never both. i is a parameter specifying the probability
that the switch i is on. The resulting PF is a probability measure over the infinite product of
independent binary outcomes. It might look too simple but expressive enough for Bayesian
networks, Markov chains and HMMs (Sato, 1995; Sato & Kameya, 1997).
(

Y

8
>
<
>
:

3.2 Extending PF to PDB

In this subsection, we extend PF to a probability measure PDB over the possible world s
for L, i.e. the set of all possible truth assignments to ground atoms in L through the least
13. This naming of PF , despite its being a probability measure, partly reects the observation that it behaves
like an infinite joint distribution PF (A1 = x1 ; A2 = x2 ; : : :) for an infinite random vector hA1 ; A2 ; : : :i
of which PF(n) (A1 = x1 ; : : : ; An = xn ) (n = 1; 2; : : :) are marginal distributions. Another reason is
intuitiveness. These considerations apply to PDB defined in the next subsection as well.
14. Here clauses are not necessarily ground.
397

fiSato & Kameya

Herbrand model (Lloyd, 1984; Doets, 1994). Before proceeding however, we need a couple
of notations. For an atom A, define Ax by
Ax = A if x = 1
Ax = :A if x = 0:
Next take a Herbrand interpretation  2 
F of F . It makes some atoms in F true and
others false. Let F be the set of atoms made true by  . Then imagine a definite clause
program DB0 = R [ F and its least Herbrand model MDB0 (Lloyd, 1984; Doets, 1994).
MDB0 is characterized as the least fixed point of a mapping TDB0 (1) below
is some A B1 ; : : : ; Bk 2 DB0 (0  k)
TDB0 (I ) def
= A there
such that fB1 ; : : : ; Bk g  I
where I is a set of ground atoms.15 Or equivalently, it is inductively defined by
I0 = ;
In+1 = TDB0 (In )
MDB0 =
In :
(

(

fi
fi
fi
fi
fi

)

[

n

Taking into account the fact that MDB0 is a function of  2 
F , we henceforth employ a
functional notation MDB ( ) to denote MDB0 .
Turning back, let A1 ; A2 ; : : : be again an enumeration, but of all ground atoms in L.16
Form 
DB , similarly to 
F , as the Cartesian product of denumerably many f0; 1g's and identify it with the set of all possible Herbrand interpretations of the ground atoms A1 ; A2 ; : : :
in L, i.e. the possible world s for L. Then extend PF to a probability
measure PDB over 
DB
(
n)
as follows. Introduce a series of finite joint distributions PDB (A1 = x1 ; : : : ; An = xn ) for
n = 1; 2; : : : by
[Ax1 ^ 1 1 1 ^ Axn ]F def
= f 2 
F j MDB ( ) j= Ax1 ^ 1 11 ^ Axn g
def
(n) (A = x ; : : : ; A = x ) = P ([Ax ^ 11 1 ^ Ax ] ):
PDB
1
1
n
n
F
n F
1
1

n

1

1

n

n

(n) 's satisfy the
Note that the set [Ax1 ^ 1 1 1 ^ Axn ]F is PF -measurable and by definition, PDB
compatibility condition
(n+1) (A = x ; : : : ; A = x ) = P (n) (A = x ; : : : ; A = x ):
PDB
1
1
n+1
n+1
1
n
n
DB 1
1

n

X

xn+1

Hence there exists a probability measure PDB over 
DB which is an extension of PF such
that
PDB (A1 = x1 ; : : : ; An = xn ) = PF (A1 = x1 ; : : : ; An = xn )
15. I defines, mutually, a Herbrand interpretation such that a ground atom A is true if and only if A 2 I .
A Herbrand model of a program is a Herbrand interpretation that makes every ground instance of every
clause in the program true.
16. Note that this enumeration enumerates ground atoms in F as well.
398

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

for any finite atoms A1; : : : ; An in F and for every binary vector hx1 ; : : : ; xni (xi 2 f0; 1g; 1 
i  n). Define the denotation of the program DB = F [ R w.r.t. PF to be PDB . The denotational semantics of parameterized logic programs defined above is called distribution
semantics. As remarked before, we regard PDB as a kind of infinite joint distribution
PDB (A1 = x1 ; A2 = x2 ; : : :). Mathematical properties of PDB are listed in Appendix A
where our semantics is proved to be an extension of the standard least model semantics in
logic programming to possible world semantics with a probability measure.

3.3 Programs as Distributions

Distribution semantics views parameterized logic programs as expressing distributions. Traditionally distributions have been expressed by using mathematical formulas but the use of
programs as (discrete) distributions gives us far more freedom and exibility than mathematical formulas in the construction of distributions because they have recursion and arbitrary composition. In particular a program can contain infinitely many random variables
as probabilistic atoms through recursion, and hence can describe stochastic processes that
potentially involve infinitely many random variables such as Markov chains and derivations
in PCFGs (Manning & Schutze, 1999).17
Programs also enable us to procedurally express complicated constraints on distributions
such as \the sum of occurrences of alphabets a or b in an output string of an HMM must be
a multiple of three". This feature, procedural expression of arbitrarily complex (discrete)
distributions, seems quite helpful in symbolic-statistical modeling.
Finally, providing mathematically sound semantics for parameterized logic programs
is one thing, and implementing distribution semantics in a tractable way is another. In
the next section, we investigate conditions on parameterized logic programs which make
probability computation tractable, thereby making them usable as a means for large scale
symbolic-statistical modeling.
4. Graphical EM Algorithm

According to the preceding section, a parameterized logic program DB = F [ R in a
first-order language L with a parameterized basic distribution PF (1 j ) over the Herbrand
interpretations of ground atoms in F specifies a parameterized distribution PDB (1 j ) over
the Herbrand interpretations for L. In this section, we develop, step by step, an ecient EM
algorithm for the parameter learning of parameterized logic programs by interpreting PDB
as a distribution over the observable and non-observable events. The new EM algorithm is
termed the graphical EM algorithm. It is applicable to arbitrary logic programs satisfying
certain conditions described later provided the basic distribution is a direct product of
multi-ary random switches, which is a slight complication of the binary ones introduced in
Section 3.1.
From this section on, we assume that DB consists of usual definite clauses containing
(universally quantified) variables. Definitions and changes relating to this assumption are
17. An infinite derivation can occur in PCFGs. Take a simple PCFG fp : S ! a; q : S ! SS g where S is a
start symbol, a a terminal symbol, p + q = 1 and p; q > 0. In this PCFG, S is rewritten either to a with
probability p or to SS with probability q . The probability of the occurrence of an infinite derivation is
calculated as max f0; 1 0 (p=q)g which is non-zero when q > p (Chi & Geman, 1998).
399

fiSato & Kameya

listed below. For a predicate p, we introduce iff (p), the iff definition of p by
iff(p) def
= 8x (p(x) $ 9y1(x = t1 ^ W1 ) _ 1 11 _ 9yn (x = tn ^ Wn )) :
Here x is a vector of new variables of length equal to the arity of p, p(ti) Wi (1  i 
n; 0  n), an enumeration of clauses about p in DB, and yi , a vector of variables occurring
in p(ti) Wi. Then define comp(R) as follows.
head(R) def
= fB j B is a ground instance of a clause head appearing in Rg
iff (R) def
= fiff (p) j p appears in a clause head in Rg
Eq def
= ff (x) = f (y) ! x = y j f is a function symbolg
[ ff (x) 6= g(y) j f and g are different function symbolsg
[ ft 6= x j t is a term properly containing xg
comp(R) def
= iff (R) [ Eq
Eq , Clark's equational theory (Clark, 1978), deductively simulates unification. Likewise
comp(R) is a first-order theory which deductively simulates SLD refutation with the help
of Eq by replacing a clause head atom with the clause body (Lloyd, 1984; Doets, 1994).
We here introduce some definitions which will be frequently used. Let B be an atom.
An explanation for B w.r.t. DB = F [ R is a conjunction S such that S; R ` B, and as a
set comprised of its conjuncts, S  F holds and no proper subset of S satisfies this. The
set of all explanations for B is called the support set for B and designated by DB (B).18

4.1 Motivating Example

First of all, we review distribution semantics by a concrete example. Consider the following
program DBb = Fb [ Rb in Figure 1 modeling how one's blood type is determined by blood
type genes probabilistically inherited from the parents.19
The first four clauses in Rb state a blood type is determined by a genotype, i.e. a pair of
blood type genes a, b and o. For instance, btype('A'):- (gtype(a,a) ; gtype(a,o) ;
gtype(o,a)) says that one's blood type is A if his (her) genotype is ha; ai, ha; oi or ho; ai.
These are propositional rules.
Succeeding clauses state general rules in terms of logical variables. The fifth clause
says that regardless of the values of X and Y, event gtype(X,Y) (one's having genotype
hX; Yi) is caused by two events, gene(father,X) (inheriting gene X from the father) and
gene(mother,Y) (inheriting gene Y from the mother). gene(P,G):- msw(gene,P,G) is a
clause connecting rules in Rb with probabilistic facts in Fb. It tells us that the gene G
is inherited from a parent P if a choice represented by msw(gene,P,G)20 is made. The

18. This definition of a support set differs from the one used by Sato (1995) and Kameya and Sato (2000).
19. When we implicitly emphasize the procedural reading of logic programs, Prolog conventions are employed
(Sterling & Shapiro, 1986). Thus, ; stands for \or", , \and" :- \implied by" respectively. Strings
beginning with a capital letter are (universally quantified) variables, but quoted ones such as 'A' are
constants. The underscore is an anonymous variable.
20. msw is an abbreviation of \multi-ary random switch" and msw(1; 1; 1) expresses a probabilistic choice from
finite alternatives. In the framework of statistical abduction, msw atoms are abducibles from which
explanations are constructed as a conjunction.
400

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

8
>
>
>
>
>
>
>
<

btype('A')
btype('B')
btype('O')
btype('AB')
gtype(X,Y)
gene(P,G)

::::::-

(gtype(a,a) ; gtype(a,o) ; gtype(o,a)).
(gtype(b,b) ; gtype(b,o) ; gtype(o,b)).
gtype(o,o).
(gtype(a,b) ; gtype(b,a)).
gene(father,X), gene(mother,Y).
msw(gene,P,G).

Rb

=

Fb

= fmsw(gene,father,a); msw(gene,father,b); msw(gene,father,o);

>
>
>
>
>
>
>
:

msw(gene,mother,a); msw(gene,mother,b); msw(gene,mother,o)g

Figure 1: ABO blood type program DBb
genetic knowledge that the choice of G is by chance and made from fa; b; og is expressed by
specifying a joint distribution Fb as follows.
PF (msw(gene,t,a) = x; msw(gene,t,b) = y; msw(gene,t,o) = z j a ; b ; o ) def
= axby oz
where x; y; z 2 f0; 1g, x + y + z = 1, a; b; o 2 [0; 1], a + b + o = 1 and t is either
father or mother. Thus a is the probability of inheriting gene a from a parent. Statistical
independence of the choice of gene, once from father and once from mother, is expressed
by putting
PF ( msw(gene,father,a) = x; msw(gene,father,b) = y; msw(gene,father,o) = z;
msw(gene,mother,a) = x0; msw(gene,mother,b) = y 0; msw(gene,mother,o) = z 0
j a ; b ; o )
= PF (x; y; z j a; b; o)PF (x0; y0 ; z0 j a; b ; o):
In this setting, atoms representing our observation are obs(DBb ) = fbtype('A'); btype('B');
btype('O'); btype('AB')g. We observe one of them, say btype('A'), and infer a possible
explanation S , i.e. a minimal conjunction of abducibles msw(gene,1,1) such that
S; Rb ` btype('A').
S is obtained by applying a special SLD refutation procedure to the goal btype('A')
which preserves msw atoms resolved upon in the refutation. Three explanations are found.
S1 = msw(gene,father,a) ^ msw(gene,mother,a)
S2 = msw(gene,father,a) ^ msw(gene,mother,o)
S3 = msw(gene,father,o) ^ msw(gene,mother,a)
So DB (btype(a)), the support set for btype(a), is fS1 ; S2 ; S3g. The probability of each
explanation is respectively computed as PF (S1) = a2 and PF (S2 ) = PF (S3) = ao. From
Proposition A.2 in Appendix A, it follows that PDB (btype('A')) = PDB (S1 _ S2 _ S3) =
PF (S1 _ S2 _ S3 ) and that
PDB (btype('A') j a ; b ; o ) = PF (S1 ) + PF (S2 ) + PF (S3 )
= a2 + 2ao:
b

b

b

b

b

b

b

b

b

b

b

b

b

401

b

b

fiSato & Kameya

Here we used the fact that S1, S2 and S3 are mutually exclusive as the choice of gene is
exclusive. Parameters, i.e. a, b and o are determined by ML estimation performed on a
random sample such as fbtype('A'); btype('O'); btype('AB')g of btype as follows.
ha ; b ; oi = argmaxh ; ; i PDB (btype('A'))PDB (btype('O'))PDB (btype('AB'))
= argmaxh ; ; i (a2 + 2ao)o2 ab
This program contains neither function symbol nor recursion though our semantics
allows for them. Later we see an example containing both, a program for an HMM (Rabiner
& Juang, 1993).
a

b o

a

b o

b

b

b

4.2 Four Simplifying Conditions

in Figure 1 is simple and probability computation is easy. This is not generally the
case. Since our primary interest is learning, especially ecient parameter learning of parameterized logic programs, we hereafter concentrate on identifying what property of a program
makes probability computation easy like DBb, thereby makes ecient parameter learning
possible.
To answer this question precisely, let us formulate the whole modeling process. Suppose
there exist symbolic-statistical phenomena such as gene inheritance for which we hope
to construct a probabilistic computational model. We first specify a target predicate p
whose ground atom p(s) represents our observation of the phenomena. Then to explain
the empirical distribution of p, we write down a parameterized logic program DB = F [ R
having a basic distribution PF with parameter  that can reproduce all observable patterns
of p(s). Finally, observing a random sample p(s1); : : : ; p(sT ) of ground atoms of p, we
adjust  by ML estimation, i.e. by maximizing the likelihood L() = Tt=1 PDB (p(st) j ) so
that PDB (p(1) j ) approximates as closely to the empirically observed distribution of p as
possible.
At first sight, this formulation looks right, but in reality it is not. Suppose two events
p(s) and p(s0 ) (s 6= s0) are observed. We put L() = PDB (p(s) j  )PDB (p(s0) j ). But this
cannot be a likelihood at all simply because in distribution semantics, p(s) and p(s0 ) are
two different random variables, not two realizations of the same random variable.
A quick remedy is to note that in the case of blood type program DBb where obs(DBb) =
fbtype('A'); btype('B'); btype('O'); btype('AB')g are observable atoms, only one of
them is true for each observation, and if some atom is true, others must be false. In other
words, these atoms collectively behave as a single random variable having the distribution
PDB whose values are obs(DBb ).
Keeping this in mind, we introduce the following condition. Let obs(DB) ( head(R))
be a set of ground atoms which represent observable events. We call them observable atom s.
DBb

Q

b

Uniqueness condition:
0
PDB (G ^ G ) = 0

for any G 6= G0 2 obs(DB), and

402

P

G2obs(DB) PDB

(G) = 1.

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

The uniqueness condition enables us to introduce a new random variable Yo representing
our observation. Fix an enumeration G1 ; G2 ; : : : of observable atoms in obs(DB) and define
Yo by21
Y o (! ) = k

iff ! j= Gk for ! 2 
DB (k  1):
(5)
Let Gk T; Gk ; : : : ; Gk 2 obs(DB) be a random sample of size T . Then L() = Tt=1 PDB (Gk j
) = t=1 PDB (Yo = kt j ) qualifies for the likelihood function w.r.t. Yo .
The second condition concerns the reduction of probability computation to addition.
Take again the blood type exmaple. The computation of PDB (btype('A')) is decomposed
into a summation because explanations in the support set are mutualy exclusive. So we
introduce
Q

Q1

2

t

T

b

Exclusiveness condition:

For every G 2 obs(DB) and the support set DB (G), PDB (S ^ S 0) = 0 for any S 6=
S 0 2 DB (G).
Using the exclusiveness condition (and Proposition A.2 in Appendix A), we have
PDB (G) =
PF (S ):
X

S2

DB

(G)

From a modeling point of view, it means that while a single event, or a single observation,
G, may have several (or even infinite) explanations DB (G), only one of DB (G) is allowed
to be true for each observation.
Now introduce 9DB , i.e. the set of all explanations relevant to obs(DB) by
9DB def
=
DB (G)
[

G2obs(DB)

and fix an enumeration S1; S2 ; : : : of explanations in 9DB . It follows from Proposition A.2,
the uniqueness condition and the exclusiveness condition that
PDB (Si ^ Sj ) = 0 for i 6= j
and
PDB (S ) =
PDB (S )
X

S 29DB

X

X

G2obs(DB) S 2

(G)
PDB (G)
DB

=
G2obs(DB)
= 1:
So we are able to introduce under the uniqueness condition and the exclusiveness condition
yet another random variable Xe, representing an explanation for G, defined by
Xe (!) = k iff ! j= Sk for ! 2 
DB :
(6)
The third condition concerns termination.

21.

X

G2obs(DB) PDB (G) = 1 only guarantees that the measure of f ! j ! j= Gk for some k ( 1)g is one, so
there can be some ! satisfying no Gk 's. In such case, we put Yo (!) = 0. But values on a set of measure
zero do not affect any part of the discussion that follows. This also applies to the definition of Xe in (6).
P

403

fiSato & Kameya

Finite support condition:

For every G 2 obs(DB) DB (G) is finite.
PDB (G) is then computed from the support set DB (G) = fS1 ; : : : ; Sm g (0  m), with
the help of the exclusiveness condition, as a finite summation mi=1 PF (Si). This condition
prevents an infinite summation that is hardly computable.
The fourth condition simplifies the probability computation to multiplication. Recall
that an explanation S for G 2 obs(DB) is a conjunction a1 ^ 1 11 ^ am of some abducibles
fa1; : : : ; amg  F (1  m). In order to reduce the computation of PF (S ) = PF (a1 ^11 1^ am)
to the multiplication PF (a1) 1 11 PF (am ), we assume
P

Distribution condition:

F is a set Fmsw of ground atoms with a parameterized distribution Pmsw specified below.

Here atom msw(i,n,v) is intended to simulate a multi-ary random switch whose name is i
and whose outcome is v on trial n. It is a generalization of primitive probabilistic events
such as coin tossing and dice rolling.
1. Fmsw consists of probabilistic atoms msw(i,n,v). The arguments i, n and v are ground
terms called switch name, trial-id and a value (of the switch i), respectively. We
assume that a finite set Vi of ground terms called the value set of i is associated with
each i, and v 2 Vi holds.
2. Write Vi as fv1 ; v2 ; : : : ; vmg (m = jVi j). Then, one of the ground atoms f msw(i,n,v1),
msw(i,n,v2 ), . .. , msw(i,n,vm )g becomes exclusively true (takes on value 1) on each
trial. With each i, a parameter i;v 2 [0; 1] such that v2V i;v = 1 is associated. i;v
is the probability of msw(i,1,v) being true (v 2 Vi).
3. For each ground terms i, i0 , n, n0, v 2 Vi and v0 2 Vi0 , random variable msw(i,n,v) is
independent of msw(i0 ,n0 ,v0 ) if n 6= n0 or i 6= i0 .
In other words, we introduce a family of parameterized finite distributions P(i;n) such that
P(i;n)(msw(i,n,v1 ) = x1 ; : : : ; msw(i,n,vm ) = xm j i;v ; : : : ; i;v )
x
x
if mk=1 xk = 1
def
= 0i;v 1 11 i;v o.w.
(7)
P

i

(

1

1

1

m
m

P

m

where m = jVij, xk 2 f0; 1g (1  k  m), and define Pmsw as their infinite product
Pmsw def
= P(i;n) :
Y

i;n

Under this condition, we can compute Pmsw(S ), the probability of an explanation S, as the
product of parameters. Suppose msw(ij ,n,v) and msw(ij0 ,n0,v0) are different conjuncts in
an explanation S = msw(i1 ,n1,v1 ) ^ 11 1^ msw(ik ,nk ,vk ). If either j 6= j 0 or n 6= n0 holds,
they are independent by construction. Else if j = j 0 and n = n0 but v 6= v 0, they are not
independent but Pmsw(S ) = 0 by construction. As a result, whichever condition may hold,
Pmsw (S ) is computed from the parameters.
404

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

4.3 Modeling Principle

Up to this point, we have introduced four conditions, the uniqueness condition, the exclusiveness condition, the finite support condition and the distribution condition, to simplify
probability computation. The last one is easy to satisfy. We just adopt Fmsw together with
Pmsw . So, from here on, we always assume that Fmsw has a parameterized distribution Pmsw
introduced in the previous subsection. Unfortunately the rest are not satisfied automatically. According to our modeling experiences however, it is only mildly dicult to satisfy
the uniqueness condition and the exclusiveness condition as long as we obey the following
modeling principle.
Modeling principle: DB = Fmsw [ R describes a sequential decision process
(modulo auxiliary computations) that uniquely produces an observable atom
G 2 obs(DB) where each decision is expressed by some msw atom.22
Translated into programming level, it says that we must take care when writing a program so that for any sample F 0 from Pmsw, there must uniquely exist goal G (G 2 obs(DB))
which has a successful refutation from DB0 = F 0 [ R. We can confirm the principle by the
blood type program DBb = Fb [ Rb. It describes a process of gene inheritance, and for
an arbitrary sample Fb0 from Pmsw, say Fb0 = fmsw(gene,father,a); msw(gene,mother,o)g,
there exists a unique goal, btype('A') in this case, that has a successful SLD refutation
from Fb0 [ Rb.
The idea behind this principle is that a decision process always produces some result (an
observable atom), and different decision processes must differ at some msw thereby entailing
mutually exclusive observable atoms. So the uniqueness condition and the exclusiveness
condition will be automatically satisfied.
Satisfying the finite support condition is more dicult as it is virtually equivalent to
writing a program DB for which all solution search for G (G 2 obs(DB)) always terminates. Apparently we have no general solution to this problem, but as far as specific models
such as HMMs, PCFGs and Bayesian networks are concerned, it can be met. All programs
for these models satisfy the finite support condition (and other conditions as well).

4.4 Four Conditions Revisited

In this subsection, we discuss how to relax the four simplifying conditions introduced in Subsection 4.2 for the purpose of exible modeling. We first examine the uniqueness condition
considering its crucial role in the adaptation of the EM algorithm to our semantics.
The uniqueness condition guarantees that there exists a (many-to-one) mapping from
explanations to observations so that the EM algorithm is applicable (Dempster et al., 1977).
It is possible, however, to relax the uniqueness condition while justifying the application
of the EM algorithm. We assume the MAR (missing at random) condition introduced by
Rubin (1976) which is a statistical condition on how a complete data (explanation) becomes an incomplete data (observation), and is customarily assumed implicitly or explicitly
in statistics (see Appendix B). By assuming the MAR condition, we can apply our EM
22. Decisions made in the process are a finite subset of Fmsw .
405

fiSato & Kameya

algorithm to non-exclusive observations O such that O P (O)  1 where the uniqueness
condition is seemingly destroyed.
Let us see the MAR condition in action with a simple example. Imagine we walk along
a road in front of a lawn. We occasionally observe their state such as \the road is dry but
the lawn is wet". Assume that the lawn is watered by a sprinkler running probabilistically.
The program DBrl = Rrl [ Frl in Figure 2 describes a sequential process which outputs
an observation observed(road(x),lawn(y)) (\the road is x and the lawn is y") where
x; y 2 fwet; dryg.
Rrl = { observed(road(X),lawn(Y)):P

Frl

=

msw(rain,once,A),
( A = yes, X = wet, Y = wet
; A = no, msw(sprinkler,once,B),
( B = on, X = dry, Y = wet
; B = off, X = dry, Y = dry ) ). }
{ msw(rain,once,yes), msw(rain,once,no),
msw(sprinkler,once,on), msw(sprinkler,once,off) }

Figure 2: DBrl
The basic distribution over Frl is specified like PF (1) in Subsection 4.1, so we omit it.
msw(rain,once,A) in the program determines whether it rains (A = yes) or not (A = no),
whereas msw(sprinkler,once,B) determines whether the sprinkler works fine (B = on)
or not (B = off). Since for each sampled values of A = a (a 2 fyes; nog) and B = b
(b 2 fon; offg), there uniquely exists an observation observed(road(x),lawn(y)) (x; y 2
fwet; dryg), there is a many-to-one mapping  : (a; b) = hx; yi. In other words, we
can apply the EM algorithm to the observations observed(road(x),lawn(y)) (x; y 2
fwet; dryg). What would happen if we observe exclusively either a state of the road or
that of the lawn? Logically, this means we observe 9y observed(road(x),lawn(y)) or
9x observed(road(x),lawn(y)). Apparently the uniqueness condition is not met, because
9y observed(road(wet),lawn(y)) and 9x observed(road(x),lawn(wet)) are compatible
(they are true when it rains). Despite the non-exclusiveness of the observations, we can still
apply the EM algorithm to them under the MAR condition, which in this case translates
into that we observe either the lawn or the road randomly regardless of their state.
We now briey check other conditions. Basically they can be relaxed at the cost of
increased computation. Without the exclusiveness condition for instance, we would need an
additional process of transforming the support set DB (G) for a goal G into a set of exclusive
explanations. For instance, if G has explanations fmsw(a,n,v); msw(b,m,w)g, we have to
transform it into fmsw(a,n,v); :msw(a,n,v) ^ msw(b,m,w)g and so on.23 Clearly, this
transformation is exponential in the number of msw atoms and eciency concern leads to
assuming the exclusiveness condition.
The finite support condition is in practice equivalent to the condition that the SLD tree
for G is finite. So relaxing this condition might induce infinite computation.
b

23. :msw(a,n,v ) is further transformed to a disjunction of exclusive msw atoms like
406

W

6

0
2 msw(a,n,v ).

v 0 =v;v 0 Va

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

Relaxing the distribution condition and accepting probability distributions other than
serve to expand the horizon of the applicability of parameterized logic programs. In
particular the introduction of parameterized joint distributions P (v1; : : : ; vk ) like Boltzmann distributions over switches msw1 ; : : : ; mswk where v1; : : : ; vk are values of the switches,
makes them correlated. Such distributions facilitate writing parameterized logic programs
for complicated decision processes in which decisions are not independent but interdependent. Obviously, on the other hand, they increase learning time, and whether the added
exibility of distributions deserves the increased learning time or not is yet to be seen.
Pmsw

4.5 Naive Approach to EM Learning

In this subsection, we derive a concrete EM algorithm for parameterized logic programs
DB = Fmsw [ R assuming that they satisfy the uniqueness condition, the exclusiveness
condition and the finite support condition.
To start, we introduce Yo, a random variable representing our observations according
to (5) based on a fixed enumeration of observable atoms in obs(DB). We also introduce
another random variable Xe representing their explanations according to (6) based on some
fixed enumeration of explanations in 9DB . Our understanding is that Xe is non-observable
while Yo is observable, and they have a joint distribution PDB (Xe = x; Yo = y j ) where
 denotes relevant parameters. It is then immediate, following (1) and (2) in Section 2, to
derive a concrete EM algorithm from the Q function defined by Q( j 0 ) def
= x PDB (x j
y;  0) ln PDB (x; y j ) whose input is a random sample of observable atoms and whose output
is the MLE of .
In the following, for the sake of readability, we substitute an observable atom G (G 2
obs(DB)) for Yo = y and write PDB (G j ) instead of PDB (Yo = y j ). Likewise we
substitute an explanation S (S 2 9DB ) for Xe = x and write PDB (S; G j ) instead of
PDB (Xe = x; Yo = y j ). Then it follows from the uniqueness condition that
0
if S 62 DB (G)
PDB (S; G j ) =
Pmsw (S j ) if S 2 DB (G):
We need yet another notation here. For an explanation S, define the count of msw(i,n,v)
in S by
i;v (S ) def
= jf n j msw(i,n,v) 2 S gj :
We have done all preparations now. Suppose we make some observations G = G1 ; : : : ; GT
where Gt 2 obs(DB) (1  t  T ). Put
I def
= fi j msw(i,n,v) 2 S 2 DB (Gt); 1  t  T g
 def
= fi;v j msw(i,n,v) 2 S 2 DB (Gt); 1  t  T g:
I is a set of switch names that appear in some explanation for one of the Gt 's and  denotes
parameters associated with these switches.  is finite due to the finite support condition.
P

(

407

fiSato & Kameya

Various probabilities and the Q function are computed by using Proposition A.2 in
Appendix A together with our assumptions as follows.
PDB (Gt j ) = PDB
=
Pmsw (S j  )
(8)
DB (Gt ) 
fi
fi
fi

_

Pmsw (S j  )

=

Q( j 0 ) def
=

=
where
 (i; v;  ) def
=

Y

t=1 S29DB
i2I;v2Vi
T
X
t=1

X

S2

i;v (S )
i;v

i2I;v2Vi
T
X
X

X



DB

(Gt )

PDB (S j Gt ; 0 ) ln PDB (S; Gt j )

 (i; v; 0 )ln i;v 

1

PDB (Gt j ) S 2

X

i2I;v2Vi

X

DB

(Gt )

!
(i; v; 0 )
0
(i; v;  ) ln P
0 0
v0 2Vi  (i; v ;  )

(9)

Pmsw (S j )i;v (S )

Here we used Jensen's inequality to obtain (9). Note that PDB (Gt j )01 S2 (G )
Pmsw (S j  )i;v (S ) is the expected count of msw(i,1,v) in an SLD refutation of Gt. Speaking
of the likelihood function L() = Tt=1 PDB (Gt j ), it is already shown in Subsection 2.2
(footnote) that Q( j 0 )  Q(0 j 0 ) implies L()  L(0 ). Hence from (9), we reach
the procedure learn-naive( ,G) below that finds the MLE of the parameters. The array
variable [i; v] stores (i; v; ) under the current .
P

DB

t

Q

DB

1:
2:
3:
4:
5:
6:

procedure

learn-naive(DB; G )

begin

Initialize T with appropriate values and " with a small positive number ;
(0) := t=1 ln PDB (Gt j  );
% Compute the log-likelihood.
P

repeat
foreach

i 2 I; v 2 Vi do

[i; v ] :=

T
X

1

PDB (Gt j  ) S 2
foreach i 2 I; v 2 Vi do
[i; v]
i;v := P
0;
0
v 2Vi  [i; v ]
m := m +P1;
(m) := Tt=1 ln PDB (Gt j )
until (m) 0 (m01) < "

7:
8:
9:
10:
11:
12: end

t=1

X

DB

(Gt )

Pmsw (S j  )i;v (S );

% Update the parameters.
% Compute the log-likelihood again.
% Terminate if converged.

This EM algorithm is simple and correctly calculates the MLE of , but the calculation of PDB (Gt j ) and [i; v](Line 3, 6 and 10) may suffer a combinatorial explosion of
explanations. That is, j DB (Gt)j often grows exponentially in the complexity of the model.
For instance, j DB (Gt)j for an HMM with N states is O(N L), exponential in the length L
of an input/output string. Nonetheless, suppressing the explosion to realize ecient computation in a polynomial order is possible, under suitable conditions, by avoiding multiple
computations of the same subgoal as we see next.
408

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

4.6 Inside Probability and Outside Probability for Logic Programs

In this subsection, we generalize the notion of inside probability and outside probability
(Baker, 1979; Lari & Young, 1990) to logic programs. Major computations in learn-naive( ,G)
are those of two terms in Line 6, PDB (Gt j ) and S2 (G ) Pmsw(S j )i;v (S). Computational redundancy lurks in the naive computation of both terms. We show it by an example.
Suppose there is a propositional program DBp = Fp [ Rp where Fp = fa; b; c; d; mg and
DB

P

DB

8
>
>
>
>
>
<

Rp = >
>
>
>
>
:

f
f
g
g
h

t

a^g
b^g
c
d^h
m:

(10)

Here f is an observable atom. We assume that a, b, c, d and m are independent and also
that fa; bg and fc; dg are pair-wise exclusive. Then the support set for f is calculated as
DB (f) = fa ^ c; a ^ d ^ m; b ^ c; b ^ d ^ m g:
Hence, in light of (8), we may compute PDB (f) as
PDB (f) = PF (a ^ c) + PF (a ^ d ^ m) + PF (b ^ c) + PF (b ^ d ^ m):
(11)
This computation requires 6 multiplications (because PF (a ^ c) = PF (a)PF (c) etc.) and
3 additions. On the other hand, it is possible to compute PDB (f) much more eciently by
factoring out common computations. Let A be a ground atom. Define the inside probability
fi (A) of A as
fi (A) def
= PDB (A j ):24
(12)
Then by applying Theorem A.1 in Appendix A to
comp(Rp) ` f $ (a ^ g) _ (b ^ g); g $ c _ (d ^ h); h $ m
(13)
which unconditionally holds in our semantics, and by using the independent and the exclusiveness assumption made on Fp, the following equations about inside probability are
derived.
fi (f) = fi (a)fi (g) + fi (b)fi (g)
fi (g) = fi (c) + fi (d)fi (h)
(14)
fi (h) = fi (m)
PDB (f)(= fi (f)) is obtained by solving (14) about fi (f), for which only 3 multiplications
and 2 additions are required.
It is quite straightforward to generalize (14) but before proceeding, look at a program
DBq = fmg [ fg:-m ^ m; g:-mg where g is an observable atom and m the only msw atom.
We have g $ (m ^ m) _ m in our semantics, but to compute P (g) = P (m)P (m) + P (m) is
clearly wrong as it ignores the fact that clause bodies for g, i.e. m^m and m are not mutually
exclusive, and atoms in the clause body m^m are not independent (here P (1) = PDB (1)).
Similarly, if we set a = b = c = d = m, the equation (14) will be totally incorrect.
p

p

p

p

p

p

p

p

p

p

p

8
>
<
>
:

p

q

24. Note that if A is a fact in F , fi (A) = Pmsw (A j ).

409

fiSato & Kameya

We therefore add, temporarily in this subsection, two assumptions on top of the exclusiveness condition and the finite support condition so that equations like (14) become
mathematically correct. The first assumption is that \clause" bodies are mutually exclusive i.e. if there are two clauses B W and B W 0 , PDB (W ^ W 0 j ) = 0, and the
second assumption is that body atoms are independent, i.e. if A B1 ^ 1 1 1 ^ Bk is a rule,
PDB (B1 ^ 11 1 ^ Bk j ) = PDB (B1 j ) 11 1 PDB (Bk j ) holds.
Please note that \clause" used in this subsection has a special meaning. It is intended to
mean G  where G is a goal and  is a tabled explanation for G obtained by OLDT search
both of which will be explained in the next subsection.25 In other words, these additional
conditions are not imposed on a source program but on the result of OLDT search. So
clauses for auxiliary computations do not need to satisfy them.
Now suppose clauses about A occur in DB like
A
A

B1;1 ^ 1 1 1 ^ B1;i1

11 1

BL;1 ^ 11 1 ^ BL;iL

where Bh;j (1  h  L; 1  j  ih) is an atom. Theorem A.1 in Appendix A and the above
assumptions ensure
i
i
fi (A) = fi (B1;j ) + 1 11 + fi (BL;j ):
(15)
1
Y

L
Y

j =1

j =1

(15) suggests that fi (Gt) can be considered as a function of fi (A) if these equations about
inside probabilities are hierarchically organized in such a way that fi(Gt) belongs to the top
layer and any fi(A) appearing on the left hand side only refers to fi (B)'s which belong to the
lower layers. We refer to this condition as the acyclic support condition. Under the acyclic
support condition, equations of the form (15) have a unique solution, and the computation
of PDB (G j ) via inside probabilities allows us to take advantage of reusing intermediate
results stored as fi (A), thereby contributing to faster computation of PDB (Gt j ).
Next we tackle a more intricate problem, the computation of S2 (G ) Pmsw(S j
)i;v (S ). Since the sum equals n msw(i,n,v )2S 2 (G ) Pmsw (S j ), we concentrate
on the computation of
(Gt; m) def
=
Pmsw (S j  )
P

P

DB

P

DB

t

t

X

( )

m2S 2 DB Gt

where m = msw(i,n,v). First we note that if an explanation S contains m like S = a1 ^1 11^
ah ^ m, then we have fi (S ) = fi (a1 ) 1 1 1 fi (ah)fi (m). So (Gt ; m) is expressed as
(Gt; m) = ff(Gt ; m)fi (m)

(16)
where ff(Gt; m) = @@fi(G(m;)m) and ff(Gt; m) does not depend on fi (m). Generalizing this observation to arbitrary ground atoms, we introduce the outside probability of ground atom A
w.r.t. Gt by
(Gt)
ff(Gt; A) def
= @fi
@fi (A)
t

25. The logical relationship (13) corresponds to (20) where f, g and h are table atoms.
410

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

assuming the same conditions as inside probability. In view of (16), the problem of computing (Gt; m) is now reduced to that of computing ff(Gt; m), which is recursively computable
as follows. Suppose A occurs in the ground program DB like
B1
BK

A ^ W1;1 ; 11 1 ; B1

11 1

A ^ WK;1 ; 11 1 ; BK

A ^ W1;i1
A ^ WK;iK :

As fi (Gt) is a function of fi (B1 ); : : : ; fi(BK ) by our assumption, the chain rule of derivatives
leads to
@fi (Gt )
@fi (A ^ WK;i )
@fi (Gt ) @fi (A ^ W1;1 )
+
11
1
+
ff(Gt ; A) =
@fi (B1 )
@fi (A)
@fi (BK )
@fi (A)
and hence to26
ff(Gt; Gt ) = 1
(17)
i
i
ff(Gt ; A) = ff(Gt ; B1 ) fi (W1;j ) + 11 1 + ff(Gt ; BK ) fi (WK;j ):
(18)












K

1
X

K
X

j =1

j =1

Therefore if all inside probabilities have already been computed, outside probabilities are
recursively computed from the top (17) using (18) downward along the program layers. In
the case of DBp with f and m being chosen atoms, we compute
ff(f; f) = 1
ff(f; g) = fi (a) + fi (b)
(19)
ff(f; h) = ff(f; g)fi (d)
ff(f; m) = ff(f; h):
From (19), the desired sum (f; m) is calculated as
(f; m) = ff(f; m)fi (m) = (fi (a) + fi (b))fi (d)fi (m)
which requires only two multiplications and one addition compared to four multiplications
and one addition in the naive computation.
Gains obtained by computing inside and outside probability may be small for this case,
but as the problem size grows, they become enormous, and compensate enough for additional restrictions imposed on the result of OLDT search.
8
>
>
>
<
>
>
>
:

4.7 OLDT Search

To compute inside and outside probability recursively like (15) or (17) and (18), we need
at programming level a tabulation mechanism for structure-sharing of partial explanations
26. Because of the independence assumption on body atoms, Wh;j (1  h  K; 1  j  ih ) and A are
independent. Therefore
@fi (A ^ Wh;j ) = @fi (A)fi (Wh;j ) = fi (W ):
h;j
@fi(A)
@fi (A)
411

fiSato & Kameya

between subgoals. We henceforth deal with programs DB in which a set table(DB) of table
predicate s are declared in advance. A ground atom containing a table predicate is called
a table atom. The purpose of table atoms is to store their support sets and eliminate the
need of recomputation, and by doing so, to construct hierarchically organized explanations
made up of the table atoms and the msw atoms.
Let DB = Fmsw [ R be a parameterized logic program which satisfies the finite support
condition and the uniqueness condition. Also let G1 ; G2; : : : ; GT be a random sample of
observable atoms in obs(DB). We make the following additional assumptions.

Assumptions:

For each t (1  t  T ), there exists a finite set f1t; : : : ; Kt g of table atoms associated
t (0  k  K ; 1  j  m ) such that
with conjunctions Sk;j
t
k
t

e

comp(R)





` Gt $ S0t;1 _ 1 1 1 _ S0t;m
^ 1t $ S1t;1 _ 11 1 _ S1t;m ^ 1 11 ^ Kt t $ SKt t ;1 _ 11 1 _ SKt t ;mKt
e



e

e

0

e





e

1

e

(20)



where
t (0  k  K ; 1  j  m ) is, as a set, a subset of F
 each Sk;j
t
msw [ fk+1 ; : : : ; K g
k
(acyclic support condition). As a convention, we put 0 = Gt and call respectively
t def
t (k  0) a t-explanation
DB
= f0 ; 1t; : : : ; Kt g the set of table atoms for Gt and Sk;j
for kt .27 The set of all t-explanations for k is denoted by DB (kt ) and we consider
DB (1) as a function of table atoms.
t ^ St ) =
 t-explanations are mutually exclusive, i.e. for each k (0  k  Kt ), PDB (Sk;j
k;j 0
0 (1  j 6= j 0  mk ) (t0exclusiveness condition).
t (0  k  K ; 1  j  m ) is a conjunction of independent atoms (independent
 Sk;j
t
k
condition).28
These assumptions are aimed at ecient probability computation. Namely, the acyclic
support condition makes dynamic programming possible, the t-exclusiveness condition reduces PDB (A _ B) to PDB (A)+ PDB (B) and the independent condition reduces PDB (A ^ B )
to PDB (A)PDB (B). There is one more point concerning eciency however. Note that the
t 29 imcomputation in dynamic programming proceeds following the partial order on DB
posed by the acyclic support condition and access to the table atoms will be much simplified
t respecting the said partial
if they are linearly ordered. We therefore topologically sort DB
t
order and call the linearized DB satisfying the three assumptions (the acyclic support condition, the t-exclusiveness condition and the independent condition) a hierarchical system
t = h t ;  t ; : : : ;  t i ( = G ) assuming
of t-explanations for Gt . We write it as DB
0
t
DB (1) is
0 1
K
30
implicitly given. Once a hierarchical system of t-explanations for Gt is successfully built
e

t

e

t

e

e

e

e

e

t

e

27. Prefix \t-" is an abbreviation of \tabled-".
28. The independence mentioned here only concerns positive propositions. For B1 ; B2 2 head(DB), we say
B1 and B2 are independent if PDB (B1 ^ B2 j ) = PDB (B1 j )PDB (B2 j ) for any .
29. i precedes j if and only if the top-down execution of i w.r.t. DB invokes j directly or indirectly.
30. So now it holds that if i precedes j then i < j .
412

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

from the source program, equations on inside probability and outside probability such as
(14) and (19) are automatically derived and solved in time proportional to the size of the
equations. It plays a central role in our approach to ecient EM learning.
One way to obtain such t-explanations is to use OLDT search (Tamaki & Sato, 1986;
Warren, 1992), a complete refutation method for logic programs. In OLDT search, when
a goal G is called for the first time, we set up an entry for G in a solution table and store
its answer substitutions G there. When a call to an instance G0 of G occurs later, we stop
solving G0 and instead try to retrieve an answer substitution G stored in the solution table
by unifying G0 with G. To record the remaining answer substitutions of G, we prepare a
lookup table for G0 and hold a pointer to them.
For self-containedness, we look at details of OLDT search using a sample program
DBh = Fh [Rh in Figure 431 which depicts an HMM32 in Figure 3. This HMM has two states
fs0; s1g. At a state transition, it probabilistically chooses the next destination from fs0; s1g
a,b
s1

s0

a,b

a,b

a,b

Figure 3: Two state HMM
Fh

Rh

=

=

8
<
:
8
>
>
>
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
>
>
>
:

f1: values(init, [s0,s1]).
f2: values(out(_),[a,b]).
f3: values(tr(_), [s0,s1]).

h1: hmm(Cs):msw(init,once,Si),
hmm(1,Si,Cs).
h2: hmm(T,S,[C|Cs]):- T=<3,
msw(out(S),T,C),
msw(tr(S),T,NextS),
T1 is T+1,
hmm(T1,NextS,Cs).
h3: hmm(T,_,[]):- T>3.

%
%
%
%
%
%
%
%
%

To generate a string (chars) Cs,
Set initial state to Si, and then
Enter the loop with clock = 1.
Loop:
Output C in state S.
Transit from S to NextS.
Put the clock ahead.
Continue the loop (recursion).
Finish the loop if clock > 3.

Figure 4: Two state HMM program DBh
31. f1, f2, f3, h1, h2 and h3 are temporary marks, not part of the program.
32. An HMM defines a probability distribution over strings in the given set of alphabets, and works as a
stochastic string generator (Rabiner & Juang, 1993) such that an output string is a sample from the
defined distribution.
413

fiSato & Kameya

and also an alphabet from fa; bg to emit. Note that to specify a fact set Fh and the associated distribution compactly, we introduce here a new notation values(i,[v1,...,vm]). It
declares that Fh contains msw atoms of the form msw(i,n,v) (v 2 fv1 ; : : : ; vmg) whose distribution is P(i;n) given by (7) in Subsection 4.2. For example, (f3), values(tr( ),[s0,s1])
introduces msw(tr(t),n,v) atoms into the program such that t can be any ground term,
v 2 fs0; s1g and for a ground term n, they have a distribution
x y
P(tr(t);n) (msw(tr(t),n,s0) = x; msw(tr(t),n,s1) = y j i;s0 ; i;s1) = i;s
0 i;s1
where i = tr(t), x; y 2 f0; 1g and x + y = 1.
This program runs like a Prolog program. For a non-ground top-goal hmm(S), it functions as a stochastic string generator returning a list of alphabets such as [a,b,a] in the
variable S as follows. The top-goal calls clause (h1) and (h1) selects the initial state by executing subgoal msw(init,once,Si)33 which returns in Si an initial state probabilistically
chosen from fs0, s1g. The second clause (h2) is called from (h1) with ground S and ground
T. It makes a probabilistic choice of an output alphabet C by asking msw(out(S),T,C) and
then determines NextS, the next state, by asking msw(tr(S),T,NextS). (h3) is there to
stop the transition. For simplicity, the length of output strings is fixed to three. This way
of execution is termed as sampling execution because it corresponds to a random sampling
from PDB . If the top-goal is ground like hmm([a,b,a]), it works as an acceptor, i.e.
returning success (yes) or failure (no).
If all explanations for hmm([a,b,a]) are sought for, we keep all msw atoms resolved upon
during the refutation as a conjunction (explanation), and repeat this process by backtracking until no more refutation is found. If we need t-explanations however, backtracking must
be abandoned because sharing of partial explanations through t-explanations, the purpose
of t-explanations itself, becomes impossible. We therefore instead use OLDT search for all
h

t1:
t2:
t3:
t4:
t4':
:
t7:
t8:
t9:

top_hmm(Cs,Ans):- tab_hmm(Cs,Ans,[]).
tab_hmm(Cs,[hmm(Cs)|X],X):- hmm(Cs,_,[]).
tab_hmm(T,S,Cs,[hmm(T,S,Cs)|X],X):- hmm(T,S,Cs,_,[]).
e_msw(init,T,s0,[msw(init,T,s0)|X],X).
e_msw(init,T,s1,[msw(init,T,s1)|X],X).
hmm(Cs,X0,X1):- e_msw(init,once,Si,X0,X2), tab_hmm(1,Si,Cs,X2,X1).
hmm(T,S,[C|Cs],X0,X1):T=<3, e_msw(out(S),T,C,X0,X2), e_msw(tr(S),T,NextS,X2,X3),
T1 is T+1, tab_hmm(T1,NextS,Cs,X3,X1).
hmm(T,S,[],X,X):- T>3.

Figure 5: Translated program of DBh
33. If msw(i,n,V) is called with ground i and ground n, V, a logical variable, behaves like a random variable.
It is instantiated to some term v with probability i;v selected from the value set Vi declared by a values
atom. If, on the other hand, V is a ground term v when called, the procedural semantics of msw(i,n,v)
is equal to that of msw(i,n,V) ^ V = v.
414

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

t-explanation search. In the case of the HMM program for example, to build a hierarchical
system of t-explanations for hmm([a,b,a]) by OLDT search, we first declare hmm=1 and
hmm=3 as table predicate.34 So a t-explanation will be a conjunction of hmm=1 atoms, hmm=3
atoms and msw atoms. We then translate the program into another logic program, analogously to the translation of definite clause grammars (DCGs) in Prolog (Sterling & Shapiro,
1986). We add two arguments (which forms a D-list) to each predicate for the purpose of
accumulating msw atoms and table atoms as conjuncts in a t-explanation. The translation
applied to DBh yields the program in Figure 5.
In the translated program, clause (t1) corresponds to the top-goal hmm(l) with an
input string l, and a t-explanation for the table atom hmm(l) will be returned in Ans. (t2)
and (t3) are auxiliary clauses to add to the callee's D-list a table atom of the form hmm(l)
and hmm(t,s,l) respectively (t: time step, s: state). In general, if p=n is a table predicate
in the original program, p=(n + 2) becomes a table predicate in the translated program and
an auxiliary predicate tab p=(n +2) is inserted to signal the OLDT interpreter to check the
solution table for p=n, i.e. to check if there already exist t-explanations for p=n. Likewise
clauses (t4) and (t4') are a pair corresponding to (f1) which insert msw(init,T,1) to the
callee's D-list with T = once. Clauses (t7), (t8) and (t9) respectively correspond to (h1),
(h2) and (h3).
hmm([a,b,a]):[hmm([a,b,a])]
[ [msw(init,once,s0), hmm(1,s0,[a,b,a])],
[msw(init,once,s1), hmm(1,s1,[a,b,a])] ]
hmm(1,s0,[a,b,a]):[hmm(1,s0,[a,b,a])]
[ [msw(out(s0),1,a), msw(tr(s0),1,q0), hmm(2,s0,[b,a])],
[msw(out(s0),1,a), msw(tr(s0),1,s1), hmm(2,s1,[b,a])] ]
hmm(1,s1,[a,b,a]):[hmm(1,s1,[a,b,a])]
[ [msw(out(s1),1,a), msw(tr(s1),1,s0), hmm(2,s0,[b,a])],
[msw(out(s1),1,a), msw(tr(s1),1,s1), hmm(2,s1,[b,a])] ]
hmm(2,s0,[b,a]):[hmm(2,s0,[b,a])]
[ [msw(out(s0),2,b), msw(tr(s0),2,s0), hmm(3,s0,[a])],
[msw(out(s0),2,b), msw(tr(s0),2,s1), hmm(3,s1,[a])] ]
hmm(2,s1,[b,a]):[hmm(2,s1,[b,a])]
[ [msw(out(s1),2,b), msw(tr(s1),2,s0), hmm(3,s0,[a])],
[msw(out(s1),2,b), msw(tr(s1),2,s1), hmm(3,s1,[a])] ]
hmm(3,s0,[a]):[hmm(3,s0,[a])]
[ [msw(out(s0),3,a), msw(tr(s0),3,s0), hmm(4,s0,[])],
[msw(out(s0),3,a), msw(tr(s0),3,s1), hmm(4,s1,[])] ]
hmm(3,s1,[a]):[hmm(3,s1,[a])]
[ [msw(out(s1),3,a), msw(tr(s1),3,s0), hmm(4,s0,[])],
[msw(out(s1),3,a), msw(tr(s1),3,s1), hmm(4,s1,[])] ]
hmm(4,s0,[]):[hmm(4,s0,[])]
[[]]
hmm(4,s1,[]):[hmm(4,s1,[])]
[[]]

Figure 6: Solution table
34. In general, p=n means a predicate p with arity n. So although hmm=1 and hmm=3 share the predicate name
hmm, they are different predicates.
415

fiSato & Kameya

Then after translation, we apply OLDT search to top hmm([a,b,a],Ans) while noting (i) the added D-list does not inuence the OLDT procedure, and (ii) we associate with
each solution of a table atom in the solution table a list of t-explanations. The resulting
solution table is shown in Figure 6. The first row reads that a call to hmm([a,b,a]) occurred and entered the solution table and its solution, hmm([a,b,a]) (no variable binding generated), has two t-explanations, msw(init,once,s0) ^ hmm(1,s0,[a,b,a]) and
msw(init,once,s1) ^ hmm(1,s1,[a,b,a]). The remaining task is the topological sorting of the table atoms stored in the solution table respecting the acyclic support condition.
This can be done by using depth-first search (trace) of t-explanations from the top-goal for
example. Thus we obtain a hierarchical system of t-explanations for hmm([a,b,a]).

4.8 Support Graphs

Looking back, all we need to compute inside and outside probability is a hierarchical system
of t-explanations, which essentially is a boolean combination of primitive events (msw atoms)
and compound events (table atoms) and as such can be more intuitively representable as a
graph. For this reason, and to help visualizing our learning algorithm, we introduce a new
data-structure termed support graphs, though the new EM algorithm in the next subsection
itself is described solely by the hierarchical system of t-explanations.
As illustrated in Figure 7 (a), the support graph for Gt is a graphical representation of
t = h t ;  t ; : : : ;  t i ( t = G ) for G in (20).
the hierarchical system of t-explanations DB
t
t
0 1
0
K
It consists of totally ordered disconnected subgraphs, each of which is labeled with the
t (0  k  K ). A subgraph labeled  t comprises two
corresponding table atom kt in DB
t
k
special nodes (the start node and the end node) and explanation graphs, each corresponding
t in
t
to a t-explanation Sk;j
DB (k ) (1  j  mk ).
t is a linear graph in which a node is labeled either with a
An explanation graph of Sk;j
t . They are called a table node and a switch
table atom  or with a switch msw(1,1,1) in Sk;j
node respectively. Figure 7 (b) is the support graph for hmm([a,b,a]) obtained from the
solution table in Figure 6. Each table node labeled  refers to the subgraph labeled  , so
data-sharing is achieved through the distinct table nodes referring to the same subgraph.
t

e

e

e

e

4.9 Graphical EM Algorithm

We describe here an ecient EM learning algorithm termed the graphical EM algorithm
(Figure 8) introduced by Kameya and Sato (2000), that runs on support graphs. Suppose
we have a random sample G = G1; : : : ; GT of observable atoms. Also suppose support
graphs for Gt (1  t  T ), i.e. hierarchical systems of t-explanations satisfying the acyclic
support condition, the t-exclusiveness condition and the independent condition, have been
successfully constructed from a parameterized logic program DB satisfying the uniqueness
condition and the finite support condition.
The graphical EM algorithm refines learn-naive( ,G ) by introducing two subroutines,
get-inside-probs(
, G ) to compute inside probabilities and get-expectations(
, G ) to compute outside probabilities. They are called from the main routine learn-gEM( ,G ). When
learning, we prepare four arrays for each support graph for Gt in G :
 P [t;  ] for the inside probability of  , i.e. fi ( ) = PDB ( j ) (see (12))
DB

DB

DB

DB

416

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling
k

(a)

explanation graph

msw

Gt:
start

k

msw

end

msw

msw

msw

:
k :
start

end
msw

msw

:

(b)
msw(init,once,s0)

hmm(1,s0,[a,b,a])

hmm([a,b,a]):

start

end

msw(init,once,s1)

msw(out(s0),1,a)

hmm(1,s1,[a,b,a])

msw(tr(s0),1,s0)

hmm(2,s0,[b,a])

hmm(1,s0,[a,b,a]):

end

start

msw(out(s0),1,a)

msw(tr(s0),1,s1)

hmm(2,s1,[b,a])

msw(out(s1),1,a)

msw(tr(s1),1,s0)

hmm(2,s0,[b,a])

hmm(1,s1,[a,b,a]):

end

start

msw(out(s1),1,a)

msw(tr(s1),1,s1)

hmm(2,s1,[b,a])

Figure 7: A support graph (a) in general form, (b) for Gt = hmm([a,b,a]) in the HMM
program DBh. A double-circled node refers to a table node.
 Q[t;  ] for the outside probability of  w.r.t. Gt , i.e. ff(Gt;  ) (see (17) and (18))
 R[t; ; S ] for the explanation probability of S (2 DB (kt )), i.e. PDB (S j )
e

e

417

e

e

fiSato & Kameya
1: procedure learn-gEM (DB; G )
2: begin
3: Select some  as initial
4:
5:
6:
7:
8:
9:

1: procedure get-inside-probs (DB; G )
2: begin
3: for t := 1 to T do begin
4:
Let 0t = Gt;
5:
for k := Kt downto 0 do begin
6:
P [t; kt ] := 0;
7:
foreach Se 2 eDB (kt ) do begin
8:
Let Se = fA1 ; A2 ; : : : ; AjSejg;
9:
R[t; kt ; Se] := 1;
10:
for l := 1 to jSej do
11:
if Al = msw(i,1,v ) then
12:
R[t; kt ; Se] 3 = i;v
13:
else R[t; kt ; Se] 3 = P [t; Al ];
14:
P [t; kt ]+= R[t; kt ; Se]
15:
end /* foreach Se */
16:
end /* for k */
17: end /* for t */
18: end.

parameters;

get-inside-probs
(DB; G);
P
(0) := Tt=1 ln P [t; Gt ];
repeat

get-expectations (DB; G );
i 2 I; v 2 Vi do
 [i; vP
] :=
T [t; i; v]=P [t; G ];
t
t=1
foreach i 2 I; v P
2 Vi do
i;v := [i; v]= v0 2Vi  [i; v0 ];
get-inside-probs (DB; G );
m := m +
1;
P
(m) := Tt=1 ln P [t; Gt ]
until (m) 0 (m01) < "
foreach

10:
11:
12:
13:
14:
15:
16: end.

1: procedure get-expectations (DB; G ) begin
2: for t := 1 to T do begin
3:
foreach i 2 I; v 2 Vi do  [t; i; v] := 0;
4:
Let 0t = Gt; Q[t; 0t] := 1:0;
5:
for k := 1 to Kt do Q[t; kt ] := 0;
6:
for k := 0 to Kt do
7:
foreach Se 2 eDB (kt ) do begin
8:
Let Se = fA1 ; A2 ; : : : ; AjSejg;
9:
for l := 1 to jSej do
10:
if Al = msw(i,1,v ) then  [t; i; v] += Q[t; kt ] 1 R[t; kt ; Se]
11:
else Q[t; Al ] += Q[t; kt ] 1 R[t; kt ; Se]=P [t; Al ]
12:
end /* foreach Se */
13: end /* for t */
14: end.

Figure 8: graphical EM algorithm.
 [t; i; v] for the expected count of msw(i,1,v), i.e.

P

S2

DB

(Gt) Pmsw (S j )i;v (S )

and call the procedure learn-gEM( ,G) in Figure 8. The main routine learn-gEM( ,G) initially computes all inside probabilities (Line 4) and enters a loop in which get-expectations( ,G )
is called first to compute the expected count [t; i; v] of msw(i,1,v) and parameters are updated (Line 11). Inside probabilities are renewed by using the updated parameters before
entering the next loop (Line 12).
DB

DB

DB

418

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

The subroutine get-inside-probs( ,G ) computes the inside probability fi ( ) = PDB ( j )
(and stores it in P [t;  ]) of a table atom  from the bottom layer up to the topmost layer 0 =
Gt (Line 4) of the hierarchical system of t-explanations for Gt (see (20) in Subsection 4.6).
It takes a t-explanation S in DB (kt ) one by one (Line 7), decomposes S into conjuncts and
multiplies their inside probabilities which are either known (Line 12) or already computed
(Line 13).
The other subroutine get-expectations( ,G ) computes outside probabilities following the
recursive definitions (17) and (18) in Subsection 4.6 and stores the outside probability
ff(Gt;  ) of a table atom  in Q[t;  ]. It first sets the outside probability of the top-goal
0 = Gt to 1:0 (Line 4) and computes the rest of outside probabilities (Line 6) going down
the layers of the t-explanation for Gt described by (20) in Subsection 4.6. (Line 10) adds
Q[t; kt ] 1 R[t; kt ; S ] = ff(Gt ; kt ) 1 fi(S ) to [t; i; v], the expected count of msw(i,1,v), as a
contribution of msw(i,1,v) in S through kt to [t; i; v]. (Line 11) increments the outside
probability Q[t; Al ] = ff(Gt; Al ) of Al according to the equation (18). Notice that Q[t; kt ]
has already been computed and R[t; kt ; S]=P [t; Al ] = fi(W ) for S = Al ^ W . As shown in
Subsection 4.5, learn-naive( ,G ) is the MLE procedure, hence the following theorem holds.
Theorem 4.1 Let DB be a parameterized logic program, and G = G1; : : : ; GT a ranDB

e

e

e

DB

e

e

e

e

e

DB

dom sample of observable atoms. Suppose the five conditions (uniqueness, finite support
(Subsection 4.2), acyclic support, t-exclusiveness and independence (Subsection 4.7)) are
met. ThenQlearn-gEM (DB; G ) finds the MLE 3 which (locally) maximizes the likelihood
L(G j ) = Tt=1 PDB (Gt j  ).

(Proof) Sketch.35 Since the main routine learn-gEM( ,G ) is the same as learn-naive( ,G)
except the computation of [i; v] = Tt=1 [t; i; v], we show that [t; i; v] = S2 (G ) Pmsw(S j
)i;v (S ) (= n msw(i,n,v )2S2 (G ) Pmsw (S j )). However,
DB

P

P

DB

P

DB

[t; i; v]

DB

P

=

X

X

0kKt

t

t

X

n msw(i,n,v )2Se2 e ( t )
DB k

ff(Gt ; kt )fi (Se)

(see (Line 10) in get-expectations(DB; G))
= ff(Gt; msw(i,n,v))fi(msw(i,n,v))
n
= (Gt; msw(i,n,v)) (see the equation (16))
n
=
Pmsw (S j  ):
Q.E.D.
X

X

X

X

n msw(i,n,v )2S2

DB

(Gt )

Here we used the fact that if S contains msw(i,n,v) like S = S0 ^ msw(i,n,v), fi(S) =
fi (S 0 )fi (msw(i,n,v )) holds, and hence
ff(Gt ; kt )fi (S ) = ff(Gt ; kt )fi (S 0 )fi (msw(i,n,v ))
= (contribution of msw(i,n,v) in S through kt to ff(Gt; msw(i,n,v)))fi (msw(i,n,v)):
e

e

e

e

e

e

e

e

35. A formal proof is given by Kameya (2000). It is proved there that under the common parameters ,  [i; v]
in learn-naive(DB,G ) coincides with [i; v] in learn-gEM(DB,G ). So, the parameters are updated to
the same values. Hence, starting with the same initial values, the parameters converge to the same
values.
419

fiSato & Kameya

The five conditions on the applicability of the graphical EM algorithm may look hard
to satisfy at once. Fortunately, the modeling principle in Section 4.3 still stands, and with
due care in modeling, it is likely to lead us to a program that meets all of them. Actually,
we will see in the next section, programs for standard symbolic-statistical frameworks such
as Bayesian networks, HMMs and PCFGs all satisfy the five conditions.
5. Complexity

In this section, we analyze the time complexity of the graphical EM algorithm applied
to various symbolic-statistical frameworks including HMMs, PCFGs, pseudo PCSGs and
Bayesian networks. The results show that the graphical EM algorithm is competitive with
these specialized EM algorithms developed independently in each research field.

5.1 Basic Property

Since the EM algorithm is an iterative algorithm and since we are unable to predict when
it converges, we measure time complexity by the time taken for one iteration. We therefore
estimate time per iteration on the repeat loop of learn-gEM (DB; G) (G = G1 ; : : : ; GT ). We
observe that in one iteration, each support graph for Gt (1  t  T ) is scanned twice, once
by get-inside-probs (DB; G ) and once by get-expectations (DB; G). In the scan, addition is
performed on the t-explanations, and multiplication (possibly with division) is performed
on the msw atoms and table atoms once for each. So time spent for Gt per iteration by the
graphical EM algorithm is linear in the size of the support graph, i.e. the number of nodes
in the support graph for Gt. Put
1tDB def
=
DB ( )
e

[

e

t
 2DB

num def
= 1max
j1e t j
tT DB
maxsize def
=
max
jSej:
t
e
e
1tT;S 21DB
t is the set of table atoms for G , and hence 1t is the set of all t-explanations
Recall that DB
t
DB
appearing in the right hand side of (20) in Subsection 4.7. So num is the maximum number
of t-explanations in a support graph for the Gt's and maxsize the maximum size of a texplanation for the Gt's respectively. The following is obvious.
e

Proposition 5.1 The time complexity of the graphical EM algorithm per iteration is linear

in the total size of support graphs, O (nummaxsize T ) in notation, which coincides with the
space complexity because the graphical EM algorithm runs on support graphs.

This is a rather general result, but when we compare the graphical EM algorithm with
other EM algorithms, we must remember that the input to the graphical EM algorithm is
support graphs (one for each observed atom) and our actual total learning time is
OLDT time + (the number of iterations) 2O(nummaxsizeT )
420

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

where \OLDT time" denotes time to construct all support graphs for G . It is the sum of
time for OLDT search and time for the topological sorting of the table atoms, but because
the latter is part of the former order-wise,36 we represent \OLDT time" by time for OLDT
search. Also observe that the total size of support graphs does not exceed time for OLDT
search for G order-wise.
To evaluate OLDT time for a specific class of models such as HMMs, we need to know
time for table operations. Observe that our OLDT search in this paper is special in the
sense that table atoms are always ground when called and there is no resolution with solved
goals. Accordingly a solution table is used only
 to check if a goal G already has an entry in the solution table, i.e. if it was called
before, and
 to add a new searched t-explanation for G to the list of discovered t-explanations
under G's entry.
The time complexity of these operations is equal to that of table access which depends
both on the program and on the implementation of the solution table.37 We first suppose
programs are carefully written in such a way that the arguments of table atoms used as indecies for table access are integers. Actually all programs used in the subsequent complexity
analysis (DBh in Subsection 4.7, DBg and DBg0 in Subsection 5.3, DBG in Subsection 5.5)
satisfy or can satisfy this condition by replacing non-integer terms with appropriate integers. We also suppose that the solution table is implemented using an array so that table
access can be done in O(1) time.38
In what follows, we present a detailed analysis of the time complexity of the graphical
EM algorithm applied to HMMs, PCFGs, pseudo PCSGs and Bayesian networks, assuming
O(1) time access to the solution table. We remark by the way that their space complexity
is just the total size of solution tables (support graphs).


5.2 HMMs

The standard EM algorithm for HMMs is the Baum-Welch algorithm (Rabiner, 1989; Rabiner & Juang, 1993). An example of HMM is shown in Figure 3 in Subsection 4.7.39 Given
T observations w1 ; : : : ; wT of output string of length L, it computes in O (N 2 LT ) time in
each iteration the forward probability fftm(q) = P (ot1 ot2 1 11 otm01; q j ) and the backward
probability fimt (q) = P (otm otm+1 1 1 1 otL j q; ) for each state q 2 Q, time step m (1  m  L)
and a string wt = ot1 ot2 11 1 otL (1  t  T ), where Q is the set of states and N the number of
states. The factor N 2 comes from the fact that every state has N possible destinations and

36. Think of OLDT search for a top-goal Gt . It searches for msw atoms and table atoms to create a solution table, while doing some auxiliary computations. Therefore its time complexity is never less
than O(jthe number of msw atoms and table atoms in the support graph for Gt j), which coincides with
the time we need to topologically sort table atoms in the solution table by depth-first search from 0 = Gt .
37. Sagonas et al. (1994) and Ramakrishnan et al. (1995) discuss about the implementation of OLDT.
38. If arrays are not available, we may be able to use balanced trees, giving O(log n) access time where n
is the number data in the solution table, or we may be able to use hashing, giving average O (1) time
access under a certain condition (Cormen, Leiserson, & Rivest, 1990).
39. We treat here only \state-emission HMMs" which emit a symbol depending on the state. Another type,
\arc-emission HMMs" in which the emitted symbol depends on the transition arc, is treated similarly.
421

fiSato & Kameya

we have to compute the forward and backward probability for every destination and every
state. After computing all ffmt (q)'s and fimt (q)'s, parameters are updated. So, the total
computation time in each iteration of the Baum-Welch algorithm is estimated as O(N 2LT )
(Rabiner & Juang, 1993; Manning & Schutze, 1999).
To compare this result with the graphical EM algorithm, we use the HMM program
DBh in Figure 4 with appropriate modifications to L, the length of a string, Q, the
state set, and declarations in Fh for the output alphabets. For a string w = o1o2 11 1 oL,
hmm(n,q ,[om ; om+1 ; : : : ; oL ]) in DBh reads that the HMM is in state q 2 Q at time n and
has to output [om ,om+1 ,...,oL] until it reaches the final state. After declaring hmm=1 and
hmm=3 as table predicate and translation (see Figure 5), we apply OLDT search to the goal
top hmm([o1,...,oL ],Ans) w.r.t. the translated program to obtain all t-explanations
for hmm([o1,...,oL]). For a complexity argument however, the translated program and
DBh are the same, so we talk in terms of DBh for the sake of simplicity. In the search,
we fix the search strategy to multi-stage depth-first strategy (Tamaki & Sato, 1986). We
assume that the solution table is accessible in O(1) time.40 Since the length of the list in
the third argument of hmm=3 decreases by one on each recursion, and there are only finitely
many choices of the state transition and the output alphabet, the search terminates, leaving
finitely many t-explanations in the solution table like Figure 6 that satisfy the acyclic support condition respectively. Also the sampling execution of hmm(L) w.r.t. DBh is nothing
but a sequential decision process such that decisions made by msw atoms are exclusive,
independent and generate a unique string, which means DBh satisfies the t-exclusiveness
condition, the independence condition and the uniqueness condition respectively. So, the
graphical EM algorithm is applicable to the set of hierarchical systems of t-explanations for
hmm(wt ) (1  t  T ) produced by OLDT search for T observations w1 ; : : : ; wT of output
string. Put wt = ot1 ot2 1 11 otL. It follows from
t
DB
= fhmm(m,q,[otm ,...,otL]) j 1  m  L + 1; q 2 Qg [ fhmm([ot1,...,otL])g
h

DBh

(

(

msw(out(q),m,om ); msw(tr(q),m,q 0);
hmm(m + 1,q0 ,[otm+1 ,...,otL ])

fi
fi
fi
fi
fi

)

) =
(1  m  L)
that for a top-goal hmm([ot1 ,...,otL]), there are at most O(NL) calling patterns of hmm=3
and each call causes at most N calls to hmm=3, implying there occur O(NL 1 N ) = O(N 2L)
calls to hmm=3. Since each call is computed once due to the tabling mechanism, we have
num = O(N 2 L). Also maxsize = 3. Applying Proposition 5.1, we reach
e

hmm(m,q ,[otm ,...,otL ])

q0 2 Q

Proposition 5.2 Suppose we have T strings of length L. Also suppose
each table operation
2

in OLDT search is done in O (1) time. OLDT time by DBh is O(N LT ) and the graphical
EM algorithm takes O(N 2 LT ) time per iteration where N is the number of states.

O(N 2LT ) is the time complexity of the Baum-Welch algorithm.
algorithm runs as eciently as the Baum-Welch algorithm.41

So the graphical EM

40. O(1) is possible because in the translated program DBh in Section 4.7, we can identify a goal pattern of
hmm(1,1,1,1,1) by the first two arguments which are constants (integers).
41. Besides, the Baum-Welch algorithm and the graphical EM algorithm whose input are support graphs
generated by DBh update parameters to the same value if initial values are the same.
422

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

By the way, the Viterbi algorithm (Rabiner, 1989; Rabiner & Juang, 1993) provides for
HMMs an ecient way of finding the most likely transition path for a given input/output
string. A similar algorithm for parameterized logic programs that determines the most
likely explanation for a given goal can be derived. It runs in time linear in the size of the
support graph, thereby O(N 2 L) in the case of HMMs, the same complexity as the Viterbi
algorithm (Sato & Kameya, 2000).

5.3 PCFGs

We now compare the graphical EM algorithm with the Inside-Outside algorithm (Baker,
1979; Lari & Young, 1990). The Inside-Outside algorithm is a well-known EM algorithm
for PCFGs (Wetherell, 1980; Manning & Schutze, 1999).42 It takes a grammar in Chomsky
normal form. Given N nonterminals, a production rule in the grammar takes the form
i ! j; k (1  i; j; k  N ) (nonterminals are named by numbers from 1 to N and 1 is a
starting symbol) or the form i ! w where 1  i  N and w is a terminal. In each iteration,
it computes the inside probability and the outside probability of every partial parse tree
of the given sentence to update parameters for these production rules. Time complexity is
measured by time per iteration, and is described by N , the number of nonterminals, and
L, the number of terminals in a sentence. It is O(N 3 L3T ) for T observed sentences (Lari
& Young, 1990).
To compare the graphical EM algorithm with the Inside-Outside algorithm, we start
from a propositional program DBg = Fg [ Rg below representing the largest grammar
containing all possible rules i ! j; k in N nonterminals where nonterminal 1 is a starting
symbol, i.e. sentence.
Fg
Rg

d,d0],[j ,k]) j 1  i; j; k  N; d; d0 are numbersg
= fmsw([if,[msw(
i,d,w ) j 1  i  N; d is a number; w is a terminalg

=

8
>
<
>
:

S

q(i,d0,d2 ) :- msw(i,[d0,d2 ],[j ,k ]),
q(j ,d0,d1 ),
q(k ,d1 ,d2).
n

q(i,d,d +1) :- msw(i,d,wd+1 ).

fi
fi
fi

fi
fi
fi
fi
fi
fi
fi

1  i; j; k  N;
0  d0 < d1 < d2  L

1  i  N; 0  d  L 0 1

9
>
=
>
;

o

Figure 9: PCFG program DBg
DB g is an artificial parsing program whose sole purpose is to measure the size of an
OLDT tree43 created by the OLDT interpreter when it parses a sentence w1 w2 1 11 wL. So
42. A PCFG (probabilistic context free grammar) is a backbone CFG with probabilities (parameters) assigned to each production rule. For a nonterminal A having
n production rules fA ! ffi j 1  i  ng, a
P
probability pi is assigned to A ! ffi (1  i  n) where ni=1 pi = 1. The probability of a sentence s is
the sum of probabilities of each (leftmost) derivation of s. The latter is the product of probabilities of
rules used in the derivation.
43. To be more precise, an OLDT structure, but in this case, it is a tree because DBg contains only constants
(Datalog program) and there never occurs the need of creating a new root node.
423

fiSato & Kameya
(1)

Td

q(1,d,L)
2 j N

2 k N
q(1,d,d+1),
q(1,d+1,L)

q(1,d+1,L)
(1)

[Note] q
1 i

2 k N

q(1,d,d+1),
q(k,d+1,L)

q(j,d,d+1),
q(1,d+1,L)

q(j,d,d+1),
q(k,d+1,L)

q(k,d+1,L)

q(1,d+1,L)

q(k,d+1,L)

(k)

Td+1
q(i,d,d) already appears
d+1 d d L, 1 i
d-d L-d-2

Td+1

d+2 e L-1
1 j N

2 k N
q(j,d,e),
q(1,e,L)

q(j,d,e),
q(k,e,L)

d+2 e e
1 i N,
1 j N,

q(i,d,e),
q(j,e,e),
q(1,e,L)

Td

q(k,e,L)

N

q

N

q(j,e,e),
q(1,e,L)
q(1,e,L)

...
p(i)

p(1) p(2) p(N)

Figure 10: OLDT tree for the query

q(1,d,L)

the input sentence w1 w2 11 1 wL is embedded in the program separately as msw(i,d,wd+1)
(0  d  L0 1) in the second clauses of Rg (this treatment does not affect the complexity argument). q(i,d0,d1) reads that the i-th nonterminal spans from position d0 to position d1,
i.e. the substring wd +1 1 11 wd . The first clauses q(i,d0 ,d2 ) :- msw(1,1,1), q(j ,d0 ,d1),
q(k,d1 ,d2) are supposed to be textually ordered according to the lexicographic order for
tuples hi; j; k; d0 ; d2 ; d1i. As a parser, the top-goal is set to q(1,0,L).44 It asks the
parser to parse the whole sentence w1 w2 1 1 1 wL as the syntactic category \1" (sentence).
We make an exhaustive search for this query by OLDT search.45 As before, the multistage depth-first search strategy and O(1) time access to the solution table are assumed.
Then the time complexity of OLDT search is measured by the number of nodes in the
OLDT tree. Let Td(k) be the OLDT tree for q(k,d,L). Figure 10 illustrates Td(1) for d
(0  d  L 0 3) where msw atoms are omitted. As can be seen, the tree has many similar
subtrees, so we put them together (see Note in Figure 10). Due to the depth-first strategy,
Td(1) has a recursive structure and contains Td(1)
+1 as a subtree. Nodes whose leftmost atom
is not underlined are solution nodes, i.e. they solve their leftmost atoms for the first time in
the entire refutation process. The underlined atoms are already computed in the subtrees
to their left.46 They only check the solution table if there are their entries (= already
0

1

44. L here is not a Prolog variable but a constant denoting the sentence length.
45. q is a table predicate.
0 00
0
46. It can be inductively proved that Td(1)
+1 contains every computed q(i,d ,d ) (0  d  L 0 3; d + 1  d <
d00  L; 1  i  N; d00 0 d0  L 0 d 0 2).
424

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

computed) in O(1) time. Since all clauses are ground, such execution only generates a
single child node.
(1)
(k)
We enumerate h(1)
d , the number of nodes in Td but not in Td+1 (1  k  N ). From
(k)
3
2 47
Figure 10, we see h(1)
d = O(N (L 0 d) ). Let hd (2  k  N ) be the number of nodes in
k)
Td(+1
not contained in Td(1)+1. It is estimated as O(N 2 (L 0 d 0 2)). Consequently, the number
(k)
N
3
2
of nodes that are newly created in Td(1) is h(1)
d + k=2 hd = O (N (L 0 d) ). As a result,
L
0
3
3
3
48
total time for OLDT search is computed as d=0 hd = O(N L ) which is also the size of
the support graph.
We now consider a non-propositional parsing program DBg0 = Fg0 [ Rg0 in Figure 11
whose ground instances constitute the propositional program DBg . DBg0 is a probabilistic
variant of DCG program (Pereira & Warren, 1980) in which q'/1, q'/6 and between/3 are
declared as table predicate. Semantically DBg0 specifies a probability distribution over the
atoms of the form fq'(l) j l is a list of terminalsg.
P

P

Fg0

Rg0

t,[sj ,sk ]) j 1  i; j; k  N; t is a numberg
= fmsw([sfi,msw(
si ,t,w) j 1  i  N; t is a number; w is a terminalg

=

8
>
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
>
:

q'(S) :- length(S,D), q'(s1 ,0,D,0, ,S-[]).
q'(I,D0,D2,C0,C2,L0-L2) :- between(D0,D1,D2),
msw(I,C0,[J,K]),
q'(J,D0,D1,s(C0),C1,L0-L1),
q'(K,D1,D2,s(C1),C2,L1-L2).
q'(I,D,s(D),C0,s(C0),[W|X]-X) :- msw(I,C0,W).

Figure 11: Probabilistic DCG like program DBg0
The top-goal to parse a sentence S = [w1; : : : ; wL] is q'([w1; : : : ; wL]). It invokes
q'(s1 ,0,D,0, ,[w1 ,: : :,wL ]-[]) after measuring the length D of an input sentence S by
calling length=2. 49 50 In general, q'(i,d0,d2 ,c0 ,c2 ,l0-l2) works identically to q(i,d0 ,d2 )
but three arguments, c0 , c2 and l0-l2, are added. c0 supplies a unique trial-id for msws to be
used in the body, c2 the latest trial-id in the current computation, and l0 -l2 a D-list holding
a substring between d0 and d2 . Since the added arguments do not affect the shape of the

47. We here focus on the subtree Td0 . j , i0 and j 0 range from 1 to N , and fif(e; e0 ) j d + 2  e0 < e  L 0 1 gfi =
O((L 0 d)2 ). Hence, the number of nodes in Td0 is O(N 3 (L 0 d)2 ). The number of nodes in Td(1) but
(1)
0
neither in Td(1)
= O (N 3 (L 0 d)2 ).
+1 nor in Td is negligible, therefore hd
(1)
(1)
48. The number of nodes in TL01 and TL02 is negligible.
49. To make the program as simple as possible, we assume that an integer n is represented by a ground term
fi

(n)
z }| {

fi

s(1 1 1s (0)1 1 1). We also assume that when D0 and D2 are ground, the goal between(D0, D1, D2)
returns an integer D1 between them in time proportional to jD1 0 D0j.
50. We omit an obvious program for length(l,sn ) which computes the length sn of a list l in O(jlj) time.

sn =

def

425

fiSato & Kameya

search tree in Figure 10 and the extra computation caused by length=2 is O(L) and the
one by the insertion of between(D0,D1,D2) is O(NL3) respectively,51 OLDT time remains
O(N 3 L3), and hence so is the size of the support graph.
To apply the graphical EM algorithm correctly, we need to confirm the five conditions
on its applicability. It is rather apparent however that the OLDT refutation of any topgoal of the form q'([w1 ,: : :,wL]) w.r.t. DBg0 terminates, and leaves a support graph
satisfying the finite support condition and the acyclic support condition. The t-exclusiveness
condition and the independent condition also hold because the refutation process faithfully
simulates the leftmost stochastic derivation of w1 11 1 wL in which the choice of a production
rule made by msw(si ,sc,[sj ,sk ]) is exclusive and independent (trial-ids are different on
different choices).
What remains is the uniqueness condition. To confirm it, let us consider another program DBg00 , a modification of DBg0 such that the first goal length(S,D) in the body of the
first clause and the first goal between(D0,D1,D2) in the second clause of Rg0 are moved to
the last position in their bodies respectively. DBg00 and DBg0 are logically equivalent, and
semantically equivalent as well from the viewpoint of distribution semantics. Then think of
the sampling execution by the OLDT interpreter of a top-goal q'(S) w.r.t. DBg00 where
S is a variable, using the multi-stage depth-first search strategy. It is easy to see first that
the execution never fails, and second that when the OLDT refutation terminates, a sentence
[w1 ; : : : ; wL ] is returned in S, and third that conversely, the set of msw atoms resolved upon
in the refutation uniquely determines the output sentence [w1 ; : : : ; wL].52 Hence, if the
sampling execution is guaranteed to always terminate, every sampling from PF 00 (= PF 0 )
uniquely generates a sentence, an observable atom, so the uniqueness condition is satisfied
by DBg00 , and hence by DBg0 .
Then when is the sampling execution guaranteed to always terminate? In other words,
when does the grammar only generate finite sentences? Giving a general answer seems
dicult, but it is known that if the parameter values in a PCFG are obtained by learning
from finite sentences, the stochastic derivation by the PCFG terminates with probability
one (Chi & Geman, 1998). In summary, assuming appropriate parameter values, we can
say that the parameterized logic program DBg0 for the largest PCFG with N nonterminal
symbols satisfies all applicability conditions, and the OLDT time for a sentence of length
L is O(N 3 L3)53 and this is also the size of the support graph. From Proposition 5.1, we
conclude
g

g

Proposition 5.3 Let DB be a0 parameterized logic program representing a PCFG with N

nonterminals in the form of DBg in Figure 11, and G = G1 ; G2 ; : : : ; GT be the sampled atoms
representing sentences of length L. We suppose each table operation in OLDT search is done
in O(1) time. Then OLDT search for G and one iteration in learn-gEM are respectively
done in O(N 3 L3T ) time.

51.

between(D0,D1,D2)

is called O(N (L 0 d)2 ) times in Td(1) . So it is called

0 O(N (L 0 d)2 ) = O(NL3 )

PL 3
d=0

times in T .
52. Because the trial-ids used in the refutation record which rule is used at what step in the derivation of
w1 1 1 1 wL .
53. In DBg , we represent integers by ground terms made out of 0 and s(1) to keep the program short. If
we use integers instead of ground terms however, the first three arguments of q'(1,1,1,1,1,1) are enough
to check whether the goal is previously called or not, and this check can be done in O(1) time.
(1)
0

0

426

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling
O (N 3 L 3 T )

is also the time complexity of the Inside-Outside algorithm per iteration
(Lari & Young, 1990), hence our algorithm is as ecient as the Inside-Outside algorithm.

5.4 Pseudo PCSGs

PCFGs can be improved by making choices context-sensitive, and one of such attempts is
pseudo PCSGs (pseudo probabilistic context sensitive grammars) in which a rule is chosen probabilistically depending on both the nonterminal to be expanded and its parent
nonterminal (Charniak & Carroll, 1994).
A pseudo PCSG is easily programmed. We add one extra-argument, N, representing
the parent node, to the predicate q'(I,D0,D2,C0,C2,L0-L2) in Figure 11 and replace
msw(I,C0,[J,K]) with msw([N,I],C0,[J,K]). Since the (leftmost) derivation of a sentence
from a pseudo PCSG is still a sequential decision process described by the modified program,
the graphical EM algorithm applied to the support graphs generated from the modified
program and observed sentences correctly performs the ML estimation of parameters in the
pseudo PCSG.
A pseudo PCSG is thought to be a PCFG with rules of the form [n; i] ! [i; j ][i; k]
(1  n; i; j; k  N ) where n is the parent nonterminal of i, so the arguments in the previous
subsection are carried over with minor changes. We therefore have (details omitted)

Proposition 5.4 Let DB be a parameterized logic program for a pseudo PCSG with N

nonterminals as shown above, and G = G1; G2 ; : : : ; GT the observed atoms representing
sampled sentences of length L. Suppose each table operation in OLDT search can be done
in O(1) time. Then OLDT search for G and each iteration in learn-gEM is completed in
O(N 4 L3T ) time.

5.5 Bayesian Networks

A relationship between cause C and its effect E is often probabilistic such as the one between diseases and symptoms, and as such it is mathematically captured as the conditional
probability P (E = e j C = c) of effect e given the cause c. What we wish to know however is
the inverse, i.e. the probability of a candidate cause c given evidence e, i.e. P (C = c j E = e)
which is calculated by Bayes' theorem as P (E = e j C = c)P (C = c)= c0 P (E = e j C =
c0)P (C = c0 ). Bayesian networks are a representational/computational framework that fits
best this type of probabilistic inference (Pearl, 1988; Castillo et al., 1997).
A Bayesian network is a graphical representation of a joint distribution P (X1 = x1 ; : : : ;
XN = xN ) of finitely many random variables X1 ; : : : ; XN . The graph is a dag (directed
acyclic graph) such as ones in Figure 12, and each node is a random variable.54
In the graph, a conditional probability table (CPT) representing P (Xi = xi j 5i = ui)
(1  i  N ) is associated with each node Xi where 5i represents Xi's parent nodes and ui
their values. When Xi has no parent, i.e. a topmost node in the graph, the table is just a
marginal distribution P (Xi = xi). The whole joint distribution is defined as the product of
P

54. We only deal with discrete cases.
427

fiSato & Kameya

A

B

A

D

C

B

C

E

D

F

E

( G 1 ) Singly-connected

F

( G 2 ) Multiply-connected

Figure 12: Bayesian networks
these conditional distributions:
P (X1 = x1 ; : : : ; XN

= xN )55 =

N
Y
i=1

P (Xi = xi j 5i = ui ):

(21)

Thus the graph G1 in Figure 12 defines
PG (a; b; c; d; e; f ) = PG (a)PG (b)PG (c j a)PG (d j a; b)PG (e j d)PG (f j d)
where a, b, c, d, e and f are values of corresponding random variables A, B, C , D, E
and F , respectively.56 As mentioned before, one of the basic tasks of Bayesian networks
is to compute marginal probabilities. For example, the marginal distribution PG (c; d) is
computed either by (22) or (23) below.
PG (c; d) =
PG (a)PG (b)PG (c j a)PG (d j a; b)PG (e j d)P (f j d)
(22)
1

1

1

1

1

1

1

1

X

1

a;b;e;f

1

1

1

1

0

=

X
@

a;b

1

10

PG1 (a)PG1 (b)PG1 (c j a)PG1 (d j a; b)A @

1
X

e;f

PG1 (e j d)PG1 (f j d)A (23)

(23) is clearly more ecient than (22). Observe that if the graph were like G2 in
Figure 12, there would be no way to factorize computations like (23) but to use (22) requiring
exponentially many operations. The problem is that computing marginal probabilities is
NP-hard in general, and factorization such as (23) is assured only when the graph is singly
connected like G1 , i.e. has no loop when viewed as an undirected graph. In such case, the
computation is possible in O(jV j) time where V is the set of vertices in the graph (Pearl,
1988). Otherwise, the graph is called multiply-connected, and might need exponential time
to compute marginal probabilities. In the sequel, we show the following.
 For any discrete Bayesian network G defining a distribution PG (x1 ; : : : ; xN ), there is a
parameterized logic program DBG for a predicate bn(1) such that PDB (bn(x1,: : :,xN ))
= PG(x1 ; : : : ; xN ).
G

55. Thanks to the acyclicity of the graph, without losing generality, we may assume that if Xi is an ancestor
node of Xj , then i < j holds.
56. For notational simplicity, we shall omit random variables when no confusion arises.
428

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

 For arbitrary factorizations and their order to compute a marginal distribution, there

exists a tabled program that accomplishes the same computation in the specified way.
 When the graph is singly connected and evidence e is given, there exists a tabled
program DBG such that OLDT time for bn(e) is O(jV j), and hence the time
complexity per iteration of the graphical EM algorithm is O(jV j) as well.
Let G be a Bayesian network defining a joint distribution PG(x1 ; : : : ; xN ) and fPG (Xi =
xi j 5i = ui ) j 1  i  N; xi 2 val(Xi ); ui 2 val(5i )g the conditional probabilities
associated with G where val(Xi) is the set of Xi's possible values and val(5i) denotes
the set of possible values of the parent nodes 5i as a random vector, respectively. We
construct a parameterized logic program that defines the same distribution PG (x1 ; : : : ; xN ).
Our program DBG = FG [ RG is shown in Figure 13.


FG

= f msw(par(i,ui),once,xi) j 1  i  N; ui 2 val(5i); xi 2 val(Xi) g

RG

= f bn(X1 ,: : :,XN ):-

5i),once,Xi). g

VN

i=1 msw(par(i,

Figure 13: Bayesian network program DBG
FG is comprised of msw atoms of the form msw(par(i,ui ),once,xi ) whose probability is
exactly the conditional probability PG (Xi = xi j 5i = ui). When Xi has no parents, ui is
the empty list []. RG is a singleton, containing only one clause whose body is a conjunction
of msw atoms which corresponds to the product of conditional probabilities. Note that we
intentionally identify random variables X1 ; : : : ; XN with logical variables X1 ; : : : ; XN for
convenience.

Proposition 5.5 DBG denotes the same distributions as G.

(Proof) Let hx1 ; : : : ; xN i be a realization of the random vector hX1; : : : ; XN i. It holds by
construction that
PDBG (bn(x1 ,: : :,xN ))

=

N
Y
h=1
N
Y

Pmsw (msw(par(i,ui ),once,xi ))

=
PG (Xi = xi j 5i = ui )
h=1
= PG (x1 ; : : : ; xN ):
Q:E:D:
In the case of G1 in Figure 12, the program becomes57
bn(A,B,C,D,E,F)

:-

msw(par('A',[]),once,A),
msw(par('C',[A]),once,C),
msw(par('E',[D]),once,E),

57. 0 A0 ; 0 B0 ; : : : are Prolog constants used in place of integers.
429

msw(par('B',[]),once,B),
msw(par('D',[A,B]),once,D),
msw(par('F',[D]),once,F).

fiSato & Kameya

and the left-to-right sampling execution gives a sample realization of the random vector
h A; B; C; D; E; F i. A marginal distribution is computed from bn(x1 ,: : :,xN ) by adding a new
clause to DBG. For example, to compute PG (c; d), we add bn(C,D):- bn(A,B,C,D,E,F)
to DBG (let the result be DB0G ) and then compute PDB0 (bn(c,d)) which is equal to
PG (c; d) because
PDB0 (bn(c,d)) = PDB (9 a; b; e; f bn(a,b,c,d,e,f ))
=
PDB (bn(a,b,c,d,e,f ))
a;b;e;f
= PG (c; d):
Regrettably this computation corresponds to (22), not to the factorization (23). Ecient
probability computation using factorization is made possible by carrying out summations
in a proper order.
We next sketch by an example how to carry out specified summations in a specified
order by introducing new clauses. Suppose we have a joint distribution P (x; y; z; w) =
1 (x; y )2 (y; z; w)3(x; z; w) such that 1(x; y ), 2(y; z; w) and 3 (x; z; w) are respectively
computed by atoms p1 (X,Y), p2 (Y,Z,W) and p3 (X,Z,W). Suppose also that we hope to
compute the sum
P (x) = 1(x; y )
2 (y; z; w )3 (x; z; w)
1

1

1

G1

1

G1

X

G1

G1

1

X

X

y

z;w

!

in which we first eliminate z; w and then y. Corresponding to each elimination, we introduce
two new predicates, q(X,Y) to compute 4 (x; y) = z;w 2 (y; z; w)3 (x; z; w) and p(X) to
compute P (x) = y 1 (x; y)4(x; y) as follows.
P

P

p(X)
q(X,Y)

::-

p1(X,Y), q(X,Y).
p2(Y,Z,W), p3 (X,Z,W).

Note that the clause body of q=2 contains Z and W as (existentially quantified) local variables
and the clause head q(X,Y) contains variables shared with other atoms. In view of the
correspondence between and 9, it is easy to confirm that this program realizes the
required computation. It is also easy to see by generalizing this example, though we do
not prove here, that there exists a parameterized logic program that carries out the given
summations in the given order for an arbitrary Bayesian network, in particular we are
able to simulate VE (variable elimination, Zhang & Poole, 1996; D'Ambrosio, 1999) in our
approach.
Ecient computation of marginal distributions is not always possible but there is a
well-known class of Bayesian networks, singly connected Bayesian networks, for which there
exists an ecient algorithm to compute marginal distributions by message passing (Pearl,
1988; Castillo et al., 1997). We here show that when the graph is singly connected, we can
construct an ecient tabled Bayesian network program DBG assigning a table predicate
to each node. To avoid complications, we explain the construction procedure informally
and concentrate on the case where we have only one interested variable. Let G be a singly
P



430

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

connected graph. First we pick up a node U whose probability PG (u) is what we seek. We
construct a tree G with the root node U from G, by letting other nodes dangling from U .
Figure 14 shows how G1 is transformed to a tree when we select node B as the root node.
B

D

A

E

F

C


Transformed graph G1

Figure 14: Transforming G1 to a tree
Then we examine each node in G one by one. We add for each node X in the graph a
corresponding clause to DBG whose purpose is to visit all nodes connected to X except the
one that calls X . Suppose we started from the root node U1 in Figure 15 where evidence
u is given, and have generated clause (24). Now we proceed to an inner node X (U1 calls
X ). In the original graph G, X has parent nodes fU1 ; U2; U3g and child nodes fV1 ; V2 g. U3
is a topmost node in G.


U1
X
U2

V2
U3

V1

Tree G 

Figure 15: General situation
For node X in Figure 15, we add clause (25). When it is called from the parent node
U1 with U1 being ground, we first generate possible values of U2 by calling val U2 (U2),
and then call call X U2 (U2 ) to visit all nodes connected to X through U2 . U3 is similary
treated. After visiting all nodes in G connecting to X through the parent nodes U2 and
U3 (nodes connected to U1 have already been visited), the value of random variable X is
determined by sampling the msw atom jointly indexed by 'X' and the values of U1 , U2 and
431

fiSato & Kameya
U3 . Then we visit X 's children, V1 and V2 . For a topmost node U3 in the original graph,
we add clause (26).
tbn(U1 ) :- msw(par('U1 ',[]),once,U1 ), call
call

U1 X (U1 ).

U1 X (U1 ) :- val U2 (U2), call X U2 (U2 ),
val U3 (U3), call X U3 (U3 ),
msw(par('X',[U1 ,U2 ,U3 ]),once,X),
call X V1 (X), call X V2(X).

(24)

(25)

(26)
Let DBG be the final program containing clauses like (24), (25) and (26). Apparently
DBG can be constructed in time linear in the number of nodes in the network. Also
note that successive unfolding (Tamaki & Sato, 1984) of atoms of the form call ...(1)
in the clause bodies that starts from (24) yields a program DB0G similar to the one in
Figure 13 which contains msw atoms but no call ...(1)'s. As DBG and DB0G define the
same distribution,58 it can be proved from Proposition 5.5 that PG (u) = PDB0 (bn(u)) =
PDB (tbn(u)) holds (details omitted). By the way, in Figure 15 we assume the construction
starts from the topmost node U1 where the evidence u is given, but this is not necessary.
Suppose we change to start from the inner node X. In that case, we replace clause (24)
with call X U1(U1 ) :- msw(par('U1',[]),once,U1 ) just like (26). At the same time we
replace the head of clause (25) with tbn() and add a goal call X U1 (u) to the body
and so on. For the changed program DB00G , it is rather straightforward to prove that
PDB00 (tbn()) = PG(u) holds. It is true that the construction of the tabled program
DBG shown here is very crude and there is a lot of room for optimization, but it suces
to show that a parameterized logic program for a singly connected Bayesian network runs
in O(jV j) time where V is the set of nodes.
To estimate time complexity of OLDT search w.r.t. DBG , we declare tbn and every
predicate of the form call ...(1) as table predicate and verify the five conditions on the
applicability of the graphical EM algorithm (details omitted). We now estimate the time
complexity of OLDT search for the goal tbn(u) w.r.t. DBG .59 We notice that calls occur
according to the pre-order scan (parents { the node { children) of the tree G , and calls
to call Y X (1) occur val(Y ) times. Each call to call Y X (1) invokes calls to the rest of
nodes, X 's parents and X 's children in the graph G except the caller node, with diffrent
set of variable instantiations, but from the second call on, every call only refers to solutions
stored in the solution table in O(1) time. Thus, the number of added computation steps in
call

X U3(U3 ) :- msw(par('U3 ',[]),once,U3 ).







G

G



G






58. Since distribution semantics is based on the least model semantics, and because unfold/fold transformation (Tamaki & Sato, 1984) preserves the least Herbrand model of the transformed program, unfold/fold
transformation applied to parameterized logic programs preserves the denotation of the transformed
program.
59. DBG is further transformed for the OLDT interpreter to collect msw atoms like the case of the HMM
program.
432

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

OLDT search by X is bounded from above, by constant O(val(U 1)val(U 2)val(U 3)val(X ))
in the case of Figure 15. As a result OLDT time is proportional to the number of nodes
in the original graph G (and so is the size of the support graph) provided that the number
of edges connecting to a node, and that of values of a random variable are bounded from
above. So we have

Proposition 5.6 Let G be a singly connected Bayesian network defining distribution PG ,

V the set of nodes, and DBG the tabled program derived as above. Suppose the number of
edges connecting to a node, and that of values of a random variable are bounded from above
by some constant. Also suppose table access can be done in O(1) time. Then, OLDT time
for computing PG(u) for an observed value u of a random variable U by means of DBG is
O(jV j) and so is time per iteration required by the graphical EM algorithm. If there are T
observations, time complexity is O(jV jT ).

O(jV j) is the time complexity required to compute a marignal distribution for a singly
connected Bayesian network by a standard algorithm (Pearl, 1988; Castillo et al., 1997),
and also is that of the EM algorithm using it. We therefore conclude that the graphical
EM algorithm is as ecient as a specialzed EM algorithm for singly connected Bayesian
networks.60 We must also quickly add that the graphical EM algorithm is applicable to
arbitrary Bayesian networks,61 and what Proposition 5.6 says is that an explosion of the
support graph can be avoided by appropriate programming in the case of singly connected
Bayesian networks.
To summarize, the graphical EM algorithm, a single generic EM algorithm, is proved to
have the same time complexity as specialized EM algorithms, i.e. the Baum-Welch algorithm
for HMMs, the Inside-Outside algorithm for PCFGs, and the one for singly connected
Bayesian networks that have been developed independently in each research field.
Table 1 summarizes the time complexity of EM learning using OLDT search and the
graphical EM algorithm in the case of one observation. In the first column, \sc-BNs"
represents singly connected Bayesian networks. The second column shows a program to use.
DBh is an HMM proram in Subsection 4.7, DBg0 a PCFG program in Subsection 5.3 and
DBG a transformed Bayesian network program in Subsection 5.5, respectively. OLDT time
in the third column is time for OLDT search to complete the search of all t-explanations.
gEM in the fourth column is time in one iteration taken by the graphical EM algorithm
to update parameters. We use N , M , L and V respectively for the number of states in
an HMM, the number of nonterminals in a PCFG, the length of an input string and the
number of nodes in a Bayesian network. The last column is a standard (specialized) EM
algorithm for each model.


60. When a marginal distribution of PG for more than one variable is required, we can construct a similar
tabled program that computes marginal probabilities still in O(jV j) time by adding extra-arguments
that convey other evidence or by embedding other evidnece in the program.
61. We check the five conditions with DBG in Figure 13. The uniqueness condition is obvious as sampling
always uniquely generates a sampled value for each random variable. The finite support condition is
satisfied because there are only a finite number of random variables and their values. The acyclic support
condition is immediate because of the acyclicity of Bayesian networks. The t-exclusiveness condition and
the independent condition are easy to verify.
433

fiSato & Kameya

Model
Program
HMMs
DBh
PCFGs
DBg0
DBG
sc-BNs
user model


OLDT time
O (N 2L)
O (M 3 L 3 )
O(jV j)
O(jOLDT treej)

gEM
Specialized EM
2
O(N L)
Baum-Welch
3
3
O (M L )
Inside-Outside
O(jV j)
(Castillo et al., 1997)
O (jsupport graphj)

Table 1: Time complexity of EM learning by OLDT search and the graphical EM algorithm

5.6 Modeling Language PRISM

We have been developing a symbolic-statistical modeling laguage PRISM since 1995 (URL
= http://mi.cs.titech.ac.jp/prism/) as an implementation of distribution semantics
(Sato, 1995; Sato & Kameya, 1997; Sato, 1998). The language is intented for modeling
complex symbolic-statistical phenomena such as discourse interpretation in natural language
processing and gene inheritance interacting with social rules. As a programming language,
it looks like an extension of Prolog with new built-in predicates including the msw predicate
and other special predicates for manipulating msw atoms and their parameters.
A PRISM program is comprised of three parts, one for directives, one for modeling and
one for utilities. The directive part contains declarations such as values, telling the system
what msw atoms will be used in the execution. The modeling part is a set of non-unit definite
clauses that define the distribution (denotation) of the program by using msw atoms. The
last part, the utility part, is an arbitary Prolog program which refers to predicates defined
in the modeling part. We can use in the utility part learn built-in predicate to carry out
EM learning from observed atoms.
PRISM provides three modes of execution. The sampling execution correponds to a
random sampling drawn from the distribution defined by the modeling part. The second
one computes the probability of a given atom. The third one returns the support set for a
given goal. These execution modes are available through built-in predicates.
We must report however that while the implementation of the graphical EM algorithm
with a simpified OLDT search mechanism has been under way, it is not completed yet. So
currently, only Prolog search and learn-naive(DB; G) in Section 4 are available for EM learning though we realized, partially, structrure sharing of explanations in the implemention
of learn-naive(DB; G ). Putting computational eciecy aside however, there is no problem
in expressing and learning HMMs, PCFGs, pseudo PCSGs, Bayesian networks and other
probailistic models by the current version. The learning experiments in the next section
used a parser as a substitute for the OLDT interpreter, and the independently implemented
graphical EM algorithm.
6. Learning Experiments

After complexity analysis of the graphical EM algorithm for popular symbolic-probabilistic
models in the previous section, we look at an actual behavior of the graphical EM algorithm
with real data in this section. We conducted learning experiments with PCFGs using two
434

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

corpora which have contrasting characters, and compared the performance of the graphical
EM algorithm against that of the Inside-Outside algorithm in terms of time per iteration
(= time for updating parameters). The results indicate that the graphical EM algorithm
can outperform the Inside-Outside algorithm by orders of magnigude. Detalis are reported
by Sato, Kameya, Abe, and Shirai (2001). Before proceeding, we review the Inside-Outside
algorithm for completeness.

6.1 The Inside-Outside Algorithm

The Inside-Outside algorithm was proposed by Baker (1979) as a generalization of the
Baum-Welch algorithm to PCFGs. The algorithm is designed to estimate parameters for a
CFG grammar in Chomsky normal form containing rules expressed by numbers like i ! j; k
(1  i; j; k  N for N nonterminals, where 1 is a starting symbol). Suppose an input
sentence w1 ; : : : ; wL is given. In each
iteration, it first computes in a bottom up manner
3
inside probabilities3 e(s; t; i) = P (i ) ws; : : : ; wt) and then computes outside probabilities
f (s; t; i) = P (S )
w1 ; : : : ; ws01 i wt+1 ; : : : ; wL ) in a top-down manner for every s, t and
i (1  s  t  L; 1  i  N ). After computing both probabilities, parameters are
updated by using them, and this process iterates until some predetermined criterion such
as a convergence of the likelihood of the input sentence is achieved. Although Baker did
not give any analysis of the Inside-Outside algorithm, Lari and Young (1990) showed that it
takes O(N 3 L3 ) time in one iteration and Lafferty (1993) proved that it is the EM algorithm.
While it is true that the Inside-Outside algorithm has been recognized as a standard EM
algortihm for training PCFGs, it is notoriously slow. Although there is not much literature
explicitly stating time required by the Inside-Outside algorithm (Carroll & Rooth, 1998;
Beil, Carroll, Prescher, Riezler, & Rooth, 1999), Beil et al. (1999) reported for example
that when they trained a PCFG with 5,508 rules for a corpus of 450,526 German subordinate clauses whose average ambiguity is 9,202 trees/clause using four machines (167MHz
Sun UltraSPARC22 and 296MHz Sun UltraSPARC-II22), it took 2.5 hours to complete
one iteration. We discuss later why the Inside-Outside algorithm is slow.

6.2 Learning Experiments Using Two Corpora

We report here parameter learning of existing PCFGs using two corpora of moderate size
and compare the graphical EM algorithm against the Inside-Outside algorithm in terms
of time per iteration. As mentioned before, support graphs, input to the garphical EM
algorithm, were generated by a parser, i.e. MSLR parser.62 All measurements were made
on a 296MHz Sun UltraSPARC-II with 2GB memory under Solaris 2.6 and the threshold
for an increase of the log likelihood of input sentences was set to 1006 as a stopping criterion
for the EM algorithms.
In the experiments, we used ATR corpus and EDR corpus (each converted to a POS
(part of speech)-tagged corpus). They are similar in size (about 10,000) but contrasting in
their characters, sentence length and ambiguity of their grammars. The first experiment
employed ATR corpus which is a Japanese-English corpus (we used only the Japanese part)
developed by ATR (Uratani, Takezawa, Matsuo, & Morita, 1994). It contains 10,995 short
62. MSLR parser is a Tomita (Generalized LR) parser developed by Tanaka-Tokunaga Laboratory in Tokyo
Institute of Technology (Tanaka, Takezawa, & Etoh, 1997).
435

fiSato & Kameya

conversational sentences, whose minimum length, average length and maximum length are
respectively 2, 9.97 and 49. As a skeleton of PCFG, we employed a context free grammar
Gatr comprising 860 rules (172 nonterminals and 441 terminals) manually developed for
ATR corpus (Tanaka et al., 1997) which yields 958 parses/sentence.
Because the Inside-Outside algorithm only accepts a CFG in Chomsky normal form, we
converted Gatr into Chomsky normal form G3atr . G3atr contains 2,105 rules (196 nonterminals and 441 terminals). We then divided the corpus into subgroups of similar length
like (L = 1; 2); (L = 3; 4); : : : ; (L = 25; 26), each containing randomly chosen 100 sentences.
After these preparations, we compare at each length the graphical EM algorithm applied to
Gatr and G3atr against the Inside-Outside algorithm applied to G3atr in terms of time per
iteration by running them until convergence.
(sec)

(sec)

(sec)

60
I-O
50

0.7

0.04

0.6

0.035

0.5

40

0.4

0.03

I-O
gEM (original)
gEM (Chomsky NF)

0.025
0.02

30
0.3
20

0.015

0.2

10

0.01

0.1

0.005

5

10

15

20

25

L

L

L
0

gEM (original)
gEM (Chomsky NF)

0

5

10

15

20

25

0

5

10 15 20 25

Figure 16: Time per iteration : I-O vs. gEM (ATR)
Curves in Figure 16 show the learning results where an x-axis is the length L of an input
sentence and a y-axis is average time taken by the EM algorithm in one iteration to update
all parameters contained in the support graphs generated from the chosen 100 sentences
(other parameters in the grammar do not change). In the left graph, the Inside-Outside
algorithm plots a cubic curve labeled \I-O". We omitted a curve drawn by the graphical
EM algorithm as it drew the x-axis. The middle graph magnifies the left graph. The curve
labeled \gEM (original)" is plotted by the graphical EM algorithm applied to the original
grammar Gatr whereas the one labeled \gEM (Chomsky NF)" used G3atr. At length 10, the
average sentence length, it is measured that whichever grammar is employed, the graphical
EM algorithm runs several hundreds times faster (845 times faster in the case of Gatr
and 720 times faster in the case of G3atr) than the Inside-Outside algorithm per iteration.
The right graph shows (almost) linear dependency of updating time by the graphical EM
algorithm within the measuared sentence length.
Although some difference is anticipated in their learning speed, the speed gap between
the Inside-Outside algorithm and the graphical EM algorithm is unexpectedly large. The
most conceivable reason is that ATR corpus only contains short sentences and Gatr is not
436

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

much ambiguous so that parse trees are sparse and generated support graphs are small,
which affects favorably the perforamnce of the graphical EM algorithm.
We therefore conducted the same experiment with another corpus which contains much
longer sentences using a more ambiguous grammar that generates dense parse trees. We
used EDR Japanese corpus (Japan EDR, 1995) containing 220,000 Japanese news article
sentences. It is however under the process of re-annotation, and only part of it (randomly
sampled 9,900 sentences) has recently been made available as a labeled corpus. Compared
with ATR corpus, sentences are much longer (the average length of 9,900 sentences is 20,
the minimum length 5, the maximum length 63) and a CFG grammar Gedr (2,687 rules,
converted to Chomsky normal form grammar G3edr containing 12,798 rules) developed for
it is very ambiguous (to keep a coverage rate), having 3:0 2 108 parses/sentence at length
20 and 6:7 2 1019 parses/sentence at length 38.
(sec)

5000

(sec)

(sec)
3

10

6000
I-O

8

I-O
gEM (original)

2.5

gEM (original)

2

4000

6
1.5

3000

4
1

2000

2

1000
0

L
5 10 15 20 25 30 35 40

0

0.5

L
5 10 15 20 25 30 35 40

0

L
5 10 15 20 25 30 35 40

Figure 17: Time per iteration : I-O vs. gEM (EDR)
Figure 17 shows the obtained curves from the experiments with EDR corpus (the graphical EM algorithm applied to Gedr vs. the Inside-Outside algorithm applied to G3edr) under
the same condition as ATR corpus, i.e. plotting average time per iteration to process 100
sentences of the designated length, except that the plotted time for the Inside-Outside algorithm is the average of 20 iterations whereas that for the graphical EM algorithm is the
average of 100 iterations. As is clear from the middle graph, this time again, the graphical
EM algorithm runs orders of magnitude faster than the Inside-Outside algorithm. At average sentence length 20, the former takes 0.255 second whereas the latter takes 339 seconds,
giving a speed ratio of 1,300 to 1. At sentence length 38, the former takes 2.541 seconds but
the latter takes 4,774 seconds, giving a speed ratio of 1,878 to 1. Thus the speed ratio even
widens compared to ATR corpus. This can be explained by the mixed effects of O(L3 ),
time complexity of the Inside-Outside algorithm, and a moderate increase in the total size
of support graphs w.r.t. L. Notice that the right graph shows how the total size of support
graphs grows with sentence length L as time per iteration by the graphical EM algorithm
is linear in the total size of support graphs.

437

fiSato & Kameya

Since we implemented the Inside-Outside algorithm faithfully to Baker (1979), Lari and
Young (1990), there is much room for improvement. Actually Kita gave a refined InsideOutside algorithm (Kita, 1999). There is also an implementation by Mark Johnson of the
Inside-Outside algorithm down-loadable from http://www.cog.brown.edu/%7Emj/. The
use of such implementations may lead to different conclusions. We therefore conducted
learning experiments with the entire ATR corpus using these two implementations and
measured updating time per iteration (Sato et al., 2001). It turned out that both implementations run twice as fast as our naive implementation and take about 630 seconds per
iteration while the graphical EM algorithm takes 0.661 second per iteration, which is still
orders of magnitude faster than the former two. Regrettably a similar comparison using
the entire EDR corpus available at the moment was abandoned due to memory overow
during parsing for the construction of support graphs.
Learning experiments so far only compared time per iteration which ignore extra time
for search (parsing) required by the graphical EM algorithm. So a question naturally arises
w.r.t. comparison in terms of total learning time. Assuming 100 iterations for learning
ATR corpus however, it is estimated that even considering parsing time, the graphical
EM algorithm combined with MSLR parser runs orders of magnitude faster than the three
implementations (ours, Kita's and Johnson's) of the Inside-Outside algorithm (Sato et al.,
2001). Of course this estimation does not directly apply to the graphical EM algorithm
combined with OLDT search, as the OLDT interpreter will take more time than a parser
and how much more time is needed depends on the implementaiton of OLDT search.63
Conversely, however, we may be able to take it as a rough indication of how far our approach,
the graphical EM algorithm combined with OLDT search via support graphs, can go in the
domain of EM learning of PCFGs.

6.3 Examing the Performance Gap

In the previous subsection, we compared the performance of the graphical EM algorithm
against the Inside-Outside algorithm when PCFGs are given, using two corpora and three
implementations of the Inside-Outside algorithm. In all experiments, the graphical EM
algorithm considerably outperformed the Inside-Outside algorithm despite the fact that
both have the same time complexity. Now we look into what causes such a performance
gap.
Simply put, the Inside-Outside algorithm is slow (primarily) because it lacks parsing.
Even when a backbone CFG grammar is explicitly given, it does not take any advantage of
the constraints imposed by the grammar. To see it, it might help to review how the inside
probability e(s; t; A), i.e. P(nonterminal A spans from s-th word to t-th word) (s  t), is
calculated by the Inside-Outside algorithm for the given grammar.
e(s; t; A) =

r=
t01
X

P(A ! BC )e(s; r; B)e(r + 1; t; C )
s.t. A!BC in the grammar r=s
Here P(A ! BC ) is a probability associated with a production rule A ! BC . Note that for
a fixed triplet (s; t; A), it is usual that the term P(A ! BC )e(s; r; B )e(r +1; t; C ) is non-zero
X

B;C

63. We cannnot answer this question right now as the implementation of OLDT search is not completed.
438

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

only for a relatively small number of (B; C; r)'s determined from successful parses and the
rest of combinations always give 0 to the term. Nonetheless the Inside-Outside algorithm
attempts to compute the term in every iteration for all possible combinations of B, C and
r and this is repeated for every possible (s; t; A), resulting in a lot of redundancy. The same
kind of redundancy occurs in the computation of outside probability by the Inside-Outside
algorithm.
The graphical EM algorithm is free of such redundancy because it runs on parse trees (a
parse forest) represented by the support graph.64 It must be added, on the other hand, that
superiority in learning speed of the graphical EM algorithm is realized at the cost of space
complexity because while the Inside-Outside algorithm merely requires O(NL2 ) space for
its array to store probabilities, the graphical EM algorithm needs O(N 3 L3 ) space to store
the support graph where N is the number of nonterminals and L is the sentence length.
This trade-off is understandable if one notices that the graphical EM algorithm applied
to a PCFG can be considered as partial evaluation of the Inside-Outside algorithm by the
grammar (and the introduction of appropriate data structure for the output).
Finally we remark that the use of parsing as a preprocess for EM learning of PCFGs is
not unique to the graphical EM algorithm (Fujisaki, Jelinek, Cocke, Black, & Nishino, 1989;
Stolcke, 1995). These approaches however still seem to contain redundancies compared with
the graphical EM algorithm. For instance Stolcke (1995) uses an Earley chart to compute
inside and outside probability, but parses are implicitly reconstructed in each iteration
dynamically by combining completed items.
7. Related Work and Discussion

7.1 Related Work

The work presented in this paper is at the crossroads of logic programming and probability
theory, and considering an enormous body of work done in these fields, incompleteness is
unavoidable when reviewing related work. Having said that, we look at various attempts
made to integrate probability with computational logic or logic programming.65 In reviewing, one can immediately notice there are two types of usage of probability. One type,
constraint approach, emphasizes the role of probability as constraints and does not necessarily seek for a unique probability distribution over logical formulas. The other type,
distribution approach, explicitly defines a unique distribution by model theoretical means
or proof theoretical means, to compute various probabilities of propositions.
A typical constraint approach is seen in the early work of probabilistic logic by Nilsson
(1986). His central problem, \probabilistic entailment problem", is to compute the upper
and lower bound of probability P() of a target sentence  in such a way that the bounds
are compatible with a given knowledge base containing logical sentences (not necessarily
logic programs) annotated with a probability. These probabilities work as constraints on
64. We emphasize that the difference between the Inside-Outside algorithm and the graphical EM algorithm
is solely computational eciency, and they converge to the same parameter values when starting from the
same initial values. Linguistic evaluations of the estimated parameters by the graphical EM algorithm
are also reported by Sato et al. (2001).
65. We omit literature leaning strongly toward logic. For logic(s) concerning uncertainty, see an overview
by Kyburg (1994).
439

fiSato & Kameya

the possible range of P(). He used the linear programming technique to solve this problem
that inevitably delimits the applicability of his approach to finite domains.
Later Lukasiewicz (1999) investigated the computational complexity of the probabilistic
entailment problem in a slightly different setting. His knowledge base comprises statements
of the form (H j G)[u1 ; u2 ] representing u1  P(H j G)  u2 . He showed that inferring
\tight" u1 ; u2 is NP-hard in general, and proposed a tractable class of knowledge base called
conditional constraint trees.
After the inuential work of Nilsson, Frish and Haddawy (1994) introduced a deductive system for probabilistic logic that remedies \drawbacks" of Nilsson's approach, that
of computational intractability and the lack of a proof system. Their system deduces a
probability range of a proposition by rules of probabilistic inferences about unconditional
and conditional probabilities. For instance, one of the rules infers P (ff j ) 2 [0 y] from
P (ff _ fi j ) 2 [x y ] where ff,fi and  are propositional variables and [x y] (x  y ) designates
a probability range.
Turning to logic programming, probabilistic logic programming formalized by Ng and
Subrahmanian (1992) and Dekhtyar and Subrahmanian (1997) was also a constraint approach. Their program is a set of annotated clauses of the form A :  F1 : 1; : : : ; Fn : n
where A is an atom, Fi (1  i  n) a basic formula, i.e. a conjunction or a disjunction of
atoms, and j (0  j  n) a sub-interval of [0; 1] indicating a probability range. A query
9 (F1 : 1 ; : : : ; Fn : n) is answered by an extension of SLD refutation. On formalization,
it is assumed that their language contains only a finite number of constant and predicate
symbols, and no function symbol is allowed.
A similar framework was proposed by Lakshmanan and Sadri (1994) under the same syntactic restrictions (finitely many constant and predicate symbols but no function
symbols)
in a different uncertainty setting. They used annotated clauses of the form A c B1 ; : : : ; Bn
where A and Bi (1  i  n) are atoms and c = h[ff; fi ]; [; ]i, a confidence level, represents
a belief interval [ff; fi ] (0  ff  fi  1) and a doubt interval [; ] (0      1), which
an expert has in the clause.
As seen above, defining a unique probability distribution is of secondary or no concern
to the constraint approach. This is in sharp contrast with Bayesian networks as the whole
discipline rests on the ability of the networks to define a unique probability distribution
(Pearl, 1988; Castillo et al., 1997). Researchers in Bayesian networks have been seeking for
a way of mixing Bayesian networks with a logical representation to increase their inherently
propositional expressive power.
Breese (1992) used logic programs to automatically build a Bayesian network from a
query. In Breese's approach, a program is the union of a definite clause program and a set
of conditional dependencies of the form P(P j Q1 ^ 1 11 ^ Qn ) where P and Qi s are atoms.
Given a query, a Bayesian network is constructed dynamically that connects the query and
relevant atoms in the program, which in turn defines a local distribution for the connected
atoms. Logical variables can appear in atoms but no function symbol is allowed.
Ngo and Haddawy (1997) extended Breese's approach by incorporating a mechanism
reecting context. They used a clause of the form P(A0 j A1 ; : : : ; An ) = ff L1 ; : : : ; Lk ,
where Ai's are called p-atoms (probabilistic atoms) whereas Lj 's are context atoms disjoint
from p-atoms, and computed by another general logic program (satisfying certain restric440

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

tions). Given a query, a set of evidence and context atoms, relevant ground p-atoms are
identified by resolving context atoms away by SLDNF resolution, and a local Bayesian network is built to calculate the probability of the query. They proved the soundness and
completeness of their query evaluation procedure under the condition that programs are
acyclic66 and domains are finite.
Instead of defining a local distribution for each query, Poole (1993) defined a global distribution in his \probabilistic Horn abduction". His program consists of definite clauses and
disjoint declarations of the form disjoint([h1 :p1,...,hn:pn]) which specifies a probability distribution over the hypotheses (abducibles) fh1; : : : ; hn g. He assigned probabilities to
all ground atoms with the help of the theory of logic programming, and furthermore proved
that Bayesian networks are representable in his framework. Unlike previous approaches, his
language contains function symbols, but the acyclicity condition imposed on the programs
for his semantics to be definable seems to be a severe restriction. Also, probabilities are not
defined for quantified formulas.
Bacchus et al. (1996) used a much more powerful first-order probabilistic language than
clauses annotated with probabilities. Their language allows a statistically quantified term
such as k (x)j(x) kx to denote the ratio of individuals in a finite domain satisfying (x) ^
(x) to those satisfying (x). Assuming that every world (interpretation for their language)
is equally likely, they define the probability of a sentence ' under the given knowledge
('^KB)

base KB as the limit limN !1 ##worlds
worlds (KB) where #worldsN () is the number of
possible worlds containing N individuals satisfying , and  parameters used in judging
approximations. Although the limit does not necessarily exist and the domain must be finite,
they showed that their method can cope with diculties arising from \direct inference" and
default reasoning.
In a more linguistic vein, Muggleton (1996, and others) formulated SLPs (stochastic
logic programs) procedurally, as an extension of PCFGs to probabilistic logic programs.
So, a clause C , which must be range-restricted,67 is annotated with a probability p like
p : C . The probability of a goal G is the product of such ps appearing in its refutation but
with a modification such that if a subgoal g can invoke n clauses, pi : Ci (1  i  n) at
some refutation step, the probability of choosing k-th clause is normalized to pk = ni=1 pi.
More recently, Cussens (1999, 2001) enriched SLPs by introducing a special class of
log-linear models for SLD refutations w.r.t. a given goal. He for example considers all
possible SLD refutations for the most general goal s(X ) and defines probability P(R)
of a refutation R as P(R) = Z 01 exp( i i (R; i)). Here i is a number associated with
a clause Ci and  (R; i) is a feature, i.e. the number of occurrences of Ci in R. Z is the
normalizing constant. Then, the probability assigned to s(a) is the sum of probabilities of
refutation for s(a).



N

N



P

P

66. The condition says that every ground atom A must be assigned a unique integer n(A) such that n(A) >
n(B1 ); : : : ; n(Bn ) holds for any ground instance of a clause of the form A B1 ; : : : ; Bn . Under this
condition, when a program includes p(X ) q(X; Y ), we cannot write recursive clauses about q such as
q (X; [H jY ]) q(X; Y ).
67. A syntactic property that variables appearing in the head also appear in the body of a clause. A unit
clause must be ground.
441

fiSato & Kameya

7.2 Limitations and Potential Problems

Approaches described so far have more or less similar limitations and potential problems.
Descriptive power confined to finite domains is the most common limitation, which is due
to the use of the linear programming technique (Nilsson, 1986), or due to the syntactic
restrictions not allowing for infinitely many constant, function or predicate symbols (Ng
& Subrahmanian, 1992; Lakshmanan & Sadri, 1994). Bayesian networks have the same
limitation as well (only a finite number of random variables are representable).68 Also there
are various semantic/syntactic restrictions on logic programs. For instance the acyclicity
condition imposed by Poole (1993) and Ngo and Haddawy (1997) prevents the unconditional
use of clauses with local variables, and the range-restrictedness imposed by Muggleton
(1996) and Cussens (1999) excludes programs such as the usual membership Prolog program.
There is another type of problem, the possibility of assigning conicting probabilities
to logically equivalent formulas. In SLPs, P(A) and P(A ^ A) do not necessarily coincide
because A and A ^ A may have different refutations (Muggleton, 1996; Cussens, 1999, 2001).
Consequently in SLPs, we would be in trouble if we naively interpret P(A) as the probability
of A's being true. Also assigning probabilities to arbitrary quantified formulas seems out of
scope of both approaches to SLPs.
Last but not least, there is a big problem common to any approach using probabilities:
where do the numbers come from? Generally speaking, if we use n binary random variables in
a model, we have to determine 2n probabilities to completely specify their joint distribution,
and fulfilling this requirement with reliable numbers quickly becomes impossible as n grows.
The situation is even worse when there are unobservable variables in the model such as
possible causes of a disease. Apparently parameter learning from observed data is a natural
solution to this problem, but parameter learning of logic programs has not been well studied.
Distribution semantics proposed by Sato (1995) was an attempt to solve these problems
along the line of the global distribution approach. It defines a distribution (probability
measure) over the possible interpretations of ground atoms for an arbitrary logic program
in any first order language and assigns consistent probabilities to all closed formulas. Also
distribution semantics enabled us to derive an EM algorithm for the parameter learning of
logic programs for the first time. As it was a naive algorithm however, dealing with large
problems was dicult when there are exponentially many explanations for an observation
like HMMs. We believe that the eciency problem is solved to a large extent by the
graphical EM algorithm presented in this paper.

7.3 EM Learning

Since EM learning is one of the central issues in this paper, we separately mention work
related to EM learning for symbolic frameworks. Koller and Pfeffer (1997) used in their
approach to KBMC (knowledge-based model construction) EM learning to estimate parameters labeling clauses. They express probabilistic dependencies among events by definite clauses annotated with probabilities, similarly to Ngo and Haddawy's (1997) approach,
and locally build a Bayesian network relevant to the context and evidence as well as the
68. However, RPMs (recursive probability models) proposed by Pfeffer and Koller (2000) as an extension
of Bayesian networks allow for infinitely many random variables. They are organized as attributes of
classes and a probability measure over attribute values is introduced.
442

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

query. Parameters are learned by applying to the constructed network the specialized EM
algorithm for Bayesian networks (Castillo et al., 1997).
Dealing with a PCFG by a statically constructed Bayesian network was proposed Pynadath and Wellman (1998), and it is possible to combine the EM algorithm with their method
to estimate parameters in the PCFG. Unfortunately, the constructed network is not singly
connected, and time complexity of probability computation is potentially exponential in the
length of an input sentence.
Closely related to our EM learning is parameter learning of log-linear models. Riezler (1998) proposed the IM algorithm in his approach to probabilistic constraint programming. The IM algorithm is a general parameter estimation algorithm from incomplete data for
log-linear models whose probability function P(x) takes the form P(x) =
Z 01 exp( ni=1 i i (x)) p0 (x) where (1 ; : : : ; n ) are parameters to be estimated, i (x) the
i-th feature of an observed object x and Z the normalizing constant. Since a feature can
be any function of x, the log-linear model is highly exible and includes our distribution
Pmsw as a special case of Z = 1. There is a price to pay however; the computational cost
of Z . It requires a summation over exponentially many terms. To avoid the cost of exact
computation, approximate computation by a Monte Carlo method is possible. Whichever
one may choose however, learning time increases compared to the EM algorithm for Z = 1.
The FAM (failure-adjusted maximization) algorithm proposed by Cussens (2001) is an
EM algorithm applicable to pure normalized SLPs that may fail. It deals with a special
class of log-linear models but is more ecient than the IM algorithm. Because the statistical
framework of the FAM is rather different from distribution semantics, comparison with the
graphical EM algorithm seems dicult.
Being slightly tangential to EM learning, Koller et al. (1997) developed a functional
modeling language defining a probability distribution over symbolic structures in which
they showed \cashing" of computed results leads to ecient probability computation of
singly connected Bayesian networks and PCFGs. Their cashing corresponds to the computation of inside probability in the Inside-Outside algorithm and the computation of outside
probability is untouched.
P

7.4 Future Directions

Parameterized logic programs are expected to be a useful modeling tool for complex symbolicstatistical phenomena. We have tried various types of modeling, besides stochastic grammars and Bayesian networks, such as the modeling of gene inheritance in the Kariera tribe
(White, 1963) where the rules of bi-lateral cross-cousin marriage for four clans interact with
the rules of genetic inheritance (Sato, 1998). The model was quite interdisciplinary, but
the exibility of combining msw atoms by means of definite clauses greatly facilitated the
modeling process.
Although satisfying the five conditions in Section 4
 the uniqueness condition (roughly, one cause yields one effect)
 the finite support condition (there are a finite number of explanations for one observation)
 the acyclic support condition (explanations must not be cyclic)
443

fiSato & Kameya

 the t-exclusiveness condition (explanations must be mutually exclusive)
 the independence condition (events in an explanation must be independent)

for the applicability of the graphical EM algorithm seems daunting, our modeling experiences so far tell us that the modeling principle in Section 4 effectively guides us to successful
modeling. In return, we can obtain a declarative model described compactly by a high level
language whose parameters are eciently learnable by the graphical EM algorithm as shown
in the preceding section.
One of the future directions is however to relax some of the applicability conditions,
especially the uniqueness condition that prohibits a generative model from failure or from
generating multiple observable events. Although we pointed out in Section 4.4 that the MAR
condition in Appendix B adapted to our semantics can replace the uniqueness condition and
validates the use of the graphical EM algorithm even when a complete data does not uniquely
determine the observed data just like the case of \partially bracketed corpora" (Pereira &
Schabes, 1992), we feel the need to do more research on this topic. Also investigating
the role of the acyclicity condition seems theoretically interesting as the acyclicity is often
related to the learning of logic programs (Arimura, 1997; Reddy & Tadepalli, 1998).
In this paper we only scratched the surface of individual research fields such as HMMs,
PCFGs and Bayesian networks. Therefore, there remains much to be done about clarifying
how experiences in each research field are reected in the framework of parameterized logic
programs. For example, we need to clarify the relationship between symbolic approaches
to Bayesian networks such as SPI (Li, Z. & D'Ambrosio, B., 1994) and our approach.
Also it is unclear how a compiled approach using the junction tree algorithm for Bayesian
networks can be incorporated into our approach. Aside from exact methods, approximate
methods of probability computation specialized for parameterized logic programs must also
be developed.
There is also a direction of improving learning ability by introducing priors instead of ML
estimation to cope with data sparseness. The introduction of basic distributions that make
probabilistic switches correlated seems worth trying in the near future. It is also important
to take advantage of the logical nature of our approach to handle uncertainty. For example,
it is already shown by Sato (2001) that we can learn parameters from negative examples
such as \the grass is not wet" but the treatment of negative examples in parameterized
logic programs is still in its infancy.
Concerning developing complex statistical models based on the \programs as distributions" scheme, stochastic natural language processing which exploits semantic information
seems promising. For instance, unification-based grammars such as HPSGs (Abney, 1997)
may be a good target beyond PCFGs because they use feature structures logically describable, and the ambiguity of feature values seems to be expressible by a probability
distribution.
Also building a mathematical basis for logic programs with continuous random variables
is a challenging research topic.

444

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling
8. Conclusion

We have proposed a logical/mathematical framework for statistical parameter learning of
parameterized logic programs, i.e. definite clause programs containing probabilistic facts
with a parameterized probability distribution. It extends the traditional least Herbrand
model semantics in logic programming to distribution semantics , possible world semantics
with a probability distribution over possible worlds (Herbrand interpretations) which is
unconditionally applicable to arbitrary logic programs including ones for HMMs, PCFGs
and Bayesian networks.
We also have presented a new EM algorithm, the graphical EM algorithm in Section 4,
which learns statistical parameters from observations for a class of parameterized logic programs representing a sequential decision process in which each decision is exclusive and
independent. It works on support graph s, a new data structure specifying a logical relationship between an observed goal and its explanations, and estimates parameters by computing
inside and outside probability generalized for logic programs.
The complexity analysis in Section 5 showed that when OLDT search, a complete tabled
refutation method for logic programs, is employed for the support graph construction and
table access is done in O(1) time, the graphical EM algorithm, despite its generality, has
the same time complexity as existing EM algorithms, i.e. the Baum-Welch algorithm for
HMMs, the Inside-Outside algorithm for PCFGs and the one for singly connected Bayesian
networks that have been developed independently in each research field. In addition, for
pseudo probabilistic context sensitive grammars with N nonterminals, we showed that the
graphical EM algorithm runs in time O(N 4 L3) for a sentence of length L.
To compare actual performance of the graphical EM algorithm against the InsideOutside algorithm, we conducted learning experiments with PCFGs in Section 6 using two
real corpora with contrasting characters. One is ATR corpus containing short sentences for
which the grammar is not much ambiguous (958 parses/sentence), and the other is EDR
corpus containing long sentences for which the grammar is rather ambiguous (3:0 2 108
at average sentence length 20). In both cases, the graphical EM algorithm outperformed
the Inside-Outside algorithm by orders of magnitude in terms of time per iteration, which
suggests the effectiveness of our approach to EM learning by the graphical EM algorithm.
Since our semantics is not limited to finite domains or finitely many random variables
but applicable to any logic programs of arbitrary complexity, the graphical EM algorithm is
expected to give a general yet ecient method of parameter learning for models of complex
symbolic-statistical phenomena governed by rules and probabilities.
Acknowledgments

The authors wish to thank three anonymous referees for their comments and suggestions.
Special thanks go to Takashi Mori and Shigeru Abe for stimulating discussions and learning
experiments, and also to Tanaka-Tokunaga Laboratory for kindly allowing them to use
MSLR parser and the linguistic data.

445

fiSato & Kameya
Appendix A. Properties of

PDB

In this appendix, we list some properties of PDB defined by a parameterized logic program
DB = F [ R in a countable first-order language L.69 First of all, PDB assigns consistent
probabilities70 to every closed formula  in L by
PDB () def
= PDB (f! 2 
DB j ! j= g)
while guaranteeing continuity in the sense that
limn!1 PDB ((t1 ) ^ 1 11 ^ (tn)) = PDB (8x(x))
limn!1 PDB ((t1 ) _ 1 11 _ (tn)) = PDB (9x(x))
where t1 ; t2 ; : : : is an enumeration of ground terms in L.
The next proposition, Proposition A.1, relates PDB to the Herbrand model. To prove
it, we need some terminology. A factor is a closed formula in prenex disjunctive normal
form Q1 1 1 1 QnM where Qi (1  i  n) is either an existential quantification or a universal
quantification and M a matrix. The length of quantifications n is called the rank of the
factor. Define 8 as a set of formulas made out of factors, conjunctions and disjunctions.
Associate with each formula  in 8 a multi-set r() of ranks by
;
if  is a factor with no quantification
r () =
fng
if  is a factor with rank n
r(1) ] r (2 ) if  = 1 _ 2 or  = 1 ^ 2:
Here ] stands for the union of two multi-sets. For instance f1; 2; 3g]f2; 3; 4g = f1; 2; 2; 3; 3; 4g.
We use the multi-set ordering in the proof of Proposition A.1 because the usual induction
on the complexity of formulas does not work.
Lemma A.1 Let  be a boolean formula made out of ground atoms in L. PDB() =
PF (f 2 
F j MDB ( ) j= g).
(Proof) We have only to prove the lemma about a conjunction of atoms of the form D1x ^
1 11 ^ Dnx (xi 2 f0; 1g; 1  i  n).
PDB (D1x ^ 1 11 ^ Dnx ) = PDB (f! 2 
DB j ! j= D1x ^ 11 1 ^ Dnx g)
= PDB (D1 = x1 ; : : : ; Dn = xn)
= PF (f 2 
F j MDB ( ) j= D1x ^ 1 11 ^ Dnx g) Q.E.D.
8
>
<
>
:

1

n

1

n

n

1

1

n

Proposition A.1 Let  be a closed formula in L. PDB() = PF (f 2 
F j MDB( ) j= g).
69. For definitions of 
F , PF , MDB ( ), 
DB , PDB and others used below, see Section 3.
70. By consistent, we mean probabilities assigned to logical formulas respect the laws of probability such as
0  P (A)  1, P (:A) = 1 0 P (A) and P (A _ B ) = P (A) + P (B ) 0 P (A ^ B ).
446

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

(Proof) Recall that a closed formula has an equivalent prenex disjunctive normal form
that belongs to 8. We prove the proposition for formulas in 8 by using induction on the
multi-set ordering over fr() j  2 8g. If r() = ;,  has no quantification. So the
proposition is correct by Lemma A.1. Suppose otherwise. Write  = G[Q1 Q2 11 1 Qn F ]
where Q1 Q2 1 11 QnF indicates a single occurrence of a factor in G.71 We assume Q1 = 9x
(Q1 = 8x is similarly treated). We also assume that bound variables are renamed to avoid
name clash. Then G[9xQ2 11 1 Qn F ] is equivalent to 9xG[Q2 1 11 QnF ] in light of the validity
of (9xA) ^ B = 9x(A ^ B) and (9xA) _ B = 9x(A _ B) when B contains no free x.
PDB () = PDB (G[Q1 Q2 1 11 QnF ])
= PDB (9xG[ Q2 11 1 Qn F [x]])
= klim
P (G[ Q2 11 1 Qn F [t1 ]] _ 1 11 _ G[Q2 11 1 Qn F [tk ]])
!1 DB
= klim
P (G[ Q2 11 1 Qn F [t1 ] _ 1 11 _ Q2 11 1 Qn F [tk ]])
!1 DB
= klim
P (f 2 
F j MDB ( ) j= G[ Q2 1 11 QnF [t1] _ 11 1 _ Q2 1 11 QnF [tk ] ]g)
!1 F
(by induction hypothesis)
= PF (f 2 
F j MDB ( ) j= 9xG[Q2 1 11 QnF [x]]g)
= PF (f 2 
F j MDB ( ) j= g)
Q.E.D.
We next prove a theorem on the iff definition introduced in Section 4. Distribution
semantics considers the program DB = F [ R as a set of infinitely many ground definite
clauses such that F is a set of facts (with a probability measure PF ) and R a set of rules,
and no clause head in R appears in F . Put
head(R) def
= fB j B appears in R as a clause headg:
For B 2 head(R), let B Wi (i = 1; 2; : : :) be an enumeration of clauses about B in R.
Define iff (B), the iff (if-and-only-if) form of rules about B in DB72 by
iff (B) def
= B $ W1 _ W2 _ 1 1 1
Since MDB ( ) is a least Herbrand model, the following is obvious.
Lemma A.2 For B in head(R) and  2 
F , MDB( ) j= iff (B).
Theorem A.1 below is about iff (B). It states that at general level, both sides of the iff
definition p(x) $ 9y1 (x = t1 ^ W1 ) _ 1 11 _ 9yn (x = tn ^ Wn) of p(1) coincide as random
variables whenever x is instantiated to a ground term.
Theorem A.1 Let iff (B ) = B $ W1 _ W2 _11 1 be the iff form of rules about B 2 head(R).
PDB (iff (B )) = 1 and PDB (B ) = PDB (W1 _ W2 _ 1 11).
71. For an expression E , E [ ] means that  may occur in the specified positions of E . If 1 _ 2 in E [1 _ 2 ]
indicates a single occurrence of 1 _ 2 in a positive boolean formula E , E [1 _ 2 ] = E [1 ] _ E [2 ] holds.
72. This definition is different from the usual one (Lloyd, 1984; Doets, 1994) as we are here talking at ground
level. W1 _ W2 _ 1 1 1 is true if and only if one of the disjuncts is true.
447

fiSato & Kameya

(Proof)

PDB (iff (B ))

=

PDB (f! 2 
DB j ! j= B ^ (W1 _ W2 _ 11 1)g)
+PDB (f! 2 
DB j ! j= :B ^ :(W1 _ W2 _ 1 11)g)

= klim
P (f! 2 
DB j ! j= B ^
!1 DB

k
_

i=1

Wi g)

+ klim
P (f! 2 
DB j ! j= :B ^ :
!1 DB
= klim
P (f 2 
F j MDB ( ) j= B ^
!1 F

k
_
i=1

k
_
i=1

Wi g)

Wi g)
k
_

+ klim
P (f 2 
F j MDB ( ) j= :B ^ : Wi g)
!1 F
i=1
(Lemma A.1)
= PF (f 2 
F j MDB ( ) j= iff (B)g)
= PF (
F ) (Lemma A.2)
= 1
It follows from PDB (iff (B)) = 1 that
PDB (B ) = PDB (B ^ iff(B )) = PDB (W1 _ W2 _ 1 1 1):
Q.E.D.
We then prove a proposition useful in probability computation. Let DB (B ) be the
support set for an atom B introduced in Section 4 (it is the set of all explanations for B). In
the sequel, B is a ground atom. Write DB (B) = fS1 ; S2; : : :g and DB (B) = S1 _S2 _1 1173
Define a set 3B by
3B def
= f! 2 
DB j ! j= B $ DB (B)g:
Proposition A.2 For every B 2 head(R), PDB(3B ) = 1 and PDB (B) = PDB( DB (B)).
(Proof) We first prove PDB (3B ) = 1 but the proof exactly parallels that of Theorem A.1
except that W1 _ W2 _ 1 1 1 is replaced by S1 _ S2 _ 11 1 using the fact that B $ S1 _ S2 _ 11 1
is true in every least Herbrand model of the form MDB ( ). Then from PDB (3B ) = 1, we
have
PDB (B ) = PDB (B ^ (B $
DB (B )))
= PDB ( DB (B)):
Q.E.D.
Finally, we show that distribution semantics is a probabilistic extension of the traditional
least Herbrand model semantics in logic programming by proving Theorem A.2. It says that
the probability mass is distributed exclusively over possible least Herbrand models.
Define 3 as the set of least Herbrand models generated by fixing R and varying a subset
of F in the program DB = F [ R. In symbols,
W

_

W

_

_

W

73. For a set K = fE1 ; E2 ; : : :g of formulas, K denotes a (-n infinite) disjunction E1 _ E2 _ 1 1 1
448

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

3 def
= f! 2 
DB j ! = MDB () for some  2 
F g:
Note that as 3 is merely a subset of 
DB , we cannot conclude PDB (3) = 1 a priori, but the
next theorem, Theorem A.2, states PDB (3) = 1, i.e. distribution semantics distributes the
probability mass exclusively over 3, i.e. possible least Herbrand models.
To prove the theorem, we need some preparations. Recalling that atoms outside head(R)[
F have no chance of being proved from DB, we introduce
30 def
= f! 2 
DB j ! j= :D for every ground atom D 62 head(R) [ F g:
For a Herbrand interpretation ! 2 
DB , !jF (2 
F ) is the restriction of ! to those atoms
in F .
Lemma A.3 Let ! 2 
DB be a Herbrand
interpretation.
0
! = MDB ( ) for some  2 
F iff ! 2 3 and ! j= B $ DB (B ) for every B 2 head(R).
(Proof) Only-if part is immediate from the property of the least Herbrand model. For
if-part, suppose ! satisfies the right hand side. We show that ! = MDB (!jF ). As ! and
MDB (! jF ) coincide w.r.t. atoms not in head(R), it is enough to prove that they also give
the same truth values to atoms in head(R). Take B 2 head(R) and write DB (B) =
S1 _ S2 _ 1 11 Suppose ! j= B $ S1 _ S2 _ 11 1 Then if ! j= B , we have ! j= Sj for some j ,
thereby !jF j= Sj , and hence MDB (!jF ) j= Sj , which implies MDB (!jF ) j= B. Otherwise
! j= :B . So ! j= :Sj for every j . It follows that MDB (! jF ) j= :B . Since B is arbitrary,
we conclude that ! and MDB (!jF ) agree on the truth values assigned to atoms in head(R)
as well.
Q.E.D.
W

W

Theorem A.2 PDB(3) = 1.

(Proof) From Lemma A.3, we have
3 = f! 2 
DB j ! = MDB ( ) for some  2 
F g
= 30 \
3B :
\

B2head(R)

PDB (3B ) = 1 by Proposition A.2. To prove PDB (30 ) = 1, let D1; D2 ; : : : be an enumeration
of atoms not belonging to head(R) [ F . They are not provable from DB = F [ R, and
hence false in every least Herbrand model MDB ( ) ( 2 
F ). So
PDB (30 )

= mlim
!1 PDB (f! 2 
DB j ! j= :D1 ^ 11 1 ^ :Dm g)
= mlim
!1 PF (f 2 
F j MDB ( ) j= :D1 ^ 1 1 1 ^ :Dm g)
= PF (
F ) = 1:
Since a countable conjunction of measurable sets of probability measure one has also
probability measure one, it follows from PDB (3B ) = 1 for every B 2 head(R) and PDB (30) =
1 that PDB (3) = 1.
Q.E.D.
449

fiSato & Kameya
Appendix B. The MAR (missing at random) Condition

In the original formulation of the EM algorithm by Dempster et al. (1977), it is assumed
that there exists a many-to-one mapping y = (x) from a complete data x to an incomplete
(observed) data y. In the case of parsing, x is a parse tree and y is the input sentence and x
uniquely determines y. In this paper, the uniqueness condition ensures the existence of such
a many-to-one mapping from explanations to observations. We however sometimes face a
situation where there is no such many-to-one mapping from complete data to incomplete
data but nonetheless we wish to apply the EM algorithm.
This dilemma can be solved by the introduction of a missing-data mechanism which
makes a complete data incomplete. The missing-data mechanism, m, has a distribution
g (m j x) parameterized by  and y , the observed data, is described as y = m (x). It says
x becomes incomplete y by m. The correspondence between x and y , i.e. fhx; y i j 9m(y =
m (x))g naturally becomes many-to-many.
Rubin (1976) derived two conditions on g (data are missing at random and data are
observed at random) collectively called the MAR (missing at random) condition, and showed
that if we assume a missing-data mechanism behind our observations that satisfies the MAR
condition, we may estimate parameters of the distribution over x by simply applying the
EM algorithm to y, the observed data.
We adapt the MAR condition to parameterized logic programs as follows. We keep a
generative model satisfying the uniqueness condition that outputs goals G such as parse
trees. We further extend the model by additionally inserting a missing-data mechanism
m between G and our observation O like O = m (G) and assume m satisfies the MAR
condition. Then the extended model has a many-to-many correspondence between explanations and observations, and generates non-exclusive observations such that P (O ^ O0 ) > 0
(O 6= O0 ), which causes O P (O)  1 where P (O) = G:9m O= (G) PDB (G). Thanks to
the MAR condition however, we are still allowed to apply the EM algorithm to such nonexclusive observations. Put it differently, even if the uniqueness condition is seemingly
destroyed, the EM algorithm is applicable just by (imaginarily) assuming a missing-data
mechanism satisfying the MAR condition.
P

P

m

References

Abney, S. (1997). Stochastic attribute-value grammars. Computational Linguistics, 23 (4),
597{618.
Arimura, H. (1997). Learning acyclic first-order horn sentences from entailment. In
Proceedings of the Eighth International Workshop on Algorithmic Learning Theory.
Ohmsha/Springer-Verlag.
Bacchus, F., Grove, A., Halpern, J., & Koller, D. (1996). From statistical knowledge bases
to degrees of belief. Artificial Intelligence, 87, 75{143.
Baker, J. K. (1979). Trainable grammars for speech recognition. In Proceedings of Spring
Conference of the Acoustical Society of America, pp. 547{550.

450

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

Beil, F., Carroll, G., Prescher, D., Riezler, S., & Rooth, M. (1999). Inside-Outside estimation
of a lexicalized PCFG for German. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics (ACL'99), pp. 269{276.
Breese, J. S. (1992). Construction of belief and decision networks. Computational Intelligence, 8 (4), 624{647.
Carroll, G., & Rooth, M. (1998). Valence induction with a head-lexicalized PCFG. In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing
(EMNLP 3).
Castillo, E., Gutierrez, J. M., & Hadi, A. S. (1997). Expert Systems and Probabilistic
Network Models. Springer-Verlag.
Charniak, E., & Carroll, G. (1994). Context-sensitive statistics for improved grammatical language models. In Proceedings of the 12th National Conference on Artificial
Intelligence (AAAI'94), pp. 728{733.
Chi, Z., & Geman, S. (1998). Estimation of probabilistic context-free grammars. Computational Linguistics, 24 (2), 299{305.
Chow, Y., & Teicher, H. (1997). Probability Theory (3rd ed.). Springer.
Clark, K. (1978). Negation as failure. In Gallaire, H., & Minker, J. (Eds.), Logic and
Databases, pp. 293{322. Plenum Press.
Cormen, T., Leiserson, C., & Rivest, R. (1990). Introduction to Algorithms. The MIT Press.
Cussens, J. (1999). Loglinear models for first-order probabilistic reasoning. In Proceedings of
the 15th Conference on Uncertainty in Artificial Intelligence (UAI'99), pp. 126{133.
Cussens, J. (2001). Parameter estimation in stochastic logic programs. Machine Learning,
44 (3), 245{271.
D'Ambrosio, B. (1999). Inference in Bayesian networks. AI Magazine, summer, 21{36.
Dekhtyar, A., & Subrahmanian, V. S. (1997). Hybrid probabilistic programs. In Proceedings
of the 14th International Conference on Logic Programming (ICLP'97), pp. 391{405.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete
data via the EM algorithm. Royal Statistical Society, B39 (1), 1{38.
Doets, K. (1994). From Logic to Logic Programming. The MIT Press.
Flach, P., & Kakas, A. (Eds.). (2000). Abduction and Induction { Essays on Their Relation
and Integration. Kluwer Academic Publishers.
Frish, A., & Haddawy, P. (1994). Anytime deduction for probabilistic logic. Journal of
Artificial Intelligence, 69, 93{122.
Fujisaki, T., Jelinek, F., Cocke, J., Black, E., & Nishino, T. (1989). A probabilistic parsing
method for sentence disambiguation. In Proceedings of the 1st International Workshop
on Parsing Technologies, pp. 85{94.
Japan EDR, L. (1995). EDR electronic dictionary technical guide (2nd edition). Technical
report, Japan Electronic Dictionary Research Institute, Ltd.
451

fiSato & Kameya

Kakas, A. C., Kowalski, R. A., & Toni, F. (1992). Abductive logic programming. Journal
of Logic and Computation, 2 (6), 719{770.
Kameya, Y. (2000). Learning and Representation of Symbolic-Statistical Knowledge (in
Japanese). Ph. D. dissertation, Tokyo Institute of Technology.
Kameya, Y., & Sato, T. (2000). Ecient EM learning for parameterized logic programs.
In Proceedings of the 1st Conference on Computational Logic (CL2000), Vol. 1861 of
Lecture Notes in Artificial Intelligence, pp. 269{294. Springer.
Kita, K. (1999). Probabilistic Language Models (in Japanese). Tokyo Daigaku Syuppan-kai.
Koller, D., McAllester, D., & Pfeffer, A. (1997). Effective Bayesian inference for stochastic programs. In Proceedings of 15th National Conference on Artificial Intelligence
(AAAI'97), pp. 740{747.
Koller, D., & Pfeffer, A. (1997). Learning probabilities for noisy first-order rules. In Proceedings of the 15th International Joint Conference on Artificial Intelligence (IJCAI'97),
pp. 1316{1321.
Kyburg, H. (1994). Uncertainty logics. In Gabbay, D., Hogger, C., & Robinson, J. (Eds.),
Handbook of Logics in Artificial Intelligence and Logic Programming, pp. 397{438.
Oxford Science Publications.
Lafferty, J. (1993). A derivation of the Inside-Outside Algorithm from the EM algorithm.
Technical report, IBM T.J.Watson Research Center.
Lakshmanan, L. V. S., & Sadri, F. (1994). Probabilistic deductive databases. In Proceedings
of the 1994 International Symposium on Logic Programming (ILPS'94), pp. 254{268.
Lari, K., & Young, S. J. (1990). The estimation of stochastic context-free grammars using
the Inside-Outside algorithm. Computer Speech and Language, 4, 35{56.
Li, Z., & D'Ambrosio, B. (1994). Ecient inference in Bayes networks as a combinatorial
optimization problem. International Journal of Approximate Reasoning, 11, 55{81.
Lloyd, J. W. (1984). Foundations of Logic Programming. Springer-Verlag.
Lukasiewicz, T. (1999). Probabilistic deduction with conditional constraints over basic
events. Journal of Artificial Intelligence Research, 10, 199{241.
Manning, C. D., & Schutze, H. (1999). Foundations of Statistical Natural Language Processing. The MIT Press.
McLachlan, G. J., & Krishnan, T. (1997). The EM Algorithm and Extensions. Wiley
Interscience.
Muggleton, S. (1996). Stochastic logic programs. In de Raedt, L. (Ed.), Advances in
Inductive Logic Programming, pp. 254{264. IOS Press.
Ng, R., & Subrahmanian, V. S. (1992). Probabilistic logic programming. Information and
Computation, 101, 150{201.
Ngo, L., & Haddawy, P. (1997). Answering queries from context-sensitive probabilistic
knowledge bases. Theoretical Computer Science, 171, 147{177.
Nilsson, N. J. (1986). Probabilistic logic. Artificial Intelligence, 28, 71{87.
452

fiParameter Learning of Logic Programs for Symbolic-statistical Modeling

Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann.
Pereira, F. C. N., & Schabes, Y. (1992). Inside-Outside reestimation from partially bracketed
corpora. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics (ACL'92), pp. 128{135.
Pereira, F. C. N., & Warren, D. H. D. (1980). Definite clause grammars for language analysis
| a survey of the formalism and a comparison with augmented transition networks.
Artificial Intelligence, 13, 231{278.
Pfeffer, A., & Koller, D. (2000). Semantics and inference for recursive probability models. In
Proceedings of the Seventh National Conference on Artificial Intelligence (AAAI'00),
pp. 538{544.
Poole, D. (1993). Probabilistic Horn abduction and Bayesian networks. Artificial Intelligence, 64 (1), 81{129.
Pynadath, D. V., & Wellman, M. P. (1998). Generalized queries on probabilistic context-free
grammars. IEEE Transaction on Pattern Analysis and Machine Intelligence, 20 (1),
65{77.
Rabiner, L. R. (1989). A tutorial on hidden markov models and selected applications in
speech recognition. Proceedings of the IEEE, 77 (2), 257{286.
Rabiner, L. R., & Juang, B. (1993). Foundations of Speech Recognition. Prentice-Hall.
Ramakrishnan, I., Rao, P., Sagonas, K., Swift, T., & Warren, D. (1995). Ecient tabling
mechanisms for logic programs. In Proceedings of the 12th International Conference
on Logic Programming (ICLP'95), pp. 687{711. The MIT Press.
Reddy, C., & Tadepalli, P. (1998). Learning first-order acyclic horn programs from entailment. In Proceedings of the 15th International Conference on Machine Learning;
(and Proceedings of the 8th International Conference on Inductive Logic Programming). Morgan Kaufmann.
Riezler, S. (1998). Probabilistic Constraint Logic Programming. Ph.D. thesis, Universitat
Tubingen.
Rubin, D. (1976). Inference and missing data. Biometrika, 63 (3), 581{592.
Sagonas, K., T., S., & Warren, D. (1994). XSB as an ecient deductive database engine.
In Proceedings of the 1994 ACM SIGMOD International Conference on Management
of Data, pp. 442{453.
Sato, T. (1995). A statistical learning method for logic programs with distribution semantics.
In Proceedings of the 12th International Conference on Logic Programming (ICLP'95),
pp. 715{729.
Sato, T. (1998). Modeling scientific theories as PRISM programs. In Proceedings of ECAI'98
Workshop on Machine Discovery, pp. 37{45.
Sato, T. (2001). Minimum likelihood estimation from negative examples in statistical abduction. In Proceedings of IJCAI-01 workshop on Abductive Reasoning, pp. 41{47.
Sato, T., & Kameya, Y. (1997). PRISM: a language for symbolic-statistical modeling.
In Proceedings of the 15th International Joint Conference on Artificial Intelligence
(IJCAI'97), pp. 1330{1335.
453

fiSato & Kameya

Sato, T., & Kameya, Y. (2000). A Viterbi-like algorithm and EM learning for statistical
abduction. In Proceedings of UAI2000 Workshop on Fusion of Domain Knowledge
with Data for Decision Support.
Sato, T., Kameya, Y., Abe, S., & Shirai, K. (2001). Fast EM learning of a family of PCFGs.
Titech technical report (Dept. of CS) TR01-0006, Tokyo Institute of Technology.
Shen, Y., Yuan, L., You, J., & Zhou, N. (2001). Linear tabulated resolution based on Prolog
control strategy. Theory and Practice of Logic Programming, 1 (1), 71{103.
Sterling, L., & Shapiro, E. (1986). The Art of Prolog. The MIT Press.
Stolcke, A. (1995). An ecient probabilistic context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics, 21 (2), 165{201.
Tamaki, H., & Sato, T. (1984). Unfold/fold transformation of logic programs. In Proceedings
of the 2nd International Conference on Logic Programming (ICLP'84), Lecture Notes
in Computer Science, pp. 127{138. Springer.
Tamaki, H., & Sato, T. (1986). OLD resolution with tabulation. In Proceedings of the 3rd
International Conference on Logic Programming (ICLP'86), Vol. 225 of Lecture Notes
in Computer Science, pp. 84{98. Springer.
Tanaka, H., Takezawa, T., & Etoh, J. (1997). Japanese grammar for speech recognition
considering the MSLR method. In Proceedings of the meeting of SIG-SLP (Spoken
Language Processing), 97-SLP-15-25, pp. 145{150. Information Processing Society of
Japan. In Japanese.
Uratani, N., Takezawa, T., Matsuo, H., & Morita, C. (1994). ATR integrated speech and
language database. Technical report TR-IT-0056, ATR Interpreting Telecommunications Research Laboratories. In Japanese.
Warren, D. S. (1992). Memoing for logic programs. Communications of the ACM, 35 (3),
93{111.
Wetherell, C. S. (1980). Probabilistic languages: a review and some open questions. Computing Surveys, 12 (4), 361{379.
White, H. C. (1963). An Anatomy of Kinship. Prentice-Hall.
Zhang, N., & Poole, D. (1996). Exploiting causal independence in Bayesian network inference. Journal of Artificial Intelligence Research, 5, 301{328.

454

fiJournal of Artificial Intelligence Research 15 (2001) 1-30

Submitted 11/00; published 7/01

Goal Recognition through Goal Graph Analysis
j.hong@ulst.ac.uk

Jun Hong
School of Information and Software Engineering
University of Ulster at Jordanstown
Newtownabbey, Co. Antrim BT37 0QB, UK

Abstract
We present a novel approach to goal recognition based on a two-stage paradigm of graph
construction and analysis. First, a graph structure called a Goal Graph is constructed to
represent the observed actions, the state of the world, and the achieved goals as well as
various connections between these nodes at consecutive time steps. Then, the Goal Graph
is analysed at each time step to recognise those partially or fully achieved goals that are
consistent with the actions observed so far. The Goal Graph analysis also reveals valid
plans for the recognised goals or part of these goals.
Our approach to goal recognition does not need a plan library. It does not suer from
the problems in the acquisition and hand-coding of large plan libraries, neither does it have
the problems in searching the plan space of exponential size. We describe two algorithms
for Goal Graph construction and analysis in this paradigm. These algorithms are both
provably sound, polynomial-time, and polynomial-space. The number of goals recognised
by our algorithms is usually very small after a sequence of observed actions has been
processed. Thus the sequence of observed actions is well explained by the recognised goals
with little ambiguity. We have evaluated these algorithms in the UNIX domain, in which
excellent performance has been achieved in terms of accuracy, eciency, and scalability.

1. Introduction
Plan recognition involves inferring the intentions of an agent from a set of observations. A
typical approach to plan recognition uses an explicit representation of possible plans and
goals, often called a plan library, and conducts some type of reasoning on the basis of a set
of observations to identify plans and goals from the plan library, that could have caused the
observations.
Plan recognition is useful in many areas, including discourse analysis in natural language question-answering systems, story understanding, intelligent user interfaces, and
multi-agent coordination. Much early research in plan recognition has been done in natural language question-answering systems (Allen & Perrault, 1980; Allen, 1983; Sidner,
1985; Litman & Allen, 1987; Carberry, 1988; Pollack, 1990; Grosz & Sidner, 1990). In
these systems, plan recognition has been used to support intelligent response generation; to
understand sentence fragments, ellipsis and indirect speech acts; to track a speakers ow
through a discourse; and to deal with correctness and completeness discrepancies between
the knowledge of users and systems.
Plan recognition can enhance user interfaces. The recognition of the users goals and
plans from his interaction with the interface facilitates intelligent user help (Carver, Lesser,
& McCue, 1984; Hu & Lesser, 1988; Goodman & Litman, 1992; Bauer & Paul, 1993;
Lesh & Etzioni, 1995). Plan recognition enables the interface to assist the user in task
c
2001
AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiHong

completion, and error detection and recovery (Wilensky & et al., 1988). The interface does
this by watching over the users shoulder to infer his goals and plans. It can then decide
what help and assistance the user needs.
In story understanding (Schank & Abelson, 1977; Wilensky, 1983; Charniak & Goldman,
1993), it is useful to recognise the goals and plans of the characters from the described
actions in order to understand what the characters were doing. In multi-agent coordination,
ecient and eective coordination among multiple agents requires modelling each agents
goals and plans (Huber, Durfee, & Wellman, 1994; Huber & Durfee, 1995).
Given a set of observations, most plan recognition systems (Allen & Perrault, 1980;
Carberry, 1986; Litman & Allen, 1987; Kautz, 1987; Pollack, 1990)) search a space of
possible plan hypotheses for candidate plans and goals that account for the observations.
To form the search space in a given domain, some kind of plan library is required. For
instance, in Kautzs event hierarchy (Kautz, 1987), plan decompositions are required that
describe how low-level actions make up complex actions. Despite its obvious advantage of
expressive richness, use of a plan library has a few limitations. First, it is not able to deal
with new plans whose types do not appear in the plan library. Second, acquiring and handcoding the plan library in a large and complex domain presents a tedious or impractical task.
Third, in some other domains, the knowledge about plans might not be readily available.
Some attempts (Mooney, 1990; Forbes, Huang, Kanazawa, & Russell, 1995; Lesh &
Etzioni, 1996; Albrecht, Zukerman, & Nicholson, 1998; Bauer, 1998) have recently been
made to apply machine learning techniques to the automated acquisition and coding of plan
libraries. Even when leaving aside the plan library consideration, searching the plan space
can, however, be exponentially expensive because the number of possible plan hypotheses
can be exponential in the number of actions (Kautz, 1987). Most plan recognition systems
have been developed in domains in which peoples behaviour is characterized by fewer than
100 plans and goals (Lesh & Etzioni, 1996).
In this paper, we focus on goal recognition, a special case of plan recognition. We
introduce a novel approach to goal recognition, in which graph construction and analysis is
used as a paradigm. Our approach signicantly diers from most plan recognition systems.
First, our approach does not need a plan library. Instead we dene what constitutes a valid
plan for a goal. We need to consider only how observed actions can be organised into plans.
Hence we do not have the problems associated with the acquisition and hand-coding of the
plan library in a large and complex domain as well as the availability of planning knowledge
in a domain. Most plan recognition systems cannot recognise any new plans whose types do
not appear in the plan library. Without using a plan library, our approach does not suer
from this limitation. Second, instead of immediately searching for a plan in the plan space
as in most plan recognition systems, our approach explicitly constructs a graph structure,
called a Goal Graph, which is then analysed to recognise goals and plans. Our approach
therefore does not have the problems in searching the plan space of exponential size. Third,
our approach recognises only partially or fully achieved goals that are consistent with the
actions observed so far. The number of such recognised goals is usually very small after a
sequence of observed actions has been processed. Thus the sequence of observed actions is
well explained by the recognised goals with little ambiguity.
It should be emphasised that in our approach to goal recognition, the purpose of recognising partially or fully achieved goals is to explain past actions rather than predicting
2

fiGoal Recognition through Goal Graph Analysis

future actions. This is particularly useful in such problem areas as story understanding,
software advisory systems, database query optimisation, and customer data mining. These
problem areas have several specic characteristics. First, most actions have been either described or observed. Second, it is very likely that the users intended goal has been partially
or fully achieved by these actions. Third, recognising the intended goal aims at explaining past actions rather than predicting future actions. Finally, distinguishing partially or
fully achieved goals from the others greatly reduces ambiguity involved in recognising the
intended goal.
In story understanding, most actions of the characters are described in the story. Recognising goals and plans that account for the described actions enables better understanding
of what the characters were doing. In software advisory systems, after a user has been
observed to issue a sequence of operations in a software application, the system can rst
recognise the task the user has performed. The system can then decide whether the user
has performed the task in a suboptimal way, and advice can be given to the user as to
how to better perform the task. In database query optimisation, after a user has conducted a sequence of data retrieval and manipulation operations, recognising the underlying
query can lead to advice on query optimisation when the query has not been executed in
an optimal way. In customer data mining, the individual customers shopping goals can
be recognised from logged customer on-line shopping data. This can also form a basis for
performing other customer data mining tasks.
Our algorithms for Goal Graph construction and analysis are both provably sound,
polynomial-time, and polynomial-space. Our empirical results in the UNIX domain show
that our algorithms perform well in terms of accuracy, eciency, and scalability. They also
show that our algorithms can be scaled up and applied to domains in which there are tens
of thousands of possible goals and plans. Though our algorithm for Goal Graph analysis
is not complete, it recognised every goal that was intended and successfully achieved by
the subject in the UNIX data set we used in our evaluation. Since our new graph-based
approach to goal recognition is fundamentally dierent from the existing methods for plan
recognition, it provides an alternative to these methods and a new perspective on the plan
recognition problem.
The rest of the paper is organised as follows. First, we give an overview of our novel
approach to goal recognition. In Section 3, we discuss the domain representation. In
Section 4 we dene Goal Graphs, valid plans, and consistent goals. In Section 5 we present
our goal recognition algorithms together with our analysis of these algorithms. In Section 6
we discuss the empirical results. We summarise the paper and discuss limitations and future
work in the last section.

2. A Novel Approach to Goal Recognition
In this section, we describe the basic assumptions we make on the goal recognition problem
and outline our approach. We discuss previous work on planning with Planning Graphs
and a graph-based approach to goal recognition. We briey describe empirical results that
are in favour of our approach.
3

fiHong

2.1 Basic Assumptions
We start with an example in the UNIX domain. When we observe that a user types two
commands, cd papers and ls, one after another, we should be able to infer that the user
wants to nd a le or subdirectory in the directory, papers, for two reasons. First, this goal
has been fully achieved. Second, it is relevant to both commands in a consistent way in
the sense that the rst command satises one of the preconditions of the second command
and the second command achieves the recognised goal. The recognised goal might be just
an intermediate goal of the user. The users intended goal might be one of the le related
goals, for instance, deleting a le from the directory. Since these commands can both be
part of plans for almost all the le related goals, it is impossible for us to uniquely identify
the users intended goal at the current time step. Yet the goal of nding a le or directory
well explains these commands. If the user next types a command, rm oldpaper.tex, we
can then infer that the users goal is to delete a le, oldpaper.tex, in the directory, papers,
because this goal is now fully achieved and it is relevant to all the commands observed so
far in a consistent way in the sense that the rst command satises one of the preconditions
of the second and third commands, the second command satises one of the preconditions
of the third command, and the third command achieves the recognised goal.
This example highlights the way our new approach to goal recognition works. We make
the following assumptions on the goal recognition problem. First, a set of actions are
observed at consecutive time steps.1 Second, the initial state of the world immediately
before the set of actions are observed is known.2 Third, we have domain knowledge on
actions and goals, that is, we know the preconditions and eects of every observed action,
and every possible goal is explicitly specied by a set of goal descriptions.
Given these assumptions, when an action is observed at a time step, we want to infer
which goals have been partially or fully achieved at this time step and whether each of the
achieved goals is relevant to the strict majority of the actions observed so far in a consistent
way in the sense that these actions can be organised into a plan structure for the goal or
part of it.
2.2 Goal Recognition with Goal Graph
We propose to use a graph structure, called a Goal Graph, in our new approach to goal
recognition. We view the goal recognition problem as a process of graph construction and
analysis. In a Goal Graph, action nodes represent the actions observed at consecutive time
steps; proposition nodes represent the state of the world at consecutive time steps, as it is
changed from the initial state to the subsequent states by the observed actions; and goal
nodes represent the goals that are partially or fully achieved at consecutive time steps.
Edges in a Goal Graph explicitly represent relations between actions and propositions as
well as relations between propositions and goals. Based on these explicit relations in the
constructed Goal Graph, causal links between either two actions or an action and a goal
can be recognised. Having recognised these causal links, it can be decided whether a fully
or partially achieved goal at a time step is relevant to the strict majority of the observed
1. The observed actions can be partially ordered in the sense that more than one action can be observed
at a time step and there is no temporal ordering constraint on these actions.
2. Our approach however reasons about the state of the world at subsequent time steps.

4

fiGoal Recognition through Goal Graph Analysis

actions so far in a consistent way in the sense that these relevant actions can be organised
into a plan structure for the goal or part of it. In our approach, extraneous, redundant, and
partially ordered actions in plans are all handled.
Our attempt to use graph construction and analysis as a paradigm for goal recognition
is in spirit inuenced by Blum and Fursts eorts on planning with Planning Graphs (Blum
& Furst, 1997). They introduced a new graph-based approach to planning in STRIPS
domains, in which a graph structure called a Planning Graph is rst constructed explicitly
rather than searching immediately for a plan as in standard planning methods. Many useful
constraints inherent in the planning problem are made explicitly available in a Planning
Graph to reduce the amount of search needed. The Planning Graph is then analysed to
generate possible plans.
Our Goal-Graph-based approach to goal recognition can be seen as a counterpart of
planning with Planning Graph. Though graph structures are used in both approaches,
they are composed of dierent kinds of nodes and edges. In a time step, a Planning Graph
represents all the possible propositions either added by the actions in the previous time step
or brought forward by maintenance actions from the previous time step, and all the possible
actions whose preconditions have been satised by the propositions in the time step. On the
other hand, a Goal Graph, in a time step, represents only the propositions either added by
the actions observed in the previous time step or brought forward by maintenance actions
from the previous time step, and the actions observed in the time step. In addition, a Goal
Graph, in a time step, also represents all the possible goals, either fully or partially achieved
in the time step, while a Planning Graph does not represent any goal at all. Accordingly,
a Planning Graph represents only relations between actions and propositions, while a Goal
Graph also represents relations between propositions and goals.
The analysis of a Planning Graph aims to search for all the possible subgraphs of the
Planning Graph, that form valid plans for the only given goal. On the other hand, the
analysis of a Goal Graph aims to search for every possible partially or fully goal for which
there exists a subgraph of the Goal Graph, consisting of the strict majority of the observed
actions. Such a subgraph forms a valid plan for the goal or part of it and shows that the
strict majority of the observed actions are relevant to the goal in a consistent way.
The domain representation can be the same for both planning with Planning Graph
and goal recognition with Goal Graph. In this regard, previous eorts on handling more
expressive representation languages (Gazen & Knoblock, 1997; Anderson, Smith, & Weld,
1998; Koehler, Nebel, Homann, & Dimopoulos, 1997) are still useful for goal recognition.
These languages allow use of disjunctive preconditions, conditional eects, and universally
quantied preconditions (goal descriptions) and eects in action and goal representation.
Our ADL-like domain representation is actually based on this work, that allows use of
conditional eects, universally quantied eects, and existentially and universally quantied
preconditions and goal descriptions in the action and goal representation.
Our Goal-Graph-based approach further extends Lesh and Etzionis previous work on
use of a graph representation of actions and goals for the goal recognition problem (Lesh &
Etzioni, 1995). They used a graph representation, called a consistency graph, for the goal
recognition problem. A consistency graph consists of action and goal nodes representing
possible actions and goals, and edges representing possible connections between nodes in
5

fiHong

the graph. Initially, action and goal nodes are fully connected to each other in a consistency
graph, and inconsistent goals are then repeatedly pruned from the consistency graph.
There are a number of major dierences between Lesh and Etzionis approach and ours.
First, two dierent graph representations are used. Apart from action and goal nodes, a
consistency graph does not have nodes representing the propositions that model how the
state of the world is changed by the observed actions. Therefore, the consistency graph
does not explicitly reveal causal links over actions and goals. Neither does their system
know whether a goal has been partially or fully achieved by the observed actions. Our Goal
Graph consists of action, goal, and proposition nodes. It explicitly reveals causal links over
actions and goals, hence our system knows how the observed actions are composed into
valid plans for the recognised goals or part of these goals. Our systems also knows whether
a goal is partially or fully achieved by the observed actions.
Second, goal consistency is dened dierently. In Lesh and Etzionis approach, a goal
is consistent if there exists a plan that includes the observed actions and achieves the
goal. In our approach, a goal is consistent if it has been partially or fully achieved by the
observed actions and is relevant to the strict majority of the observed actions. Also, two
dierent recognition processes are used. In their approach a pruning process is used to
prune inconsistent goals from the consistency graph. The pruning process guarantees that
the goals pruned from the consistency graph are inconsistent goals. However, the number of
consistent goals, still remaining in the consistency graph after pruning, is usually large. Thus
ambiguity on the intended goal remains an issue to be addressed. Our approach instead
uses a graph analysis process to directly recognise consistent goals from only those fully or
partially achieved goals. The number of consistent goals recognised in the Goal Graph is
usually small. Third, their approach requires that every observed action is relevant to the
goal, while only the strict majority of the observed actions are required to be relevant to
the goal in our approach.
We have developed two algorithms, GoalGraphConstructor and GoalGraphAnalyser,
based on our two-stage paradigm of Goal Graph construction and analysis. The GoalGraphConstructor algorithm takes a set of actions as they are observed at dierent time
steps and constructs a Goal Graph. The GoalGraphAnalyser algorithm analyses the constructed Goal Graph to recognise consistent goals and valid plans. We prove that our
algorithms are sound, polynomial-time, and polynomial-space.
Our algorithms have been implemented in Prolog and tested in the UNIX domain on a
desktop with a Pentium III processor at 600 MHz. We used a set of data, collected in the
UNIX domain at the University of Washington, with a domain representation of 35 action
schemata and 249 goal schemata. On the entire UNIX data set, on average it took just a few
CPU seconds to update the Goal Graph when an observed action was processed, and usually
only a very small number of consistent goals remained after a sequence of actions had been
observed. In all those test cases where the intended goals had been successfully achieved
by the subjects, these intended goals were all among the remaining goals recognised after
complete sequences of actions had been observed. To test the scalability of our algorithms,
we tested them on a series of spaces of approximate 104 , 2  104 , up to 105 candidate goals
respectively in the UNIX domain, where the approximate linear time performance has been
achieved. Our empirical results show that our algorithms can be scaled up and applied to
domains in which there are tens of thousands of possible goals and plans.
6

fiGoal Recognition through Goal Graph Analysis

3. The Domain Representation
We use an ADL-like representation (Pednault, 1989), including actions with conditional and
universally quantied eects, and existentially as well as universally quantied preconditions
and goal descriptions. In our approach to goal recognition, a goal recognition problem
consists of
 A set of action schemata specifying primitive actions.
 A nite, dynamic universe of typed objects where objects can be either added or
deleted by an action.
 A set of propositions called the Initial Conditions.
 A set of goal schemata specifying possible goals.
 A set of actions that are observed at consecutive time steps.3
The solution to a goal recognition problem consists of a set of partially or fully achieved
goals that are consistent with the set of observed actions together with the valid plans
consisting of observed actions for the recognised goals or part of them.
The goal schema consists of a set of goal descriptions (GDs) that are dened by the
following EBNF denitions.
<GD>
<GD>
<GD>
<GD>
<GD>
<GD>
<GD>
<GD>
<GD>

::=
::=
::=
::=
::=
::=
::=
::=
::=

<term>
(not <term>)
(neg <term>)
(and <GD>*)
(imply <GD> <GD>)
(exist <term> <GD>)
(forall <term> <GD>)
(eq <argument> <argument>)
(neq <argument> <argument>)

The action schema consists of a set of preconditions and a set of eects. The set of
preconditions are dened as the same as goal descriptions. The set of eects are dened by
the following EBNF denitions.
<effect>
<effect>
<effect>
<effect>
<effect>

::=
::=
::=
::=
::=

<term>
(neg <term>)
(and <effect>*)
(when <GD> <effect>)
(forall <term> <effect>)

3. When we say an observed action, we mean an action that has been observed and successfully executed.
We ignore those invalid actions. In the UNIX domain, for instance, these invalid actions are those issued
commands that UNIX failed to execute and responded only in error messages.

7

fiHong

In the above two sets of EBNF denitions, a <term> is an atomic expression of the form:
<term> ::= (<predicate-name> <argument>*)
<argument> ::= <constant-name>
<argument> ::= <variable-name>
We use eq and neq to specify equality and inequality constraints. We have two negation
connectives: neg and not. We use (neg A) to specically mean that the truth value of A is
made explicitly known to be false by an action. We use (not A) to mean that the truth value
of A is known to be false either explicitly or implicitly. The latter kind of representation can
be used when it is not necessary to represent that the truth value of A is explicitly known
to be false as long as it is known to be false. The closed world assumption is therefore
implemented as follows. In the initial state of the world, we explicitly represent only the
propositions known to be true in the Initial Conditions. Any proposition not explicitly
represented in the state of the world is implicitly known to be false. Actions however may
add some propositions explicitly known to be false to the state of the world. A proposition
can become explicitly known to be false only if it has been made explicitly known to be
false by an action. It is important to represent the propositions that are explicitly known
to be false, because we want to explicitly represent the eects of actions so that causal links
between either two actions or an action and a goal can be established.
Both goal and action schemata are parameterised by typed variables that are represented
by terms with object type predicates. A goal is a ground instance of a goal schema. An
action is a ground instance of an action schema. The set of goal descriptions for a goal must
be satised in the state of the world when the goal is fully achieved. If some but not all the
goal descriptions are satised instead, the goal is partially achieved. Positive literals in the
goal descriptions represent propositions true in the state of the world. Negative literals in
the goal descriptions represent propositions known to be false in the state of the world. We
use imply to specify dependency constraints on goal descriptions. If a goal description GD2
is implied by another goal description GD1 , GD2 can only be satised when GD1 is satised
but GD1 can be satised without GD2 being satised. A goal description can be existentially
and universally quantied over a dynamic universe of objects.
The set of preconditions must be satised in the state of the world before the action
can be executed. The set of preconditions have the same syntax and semantics as the
set of goal descriptions. The set of eects are taken in the state of the world when the
action is executed. Positive literals in the eects represent propositions true in the state
of the world after the action is executed. These propositions are added to the state of the
world. Negative literals in the eects represent propositions no longer true in the state of
the world after the action is executed. These propositions are deleted from the state of
the world, while the negations of these propositions are added to the state of the world,
representing that these propositions are explicitly known to be false in the state of the world
after the action is executed. Furthermore, a conditional eect consists of an antecedent and
a consequent, where the antecedent is a set of preconditions and the consequent is a set
of eects. The eects in the consequent can be taken only when the preconditions in the
antecedent are satised in the state of the world before the action is executed. An eect in
an action schema can be universally quantied over a dynamic universe of objects.
8

fiGoal Recognition through Goal Graph Analysis

We use a simple example domain extended from Pednaults famous example (Pednault,
1988). It involves transportation of two physical objects, a dictionary, and a chequebook,
between home and oce using a briefcase. We assume that only one physical object can be
carried in the briefcase at a time. The extended briefcase domain consists of
 A special physical object: a briefcase.
 Two physical objects: a dictionary and a chequebook.
 Two locations: home and oce.
 Three action schemata:
Moving the briefcase from one location to another,
Putting a physical object in the briefcase, and
Taking out a physical object from the briefcase.
 Three goal schemata:
Moving a physical object from one location to another,
Keeping a physical object at a location, and
Keeping a physical object in the briefcase.
The action and goal schemata of this example domain are shown in Figure 1. They are
used throughout the paper.
In the actual implementation of our goal recognition algorithms, universally quantied
preconditions and eects, and conditional eects in action schemata are eliminated; and
equivalent schemata are created. We use a particular approach we call dynamic expansion.
Dynamic expansion involves two steps. In the rst step, universally quantied preconditions
and eects in an action schema are dynamically compiled into the corresponding Herbrand
bases, taking into account the universe of objects at the current time step. These universally
quantied preconditions and eects can only be dynamically compiled because we assume
that the universe of objects can be dynamically changed. This assumption is needed in a
domain like the UNIX shell system where destruction and creation of objects are required.
Under our assumption on the dynamic universe of objects, for each object in the universe,
its object type must have been declared at the time step immediately before the action is
executed. For each object in the initial universe of objects, its type must be declared in the
Initial Conditions. An object can be either added to or deleted from the universe of objects
by an action at a time step, with an eect either stating a proposition about the new object
or negating a proposition about the existing object.
For instance, suppose that at the time step immediately before an instance of action
schema mov-b shown in Figure 1 is executed, the universe of objects consists of three physical
objects: B, C, and D. Action schema mov-b is then dynamically compiled into action schema
mov-b-1 as follows.
9

fiHong

(:action mov-b
:paras (?l ?m - loc)
:pre (and (neq ?l ?m)(at B ?l))
:eff (and (at B ?m) (neg (at B ?l))
(forall (?z - physob)
(when (in ?z)
(and (at ?z ?m)
(neg (at ?z ?l)))))) )
(:action put-in
:paras (?x - physob ?l loc)
:pre (and (neq ?x B)(at ?x ?l)(at B ?l))
(forall (?z - physob)
(not (in ?z))))
:eff (in ?x) )
(:action take-out
:paras (?x - physob)
:pre (in ?x)
:eff (neg (in ?x)) )
(:goal move-object
:paras (?x - physob ?l ?m - loc)
:goal-des (and (neq ?l ?m)
(neq ?x B)
(imply (neg (at ?x ?l))
(at ?x ?m))) )
(:goal keep-object-at
:paras (?x - physob ?l - loc)
:goal-des (and (neq ?x B)
(imply (at ?x ?l)
(not (in ?x)))) )
(:goal keep-object-in
:paras (?x - physob)
:goal-des (in ?x) )
Figure 1: The action and goal schemata of the extended briefcase domain

10

fiGoal Recognition through Goal Graph Analysis

(:action mov-b-1
:paras (?l ?m - loc)
:pre (and (neq ?l ?m) (at B ?l))
:eff (and (at B ?m) (neg (at B ?l))
(when (in B)
(and (at B ?m)
(neg (at B ?l))))
(when (in C)
(and (at C ?m)
(neg (at C ?l))))
(when (in D)
(and (at D ?m)
(neg (at D ?l))))) )
In the second step, the conditional eects in mov-b-1 are further eliminated. Assume
that, at this time step, the following propositions are true: (at B H), (at C H), (at D H),
and (in D). Those conditional eects in mov-b-1, whose antecedents are not satised at
the time step, are removed. We therefore have action schema mov-b-2.
(:action mov-b-2
:paras (?l ?m - loc)
:pre (and (neq ?l ?m) (at B ?l))
:eff (and (at B ?m) (neg (at B ?l))
(when (in D)
(and (at D ?m)
(neg (at D ?l))))) )
The antecedent of the remaining conditional eect in mov-b-2 is already satised at the
time step and it is moved into the existing preconditions. We nally have action schema
mov-b-3 at the current time step. Action schema mov-b-3 is equivalent to the original
action schema mov-b at the current time step. It is mov-b-3 that is actually used for the
action schema, Moving the briefcase from one location to another, at the time step.
(:action mov-b-3
:paras (?l ?m - loc)
:pre (and (neq ?l ?m) (at B ?l)
(in D))
:eff (and (at B ?m) (neg (at B ?l))
(at D ?m) (neg (at D ?l))))
The universally quantied goal descriptions in a goal schema are treated in the same
way as the universally quantied preconditions in an action schema.

4. Goal Graphs, Valid Plans and Consistent Goals
In this section, we rst describe the structure of the Goal Graph. We then dene what we
mean when we say a set of observed actions forms a valid plan for achieving a goal given
11

fiHong

mov-b H O

put-in D H

mov-b O H

Actions

at B O
at D H

at B H
 at B O

at B H

at B O

 at B O

 at B H

at C H

at D H

at D H

at D O

at C H

at C H

 at D H
at C H
in D

in D

keep-object-at D H

keep-object-at D H

keep-object-at C H

keep-object-at C H
keep-object-in D*

Propositions

move-object D H O*

keep-object-at C H

Goals

keep-object-at C H
keep-object-in D
keep-object-at D O

Level 1

Level 2

Level 3

Level 4

Figure 2: A Goal Graph for an example of the extended briefcase domain
the Initial Conditions. We nally dene what we mean when we say a goal is consistent
with a set of observed actions.
4.1 Goal Graphs
A Goal Graph represents the actions observed, the propositions true or explicitly known to
be false, and the fully or partially achieved goals at consecutive time steps. A Goal Graph
also explicitly represents connections between propositions, actions, and goals in the graph.
A Goal Graph is a directed, levelled graph. The levels alternate between proposition
levels containing proposition nodes (each labelled with a proposition or negation of a proposition), representing the state of the world at consecutive time steps; goal levels containing
goal nodes (each labelled with a goal), representing goals fully or partially achieved at consecutive time steps; and action levels containing action nodes (each labelled with an action),
representing actions observed at consecutive time steps. The levels in a Goal Graph start
with a proposition level at time step 1, consisting of one node for each proposition true
in the Initial Conditions. They end with a goal level at the last time step, consisting of a
node for each of the goals either fully or partially achieved by the actions observed so far.
These levels are: propositions true at time step 1, goals achieved at time step 1, actions
observed at time step 1; propositions true or explicitly known to be false at time step 2,
goals achieved at time step 2, actions observed at time step 2; propositions true or explicitly
known to be false at time step 3, goals achieved at time step 3, and so forth.
The goal nodes in goal-level i are connected by description edges to their goal descriptions in proposition-level i. The action nodes in action-level i are connected by precondition
edges to their preconditions in proposition-level i, and by eect edges to their eects in
proposition-level i + 1. Those proposition nodes in proposition-level i are connected via
persistence edges to the corresponding proposition nodes in proposition-level i + 1, if their
truth values have not been aected by the actions in action-level i. These persistence edges
represent the eects of maintenance actions that simply bring forward proposition nodes in
proposition-level i, not aected by the actions in action-level i, to proposition-level i + 1.
In the example shown in Figure 2, three actions have been observed at three consecutive time steps: (mov-b O H), (put-in D H), and (mov-b H O). The Initial Conditions
12

fiGoal Recognition through Goal Graph Analysis

consist of: (at B O), (at D H), and (at C H). Action and goal nodes are on the top and
bottom parts of the graph respectively. The proposition nodes are in the middle part of
the graph. The edges connecting proposition nodes and the action node in the same level
are precondition edges. The edges connecting the action node in one level and propositions
in a subsequent level are eect edges. The edges connecting proposition nodes and goal
nodes in the same level are description edges. The edges connecting proposition nodes in
one level to proposition nodes in a subsequent level are persistence edges. The goal nodes
in bold represent consistent goals, among which the goal nodes in italics represent partially
achieved goals, while the others represent fully achieved goals. The edges in bold show
causal link paths. The goal nodes with an asterisk represent the recognised goals.
4.2 Valid Plans
We now dene what we mean when we say a set of observed actions forms a valid plan for
a goal, given the Initial Conditions.
Definition 1 (Causal Link) Let ai and aj be two observed actions at time steps i and j
respectively, where i < j. There exists a causal link between ai and aj , written as ai  aj ,
if and only if one of the eects of ai satises one of the preconditions of aj .
For instance, in the example shown in Figure 2, there exists a causal link between actions
(mov-b O H) at time step 1 and (put-in D H) at time step 2, since one of the eects of
the rst action, (at B H), satised one of the preconditions of the second action.
A goal can be treated as an action with goal descriptions as its preconditions and an
empty set of eects. Therefore, causal links can also be established from observed actions
to goals.
For instance, in the example shown in Figure 2, there exists a causal link between action
(mov-b H O) at time step 3 and goal (move-object D H O) at time step 4, since one of
the eects of action, (at D O), satised one of the goal descriptions of the goal.
Now a valid plan for a goal can be dened on the basis of temporal ordering constraints
and causal links over a set of observed actions. A valid plan P for a goal g, given the Initial
Conditions, is represented as a 3-tuple, < A, O, L >, in which A is a set of observed actions,
O is a set of temporal ordering constraints over A, and L is a set of causal links over A.
Definition 2 (Valid Plan) Let g be a goal, and P =< A, O, L >, where A is a set of
observed actions, O is a set of temporal ordering constraints, {ai < aj }, over A, and L is a
set of causal links, {ai  aj }, over A. Let I be the Initial Conditions. P is a valid plan
for g, given I, if and only if
1. the actions in A can be executed in I in any order consistent with O;
2. the goal g is fully achieved after the actions in A are executed in I in any order
consistent with O.
For instance, in the example shown in Figure 2, given the Initial Conditions, I = {(at
B O), (at D H), (at C H)}, P = ({a1 = (mov-b O H), a2 = (put-in D H), a3 = (mov-b
H O)}, {a1 < a2 , a2 < a3 }, {a1  a2 , a1  a3 , a2  a3 }) is a valid plan for goal g =
(move-object D H O).
13

fiHong

4.3 Consistent Goals
We now dene what we mean when we say a goal is consistent with a set of observed
actions. A set of observed actions is represented by a 2-tuple, < A, O >, in which A is a
set of observed actions and O is a set of temporal ordering constraints, {ai < aj }, over A.4
Definition 3 (Relevant Action) Given a goal g and a set of observed actions, < A, O >,
an action a  A is said to be relevant to g in the context of < A, O >, if and only if
1. there exists a causal link, a  g; or
2. there exists a causal link, a  b, where b  A is relevant to g and a < b is consistent
with O.
Definition 4 (Consistent Goal) A goal g is consistent with a set of observed actions,
< A, O >, if and only if the strict majority of a  A are relevant to g in the context of
< A, O >.
Proposition 1 (Valid Plan for Consistent Goal) Let < A, O > be a set of observed
actions, I be the Initial Conditions of < A, O >, g be a goal consistent with < A, O >.
There exists a set of causal links, L = {ai  aj }, over A and given I, P =< A, O, L >
is a valid plan for either g when g is fully achieved in the time step after < A, O > has
been observed or the achieved part of g when g is partially achieved in the time step after
< A, O > has been observed.
Proof. When g is fully achieved in the time step after a set of actions has been observed,
it directly follows Denitions 3 and 4 that there exists a set of causal links, L = {ai  aj },
over A. It then follows Denition 2 that given I, P =< A, O, L > is a valid plan for g.
When g is partially achieved in the time step after a set of actions has been observed,
let g be the achieved part of g. So g is fully achieved in the time step after the set of
actions has been observed, and it directly follows Denitions 2, 3 and 4 that there exists a
set of causal links, L = {ai  aj }, over A and given I, P =< A, O, L > is a valid plan for
g. 2
For instance, in the example shown in Figure 2, we have < A, O > = < {a1 = (mov-b O
H), a2 = (put-in D H), a3 = (mov-b H O)}, {a1 < a2 , a2 < a3 } >, and g = (move-object
D H O) is a fully achieved goal in the time step after < A, O > has been observed. According
to Denition 3 and 4, g is consistent with < A, O > because there exist causal links, a3  g
between a3 and g, a2  a3 between a2 and a3 , a1  a3 between a1 and a3 , and a1  a2
between a1 and a2 . Let I be the Initial Conditions of < A, O >, L = {a1  a2 , a1  a3 ,
a2  a3 }, according to Proposition 1, P =< A, O, L > is a valid plan for g. Furthermore,
causal link, a3  g, explains the purpose of a3 .
In summary, according to Denition 4 and Proposition 1, when we say a goal is consistent
with a set of observed actions, we mean that the strict majority of the observed actions are
relevant to the goal and the set of observed actions forms a valid plan for the goal or the
achieved part of it.
4. We assume that actions are observed at consecutive time steps but more than one action can be observed
at a time step.

14

fiGoal Recognition through Goal Graph Analysis

5. Goal Recognition Algorithms
We now describe our goal recognition algorithms. Our goal recognition algorithms run in a
two-stage cycle at each time step. In the rst stage, the GoalGraphConstructor algorithm
takes the actions observed in the time step and tries to extend the Goal Graph. In the second
stage, the GoalGraphAnalyser algorithm analyses the constructed Goal Graph to recognise
those fully or partially achieved goals, that are consistent with the actions observed so far,
and the valid plans for these goals or part of them. This two-stage cycle continues until no
action is observed at the next time step.
5.1 Constructing a Goal Graph
We use a 4-tuple < P, AO , GR , E > to represent a Goal Graph, where P is a set of proposition nodes, AO is a set of action nodes, GR is a set of goal nodes, and E is a set of edges. A
proposition node is represented by prop(p, i), where p is a positive or negative ground literal,
i is a time step. An action node is represented by action(a, i), where a is an observed action
and i is a time step. A goal node is represented by goal(g, i), where g is a goal and i is a
time step. A precondition edge is represented by precondition-edge(prop(p, i), action(a, i)),
an eect edge is represented by eect-edge(action(a, i), prop(p, i + 1)), a description edge is
represented by description-edge(prop(p, i), goal(g, i)), and a persistence edge is represented
by persistence-edge(prop(p, i  1), prop(p, i)).
The GoalGraphConstructor algorithm consists of two algorithms: the goal expansion
algorithm and the action expansion algorithm. The GoalGraphConstructor algorithm starts
with a Goal Graph, < P, {}, {}, {} >, that consists of only proposition-level 1 with nodes
representing the Initial Conditions.
Given a Goal Graph ending with proposition-level i, the goal expansion algorithm rst
extends the Goal Graph to goal-level i, with nodes representing goals fully or partially
achieved at time step i. The algorithm goes through every possible ground instance of goal
schemata. For every goal instance, it rst gets a set of goal descriptions. It then eliminates
all the universally quantied goal descriptions by dynamic expansion to get an equivalent
set of goal descriptions. A goal node is added onto goal-level i to represent an achieved
goal, if at least one of its goal descriptions has been satised at proposition-level i. It can
then be decided whether a goal is fully or partially achieved, based on whether all or some
of its goal descriptions have been satised respectively at proposition-level i. Meanwhile,
if a node in proposition-level i satises a goal description, a description edge connecting
the proposition node and the goal node is added onto the Goal Graph. Figure 3 shows the
goal expansion algorithm. The algorithm takes a Goal Graph < P, AO , GR , E > ending
with proposition-level i, time step i, and a set of goal schemata G as input. It returns an
updated Goal Graph ending with goal-level i after the goal expansion.
When actions are observed at time step i, the action expansion algorithm then extends the Goal Graph ending with goal-level i, to action-level i, with nodes representing
the observed actions. At the same time, the algorithm also extends the Goal Graph to
proposition-level i + 1, with nodes representing propositions true or explicitly known to be
false after the actions have been observed.
For every action observed at time step i, the algorithm rst instantiates an action schema
with the observed action to get a precondition set and an eect set. It then eliminates
15

fiHong

Goal-Expansion(< P, AO , GR , E >, i, G)
1. For every Gk  G
For every instance g of Gk
a. Get a set of goal descriptions Sg .
b. Get the equivalent set of Sg , Sg .
c. For every pg  Sg , where pg = not(pg ),
If prop(neg(pg ), i)  P , then
Add description-edge(prop(neg(pg), i), goal(g, i)) to E.
d. For every pg  Sg , where pg 	= not(pg ),
If prop(pg , i)  P , then
Add description-edge(prop(pg, i), goal(g, i)) to E.
e. If one of the goal descriptions of g is satised, then
Add goal(g, i) to GR .
2. Return with < P, AO , GR , E >.
Figure 3: The goal expansion algorithm

all the universally quantied preconditions and eects, as well as conditional eects, by
dynamic expansion to get equivalent precondition and eect sets. Meanwhile, if a node in
proposition-level i satises a precondition of the action, a precondition edge, connecting the
proposition node and the action node, is added onto the Goal Graph. For every eect of
the action, the action expansion algorithm simply adds a proposition node representing the
eect to proposition-level i + 1. The eect edge from the action node to the proposition
node is also added onto the Goal Graph.
After the above expansion, every proposition node at proposition-level i is brought
forward to proposition-level i + 1 by a maintenance action, if its truth value has not been
changed by an action observed at time step i (and it has not been added onto the Goal Graph
by an action observed at time step i).5 Persistence edges, connecting the corresponding
proposition nodes at the two proposition levels, are added onto the Goal Graph.
Figure 4 shows the action expansion algorithm. The algorithm takes a Goal Graph
< P, AO , GR , E > ending with goal-level i, the set of actions observed at time step i, Ai ,
time step i, and a set of action schemata A as input. It returns an updated Goal Graph
ending with proposition-level i + 1 after the action expansion. The expansion of the Goal
Graph from proposition-level i to proposition-level i + 1 simulates the eects of executing
the actions observed at time step i.
If otherwise there is no action observed at time step i, the GoalGraphConstructor algorithm nishes with nodes in goal-level i, representing all the possible goals either fully or
partially achieved after all these actions have been observed.

5. Our goal recognition algorithms allow redundant actions.

16

fiGoal Recognition through Goal Graph Analysis

Action-Expansion(< P, AO , GR, E >, Ai , i, A)
1. For every ai  Ai
a. Add action(ai, i) to AO .
b. Instantiate an action schema in A with ai to get a precondition set
SP , and an eect set SE .
c. Get the equivalent sets of SP and SE , SP  and SE .
d. For every pp  SP , where pp = not(pp ),
If prop(neg(pp, i)  P , then
Add precondition-edge(prop(neg(pp, i), action(ai, i)) to E.
e. For every pp  SP , where pp 	= not(pp ),
If prop(pp, i)  P , then
Add precondition-edge(prop(pp, i), action(ai, i)) to E.
f. For every pe  SE
i. Add prop(pe, i + 1) to P .
ii. Add eect-edge(action(ai, prop(pe, i + 1)) to E.
2. For every prop(p, i)  P
If prop(p, i + 1) 
/ P , then
If prop(p, i + 1) 
/ P , then Add prop(p, i + 1) to P ;
Add persistence-edge(prop(p, i), prop(p, i + 1)) to E.
3. Return with < P, AO , GR , E >.
Figure 4: The action expansion algorithm
Theorem 1 (Polynomial Size and Time) Consider a goal recognition problem with s
observed actions in t time steps, a nite number of objects at each time step, p propositions
in the Initial Conditions, and m goal schemata each having a constant number of parameters.
Let l1 be the largest number of eects of any action schema, and l2 be the largest number
of goal descriptions of any goal schema. Let n be the largest number of objects at all time
steps. Then, the size of the Goal Graph of t + 1 levels created by the GoalGraphConstructor
algorithm, and the time needed to create the graph, are polynomial in n, m, p, l1 , l2 , and s.
Proof. The maximum number of nodes in any proposition level is O(p + l1 s). Let k
be the largest number of parameters in any goal schema. Since any goal schema can be
instantiated in at most nk distinct ways, the maximum numbers of nodes and edges in any
goal level are O(mnk ) and O(l2 mnk ) respectively. It is obvious that the time needed to
create both nodes and edges in any level is polynomial in the number of nodes and edges
in the level. 2
Theorem 2 The GoalGraphConstructor algorithm is sound: Any goal it adds to the Goal
Graph at time step i is one either fully or partially achieved at time step i in the state of the
world. The algorithm is complete: If a goal has been either fully or partially achieved by the
observed actions up to time step i  1, then the algorithm will add it to the Goal Graph at
time step i, under the assumption that all the possible goals are restricted to the categories
of goal schemata.
17

fiHong

Proof (soundness). Proposition-level 1 of the Goal Graph consists of only the Initial
Conditions, representing the state of the world at time step 1 before any action has been
observed. The Goal Graph is extended from proposition-level i  1 to proposition-level i, by
adding only the eects of the actions observed at time step i1, and bringing forward all the
other proposition nodes that have not been aected by these actions from proposition-level
i  1 to proposition-level i. Therefore, proposition-level i of the Goal Graph represents the
state of the world at time step i, that has been changed from the Initial Conditions after
the actions have been observed at time steps 1, ..., i 1.
A goal added to the Goal Graph at time step i by the algorithm is a fully or partially
achieved goal in proposition-level i of the Goal Graph. Therefore, it is a goal that is fully
or partially achieved in the state of the world at time step i.
Proof (completeness). Suppose a goal has been either fully or partially achieved by the
actions observed at time steps 1, ..., i 1. This goal is then either fully or partially achieved
in proposition-level i of the Goal Graph. Since goal-level i of the Goal Graph consists
of all the possible instances of the goal schemata, that are fully or partially achieved in
proposition-level i of the Goal Graph, and the goal is an instance of a goal schema, it is one
of the fully or partially achieved goal instances in proposition-level i. The algorithm will
therefore add the goal to goal-level i in the Goal Graph. 2
5.2 Recognising Consistent Goals and Valid Plans
The GoalGraphAnalyser algorithm analyses the constructed Goal Graph to recognise consistent goals and valid plans. We assume that the strict majority of the observed actions are
relevant to the goal intended by the agent in the context of the agents actions. Therefore,
the goal intended by the agent is consistent with the set of observed actions, and a goal
may be the intended goal if it is consistent with the set of observed actions. In order to
decide whether a goal is consistent with a set of observed actions, that is, whether it is
relevant to the strict majority of the observed actions, we need to recognise causal links
between either two observed actions or an observed action and the goal. We now dene
two particular types of paths, we call causal link paths, in the constructed Goal Graph. We
prove in Theorems 3 and 4 that causal links can be recognised by identifying causal link
paths.
Definition 5 Given a Goal Graph, let ai be an action observed at time step i and gj be a
goal fully or partially achieved in time step j, where i < j. A path that connects ai to gj
via an eect edge, zero or more persistence edges, and a description edge, is called a causal
link path between ai and gj .
Theorem 3 Given a Goal Graph, there exists a causal link, ai  gj , between an action ai
at time step i and a goal gj at time step j, where i < j, if ai is connected to gj via a causal
link path.
Proof. According to Denition 5, a causal link path between ai and gj consists of an
eect edge, zero or more persistence edges, and a description edge. The eect edge on the
path connects ai to a proposition node in proposition-level i + 1, representing one of the
eects of ai . When j = i + 1, there is no persistence-edge on the path and this proposition
18

fiGoal Recognition through Goal Graph Analysis

node is connected to gj by a description edge. When j > i + 1, this proposition node is
brought forward to proposition-level j via j i1 persistence-edges by j i1 maintenance
actions, and the brought-forward proposition node in proposition-level j is connected to gj
by a description edge. In either case, one of the eects of ai satised one of the goal
descriptions of gj . Since a goal can be treated as an action with the goal descriptions as the
preconditions and an empty set of eects, according to Denition 1, there exists a causal
link between ai and gj . 2
Definition 6 Given a Goal Graph, let ai and aj be two actions observed at time steps i
and j respectively, where i < j. A path that connects ai to aj via an eect edge, zero or
more persistence edges, and a precondition-edge, is called a causal link path between ai and
aj .
Theorem 4 Given a Goal Graph, there exist a causal link, ai  aj , between an action ai
at time step i and an action aj at time step j, where i < j, if ai is connected to aj via a
causal link path.
The proof of Theorem 4 is similar to Theorem 3. The details of the proof are omitted.
Given a constructed Goal Graph < P, AO , GR , E > of t levels, the GoalGraphAnalyser
algorithm shown in Figure 5 recognises every consistent goal from the goals in goal-level t,
by deciding whether the strict majority of the observed actions are relevant to it. This is
done by rst nding those relevant actions from the observed actions, that are connected to
the goal by causal link paths. For each of the already-known relevant actions, the algorithm
tries to nd more relevant actions from the observed actions, that are connected to it by
causal link paths. This continues until no more relevant action is found. The consistent
goal recognised and the valid plan for the goal or part of it are represented by a 3-tuple,
< gt, < AO , O, La >, Lg >, where gt is the goal, La is a set of causal links over the observed
actions, and Lg is a set of causal links between some of the observed actions and the goal.
< AO , O, La > represents a valid plan for gt or part of it, and Lg further explains the
purposes of some of the observed actions.
Proposition 2 The GoalGraphAnalyser algorithm is sound: Any goal g it recognises at
time step t is consistent with the observed actions so far, and the plan it organises for g or
part of g is valid.
Proof. The GoalGraphAnalyser algorithm recognises a goal g at time step t, when the
strict majority of the observed actions are connected to g, either directly by a causal link
path or indirectly by a chain of causal link paths. When an observed action is connected
to g directly by a causal link path, according to Theorem 3 and Denition 3, there exists
a causal link between the observed action and g, and the observed action is relevant to
g. When an observed action is connected to g indirectly by a chain of causal link paths,
according to Theorem 3, Theorem 4, and Denition 3, there is a chain of causal links
between the observed action and g, and the observed action is relevant to g. Since the strict
majority of the observed actions are relevant to g, according to Denition 4, g is consistent
with the set of observed actions. Furthermore, according to Proposition 1, the plan the
GoalGraphAnalyser algorithm organises for g or part of g, < AO , O, La >, is a valid plan.2
19

fiHong

GoalGraphAnalyser(< P, AO , GR , E >, t)
1. For every gt  GR in goal-level t
a. AO   {}, A  {}, Lg  {}, La  {}.
b. For every ai  AO connected to gt by a causal link path
Add ai  gt to Lg ; and
Add ai to AO ; and
Add ai to A.
c. If A = {} and for most of ai  AO , ai  AO , then
Get all the ordering constraints, O, over AO ; and
Add < gt , < AO , O, La >, Lg > to GoalPlan.
d. If A 	= {}, then
Remove an action aj from A; and
For every ai  AO connected to aj by a causal link path
Add ai  aj to La ; and
/ AO , then Add ai to AO , ai to A; and
If ai 
Go to 1c.
2. Return with GoalPlan.
Figure 5: The GoalGraphAnalyser algorithm

In the example shown in Figure 2, the goal nodes in bold represent three consistent
goals, among which the goal node in italics represents a partially achieved goal, while the
other two represent two fully achieved goals. The edges in bold show causal link paths.

Theorem 5 (Polynomial Space and Time) Consider a t-level Goal Graph. Let l1 be
the number of fully or partially achieved goals at time step t, m1 be the largest number of
goal descriptions in any of these goals, l2 be the number of the observed actions, and m2
be the largest number of preconditions in any of these actions. The space size of possible
causal link paths, that connect the goals to the observed actions and that connect the observed
actions to other observed actions, and the time needed to recognise all the consistent goals,
are polynomial in l1 , l2 , m1 , and m2 .

Proof. Persistence edges do not branch in a Goal Graph. For each of the goals in
goal-level t, the maximum number of paths searched for those observed actions, that are
connected to the goal by causal link paths and hence relevant to it, is O(m1 ). For each of
the relevant actions to the goal, the maximum number of paths searched for those observed
actions, that are connected to the relevant action by causal link paths and hence also
relevant to the goal, is O(m2 ). There are at maximum only l1 goals in goal-level t and
l2 relevant actions to any of these goals. So the space size of possible causal link paths is
O(l1 (m1 + l2 m2 )). The time needed to recognise all the consistent goals is polynomial in
the space size. 2
20

fiGoal Recognition through Goal Graph Analysis

5.3 Goal Redundancy
The GoalGraphAnalyser algorithm recognises fully or partially achieved goals at a time step,
that are consistent with the actions observed so far. Among these consistent goals, fully
achieved goals better explain the actions observed so far. For instance, in the example shown
in Figure 2, two consistent goals are recognised by the GoalGraphAnalyser algorithm at time
step 4: (move-object D H O) is fully achieved and (keep-object-at D O) is partially
achieved. Between these two consistent goals, the fully achieved goal better explains the
observed actions so far. If, for instance, another action (take-out D) is observed at next
time step, (keep-object-at D O) becomes fully achieved and remains consistent with the
observed actions. So at that time step, it best explains the observed actions. A partially
achieved goal, that is consistent with the observed actions so far, can remain consistent
when more actions are observed in the future and becomes fully achieved. So choosing the
fully achieved goal and making the partially achieved goal redundant does not rule out the
possibility of the partially achieved goal remaining consistent and becoming fully achieved
in the future. Based on this principle, we can make a partially achieved consistent goal
at a time step redundant, if its satised goal descriptions are implied by the satised goal
descriptions of another fully or partially achieved consistent goal.
Definition 7 A partially achieved consistent goal at a time step is redundant, if the set of
its satised goal descriptions is either a subset of the goal descriptions of a fully achieved consistent goal or a proper subset of the satised goal descriptions of another partially achieved
consistent goal at the same time step.
For instance, at time step 4 the set of satised goal descriptions of (keep-object-at D
O) is a subset of the goal descriptions of (move-object D H O). The partial achievement
of (keep-object-at D O) has been implied by the full achievement of (move-object D H
O). So (keep-object-at D O) is made redundant by (move-object D H O) at time step
4.
A fully achieved consistent goal at a time step, however, can be made redundant only if
its goal descriptions are implied by the goal descriptions of another fully achieved consistent
goal at the same time step.
Definition 8 A fully achieved consistent goal at a time step is redundant, if the set of its
goal descriptions is a subset of the goal descriptions of another fully achieved consistent goal
at the same time step.
5.4 The Most Consistent Goals
After all the redundant goals have been removed from the set of consistent goals at a time
step, there might still be more than one consistent goal in the set. If this is the case, the
numbers of observed actions that are relevant to these remaining consistent goals will be
compared. Those remaining goals that have the maximum number of relevant actions will
be chosen as the most consistent goals at the time step.
Definition 9 Given a set of consistent goals at a time step, a consistent goal in the set is
the most consistent goal in the set, if it has the maximum number of relevant actions among
all the consistent goals in the set.
21

fiHong

For instance, in the example shown in Figure 2, if another action (take-out D) is
observed at time step 4, both (move-object D H O) and (keep-object-at D O) are consistent goals at time step 5, and neither of them is redundant. (keep-object-at D O)
is relevant to all the ve observed actions, while (move-object D H O) is relevant to only
four of the observed actions. According to Denition 9, (keep-object-at D O) is the most
consistent goal at time step 5.
In the example shown in Figure 2, the goal nodes with an asterisk represent the consistent
goals eventually remaining after the two processes of removing redundant goals and selecting
the most consistent goals.

6. Experimental Results
We implemented our goal recognition algorithms in Prolog and tested them in terms of
accuracy, eciency, and scalability on a desktop with a Pentium III processor at 600 MHz.
We tested our algorithms on a set of data in the UNIX domain collected at the University
of Washington. To collect the data, the subjects were given goals described in English rst
and they then tried to achieve each goal by executing UNIX commands. The commands
issued to UNIX by the subject and the responses from UNIX to these commands were
recorded in the data set. Some of the commands issued by the subject were not valid and
could not be executed in UNIX. So the responses from UNIX to these invalid commands
were actually error messages. For each of the goals the subjects tried to achieve, they
indicated success or failure with regard to the achievement of the goal.
There are 14 goals in the UNIX data set, and each of these goals was tried by 5 subjects
on average. As shown in Table 1, these goals can be classied into four types. The rst type
of goals are those of locating a le that has some of the properties, such as extension, size,
contents, ownership, date, word count, and compression. The second type of goals are those
of locating a machine that has some of the properties, such as load and logged-in users. The
third type of goals are those of locating a printer that has some of the properties, such as
print jobs and out of paper. The fourth type of goals are those of compressing all or large
les in a directory. For the fourth type of goals, universally quantied goal descriptions
are needed in the corresponding goal schemata. In addition, there are also two compound
goals, G3 and G9 , that are the conjunctions of two goals of the second type.
To test our algorithms, the sequences of UNIX commands, recorded in the data set, were
taken as the observed actions at consecutive time steps. We took only the valid commands,
that were successfully executed in UNIX, and ltered out the invalid commands, that
UNIX failed to execute and responded only in the error messages. We created 35 action
schemata for a set of commonly used UNIX commands, including those executed by the
subjects. We also created 249 goal schemata, including 129 le-search goal schemata (for
goals of locating a le that has some properties), 15 non-le-search goal schemata (for goals
of locating a machine or a printer that has some properties and goals of compressing all
or large les in a directory) and 105 goal schemata of paired non-le-search goals. The
14 goals in the UNIX data set are the instances of some of these goal schemata. We rst
tested our algorithms with respect to accuracy and eciency, that is, the number of goals
remaining after a sequence of observed actions has been processed, and the average time
22

fiGoal Recognition through Goal Graph Analysis

G1
G2
G3
G4
G5
G6
G7
G8
G9
G10
G11
G12
G13
G14

Find a le named core.
Find a le that contains motivating and whose name ends in .tex.
Find a machine that has low (< 1.0) load; and
determine if Oren Etzioni is logged into the machine named chum.
Compress all large (> 10,000 bytes) les in the Testgrounds subdirectory tree.
Compress all les in the directory named backups [Dont use *].
Find a large le (> 100,000 bytes) that hasnt been changed for over a month.
Find a le that contains less than 20 words.
Find a laser printer in Sieg Hall that has an active print job.
Find a Sun on the fourth oor that has low (< 1.0) load;
and determine if Dan Weld is active on the machine named chum.
Find a printer that is out of paper.
Find a le named oldpaper in neal/Testgrounds subdirectory.
Find a le of length 4 in neal/Testgrounds subdirectory.
See if Dan Weld is logged into chum.
Find a machine that Dan Weld is logged into.

Table 1: The 14 goals in the UNIX data set collected at the University of Washington
taken to construct a Goal Graph, analyse the constructed Goal Graph, and run through a
cycle of Goal Graph construction and analysis, when an action was observed at a time step.
Table 2 gives a summary of the empirical results showing the accuracy of our algorithms.
The rst column shows the goals the subject tried to achieve. The achieved goals were the
goals fully or partially achieved after the last observed action had been processed. The
consistent goals were the fully or partially achieved goals that were consistent with the
sequence of observed actions.6 The remaining goals were the goals that remained after the
redundant goals had been removed and the most consistent goals had been selected. The
last column shows whether the given goal was among the remaining goals.
As shown in Table 2, our algorithms successfully recognised 13 out of 14 given goals.
They failed to recognise only one given goal, G10 , simply because the sequence of commands
executed by the subject actually failed to achieve the goal. In terms of the UNIX data set,
goal recognition occurs when our algorithms return a single, consistent goal. This occurred
on G2 , G4 , G7 , G8 , G9 , G13 , and G14 . On G1 , G3 , G6 , G11 , and G12 , more than one goal
was recognised, including the goal given to the subject. On G1 , G6 , G11 , and G12 , our
algorithms recognised that the subject tried to nd one of the les with some properties
in the directory but did not know which le it was. For instance, on G1 , the intended
goal was to nd a le named core. The subject successfully found the le named core in
the directory, other, by executing a command, ls, to list all les in the directory. Since
there were other les, greenmouse, paper.tex, and action.ps.Z, in the same directory,
6. In our experiments on the UNIX data set, a goal, to which more than two third of the observed actions
were relevant, was recognised as a consistent goal. The threshold on the number of relevant actions is
dependent on the application domain though the strict majority of the actions must be relevant. If the
threshold is too high, our algorithms might fail to recognise the intended goal. On the other hand, if it is
too low, the set of recognised goals might be too large to provide much value because of great ambiguity
on the intended goal.

23

fiHong

goal
G1
G2
G3
G4
G5
G6
G7
G8
G9
G10
G11
G12
G13
G14

achieved
goals
15
26
14
56
46
107
85
6
22
9
12
60
1
8

consistent
goals
15
6
4
18
33
47
4
4
5
0
12
44
1
2

remaining
goals
4
1
2
1
6
4
1
1
1
0
2
6
1
1

given goal
recognised















Table 2: Empirical results of the UNIX domain showing the accuracy of our algorithms

and all these les were also listed by the same command, our algorithms recognised four
goals, nding a le named core, nding a le named greenmouse, nding a le named
paper with extension tex, and nding a le named action with extension ps which
is also compressed. On G3 , our algorithms recognised that the subject tried to nd one
of the users on a machine but did not know who he was. This was as good as a human
observer could do because you simply could not tell from the observed actions which le or
user the subject was trying to nd. These recognised goals can be generalised into a single,
consistent goal nding a le in the directory or nding a user using a machine, where
variables are allowed in the recognised goals.
On G5 , our algorithms recognised 6 consistent goals. Among these goals, ve goals
were to nd each of the ve les with the same properties of compression and extension
in the directory named backups. Another goal was to gunzip all les in the directory
named backups. A human observer could probably do better by recognising the goal of
gunzipping all les in the directory named backups, because it accounted better for the
gunzip command that had been observed. It was very unlikely that the subject gunzipped
all les in the directory in order to nd a le with gunzip compression.
Among the goals we tested, G1 , G2 , G3 , and G4 were originally tested by Lesh and
Etzioni (1995). Our empirical results show a signicant improvement on the accuracy of
the goal recogniser implemented by them in terms of the remaining goals. Our algorithms
have 4, 1, 2, and 1 remaining goals on G1 , G2 , G3 , and G4 respectively, while their goal
recogniser has 155, 37, 1, and 15 remaining goals on G1 , G2 , G3 , and G4 respectively, after
the last observed action has been processed. Furthermore, the 4 and 2 remaining goals that
our algorithms have on G1 and G3 can be generalised into two single goals. These results
show that our algorithms perform extremely well with regard to accuracy.
24

fiGoal Recognition through Goal Graph Analysis

goal
G1
G2
G3
G4
G5
G6
G7
G8
G9
G10
G11
G12
G13
G14

length of
observation
2.25
16
3.0
20.5
9
8.78
9.11
3.5
12
15
7
17
1
2

construction
time
0.535
0.382
0.021
1.036
0.426
12.185
16.473
0.014
0.034
0.018
0.051
0.821
0.010
0.013

analysis
time
0.013
0.102
0.009
3.609
0.565
4.143
8.581
0.002
0.050
0.007
0.020
0.774
0.001
0.004

time
per cycle
0.547
0.484
0.030
4.645
0.991
16.329
25.054
0.017
0.084
0.025
0.071
1.595
0.011
0.017

Table 3: Empirical results of the UNIX domain showing the eciency of our algorithms

It is worth noting that Lesh and Etzionis algorithm converges before the last observed
action has been processed. So their algorithm works towards goal prediction, while our algorithms emphasise the explanation of the observed actions, by recognising fully or partially
achieved goals that are consistent with these actions. Their algorithm can quickly prune
out inconsistent goals to get a converged set of hypothesised goals, though the number of
hypothesised goals in the set can sometimes be large. The next step of their work might
be to assign probabilities to these hypothesised goals to dierentiate a single goal from the
others, when there exists only one intended goal. On the other hand, it is less desirable for
the goals recognised by our algorithms to be dierentiated from each other as the accuracy
of the algorithms is usually high. Our algorithms, however, cannot recognise a goal until it
has been fully or partially achieved.
Table 3 gives a summary of the empirical results showing the eciency of our algorithms.
The length of observation was the average number of observed actions executed by the
subjects to achieve the given goal. The construction time was the average time it took to
construct the Goal Graph at a time step. The analysis time was the average time it took
to analyse the constructed Goal Graph at a time step. The time per cycle was the average
time it took to go through a two-stage cycle of Goal Graph construction and analysis,
when an observed action was processed at a time step, including constructing the Goal
Graph, recognising the consistent goals, removing redundant goals, and selecting the most
consistent goals. As shown in Table 3, on average at a time step, it took 2.287 CPU seconds
to construct the Goal Graph, 1.277 CPU seconds to analyse the Goal Graph, and 3.564
CPU seconds to process an observed action. Since our algorithms have been written in the
less ecient Prolog and run on a desktop with a Pentium III processor at 600 MHz, more
eciency could be achieved. The construction time, analysis time, and time per cycle could
25

fiHong

Average CPU Seconds

250
200

C-Time

150

A-Time
100

P-Time

50
0
10080

19790

29665

39540

49415

59290

69165

81015

90890 100765

Number of Goals

Figure 6: Empirical results of the UNIX domain showing the scalability of our algorithms

be reduced to well below a CPU second, if the algorithms are coded in a more ecient
programming language and run on a faster machine.
Compared to the empirical results on the eciency of the goal recogniser implemented
by Lesh and Etzioni (1995), on average, it took 0.547, 0.484, 0.030, and 4.645 CPU seconds
to process an observed action by our algorithms on G1 , G2 , G3 , and G4 respectively, while
1.616, 1.643, 0.648, and 1.610 CPU seconds were taken by their goal recogniser to process
an observed action on the same goals. The average time to process an observed action
was roughly around 1.4 CPU seconds on both a desktop with a Pentium III processor at
600 MHz by our algorithms coded in Prolog and a SPARC 10 by their goal recogniser
coded in Lisp. Given that two dierent programming languages and machines were used
for the implementation of two dierent systems, this comparison is hardly meaningful. It is
however apparent that the use of more ecient programming languages and machines can
signicantly speed up our algorithms.
We also tested the scalability of our goal recognition algorithms in the UNIX domain.
We tested how the eciency of our algorithms was aected by the number of possible goals,
that was in turn aected by the number of objects in the universe of objects. We created a
series of spaces of approximate 104 , 2  104 , 3  104 , up to 105 possible goals respectively
based on the data recorded on G7 in the UNIX data set.7 For doing this, We changed the
les and directories in the le hierarchy, as well as the properties of the les, in the original
data set for G7 , to increase or decrease the number of objects in the universe of objects,
while keeping the les and directories related to the intended goal and the properties of
the related les unchanged. In the sense that while part of the Initial Conditions on the
intended goal remained the same, the rest of the Initial Conditions were changed to create
7. The number of goal schemata remains unchanged.

26

fiGoal Recognition through Goal Graph Analysis

the appropriate number of candidate goals. The change in the le hierarchy reected the
change in the complexity of the le hierarchy.
The original sequences of commands recorded in the data set were used in the experiments, in conjunction with the dierent sets of the Initial Conditions, for the creation of
dierent spaces of candidate goals. Figure 6 shows that the average CPU time taken at
a time step to construct and analyse the Goal Graph (shown as C-Time and A-Time respectively in Figure 6), and to process the observed action as a whole (shown as P-Time in
Figure 6) was approximately linear in the number of candidate goals.

7. Conclusion and Future Work
In this paper, we presented a new approach to goal recognition in which a graph structure
called a Goal Graph is constructed and analysed for goal recognition. We described two
algorithms for constructing and analysing a Goal Graph. Our algorithms recognise partially
or fully achieved goals that are consistent with the observed actions, and reveal valid plans
for the recognised goals or part of them. Our algorithms do not need a plan library. They
allow redundant, extraneous, and partially ordered actions. They are sound, polynomialtime, and polynomial-space.
Our empirical experiments show that our algorithms recognise goals with great accuracy.
They are computationally ecient. They can be scaled up and applied to domains where
there are tens of thousands of goals and plans. Even though one of our goal recognition
algorithms, the GoalGraphAnalyser algorithm, is not complete, they recognised all the
intended goals in the UNIX data set that were successfully achieved by the subjects.
A limitation of our goal recognition algorithms is that sometimes more than one goal
is recognised, though the number of recognised goals is usually very small. Our algorithms
cannot tell which goal is the most probable one when there is only one intended goal. For
instance, on G5 in the UNIX data set, our algorithms recognised 6 goals. Even though the
intended goal, gunzipping all les in the directory called backups, was among these goals,
and it was probably the most likely one compared to the others, our algorithms could not
dierentiate it from the others. Sometimes our algorithms can recognise a unique goal that
implies the intended goal but cannot make it more specic. For instance, on G7 in the UNIX
data set, our algorithms recognised a unique goal, nding the le, index.tex, with its word
count and extension. Even though the achievement of this goal implied the achievement of
the intended goal, nding a le that contains less than 20 words, our algorithms could not
be more specic. These problems are due to the incomplete information we had from the
observed actions. On G7 , the observed actions did not have any indication on the likelihood
of the recognised goals being the intended goal. The subject actually achieved G7 because
he knew that the word count was less than 20 words by knowing the word count of the le,
but no observed action was directly used to achieve this.
Several attempts (Carberry, 1990; Charniak & Goldman, 1993; Huber et al., 1994;
Forbes et al., 1995; Pynadath & Wellman, 1995; Bauer, 1995; Albrecht et al., 1998) have
been made to incorporate uncertainty representation and reasoning techniques into plan
recognition for handling uncertain and incomplete information, so that a single plan can
be distinguished from a set of candidate plans. However, these systems rely heavily on the
availability of planning knowledge and to a certain extent the use of plan libraries. In future
27

fiHong

work, we intend to investigate the possibility of incorporating uncertainty representation
and reasoning mechanisms into our Goal-Graph-based approach to goal recognition, so that
a unique, specic goal can be recognised when there is only one intended goal, while all the
good features of our Goal-Graph-based approach are kept.
In future work, we also intend to explore the extent to which our Goal Graph representation can be used for probabilistic goal recognition. In particular, we will consider problem
settings in which the eects of actions are probabilistic and the objective of goal recognition
is to recognise consistent goals with the highest probability of achievement.
Another area of future work is to recognise goals even before they have been partially
or fully achieved by the actions observed so far. In this regard, our current goal recognition
algorithms can be used to recognise goals that each sequence of actions in a very large data
set can achieve. Then each sequence of actions in the data set and the recognised goals can
be used to acquire a probabilistic model of goal prediction by a machine learning method.
This probabilistic model takes the actions observed so far and predicts possible goals even
when they are not partially or fully achieved.

Acknowledgments
This paper is a revised and extended version of a paper appeared in the Proceedings of
AAAI-2000 (Hong, 2000). The author wishes to thank Neal Lesh for providing the UNIX
data set, and anonymous referees for their insights and constructive criticisms, which have
helped improve the paper signicantly.

References
Albrecht, D. W., Zukerman, I., & Nicholson, A. E. (1998). Bayesian model for keyhole
plan recognition in an adventure game. User Modeling and User-Adapted Interaction,
8 (1-2), 547.
Allen, J. F. (1983). Recognizing intentions from natural language utterances. In Brady, M.,
& Berwick, B. (Eds.), Computational Models of Discourse, pp. 107166. MIT Press,
Cambridge, MA.
Allen, J. F., & Perrault, C. R. (1980). Analyzing intention in utterances. Articial Intelligence, 15, 143178.
Anderson, C., Smith, D. E., & Weld, D. (1998). Conditional eects in Graphplan. In
Proceedings of the 4th International Conference on AI Planning Systems, pp. 4453.
Bauer, M. (1995). Dempster-Shafer approach to modeling agent preferences for plan recognition. User Modeling and User-Adapted Interaction, 5 (3-4), 317348.
Bauer, M. (1998). Acquisition of abstract plan descriptions for plan recognition. In Proceedings of AAAI-98, pp. 936941.
Bauer, M., & Paul, G. (1993). Logic-based plan recognition for intelligent help systems. In
Backstrom, C., & Sandewall, E. (Eds.), Current Trends in AI Planning, pp. 6073.
IOS Press.
28

fiGoal Recognition through Goal Graph Analysis

Blum, A. L., & Furst, M. L. (1997). Fast planning through Planning Graph analysis.
Articial Intelligence, 90, 281300.
Carberry, S. (1986). User models: the problem of disparity. In Proceedings of the 11th
International Conference on Computational Linguistics, pp. 2934.
Carberry, S. (1988). Modeling the users plans and goals. Computational Linguistics, 14 (3),
2337.
Carberry, S. (1990). Incorporating default inferences into plan recognition. In Proceedings
of AAAI-90, pp. 471478.
Carver, N. F., Lesser, V. R., & McCue, D. L. (1984). Focusing in plan recognition. In
Proceedings of AAAI-84, pp. 4248.
Charniak, E., & Goldman, R. P. (1993). A Bayesian model of plan recognition. Articial
Intelligence, 64, 5379.
Forbes, J., Huang, T., Kanazawa, K., & Russell, S. (1995). The BATmobile: Towards a
Bayesian automated taxi. In Proceedings of IJCAI-95, pp. 18781885.
Gazen, B., & Knoblock, C. (1997). Combining the expressivity of UCPOP with the eciency
of Graphplan. In Proceedings of the 4th European Conference on Planning, pp. 221
233.
Goodman, B. A., & Litman, D. J. (1992). On the interaction between plan recognition and
intelligent interfaces. User Modeling and User-Adapted Interaction, 2, 83115.
Grosz, B. J., & Sidner, C. L. (1990). Plans in discourse. In P. R. Cohen, J. M., & Pollack,
M. E. (Eds.), Intentions in Communication, pp. 417444. MIT Press, Cambridge, MA.
Hong, J. (2000). Goal Graph construction and analysis as a paradigm for plan recognition.
In Proceedings of AAAI-2000, pp. 774779.
Huber, M. J., & Durfee, E. H. (1995). Deciding when to commit to action during
observation-based coordination. In Proceedings of the First International Conference
on Multi-Agent Systems, pp. 163170.
Huber, M. J., Durfee, E. H., & Wellman, M. P. (1994). The automated mapping of plans for
plan recognition. In Proceedings of the 10th Conference on Uncertainty in Articial
Intelligence, pp. 344351.
Hu, K., & Lesser, V. (1988). A plan-based intelligent assistant that supports the software
development process. In Proceedings of the ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, pp. 97106.
Kautz, H. A. (1987). A Formal Theory of Plan Recognition. PhD Thesis, University of
Rochester.
Koehler, J., Nebel, B., Homann, J., & Dimopoulos, Y. (1997). Extending planning graphs
to an ADL subset. In Proceedings of the 4th European Conference on Planning, pp.
273285.
Lesh, N., & Etzioni, O. (1995). A sound and fast goal recognizer. In Proceedings of IJCAI95, pp. 17041710.
29

fiHong

Lesh, N., & Etzioni, O. (1996). Scaling up goal recognition. In Proceedings of the 5th
International Conference on Principles of Knowledge Representation and Reasoning,
pp. 178189.
Litman, D. J., & Allen, J. F. (1987). A plan recognition model for sub-dialogues in conversation. Cognitive Science, 11 (2), 163200.
Mooney, R. J. (1990). Learning plan schemata from observation: Explanation-based learning
for plan recognition. Cognitive Science, 14 (4), 483509.
Pednault, E. P. D. (1988). Synthesizing plans that contain actions with context-dependent
eects. Computational Intelligence, 4 (4), 356372.
Pednault, E. P. D. (1989). ADL: Exploring the middle ground between STRIPS and the
Situation Calculus. In Proceedings of the 1st International Conference on Knowledge
Representation and Reasoning, pp. 324332.
Pollack, M. E. (1990). Plans as complex mental attitudes. In Cohen, P. R., Morgan,
J., & Pollack, M. E. (Eds.), Intentions in Communication, pp. 77101. MIT Press,
Cambridge, MA.
Pynadath, D. V., & Wellman, M. P. (1995). Accounting for context in plan recognition, with
application to trac monitoring. In Proceedings of the 11th Conference on Uncertainty
in Articial Intelligence, pp. 472481.
Schank, R., & Abelson, R. (1977). Scripts, Plans, Goals, and Understanding. Erlbaum.
Sidner, C. L. (1985). Plan parsing for intended response recognition in discourse. Computational Intelligence, 1 (1), 110.
Wilensky, R. (1983). Planning and Understanding. Addison-Wesley Publishing Company,
Reading, MA.
Wilensky, R., & et al. (1988). The Berkeley unix consultant project. Computational Linguistics, 14 (4), 3584.

30

fiJournal of Artificial Intelligence Research 15 (2001) 289-318

Submitted 3/01; published 10/01

Ecient Methods for Qualitative Spatial Reasoning
renz@dbai.tuwien.ac.at

Jochen Renz
Institut f
ur Informationssysteme, Technische Universit
at Wien
Favoritenstr.9, A-1040 Wien, Austria

nebel@informatik.uni-freiburg.de

Bernhard Nebel
Institut f
ur Informatik, Albert-Ludwigs-Universit
at
Am Flughafen 17, D-79110 Freiburg, Germany

Abstract

The theoretical properties of qualitative spatial reasoning in the RCC-8 framework
have been analyzed extensively. However, no empirical investigation has been made yet.
Our experiments show that the adaption of the algorithms used for qualitative temporal
reasoning can solve large RCC-8 instances, even if they are in the phase transition region
{ provided that one uses the maximal tractable subsets of RCC-8 that have been identified
by us. In particular, we demonstrate that the orthogonal combination of heuristic methods
is successful in solving almost all apparently hard instances in the phase transition region
up to a certain size in reasonable time.
1. Introduction

Representing qualitative spatial information and reasoning with such information is an
important subproblem in many applications, such as natural language understanding, document interpretation, and geographical information systems. The RCC-8 calculus (Randell,
Cui, & Cohn, 1992b) is well suited for representing topological relationships between spatial
regions. Inference in the full calculus is, however, NP-hard (Grigni, Papadias, & Papadimitriou, 1995; Renz & Nebel, 1999). While this means that it is unlikely that very large
instances can be solved in reasonable time, this result does not rule out the possibility that
we can solve instances up to a certain size in reasonable time. Recently, maximal tractable
subsets of RCC-8 were identified (Renz & Nebel, 1999; Renz, 1999) which can be used to
speed up backtracking search for the general NP-complete reasoning problem by reducing
the search space considerably.
In this paper we address several questions that emerge from previous theoretical results
on RCC-8 (Renz & Nebel, 1999; Renz, 1999): Up to which size is it possible to solve
instances in reasonable time? Which heuristic is the best? Is it really so much more ecient
to use the maximal tractable subsets for solving instances of the NP-complete consistency
problem as the theoretical savings given by the smaller branching factors indicate or is
this effect out-balanced by the forward-checking power of the interleaved path-consistency
computations? This was the case for similar temporal problems (pointisable vs. ORD-Horn
relations) (Nebel, 1997). Is it possible to combine the different heuristics in such a way that
more instances can be solved in reasonable time than by each heuristic alone?
We treat these questions by randomly generating instances and solving them using
different heuristics. In doing so, we are particularly interested in the hardest randomly
c 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiRenz & Nebel

generated instances which leads to the question of phase-transitions (Cheeseman, Kanefsky,
& Taylor, 1991): Is there a parameter for randomly generating instances of the consistency
problem of RCC-8 that results in a phase-transition behavior? If so, is it the case that the
hardest instances are mainly located in the phase-transition region while the instances not
contained in the phase-transition region are easily solvable? In order to generate instances
which are harder with a higher probability, we generate two different kinds of instances. On
the one hand we generated instances which contain constraints over all RCC-8 relations, on
the other hand we generated instances which contain only constraints over relations which
are not contained in any of the maximal tractable subsets. We expect these instances to be
harder on average than the former instances.
The algorithmic techniques we use for solving these randomly generated instances are
borrowed from similar work on qualitative temporal reasoning (Nebel, 1997; van Beek &
Manchak, 1996; Ladkin & Reinefeld, 1992). Additionally, we make use of the fragments
of RCC-8, named Hb 8, Q8 , and C8, that permit polynomial-time inferences (Renz & Nebel,
1999; Renz, 1999). In the backtracking algorithm, which is used to solve the reasoning
problem for full RCC-8, we decompose every disjunctive relation into relations of one of
these tractable subsets instead of decomposing them into its base relations. This reduces
the average branching factor of the backtracking tree from 4.0 for the base relations to
1.4375 for Hb 8, to 1.523 for C8, and to 1.516 for Q8 . Although these theoretical savings
cannot be observed in our experiments, using the maximal tractable subsets instead of the
base relations leads to significant performance improvements.
This paper is structured as follows. In Section 2, we give a brief sketch of the RCC-8
calculus and of the algorithms used for solving instances of RCC-8. In Section 3 we describe
the procedure for randomly generating instances, the different heuristics we apply for solving
these instances, and how we measure the quality of the heuristics. In Section 4 we evaluate
different path-consistency algorithms in order to find the most ecient one to be used for
forward-checking in the backtracking search. In Section 5 we observe a phase-transition
behavior of the randomly generated instances and show that the instances in the phasetransition region are harder to solve than the other instances. In Section 6 we report on
the outcome of running the different heuristics for solving the instances and identify several
hard instances which are mainly located in the phase-transition region. In Section 7 we try
to solve the hard instances by orthogonally combining the different heuristics. This turns
out to be very effective and leads to a very ecient solution strategy. Finally, in Section 8
we evaluate this strategy by trying to solve very large instances.1
2. The Region Connection Calculus RCC-8

The Region Connection Calculus (RCC) is a first-order language for representation of and
reasoning about topological relationships between extended spatial regions (Randell et al.,
1992b). Spatial regions in RCC are non-empty regular subsets of some topological space
which do not have to be internally connected, i.e., a spatial region may consist of different
disconnected pieces. Different relationships between spatial regions can be defined based on
one dyadic relation, the connected relation C( ) which is true if the topological closures
of the spatial regions and share a common point.
a; b

a

b

1. The programs are available as an online appendix.

290

fiEfficient Methods for Qualitative Spatial Reasoning

fi fi


fi
fi


fi fi


  



fi
fi


 


X

X

Y

Y

DC(X; Y)

X

Y

Y

X

TPP(X; Y) TPP 1 (X; Y)

EC(X; Y)

X

X Y

Y

PO(X; Y)

X

Y

Y

X

NTPP(X; Y) NTPP 1 (X; Y)

EQ(X; Y)

Figure 1: Two-dimensional examples for the eight base relations of RCC-8
The Region Connection Calculus RCC-8 is a constraint language formed by the eight
jointly exhaustive and pairwise disjoint base relations DC, EC, PO, EQ, TPP, NTPP, TPP 1 ,
and NTPP 1 definable in the RCC-theory and by all possible unions of the base relations|
giving a total number of 28 = 256 different relations. The base relations have the meaning
of DisConnected, Externally Connected, Partial Overlap, EQual, Tangential Proper Part,
Non-Tangential Proper Part, and their converses. Examples for these relations are shown
in Figure 1. Constraints are written in the form
where are variables for spatial
regions and is an RCC-8 relation. We write the union of base relations as f g. The
union of all base relations, the universal relation, is written as fg. Apart from union
([), other operations on relations are defined, namely, converse ( ), intersection (\), and
composition (). The formal definitions of these operations are:
xRy

x; y

R

R; S

^

8
8
8
8

x; y
x; y
x; y
x; y

:
:
:
:

( [ )
( \ )

x R

S y

x R

S y

xR

^

(  )

x R

y

S y

$
$
$
$ 9

_
^
,
:(

xRy

xS y

xRy

xS y

yRx
z

xRz

,
,

^

)

zSy :

The composition of base relations can be computed from the semantics of the relations and is
usually provided as a composition table (Randell, Cohn, & Cui, 1992a; Bennett, 1994). The
RCC-8 composition table corresponds to the given extensional definition of composition only
if the universal region is not permitted (Bennett, 1997). Based on this table, compositions
of disjunctive relations can be easily computed. In the following, Sb denotes the closure of
a set of RCC-8 relations S under composition, intersection, and converse.
A finite set of RCC-8 constraints  describing the topological relationships of different
regions can be represented by an  matrix , where each entry represents the RCC-8
relation holding between region and region . Without loss of generality, = fEQg and
=
can be assumed. The fundamental reasoning problem (named RSAT) in this
framework is deciding consistency of a set of spatial formulas , i.e., whether there is a
spatial configuration where the relations between the regions can be described by . All
other interesting reasoning problem can be reduced to it in polynomial time (Golumbic
& Shamir, 1993). Unfortunately, RSAT is NP-complete (Renz & Nebel, 1999), i.e., it is
unlikely that there is any polynomial algorithm for deciding consistency. However, it was
shown in Nebel's (1995) paper that there are subsets S of RCC-8 for which the consistency
n

n

i

Mji

n

M

j

^

Mij

291

Mij

Mii

fiRenz & Nebel

problem (written RSAT(S)) can be decided in polynomial time.2 In particular the set of
eight base relations B was shown to be tractable. From that it follows that Bb consisting of
32 relations is also tractable. An even larger tractable subset containing all base relations
is Hb 8 (Renz & Nebel, 1999), which contains 148 out of the 256 RCC-8 relations. This set
was also shown to be maximal with respect to tractability, i.e., if any other RCC-8 relation
is added, the consistency problem becomes NP-complete. Renz (1999) made a complete
analysis of tractability of RSAT by identifying all maximal tractable subsets which contain
all base relations, altogether three subsets Hb 8, Q8 (160 relations), and C8 (158 relations).
NP 8 is the set of relations that by themselves result in NP-completeness when combined
with the set of base relations. It contains the following 76 relations which are not contained
in one of Hb 8 Q8 or C8 (Renz, 1999):
NP 8 = f j fPOg 6 and (fNTPPg  or fTPPg  )
and (fNTPP 1g  or fTPP 1g  )g
[ ffEC NTPP EQg fDC EC NTPP EQg
fEC NTPP 1 EQg fDC EC NTPP 1 EQgg
The maximal tractable subsets contain the following relations (Renz, 1999):
Hb 8 = (RCC-8 n NP 8 ) n f j (fEQ NTPPg  and fTPPg 6 )
or (fEQ NTPP 1 g  and fTPP 1g 6 )g
C8 = (RCC-8 n NP 8 ) n f j fECg  and fPOg 6 and
\ fTPP NTPP TPP 1 NTPP 1 EQg =6 ;g
Q8 = (RCC-8 n NP 8 ) n f j fEQg  and fPOg 6 and
\ fTPP NTPP TPP 1 NTPP 1 g 6= ;g
;

;

R

R

R

R

R

;

;

;

;

;

;

;

;

;

;

;

;

;

R

R

R

;

;

R

R

;

R

;

;

:

R

R

R

R

;

;

R

R

R

;

R

;

All relations of Q8 are contained in one of Hb 8 or C8, i.e., Hb 8 [ C8 = RCC-8 n NP 8 .
Although Hb 8 is the smallest of the three maximal tractable subsets, it best decomposes the
RCC-8 relations: When decomposing an RCC-8 relation
into sub-relations of one of the
maximal tractable subsets, i.e., = 1 [ [ , one needs on average 1.4375 Hb 8 relations,
1.516 Q8 relations, and 1.523 C8 relations for decomposing all RCC-8 relations. Renz (2000)
gives a detailed enumeration of the relations of the three sets.
R

R

S

:::

Si

Sk

2.1 The Path-Consistency Algorithm

As in the area of qualitative temporal reasoning based on Allen's interval calculus (Allen,
1983), the path-consistency algorithm (Montanari, 1974; Mackworth, 1977; Mackworth &
Freuder, 1985) can be used to approximate consistency and to realize forward-checking
(Haralick & Elliot, 1980) in a backtracking algorithm.
The path-consistency algorithm checks the consistency of all triples of relations and
eliminates relations that are impossible. This is done by iteratively performing the following
operation
\ 
Mij

Mij

Mik

Mkj

2. Strictly speaking, this applies only to systems of regions that do not require regularity.

292

fiEfficient Methods for Qualitative Spatial Reasoning

Algorithm: Path-consistency
Input: A set  of binary constraints over the variables x1 ; x2 ; : : : ; x

n

of 

represented by an  matrix .
path-consistent set equivalent to ; fail, if such a set does not
exist.
n

Output:

n

M

1. := f( ) ( ) j 1 

6= 6= g;
( indicates the -th variable of . Analogously for and )
2. while 6= ; do
3. select and delete a path ( ) from ;
4. if revise( ) then
5.
if
= ; then return fail
6.
else := [ f(
) ( ) j 1   6= 6= g.
Q

i; j; k ;

k; i; j

i

i; j; k

n; i < j; k

i; k

j

i

j

k

Q

p; r; q

Q

p; r; q

Mpq

Q

Q

p; q; s ;

s; p; q

s

n; s

p; s

q

Function: revise(i; k; j )
Input: three labels i, k and j indicating the variables x ; x ; x of 
Output: true, if M is revised; false otherwise.
Side effects: M and M revised using the operations \ and 
i

j

k

ij

ij

ji

over the constraints involving , , and .
xi

xk

xj

1. oldM := ;
2.
:= \ (  );
3. if (oldM = ) then return false;
4.
:= ;
5. return true.
Mij

Mij

Mij

Mj i

Mij

Mik

Mkj

Mij

^

Figure 2: Path-consistency algorithm.
for all triples of regions
until a fixed point is reached. If
= ; for a pair
, then we know that is inconsistent, otherwise is path-consistent. Computing
can be done in ( 3) time (see Figure 2). This is achieved by using a queue of triples of
regions for which the relations should be recomputed (Mackworth & Freuder, 1985). Pathconsistency does not imply consistency. For instance, the following set of spatial constraints
is path-consistent but not consistent:
i; j; k

i; j

M

M ij

M

M

M

O n

lHH - l
HHH
l? HHHj- l?

X

DC _ TPP

Z

TPP _ TPP 1
EC _ TPP
EC _ TPP

EQ _ NTPP

Y

EC _ NTPP

W

On the other hand, consistency does not imply path-consistency, since path-consistency is
not a form of consistency (in its logical sense), but a form of disjunctive non-redundancy.
Nevertheless, path-consistency can be enforced to any consistent set of constraints by ap293

fiRenz & Nebel

Algorithm: Consistency
Input: A set  of RCC-8 constraints over the variables x1 ; x2 ; : : : ; x

and a subset S  RCC-8 that contains all base relations
and for which Decide is a sound and complete decision
procedure.
Output: true, iff  is consistent.
1. Path-Consistency()
2. if  contains the empty relation then return false
3. else choose an unprocessed constraint
and
split into 1
2 S such that 1 [ [ =
4. if no constraint can be split then return Decide()
5. for all refinements (1   ) do
6.
replace
with
in 
7.
if Consistency() then return true

n

xi Rxj

R

S ; : : : ; Sk

Sl

xi Rxj

l

S

:::

Sk

R

k

xi Sl xj

Figure 3: Backtracking algorithm for deciding consistency.
plying a path-consistency algorithm. If only relations in Hb 8, Q8 , or C8 are used, however,
the path-consistency algorithm is sucient for deciding consistency, i.e., path-consistency
decides RSAT(Hb 8), RSAT(Q8 ), and RSAT(C8 ), (Renz & Nebel, 1999; Renz, 1999).
2.2 The Backtracking Algorithm

In order to solve an instance  of RSAT, we have to explore the corresponding search space
using some sort of backtracking. In our experiments, we used a backtracking algorithm
employed for solving qualitative temporal reasoning problems (Nebel, 1997), which is based
on the algorithm proposed by Ladkin and Reinefeld (1992). For this algorithm (see Figure 3)
it is necessary to have a subset S  RCC-8 for which consistency can be decided by using a
sound and complete (and preferably polynomial) decision procedure Decide. If S contains
all base relations, thenS each relation 2 RCC-8 can be decomposed into sub-relations
2 S such that =
. The size of a particular decomposition is the minimal number
of sub-relations which is used to decompose . The backtracking algorithm successively
selects constraints of , backtracks over all sub-relations of the constraints according to
their decomposition and decides sub-instances which contain only constraints over S using
Decide.
The (optional) procedure Path-consistency in line 1 is used for forward-checking
and restricts the remaining search space. Nebel (1997) showed that this restriction does
not effect soundness and completeness of the algorithm. If enforcing path-consistency is
sucient for deciding RSAT(S), Decide() in line 5 is not necessary. Instead it is possible
to always return true there.
The eciency of the backtracking algorithm depends on several factors. One of them is,
of course, the size of the search space which has to be explored. A common way of measuring
R

Si

R

i

Si

Si

R

294

fiEfficient Methods for Qualitative Spatial Reasoning

the size of the search space is the average branching factor of the search space, i.e., the
average number of branches each node in the search space has (a node is a recursive 2call of
) 2,
Consistency). Then the average size of the search space can be computed as (
2
where (
) 2 is the number of constraints which have to be split when variables are
given. For the backtracking algorithm described in Figure 3 the branching factor depends
on the average number of relations of the split set S into which a relation has to be split.
The less splits on average the better, i.e., it is to be expected that the eciency of the
backtracking algorithm depends on the split set S and its branching factor. Another factor
is how the search space is explored. The backtracking algorithm of Figure 3 offers two
possibilities for applying heuristics. One is in line 3 where the next unprocessed constraint
can be chosen, the other is in line 5 where the next refinement can be chosen. These two
choices inuence the search space and the path through the search space.
b

b

n

n =

n

n =

n

3. Test Instances, Heuristics, and Measurement

There is no previous work on empirical evaluation of algorithms for reasoning with RCC-8
and no benchmark problems are known. Therefore we randomly generated our test instances
with a given number of regions , an average label-size , and an average degree of
the constraint graph. Further, we used two different sets of relations for generating test
instances, the set of all RCC-8 relations and the set of hard RCC-8 relations NP 8, i.e., those
76 relations which are not contained in any of the maximal tractable subsets Hb 8, C8 , or
Q8 . Based on these sets of relations, we used two models to generate instances, denoted
by ( ) and ( ). The former model uses all relations to generate instances, the
latter only the relations in NP 8. The instances are generated as follows:
1. A constraint graph with nodes and an average degree of for each node is generated.
This is accomplished by selecting 2 out of the ( 1) 2 possible edges using a
uniform distribution.
2. If there is no edge between the th and th node, we set = to be the universal
relation.
3. Otherwise a non-universal relation is selected according to the parameter such that
the average size of relations for selected edges is . This is accomplished by selecting
one of the base relations with uniform distribution and out of the remaining 7 relations
each one with probability ( 1) 7.3 If this results in an allowed relation (i.e., a relation
of NP 8 for ( ), any RCC-8 relation for ( )), we assign this relation to the
edge. Otherwise we repeat the process.
The reason for also generating instances using only relations of NP 8 is that we assume
that these instances are dicult to solve since every relation has to be split during the
backtracking search, even if we use a maximal tractable subclass as the split set. We only
generated instances of average label size = 4 0, since in this case the relations are equally
distributed.
n

A n; d; l

l

d

H n; d; l

n

d

nd=

i

n n

j

=

Mij

Mji

l

l

l

=

H n; d; l

A n; d; l

l

:

3. This method could result in the assignment of a universal constraint to a selected link, thereby changing
the degree of the node. However, since the probability of getting the universal relation is very low, we
ignore this in the following.

295

fiRenz & Nebel

This way of generating random instances is very similar to the way random CSP instances over finite domains are usually generated (Gent, MacIntyre, Prosser, Smith, &
Walsh, 2001). Achlioptas et al. (1997) found that the standard models for generating
random CSP instances over finite domains lead to trivially awed instances for ! 1,
i.e., instances become locally inconsistent without having to propagate constraints. Since
we are using CSP instances over infinite domains, Achlioptas et al.'s result does not necessarily hold for our random instances. We, therefore, analyze in the following whether
our instances are also trivially awed for ! 1. In order to obtain a CSP over a finite domain, we first have to transform our constraint graph into its dual graph where
each of the ( 1) 2 edges
of our constraint graph corresponds to a node in the
dual graph. Moreover, each of the variables of the constraint graph corresponds to
1 edges in the dual graph, i.e., the dual graph contains ( 1) edges and ( 1) 2
nodes. In the dual graph, each node corresponds to a variable over the eight-valued domain
D = fDC EC PO TPP TPP 1 NTPP NTPP 1 EQg. Ternary constraints over these variables are imposed by the composition table, i.e., the composition rules


must hold for all connected
of the dual graph ( =
  triples of nodes
n
for all ). There are 3 = ( 1)( 2) 6 connected triples in the dual graph. The


n(n 1)=2
overall number of triples in the dual graph is
.
2 unary constraints on
3
 
the domain of the variables are given, i.e., there are nd=3 2 triples in the dual graph
where all nodes are restricted by unary constraints. Therefore, the expected number
of connected triples for which unary constraints are given can be computed as
!
!
2
3  3
!
=
( 1) 2
3
For ! 1, the expected number of triples 1 tends to 3 6. For the instances generated
according to the model ( ), the probability that the unary constraints which are
assigned to a triple lead to a local inconsistency is about 0 0036% (only 58,989 out of the
2553 = 16 581 375 possible assignments are inconsistent). Since one locally inconsistent
triple makes the whole instance inconsistent, we are interested in the average degree for
which the expected number of locally inconsistent triples is equal to one. For the model
( ) this occurs for a value of = 11 90, and 1 = 0 5 for = 9 44. For = 100,
the expected number of locally inconsistent triples is one for = 13 98, and 100 = 0 5 for
= 11 10. For the model ( ), none of the possible assignments of the triples leads to
a local inconsistency, i.e., all triples of the randomly generated instances of the ( )
model are locally consistent.4 This analysis shows that contrary to what Achlioptas et
al. found for randomly generated CSP instances over finite domains, the model ( ),
and the model ( ) for small do not suffer from trivial local inconsistencies.
n

n

n n

=

Mij

n

n

n n

;

;

;

;

;

;

n n

;

Mij

Mik

Mij ; Mik ; Mkj

i; j

n n

=

n

Mkj

^

Mij

Mj i

=

nd=

Mij

n
CT

E

n

nd=

n

:

EC T

n n

n

=

EC T

d =

A n; d; l

;

;

;

d

n
E
IT

A n; d; l

d

:

E

IT

:

d

d

d

:

:

:

n

E

IT

:

H n; d; l

H n; d; l

H n; d; l

A n; d; l

d

4. This is similar to the result for CSPs over finite domains that by restricting the constraint type, e.g., if
only \not-equal" constraints as in graph-coloring are used, it is possible to ensure that problems cannot
be trivially awed.

296

fiEfficient Methods for Qualitative Spatial Reasoning

We solve the randomly generated instances using the backtracking algorithm described
in the previous section. The search space on which backtracking is performed depends on
the split set, i.e., the set of sub-relations that is allowed in the decompositions. Choosing the
right split-set inuences the search noticeably as it inuences the average branching factor
of the search space. We choose five different split sets, the three maximal tractable subsets
Hb 8 Q8 and C8, the set of base relations B and the closure of this set Bb which consists of
38 relations. These sets have the following branching factors B: 4.0, B:b 2.50 ,Hb 8: 1.438,
C8: 1.523, Q8 : 1.516. This is, of course, a worst case measure because the interleaved pathconsistency computations reduce the branching factor considerably (Ladkin & Reinefeld,
1997).
Apart from the choice of the split set there are other heuristics which inuence the eciency of the search. In general it is the best search strategy to proceed with the constraint
with the most constraining relation (line 3 of Figure 3) and the least constraining choice of
a sub-relation (line 5 of Figure 3). We investigated two different aspects for choosing the
next constraint to be processed (Nebel, 1997).
static/dynamic: Constraints are processed according to a heuristic evaluation of their
constrainedness which is determined statically before the backtracking starts or dynamically during the search.
local/global: The evaluation of the constrainedness is based on a local heuristic weight
criterion or on a global heuristic criterion (van Beek & Manchak, 1996).
This gives us four possibilities we can combine with the five different split sets, i.e., a
total number of 20 different heuristics. The evaluation of constrainedness as well as how
relations are decomposed into relations of different split sets depends on the restrictiveness
of relations, which is a heuristic criterion (van Beek & Manchak, 1996). Restrictiveness
of a relation is a measure of how a relation restricts its neighborhood. For instance, the
universal relation given in a constraint network does not restrict its neighboring relations at
all, the result of the composition of any relation with the universal relation is the universal
relation. The identity relation, in contrast, restricts its neighborhood a lot. In every triple
of variables where one relation is the identity relation, the other two relations must be equal.
Therefore, the universal relation is usually the least restricting relation, while the identity
relation is usually the most restricting relation. Restrictiveness of relations is represented
as a weight in the range of 1 to 16 assigned to every relation, where 1 is the value of the
most and 16 the value of the least restricting relation. We discuss in the following section
in detail how the restrictiveness and the weight of a relation is determined.
Given the weights assigned to every relation, we compute decompositions and estimate
constrainedness as follows. For each split set S and for each RCC-8 relation we compute
the smallest decomposition of into sub-relations of S, i.e., the decomposition which requires the least number of sub-relations of S. If there is more than one possibility, we choose
the decomposition with the least restricting sub-relations. In line 5 of the backtracking algorithm (see Figure 3), the least restricting sub-relation of each decomposition is processed
first. For the local strategy, the constrainedness of a constraint is determined by the size of
its decomposition (which can be different for every split set) and by its weight. We choose
the constraint with the smallest decomposition larger than one and, if there is more than
;

;

R

R

297

fiRenz & Nebel

one such constraint, the one with the smallest weight. The reason for choosing the relation
with the smallest decomposition is that it is expected that forward-checking refines relations with a larger decomposition into relations with a smaller decomposition. This reduces
the backtracking effort. For the global strategy, the constrainedness of a constraint
is
determined by adding the weights of all neighboring relations
with
and
to
the weight of . The idea behind this strategy is that when refining the relation with
the most restricted neighborhood, an inconsistency is detected faster than when refining a
relation with a less restricted neighborhood.
In order to evaluate the quality of the different heuristics, we measured the run-time used
for solving instances as well as the number of visited nodes in the search space. Comparing
different approaches by their run-time is often not very reliable as it depends on several
factors such as the implementation of the algorithms, the used hardware, or the current
load of the used machine which makes results sometimes not reproducible. For this reason,
we ran all our run-time experiments on the same machine, a Sun Ultra 1 with 128 MB of
main memory. Nevertheless, we suggest to use the run-time results mainly for qualitatively
comparing different heuristics and for getting a rough idea of the order of magnitude for
which instances can be solved.
In contrast to this, the number of visited nodes for solving an instance with a particular
heuristic is always the same on every machine. This allows comparing the path through the
search space taken by the single heuristics and to judge which heuristic makes the better
choices on average. However, this does not take into account the time that is needed to make
a choice at a single node. Computing the local constrainedness of a constraint is certainly
faster than computing its global constrainedness. Similarly, computing constrainedness
statically should be faster than computing it dynamically. Furthermore, larger instances
require more time at the nodes than smaller instances, be it for computing path-consistency
or for computing the constrainedness. Taking running-time and the number of visited nodes
together gives good indications of the quality of the heuristics.
A further choice we make in evaluating our measurements is that of how to aggregate
the measurements of the single instances to a total picture. Some possibilities are to use
either the average or different percentiles such as the median, i.e., the 50% percentile. The
% percentile for a value 0
100 is obtained by sorting the measurements in increasing
order and picking the measurement of the % element, i.e., % of the values are less than
that value. Suppose that most instances have a low value (e.g. running time) and only a
few instances have a very large value. Then the average might be larger than the values of
almost all instances, while in this case the median is a better indication of the distribution
of the values. In this case the 99% percentile, for instance, gives a good indication of the
value of the hardest among the \normal" instances. We have chosen to use the average
value when the measurements are well distributed and to use both 50% and 99% percentile
when there are only a few exceptional values in the distribution of the measurements.
xRy

S; T

R

d

xS z

zT y

R

< d <

d

d

4. Empirical Evaluation of the Path-Consistency Algorithm

Since the eciency of the backtracking algorithm depends on the eciency of the underlying
path-consistency algorithm, we will first compare different implementations of the pathconsistency algorithm. In previous empirical investigations (van Beek & Manchak, 1996) of
298

fiEfficient Methods for Qualitative Spatial Reasoning

reasoning with Allen's interval relations (Allen, 1983), different methods for computing the
composition of two relations were evaluated. This was mainly because the full composition
table for the interval relations contains 213  213 = 67108864 entries, which was too large
at that time to be stored in the main memory. In our setting, we simply use a composition
table that specifies the compositions of all RCC-8 relations, which is a 256  256 table
consuming approximately 128 KB of main memory. This means that the composition of
two arbitrary relations is done by a simple table lookup.
Van Beek and Manchak (1996) also studied the effect of weighting the relations in
the queue according to their restrictiveness and process the most restricting relation first.
Restrictiveness was measured for each base relation by successively composing the base
relation with every possible label, summing up the cardinalities, i.e., the number of base
relations contained in the result of the composition, and suitably scaling the result. The
reason for doing so is that the most restricting relation restricts the other relations on
average most and therefore decreases the probability that they have to be processed again.
Restrictiveness of a complex relation was approximated by summing up the restrictiveness
of the involved base relations. Van Beek and Manchak (1996) found that their method of
weighting the triples in the queue is much more ecient than randomly picking an arbitrary
triple. Because of the relatively small number of RCC-8 relations, we computed the exact
restrictiveness by composing each relation with every other relation and summing up the
cardinalities of the resulting compositions. We scaled the result into weights from 1 (the
most restricting relation) to 16 (the least restricting relations).
This gives us three different implementations of the path-consistency algorithm. One in
which the entries in the queue are not weighted, one with approximated restrictiveness as
done by van Beek and Manchak, and one with exact restrictiveness.5 In order to compare
these implementations, we randomly generated instances with 50 to 1,000 regions. For
each value of the average degree ranging from 8.0 stepping with 0.5 to 11.0 we generated
10 different instances. Figure 4 displays the average CPU time of the different methods
for applying the path-consistency algorithm to the same generated instances. It can be
seen that the positive effect of using a weighted queue is much greater for our problem
than for the temporal problem (about 10 faster than using an ordinary queue without
weights compared to only about 2 faster (van Beek & Manchak, 1996)). Determining the
weights of every relation using their exact restrictiveness does not have much advantage over
approximating their restrictiveness using the approach by van Beek and Manchak (1996),
however. For our further experiments we always used the \exact weights" method because
determining the restrictiveness amounts to just one table lookup.
As mentioned in the previous section, one way of measuring the quality of the heuristics
is to count the number of visited nodes in the backtrack search. In our backtracking
algorithm, path-consistency is enforced in every visited node. Note that it is not adequate
to multiply the average running-time for enforcing path-consistency of an instance of a
particular size with the number of visited nodes in order to obtain an approximation of
the required running time for that instance. The average running-time for enforcing pathconsistency as given in Figure 4 holds only when all possible paths are entered into the
queue at the beginning of the computation (see line 1 of Figure 2). These are the paths
5. For the weighted versions we select a path (i; k; j ) from the queue Q in line 3 of the algorithm of Figure 2
according to the weights of the different paths in Q which are computed as specified above.

299

fiRenz & Nebel

Average CPU time of PCA using different queue methods for A(n,d,4.0)
1000
100

"exact" weights
"approx." weights
no weights

CPU time (sec)

10
1
0.1
0.01
0.001
100

200

300

400

500
600
nodes

700

800

900

1000

Figure 4: Comparing the performance of the path-consistency algorithm using different
methods for weighting the queue (70 instances/data point, = 8 0 11 0)
d

:

:

which have to be checked by the algorithm. The path-consistency computation during the
backtracking search is different, however. There, only the paths involving the currently
changed constraint are entered in the queue, since only these paths might result in changes
of the constraint graph. This is much faster than the full computation of path-consistency
which is only done once at the beginning of the backtrack search.
5. The Phase-Transition of RCC-8

When randomly generating problem instances there is usually a problem-dependent parameter which determines the solubility of the instances. In one parameter range instances are
underconstrained and are therefore soluble with a very high probability. In another range,
problems are overconstrained and soluble with a very low probability. In between these
ranges is the phase-transition region where the probability of solubility changes abruptly
from very high to very low values (Cheeseman et al., 1991). In order to study the quality
of different heuristics and algorithms with randomly generated instances of an NP-complete
problem, it is very important to be aware of the phase-transition behavior of the problem.
This is because instances which are not contained in the phase-transition region are often
very easily solvable by most algorithms and heuristics and are, thus, not very useful for
comparing their quality. Conversely, hard instances which are better suited for comparing
the quality of algorithms and heuristics are usually found in the phase-transition region.
In this section we identify the phase-transition region of randomly generated instances
of the RSAT problem, both for instances using all RCC-8 relations and for instances using
only relations of NP 8. Similarly to the empirical analysis of qualitative temporal reasoning
problems (Nebel, 1997), it turns out that the phase-transition depends most strongly on the
average degree of the nodes in the constraint graph. If all relations are allowed, the phased

300

fiEfficient Methods for Qualitative Spatial Reasoning

Probability of satisfiability for A(n,d,4.0)

Median CPU time for A(n,d,4.0)
CPU time(s)

Probability (%)
100

0.6
0.5
0.4
0.3
0.2
0.1
0

50
100
80
60
4 6
8 10
12 14
average degree
16 18

40

nodes

100
80
60

4 6
8 10
12 14
average degree
16 18

20

40

nodes

20

Figure 5: Probability of satisfiability and median CPU time for (
Hb 8/static/global heuristic (500 instances per data point)

A n; d;

4 0) using the
:

transition is around = 8 to = 10 depending on the instance size (see Figure 5). Because
of the result of our theoretical analysis of the occurrence of trivial aws (see Section 3), it can
be expected that for larger instance sizes the phase-transition behavior will be overlaid and
mainly determined by the expected number of locally inconsistent triples which also depends
on the average degree . Thus, although it seems that the phase-transition shifts towards
larger values of as the instance size increases, the phase-transition is asymptotically below
= 9 44, the theoretical value for ! 1 (see Section 3). Instances which are not pathconsistent can be solved very fast by just one application of the path-consistency algorithm
without further need for backtracking. When looking at the median CPU times given in
Figure 5, one notices that there is a sharp decline of the median CPU times at the phase
transition. This indicates that for values of the average degree which are higher than where
the phase-transition occurs, at least 50% of the instances are not path-consistent.
When using only \hard" relations, i.e., relations in NP 8, the phase-transition appears
at higher values for , namely, between = 10 and = 15 (see Figure 6). As the median
runtime shows, these instances are much harder in the phase-transition than in the former
case. As in the previous case, but even more strongly, it seems that the phase-transition
shifts towards larger values of as the instance size increases, and also that the phasetransition region narrows.
In order to evaluate the quality of the path-consistency method as an approximation to
consistency, we counted the number of instances that are inconsistent but path-consistent
(see Figure 7), i.e., those instances where the approximation of the path-consistency algorithm to consistency is wrong. First of all, one notes that all such instances are close to the
phase transition region. In the general case, i.e., when constraints over all RCC-8 relations
are employed, only a very low percentage of instances are path-consistent but inconsistent.
Therefore, the figure looks very erratic. More data points would be required in order to
obtain a smooth curve. However, a few important observations can be made from this
figure, namely, that path-consistency gives an excellent approximation to consistency even
for instances of a large size. Except for very few instances in the phase-transition region,
almost all instances which are path-consistent are also consistent. This picture changes
d

d

d

d

d

:

n

d

d

d

d

301

fiRenz & Nebel

Probability of satisfiability for H(n,d,4.0)

Median CPU time for H(n,d,4.0)

Probability (%)

CPU time(s)

100

2
1.5

50

1
80
60
40

6 8
10 12
14 16
average degree
18 20

80

0.5

60

0

nodes

6 8
10 12
14 16
average degree
18 20

20

40

nodes

20

Figure 6: Probability of satisfiability and median CPU time for (
Hb 8/static/global heuristic (500 instances per data point)

H n; d;

Percentage points of incorrect PCA answers for A(n,d,4.0)

4 0) using the
:

Percentage points of incorrect PCA answers for H(n,d,4.0)

PC-Failures (%)
PC-Failures (%)
0.6
0.5

80
70
60
50
40
30
20
10
0

0.4
0.3
0.2

100

0.1

80
60

0
4

40

6

8 10
12 14
average degree
16 18

nodes

80
60
40

6

8 10
12 14
16 18
average degree
20

20

nodes

20

Figure 7: Percentage points of incorrect answers of the path-consistency algorithm for
( 4 0) and ( 4 0)
A n; d;

:

H n; d;

:

when looking at the ( 4 0) case. Here almost all instances in the phase-transition
region and many instances in the mostly insoluble region are path-consistent, though only
a few of them are consistent.
For the following evaluation of the different heuristics we will randomly generate instances with an average degree between = 2 and = 18 in the ( 4 0) case and
between = 4 and = 20 in the ( 4 0) case. This covers a large area around the
phase-transition. We expect the instances in the phase-transition region of ( 4 0) to
be particularly hard which makes them very interesting for comparing the quality of the
different heuristics.
H n; d;

:

d

d

d

H n; d;

d

A n; d;

:

:

H n; d;

:

6. Empirical Evaluation of the Heuristics

In this section we compare the different heuristics by running them on the same randomly
generated instances. For the instances of ( 4 0) we ran all 20 different heuristics
A n; d;

302

:

fiEfficient Methods for Qualitative Spatial Reasoning

Number of hard instances for A(n,d,4.0)

Number of hard instances for H(n,d,4.0)

#Hard Instances

#Hard Instances

10
9
8
7
6
5
4
3
2
1
0

450
400
350
300
250
200
150
100
50
0

100
80
60

4 6
8 10
12 14
16 18
average degree
20

40

nodes

80
60
40

6 8
10 12
14 16
average degree
18 20

20

nodes

20

Figure 8: Number of instances using more than 10,000 visited nodes for some heuristic for
( 4 0) and ( 4 0)
A n; d;

:

H n; d;

:

(static/dynamic and local/global combined with the five split sets B Bb Hb 8 C8 Q8 ) on the
same randomly generated instances of size = 10 up to = 100. For the instances of
( 4 0) we restricted ourselves to instances with up to = 80 regions because larger
ones appeared to be too dicult.
In first experiments we found that most of the instances were solved very fast with
less than 1,000 visited nodes in the search space when using one of the maximal tractable
subsets for splitting. However, some instances turned out to be extremely hard, they could
not be solved within our limit of 2 million visited nodes, which is about 1.5 hours of CPU
time. Therefore, we ran all our programs up to a maximal number of 10,000 visited nodes
and stored all instances for which at least one of the different heuristics used more than
10,000 visited nodes for further experiments (see next section). We call those instances
the hard instances. The distribution of the hard instances is shown in Figure 8. It turned
out that for the heuristics using B as a split set and for the heuristics using dynamic and
global evaluation of the constrainedness many more instances were hard than for the other
combinations of heuristics. We, therefore, did not include in Figure 8 the hard instances
of the B/dynamic/global heuristic for ( 4 0) and the hard instances for the heuristics
b dynamic/global heuristic for ( 4 0).
using B as a split set and the B/
As Figure 8 shows, almost all of the hard instances are in the phase-transition region.
For ( 4 0) only a few of the 500 instances per data point are hard while for ( 4 0)
almost all instances in the phase-transition are hard. Altogether there are 788 hard instances
for ( 4 0) (out of a total number of 759,000 generated instances) and 75,081 hard
instances for ( 4 0) (out of a total number of 594,000 generated instances). Table 1
shows the number of hard instances for each heuristic except for those which were excluded
as mentioned above. The heuristics using Hb 8 as a split set solve more instances than the
heuristics using other split sets. Using C8 or Q8 as a split set does not seem to be an
improvement over using B.b Among the different ways of computing constrainedness, static
and global appears to be the most effective combination when using one of the maximal
tractable subsets as a split set. For some split sets, dynamic and local also seems to be an
;

n

H n; d;

;

;

;

n

:

n

A n; d;

:

H n; d;

A n; d;

:

A n; d;

:

:

H n; d;

H n; d;

:

303

:

fiRenz & Nebel

Heuristics

Hb8 /sta/loc
Hb8 /sta/glo
Hb8 /dyn/loc
Hb8 /dyn/glo
C8 /sta/loc
C8 /sta/glo
C8 /dyn/loc
C8 /dyn/glo
Q8 /sta/loc
Q8 /sta/glo
Q8 /dyn/loc
Q8 /dyn/glo
Bb/sta/loc
Bb/sta/glo
Bb/dyn/loc
Bb/dyn/glo
B/sta/loc
B/sta/glo
B/dyn/loc
B/dyn/glo
total

(

A n; d;

4:0)

64
42
52
100
81
58
78
108
81
54
74
104
68
89
70
162
163
222
209
(303)
788

(
4:0)
21; 129
10; 826
9; 967
24; 038
28; 830
15; 457
32; 926
41; 565
24; 189
13; 189
13; 727
29; 448
23; 711
13; 831
29; 790
{
{
{
{
{
75; 081

H n; d;

H

(80; 14:0; 4:0)
331
227
217
345
373
277
412
428
346
239
255
368
344
249
379
{
{
{
{
{
486

Table 1: Number of hard instances for each heuristic
effective combination while combining dynamic and global is in all cases the worst choice
with respect to the number of solved instances.
In Figure 9 we compare the 50% and 99% percentiles of the different heuristics on
( 4 0). We do not give the average run times since we ran all heuristics only up to at
most 10,000 visited nodes which reduces the real average run time values. Each data point
is the average of the values for = 8 to = 10. We took the average of the different degrees
in order to cover the whole phase-transition region which is about = 8 for instances of
size = 10 and = 10 for instances of size = 100. For all different combinations of
computing constrainedness, the ordering of the run times is the same for the different split
sets: B  Bb C8 Hb 8 Q8 . The run times of using static/local, static/global, or dynamic/local
for computing constrainedness are almost the same when combined with the same split set
while they are longer for all split sets when using dynamic/global (about 3 times longer
when using Bb as a split set and about 1.5 times longer when using the other split sets).
The 99% percentile run times are only about 1.5 times longer than the 50% percentile run
times. Thus, even the harder among the \normal" instances can be solved easily, i.e., apart
from a few hard instances, most instances can be solved eciently within the size range
we analyzed. The erratic behavior of the median curves results from an aggregation of the
effect which can be observed in Figure 5, namely, that some of the median elements in the
phase-transition are inconsistent and easily solvable.
A n; d;

:

d

d

d

n

d

>

;

n

;

304

fiEfficient Methods for Qualitative Spatial Reasoning

Median CPU time using STATIC,LOCAL for A(n,d,4.0)

99%-Percentile CPU time using STATIC,LOCAL for A(n,d,4.0)

0.6

0.6
B-split
B^-split
C-split
H^8-split
Q-split

0.4
0.3
0.2
0.1

20

30

40

50
60
nodes

70

80

90

0.2

100

10

Median CPU time using STATIC,GLOBAL for A(n,d,4.0)

20

30

40

50
60
nodes

70

80

90

100

99%-Percentile CPU time using STATIC,GLOBAL for A(n,d,4.0)

0.6

0.6
B-split
B^-split
C-split
H^8-split
Q-split

0.4

B-split
B^-split
C-split
H^8-split
Q-split

0.5
CPU time (sec)

0.5
CPU time (sec)

0.3

0
10

0.3
0.2
0.1

0.4
0.3
0.2
0.1

0

0
10

20

30

40

50
60
nodes

70

80

90

100

10

Median CPU time using DYNAMIC,LOCAL for A(n,d,4.0)

20

30

40

50
60
nodes

70

80

90

100

99%-Percentile CPU time using DYNAMIC,LOCAL for A(n,d,4.0)

0.6

0.6
B-split
B^-split
C-split
H^8-split
Q-split

0.4

B-split
B^-split
C-split
H^8-split
Q-split

0.5
CPU time (sec)

0.5
CPU time (sec)

0.4

0.1

0

0.3
0.2
0.1

0.4
0.3
0.2
0.1

0

0
10

20

30

40

50
60
nodes

70

80

90

100

10

Median CPU time using DYNAMIC,GLOBAL for A(n,d,4.0)

20

30

40

50
60
nodes

70

80

90

100

99%-Percentile CPU time using DYNAMIC,GLOBAL for A(n,d,4.0)

0.6

0.6
B-split
B^-split
C-split
H^8-split
Q-split

0.4

B-split
B^-split
C-split
H^8-split
Q-split

0.5
CPU time (sec)

0.5
CPU time (sec)

B-split
B^-split
C-split
H^8-split
Q-split

0.5
CPU time (sec)

CPU time (sec)

0.5

0.3
0.2
0.1

0.4
0.3
0.2
0.1

0

0
10

20

30

40

50
60
nodes

70

80

90

100

10

20

30

40

50
60
nodes

70

80

90

100

Figure 9: Percentile 50% and 99% CPU time of the different heuristics for solving
( 4 0) ( = 8 0 to = 10 0, 2,500 instances per data point)
A n; d;

:

d

:

d

:

305

fiRenz & Nebel

For the runtime studies for ( 4 0) we noticed that there are many hard instances
for 40 (see Figure 8), for = 80 almost all instances in the phase-transition region are
hard (see last column of Table 1). Also, as Table 1 shows, the number of hard instances
varies a lot for the different heuristics. Therefore, it is not possible to compare the percentile
running times of the different heuristics for 40. For = 80 and = 14 (see last column
of Table 1), for instance, the 50% and 99% percentile element of the C8/dynamic/global
heuristic is element no.36 and element no.72, while it is element no.141 and element no.280
of the Hb 8/dynamic/local heuristic (out of the 500 sorted elements), respectively.
For this reason we show the results only up to a size of = 40 (see Figure 10). Again, we
took the average of the different degrees from = 10 to = 15 in order to cover the whole
phase-transition region. The order of the run times is the same for different combinations
of computing constrainedness: B  Bb C8  Q8 Hb 8, while Hb 8 is in most cases the fastest.
As for the ( 4 0) instances, the run times for dynamic/global were much longer than
the other combinations. The 99% percentile run times of the static/global combination
and for Hb 8 and Q8 of the dynamic/local combination are faster than those of the other
combinations. Although the median CPU times are about the same as for ( 4 0) for
40, the percentile 99% CPU times are much longer. As it was already shown in Figure 7
and 8, this is further evidence that there are very hard instances in the phase-transition
region of ( 4 0).
H n; d;

n >

:

n

n >

n

d

n

d

;

A n; d;

d

;

:

A n; d;

:

n <

H n; d;

:

7. Orthogonal Combination of the Heuristics

In the previous section we studied the quality of different heuristics for solving randomly
generated RSAT instances. We found that several instances which are mainly located in the
phase-transition region could not be solved by some heuristics within our limit of 10,000
visited nodes in the search space. Since the different heuristics have a different search space
(depending on the split set) and use a different path through the search space (determined
by the different possibilities of computing constrainedness), it is possible that instances are
hard for some heuristics but easily solvable for other heuristics. Nebel (1997) observed that
running different heuristics in parallel can solve more instances of a particular hard set of
temporal reasoning instances proposed by van Beek and Manchak (1996) than any single
heuristic alone can solve, when using altogether the same number of visited nodes as for
each heuristic alone. An open question of Nebel's investigation (Nebel, 1997) was whether
this is also the case for the hard instances in the phase-transition region.
In this section we evaluate the power of \orthogonally combining" the different heuristics
for solving RSAT instances, i.e., running the different heuristics for each instance in parallel
until one of the heuristics solves the instance. There are different ways for simulating
this parallel processing on a single processor machine. One is to use time slicing between
the different heuristics, another is to run the heuristics in a fixed or random order until
a certain number of nodes in the search space is visited and if unsuccessful try the next
heuristic (cf. Huberman, Lukose, & Hogg, 1997). Which possibility is chosen and with
which parameters (e.g., the order in which the heuristics are run and the number of visited
nodes which is spent for each heuristic) determines the eciency of the single processor
simulation of the orthogonal combination. In order to find the best parameters, we ran all
heuristics using at most 10,000 visited nodes for each heuristic on the set of hard instances
306

fiEfficient Methods for Qualitative Spatial Reasoning

Median CPU time using STATIC,LOCAL for H(n,d,4.0)

99%-Percentile CPU time using STATIC,LOCAL for H(n,d,4.0)

0.045

5
B-split
C-split
B^-split
Q-split
H^8-split

CPU time (sec)

0.035
0.03

B-split
C-split
B^-split
Q-split
H^8-split

4
CPU time (sec)

0.04

0.025
0.02
0.015
0.01

3

2

1

0.005
0

0
10

15

20

25
nodes

30

35

40

10

Median CPU time using STATIC,GLOBAL for H(n,d,4.0)

20

25
nodes

30

35

40

99%-Percentile CPU time using STATIC,GLOBAL for H(n,d,4.0)

0.045

5
B-split
C-split
B^-split
Q-split
H^8-split

0.035
0.03

B-split
C-split
B^-split
Q-split
H^8-split

4
CPU time (sec)

0.04
CPU time (sec)

15

0.025
0.02
0.015
0.01

3

2

1

0.005
0

0
10

15

20

25
nodes

30

35

40

10

Median CPU time using DYNAMIC,LOCAL for H(n,d,4.0)

20

25
nodes

30

35

40

99%-Percentile CPU time using DYNAMIC,LOCAL for H(n,d,4.0)

0.045

5
B-split
C-split
B^-split
Q-split
H^8-split

0.035
0.03

B-split
C-split
B^-split
Q-split
H^8-split

4
CPU time (sec)

0.04
CPU time (sec)

15

0.025
0.02
0.015
0.01

3

2

1

0.005
0

0
10

15

20

25
nodes

30

35

40

10

Median CPU time using DYNAMIC,GLOBAL for H(n,d,4.0)

20

25
nodes

30

35

40

99%-Percentile CPU time using DYNAMIC,GLOBAL for H(n,d,4.0)

0.045

5
B-split
C-split
B^-split
Q-split
H^8-split

0.035
0.03

B-split
C-split
B^-split
Q-split
H^8-split

4
CPU time (sec)

0.04
CPU time (sec)

15

0.025
0.02
0.015
0.01

3

2

1

0.005
0

0
10

15

20

25
nodes

30

35

40

10

15

20

25
nodes

30

35

40

Figure 10: Percentile 50% and 99% CPU time of the different heuristics for solving
( 4 0) ( = 10 0 to = 15 0, 5,500 instances per data point)
H n; d;

:

d

:

d

:

307

fiRenz & Nebel

A(n; d; 4:0)
Heuristics Solved Instances 1. Response
91:88%
19:80%
Hb8 /sta/loc
Hb8 /sta/glo
94:67%
12:56%
Hb8 /dyn/loc
93:40%
24:37%
Hb8 /dyn/glo
87:31%
13:58%
C8 /sta/loc
89:72%
6:35%
C8 /sta/glo
92:64%
5:20%
C8 /dyn/loc
90:10%
5:96%
C8 /dyn/glo
86:63%
6:60%
Q8 /sta/loc
89:72%
9:77%
Q8 /sta/glo
93:15%
12:06%
Q8 /dyn/loc
90:61%
10:15%
Q8 /dyn/glo
86:80%
12:82%
91:37%
1:40%
Bb/sta/loc
bB/sta/glo
88:71%
1:27%
Bb/dyn/loc
91:12%
0:89%
Bb/dyn/glo
79:44%
0:89%
B/sta/loc
79:31%
0:51%
B/sta/glo
71:83%
0:25%
B/dyn/loc
73:48%
0:51%
B/dyn/glo
{
0:13%
combined
99:87%

H (n; d; 4:0)
Solved Instances 1. Response
71:86%
6:92%
85:58%
14:26%
86:73%
22:28%
67:98%
15:00%
61:60%
1:47%
79:41%
5:04%
56:15%
2:26%
44:64%
2:40%
67:78%
1:63%
82:43%
3:61%
81:72%
1:83%
60:78%
4:61%
68:42%
1:84%
81:58%
5:22%
60:32%
2:56%
{
1:83%
{
1:67%
{
1:13%
{
0:42%
{
0:49%
96:48%

Table 2: Percentage of solved hard instances for each heuristic and percentage of first response when orthogonally running all heuristics. Note that sometimes different
heuristics are equally fast. Therefore the sum is more than 100%.

identified in the previous section (those instances for which at least one heuristic required
more than 10,000 visited nodes) and compared their behavior. Since we ran all heuristics
on all instances already for the experiments of the previous section, we only had to evaluate
their outcomes. This led to a very surprising result for the ( 4 0) instances, namely, all
of the 788 hard instances except for a single one were solved by at least one of the heuristics
using less than 10,000 visited nodes. In Table 2 we list the percentage of hard instances
that could be solved by the different heuristics and the percentage of first response by each
of them when running the heuristics in parallel (i.e., which heuristic required the smallest
number of visited nodes for solving the instance). It turns out that the heuristics using Hb 8
as a split set did not only solve more instances than the other heuristics, they were also
more often the fastest in finding a solution. Although the heuristics using the other two
maximal tractable subsets Q8 and C8 as a split set did not solve significantly more instances
than the heuristics using B,b they were much faster in finding a solution. Despite solving
the least number of instances, the heuristics using B as a split set were in some cases the
fastest in producing a solution.
A n; d;

308

:

fiEfficient Methods for Qualitative Spatial Reasoning

First Response for Solving the Hard Instances of A(n,d,4.0)

First Response for Solving the Hard Instances of H(n,d,4.0)

20
700
Number of solved instances

Number of solved instances

inconsistent
consistent
15

10

5

inconsistent
consistent

600
500
400
300
200
100

0

0
1

10
100
1000
Minimal number of visited nodes

10000

1

10
100
1000
Minimal number of visited nodes

10000

Figure 11: Fastest solution of the hard instances when running all heuristics in parallel
When comparing the minimal number of visited nodes of all the heuristics for all the
hard instances, we found that only five of them (which were all inconsistent) required
more than 150 visited nodes. This is particularly remarkable as all these instances are
from the phase-transition region of an NP-hard problem, i.e., instances which are usually
considered to be the most dicult ones. Further note that about 15% (120) of the 788 (pathconsistent) instances were inconsistent, which is much higher than usual (cf. Figure 7).
Interestingly, most of those inconsistent instances were solved faster than the consistent
instances. At this point, it should be noted that combining heuristics orthogonally is very
similar to randomized search techniques with restarts (Selman, Levesque, & Mitchell, 1992).
However, in contrast to randomized search, our method can also determine whether an
instance is inconsistent. In Figure 11 we chart the number of hard instances solved with the
smallest number of visited nodes with respect to their solubility. Due to the low number
of hard instances of ( 4 0), the figure on the left looks a bit ugly but one can at least
approximate the behavior of the curves when comparing it with the second figure on the
right which is the same curve for ( 4 0) (see below). The oscillating behavior of the
inconsistent instances (more instances are solved with an odd than with an even number
of visited nodes) might be due to the sizes of the instances|we generated instances with
an even number of nodes only. The most dicult instance ( = 56 = 10) was solved
b static/global heuristic using about 91,000 visited nodes while all
as inconsistent with the B/
heuristics using one of the maximal tractable subsets as a split set failed to solve it even
when each was allowed to visit 20,000,000 nodes in the search space.
We did the same examination for the set of 75,081 hard instances of ( 4 0). 2,640 of
these instances could not be solved by any of the 20 different heuristics using 10,000 visited
nodes each. Their distribution is shown in Figure 12(a). Similar to the hard instances
of ( 4 0), the heuristics using Hb 8 as a split set were the most successful ones for
solving the hard instances of ( 4 0), as shown in Table 2. They solved more of the
hard instances than any other heuristics and produced the fastest response of more than
50% of the hard instances. There is no significant difference between using C8 Q8 or Bb
as a split set, neither in the number of solved instances nor in the percentage of first
response. Like in the previous case, computing constrainedness using the static/global or the
dynamic/local heuristics resulted in more successful paths through the search space by which
A n; d;

:

H n; d;

:

n

;d

H n; d;

A n; d;

:

:

H n; d;

:

;

309

;

fiRenz & Nebel

First Response for Solving the Hard Instances of H(n,d,4.0)
Number of hard instances for H(n,d,4.0) using orthogonal combination
inconsistent
consistent

#Hard Instances
100
80
60
40
20
0

80
60
40

6 8
10 12
14 16
average degree
18 20

nodes

Number of solved instances

100

80

60

40

20

20
0
20000

40000
60000
80000
Number of visited nodes

100000

(a)
(b)
Figure 12: Hard instances using orthogonal combination of all heuristic for ( 4 0),
(a) shows their distribution, (b) shows their fastest solution when using up to
100,000 visited nodes per heuristic
H n; d;

:

more instances were solved within 10,000 visited nodes than by the other combinations. On
average they produced faster solutions than the other combinations.
The same observations as for ( 4 0) can be made when charting the fastest solutions
of the hard instances of ( 4 0) (see Figure 11). About 29% (21,307) of the solved
instances are inconsistent. Most of them were, again, solved faster than the consistent
instances. More than 75% of the hard instances can be solved with at most 150 visited nodes.
90% can be solved with at most 1,300 visited nodes. Since the Hb 8/dynamic/local heuristic
alone solves more than 86% of the instances, it seems dicult to combine different heuristics
in a way that more hard instances can be solved while using not more than 10,000 visited
nodes altogether. However, when orthogonally combining the two best performing heuristics
(Hb 8 /dynamic/local and Hb 8/static/global) allowing each of them a maximal number of 5,000
visitable nodes, we can solve 92% (69,056) of the hard instances.
We tried to solve the 2,640 hard instances of ( 4 0) which are not solvable using
orthogonal combination of heuristics with at most 10,000 visited nodes by using a maximal
number of 100,000 visited nodes. 471 of these instances are still not solvable, more than
75% of the solved instances are inconsistent. The fastest response for the solved instances
is charted in Figure 12(b). The most successful heuristics in giving the fastest response
are Hb 8 /dynamic/local (42.5%) and Hb 8/static/global (26.6%). The three heuristics using
static/global computation of constrainedness combined with using Q8 C8 and Bb as a split
set gave the fastest response for 15.9% of the solved instances where the Bb strategy was by
far the best among the three (9.4%).
A n; d;

H n; d;

:

:

H n; d;

:

;

;

8. Combining Heuristics for Solving Large Instances

In the previous section we found that combining different heuristics orthogonally can solve
more instances using the same amount of visited nodes than any heuristic alone can solve. In
this section we use these results in order to identify the size of randomly generated instances
310

fiEfficient Methods for Qualitative Spatial Reasoning

up to which almost all of them, especially those in the phase-transition region, can still be
solved in acceptable time. Since many instances of ( 4 0) are already too dicult for
a size of = 80 (see Figure 12), we restrict our analysis to the instances of ( 4 0) and
study randomly generated instances with a size of more than = 100 nodes.
For instances of a large size allowing a maximal number of 10,000 visited nodes in
the search space is too much for obtaining an acceptable runtime. 10,000 visited nodes for
instances of size = 100 corresponds to a runtime of more than 10 seconds on a Sun Ultra1,
for larger instances it gets much slower. Therefore, we have to restrict the maximal number
of visited nodes in order to achieve an acceptable runtime. Given a multi-processor machine,
the different heuristics can be run orthogonally on different processors using the maximal
number of visited nodes each. If the orthogonal combination of the different heuristics is
simulated on a single-processor machine, the maximal number of nodes has to be divided
by the number of used heuristics to obtain the available number of visitable nodes for each
heuristic. Thus, the more different heuristics we use, the less visitable nodes are available
for each heuristic. Therefore, in order to achieve the best performance, we have to find
the combination of heuristics that solves most instances within a given number of visitable
nodes. The chosen heuristics should not only solve many instances alone, they should also
complement each other well, i.e., instances which cannot be solved by one heuristic should
be solvable by the other heuristic.
We started by finding the optimal combination of heuristics for the set of 788 hard
instances of ( 4 0). From our empirical evaluation given in Section 6 we know how
many visited nodes each heuristic needs in order to solve each of the 788 hard instances.
Therefore, we computed the number of solved instances for all 220 possible combinations
of the heuristics using an increasing maximal number of visitable nodes for all heuristics
together. Since we only tried to find the combination which solves the most instances,
this can be computed quite fast. The results are given in Table 3. They show that a
good performance can be obtained with a maximal number of 600 visited nodes. In this
case four heuristics were involved, i.e., 150 visitable nodes are spent on each of the four
heuristics. Since the same combination of heuristics (Hb 8/static/global, Hb 8 /dynamic/local,
b static/local) is also the best for up to 1,000 visitable nodes, we choose
C8/dynamic/local, B/
this combination for our further analysis. We choose the order in which they are processed to
b static/local according
be 1. Hb 8/dynamic/local, 2. Hb 8 /static/global, 3. C8/dynamic/local, 4. B/
to their first response behavior given in Table 2. Note that although the two heuristics
b static/local do not show a particularly good performance when
C8/dynamic/local and B/
running them alone (see Table 2), they seem to best complement the other two heuristics.
What we have to find next is the maximal number of visitable nodes we spend for the
heuristics. For this we ran the best performing heuristic (Hb 8/dynamic/local) on instances
of the phase-transition region of varying sizes. It turned out that for almost all consistent
instances the number of visited nodes required for solving them was slightly less than twice
the size of the instances while most inconsistent instances are also not path-consistent and,
thus, solvable with only one visited node. Therefore, we ran the four heuristics in the
following allowing 2 visited nodes each, where is the size of the instance, i.e., together
we allow at most 8 visitable nodes. We randomly generated test instances according to
the ( 4 0) model for a size of = 110 regions up to a size of = 500 regions with
a step of 10 regions and 100 instances for each size and each average degree ranging from
H n; d;

:

n

A n; d;

n

n

A n; d;

:

n

n

n

A n; d;

:

n

n

311

:

fiRenz & Nebel

Max Nodes Solved Instances
100
516
200
705
300
759
400
769
500
774
600
778
700
780
800
783
900
784
1100
785
1300
786
3900
787

Combination of Heuristics
Hb 8-d-l
Hb 8-s-g
Hb 8-s-g, Hb 8-d-l
Hb 8-s-g, C8 -d-l
Hb 8-s-g, Hb 8-d-l, C8 -d-l
b
Hb 8-s-g, Hb 8-d-l, C8 -d-l, B-s-l
b
b
b
H8-s-g, H8-d-l, C8 -d-l, B-s-l
b
Hb 8-s-g, Hb 8-d-l, C8 -d-l, B-s-l
b
b
b
H8-s-g, H8-d-l, C8 -d-l, B-s-l
b B-s-g
b
Hb 8-s-g, Hb 8-d-l, C8 -d-l, B-s-l,
b B-s-g
b
Hb 8-s-g, Hb 8-d-l, B-s-l,
b
b
b
H8-s-g, H8-d-l, B-d-l

Table 3: Best performance of combining different heuristics for solving the 787 solvable hard
instances of ( 4 0) with a fixed maximal number of visited nodes
A n; d;

:

Probability of satisfiability for A(n,d,4.0)

Probability (%)

Average number of visited nodes for A(n,d,4.0)

Visited nodes

100

1000
900
800
700
600
500
400
300
200
100

50

4 6
8 10
12 14
average degree
16 18

500
450
400
350
300
250 nodes
200
150

4 6
8 10
12 14
average degree
16 18

500
450
400
350
300
250 nodes
200
150

Figure 13: Probability of satisfiability for ( 4 0) (100 instances per data point) and
average number of visited nodes of the path-consistent instances when using
orthogonal combination of the four selected heuristics
A n; d;

:

= 2 0 to = 18 0 with a step of 0.5, a total number of 132,000 instances. Since solving
large instances using backtracking requires a lot of memory, we solved the instances on a
Sun Ultra60 with 1GB of main memory.
The generated instances display a phase-transition behavior which continues the one
given in Figure 5. The phase-transition ranges from = 10 0 for = 110 to = 10 5
for = 500 (see Figure 13). Apart from 112 instances, all other instances we generated were solvable by orthogonal combination of the four heuristics (Hb 8 /static/global,
b static/local) spending less than 2n visited nodes
Hb 8/dynamic/local, C8/dynamic/local, B/
d

:

d

:

d

n

312

:

n

d

:

fiEfficient Methods for Qualitative Spatial Reasoning

Percentile 70% CPU time using orthogonal combination for A(n,d,4.0)

Percentile 99% CPU time using orthogonal combination for A(n,d,4.0)
CPU time (s)

CPU time (s)
100
25
20
15
10
5
0

80
60

4 6
8 10
12 14
average degree
16 18

40

500
450
400
350
300
250 nodes
200
150

20
0
4 6
8 10
12 14
average degree
16 18

500
450
400
350
300
250 nodes
200
150

Figure 14: Percentile 70% and 99% CPU time of the orthogonal combination of four different heuristics for solving large randomly generated instances of ( 4 0)
A n; d;

:

each. In Figure 13 we give the average number of visited nodes of the path-consistent
instances. It can be seen that for our test instances the average number of visited nodes
is linear in the size of the instances. The percentile 70% CPU time for instances of the
phase-transition with a size of = 500 regions is about 20 seconds, the percentile 99% CPU
time is about 90 seconds. Up to a size of = 400 regions, the percentile 99% CPU time is
less than a minute (see Figure 14).
131,240 of our test instances were already solved by the Hb 8/static/global heuristic,
for 71 instances the Hb 8/dynamic/local heuristic was required and for 577 instances the
C8/dynamic/local heuristic produced the solution. None of the 112 instances which were
b static/local heuristic. We
not solved by one of those three heuristics were solved by the B/
tried to solve these instances using the other heuristics, again using a maximal number of 2
visited nodes each. The best performing among those heuristics was the C8/dynamic/global
heuristic which solved 87 of the 112 instances followed by the C8 /static/global heuristic (83)
and the Q8 /dynamic/global heuristic (63). 7 instances were not solved by any heuristic
within a maximal number of 2 visited nodes.
n

n

n

n

9. Discussion

We empirically studied the behavior of solving randomly generated RSAT instances using
different backtracking heuristics some of which make use of the maximal tractable subsets
identified in previous work. We generated instances according to two different models of
which the \general model" allows all 256 RCC-8 relations to be used while the \hard
model" allows only relations which are not contained in any of the maximal tractable
subsets. A theoretical analysis of the two models showed that the model and the model
for a small average degree of the nodes in the constraint graph do not suffer from trivial
local inconsistencies as it is the case for similar generation procedures for CSPs with finite
domains (Achlioptas et al., 1997). It turned out that randomly generated instances of
both models show a phase-transition behavior which depends most strongly on the average
degree of the instances. While most instances outside the phase-transition region can be
A

H

H

A

313

fiRenz & Nebel

solved eciently by each of our heuristics, instances in the phase-transition region can be
extremely hard. For the instances of the general model, most path-consistent instances
are also consistent. Conversely, path-consistency is a bad approximation to consistency for
instances of the hard model. These instances are also much harder to solve than instances
of the general model.
When comparing the different heuristics, we found that the heuristics using one of the
maximal tractable subsets as a split set are not as much faster in deciding consistency of
RSAT instances as their theoretical advantage given by the reduced average branching factor
and the resulting exponentially smaller size of the search space indicates. This is because
using path-consistency as a forward checking method considerably reduces the search space
in all cases. Nevertheless, using one of the maximal tractable subsets as a split set, in
particular Hb 8, still leads to a much faster solution and solves more instances in reasonable
time than the other heuristics. Although the two maximal tractable subsets Q8 and C8
contain more relations than Hb 8 , their average branching factor is lower, i.e., when using
Hb 8 one has to decompose more relations (256 148 = 108) than when using the other two
sets (96 and 98 relations, respectively), but Hb 8 splits the relations better than the other
two sets. Most relations can be decomposed into only two Hb 8 sub-relations, while many
relations must be decomposed into three C8 sub-relations or into three Q8 sub-relations.
This explains the superior performance of heuristics involving Hb 8 for decomposition.
Among the instances we generated, we stored those which could not be solved by all
heuristics within a maximum number of 10,000 visited nodes in the search space in order to
find out how the different heuristics perform on these hard instances. We found that almost
all hard instances are located in the phase-transition region and that there are many more
hard instances in the hard model than in the general model. We orthogonally combined all
heuristics and ran them on all hard instances. This turned out to be very successful. Apart
from one instance, all hard instances of the general model could be solved, most of them
with a very low number of visited nodes. The hard instances of the hard model were much
more dicult: many of them could not be solved by any of the heuristics. Nevertheless,
many more instances were solved by orthogonally combining the heuristics than by each
heuristic alone. Again, most of them were solved using a low number of visited nodes.
Based on our observations on orthogonally combining different heuristics, we tried to
identify the combination of heuristics which is most successful in eciently solving many
instances and used this combination for solving very large instances. It turned out that
the best combination involves only heuristics which use maximal tractable subsets for decomposition. With this combination we were able to solve almost all randomly generated
instances of the phase-transition region of the general model up to a size of = 500 regions
very eciently. This seems to be impossible when considering the enormous size of the
search space, which is on average 1039323 for instances of size = 500 when using Hb 8 as a
split set.
Our results show that despite its NP-hardness, we were able to solve almost all randomly generated RSAT instances of the general model eciently. This is neither due to the
low number of different RCC-8 relations (instances generated according to the hard model
are very hard in the phase-transition region) nor to our generation procedure for random
instances which does not lead to trivially awed instances asymptotically. It is mainly due
to the maximal tractable subsets which cover a large fraction of RCC-8 and which lead
n

n

314

fiEfficient Methods for Qualitative Spatial Reasoning

to extremely low branching factors. Since there are different maximal tractable subsets,
they allow choosing between many different backtracking heuristics which further increases
eciency: some instances can be solved easily by one heuristic, other instances by other
heuristics. Heuristics involving maximal tractable subclasses showed the best behavior but
some instances can be solved faster when other tractable subsets are used. The full classification of tractable subsets gives the possibility of generating hard instances with a high
probability. Many randomly generated instances of the phase-transition region are very
hard when using only relations which are not contained in any of the tractable subsets
and consist of more than = 60 regions. The next step in developing ecient reasoning
methods for RCC-8 is to find methods which are also successful in solving most of the hard
instances of the hard model.
n

The results of our empirical evaluation of reasoning with RCC-8 suggest that analyzing
the computational properties of a reasoning problem and identifying tractable subclasses of
the problem is an excellent way for achieving ecient reasoning mechanisms. In particular
maximal tractable subclasses can be used to develop more ecient methods for solving
the full problem since their average branching factor is the lowest. Using the refinement
method developed in Renz's (1999) paper, tractable subclasses of a set of relations forming
a relation algebra can be identified almost automatically. This method makes it very easy
to develop ecient algorithms. A further indication of our empirical evaluation is that it
can be much more effective (even and especially for hard instances of the phase-transition
region) to orthogonally combine different heuristics than to try to get the final epsilon out of
a single heuristic. This answers a question raised by Nebel (1997) of whether the orthogonal
combination of heuristics is also useful in the phase-transition region. In our experiments
this lead to much better results even when simulating the orthogonal combination of different
heuristics on a single processor machine and spending altogether the same resources as
for any one heuristic alone. In contrast to the method of time slicing between different
heuristics, we started a new heuristic only if the previous heuristic failed after a certain
number of visited nodes in the search space. The order in which we ran the heuristics
depended on their performance and on how well they complemented each other, more
successful heuristics were used first. This is similar to using algorithm portfolios as proposed
by Huberman et al. (1997). Which heuristics perform better and which combination is the
most successful one is a matter of empirical evaluation and depends on the particular
problem. Heuristics depending on maximal tractable subclasses, however, should lead to
the best performance.
For CSPs with finite domains there are many theoretical results about localizing the
phase-transition behavior and about predicting where hard instances are located. In contrast to this, there are basically no such theoretical results for CSPs with infinite domains
as used in spatial and temporal reasoning. As our initial theoretical analysis shows, theoretical results on CSPs with finite domains do not necessarily extend to CSPs with infinite
domains. It would be very interesting to develop a more general theory for CSPs with
infinite domains, possibly similar to Williams and Hogg's \Deep Structure" (Williams &
Hogg, 1994) or Gent et al.'s \Kappa" theory (Gent, MacIntyre, Prosser, & Walsh, 1996).
315

fiRenz & Nebel
Acknowledgments

We would like to thank Ronny Fehling for his assistance in developing the programs, Malte
Helmert for proof reading the paper, and the three anonymous reviewers for their very
helpful comments.
This research has been supported by DFG as part of the project fast-qual-space, which
is part of the DFG special research effort on \Spatial Cognition". The first author has been
partially supported by a Marie Curie Fellowship of the European Community programme
\Improving Human Potential" under contract number HPMF-CT-2000-00667. A preliminary version of this paper appeared in the Proceedings of the 13th European Conference on
Artificial Intelligence (Renz & Nebel, 1998).
References

Achlioptas, D., Kirousis, L., Kranakis, E., Krizanc, D., Molloy, M., & Stamatiou, Y. (1997).
Random constraint satisfaction: a more accurate picture. In 3rd Conference on the
Principles and Practice of Constraint Programming (CP'97), Vol. 1330 of LNCS, pp.
107{120. Springer-Verlag.
Allen, J. F. (1983). Maintaining knowledge about temporal intervals. Communications of
the ACM, 26 (11), 832{843.
Bennett, B. (1994). Spatial reasoning with propositional logic. In Doyle, J., Sandewall,
E., & Torasso, P. (Eds.), Principles of Knowledge Representation and Reasoning:
Proceedings of the 4th International Conference, pp. 51{62, Bonn, Germany. Morgan
Kaufmann.
Bennett, B. (1997). Logical Representations for Automated Reasoning about Spatial Relationships. Ph.D. thesis, School of Computer Studies, The University of Leeds.
Cheeseman, P., Kanefsky, B., & Taylor, W. M. (1991). Where the really hard problems are.
In Proceedings of the 12th International Joint Conference on Artificial Intelligence,
pp. 331{337, Sydney, Australia. Morgan Kaufmann.
Gent, I., MacIntyre, E., Prosser, P., Smith, B., & Walsh, T. (2001). Random constraint
satisfaction: Flaws and structure. CONSTRAINTS, 6 (4), 345{372.
Gent, I., MacIntyre, E., Prosser, P., & Walsh, T. (1996). The constrainedness of search. In
Proceedings of the 13th National Conference on AI (AAAI'96), pp. 246{252.
Golumbic, M. C., & Shamir, R. (1993). Complexity and algorithms for reasoning about time:
A graph-theoretic approach. Journal of the Association for Computing Machinery,
40 (5), 1128{1133.
Grigni, M., Papadias, D., & Papadimitriou, C. (1995). Topological inference. In Proceedings
of the 14th International Joint Conference on Artificial Intelligence, pp. 901{906,
Montreal, Canada.
316

fiEfficient Methods for Qualitative Spatial Reasoning

Haralick, R. M., & Elliot, G. L. (1980). Increasing tree search eciency for constraint
satisfaction problems. Artificial Intelligence, 14, 263{313.
Huberman, B., Lukose, R., & Hogg, T. (1997). An economics approach to hard computational problems. Science, 275, 51{54.
Ladkin, P. B., & Reinefeld, A. (1992). Effective solution of qualitative interval constraint
problems. Artificial Intelligence, 57 (1), 105{124.
Ladkin, P. B., & Reinefeld, A. (1997). Fast algebraic methods for interval constraint problems. Annals of Mathematics and Artificial Intelligence, 19 (3,4).
Mackworth, A. K. (1977). Consistency in networks of relations. Artificial Intelligence, 8,
99{118.
Mackworth, A. K., & Freuder, E. C. (1985). The complexity of some polynomial network
consistency algorithms for constraint satisfaction problems. Artificial Intelligence, 25,
65{73.
Montanari, U. (1974). Networks of constraints: fundamental properties and applications to
picture processing. Information Science, 7, 95{132.
Nebel, B. (1995). Computational properties of qualitative spatial reasoning: First results. In
Wachsmuth, I., Rollinger, C.-R., & Brauer, W. (Eds.), KI-95: Advances in Artificial
Intelligence, Vol. 981 of Lecture Notes in Artificial Intelligence, pp. 233{244, Bielefeld,
Germany. Springer-Verlag.
Nebel, B. (1997). Solving hard qualitative temporal reasoning problems: Evaluating the
eciency of using the ORD-Horn class. CONSTRAINTS, 3 (1), 175{190.
Randell, D. A., Cohn, A. G., & Cui, Z. (1992a). Computing transitivity tables: A challenge
for automated theorem provers. In Proceedings of the 11th CADE. Springer-Verlag.
Randell, D. A., Cui, Z., & Cohn, A. G. (1992b). A spatial logic based on regions and
connection. In Nebel, B., Swartout, W., & Rich, C. (Eds.), Principles of Knowledge
Representation and Reasoning: Proceedings of the 3rd International Conference, pp.
165{176, Cambridge, MA. Morgan Kaufmann.
Renz, J. (1999). Maximal tractable fragments of the Region Connection Calculus: A complete analysis. In Proceedings of the 16th International Joint Conference on Artificial
Intelligence, pp. 448{454, Stockholm, Sweden.
Renz, J. (2000). Qualitative Spatial Reasoning with Topological Information. Ph.D. thesis,
Institut fur Informatik, Albert-Ludwigs-Universitat Freiburg.
Renz, J., & Nebel, B. (1998). Ecient methods for qualitative spatial reasoning. In Proceedings of the 13th European Conference on Artificial Intelligence, pp. 562{566, Amsterdam, The Netherlands. Wiley.
317

fiRenz & Nebel

Renz, J., & Nebel, B. (1999). On the complexity of qualitative spatial reasoning: A maximal
tractable fragment of the Region Connection Calculus. Artificial Intelligence, 108 (12), 69{123.
Selman, B., Levesque, H. J., & Mitchell, D. (1992). A new method for solving hard satisfiability problems. In Proceedings of the 10th National Conference of the American
Association for Artificial Intelligence, pp. 440{446, San Jose, CA. MIT Press.
van Beek, P., & Manchak, D. W. (1996). The design and experimental analysis of algorithms
for temporal reasoning. Journal of Artificial Intelligence Research, 4, 1{18.
Williams, C. P., & Hogg, T. (1994). Exploiting the deep structure of constraint problems.
Artificial Intelligence, 70, 73{117.

318

fi	
fffi 	


	"!$#%'&)(**#+,#.-/#0#1

>

2345'67(08**9:4
!6;*<8*=#

?A@CBEDGFH?AIJLKM?ONQPSRSJUTWVRYXZ@[T]\^?A_'`@aIb_'I@aT]TcRCVedf?gI`?AVihj?'NkcRaX]lST

mHnQoprq]stouwvHn]qyxxqz{n|qpr}Q}]q

~~y]~y0 ~Q yy )

a.$EQr'yG3$3)3e$GrrG
{)rHG)GY$3)3
gr0
C{rEQ)
rG3
'A]qyxGnQo)}]qyprxGno

A30QyG7 g| yG3

a.$A^{)QC'3G3S'r){3)r
ar=A|{)3)$)rr0
)rr

Ar]
'a7=WQAASGH{G=$EU)=
=7;A";3]''Hrr/r$";Ga3S0=a);=3
'S=UrA=r;"QS3GHrGY
$"rGr)=Hr a{=HYr0=UrS]
{ =]G)HU=)=|{U'U{=^A]{ a0H
0"rGQ{QrS{=$]]$=gQw;AA =r
={QUUQ3Gr3'G={=YGSG'e='$0H
=a=r

 	ff
fiA	 
		!"#$&%('#)*	,+.-/0	)&21430576	698	:	;9)<$>=0@?BA9C#D	E9F$0)130576	6	Gff	)&HC9'F%0)I.;#9D#)	C9E7J
K0FL)&0C9)&07$MF$'N#O'FPC9)&'N9	#$KRQS'#T90UBVWE#X#$&%('#)*	O	)&@T9)&K$TBFK#KKXDN)>	C9E#
+ZYH[,\L!G%,E#'7,#'#T9()&0C9)&>0	$)	#T9'NQ^]F	)&	#W%,E#,$!E#W_9*7W)&0C9)&07$"K07;#MF97`a;#0#K0U
VWE#"F>'NKF$'Nb9$&%0PD#)	C9Ec$ME#'N)&P7#TdC9)<'N9	#$&S0)&]7"Fe7PDN7	$ff$'#'	7f'#)gE9	#T9#D
;9#K0)&$Mh_7$&X_i)&0Fffjf	U@VWE#O#$k%'N)>*	P	)<l#K0)&0h#D	m_#DX;#Tn_oT9]70)&@	)&0FH)&'NQ
QFD	PC9)<'NK>_#Dp+Z[HD	'	$!91q305	5	r	Gs$'tQST9K0F/T9FDN#'	>+2A9E7%u?wvH$ME#0)&0135	5930GU
VxE#d;#>!.;##c'F($ME#>d#$&%'N)*7L)&SE#0]Np'Ny'	]##D@$ME#tC9)&'N#0Qz'F7f0)&0#K	US[
	)&D	ON;9QS90){'F,FD	'N)&$ME9QdlE9]	t0pT9]	'NCT|$'@!}tK07$pK'NQC9;#$O$ME#*FE#'N'#T~7J
FK$	17	QbC##K_;#T9PC9)>;9#_#D9F>TtQS$ME#'#T9,+2F0);#j"14305	576	G1'N)/$ME#,'N;9#T9TtK'##T9$>'#7J
#DtQS$ME#'#Tm+.W'N)<]N$=	19A9;#0)QS'##T9$014?'#'NC90)1305	6	57Gff$K	U"x'F%]	0)$ME#PFD	'N)<$!E9QS,	)&H'F%
+Z	0#>0q1H'##D41N?BF0);#j"1q305	5	7G%,E#0O	C9C#Tt$>'PT90#K'N9#K$>T!"#$&%('#)*	F+2WMGU
n	)<D	#$&%'N)*7P9FK$FD	'N)<$!E9QS	)&O_7$M)hK$M	#	1"FS$ME#R)<;#)&@M;9QbQS#Dp'F]	0)S	p7J
C9'##0	$Fn_7)&D	t#;9QS90)L'hE#T9T90n$MF$>0US[C9'	>_#	C9C9)<'NFKE$>'$MFK*7b$ME#lC9)&'N#0Q
$'p)&'#)&$S$'Xo7)*F'F]RWE9FnX'N7$@W	)&'R9F>T~QS$ME#'#T9_*h\L9#tA97QC#_#DB+2\{0Q	|?
\L0Qb	q1g35	6F48qW0hf1ff35	5		GUHVWE#P	C9C9)&'NhKENT9lFKK0;9)>F$)&M;#$P9;#$HP#$M)&0QS>'F%^
K'N7]	0)&D	0#K	U
E (**=#|fiyfi96	Q	6$0	ff]	3ff5'	ff4
!fi

G!3Z6F

fi

y)

a{yy

I @$ME#LC9	C90)x%L%W2'NK0;#H'NO$ME#SQS0	7J4TX$ME#'N)&@9'#))&'F%TO.)&'#Q $MF$>>$K0FgQSKE9	7J
K0U"VWE#H	$!;#$>'#@90E##T$ME#>uQS$!E#'NT9$ME9F$(S#'#T9,W)&F$]	O#0#$]	P$>'lC9	)&$>K0;#	)
$$>_#D7L'F"$ME#S] F;#c'F"$P#DNEN'N)&01/9;#$P)F$ME#0)xT90C90#T9c'Ny$ME#)LQS0	7J]FF;#0UlVxE#c'N7J
0)&]FF$'Ny0FT9L$'tbQC#t	#TX'F2$0]	0)&FKK0;9)h$lC9)&'#KT4;9)&08"+fP	'#)&T4	q1q\PE9	E9)7Q	#21
N	7*N*F'7_1?A97;#f1/305	5 	GUgVxE#H$KE9#;#cNT9u7O$QF$L'hg$ME#PQS0	#x'Fe$!E#P;9#_#>$M	7$jJ
F$T@#'#T9,	#T@F'	>$QF$H'F$ME#PC9	)&$>$>'#S;9#K$'NqU"VWE#PQbFtFQw'Fg$!E#WC9	C0)W$'
$M;#T9@C' %0)I;#gQS077J 4Ty$KE9#9;#ST4;#P$'-s!*0X+&30576		G1#	#T	C9C#@$W$'O(x0U
-s!.*S#$>_hpC9)&'NC'	TE#W$ME#'#)&tO$ME#LK'#	$#$'FffMC#OD	F01 E#0#KP
$ 	 W	C9C#K0F$'N
$'@'	$=0Q	9ypFKE#_#t+."XMG+Z[,K*7	1qx_7$'Nq1q? A#&<#'F%W!*	21ff305	67	GL$M)hD#E	$&2'N)&%W	)<TU(V '
T9]	'NCpQd0	7J 4Tp$ME#'N)&{f'N)HWL'#$!E#l#S'F(X01q$LcQC9'N)<$M	7$,$ME9h$,%S$P;9CX
)	QS%'N)*Hb%,E#KEO9'7$MES$ME#H#$k%'N)>*	/K0	,$!;#T9TU"'7$MEX"	#TtWK0	,0bF
T9j0)&0	$")<0F=0h$'N#"'Ffiff!\P)	C9E#K0F4Qd'NT9 t+2\{XMGUbA#K$'NSH%(W)&]#%|$ME#T9 a#$'N#"'F
X(	#Ttx(FMC9KFqK0F>"'F\LX(	#Tdf'N)>Ql;#F$>W$ME#,F>'NKF$>TtC9)&'N9	#$&T9$M)&9;#$'N
F,'	$=0Qb	9\{_9#xT9>$M)&9;#$'NqU
vL;9)LQbFp)&M;#$S	)&SRA#K$'N
 9UlIX$ME#P>K$'Np-ff!*0 	 l	C9C9)&'#FKEyPT90)<]7Tp)&'NQz
] 7)&F$'N9FqC90)&MCK$]		U:q
$ ~T90#'7$,$!E#,C9	)&$$'Nd;9#K$'Nth'#K_h$T%W$MEb$ME#,'	$=0Q79
T9$M)&9;#$'NqU VWE#@] 	)<_h$'N9F,C0)&MC9K$>]7K'N))<MC9'N#T9b$'X_7$M)&'#T4;#K#D|N$!)y] 	)&	#@	#T
T90)&]#_#DdK'N	]7;9#K$'N@'he$ME#>L] 	)&	#PFW	y;9C9C90)H9'N;9#Ty'NO$ME#S#DNF$]	L'	DN7)&$ME9Q
'FcU"VDNE7$0##D'F$ME#W9'N;9#T@0FT9W$>'lPQd_#QS=0F$>'#C9)&'N#0Qw2'N)s%,E#KEO$!F$'N9	)&$&SK'N7J
T9$'N#l7)&l'	$MEX#KM	)<	#TXM;7}tK0	$H2'N)xD	'N9FffQS#Ql;9QUP[W$H$ME#c$MF$'N97)&C9'	7$L$ME#
9'#;9#TRF$$Mh_#TUyVWE#K'N7]	y.;9#K$>'#|QS0	$>'##Tm	'F]	SK'NQC9;#$MF$>'#9Fi_7$M)hK$M	#	U
2$LP	C9C9)<' #QF$TX	#TX$ME#P>$MF$'N9	)<$&@K'N#T9$'N#L'h"$ME#P	C9C9)&'FNQh$L;9#K$'NX#T9c$ME#
QS0	7J 4T ;9F$>'##0UR-s!*0 	 	C9C9)&'NFKEnC9)&'F]NT9ty#$0Qh$Kt%Wp'F,$!)FK$M	#R7C9C9)&'FNjJ
QF$>_#DX$ME#S;9#K$'N $'T9)&T 'N)&T90)&S'FLFKK0;9)FKn$ME9)&'N;#D#ERV/N'N)c0)&0
U p@ME#'F% $ME9F$
$ME#S	C9C9)&'FNQh$'NO'N#$MF#TX	A9	;# $,F2U 	 ,7C9C9)&'NFKE@H$ME#LM	QSPh,$ME9h$('he$ME#
 a)&$W'#)&T90)
	C9C9)&'F#QF$'N_X-s!*0 	 P	C9C9)&'NhKEqUHVWE#P#$0#'Ny'F(-s!*0 	 P	C9C9)&'NhKEOP#'	$HT9)&K$0
U p
C9)&'NC'	@X#% hK$] F$'N7J#T90C0#T90#$tKE#0QS@9FTn'NnVg#'#)P>0)&dQd$ME#'NT9S$'XFKE#]7
$ME#0U"I@A#K$'NOSC0)&QS07$W'NbMQF#$&%'N)*7"%0)&HK'N#T4;#K$TUVWE#HC0)&QS0	$>,ME#'F%
$ME9F$$ME#c'N#$MF#TX	C9C9)&'FNQh$'N#,	)&L9;#$lh$$M)FK$>]7	U

g   " !#%$

j

 
&#%'e

 $ME#yK$'N %>$M	#MEBo.)7QS%'N)*n %,E#KEB'	$ME X	#TB(x@K0	 9X$!;#T9TU
 #T9T|9'7$MEX"XP	#Tn(xLK0	p0XFLT9j0)&07$P$&CL'F\{X01q$ME#ST9j 0)<0#K_#Dy
$ME#LK'N9#K$>]#$&	#T@$ME#L0#0)<D	O.;9#K$>'#;#T@@K'#QC9;#$#Dt$!E#PC9)&'N9	#$k	U
Ip$!E#C9	C0)c%(b%WW)&>$M)&K$P'N;9)<]	S$'#9	)&i] F;#T~;9#$0(
U p$ME#;#ST9 a#X\{
$'@9cK'N#$#D@'F()4NTp#;9QS90)x'F"$'#KE9h$Kl;9#$0+
U *FKEy;9#$PE9Fu7F>'NKF$TX#9	)&
)	#T9'#Q]F	)&	#
 ,.-/ r 023 1t+Z'N
) /N3 0
3L23 1 Gfi
U *"FKE@>$MF$5
 7
4, 68/9, # 0:, ( 0
;<;
;0,>=1l'hsO\L E9FL	
F>'NKF$7
T ff>0#0)&D	b.;9#K$'N? X19%,E#KE@L;#T$'ST9 a#c$ME#l'	$=0Q79OYL>$M)&9;#$'N> @

A+"4, GB6DC2EF  
H G JILK
& +

+&30G

VWE#@T90#'NQd_9h$'N)NMff1/K0FT~$ME#@C9	)&$$'Ni;9#K$'Nq1$ME##'N)QbF=0h$'Ni.hK$'N)LT9a#T 	

56PO H G C EF H G JILK ;
& +

Q
R

fiy

\ X"E90]7;9#$K0FT7ffME#T9T90?419	#T ff>]N# t;9#$0U"VxE#W$MF$>(]7K$'N)i
{
4, K0	9H$ME#'N;#D#E	$
4
4
41;
'Feh,9#DOT9]#T9TX_7$'S$ME#PC9F)'h  +2E#T9T90;9#$MGW7#T  +f]N#l;9#$MG19'N) 4, 6 /  4 0 N
YP;9)&#Dn'NC90)h$'N~C9E9F>	1]N#p;9#$>	)&@K	QCT1W$ME9F$S 4 6 	 4 1'N) 4, 6 /N4 0 	 4 1?;"VWE#
T9$M)&9;#$'NXF>'NKF$>T@%W$ME@$ME#PE#T9T90X;9#$xf'N)$ME#LK	QC9TXC9E9Fc

A+ ff4 
 	 4 Gfi6 C E  F fi+ H G	 4 G I K
H G I K L;#TO2'N)Nff 720))&#D O$ME#L] F;#H'F"E#T9T90X;9#$0U
%,E#0)& fi0+ 	 4 GB6 O G  G 
E
F
C

G
BE#0 HM;9#$>$!;#$T%W$ME
= =   =  


@+ g4, G 6 W
3 r;    , ,  3  ,
& +

+2	G

& +



Q# Q#



 
# 

+	G

	#TF'L$ME#W)&>$M)&K$'N  6  0FffQC9'7T'Nc$ME#%DNE	$>g$ME#%4*N#'F%,S"'##$MF#TU
Ic$ME#(C9	C0)/%(%Wa )<$M)&K $ff'N;9)&>]7"$'LHMC9KF KFff'F(xff%W$MES$!E#"2'	'F%W#DP0#0)&D	
;9#K$'N
=




4, B683

@+ ffG

  "



 

#

/9,





_e+!

G" +<3

3 ,



G9+&3 #
3 ff+$



G>G1

+ZNG

%,E#0)&% 6 O &E  # #
,
0	#T%'HPS;9#K$'Ny)&'NQO!;9#$)( 'F"$ME#l)<0Fg#T90#'	$>T
7+
 *L1$'S$ME#P7$0)&] h (+2r030G1	 2U 	U 1

@

 ,(.-/*/0

+.r0030G

;

p y%WPhM;9QSy$ME9F$1'	 	9FN$>K.;9#K$>'#qU VxE#0)&F'n$ME#)<$M)&K$'N 'N|%DNE7$
; A#DNQS'	TB	#T #'7>	J'N)O#$&%'N)*7t	)&y$k%'p!;#KE MC9KFP#$&%'N)*7+2x0F21
 6 r91j325476 
6 33
)&MC9K$>]7	UWI@DNQd'	T
 305	57	G1#%W$MEhK$] F$'N.;9#K$>'##98"+!:4GB6 <;>=&?A@ 	#TB +!:4GBw
#$&%'N)*7E:K0	b9,	7)&0F4#;9QS90)17%,E#L#'7>	J'N):W)&$!)&KC $DE C T$'Pu#'N7Jk#D#F$]		8#$ME#
#

#

K'N#$!)F	$HL07f'#)&KT7@2'N)&K#DO$ME#P%DNE7$u7#To#F>L$'t9#'N7Jk#D#F$]		UFa)&'#QE#0)&S'N
%,E#0#]	0)x%P)&!20)$'WH$W%'N;#T9L%x$!E)&!20)&0#KP$>'+fNGU
F 'N)x_720)&0#KO$ME#SK0);#KF"$!E#_#D7L$'9]FF;9F$TR7)&
 EfiH	#T cUlVWE# a)&>$L$>0Q )&;#)&
M;9QQd_#DS  $>0)QSff%,E#L$ME#W#N$$0)Q)&9;#)&M;9QQS#DS = $0)>QS,+Z%,E#0)&H~$ME#,#;9Ql0)
'FgE#T9T90;9#$W	#T@ $ME#,$>'	$MF4#;9QS90)'Fg;9#$MGU"VxE#,9C9)&'N#2'N)"C97)&$$'Nd.;9#K$'N#
	)&SQS	)cp9F$M;9)<d'	$MEXX$!E#lK	QbC9T|	#Tp$ME#;9#K	QCTmC9E9F>	UVWE#'N#XT9j 0)<0#K
L$ME9F$H$!E#lK	QbC9TpC9E9FS$ME#SM;9QQbF$'N@L'F]	0)HE#T9T90p;9#$01q%,E#X$ME#S;9#K_7QC9T
C9E9FH$W' ]70)"Fq$ME#L;9#$>0U"VWE#;#$ME9)&'N;#DNE#'#;#$($!E#u)&0QbF#_#DOC9	C90)%,%x$!F*S'N#t	'N;#$
] h_;9h$#D ;
[H,QS07$'N#TX	9'F]	cK'NQC9;#$#%
D  )&9;#)&P	O9C9'N#07$F/M;9QQh$'N@'NC0)F$'NqUVWE#;#
9FK$SK0FK0;#_h$'N 'F  b_7$M)hK$M	#	URA9	QbC##D|9FT~QS$!E#'NT9'h 0)SXC9'7#%Wp'N;#$0U
;#$@$ME#pQS$ME#'#T9	)&yK'NQC9;#$Mh$'N9F ]70)&|C0#]		Uw[u#'	$!E#0)@F$0)9F$]	y$'n'N'N*
2'N)OT9$0)QS#$K~	C9C9)<'NFKE#01L9FTB'N QS077J 4T^$ME#'N)< hFT9]	'#K0F$TB	 +2-/$>0)&'N ?
[L#T90)&'Nq1/305	6 8A97;#$uhfU 1305	5 G8L7C9C90XI
? H'#T4)&DN;#=	1ff30575	6	GU"VxE#PQS$ME#'#T9uC9)&'#C9'	TX	
+2-/$0)<'NP? [u#T90)&>'Nq1430576 98	L	C9C0tJ
? H'#T4)&DN;#=	1q305	5	67Gg	)<7C9C#K0	#L$'PX01F%,E#LA9	;#
$LF2U +&305	K5 G	GT9]	'NCTR>KE#0QSS%,E#KEXE9Fu0p	C9C#Tp$'@9'	$!EWP7#TX0U,VWE###$
K$'N%Wg9cT9]	'	$T@$'-s!*0 	 ,QS$ME#'#TB
U pc	$0#T@$>'$M;#T9O$014;9#T90)&$!	#T@$,)&F$'N#
%W$MEN$#DO$ME#'N)&,7#T	C9C#@$W$'tx0U

QML

fi

y)

ff 

!  $  	"ff

] 





a{yy

 
&#%'

HK07$ L	C9C0 	#T H'#T4)&DN;#=N+&35	5	6	GS7$M)&'#T4;#KT1W$'i$ME##0;9)h,#$&%('#)*pK'#QQS;9#$&	1

C9'F%0)I;#u7C9C9)&'FNQF$>'# Qd$ME#'NT T4;#p$'|-ff!*09U I $ME#@K$'N %%xPC9)&>0	$-s!*0	 
QS$ME#'#TppOT9j0)&07$L%	1q'@$!E9F$L$LK0	p9bN$0#T9Ti$'W0U p%xF'@T90QS'N#$!)F$	1
F$0)$ME#,K$>'#@$ME9F$W-s!*0 	 uQd$ME#'NT@LQS'N)&LD70#0)Fq$ME9	@A#	@	C9C9)&'#FKEqU


 fi ff)qWxGn

YLa#lb#%B0#0)&D	b.;9#K$>'#

 +  4 0!G 6"$#&% 3
@

=



 

#

 ,

+2	G

2+ A##KP$!E#LT90C90#T90#KS'Fg]F	)&'N;#x.;9#K$'N#x'N 4, 1'%B	#T 4 x;#$L'N7]#'#;#019@%,E9h$"2'	'F%W
%@%W,#'	$tQS07$'Nn$ME#>Fc.;9#K$'N9hu	)<DN;9QS07$0U i @T9'$!E#O'X$ME9F$S$ME#@T90C0#T90#K
'Ny'	$ME#0)H]F	)&	#P!;#KEph(
 R7#T  4 >$M	#T9L'N;#$PQS'#)&SK07)&	U GRVxE#K'N))&!C9'N#T9#DC97)&$$'N
;9#K$'N	#TC9)<'N9	#$&yT9>$M)&9;#$'NX	)& @

. 6



HG C E F

) * 6 C E  F






+$G	G
+	G

 L$M)<0F$  F#$0)9F]F	)&	#0UVWE#,E#C#P;#W@#$0#T9#Dt$!E#P	C9C9)&'NFKEO$'ST90)&]	SQS0	7J
p
4T~$ME#'#)&2'N)S W0U vL ;9)	C9C9)&'NhKEnT9j0)&S)&'NQ $ME9F$b'Fd+2L	C9C0~? H'#T4)&DN;#=	1c305	5	6	G1
%,E#KE@T90	$> 4   %W$ME 19$ME#P#FH] 7)&	#L_ym1#c;9h$'Np+	GU
VxE#dQd'	$] F$>'#@2'N)H9 )&#D	#DX_+R7#T  4 7#T T9 a # #DX$ME#	' ]7L;9#K$'N#PlhW2'	' %x0U
ZP$!E#@0#0)&D	i.;9#K$'N t'FL$ME#@#0	)b2'N)Q@1 O  , 1"$ME#0 $ME#@K'N))<MC9'N#T9#D~C9)&'#9	#$&
T9$M)&9;#$'N OFK$'N)&F21P+Z'huK'N;9)<'N#@K07~;#X'	$ME#0)l.;9#K$'N#7#T~$H$ME#C9)&'#9	#$&
T9$M)&9;#$'NiK0	X9d.FK$>'N)&FG1q	#TpK'NQC9;#$!F$'N#LK0	X9bT9'N#S$M)FK$M7#	UcVWE#tC9	)7QS$0),
K0	c9$ME#'N;#DNE7$/'FaFff,E#'NQd'	$'NC7uC97)	QS$0) $ME9F$/MQS'#'	$ME#P9)&#D	ffc$ME#$M);#"0#0)<D	H.;9#K$>'#
	$>'C#K$!;9)&F(
 iL#K0)&0hTy.)&'#Q r9ULV 'tD7$,$ME#S'N)<D7_9hg0#0)&D7@;9#K$'N.)&'#Q   %S#T
$'>$

 6B,r -+2 -/N3 0! 0/.0.1.321 04 63
+26	G
F4'#)K'N7]	0#0	$,Qb	#C9;#_h$'Nq1q$Wu;#!;#$'bT9 a#P$ME#H;9#K$'Nq1

4 0 6&3@  

(+  !G

%,E#0)&

"  


98
  
5 676, : ; 6=<  
<

 

 5

+25	G

+&30r	G

A##K 5 4 1	$ME#HQS0	S]	K$>'N)1F(C9E7N>K0h@QS'N)&WQS07#_#Dh.;# $ME9	  4 1	$	C9C9)&'NC9)<_h$W$'LK'N#>T90)
$,F)&P	#T@$M)&0h$  4 FH9#DtT90C0#T90	$H'N 5 4 UeV 'SFK$!F$LM;#KE@S$M)&0h$MQS07$"$ME#H2'	' %x_#D
F!;9QC#$'NOuQbFT9	U

Q1>

fiy



rs]ypxou;o o)x3}

4 U





/
	Qxo Qsj



F4'N)S0hKE 4NT

54

7#T ff1u+<30r	GcK0	~p;9#9;#~'7]7T~2'N)

: $  6fiff + 5 4 0 GcT90#'	$>$ME#X	]70)&'h+&30r	GU B
 E#0 $ME#7]	0)&y;9#K$'N @;#T1(
$M)7#&f'#)QS$'c;9#K$'N@'F 5 4 7#T B@

40 G G G
+&3	30G
VWE#;#
 
 + 5 4 !0 G 6&3_   " K  5
+&30	G

%,E#0)&  ,K'N#>T90)<T$'tlc;9#K$'N'F 5 4 1j	#T ffUex'	$L$!E9F$


<   6  < ( < ff   6   3 <    " 5  "    < 5   < ff   6 
+&3
	G
5<
5




 <  < 5


<
< <
VWE#p)&F$'N#!E#_C 9$&%0  4 	#T 5 4 1,Qd'N)&C9)<K> 9;9F$'N#R+&3r	G1L+&3
	G1W7#T~$ME#X	]70)&$&J
#$&|FM;9QC#$>'#q1P%(*##'F%,X	#TpPK0FT|$ME#:qD	0#T4)<V/)	#&2'N)QbF$'N|+$H'#K*20)1
305 7 	GUL07$M)F$>'@$ME#L$M)	#&2'N)QbF$'N@L$ME#I	]	0)<$#$&RF!;9QC#$'NqUPVWE#S]FFT9$&p'F$ME#
F!;9QC#$'N@F$ %63LWQC'N)&$M7	$2'N)$!E#L$KE9#9;#,T9K0;#>TX_$ME#PC9	C0)U
 In$ME#79'F]	@%E9]	QFT9;#@'hu$ME#.hK$l$!E9F$   GG O$ME#@	]70)&'h  GG U~W0#Ky$ME#
 B
6 r,- 2>14)&9;#)&0QS07$uQS07$'N#Ty_n+26	G$M)7#F$W$' 

< 6  B
6 r -+2
+&3NG
5<
0	)&	1 %,E#0O$ME#HK'N#$M)h_7$WH07f'N)<KT	#T yHK0	))<T@$>'3	1#%LD	$  6&
3 _c U
VxE#p	]	0)<'N 'F+&30r7G7$M)FK$!	#p2'N) 6 r91H%,E#KE Q	*Fy$yQC9'	>#|$>'|%,)&$
40 6

 + 5 !G J(@+  !GM

$   &   *

+

	nFD	09)FKOC9)<'Ni2'N
) ff|	#T|E#0#K\UffV 'K)&K0;9Qc]	07$S$ME#tC9)&'N#0Q@1"7p	C9C9)&'F#_QbF$
T9K0)&C#$'NO'he\B,9;#$,7t;##D$ME#xFK$"$ME9h$"$ME#H	]	0)<'NO'F(+&30r	Gs(9;#$L$M)>FDNE	$<f'N)<%7)&T
F$ 6Br914	#T@9C9	#T9#D  + 5 4 0!G"	)<'N;9#T 6 r;##DV/N'N)xA#0)&0U
[x$ %6Br91

)  * 6  3C "

	#T@9;9F$'Nn+&30r	G9K'#QS

JH

 

C

+&30	G





C3E"  

C
VWE#L7]	0)&'N'Fe$!E#H9;9F$'NOK0	y9PK07))&T@'N;#$W9C#K$X$''##$MF  6




 + 5 4 !0 r	G 6    5  5 "B+&3 3 5 G9+&33 5 G
5 6


#

 

E

	#T



[H>'419$ME#LT9$M)<_9;#$>'#n+&30	GK0	y9L9C9)&TXF,c;9#K$'N@'h


)  * 6  5 H  +&33 5

Q!



G
#

E H

5 4 19OFK$'N)&F

2'N)Q
+&3MG	G

fi

y)

a{yy

VWE#L$M)>;9#K0F$TV/N'N)0)<L	C9C9)&'F#QF$'NO'F  

 + 5 !G


40 6






40



  < <

 + 5 Mr	G>" 
 Q#





+&3
	G



 * *

 Lc;9#K$'Ny'F 5 4 	#T ff147#T@K0	P]#%TpF,7@X$MEO'N)&T90)x	C9C9)&'FNQh$'N@'F"\U :q$
;##'F% K'##T90)S$ME#@)&;#)&0Qd0	 $OQS0	$>'##T|_B+.6	GUXA#$$#D .6 3t>$M)FDNE7$&2'N)&%W	)&TX$'
QC#0QS07$0U(VxE#l)&9;#)&0QS07$01  6Br1	2'N)Wh f1K0	l	C9C9)&'F#QF$O07f'N)<KT;##D+&3NGF


 6 <   <   6Br -+2
<5
<5

+&306	G

VWE#H9;9F$'N#(7)&,;#TO$'P>$(;9Cb$ME#4#T@C9'	7$"9;9F$'N#0U"x0#K!2'N)&$MEb%W%W )<!f0)s$ME#0Q
FW$!E#PQS0	7J 4TX;9F$>'##0U
VxE#W20F#$&y'Fg$ME#HKE#0QSLHOK'NQC9;#$#D$!E#PC9	)&$FqT90)&] h$]	W'F"\^%W$MEO)&MC9K$
$' pF(
$ 6rU,y$ME#PC9	C0)H%P%W")&$M)<K$H'N;9)&>]7L$'
6 ;aVxE#l)&] 7	$L9C9)&'N#
+2-s!*01305	6	8E9F$>$MFKE9	)&#NS?H0)&$ME#21g35	5	5	9G	)&

<
<

(





8
<   7
6
6  :
	
<  * *
= M6 +$ 3 6   8 G+,  3 5  G 8 
:( 	
8 8
ff
6&3 6M+!P3 6  G ( :
ff 	 " 
+&33 5 G
5
 #



 * *

(

+&305	G
+2	r	G

*ff9C9)&>'##Sf'#)S'	$ME#0)cT90)&] h$]	tK07|F'R9@T90)<]7T~~y$M)>FDNE	$<f'N)<%7)&Tp%W	U F4'#)l
$ME#cT90)&] F$>]7u	)<L$M)FK$M7#	U F4'N)WxWK'NQC9;#$>_#DO$ME#c$0)QSxHC9)&'N#0QF$K	1q	#T>''N#
/f'#)&KTS$',Q	*h";9)&$ME#0)/	C9C9)&'F#QF$'N#0U/W%~7C9C9)&'FNQF$>'#P>KE#0QS/$ME9F$/%(E9]	T9]#>T
%WePT9K0;#>TF$0)U
V 'W]FF;9F$"$!E#"9;9		$>$>ff)&9;#)&TL2'N)q0	)##DL'N#E9F$'xK'NQC9;#$  
 fi Uff:q$/;#gK'N#>T90)

$ME#H;9#K$'N
e+ 4 fi
G 6&3  + 54  003
G 6 
 

%,E#0)& 5 4  ,#'S'N#D	0)W{)&,]7K$'N)199;#$$W(K'##T90)&T$'PL;9#K$'NO'F 4 D	]	0O	p+&3NGU
x'	$L$ME9F$

  + 5 4  030G   5 

 

<

x#Do+&3NG%(LD7$

<





6 <
<





6&3 <

<





"

<   &
6 3 <  + 5 4  030G
< 
< 



< <
< 5  <   

V E#X)&DNE	$OE9	#T~T9yO	$!)FK$M	#	1W9;#$OK0	|9X	C9C9)&'F#_QbF$T ;#>_#D 
W
F$
T90)&] h$]	u)<;#)&Ty2'N)07)##D@K0	O$ME#;#,9cK'NQC9;#$T7@$ME#P	C9C9)&'F#QF$'N>@

%,E#0)&

5 4

<   &
6 3 <  + 5 4  030G  3 <  +  5 4 0030G
< 
< 
< 

,S>'	;#$'N@'Fff$ME#PQS077J4TX9;9F$'N#01/+&306	GU

Q

6

3	U VWE#

fiy






]q



]qyq

  q q]s ]p&Qq]s

 	 	;p/qz{nq]s

3	
fiff



 qWxGn 

A9	;#$HF2U +&305	5KG	GC#'##0)&T$ME#S	C9C#K0F$'Ny'F"QS0	7J4T$ME#'#)&t$>'tW0U@$!E#HM;9#K$>'#
%@%W,ME#'F% $ME9F$b$ME#)	C9C9)&'NhKEnK0	|0 F(a)&$b'N)&T90)	C9C9)<' #QF$'Nn'h{-s!*0	 
QS$ME#'#TU"V '$M7#ME$ME#,%PC9)&>0	$,b#%B] 	)<_h$'N9F/T90)&] F$'N'F"-ff!.*	 ,Qd$ME#'NTU



]q

		

fi|qyq

  )qeq]s |p&

 		;p0qz{n

q]s3

fiff



i$ME#SK$'Nn%t)&]#%A9	;#$lF2U	 c%'N)* UVWE#RFT9'#C#$l@]F	)&F$'N9h7C9C9)&'NFKE>@L2'N)Q 
;9#K$'N+Z%W$ME{N$!)W]F	)&	#MGs%,E#KES"x' %0)/9'N;9#Tdf'N)   	#TSKE#'#'	$ME#] 	)<_7#'LF
$'PQhNQS=H$ME#;9#K$'NqUff[L	C9C9)&'F#_QbF$T9$M)&9;#$'Nq19+ 4, 0 5 4 G1F"KE#'	0S%x$!EPC9	)>	QS$0)
]	K$'#) 5 4 URVWE#C97)	QS$0) 5 4 1"$M;9#T|>'p$ME9F
$ 9+ 4, 0 5 4 GctFbK'7FC9'	>#X$' ) + ff4, GUp[H
Qd0FM;9)&'hK'	>0#t9$&%0X$&%'@T9$M)&9;#$'N#tL;#9FK*R: _#0)bT9$M	#KOd;#>TU2$c
T9 a#Tp	
9+ 4, 0 5 4 G
X
+ ?0 ) 
G 6  9+ 4, 0 5 4 G9
+2930G
) + ff4, G
VWE#

54

HG

WKE#'	0@>'$ME9F$X+?0 ) GsLQS#QS=TU9+

4, 0 5 4 GB6

9+


BE#0 )

=



4, 0 5 4 GWKE#'70@$'LFK$'N)&F2192U 	U

H5  +&3 3 5  G E H



#

+2		G

#

W$!E#l'	$=0Q	9OT9$!)&9;#$'Ny$ME#PL;#_9hK*y:q#0){T9$M	#Kc$M	*F$ME#H2'N)Q
+


%,E#0)&

8
6
@+ 4 G & +
G
%
3

+2 	 G

F+ 5 4 G



=









3 5 G9+&33 5 GG
+2FNG
5

x#Dt$ME#HFK$W$!E9F$ + ?0 ) G rS%LD	$,c' %0)W'N;9#T'#@ @
  P3
+2		G
LQS#QS=T~%W$MEp)<MC9K$S$' 5 4 $'yD	$P	n;9C9C90)S9'#;9#TR'N 3@ c
 U@VWE#t]7K$'N) 5 4 






+ 5 4 GB6

0 ) G 6 ff"







" 

#

+ 5 

"B+&3

!

"!

#

#

T9$0)Qd_#Tp7t>'	]N#DO$ME#Hf'7'F%W#DO$W'Fff;9h$'N#0U

<

6

 Br



<5

+2G	G

VxE#'N;#DNEO$ME#L	C9C9)&'FNQh$'NO,%g*##' %HO_y$!F$$K0FC9E7NKH$>0)F$M;9)&@+2-ff	)&2135	6	6	G
F L$ME#O9F]	Qd0	7J4TR$ME#'#)&	1gA9	;#$PF2U	 S	C9C#K0h$'Np'h($ME#S$ME#'N)&y$'Wcd#% 	#T
	$>0)&$#D41"E#0#KO%t%xW)&!20)L$ME#@	' ]7t	C9C9)&'NhKEphP$ME#@A#	p	C9C9)&8 'NhKEqU@[ '#'N*pF$@+2FNG
ME#'F%W$ME9F$$ME#W	C9C9)&'NFKEOE9FK'NQC9;#$MF$>'#9F920F#8$&@'N#O%,E#0+
 6
K0	tLC9)<T@F
c;9#K$'NO'F 5 4 U F 'N)W01$ME#LK0FK0;#F$'N@'h 6 @+ /4, G x_7$M)hK$M	#	14]70O%,E#0$
 c.FK$>'N)&F2U
A9	;#$LF2UM;#D	D	$9C#'	$#DFK$]FF$'N7JT90C90#T907$LK'N7]	#$&C9)&'#C90)&$H$'T9]	'NC.;9)<$ME#0)
	C9C9)&'F#QF$'N#@$
'   U VWE#;#@'N#pE9F@$'n$MF'N)$ME#pA#	 	C9C9)<'NFKEn2'N)O] 	)<'#;#FK$] F$>'#

Q

&%

fi

y)

a{yy

;9#K$'N#0Uc[up7C9C9)&'FNQF$>'#q1    :!: 1 LT9]	'NC9T|*F0C##D$&%'t$!E#_#D7L_iQS#T @t+&30Gx$L
K'	c0#'N;#DNEO$' #84	#TR+.	G$Wud$M)FKC $M7#H;9#K$'N'F 5 4 UffVWE#0R+2KG	GL	C9C9)&'F#QF$T7

<

#

*s]	0S%,E#0 5 4

:!:

  

<5

6

+2 	 G

C Br

 KE#'	>0$'SQS#QS= 1	$!E#,9;9F$kb_p+.		G/,F$$!F#Tjs	#TO'N#j ) 
FK$'N)&F2UffVWE#ff2'	'F%Wff)&'NQ^$ME#FK$ff$ME9F$X+?0 ) G6BrHje7#Tl'##Sj ) 6 #1#	#TS$ME#FK$ff$ME9F$
KE#'	>0S$'lWFK$'#)&F2UeVxEN;#A#		C9C9)&'NFKES9FT9;9h$W2'N)/'N#$!F#_#D	b	)#$M)	)<K'	>
	C9C9)&'F#QF$'Nb$'P
 P172'N)"cD	0#0)>F4T9$M)<_9;#$>'#q1 ) UffvL#L%WS'F' ]70)&K'NQS#DS$ME#T4)%,9FK*41
L$'O$M)&0F$L!QFe#;9QS90)H'F] 	)<_7#chK$	#TX	C9C9)&'FNQh$S$ME#l)&>$08q+ZN	7*N*F'7_O?
	'N)<T4	q1g35	5	598qA9	;#ff?	'N)<T4	q1ff305	5 G7GU(Iy$ME#PC9	C90)H%P>$M;#T9-ff!*0 	 P7C9C9)&'NFKE@%,E#KEX;#
V/0#'N)/0)&$'LD	]	,H#$0QbF$K%c'F 9;#T9#Dt	S	)#$M)7)&SK'	H	C9C9)&'FNQh$'NS$'L
 cU
b$ME#HM;9#K$'Nb%,%x)&T90)&]	L-ff!*0 	 Qd$ME#'NTb)&'NQ {] 	)<_h$'N9FC90)&!C9K$]		147#TtME#'F%
$ME9F$WA#	7C9C9)&'NFKELSMCK_hgK0F>L'Fs-s!.* 	 L	C9C9)&'NFKEqU
		

 q$WxGn

	SqpGoqyxo

 Cyp&	;yzGxo |

s|q

y$ME#LK$>'#@%l)&T90)<]7d-s!.* 	 PQS$ME#'#Ty.)<'NQb] 	)&F$>'#9FeC0)&MCK$]		UPVWE#c_#9;9F$&
N' 3@T90)&]	TXy$ME#lC9)<'NK1a	#Ty$ME#S$M	#ME9QS07$L'FffK'N7]	N$&@'F"$ME#S	C9C9)&'F#_QbF$'N
;9#K$'N%x$!Et)&!C9K$W$'c$ME#W]F	)&F$'N9Fq]F	)&	#,	)&H#% K'#	$M)<_9;#$>'##,QFT9WO$ME#WC9	C0)U
Fa;9)<$ME#0)QS'N)<	10%"F'WT90Qd'N#$M)h$ff$ME9F$$!E#"A#	P	C9C9)&'NhKEHffMC9KF9K0hff'F4-ff!.* 	 ffQS$ME#'#TU
[HWA#K$>'# 9U c$ 9S)&0F/C9	)	QS$>0)"$!E9F$W$M	*hW] F;#x.)<'NQ rS$'3	U:q$,;#HT9 a#
$ yT90C90#T907$PC9	)&$$'N@	#TyT9$M)&9;#$'N;9#K$'Nq1

*
CGH E F H G JILK
* H G JILK
) * 6DC E F *

* 6



+2	6	G

& +

& +

x'	$L$ME9F$
#

6P^	#T ) 6 )
#

UeI	$!)&'NT4;#K#D	O#$0)9h )<0F]	K$'#)

* 6
%,E#0)&

+2	5	G



HG

*ff
fi ; O


CE

JH

  

^ HD	]	0@7o+$G7GU W#DO	0#0>	 L#9;9F$&	1
 * 6    ) * C E O    H 

CE

O  H   
8

6 C EC C E  C 19%LD	$
  O    

+	r	G

!

!

CE

HG
V/	*	#DO'	DN	)&$ME9QdW'N@9'7$MEOT9,'hx+30G1#%,'N#$!F
 
 3   5
'	D * |'	D .
!

 4 19$u;#,)<%,)&$t+2	67G(h

+930G
+		G

x'	$L$ME9F$x$ME#l)&DNE7$,E9	#TyT9c.;9#K$'Nu#'	$!E#_#D@9;#$N3 (+  4 0!GFWT9a#TXR+.5	GU".$,LF'
%'N)&$MEy#'	$#D@$ME9F$L$!E#dQd'	$] F$>'#@2'N)HT9a##DX$ME# ( ;9#K$'NXK'NQSPQS;#KEXQS'N)&S9F$M;9)F
E#0)&	U

Q

fiy

4

f g$ME#L7]	0)&$#$&XFM;9QC#$>'#@E#'	T9W$ME#0O%LK0	O;#
T90C0#T90	$H'N 5 4 G	#T)&%H)&$t+		GF

3@  *

54

F$ME#L#T90C0#T90	$L]	K$>'N)L+Z%W$ME

40

+ 	 G

 + 5 !G

%,E#0)&  PFHT9a#Tn_|+&3730GULVWE#LD	]	PO] 7)&F$'N9F fff$>'-s!*0	 PQS$ME#'#T @H$M)&0F$ 5 4 F
#$0)9F]F	)&	#S]	K$'N)x	#TKE#'N'7L$,$'OQS#_Qd=  U2$WK0	y9{ .;9)&$!E#0),C9)&'F]	T@$!E9F$  L
K'N7]	@;9#K$'Ny 5 4 ULV '@S$ME#LT9 a#$ME#SW>_7X'F  F  6    ;qYLj0)&07$F$'N
 
'F,+&30r	G%W$ME@)<MC9K$H$' 5 4 1N#T9
6cv
+ FNG
%,E#0)&

 8
98
8
v  6 6 , ,  3 6 ,
6, 
+ 		G

#DbK' ]F	)&	#KtQbF$M)&q
1 	 PC9'	$]	b0QST9 a#$	
U *ff9;9F$'N|+ F#GW0#M;9)<L$ME9F$L9'	$Mff
E 	
	#T  	)&P#'#7J#DN;#	)UW0#K  ,C'	$]	ST9 a#$PQC###D@$ME9F$  HK'N7]	qU
[HST9K0;#T _ A#K$'N7
 9U 1g$!E#@T9 }OK0;#$&|n	]70)&$#D +&30r7GH2'N
) 6  rX)&0QbF#t	#Tn
'	]	Tp7@$ME#lV/N'N)x0)&P	C9C9)<'NFKEFHT9K0;#Tpy$ME9F$HK$'NqULf"%(S;#S$ME#+
 a)<$,'#)&T90)
	C9C9)&'F#QF$'NO$ME#PA#7	C9C9)&'NFKEbH'N#$Mh_#T
U F4'#)
6371N%PE9]	
  # + 5 !G

40 6

%,E#0)&

40 6

 + 5 Mr	G

.hK$ fi
#



=



+ 5 

Q#



5

'F]	0)&$Qh$$ME#  ;9#K$'N

3@'	D  *

8
6
 
: ff 	
%

40

 + 5 !r	G "
"^+&3

40

 + 5 G

3 5



G_+<33

+G	G

5



;

GG

40 ;

+ 	 G

 fi # + 5 !G

V @
' $ME#+.F'@+2E9F$$MhKE9	)<NNO?H0)&$!E#f1305	5	57	GGH#'	$S$ME9F$H;9h$'N~+ 7GWK0	iF'
9c	$0)>C9)&$T$0)QS'Fff$ME#LT9]	0)&D	0#K$k%0O$ME#c$&%('cT9$M)&9;#$'N#01 ) * 7#T ) * 1 $ME9h$(
X+ ) *


[H>'#'	$>_#Db$ME9F$

0 ) * GB6


GH

) *

)  * 6 _ *
)*



"  +5

40


)  *  )) * * 6   * " fi # + 5 4 0!G
HG

X+ )  * 0 )  * GB6  )  * _ ) * 6  fi # + 5 4 0!G 3  + 5 4 0

)*
HG
X+ ) 


*

0 ) * GB6

+	6	G
G

+	5	G





G

+ZNr	G

	#T;##DO$ME#LFK$$ME9F$x$ME#LT9]	0)&D	0#K LF%#,#'N7Jk#DNh$]	t+ 	GH'N#$MF#TU
[x$ %63
  # + 5 4 0030G 6 + 5 4 G
+Z430G
%,E#0)& #W$ME#H'N#&K$]	H;9#K$'NO'N#$MF#T7A#7t	C9C9)&'NhKEq8#t+2hNGUgx'	$H$ME9F$WF$ %63	1
+  7G">$M	#ME#L$ME#P#9;9F$k|+2		G@$!E#lA#		C9C9)<'NFKEqU"2$H,$ME#;#WK0	)&y$M	#ME#TX$ME9F$

Q
Q

fi

y)

a{yy

A#		C9C9)<'NFKEOL a)&>$,'N)<T90)W	C9C9)&'F#_QbF$'N@$>'t-ff!*0	 L	C9C9)&'#FKEqU#KT907$MFp(7)90)H	#T
] 7@T9P:	7)P+&305	575	G"F'@)&T90)&]	Ty$ME#PA#	@	C9C9)&'NhKE@7;#>_#D@SK0;9QS;#	7$,9C9	#'NqU
x#D+ 	6	G14+ 	5	G1q+ZNr	G1		th$>0)9F$W72'N)Qh$'ND7'NQS$M)&KHT90)&] F$>'#'Fff-ff!.* 	 WQS$ME#'#T
K0	c9K'N#$M)>;#K$TUffVWE#] 	)&F$>'#9FNT90)<]FF$'NSC9)&07$TE#0)&WF'LE#C#ff_d$M	#ME##DP9*	
%W$MEy'	$ME#0)L)< a#0QS0	$>L*F@V [u-B7#T#0	)c)&MC9'##K'N))<K$'NqU F 'N)HOT9$MFTpT9K0;#'N
'Fff$ME#,C9'7_7$L)&!20)$'+2E9F$>$MFKE9	)&#NP?H0)&$!E#f1q	r	r7r91q305	5	5	9GU


  
 Wq]s' fi

 		;p0o qyxo Qs$

]pv+

 !*0	 Qd$ME#'NT T9K0;#T
  $ME#@K$'Nq1WQS077J4T $ME#'#)&p2'N)xt @T9]	'NCTB;##Dm-s

#
|$ME#XC9)&]#'#;#tK$>'##0U/F 'N)x0FKE., t7`a;#0#KT 7 O &E  #  ,  "  1%,E#KE K0	
9O]#%T|F+
 4T90UXVWE#@QS0	7J4T~	C9C9)&'F#QF$'NX$!E# 0pM;# D7D	$L $M E9h$P$ME#@
 C9)&'N97#$K
4T9,QtP)&0C#FKT	b$ME#)WQS0	b] F;#01$ME9F$" O &E # #  5  " ;H0C##DO$ME#_@Qd_#T1
-ff!*0 	 QS$ME#'#T FT4	C#$>T $>'|T9]	'NCBQS0	7J 4T^KE#0QS @2'N)O( x0UIBA#K$'NP
 9Ujp%
M;#D	D7$(	b	C9C9)&'FNQh$'NQS$ME#'#TO%,E#KEbK0	u;#TO$'cK'NQC9;#$,hq$ME#x;9	7$$W)&9;#_)<T
2'N)LQC#0QS07$#DR-ff!*0 	 d7C9C9)&'NFKEqU@vL;9)P	C9C9)&'#FKEoS9;#$tD70#0)F(7#TRT9'#l#'	$cT90C90#T
'NO$ME#{f'#)Q 'F"FK$] h$'NO;9#K$'Nq
U F4'N)'7$ME#0)WFK$]FF$'N@#T90C0#T90	$S	C9C9)&'NFKE#,)&D#	)&T9#D
$ME#S	C9C#K0h$'Ny'FsQd0	7J 4T$!E#'N)&O$'(xH+2H2$019W'h.Q79q14? V/)&MCq135	5	5989H0	)#L?
A9	;#21q305	5	67GUgA##KH2'N)"!(#$&%'N)*{'NC90)h$'N
 % $"$>'t3	17%(H%WqT4)&'N$
C % )&'NQ h.;9)<$ME#0)
9;9F$'N#0U
		


	

 ' Qs    q$WxGn

z{n]vq 

-ff!*0 	 lQS$ME#'#T1ffFlC9)<0	$>TpRA#K$'Ni41 #'	$cT9)&K$R;#!;#ff2'N)Px01gK0	;#O'FW$ME#
	$!)FK$M	#$&R'FH$ME#C97)&$FT90)&] F$>]7tF$$.6 r9UXV'X'F]	0)&K'NQd$ME#tC9)&'N#0Q@1%t!;#D	D	$
nQS$ME#'#T 9hT~'N V/N'N)S>0)&@C9	#>'#qU VWE#@p]	0)<|D	0#0)F,Qd$ME#'NTB	#T #'	$
T90C90#T907$H'NO$ME#PFK$]FF$'Nb;9#K$'NqU(VxE#HQS$ME#'#T@09	#,K0FK0;#F$'N'FeF$!E#P#KM	)&
$0)Qd,)&9;#_)<T@2'N)N$0#T9#D-s!*0 	 ,QS$ME#'#T@2'N)WU
: $,;#WT9 a##%B0#0)&D7;9#K$'N

fi0 4, 0 5 4 0 4 GB6&3

ff +fi
@

%,E#0)&Pr
fi

=






3	1

#



+figGB6  E

&

	#T



#
#



6 E

&

ff + fi
A##KfipW$!E#,QC'N)&$M	7$WC9	)	Qd$0)1 t
#'	$Mh$'N9FqK;9QS#0UWx'	$L$ME9h$

B683

 ff +2r	G
t



=


#

/9,



3 ,

_e+ +figGG>"^+&3








/2,

_e+

, 3 5  G"



 fi(+ 


#
#



 5  "







G9+&3 #
3 ff+ +figGG1




;

0 4, 0 5 4 0 4 G%Wg9P)&!20))&T@$>'dh




G" +<3


3 ,

+ZN	G



G9+&3 #
3 ff+



G

1

 ff +figG'F$']	'	T
O

fiy

ff +&3G+6 
	 #T t
T9a#

;>pt;#@V/N'N){0)&	C9C9)&'FNQh$'NX'F
B6

 ff  +fi/G

 ff +2r7G "
t

ff #1 $ME#0%LK0	O%,)&$
Z  ff  	C9C9)&'FNQh$ t

6



:q$,;#L#'F% T9a#l$!E#Hf'	'F%W#D;9#K$'N

 +fiB0 fi

0 5 4 G 6 3_






  < <

#



 ff +&30G
t

  ff 
fi   
 *


fi 



 ff + figGL%W$MEi)&MC9K$S$' fi"U@:q$;#
t

+Z	NG

O
CG E F  JH "



+Z 	 G

;

 ff  +&30G

*



;

ff ;

 

 

 5

+ZN	G

5 4 0 fifi0 e1%,E#KE@	)&H'N#$MF#T7	]	0)<$#Dl$!E#W2'	' %x_#D
+Z G	G
5  6  , ) * - 

V E#  	)&,hM;9QSTO$'9H;9#K$'N#'F
W
9;9F$'N#
@

HG

%,E#0)&

O
C E F * ff ; O  H H 
O GCE F 

)*  6

*

ff ;

+Z 	 G



ff 	   ff _n+ZN	G%L'N#$Mh_ 
)<0C#_hK_#D w



  +fiB0 fifi0 5 4 GB683@ 



O
CG E F
*

 

H

ff ;

 5

    " K

+ZN6	G

ff 7  ff ;Nd]N% 'F(+Z	NG/'##WK0	SK'N#>T90)
%,E#0)&x$ME#WT9a#$'NO'F 5 4 'N#$!F#Tt	)&0C#FK#D ^

  F,	@	C9C9)<' #QF$'NO$>'  ;4VWE#H'N#0)&]FF$'NM;#D	D	$>7@	C9C9)&'FNQh$'NO$'  U
 fi+ B0 5 4 B
G 6  3+ 03 0 5 4 G    3+ 03 0 5 4 G
+ZN5	G

VWE#0$ME#PQS0	7J4TX;9h$'N#WK0	y9L$Mh$TF




 6 < 6 <
<5
<



5  


 <

#

 




< 5 



6Br
#

+2	r	G

VWE#P)&9;#)&TX$0)QSW#T9TX@$ME#PV/N'N)C9	#>'#@'F   yK0	@9S	C9C9)&'F#_QbF$T7

4 0 6
6 <

 +2r003

 + 5 !r	G

<
<







 * *

<

 




0 5 4 GB6



 * *  



  +2r03
  

 <

<

#



0 54 G





 * *  



#

VWE#S#D7D	$,FT9]F	7$MFD	Ly%'N)*7_#Db%W$ME   )F$ME#0)$ME9	  H$ME9F$W$!E#lC9	)&$>_hT90)&] F$>]7,'F
  %W$ME@)&MCK$,$' F$ %6Br	#T 
fi 63LK0	@P9C9)&TXF;9#K$'N#,'h 5 4 ; iPT9a#

40 6

 ff  + 5 !G

  + 5 4 M0 r	G>" 



 

#



 
<
  <








 * *  


+2930G
#

fi

y)

a{yy

D#E	$H'Fg$ME#S	9'F]	HT9K0;#'Ny'N#cK0	OK'N#T90)

 ff

 

+2		G


	#TE#0#KS$ME#PQS077J4TX9;9F$'N#WK07@9P>$MF$TF


ff
 6 <    <    <    6Br
<5
<5
<5

+2 	 G

V 'T9]	'NCnO2ff2'N)L$!E#t	C9C9)&'F#_QbF$'N#P$;#PK'##T90)P$!E#K0FO%,E#0)&
'N)&T90)K0F	UEHK0h/$ME9F$WF$, 6Br1N$!E#LT9>$M)&9;#$'Ny,D	]	0y	



) fi * 6  5 H  +&33 5



8

G

&#

8

6

371$ME#Na)<$

E H ;
)+



ff



  
3
ff +2r	G

<
7
6
6
t

"
+2FNG



:
	



:
	
 *    



fi   

<
<
  :
 	
$ME#,$>0)QSK0	O9LK'NQC9;#$>TFWL;9#K$'NO'F 5 4 U"VWE#L	C9C9)&'#C9)&F$L9C9)&'N#,7)&,T90)<]7T


<

 

*

#

676  ff 

+&3G

#

*

[H
|	C9C90#T9 LU/[,$ME#	' ]7t$0)>QSSK0	|9;#T|nK'NQbC9;#$#DR	|	C9C9)&'F#QF$'Ni$' 
x#Do+230G%(L'##$MF
ff
#

 + 5 4 0!G

6

 +5

VWE#;#,;##Dp+2		G%PE9]	

40

6

8

!r	G>"  O
 ff +2r	G



  # +fiG

 3
" 

 Q#

:
	

<
<

  ff 
fi   
* : 		ff 


ff #

<5

+2G	G

6

+2 	 G
r

VWE#4#TC'		$H;9h$'N#,$!EN;#W'##$MF#T	)&


5 6

3

8fi fi 6 t
 ff +.r	G

8

:
	



" 


#

3

 < <

  ff 
fi   
* :
	
 

U

+2		G

 ff #  +fiG

VWE#PQS077J4TX9;9F$'N#L	)&L'N#$!F#T	;##D+. 	 G$'D	$

<

#

+2	6	G

x'	$@$ME9F$S$ME#@>KE#0QSX)&M;#$#Di.)&'#Q  6S3	C9C9)&'NhKEnK0	|h>'R9y'N#$MF#T|)&'NQ X!FT9T9
C9'7_7$t	C9C9)<'NFKEq1+.(E9h$$MFKE9	)&#N@? H0)&$ME#21H305	5	57	GU [K'N7;#'N|QSDNE7$	)&)&D#	)&T9#D
$ME#c_7$0)C9)<$MF$'N'F  VWE#c] 	)<_h$'N9F/T90)&] F$'NXQSDNE7$H$0QC#$W'N#c$'@9]	$ME9F$x#K
 t	n;9C9C90)'N;9#T|$7
' 3@
 c1(7#T 
t	n	C9C9)&'FNQh$'Nn$'  1ff$ME#0 
tF'p	
;9C9C90)L'N;9#TULA9;#KEX	)<DN;9QS07$,	)&S#'	$HK'N))&K$0UW.$,LT9j}tK0;#$L$'O$M	#MEy%,E#$ME#0)   K0	
9bK'N#T90)&TRFL	X;9C9C90){'N)x'F%0)L9'#;9#TXf'#),OD	]	0
	#T tULIy.hK$,]	02'N
)
6 91


'N#LK0	yF$WQS'	$xM  ( 4  # 1 +.(E9h$$MFKE9	)&#NS?H0)&$ME#21/305	5	5	9G1	2'N)SD	0#0)>F  ;
i(7$0)C9)&$  F"HK'	,	C9C9)<' #QF$'Nc$'  F"x;9#K$'NS'h 5 4 ;	f$ME#W	C9C9)&'FNQh$'N#
	)&@K'	>0#'N;#DNEn$!E#0|$ME#@DN)>FT90	$
    Ql;#>$tF'R%,	C9C9)&'F#_QbF$T~7     %,E#KE


;#$QF$@0FT9H$'+&36	GU/[ >_Qd	)x	$>0)C9)&$MF$>'#	C9C#,$>'  ff  ;


R



fiy

[ C9C#K0F$'Ni'F,-ff!.*	 SQS$ME#'#Tp$'	7X$'#KE9F>$Kl>N$>0Q@12U 	Udf'N)L7	 O1)&l'#X$ME#
L
] hT9$&~'FL$ME#y_7]	0)&$>_#$& hM;9QC#$'Nn%HE#KE T90C90#T9@'Nn$!E#K'N7]	0)&D	0#Ky'FL$ME#Vg#'N)
0)&9C9	#'Ni'F  4
 ffU@X'N)&O_QbC9'N)&$!		$>i$ME#)>FT9;#l'h,K'N7]	0)&D	0#K BpE9FS$'Mh$&f
BB3t+2-s!.*91g35	6		GUff.$,H$e	O'#C90y;#>$'N@%,E#$!E#0)$ME#l)>FT9;#,'FffK'N7]	0)&D	0#K{f'#)$ME#
0#0)&D	b;9#K$'N@T9K0)<_TX_|+Z#G"'#)"n+Z 	GMF$> 4,$ME#WK'##T9$>'#qU

ff  
	  

 
&'Rfiff

H

 	 fi 	  	 4
 y





{$ME#g>K$'NL%(7C9C#L$!E#"] 7)&'N;#e7C9C9)&'FNQF$>'#LKE#0QS/T9]	'NC9TSL$ME#C9)&]#'#;#gK$>'#
$'n$&%('nT9j0)&0	$yKF@'F#$k%'N)>*	01W97QS $!E#>D#QS'	T #$&%'N)*nT9a#T 7~$ME#XDNQd'	T
FK$]FF$'Ni;9#K$'N| 8"!+ :4N
G 6 #<;>=&# ?A@ 1"7#T|$ME##'	7J'N)#$&%'N)*XT9a#T 	p$!E#FK$] F$>'#
;9#K$'N B$+ :4
G 6z
3 3
01: r ;aIX$ME#lC9	C0)L%S%W()&$!)&K$P'N;9)&>]7L$'@$ME9)&T9j0)&07$
2
C
C
E
	C9C9)&'F#QF$'N KE#0QdE90]##D|$ME#'##kK$>]7;9#K$'N#  ff ## 0  ff #( 0  ff (0( ;2$O@_7$0)&>$#D~$'
K'NQC97)&L$ME#P	C9C9)&'F#QF$'N#2'N)>D#QS'	T4FgxuFxT90)&]	TR+.A9	;#$,F2U1 305	5 G	G%W$MEO$!E#'	
'N#$Mh_#TpE#0)&	ULvxWC9	)&$K0;#	)H	$>0)&$L  ff #( ;qVWE#+
 4#TRC9'7_7$H;9F$>'##P)&M;#$#D@)&'NQ $ME#
'N#&K$]	H;9#K$'N7)&


5 6







6&3Wr; 8+



8 fi

3 8"+
 G+&3 #



=

"

 

 ;
#

/+ 5  3

3 8+
 GGW+&3 R



 GG  " 


8"+

 

 G>G    E








#




#

(

 
5

1


3 5

+<3

+2	5	G


G>"



3 
 +&3 R
(





5 G 

+$G	r	G

Z  4 2$ME#0   6wr;q[LC9C9)&'FNQF$>'#@2'N)HDNQd'	T4F"WPFLT90)<]7Tp~+2A9	;#ff$PF2U1ff35	5G	G
9E FW$!E#Hf'	'F%W#D4#TC'		$H9;9F$'N


5 6


8 fi




"

=

 

  ;Q#

/+ 5  3  G






 " 



1 




+$G930G

%,E#0)&PhDNF  6Br@19j 4/
 2>8F<;#>$W_*h+$G	r7G1  ,h>'T90C90#T907$L'N 5 1!>63 0/.0./.0  3 3	U
VxE#9C9)&>'# #@2'N)  +.)&!f0)O$>'mA9	;#L$XF2U1P35	5Gof'N)O9FK$@9C9)&'N#!G@'N'#*|]	0)&
T9j0)&0	$)&'N"
Q   ;9.$Qb0,$>gC'	#P$!E9F$(#;9QS0)&K0F@$ME#tQ9H]	0)&bK'7	U"bFK$
$ME#cQS_7)&$ky@$M)>;#K$M;9)&L|+2	5	G	#T|$+ G930GL%'N)&$MEO#'	$#D41q	#T9C90)<_Qd0	$Mhe)&!;#$>,ME#'F%
$ME9F$HF	)F,	C9C9)&'F#QF$'N'Fe
 ^D7'NW$!E#t#TXK'	S)&M;#$0U2$HuC'	#S$ME9F$$ME##
 
	#T 8"+  GWC#0O$ME#cM	QSP)&'7	UvL#PK0	@)&!20)$'X+2A9	;#?	'#)&T4	q130575	5	Gsf'#)(dT9>K0;#'Ny'N
$ME#LC9'	7$0U"2$xLQh$$0)'F/.;#$M;9)<L$M;#T9O$'	]7$DNF$L$!E#P	9'F]	P)&F$>'##ME#C#L_T9$MF2U

$ fi ff&%

	;ypo sxq



('   x/

V 'R$>$$ME#7C9C9)&'FNQF$>'#|KE#0QSOT9]	'NC9T  A#K$'N5
 91(#;9QS0)&K0FH9C90)&QS07$@%0)&
K'N#T4;#K$TUffA9Qh9x$k%'N)>*	q%0)&ffKE#'	0L>'W$ME9F$pK07P9K'NQC9;#$TS	L9FK$0#;9QS0)h$'NqU
F4'#)lh$ME#@9C90)<_Qd0	$b$ME##$&%'N)*o$'NC9'7'7D	p%WF+
 4#T~$>'
 )i
 )#G98 4DN;9)<3	UpVxE#
KE#'	KP'Fg$!E#P#$k%'N)>*S09	#u;#H$'SK'NQC97)&L$ME#P)&M;#$x%W$MEO$ME#'	L'hsA9	;#q$,hfUZ+&305	5 G7GUgV '
K'NQC97)&$ME#tC0)I2'N)Q	#Kb'FW'N;9)LQS$ME#'#T9P%W$MEi$ME#)PQd$ME#'NTn%)&0C90F$>TR$ME#bC0)&QS0	$
ML

fi

y)

@

a{yy

+2#
 )@ )+G	G%W$MEO$>'NCOT9'F%,tC9)<'NC9FDNF$>'#'F"9!20U(VxE#PFK$] F$>'#
.;9#K$>'#@%WFKE#'70O$'tP'N#L'FffDNQS'7TX	#T#'	7J'N)U

FffDN;9)&3 HVWE9)&H070)W

K'N#T4;#K$TO	c$ME#0Q 2'N)/DNQd'	Tdx0U30r0!r	r	rW#$&%'N)*7%0)&D	0#0)F$>Td7l)>	#T9'NQSPKE#'#'	#D
%DNE	$L]FF;#P  3L3003 UHVWE#9'	$>$'NQ _	0)L;9#$1'N)x$ME#S]N#;9#$L'F0FKEX#$&%'N)*O%WF
#$M	7$F$Ti$'@=0)&'4UVW E#b*FE#'#'NT1 c1%WFLK'NQbC9;#$TR79FK$P0#;9QS0)F$>'#X'Fh$ME#
$Mh$,X$!E#dE#DNE#0){$&%('O	0)&0UPVxE#t	C9C9)&'F#_QbF$P]FF;#'F+3@  %h,K'NQbC9;#$TR7 ff  8
5 4 SK'NQbC9;#$TR7>'	]N#DX$ME#N 4#TmC'		$S9;9F$'N#c'N#$MF#Ti.)&'#Q +2 	GUOVWE#D	'#'#T4#P'F
	C9C9)&'F#QF$'NOKE#0QSc%h($>$T7t$ME#{f'7'F%W#D@QS0FM;9)&


ff
6&3   ~
3 3

pHQC#0QS0	$>T S	C9C9)&'F#_QbF$'NbKE#0Qd

ff

+$G		G

1  ff #( 1  ff (( U H] 	7$HC9)&>'N#,	)&H%'N)*FT
'N;#$O[uC9C0#T9,U F 'N)"SC9)&'NC0)K'NQC97)&'Nb%(LF')&!JQC#0QS07$T$!E#uA#	OQS$ME#'#TU"VWE#
D	'#'NT4#>"'Fg	C9C9)&'F#QF$'Nc2'N)/$ME#HA#	SKE#0QdW(]FF;9F$T@	SM;9#>$$M;#$#D ff  17$+ G7	Gff	
  : :
19QS0	$>'##Tt@A#K$'N%9Uj9U3	12'N)ff!C9K 4KHf'#)QS;#_{+2A9	;#q$(F2U1q30575 G	GU/VWE#u)<M;#$

	)&OC9)&C 07$TRi$ME#bf'N)>Q 'F,E#$'	D#)	QSc_ FffDN;9)&@	#T FffDN;9)& %
U ptF'p)&0C0F$Tn$ME#
9C90)&QS07$P%W$MEo%(DNE7$P	#Tn#_hP$M7*	#D@] h_;#S9$&%0yJk@	#TR1q$ME#)&M;#$l7)&FDNF
C9)&07$TOO$ME#2'N)Q'h E#$>'	DN)	Qd"%
 FffDN;9)<,	#%
T FffDN;9)<u9UffVWE# a#T9#D	,	)<WM;9QQ7)&=T
@$ME#H2'N)Q 'FsQd0	#W$M	9;#F$>T@_yVg7#O3	U
F 'N)L!QF"%DNE7$  ff #( 	#Tn$ME#@A#	p	C9C9)&'NFKEiME#'F% K'	@)&M;#$01s%,E#KEi%WFP9C9K$TU
;#$P$ME#SQC9)&'F]	0Qd0	$SFKE#]	Tn	X$ME#  ff (( >KE#0QSbS)&0Q	)>*0	#	1/$PDN]	@QS0	o] F;#'F
r ; r7r		5O%,E#KEnK'NQC9	)&SM;9#>$M	7$Fn%hDNF#$S$ME#@QS0	n] h_;#@'hur; r937
3 	5@)&0C'N)&$T|	
ME#'NCi$dhfU UVWE#pM;#D7D	$L$ME#@;#O'F,QSN$M;9)<tT9$M)<_9;#$>'##%,E#KE|)<;#)&S	$!)&'NT4;#K$>'#
'FffN$!)]F	)&F$'N9hg]F	)&	#08QS'N)<L$ME9	p3r	rS#$M)S]F	)&F$'N9F/] 7)&	#l	)&P#T9Ty2'N)H
K'NQC'N#07$uQS#$M;9)&	UWVWE#L)&M;#$L@!;9#$M	7$F#K0)&0F>ly$ME#cK'NQC9;#$MF$>'#OK'	$0UWvL$ME#
'	$ME#0)E9	#T@$!E#LN$M)>LK'NQC9;#$!F$'N9FqK'	$2'N)  ff (( ' ]70)  ff #( WQ7)&D	9F2U"VWE#,Q7*F$ME#  ff ((
KE#0QSPK'#QC9;#$MF$'N9hF$>$M)FK$]	H'F]	0)$ME#lQd#$M;9)&LT9$!)&9;#$'NqU
V 'S$M;#T9$!E#P)&'N9;#$M#>('hg$ME#HKE#0QdW$ME#H%(DNE7$WK0hx%(0)<,#K0)&0F>TUff[,x$ME#,%DNE7$
K0F%0)&O#K0)&0FT T9DN)FT4h$'Nn%WFl#'	$>KT |FW$ME#bf'N;9)SQS$!E#'NT90U|VWE#@C9'7_7$$'p
#'	$TbWF $ME#W$ME9)&LQS$ME#'#T9(7C9C90	)$'PuQS'#)&,)&'N9;#>$"$ME9	d$ME#,A#		C9C9)&'#FKEqUff;#$(;9#*F
$ME#MQh#%DNE	$>gK0F>  ff (( T9Tt#'	$ffC90)I2'N)Q %f17$ff"HC9'#'N)ff	C9C9)<' #QF$'Nuf'#)	)&D7(%DNE7$0U
VWE#P$u)<M;#$,	)&L'##$MF#T7  ff #( KE#0QS	U
YP;9)<_#D@$!E#lK'#;9)&S'F(9C90)<_Qd0	$Mh$'NX$L%WFW2'N;9#TX$ME9F$1 f'N)x'NQdd#$&%'N)*7H%W$MEy_7)&D	
%DNE	$>01/$ME#
 4#T|C9'	7$l9;9F$'N#bK'N	]70)&D	Tp$'X	i'N)&T90)P 4#TmC'		$8"%,E#@'	]##Dp$ME#
4#TC9'7_7$W9;9F$'N# 5 4 '	KF$L9$&%0O$k%'S]	K$'#)& 5 4 # 	#T 5 4 ( U"V 'S'	]	L$ME#LC9)&'N#0Q %
##

1>

fiy

6

ff

#0#

ff

(0(

ff

8

MQF%DNE7$  3L3 03

Jkr9U rFNrh
r9U r93	
r9U r	r7	5
r9U r93 

#(

,



6

8

	)&D7P%DNE	$>  3W0!

Jkr9U rF7Nr
r9U r7 93
Jkr9U rF#G
r9U r75 G	


V/	#t3 
@ X0	'F f'#)W)	#T9'NQdyD	0#0)F$>TDNQS'7Tp#$&%'N)*70UP3r0Mr	r7rl#$&%'N)*7W%0)&S)	7J
T9'NQdiK$Tm7XKE#'#'	#DX$ME#%DNE7$L)&'NQz$!E#)7#D	  3L3 003 UVWE#OC0)&QS0	$
%WFW)&0C0F$T7OKE#'#'	#DO$ME#L%D#E	$.)&'#Qc_7)&D	0)W)7#D	  3W0! 




4000

4500

3500

4000

3500
3000
3000
2500
2500
2000
2000
1500
1500
1000
1000

500

0
0.12

500

0.1

0.08

@

0.06

0.04

0.02

0

0.02

0.04

0.06

0.08

0

0

0.01

0.02

0.03

0.04

0.05

0.06

0.07

0.08

3 0

FffDN;9)&P HW$>'	DN)	Qd2'N)  ff #  	#TA#	PKE#0QS2'N) MQF9%DNE7$01F$M	*7#DL] F;#_  L3 03 1 2'N)
DNQS'	Tt#$&%'N)*70U/VWE#WC#'7$"'NS$!E#(!2$"ME#'F%~E#$'7DN)	QS/2'N) 2'N)/$ME#W>KE#0QS   ff ##
	#T  ff #( 	VxE#lT9T@#'	$"E9]	,	7P'F]	0)<_7C#(	#T$!E#lK0	)<bME#'F%|$ME#WQC9)&'F]	0QS07$0U
 ff ## 1qD	]	S@QS0	y'F"JkrU rFNrb%,E#  ff #( D	]	P@QS0	X'hr9U r30	9ULVWE#tC#'	$c'Ny$ME#

;



)&DNE7$(!E#' %xW$ME#PE#$'	D#)	Qw2'N)$ME#lA#7tKE#0QS	U"VxE#PQS0	@WD7]707r9Ujr930 9 U

F T4	C#$>Tp$ME#S2'	' %x_#Dy$M)h$D		US[HP'#'NXFP$!E#'	KF$'N#S%0)&T9$K$Ti%(b$'NC9CTp$ME#
4#TC9'7_7$H;9F$>'##014	#T)&>$M	)&$T@$W%x$!ES#% C'		$ 54  U"VWE#L#% C9'7_7$H%hWKE#'70@	
0	)<KE##DF'N#D@$ME#c_#O9$&%0 5 4 # 	#T 5 4 ( 14%,E#KEXDN]	P@QS#QS;9Qz] F;#'h ULvL#K$ME#
%WFT9'N#PK'#	]	0)<D	0#KL$'t	O'#)&T90)L
3 4#TC'		$W'#KK0;9))&TU
H;9QS0)&K0F/9C90)&QS07$H%(0)<lF'OK'N#T4;#K$Ty2'N)W#'	7J'N)H#$k%'N)>*	0U F4'N)$!E#l	C9C9)&'F#QJ
$'NS>KE#0QS$'L%'N)*{%( $ME#'N;#T@9,	#H$'P	C9C9)&'F#QF$  'F]	0)ffL)7#D	W'Fq] F;#0U"VxE#
QS'	$>]FF$Ty$ME#LC0)&QS07$LT9K0)&9TR' %cU(x'	7J'N),#$&%'N)*701#%,E#'	>P$'NC'	'	D	O,D	]	0y	
FffDN;9)&X3O%(0)<)7#T9'NQSpD	0#0)h$T~	iKE#'#'	#DX%D#E	$	#T|#Ft)>	#T9'NQSi.)&'#Q rX	#T
r9U 79
U F4'N)H0FKEX#$&%('#)*O
 w%h,K'NQbC9;#$Tyf'N)Lhff$!E#d'	$$'#Q	0)H$Mh$0UPvL;#$L'FWFff$ME#
$Mh$01F$&%'L$MF$ff%0)&xKE#'	>0SM;#KE$ME9h$ff_
  (QF#QS=T	#TtQS#QS=T)<MC9K$]		UVWE#
9'7$$'NQz	0)H%WFL#$M	7$F$Ti%W$MEX0FKEy'FW$ME#SKE#'	>0X$&%('O$Mh$01	#TR7C9C9)&'FNQF$>'##P$'
$ME#@*FE#'N'#TB%0)&@$ME#0nK'NQbC9;#$TU|VWE#t9C90)&QS07$O%hd)<0C90F$Tn2'N)@3r 0Mr	r7rOM;#KE #$&J
%'N)*70U[,DNh_
P;#TFLOQS0FM;9)&c'FffD	'N'#T4#L'F"	C9C9)&'F#QF$'Nq
U FffD#;9)&9
 GO	#
T ME#'F%
$ME#OK'N))&!C9'N#T9#DRE#$'7DN)	QSU p@)&0C0F$T|$ME#OC0)&QS07$Sf'#)lT9j 0)<0	$%DNE7$)	#D		1
 r ;j 0Mr ;j6 U FffDN;9)&S6@	#Tp5OME#'F% $ME#O)&] 	7$lE#$'7DN)	QSUPVWE#E#$'	DN)>	QSL	)&SM;9QQ7)&=T
 ] F;#H'F $M	9;#F$Ty_O$!	#l9Uff$ME#,K0hL$,	C9C907)&W$ME9F$  ff (( W#T9TobC9'#'N)
7QS07O
	C9C9)&'F#QF$'NqU  ff #( FQS'	$,h%W#(D#0]7,$ME#S9$H)&M;#$HME#'F%W#D$ME9F$]	0f'N)x#'		J'#)WFK!J






!

fi

y)

a{yy

4500

4000

3500

3000

2500

2000

1500

1000

500

0
2

0

2

4

6

8

10

12

14

16
3

FffDN;9)&

x 10

@HW$>'	DN)	Q

F'
2'N)  ff (( 	C9C#TO$'LDNQS'	T@#$&%('#)*H%W$MESMQbF9%(DNE7$0UffVWE#HQS0	
'N#$MF#T@Lr9U r	r	759Ugx'	$L$ME9h$ WK0FTp730r  @$!E# 4DN;9)&




7000

7000

6000

6000

5000

5000

4000

4000

3000

3000

2000

2000

1000

1000

8000

7000

6000

5000

4000

3000

2000

0
0.8

0.6

0.4

@

0.2

0

0.2

0.4

0.6

0.8

1

1.2

0
0.5

1000

0

0.5

1

1.5

2

0

2.5

0

0.5

1

3 0
; 

1.5

FffDN;9)&L HW$>'	DN)	Qdqf'#)$ME#  ff #  	#TA#	P>KE#0QS/2'N) 	)&D	%(DNE7$01F$!	*	#DL]FF;#ff_  W M
f'#)DNQS'	TX#$k%'N)>*	0UVWE#lE#$'7DN)	Q '#@$ME#c!2$HME#'F%W f'N)  ff ## KE#0QSSE90]##D 
SQS0	b'F Wr rF	Nr1F'N#PF$$ME#LK07$0)x2'N)  ff #( >KE#0QSPE9]##DtSQS0	O'Fer r	 9371
	#T@'N#PF$x$ME#P)&DNE7$WW2'N)A#	OKE#0Qd	1aE9]#_#DOQS0	O'F"r r75	G 9U

3 ;

ff

#0#

ff

(0(

ff



;

#(

A9Qh %(DNE7$
 r ;0!r; 	

  K0h     K0F
r9Ujr	C r93
Jkr9UjrG93
r9Ujr		6
r9U r3	3
r9U 	N
r9U 7	r

_7)&D	P%DNE7$
 r ;  0Mr ; 6

  K0F    K0F
Jkr9U30C G
JkrU r		5
r9U r	7
r9U r930
r9U r	57r
r9U 93	3

V/	#l@X0	'F 2'N)W)	#T9'#QS@D	0#0)F$TX#'	7J'N),#$&%'N)*70UH30r0!r	r	rS#$&%('#)*	%0)&S)	7J
T9'NQdbK$T7SKE#'#'	#Dc$ME#,%DNE7$/)&'NQ$!E#,)	#D	  r;0!r; 	 U F4'N)s0FKE#$&%('#)*
$ME#y]N#p$!F$%W$ME QF#Ql;9Q 
  	#T QS#_QS;9Q %(0 )<@T907$ 4TUVWE#
]###'#T9L'F0FKEi#$k%'N)>*Oc$ME#0y#$M	7$F$Tp%x$!EX$ME#ST90	$ 4Tn$MF$>01	#T
$ME#HK'N))<MC9'N#T9#DS'	
D  H	C9C9)&'FNQh$Tt7] 	)<'#;#(>KE#0QSUeVxE#,9C90)&QS07$W%WF
)&0C0F$TX	OKE#'#'	#DO$ME#L%DNE	$>.)<'NQ ST9j 0)<0	$,)>	#D	  r ;j 0Mr ;j6






2

2.5

fiy

5000

4500

4000

3500

3000

2500

2000

1500

1000

500

0
0.5

@

0

0.5

M E#'F%W
f'#)  ff
DNQS'	T@%W$ME	)&D	L%DNE7$

FffDN;9)&P HVWE#E#$>'	DN)	Q



1

((

1.5

2

2.5

1HE9]N#D~|QS0	 'F W
3 r; rFNGU VWE#p#$&%'N)*i

3500
4000

10000

3000

9000

3500

8000

2500

3000
7000

2500

2000

6000

2000

1500

5000

4000

1500

1000
3000

1000
2000

500
500

0

0

1

2

3

4

5

6

7

0

3

x 10

@

1000

0

0.05

0.1

0.15

0.2

0
5

0.25



0

5

10

15

20

25

30

35

FffDN;9)& G HW$>'	DN)	Qd2'N)b]N#|$MF$>%W$MEBQbFNQS;9Q '	D c1xf'#)t#'	7J'N)y#$k%'N)>*	O%W$ME
%D#E	$b$M	*7_#Dn] h_;#O  r Mr 	 UnVWE#E#$>'	DN)	Q 'N|$ME#y!f$OME#'F%W
f'#)  ff ##
KE#0QS	1ffF$P$ME#OK07$0)  ff #( >KE#0QS @	#Tn$ME9F$SF$P$ME#@)&DNE7$  ff (( UVxE#tKE#0QS  ff ##
DN]	LQS0	O'F"r9U r7r93	1  ff #( DN]	ubQS0	@'her9U r	76l7#T  ff (( DN]	LQS07O'FsrU 	N

0 ;



6000

4000

9000

8000

3500

5000
7000
3000

4000

6000

2500

5000
3000

2000

4000
1500

2000

3000

1000

2000
1000

500

0
0.14

1000

0.12

FffDN;9)&

0.1

0.08

0.06

0.04

0
0.05

0.02

0

0.05

0.1

0.15

0.2

0.25

0

0.3

0

0.5

1

1.5

2

@HW$>'	DN)	QdOf'N)O]##|$MF$X%W$ME^QS#QS;9Q '	D P1uf'N)y#'	7J'N)#$&%'N)*7@%Wff $ME
%D#E	$b$M	*7_#Dn] h_;#O  rM0 r; 	 UnVWE#E#$>'	DN)	Q 'N|$ME#y!f$OME#'F%W
f'#) 
KE#0QS	1ffF$P$ME#OK07$0)  ff >KE#0QS @	#Tn$ME9F$SF$P$ME#@)&DNE7$  ff UVxE#tKE#0QS  ff
DN]	LQS0	O'F/Jkr9U rKG93	1  ff DN]	PQS0	O'her9U r9373u7#T  ff DN]	LQS07O'FsrU 	 	r


#(

((

#(

(0(

&%

##
##

2.5

3

3.5

fi

y)

3000

2500

a{yy

3500

3500

3000

3000

2500

2500

2000

2000

1500

1500

1000

1000

500

500

2000

1500

1000

500

0
0.26

0.24

0.22

@

0.2

0.18

0.16

0.14

0.12

0.1

0.08

0

0

0.02

0.04

0.06

0.08

0.1

0.12



0.14

0.16

0
0.02

0.04

0.06

0.08

0.1

0.12

0.14

FffDN;9)&P6 HW$>'	DN)	Qd2'N)b]N#|$MF$>%W$MEBQbFNQS;9Q '	D c1xf'#)t#'	7J'N)y#$k%'N)>*	O%W$ME
%D#E	$S$M7*	#Dp]FF;#t  r  !r 6 UXVWE#E#$'	DN)>	Q 'Nn$ME#@!f$OME#'F%W
2'N)  ff ##
ff
ff

KE#0QS	1ffF$P$ME#OK07$0)  #( >KE#0QS@	#Tn$ME9F$SF$P$ME#@)&DNE7$  (( UVxE#tKE#0QS  ff ##
DN]	LQS0	O'F/Jkr9U39G 1  ff #( DN]	PQS0	O'her9U r	7l7#T  ff (0( DN]	LQS07O'FsrU r	5

; 0 ;

2500



2500

4000

3500

2000

2000
3000

2500

1500

1500

2000

1000

1000

1500

1000

500

500
500

0
0.09

0.08

0.07

@

0.06

0.05

0.04

0.03

0.02

0.01

0

0.01

0
0.01

0

0.01

0.02

0.03

0.04



0.05

0
0.2

0

0.2

0.4

FffDN;9)&P5 HW$>'	DN)	QdOf'N)O]##|$MF$X%W$ME^QS#QS;9Q '	D P1uf'N)y#'	7J'N)#$&%'N)*7@%W$ME
%D#E	$S$M7*	#Dp]FF;#t  r  !r 6 UXVWE#E#$'	DN)>	Q 'Nn$ME#@!f$OME#'F%W
2'N)  ff ##
KE#0QS	1ffF$P$ME#OK07$0)  ff #( >KE#0QS @	#Tn$ME9F$SF$P$ME#@)&DNE7$  ff (( UVxE#tKE#0QS  ff ##
DN]	LQS0	O'F/Jkr9U r7	591  ff #( DN]	PQS0	O'her9U r93l7#T  ff (0( DN]	LQS07O'FsrU 930

; 0 ;





0.6

0.8

1

fiy

-5

-9

-6
-10

-7
-11
-8
-12
-9

-13
-10

-11

-14
M=1,C=1

M=1,C=2

M=2,C=2

SJJ

M=1,C=1

M=1,C=2

M=2,C=1

@

FffDN;9)&30r WVWE#,C#'	$"'#S$ME#W!2$"ME#'F%W"V/);#W'	Dc*FE#'#'NTyT9]#T9T7S$ME#,#;9QS90)ff'h C9F$$>0)#
f'N)  ff ## 1  ff #( 1  ff (0( 	#TA#7tKE#0QSL2$0)$M)F##D'NODNQd'	T#$&%'N)*708#$ME#PC#'	$

'N$!E#P)&DNE	$ME#'F%W$ME#H$M);#L'	DS_*hE#'#'NTXT9]#T9TX	b$ME#PN;9QS0)"'heC9F$$0)>#"2'N)
#'		J'#),#$&%'N)*7

$] h$'NS;9#K$'NO$($!E#u$0U/[,D#F$ME#'N;#T9LQS07$'N#T@$ME9F$sf'#)ff'NQdW'F$ME#L#$k%'N)>*	
$ME# 4NT@C9'7_7$;9h$'N#K'N	]70)&D	T$'S	S'N)<T90)"4NT@C9'7_7$01#$ME#,E#0;9)<>$KLT9K0)&9TX9!2'N)&
%WFW;#T$>''	]	L$ME#LC9)&'N#0Q@U
vx$ME#H$ME9)&xKE#0Qd  ff #( ($!E#,QS'	$)&'N9;#$	#TtF'cNT9,)&0h'N9	#tFKK0;9)>F$,)&!;#$>0U
2$'N;#$MC0)If'#)QST'N#t7  ff (( _b$ME#HK0FW'hgDNQS'	T@#$&%('#)*	ff%x$!E'F%~%DNE7$0B
U *"QC#)&K0F
]#T90#KO$ME#;#,!;#D	D	$L$!E9F$,$!E#lKE#'	K'F(O>KE#0QSSP#'	$L$!)FDNE7$&f'#)&%W	)&T	#TXT90C90#T9c'N
$ME#PFK$>]FF$'N.;9#K$'Ny	#Th>'tC9	)	Qd$0)] F;#0U
V 'X$M;#T9p$!E#t0	)##DnK0	C9	#$t'FH$ME#@] 	)<'#;#l>KE#0QSOC9)&'NC9'7T|%t$'#'N*X;9C|y$'F
C9)&'N#0Q !;#D	D	$Tn	Rx	$>'Np$F2U +<305	5		GH7]	'	]#_#Dn#_97)&RQhD	0UXVWE#@#_97)&RQhD	
	)&@'hu=X
 )np %,E#KE 0FKE|QFD	@K'##$O'Fu$ME#0)b]	0)&$>K0h,'N)bE#'N)&='N7$MFW9	)&O%W$ME
9;9FeC9)<'N9	#$&	1 %W$MEy0FKEy'#K0F$'NX'F"$!E#d97)W'#KK0;9C#TR%W$MEyC9)&'N9	#$&'hr;  ;pS$'#'N*
X
3 )o6 )R3 G@#$k%'N)>*@	#TX$M)&Tp$'O0	)X$P;##D'	$MEy$ME#SDNQS'7Tn	#Tp#'		J'#)LFK$] F$>'#
;9#K$'N#0UH;9Ql0)L'FWC9F$$0)>#P;#Tp%WFP	r	r7r914%HE#O$ME#t#;9QS90)H'F0C9'#KE#c%hP	r	r9USVWE#
9C90)&QS07$S%hP)&0C90F$>Tp2'N)S30rT9 0)&07$l#$&%'N)*70U F 'N)L0FKEp#$&%'N)*$M);#*FE#'N'#T~%WF
K'NQC9;#$>Td7P9FK$/0N;9Qd0)F$'NqUffVWE#WA#	SQS$ME#'#Tl#T9Tt'F%0)*FE#'N'#T9_bFQS'	$ffF9$ME#
K0FU"2$$ME#;#,	C9C0	)&$ME9F$$ME#L$ME9)<PC9)&'NC9'7T@KE#0QS,E9]	Ll$$0)0	)>#_#D@C90)kf'N)>Q	#K
$ME9	O$!E#PA#	7C9C9)&'NFKEqU"VWE#P)&!;#$>u	)<LM;9QQ	)<=Ty 4DN;9)&3r9U
g G9fiAGF 
$ME#,>K$'N@%LM;9QQb	)&=L$ME#PK'#	$M)<_9;#$>'##,'Fff$ME#LC9	C90)1 	#TT907$jf@M;#x2'N);#$M;9)&
)&07)&KEqUXVWE#@QFnK'N7$M)&9;#$'N#S'F,$!E#C9	C0)	)&@C9)&07$Tn|A#K$'N79UXnA#K$'N7
-ff!*0 	 tQS$ME#'#TR7$M)&'#T4;#KT1()<!JT90)&]	T|)&'NQ 	n] 	)<_h$'N9FWC90)&!C9K$]	X	#Tm7C9C#T $'
WU~-ff!*0 	 @	C9C9)&'#FKEnD7]7tX#$0Qh$K@%p'h{9;#T9#D 	|	)>#$!)	)&|K'	7C9C9)&'FNjJ
QF$>'#n$.
' 3@
  ;W'F%]	0)c$tME#'#;#TB#'	$T $ME9F$$!E#@! '#)&$t#T9T $'p]FF;9F$E#DNE#0)
'N)&T90)$0)>QS_#K0)<0FH%W$MEO$ME#H'N)&T90)	#TQSDNE7$,9L]70OC'N#0	$>_hqf'#)(7t	)>#$!)	)&tK'	>
	C9C9)&'F#QF$'NqU


Q



fi

y)

a{yy

V E#@] 	)&F$>'#9FWT90)&] h$'N|$!	#!E#t$ME9h$tA#	R	C9C9)<'NFKEn@yMC9KFHK0F@'h{-s!*0	 
x
	C9C9)&'#FKEq1$ME#;#t>0)&]N#D|FtX9*|%x$!E|$ME#yN$#D|$ME#'#)&	U VxE#OT90)&] h$'N C9)&'NK>tT9'#
#'	$PQ7*Fl	7F!;9QC#$'N#l)<DN	)&T9#DO$ME#S$M);#K$M;9)<l'h(0#0)&D	.;9#K$>'#7#ToE#0#Kb$cPF'
	C9C#K0	#$'WU(VWE#L]FFT9$&y'Fs-s!.* 	 LQS$ME#'#T@HM;9#&K$W$'b$ME#PK'N#T9$'Nq1 $ME9F$W)hT9_;#
'FgK'#	]	0)<D	0#KPME#'#;#TPDN)<0F$0)$ME9	X3	8#PA#K$>'# 9Uff2$xH$g	O'#C90O9;#$'N@%HE#$ME#0)
'N#LK0	yC9)&'F]	H$ME9F$WM;#KEOK'N#T9$'NXE#'	T9xf'N)$ME#SB0#0)&D7;9#K$'NqU
[LC9C#K0F$'Nn'Fu-s!.* 	 S$ME#'N)&X$'XWSt#'	$c$M)FDNE7$&2'N)&%W	)&T1q$)&9;#)&K'NQC9;#$!F$'N
'Fff'NQSS]	0)FD	%,E#KEX	)&P#'	$W$!)FK$M	#	U pPC9)<0	$>TodKE#0QdPy%,E#KE$ME#l^0#0)&D	
;9#K$'NyP7C9C9)&'FNQF$>To7OVg#'#)W0)<1q%,E#KEXD7]7ub$M)FK$!	#d7C9C9)&'FNQF$>'#@$'O$ME#
$0)Qd)&;#)&T 2'N)@-ff!*0 	 XQS$ME#'#TU "	)&'N;#X	C9C9)&'FNQh$'N KE#0QS@T90C90#T9#DB'# $ME#
T9DN)&X'FP$!E#V/0#'N)b0)&@C97#'N 	)&T90)<]7TU H#*Fp$ME#X	C9C9)&'NFKE|+2A9	;#H$@F2U1
305	K5 G	G1$!E#tKE#0QSST9K0;#T~E#0)<	)<tQC#0)hP$ME#pT9'p#'	$c	$M)<'NT4;#K@#$M)] 	)&F$>'#9F
] 7)&	#08	K'NQbC9	)&"9;9F$'N#+2	5	G7#T$+ G93GUq[u#'7$ME#0)C9'7$]	WFMC9K$/'F9$ME#	C9C9)&'FNQh$'N#
$ME9F$"$!E#t	)&HD	0#0)Fq	#T@#'	$(FK$>]#$&S.;9#K$>'#OT90C90#T907$084E#0#KL$ME#t7)&,	C9C#K0	#P$'S
9)&'NhTKFH'F"(x0UWvx"K'N;9)<L2'N)HSD	]	0hK$] F$'N;9#K$'Ny$uQSDNE7$L9lC'	#$'bK'NQS
;9Cb%W$MES$MF'N)IJkQbFT9u7C9C9)&'FNQF$>'##"%,E#KE@	)&,$$0)ff$!E9	S$ME#HKE#0QS"T9K0;#>TE#0)&	U";#$
0QC#)&K0FH] F;9F$>'#|'NnMQbFWK0F#$&%'N)*7l!E#' % $ME9h$$ME#@9;9F$&n'FP	C9C9)&'FNQh$'N#S
9$>$0)$ME9	O$!E#'	L'N#$Mh_#T.)<'NQ '	$ME#0)]F	)&F$'N9F/QS$ME#'#T90U
VxE#ffK'NQC9;#$MF$>'#9F	QC#K$k	19)&'#9;#$M#ff	#TLD	0#0)h$&lQb	*Fff$ME#KE#0QSg]70)&uh$$M)FK!J
$]		U H72'N)&$M;99F$>b$ME#'N)&$K0FqDN;97)	7$W)&DN	)&T9#DS$ME#L]FFT9$&@'FeV/N'N)0)<xC9	#>'#
 fiX	)&,Qd>#D4U(VxE#7#'	$ME#0)ff'#C90b>M;#,%HE#KEO#T9($>'l9LFT9T4)&>Ttb$ME#,#0	)e.;#$!;9)&	U
2$L%'N;#TR	$>0)&$#D$>'@E#' %$!E#KE#0QSPC90)kf'N)>Qz'Np)&0F/%'N)&TpT4F$!F$0
U p	)&
C9)&07$9C#'N)&#DS$ME#,	C9C#K0	#$&t'F$ME#>,QS$ME#'#T9"'Nd$ME#,E9	#T#J%,)<$>$0T9D	$T4F$!Jk9F	U



	

 





   





p@	)&bN$M)<0QSpDN)F$>!.;#ff$'y$ME#O] 	)&'N;#	#'#	9QS'N;#S)&!f0)<01/%,E#'	9C9;#$b%(07$l'N#D
%WlOQC9)&'F]##D$ME#PC9	C0)UBp
 L	)&LF'S$ME9	9*h.;#q$'SYP)cUN UL7C9C90O2'N)E#xM;#D	D	$>'##0U
	 	   
     	  F  M!    	
  	ff	 	   	  >!  #%'
d$ME#"K$'Nd%(WC9)&>0	$ff9C9)& 8 'N#sf'#)/$ME#ffLa )&$ff'#)&T90)ff	C9C9)&'F#_QbF$'N?P
 QS$M E#'#T7$M)&'#T4;#KT
~A#K$'N59 U V'nK'NQC9;#$ 6	  1$ME#]	0)FD	#DR
M
$
7

F
*
0

|

F
'
	
]
0

c
)
M
$
#
E


.
=
 5 H  +<3 3 5  G H  1	%T9a #,L)	#T9'#Q^] 	)&	#fiff  6  %,E#0)&  6 FKO$>'N)&FT9  +$M, )& 93 ;#$5 'NG U
&

&E 
E
C
x' %

= H
98
6ff 6  C    5  +&3 3 5  G E H 
+$G 	G
H  8 &
 8


< 6ff   7
6 6	 
+$GFNG


#



g

G



Q

r





A

c



e

#

#

#

#

#

6ff

98

<  *

#




&
#

0
E
6  / C   E  + 5  " C E      +&33 5  G 1
& #
8
6 6 r




+$G		G
+$GG	G

fiy

6




8
(

6 E


#

&

3 5  G



#

+$G 	 G

 5  +&3
(

YLj0)&0	$>_h$'N@'F  ff n+ZN	G	#T@$M7*	#Dt]	0)hD	,%x$!E@$ME#HFK$'N)<_haT9$!)&9;#$'NyNT9

<

  ff 
fi   
 *


%,E#0)&





 +5

<







0 GB6 5 <

=







0 1G 6	

 + 54

 #

  e+


<



6 3



G"B+&33

5



G

<





<

8



+$G	6	G

3

  g+<3




ff+

GG

+$G	5	G

VWE# a)&$L$>0)Q # $0)>QP))<]F	7$l90hK0	;#'FP+$GG	GUPX'#;9)H_QbC#0Qd0	$Mh$'N#
;#T19$H9C9)&'N

(

+5





0 BG 6 / 3



5

ff+



G

33
+&3 3#ff+

(

3



5



GG

VWE#;#,7o+$GKG	G%(PE9]	

ff
#

6


=


3


 5 _ 

=

#






5

3 5





 5 )ff+
#

=

3



#

 +5


#







33 5 
,1  q
+
33 ff+ G

3
G



+	r	G
G

+930G


3 5

Gg+&3


3 5

G>"^+&3





$0)Q 

(

6Br

"B+&3





/ ff+ 5

G( "

*





3

(

1A2+

ff 
<  fi 
< 









0 1G 6




G


3

G9+&3

8



e+

G


+		G

6

ff
FffN 8 T|C9'7_7$S;9h$'N#	)&'N#$!F#Tm7pC9;#$$#D    zr91g	#T|F'X#'	$#D$ME#@C9'7_7$S$ME9F$
 
6 Br9U

 6


5 6

8 fi





33

e+



G 
"
 +
e
G

# $ 



=



 ;
#

ff+$

5 3



ff+$

3

G0+&3

ff+$


G



GG

2+$

G" 



 ](



<

<5

=




#







8

0 1G 6  
 

+ 	G

eff $    #%'
3

 

 +5



I  )!MQ'
e
ff
fi    

o$ME#PK$'Ny%tC9)&>0	$H2'N)QS;#Fcf'#) ff  1 f'N)( 6z3 0M0 630Mb$ME9F$L%WFL;#TX2'N)H$ME#
9C90)&QS07$HT9K0)&9TXA#K$'N4U


		 
ff

##

 
 

6


=


#



+ 5 

5





	

  
  	 

3 5

"B+<3



A

G9g+<3

3 5

G



GG

3



=





#



+ 5  ff+



G " +&3

3 5



G+&3 #
3 ff+



GG

+FNG

fi

y)

ff

6

#(

=







3

#




"

	



5



 3
#

ff+



5



ff+

5



3 5

" +<3





8



=

+ 5 




G

(

a{yy



3 5 G>G3

+&3 3 5 G

G9+&3

3 +&33


ff+

3G 3 3 33e+ 5  G




 5 +&33 5 G ;







G>G

 +

=


#

 6
G



f+



(



+ 5 )ff+



G



3 5

G>"^+&3

3

G9+&3

e+



GG

(

8
(

+		G

%,E#0)&6 ( 6 O  E  # # (
.$,POQF$$>0)W'F"T 9$!Fg$'@C#;#D@X$ME#l7C9C9)&'NC9)&F$c;9#K$'N#H2'N),bMCK 4KO#$k%'N)>*41 _*h
$ME#LDNQd'	T'N)W#'7>	J'N)U(A##KS$W,$'#'K0;9QS90)<'NQSL%LT9'@#'	$,C9)&07$W9C#K$X$ME#c$0)QS
)&9;#)&T@2'N)  ff (( 1#f'#)"SD	0#0)>FFK$] F$>'#b.;9#K$'NqU#$0hT@%(LC9)&07$W$ME#H9C9)&'N#,;#>T
2'N)$ME#LDNQd'	T	#T#'	7J'N).;9#K$>'##0U
F 'N)A#DNQS'	T#$&%'N)*


ff

6

((

ff

= 


 E
3pr; 

#(

3
6

ff

#(

+<33

5



#

 



F 'N)#'	7J'N)W#$&%'N)*
((



#

 


5

G

3pr; 



=


fi 

E

#


(

 5  +&3






 >
;   #


<
+
3
G
0
+
5 5
  5 
& #  Q
 #  

 ;   ;>
 
 
+5
8+
G "


 ; #  Q
; #

E

"

ff




#

3

3

5

3 5G

3


8"+



3



+5





<+ 3
ff+

G0+ 5 

3

3

8"+

+5 


3 5
G


(

G



8"+

"

 GG 


5

3

3

3

3

5





3 5

+&3

G

 G>G



 

5

+<3

3 5



3

G

G


+G	G

C G

 |3G9'	Da+&3"

 # &  # 

 >
;   # 
5  |3G0+ 5  ~30G

" 
   5  +&3 5  0G + ff

+
G
 +  G
ff
& #   #  

 ;>
 ;>
5  |03 G"B+ 5  |30G'	Da+&3 "
 
fi  
 +ff
 +  G
ff+  G
 ; #   Q
; # 

E



8"+

ff+




GG"

3

3

C

G

 
 





5 +&33 5 G+ 	G

I@9'	$!EO$ME#,9C9)&>'##d+KG	G"	#TR+  	G$xKE#'	>0O$'9P$ME#0)2/'N) 6a1#%,E#KE#]	0)xx'F%0)8
F'$!E#PFK$] F$>'#O;9#K$'Nq1919R+  	G)ff+$:4GB63 3
;
C
E
C
VxE#L]	K$'N) 5 4 WT9$0)QS#Tp7t'7]##D4#TC'		$H9;9F$'N#W'N#$!F#T7t$>$#D
ff
<    6Br;
<5

VWE#LDN)>FT90	$L)&9;#)&T@2'N)0	)##D@W] F;9F$>To7   ff     F$W$ME#4#TC'		$0U


:R



fiy

 !  4 






[HK*7	1YU1x	$'#q1g\U 1a?A#&#'F%WM*721gVLUff+&305	67	GUL[w07)##DFD	'N)<$!E9Qz2'N),'	$=0Q	9iQJ
KE##0
U 	ff
fiff	01 F13? aM3 G	5U
[HD	'	$M1N U9|Ug+&305	57r	GUgVxE#L$M);#K$M;9)<L'Fs9	H#$k%'N)>*	2'N)]#!;9Fg)&K'	D##$>'#qU	
fiff	ff
fi
!"
fi #$&%'"	
fi%%()*	01 +9U
	)0)1HYU1(? ] 	 T9R:7	)1,-/UP+&35	5	5	GU "	)<_h$'N9FLK0;9QS;#		$yC97#'N 2'N)7$M)FK$!	#
T9>$M)&9;#$'N#0U-,.&/*	%0 '1
fi #$ff%2'"2
fi%ff%3)*	54$7689;:N1=<?>719 	#N	9U

E9F$$MhKE9	)<NN1	LU1N?^H0)&$ME#214AqU9AqUq+&305	5	57	GU 077J4T@$!E#'N)&S2'N)ff>$'NKE9F$>KHK'N9#K$'N#$
#$k%'N)>*	0UqV KEqU9)&0CqU#kIA#K!JkWA#[xJk5	5Jkr 91YL0C9	)&$MQS07$"'h 'NQC9;#$0)A#K0#Kl	#Tb[u;#$'#QJ
$'Nq14#T9	y#$>$!;#$P'F"A#K0#K	U

E9F$$MhKE9	)<NN1NLU1 ?H0)&$ME#21AqU4AqUg+&35	5	5	9GUff-s!*0	 uQd0	7J4T$ME#'N)<)&'NQd] 	)&F$>'#9F
]N%,C'		$0UA9;99QS$$Ty$' * *fi* Vg)>	#0U#'N@x0;9)Fgx$&%('#)*	U
E9F$$MhKE9	)<NN1{U14? H0)&$ME#21qAqUaAqUff+2	r	r7r	GU72'N)QF$>'#OD	'NQS$M)<	#TC#!*0	 LQS0	7J4T
$ME#'N)&	U@,	A0B:*6ADC0EF
ff:.AHGIJA 1KKe+ 	G13
7r a3
309U

ME#'NCq1	|U0{U1 : 0%H)&0#K	1	PU 10N7	*N*h'	91	VLU1F?|	'#)&T4	q1F|U >U+<305	5 	GU0[LC9C9)&'FNQh$#DPC9'	$>0)&'N)
T9>$M)&9;#$'N#SR!P#$&%('#)*	P;#>_#DpQS#$M;9)&0U@Ip	'#)&T4	q1|U/U1/H0	)#01/|Uq4U1ff?
A#'	_14AqUg+ *sT90U GL1 5M&	;6ON/P&%'" ';QR
fi&SBH9;668ffR6;
fiQ6T<?>	U9pIVBC9)&0U
\L0Qb	q19AqU19? \L0Qb	q1#YU +<305	6FNGUA#$'NKE9F$>KP)<_hF$>'#q1 D	9#,T9$!)&9;#$'Nq1 	#Ty$ME#P9		
)&$'N)>F$'N@'h"_QbFD	0UR'VUUUXWP&L6;
fiffL6T&FB$
fi
fi"F2%(6;Y6T	MZE[7:*2R'"2
fi%ff%3Y\
*2"J1 ]F"1 	93  h43	U



Hf$1ff|U1ffW'FQ	9q1HLU1? Vg)&!Cq1 lUW+&35	5	5	GUXX'#T9_#T90C0#T90	$Qd0	7J4T~$ME#'N)<RF
'#K0FgQd$ME#'NTf'#)W	C9C9)&'F#_QbF$LC9)&'NC9FDNh$'NO'Fff_72'N)QbF$'NqU-N1
fi^_`CQ-aJ/
fi
fiffbff
N1/*9%68
fiQT6J1 <c>	dP<"eh145 a30r	U
x_7$'Nq1L\bU *xU1WYPN	q1W-/U1Fa)&	1HHU1,? x0F211HLUP+&30575		GU^VWE#X%7*F0C^FD	'N)&$ME9Q
;9#M;9C90)&]#Tp#0;9)Fg#$&%'N)*70U@J	012f*]?g	13730	6a373MG93	U

2'N)

x'N)&]#$>=	1 *xU/4U1ffA9;#0)QS'N#T9$1"PU 4U1ff? '#'NC90)1e\bU(+&35	6	5	GU@'N;9#T9T|K'N#T9$'N##D @ Fff#_#
_720)&0#K2'N) T9K'N#W;9#T90)/K0	)&KW)&'#;9)&K0U9F@ ';92"Hh	
fi2
fi5ffT
fi #@\
ff%'	
fi%ff%(i2"?CB$9""M&&6_ 'I
ff:*# ';
ff:T"& ';92"UMX'N;97$MF H%c1N[N@0[H'7KF$'N
f'N) x[u>U
N	7*N*F'7_1gVLU1/?7'N)&T4	q1/|UUW+&30575	5	GU "	)<_h$'N9F(C9)<'N9	#>$K7f0)<0#K	#TR$!E#Qb)IJT9$
T4F$M	9F>	U,L/"2%= '
fi #Hff%2'"	
fi%%()*2"54H;6;"&P7:N1=<?>	147593 	79U

	0#0q1qLU1#H'N#D419[SU19?F0);#j"1PU+&35	5		GU'#K*7#DtD	9#HM	QC##D@]70)&	)&D	PC9)<'N9J
#$KL9C90)&$#$0QS0U !
 '"2
fi	
fiff2%,L/"2%	&'Ij1/QRFQ-aJ/
fi-J
fi/Mff;6U4A9C9KF
>M;#P'N
 H0F p'N)<Ty[uC9C#K0F$'N#L'h ,#K0)<$MF H0h'N##D4U

	'N)<T4	q1#mU#U1#\PE9	E9)>	Q	#21MffU17N		*#*F'	91#AqUNVLU1#? A97;#f19:ffU+&35	5 	GU [L7$M)&'#T4;#K$'N$>'P] 7)&jJ
F$'N9FgQd$ME#'NT9x2'N)WD#)	C9E#K0FffQS'NT90UHy	'N)<T4	q14|U4>Ue+ *ffTU G1lkl	ff!mGI9a2:*%
QDM%)6U
 L

fi

y)

a{yy

L	C9C0q1gcUq4U1q? H'#T4)&DN;#=	1FWUqHU"+&305	5	67GUL"'7$>=0Q	9XQFKE##0	)##D;##DXQS0	 4T
$ME#'N)&t7#T@#0	)H)&MC'N#LK'N)>)&K$'NqUff	'N)&T4	q1#|U9U19H0	)>#019|U#4U19?A#'	91aAqU#[SU
+ *ffT90UjGL1 M&2"76 ON/9%'" '8QR
fiffOB$9;668ffR68
fiQT6T<?>	UoIVBC9)&U
H0	)#1W|U1"? A9	;#21L:ffUL+&305	5	67GUB:	)&D7T9]#_h$'NBQS$ME#'#T9O2'N)t7C9C9)&'FNQF$>C9)&'N97#$K
_720)&0#KO%W$MEp)h$L'F(K'#	]	0)<D	0#K	UtIhB$9"Mff&6T '
ff:*';/"
fi2
ff:m ';9	D&
 2""
fiff	
fiRff "
fi #$&%	'	
fi%ff%(i2"UF'#)&DN	OL	;7Q	9qU

F0);#j"1 cUg+&30575	6	GUEHT4;#K$'NX'FffK'NQC9;#$Mh$'N9FK'NQbC##$&9		y#$&%('#)*	>$ME9)&'N;#DNE
)&0QS'F] F9'h %0	*cT90C0#T90#K0U" 	
fi2
fi&	MT
fi #$ff% '"	
fi%%()*2"?CBHP"M&6
 'T
ff:*SW	
ff:S& ';92"U	A9	@pF$' 1[ @#X'N)&DN7tL	;7Q	9qU
:	;9)<$>=0q1	AqU10?~A9C#D	E9F$0)1	YU	+&305	676	GUF:q'NK0h	K'NQC9;#$Mh$'N#%W$MEcC9)&'N9	#$"'NHDN)>	C9E#K0F
$M);#K$M;9)<l	#Tp$!E#_)S	C9C#K0F$'Nn$>'9C90)&$S#$0QSUS,L/"2%&'
ff:* 4H?%_
fi
fiY68
fiff"&%
J
fi1 ?>	130 	F U
x0F21HLU+<305	5		GU '#9#K$'N#$g0	)>#_#DL'ha9! #$&%'N)*70U?
fi #Hff%'	
fi%ff%(i2"1]F1  3a3	306U

J 
fi
fiY68
fiff%%3MFW:*FUg[HT9T9'N7J p>	1HT9%('#'#TK$&	14[SU
-/0	)&21/4U+&305	6	67GU!B$9	V	;ff%( 6;
fiff 4H"?6;2&Sh'	
fi%ff%(i2
68
fiQT6UX'N)<DN	XL	;7Q	9q1ffA9	
-ff	)&214\bUg+&30576	6	GU

oh$'41N[lU

-/$0)&>'Nq19LU14?^[L#T90)&'Nq1 4U H{Ue+&35	6 	GU [ QS0	 4 TX$ME#'N)&O0	)##D@FD	'N)&$ME9Qw2'N)W#0;9)>F
#$k%'N)>*	0U-Q-aJ%(ff
R6;
fiQ61=<F145	5	 a 30r9305U
-ff!*091/VLU"+&30576		GUL'N7]	0)&D70#KlK'##T9$>'#X'F($!E#l$!	Cy;9F$>'#yf'#)W$ME#b_a#$!Jk)>	#D	Tn>_#D
D	F,QS'#T92H
U ,	AB:6A2DC0EF
ff:.AHGIJA =1 <fig!+ G	G1305 93 a305 	69U





H'#K*20)1H{U4VLUg+&35 		GU-2ff
	&%36; 6+ a)&$xT9$>'#9GU"-")<_#K$>'N;9#]	0)&>$&C9)&>0U

A9	;#21N:sU1	? 	'N)<T4	q1F|U	U4+&30575G	GU?*ff9C#'7$>_#DS$M)>FK$M	#M;9#$M)>;#K$M;9)&"S7$M)FK$!	##$&%'N)*70U
nV'N;9)<$=0*		1sYUgAqU1/X'	=0)1/|UgLU 1e?zHFQS'41/|U*xUW+*ffT90UjG1 5M	;6 ffhN1/*9%
'" ';QD&
fiOBHP"76"6;ffJ68
fiQ6Tg	U9pVBC9)<0U
A9	;#21W:ffU1? 	'N)<T4	q1|Ue>Uu+<305	5	5	GUR[x$$M)FK$>'N)LT999	QSK@n2T#f'#)&%W	)&T~#$&%'N)*70U
Q-aJ/
fi
fi&4U"IC9)&U

N1/*9%

A9	;#21:ffU"SU1ff#		*#*F'	91VLU1ff? 	'#)&T4	q1ff|UffU,+<305	5G	GUX0	4T~$!E#'N)&X2'N)cDNQS'	T 9!
#$k%'N)>*	0U ,.&/*	% ' 
fi #$ff%2'"2
fi%ff%3)*	54$7689;:N1L+91 G93 GU
A9E7%(	1F|U[lU 1	? vH$!E#0)&,+&305	530GUN-)&'N9	#$KHT9_hDN#'	";##DlL)<!f'N)>Ql;#F$>'#P'h4$ME#_7$0)#$<J
3 F9Q)*N#'F%WT9D	S9F	UHEF
ff:.A0'" '8&"Q AlE[M*A10K>	U

!>

fiJournal of Artificial Intelligence Research 15 (2001) 189-206

Submitted 2/01; published 9/01

ATTac-2000: An Adaptive Autonomous Bidding Agent
Peter Stone
Michael L. Littman

pstone@research.att.com
mlittman@research.att.com

AT&T Labs Research, 180 Park Avenue
Florham Park, NJ 07932 USA

Satinder Singh
Michael Kearns

satinder.baveja@syntekcapital.com
michael.kearns@syntekcapital.com

Syntek Capital, 423 West 55th Street
New York, NY 10019 USA

Abstract

The First Trading Agent Competition (TAC) was held from June 22nd to July 8th,
2000. TAC was designed to create a benchmark problem in the complex domain of emarketplaces and to motivate researchers to apply unique approaches to a common task.
This article describes ATTac-2000, the first-place finisher in TAC. ATTac-2000 uses a principled bidding strategy that includes several elements of adaptivity . In addition to the success
at the competition, isolated empirical results are presented indicating the robustness and
effectiveness of ATTac-2000's adaptive strategy.

1. Introduction
The first Trading Agent Competition (TAC) was held from June 22nd to July 8th, 2000, organized by a group of researchers and developers led by Michael Wellman of the University
of Michigan and Peter Wurman of North Carolina State University (Wellman, Wurman,
O'Malley, Bangera, Lin, Reeves, & Walsh, 2001). Their goals included providing a benchmark problem in the complex and rapidly advancing domain of e-marketplaces (Eisenberg,
2000) and motivating researchers to apply unique approaches to a common task. A key
feature of TAC is that it required autonomous bidding agents to buy and sell multiple
interacting goods in auctions of different types.
Another key feature of TAC was that participating agents competed against each other
in a preliminary round and many practice games leading up to the finals. Thus, developers
changed strategies in response to each others' agents in a sort of escalating arms race.
Leading into the competition day, a wide variety of scenarios were possible. A successful
agent needed to be able to perform well in any of these possible circumstances.
This article describes ATTac-2000, the first-place finisher in TAC. ATTac-2000 uses a
principled bidding strategy, which includes several elements of adaptivity . In addition to the
success at the competition, isolated empirical results are presented indicating the robustness
and effectiveness of ATTac-2000's adaptive strategy.
The remainder of the article is organized as follows. Section 2 presents the details
of the TAC domain. Section 3 introduces ATTac-2000, including the mechanisms behind
its adaptivity. Section 4 describes the competition results and the results of controlled
experiments testing ATTac-2000's adaptive components. Section 5 compares ATTac-2000
c 2001


AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiStone, Littman, Singh, & Kearns

with some of the other TAC participants. Section 6 presents possible directions for future
research and concludes.

2. TAC

A TAC game instance pits 8 autonomous bidding agents against one another. Each TAC
agent is a simulated travel agent with 8 clients, each of whom would like to travel from TACtown to Boston and back again during a common 5-day period. Each client is characterized
by a random set of preferences for the possible arrival and departure dates; hotel rooms
(The Grand Hotel and Le Fleabag Inn); and entertainment tickets (symphony, theater, and
baseball). To obtain utility for a client, an agent must construct a travel package for that
client by purchasing airline tickets to and from TACtown and securing hotel reservations;
it is possible to obtain additional utility by providing entertainment tickets as well. A TAC
agent's score in a game instance is the difference between the sum of its clients' utilities for
the packages they receive and the agent's total expenditure.
TAC agents buy ights, hotel rooms and entertainment tickets in different types of
auctions. The TAC server, running at the University of Michigan, maintains the markets
and sends price quotes to the agents. The agents connect over the Internet and send bids
to the server that update the markets accordingly and execute transactions.
Each game instance lasts 15 minutes and includes a total of 28 auctions of 3 different
types.

Flights (8 auctions): There is a separate auction for each type of airline ticket: ights to
Boston (inights) on days 1{4 and ights from Boston (outights) on days 2{5. There
is an unlimited supply of airline tickets, and their ask price periodically increases or
decreases randomly by from $0 to $10. In all cases, tickets are priced between $150
and $600. When the server receives a bid at or above the ask price, the transaction
is cleared immediately at the ask price. No resale of airline tickets is allowed.

Hotel Rooms (8): There are two different types of hotel rooms|the Boston Grand Hotel

(BGH) and Le Fleabag Inn (LFI)|each of which has 16 rooms available on days 1{4.
The rooms are sold in a 16th-price ascending (English) auction, meaning that for each
of the 8 types of hotel rooms, the 16 highest bidders get the rooms at the 16th highest
price. For example, if there are 15 bids for BGH on day 2 at $300, 2 bids at $150, and
any number of lower bids, the rooms are sold for $150 to the 15 high bidders plus one
of the $150 bidders (earliest received bid). The ask price is the current 16th-highest
bid. Thus, agents have no knowledge of, for example, the current highest bid. New
bids must be higher than the current ask price. No bid withdrawal and no resale is
allowed. Transactions only clear when the auction closes. To prevent agents from all
waiting until the end of the game to bid on hotel rooms, hotel auctions can close after
an unspecified period (roughly one minute) of inactivity (no new bids received).

Entertainment Tickets (12): Baseball, symphony, and theater tickets are each sold for
days 1{4 in continuous double auctions. Here, agents can buy and sell tickets, with
transactions clearing immediately when one agent places a buy bid at a price at least
as high as another agent's sell price. Unlike the other auction types in which the
190

fiATTac-2000: An Adaptive Autonomous Bidding Agent

goods are sold from a centralized stock, each agent starts with a random endowment
of entertainment tickets. The prices sent to agents are the bid-ask spreads, i.e., the
highest current bid price and the lowest current ask price (due to immediate clears, ask
price is always greater than bid price). When a bid that beats the current bid (ask)
price arrives, the sale price is the standing bid (ask) price, as opposed to the arriving
ask (bid) price. In this case, bid withdrawal and ticket resale are both permitted.
In addition to unpredictable market prices, other sources of variability from game instance to game instance are the client profiles assigned to the agents and the random initial
allotment of entertainment tickets. Each TAC agent has 8 clients with randomly assigned
travel preferences. Clients have parameters for ideal arrival day, IAD (1{4); ideal departure day, IDD (2{5); grand hotel value, GHV ($50{$150); and entertainment values, EV
($0{$200) for each type of entertainment ticket.
The utility obtained by a client is determined by the travel package that it is given in
combination with its preferences. To obtain a non-zero utility, the client must be assigned
a feasible travel package consisting of an arrival day AD with the corresponding inight,
departure day DD with the corresponding outight, and hotel rooms of the same type (BGH
or LFI) for each day d such that AD  d < DD. At most one entertainment ticket can
be assigned for each day AD  d < DD, and no client can be given more than one of the
same entertainment ticket type. Given a feasible package, the client's utility is defined as
1000 , travelPenalty + hotelBonus + funBonus
where
 travelPenalty = 100(jAD , IAD j + jDD , IDD j)
 hotelBonus = GHV if the client is in the GBH, 0 otherwise.
 funBonus = sum of relevant EV's for each entertainment ticket type assigned to the
client.
A TAC agent's final score is simply the sum of its clients' utilities minus the agent's
expenditures. Throughout the game instance, it must decide what bids to place in each of
the 28 auctions. At the end of the game, it must submit a final allocation of purchased
goods to its clients.
The client preferences, allocations, and resulting utilities from one particular game from
the TAC finals (Game 3070 on the TAC server) are shown in Tables 1 and 2.
For full details on the design and mechanisms of the TAC server, see Wellman et al. (2001).

3.

ATTac-2000
ATTac-2000 finished first in the Trading Agent Competition using a principled bidding
strategy, which included several elements of adaptivity . This adaptivity gave ATTac-2000
the exibility to cope with a wide variety of possible scenarios at the competition. In this
section, we describe ATTac-2000's bidding strategy, its method for determining the best
allocation of goods to clients, and its three forms of adaptivity. ATTac-2000's high-level
strategy is summarized in Table 3.
191

fiStone, Littman, Singh, & Kearns

Client
1
2
3
4
5
6
7
8

IAD
Day 2
Day 1
Day 4
Day 1
Day 1
Day 2
Day 1
Day 1

IDD GHV BEV
Day 5 73 175
Day 3 125 113
Day 5 73 157
Day 2 102 50
Day 3 75
12
Day 4 86 197
Day 5 90
56
Day 3 50
79

SEV TEV
34
24
124 57
12 177
67
49
135 110
8
59
197 162
92 136

Table 1: ATTac-2000's client preferences from game 3070. BEV, SEV, and TEV are EVs
for baseball, symphony, and theater respectively.

Client
1
2
3
4
5
6
7
8

AD
Day 2
Day 1
Day 3
Day 1
Day 1
Day 2
Day 1
Day 1

DD
Day 5
Day 2
Day 5
Day 2
Day 2
Day 3
Day 5
Day 2

Hotel Ent'ment Utility
LFI
B4
1175
BGH
B1
1138
LFI
T3, B4
1234
BGH
None
1102
BGH
S1
1110
BGH
B2
1183
LFI S2, B3, T4 1415
BGH
T1
1086

Table 2: ATTac-2000's client allocations and utilities from game 3070. Client 1's \B4" under
\Ent'ment" indicates baseball on day 4.

3.1 Bidding Strategy

TAC was defined so as to be simple enough to have a low barrier to entry, yet complex
enough to prevent tractable solution via direct game-theoretic analysis. Given that an
optimal solution is not attainable, we use a principled approach that takes advantage of
details of the TAC scenario. In general, ATTac-2000 aims to be robust to the parameter
space defined by TAC as well as to conceivable opponent strategies.
At every bidding opportunity, ATTac-2000 begins by computing the most profitable
allocation of goods to clients (which we shall denote G ), given the goods that are currently
owned and the current prices of hotels and ights. (See Section 3.3 for a caveat.) For the
purposes of this computation, ATTac-2000 allocates, but does not consider buying or selling,
entertainment tickets. In most cases, G is computed using integer linear programming, as
described in Section 3.2.
ATTac-2000's high-level bidding strategy is based on the following two observations:
192

fiATTac-2000: An Adaptive Autonomous Bidding Agent

1. While the auctions are open:
 Obtain updated market prices.
 Compute G: the most profitable allocation of goods given current holdings and
prices.
 Bid in 1 of 2 different modes
Passive: bid to keep options open
Active: at end, bid aggressively on packages
2. Allocate:
 Compute G with closed auctions and allocate purchased goods to clients.
Table 3: An overview of ATTac-2000's high-level strategy.
1. Since airline prices periodically increase or decrease with equal probability, the expected change in price for each airline auction is $0. Indeed, it can be shown that if
the airline auction is considered in isolation, waiting until the very end of the game to
purchase tickets is an optimal strategy (except in the rare case that the price reaches
the lowest allowed value).
2. Since hotel prices are monotonically increasing, as the game proceeds, the hotel prices
approach the eventual closing prices.
Therefore, ATTac-2000 aims to delay most of its purchases, and particularly its airline
purchases, until late in the game. ATTac-2000's high-level bidding strategy is based on the
premise that it is best to delay \committing" to the current G for as long as possible.
Although it continually reevaluates G, and is therefore never technically committed to
anything, the markets are such that it is rarely advantageous to change a client's travel
package if it would mean wasting an airline ticket or an expensive hotel room (thus requiring
additional ones to be purchased).
ATTac-2000 accomplishes this delay of commitment by bidding in two different modes:
passive and active. The passive mode, which lasts most of the game, is designed to keep as
many options open as possible. During the passive mode, ATTac-2000 computes the average
time it takes for it to compute and place its bids, Tb (Tb is the average time it takes to go
through one iteration of the loop in step 1 of Table 3). We found that Tb ranged from 10
seconds to well over a minute, and was primarily dependent upon the server's load. Call
the time left in the game Tl . When Tl  2  Tb , ATTac-2000 switches to its active mode,
during which it buys the airline tickets required by the current G and places high bids for
the required hotel rooms. Note that ATTac-2000 expects to run at most 2 bidding iterations
in active mode. In fact, only 1 such iteration is necessary, but there is a huge cost to failing
to complete the iteration before the end of the game. Planning for 2 active iterations leaves
room for some error.
Based on the current G , its current mode, and Tl, ATTac-2000 bids for ights, hotel
rooms, and entertainment tickets.
193

fiStone, Littman, Singh, & Kearns

3.1.1 Flights

While in the passive mode, ATTac-2000 does not bid in the airline auctions. In active mode,
ATTac-2000 buys all currently unowned airline tickets needed for the current G. In most
cases, that means that it only bids for airline tickets during its first bidding opportunity
in the active mode. However, in the face of drastically changing (hotel and entertainment
ticket) prices, G could change suciently to necessitate purchasing additional ights, instead of simply using the ones that have already been purchased.
3.1.2 Hotels

When in the passive mode, ATTac-2000 bids in the hotel auctions either to try to win hotels
cheaply should the auction close early, or to try to prevent the hotel auctions from closing
early. It might be advantageous to prevent a hotel auction from closing if no rooms are
currently desired in order to keep open the option of switching to that hotel should future
market prices warrant it.
For each hotel room of type i (such as \Grand Hotel, night 3"), let Hi be the number
of rooms of type i needed for G . Based on the current price of i, Pi , ATTac-2000 tries to
acquire n rooms where

8
>>
< max(8Hi; 4)
n = > max(H ; 2)
>: max(Hi; 1)
i

if Pi = 0 (only true at the outset of the game)
if Pi  10
if Pi  20
if Pi  50:

If ATTac-2000's outstanding bids would already win n rooms should the auction close at
the current price, then ATTac-2000 does nothing: should the auction close prematurely,
ATTac-2000 wins the n rooms cheaply, and competitors lose the opportunity to get any
rooms of type i later in the game. Otherwise, ATTac-2000 bids for n rooms at $1 above
the current ask price. The formula for computing n was selected so as to risk wasting up
to $40{$50 per room type for the benefit of maintaining exibility later in the game. The
exact parameters here were chosen in an ad-hoc fashion without detailed experimentation.
Our intuition is that ATTac-2000's performance is not very sensitive to their exact values.
In the active mode, ATTac-2000 bids on hotel rooms based on their marginal value within
allocation G . Let V (G ) be the value of G (the income from all clients, minus the cost of
the yet-to-be-acquired goods). Let G0c be the optimal allocation should client c fail to get
its hotel rooms. Note that G0c might differ from G in the distribution of entertainment
tickets as well as in the ights and hotels of client c. ATTac-2000 bids for the hotel rooms
assigned to client c in G at a price of V (G ) , V (G0c ). Since at this point ights are a
sunk cost, this price tends to be more than $1000.
Notice that ATTac-2000 bids the full marginal utility for each hotel room required by
the client's travel package. An alternative would have been to divide the marginal utility
over the number of rooms in the package, which would have eliminated the risk of spending
more on hotels than the itinerary is worth. On the other hand, failing to win a single hotel
room is enough to invalidate the entire itinerary. ATTac-2000 bids the full marginal utility
to maximize the chance that valid itineraries are obtained for all clients. In a combinatorial
194

fiATTac-2000: An Adaptive Autonomous Bidding Agent

auction, the bidder would be able to be place a bid for the conjunction of the desired rooms
and would therefore not need to choose between these two alternatives.
3.1.3 Entertainment Tickets

ATTac-2000's bidding strategy for the entertainment tickets hypothesizes that for each

ticket, the opponent buy (sell) price remains constant over the course of a single game
(but may vary from game to game). So as to avoid underbidding (overbidding) for that
price, ATTac-2000 gradually decreases (increases) its bid over the course of the game. The
initial bids are always as optimistic as possible, but by the end of the game, ATTac-2000 is
willing to settle for deals that are minimally profitable. In addition, this strategy serves to
hedge against ATTac-2000's early uncertainty in its final allocation of goods to clients.
On every bidding iteration, ATTac-2000 places a buy bid for each type of entertainment
ticket, and a sell bid for each type of entertainment ticket that it currently owns. In all cases,
the prices depend on the amount of time left in the game (Tl), becoming less aggressive as
time goes on (see Figure 1).
Buy value

200

}$50
Bid Price ($)

Owned, unallocated
sell value

}$20

100
Owned,allocated
sell value
$30

0

5

10

Game Time (min.)

15

Figure 1: ATTac-2000's bidding strategy for entertainment tickets. The black circles indicate the calculated values of the tickets to ATTac-2000. The lines indicate the bid
prices corresponding to those values. For example, the solid line (which increases
over time) corresponds to the buy price relative to the buy value. Correspondence between the text and the lines is indicated by similar line types and boxes
surrounding the text.
For each owned entertainment ticket E , if E is assigned in G, let V (E ) be the value
of E to the client to whom it is assigned in G (\owned, allocated sell value" in Figure 1).
ATTac-2000 offers to sell E for min(200; V (E ) +  ) where  decreases linearly from 100 to
20 based on Tl.1 If there is a current bid price greater than the resulting sell price, then
ATTac-2000 raises its sell price to 1 cent lower than the current bid price in order to get as
high a price as possible.
If E is owned but not assigned in G (because all clients are either unavailable that night
or already scheduled for that type of entertainment in G ), let V (E ) be the maximum value
1. Recall that $200 is the maximum possible value of E to any client under the TAC parameters.

195

fiStone, Littman, Singh, & Kearns

for E over all clients, i.e. the greatest possible value of E given the client profiles (\owned,
unallocated sell value" in Figure 1). ATTac-2000 offers to sell E for max(50; V (E ) ,  )
where  increases linearly from 0 to 50 based on Tl . Once again, ATTac-2000 raises its price
to meet an existing bid price that is greater than its target price. This strategy reects
the increasing likelihood as the game progresses that G will be close to the final client
allocation, and thus that any currently unused tickets will not be needed in the end. When
in active mode, ATTac-2000 assumes that G is final and offers to sell any unneeded tickets
for $30 in order to obtain at least some value for them (represented by the discrete point at
the bottom right in Figure 1). Below $30, ATTac-2000 would rather waste the ticket than
allow a competitor to make a large profit.
Finally, ATTac-2000 bids to buy each type of entertainment ticket E (including those
that it is also offering to sell) based on the increased value that would be derived by owning
E . Let G0E be the optimal allocation that would result were E owned (\buy value" in
Figure 1). Note that G 0E could have different ight and hotel assignments than G so as to
make most effective use of E . Then, ATTac-2000 offers to buy E for V (G0E ) , V (G) ,  ,
where  decreases linearly from 100 to 20 based on Tl.
All of the parameters described in this section were chosen arbitrarily without detailed
experimentation. Again our intuition is that, unless opponents know and explicitly exploit
these values, ATTac-2000's performance is not very sensitive to them.

3.2 Allocation Strategy
As is evident from Section 3.1, ATTac-2000 relies heavily on computing the current most
profitable allocation of goods to clients, G . Since G changes as prices change, ATTac-2000
needs to recompute it at every bidding opportunity. By using an integer linear programming
approach, ATTac-2000 was able to compute optimal final allocations in every game instance
during the tournament finals|one of only 2 entrants to do so.2
Most TAC participants used some form of greedy strategy for allocation (Greenwald
& Stone, 2001). It is computationally feasible to quickly determine the maximum utility
achievable by client 1 given a set of purchased goods, move on to client 2 with the remaining
goods, etc. However, the greedy strategy can lead to suboptimal solutions. For example,
consider 2 clients A and B with identical travel days IAD and IDD as well as identical
entertainment values EV , but with A's GHV = $50 and B 's GHV = $150. If the agent
has exactly one of each type of hotel room for each day, the optimal assignment is clearly
to assign the BGH to client B . However, if client A's utility is optimized first, it will be
assigned the BGH, leaving B to stay in LFI. The agent's resulting score would be 100 less
than it could have been.
As an improvement over the basic greedy strategy, we implemented a heuristic approach
that implements the greedy strategy over 100 random client orderings and chooses the most
profitable resulting allocation. Empirically, the resulting allocation is often optimal, and
never far from optimal. In addition, it is always very quick to compute. In a set of seven
games from just before the tournament, the greedy allocator was run approximately 600
times and produced allocations that averaged 99.5% of the optimal value.
2. As computed by Shou-de Lin of the TAC organizing team.

196

fiATTac-2000: An Adaptive Autonomous Bidding Agent

As the competition drew near, however, it became clear that every point would count.
We therefore implemented an allocation strategy that is guaranteed to find the optimal
allocation of goods.3 The integer linear programming approach used by ATTac-2000 works
by defining a set of variables, constraints on these variables, and an objective function.
An assignment to the variables represents an allocation to the clients and the constraints
ensure that the allocation is legal. The objective function encodes the fact that we seek the
allocation with maximum value (utility minus cost).
The following notation is needed to describe the integer linear program. The formal notation is included for completeness; an equivalent English description follows each equation.
The symbol c is a client (1 through 8). The symbol f is a feasible travel package, which
consists of: the arrival day AD(f ) (1 through 4); the departure day DD(f ) (2 through 5),
and the choice of hotel H (f ) (BGH or LFI). There are 20 such travel packages. Symbol e
is an entertainment ticket, which consists of: the day of the event D(e) (1 through 4), and
the type of the event T (e) (baseball b, symphony s, or theater t). There are 12 different
entertainment tickets. Symbol r is a resource (AD, DD, BGH, or LFI).
Using this notation, the 272 variables are: P (c; f ), which indicates whether client c will
be allocated feasible travel package f (160 variables); E (c; e), which indicates whether client
c will be allocated entertainment ticket e (96 variables); and, Br (d) is the number of copies
of resource r we would like to buy for day d (16 variables).
There are also several constants that define the problem: or (d) is the number of tickets
of resource r currently owned for day d, pr (d) is the current price for resource r on day d,
uP (c; f ) is utility to customer c for travel package f , and uE (c; e) is the utility to customer
c for entertainment ticket e.
Given this notation, the objective is to maximize utility minus cost

X
c;f

uP (c; f )P (c; f ) +

,
,

X
d2f2;3;4;5g

X
c;e

uE (c; e)E (c; e)

pDD (d)BDD(d)

X

d2f1;2;3;4g;r2fBGH;LFI;ADg

pr (d)Br (d)

subject to the following 188 constraints:
 For all c, Pf P (c; f )  1: No client gets more than one travel package (8 constraints).
 For all d 2 f1; 2; 3; 4g,

X X

c f jAD(f )=d

P (c; f )  oAD (d) + BAD (d);

For all d 2 f1; 2; 3; 4g and h 2 fBGH; LFIg,

X

X

P (c; f )  oh (d) + Bh (d);
c f jH (f )=h & AD(f )d<DD(f )
3. The general allocation problem is NP-complete, as it is equivalent to the set-packing problem (Garey &
Johnson, 1979). Exhaustive search is computationally intractable even with only 8 clients.
197

fiStone, Littman, Singh, & Kearns

For all d 2 f2; 3; 4; 5g,

X X
c f jDD(f )=d

P (c; f )  oDD (d) + BDD (d) :

The demand for resources from the selected travel packages must not exceed the sum
of the owned and bought resources (16 constraints).

 For all e, Pc E (c; e)  oE (e): The total quantity of each entertainment ticket allocated
does not exceed what is owned (12 constraints).

 For all c and e, Pf jAD(f )D(e)<DD(f ) P (c; f )  E (c; e): An entertainment ticket can

only be used if its day is between the arrival and departure day of the selected travel
package (96 constraints).

 For all c and d 2 f1; 2; 3; 4g, PejD(e)=d E (c; e)  1: Each client can only use one
entertainment ticket per day (32 constraints).

 For all c and y 2 fb; s; tg, PejT (e)=y E (c; e)  1: Each client can only use each type of
entertainment ticket once (24 constraints).

 All variables are integers.
The solution to the resulting integer linear program is a value-maximizing allocation
of owned resources to customers along with a list of resources that need to be purchased.
Using the linear programming package \LPsolve", ATTac-2000 is usually able to find the
globally optimal solution in under one second on a 650 MHz Pentium II.
Note that this is not by any means the only possible formulation of the allocation.
Greenwald, Boyan, Kirby, and Reiter (2001) studied a variant and found that it performed
extremely well on a collection of large, random allocation problems.
The above approach is guaranteed to find the optimal allocation, and usually does so
quickly. However, since integer linear programming is an NP-complete problem, some inputs
can lead to significantly longer solution times. In a sample of 32 games taken shortly before
the finals, the allocator was called 1866 times. In 93% of the cases, the optimization took a
second or less. Less than 1% took 6 or more seconds. However, the 3 longest running times
were all over a minute and all came from the same game. ATTac-2000 used the strategy
that if an integer linear program takes 6 or more seconds to solve, the above-mentioned
greedy strategy over random client orderings is used as a fall-back strategy for the rest of
the game. This fall-back strategy was not needed during the tournament finals.

3.3 Adaptivity

In a TAC game instance, the only information available to agents is the ask prices|
individual bids are not visible. After each game, transaction-by-transaction data is available,
but the lack of within-game information precluded competitors from using detailed models
of opponent strategies in decision making. ATTac-2000 instead adapts its behavior on-line
in three different ways: adaptable timing of bidding modes; adaptable allocation strategy;
and adaptable hotel bidding.
198

fiATTac-2000: An Adaptive Autonomous Bidding Agent

3.3.1 Timing of Bidding Modes

ATTac-2000 decides when to switch from the passive to the active bidding mode based on

the observed server latency Tb during the current game instance (see Section 3.1).
3.3.2 Allocation

ATTac-2000 adapts its allocation strategy based on the amount of time it takes for the

integer linear programming approach to determine optimal allocations in the current game
instance (see Section 3.2).
3.3.3 Hotel Bidding

Perhaps most significantly, ATTac-2000 predicts the closing prices of hotel auctions based
on their closing prices in previous games. Hotel bidding in TAC was particularly challenging
due to the extreme volatility of prices near the end of the game. As stated in Section 3.1.2,
at the end of the game ATTac-2000 bids the marginal utility for each desired hotel room,
which was often in excess of $1000.
During the preliminary competition, few agents bid their marginal utilities on hotel
rooms. Those that did, however, generally dominated their competitors; such agents were
high-bidders, bidding  $1000, always winning the hotels on which they bid, but paying far
less than their bids. Having observed a dominant strategy during the preliminary rounds,
most agents, including ATTac-2000, adopted this high-bidding strategy during the actual
competition. The result was many negative scores, as prices skyrocketed in the last moments
of the game once there were 16 high bids for a given room.
In Section 3.1, we stated that ATTac-2000 computes G based on the current prices of
the hotel rooms. Should the prices eventually become very high, ATTac-2000 would either
end up paying too high a price for the hotel rooms or else fail to get travel packages for
some of its clients. The only alternative was to avoid counting on obtaining contentious
hotel rooms.
Since strategies were changing up to the last minute before the finals, there was no way
to identify a priori which hotels would be most contentious or whether hotel prices would
actually skyrocket in the tournament. Therefore, ATTac-2000 divided the 8 hotel rooms
into 4 equivalence classes, exploiting symmetries in the game (hotel rooms on days 1 and
4 should be equally in demand as should rooms on days 2 and 3), assigned priors to the
expected closing prices of these rooms, and then adjusted these priors based on the observed
closing prices during the tournament.
As expected, the Grand Hotel on days 2 and 3 turned out to be most contentious during
the finals. Le Fleabag Inn on the same days was also fairly contentious. Whenever the
actual price for a hotel was less than the predicted closing price, ATTac-2000 used the
predicted hotel closing price for computing all of its allocation values.
One additional method for predicting whether hotel prices would skyrocket in a given
game is to notice who the participants are and whether or not they tended to be highbidders in past games (see Figure 2). Although such information was not available via the
server's API, a game's participants were always published beforehand on the TAC web page.
By automatically downloading this information from the web (a practice whose ethicality
was questioned at the competition), and matching against a precompiled database of which
199

fiStone, Littman, Singh, & Kearns

agents were high-bidders in the past, ATTac-2000 would only use the predicted hotel closing
prices in games with 3 or more high-bidders involved: in games with fewer high-bidders, the
prices of hotel rooms almost never skyrocketed4 . As it turned out, all but one of ATTac2000's games in the semi-finals, and all games in the finals, involved several high-bidders,
thus triggering the use of predicted hotel closing prices.
RiskPro grand day 2 recent

aster grand day 2

250

1400

Aster: Grand Day 2

1200

Bid Price ($)

200

Bid Price ($)

1200

RiskPro: Grand Day 2

200
150

100
100

1000

800
800

600

400
400

50
200

0
0

0

100

200

300

5

400

500

600

10

Game Time (min.)

700

800

0
0

900

15

0

100

200

5

300

400

500

10

600

Game Time (min.)

700

800

900

15

Figure 2: Graphs of two different agents' bidding patterns over many games. Each line
represents one game's worth of bidding in a single auction. Left: RiskPro never
bids over $250 in the games plotted. Right: Aster consistently bids over $1000
for rooms.
Empirical testing (Section 4) indicates that this strategy is extremely beneficial in situations in which hotel prices do indeed escalate, while it does not lead to significantly degraded
performance when they do not.

4. Results

TAC consisted of a preliminary round that ran over the course of a week and involved
roughly 80 games for each of the 22 participants. The top 12 finishers were invited to
the semi-finals and finals in Boston, MA on July 8th. Since agents and conditions were
constantly changing, and since only 13 games were played by each agent in the semi-finals
and finals, the competition does not provide a controlled testing environment. In this
section, we describe ATTac-2000's success in the tournament, but also present empirical
results of controlled tests that demonstrate the effectiveness and robustness of ATTac-2000's
adaptive strategy.

4.1 The Competition

ATTac-2000's scores in the 88 preliminary-round games ranged from ,3000 to over 4500
(mean 2700, std. dev. 1600). A good score in a game instance is in the 3000 to 4000 range.
We noticed that there were many very bad scores (12 less than 1000 and seven less than 0).
4. With just 2 high-bidders, the only way to have the price escalate would be if they bid for a combined
total of 16 rooms of the same hotel type. That could only happen if all of their clients were to stay in
the same hotel on the same night, a very unlikely scenario given the TAC parameters.

200

fiATTac-2000: An Adaptive Autonomous Bidding Agent

This is largely the result of ATTac-2000 not yet being imbued with its adaptive timing of
bidding modes. During the preliminary round, ATTac-2000 shifted from passive to active
bidding mode with 50 seconds left in the game instance. While 50 seconds is usually plenty
of time to allow for at least 2 iterations through ATTac-2000's bidding loop, there were
occasions in which the network and server lags were such that it would take more than 50
seconds to obtain updated market prices and submit bids. In this case, ATTac-2000 would
either fail to buy airline tickets, or worse still, would buy airline tickets but not get the
final hotel bids in on time. Noticing that the server lag tended to be consistent within a
game instance (perhaps due to the trac patterns generated by the participating agents),
we introduced the adaptive timing of bidding modes described in Section 3.3. After this
change, ATTac-2000 was always able to complete at least one, and usually two, bidding
loops in the active bidding phase.
The adaptive allocation strategy never came into play in the finals, as ATTac-2000 was
able to optimally solve all of the allocation problems that came up during the finals very
quickly using the integer linear programming method.
However, the adaptive hotel bidding did play a big role. ATTac-2000 performed as well
as the other best teams in the early TAC games when hotel prices (surprisingly) stayed low,
and then out-performed the competitors in the final games of the tournament when hotel
prices suddenly rose to high levels. Indeed, in the last 2 games, some of the popular hotels
closed at over $400. ATTac-2000 steered clear of these hotel rooms more effectively than its
closest competitors.
Table 4 shows the scores of the 8 TAC finalists (Wellman et al., 2001). ATTac-2000's
consistency (std. dev. 443 as opposed to 1600 in the preliminaries) is apparent: it avoided
having any disastrous games, presumably due in large part to its adaptivity regarding timing
and hotel bidding.
Rank
1
2
3
4
5
6
7
8

Team

Avg. Score Std. Dev.
ATTac-2000 3398
443
RoxyBot
3283
545
aster
3068
493
umbctac1 3051
1123
ALTA
2198
1328
m rajatish 1873
1657
RiskPro
1570
1607
T1
1167
1593

Institution
AT&T Labs { Research
Brown University, NASA Ames Research
STAR Lab, InterTrust Technologies
University of Maryland at Baltimore County
Artificial Life, Inc.
University of Tulsa
Royal Inst. Technology, Stockholm University
Swedish Inst. Computer Science, Industilogik

Table 4: The scores of the 8 TAC finalists in the semi-finals and finals (13 games).

4.2 Controlled Testing

In order to evaluate ATTac-2000's adaptive hotel bidding strategy in a controlled manner,
we ran several game instances with ATTac-2000 playing against two variants of itself:
201

fiStone, Littman, Singh, & Kearns

1. High-bidder always computed G based on the current hotel prices (as opposed to
using priors and averages of past closing prices).
2. Low-bidder always computed G as in variant 1, but also only bid for hotel rooms at
$50 over the current ask price (as opposed to the marginal utility, which tended to be
more than $1000).
At the extremes, with ATTac-2000 and 7 high-bidders playing, at least one hotel price
skyrockets in every game since all agents bid very high for the hotel rooms. On the other
hand, with ATTac-2000 and 7 low-bidders playing, hotel prices never skyrocket since all
agents but ATTac-2000 bid close to the ask price. Our goal was to measure whether ATTac2000 could perform well in both extreme scenarios as well as various intermediate ones.
Table 5 summarizes our results.
#high agent 2
7 (14)
,
6 (87)
,
5 (84)
,
4 (48)
,
3 (21)
,
2 (282)
,

agent 3 agent 4 agent 5 agent 6 agent 7 agent 8

9526 |||||||||||||,!
10679 ||||||||||,!
1389
10310 |||||||,!
, 2650
10005 ||||,!
,|||| 4015
5067 ,!
,||||||| 3639
209
,|||||||||| 2710

Table 5: The difference between ATTac-2000's score and the score of each of the other
seven agents averaged over all games in a controlled experiment. All differences
are statistically significant at the 0:001 level, except the one marked in italics.
Each row corresponds to a different number of high-bidders (excluding ATTac2000 itself). The first column presents the number of high-bidders as well as the
number of experiments we ran for that scenario (in parentheses). The column
labeled \agent i" shows how much better ATTac-2000 did on average than agent i.
Scores above the stair-step line are for high-bidders (variant 1) and scores below
the line are for low-bidders (variant 2). Results for identical agents are averaged
to obtain a single average score difference for each type of agent in each row. In
all cases, ATTac-2000 beats the other agents.
Each row of Table 5 corresponds to a different number of high-bidders in the game;
for example, the row labeled with 4 high-bidders corresponds to ATTac-2000 playing with
4 copies of variant 1 and 3 copies of variant 2. Results for identical agents are averaged
to obtain a single average score difference for each type of agent in each row. In the first
column, we also show in parentheses the number of games played for the results in each
row|each row reects a different number of runs. In all cases, we ran enough game instances
to achieve statistically significant results. However, in some cases we ran more instances
than turned out to be required. The column labeled agent i shows the difference between
ATTac-2000's score and the score of agent i averaged over all games. In all scenarios, these
202

fiATTac-2000: An Adaptive Autonomous Bidding Agent

differences are positive, showing that ATTac-2000 outscored all other agents on average.5
Statistical significance was computed from paired T-tests; all results are significant at the
0:001 level except for the one marked in italics. As mentioned before, if the number of
high-bidders is greater than or equal to 3, we expect the price for contentious hotels to rise,
and in all such scenarios ATTac-2000 significantly outperforms all the other agents. The
large score differences appearing in the top rows of Table 5 are mainly due to the fact that
the other agents get large, negative scores since they end up buying many expensive hotel
rooms.
In these experiments, ATTac-2000 always uses its adaptive hotel price expectations, even
when there are only 2 high-bidders. In the last row, when the number of high-bidders is 2,
very little bidding up of hotel prices is expected and in this case, we do not get statistical
significance relative to the two high-bidders (agent 2 and agent 3), since their strategies are
nearly identical to ATTac-2000's in this case. We do get high statistical significance relative
to all the other agents (copies of variant 2), however. Thus, ATTac-2000's adaptivity to
hotel prices seems to help a lot when hotel prices do skyrocket and does not seem to
prevent ATTac-2000 from winning on average when they don't.
The results of Table 5 provide strong evidence for ATTac-2000's ability to adapt robustly
to varying number of competing agents that bid up hotel prices near the end of the game.
Note that ATTac-2000 is not designed to perform well against itself. If 8 copies of ATTac2000 play against each other repeatedly, they will all favor the same hotel rooms and thus
consistently all get large negative scores. It would be interesting to determine whether there
exists a strategy that is both harmful to ATTac and beneficial to the adversary.

5. Related Work
Although there has been a good deal of research on auction theory, especially from the perspective of auction mechanisms (Klemperer, 1999), studies of autonomous bidding agents
and their interactions are relatively few and recent. TAC is one example. FM97.6 is another auction test-bed, which is based on fishmarket auctions (Rodriguez-Aguilar, Martin,
Noriega, Garcia, & Sierra, 2001). Automatic bidding agents have also been created in this
domain (Gimenez-Funes, Godo, Rodriguez-Aguiolar, & Garcia-Calves, 1998). There have
been a number of studies of agents bidding for a single good in multiple auctions (Ito,
Fukuta, Shintani, & Sycara, 2000; Anthony, Hall, Dang, & Jennings, ; Preist, Bartolini, &
Phillips, 2001). Outside of, but related to, the auction scenario, automatic shopping and
pricing agents for internet commerce have been studied within a simplified model (Greenwald & Kephart, 1999).
Twenty-two agents from 6 countries entered TAC, 12 of which qualified to compete
in the semi-finals and finals in Boston. The designs of these agents were motivated by a
wide variety of research interests including machine learning, artificial life, experimental
economics, real-time systems, and choice theory (Greenwald & Stone, 2001).
Our own approach was motivated by our research interests in multiagent learning (Littman,
1994; Stone, 2000; Singh, Kearns, & Mansour, 2000). Based on the problem description,
we expected to find several learning opportunities in the domain. As noted above, detailed
5. In general, ATTac-2000's average score decreased with increasing numbers of high-bidders, as games
became more volatile.

203

fiStone, Littman, Singh, & Kearns

opponent modeling was precluded by the system dynamics. Nonetheless, ATTac-2000's
adaptivity is one of the keys to its success, particularly in avoiding skyrocketing hotels.
The 2nd and 3rd place agents both used a different strategy to prepare for the possibility
of skyrocketing hotels. Rather than avoiding popular hotels entirely by tracking closing
prices across game instances, they both discouraged their agents from bidding for too many
of any particular hotel room, thus spreading their demand across the rooms (Greenwald &
Stone, 2001). While such a strategy is safer in the limit (i.e., it continues to work even if
everyone uses it), it has a greater potential to cost the agent in the event that hotel prices
do not skyrocket, since the agent will still distribute its demand to the less desirable rooms.
On the other hand, ATTac-2000 would notice that the prices are not skyrocketing and thus
bid for the optimal travel packages given current prices.

6. Conclusion and Future Work

TAC-2000 was the first autonomous bidding agent competition. While it was a very successful event, some minor improvements would increase its interest from a multiagent learning
perspective.

 Currently, there is no incentive to buy airline tickets until the end of the game. Were

the price of ights to tend to increase, or were supply limited, agents would have to
balance the advantage of keeping their options open against the savings of committing
to travel packages earlier6 .

 The information structure of the TAC setup was such that it was impossible to observe

the bidding patterns of individual agents during games. Nonetheless, the strategic
behavior of individual agents often profoundly affected market dynamics|particularly
in the hotel auctions. It seems that it would be beneficial to be able to directly observe
the behavior of each individual agent. Were there to be information available regarding
the bidding behavior of the agents during the game (such that other agents could infer
clients' preferences, and therefore market supply, demand, and prices), TAC agents
would potentially be able to learn to predict market behavior as a game proceeds.

With or without these modifications, we hope to be able to participate in future TACs,
with the goal of adding additional adaptive elements to ATTac-2000.
Another direction of future research is to apply the lessons learned from TAC to real
simultaneous interacting auctions. It is straightforward to write bidding agents to participate in on-line auctions for a single good if the value to the client is fixed ahead of time:
the agent can bid slightly over the ask price until the auction closes or the price exceeds
the value. However, when the values of multiple goods interact, such as is the case in TAC,
agent deployment is not nearly so straightforward.
One such real application is the Federal Communications Commission's auctioning off
of radio spectrum (Weber, 1997; Cramton, 1997). Especially for companies that are trying
to achieve national coverage, the values of the different licenses interact in complex ways.
Perhaps autonomous bidding agents will be able to affect bidding strategies in such future
6. This change has been adopted in the specification of TAC-01.

204

fiATTac-2000: An Adaptive Autonomous Bidding Agent

auctions. Indeed, in related research we have begun down this path by creating straightforward bidding agents in a realistic FCC Auction Simulator (Csirik, Littman, Singh, &
Stone, 2001).
In a more obvious application, an extended version of ATTac-2000 could potentially
become useful to real travel agents, or to end users who wish to create their own travel
packages.

Acknowledgements
We would like to thank the TAC team at the University of Michigan, including Michael Wellman, Peter Wurman, Kevin O'Malley, Daniel Reeves, and William Walsh, for constructing
the TAC server and responding promptly and cordially to our many requests while conducting the research reported here. We would also thank the anonymous reviewers for their
helpful comments and suggestions.

References

Anthony, P., Hall, W., Dang, V. D., & Jennings, N. R. Autonomous agents for participating
in multiple on-line auctions..
Cramton, P. C. (1997). The FCC spectrum auctions: An early assessment. Journal of
Economics and Management Strategy, 6 (3), 431{495.
Csirik, J. A., Littman, M. L., Singh, S., & Stone, P. (2001). FAucS: An FCC spectrum auction simulator for autonomous bidding agents. In Proceedings of the Second International Workshop on Electronic Commerce. To appear. Available at
http://www.research.att.com/~pstone/papers.html.
Eisenberg, A. (2000). In online auctions of the future, it'll be bot vs. bot vs. bot. The New
York Times. August 17th.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the
Theory of NP-completeness. Freeman, San Francisco, CA.
Gimenez-Funes, E., Godo, L., Rodriguez-Aguiolar, J. A., & Garcia-Calves, P. (1998). Designing bidding strategies for trading agents in electronic auctions. In Proceedings of
the Third International Conference on Multi-Agent Systems, pp. 136{143.
Greenwald, A., Boyan, J., Kirby, R. M., & Reiter, J. (2001). Bidding algorithms for simultaneous auctions. In Proceedings of Third ACM Conference on E-Commerce, p. to
appear.
Greenwald, A., & Kephart, J. O. (1999). Shopbots and pricebots. In Proceedings of the
Sixteenth International Joint Conference on Artificial Intelligence, pp. 506{511.
Greenwald, A., & Stone, P. (2001). Autonomous bidding agents in the trading agent competition. IEEE Internet Computing, 5 (2), 52{60.
205

fiStone, Littman, Singh, & Kearns

Ito, T., Fukuta, N., Shintani, T., & Sycara, K. (2000). Biddingbot: a multiagent support
system for cooperative bidding in multiple auctions. In Proceedings of the Fourth
International Conference on MultiAgent Systems, pp. 399{400.
Klemperer, P. (1999). Auction theory: A guide to the literature. Journal of Economic
Surveys, 13 (3), 227{86.
Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In Proceedings of the Eleventh International Conference on Machine Learning,
pp. 157{163 San Mateo, CA. Morgan Kaufman.
Preist, C., Bartolini, C., & Phillips, I. (2001). Algorithm design for agents which participate in multiple simultaneous auctions. In Agent Mediated Electronic Commerce III
(LNAI), pp. 139{154. Springer-Verlag, Berlin.
Rodriguez-Aguilar, J. A., Martin, F. J., Noriega, P., Garcia, P., & Sierra, C. (2001). Towards
a test-bed for trading agents in electronic auction markets. AI Communications. In
press. Available at http://sinera.iiia.csic.es/~pablo/pncve.html.
Singh, S., Kearns, M., & Mansour, Y. (2000). Nash convergence of gradient dynamics in
general sum games. In Proceedings of the Sixteenth Conference on Uncertainty in
Artificial Intelligence (UAI), pp. 541{548.
Stone, P. (2000). Layered Learning in Multiagent Systems: A Winning Approach to Robotic
Soccer. MIT Press.
Weber, R. J. (1997). Making more from less: Strategic demand reduction in the FCC
spectrum auctions. Journal of Economics and Management Strategy, 6 (3), 529{548.
Wellman, M. P., Wurman, P. R., O'Malley, K., Bangera, R., Lin, S.-d., Reeves, D., & Walsh,
W. E. (2001). A trading agent competition. IEEE Internet Computing, 5 (2), 43{51.

206

fiJournal of Artificial Intelligence Research 15 (2001) 383-389

Submitted 6/01; published 11/01

Research Note

Finding a Path is Harder than Finding a Tree

Christopher Meek

meek@microsoft.com

Microsoft Research,
Redmond, WA 98052-6399 USA

Abstract

I consider the problem of learning an optimal path graphical model from data and show
the problem to be NP-hard for the maximum likelihood and minimum description length
approaches and a Bayesian approach. This hardness result holds despite the fact that the
problem is a restriction of the polynomially solvable problem of finding the optimal tree
graphical model.
1. Introduction

The problem of learning graphical models has received much attention within the Artificial Intelligence community. Graphical models are used to represent and approximate joint
distributions over sets of variables where the graphical structure of a graphical model represents the dependencies among the set of variables. The goal of learning a graphical model
is to learn both the graphical structure and the parameters of the approximate joint distribution from data. In this note, I present a negative hardness result on learning optimal
path graphical models.
Path graphical models are an interesting class of graphical models with respect to learning. This is due the fact that, in many situations, restricting attention to the class of path
models is justified on the basis of physical constraints or temporal relationships among the
variables. One example of this is the problem of identifying the relative positions of loci on
a segment of DNA (e.g., Boehnke, Lange & Cox, 1991). In addition, one might be interested
in obtaining a total order over a set of variables for other purposes such as visualization
(e.g., Ma & Hellerstein, 1999).
The main positive results on the hardness of learning graphical models are for learning
tree graphical models. These have been presented for maximum likelihood (ML) criterion
(Edmonds, 1967; Chow & Liu, 1968) and adapted to a Bayesian criterion by Heckerman,
Geiger, & Chickering (1995). Two NP-hardness results for learning graphical models have
appeared in the literature. Those are the NP-hardness of finding the optimal Bayesian
network structure with in-degree greater than or equal to two using a Bayesian optimality
criterion (Chickering, 1996) and the problem of finding the ML optimal polytree (Dasgupta,
1999).
In this note, I present a proof of the hardness of finding an optimal path graphical
models for the maximum likelihood (ML) criterion, the minimum description length (MDL)
criterion, and a Bayesian scoring criterion. Unlike the ML hardness result of Dasgupta, I
provide an explicit construction of a polynomial sized data set for the reduction and, unlike
the Bayesian hardness result of Chickering (1996), I use a common \uninformative" prior.

c 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiMeek

2. Optimal Graphical Models

One of the primary goals when learning a graphical model is to obtain an approximate joint
distribution over a set of variables from data. In this note, I focus on directed graphical
models for a set of discrete variables fX1 ; : : : ; X g. One component of a directed graphical
model is its directed graphical structure that describes dependencies between the variables.
A directed graphical model represents a family of distributions that factor according to the
graphical structure G of the directed graphical model, more specifically,
n

P (X1 ; : : : ; X ) =
G

n

Y P (X jpa
n

=1

i

G

(X ))
i

i

where pa (X ) denotes the possibly empty set of parents of vertex X in graph G. The
subscript G is omitted when it is clear from context. The most common methods guiding
the choice of a distribution from a family of distributions are maximum likelihood estimation
and Bayesian estimation. Given a graphical structure and a set of cases for the variables
(also a prior distribution over the distributions in the case of the Bayesian approach), these
methods provide an approximate joint distribution. For more details on graphical models
and estimation see Heckerman (1998).
This leaves open the question of how one should choose the appropriate graphical structure. In the remainder of this section, I present the maximum likelihood (ML) criterion, the
minimum discrimination length (MDL) criterion, and a Bayesian criterion for evaluating
directed graphical models given a set of cases D. A value of the variable X is denoted by
x and a value of the set of variables pa(X ) is denoted by pa(x ). The number of cases in
D in which X = x and pa(X ) = pa(x ) is denoted by N (x ; pa(x )) and the total number
of cases in D is denoted by N .
One important property common to these scoring criteria is that the scores factor according to the graphical structure of the model. That is, the score for a graph G and data
set D can be written as a sum of local scores for each of the variables
G

i

i

i

i

i

i

i

i

i

i

Score(G; D) =

i

i

X LocalScore(X ; pa(X )):
i

i

i

The local score for a variable X is only a function of the counts for X and pa(X ) in the
data set D and the number of possible assignments to the variables X and pa(X ). Thus
the structure of the graphical model determines which particular variables and counts are
needed in the computation of the local score for a variable.
The log maximum likelihood scoring criterion for a graphical model is
i

Score

ML

X LocalScore

(G; D) =

ML

i

i

i

i

(X ; pa(X ))
i

i

i

LocalScore

ML

(X ; pa(X )) = N  H (X jpa(X ))
i

i

D

i

i

(1)

where H (X jpa(X )) is the empirical conditional entropy of X given its parents, and is
equal to
N (x ; pa(x )) N (x ; pa(x ))
log
:
N
N (pa(x ))
( )
D

i

i

X

i

i

i

i

i

i

Xi ;pa Xi

384

fiFinding a Path is Harder than Finding a Tree

One practical shortcoming of the ML score is that in comparing two models with graphical
structure G and G0 where G contains a proper subset of the edges of G0 the ML score will
never favor G. Thus, when using an ML score to choose among models without restricting
the class of graphical structures, a fully connected structure is guaranteed to have a maximal
score. This is problematic due to the potential for poor generalization error when using the
resulting approximation. This problem is often called overfitting. When using this principle
it is best to restrict the class of alternative structures under consideration in some suitable
manner.
The minimum description length score can be viewed as a penalized version of the ML
score

Score

M DL

(G; D) = Score

d log N

(G; D)

X LocalScore
ML

=

2

M DL

(G; D)

i

LocalScore

M DL

(X ; pa(X )) =
i

i

#(pa(X ))  (#(X )
2

LocalScore

i

ML

P

i

1)  log N

(2)

where d = (#(pa(X ))  (#(X ) 1)) and #(Y ) is used to denote the number of possible
distinct assignments for a set of variables Y and the number of assignments for the empty
set of variables is #(;) = 1. The penalty term leads to more parsimonious models, thus,
alleviating the overfitting problem described above.
Finally, a Bayesian score requires a prior over the alternative models and, for each model,
a prior over the distributions. A commonly used family of priors for directed graphical models is described by Cooper & Herskovits (1992). In their approach, one assumes a uniform
prior on alternative graphs, P (G) / 1, and an \uninformative" prior over distributions.
These assumptions lead to the following scoring function;
i

i

Score

i

Bayes

(G; D) = log P (DjG) + log P (G)
/
LocalScore
(X ; pa(X ))

X

Bayes

i

i

i

LocalScore

Bayes

(X ; pa(X )) =
i

log

Y

i

(

pa xi

(#(X ) 1)!
(#(
X
)
1) + N (pa(x )))!
)
i

i

i

Y N (x ; pa(x ))!
i

i

(3)

xi

Although not as apparent as in the MDL score, the Bayesian score also has a built-in
tendency for parsimony that alleviates the problems of overfitting. The hardness results
presented below can be extended to a variety of alternative types of priors including the
BDe prior with an empty prior model (see Heckerman et al. 1995).
The problem of finding the optimal directed graphical model for a given class of structures G and data D is the problem of finding the structure G 2 G that maximizes Score(G; D).
385

fiMeek

3. NP-Hardness of Finding Optimal Paths

In this section, I consider the problem of finding the optimal directed graphical model
when the class of structures is restricted to be paths. A directed graphical structure is a
path if there is one vertex with in-degree zero and all other vertices have in-degree one. I
show that the problem of finding the optimal path directed graphical model is NP-hard for
the commonly used scoring functions described Section 2. To demonstrate the hardness
of finding optimal paths the problem needs to be formulated as a decision problem. The
decision problem version of finding the optimal path directed graphical model is as follows
The optimal path (OP) decision problem: Is there a path graphical model with
score greater than or equal to k for data set D?
In this section I prove the following theorem.
Theorem 1 The optimal path problem is NP-Hard for the maximum likelihood score, the
minimum description length score and a Bayesian score.

To prove this, I reduce the Hamiltonian Path (HP) decision problem to the OP decision
problem.
The Hamiltonian path (HP) decision problem: Is there a Hamiltonian path in
an undirected graph G?
A Hamiltonian path for an undirected graph G is a non-repeating sequence of vertices
such that each vertex in G occurs on the path and for each pair of adjacent vertices in
the sequence there is an edge in G. Let the undirected graph G = hV; E i have vertex set
V = fX1 ; : : : ; X g and edge set E .
The HP decision problem is NP-complete. Loosely speaking, this means that the HP
decision problem is as computationally dicult as a variety of problems for which no known
algorithm exists that runs in time that is a polynomial function of the size of the input.
Theorem 1 indicates that the OP decision problem is at least as dicult as any NP-complete
problem. For more information about the HP decision problem and NP-completeness see
Garey & Johnson (1979).
I reduce the HP decision problem for G to the OP decision problem by constructing a
set of cases D with the following properties;
n

#(X ) = #(X )
i

(i)

j

LocalScore(X ; ;) = LocalScore(X ; ;) = 

(ii)

LocalScore(X ; fX g) 2 fff; fi g

(iii)

i

i

j

ff<fi

j

LocalScore(X ; fX g) = LocalScore(X ; fX g)

(iv)

LocalScore(X ; fX g) = fi iff fX ; X

(v)

j

i

i

i

j

i

386

j

j

g2E

fiFinding a Path is Harder than Finding a Tree

For such a data set, the problem of the existence of a Hamiltonian path is equivalent
to the existence of a path graphical model with score equal to k =  + (jV j 1)  fi
where jV j = n is the number of vertices in the undirected graph G. Thus, to reduce the
HP problem to the OP problem one needs to eciently construct a polynomial sized data
set with these properties. In other words, by such a construction, a general HP decision
problem can be transformed into an OP decision problem. Because the size of the input
to the OP problem is a polynomial function of the size of the input for the HP problem, if
one can find an algorithm solve the OP problem in polynomial time then all NP-complete
problems can be solved in polynomial time.
I construct a data set for graph G assuming that each variable is ternary to satisfy
condition (i). For each pair of vertices X and X (i < j ) for which there is an edge in G,
add the following 8 cases in which every variable X (k 6= i; j ) is zero.
i

j

k

X1 : : : X
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0

i

1

X X +1 : : : X
1
0:::0
1
0:::0
1
0:::0
1
0:::0
2
0:::0
2
0:::0
2
0:::0
2
0:::0
i

i

j

1

X

j

1
1
1
2
1
2
2
2

X +1 : : : X
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
j

n

For each pair of vertices X and X (i < j ) for which there is not an edge in G, add the
following 8 cases.
i

j

X1 : : : X
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0

i

1

X X +1 : : : X
1
0:::0
1
0:::0
1
0:::0
1
0:::0
2
0:::0
2
0:::0
2
0:::0
2
0:::0
i

i

j

1

X

j

1
1
2
2
1
1
2
2

X +1 : : : X
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
j

n

For a set of cases constructed as described above, the pairwise counts for a pair of variables
X and X connected by an edge in G are
i

j

X

i

X

j

0
1
2
0 4(n2 5n + 6) 4(n 2) 4(n 2)
1
4(n 2)
3
1
4(n 2)
1
3
2
387

fiMeek

The pairwise counts for a pair of variables X and X not connected by an edge in G are
i

j

X

i

X

j

0
1
2
0 4(n2 5n + 6) 4(n 2) 4(n 2)
1
4(n 2)
2
2
2
4(n 2)
2
2

Condition (ii) is satisfied because the marginal counts for each variable are identical. There
are two types of pairwise count tables, thus, there are at most two values for a given type
of pairwise LocalScore. By using the two pairwise count tables and Equations 1, 2, and 3,
one can easily verify that the local scores for the two tables satisfy condition (iii). It follows
from the symmetry in the two types of pairwise tables and condition (ii) that condition (iv)
is satisfied. It follows from the construction that condition (v) is satisfied. Furthermore,
the set of cases is eciently constructed and has a size which is polynomially bounded by
the size of the graph G proving the result.
4. Conclusion

In this note, I show that the problem of finding the optimal path graphical models is NPhard for a variety of common learning approaches. The negative result for learning optimal
path graphical models stands in contrast to the positive result on learning tree graphical
models. This hardness result highlights one potential source of the hardness. That is,
one can make an easy problem dicult by choosing an inappropriate subclass of models.
Perhaps, by carefully choosing a broader class of models than tree graphical models one can
identify interesting classes of graphical models for which the problem of finding an optimal
model is tractable.
Another interesting class of graphical models not described in this note is the class of
undirected graphical models (e.g., Lauritzen, 1996). The methods for learning undirected
graphical models are closely related to the methods described in Section 2. In fact, for the
case of undirected path models, the scoring formulas described in Section 2 are identical
for each of the common approaches. Therefore, the NP-hardness result for directed path
models presented in this note also applies to problem of learning undirected path models.
Finally, it is important to note that good heuristics exist for the problem of finding
weighted Hamiltonian paths (Karp & Held, 1971). These heuristics can be used to identify
good quality path models and rely on the fact that the optimal tree model can be easily
found and will have a score at least as large as any path model.
References

Boehnke, M., Lange, K., & Cox, D. (1991). Statistical methods for multipoint radiation
hybrid mapping. American Journal of Human Genetics, 49, 1174{1188.
Chickering, D. (1996). Learning Bayesian networks is NP-complete. In Fisher, D., & Lenz,
H. (Eds.), Learning from Data, pp. 121{130. Springer-Verlag.
Chow, C., & Liu, C. (1968). Approximating discrete probability distributions with dependence trees. IEEE Transactions on Information Theory, 14, 462{467.
388

fiFinding a Path is Harder than Finding a Tree

Cooper, G., & Herskovits, E. (1992). A Bayesian method for the induction of probabilistic
networks from data. Machine Learning, 9, 309{347.
Dasgupta, S. (1999). Learning polytrees. In Proceedings of the Fifteenth Conference on
Uncertainty in Artificial Intelligence, Stockholm, Sweden, pp. 134{141. Morgan Kaufmann.
Edmonds, J. (1967). Optimum branching. J. Res. NBS, 71B, 233{240.
Garey, M., & Johnson, D. (1979). Computers and intractability: A guide to the theory of
NP-completeness. W.H. Freeman, New York.
Heckerman, D. (1998). A tutorial on learning with Bayesian networks. In Jordan, M. (Ed.),
Learning in Graphical Models, pp. 301{354. Kluwer Academic Publishers.
Heckerman, D., Geiger, D., & Chickering, D. (1995). Learning Bayesian networks: The
combination of knowledge and statistical data. Machine Learning, 20, 197{243.
Karp, R., & Held, M. (1971). The traveling-salesman problem and minimum spanning trees:
Part ii. Mathematical Programming, 1, 6{25.
Lauritzen, S. (1996). Graphical Models. Oxford University Press.
Ma, S., & Hellerstein, J. (1999). Ordering categorical data to improve visualization. In
Proceedings of the IEEE Symposium on Information Visualization, pp. 15{17.

389

fi
	ff
fi 
			 ! #"$ % 
'&)( *,+.-//!(102-34657-8:9

;<=>?  @A4:B
/!(CED=	%&@F( /:B
/!(

GIHKJMLONQPSRAPST:HKUVRXWZY[LOLV\]H)R_^#`aPSHbYcUVRXLO`VH)\]RedgfihHFW<NQPST:HKU
TUkjQLVRXUOT:hS`mlnT:RXW:HFoFNVfih

prq2s]t#uwvAxKq2vy{z|q$}
K''?{:ffE_.::<: 

{
A,6E
,
':] 

~,2{~i{{F2ff

xKq}ffyprq$}  s#uwffVq$}y

7w7SF{<$

K''?{:ffE_.::<: 

{
A,6E
,
':] 

ZA]w
?Ew?6<16?OE617?17ff'
1E1!:<66
 F<:1EE
{ff1!{?w6E
6<11F1!6<

ff6X 
:<XE?76E1
X11:16<:)E1!:<)E6EE<
!  
:<QEF6Qff< i. 1.7612.1
 .E
 166676E1$SX1.76E1$)1.F1!1$KE11c1<X1
6_716ff6.766E11g._ !.6
 1
E6?EQ176<i !iEOE6<:!11Q :_?1O?.
1)
66F1Q6 1! <K1ffw6K<6K1<E1!11.E61
O61)1Q:1
Q.E<<AK66 AK<K<1<
6
:6F1F1? 1 
ESE1!:<66ff<6<QQ16w1< :)6<
1 
,
E<7A6KE
1! 1$E

7!1E
E





 


	fiff 	






 ff

 w
! 
  

"$#&%')(+*-,.%/,1023*
465+7189*
#:8;2<*
#&0=*->71(+0@?A*
237B%-CD7189E-0%,10%-2F*->G,10210%,.C.(H8;#:I$%715+,.%-4KJ%-#+L
5)%L-0NMG,1*O
CD0212186#+LAPQIRJSMUTWV
XY(+0ZCD*
?'),10(+0#&218;*
#=*->[%-#&%')(+*-,W%$862%-#8;?'\*-,171%-#]7S'&,.*^CD021286#@%-#]_=I$JM`23_a23730?cb
_-0d7R8;7Y8;2Z%-?*
#+LA71(&0e73*
5+L
(+0237<'&,1*-f)490?2Z8;#cCD*
?')5+71%718;*
#&%-4g4;8;#&L
5&8;23718;Cd2U%-#&hiI$JMV)"$CdCD*-,.h&86#+L73*
j 89,.237kPmln-oalTWp
qFrtsWu)v^w-xmsy=z{r}|z{~DWwax.~yez{~sH|dzW$dwxAsfizr]s-rsdx3dzs-1|Hx3ddx3drtW.w-r&
!s-z{r&zr]kddDxADzQ~NwmA|z{~Ds-Adz
&s-Qz{r^izrdwxWAs-Qzwr+yRxms-v+DxAv^sr.dz{r]i9.-z.swx
utv^wr[QzQWs{~.vawxdx!w~DwAdr)QzQw-xNDrtQzQz!~Az{rHvaA.u+.!s-Qzwrvas-va@x1D
Dzdx/wm/v+c|z{~D.w-^xW~dz{WsD;!wH|z{~DsfiDx1Dzs-iva/x3ddx3drtWisrt|yevadx3fiDy
|]dDxNz{r\Nva=z|Dr)QzQQw3kvaDrtQzQ

X (&0,10d>{0d,.0#&CD073*%-#H0#]71897_P0-VL+V9b%'&,1*
#&*
5&#tTR8;2BL-0#+0d,W%-4;49_Cd%-46490hH%-#}srtsu)v^wxb71(+00#]71897_
Y
73*R(&8;C.(71(+0%-#&%'t(+*-,k,10d>0d,.2=8;2k89712x3ddx3dr)Qb%-#&h71(&0'&,10dEa89*
5&2e,10d>{0d,.0#&CD073*71(+0/21%-?A00#]71897_
8;2$71(+0A%-#&%'t(+*-,2sr).W1|]Dr)QV+*-,k86#&2371%-#&CD0-bg8;#:71(+02371%730?A0#7}awvr
ks-/srsWu
ut;@mFGs~
var]
xW[Wb)71(+0e'&,1*
#&*
5&#va@8;271(+0k%-#)%')(+*-,<%-#&h71(&0k#+*
5&#+wvrH8;2Z71(&0e%-#730CD0h+0#]7dV
"$#%-#&%')(+*-,.86CY'&,1*-ft490?Cd%-#f\0Fh+021CD,.89f\0h/%-2U4;_^8;#&L=23*
?0dR(+0d,10$f[0d70d0#71(+0F,1021*
4;5+7189*
#%-#&h
71(+0FL-0#+0d,.%718;*
#*->%-#&%')(+*-,W%ab^71(&0$>*-,.?0d,730d,.?f\08;#&Lk71(+0Fh&8;21%f)f&,10dEa8;%718;#+Le*->S71(+0F,10d>0d,10#&CD0B%-#&h


t



U

G



6 fi

-//!(]w %% !@6X
@ :
 ]
!fi>i
 w=!	%&% 		&:%2% : @

fi

i{9  ] DA2

2{~i{

71(+0c4;%73730d,Af\08;#&L71(+0c%f&f),10dE^86%718;#+L:>{*-,.?*->$71(+0,10d>0d,10#&CD0c73*%-#0#]71897_-VXY()8;2k't%'[0d,N>*^Cd5)2302
0D+Cd4;5&2189E-04;_*
#`71(+0,.023*
4;5+7189*
#*->Y%-#&%')(+*-,.%%-#&h#&*-7k*
#71(&089,kL-0#+0d,W%7189*
#Vi"$#&%')(+*-,.%Cd%-#f\0
Cd4;%-212.8)0h8;#}?A%-#_h&89[0d,10#]7A%_a2dbYh+0d'\0#&h&86#+LH5+'\*
#71(+0i')%,.718;Cd5&4;%,CD,W89730d,.8;%*
#+0:C.(+*^*
2302/73*
0?')4;*_-VY0dL
%,Wh&8;#+L71(&0i0490?0#]771(&%7Cd%,1,.8;02A*
5+7A71(&0,10d>0d,10#&CD0P71(+0%-#&%'t(+*-,TWbZ>*-,0D+%-?NO
')490-btCd490%,$h)8;23718;#&CD718;*
#&2Z21(+*
5&46hf[0=?A%-h+0ef\0d7U0d0#:'&,1*
#+*
?8;#&%-4%-#&%')(&*-,.%abt%-h30CD7189E%-4%-#&%'t(+*-,.%ab
h+0Dt#)89730Ah+021CD,.8;'&7189*
#&2db*
#+0DO%-#&%')(+*-,W%ab215&,1>%-CD0DOCD*
5)#7@%-#&%')(+*-,.%abE-0d,1f)%-49O!')(+,.%-230A%-#)%')(+*-,.%abK%-#&h
718;?0/%-#&htfi*-,N49*^Cd%718;*
#,10d>0d,10#&CD02dVcXY()8;2B')%'\0d,e>*^Cd5)2302k*
#71(+0,.023*
4;5+7189*
#*->U'),1*
#+*
?A8;#)%-4%-#&h
%-h30CD7189E%-4g%-#&%')(+*-,W%aV (
 7/8;2A<8;h+049_%L-,10d0h71(&%771(+0'&,1*aCD0212*->B,1021*
49E^86#+L%-#&%')(+*-,W%8;##&%715&,.%-4Y4;%-#+L
5)%L-0730Da712
?A%_if\0@215&'&'\*-,1730hif^_%/E%,.8;0d7_i*->G2173,.%730dL
8902F71(&%7$0?')49*fi_h&8\0d,10#]7<a8;#&h&2<*->a#+*<4;0h+L-0-VB_
h&8\0d,10#]7<a8;#&h&2Y*->^#+*fi<490h+L-0=0N?0%-#71(+0kE%,.89*
5)2<23*
5+,WCD02<*->G86#+>{*-,W?A%7189*
#i5&2.5&%-4;49_0?')49*fi_-0h
>*-,%-#&%')(+*-,.%F,.023*
4;5+7189*
#gb
8;#&Cd4;5&h)8;#+L<?*-,.')(+*
49*-L
8;Cd%-4a%L-,10d0?A0#7dba23_a#71%-CD718;CY')%,.%-4;490468;21?cb230?A%-#]718;C
8;#+>*-,.?A%718;*
#b+h&8;21CD*
5+,W230B2373,.5&CD715+,.0-b+73*-')8;Cd%-4a#+*R490h+L-0-b)%-#&h23**
#V
I$%715+,.%-44;%-#+L
5&%L-0/'&,1*aCD021218;#+LPQI$JMTWbS%-#&hgb23'\0Cd8[Cd%-4;49_-b%-#&%'t(+*-,.%c,1021*
4;5+7189*
#b5&2302=?A%-#_
,1023*
5&,.CD02<%-#&h23*
5+,.CD02<*->8;#+>*-,.?A%7189*
#>*-,$7U*,10%-23*
#)2dpFPmlTY#]5)?0d,1*
5&2<,1023*
5+,.CD02R%,10=%E%-864;%f)490
73*71(&0=21Cd890#]718tC=CD*
?A?@5&#&897_t[%-#&h`P
T<(]5)?A%-#&2Y0?A')49*_c?A%-#]_i23*
5+,WCD02<*->8;#&>{*-,.?%7189*
#i8;#c*-,Wh+0d,
73*A,1021*
49E-0kh&8\0d,10#]7Y4;8;#+L
5)8;23718;C<')(+0#+*
?0#)%aV
 0Y'),10230#]7K%-#%-49L-*-,.8971(&?71(&%7CD*^*-,.h&86#&%7302h&8\0d,10#]7>*-,.?A2K*->[a#+*<4;0h+L-0f]_@h&8;23718;#&L
5&8;21(&86#+L
f\0d7U0d0#4;8;#+L
5&8623718;C@^#&*<490h&L-0:PQCD*
#&2373,W%-8;#712@%-#&h`'&,10d>0d,10#&CD02WTe%-#&hh)8;%-49*-L
5+0DO2373,W5&CD715+,10Aa#+*fi<4O
0h+L-0PQ%-#&%'t(+*-,.8;C@%-CdCD0212189ft8;4;897_23')%-CD0fiTWV@XY(+0%-49L-*-,.8971()?86h+0#7189)02$71(+0#+*
5&#:')(+,.%-210=73*c<()8;C.(%
71(&89,WhaO!'\0d,.23*
#'\0d,.21*
#&%-4G*-,h+0?A*
#&2373,.%7189E-0'&,.*
#+*
5&#`*-,N%-hm0CD7189E%-4%-#&%')(+*-, - ,10d>0d,.2N8;#%^')%-#aO
8;21(h&86%-49*-L
5+0-V  0ACd%-4;471(&862$%-4;L-*-,.8971(&?"$<F8YPQ%-#&%')(+*-,W%/,.023*
4;5+7189*
#8;#h&86%-49*-L
5+02WTWVk"$<F8K%-2
8;?'t490?0#]730hc8;#cM,1*
4;*-L+V
 #^0CD7189*
#f\049*ebU0'&,10230#]7N,104;%730hU*-,1*
#%-#&%'t(+*-,.%:,1023*
4;5&7189*
#`86#h)8;%-49*-L
5+02dV  #
^0CD7189*
#^bU0215+L-L-0217=%-#%-#)#+*-71%7189*
#2.C.(+0?0>{*-,kCd%')715+,.8;#+La')%-#&8;21(h&86%-49*-L
5+0A2373,.5)CD715+,10-V  #
^0CD7189*
#N+b
%-#%-CdCD02.2189f)8;46897_e23't%-CD0Uft%-230hN*
#@71(&8;2K%-#&#+*-71%7189*
#2.C.(+0?0Z8;2Kh+0D[#+0hV  #A^0CD7189*
#A^b-U0
'&,10210#7R71(+0@%-4;L-*-,.8971(&?"RR$8!V<K8;#&%-4;49_-b[%-#0Da'\0d,.8;?0#]71%-421715&h+_*->71(+0@%-49L-*-,.8971(&?8;2<'&,10230#]730h
8;#^0CD718;*
#^V
K) {   
  


 [ U

I

    i- 

 G  g 


+*-,G%-#)%')(+*-,.%F,1021*
4;5+7189*
#N8;#Nh&8;%-4;*-L
5+02db
%F'&,1*
4689>{0d,W%7189*
#k*->[?0d71(+*ah&2Kf)%-230hN*
#h&8;%-49*-L
5+02373,.5)CD715+,10
PQh&8;2.CD*
5+,.230DO!*-,.8;0#730h/%'&'&,1*
%-CW(+02WTG(&%E-0$f\0d0#h+0dE-049*-'\0hV"$?*
#+L@71(+0230-b^0$21(+*
5&46hA4;89-0Y73*@023'\0DO
Cd8;%-4;4;_N%-C.^#&*<490h&L-0$71(&0<U*-,1A*->S$,1*
21NPmln
-]bgln-oalTWb&8;#<(&8;CW(A71(+0F8;#a[5+0#&CD0Y*->gh&86%-49*-L
5+0R2173,.5&CO
715+,108;#H%-#)%')(+*-,.%,1023*
465+7189*
#8;215&23718)0hgVN$,1*
21-2B*-,.>*^Cd5)2302B23'\0Cd8tCd%-4;4;_i*
#71%-23]O!*-,.890#]730h
h&8;%-4;*-L
5+02dV$71(&0d,k23715&h&8;02db215&CW(%-2k71(+*
230A')5&f)4;8;21(&0hf^_$,1*
23:$s-Pmln-o-^bln-n-
TWb'&,10230#]7k%
CD0#]730d,.8;#+L>,.%-?0dU*-,1%-2%?*^h&0473*0Da')4;%-8;#71(+0CD*
(+0d,10#&CD0i*->B49*aCd%-4Yh&8621CD*
5+,.230i210dL
?0#712/8;#
<(&86C.(71(+021'[0%-0d,2e>*^Cd5&2F*->Z%73730#]7189*
#8;2F,104;%730hH73*,10d>0d,1,.8;#+L0Da'&,10212.89*
#&2dVXY(&862B?*^h&04(&%-2
%-C.()890dE-0hc215&CdCD02121>5&4),10215&4;7128;#%-#&%')(&*-,.%k,.023*
4;5+7189*
#/8;#?*
#+*
49*-L
5+02b]f)5&7U*
5&4;h,10]5)89,10RCD0d,.71%-8;#
?*ah&8tCd%718;*
#&2$73*if\0N2.5&CdCD02123>Q5&4;49_%'&')4;890h:73*ih&86%-49*-L
5+02dV"$49*
#+L71(+*
230468;#+02dbU_],.*
#%-#&h^730#]7
&NY1Yfi3.UmDm=g!A!ZW[m!WWgWfiW-mmWY!N.-31!3	ZQff
!fi
 fifiRBW WW!! 3 i3. 
 -k! 3/!AW!fi
  R 

  d!/ c!fi= W Wm$[=. .!3
 WfiW

  fiA m!WW Wfi.U
  = 
 !A "fi ..$g!Y![W
 #$&%U Z!3(
 'fiW
fi Zfi )%*,+-#/.0#13254!%ff+6%87ff9-#$%;:=<1
?>

fi@ 2~,7



2	AK$)B

$CA



2B2{EDX<{$7

 G
 F





7BEHK.{JI22

Pmln-n-o
T(&%E-0h&0dE-049*-'\0hk0Da730#&2189*
#)2*->+71(+0CD0#]730d,.8;#+LF?0d71(+*ahk>*-,K%'&')4;8;Cd%718;*
#e73*Fh&8;%-49*-L
5+02VSXY(+0d_
CD*
#&Cd4;5)h+0B71(&%7<CD0#]730d,.8;#+LA862Y%-2YCD*
#&218623730#7R8;#ch&8;%-49*-L
5&02Y%-2Y897Y8;2<8;#c?*
#+*
49*-L
5&02dV
IR0dE-0d,171(+04902.2dbS%-CdCD*-,.h)8;#+Lc73*^73,.5&f[0N%-#&h j %-(&#}Pmln-n-n
TWb71(+0ACD,.5&Cd86%-4K'[*
86#7F*->U71(+0ACD0#730d,W8;#+L
?*ah+048;2e71(+0Cd%-#&h&86h&%7304;8;217dV$,1*
23dFsPmln-n-
T=2371%730/71(&%7k71(&862e4;8;237B?A%_Hf\0*-,.h+0d,10h5)218;#+L
h&8\0d,10#]7=>Q%-CD73*-,.2dbf)5+7@71(+0d_*
#)49_5&230c8;#&>{*-,.?%7189*
#`%f\*
5+7@L-,.%-?A?%718;Cd%-4,1*
4902dV j *U0dE-0d,bZ897N8;2
h&;8 KCd5&497F73*h+0Dt#&0AL-,.%-?A?A%7186Cd%-4,1*
4902k8;#H>,10d0DO!*-,WhaO!*-,.h+0d,k46%-#+L
5&%L-02@4;89-0/$0d,.?%-#*-,@^'t%-#&8;21(
<8971(&*
5+7Z5&218;#+L230?%-#718;Ck8;#+>*-,.?A%7189*
#V 4
F#71(+0*-71(+0d,@()%-#&hb*-,1Cd%,.,.890h*
5&7kf^G
_ LGC1-0d,.7N%-#)h^73,.5&f[0:Pmln-n-n
Te*
M
# LG#&L
4;8;21(h&0d71%-8;4;2
%?0d71(+*ah>{*-,,.023*
49Ea8;#+L'),1*
#+*
?A8;#)%-4%-#&%'t(+*-,.%8;#h&8;%-49*-L
5&02=R8971(`%'),10Cd8;2189*
#`*->$-^V  N %-#&h
%,10Cd%-4;4,W%730i*->F-o^V  NVXY(&8;2N?0d71(+*ah8;2Nf)%-210h*
#71(+0ih&8;2173,.89f)5+718;*
#*->Fh&8;%-49*-L
5+0c%-CD712/%-2A%-#
%-49730d,.#)%7189E-0e73*A71(+0eCD0#]730d,.8;#&L?0d71(+*ahV
&5&,171(+0d,.?*-,.0-Pb O:%,1?7 R Q #&0dDOU%,WCD*dGs9Pmln-n-n
T0?')(&%-218;d0R71(+0B8;?A'[*-,.71%-#&CD0<*->h&8;21CD*
5+,W230DO!73*-')8;C
a#+*<4;0h+L-0k%-2<%CD*
?')490?0#]71%,1_c?0d71(+*ahi>*-,<%-#&%'t(+*-,.%,1021*
4;5+7189*
#i86#ch&8;%-49*-L
5+02<8;#c<(&86C.(c215&CW(
a#+*<4;0h+L-0e8;2Y#+0CD02.21%,1_>{*-,R49*
#+LOh&8;2171%-#&CD0e%-#&%')(+*-,W%N,1023*
4;5+718;*
#V
SK 




G ]{
 

 TMU  
 G  g 

ff

 
 

 w i

+*-,N215&CdCD02121>5&4G%-#&%')(+*-,.%i,1023*
4;5+718;*
#8;#h&86%-49*-L
5+02dbS0%-21215&?071(&%7k897k862e021230#]718;%-4G73*8;h+0#]7189>_
h&8;%-4;*-L
5+0R2373,W5&CD715+,10-VXY(+0d,.0d>{*-,10-b^U0R'&,.*-'[*
210R%-#%-#&#+*-71%7189*
#21CW(+0?0$>{*-,Y^')%-#)8;21(h)8;%-49*-L
5+02G71(&%7
8;2Rf)%-230h:*
#U*-,1Cd%,1,.8;0h*
5+7Ff^_:F%-4;46%,.h+*HPmln-n-
TWb<(+*%'&')468902Y71(+0N71(+0d*-,W8902$')5+7<>*-,1%,.h:f]_
a%-C.^2NdsPmln
fi]T<CD*
#&CD0d,.#&8;#&LPQCD*
#E-0d,.2.%7189*
#&%-4{TZ715+,.#aO!71%a8;#+L+V
 0i5&210%-#%-#&#&*-71%7189*
#21C.(&0?0cf)%-230h*
#71(+023071(+0d*-,W8902N>{*-,71(&,10d0c?A%-8;#`,10%-23*
#&2dV`8;,.237db
%-2B897F8;2F%L-0#+0d,.%-4K%'&'&,.*
%-C.(H73*ch&8;%-4;*-L
5+0@?A*^h+0468;#+L+b\897B8;2F%'&')4;8;Cd%ft490e73*i%-4;47_^'\02F*->h&86%-49*-L
5+02db
8;#&Cd465&h&8;#+L:f\*-71(71%-23]O!*-,.890#]730h%-#)h}8;#+>*-,.?A%718;*
#aO!,10d73,.890dE%-4O!*-,.8;0#730hh)8;%-49*-L
5+02dW
V VU*
#&230^5+0#]7149_-b
71(+0N5&230=*->G215&CW(%?*ah+04S%-2F%/f)%-218;2<>{*-,Bh+0dE-04;*-')8;#+L/*
5+,$%-#)%')(+*-,$,1023*
4;5+718;*
#i'&,1*aCD0h&5+,.0=%-4;49*fi<2
5&2B73*:%'&')49_71(+0A'&,.*^CD0h&5&,1073*%-#]_:7_]'\0A*->Zh+*
?%-8;#bg71(^5&2e*\0d,.8;#&Li%-#%-h+E%-#71%L-0*E-0d,@'&,1*aCD0DO
h&5+,.02kf)%-210h*
#h&8;2.CD*
5+,.230?*^h&04;2k23'\0Cd8[CA73*')%,1718;Cd5)4;%,=h+*
?%-8;#&2dVH^0CD*
#&hb71(&862=%-#&#&*-71%7189*
#
21CW(+0?0eCd%-#f\0R0%-2.8;49_%'&'t4;890hA73*A%-5&73*
?A%718;CF'&,1*aCD0212302U<8;71(+*
5+7?0d71%-4;8;#+L
5)8;23718;CRCD*
#&218;h+0d,W%7189*
#&2dV
"$4971(+*
5+L
(8;#*
5+,ZU*-,171(+0B%-#&#+*-71%7189*
#c71%-21/(&%-2f\0d0#'\0d,1>*-,.?0hf]_(&%-#&hba>*-,Yh&8;%-49*-L
5&0DO!f)%-230h
%'&')468;Cd%7189*
#&286#<(&8;CW(A*
5+,'&,1*aCD0h&5+,10Y?89L
(7f\0Y0?kf\0h&h&0hP0-VL+V9b&8;#Ah&86%-49*-L
5+0<?A%-#)%L-0?0#723_a2mO
730?A2WTWb%-#&#&*-71%7189*
#71%-21^2F?@5&237<f\0='[0d,.>{*-,.?A0hi%-5+73*
?A%718;Cd%-4649_-VRK8;#&%-4649_-b&U0=%-#730h:73*f)%-230=*
5+,
*R#'&,.*^CD0h&5&,10Y*
#23715&h&8902*->71(+0R8;#+t5+0#&CD0Y*->gh)8;%-49*-L
5+0<2173,.5&CD715+,10<*
#/%-#&%'t(+*-,.%e,.023*
4;5+7189*
#A71(&%7
U0d,10kCd%,1,.8;0h*
5+7Yf^_+*`Pmln-o
-TWbtR(+*
230k%'&'),1*
%-C.(b&86#715+,.#b&8;2ft%-230hc*
#i71()%7Y*->+%-C1a2@dsV
"$CdCD*-,.h&8;#&L73*c71(&0230N71(+0d*-,.8902db71(+0Nf)%-218;C@5&#&897$*->CD*
#]E-0d,.21%718;*
#8;2$71(+0Awdbg<(&86C.(8;#+>*-,.?A2
71(+0i4;8623730#+0d,%f\*
5+7A%-#%-CD7189*
#bU,10^5+0237db^5+023718;*
#bG0d71CX
V O*E-02/%,10Cd%,1,.8;0h*
5+7Nf]_?0%-#&2*->
+Dx3srt~WZV Y:"R#)hc5+73730d,.%-#&CD02R%,103*
8;#+0h73*-L-0d71(+0d,$73*Af\0CD*
?0Qaxr+~WV
a86#&CD0N*
5+,B*-,.:%-2eh+*
#&0A5&218;#+L23'\*--0#h&86%-49*-L
5+02F71(&%7e(&%-hf\0d0#H73,W%-#&21CD,.89f\0hb\715+,W#&2B%,10
%-#&#+*-71%730h8;#71(+0B730D^712<%-#&h5+73730d,.%-#&CD02Y%,.0eh+04;8;?A8;730hf^_71(+0e5)230F*->')5&#)CD715&%7189*
#?A%,1a2*-,Zf]_
71(+0@0#&h&2<*->715+,.#&2V<Y0%-h&86#+L/%')5&#&CD715&%718;*
#?A%,.PV9\b [
b ]9bSV9V9V T%-4;49*fi<2R5)2Y73*,10CD*-L
#&89d0@71(+0@0#&h
*->%-#i5+73730d,.%-#)CD0-VGXY(+0230k71%-23^2<h+*#+*-7Y%\0CD7<71(+0=%-#&%')(+*-,.%O!,.023*
4;5+7189*
#'),1*^CD02.2dV
^-._Y1! mW\!B`afi! Ab
!fi;\W?fi6WfiZ WmW ?Qm0\!Z?Q0\!W
3c-G!mm1D!me!<R 16 U!d B-m!!edfg6fi D6k
 W !mZ
 0dfg6fi m!W !mZ
 0]
 B3WUW
fi!-!.)Y
 16D
 !m \
h W8Y
 !  !mgW!U\.i6fi Wm!Q! g!!1!3
j lk?2m2=%"+-n$&opZ
% k<fi W K  WWd!<Yd!mGkqfi!fi{ .
 fi]
 W!WW r, m3.Ut

 !fi ."
a
 fi!1!sY16 fi !fimm.!K!mmt 3d uZ
 3g+

 -mD
 !fi
v .mm
?>fiff

fi

i{9  ] DA2

2{~i{

"$2Y%,102.5&497db+U0e'&,1*-'\*
230F71(+0e>*
4;49*fi<8;#+L%-#&#&*-71%7189*
#i21CW(+0?0e>*-,<h&8;%-4;*-L
5+0e2373,.5&CD715&,10-p
68 28;h+0#]718)0h`f]_%HC.(&%-#+L-0c*->F23'\0%-0d,A8;#71(+0ih&86%-49*-L
5+0-U0%-C.(}CW(&%-#+L-0c*->F23'\0%-0d,
'),10215+'&'\*
2302<%/#+0d715+,.#gVRF#i71(&862<'\*
8;#]7db)U0=?A%-0N%h&8;23718;#)CD7189*
#cf\0d7U0d0#:7U*h&89[0d,10#]7
a8;#&h)2*->S715+,W#&2dp

w t}ffsyx wbz

{

{

" # su}!|{usy{s t#}ffsWx0} w	z 8;2*
#+0R71()%7U%-h&h)28;#&>{*-,.?%7189*
#73*N71(+0Fh&8;%-49*-L
5+0-VGa5)C.(
$
715&,.#&2ACD*
#&217189715+730i<()%78;2Cd%-4;490hvaAutxz{Asx~~D wm.w-r&dx.~ds-QzQw-r&V^'\0%-0d,.2
5)23071(+089,c8;#]730d,1E-0#718;*
#&273*'&,1*Ea8;h+08;#+>*-,.?A%718;*
#71()%7>Q%-Cd8;4;8;71%730271(+0'&,1*-L-,10212*->
71(&073*-'t8;C*->$CD*
#E-0d,W21%7189*
#V  #]730d,1E-0#718;*
#&2?A%_`f\0 'sq$=|{u)~Xx0} wc!z <(+0#71(+0d_
>*-,.?@5&4;%7308;#]E^8971%718;*
#&2db,10^5&89,10?0#]712dbZ*\0d,.2dbY,.0d'[*-,.712dbZ0d71CV9bR*-, }uq$y{s~x0} wz
R(+0#71(+0d_%-#&23U0d,Z*-,Y0dE%-465&%730e71(+0F'&,10dEa89*
5&2Z21'[0%-0d, Q2Y8;#]730d,1E-0#718;*
#VK8;#&%-4;49_-ba71(+0d_
Cd%-#c%-4623*=f\0 z|= u) su}!|{usy{s~x0} w J z b^<(&8;CW(/862%@,10%-CD7189*
#71()%7f\0dL
8;#&2U%-2Z%
,.023'\*
#&230@73*c71(+0N'&,10dEa89*
5&2F23'\0%-0d,2B8;#]730d,1E-0#]7189*
#b%-#&h:0#&h)2$%-2k%-#H8;#73,.*^h&5)CD7189*
#*->
#&0d8;#+>*-,.?A%7189*
#gV
" ys's]t#'s t#}ffsx w	z ,10d'),10230#]712B%-#H0?'&7_715+,W#bg<(&86C.(8;2B^5&89730N7_^')8;Cd%-4*->
%4;8;23730#+0d,BR(+*
230A%-8;? 862B71(+0>*-,.?A%-4,1086#+>{*-,WCD0?0#7e%-#)hH,.%718tCd%718;*
#H*->71(+0ACd%-237k*->
CD*
#]E-0d,.21%718;*
#&%-4g,1*
4;02dVUa5&CW(c8;#]730d,1E-0#718;*
#&2<4;%-C.86#+>{*-,W?A%7189*
#V

x z PQ%-4;21*}Cd%-4;490h ur #q2s2u T8;2i%210]5+0#)CD0*->=715&,.#&2c(+0%-h+0hf]_ %-#
86#&89718;%7189*
#`8;#]730d,1E-0#718;*
#715+,W# P  X  Tk%-#&h0#)h+0hf^_%,10%-CD7189*
#8;#]730d,1E-0#]7189*
#715+,.# P  X  TWV
X<(&8;2>{*-,W? *->=%-#&%')(&*-,.%ab$86#R(&8;C.( 71(+0H,.0d>{0d,10#)CD0%')'[0%,W2<8971(&8;# %-# %-h1%-CD0#&CD_ ')%-89,b
%')'[0%,W2Z73*Af\0BE-0d,1_CD*
?A?*
#86#ch&8;%-49*-L
5+02=PQ+*\bSln-o
-TWV



)q$usq2}x 

VXY(+073*-'t8;C?@5&237gf\0%<4;0Da8;Cd%-489730? 71(&%78;2,10d>0d,1,10hB73*R>{,.0]5+0#]7149_-V"RCdCD*-,.h)8;#+L
73*Y*aC.()% Pmln-n-o
TWb$>{*
5&,>0%715+,102%,1071%-0# 8;#]73*`%-CdCD*
5)#7c8;#}71(+02304;0CD7189*
#}*->B71(&0f\0237
Cd%-#)h&8;h&%730>*-,e%ch&8;21CD*
5&,.230N73*-')8;CpF>,10]5&0#&CD_-bg0dE-0#h&862373,.89f)5&7189*
#b)'\*
218;7189*
#:*->G),W237F73*--0#b
%-#)h@210?A%-#7186C%-h+0^5&%-CD_-V"()89L
(&49_F>,10]5&0#70490?0#]7S71()%7*aCdCd5+,.2K8;#]730#&218;E-049_e8;#@%<')%-2.21%L-0
*->71(&0Rh&8;%-4;*-L
5+0Yf)5+7Uh+*^02G#+*-7U%'&'\0%,>*-,49*
#&L=2373,10d71CW(+028;2G#&*-74;89-04;_=73*@f\0<%eL-*^*^hC.(&*
8;CD0
>*-,h&8;21CD*
5&,.23073*-'t8;CV  #`71(+0c21%-?0U%_-bG#+08971(+0d,N8;2@%-#0490?0#]7@R(+*
230/),.237@%'&'\0%,.%-#&CD0
*aCdCd5+,.2$%4;*
#+LA%_i>,1*
? 71(&0kf\0dL
8;#&#&86#+LN71(+0kf\0237<CW(+*
8;CD0-q
V O*-,10d*fiE-0d,b230?A%-#]718;Ck%-h+0^5&%-CD_
?@5&237<f[0BCD*
#&2.8;h+0d,10h>*-,<71(+0eCd%-#)h&8;h&%730-b)%-#)hc897Y?=5)237Zf\0e%-2123021230hf]_71(+0k%-#)#+*-71%73*-,V

w y x wb xc} z

%-230h`*
#71(&0%f\*E-0DO?0#]7189*
#+0h2373,.5)CD715+,10-b71(+0#b71(+0>*
4;49*fi<8;#+Lc71%L
2@%,10/CD*
#)218;h+0d,10h#+0CO
02121%,._i>{*-,Bh&86%-49*-L
5+0=2173,.5&CD715+,10N%-#&#+*-71%718;*
#p } w b } w b  w b  x b\%-#&h wb xc} VtXY(&0@"$M %-#&h
XRFM  V71%L
2G<864;4^f[0Y5)230h73*=h&0Dt#+0Z71(+0<%-#)%')(+*-,.8;CZ%-CdCD02.2189f)8;46897_N23')%-CD0-b^%-#&hA71(+0<,10?A%-8;#&86#+LF71%L
2
<8;464
f\0Z5&230h@73*B*-f&71%-8;#@71(+0Z%-h1%-CD0#&CD_@')%-89,.2dVXY(+0  X  71%L+b],10d'&,10230#]718;#+LF?A8a0hN8;#]730d,1E-0#718;*
#&2db
8;2B#+*-7B8;#)Cd4;5&h+0h:218;#&CD0?A89^0hH8;#]730d,1E-0#]7189*
#&2BCd%-#Hf\0%-#&#+*-71%730h%-2  X  ')4;5)2  X  VNXY(&8;2$71%-238;2
h+*
#+0e86#71(+0k%-#&#+*-71%718;*
#c')(&%-230-V
"$#N0Da%-?A')490*->t%-#%-#&#&*-71%730hAh&8;%-49*-L
5&0<8;71(=71%L
2G8;2'&,10230#]730h8;#NK89L
5+,10BlV  #@71(+0Zh&86%-49*-L
5+0-b
71(+08;h+0#]718)0d,P!BMUT8;#&h&8;Cd%730271(+0U715+,.#e*->t%Y,.%-8;4;U%_eCD*
?')%-#]_e0?')49*fi_-0d0-b%-#&hk71(+0U86h+0#7189)0d,U-P B&T
8;#&h)8;Cd%730271(+0kCd4;8;0#7d2Z715&,.#V
F#&0*->+71(+0?A*
237S8;?A'[*-,.71%-#7%-h&E%-#]71%L-02K*->a71(&8;2%-#)#+*-71%7189*
#@21C.(+0?A08;2S89712CD*
?'t%7189f)8;4;8;7_$<8971(
?*
237*->+71(+0h&8;%-49*-L
5&0DO%-#&#+*-71%7189*
#@21C.(&0?02S5)230h=86#kh&8;%-4;*-L
5+0G23_a23730?A2dVKIR*-718;CD0-b->{*-,8;#)2371%-#&CD0-b71(&%7
71(+0%-h3%-CD0#&CD_H't%-89,.2B21(+*fi71(+0A2.%-?0/2373,W5&CD715+,10A%-2k71(+0.w-r&dx.~ds-QzQw-rtss%'&')46890h73*i71%-23]O
*-,.890#]730hAh&86%-49*-L
5+02h+0Dt#&0h8;#N71(+0<h&86%-49*-L
5+0Z2373,.5&CD715&,10Zf]b
_ V%,.4;0d7371%/dKs9tPmln-n
-TW
V O*-,10d*fiE-0d,b^*
5+,
?>i>

fi@ 2~,7



2	AK$)B



!P8
!Pp
 8

P

!  "J

! 

P

-

!P"J
!P-

)

!  "J
P,

!  8
! 

p

!P-

!P"J
!P-
 "J





2B2{EDX<{$7

 G
 F





7BEHK.{JI22

0;?
6/iZ

J



$CA

!  "J
! 

-

a/", _!()?!)i! 
6a;_Zi// ai/!ZP 
 &)?i 
3 :S<! 
 &
-!Z & 
e?&  !   05ie" &
 ?a upimis?ms
eg ?
-0?Z/Zrui&Ji/3iZc/?Z0a3_i=3a0!Z
iaa
 i;((_p i 0;(& q! _pi;!!
5igZ 0 //_ c3i s?i3a/= 
  ?upi\b p?m!!
5igi3c& /8 
b pimi!6i; =qc s(ffa &)u0a?;pc p 
cff& 
- /8/!=_ i(Z&//?i3a)i"a/"
ii&g a 
i! p (& eiZ qq,g, 
6/!a!\?i3a  !giP,3ca
q?8?! p cff& ffi5p!?m!a
-Zu0  0si&/ffii&0 = )a
 
5i 
c !u r
 /
 :#w! 
c !u pau
 ?ceg, 
=-&ip0&Z Zcagm

5i8
 !!q6&& 66sq, i6
=? /i ff8/i ff?0)(i?a 
,; ??
6/?i&ig
q(i ?qc66)ppZ!r
p0bZZg/ii!3_! a 

nWh\.m$W g fi3Yg!f j


8;L
5+,10AlpffLa%-?'t490F*->K%-#%-#&#+*-71%730hh)8;%-49*-L
5+0B>,1*
?
	wxut^~fiffWrdw&x3drRDxW~Dwr
?>

fi

i{9  ] DA2

2{~i{

21CW(+0?0R862G%-4;23*@CD*
?')%7189ft490Y<8971(A71(+*
230$71(&%7%,10Rf)%-230h/*
#5+73730d,W%-#&CD0R>Q5&#&CD718;*
#&2db215)C.(%-2G71(+0R*
#+0
h+0Dt#&0h8;#A<e
" OHaJf^_N"$4;490#/%-#&h V*-,10@Pmln-n
-TWVUYe
" OHaJ8;#&h)8;Cd%7302G(+*fi 5&73730d,.%-#&CD02%,10<,.04;%730h
73*71(+0Nh&8;21CD*
5+,W230kf^_?0%-#&2$*->SDwxGsx3|ksrt|:1s
dfiGsx3|wwfizr]@^r[Qzwr+~WVeX<(+0@86#730d,1'),10d71%7189*
#
*->71(+0230e>Q5&#&CD7189*
#)2f)5&8;4;h)2G71(+0e%-h3%-CD0#&CD_]O!')%-89,R2373,.5&CD715+,.0-V86#&%-4;49_-ba*
5+,Y73*-'t8;CB2373,.5&CD715&,10B0Da()89f)89712
71(+0F21%-?0F>0%715+,102%-271(+0NQxmsr&~Ds
QzQwrH2373,W5&CD715+,10R*-
> V%,.4;0d7371%Gs9a*-,71(+0!s~.N9ddgh+0Dt#+0h/f]_
"$4;490#c%-#&E
h V*-,10-V
 ]  
 
gZ#  
 ffU   fi,
%-230h5+'\*
#i71(+0N%f\*E-0DO?0#]7189*
#+0hH%-#)#+*-71%7189*
#b%-#%-#&%')(+*-,W8;Ck%-CdCD0212.89f)8;4;8;7_c23')%-CD0N8;2R'&,1*-'\*
230h
>*-,=^')%-#&8;2.(8;#:*-,.h&0d,B73*i,1023*
49E-0%-#&%'t(+*-,.2B8;#:71(+0>*-,.?*->U'[0d,W23*
#&%-4'&,1*
#+*
5)#&2dbh+0?*
#&2173,.%7189E-0
'&,1*
#&*
5&#&2db+%-#&hi%-hm0CD7189E%-4g%-#&%'t(+*-,.2dV
 





u)~E}ff/ys

"$CdCD*-,.h&8;#+L73*+* Pmln-o
-TWb71(+0At,.237e?0#]7189*
#*->Z%,10d>{0d,.0#7@8;#%i230^5+0#&CD0/*->ZCD*
#730Da712N8;2e'\0d,3O
>*-,.?0h<8971(%>Q5&4;4K#+*
5&#:')(&,.%-230-VN"<>730d,e71(&%7dbSf]_:5&218;#+L%-#%-#&%'t(+*-,B71(+0A23'\0%-0d,eh&8;21')4;%_a2F%-#
5&#&h&0d,.2371%-#&h&86#+L71(&%7@210]5+0#)CD0/(&%-2@#+*-7kf\0d0#`Cd49*
230hh+*<#gV  0%-21210d,17k71(&%7@7U*h&8\0d,10#7=230DO
^5+0#&CD02=L-0#+0d,.%730?*
237e*->Z71(+0%-#&%')(+*-,W2e73*f\0A>*
5&#&h8;#h&8;%-4;*-L
5+02dpk71(&0/%-h1%-CD0#&CD_H')%-89,@%-#&h
71(+0N73*-')8;C@21CD*-'\0-VeXY(+0@>*-,.?0d,FL-0#+0d,.%7302B,.0d>{0d,10#)CD02F73*%-#]_4;*^Cd%-4K#+*
5&#')(+,W%-230-b%-#&h:71(+04;%73730d,
L-0#+0d,.%7302R,10d>0d,10#&CD02Z73*/71(+0e?A%-8;#73*-'t8;CB*->71(+0kh&8;%-4;*-L
5+0-V
%-230h*
#i71(&8;2b+0e'),1*-'\*
230B71(&%7Y71(+0=%-#&%')(+*-,.86CB%-CdCD0212189f)864;897_23')%-CD0e>*-,R%-#]_L
89E-0#%-#)%')(+*-,
?A%_f\0eh+0D[#+0hc%-2Z71(+0k210d7Y*->#&*
5&#c')(+,W%-230271%-0#>,1*
?cp
{

71(&0k%-h1%-CD0#&CD_')%-89,YCD*
#]71%-8;#&8;#+L71(&0k%-#&%')(+*-,b+')4;5&2
{

71(&0k%-h1%-CD0#&CD_')%-89,Z'&,10CD0h)8;#+LN71(+0k%-h1%-CD0#&CD_')%-89,YCD*
#]71%-8;#&86#+L71(+0k%-#&%'t(+*-,b+')4;5)2
{

%-#]_c%-h1%-CD0#&CD_')%-89,Y8;#&Cd465&h&8;#+L=71(+0k%-h1%-CD0#&CD_')%-89,YCD*
#]71%-8;#&86#+L71(+0k%-#&%'t(+*-,b+')4;5)2
{

71(&0k#+*
5&#')(&,.%-230B,10d'&,.0230#7186#+LN71(+0k?A%-8;#73*-'t8;CF*->K71(+0kh&8;%-4;*-L
5+0-V


 

q2t y{z|qy uuwys
s

}yiy ~q2v

^ 0dE-0d,.%-4*-,.^2$%f\*
5+7<%-5+73*
?A%7186Ce73*-')8;C=h+0d730CD7189*
#:(&%E-0@f\0d0#c')5+ft4;8;21(+0h! Y0d_a#&%,ePmln-n-n
TWb#"*
5aO
?A%-#&2FPmln-nalTU*-, j 0%,.237ePmln-n]T$! V  # O%,17?R Q #+0dDO%,.CD*ds9Pmln-n-n
TU%-#%-5+73*
?%718;C$73*-')8;C$h+0d730CO
7189*
#i%-4;L-*-,.8971(&? %-2R%'&')4;890h73*%-#)%')(+*-,.%N,1021*
4;5+7189*
#c8;2Y'&,10230#]730hV
XY()8;2Z%-49L-*-,.8971()? 230490CD712<#&*
5&#')(+,.%-2102ePQIRMTU*^CdCd5&,1,.8;#+LNf\0d>*-,10e%-#i%-#&%')(&*-,VXY(+0230eI$MG2R%,10
8;#&Cd465&h+0hB8;#e%R4;8;23771(&%7S8;271(+0#eU089L
(]730h
V LG%-C.(@718;?A071(+0I$M:%'&'\0%,.28;#=%<#+0d`715+,.#P>,10^5+0#&CD_&TWb
89712UU089L
(7Y862U86#&CD,10%-230hba%-#&h0%-CW(c718;?0F71(+0BI$Mh+*^02U#&*-7%')'[0%,Y86#/%N#+0d 715+,W#PQ86#+>{,.0]5+0#)CD_+TWb
89712UU089L
(7Z8;2h+0CD,.0%-230hVG"$CdCD*-,.h&8;#&L=73*71(&862U%-4;L-*-,.8971(&?cb^71(+0Bh&8;%-4;*-L
5+0R73*-'t8;CF?A%_/f[0Fh+0d730d,.?8;#+0h
f^_:89712B2.%-4;890#&CD0-bg8V0-V9bf]_h+0d730d,.?A86#&8;#+L/71(+0AIRM<8971(:71(+0A(+0%E^8;0237B08;L
(7PQ(&8;L
(>{,10^5+0#&CD_:8;#H%
21(+*-,.7<h&8;2371%-#)CD0fiT*aCdCd5+,1,.8;#&Lf[0d>*-,10@%-#%-#&%')(&*-,V  #*-,.h+0d,<73**-f&71%-8;#71(&8;2Y86#+>{*-,W?A%7189*
#PU089L
(]7WTWb
71(+0k%-4;L-*-,.8971(&? 5&2102Z71(+0B>*
4;49*R8;#+LN7U*/CD*^0 K/Cd890#]712dp
{

&%
{



pGCD*]0KCd890#7<*->>,10^5+0#&CD_

 pGCD*^0K/Cd890#]7Y*->K8;#+>,10^5+0#&CD_
?>

fi@ 2~,7



2	AK$)B

$CA



2B2{EDX<{$7

 G
 F





7BEHK.{JI22

V % 8;#)CD,10%-2302B71(+0A2.%-4;890#&CD0N*->%c,10d>0d,1,.8;#+L0Da'&,1021218;*
#<(+0#71(+00#]71897_H%'&'\0%,.2B86#71(+0ACd5+,1,10#]7
;8 #]730d,1E-0#]7189*
#c715+,.#gVV  h&0CD,10%-2302Y71(&0e21%-4;890#&CD0B*->0Da'&,1021218;*
#&271(&%7<%'&'\0%,10hc86#'&,10dEa89*
5&2Z8;#]730d,3O
E-0#]7189*
#c715+,.#)2f)5+7Y#+*-7Y8;#71(&0eCd5+,1,10#]7Z*
#+0-b)8;#)h&8;Cd%718;#+LN%A49*
2.2*->K86?'\*-,171%-#&CD0-VU*-71(CD*^0K/Cd890#]712
*-f^E^89*
5)2149_i%\0CD7B71(+021%-4;890#&CD0@*->G0Da'&,102.2189*
#&2$8;#,10D)0CD718;#+L71(+089,F>,10^5+0#&CD_%-#&h:71(+08;,$h&862371%-#&CD0
>,1*
?71(&0RCd5+,1,.0#78;#]730d,1E-0#718;*
#715+,W#A<(+0d,.0<71(+0$%-#&%')(+*-,(&%-2f[0d0#/>*
5&#&hVKXY(+0R0D^'&,.0212189*
#<8971(
71(+0(&89L
(+0237@21%-4;890#)CD0A<8;4;4f[071(+0?A*
237k>Q%E-*-,10hCd%-#)h&8;h&%730%-#730CD0h+0#]7N*
#71(&0<(+*
4;0/4;8;217k%-#&h
71(+0d,10d>*-,10e71(&0e?*
237Y,104;0dE%-#]7Y73*-')8;CF>*-,Y71(+0=Cd5+,1,10#]7Y8;#]730d,1E-0#718;*
#c715+,.#V
XY()8;2%-5+73*
?A%718;CY73*-')8;CZh+0d730CD7189*
#/?0d71(+*ahA(&%-271(&0Z>{*
4;4;*<8;#&LF%-h&E%-#]71%L-0<*fiE-0d,*-71(+0d,?0d71(+*ah&2dp
897Fh+*^02$#&*-7R*-f&71%-86#:%218;#&L
490k73*-')86Cb[ft5+7R,W%71(+0d,B%/4;86237<*->73*-')8;C@Cd%-#&h&8;h&%7302$*-,.h&0d,10hf^_21%-46890#&CD0-V
XY(&%78;2U8;?'\*-,171%-#]7>*-,*
5+,%-#&%')(+*-,.%=,1023*
465+7189*
#23_^21730?f\0Cd%-5&210-ba8;>g71(+0F(&89L
(+0237mO!,W%-#+-0hCd%-#&h&8O
h&%730Fh+*^02#+*-7U>Q5&4t4;4^71(+0$,10490dE%-#]7UCD*
#&2173,.%-8;#]712db]71(&0#71(+0$#+0Da7U(&8;L
(+0237GCd%-#&h)8;h&%730FCd%-#f\0<73021730hV
h V  VGX<(+0230
 #&89718;%-4649_-b]E%-4;5+02Y*->l '5&#&89712Z%-#&hHlk5&#&897dba,1023'\0CD7189E-04;_-b&0d,.0k%-212189L
#&0h73* V % %-#&C
E%-4;5+02ZU0d,10k%,1,W89E-0hc%7Y0Da'[0d,W8;?0#]71%-4;49_-baf)5+7Z>Q5+,171(+0d,Y21715&h+_CD*
5&4;hc490%-hc73*/?*-,10e'&,.0Cd8;230FE%-465+02dV

 
!  *) 
   1 G  g  
 [U      ?
 #71(&8;2A210CD7189*
#b71(+0%-#&%'t(+*-,.%H,1021*
4;5+7189*
#%-49L-*-,W8971(&? f)%-210h*
#%`CD*
#&2373,.%-8;#]7%-#&h'&,10d>0d,10#&CD0
%'&'&,.*
%-C.(862'&,10230#]730hV
(K 

+,

Qys~E}q{s!~Oq2s

}u.- uw}us#ur~qg~q2s

qg#}y2q$a

y

q{sqg#y2}q}u)~Eyvty{s

"$CdCD*-,.h&8;#+L/73*/F%-(&490f %-/ C1Pmln-nalTWb71(+0d,10@%,10=%7$'&,10210#7R7U*/f)%-2.8;Ce%'&'&,.*
%-C.(+02F8;#i%-#)%')(+*-,.%,102mO
*
4;5+718;*
#p=PmlT<71(+0N73,.%-h&89718;*
#&%-4S%')'&,1*
%-C.(gb<(&8;CW(L-0#+0d,W%-4;49_h+0d'\0#&h)2<5+'\*
#:4;8;#+L
5&8623718;CBa#+*R490h+L-0-b
%-#&h P
T=71(+0ch&8;2.CD*
5+,.230DO!*-,.8;0#730h%'&'&,1*
%-CW(b86#<()8;C.(`71(+0,10230%,.CW(+0d,73,.8902@73*?*ah+04UCD*
?')490D
h&8;2.CD*
5+,.230B2373,.5)CD715+,102Y%-#&hi71(+0#i5&2302Y71(+0230k2373,W5&CD715+,10273*A,.023*
49E-0k%-#&%'t(+*-,.%aV
"$?*
#+L`71(+073,.%-h)897189*
#&%-4Z%'&'),1*
%-C.(+02bY71(+0iU*-,1*-
> O8;73-*E Pmln-n-o
TWb$U%-46h+<8;#Pmln-n
-TWb$%-#&h
+0d,1, %-Q #&h+0dAds9Pmln-n-n
TU%,10R%-4;4)f)%-230h*
#/%@CD*
?kft8;#&%7189*
#*->g4;8;#+L
5&8623718;CU^#&*<490h&L-0@PQ490D+8;Cd%-4b^?*-,3O
')(+*
4;*-L
8;Cd%-4b23_a#]71%-CD718;Cb%-#)htfi*-,=230?%-#718;CTF>{*-,=71(+0A,1023*
465+7189*
#*->Z%-#&%')(+*-,W%aVXY(+0230A%')'&,1*
%-C.(&02
%'&')4;_:4;86#+L
5&8;237186C=a#+*fi<490h+L-0-b8;#H71(&0%_*->YCD*
#&2173,.%-8;#]712k%-#&h'&,10d>0d,10#&CD02dbK>*
4;49*R8;#+Lc71(+0/*-,1
*-
> V%,1f\*
#+0464%-#&hU,1*fi<#Pmln-o-o
TR%-#&h<8;CW(%-#&hJ5+'\0d,.&*_Pmln-o-o
TWbg8;#cR(&8;C.(215&C.(:23_a23730?A2$%,10
'&,1*-'\*
230hc%-2R%N730C.(&#)8;]5&0e>{*-,RCD*
?kf)86#&8;#+L230dE-0d,.%-48;#+>*-,.?A%7189*
#c23*
5&,.CD02dV
XY(&0230/%'&'),1*
%-C.(+02%,.0/f)%-210hb8;#715)897189E-049_-b*
#71(&0/>*
4;49*fi<8;#+Li71(&,10d021730d')2dpPmlTkh+0Dt#&8;#&L%-#
%-#&%')(&*-,.8;CF%-CdCD0212189ft8;4;897_/23')%-CD0-bKP
T%'&')4;_^8;#&L@CD*
#&2173,.%-8;#]712db)%-#&hP
TZ%'&')49_a8;#+L@'&,10d>0d,10#&CD02dV
"CD*
#&2373,.%-86#7<%-#)h'&,10d>0d,10#&CD0k23_a23730? ?@5&237Yh&0Dt#+0-b+*
#c71(+0B*
#&0k(&%-#&hba71(+0=%-#&%')(+*-,.86C$%-CdCD023O
2189ft8;4;897_=23')%-CD0-VXY(&%7862db]897U?=5)237G*-f&71%-86#/%@4;8;237G<8971(%-464&71(+0$'\*
212189f)4;0YCd%-#&h&8;h&%730$%-#730CD0h&0#712dVZF#
71(+0F*-71(+0d,Y(&%-#)hba71(+0B23_^21730? ?=5&217U%-4;21*Nh&0Dt#+0$71(+0B730D^7Z230dL
?A0#712Y86#/R(&8;C.(/71(+0e%-#]730CD0h+0#]7<Cd%-#
f\0B>{*
5)#&hVXY(&862Z23730d'(&%-2R%L-,10%7R86?'\*-,171%-#&CD0F>{*-,R71(+0e,10?%-8;#&8;#+LN23730d')2R8;#71(+0e'),1*^CD02.2Zf\0Cd%-5&230
%h+0Dt#&89718;*
#c*->71(+0@%-#&%')(+*-,.86Ck%-CdCD021218;f)8;4;897_c23')%-CD0@71(&%7$862<73*^*#)%,1,1*,10215)49712<8;#71(+0=0DaCd4;5)2189*
#
*->E%-4;8;h%-#730CD0h+0#]712dVkJS89-0d<8;210-b[%h+0Dt#&8;7189*
#i*->71(&0@%-#&%'t(+*-,.8;C=%-CdCD0212189f)864;897_i21')%-CD0N71(&%7$862<73*^*
f&,1*
%-h,10215&4971286#4;%,1L-0Cd%-#)h&8;h&%730i4;8623712dbGR8971(%CD*-,1,1021'[*
#)h&8;#+LH8;#&CD,.0%-230i8;#71(+0i4689-04;8;(+*^*ah`*->
0d,1,1*
#&0d*
5&2N%-#&%')(+*-,.%:,1021*
4;5+7189*
#V $215&%-4;4;_-b%-#&%')(+*-,.%:,1021*
4;5+7189*
#23_a23730?A2Nf)%-230h*
#4;86#+L
5&8;237186C
a#+*<4;0h+L-0/PQ+0d,1, %-Q #&h+0d@0d7<%-4V9bSln-n-n
TRh+0D[#+0e%-#%-CdCD0212.89f)8;4;8;7_23')%-CD0=5)218;#+Lcr'&,10dEa89*
5&2<230#730#)CD02
73*A71(+0=%-#&%')(+*-,b+R(+0d,10r8;2E%,W8;%f)490e%-CdCD*-,.h)8;#+L73*A71(+0k^8;#)h/*->K71(+0e%-#)%')(+*-,.%aV
F#)CD0Y71(+0<4;86237K*->['\*
212.89f)490Cd%-#&h&8;h)%7302G8;2h+0D[#+0hb230dE-0d,W%-4)CD*
#&2373,W%-8;#712%,10<%'&'t4;890h8;#N*-,.h&0d,73*
,10?*fiE-0A8;#&CD*
?'t%7189f)490@%-#730CD0h+0#]712dVNXY(+0CD*
#&2373,W%-8;#7B23_a23730? CD*
#&2.8;23712$*->CD*
#&h)897189*
#&2$71(&%7B?@5&237
f\0@?A0d7db\%-#)h:Cd%-#)h&8;h&%7302$71(&%7Bh+*#+*-7$>5&49t4;471(+0210NCD*
#)h&897189*
#&2R<8;464g#+*-7Ff\0=CD*
#&2186h+0d,10h'[*
2.2189f)490
?>	

fi

i{9  ] DA2

2{~i{

%-#]730CD0h+0#712Y>{*-,U71(+0F%-#&%')(+*-,VJ0Da86Cd%-4b^?A*-,1')(+*
49*-L
86Cd%-4b]23_a#]71%-CD718;Cd%-4b+%-#&h230?A%-#]718;CF8;#+>*-,.?A%7189*
#
%,10e73,W%-h&897189*
#&%-4649_5&230h73*/h+0Dt#+0B71(+0=CD*
#&2373,.%-8;#]712dV
K8;#&%-4649_-bY%>{730d,c,.0?*Ea8;#+L8;#&CD*
?A')%7189f)490Cd%-#&h&8;h&%7302bR89>B71(+0,10?%-8;#&8;#+L`4;8;237CD*
#71%-8;#)2c?*-,10
71(&%-#*
#+0B%-#]730CD0h+0#]7db&'&,10d>0d,10#&CD02Z%,10B%'&'t4;890h8;#*-,.h+0d,73*AC.(+*^*
230e%N218;#&L
490R%-#]730CD0h+0#]7dV  #71(&8;2
Cd%-230-b+5&#&4689-0Y71(&%7U*->gCD*
#&2373,.%-86#712db^'&,10d>0d,10#&CD02U%,10F%-2123*aCd8;%730h<8971(/4;89-04;8;(&*]*ah49*U0d,71(&%-#:l '1',NV
V%-#&h)8;h&%7302B>Q5&4t464;8;#+L%i'&,10d>0d,10#&CD0-b71(&0#bS(&%E-0/%cL-,.0%730d,@4;8;-04;8;(+*^*^h*->f\08;#+L71(+0%-#]730CD0h+0#]7
71(&%-#71(+*
230#+*-7R>5&49t4;4;8;#&LN8;7dVRXY(+0='&,10d>0d,10#&CD0@23_^21730??@5&237$f\0=h+02.89L
#+0hif\0%,.86#+L8;#:?A8;#&hi71(&%7
*
#&49_*
#+0ACd%-#&h&86h&%730?=5&217B,10?A%-8;#%7B71(+00#&hgV@XY()8;2Rt#&%-4KCd%-#&h&8;h)%730<8;4;4Sf\0@'),1*-'\*
230h%-2B71(+0
%-#]730CD0h+0#7>*-,@71(+0%-#)%')(+*-,VJg0D+8;Cd%-4b?*-,1't(+*
49*-L
8;Cd%-4b23_a#]71%-CD718;Cb%-#&h230?A%-#]718;C8;#+>*-,.?A%7189*
#
%,10k5)215&%-4;49_/5&230hc8;#c*-,Wh+0d,Z73*/h+0D[#+0F71(+0e'&,10d>0d,10#&CD0e21_^23730?iV
XY(&0$U*-,1a2Z*-
> O8973-*fiEPmln-n-o
TZ%-#&hi&0d,1, %-Q #&h+0d/s9Pmln-n-n
TY21(&* 71(&%7<%-#)%')(+*-,.%@,1023*
465+7189*
#
23_a23730?A2cft%-230h *
#CD*
#&2173,.%-8;#]712i%-#&h '&,10d>0d,10#&CD02Cd%-# _^8;04;h 215&CdCD02121>5&4$,102.5&49712<(+0# %')')4;890h
73*`#&*
#aOh&8;%-49*-L
5&0i730D^712V j *fi0dE-0d,bR71(+0230*-,1a24;%-C1%-h+0^5&%730:'&,1*-'\*
21%-4;2A>{*-,/71(+0%-#&%'t(+*-,.8;C
%-CdCD021218;f)8;4;897_23')%-CD0-V&5&,171(+0d,.?*-,.0-bY71(+0230:%'&'&,1*
%-CW(+024;%-C1}CD*
#)218;23730#&CD_86#71(+073,10%71?0#]7*->
*-71(+0d,R^8;#)h&2U*->730Da712db&>*-,Y0D+%-?')490-bth&8;%-49*-L
5+02V
+, 

x 32

w uq2s#qg#y2}q}ur~y{v't ysIq2v/2y2}ff!z



 z

 #i71(&8;2R230CD7189*
#bt71(+0=86#715&8;7189E-0k%-49L-*-,.8;71(&? >*-,R%-#&%'t(+*-,.%A,.023*
4;5+7189*
#8;#23'\*--0#h&8;%-49*-L
5+0=23_^21730?A2
PQ"$<F8{TN8;2N'),10230#]730hV}"$<F8Z*-'\0d,.%7302AR8971(23_a#71%-CD718;C8;#+>*-,.?A%7189*
#'&,.*Ea8;h+0hf^_`71(+0PRMM
')%,17186%-4K')%,.230d,/PQ&0d,1, %-Q #&h+0d-bKMK%-49*
?A%,5
b 4 O*-,10#+*+bln-n-o
TWViP $MGM 8;2Ff)%-230hH*
#%c')%,1718;%-4K,.0d'&,10DO
230#]71%7189*
#*->2149*-7R5)#&8tCd%7189*
#iL-,.%-?A?A%,F%-#&%-49_a218;2kPQ&0d,1, %-Q #&h+0d@0d7F%-4V9bSln-n-n
TWV$XY(&8;2<')%,1718;%-4,.0d'&,10DO
230#]71%7189*
#HL
89E-02k23*
?0*->U71(+0A5+73730d,.%-#&CD0/CD*
#&237189715+0#]712db2.5&C.(H%-2eI$MG2bMGM2dbgE-0d,1f)%-4CW(]5)#+^2b%-#&h
')%,17186%-4t8;#&>{*-,.?%7189*
#/%f\*
5+7215+f\*-,.h)8;#&%730hCd46%-5&2302dVXY(^5&2dba"$<F8\CD*
?kf)86#+02U7U*@a8;#&h)2G*->ga#+*fi<4O
0h+L-0Y%f\*
5+7h&8;%-4;*-L
5+02dpPmlTK468;#+L
5&8;21718;Ca#+*<4;0h+L-0-b
215&CW(%-2490D+8;Cd%-4b-?*-,1')(&*
49*-L
8;Cd%-4b-%-#&h23_a#]71%-CD718;C
a#+*<4;0h+L-0-g%-#&hP
TR^#&*<490h&L-0N%f\*
5+7$71(+0h&8;%-4;*-L
5+0-2F2373,.5&CD715+,.0@897121049>b[<(&86C.(:8;2$f)%-230h:*
#71(+0
%-#&#+*-71%718;*
#*->Y%-h1%-CD0#&CD_'t%-89,.2 * %-#&h^#&*<490h&L-0/%f\*
5+7e71(+0/73*-')8;C*->Z71(+0/h&86%-49*-L
5+0PQ?A%-#^5&%-4;4;_
%-#&#+*-71%730h[TWVK89L
5+,10kA21(+*R2Z71(+0e%-#&%')(&*-,.%,1023*
465+7189*
#'&,1*aCD0h&5+,10-V
"$<F8g8;2Yf)%-230hb&8;#]715&89718;E-049_-ba*
#c71(+0e>*
4;49*fi<8;#+LN71(+,10d0e21730d')2dp
lV$Ff&71%-8;#c%-4;4'\*
212.89f)490F%-#730CD0h&0#712R>{,.*
? h&8;%-49*-L
5&0F2173,.5&CD715+,10e%-#)hc73*-')8;CB%-2<>{*
4;4;*<2dp
PQ%T71%-071(+*
210UI$MG2S71(&%7S%,.0U8;#)Cd4;5&h+0hB8;#k71(+02.%-?0U%-h3%-CD0#&CD_k')%-89,UPQ"$MUT%-2S71(+0%-#&%'t(+*-,b
%-#)h
Pf[T/71%-0H71(&*
230HIRM2/71()%7%,.086#&Cd4;5&h+0h}8;#}71(&0:'),10dE^8;*
5&2/"$M73*71(&%7cCD*
#71%-86#&8;#+L71(+0
%-#)%')(+*-,b&%-#&h
PQCT71%-0B71(&*
230BIRM271(&%7Z%,10B8;#&Cd4;5&h&0h8;#/71(+0F?*
237,10CD0#]7Y5&#&Cd49*
230h"$MCD*
#71%-86#&8;#+L@71(+0
"$M CD*
#71%-8;#)8;#+L71(+0e%-#&%'t(+*-,b&%-#&h
PQhtT/71%-0=71(+0e73*-')86CF*->71(&0eh&8;%-49*-L
5+0
^V<F8;2.Cd%,.hi8;#&CD*
?A')%7189f)490B%-#]730CD0h+0#]712<f^_%')')49_a8;#+LN4;8;#+L
5&8623718;C$CD*
#&2373,.%-8;#]712db&%-2<>{*
4;4;*<2dp
PQ%T>*-,Y'&,.*
#+*
?A8;#&%-4\%-#&%'t(+*-,.%ap
6h\$B
QWm/.!<.R WWefi!
WRWfiWk!mfi!c uW3/!fi$\.6/
fffi."
%"2n*87#Q9 ;:=< j> )
 NgN!= mQ!.G_
 .6 

 .fiWFgAW-m.!! <@_Z
 =
fi .  fimS!
 !g!fi$BWQ WmfiRW3
?d

fi@ 2~,7



2	AK$)B

$CA



2B2{EDX<{$7

 G
 F





7BEHK.{JI22

@BA1CEDGFH?IBAJFLKBMJNBO?PGQJRSOGTVUXWZY[P\UXWBWJNE]B]
P1FG^_WBTWB@B`EOGKba_^cEFedGfJdGgBcEC?AhW
P1FG^_WBW1Nea_dGfEdgBcEC?Ai?DjdEDBDGFJkBk1ilminEi=^Boek=gEdEDFep?AJC=qrW
P1FG^sP.SN?Rtasd_nEik^tP\U[WBWJNu]jCp_d1nBnvTB@ukwUxdGf1^JFEDFH1FGf1^tDGdGfJHuiH1d?^JFJk1]
pA1C=qrWBWJN
yJCGArFBdJD=ctTB@zifrPuSN?RZY{dGgBgEnGorDGCGfk=^BAJduif.kjCpjqC?AgcEC1nC?|.iDGd1n
d?|AJFBF=qFf1^vlEF?^?}EFBFGfhT@rdGf1HeWTBWB@B`EOKh^JC_Cl1^JduiftPuSN?R~
FGf1H_p1CGA
yJCGArFBdJD=ctTB@zifrPuSN?R~mY&dgBgunGoDGCGf.k^BAJduifkCpkofB^JdED^i?DbDGCGfJHui=^iCGf.k
lEFG^}EFBFGfbTB@rdfJHtWTBWB@B`EOKb^JCtCGl1^JduiftP.SN?RJ
FGf1H_p1CGA
yJCGArd1nnbT@ifsP.SNGRJ\Y&dGgBgEnGo_nEif1|?Imik^i?DjdGfJHeH.ikDGCGI1A.kFtk^BAI.D^I1AJdBn
g1A1Fp1F?AJFfDGFEkUift^cEFtC?A1H1FGAeH1FEkBDA.ilEFHsifk^1FGgshlEFBnCG}z]
IBfB^iGnP.SN?RJ5fia~
FGf1H_p1CGA
KJFG^I1AftP.SN?RJ
FGf1Heg1A1CEDGFH?IBAJF

K89L
5+,10k^pG"$#&%')(&*-,.%N,1023*
4;5&7189*
#'&,1*aCD0h&5+,10
8VYh)8;21Cd%,.h71(+*
210k%-#730CD0h&0#712R71(&%7<h+*/#+*-7Y%L-,10d0=86#L-0#&h+0d,b&#^5&?=f[0d,b&%-#&h'\0d,.23*
#
8;8!Vh)8;21Cd%,.h:71(+0A%-#]730CD0h+0#]712e71(&%7e%,10A#&*
#aOCD*O!,10d>0d,10#7=%-CdCD*-,.h&8;#+L73*c71(+0N>*
4;49*fi<8;#+L
,W5&490-p
" '&,1*
#+*
5)#M8;2N#+*
#aOCD*O!,10d>0d,10#]718;%-4<8971(%PQ#+*
#aO!,10Dt0Da89E-0*-,A#+*
#+O!,10Cd89'&,1*aCd%-4{T
#&*
5&#c')(+,W%-230BI8;>S%-#]_*->71(+0e>*
4;49*fi<8;#+LCD*
#&h)897189*
#&2 3 (+*
4;hgp
{ M}%-#&hcI %,.0e8;#71(+0e21%-?0e5+73730d,W%-#&CD0e%-#&hcCd4;%-5&210-b&%-#&hcM%-#&hcI?*ah&89>_A71(+0
(&0%-hc*->K71(+0e21%-?0kI$M
{ M%-#&hI%,10A8;#:71(+021%-?A0A5+73730d,.%-#&CD0A%-#&hCd4;%-5)230-bg%-#&hHMh+*^02F#+*-7e?*ah&89>_
71(&0e(+0%-hi*->K%-#_cI$M
Pf[T/>*-,<%-h30CD7189E-0@%-#&%')(+*-,.%ap
8VYh)8;21Cd%,.h71(+*
210k%-#730CD0h&0#712R71(&%7<h+*/#+*-7Y%L-,10d0=86#L-0#&h+0d,
8;8!Vh)8;21Cd%,.h71(&*
230%-#]730CD0h+0#]712/R(+*
230(+0%-h#+*
5&#862A#+*-7A*->F71(+0:490Da86Cd%-4YCd%730dL-*-,1_
 OEOHF
I 
 V<
^V  >G?*-,.0=71(&%-#*
#+0N%-#730CD0h&0#7e8;2$490d>7db[t49730d,$71(+0=,.0?A%-8;#&8;#&LA%-#730CD0h+0#]712Bf^_%'&')4;_^8;#&LA71(+0
>*
4;49*fi<8;#+LNU089L
(]730hi'&,10d>0d,10#&CD02p
PQ%T>*-,Y'&,.*
#+*
?A8;#&%-4\%-#&%'t(+*-,.%ap
8VY%-#]730CD0h+0#]712$71(&%7<%,10e8;#71(&0k21%-?0k"$M}%-2<71(+0e%-#&%')(&*-,=P08;L
(7 -
T
#fi6m_YZY.1D6fi!K31!.!mQ!!t.!\!-m,d.
W6 .!mm.9Q;:^ > 
 .3$<!mS!mQ!!<!fi!fi6m6fi !"
 !fimKm!g
]
 WfiF
 3WQ9 :: j=> 
 [Kfi!W-
mfi!fi
 Wffdfg6fi fi!Wcifi6m;fi !
 !Y.fi!3(
 . ]q\h fimZmfi!fi1!YWfi3=!R!fi
QidW!Gi
.6Y
 .!efi!1 fi3d
 $!fi.! W+.!3\
h B.!U
J. !3.KZ
 -.fim 3WfiG[fiufiW
Y
 d!!
.6Y
 .!$RS!-W!W9
?

fi

i{9  ] DA2

2{~i{

8;8!V%-#]730CD0h+0#]712i71(&%7i%,.086#71(&0:'),10dE^8;*
5&2/"$M 73*71(&%7cCD*
#]71%-8;#&8;#&L`71(+0%-#)%')(+*-,
PU089L
(]fi
7  B 'T
8;868VU%-#]730CD0h+0#]712$71(&%7<%,10e8;#71(&0k?*
237Y,.0CD0#7<5)#&Cd49*
230hc"$MPU089L
(]7 B'T
89EtVY%-#]730CD0h+0#]712F8;#71(+0B73*-')8;C/P089L
(]
7 l
T
EtVY%-#]730CD0h+0#]712K71(&%7S%')'[0%,S<8971(k71(+0E-0d,1f@*->+71(+0%-#)%')(+*-,S?A*-,1071(&%-#=*
#&CD0$PU089L
(]7
 
T
E^8VY%-#]730CD0h+0#]712B71(&%7F%,10N8;#71(&0@21%-?A0='\*
2189718;*
#c<8971(,10d>0d,10#&CD0@73*/71(+0@E-0d,1fH%-2$71(+0
%-#)%')(+*-,=Pf\0d>*-,10B*-,R%>730d,TDPU089L
(
7  
T
E^8;8!V%-#]730CD0h+0#]712Y71(&%7Y%,.0e8;#71(&0e21%-?0F'\*
21897189*
#/<8971(,10d>0d,10#&CD0B73*N71(+0e5+73730d,W%-#&CD0e%-2
71(&0k%-#&%')(+*-,@P089L
(]
7  
T
E^8;868V71(&0e#+0%,10237Z%-#]730CD0h+0#7<73*71(+0B%-#&%')(+*-,kPQ5&210h<(+0#?*-,10F71(&%-#*
#+0BCd%-#&h)8;h&%730
*-f)71%-8;#&2Z71(+0=(&89L
(+0237E%-4;5+0fiT
Pf[T/>*-,<%-h30CD7189E%-4%-#&%'t(+*-,.%ap
8VY%-#]730CD0h+0#]712$71(&%7<%,10e8;#71(&0k21%-?0k"$M}%-2<71(+0e%-#&%')(&*-,=P08;L
(
7  -
T
8;8!V%-#]730CD0h+0#]712i71(&%7i%,.086#71(&0:'),10dE^8;*
5&2/"$M 73*71(&%7cCD*
#]71%-8;#&8;#&L`71(+0%-#)%')(+*-,
PU089L
(]fi
7 l 'T
8;868VU%-#]730CD0h+0#]712$71(&%7<%,10e8;#71(&0k?*
237Y,.0CD0#7<5)#&Cd49*
230hc"$MPU089L
(]
7 l 'T
89EtVY%-#]730CD0h+0#]712F8;#71(+0B73*-')8;C/P089L
(]
7  -
T
EtVY%-#]730CD0h+0#]71271(&%721(&%,.071(+021%-?0a8;#&hF*->a?*ah&8)0d,W2P0-VL+V9b'),10d'\*
21897189*
#&%-4')(&,.%-2302db
%-hm0CD7189E-02bt%-#&hc23**
#tTBPU089L
(fi
7  
T
E^8VY%-#]730CD0h+0#]712F<8971(c0D+%-CD7149_c71(+0=21%-?0=?A*^h&89)0d,.2eP0-VL+V9b\71(+0=2.%-?0k%-h30CD7189E-0,10h T
PU089L
(]fi
7  
T
E^8;8!V%-#]730CD0h+0#]712$71(&%7<%L-,10d0k86#c#]5&?=f\0d,kP08;L
(
7  
T
E^8;868V71(&0e#+0%,10237Z%-#]730CD0h+0#7<73*71(+0B%-#&%')(+*-,kPQ5&210h<(+0#?*-,10F71(&%-#*
#+0BCd%-#&h)8;h&%730
*-f)71%-8;#&2Z71(+0=(&89L
(+0237E%-4;5+0fiT
XY(&0230'&,10d>0d,10#&CD02NU0d,10ch+0dE-049*-'\0h%-2A%:,10215&497@*-><71(&00?A')89,.8;Cd%-4U23715&h&_0D^'t4;%-8;#+0h8;#71(+0
>*
4;49*R8;#+L230CD7189*
#V
   Tt    

 #71(&8;2F230CD7189*
#bS%-#0D^'\0d,.8;?A0#71%-4K23715&h&_*->U71(+0%-49L-*-,.8971(&? 862R'),10230#]730hb8;#&Cd465&h&8;#+L/%ch+0d0d'h+0DO
21CD,.8;'&7189*
#B*->+71(+0G0D^'\0d,.8;?A0#712dbCD*-,1'\*-,.%R%-#&he73*^*
4;25)230hb%-2U04;4%-2%<21715&h+_B%f\*
5+7g71(+0U8;?'\*-,171%-#&CD0
*->K71(+0e%-#&%')(&*-,.8;CB%-CdCD021218;f)8;4;897_/23't%-CD0-V
K{

\

Qy{}!y2}q\y]y{v/~1?q2s

u)~E}ff=#y{sIy-)ur\iu}ff'z

usi~

 #c*-,.h+0d,Y73*0dE%-4;5)%730e71(+0e%-#&%')(&*-,.%,1023*
465+7189*
#c%-49L-*-,.8;71(&?'&,1*-'\*
230hc8;#71()8;2Z')%'\0d,ba71(+0eL-0#&0d,.%-4
'&,1*aCD0212Z*
5&714;8;#+0h8;#iK89L
5&,10eNU%-2<>{*
4649*U0hV
F%71%/>{*-,Y71(&0e0dE%-465&%7189*
#cU0d,10e71%-0#>{,.*
? 71(+L
0 	wxut^
~ ffWrDw &x1Dr RDxW~Dwr)bt%ACD*-,1't5&2Z*->B '
73,.%-#&2.CD,.89f\0h23'\*--0#`^')%-#&8621(h&8;%-49*-L
5+02e'&,1*Ea8;h+0h:f^_71(+0A%-215+,.h+0AMG,1*30CD7/PQ%-215+,.h+0/M,1*30CD7db
ln-n-o
TWVGXY(&0230Yh&8;%-49*-L
5&02%,10YCD*
#E-0d,W21%7189*
#&2Gf[0d70d0#%B,.%-8;49%_@CD*
?')%-#]_N0?')49*fi_-0d0Y%-#&hA%BCd4;890#]7dV
XY(+0U73,.%-#&2.CD,.89'&73*-,K5&230h@8;#=71(+0U%-215&,.h+0M,1*30CD7K'&,1*Ea8;h+02S715+,.#@%-#&h@23'\0%-0d,?A%,1a5+'gVF5+7*->\B '
h&8;%-4;*-L
5+02dbgJ 'cU0d,10A2304;0CD730h>*-,e71(+073,.%-8;#)8;#+LP73,.%-8;#&86#+LcCD*-,1')5&2WT$%-#)h71(+0A,10?A%-86#&8;#+LliU0d,10
?

fi@ 2~,7



2	AK$)B

$CA



2B2{EDX<{$7

 G
 F





7BEHK.{JI22

X x        $
      xu        
x= $x
$XxxxXXx$ 
X x         $    
          
    

x      $ 
X G       
    G        
8X$ ?X$
           G   
     
x=
x$ 
xXX$x$ 
xXx$X$ 
Xx$ 
 $$

  x 
x      $ 
     
       
XBx x;xX 
X=[==X 
  G     8
u    $

8;L
5+,10k^pG&5)4;4\0dE%-465&%7189*
#c'&,.*^CD0212
,10230d,.E-0h>{*-,71(+0Rt#&%-4t0dE%-465&%7189*
#HP730237YCD*-,1')5)2WTWVXY(+0230BB'h&8;%-49*-L
5&02UCD*
#]71%-8;#i='&,1*
#&*
?A8;#&%-4
%-#&%')(&*-,.2Y%-#&hi-
A%-hm0CD718;E%-4g%-#)%')(+*-,.2dV
 0(&%-hH7U*%-8;?A2e8;#H5)218;#+Lc71(+073,.%-8;#&8;#&LCD*-,.')5&2dp/PmlTB73*i023718;?%73071(+086?'\*-,171%-#&CD0*->71(+0
2373,.5)CD715+,.%-4%-#&%')(+*-,W8;C%-CdCD0212.89f)8;4;8;7_23't%-CD0-b%-#&h P
T=73*Hh+0Dt#+0%-#%-h+0^5&%730i230d7N*->RCD*
#)2373,.%-8;#]712
%-#&h'&,.0d>{0d,10#)CD02P0D^'\0d,.86?0#71
2 'abGlb^b%-#&h
TWVAXY(+0730217kCD*-,1')5&2F%-2B,10210d,1E-0hH73*i*-f&71%-86#H71(+0
t#&%-4\0dE%-4;5&%718;*
#V
 #%-h)h&897189*
#bt71(+0@0#7189,.0@CD*-,1't5&2<%-2F?A%-#^5&%-4;49_i%-#&#&*-71%730hR8971(7*ch&89[0d,10#]7<L-*
%-462dp=PmlT<73*
8;h+0#]7189>_F>5+,.71(+0d,Sh&8621CD*
5+,.2302373,.5&CD715+,W%-4
'&,1*-'\0d,1718;02215&CW(=%-2%-h1%-CD0#&CD_e')%-8;,.2%-#&h=73*-')8;Cd2b%-#)hP
Tg73*
8;h+0#]7189>_R%-#&%'t(+*-,.2g%-#&hk%-#730CD0h+0#]712dV"$4971(+*
5+L
(BU0G%-#&#&*-71%730he71(+0GCD*-,.')5&2g?A%-#^5&%-4;4;_-b71(+0d,10%,.0G%7
'&,10210#7K23*
?0%-5+73*
?A%718;C23_a23730?A2>*-,K'[0d,.>{*-,.?8;#+L<%-h1%-CD0#&CD_e't%-89,S71%L-L
86#+LAP71(+0U%-215&,.h+0M,1*30CD7
PQ%-215+,.h&0UMG,1*m0CD7dbtln-n-o
TWb>*-,0D+%-?')4;0fiTWb
%-2K04;4^%-2K>*-,K%-5&73*
?A%718;CU73*-')8;C71%L-L
8;#+LPQ<0d_^#&%,b&ln-n-n
T
*-,Z%-5+73*
?A%718;CF73*-'t8;C<0Da73,.%-CD7189*
#PQ210d0$71(&0F?A0d71(+*^h>*-,Z%-#&%')(+*-,.%=,1023*
465+7189*
#h+021CD,.89f\0h/8;#a0CD7189*
#
]TWV
XY(&0%-#&#+*-71%7189*
#*->eCD*
#]E-0d,.21%7189*
#&%-4$2373,.5)CD715+,10i%-2/Cd%,.,.890h*
5&7/%-2h&021CD,.89f\0h8;#71(+0#+0Da7
')%,.%L-,W%')(V"R#8;?A'[*-,.71%-#7%-23'\0CD7*->Rh)8;%-49*-L
5+0c2373,.5)CD715+,10c%-#&#+*-71%7189*
#}8;2@71(+0c73,.%-8;#&86#+L't(&%-230-b
<(&86C.(c%-21215&,102Z,104;86%f)8;4;897_A%-?*
#+L%-#)#+*-71%73*-,.2dV
XY(&0%-#&#+*-71%7189*
#')(&%-230%-2k%-CdCD*
?A')4;8;21(&0h%-2e>*
4;49*fi<2dp/PmlTB7U*%-#&#+*-71%73*-,W2kU0d,102304;0CD730hb
P
T<%-#%L-,10d0?A0#7 9 %-2<,10%-CW(+0h:f\0d7U0d0#71(&0k7U*%-#&#+*-71%73*-,.2F<8;71(c,10dL
%,.h:73*71(&0@%-#&#&*-71%7189*
#
21CW(+0?0R5)218;#+Le%B73,W%-8;#&8;#+LeCD*-,.')5&2dbtP
T71(+0R%-#)#+*-71%7189*
#A%-271(+0#Cd%,1,W890h*
5+7f^_f\*-71(A%-#&#+*-71%73*-,W2
8;#N')%,W%-4;4904^*E-0d,U71(+0Z730237GCD*-,.')5&2db%-#)hiP]TK%B,104;86%f)8;4;897_e23715&h+_=U%-2Cd%,1,.890hN*
5&7*
#71(+0<%-#&#&*-71%7189*
#
6P V%,.4;0d7371%/dKs9\ln-n
-TWVGXY(+0Z,10468;%f)8;4;8;7_e23715&h&_@5&210h71(+0B-sWu
uasR2371%718;237186CZ71(&%7G?0%-2.5+,10271(+0R
% K/#aO
<h\.!mZdg-Wfigg1SDYZm\!UmDR!..!WggfiF .3R!!Km.!fim
?d

fi

i{9  ] DA2

2{~i{

897_f\0d70d0#71(+0=%-#)#+*-71%7189*
#&2<*->71(+0e7U*%-#&#+*-71%73*-,.2Rf]_c?A%a8;#+LB35&h&L
?0#712R%f\*
5+7<Cd%730dL-*-,.8902V
+*-,$CD*
?')5+718;#&L71(+0NsWuuasHPdT2171%718;23718;Cb)210d0@a890dL-04g%-#)h V%-237304;4;%-#`Pmln-o-o
TWV 8
U0Cd%-5&230Z715&,.#&2%,10Z?A%,1-0hh&5+,W8;#+LY71(+073,.%-#)21CD,.89'&718;*
#k')()%-230-b-71(+0Z%-#&#+*-71%73*-,?0d,.049_kCd4;%-212.8)02
715+,.#)2=%-CdCD*-,.h)8;#+L:73*:71(&0/715+,W#7_]'\02h+021CD,W89f\0h8;#^0CD7189*
#%-#&h71(+0#,104;%7302@0%-C.(8;#)89718;%7189E-0
8;#]730d,1E-0#]7189*
e
# 1  73*89712F,.0%-CD7189*
#8;#]730d,1E-0#]7189*
e
# J  bg71(+0d,10df^_h+0Dt#&8;#&L%-h3%-CD0#&CD_:')%-89,W2dVa8;#&CD0
71(&8;271%-212.8;?')49_,10]5)89,102ZCd4;%-212.8tCd%7189*
#b&8;7Y8;20%-218;49_?0%-215+,.0hi5&218;#&L@71(&0sWuuas2371%718;237186CV
VU*
#)Cd5+,1,10#]7149_-bg73*-')86Cd2B0d,.08;h+0#]718)0hVXY()8;2$71%-23U%-2=%-4;23*i218;?A')490-bg218;#)CD071(+0ACD*-,1')5&2k5&230h
>*-,G71(+0230R0D^'\0d,.86?0#712G8;2*-,1L
%-#&8;d0h8;#]73*=21(&*-,17h&8;%-49*-L
5&02G%-#&h0%-CW(h&8;%-49*-L
5+0Y()%-2*
#&49_N*
#+0R?A%-8;#
73*-')8;C$*-,Y71(&0?0-b&%-#&hc218;#)CD0$71(&0230e%,10e8;#]73,1*ah&5&CD0hCd490%,.49_/f]_?0%-#&2*->71(+0eCd4;890#]7d2Z8;#]730d,1E-0#]7189*
#
%7e71(+0f\0dL
8;#)#&8;#+L*->0%-CW(h&86%-49*-L
5+0-V"R2=%,10215&4;7db0h&0d730CD730h#+*ih&8621CD,10d')%-#&Cd8;02Ff[0d70d0#`%-#aO
#+*-71%73*-,.2e<8971(,10dL
%,.h73*c71(+0N73*-')8;CN8;h&0#718[Cd%7189*
#VeXY(+0d,.0d>{*-,10-b71(&0d,10NU%-2B#+*c#+0d0h:73*i?0%-2.5+,10
71(&8;271%-215)218;#+LN71(+0-sWu
uas=2371%718;21718;CV
"$CdCD*-,.h&8;#&L=73
* V%,.4;0d7371%cGs9;b+%/?0%-215&,10?0#]7Uf\0d70d0
# ' -oN%-#&
h ' Bo 'N%-4;49*fi<25&2U73*?A%-0
'\*
21897189E-0cCD*
#&Cd465&2189*
#&2b%-#&h8;
> 8;2NL-,.0%730d,/71(&%-
# ' Bo 'abZ0i(&%E-073*-71%-4<,104;8;%f)864;897_f[0d70d0#}71(+0
,10215)49712*->K71(+0e%-#&#+*-71%73*-,.2V
 #71(+*
230iCd%-2102A<(+0d,.0i%Hh&8;21CD,10d't%-#&CD_%-2A>*
5&#&hf\0d7U0d0#71(+0i%-#)#+*-71%73*-,.2db71(+0c>*
4;49*fi<8;#+L
CD,.89730d,W89*
#c%-2R%'&'t4;890hp0%-CW(h&8;%-49*-L
5+0kU%-2$%-212189L
#&0hi%?A%-86#%-#&#+*-71%73*-,$<(+*
210=%-#&#+*-71%718;*
#%-2
CD*
#&2186h+0d,10hh&0Dt#&897189E-0@8;#i71(&0=0dE-0#]7F71(&%7R71(&0d,10=U0d,10Nh&8;21CD,.0d')%-#&Cd8902Zf\0d70d0#71(+0=7U*c%-CdCD*
5&#712V
 #*-,Wh+0d,R73*cL
5)%,.%-#730d0Nf)%-46%-#&CD0-b0%-C.(H%-#&#&*-71%73*-,eU%-2$71(+0?A%-86#%-#&#&*-71%73*-,e>{*-,F0D+%-CD7149_:B ',N*->
71(+0kh)8;%-49*-L
5+02dV
F#)CD0<f\*-71(A%-#&#+*-71%73*-,W2U()%-hNt#&8;2.(+0hN71(+0R%-#)#+*-71%7189*
#b^71(+0Y,10468;%f)8;4;8;7_=21715&h+_@U%-2UCd%,1,.890hA*
5+7db
<8971(%:,10215)4971%-#7-sWu
uas?0%-215&,10?0#]7@*-fi
> z' nalV  0c71(&0d,10d>{*-,.0iCD*
#&218;h+0d,N71(+0c%-#&#&*-71%7189*
#
*-f&71%-8;#&0hc>{*-,<71(+0B0dE%-465&%7189*
#c73*Af\0B73*-71%-4;4;_,.04;8;%f)490-V
a86#&CD071(+0c%-#&#+*-71%730h730Da712U*
5&4;hf\0'&,1*aCD021230hf^_%-#%-#&%'t(+*-,.%:,1023*
4;5&7189*
#23_^21730?cbGU0
h+0dE-049*-'\0hi%-#:&
 O:J71%L-L
8;#+L/>{*-,W?A%7dV
$0#&0d,.%-4;49_-b+71(&862R&
 O:J?A%,.^5+'cR8;4;4\(&%E-0k71(+0e>*
4;49*R8;#+LN>{*-,W?cp

m	
fiffJu.E.u1E.

Ju.E.u1E.E.E,JmuJ.E.ZEEmZ.u;

XY(^5&2db+71(&0B>{*
4;4;*<8;#&LN#&*-71%7189*
#&2<%,.0e5&230hi8;#0%-CW(Cd%-210-p
XS*-')8;Cp
{

#ffEffV1#.

J

"$h1%-CD0#&CD_')%-89,.2p
{

  "!#


E  m

$%&"m&ff
wJm



R(+0d,10   8;2%-#A8;h+0#]718tCd%718;*
##]5&?=f\0d,5&230hN73*k%,.,.%-#+L-0Y71(+0R%-h3%-CD0#)CD_@'t%-89,.28;#230^5+0#]718;%-4
*-,Wh+0d,
{

 #]730d,1E-0#718;*
#i715+,.#)2dp

'uZ)(;+*..,Em&-
ZG"
.ff"""
 m
fifi!W&!
.6Y.!@-Wfi!..!=fi!dmGm-
!k=Gfi. 3e!m-W! 09 /F123 1 tfi
 1!m!&45
4 56/F.731 tfi  .!m98 f+WY.0:4544 > 




?D

fi@ 2~,7



2	AK$)B

$CA



2B2{EDX<{$7

 G
 F





7BEHK.{JI22

R(+0d,10kX "BMffL ?A%_cf[0  A*-,   PQY0%-CD718;*
#*-,  #)89718;%7189E-0fiT%-#)h:aMLG"<;(LG 8;2Y71(+0=8;#&h&8O
Cd%73*-,$>*-,Y71(+0k23'\0%-0d,YR(+*
230B715+,.#897Z8;2
{

V*
#718;#^5&8;#&L@715&,.#&2dp

 =*..,Em&-
ZG>ff?@ff?m""
"Em
 E

XY(&0B>{*-,.?%7<8;20Da0?')4;8t0h86#c8;L
5+,10B+V

CB
AED JZFB
A  CFHMGJIL@N?K IL?K,K B  KsPNaPOQNILKFRSK"B
A
AED  PB HGJILiK KB
A  CFM @NIL,K  sK PaN POQN ILK KB
A
A



AED C PFB M@NILKrKePNaPOQNILKaCRSKB
A

AED  P@B B
AED CFM@NILKrKePNaPOQNILKaKB
A
AED  P@B B
AED

0;?
6/iZ
5
i! " (ffa siZ !eeg, 
6/!a!ii3a 0 giP,c3g

ui-i! p (& ff?;,!p!?m!a
-Zu0& 00qi!/ffa!?i3&  !
= )a
0
5i8

( !u r
6/aip&\!Z 

5

K89L
5+,10B+pL+%-?')490F*->G&O:J%-#&#&*-71%7189*
#
" 20Da')4;%-86#+0h%f\*E-0-b@8;#%-h&h)897189*
# 73*71(&8;22173,.5&CD715+,.%-4k%-#&#+*-71%7189*
#b=71(+0`CD*-,.')5&2i%-2:%-4;23*
$
%-#&%')(&*-,.8;Cd%-4;49_%-#&#+*-71%730h f^_?A%,1a8;#+L5+'}71(&0:%-#&%'t(+*-,.8;Cc,.04;%7189*
#&2/f\0d7U0d0# %-464Y'&,1*
#&*
?A8;#&%-4
%-#&h`%-hm0CD718;E%-4%-#)%')(+*-,.2=%-#&hH71(+089,@CD*-,1,10CD7@%-#730CD0h+0#]712dV  #H*-,.h+0d,=73*L
5&%,W%-#730d071(+0A,102.5&49712db
71(&8;2%-#)#+*-71%7189*
#%-2'\0d,1>*-,.?0hNf^_=7*kh)8[0d,.0#7%-#&#+*-71%73*-,W2G8;#N')%,W%-4;4904a%-#&h%B,10468;%f)8;4;8;7_e23715&h&_
*->71(+0$215+f)230^5+0#]7G%-#&#+*-71%718;*
#/%-271(+0#/Cd%,.,.890hA*
5&7dVB#&CD0R%L
%-8;#gb]71(+0$%-#&#+*-71%7189*
#%-2G73,.0%730h
%-2%Cd4;%-212189tCd%7189*
#71%-23[bCD*
#&218623718;#+L*->R230490CD7186#+L71(+0c%'&'&,1*-'&,W8;%730/04;0?0#71286#71(+0cCd%-#&h)8;h&%730
4;8;217PU0A0237186?A%730h%-#%E-0d,.%L-0*->^V '\*
212.89f)490N%-#730CD0h&0#712@'\0d,e%-#&%')(+*-,e%>730d,=%')')49_a8;#+LCD*
#aO
2373,.%-86#712WTWV XY(+0i,.04;8;%f)8;46897_23715)h+_`*->F71(+0?A%-#]5)%-4<%-#&%')(&*-,.8;C%-#&#+*-71%7189*
#,10215)49730h8;#%-sWu
uas
?0%-215&,10?0#]7Y*-5
>  w' o
]V
 #%-h&h&89718;*
#b71(+0ZCD*-,1')5&2K%-2K71%L-L-0h5&2.8;#+L$71(+0ZMUe=71%L-L-0d,730CW(&#&8;^5+0Zh+021CD,W89f\0h=f^_=M4;%F%-#&h
M,W890d73*Pmln-n-o
TWV=+,1*
?71(&8;2btU0N*-f&71%-8;#+0h:?*-,1')(&*
49*-L
8;Cd%-4%-#&h490Da86Cd%-4S8;#&>{*-,.?%7189*
#VRX<(+0@CD*-,.')5&2
%-2G71(+0#')%,.230h5&218;#+LB71(&0$P $MGM`')%,1718;%-4+'t%,.230d,G'),1*-'\*
230hf^l
_ O:%,1?7 R Q #&0dDOU%,WCD*cKs-Pmln-n-o
TG8;#
*-,.h+0d,73*N*-f&71%-8;#23_a#]71%-CD718;CF8;#+>*-,.?A%7189*
#VK8;#&%-4649_-b71(+0F'&,.*-'[*
210h/%-#&%'t(+*-,.%=,1023*
4;5+718;*
#/%-49L-*-,W8971(&?
%-2<%'&')46890hV
?ff

fi

K

1E


<:1

^

 ff

>fi >
fi 

{E
E ?! 6

i{9  ] DA2

2{~i{



6ff

d >
 	

 ET

U

:!

fi
ff fi


 WV

WXUJUZYL[ \


  	

W -
 E1.<:<611:6w1<_\	fiff 	
1E`_	 <:1a_g	fiK

?]

1?6


ff 
D

nWh\Kd!mm3dg
R$!fiK!ZWQWmfiFWSW!K.fiW
\h Kd!mm3dg
 R$!fiKfi!mfiQ WmR.g!U!fiKfimd.fiU!fiWfiW
o \h Kd!mm3dg
 R$!fi WR.SdW!!W .mfi$WmU!fiWfiW
7W\
h Kd!mm3dg
 R$!fi!fi a

 ! WW 
% \h Kd!mm3dg
 Rmgfi!

b

X%f)490AlpUa73,.5&CD715+,.%-4%-#)%')(+*-,.8;CF%-CdCD0212.89f)8;4;8;7_/21')%-CD0e,10215)49712
^0dE-0d,W%-4g23715&h&8;020d,10k71(+0#cCd%,1,.890hi*
5+7Y8;#*-,.h+0d,<73*A8;h+0#]7189>{_/71(+0e8;?'\*-,171%-#)CD0B*->h+0D[#&8;#+LN%-#
-% h+0^5&%730A%-#&%')(&*-,.8;CN%-CdCD0212189ft8;4;897_23')%-CD0%-#&h*->Uh+0Dt#&8;#&L%CD*
#&2373,W%-8;#7e%-#)h'&,10d>0d,10#&CD023_a23730?
f)%-230h*
#:71(&8;2$23')%-CD0-V  #71(+0230N23715&h)8902db)U0NCD*
?')%,10h:71(+0@*
5+73')5+7R*->71(+0N%-#&%')(+*-,W%A,1023*
465+7189*
#
23_a23730? R8971(71(+0e?A%-#^5&%-4g%-#&#&*-71%7189*
#i%-#&hcL-0#&0d,.%730h210dE-0d,.%-4g2371%718;21718;Cd%-4g,102.5&49712dV
\ 

ac

}Eziy2}q2s#uy-Xuq2sq #y2}ffq$ur~?~ 'v'

~?q$u

 #*-,.h+0d,=73*:2.(+*71(+0/8;?A'[*-,.71%-#&CD0A*->Yh+0Dt#)8;#+Li%-#%-h&0]5&%730%-#&%')(&*-,.8;CA%-CdCD0212.89f)8;4;8;7_23')%-CD0-b%
23715&h&_*-><71(+0c49*aCd%7189*
#*->R71(+0c%-#730CD0h+0#]7A*->R0%-C.('&,.*
#+*
?A8;#&%-4U%-#&h%-h30CD7189E%-4Z%-#&%')(+*-,.%:%-2
h+*
#+0e5)218;#+LN71(+0e73,W%-8;#&8;#+LNCD*-,1')5)2dVXY(+0B,102.5&49712Z%,10eL
8;E-0#i8;#iX%f)490AlVed
"$2Cd%-#Nf\0210d0#8;#@71(+071%f)490-bn-^V n N *->t71(+0Z%-#730CD0h&0#712G0d,10Y4;*^Cd%730h8;#@71(+0'&,.*-'[*
210h@2173,.5&CO
715+,.%-4K%-#&%'t(+*-,.8;C@%-CdCD0212189ft8;4;897_i23')%-CD0-V  7B8;2$023718;?A%730h71(&%7F71(+0N,10?%-8;#&8;#+L%-#730CD0h&0#712P+V;il NT
%,10i49*aCd%730h8;#71(+0c215+f)73*-')8;Cd2@*->R71(&0h)8;%-49*-L
5+02dV ( /  #*-,.h&0d,@73*86#&CD*-,1'\*-,.%73071(+0230c,10?%-8;#&8;#+L
%-#]730CD0h+0#712@8;#]73*71(+0A%-#&%')(+*-,W8;CN%-CdCD0212189f)864;897_23')%-CD0-b*
#&0A?A89L
(]7$0?A')49*_%c2373,.%730dL-_71(&%7e5)2302
71(+0 -1tvv~i#q$u PQ8V0-V9ba%-4;4&71(+0$#+*
5&#')(+,.%-2302G>{,.*
?71(+0Rf\0dL
8;#&#&8;#&L$*->71(&0Rh&8;%-4;*-L
5+0<73*=71(+0$%-#)%')(+*-,
?A89L
(]7f\0F5)230htTWV j *U0dE-0d,bt%-2Z21(&*<#c8;#X%f)490e^b+*
5+,Z'&,.*-'[*
2.%-4t>*-,Z71(+0e%-#&%'t(+*-,.8;C$%-CdCD0212189ft8;4;897_
23')%-CD0PQ(+0d,10%>730d,F,10d>{0d,.,10hi73*%-2 ~E}t#t#}q2v TWbt,10h)5&CD02Y71(+0@%E-0d,.%L-0A#^5&?=f\0d,Y*->Cd%-#&h&8;h)%7302<'\0d,
%-#&%')(&*-,Pf\0d>*-,10%')')49_a8;#+LCD*
#&2173,.%-8;#]712WTB73*l 'aV fiH>,1*
? 71(+0+V;lDH71()%7@U*
5&4;hf\0/*-f&71%-86#+0h`8;>
71(+0 -1tv'v(~?q$u %')'&,1*
%-C.(U0d,10%-h+*-'&730hV  #`*-71(+0d,.2@U*-,.h&2db5&2.8;#+Li71(+0 -1tv'vc~i#q$u %'&'&,1*
%-CW(
U*
5&4;h:8;#&CD,10%-230@71(+0#^5&?kf\0d,$*->G'\*
21218;f)490kCd%-#&h)8;h&%7302$f]_%/>Q%-CD73*-,B*->G71(&,10d0-b\71(+0d,10df^_L-,10%7149_:8;#aO
CD,10%-2186#+Lf[*-71(71(+0,10^5&89,.0h`CD*
?'t5+71%7189*
#&%-40D\*-,17%-#&h71(+0'\*
212189ft8;4;897_*->$230490CD718;#+LH8;#)CD*-,1,10CD7
%-#]730CD0h+0#712V<I<*-7186CD0-bt73*^*+bt71()%7<71(+0230=0D^'\0d,.86?0#712Y0d,10='\0d,1>{*-,W?0hc*E-0d,B%ACD*
46490CD7189*
#*->K2.(+*-,17
h&8;%-4;*-L
5+02iPQ%,1*
5&#&h--*-,Wh&2N'\0d,Ah&8;%-49*-L
5+0fiTWVXY(&0230c'&,1*-f)4;0?A2N<8;4;4f\00dE-0#}?*-,10i%-Cd5&7308;#
49*
#+L-0d,Rh&8;%-49*-L
5+02V
$71(&0d,k,10230%,WC.(+0d,.2@(&%E-0'&,1*-'\*
230h5&218;#+Li%i<8;#&h+*fi <8971(%&a0h#^5&?kf\0d,e*->Z230#]730#&CD02=73*
h+0Dt#&0G71(+0U%-#&%')(+*-,.86CG%-CdCD021218;f)8;4;897_B23')%-CD0-VXY(&8;2g7_^'[0U*->&%'&'&,.*
%-C.(N?A89L
(]7f\0Cd%-46490h=%)f 'sy f y~Eususu)~ %'&'),1*
%-C.(V&*-,0Da%-?A')490-b+0d,., %-Q #&h+0d@s-[Pmln-n-n
TK'&,.*-'[*
210Y5&218;#+L$71(+0Z71(+,.0d0Z'&,10dEa89*
5&2
230#]730#&CD02R73*h+0Dt#+0B71(&0k%-CdCD0212189ft8;4;897_/23')%-CD0k>{*-,R'&,1*
#+*
5&#)2Z%-#&hc71(+0B>*
5+,Y'&,.0dE^89*
5)2Z230#730#)CD02<>*-,
%-h30CD7189E%-4+%-#&%')(+*-,.%F8;#A^'t%-#&8;21(VS+*-ff
, LG#&L
4;8;21(b ;e%-?A0d_
%-?A%APmln-n
-TK'&,.*-'[*
2102S71(&0Y21%-?0Z23')%-CD0Z>*-,
71(+0U'&,1*
#+*
?8;#&%-4V j *U0dE-0d,b]71(+0d,108;2S#&*$2373,W5&CD715+,.%-435)23718tCd%7189*
#k>{*-,K71(+0210h&0Dt#&897189*
#)2dVg+0d,1, %-Q #)h+0d
: d.!mS!.\!fiSQ!Z
^!WfiW!K.mmmfi R.mS\WfiW31!3RWfifi!Wfi!fifiJ
 !!fi.!fi
!Y
 1!S..K!mfi!RQQ!
 t1!
 !
 !Y
 WfiW.!099 9 
 !gm.!mW ! > 
  < !fi fi [Z
 3$dA
f !.gfiW!fiY
 WR!W+

 !fifi . 
 fifffi
 figmd!fi
 fi!m\!fi`3

24

?>

fi@ 2~,7



2	AK$)B

"$#&%')(&*-,.8;CF%-CdCD0212189ft8;4;897_/23')%-CD0
XS*-71%-4gCd%-#&h)8;h&%7302
VZ%-#&h&8;h&%7302'\0d,Y%-#&%'t(+*-,
MG,1*-'\*-,17189*
#



$CA

2B2{EDX<{$7

a73,.5&CD715+,.%-4
lb '
-
l 'aV fi

l'1',N

&5)4;423')%-CD0
^b -oB'
+V;lD
aloN

 G
 F





7BEHK.{JI22

 8;#&h+*fi*->5+73730d,.%-#&CD02
lb -n-
l^V '

l-N

X%f)490k^pV%-#&h&8;h)%730273*f\0F'&,1*aCD021230hc>*-,Y0%-C.(:%-#&%')(&*-,.8;CF%-CdCD0212189ft8;4;897_/23')%-CD0
dSs%-#&g
h ;e%-?0d_
%-?A%$'\0d,1>*-,.?0h@230dE-0d,.%-4a0?'t89,.8;Cd%-4]23715&h&8902S73*B21(+*fi71(&0U*-')718;?A%-4^23')%-CD0>{*-,K0%-CW(
0Da'[0d,W8;?0#]7dVSX%f)490GZf\049*fi21(+*fi<2g71(&0,10215&4;712*->+%<23715&h&_R<()8;C.(BU0'\0d,1>*-,.?0he5)218;#+L71(+0&	wx!u[]~
ffWrdw-Qx1D
r  RDxW~Dwr&b+71(&0BL-*
%-4\*-><(&86C.(U%-273*Ah&0Dt#+0F%-#c%-#&%')(+*-,.86CR%-CdCD0212.89f)8;4;8;7_23't%-CD0$ft%-230h*
#
h% f 'sy f y-_~Eusuws#u)~ 71(&%7ZCd%-#71(+0#f\0$%-h)%'&730h73*h&8;%-49*-L
5+02Uf^_?0%-#&2*->Si% f sy f yl '@5+73730d,.%-#)CD02
tEuw}q2s#u)~ V"$2G71(+0$71%f)490$21(+*fi<2db\l-lR5+73730d,W%-#&CD02U>{*-,'&,1*
#&*
?A8;#&%-4&%-#)%')(+*-,.%=%-#&h
>*-,N%-hm0CD7189E%-4Z%-#&%')(&*-,.%%,10c#+0d0h+0h8;#`*-,.h+0d,73*HCD*E-0d,/71(+0c21%-?0c#]5)?kf\0d,@*->R%-#]730CD0h+0#712/%-2
%-2YCD*E-0d,10h5&218;#&L=71(+0B2373,W5&CD715+,.%-4%-#&%'t(+*-,.8;C$%-CdCD0212189ft8;4;897_23')%-CD0P<()8;C.(%-2Yh+0Dt#&0h/ft%-230h*
#
%-h1%-CD0#&CD_'t%-89,.2k%-#)h71(+073*-')8;CTWVa86#&CD071(+0%-#&%')(&*-,.8;C21')%-CD05)218;#+L:%<86#&h+**-><5+73730d,.%-#)CD02
8;2B#+*-7Bft%-230hH*
#%-#]_'),.8;#&Cd89't490-btft5+7F,.%71(+0d,e*
#H0?A')89,.8;Cd%-4K23715)h&8902dbg897e?%_:E%,1_:>,1*
? *
#+0730Da7
73*%-#+*-71(&0d,%-#&h71(+0d,10d>*-,10i8;286#&%-h+0^5&%730-
V O*-,10d*fiE-0d,bZ71(+0i2373,W5&CD715+,.%-4%-#&%')(&*-,.8;C%-CdCD0212189ft8;4;897_
23')%-CD0eCd%-#CD*E-0d,$*
#&49_71(&*
230eCd%-2302Y71()%7Z,10d>{0d,<73*IRM2Z8;#]73,1*^h)5&CD0h%7Y71(+0B*
5+71230d7<*->S71(+0eh)8;%-49*-L
5+0
P73*-')8;Cd2TWb)#+*-7Y71(+*
210e<8971(c%N<86#&h+* *->230#]730#&CD02W5&73730d,.%-#&CD02F%'&'&,1*
%-CW(V
 #HCD*
#&Cd4;5&2.89*
#bg897FU*
5&4;h%'&'\0%,B71(&%7e71(+0 ~}t#t#}q{v %-#)%')(+*-,.8;CN%-CdCD0212.89f)8;4;8;7_23't%-CD0A8;2F73*
f\0B'&,10d>0d,1,10hb+%7<4;0%-237Y>*-,<%-#&%')(+*-,W%N,1023*
4;5+718;*
#c8;#ch&8;%-49*-L
5&02dV

j

esy

f

y-FtEuw}q2su

`kxmw q$rtsu)v^wxl ~
+dxmsr[WN!w[
"R#)%')(+*-,2Y5&73730d,.%-#&CD0
O.l
O
O
O!
O
O
O
Oo
On
O.l'
O.l-l
O.l
O.l
O.lD

p

&' ,.*
#+*
?A8;#&%-4
%-#&%')(+*-,W%
%-#730CD0h+0#]712

]V 
+V;l
 'aV 

-]V'
o 'aV 
B
o-^V 
o-o^V 
nalV o
nalV o
n-^V;l
n-^V 
N

mn



n-o^V
n-o^V
l'1'aV'

N%-hm0CD7189E%-4

%-#&%')(+*-,W%
%-#730CD0h+0#]712
lo^V
-+V 
-^V 
-^V 

]V n
^lV'
^V 
^V 
oalV 
oalV 

m \

n+V 
n
]V
n
]V
l'1'aV'

XS%f)4;0k^pffLG?'t89,.8;Cd%-4\23715&h&_/*->K%-#&%')(+*-,W8;CB%-CdCD0212189ft8;4;897_/23')%-CD0Bft%-230hi*
#i%A<8;#&h+*fi *->K5&73730d,.%-#&CD02
?G

fi

$230hc'&,10d>0d,10#&CD02

L^'\0d,.8;?A0#7

I<*+V




p

{
{

{

{

{

{

{

{

{

{

{

{

{

l

'

r



{

 q
L^'\0d,.8;?A0#7
I<*+V
l

o

l

'

l

i{9  ] DA2

2{~i{

MG,1*
#+*
?A8;#)%-4%-#&%')(+*-,.%
     o n
{



{

{

{

l'

l-l

l

{

{

{

{

{

{

{

{

{

{

{

{

{

{

{

o  p


M,10Cd8;2.89*
#

"$h30CD7189E%-4%-#&%')(+*-,.%
     o

{

{
{

{

{

{

{

{

{

{

{

{

{

{

{

{

{

{

{

{

{

{

{

{

{

{

{

{
{

N

-n^V'
-^V 
^V o
oalV 
N

-^V 
-^V o
o^V n
oalV 

nWf+!
!mK[md!3sYWZd
+f !
 !mK[m d!3sY
 W Z
 d

b

XS%ft490B+pffLa'\0d,.8;?0#]7Z215&?A?%,1_

ts

\

Qys~E}q{sq{s

#}u.- uw}usuE~Eu

"$2B>{*-,k71(+08;?'\*-,171%-#)CD0@*->Zh+0D[#&8;#+L%-#%-h+0^5&%730/CD*
#)2373,.%-8;#]7e%-#&hH'&,.0d>{0d,10#)CD0A230d7ef)%-230h*
#h&8O
%-49*-L
5+02373,.5)CD715+,10/5)218;#+L71(+0/%-CdCD0212.89f)8;4;8;7_23')%-CD0h+0Dt#&0h8;#^0CD7189*
#`+bU0/f\0dL
8;#f]_%-h&*-'&718;#+L
71(+0CD*
#&2373,.%-8;#]7k%-#&h'&,10d>0d,10#&CD0210d7=h+0dE-049*-'\0hf]_H+0d,1, %-Q #&h+0d$s-ZPmln-n-n
T=%-#&h`h+021CD,.89f\0hf\0DO
49*fi 8;#a0CD7189*
#^V+V@X<(&8;2$CD*
#&2373,.%-8;#]7B%-#&h:'&,10d>0d,10#&CD0230d7e(&%-2Ff\0d0#21(+*fi<#:73*cf\0@%-h+0^5&%730>*-,
'&,1*
#&*
?A8;#&%-4%-#&h%-hm0CD718;E%-4Y%-#)%')(+*-,.%86##+*
#aOh&8;%-4;*-L
5+0ih&8;21CD*
5&,.230-V}XS*71(&8;2210d7dbY8;#+>*-,.?A%7189*
#
%f\*
5+7=h)8;%-49*-L
5+02173,.5&CD715+,10<8;4;4Kf\0%'&'t4;890h8;#*-,.h+0d,@73*71%-0c%-h+E%-#71%L-0*->Y89712@8;#at5+0#)CD0-VIR*-7
*
#&49_8;2Zh&8;%-4;*-L
5+0e2373,.5&CD715&,10e5&230h73*h&0Dt#+0F71(+0e%-#&%')(&*-,.8;CB%-CdCD021218;f)8;4;897_/23't%-CD0-b&f)5+7Z897Y8;2Z5)230h73*
h+0Dt#&0B'&,10d>0d,10#&CD02Y%-2ZU04;4V
+*-,=71(&8;2e21715&h+_-b230dE-0d,W%-40Da'[0d,W8;?0#]712B0d,.0Cd%,1,.890h*
5&7k5&218;#&L71(&0A73,.%-8;#&86#+LcCD*-,1')5&2dVAX<(+0230
0Da'[0d,W8;?0#]712@86#E-*
49E-0hCW(&%-#+L-02A86#`71(&0CD*
#)2373,.%-8;#]7A%-#&h'&,10d>0d,10#&CD0c230d7A8;#*-,.h&0d,@73*h&0Dt#+071(+0
CD*
#a)L
5&,.%7189*
# (( 71(&%7YU*
5&4;h(&%E-0e*-')718;?=5)? '&,10Cd8;2.89*
#VKY0215&4;712Y%,10k215)?A?A%,.89d0h8;#XS%f)4;0B+V

uvxwvzy|{~}C}CQfi9FFFJ)J

XY(+0d,.0%,107U*:h&89[0d,10#]7=%')'&,1*
%-C.(&02k73*?A%-#&%L
8;#&L71(+0/'&,10d>0d,10#&CD0/210d7db*-,Wh+0d,10h?%-#&%L-0?0#]7
%-#&hU089L
(730hc?%-#&%L-0?0#]7dV$,.h&0d,10h?A%-#&%L-0?0#]7Y8;2Uf)%-230h*
#ch&8;2.Cd%,.h&8;#+L=71(+*
230B%-#]730CD0h+0#712
71(&%7Kh+*R#&*-7>Q5&4t4;4-%<'&,10d>0d,10#&CD0U89>a71(+0d,10862%-#]_kCd%-#&h&8;h)%730G71(&%7S>5&49t4;4;2897dV  089L
(]730h@?%-#&%L-0?0#]7
8;2f)%-230h*
#%-21218;L
#&8;#+L@%=U089L
(]773*@0%-CW('&,10d>0d,10#&CD0B%-#&h/71(+0#c230490CD7186#+L=71(&0$Cd%-#&h)8;h&%730F<8971(/71(+0
?A%+8;?@5&?E%-465+0-V
+*-,G71(+02300D^'\0d,.86?0#712b-71(+0Z23_^21730? U%-273,W%-8;#+0hN23*B%-2K73*B*-f&71%-8;#@71(+0Zf\0237K230d7*->)'),10d>{0d,.0#&CD02dV
a5+ft230]5&0#7149_-b[U0N%')')4;890hf[*-71(:*->71(+0@%')'&,1*
%-C.(&02F73*'),10d>{0d,.0#&CD0?A%-#&%L-0?0#]7B73**-f&71%-8;#:71(+0
f\0237A,102.5&497N<8971(71(&0i73,.%-8;#&86#+LHCD*-,1')5&2dV F#&CD0iU0i*-f&71%-86#+0h71(+0f[0217A230d7*->F'&,10d>0d,10#&CD02/%-#&h
Jh\!6 o6#$2ki+-n2x#$AS3B!!<fi`!GK
[Q.!KB!
!mmS!.YDF!fi
QQ! fi3e
.SUmfi!!Qm
+!Kfi-!Z.
fi!dmmm
?d

fi@ 2~,7



2	AK$)B

$CA



2B2{EDX<{$7

 G
 F





7BEHK.{JI22

89712f\0237?A%-#&%L-0?0#]7dbRU00dE%-465&%730h 71(+0H23_^21730? R8971(}71(&0:730217CD*-,.')5&28;#*-,.h+0d,73**-f)71%-8;#
8;#&h&0d'[0#)h+0#7U,10215)49712dV
^*+ba>*-,0D^'\0d,.8;?A0#71
2 'ablba%-#)h^ba*-,.h&0d,10h?A%-#&%L-0?0#]7U%-2Z%'&'t4;890hA73*N*-f&71%-86#71(+0Ff\0237U230d7
*->Y'&,.0d>{0d,10#)CD02dVXY(&0#b86#0Da'[0d,W8;?0#]7@^bU0%'&'t4;890hHU089L
(]730h?%-#&%L-0?0#]7@73*8;?'),1*E-071(+0
,10215)49712Z8;#71(+0e73,W%-8;#&8;#+LNCD*-,1')5)2dV



\

 2x c

\iuw}'z|uws

?

qg~Euv's#u z

's{t=~E'sZ-1y2}ffz

qys

ys#v=

0 f\0dL
%-#<8;71(71(+0iCD*
#&2373,.%-86#7A%-#&h'),10d>{0d,.0#&CD0i230d75&210hf]_+0d,1, %-Q #)h+0dd=sFPmln-n-n
TWV XY(&8;2
 i
%-49L-*-,.8;71(&? 8;2f)%-230h*
#468;#+L
5&8;21718;C8;#+>*-,.?A%7189*
#*
#)49_-bY%-#&h}8;712,102.5&49712A(&%E-0f[0d0#215&CdCD02121>5&4649_
73023730hN*fiE-0d,G%F#+*
#aOh&8;%-4;*-L
5+0UCD*-,.')5&2<8971(@%$,10215&497186#+L<'&,.0Cd8;2189*
#k*->[o- N >*-,'&,1*
#+*
?A86#&%-4%-#&%'t(+*-,.%
,1023*
465+7189*
#PU0k(&%E-0@#+*A8;#+>*-,.?A%718;*
#c%f[*
5&7Z71(+0B'&,10Cd8;2.89*
#>{*-,R%-hm0CD718;E%-4g%-#)%')(+*-,.%TWV
XY(&0@8;#)89718;%-4gCD*
#atL
5+,.%7189*
#:8;#&Cd4;5)h+0hc71(+0@>{*
4649*<86#+LCD*
#&2373,W%-8;#7F%-#&h:'&,10d>0d,10#&CD0N230d7F%-#&h:h+0D&O
#&89718;*
#*->%-#&%'t(+*-,.8;CB%-CdCD0212.89f)8;4;8;7_/21')%-CD0-V

uvHCvzyhFFQJ}C`F@fifi?`aafiFfi

+ *-,'&,1*
#+*
?A86#&%-4]%-#)%')(+*-,.%F,1021*
4;5+7189*
#b-71(&0Y%-#&%')(+*-,.86C%-CdCD0212.89f)8;4;8;7_=21')%-CD0YCD*
#&218;21730h*->t71(&0Z71(+,10d0
'&,10dEa89*
5&2G715+,.#&2G73*N71(+0R%-#&%'t(+*-,V+*-,Y%-h30CD7189E%-4t%-#&%'t(+*-,.%ab^71(+0$21')%-CD0$CD*
#)218;23730h/*->g71(+0R'&,10dEa89*
5&2
>*
5+,Y715+,.#)2dV

uvHCv0JQS@}CFaJC

 #c71(+0eCd%-210k*->'&,1*
#+*
?A86#&%-4\%-#&%')(+*-,.%ab)71(+0kCD*
#&2373,W%-8;#712<8;#&Cd4;5&h&0hp

{

lV<?A*-,1')(+*
49*-L
86Cd%-4%L-,10d0?0#]7dpYh&8;2.Cd%,.hi71(+0@%-#730CD0h&0#712F<()8;C.(%,10=8;#)CD*
?')%7189f)4;0e?*-,3O
't(+*
49*-L
8;Cd%-4;4;_:PL-0#&h+0d,b)#]5)?kf\0d,b+%-#&hc'\0d,.21*
#tT
^V<21_^#]71%-CD718;CCD*
#]730Da7dpNh)8;21Cd%,.hH71(&0%-#730CD0h&0#712@<(&86C.(%,10/#+*
#aOCD*O!,10d>0d,10#]7@%-CdCD*-,.h)8;#+L
73*/J%'&')86#c%-#&hcJg0%-212@Pmln-n]T
 #c71(+0eCd%-210k*->K%-hm0CD718;E%-4S%-#&%')(+*-,.%ab+71(&0kCD*
#&2373,.%-86#712Y86#&Cd4;5&h+0hgp
{

lV<?A*-,1')(+*
49*-L
86Cd%-4%L-,10d0?0#]7dpYh&8;2.Cd%,.hi71(+0@%-#730CD0h&0#712F<()8;C.(%,10=8;#)CD*
?')%7189f)4;0e?*-,3O
't(+*
49*-L
8;Cd%-4;4;_:PL-0#&h+0d,DT
^V<MG,1*-'\0d,3O#+*
5&#+O!')(+,.%-230F0D+Cd4;5&218;*
#p0D+Cd4;5&h+0e#+*
5)#c')(+,.%-2102Y(&%Ea8;#+L/%'&,1*-'\0d,<#+*
5)# ( - %-2
(&0%-h

uvHCvxw{~}C}CQfifi
{

 #c71(+0eCd%-210k*->'&,1*
#+*
?A86#&%-4\%-#&%')(+*-,.%ab)71(+0e'&,10d>0d,10#&CD02<%,10B>{*-,p
l V<Cd%-#)h&8;h&%7302Y86#71(+0k21%-?0k715+,.#c%-2Y71()%7Y*->S71(&0k%-#&%')(+*-,
^V<Cd%-#)h&8;h&%7302Y86#71(+0e'&,10dEa89*
5&2715&,.#
^V<Cd%-#)h&8;h&%730B'&,.*-'[0d,Y#&*
5&#&2*-,R86#&h+0Dt#&8;730$I$MG2

Jf+!-\fifi.!Wgfi. qZfiZ`3_<Wm!Dmmh\iW<Wm1!-\fi!$.3W
.!
 fiWg DmR3 fi 1!mW d!mmmfi d!g 
&W m!.a Wfi.!m
?d	

fi

i{9  ] DA2

2{~i{

+V=P>*-,<'\0d,.21*
#&%-4\'&,1*
#+*
5)#&2WTCd%-#&h&8;h)%730B'&,1*-'\0d,Y#+*
5&#&2
^V<Cd%-#)h&8;h&%7302S71(&%7S()%E-0f\0d0#e,10d'\0%730h@?*-,10G71()%-#k*
#&CD0BP,10d'\0%730h=>{*-,.?2%-#&h=,10d'\0%730h
?A0#7189*
#)2WT
^V<Cd%-#)h&8;h&%7302<71(&%7R(&%E-0=%'&'\0%,10h?*-,10e71()%-#i*
#&CD0@8;#cCD*
#&2373,.5)CD7189*
#i<8;71(c71(+0eE-0d,1f:8;#
CD*
#)2373,.5&CD7189*
#i<8971(71(+0k%-#)%')(+*-,
]V<Cd%-#)h&8;h&%730286#=71(&0Y21%-?0Z'\*
21897189*
#N%-271(&0Y%-#&%')(+*-,<8;71(N,10d>{0d,.0#&CD0Z73*B71(+0ZE-0d,1fPf\0d>*-,10
*-,$%>{730d,DT
o^V<Cd%-#)h&8;h&%7302Y86#71(+0k21%-?0k'[*
2.897189*
#<8971(,10d>0d,10#&CD0k73*A71(+0k5+73730d,W%-#&CD0k%-2Y71(&0e%-#&%')(+*-,
n^V<Cd%-#)h&8;h&%7302Y#&*-7<8;#cCd89,.Cd5)?A2371%-#]718;%-4%-h15&#&CD712
l'aV<Cd%-#)h&8;h&%7302Y?A*
237Y,10d'\0%730hi8;#i71(+0B730D^7
l-lV<Cd%-#)h&8;h&%7302=?*
237k*->730#%')'[0%,W8;#+Li8;#CD*
#)2373,.5&CD7189*
#<8971(H71(&0AE-0d,1f`86#CD*
#&2373,.5)CD7189*
#
R8971(71(+0k%-#&%'t(+*-,
l^VY71(&0kCd49*
230237RCd%-#&h&8;h&%730B73*71(+0k%-#&%')(&*-,
{

 #c71(+0eCd%-210k*->K%-hm0CD718;E%-4S%-#&%')(+*-,.%ab+71(&0e'&,10d>0d,10#&CD02Y%,.0e>{*-,p
l V<Cd%-#)h&8;h&%7302Y86#71(+0k21%-?0k715+,.#c%-2Y71()%7Y*->S71(&0k%-#&%')(+*-,
^V<Cd%-#)h&8;h&%7302Y86#71(+0e'&,10dEa89*
5&2715&,.#
^V<Cd%-#)h&8;h&%7302K21(&%,W8;#+L<71(&02.%-?0a8;#&h=*->)?*ah&8t0d,%-271(+0Z%-#&%')(&*-,P0-VL+V9b^%R'),10d'\*
21897189*
#&%-4
't(+,.%-230fiT
+V<Cd%-#)h&8;h&%7302e2.(&%,.8;#+L71(+0A2.%-?0?*ah&8)0d,B%-2B71(&0A%-#&%')(+*-,/P0-VL+V9b71(+0A21%-?A0%-h30CD7189E-0-p
,.*3%a&,.0htT
^V<Cd%-#)h&8;h&%7302Y%L-,.0d08;#+L8;#c#^5&?=f\0d,
^V<Cd%-#)h&8;h&%7302Y?A*
237Y*->730#i,10d'\0%730hi8;#71(&0e730D^7
]V<Cd%-#)h&8;h&%7302=%'&'\0%,.8;#+Li?*
217k*->730#8;#CD*
#)2373,.5&CD7189*
#<8971(H71(&0AE-0d,1f`86#CD*
#&2373,.5)CD7189*
#
R8971(71(+0k%-#&%'t(+*-,
o^VY71(&0kCd49*
230237RCd%-#&h&8;h&%730B73*71(+0k%-#&%')(&*-,

uvHCvHh`?zF

F89E-0#71(&8;2R),.237BCD*
#atL
5+,.%7189*
#bS%-#0dE%-4;5)%7189*
#U%-2kCd%,1,.890h*
5+7BR(&8;C.(,.0215&49730h8;#H%'&,.0Cd8;2189*
#
*->-n^V',N>{*-,Z'&,.*
#+*
?A8;#&%-4t%-#&%')(+*-,W%=,.023*
4;5+7189*
#%-#&hc-^V &N >*-,Y%-h30CD7189E%-4[%-#)%')(+*-,.%@,1023*
465+7189*
#V
IR0d0h&490212N73*H21%_-b71(+0210,.0215&49712N%,10E-0d,1_49* >*-,N'&,1*
#+*
?A86#&%-4%-#)%')(+*-,.%%-#&h0Da73,10?049_`'[*^*-,
>*-,R%-h30CD7189E%-4%-#&%')(&*-,.%aV  #i0dE%-4;5&%7186#+L71(+0k0d,.,1*-,.2db&U0=CD*
#)Cd4;5&h+0hc71()%7Y71(+0=h&0Dt#+0hi%-#&%'t(+*-,.8;C
%-CdCD021218;f)8;4;897_23't%-CD0%-2e73*^*CD*
#&2373,.%-86#+0h%-#&hH73*^*%,.f)8973,.%,.8649_h+0Dt#&0hV  7=218;?')4;_h&8;23,.0dL
%,.h+0h
71(+0e,.04;%7189*
#&21()89'cf\0d7U0d0#%-#&%')(&*-,.%A%-#&hh&8;%-49*-L
5+0e2173,.5&CD715+,10-_
V VU*
#&230^5+0#]7149_-b&U0k'&,.*-'[*
210hc71(+0
>*
4;49*R8;#+LC.(&%-#&L-02Y>*-,Y71(+0k230CD*
#)hc0D^'\0d,.86?0#7dV



\ +

\iuw}'z|uws







q2vy {t#u

~}fftt}uZs - y{}z|qys

y{sv=

+*-,R71(&8;20Da'[0d,W8;?0#]7db^71(&0eh+0Dt#&89718;*
#/*->71(+0e%-#&%'t(+*-,.8;CF%-CdCD021218;f)8;4;897_23')%-CD0kU%-2YCW(&%-#+L-0h73*A0?NO
')49*fi_B8;#+>*-,.?A%7189*
#B'),1*Ea8;h+0hBf^_B71(+0Uh&8;%-49*-L
5+02373,.5&CD715+,.0-b%-2215+L-L-023730h=f^_(O%,17?R Q #+0dDO%,.CD*NPmln-n-n
TWV
 #e%-h&h&89718;*
#b71(+0G'&,10d>0d,10#&CD02%\0CD730h@f]_F71(&862,10dEa8;230heh+0D[#&897189*
#F*->+%-#&%')(&*-,.8;C%-CdCD0212.89f)8;4;8;7_R21')%-CD0
U0d,10k?*ah&8)0h%-2<04;4!V


fi@ 2~,7



2	AK$)B

$CA



2B2{EDX<{$7

 G
 F





7BEHK.{JI22

uv0fivzyhFFQJ}C`F@fifi?`aafiFfi

XY(+0$%-h1%-CD0#&CD_')%-89,U%-#&h71(&0<73*-')8;C<*->71(+0Rh)8;%-49*-L
5+0R0d,10$5&230h/8;#A*-,.h&0d,73*@h+0Dt#+0R71(+0R%-#&%'t(+*-,.8;C
%-CdCD021218;f)8;4;897_A23')%-CD0-
V VU*
#)CD,10d73049_-ba0Bh+0D[#+0h/%-#%-#&%'t(+*-,.8;CR%-CdCD0212189f)864;897_A21')%-CD0Rf^_?0%-#)2*->S71(+0
%-h1%-CD0#&CD_')%-89,N*-><71(+0i%-#)%')(+*-,b71(&0'),10dE^8;*
5&2N%-h3%-CD0#)CD_`')%-8;,@73*H71(&0%-h3%-CD0#&CD_')%-89,N*->$71(+0
%-#&%')(&*-,bG%-h1%-CD0#&CD_')%-89,.2NCD*
#]71%-8;#&8;#+L:71(+0i%-h1%-CD0#&CD_')%-89,@*->R71(&0%-#)%')(+*-,bG%-#)hbt#&%-4649_-b71(+0
?A%-8;#73*-'t8;CB*->71(+0kh&8;%-4;*-L
5+0AP>{*-,R'&,1*
#+*
?A86#&%-4\%-2Z0464g%-2Y%-h30CD7189E%-4g%-#&%')(+*-,W%TWV

uv0fiv0{~}C}CQfifi

+*-, La'[0d,W8;?0#]7lbU0c,.0?*E-0h}71(+0c'&,1*
#+*
?8;#&%-4%-#&%')(+*-,W%'&,10d>0d,10#&CD02h+021CD,W89f\0h%f\*fiE-08;#
89730?A2=71(+,1*
5+L
(:l-lF%-#&h/71(+0$%-hm0CD7189E%-4\%-#&%')(+*-,W%e'&,10d>0d,10#&CD02h+02.CD,.89f\0h8;#/89730?A2Zk71(+,1*
5+L
(c]V
"$4;23*+ba'&,10d>0d,10#&CD02FlF%-#&hc@%f[*fiE-0F0d,10F,10d't4;%-CD0hf]_/71(+0R>*
4;49*fi<8;#+L=>{*
5&,#&0d '),10d>{0d,.0#&CD02dVXY(^5&2db
f\*-71(c>*-,Y'&,1*
#+*
?8;#&%-4\%-#&hi%-h30CD7189E%-4g%-#&%')(&*-,.%ab+71(+0e'&,.0d>{0d,10#)CD02Y%,10e>*-,p
{

lVCd%-#&h&8;h&%7302<8;#71(+0k21%-?A0k%-h1%-CD0#&CD_')%-89,Y%-2Y71()%7Y*->S71(&0k%-#&%')(+*-,
{

^VCd%-#&h&8;h&%7302<8;#71(+0e'&,.0dE^89*
5)2Z%-h3%-CD0#)CD_'t%-89,Z73*A71(+0k%-#)%')(+*-,

oG
V Cd%-#&h&86h&%7302Y8;#c%-#]_c%-h3%-CD0#)CD_'t%-89,YCD*
#71%-86#&8;#+L71(+0=%-h3%-CD0#)CD_'t%-89,Z*->S71(&0k%-#&%')(+*-,
{

pDVCd%-#&h&8;h&%7302Y71(&%7<%,10e8;#i71(+0B73*-')8;C
{

XY()8;2YC.()%-#+L-0=%-2F?A%-h+0k86#i*-,.h+0d,R73*/730217R71(+0@23_a23730?c2R'[0d,.>{*-,.?%-#&CD0e<(+0#4;8;#+L
5)8;23718;CF8;#+>*-,3O
?A%7189*
#8;2R,10?*fiE-0h%-#&h:*
#&49_ih&8;%-4;*-L
5+0=2373,W5&CD715+,10@8;#+>*-,.?A%7189*
#8;2R5)230hP'),10d>{0d,.0#&CD02Nl=71(&,1*
5+L
(
 p.TWV  #*-,Wh+0d,A73*HL
5&%,.%-#]730d0%H2.8;#+L
490/t#&%-4Z23*
465+7189*
#b*
#)49_`4;86#+L
5&8;237186CA'&,10d>0d,10#&CD0c89730? l^b>*-,
'&,1*
#&*
?A8;#&%-4\%-#&%')(&*-,.%ab)%-#&hc89730?o^b&>{*-,$%-h30CD7189E%-4%-#&%')(+*-,.%iP71(&0kCd49*
230237RCd%-#&h&8;h&%730fiTWb+,.0?A%-8;#V

uv0fivxwh`?zF

"R>{730d,86#&Cd4;5&h&86#+Li8;#+>*-,.?A%7189*
#%f\*
5+7h&8;%-4;*-L
5+0c2373,.5&CD715+,.0%-#)h`,.0?*Ea8;#+LH71(+0c4;86#+L
5&8;237186C'&,10d>0d,3O
0#&CD02dbF'&,.0Cd8;2189*
# ,.%7302c,.*
230H73*-^V N >*-,i'&,.*
#+*
?A8;#&%-4$%-#&%')(&*-,.%,1023*
4;5&7189*
# %-#&h-^V oN >*-,
%-h30CD7189E%-4[%-#)%')(+*-,.%=,1023*
4;5&7189*
#V"CD*
#&2.8;h+0d,.%f)4;0<8;#&CD,10%-210F862GL
%-8;#&0h86#71(+0F,1021*
4;5+7189*
#/*->%-h30CO
7189E%-4%-#&%'t(+*-,.%Af^_i218;?A')49_C.(&%-#+L
86#+L71(+0@h+0Dt#&8;7189*
#*->71(+0@%-CdCD0212189f)864;897_c23't%-CD0-V<XY(&%7$8;2<h&5&0e73*
71(+0e>Q%-CD7Y71(&%7R%-hm0CD718;E%-4g%-#)%')(+*-,.%#+0d0h%A4;%,1L-0d,<21')%-CD0e71(&%-#c71()%7<5&230hi86
# La'[0d,W8;?0#]7 'aV
5+7K71(+0230,102.5&49712K%,10Z23718;4;4]49*%-#)h@h&0?*
#&2373,.%73071(&%7h)8;%-49*-L
5+02373,.5&CD715&,10U86#+>{*-,W?A%7189*
#@%-49*
#+0
8;2e#&*-7=21P5 KCd890#7dVXY(^5&2dbS%i71(&89,.hH0Da'\0d,.8;?0#]7B%-2kCd%,1,.8;0h*
5+7k5)218;#+Lcf\*-71(h&86%-49*-L
5+0A2373,.5)CD715+,10
%-#&h4;8;#&L
5&8;23718;C86#+>{*-,W?A%7189*
#V^0dE-0d,.%-4E%,.8;%7189*
#)2=8;#`71(+0'&,10d>0d,10#&CD023_a23730? 0d,.086#E-023718;L
%730h
8;#&h&0d'[0#)h+0#714;_-V



\

x f

\iuw}'z|uws 
 y2}
 u}ur



's{t/~'s - y2}ffz|qy{s

z|q2sq 2uz

v't~bq2vy{t#uC~E}t#t#}u
usOy-q}uu-1u}uws#u)~ z

's - y2}ffz|qy{s

+*
4;4;*<8;#&L+b+71(+0e'&,10d>0d,10#&CD02<5&230hc8;#71(&862Z0D^'\0d,.86?0#7Z%-#&hi71(+089,15&23718tCd%718;*
#c%,10k21(+*fi<#V

uvxuvzy|{~}C}CQfifi

 #=71()8;20Da'\0d,.8;?0#]7db0),.2175&210h=%$'&,10d>0d,10#&CD0230d7K71(&%78;#&Cd465&h+0he%-4;4]71(+0Z4;8;#&L
5&8;23718;C%-#)h@h)8;%-49*-L
5+0
2373,.5)CD715+,10c'&,10d>0d,10#&CD02/h+021CD,.8;f[0h%f\*fiE-0-V X<(+0#bGE%,.89*
5)2A%-49730d,.#&%718;E-02U0d,105&210h8;#*-,.h&0d,A73*



fi

i{9  ] DA2

2{~i{

*-f&71%-8;#:%-#:*-'&718;?A%-4SCD*
#a)L
5+,W%7189*
#VF"R2$%,.0215&497db[71(+0@>{*
4;4;*<8;#&LA'&,10d>0d,10#&CD02$U0d,10@%,.,.89E-0h:%7R>*-,
71(+0B[#&%-4CD*
#a)L
5+,W%7189*
#p
{ &*-,<'&,1*
#&*
?A8;#&%-4\%-#&%')(&*-,.%ab&71(+0e'),10d>{0d,.0#&CD02Y%,10B>*-,p
 h)8;%-49*-L
5+0e2373,W5&CD715+,10B'&,.0d>{0d,10#)CD02dpYlB71(+,1*
5&L
(p
 468;#+L
5&8;21718;C<'),10d>{0d,.0#&CD02dpG^b[]bto^bt%-#&hl
{ &*-,R%-h30CD7189E%-4g%-#&%')(+*-,W%ab&71(+0e'&,.0d>{0d,10#)CD02Y%,10e>*-,p
 h)8;%-49*-L
5+0e2373,W5&CD715+,10B'&,.0d>{0d,10#)CD02dpYlB71(+,1*
5&L
(p
 468;#+L
5&8;21718;C<'),10d>{0d,.0#&CD02dpG^b&+bt^bt%-#&ho
XY()8;2Ut#&%-4230d7<*->CD*
#&2173,.%-8;#]712<%-#&h'&,.0d>{0d,10#)CD02<8;271(+0k*
#+0B71(&%7<8;2Y'&,10230#]730hi8;#^0CD718;*
#^V ^V

uvxuv0h`?zF>JF5QS@0z@zF

 7Y21(+*
5&4;hf\0B#+*-730hc71(&%7R8;#+>*-,.?A%7189*
#%f\*
5+7,10d'\0%730hiCd%-#&h&86h&%7302x!>*-,Z0Da%-?A')490-b+71(+0B'&,.*
#+*
?NO
8;#&%-4\%-#&%'t(+*-,.%N'&,10d>0d,10#&CD02Y^bSl 'ab&%-#&hl-lb+*-,Y71(+0e%-hm0CD7189E%-4%-#&%')(&*-,.%N'&,10d>0d,10#&CD02YN%-#&h: ! 8;2
5&215)%-4;49_i8;#&210d,1730h86#73*71(+0N'&,10d>0d,10#&CD023_a23730? 8;#:*-,.h&0d,$73*i%-CW(&890dE-0a#+*R490h+L-0%f\*
5+7F71(+0?A%-8;#
0#]7189718902Y*->71(&0=h&8;%-4;*-L
5+0-V j *fi0dE-0d,b8;#71(&8;2Z0Da'\0d,.8;?0#]7db)8;#&>{*-,.?%7189*
#i%f\*
5+7<71(&0=?A%-8;#i73*-')8;Ce*->
71(+0Uh&8;%-49*-L
5+0(&%-2f\0d0#=8;#&Cd4;5&h&0he%-#&hk21*R8;#+>*-,.?A%718;*
#e%f\*
5+7,10d'\0%730h@Cd%-#&h&8;h)%7302S8;2S5&#&#+0CD02.21%,1_-V
XY(+*
210B'&,10d>0d,10#&CD02ZU0d,10e71(+0d,.0d>{*-,10k,10?*E-0hgbt8;?'),1*Ea8;#+L@71(+0e,102.5&49712dV
&5&,171(+0d,.?*-,.0-b^U0B>*
5&#&h/71(&%7Z'&,1*
#+*
?8;#&%-4)%-#)%')(+*-,.%@'&,10d>0d,10#&CD02Y@%-#&hN>*-,Z'&,1*-'\0d,U#&*
5&#&2
Cd%-5&230h0d,.,1*-,.2dVXY()8;2862f\0Cd%-5&210F86#71(+0Bh+*
?%-8;#*->71(+0F0Da'[0d,W8;?0#]771(+0d,.0F862U%-#0D+%L-L-0d,.%730h5&230
*->g')46%-CD0R#&%-?02<(+0d,.0<71(+0230$'&,10d>0d,10#&CD028;#)CD*-,1,10CD7149_%'&')4;_-VKU_A,.0?*Ea8;#+L=71(+0?cbaf\0d73730d,,102.5&49712
U0d,10e*-f&71%-8;#&0hV
K8;#&%-4649_-b218;#&CD0N71(+0A5&210d>5&46#+0212$*->'),10d>{0d,.0#&CD0AnHPQCd%-#&h&8;h&%7302e71(&%7e%,10A#+*-7e8;#Cd89,WCd5&?A2371%-#]718;%-4
%-h15&#&CD7WT(&%-2#+0dE-0d,Zf\0d0#k35)23718)0h/'&,1*-'\0d,.49_-b^89773*]*N%-2*
?A8973730hV"R>730d,Z,10?*E%-4b+71(+0F'&,.0Cd8;2189*
#
>*-,Y'&,1*
#+*
?8;#&%-4\%-#&%')(+*-,W%2371%_-0h71(+0k21%-?A0-V
XY(^5&2dbK(&%E^8;#&LCD*
#&2186h+0d,10h%-464'\*
212189ft490A%'&')4;86Cd%7189*
#&2B>*-,=*-,Wh+0d,10h`'&,10d>0d,10#&CD0/?%-#&%L-0?0#]7db
%-#&hcL
8;E-0#i71(&%7R71(&8;2Ut#&%-4230d7Y*->K'&,10d>0d,10#&CD02<,10d'&,10230#]730hi71(&0k?A8;#&86?=5&?230d7<*->'),10d>{0d,.0#&CD02db&U0
CD*
#&2186h+0d,10h897A73*f[0c71(&0i*-'&718;?A%-4<230d7dV  071(+0#%'&')46890h71(&8;2*-'&7186?A%-4Y230d7A*->F'&,.0d>{0d,10#)CD0273*
71(+0$73,.%-8;#&86#+LkCD*-,1')5)2db]*-f)71%-8;#&8;#+L@%k'),10Cd8;2189*
#*->^V o N>*-,U'),1*
#+*
?A8;#)%-4&%-#&%')(+*-,W%k,1021*
4;5+7189*
#%-#&h
o^V n N >{*-,<%-hm0CD7189E%-4g%-#&%'t(+*-,.%aV

`

\

x

f

s 

\iuw}'z|uws
' s{t/~'s - y2}ffz|qy{s v't~bq2vy{t#uC~E}t#t#}u

uw/
 urIz 2
q s#qg{uwz|uwsOy-q}u.- uw}us#ur~ z

f

's - y2}ffz|qy{s

+*
4;4;*<8;#&L+b+71(+0e'&,10d>0d,10#&CD02<5&230hc8;#71(&862Z0D^'\0d,.86?0#7Z%-#&hi71(+089,15&23718tCd%718;*
#c%,10k21(+*fi<#V

uv0fivzy|{~}C}CQfifi

 #71(&8;2Rt#&%-4K0Da'\0d,.8;?0#]7db71(+0N'&,10d>0d,10#&CD0A230d7B*-f)71%-8;#+0hH8;#:71(+0'&,.0dE^89*
5)2R0Da'\0d,.8;?0#]7FU%-2k5&230h
PQ8;#&Cd465&h&8;#+L=71(+0k?A86#&8;?@5&?210d7Y*->S'),10d>{0d,.0#&CD02Y71(&%7ZU0kCD*
#&2.8;h+0d,10h73*Af\0B71(+0k*-'&718;?A%-4CD*
#atL
5+,.%O
7189*
#tTWVX<(+0#b+230dE-0d,.%-4%-49730d,W#&%7189E-02ZU0d,10e5&230h8;#*-,.h&0d,U73**-f)71%-8;#c%-#*-'&718;?A%-4['&,.0d>{0d,10#)CD0$U089L
(]7
%-21218;L
#&?0#]7dVX%f)4902:%-#&h:21(+*71(+0'&,.0d>{0d,10#)CD0/U089L
(]7N%-2.2189L
#&?0#]712@>{*-,N'&,.*
#+*
?A8;#&%-4%-#&h
%-h30CD7189E%-4g%-#&%')(+*-,W%ab&,1023'\0CD7189E-049_-V
XY()8;2Ut#&%-4230d7<*->CD*
#&2173,.%-8;#]712<%-#&h'&,.0d>{0d,10#)CD02Y%-2Z'&,10230#]730h86#^0CD7189*
#^V ^V
fi

fi@ 2~,7

MG,10d>VIR*+V
l


o



p



o
l



2	AK$)B

$CA



2B2{EDX<{$7

 G
 F





7BEHK.{JI22

$021CD,.8;'&7189*
#
"$#730CD0h&0#712R71(&%7<%,10=8;#71(+0k21%-?A0e"RM %-2Z71(+0k%-#)%')(+*-,
"$#730CD0h&0#712R71(&%7<%,10=8;#71(+0e'&,.0dE^89*
5)2U"$M73*A71(&%7
CD*
#]71%-8;#&8;#&L71(+0e%-#&%')(+*-,
"$#730CD0h&0#712R71(&%7<%,10=8;#71(+0k?*
217Z,10CD0#7$5&#&Cd49*
210hc"RM
"$#730CD0h&0#712$8;#71(+0e73*-'t8;C
"$#730CD0h&0#712R71(&%7<%'&'\0%,YR8971(71(+0eE-0d,1f*->S71(+0=%-#&%')(+*-,<?A*-,10
71(&%-#i*
#&CD0
"$#730CD0h&0#712R71(&%7<%,10=8;#71(+0k21%-?A0B'[*
2.897189*
#<8971(,.0d>{0d,10#)CD0
73*A71(&0eE-0d,1f%-2<71(+0e%-#&%')(+*-,@Pf[0d>*-,10B*-,$%>{730d,T
"$#730CD0h&0#712R71(&%7<%,10=8;#71(+0k21%-?A0B'[*
2.897189*
#<8971(,.0d>{0d,10#)CD0
73*A71(&0k5+73730d,.%-#&CD0=%-2Y71(+0e%-#&%'t(+*-,
XY(&0e#+0%,10237<%-#]730CD0h+0#]7<73*A71(&0k%-#&%')(+*-,

 089L
(]7
-
B'

B'
l



"L

X%f)490k^pGMG,10d>0d,10#&CD0BU089L
(7R%-212189L
#&?A0#7Z>*-,<'&,.*
#+*
?A8;#&%-4\%-#&%'t(+*-,.%

MG,10d>VI<*+V
l


o



p


o



$021CD,.8;'&7189*
#
"$#730CD0h&0#712$71(&%7<%,.0e8;#71(+0k21%-?A0k"RM%-2Y71(+0e%-#)%')(+*-,
"$#730CD0h&0#712$71(&%7<%,.0e8;#71(+0e'&,.0dE^89*
5)2Z"RM}73*A71(&%7
CD*
#]71%-8;#&8;#&L71(+0k%-#&%')(&*-,
"$#730CD0h&0#712$71(&%7<%,.0e8;#71(+0k?*
217Y,10CD0#]7<5&#&Cd49*
210hc"RM
"$#730CD0h&0#712F8;#71(+0B73*-'t8;C
"$#730CD0h&0#712$71(&%7<2.(&%,10B71(+0e21%-?A0e^86#&h*->?A*^h&89)0d,.2
"$#730CD0h&0#712$<8971(0D+%-CD7149_71(+0e21%-?A0k?*ah&8)0d,.2
"$#730CD0h&0#712$71(&%7<%L-,.0d0k8;#c#^5&?kf\0d,
XY(&0e#+0%,10237<%-#]730CD0h+0#]7R73*71(+0e%-#&%')(+*-,

X%f)490k^pGMG,10d>0d,10#&CD0eU089L
(]7<%-212189L
#)?0#7Z>*-,<%-h30CD7189E%-4g%-#&%')(&*-,.%



 089L
(7
-
l'

l'

-



"L

fi

i{9  ] DA2

2{~i{

uv0fiv0h`?zF>JF5QS@0z@zF

 #`*-,Wh+0d,N73*:*-f)71%-8;#71(+0*-'&718;?A%-4U'&,10d>0d,10#&CD0U089L
(7%-212.89L
#&?0#]7dbU0'\0d,1>*-,.?0h230dE-0d,.%-473021712
<8971(71(+073,.%-86#&8;#+L:CD*-,1')5&2dV`XY(]5)2db%>{730d,A4;*]*-a8;#+L%7A%-4;4U71(+0'\*
212189f)864;89718902B%-#&hL
8;E-0#`71()%771(&8;2
%-2i71(+0210d7i*->N'&,10d>0d,10#&CD02i(&%E^86#+L71(+0f\0237c,102.5&49712db$0`CD*
#&218;h+0d,.0h897c73*f\0H71(+0*-')718;?A%-4
CD*
#a)L
5&,.%7189*
#V  8;71(`71()8;2=CD*
#+)L
5+,.%7189*
#gb0*-f&71%-8;#+0h%'&,10Cd8;2.89*
#*->$Bo 'aV  N >{*-,N'&,1*
#&*
?A8;#&%-4
%-#&%')(&*-,.%N,1023*
4;5&7189*
#c%-#&hn-^V;il N >{*-,R%-hm0CD718;E%-4g%-#)%')(+*-,.%N,1021*
4;5+7189*
#V
\

n

's#q2v,u,|q2v't#qy{s

x
u)~y{}!t~ z

$218;#+L71(+0Nt#)%-4S'),10d>{0d,.0#&CD0A230d7eh+0Dt#&0hH8;# La'[0d,W8;?0#]7Fc%-#)h71(+0'&,1*-'\*
230hCD*
#&2373,W%-8;#7e210d7dbS%
f)4;86#&hN0dE%-465&%7189*
#/U%-2UCd%,1,.890h*
5+7U*E-0d,71(+0<0#]7189,10R730237CD*-,1')5&2dVXY()8;20dE%-4;5&%7189*
#/U%-2'\0d,1>*-,.?0h
8;#&h&0d'[0#)h+0#7N*->$71(+0i73,.%-86#&8;#+L'&,1*aCD0212/23*%-2A73*L
5&%,.%-#]730d071(&%7/71(+0i73,.%-8;#)8;#+L*
5)4;h(&%E-0:#+*
8;#a[5+0#&CD0F*E-0d,R71(+0Bt#&%-4\'\0d,.CD0#]71%L-02dV
"$2B%,10215&4;7db[U0*-f&71%-8;#&0hH%/'),10Cd8;2189*
#:*->oalV  N>*-,B'&,1*
#+*
?A86#&%-4S%-#)%')(+*-,.%,1023*
465+7189*
#%-#&h
oalV  N >{*-,<%-hm0CD7189E%-4g%-#&%'t(+*-,.%N,1023*
465+7189*
#V

  

    ,! 

 #71()8;2<'t%'[0d,$U0N()%E-0'&,10210#730h%-#%-49L-*-,.8971()? >*-,e8;h+0#]7189>_^8;#&LA71(+0N#+*
5&#')(+,.%-230N%-#]730CD0h+0#712
*->'&,1*
#&*
5&#&2%-#&h%-h30CD7189E%-4)%-#&%')(&*-,.286#/^')%-#&8621(Ah&8;%-4;*-L
5+02dVKXY(&8;2%-49L-*-,.8971(&?0D^'t49*
89712h&89[0d,10#]7
a8;#&h&2F*->Y8;#&>{*-,.?%7189*
#pe4;8;#&L
5&8;23718;C@a#+*R490h+L-0-bKh&8;21CD*
5+,W230fih&8;%-49*-L
5&0N2173,.5&CD715+,10/8;#+>*-,.?A%7189*
#b%-#&h
h&8;2.CD*
5+,.230Y73*-'t8;C<a#+*fi<490h+L-0-V  7U862f)%-230h*
#%=210d7*->CD*
#)2373,.%-8;#]712%-#&h'),10d>{0d,.0#&CD02GR(&8;C.(h+0d'\0#&h
*
#i%-4;4%E%-8;4;%f)490Fa#+*R490h+L-0k8;#*-,Wh+0d,Y73*A,1021*
49E-0e%-#&%')(+*-,W%aV
 #A%-h&h&8;7189*
#b-%eh+0Dt#)897189*
#@*->\71(+0Y%-#&%')(+*-,W8;C%-CdCD0212189f)864;897_@23')%-CD0Yft%-230h*
#Ah&8621CD*
5+,.230fih)8;%-49*-L
5+0
2373,.5)CD715+,10A8;#+>*-,.?A%718;*
#HU%-2k'&,10230#]730hV  0(&%E-0/21(&*<#H71(&08;?'\*-,171%-#&CD0*->71(&862e%-CdCD0212189ft8;4;897_
23')%-CD0e86#%-#)%')(+*-,.%@,1023*
465+7189*
#ba8;#CD*
#73,W%-237Z73*A%-49L-*-,.8971()?A2U71(&%7Yh+*#+*-7,1049_/*
#c%-#_215&C.(c21')%-CD0-V
Y02.5&49712Z21(+*fi71(&%7$n-^V n N*->K71(+0e%-#]730CD0h+0#712$U0d,10k49*aCd%730hi8;#71(&0e'&,1*-'\*
230hc23't%-CD0-V
K8;#&%-4649_-bU0/h+02.CD,.89f\0h%210d7=*->Z0Da'\0d,.8;?0#]712eCD*
#&CD0d,.#)8;#+L71()8;2e%-49L-*-,.8;71(&? %-#&h`%-CdCD0212189ft8;4;897_
23')%-CD05&218;#&L%CD*-,1')5&2k*->ZB ':h&8;%-49*-L
5&02dVXY(+0/%-49L-*-,.8971(&? U%-2=8;?')490?A0#730h5)218;#+LcM,.*
49*-L+V  #
*
5+,t#&%-4)0D^'\0d,.8;?A0#7db^%e'&,10Cd862189*
#A*->oalV  N%-2U%-C.(&890dE-0h>*-,'),1*
#+*
?A8;#)%-4&%-#&%')(+*-,W%e,1023*
465+7189*
#
%-#&hi%A'&,10Cd8;218;*
#/*->oalV  N U%-2R%-C.(&890dE-0h>{*-,R%-hm0CD718;E%-4g%-#)%')(+*-,.%,.023*
4;5+7189*
#gV
"$2F%/73*^*
4>{*-,F,1021*
49E^86#+LA'&,1*
#&*
?A8;#&%-4S%-#&h%-hm0CD718;E%-4%-#&%')(+*-,W%>*-,k^')%-#&8;2.(h&8;%-4;*-L
5+02db[71(&8;2
23_a23730?Cd%-#f\0k5&210h8;#215+'&'\*-,17Z*->E%,.89*
5&2<IRJSM71%-21^2db\8;#&Cd465&h&8;#+LN?A%-CW(&8;#+0=73,.%-#&214;%718;*
#b)8;#+>*-,3O
?A%7189*
#i0D^73,.%-CD718;*
#b&,10d73,.8;0dE%-4g86#+>{*-,W?A%7189*
#ba*-,<^5+0237189*
#+O%-#&23U0d,.8;#+L+V
V5&,1,10#]7149_-b\71(+0A%-5+71(+*-,.2B%,.0*-,1a8;#+L*
#H8;#&CD*-,.'[*-,W%718;#+L230?A%-#]718;C8;#+>*-,.?A%7189*
#:8;#]73*71(+0A%-4O
L-*-,.8971()?cV



  )   
T t




XY(+0A%-5&71(+*-,.2B<8621(73*71(&%-#&:I$%7189Ea8;h&%-hMG,.890d73*+bS&0d,1,.%-#M4;%ab%-#&h"R#]73*
#&89* O*
4;8;#)%>{*-,k()%Ea8;#+L
CD*
#]73,.89f)5+730h71(+089,=71%L-L-0d,JS8;h&8;
% O*-,10#+*>{*-,N(+0d,=(&049'&>Q5&4K,10dEa8;2189*
#&2e*->Z71(+0/8;h&0%-2e'&,10210#730h8;#
71(&8;2')%'\0d,e%-#)h R%>%0
4 O:5 #+ *--
b O:%a8;?8;4;8;%-#+*a%-89DOIR*]0h&%abF"$#73*
#&8;*&0d,1, %-Q #&h+0d-bF%-#&
h 
0J2 5&Q 2
M0d,.%-4\>*-,U71(&089,ZCD*
4;4;%f\*-,.%7189*
#8;#'\0d,1>*-,.?A8;#+Lk71(+0F0D^'\0d,.8;?A0#712dV  0e%,10e%-4623*=L-,W%730d>5&4\73*A210dE-0d,.%-4
%-#+*
#]_^?A*
5&2,10dEa890dU0d,.2*->\71(+0B+waxWrts-[wmZq$xQz zQs ffWr)D z9]drtW~
 <~d.sx3DvF>{*-,G(&049'&>Q5&4aCD*
?A?0#]712
*
#c0%,.46890d,<h+,.%>712*->K71(+0e')%'\0d,V


fi@ 2~,7



2	AK$)B

$CA



2B2{EDX<{$7

 G
 F





7BEHK.{JI22

XY()8;2B,10230%,WC.((&%-2ef\0d0#215+'&'\*-,1730hf]_71(+0VU*
?A8;2.8?*
Q #  #730d,.?8;#&8;23730d,W8;%-4h+0 V8;0#&Cd8;%c_XS0CO
#+*
49*-LgR Q %=P6V  V "BXRT[*->&71(+0Z^')%-#&8621(FL-*E-0d,.#)?0#7db]5&#&h+0d,g'),1*m0CD7K#^5&?kf\0d,.2X  VYn
O['

^lOV '
fiO['+l'

%-#&h j Fln-n-ofiO['1'
-o^V
gUm  )   
"$4;490#b+V9b4VU*-,10-bgO`VSPmln-n
-TWVU$,.%>7Z*->Y"sOaJGp)F8;%-49*-LA%-CD7<?%,1^5&'i8;#c230dE-0d,.%-4g46%_-0d,.2dVX0CW(V
,.0d'gV9b)XY(+
0 O:5&497189't%,17_F8;21CD*
5+,.210=$,1*
5+'ggV $#&89E-0d,W21897_*->KY*aC.(+023730d,btY*aC.(&023730d,b Ba"=V
%-215+,.h&0YM,1*30CD7$Pmln-n-o
TWV-u+wr)!sr[.w^~WZ-u+W1DvkzQs-w1
)9)~dzrKz@zQ1|@wAszr+~V\V  V "BX
PQX  VZn-ofiO!-fiO V '

TW
V $<r
J | ~2""J"fi5@ ".fi?#&
$	!   ! V

%-4;h+<86#b<$VePmln-n
-TWV V*-L
I  "qV$p j 89L
( '&,10Cd8;218;*
# CD*-,.0d>{0d,10#)CD0H<8971( 4;8;?A8;730ha#+*<4;0h+L-0%-#&h
468;#+L
5&8;21718;CY,1021*
5+,.CD02dV  #jx3wWW1|zr]-~@wm$q{	FGq 	Fw-x.fi~.v^wWuw-rSu+dxms
QzQwr[sCks
!w-x.~
zj
r x3s
Qz.s- P
y ZwD^~DKq$r[sWu)v^wx3sYD~Dw+QzQw-r&bt')'gV&-oO:%-h+,.8;hP!a')%-8;#tTWV
U_],.*
#b)@V9b,4^730#7db\"=VSPmln-n-o
TWV<"'&,104;86?A8;#&%,1_A?A*^h+04g*->CD0#730d,.86#+L8;#ih)8;%-49*-L+V  #vZxmwW..|zr]~
w3v+
 v}q$r&r)+s-~
 `.dQzr]wm:v+qB~.~dwdzs-Qzwrdwz
x 	wRut+!s-Qzwrts<
 z{r^-az6~DQzQd~sr[|
 {v ffWr)dxWr[s-QzQw-rts 	wrDx1Dr[W=w_r 	wRut+!s-Qzwrts Kzr]-az{~QzD~= 	WZff2{q 	Fl "Dfib^'&'gV
lD] tlD]- O*
#]73,10%-46P V%-#)%-h&%TWV


#
)
W

2l "

V%,.4;0d7371%ab+
 V9b+0d7Y%-4VPmln-n
-TWVGX<(+0F,104;8;%f)864;897_N*->K%h&8;%-49*-L
5&0$2373,W5&CD715+,10BCD*ah&8;#+L21CW(+0?0-V	wRut+!s
Qzwrts-K
 zr]
^z{~QzD~b  K PmlTWbl^ -^V
F%-(&49f %-/ C.tbSIkVPmln-nalTWVY
 utx3D~ddr)!s-Qzwr+~wmk z{~D.w-^xW~dd=	w.-r&zQQz{-srt|s	wRut+!s-QzQw-rtsKqB~!u&1~V
V%,1f\*
#+0464b &V9b54U,1*<#gbeVZPmln-o-o
TWV"$#&%')(&*-,.%i,1021*
4;5+7189*
#p%:?=5)49718O2373,.%730dL-_`%'&'&,1*
%-CW(V
ZxmwW..|zr]~wm
vrffWr)dxWrts
QzQwr[s&	w-rddx3drtWwr 	wRut+!s-Qzwrts z{r]
az6~DQzQd~`	
ff
Dfibt'&'gV&n- tl'+l@U5)h&%'\0237kP j 5)#+L-,1_&TWV

M(V@V71(+021862db$0d')%,.71?0#7S*-> U
V *
?')5+730d,S%-#&h  #+>{*-,W?A%7189*
#=aCd8;0#&CD0-bfiJS8;#+\*-/ ')8;#+L_$#&89E-0d,.218;7_-b
JS8;#+\ *-/ ')8;#+L+b)^U0h+0#V

LGC.-0d,17dbO`V9b54^73,.5+f\0-bOVPmln-n-n
TWV:F8;%-49*-L
5&0/%-CD712db23_a#&CW(+,1*
#&8;2.8;#+Lc5&#&89712k%-#&h`%-#)%')(+*-,.%c,.023*O
465+7189*
#V  #v
 x3w.1|-z{r]-~w3Bq$@~DDx3|
s wxW~Wv^wWuw-rva \dAsr)QzD~Ns-rt| 
 x3s
s
QzQd~wm
kzs9w1-)q

F"R?23730d,.h&%-? P j *
4;4;%-#)htTWV



< CW~gFl "



i

+0d,1,%-Q #&h+0d-b
"=V9bMK%-49*
?A%,bO`V9bE4 O*-,10#&*+b
JV&Pmln-n-o
TWV)"$#&%')(+*-,.%R,1023*
465+7189*
#k86#=5&#&,102373,.8;CD730h=730Da712
R8971(')%,.718;%-4')%,.2.8;#+L+V  #rx3wWW1|zr]~cwm/v+vq$r)r&&sWdQzr]HwmvaqF~W~DwzQs
QzQwr
dwL
x 	w-<u[a!s
QzQwr[s
 z{r^-az6~DQzQd~srt|  v
v ffWr)dxWr[s-QzQw-rts
 	w-rddx3drtWw-r 	w-<u[a!s
QzQwr[s
z{r^-az6~DQzQd~ 	W ff2!{q 	Fl Db+'&'gV&-o- ^-nall O*
#]73,10%-4U6P V%-#&%-h&%TWV
+0d,1,%-Q #&h+0d-bF"kV9bBMK%-49*
?A%,bO`V9bfi4 O*-,10#+*+bBJVePmln-n-n
TWV "$# 0?')89,.86Cd%-4R%'&'),1*
%-C.( 73*^'t%-#&8;21(
%-#)%')(+*-,.%,.023*
4;5+7189*
#gP
V s
Dv]z{r[&xms-r+~W9s-Qzwr&b  P]TWbKlnal^al^V

+*[b$VZPmln-o
-TWVkz{~D.w-^xW~dtQx+Qax3srt|q$rtsWutv^wx3s
cxzdr srt|`.wr)DxW~Ds
QzQwr[sLr^-z6~WvV
VZ%-?kf&,W8;h+L-0=^715&h)89028;#cJ8;#&L
5&8;23718;Cd2
V V%-?=f&,.86h+L-(
0 R#)89E-0d,.21897_/MG,10212dbJVZ%-?kf&,W8;h+L-0-V

F%-4;46%,.h+*+b-$VaPmln-n-
TWVtq$rgs  z{~Wz{~ 	wr&dx.~ds
dzwrts-&x3s-|s- Qz.sN|d"<1Wu[!wxVVU*
490CdCd8?*
Q #Aa86#&%')218;2V
Lh&8;Cd89*
#&0_
2 L')8;21730?0-bt[VJV9b%-4;0#&Cd8;%aV
fiff

fi

i{9  ] DA2

2{~i{

$,1*
21-bK$VPmln
--TWVXY(+0,10d'&,10230#]71%7189*
#%-#&h5)230A*->Z>*^Cd5&2=8;#%23_^21730? >*-,=5&#)h+0d,.2371%-#&h)8;#+Lch&8O
%-4;*-L
2dV  # ZxmwW..|zr]~ewmL
 kGz &
v ffWrtDxrts-Qzwrts[aw-z{r) 	wrddx3drtWkwr/q$xDQz zQs-.ffWr)Dz^Drt
 ffW,
 	g
q ffl " fib)'&'gV)
S a V%-?kf),.8;h+L-0-b O:"-P Ba"BTWV
$,1*
21-bgRVPmln-oalTWVk+*aCd5&2186#+L%-#)hh+02.CD,.89'&7189*
#8;#:#&%715+,.%-44;%-#+L
5&%L-0Nh&8;%-4;*-L
5+02dV  #;DDrt~wm
kz{~DWwax.~ Srt|]DxW~!srt|-z{r]
V V%-?=f&,.86h+L-(0 R#)89E-0d,.21897_/MG,10212db V%-?=f&,.86h+L-0-V
$,1*
21-b\$V9bJ
*
21(&8b["kV9b4  08;#&2373086#b\VPmln-o-
TWVkMG,1*Ea8;h&8;#&L%/5)#&8)0h%-CdCD*
5&#7F*->Gh&0Dt#&89730@#+*
5&#
't(+,.%-23028;#h&8;21CD*
5&,.230-V  # ZxmwW..|zr]~ewmRv+  ~Dq$r&r&&s.Qz{r^wmRva<qF~W~DwzQs-Qzwredwx
	w-<u[a!s
QzQwr[sJ
 Kzr]-az{~QzD~Wb+')'gV+-	^B' V%-?=f&,.86h+L-0-bgO" P-Ba"BTWV
$,1*
21-bG$V9bE-*
21()8b"kV9b4  086#&237308;#bG[VZPmln-n-
TWV V0#730d,.86#+L+p" >,.%-?0dU*-,1>*-,?*ah+04;8;#+L71(+0
4;*^Cd%-4gCD*
(&0d,10#&CD0e*->h&8;2.CD*
5+,.230-V 	wRut+!s-QzwrtsJz{r]
az6~DQzQd~Wb  P
TWb\B'
^--^V

j 0%,.237db_OV$Pmln-n]TWV O:5&497189O!')%,.%L-,.%')(210dL
?0#71%718;*
#*->F0D^'\*
218;73*-,1_`730Da7dV  #Zxmw..|z{r^~:wm
  rt|q$r&r&&sfi `.Qz{r]wmevaFqB~.~dwdzs-QzwrNDwx 	w-<u[a!s
QzQwr[sCz{r^-az6~DQzQd~Wba'&'gV&ntlJS%-2
V,.5&CD02db&IR0d O0Da86CD*+V
j 89,.237dbt@VPmln-oalTWVZq$r[sWu)v^wx3szr=s-Qax3ssr]-&s^Sr[|DxW~!sr[|z{r^-Va'&,.8;#+L-0d,1O0d,.4;%L+b&U0d,.4;8;#gV

;e%-?0d_
%-?%abaO`V]Pmln-n
-TWV^Y0CD*-L
#&8986#+LY,10d>0d,10#]718;%-4
4;8;#&^2dp\"$#e8;#+>*-,.?A%7189*
#F0Da73,.%-CD7189*
#='\0d,.23'\0CD7189E-0-V
 q 	F wxW~Wv^wWuwrS u+dxms-Qzwrts-gk s!wxW~kzrZxms
QzQWs yZ wd]~D
 #jx3w.1|-z{r]-~@wmRq{	FG
qFrtsWu)v^w-xms<
 ~Dw- +Qzwr&b['&'gVa^ -O:%-h+,.86hP!^')%-8;#tTWV
J%')')8;#b+[V9b4J0%-212db j VPmln-n]TWV"R#%-49L-*-,W8971(&?>{*-,'&,1*
#&*
?A8;#&%-4t%-#&%')(+*-,.%=,1023*
465+7189*
#V	wRuta
!s
QzQwr[sC z{r^-az6~DQzQd~Wb  P]TWb--^-alV
O:%,17?R Q #+0dDO%,.CD*+bgMVPmln-n-n
TWVF"$49L-*-,.8;71?*/h+0=,1023*
4;5)Cd8?*
Q #h+0N4;%/%-#u%Q >{*-,W%'&,.*
#+*
?A8;#&%-4g0#h&8i%-Q 49*-L-*
2dV
ZxmwW~dsNzdr)!wi|d dr]-&s  =s-Qaxmsb
b[ ^o-^V

@



7 



? 

h

 



O:%,17?R Q #+0dDO%,.CD*+bgMVPB'1'+lTWV YD~Dw&dz wr	wRut+!s
zQwr[sG|@9sq$r s3Dwx3sdr ez s9w1w~= U~DQxW&d
Qax3s|D kz6~ddaxW~Dwj
	 wrtwz{Nz!Dr)!w Kzr]  ~Qz.wVM(V@V^71(&0218;2db$#&89E-0d,.2186h&%-h/h&0$"$4;8;Cd%-#]730-b



"$4;8;Cd%-#]730-b[^'t%-8;#V

 LE  

O:%,17?R Q #+0dDO%,.CD*+baMV9b.4

M %-49*
?A%,b,OV[PB'1'1'TWVgY023*
4;5)Cd8?*
Q #h+0<4;%B%-#q%Q >{*-,.%ap02373,.5)CD715+,.%eh+04+h)8?%-Q 49*-L-*
K
_cCD*
#+*aCd8;?A8;0#73*A4;86#+L,5\/ R Q 237186CD*+V Zxmw~dsNzdr)!wi|d@dr]-&s7ks
Q^x3sb  b&]S^-^V

O:%,17?R Q #+0dDO%,.CD*+b&MV9ba0d7U%-4!V\Pmln-n-o
TWVK"R#)%-4;89%-h+*-,U')%,.Cd8;%-4\P$MGMYV  #VU*^04;(+*+b
d Wff r)d z9]drtdzsq$xDQz zQs- b+'&'V)--n ^&l=JS8;23f\*
#PQM*-,1715+L
%-4{TWV





j VP-LGhV TWbx3w1
x3D~.~Dw

O:%,17?R Q #+0dDO%,.CD*+bMV9b0d7B%-4VPmln-n-n
TWVcLE%-4;5&%718;*
#*->'&,1*
#+*
5&#,1023*
4;5&7189*
#%-4;L-*-,.8971(&?>{*-,k^'t%-#&8;21(
h)8;%-49*-L
5+02dV  #vZ
 xmwW..|zr]~Awm=v+ )dr[ dzQseu+dx=z &
 xms
!sDr)!wq$+!wAs-Qz.wi|d{; z{r^-)
 3^ q
fib+')'gV)-- ^-- 0#&8;CD0P  71%-49_+TWV

	@W <l "

 

 :



O:8973-*E[bSkVGPmln-n-o
TWVY*-f)5)237B'&,1*
#+*
5)#,.023*
4;5+7189*
#:<8;71(H4;8;?A89730ha#+*<4;0h+L-0-V  #tZxmw..|z{r^~wm
v+ vq$r&r&&s `.dQzr]:wmvaNqF~W~DwzQs
QzQwrdwxj
	 wRut+!s-Qzwrts Kzr]
^z{~QzD~Asrt|
v
Wff rtDxrts-Qzwrts
	 wrDx1Dr[WBwrt
	 w-<u[a!s
QzQwr[s Kzr]-az{~QzD~k	
ff !q{	
dfib
'&'Vo--n
o

O *
#]73,10%-4UP6
V %-#&%-h&%TWV



Q



i>

P
W 2 Fl 





fi@ 2~,7



2	AK$)B

$CA



2B2{EDX<{$7

 G
 F





7BEHK.{JI22

MG46%ab[UV9b\4MG,.890d73*+b\IkVPmln-n-o
TWV(R2.8;#+LAL-,.%-??A%718;Cd%-4K8;#+>0d,10#&CD0@?0d71(+*ah&2<>*-,F%-5+73*
?A%718;C=')%,17mO!*->{O
21'[0d0CW(71%L-L
8;#+L+V  
# ZxmwW..|zr]~wm
 kGzx.~D ffWr)dxWrts
QzQwr[s 	wrDx1Dr[W:w
r gsr]
&s]
 YD
~dwaxm~@s-rt
| s &s-Qzwr` Q 	l D=$,W%-#&%-h&%iP!^')%-86#tTWV
Y086#&(&%,17dbXBVZPmln-o-
TWVq$rtsu)v^wx3s`s-rt|\ds-r)QzQ3ffWr)Dxutx1d!s-Qzwr&V U
V ,1*^*
? j 04;?cbJ*
#&h+*
#%-#&h
a_^h&#&0d_-V

Y0d_a#&%,b+VV$VYPmln-n-n
TWV a71%718;23718;Cd%-4Z?*ah+04;2@>*-,A73*-')8;C230dL
?A0#71%7189*
#gV  #Zxmw..|z{r^~wm  v
qFr&r&&s `.Qz{r]wmRvaRqF~W~DwzQs
QzQwr=Dw-x	wRut+!s-QzwrtsKzr]-az{~QzD~kq 	Fl "fib^'&'gV^-
S
- O%,._^4;%-#)h-P Fa"BTWV
<86C.(b\L<V9b\4JS5+'\0d,.+*fi_-b[VPmln-o-o
TWV"R#&%'t(+*-,.%%,.C.()89730CD715+,10>*-,e%-#&%')(+*-,W%/,.023*
4;5+7189*
#gV  #tZxmw
..|z{r^~/wmg
 \..wr[s
| 	w-rddx3drtWwrqu
utz.
| ks-Qax3s sr]-&s^&
 x3wWD~.~Wzr]Q9
q <Zl "dfib
')'gVglo ^/"$5&23718;#gb&X0D+%-2=-P Ba"BTWV
Y*aC.()%abO`VPmln-n-o
TWViq 	wx!u[]~$s~1|tQ&|wm@q$r[sWu)v^wx3szrkzs9w1-)~Azrr]
 z{~.vsrt|<wxW
Q

&D~ddVMG(V@V&71(+02.8;2dbg$#&89E-0d,.218;7_*->Ga5)21230D\b&+5&21230D\Vg9;AV
a%-C.^2db j V9b++C.(+0dL
49*KbPLYV9b4|
0D[0d,.21*
#b+NV\Pmln
fi]TWVK" 12 8;?'t49023723_a23730?A%718;Cd2U>*-,71(&0<*-,1L
%-#&8;%7189*
#
*->K715+,W#71%^8;#&L>{*-,$CD*
#E-0d,.2.%7189*
#VLsr]-&s^DbQ  P ]TWb-n-a-^V
a890dL-04!b][V9bE4 V%-237304;46%-#b+V&Pmln-o-o
TWVFkwr-uasx3sdQxzQt!s-Qz{~QzD~gdwxYva~Fvaszwx3s[zdrtWD~kP#&h
0h)897189*
#tTWVO:C$,.%ZO j 864;4V
^73,.5&f[0-bO`V9bZ4 j %-(&#b=VGPmln-n-n
TWV@)5&#&CD7189*
#&%-4KCD0#]730d,.8;#+L+pe$,1*
5)#&h&8;#+L,10d>{0d,.0#718;%-4KCD*
(+0d,.0#&CD0A8;#
86#+>{*-,W?A%7189*
#c2373,.5)CD715+,10-V	wRut+!s-QzwrtsJz{r]
az6~DQzQd~Wb  P
TWb\B'
n^-+V
"*
5&?A%-#)2db@VaPmln-nalTWV+"#&0d73*^*
4>*-,h)8;21CD*
5+,.210G%-#&%-49_a218;2dpgX<(+0GE-*aCd%f)5&4;%,._F?%-#&%L-0?0#]7'),1*t490-V

sr]
+s]Db  P]TWbg-ao-n^V



fiJournal of Artificial Intelligence Research 15 (2001) 115-161

Submitted 4/01; published 8/01

The GRT Planning System: Backward Heuristic Construction
in Forward State-Space Planning
YREFANID@CSD.AUTH.GR
VLAHAVAS@CSD.AUTH.GR

Ioannis Refanidis
Ioannis Vlahavas
Aristotle University
Dept. of Informatics
54006 Thessaloniki, Greece
Abstract

This paper presents GRT, a domain-independent heuristic planning system for STRIPS worlds.
GRT solves problems in two phases. In the pre-processing phase, it estimates the distance between
each fact and the goals of the problem, in a backward direction. Then, in the search phase, these
estimates are used in order to further estimate the distance between each intermediate state and the
goals, guiding so the search process in a forward direction and on a best-first basis. The paper
presents the benefits from the adoption of opposite directions between the preprocessing and the
search phases, discusses some difficulties that arise in the pre-processing phase and introduces
techniques to cope with them. Moreover, it presents several methods of improving the efficiency of
the heuristic, by enriching the representation and by reducing the size of the problem. Finally, a
method of overcoming local optimal states, based on domain axioms, is proposed. According to it,
difficult problems are decomposed into easier sub-problems that have to be solved sequentially. The
performance results from various domains, including those of the recent planning competitions,
show that GRT is among the fastest planners.

1. Introduction
So far, planning problems have been considered as a special kind of particularly difficult search
problems (Newell & Simon, 1972) and many algorithms for decomposition, abstraction, least
commitment etc. have been proposed to cope with them. In the early 90's, researchers were arguing
that plan-space planning is more efficient than state-space planning (Barrett & Weld, 1994;
McAllester & Rosenblitt, 1991; Minton, Bresina & Drummond, 1994; Penberthy & Weld, 1992).
In the mid 90's, new algorithms appeared that achieved even better performance by transforming
planning problems either into graph solving problems (Blum & Furst, 1995, 1997) or into
satisfiability ones (Kautz & Selman, 1992, 1996, 1998). However, it has been shown that simple
search strategies with the use of domain-dependent heuristics can solve large problems (Gupta &
Nau, 1992; Korf & Taylor, 1996; Pearl, 1983; Slaney & Thiebaux, 1996).
In recent years, part of the planning community turned towards heuristic planning, adopting
known search strategies and developing powerful domain-independent heuristics that achieve
significant performance. The first planner was UNPOP (McDermott 1996, 1999) and was followed
by ASP (Bonet, Loerings & Geffner, 1997), HSP (Bonet & Geffner, 1998), HSPr (Bonet & Geffner,
1999), GRT (Refanidis & Vlahavas, 1999b), FF (Hoffmann & Nebel, 2000) and ALTALT (Nigenda,
Nguyen & Kambhampati, 2000). These domain independent heuristic planners search for solutions
either in the state-space or in the regression space. Most of them use variations of a relatively
simple idea as a guide: they estimate the distance between two states, based on estimates of the
distances between each fact of the problem and one of the two states.
 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

fiREFANIDIS & VLAHAVAS

The above planners can primarily be classified based on the forward or backward direction, in
which the heuristic is constructed and the state-space is traversed. We distinguish the following
three categories:




Forward heuristic construction, forward search (ASP, HSP, FF).
Forward heuristic construction, backward search (HSPr, ALTALT).
Backward heuristic construction, forward search (UNPOP, GRT).

Generally, the forward direction seems to be more advantageous than the backward one, both
when constructing the heuristic and when searching, because in the backward direction and in case
of incomplete goal states, problems with invalid states and unreachable facts usually arise.
However, using the forward direction for both tasks requires reconstructing the heuristic function
for each visited state, spending in this way a significant portion of the processing time, while using
opposite directions for both tasks allows constructing the heuristic once, in a pre-processing phase.
This paper presents the GRT planning system. It is the only domain independent heuristic
planner that constructs the heuristic once, in a backward direction and in a pre-processing phase.
UNPOP, although it uses the same directions, reconstructs the heuristic from scratch for each visited
state. GRT, in a pre-processing phase estimates the distance between each fact and the goals of the
problem. During the search phase, these estimates are used in order to further estimate the distance
between each visited state and the goals, guiding so the search process in a forward direction and
on a best-first basis. Constructing the heuristic once offers the ability to evaluate states very
quickly, while traversing the state-space in a forward direction allows the planner to avoid invalid
states that arise in the regression space.
The paper substantially extends previous work (Refanidis & Vlahavas, 1999b, 1999c, 2000a
and 2000b), in that it presents and proves the fundamental theory of the planner, along with many
new techniques developed on it, it extensively tests the contribution of each technique to its overall
performance and provides a thorough comparison to other planning systems.
The rest of the paper is organized as follows: Section 2 presents the data structures and the main
algorithms of the planner. Section 3 discusses the difficulties that incomplete goal states cause to
the backward direction of the construction of the heuristic and presents methods to cope with them.
The same methods are also applied to identify and enrich poor domain representations.
Two approaches to reduce the problem's size are presented in Section 4. The first one deals with
the identification and elimination of irrelevant objects and the second one concerns the adoption of
a numerical representation of resources.
Section 5 deals with the problem of local optimal states and proposes a method to cope with
them. Specifically, the XOR-constraints are introduced and used in order to decompose difficult
problems into easier sub-problems that have to be solved sequentially. Section 6 presents the
operation of GRT, Section 7 presents the related work and Section 8 presents performance results,
which show that GRT is among the fastest domain-independent planners. Finally, Section 9
concludes the paper and poses future directions.

2. The GRT Heuristic
In STRIPS (Fikes & Nilsson, 1971), each action a is represented by three sets of facts: the
precondition list Pre(a), the add-list Add(a) and the delete-list Del(a), where Del(a)  Pre(a). A
state S is defined as a finite set of facts. An action a is applicable to a state S if:
Pre(a)  S
The state resulting from the application of an action a to state S is defined as:
116

(1)

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

S' = res(S,a) = S \ Del(a)  Add(a)

(2)

Inductively we can define the state resulting from the application of a sequence of actions (a1,
a2, ..., aN) to a state S as:
S' = res(S, (a1, a2, ..., aN)) = res( res(S, (a1, a2, ..., aN-1)), aN)

(3)

with the requirement that each action ai is applicable to the state res(S, (a1, a2, ..., ai-1)), for each
i=1, 2, ..., N. In the formalization used henceforth, the set of problem constants is assumed to be
finite and no function symbols are used, so the set of actions is finite.
A planning problem P is a triplet P=(O, Initial, Goals), where O is the set of ground actions,
Initial is the initial state and Goals is a set of facts. The task is to find a sequence of actions a1, a2,
..., aN that can be applied to the initial state, so that the state resulting from their application will be
a superset of Goals. The sequences of actions are called Plans. A plan that can be applied to the
initial state is called a valid plan. A valid plan that achieves the Goals is called a solution of the
planning problem. A planning problem may have several or no solutions. In the latter case the
problem is described as unsolvable.
The next sub-section gives a brief presentation of the ASP heuristic, which was our motivation
and helps to understand the following concepts, whereas the subsequent sub-sections present the
GRT heuristic in detail.
2.1

The ASP Heuristic

In the ASP heuristic, for each action a and for each fact p  Add(a), a rule Cp is formed, where
C=Pre(a). Assuming a set of rules, it is said that a fact p is reachable from a state S if p  S or
there is a rule C  p such that each fact q  C is reachable from S.
So, a function g(p,S) is defined, which inductively assigns a number i to each fact p, where i is
an estimate of the number of steps needed to achieve p from S, i.e. the distance of p from S. More
specifically, g(p,S) is set to 0 for every fact p  S, while g(p,S) is set to i+1, i  0, for each fact p
for which a rule C  p exists, such that  g (r , S ) = i . Thus:

def

g ( p, S ) =

{

rC

0,

if p  S

i+1,

if for some Cp,

 g (r, S ) = i
rC

,

(4)

if p is not reachable from S

In the case where there are more than one rules Cp for a fact p, the rule with the minimum
cost is chosen. Note that a fact p that was initially achieved by a rule C1p, may be re-achieved,
later, by another rule C2p with smaller cost. That is because not all the preconditions of the
second rule had been achieved at the time when the first rule was applied. The task of applying
rules continues until no rule that can achieve a fact with smaller cost exists. The distances
computed in this way are unique.
For a set of facts P, their distance from S is defined as:
def

g ( P, S ) =  g ( p , S )
pP

(5)

The ASP planner uses g(P,S) to estimate the distances between each intermediate state S and the
Goals. So, the ASP heuristic function is defined as:

117

fiREFANIDIS & VLAHAVAS

def

H

( S ) = g (Goals , S )
ASP

(6)

The ASP heuristic does not take into account the delete lists of the actions. The simplified
problem that is created by ignoring the delete lists is referred to as the relaxed problem and the
corresponding actions are referred to as the relaxed actions. The complexity for constructing
HASP(S) is linear, with respect to the number of ground actions and the number of ground facts.
2.2

Backward Heuristic Construction

Instead of estimating the distance between each fact and the current state in a forward direction, as
ASP does, GRT estimates the distance between each fact and the goals in a backward direction. This
task is performed once, in a pre-processing phase. During the search phase, these estimates are
used to estimate the distance between each intermediate state and the goals. The backward or
forward estimation of the distance between two states often results in different values, since no
heuristic is precise. However, the two directions result in estimates of equal quality on average.
The estimates of the distances between each fact and the goals are stored in a table, the records
of which are indexed by the facts. We call this table the Greedy Regression Table (by which the
acronym GRT comes from), since its estimates are obtained through greedy regression from the
goals.
In order to construct the heuristic backwards, the actions of the problem have to be inverted. Let
a be an action and S and S' be two states, such that a is applicable in S and S' = res(S,a). The
inverted action a' of a is an action applicable in S', such that S = res(S', a'). The inverted action is
defined by the original action as follows:
Pre(a')=Add(a)  Pre(a) \ Del(a)
Del(a')=Add(a)
Add(a')=Del(a)

(7)

The inverted ground actions are applied to the goals, assigning progressively to each ground
fact p an estimate of its distance from the goals, in a way similar to ASP. Applying inverted actions
to the goals presupposes that the goals form a complete state. In Section 2 it is assumed that this is
always the case, whereas in Section 3 the case of incomplete goal states is treated.
2.3

Related Facts

In order to obtain more precise estimates, GRT heuristic tries to track the interactions that arise
when estimating the distances between each fact and the goals. By the word 'interaction' we mean
that achieving a fact may affect achieving other facts positively or negatively. In order to track
these interactions the notion of the related facts is introduced.
Definition 1 (Related facts). A fact q is related to another fact p, if achieving p causes fact q to be
achieved as well.
We will use the notation q

%

rel

p to denote that q is related to p. The set of all facts related to a

specific fact p is denoted as rel(p), i.e.:

rel ( p ) = {q : q %rel p}

(8)

The set of related facts of a set of facts P is defined as the union of the related facts of P-facts:
118

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

rel ( P ) =

 rel ( p)

(9)

pP

Proposition 1. For an inverted action a achieving a fact p, the related facts of p are defined as:
rel(p) = Pre(a)  rel(Pre(a))  Add(a) \ Del(a)

(10)

Proof: Formula 10 is inductive, since it defines the related facts of a fact p based on the related
facts of the preconditions of the action achieving the fact. Thus, we prove it by induction. The
formula holds for the goal facts, for which we suppose that there is a hypothetical inverted action
without preconditions achieving them. So, the goal facts are related to each other. Then, suppose
that Formula 10 holds for the preconditions of an inverted action a. It is enough to prove that it
holds also for the facts that action a adds. Let p be such a fact. The facts that hold after the
application of the action, which are the related facts of p, are the same that hold before its
application, i.e. the preconditions of the action together with their related facts, plus the facts that
the action achieves, minus the facts that the action deletes, exactly as Formula 10 states. 
According to Formula 10, facts achieved by the same action have the same related facts.
Moreover, each fact is at least related to itself.
If there was a single path to achieve a specific fact, then its related facts would be defined in a
unique way. However, this is a rare situation. Thus, there are many actions that achieve a fact,
many paths that achieve the preconditions of these actions; therefore, there is an extremely large
number of possible combinations. Storing, for each fact, the related facts for all the possible ways
of achieving it, requires huge amounts of time and space. For efficiency reasons we decided to
store only one set of related facts for each fact, the set that corresponds to the shortest path that
achieves the fact, according to the heuristic.
Proposition 2. The relation
Proof: The relation

%

rel

%

rel

is reflexive, but it is neither symmetric, nor transitive.

is reflexive, since each fact is related to itself. The relation

symmetric, since for a fact q, which is pre-requisite to achieve p, q
achieving p does not delete q) while p

%

rel

%

rel

%

rel

p may hold (if the action

q may not hold, since q may have been achieved before

% is not transitive, since from the relations q % p and p %
cannot conclude that q % r holds, since it is possible for the action achieving r to delete q.

p. Finally, the relation

is not

rel

rel

rel

rel

r we


For a fact p, dist(p) denotes its estimated distance from the goals. Next, we present some axioms
concerning the distances of the facts.
Axiom 1. The cost of achieving a set of facts {p1, p2, ..., pN} simultaneously, cannot be lower than
the maximum of their individual distances.
N

dist({p1, p2, ..., pN})

max (dist(pi))
i=1

(11)

Axiom 2. If an inverted action a achieves a fact p, the distance of p is equal to the cost of
simultaneously achieving a's preconditions plus one.
dist(p)=dist({p1,p2, ...})+1, where pi  Pre(a)
119

(12)

fiREFANIDIS & VLAHAVAS

Proposition 3. If q

%

rel

p is true for two facts q and p, then dist(q)dist(p).

Proof: We will prove Proposition 3 by induction. Proposition 3 holds for the Goals, since all the
goal facts have zero distances and are related to each other. Suppose now that Proposition 3 holds
for the set of the currently achieved facts Facts. It suffices to prove that for an action a, such that
Pre(a)Facts, Proposition 3 holds for the set FactsAdd(a).
Suppose that there is a fact pAdd(a) that has just been achieved, or re-achieved with smaller
cost. If there is another fact q  FactsAdd(a), such that q rel p, then either q has also just been

%

achieved by a and hence dist(q)=dist(p), or q is a precondition of a and then, according to Axiom2,
dist(q)<dist(p), or finally q is a related fact of an a's precondition, say q' and then dist(q')<dist(p)
(Axiom 2) and dist(q)dist(q') (Proposition 3 holds for Facts), so dist(q)<dist(p).
Let us suppose now that there is another fact q, such that p rel q. If q has been achieved by a,

%

then dist(p)=dist(q). If q has not been achieved by a, then q has previously been achieved by
another action, so q  Facts. In this case, p would also have been previously achieved by another
action, before being re-achieved by a, so also p  Facts. Since Proposition 3 holds for Facts,
distOLD(p)dist(q), where distOLD(p) the previous distance of p. But the new distance of p is smaller
than its previous distance, dist(p)<distOLD(p), so dist(p)<dist(q). Therefore, Proposition 3 holds in
every case. 
Corollary 1. If q

%

Corollary 2. If q

%

rel

rel

p and p

%

rel

p but not p

q, then dist(p)=dist(q).

%

rel

q, then q has been achieved before p.

The above two corollaries follow directly from Proposition 3. Concerning Corollary 2, the
expression 'has been achieved before' means that in the pre-processing phase, when the distances
from the goals are estimated progressively, dist(q) has been computed before dist(p). In case where
a fact has been re-achieved with smaller distance, we consider the last time.
Corollary 3. For a sequence of facts p1, p2, ..., pN, N>2, for which pi
without pi+1

%

rel

pi also holding, it is impossible to have pN

%

rel

%

rel

pi+1, i=1,2,...,N-1, hold,

p1.

Corollary 3 follows directly from Corollary's 2 time ordering relation.
Proposition 4. Facts related to each other have been achieved by the same action.
Proof: Let p and q be two facts related to each other, i.e. q

%

rel

p and p

%

rel

q. Let a1 be the action

that achieves p and a2 the action that achieves q, so pAdd(a1) and qAdd(a2). We will prove that
a1a2. Suppose that a1a2. Since q rel p, q may be an add effect of a1, a precondition of a1, or a

%

related fact of an a1's precondition. However, according to Corollary 1, dist(p)=dist(q). Thus, q
cannot be anything else than an add effect of a1, because in other case dist(q) < dist(p) would hold.
In the same way we can prove that pAdd(a2). Thus, {p,q}Add(a1)Add(a2). However, in this
case, the first action applied when computing the distances would achieve both facts. So, the facts
have been achieved by the same action. 

120

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

The related facts play a critical role when estimating the cost of achieving a set of facts
simultaneously. GRT groups the related facts and sums the maximum individual cost of each group.
For example, if q rel p, p rel r and q rel r hold for three facts q, p and r, these three facts are

%

%

%

grouped together and contribute to the total cost only with their maximum cost, which is dist(r).
However, if q rel r does not hold (since the relation
is not transitive), then p and r are
rel

%

%

grouped together, while q is not included in the same group. In this case, q belongs to another
group, which contributes separately to the total cost.
The aggregation process is performed by the function AGGREGATE, which is described below.
The function takes a set of facts {p1, p2, ...., pN} as input, together with their distances dist(pi) and
their lists of related facts rel(pi), and estimates the cost of achieving them simultaneously. The
function is used both in the pre-processing phase, in order to estimate the application cost of the
inverted actions, and in the search phase, in order to estimate the distance of each intermediate state
from the goals.
Function AGGREGATE
Input: A set of facts {p1, p2, ..., pN }, their distances dist(pi) and their lists of related facts rel(pi).
Output: An estimate of the cost of achieving the facts simultaneously.
1. Set M1 = {p1, p2, ..., pN }. Set Cost = 0.
2. While (M1  ) do:
a) Let M2 be the set of facts pi  M1 that are not included in any list
of related facts of another fact pj  M1, without pj being also
included in their list of related facts. More formally:

M2 = { pi: pi  M1,  pj  M1, pi  rel(pj)  pj  rel(pi) }
b) Let M3 be the set of those facts of M1 that are not included in M2,
but are included in at least one of the lists of related facts of
the elements of M2.

M3 = { pi: pi  M1 \ M2,  pj  M2, pi  rel(pj) }
c) Divide M2 in disjoint groups of facts that are related to each
other. For each group add the common cost of its facts to Cost.
d) Set M1 = M1 \ (M2M3).
3. Return Cost

The AGGREGATE function is illustrated with the blocks-world problem of Figure 1. Part of the
Greedy Regression Table for this problem is shown in Table 1. For simplicity, for each fact p we
do not consider as related the facts that have zero distances (i.e. the Goals) and the fact p itself.
This simplification does not affect the estimated distances.
a

a

c

b

b

c

Initial State

Goal State

Figure 1: A 3-blocks problem.
121

fiREFANIDIS & VLAHAVAS

Let us compute the distance between the initial and the goal state. The initial state consists of
the following set of facts:
( (on a table) (clear a) (on b table) (on c b) (clear c) )1
As it results from Table 1, all the initial state facts are related to (on c b), whereas (on c b) is not
related to any other fact. Thus, in the first iteration of the AGGREGATION loop, M2 is set to ((on c
b)) (step 2a) and M3 is set to ((on a table) (clear a) (on b table) (clear c)) (step 2b). So, Cost
becomes equal to the distance of (on c b), i.e. 3 (step 2c) and M1 becomes empty. A second
iteration is not performed and value 3, which is the actual distance between the initial and the goal
state, is returned.
Fact

Distance from
goals

Related facts

(on c table)

0

()

(on b c)

0

()

(on a b)

0

()

(clear a)

0

()

(on a table)

1

( (clear b) )

(clear B)

1

( (on a table) )

(on b table)

2

( (on a table) (clear a) (clear b) (clear c) )

(clear c)

2

( (on a table) (clear a) (clear b) (on b table) )

(on c b)

3

( (on a table) (clear a) (on b table) (clear c) )

...

...

...

Table 1: Part of the Greedy Regression Table for the 3-blocks problem.
Corollary 3 ensures that set M2 (step 2a of function AGGREGATE) will never be empty.
Proposition 4 ensures that M2 can always be partitioned in groups of facts that have been achieved
by the same action (step 2c). The number of iterations that function AGGREGATE performs is
bounded by the initial size of M1, however usually a single iteration is performed.
2.4

The Pre-Processing Algorithm

The estimation of the distance between each fact and the Goals and the computation of the lists of
the related facts for each facts of a problem are performed through the following algorithm:

1

For the representation of facts, actions and states we adopt the PDDL (Planning Domain Definition Language) syntax
throughout this paper. A manual for the PDDL language can be found at the URL
http://www.cs.yale.edu/pub/mcdermott/software/pddl.tar.gz

122

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

The Pre-Processing Algorithm
Input:
The action and predicate definitions of a domain and the objects of a problem.
Output:
The distance estimate from the goals dist(p) and the related facts rel(p) for each
ground fact p of a problem.
1. Let Actions be the set of all inverted ground actions in the given
problem. For each   Actions, set dist()=+.
2. Let Agenda be a list of inverted actions. Set Agenda=.
3. Let Facts be the set of all problem's ground facts. For each f 
Facts set dist(f)= +.
4. For each f  Goals set dist(f)=0 and rel(f)=Goals.
5. For each action   Actions, if AGGREGATE(Pre())<+, then
dist()=AGGREGATE(Pre())+1 and add  at the end of the Agenda.

set

6. While Agenda   do:
a) Extract the first action from the Agenda, say .
b) For every fact f  Add(), if dist(f)>dist(), then:
- dist(f)=dist()
- rel(f) = Pre()  rel(Pre())  Add()\Del()
- For every action b  Actions, such that f  Pre(b), if
AGGREGATE(Pre(b))+1<dist(b), then dist(b)=AGGREGATE(Pre(b))+1 and
push action b at the end of the Agenda.

The Agenda works on a FIFO basis. An action can be re-inserted in the Agenda if its cost
becomes smaller. Thus, each fact can be achieved several times, each time with a smaller cost. The
cost of applying the Pre-Processing Algorithm is polynomial in the number of problem ground
facts and ground actions.
Proposition 5. The Pre-Processing Algorithm preserves Axiom 2.
Proof: In step 6b, the cost of applying an action is set to be equal to the cost of achieving
simultaneously the preconditions of the action plus one. This cost is assigned to the add effects of
the action, except if lower costs have already been assigned to them. Thus, Axiom 2 is preserved.


Proposition 6. Function AGGREGATE preserves Axiom 1.
Proof: We will prove Proposition 6 by induction. Axiom 1 holds for the Goals, which have zero
distances from themselves and are related to each other. Besides, Propositions 3 and 4 and
Corollaries 1, 2 and 3 hold also for them. Suppose next that Axiom 1 and all the induced
Propositions and Corollaries hold for the currently achieved facts Facts. It suffices to prove that for
any action a, such that Pre(a)  Facts, Axiom 1 holds for the new set of achieved facts
Facts'=Facts  Add(a).
Consider a set of facts P  Facts'. We will prove that function AGGREGATE preserves Axiom 1,
with regard to the randomly selected set P. Let p be the fact with the maximum distance among the
facts of P. According to the definition of AGGREGATE function, it suffices to prove that p or
another fact of equal distance is included in M2.
123

fiREFANIDIS & VLAHAVAS

If p  P\Add(a), then for every other fact qP\Add(a), if p

%

rel

q, then dist(q)dist(p)

(according to Proposition 3, which holds for Facts) and finally dist(q)=dist(p), because p has the
maximum distance among the facts of P (the same rationale can be used in the case where there is a
sequence of facts q1, q2, ..., qN, such that p rel q1 and qi rel qi+1, i=1, 2, ..., N-1). If q Add(a) and
p

%

%

rel

%

q, p would be a precondition of a, or a related fact of a precondition of a. However, in that

case it would not be possible that p

%

rel

q, because the distance of q would be greater than the cost

of p (according to Axiom 2, which holds for the preconditions of action a) and this is in
contradiction with the hypothesis that p has the maximum distance among the facts of P.
Let us consider the case where p  Add(a). If p has just been firstly achieved, then the only facts
q, for which p rel q hold, are certainly the other just achieved or re-achieved add effects of action

%

a, which have the same application cost. If p has been re-achieved by a with smaller cost, then it is
impossible to hold p rel q for another fact q  P\Add(a). Actually, in this hypothetical case we

%

would have dist(q)distOLD(p), since Proposition 3 holds for q and the previous distance of p, and
distOLD(p)>distNEW(p), so dist(q)>distNEW(p), which is in contradiction with the hypothesis that p
has the maximum distance among the facts of P. Therefore, in any case, p or another fact of equal
cost is included in M2 and the cost of achieving simultaneously the facts of P is equal to or higher
than their maximum distance. 
We close this section by mentioning the two types of facts, the static facts and the dynamic
facts, that can be found in a problem. The first type concerns the facts that are neither added nor
deleted by any action, while the second concerns the rest of the facts. GRT classifies automatically
the facts, by analyzing the action schemas of the domain. All the procedures presented in Section 2,
i.e. the distance estimates and the related facts, concern only the dynamic facts.

3. Detecting and Enhancing Incomplete States
Backward heuristic construction induces a problem: In most of the problems the goals do not
constitute a complete state description, so it is not possible to apply inverted actions to them. For
example, in the commonly used logistics problems, where packages have to be moved between
several locations via trucks and planes, the goals do not determine the final locations of the trucks
and the planes. The source of the problem is that the GRT heuristic is constructed using a stricter
than usual regression, i.e. it uses actions, the add effects and the non-deleted preconditions of
which (i.e. the preconditions of the corresponding inverted actions) are included within the goals
(in the usual regression, actions with at least one add effect within the goals are used). In this way
GRT succeeds in obtaining more precise estimates and avoiding unreachable facts.
The solution adopted by GRT to confront the problem of incomplete goal states is to enhance the
goals with new facts, which are not in contradiction to the existing ones. For example, since the
goals of the 'logistics.a' problem (Veloso, 1992) do not determine the final locations of the two
planes, it is supposed that each one of the planes could be at any of the three airports. So, the
ground facts:
(at plane1 pgh_air) (at plane1 bos_air) (at plane1 la_air)
(at plane2 pgh_air) (at plane2 bos_air) (at plane2 la_air)
can be added to the new goal state, which is called henceforth the enhanced goal state.
It should be noted that the enhanced goal state is only used in the pre-processing phase, for the
construction of the heuristic. During the search phase, attention is paid only to reach the original
124

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

goals. In this way, completeness is never lost, even in the case where wrong facts have been
selected to enhance the Goals. However, selecting wrong facts may significantly affect the
efficiency of the heuristic function.
Two issues arise when trying to enhance the goals: The first one is how to detect the candidate
new goal facts and the second one is which of them to use. Sections 3.1 and 3.2 examine these
issues, while in Section 3.3 a similar technique is used for identifying and enriching poor domain
representations.
3.1

Detecting Missing Goal Facts

Regarding the identification of the candidate facts to enhance the goals, there are two automatic
approaches. The first one consists of a forward GRAPHPLAN-like (Blum & Furst, 1999) prepreprocessing phase that computes all binary mutual exclusion relations (or simply "mutex"
relations) among the facts of the problem. A number of optimizations of this approach are
presented in (Refanidis & Vlahavas, 1999c), based primarily on the monotonic behavior of the
mutual exclusion relations (Long & Fox, 1999; Smith & Weld, 1999) and secondly on the fact that
it is not necessary to construct a complete planning graph, since it will not be used for extracting a
plan. After the computation of the mutual exclusion relations, all the facts that are not mutually
exclusive with any goal fact are considered candidates for the enhancement of the goals. Its
advantage is that no extra information is needed, apart from the usual STRIPS domain
representation. Moreover, mutual exclusion relations that are not easily recognized by a human
expert can be detected in this way. Finally, this approach can be also exploited as a coarse-grained
reachability analysis for the problem's facts. The disadvantages of this approach are that it is time
consuming and that it does not detect mutual exclusion relations of higher order than two.
The second approach is to use domain specific knowledge in the form of axioms. For example,
an axiom can state that a truck or a plane is always located at some place. So, if the goals do not
determine where a truck is, we can deduce a set of candidate goal facts using this axiom. The
advantage of this approach is that the time needed to deduce the candidate facts is negligible, in
comparison with the time needed for the rest of the planning process. Moreover, more complicated
relations than simple binary mutual exclusion ones can be encoded. The disadvantage is that extra
labor is required in the domain encoding. However, several methods for automatic discovery of
domain axioms have been proposed, e.g. the DISCOPLAN system (Gerevini & Schubert, 1998) and
the work of Fox and Long on the automated inference of invariants (Fox & Long, 2000), and it is in
our future plans to adopt such a method in GRT.
The GRT planner uses the first approach to detect the missing goal facts. Thus, an overhead in
total solution time is imposed by the extra pre-processing work. The contribution of this work to
the total problem solving time varies from less than 10% in domains like blocks-world, to more
than 20% in domains like logistics. The ratio depends on the difficulty of the domain, i.e. how
much time is consumed by the search phase. Logistics problems are easier than blocks-world
problems, so in this domain the overhead is more severe. In the future, we intend to adopt an
automatic method for detecting domain axioms, in order to avoid this overhead.
3.2

Enhancing the Goals

GRT supports three methods of selecting among the candidate new goal facts:




Select all candidate facts.
Use the initial state facts.
Favor the most promising facts.

125

fiREFANIDIS & VLAHAVAS

The first method considers all the found facts as goal facts and assigns zero distances to them.
In most cases, the enhanced goal state obtained in this way is not a valid state, since the new facts
may be mutually exclusive to each other (but not to the original goals). The advantage of this
approach is that the heuristic construction is very fast, since many facts are achieved at the
beginning and a large number of actions become initially applicable. The disadvantage is that the
obtained heuristic is less informative, since there are small differences between the obtained
estimates. So, the best-first strategy tends towards breadth-first, visits more states, consumes more
time, but generally produces better plans than the other two methods.
The second method enhances the goals with the candidate facts that are also included in the
initial state, whereas the facts that are mutually exclusive with the selected ones, are rejected. The
advantage of this method, compared to the first one, is that it results in greater differences between
the facts' distances, and therefore in faster search phase. On the other hand, a preference for the
initial state facts is a risk, because if these are not or - even worse - they cannot be included within
the goals, the search process may become disoriented, leading to longer plans. This method is more
suitable to problems, where there are objects' properties that are unnecessary to solve the problem
and are left undetermined in the goals.
The third method tries to combine the advantages of the other two. In contrast to them, where
the enhancement of the goals is performed in a single step, prior to the construction of the heuristic,
this method adds facts to the goals progressively, in parallel with the heuristic construction.
Actually, facts are added to the goals only in the case where Agenda (Section 2.4) becomes empty.
In this case, candidate facts are progressively assigned zero distances, until a new inverted action
satisfies its preconditions. Each time a fact is selected, other candidate facts that are mutually
exclusive with the selected one are rejected from the set of candidate facts.
The method favors facts that can be combined with already achieved facts, in order to make an
inverted action applicable. The following four rules are applied in decreasing preference:





The facts that can be combined with the original goals are selected first.
Then, the facts that can be combined with other already achieved facts are selected.
Next, the facts that are included in the initial state are selected.
Finally, the remaining candidate facts are selected randomly.

Generally, this method results in the best solving speed and, in many cases, produces equal or
even better plans than the first two methods. However, especially in terms of plan quality, there are
many exceptions depending on the specific problem. It is not difficult to create problems such that
any of the methods presented above performs best. The default method for the GRT planner is the
first one, which is the only method that has been used in the AIPS-00 competition2.
Note that there are domains, like blocks-world, freecell and elevator of the AIPS-00 competition,
or the gripper and the movie domains from the AIPS-98 competition3, where the goals are complete
or near-complete state descriptions; therefore the method used in these domains does not affect
neither solution time nor solution quality. In other domains, as the mystery (AIPS-98), it is
impossible to predict, without solving the planning problem, which of the candidate facts could
actually be goal facts, so in this case the only acceptable method for goal completion is the first
one.

2
3

The official WEB page of the AIPS-00 competition can be found at the URL http://www.cs.toronto.edu/aips2000/.
The official WEB page of the AIPS-98 competition can be found at the URL
ftp://ftp.cs.yale.edu/pub/mcdermott/aipscomp-results.html

126

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

3.3

Domain Enrichment

In this section, we present an approach adopted by the GRT planner, in order to deal with poor
domain descriptions. By the word 'poor' we refer to domains where negative facts are implicitly
present in the initial state and in the actions' preconditions. GRT faced this problem twice, with the
movie and the elevator domains.
In order to explain the problem, let us consider the elevator domain, where there is one elevator,
several floors and several passengers. Each passenger is located in an initial floor and wants to
move to her/his destination floor. The domain is described by four action schemas, (board Floor
Passenger) and (depart Floor Passenger) for boarding and leaving the elevator and (up Floor1
Floor2) and (down Floor1 Floor2) for moving the elevator.
The action schema (board Floor Passenger) is defined by the following PDDL formula:
(:action board
:parameters (?f ?p)
:precondition (and (floor ?f) (passenger ?p)
(lift-at ?f) (origin ?p ?f))
:effect (boarded ?p))
The only dynamic predicate in the definition of action schema board is boarded, an add effect
denoting that the passenger has boarded the elevator. There is no precondition requiring that the
passenger is not boarded. The problem with this definition is twofold. Firstly, the action can be
applied several times to the same passenger in the same plan, i.e. a passenger may board the
elevator although she/he has already boarded. Secondly, and specifically to GRT, it is not stated
explicitly that the passengers are not initially boarded. Actually, the initial state contains static facts
only, which are not removed in the successive states. However, GRT takes into account dynamic
facts only in order to estimate distances. The result is that the initial state and all the subsequent
states are assigned zero distances from the Goals and the best-first strategy behaves as a breadthfirst one.
What is needed is the definition of a new predicate, say not_boarded. Facts of this predicate
should be added to the initial state, denoting that each passenger is initially not boarded, and the
action schema board should be changed accordingly.
GRT performs domain enrichment at run-time. The identification of the above situation is
performed in a way similar to the identification of the incomplete goal states. In this case, G RT
looks for dynamic facts of a problem that are not mutually exclusive with any initial state fact. In
case of such facts, the negations of the identified facts are defined at run-time and added to the
initial state. Furthermore, the negations are added to the preconditions lists and the delete lists of
the actions that achieve the identified facts.
In the elevator domain this is the case with the board and depart actions and the boarded and
served predicates. The not_boarded and not_served predicates are defined at run-time, the initial
state is enhanced with facts determining that each passenger is neither boarded nor served yet and
the actions board and depart are transformed accordingly. For example, the action schema board is
transformed into the following definition:
(:action board
:parameters (?f ?p)
:precondition (and (floor ?f) (passenger ?p)(lift-at ?f)
(origin ?p ?f) (not_boarded ?p))
:effect (and (not (not_boarded ?p))(boarded ?p))

127

fiREFANIDIS & VLAHAVAS

A similar situation arises in the movie domain. In this domain, the goal is to have enough snacks
so as to watch a movie. There are several action schemas of the form:
(:action get-chips
:parameters (?x)
:precondition (and (chips ?x))
:effect (and (have-chips)))
This action schema has the static fact (chips ?x) as precondition and produces the dynamic fact
(have-chips). The action can be applied several times, however once is enough to achieve the goal
of having chips. The difficulty in this domain is that the initial state implicitly declares that we do
not have chips (and dips and pops etc), but there is not any specific dynamic fact to make this clear.
Therefore, in case no domain enrichment process takes place, GRT assigns to the initial state a zero
distance from the goals. With the domain enrichment feature, GRT detects that there are facts like
the have-chips, have-dips etc that are not mutually exclusive with the initial state, defines their
negations (not_have-chips, not_have-dips etc.), adds them to the initial state and transforms the
actions accordingly.
In both of the above domains, without the domain enrichment feature the GRT planner could
only solve some of the easiest problems. However, with this feature it was able to tackle all
problems very efficiently.
Adding negative predicates in the preconditions of the actions may lead to loss of completeness,
since the actions may not be able to be applied in some states, where otherwise they could. In order
to prevent completeness, GRT treats the new preconditions as conditional preconditions, i.e. they
are not necessary for the application of an action to a state, however, if they are present in the
current state they are removed from the successor one.

4. Reducing the Size of the Problems
In this section, two methods to reduce the size of a problem, i.e. the number of ground facts and
actions, are presented. The first method refers to the identification and elimination of objects,
which are certainly not part of any solution. The second method concerns the adoption of a
numerical representation of resources, instead of the problematic atom-based representation of
numbers that has been used in domains like mystery and freecell. Reducing the size of a problem
reduces the effort needed to solve it, especially in the pre-processing phase, where distances for all
facts of a problem have to be computed.
4.1

Eliminating Irrelevant Objects

In many domains, there are objects that are irrelevant to any solution. The most typical examples
can be found in the transportation domains, like logistics, mystery and elevator, where some
packages are initially found in their destinations or for which no specific destination is determined.
So, these objects, together with all the facts and actions containing them, can be removed from the
problem description, without losing completeness.
In GRT we developed a method that detects and removes irrelevant objects. The method
concerns pure STRIPS domains without negation in the preconditions of the actions or in the goal
formula; however, it can be easily extended to cover these cases. The objects are identified before
the pre-processing phase using the following two rules:
An object is irrelevant to any solution for a specific planning problem, if:

128

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING




It does not appear in any goal fact, unless the same fact is also included in the initial state, and
there is no action containing this object in its preconditions, unless the object is also contained
in all the action's effects.

The above conditions are very strict, but they ensure that any detected object is certainly
irrelevant, so they maintain the completeness of the problem solving process.
Proposition 7. Any object satisfying the above rules can safely be removed from the problem
description, without sacrificing completeness.
Proof: Suppose that an object obj has been identified, for which the above two rules hold. We will
show that obj is not necessary to achieve any other goal fact, which does not contain obj. Let us
assume that there is a fact g  Goals, which does not contain obj. Suppose that there is an action
that achieves g, with a precondition containing obj. In this case, the second rule is violated, since
there is an action including obj in its preconditions, without obj appearing in an effect. So, fact g
can be achieved only by actions without preconditions containing obj. Thus, if we regress the goals
using actions achieving g, the established subgoals do not contain obj. However, in the same way
we can reject actions including obj in their preconditions and achieve the new established subgoals.
So, obj is not necessary to achieve any goal or subgoal of the problem. Moreover, there is no goal
fact containing obj, which has to be achieved; even if there is one, it is already present in the initial
state. Therefore, obj can safely be removed from the problem. 
The application of the above rules for the elimination of irrelevant objects can be done
progressively. Let us consider an enhanced logistics domain, where we added colors. Specifically,
we define a dynamic predicate (painted ?object ?color) denoting the color of a package, a static
predicate (color ?color) declaring the available colors, and an action schema (paint ?object
?old_color ?new_color) for painting a package. Let us assume that the goal state does not
determine the colors of the packages. In this case, the colors are irrelevant objects and can be safely
removed, together with all the facts and actions that include colors.
Suppose also that there are brushes that are used to perform the paint operation. There are two
new action schemas, (get ?brush) and (leave ?brush) and a predicate (have ?brush), which is an
effect of the get action and a precondition in the enhanced action (paint ?package ?color ?brush).
In this case, brushes are also irrelevant and should be eliminated. However, since the action paint
needs brushes and has effects not containing them (i.e. (painted ?package ?color) ), the brushes are
not removed, due to the second rule. However, after removing all the color objects, all the paint
actions are removed; thus, brushes do not violate the second rule for the remaining actions and can
be safely removed.
The disadvantage of this approach for the elimination of irrelevant objects is that it does not
remove objects that can eventually appear in a plan, but there are other better (i.e. shorter) plans not
using them. For example, in the logistics domain, suppose that we have three cities, city1, city2 and
city3 and a package that has to be transferred from one location of city1 to another location of city2.
In this case, city3, together with its locations and its truck, are not necessary to solve the planning
problem, since the package can be transferred directly from city1 to city2, without going via city3.
However, it is not easy to identify the irrelevance of city3. Actually, there are plans that transport
packages from city1 to city2 via city3. If we decide to remove city3 and its objects from the
problem representation, we take the risk of sacrificing completeness, since the problem may
become unsolvable. Deciding safely, without loss of completeness, that city3 and its objects can be
removed, can be as hard as solving the original problem.
129

fiREFANIDIS & VLAHAVAS

Other approaches on the elimination of irrelevant or redundant information, in order to achieve
better performance, have been proposed by Nebel, Dimopoulos & Koehler (1997), Scholz (1999)
and Haslum & Jonsson (2000). The work of Nebel, Dimopoulos & Koehler concerns ignoring
irrelevant facts and actions (not objects), based on heuristics that approximate a plan by
backchaining from the goals without taking into account any conflicts. Although this approach is
more powerful, in terms of elimination, than the one presented in this section, it is not solution
preserving. Furthermore, it may be more time-consuming, since it demands the construction of an
initial approximate plan.
Scholz introduces action constraints, i.e. patterns of action sequences that can be applied to the
same states and produce the same overall effects. Action constraints can be used for pruning
purposes by the state-space planners, reducing the size of the search space to the levels of the
partial-order planners (Minton, Bresina & Drummond, 1994), without losing completeness. The
work of Scholz is actually a re-investigation of the sleep sets of actions that were originally
presented by Godefroid & Kabanza (1991) and have been also examined by us, under the name
prohibited actions, in an earlier version of GRT (1999a). The experience of the authors is that
detecting and pruning redundant actions sequences is time consuming, while a more effective
approach is to employ a closed list of visited states, paying however a cost in terms of memory.
The latter approach is adopted by the GRT planning system. However Scholz considers only action
sequences of length two, which makes his approach fast enough but less effective than a closed list
of visited states structure.
Haslum and Jonsson compute a reduced set of actions for a problem, by ignoring actions that
can be equivalently replaced by sequences of other actions. Their approach is solution preserving,
it can be adopted by any STRIPS planner that pre-instantiates all the actions of a problem, and
results, for some planners, in considerable speed-up but also in longer plans.
4.2

Numerical Representation of Resources

In this section, we present an enhanced STRIPS formalism, where resources are represented by
numbers, instead of atoms. The work has been motivated by the mystery domain, but it is suitable
for any domain with resources. Moreover, it can easily be extended to cover domains where
reasoning with numbers is required.
GRT supports an explicit representation of resources in the most natural format, i.e. the
numerical format. According to this, resources are distinguished from other types of objects and are
separately declared using the following statement:
(:resources R1 R2 ... RN )
where Ri are the various resources. Furthermore, declarations of the following form are added to
the initial state description :
(amount R1 V1) (amount R2 V2) ... (amount RN VN)
denoting the initial quantity of each resource. Moreover, it is allowed for resources to participate in
relations with other atomic facts. Finally, action definitions are enhanced, so as to declare explicitly
the consumed resources.
As an example, we consider the mystery domain, which comprises some cities, connected via
edges, some packages that have to be transferred from their initial locations to their destinations
and some trucks. In the beginning, each city has an amount of fuel. For a truck to travel from a city
c1 to an adjacent city c2, c1 must have at least one unit of fuel. After the journey, the fuel of c1 is
decreased by one.

130

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

In the original domain representation, the different fuel quantities are represented by relations of
the form4:
(fuel fuel0) (fuel fuel1) (fuel fuel2)etc.
while the orderings between these quantities are represented by relations as follows:
(adjacent_fuel fuel0 fuel1) (adjacent_fuel fuel1 fuel2) etc.
and the initial amount of resources in each city as:
(city_fuel city1 fuel3) etc.
Finally, the actions that consume resources, e.g. moving a truck, are of the following form:
(:action move
:parameters (?tr ?c1 ?c2 ?f1 ?f2)
:precondition (and (truck ?tr) (city ?c1) (city ?c2)
(adjacent_cities ?c1 ?c2) (fuel ?f1) (fuel ?f2) (at ?tr ?c1)
(adjacent_fuel ?f1 ?f2) (city_fuels ?c1 ?f2))
:effect (and (not (at ?tr ?c1)) (not (city_fuel ?c1 ?f2))
(at ?tr ?c2) (city_fuel ?c1 ?f1)))
In order to have an idea of how resources are represented in GRT, let us consider the STRIPSMYSTY-X-1 problem of the mystery domain. This problem has 6 cities, so 6 resource objects are
declared:
(:resources r1 r2 r3 r4 r5 r6)
The resources are related with their corresponding cities:
(city_fuel city1 r1) (city_fuel city2 r1) ... (city_fuel city6 r6)
Propositions are added to the initial state, denoting the initial availability of each resource:
(amount r1 1) (amount r2 2) ... (amount r6 3)
Finally, action move is defined in a way that separates the resource requirements from the
precondition and the effect lists:
(:action move
:parameters (?tr ?c1 ?c2 ?f)
:precondition (and (truck ?tr) (city ?c1) (city ?c2) (at ?tr ?c1)
(adjacent_cities ?c1 ?c2) (city_fuel ?c1 ?f))
:effect (and (not (at ?tr ?c1)) (at ?tr ?c2))
:resources (amount ?f 1))
Table 2 shows the number of ground facts and ground actions for the first five problems of the
mystery distribution, for the two alternative resource representations. As it is clear from this table,
through the numerical representation of resources there is an important reduction in the number of
ground facts, which is more considerable in the case of ground actions. What is even more
important is that the size of the problem in the atom-based representation can grow illimitably, if
more levels of resource availability are added, whereas in the numerical representation the size of
the problem remains constant.

4

In the AIPS-98 competition, different predicate and object names have been used; however, in this paper we have
translated them into more meaningful ones for simplicity.

131

fiREFANIDIS & VLAHAVAS

Atom representation
ground facts
ground actions

Problem
strips-mysty-x-1
strips-mysty-x-2
strips-mysty-x-3
strips-mysty-x-4
strips-mysty-x-5

101
359
277
178
299

Numerical Representation
ground facts
ground actions

150
3596
1676
210
2325

56
310
230
144
269

48
1200
816
168
1032

Table 2: Size of the problem (number of ground facts and actions)
for the two alternative resource representations.

5.

Using XOR Constraints to avoid Local Optimal States

In this section, we tackle the problem of local optimal states. Firstly, we illustrate the problem, then
we introduce XOR-constraints and finally we present how these are exploited by GRT in order to
avoid local optima.
5.1

Local Optimal States

During the search phase, GRT always selects to expand the most promising state, according to its
heuristic. If the various facts of a problem were independent or even if GRT always managed to
track their interactions through the related facts, this strategy would be optimal. However, this is
not always the case and some times the search is led to local optimal states. Therefore, the planner
should temporarily backtrack to less promising states, before selecting the most promising ones.
Figure 2 presents an example situation:

2
1
0

Initial state
K
R
0

1

Goal state
2
1
0

2

K

0

1

R
2

Figure 2: A 3x3 grid problem.
The problem refers to a grid-like domain (McDermott, 1999), where K is a key and R is a robot.
The robot can only proceed to adjacent positions. The valid actions are get and leave the key and
move the robot. Table 3 shows part of the Greedy Regression Table for the problem of Figure 2.
According to this Table, the distance between the initial and the goal state is 10. There are two
applicable to the initial state actions, moving R to n1_0 and moving R to n0_1. After moving R to
n1_0 the resulting state has a distance from the goals equal to 9, whereas after moving R to n0_1
the resulting state has a distance from the goals equal to 11. So the planner decides to move R to
n1_0 and subsequently to n2_0. However, it is obvious that the optimal first movements are
moving the robot to n0_1, next to n0_2, getting the key etc.

132

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

Fact

Distance
from Goals

Related Facts

(at R n2_0)
(at K n2_2)
(at R n1_0)
(at R n0_0)
(at R n0_1)
(at R n2_1)
(at R n2_2)
(in R K)
(at R n1_2)
(at K n1_2)
(at R n0_2)
(at K n0_2)

0
0
1
2
3
1
2
3
3
7
4
8

()
()
()
()
()
()
()
( (at R n2_2) )
()
( (at R n1_2) )
()
( (at R n0_2) )

Table 3: Part of the Greedy Regression Table for the 3x3 grid problem.
Initially the planner does not select the optimal action, since it leads to a state with a greater
distance from the goals, according to the heuristic. In order to decide to move the robot towards the
key, the planner should go through all the other valid plans, then backtrack and move the robot to
worse states (this requires that the planner maintains a closed list of visited states and does not
revisit them). In difficult problems, the number of states that the planner has to visit before
following the optimal direction, is extremely large. This is the main reason why GRT, like many
other heuristic planners, does not handle grid-like domains efficiently.
For the 3x3 grid problem of Figure 2, an ideal planner should detect that, in order to move the
key from n0_2 to n2_2, it is necessary that the robot gets the key, so the fact (at R n0_2) should be
achieved before the fact (at R n2_0). However, the planner does not know that the facts (at R
n0_0), (at R n2_0) and (at R n0_2) are related in some way, because the domain definition does not
provide this piece of information. Therefore, it is necessary to provide the planner with information
about relations that hold between the facts of the problem.
5.2

Defining XOR-constraints

In order to avoid local optimal states, we provide GRT with knowledge of relations between facts,
where exactly one of the facts can hold in each state. We call these relations XOR-constraints.
Definition 2 (XOR-constraint). An XOR-constraint is a relation between ground facts. The
relation is valid in a state, if exactly one of the participating facts holds in that state.
The general form of an XOR-constraint schema is the following:
((xor f1 f2 ...) c1 c2 ...)
where fi are the facts that cannot co-appear in any state and ci are static facts that provide
supplementary conditions such as type constraints, relations between objects, etc.
XOR-constraints can be formalized for almost any domain. For example, in the logistics domain
we could define the following XOR-constraints:
( (xor ( at ?Truck * ) ) ( truck ?Truck ) )
( (xor ( at ?Plane * ) ) ( plane ?Plane ) )
( (xor ( at ?Package * ) (in ?Package * ) ) ( package ?Package ))

133

fiREFANIDIS & VLAHAVAS

Question marks (?) precede named variables, whereas asterisks (*) denote no-named ones. The
definitions mean that for every instantiation of the named variables that appear in an XORconstraint and for all the valid instantiations of the no-named variables, according to the predicate
definitions, exactly one ground fact can hold in each valid and complete state. The above XORconstraints schemas are general definitions that can be grounded in several ways, according to the
different ways in which their named variables can be instantiated.
In some cases, it is possible to have XOR-constraints that incorporate AND relations. For
example, if in the logistics domain the predicate (out ?Package) is defined, which means that a
package is not loaded either in a truck or in a plane, then the relevant constraint should be written:
( ( xor ( and ( ( at ?Package * ) ( out ?Package ) ) ( in ?Package * ) ) ( package ?Package ) )
Note that some facts may not appear in any XOR-constraint, while some others may appear in
more than one. Henceforth, we refer to facts that appear in at least one XOR-constraint as XORconstrained facts.
It is a requirement of the current version of GRT that the XOR-constraints are included in the
domain definition. However, they could be computed analytically, based on the mutual exclusion
relations between the facts of a problem, since mutually exclusive facts cannot appear
simultaneously in any valid state. However, providing them manually allows for some form of
guidance, since the domain engineer can leave out some of them, since they would lead to pointless
decompositions.
The notion of XOR-constraints is not new in planning. Gerevini and Schubert (1998) proposed
a method for the automatic inference of state constraints from the action definitions and the initial
state. Single valuedness constraints or sv constraints are the closest to the XOR-constraints. But sv
constraints concern instantiations of the same predicate, while XOR-constraints can be relations
between ground facts of different predicates. However, in more recent work (2000a, 2000b), they
extended their work to also infer XOR-constraints.
The object oriented domain specification formalism introduced by McCluskey & Porteous
(1997) is similar to XOR-constraints. According to this, states are not defined as collections of
facts but as collections of objects, each object having its own internal status. So, XOR-constraints
can be implicitly defined from the requirement that all object attributes are single valued.
5.3

Decomposing Problems into Sub-problems using XOR-constraints

In this section we illustrate how GRT exploits XOR-constraints within the pre-processing phase, in
order to avoid local optimal states. Specifically, using them GRT manages to establish new ordered
subgoals that have to be achieved before achieving the original goals. These subgoals are grouped
into ordered intermediate states, thus the original difficult problem is decomposed in a sequence of
easier subproblems that have to be solved sequentially.
We will present the steps of the problem decomposition process through the example of Figure
3, a 4x4 grid problem with two keys (K1 and K2) and two robots (R1 and R2).
Initial State
K2
R2

3
2
1
0
0

R1
1

2

3
2
1
0

K1
3

Goal State
R2 K2
K1
R1
0

1

Figure 3: A 4x4 grid problem.
134

2

3

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

Goal facts
(at R1 n0_0)
distance=0
-

(at R1 n1_0)
distance=1

(at R1 n1_1)
distance=2

(at R1 n2_0)
distance=2

(at R1 n3_0)
distance=3

(move R1 n1_0 n0_0)

(move R1 n1_1 n1_0)

(move R1 n2_0 n1_0)

(move R1 n3_0 n2_0)

(at K1 n1_1)
distance=0
-

(at R2 n0_3)
distance=0
-

(holding R1 K1)
distance=3

(at K1 n3_0)
distance=7

(leave R1 K1 n1_1)

(at R2 n2_2)
distance=3

(get R1 K1 n3_0)

(move R2 n2_2 n2_3)

(at R2 n1_3)
distance=1

(at R2 n2_3)
distance=2

(move R2 n1_3 n0_3)

(move R1 n2_3 n1_3)

(at R2 n3_3)
distance=3
(move R2 n3_3 n2_3)

(at K2 n1_3)
distance=0
-

(holding R2 K2)
distance=2

(at K2 n3_3)
distance=6

(leave R2 K2 n1_3)

(get R2 K2 n3_3)

Figure 4: Part of the Greedy Regression Graph for the 4x4 Grid problem.
For this domain the following XOR-constraints can be defined:
( ( xor ( at ?Robot * ) ) ( robot ?Robot ) )
( ( xor ( at ?Key * ) ( holding ?Key ) ) ( key ?Key ) )
The above definitions have four ground instantiations, one for each Robot and one for each Key.
Henceforth the notation XOROBJ will refer to the ground XOR-constraint concerning object OBJ.
The first information that can be extracted is pairs of facts, one from the initial state and one
from the goals, which belong to the same ground XOR-constraint. For the problem of Figure 3 the
following pairs can be identified:
XORR1: (at R1 n1_0)
XORR2: (at R2 n2_2)
XORK1: (at K1 n3_0)
XORK2: (at K2 n3_3)

-

(at R1 n0_0)
(at R2 n0_3)
(at K1 n1_1)
(at K2 n1_3)

The original GRT planner did not store information about the inverted actions, which achieved
the various facts in the heuristic construction phase. However, in order to exploit the XORconstraints, this information has to be stored. By storing these actions, the table structure used by
the GRT heuristic is transformed to a directed acyclic graph. We call this structure Greedy
Regression Graph or simply GRG.
The nodes of this graph are labeled with the facts of the problem. Each node retains also the
estimated distance between its fact and the goals and the corresponding related facts. It retains also
the name of the inverted action that achieved its fact. The arcs that point to a node originate from
the nodes of the preconditions of the inverted action that achieved the node's fact. Figure 4 shows
part of the GRG structure for the 4x4 grid problem (the related facts are omitted).
Based on GRG, for every ground XOR-constraint, a sequence of actions which is able to
transform the initial state fact to the corresponding goal state fact can be derived. We are interested
only in the actions that change the XOR-constraint's facts and not in actions that provide auxiliary
preconditions. For the problem of Figure 3, the actions' sequences are shown in Table 4:

135

fiREFANIDIS & VLAHAVAS

Initial state

Intermediate goals

Goal state

XORR1

(at R1 n1_0)

(at R1 n3_0)

(at R1 n1_1)

(at R1 n0_0)

XORR2

(at R2 n2_2)

(at R2 n3_3)

(at R2 n1_3)

(at R2 n0_3)

XORK1

(at K1 n3_0)

(holding R1 K1)

(at K1 n1_1)

XORK2

(at K2 n3_3)

(holding R2 K2)

(at K2 n1_3)

Figure 5: The ordering graph for the 4x4 grid problem.
XOR
constraints

Initial State
Facts

Goal State
Facts

XORR1
XORR2

(at R1 n1_0)
(at R2 n2_2)

(at R1 n0_0)
(at R2 n0_3)

XORK1
XORK1

(at K1 n3_0)
(at K2 n3_3)

(at K1 n1_1)
(at K2 n1_3)

Sequences of actions
(move R1 n1_0 n0_0)
(move R2 n2_2 n2_3) (move R2 n2_3 n1_3)
(move R2 n1_3 n0_3)
(get R1 K1 n3_0) (leave R1 K1 n1_1)
(get R2 K2 n3_3) (leave R2 K2 n1_3)

Table 4: Sequences of actions that transform the initial state facts
to the corresponding goal facts.
Checking the preconditions of the above actions, we can find facts that are members of foreign
XOR-constraints. These facts are subgoals that have to be temporarily established, before
achieving the original goals, in the forward search phase. In Table 4, the actions (get R1 K1 n3_0)
and (leave R1 K1 n1_1) of the XORK1 sequence have (at R1 n3_0) and (at R1 n1_1) as
preconditions respectively, which are members of the XORR1 relation. Similarly, the actions (get
R2 K2 n3_3) and (leave R2 K2 n1_3) of the XORK2 sequence have (at R2 n3_3) and (at R2 n1_3)
as preconditions respectively, which are members of the XORR2 relation.
There are two types of subgoals. These are the XOR-constrained facts that are either:
(I)
(II)

preconditions of a ground action in a foreign XOR sequence, or
add-effects of an action, in their own XOR sequence, which has a foreign precondition.

From the identified subgoals, we can construct a graph, conjoining the new subgoals with arcs
that denote ordering constraints, using the following rules:
1. All the subgoals are ordered after their initial state fact and before their goal fact (if any).
2. Subgoals of type (II) that are members of the same XOR-constraint are ordered according to
the ordering of their actions.
3. Subgoals of type (I) are ordered together with the corresponding subgoals of type (II), which
have resulted by the same action.
4. For a specific XOR-constraint, subgoals of type (I) are ordered before the subgoals of type (II).

136

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

We call the resulted graph the ordering graph of the problem, since it denotes the order in
which the subgoals have to be achieved. Figure 5 shows the ordering graph for the problem of
Figure 3. Lines with arcs denote ordering constraints. Double-lines without arcs denote that the two
facts are ordered together.
Proposition 8. The ordering graph is an acyclic graph.
Proof sketch: The proof can be based on the way in which the facts are achieved in the PreProcessing Algorithm (Section 2.4). Actually, facts are achieved in a specific time order (in case
where a fact has been re-achieved with smaller cost, we consider the last time it has been
achieved). We define the ordering relation < between facts, denoting that a fact has been achieved
before another in the Pre-Processing Algorithm. Similarly we define the  relation.
Ordering relations between the subgoals originate in two ways. Firstly, subgoals of type (II) of
the same XOR-constraint are ordered explicitly to each other, according to the time they have been
achieved (in Figure 5 these ordering relations are denoted with non-dashed lines with arcs).
Secondly, each subgoal of type (I) is ordered before than or at least at the same time with the
previous one of its corresponding type (II) subgoal (in Figure 5 these ordering relations are denoted
with dashed lines with arcs). Using the above equivalences, we can transform the ordering graph to
an equivalent time-ordering graph. Since a time-ordering relation cannot include cycles, the same
happens for the ordering graph. 
The ordering graph makes it possible to construct intermediate, possibly incomplete, states,
which have to be achieved sequentially. Starting from the initial state, GRT attempts to insert one
subgoal from each XOR-constraint in each intermediate state. This fact must have the following
properties:




It has not been inserted in a previous intermediate state,
it is not ordered after some other fact of the same XOR-constraint that has not yet been inserted
in a previous intermediate state, and finally
it is not ordered together with a fact of another XOR-constraint that cannot be inserted in the
current intermediate state.

In case where there are more than one facts with the above properties for a single XORconstraint, the selection among them is done arbitrarily. Finally, in case where no fact with the
above properties exists for an XOR-constraint, the intermediate state is left incomplete.
Corollary 4. It is always possible to construct the intermediate states.
Corollary 4 follows from Proposition 8. Since the ordering graph is a directed acyclic graph, it
is always possible to find at least one subgoal to be included in the next intermediate state. The
number of subgoals is an upper bound for the number of the intermediate states that will be
constructed.
From the ordering graph of Figure 5, the following intermediate states can be extracted:
Intermediate state 1: ( (at R1 n3_0) (at R2 n3_3) (in K1 R1) (in K2 R2) )
Intermediate state 2: ( (at R1 n1_1) (at R2 n1_3) (at K1 n1_1) (at K2 n1_3) )
Intermediate state 3: ( (at R1 n0_0) (at R2 n0_3) (at K1n1_1) (at K2 n1_3) )
where the last state is the goal state.
After the construction of the intermediate states, the planner has to solve three sub-problems,
which are easier than the original one; thus, the overall time to solve them is shorter than the time
137

fiREFANIDIS & VLAHAVAS

needed to solve the original problem. Note, however, that this decomposition may lead to loss of
completeness. In domains where no deadlock exists, some solutions may be pruned. In domains
where deadlocks do exist, the decomposition may produce unsolvable sub-problems. In order to
maintain completeness, the algorithm should backtrack to all the possible inverted actions that
could achieve the facts in the Pre-Processing Algorithm, even those with large application costs.
However, due to the combinatorial explosion problem, this approach is not adopted by GRT.
A usual situation is the case where the sub-problems need further decomposition. This situation
arises in two cases. The first is when two objects need each other to achieve their goals, as in the
case of grid domain, with the keys and the robot, and the second case is when there is a sequential
interaction between three or more objects. In these cases, the ordering graph of the initial problem
encodes one aspect of the interaction, while the ordering graphs of the sub-problems encode other
aspects. However, in order to avoid infinite decompositions, a cutoff level is defined.

6. The GRT Operation
GRT has been implemented in C++5. Its operation consists of several stages, which are shown in
Figure 6a.
Domain file
Problem file

Parsing

Facts and
Actions
computation

Mutex
computation

Domain
enrichment

Problem
processing

Plan

(a) The GRT operation stages
Problem processing
Irrelevant
objects
elimination

Goals
completion

Heuristic
construction

Problem
decomposition

State-space
search

Partial
solution
merging

(b) The problem processing stage

Figure 6: The overall operation of the GRT planning system.
In the first stage the domain and problem files are parsed and the initial data structures are
constructed. The second stage consists of computing the facts and the actions of the problem. The
facts are stored in a tree structure, which is indexed by their predicates and their objects and allows
for fast access, while the actions are stored in a linked list. Moreover, multiple pointers connect
each fact with the actions, where the fact appears. The computation of the facts and actions is
performed incrementally, by repeatedly applying the following steps:

If a fact has been reached, create new actions that include this fact and others already reached,
in their preconditions.

If an action has been created, add its add effects in the tree structure.
The process starts with the initial state facts and continues until no more facts and actions can
be reached. This approach is time efficient and succeeds in not generating many unreachable facts
and actions. For example, in the logistics domain, the facts denoting that a truck is located in a city
5

GRT is available on-line at http://www.csd.auth.gr/~lpis/GRT/main.html.

138

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

different than its initial location, and the corresponding actions, are not created. Note that in this
stage, both the normal and the inverted actions are computed; the former are used in the mutex
computation stage, while the latter are used in the heuristic construction stage. However, no preinstantiated actions are used during the state-space search, where the applicable actions to each
state are computed by progressively instantiating the action schemas, using constraint satisfaction
techniques (forward checking and intelligent backtracking).
The stages that follow are the computation of the mutual exclusion relations, the enrichment of
the domain, and the problem processing. The latter stage consists of several sub-stages, as it is
shown in Figure 6b, where the most important ones are the construction of the heuristic and the
state-space search. Note that when we refer to the pre-processing phase of GRT, we mean all stages
that precede the state-space search.
In the case where XOR-constraints are provided, GRT attempts to decompose the current
problem into sub-problems. If this attempt is successful, the problem processing stage is executed
recursively for each sub-problem, otherwise the current problem is solved. Finally, in the case of
decompositions, the partial solutions are merged and the overall solution is returned.

7. Related Work
This section briefly presents other domain independent heuristic state-space planning systems, by
emphasizing their similarities and differences to GRT, in terms of the way in which they construct
their heuristic and the direction they traverse the state-space. We omit certain pieces of related
work that concern specific pre-processing techniques implemented in GRT, as for example the
elimination of irrelevant objects, since they have already been presented in previous sections.
The recent evolvement of the domain independent heuristic planning started with the work of
Drew McDermott (1996, 1999) on UNPOP (UN-Partial Order Planner, UN- stands for non-).
McDermott's planner is not restricted to pure STRIPS representations, supporting the more
expressive language ADL (Pednault, 1989). The planner proceeds forward in the state-space.
Distance estimates between states are based on the so-called regression graph, which is built from
the goals using non fully-instantiated actions. UNPOP does not consider subgoals interactions and
reconstructs the regression graph from scratch for each intermediate state. Although this planner is
not competitive enough, compared to the subsequent heuristic planners, it was the faster one at the
time of its appearance. However, we have to note that UNPOP has been developed in LISP, whereas
the other heuristic planners are highly optimized C or C++ programs.
Although UNPOP was the first domain independent heuristic planner, the area has been pushed
forward by the ASP (Action Selection Planner, Bonet, Loerings & Geffner, 1997) and HSP
(Heuristic Search Planner, Bonet & Geffner, 1998) planners. The attractive feature of these
planners is the simple way the heuristic is constructed, presented in Section 2.1. ASP used a bestfirst strategy with limited agenda, while H SP uses a hill-climbing one with limited plateau search
and restarts (an in-depth presentation of the state-space search algorithms is given by Zhang, 1999).
Both ASP and HSP reconstruct their heuristic from scratch for each intermediate state. A
variation, called HSPr (r stands for regression), constructs the heuristic only once (Bonet &
Geffner, 1999). This approach resembles GRT, although HSPr constructs the heuristic forward and
searches backwards. Both approaches have the problem of incomplete goal states, however it arises
in different phases of the planning process. GRT faces this problem in the pre-processing phase, by
enhancing the goals, as it has been described in Section 3. In HSPr, the problem arises in the search
phase, in the form of invalid states in the regression state space. To cope with the problem, HSPr
computes mutual exclusion relations and checks each state in the regression state space for any

139

fiREFANIDIS & VLAHAVAS

possible violation of these relations. The disadvantage of this approach is that it is considerably
more time consuming than the GRT approach, since the HSPr has to check each visited state.
A variation of HSP, named HSP-2, changed the hill-climbing strategy to a best-first one, thus
preserving completeness and producing better plans (Bonet & Geffner, 2001). Moreover, HSP-2
uses a weighted A* algorithm (WA*) (Pearl, 1983) of the form f(S)=g(S)+Wh(S), where S is an
intermediate state, g(S) is the accumulated cost from the initial state, h(S) is the estimated cost to
reach the Goals and W is a parameter. For W=0, the search algorithm behaves as a breadth-first
one, for W=1 it behaves as the typical A* and for W it behaves as best-first. For the h(S)
function, HSP-2 supports several heuristic functions, apart from the one presented in Section 2.1.
Recently, two new planners, FF and ALTALT, appeared, which use a GRAPHPLAN-based
approach to estimate distances between the intermediate states and the goals. ALTALT (A Little of
This, A Little of That) is a regression planner based on HSPr, which faces the same problems with
invalid states as HSPr (Nigenda, Nguyen & Kambhampati, 2000). ALTALT creates a planning graph
in a pre-processing phase and uses several techniques to extract heuristic estimates of the distances
between the intermediate states and the initial state. For example, one of them returns the level in
the planning graph, where all the facts of the intermediate state appear, without any mutual
exclusion relation between them.
FF (Fast Forward) is a forward heuristic planner (Hoffmann & Nebel, 2001). In order to
estimate the distance between an intermediate state and the goals, FF creates a planning graph from
the state to the goals, using relaxed actions. Since there are no delete effects, there are no mutual
exclusion relations in the planning graph. From this graph, FF extracts a relaxed plan, the length of
which is the distance estimate. Note that, since there are no mutual exclusion relations, no
backtracking occurs during the extraction of the relaxed plan, thus the extraction is accomplished
fast enough. The FF heuristic resembles the GRT one, in that both aim in obtaining under-estimates,
but they adopt different approaches. The relaxations that FF performs are stronger, since it
completely ignores the delete effects. So the FF estimates are usually smaller than the GRT's ones
and most of the times are underestimates, whereas GRT not-rarely produces overestimates.
FF adopts a variation of the hill-climbing strategy, called enforced hill climbing, according to
which, the planner always seeks to move to a state closer to the goals, according to its heuristic. FF
achieves that by performing a bounded breadth-first search from the current state, with a maximum
depth defined by the user; so the improving state does not have to be a direct successor of the
current state. Once that an improving state is found, the new actions are added to the end of the
current plan and the hill-climbing search continues from the new state. In the case where the
bounded breadth-first search does not find an improving state, FF restarts the search from the initial
state adopting a best-first search strategy.
FF exhibited distinguishable performance at the AIPS-00 planning competition. One of the
features of FF resulting in its good performance is that it does not compute the applicable actions
for each intermediate state. Actually, FF gives priority to the first level actions of the relaxed plan.
Once that an action that produces a better state is found, it is applied and the next state is processed.
Moreover, at most of the times, no new relaxed plan has to be constructed, since it suffices to
remove the lastly applied action from the beginning of the previous relaxed plan. So, FF succeeds
in reducing drastically the cost of processing each intermediate state, paying however the cost of
loosing completeness.
The bottleneck that occurs while determining the applicable actions for each intermediate state
has also been identified by Vrakas et al. (1999, 2000). In this work, the process of finding and
applying the applicable actions has been parallelized, resulting in almost linear speedup.
Parallelizing the process of finding the applicable actions, instead of ignoring most of them, as FF

140

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

does, presents the advantage of preserving completeness; however, the cost is that a parallel
machine is required.
We close the reference to other heuristic state-space planners with the STAN planning system
(Fox & Long, 1998; Long & Fox, 1999). STAN is not a heuristic state-space planner, at least in its
basic architecture, but a graph-based planner, which uses several pre-processing techniques for
extracting useful domain information that is exploited for more efficient graph construction and
solution extraction. However, in the AIPS-00 competition a hybrid architecture was used (Long &
Fox, 2000; Fox & Long, 2001), where a heuristic state-space planning module was employed to
solve specific identified sub-problems. Thus STAN succeeded in improving its performance,
especially in cases of transportation domains.
Concerning problem decomposition, work has been done on goal ordering (Cheng & Irani,
1989; Drummond & Currie, 1989). Recently a similar approach has been proposed by Koehler
(1998) and has been extended by Koehler and Hoffmann (2000). This approach automatically
derives an ordering relation between the goal facts, which can be used by any planner to search for
increasing sets of subgoals. The advantage of this approach is that no extra information is needed,
except for the usual domain definition, while the disadvantage, with respect to the XOR-constraints
approach, is that only the goal facts are taken into account in the intermediate states that are
constructed. This approach has been adopted by the FF planning system.

8. Performance Results
In this section, we present performance results from several domains, taken from the literature and
from the two planning competitions. First, we investigate how the several techniques of GRT
contribute to its overall performance and then we compare GRT to other planners.
The measurements that follow were taken on a SUN Enterprise 3000 machine running at
167MHz, with 256 MB main memory and operating system Solaris 2.5.1. In the experiments we
set a 5 minutes time limit for all experiments and planners6.
8.1

Measuring the Effectiveness of the Related Facts

In order to measure the contribution of the related facts to the overall performance of GRT, we
tested the planner, with and without related facts, on problems from various domains. The results
(solution length and time) are presented in Figure 7 (a-f).




/HQJWK




7LPH



























:LWKRXW 5HODWHG
:LWK 5HODWHG



:LWKRXW 5HODWHG
:LWK 5HODWHG





























(a) Logistics problems (the goals have been enhanced with the most promising facts selection method)

6

In the URL http://www.csd.auth.gr/~lpis/GRT/JAIR/OnlineAppendix1.html
it can be found the executable files of all planners that took part in the comparison, the source code of GRT, the detailed
results (in MS-Excel format), the original data files, the problem description files and the script files for each planner.

141

fiREFANIDIS & VLAHAVAS





/HQJWK



:LWKRXW 5HODWHG
:LWK 5HODWHG



7LPH














:LWKRXW 5HODWHG
:LWK 5HODWHG





































(b) Blocks-world problems with 4 action schemas (push, pop, pick-up, put-down)


:LWKRXW 5HODWHG
:LWK 5HODWHG

/HQJWK





















7LPH

:LWKRXW 5HODWHG
:LWK 5HODWHG














































(c) Blocks-world problems with 3 action schemas (several cases of move)




/HQJWK






:LWKRXW 5HODWHG
:LWK 5HODWHG



7LPH





:LWKRXW 5HODWHG
:LWK 5HODWHG













































(d) FreeCell Problems




/HQJWK












:LWKRXW 5HODWHG
:LWK 5HODWHG














 





 



7LPH

:LWKRXW 5HODWHG
:LWK 5HODWHG














 



 

 









(e) Elevator problems


/HQJWK





:LWKRXW 5HODWHG
:LWK 5HODWHG



7LPH
:LWKRXW 5HODWHG
:LWK 5HODWHG




























(f) Puzzle problems

Figure 7: Solution length and time (in msecs) with and without the use of related facts
for problems from several domains.

142



fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

We can classify the above domains in three groups. The first group includes the domains where
the use of related facts clearly improves both the solution length and time. This group comprises
the logistics domain (6a), the blocks-world, when a 3-action schemas representation (move actions)
is used (6c), and the puzzle domain (6f). In these domains, there were many cases where GRT
without related facts did not solve the problems, while with the related facts it did. Moreover, in
most cases when both versions solved a problem, the version with the related facts was faster and
came up with a shorter plan.
The second group includes domains where the use of related facts does not affect the
effectiveness of the planning process. This group comprises the elevator domain, along with the
gripper, the movie and the mystery ones. In these domains, there is usually a single way to achieve
the goals, so both versions produce identical plans. However, due to the processing overhead,
imposed by the computation of the related facts, the version with the related facts is slightly slower
than the version without them.
Finally, the third group includes the domains where there is no apparent predominance between
the two versions. The freecell domain and the blocks-world domain, when a 4-action schemas
representation is used (push, pop, pick-up, put-down), fall into this class. In these domains the two
versions do not have equal performance, but there are problems where one version surpasses the
other and vice-versa.
The conclusion drawn from the above measurements is that the effectiveness of the related facts
depends on the domain. They are more suitable in domains where there are several ways to achieve
the goals, as logistics or blocks-world.
Additionally, their efficiency depends on the way the domain is codified. A typical example is
the blocks-world domain and the 4- and 3-action schemas representations. The problem with the 4action schemas representation is that pushing and stacking a block anywhere has always the same
fact as precondition, i.e. that the block is held by the arm. The consequence is that neither the
related facts, nor the distances are computed correctly. However this is not a problem of the related
facts, it is a common problem in domain independent heuristic planning, as it results from the last
planning competition. On the other hand, if a 3-action schemas representation is used, then the
paths to achieve the facts of the domain are better tracked, so larger problems can be solved and the
contribution of the related facts is significant. We believe, finally, that also in the freecell domain
there is a representation inefficiency, however we have not yet tried to construct an alternative one.
8.2

Using Several Methods to Enhance the Goals

In order to measure the effectiveness of the three proposed methods to enhance the goals, we ran
GRT using them in the logistics problems of the AIPS-00 competition. We selected this domain,
since in the other domains of the competition the goal state is either complete, or near complete, so
there is no difference among the three methods. Figure 8 shows the solution length and time for the
easiest of the logistics problems.
With regard to solution length, the first method, which considers all the candidate facts as goal
facts, always came up with better plans. As we mentioned in Section 3.2, this method produces
small differences among the estimated distances, so the search process tends to be breadth-first.
However, in most of the cases, the third method found plans of equal quality. With regard to the
solution time, the last two methods work faster, since they produce greater differences between the
distances.
In Section 3.3 we also presented a method of enriching the domain representation. As already
mentioned, we were motivated by the need to treat domains like the movie or the elevator. We do
not present comparative performance results between the domain enrichment method and the pure
GRT planner for these domains, since without this technique it is impossible for GRT to solve the
143

fiREFANIDIS & VLAHAVAS

problems. However, it would be interesting to test the efficiency of this method to other heuristic
state space planners.




/HQJWK

7LPH










$OO
,QLWLDO
*UHHG\

$OO
,QLWLDO
*UHHG\









































Figure 8: Results for logistics problems using different methods to complete the goals.
All = Consider all the candidate facts as goal facts.
Initial = Select the initial state facts.
Greedy = Favor the most promising facts.

8.3

Reducing the Size of the Problem

The work of detecting and eliminating irrelevant objects has been motivated by the need to
simplify the sub-problems resulting after the decomposition of a problem, when using XORconstraints. Performance results for this case are presented in Section 8.4. This section presents
indicative results concerning the effectiveness of the technique in the colored logistics domain that
has been mentioned in Section 4.1. For this purpose we enhanced the first group of logistics
problems of the AIPS-00 competition with the required predicates and actions and we added
propositions defining the original color of each package to the initial states. Figure 9 presents the
time needed to solve the problems, with and without the irrelevant objects elimination technique.
As it results from the experimental data, there is an improvement in the solution time of about 20%.
Note that in both cases the same plans have been found; however, this would probably not be the
case in other domains.
In order to measure the efficiency of the numerical representation of resources, we ran GRT both
in the original mystery domain and in a modified domain, where resources have been represented
by numbers. Figure 10 presents the time needed to solve the problems with both cases of GRT.
Note that in these experiments only the solvable mystery problems have been taken into account.
As it results from Figure 10, GRT was significantly faster, when a numerical representation is used.
The improvement is 65% on average. As for the solution length, in both cases the same have been
found again.
Both techniques evaluated in this section gain their speedup mainly from the pre-processing
phase, since distances for a significantly smaller number of facts have to be estimated. As for the
search phase, there is also a speedup, but is less important. Actually, the number of applicable
actions to each state is the same with the two alternative representations of resources, since these
are equivalent. Moreover, the detection of the applicable actions in the atom-based representation
takes about the same time, due to the effective constraint-satisfaction techniques that GRT uses
when instantiating the action schemata. Concerning the elimination of irrelevant objects, without
this technique, there are more applicable actions to a state, which however are usually not selected,

144

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

since they do not lead to an improving state. However, the time spent in the detection of these
actions may be not negligible.
The significance of the two techniques lies in that the overall time needed to solve the problems
remains about the same, in the case where more irrelevant objects are used, and exactly the same,
in the case where more resource levels are used. In the case of more irrelevant objects, these are
detected (in negligible cost) and eliminated from the subsequent stages (Figure 6). However, there
is some overhead imposed by the stages that precede the irrelevant objects elimination stage, from
where these objects have not been eliminated.
In the case of more resource levels, these do not lead to the generation of new ground facts and
actions, so all the pre-processing stages consume exactly the same time. As for the state-space
search, this is also executed in the same time, but only in the case where neither the initial
availability of resources, nor their consumption by the actions, nor finally the constraints over them
have been changed. If this is not the case, then we are dealing with a different planning problem,
which may be harder to solve.


7LPH



(OLPLQDWLQJ LUUHOHYDQW REMHFWV
8VLQJ DOO REMHFWV
















Figure 9: Time (in msecs) needed to solve the colored logistics problems,
with and without the irrelevant object elimination technique.

$WRP EDVHG UHSUHVHQWDWLRQ
1XPHULFDO UHSUHVHQWDWLRQ



7LPH


























Figure 10: Time (in msecs) needed to solve the solvable mystery problems,
when the original atom-based or a number-based representation for resources is used.
8.4

XOR Constraints

We tested the efficiency of the XOR-constraints based decomposition in two domains: A simplified
mystery domain, where resources have been removed, and the grid domain of the AIPS-98
competition. We did not use the logistics domain for these experiments, since logistics problems

145

fiREFANIDIS & VLAHAVAS

are not difficult for the original GRT and the small profit from solving the easier sub-problems is
compensated by the extra pre-processing cost of each sub-problem.
We removed resources from the original mystery domain because otherwise it would be
probable to obtain unsolvable subproblems. As it has been noted in Section 5, decomposing a
problem may lead to loss of completeness, thus the technique is unsuitable for domains where
deadlocks may arise, as the original mystery one. Note that by removing resources, all mystery
problems become solvable.
The XOR-constraints that have been defined for the simplified mystery domain were the
following:
( ( xor ( at ?Truck * ) ) (truck ?Truck ))
( ( xor ( at ?Package * ) (in ?Package * ) ) ( package ?Package ) )
while for the grid domain were the following ones:
( ( xor ( at-robot * ) ) )
( ( xor ( locked ?Place ) ( open ?Place ) ) ( place ?Place ) )
( ( xor ( at ?Key * ) ( holding ?Key ) ) ( key ?Key ) )
Note that in the grid domain an XOR-constraint denoting that the arm is either empty, or the
robot holds a key has not been defined, since this would lead to pointless decompositions.
In both domains, we ran GRT with and without the problem decomposition technique.
Additionally, in order to demonstrate the contribution of the irrelevant objects elimination
technique when solving the sub-problems, we conducted experiments for this case in the simplified
mystery domain. We did not consider this case in the grid domain, because no irrelevant objects
can be detected there. Figure 11 presents the results.


1R ;25V

/HQJWK



;25V

7LPH

;25V1R HOLP LQDWLRQ














1R ;25V





















;25V
;25V1R HOLP LQDWLRQ

















(a) Simplified Mystery


/HQJWK



7LP H







ZLWKRXW ;256

ZLWKRXW ;256
ZLWK ;256





ZLWK ;256


















(b) Grid Domain

Figure 11: Solution time (in msecs) and length with and without
the XOR-constraints based problem decomposition technique.
146





fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

As for the simplified mystery domain, GRT without the problem decomposition technique
generally produced shorter plans, as expected. On the other hand, the use of the XOR-constraints
accelerated the problem decomposition process, especially in case of difficult problems. Actually,
if we only consider the seven most difficult problems, the improvement achieved by the
decomposition is 60% on average. Note however that, when the irrelevant objects elimination
technique was not used, there was no improvement. In not difficult problems there is no
acceleration, since, as in the case of the logistics problems, the small profit from the faster solution
of the easier sub-problems is compensated by the cost of repeating the pre-processing phase for
each one of them.
The grid domain was the most difficult one of the AIPS-98 competition. The contestants
managed to solve only the first problem. GRT without XOR-constraints could only solve the first
problem, too. On the other hand, with the XOR-constraints based decomposition, GRT was able to
solve the first four problems in the time limit of 5 minutes, while in the fifth problem it ran out of
memory. It is worth noting that this domain produces multiple levels of decompositions. Figure 12
presents these levels for the strips-grid-y-2 problem.
As far as we know, the only planner that can cope with the grid problems effectively is FF. We
ran FF in the five grid problems and it solved the first four, within the time limit of 5 minutes, with
the following results (length/time): 14/230, 39/840, 40/7810 and 45/3280, which are considerably
better compared to the performance of GRT.
Main problem

Sub-problem 1

Sub-problem 2

Sub-problem 3.1

Sub-problem 3.1.1

Sub-problem 3.1.1.1

Sub-problem 3.1.2

Sub-problem 3.1.1.2

Sub-problem 3

Sub-problem 3.2

Sub-problem 3.3

Sub-problem 4.1

Sub-problem 3.1.3

Sub-problem 4

Sub-problem 4.2

Sub-problem 4.3

Sub-problem 3.1.1.3

Figure 12: Decomposition for the strips-grid-y-2 problem using XOR-constraints.

8.5

Best-First and Hill-Climbing Strategies

Recently we equipped GRT planner with two new features: a second optional search strategy, the
well known hill-climbing, and a closed-list of visited states, in order to avoid revisiting them.
GRT adopts the enforced hill-climbing strategy, originally presented in Hoffmann & Nebel
(2001), according to which, from each intermediate state a limited breadth first search is
performed, until an improving state is reached. When an improving state cannot be found, GRT
restarts the search from the initial state with the typical best-first strategy.
Moreover, the hill-climbing strategy has been enhanced with a fast action selection mechanism.
As it has been presented in Section 5.3, when GRT estimates the distances between the problem's
facts and the goals in the pre-processing phase, it stores in the GRG structure the action that
achieved each fact. So, in order to find an improving successor state quickly, the hill-climbing
search strategy first attempts to apply the actions that achieved the current state's facts. Once that
147

fiREFANIDIS & VLAHAVAS

an improving successor state is found, the remaining of the actions are not processed, thus avoiding
to compute all the applicable to the current state actions. Note however that it is not guaranteed that
these actions can always be applied to the current state. In case where no improving state can be
found, the remaining of the applicable to the current state actions are taken into account.
Figure 13 presents comparative performance results in logistics and elevator problems, using
both search strategies. In the logistics problems, the most promising facts selection method of
enhancing the goals has been used. As it results from the experimental data, in the logistics
problems and with the use of the hill-climbing strategy, there is a significant reduction in the
solution time of about 52%. The cost is an increment of about 3% in the length of the plans. In the
elevator problems, there is also a reduction in the solution time of about 29%, whereas the
produced plans are identical.




/HQJWK

7LP H











+LOO &OLP ELQJ



+LOO &OLP ELQJ

%HV W )LUV W

%HV W )LUV W












































(a) Logistics domain




/HQJWK

7LP H











+LOO&OLP ELQJ



+LOO&OLP ELQJ



%HV W )LUV W

%HV W )LUV W
























































(b) Elevator domain

Figure 13: Comparative results (solution length and time) between the hill-climbing
and the best-first strategies.
We tested the efficiency of the fast action selection mechanism, by also running GRT with the
hill-climbing strategy but without this mechanism in the same logistics and elevator problems.
Concerning the logistics problems, the speedup was about 47%, while the increment in the solution
length was 3% on average again. Concerning the elevator problems, the speedup was 28%, whereas
the produced plans were again identical. The conclusion from these additional measurements is that
the speedup is primarily due to the hill-climbing strategy and secondly due to the fast action
selection mechanism. The contribution of this mechanism depends on the domain and it is more
important in the logistics and less in the elevator. Its inefficiency in the elevator domain means that
the actions that are selected by this mechanism do not usually lead to an improving state or they are
not applicable, so all the applicable actions have to be computed.
Results for other domains, like blocks-world and freecell, are not presented, since in these
domains hill-climbing usually fails to find a plan and GRT restarts on a best-first basis. However, in
148

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

these domains the closed-list of states has been proved invaluable, improving drastically the
performance of GRT. For example, in the freecell domain and without the closed list of visited
states, the GRT planner in the AIPS-00 planning competition succeeded in solving problems with up
to 6 cards per suit, while with this data structure it can solve some of the more difficult ones (13
cards per suit). Note that for an efficient implementation of the closed-list of visited states a hashtable data structure has been adopted.
8.6

Comparison to other Planners

In this section, we present comparative results between the GRT planner and other planners. We
decided to use HSP-2 (Bonet & Geffner, 2001), FF (Hoffman & Nebel, 2001), STAN (Long & Fox,
2000; Fox & Long, 2000, 2001) and ALTALT (Nigenda, Nguyen & Kambhampati, 2000)7. All these
planners took part in the domain independent track of the AIPS-00 planning competition. We
selected these planners because HSP-2 and STAN are state-ofthe-art planning systems, FF has been
awarded for its outstanding performance in the last competition and ALTALT is a new but very
promising domain-independent state-space heuristic planner.
The aim of our experiments is to have an overall view of the performance of the evaluated
systems. Performing pair wise comparisons between specific optimization techniques is not
possible, since these techniques are implemented on top of different systems. Moreover, this kind
of comparisons is out of the scope of this paper, which focuses in the use of specific directions for
constructing the heuristic and traversing the space of the states, in the area of domain-independent
heuristic state-space planning, and not in the evaluation of the numerous pre-processing
optimization techniques. However, in the cases where we identify the contribution of a specific
feature in the performance of a planner, we comment on this.
In order to have fair comparisons, we used exactly the same problem and domain description
files for all planners. So, GRT ran without XOR-constraints or numerical representation of
resources. Moreover, although the irrelevant object elimination technique is an integral feature of
GRT, it had no contribution in these domains, since there were not irrelevant objects. We believe
that the absence of irrelevant objects in these domains does not mean that this technique has limited
applicability, but it is an indication that more real domains for testing purposes have to be used in
the future, since the planning tasks in our real-life are full of irrelevant objects. Finally, the domain
enrichment technique proved valuable for the elevator domain only. However, this technique, as
well as the goal enhancement one, has not to be seen as an optimization technique, but as a way to
overcome the problems that arise from the backward direction of the heuristic construction.
We tested the planners in several domains taken from the planning competitions and from the
literature, in the same workstation and within the 5 minutes time limit. The results are presented in
the following.
8.6.1

LOGISTICS

For the logistics domain we used the test suite of the AIPS-00 competition. The results are shown in
Figure 14. In this domain GRT, as well as FF and STAN, performed well, solving all the problems.
HSP and ALTALT failed to solve the large problems within the time-limit. In general, best plans are
found by STAN, which uses special domain-dependent heuristics for problems identified as
7

STAN is available at http://www.dur.ac.uk/~dcs0www/research/stanstuff/stanpage.html
FF is available at http://www.informatik.uni-freiburg.de/~hoffmann/ff.html
HSP-2 is available at http://www.ldc.usb.ve/~hector/
ALTALT is available at http://rakaposhi.eas.asu.edu/altweb/altalt.html

149

fiREFANIDIS & VLAHAVAS

transportation problems. Best solution times are achieved by FF and STAN in the small problems
and by GRT in the large ones.



/HQJWK

7LPH











))

))

*5 7

*57



+ 63


+ 63



$OW$OW

$OW$OW

67$1

67$1




































Figure 14: Solution length and time (msecs) for the logistics problems of the AIPS-00 competition.
The logistics problems in Figure 14 have incomplete goal states. GRT ran with the most
promising facts goals-completion method and with the hill-climbing strategy. However, the
incompleteness of the goal state is an advantage for the planners that construct the heuristic in a
forward direction. Motivated by this remark, we forced all the planners to solve logistics problems
with complete goal states, requiring all the trucks and planes to return to their initial location. The
results are shown in Figure 15.




/HQJWK

7LPH

+ 63





*57
$OW$OW



67$1




))



+ 63



*5 7


$OW$OW


67$1



))



































Figure 15: Solution length and time (msecs) for logistics problems with complete goal states.
In the new logistics problems, GRT, STAN and HSP-2 exhibited stable performance, solving the
problems in about the same time. For GRT, this means that the goal completion mechanism behaves
well, at least in this domain. FF failed to solve the large problems. Finally, ALTALT solved some
150

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

more problems and this is because the regression mechanism did not encounter invalid states. Note
that, although the goal state was complete in this case, GRT treated these problems as usual,
attempting to enhance the goals.
8.6.2

BLOCKS-WORLD

For blocks-world problems in the AIPS-00 competition a four-actions representation was used, i.e.
actions push, pop, stack and unstack. This representation is unsuitable for GRT, as it has been
explained in Section 7.1. So, GRT did not solve most of the blocks-world problems. Figure 16
presents the results of all planners in all blocks-world problems.
As shown in Figure 16, FF exhibits the best performance, solving the majority of the problems
and producing better plans than the other planners. The superiority of FF in this domain is due to a
technique called Added Goal Deletion, according to which the goal facts are ordered and achieved
in a progressive manner (Hoffmann & Nebel, 2001; Koehler and Hoffmann, 2000). This technique
is especially suited for the blocks-world domain and the 4-action schemas representation. However,
this technique does not always succeeds to produce good orderings and this is the reason why FF
fails to solve some of the easiest problems, which have been solved by the other planners.
As for the remaining planners, HSP-2 succeeded in solving all problems with up to 18 blocks
and one problem with 24 blocks, GRT and ALTALT solved problems up to 14 blocks and STAN up
to 12 blocks. Moreover, GRT produced plans of low quality.




/HQJWK

7LPH

))



*5 7
+ 63





67$1


$OW$OW






))



*5 7


+ 63



67$1



$OW$OW















































Figure 16: Solution length and time (msecs) for the blocks-world problems
using the 4-action schemas domain representation.
In order to demonstrate the influence of the domain representation in the efficiency of GRT, we
ran all the planners in the same problems using the alternative 3-action schemas domain
representation. The results are shown in Figure 17.
The performance of GRT is significantly improved, solving problems with up to 33 blocks and
producing better plans than the other planners. Moreover, with the exception of the smallest
problems, GRT is faster than the other planners, but FF. The latter solved less large problems, but

151

fiREFANIDIS & VLAHAVAS

solved all the smallest ones. HSP-2 solved all the problems with up to 19 blocks, while ALTALT and
STAN stopped at 14 blocks.




7LPH

/HQJWK











))

))
*57


*57

+ 63

+ 63


$OW$OW



67$1

$OW$OW
67$1
















































Figure 17: Solution length and time (msecs) for the blocks-world problems
using the 3-action schemas domain representation.
8.6.3

FREECELL

Freecell is the famous card game taken from the MS-Windows 98 distribution. The domain was
initially introduced in the AIPS-00 competition and proved one of the most difficult domains.
Figure 18 presents the performance results in this domain. Note that ALTALT could not solve these
problems and this was also the case in the competition.
In the freecell domain, the only planners that succeeded to solve some of the difficult problems
were GRT and FF. Actually, these planners solved some problems with 12 and 13 cards per suit.
HSP-2 solved problems with up to 6 cards per suit and STAN up to 3 cards per suit. Regarding the
solution quality, GRT produced better plans than FF. Regarding the solution time, FF was faster in
the small problems, whereas in the big ones the two planners had equal performance.
8.6.4

ELEVATOR

The elevator (or miconic-10) domain has been presented in Section 3.3. At least in its pure STRIPS
version, it is a relatively easy domain. So, all planners found plans of equal quality (with the
exception of HSP-2, which produced slightly more lengthy plans). However, the planners have
different performance in terms of solution time.
Specifically, FF was the fastest, followed by STAN, then GRT, then HSP-2 and finally ALTALT.
This domain favors FF, because the relaxed plan produced by its heuristic mechanism for the initial
state is actually the solution, since the original actions of the domain do not contain any delete lists.
STAN identifies this domain as a transportation domain and uses suitable techniques to solve the
problem. Finally, GRT is faster than HSP-2 and ALTALT, since GRT constructs its heuristic faster
than HSP-2. The results are presented in Figure 19.
152

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING





/HQJWK

7LPH












))
*57



+63
67$1

))




*57
+63
67$1














































Figure 18: Solution length and time (msecs) in the freecell domain.

7LPH

$OW$OW


+63

*57


67$1

))





































Figure 19: Solution time (in msecs) in the elevator domain.
8.6.5

GRIPPER

The gripper domain was introduced in the AIPS-98 planning competition. The domain concerns a
robot with two grippers that must transport a set of balls from one room to another. In the AIPS-98
competition, only HSP managed to solve the 20 problems. Figure 20 presents the results in this
domain.
153

fiREFANIDIS & VLAHAVAS





/HQJWK

7LPH










*57

*57



))

))





+ 63



+ 63

$OW$OW

$OW$OW

67$1

67$1
































Figure 20: Solution length and time (msecs) in the gripper domain.
Regarding the solution length, the five planners have been divided into two groups: GRT,
ALTALT and STAN produced identical plans of higher quality, while FF and HSP-2 produced
identical plans of lower quality. Regarding solution time, GRT is the fastest planner in all problems
apart from some of the easiest, followed closely by STAN, next comes FF, next ALTALT and last
HSP-2. Note that in this domain STAN takes advantage of its symmetry analysis, which identifies
the set of the balls and the two grippers as symmetric objects (Fox and Long, 1999).
8.6.6

HANOI

We ran the planners in 6 hanoi problems, taken by Bonet and Geffner (2001). The six problems
have three to eight disks respectively. Figure 21 presents the results.




/HQJWK
*57



$OW$OW

$OW$OW



))



))

+ 63



7LPH

*57

+ 63



67$1

67$1































Figure 21: Solution length and time (msecs) in the hanoi domain.
Regarding the solution length, all the planners found identical plans, with the exception of the
last two problems, where GRT found worse plans. Regarding the solution time, FF was the faster,
then came GRT and HSP-2, then ALTALT and last came STAN.

154

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

8.6.7

PUZZLE

We ran the planners in four 8-puzzle problems and in two 15-puzzle ones, taken by Bonet and
Geffner (2001). Two of the four 8-puzzle are hard and their optimal solution involves 31 actions,
the maximum plan length in this domain. The 15-puzzle problems are of medium difficulty. Figure
22 presents the results.



+ 63



+ 63
$OW$OW
))

/HQJWK

*57
67$1

*57




7LPH



$OW$OW
67$1





))


































Figure 22: Solution length and time (msecs) in the puzzle domain.
STAN solved only the 8-puzzle instances, but it produced the best plans. The other planners
solved all the problems, but they presented variations in the quality of their plans, with the FF
planning system producing worst plans in most of the cases. Regarding solution time, FF was the
fastest in the easier problems and GRT in the more difficult ones, followed by HSP-2 and ALTALT.
STAN was the slowest planner in this domain.

9. Conclusion and Future Work
In this paper we presented the GRT planning system, a heuristic state-space planner, which
constructs its heuristic in a domain-independent way. The fundamental difference between GRT
and other heuristic state-space planners is that GRT constructs its heuristic once, in a pre-processing
phase and in a backward direction, using regression from the goals. GRT attempts to track the
positive and negative interactions that occur between the problem facts when trying to achieve
them, in order to produce better estimates.
GRT employs several new techniques that improve its efficiency. These are the automated
identification of incomplete goal states, the identification and enrichment of inadequate domain
representations, the elimination of irrelevant objects and the adoption of a numerical representation
of resources. Finally, a knowledge-based method that uses domain axioms in the form of XORconstraints, in order to decompose difficult problems into easier sub-problems that have to be
solved sequentially, has adopted.
The paper presented extensive comparative results in a large number of domains. In the
comparisons, besides GRT, four of the most powerful domain independent planners took part. The
results showed that no planner clearly outperforms all the others.
Concerning solution time, in most of the domains GRT and FF were the fastest planners. The
explanation behind this observation lies in that these planners construct their heuristic either once
(in the case of GRT), or a few times only (in the case of FF). For example, in the elevator domain,
where delete effects do not exist and FF constructs a relaxed planning graph only once, it is
extremely fast. On the contrary, in the gripper and the puzzle domains, where FF needs to
155

fiREFANIDIS & VLAHAVAS

reconstruct the relaxed planning graphs, its efficiency decreases drastically with respect to the
GRT's one.
HSP-2 was not faster than the other planners in any domain, being always outperformed by FF.
This was expected, since the two planners use the forward direction both for the construction of
their heuristics and for traversing the state-space, however FF constructs its heuristic less times than
HSP-2. Our impression is that the FF heuristic is also more informative and more accurate than the
one of HSP-2. Concerning ALTALT, although it constructs its heuristic once, it did not manage to be
faster than the others in any domain and this is (we believe) due to the problems that arise from the
backward direction in which it traverses the state-space. So, this is an indication that in the case
where opposite directions are used for the heuristic construction and the search phase, as GRT,
ALTALT and HSPr do, it is preferable to use the backward direction for the heuristic construction
and the forward direction for the search phase. This is why the problems that arise when
constructing the heuristic backwards may be confronted more easily than the problems that arise
when traversing the state-space backwards.
Domain analysis techniques, which occur in pre-processing phase, also play an important role.
STAN, which is primarily based on these techniques, had many variations in its performance. In
transportation domains, like the logistics and the elevator ones, where STAN exploits specialized
heuristics, it was among the fastest planners. In the gripper domain, where STAN exploits its
symmetry analysis, its performance was also excellent. In other domains, as for example the
freecell or the blocks, it was not competitive due to its GRAPHPLAN basic architecture, which is not
considered a fast technology any more.
FF also employed a domain analysis technique concerning goal ordering, which played an
important role in the blocks problems. It would be very interesting to see the adaptation and the
impact of this technique to other planners as well. As far as we know, HSP-2 and ALTALT are not
using any domain analysis technique. GRT exploited only the domain enrichment technique in the
elevator domain, however this technique is an integral part of its heuristic mechanism, in order to
overcome some problems that arise from the backward heuristic construction.
An interesting observation concerns the performance of GRT in the bigger problems of the
logistics, freecell, gripper and puzzle domains, where GRT exhibited better performance than in the
smaller problems of the same domains, compared to the other planners. We believe that this is due
to the fact that GRT constructs its heuristic once, while the repeated construction of the heuristics
for the other planners is an inhibitory factor in the bigger problems.
The conclusions drawn above ignore a significant factor, which is the specific implementation,
i.e. the approaches adopted by the various planners for "trivial" tasks, such as the computation of
all the ground facts and actions of a problem or the computation of the applicable actions to a given
state, the optimization of the code and of course potential "bugs". For example, in order to find the
applicable actions to a state, GRT uses constraint satisfaction techniques to progressively instantiate
the action schemas for each state, whereas most of the other planners exploit connectivity graphs
between the facts of a problem and the pre-instantiated actions. Our experiments with GRT have
shown that a significant portion of the processing time is spent in the determination of the
applicable actions to a state. This is the reason why we have developed a parallel version of GRT,
named PGRT (Vrakas et. al., 1999; 2000), which makes use of this observation and has been
proved very efficient in all domains. However, it is in our future plans to develop a connectivity
graph also in GRT and to compare it to the existent approach.
Differences that are due to the code optimization or potential "bugs" cannot be easily detected,
but we believe that all the planners, both the oldest and the newest ones are well-optimized
programs. In the future we would like to see theoretical comparisons between the computational

156

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

complexities of the various techniques and algorithms, apart from their experimental evaluation
that is usually adopted.
Concerning plan length, GRT produced better plans than the other planners in the freecell
domain, in the gripper domain (along with other planners), in many blocks problems when a 3action schemas representation was used and in some logistics problems. STAN exhibited the best
behavior in most of the domains and we believe this is due to its GRAPHPLAN basic architecture,
which always produces optimal parallel plans and, in many cases, sequential plans also. FF behaved
well in the logistics and the blocks problems, with the 4-action schemas representation (in the latter
case probably due to the goal ordering technique), however it produced lengthy plans in other
domains, as the freecell, the gripper and the puzzle ones.
HSP-2 produced longer plans than GRT in many domains, as for example the logistics, the
freecell and the gripper domains and the blocks one, when a 3-actions representation was used.
This observation means that in these domains the related facts employed by the GRT heuristic
proved more valuable than the forward and repeated reconstruction of the HSP-2 heuristic. Finally,
ALTALT has not been distinguished for the quality of its plans in any domain.
Our general impression from the experiments is that there are specific domains that favor
specific planners. So, what is important is to investigate the reasons for that. We are currently
working in exploring the internal characteristics of each domain, classifying them into more
general categories that share common features, and associate these features with specific heuristic
search techniques. A first attempt for a domain classification can also be found in (Hoffmann,
2001).
An alternative view of the above problem concerns the way a domain is encoded. The same
planner in the same domain may alter its performance when a different representation is adopted.
We faced this problem with the blocks-world, with the 4- and 3-actions schemas domain
representations, where the performance of GRT varies significantly, while the performance of other
planners is also altered. We also faced this problem with the elevator and movie domains, which
were the motivation for the development of the domain enrichment technique. Our conviction is
that domain-independent planning is strongly domain-representation dependent.
Concerning GRT, we plan to extend it so as to handle more expressive domains, supporting most
of the features of the PDDL language (types, quantifications, negations, disjunctions, etc). At this
time we are working with an extension of GRT, which has the ability to take into account multiple
criteria (i.e. solution time, resources, safety, profit etc.). We are also interested in incorporating
domain analysis techniques, as they have been developed in STAN and DISCOPLAN, in order to take
advantage of specialized methods for handling specific types of problems or sub-problems. Finally,
we will investigate the possibility and the utility of combining domain independent planning
techniques with domain dependent ones, without loosing the generality of the planning system.

Acknowledgments
The authors would like to thank Thomas Eiter, the editor in charge for this paper, and the
anonymous reviewers for their helpful comments. We would like also to thank Dimitris Vrakas for
his careful reading and suggestions in the final version of the paper. Finally, we would like to thank
the researchers of the planning community for making their planners available, and more
specifically Blai Bonet, Hector Geffner, Joerg Hoffman, Bernhard Nebel, Derek Long, Maria Fox,
Romeo Sanchez Nigenda, XuanLong Nguyen and Subbarao Kambhampati.

157

fiREFANIDIS & VLAHAVAS

References
Barrett, A. & Weld, D. S. (1994). Partial order planning: Evaluating possible efficiency gains.
Artificial Intelligence, 67, 71-112
Biundo, S. & Fox, M. (2000). Recent advances in AI planning. 5th European Conference on
Planning (ECP-99). Durham, UK, Springer-Verlag.
Blum, A. & Furst, M. (1997). Fast planning through planning graph analysis. In Proceedings of
IJCAI-95.
Blum, A. & Furst, M. (1995). Fast planning through planning graph analysis. Artificial
Intelligence, 90, 281-300.
Bonet, B. & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129 (1-2), pp.
5-33.
Bonet, B. & Geffner, H. (1999). Heuristic planning: New results. In (Biundo & Fox, 1999).
Bonet, B. & Geffner, H. (1998). HSP: Heuristic search planner. Entry at 4th International
Conference on Artificial Intelligence Planning Systems (AIPS) Planning Competition.
Pittsburgh, 1998.
Bonet, B., Loerincs, G. & Geffner, H. (1997). A robust and fast action selection mechanism for
planning. In Proceedings of AAAI-97.
Cheng, J. & Irani, K. B. (1989). Ordering problem subgoals. In Proceedings of IJCAI-89.
Drummond, M. & Currie, K. (1989). Goal ordering in partially ordered plans. In Proceedings of
IJCAI-89.
Fikes, R. E. & Nilsson, N. J. (1971). STRIPS: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2, 189-208.
Fox, M. & Long, D. (2001). Hybrid STAN: Identifying and managing combinatorial optimization
sub-problems in planning. In Proceedings of IJCAI-2001.
Fox, M. & Long, D. (2000). Using automatically inferred invariants in graph construction and
search. In Proceedings of AIPS-2000.
Fox, M. & Long, D. (1999). The detection and exploitation of symmetry in planning problems. In
Proceedings of IJCAI-99.
Fox, M. & Long, D. (1998). The automatic inference of state invariants in TIM. Journal of
Artificial Intelligence Research, 9, 367-421.
Gerevini, A. & Schubert, L. (2000b). Discovering state constraints in DISCOPLAN: Some new
results. In Proceedings of AAAI-00.
158

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

Gerevini, A. & Schubert, L. (2000a). Extending the types of state constraints discovered by
DISCOPLAN. In Proceedings of the AIPS-00 Workshop on Analysing and Exploiting
Domain Knowledge for Efficient Planning.
Gerevini, A. & Schubert, L. (1998). Inferring state constraints for domain-independent planning. In
Proceedings of AAAI-98.
Godefroid, P. & Kabanza, F. (1991). An efficient reactive planner for synthesizing reactive plans.
In Proceedings of AAAI-91.
Gupta, N. & Nau, D.S. (1992). On the complexity of blocks world planning. Artificial Intelligence
56 (2-3), 223-254.
Haslum, P. & Jonsson, P. (2000). Planning with reduced operator sets. In Proceedings of AIPS2000.
Hoffmann, J. & Nebel, B. (2001). The FF planning system: Fast plan generation through heuristic
search. Journal of Artificial Intelligence 14, 253-302.
Hoffmann, J. (2001). Local search topology in planning benchmarks: An empirical analysis. In
Proceedings of IJCAI-2001.
Kautz, H. & Selman, B. (1998). BLACKBOX: A new approach to the application of theorem
proving to problem solving. In AIPS-98 Workshop on Planning as Combinatorial Search.
Kautz, H. & Selman, B. (1996). Pushing the envelope: Planning, propositional logic and stochastic
search. In Proceedings of AAAI-96.
Kautz, H. & Selman, B. (1992). Planning as satisfiability. In Proceedings of ECAI-92.
Koehler, J. (1998). Solving complex planning tasks through extraction of subproblems. In
Proceedings of the 4th Intl. Conf. on Artificial Intelligence Planning Systems (AIPS-98).
Koehler, J. & Hoffmann, J. (2000). On reasonable and forced goals orderings and their use in an
agenda-driven planning algorithm. Journal of Artificial Intelligence Research, 12, 339-386.
Korf, R. & Taylor, L. (1996). Finding optimal solutions to the twenty-four puzzle. In Proceedings
of AAAI-96.
Long, D. & Fox, M. (2000). Automatic synthesis and use of generic types in planning. In
Proceedings of the 5th Intl. Conf. on AI Planning and Scheduling Systems (AIPS-00).
Long, D. & Fox, M. (1999). Efficient implementation of the plan graph in STAN. Journal of
Artificial Intelligence Research, 10, 87-115.
McAllester, D. & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proceedings of AAAI91.
McCluskey, T.L. & Porteous, J.M. (1997). Engineering and compiling planning domain models to
promote validity and efficiency. Artificial Intelligence, 95, 1-65.
159

fiREFANIDIS & VLAHAVAS

McDermott, D. (1999). Using regression-match graphs to control search in planning. Artificial
Intelligence, 109 (1-2), 111-159.
McDermott, D. (1996). A heuristic estimator for means-ends analysis in planning. In Proceedings
of the 3rd International Conference on Artificial Intelligence Planning Systems (AIPS-96).
Minton, S., Bresina, J. & Drummond, M. (1994). Total-order and partial-order planning: A
comparative analysis. Journal of Artificial Intelligence Research, 2, 227-261.
Nebel, B., Dimopoulos, Y. & Koehler, J. (1997). Ignoring irrelevant facts and operators in plan
generation. In Proceedings of the 4th European Conference on Planning (ECP-97).
Newell, A. & Simon, H. (1972). Human problem solving. Englewood Cliffs, NJ. Prentice-Hall.
Nigenda R.S., Nguyen, X. & Kambhampati, S. (2000). AltAlt: Combining the advantages of
graphplan and heuristic state search. Technical Report, Arizona State University.
Pearl, J. (1983). Heuristics. Morgan Kaufmann.
Pednault, E. (1989). ADL: Exploring the middle ground between STRIPS and the situation
calculus. In Proceedings of KR-89.
Penberthy, J. & Weld, D. (1992). UCPOP: A sound and complete, partial order planner for ADL.
In Proceedings of KR-92.
Refanidis, I. & Vlahavas, I. (2000b). Heuristic planning with resources. In Proceedings of ECAI2000.
Refanidis, I. & Vlahavas, I. (2000a). Exploiting state constraints in heuristic state-space planning.
In Proceedings of the 5th Intl. Conf. on Artificial Intelligence Planning and Scheduling
Systems (AIPS-00).
Refanidis, I. & Vlahavas, I. (1999c). On determining and completing incomplete states in STRIPS
Domains. In Proceedings of the IEEE Intl. Conf. on Information, Intelligence and Systems.
Refanidis, I. & Vlahavas, I. (1999b). GRT: A domain independent heuristic for STRIPS worlds
based on greedy regression tables. In (Biundo & Fox, 1999).
Refanidis, I. & Vlahavas, I. (1999a). SSPOP: A state-space partial-order planner. In Proceedings of
the 3rd World Multiconference on Systemics, Cybernetics and Informatics.
Scholz, U. (1999). Action constraints for planning. In (Biundo & Fox, 1999).
Slaney, J. & Thiebaux, S. (1996). Linear-time near-optimal planning in the blocks world. In
Proceedings of AAAI-96.
Smith, D. & Weld, D. (1999). Temporal graphplan with mutual exclusion reasoning. In
Proceedings of IJCAI-99.

160

fiBACKWARD HEURISTIC CONSTRUCTION IN FORWARD STATE-SPACE PLANNING

Veloso, M. (1992). Learning by analogical reasoning in general problem solving. Ph.D. thesis,
Department of Computer Science, Carnegie Mellon University.
Vrakas, D., Refanidis, I. & Vlahavas, I. (2000). An operator distribution method for parallel
planning. In AAAI-2000 Workshop on Parallel and Distributed Search for Reasoning.
Vrakas, D., Refanidis, I., Milcent, F. & Vlahavas, I. (1999). On parallelizing the greedy regression
tables. In Proceedings of the 18th Workshop of the UK Planning and Scheduling SIG.
Zhang, W. (1999). State-space search: Algorithms, complexity, extensions and applications.
Springer.

161

fi