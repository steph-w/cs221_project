Journal of Artificial Intelligence Research 56 (2016) 517-545

Submitted 4/16; published 7/16

Time-Sensitive Bayesian Information Aggregation for
Crowdsourcing Systems
Matteo Venanzi

mavena@microsoft.com

Microsoft, 2 Waterhouse Square,
London EC1N 2ST UK

John Guiver

joguiver@microsoft.com

Microsoft Research, 21 Station Road,
Cambridge CB1 2FB UK

Pushmeet Kohli

pkohli@microsoft.com

Microsoft Research, One Microsoft Way,
Redmond WA 98052-6399 US

Nicholas R. Jennings

n.jennings@imperial.ac.uk

Imperial College, South Kensington,
London SW7 2AZ UK

Abstract
Many aspects of the design of efficient crowdsourcing processes, such as defining workers
bonuses, fair prices and time limits of the tasks, involve knowledge of the likely duration
of the task at hand. In this work we introduce a new timesensitive Bayesian aggregation
method that simultaneously estimates a tasks duration and obtains reliable aggregations of
crowdsourced judgments. Our method, called BCCTime, uses latent variables to represent
the uncertainty about the workers completion time, the tasks duration and the workers
accuracy. To relate the quality of a judgment to the time a worker spends on a task,
our model assumes that each task is completed within a latent time window within which
all workers with a propensity to genuinely attempt the labelling task (i.e., no spammers)
are expected to submit their judgments. In contrast, workers with a lower propensity
to valid labelling, such as spammers, bots or lazy labellers, are assumed to perform tasks
considerably faster or slower than the time required by normal workers. Specifically, we use
efficient message-passing Bayesian inference to learn approximate posterior probabilities of
(i) the confusion matrix of each worker, (ii) the propensity to valid labelling of each worker,
(iii) the unbiased duration of each task and (iv) the true label of each task. Using two realworld public datasets for entity linking tasks, we show that BCCTime produces up to
11% more accurate classifications and up to 100% more informative estimates of a tasks
duration compared to stateoftheart methods.

1. Introduction
Crowdsourcing has emerged as an effective way to acquire large amounts of data that enables
the development of a variety of applications driven by machine learning, human computation and participatory sensing systems (Kamar, Hacker, & Horvitz, 2012; Bernstein, Little,
Miller, Hartmann, Ackerman, Karger, Crowell, & Panovich, 2010; Zilli, Parson, Merrett,
c
2016
AI Access Foundation. All rights reserved.

fiVenanzi, Guiver, Kohli & Jennings

& Rogers, 2014). Services such as Amazon Mechanical Turk1 (AMT), oDesk2 and CrowdFlower3 have enabled a number of applications to hire pools of human workers to provide
data to serve for training image annotation (Whitehill, Ruvolo, Wu, Bergsma, & Movellan,
2009; Welinder, Branson, Belongie, & Perona, 2010), galaxy classification4 (Kamar et al.,
2012) and information retrieval systems (Alonso, Rose, & Stewart, 2008). In such applications, a central problem is to deal with the diversity of accuracy and speed that workers
exhibit when performing crowdsourcing tasks. As a result, due to the uncertainty over the
reliability of individual crowd responses, many systems collect many judgments from different workers to achieve high confidence in the quality of their labels. However, this can incur
a high cost either in time or money, particularly when the workers are paid per judgment,
or when a delay in the completion of the entire crowdsourcing project is introduced when
workers intentionally delay their submissions to follow their own work schedule. For example, in a typical crowdsourcing scenario, a requester must specify the number of requested
assignments (i.e., individual responses from different workers), as well as the time limit for
the completion of each assignment. He must also set the price to be paid for each response5 ,
which usually includes a participation fee and a bonus based on the quality of the submission
and the actual effort required by the task. However, it is a nontrivial problem to set a time
limit that gives the workers sufficient time to perform the task correctly without leading to
task starvation (i.e., no one working on the task after being assigned). Generally speaking,
the knowledge of the actual duration of each assignment (task instance) is useful to the
requesters for various reasons. First, a tasks duration can be used as a proxy to estimate
its difficulty, as more difficult tasks usually take longer to complete (Faradani, Hartmann,
& Ipeirotis, 2011). Second, this information is useful to set the time limit of a task and
to reduce the overall time of task completion. Third, a task requestor can use the task
duration to pay fair bonuses to workers based on the difficulty of the task they complete.
When seeking to estimate this information, however, it is important to consider that some
workers might not perform a task immediately and they might delay their submissions after
accepting the task or, at the other extreme, they might submit a poor annotation in rapid
time (Kazai, 2011). As a result, common heuristic estimates of a tasks duration (such as
the workers average or median completion time) that do not account for such aspects are
likely to be inaccurate.
Given the above, there are a number of challenges to be addressed in the various steps
of designing efficient crowdsourcing workflows. First, after all the judgments have been
collected, the uncertainty about the unknown reliability of individual workers must be
taken into account to compute the final labels. Such aggregated labels are often estimated
in settings where the true answer of each task is never revealed, as this is the very quantity
that the crowdsourcing process is trying to discover (Kamar et al., 2012). Second, when
estimating a tasks duration, the uncertainty over the completion time deriving from the
private work schedule of a worker must be taken into account (Huff & Tingley, 2015).
1.
2.
3.
4.
5.

www.mturk.com
www.odesk.com
www.crowdflower.com
www.galaxyzoo.org
A common guideline for task requesters is to consider $0.10 per minute to be the minimum wage for
ethical crowdsourcing experiments (www.wearedynamo.org).

518

fiTime-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems

Third, these two challenges must be addressed simultaneously due to the interdependencies
between the workers reliability, the time required to complete each task, and the final labels
estimated for such tasks.
In an attempt to address these challenges, there has been growing interest in developing
algorithms and techniques to compute accurate labels while minimising the set of, possibly
unreliable, crowd judgements (Sheng, Provost, & Ipeirotis, 2008). In more detail, simple solutions typically use heuristic methods such as majority voting or weighted majority voting
(Tran-Thanh, Venanzi, Rogers, & Jennings, 2013). However, these methods do not consider
the reliability of different workers and they treat all judgments as equally reliable. More
sophisticated methods such as the onecoin model (Karger, Oh, & Shah, 2011), GLAD
(Whitehill et al., 2009), CUBAM (Welinder et al., 2010), DS (Dawid & Skene, 1979) and
the Bayesian Classifier Combination (BCC) (Kim & Ghahramani, 2012) use probabilistic
models that do take reliabilities into account, nor the potential labelling biases of the workers, e.g., the tendency for a worker to consistently over or underrate items. In particular
DS represents the workers skills based on a confusion matrix expressing the reliability of
a worker for each possible class of objects. BCC works similarly to DS, but it also considers the uncertainty over the confusion matrices and aggregated labels using a principled
Bayesian learning framework. This representational power has enabled BCC to be successfully applied to a number of crowdsourcing applications including galaxy classification
(Simpson, Roberts, Psorakis, & Smith, 2013), disaster response (Ramchurn, Huynh, Ikuno,
Flann, Wu, Moreau, Jennings, Fischer, Jiang, Rodden, et al., 2015) and sentiment analysis
(Simpson, Venanzi, Reece, Kohli, Guiver, Roberts, & Jennings, 2015). More recently, Venanzi, Guiver, Kazai, Kohli, and Shokouhi (2014) proposed a communitybased extension
of BCC (i.e., CBCC) to improve predictions by leveraging groups of workers with similar confusion matrices. Similarly, Simpson et al. combined BCC with language modelling
techniques for automated text sentiment analysis using crowd judgments. This degree of
applicability and performance of BCC-based methods are a promising point of departure
for developing new data aggregation methods for crowdsourcing systems. However, none of
the existing methods can reason about the workers completion time to learn the duration
of a task outsourced to the crowd. Moreover, all these methods can only learn their probabilistic models from the information contained in the judgment set. Unfortunately, this
strategy is challenged by datasets that can be arbitrarily sparse, i.e., the workers only provide judgments for a small sub-set of tasks, and therefore the judgments only provide weak
evidence of the accuracy of a worker. In such contexts, it is our hypothesis that a wider set
of features must be leveraged to learn more reliable crowdsourcing models. In this work, we
focus on the time it takes to a worker to complete a task considered as a key indicator of
the quality of his work. Importantly, the information about the workers completion time is
made available by all the most popular crowdsourcing platforms including AMT, the Microsoft Universal Human Relevance System (UHRS) and CrowdFlower. Therefore, we seek
to efficiently combine these features into a data aggregation algorithm that can be naturally
integrated with the output data produced by these platforms. In more detail, we present
a novel timesensitive data aggregation method that simultaneously estimates the tasks
duration and obtains reliable aggregations of crowdsourced judgments. The characteristic
of timesensitivity of our method relates to its ability to jointly reason about the workers
completion time together with the judgments in the data aggregation process. In detail,
519

fiVenanzi, Guiver, Kohli & Jennings

our method is an extension of BCC, which we term BCCTime. Specifically, it incorporates
a newly developed time model that enables the method to leverage observations of the time
spent by a worker on a task to best inform the inference of the final labels. As in BCC,
we use confusion matrices to represent the labelling accuracy of individual workers. To
model the granularity in the workers time profiles, we use latent variables to represent the
propensity of each worker to submit valid judgments. Further, to model the uncertainty of
the duration of each task, we use latent thresholds to define the time interval within which
the task is expected to be completed by all the workers with high propensity to valid labelling. Then, using Bayesian message-passing inference, our method simultaneously infers
the posterior probabilities of (i) the confusion matrix of each worker, (ii) the propensity
to valid labelling of each worker, (iii) the true label of each task and (iv) the upper and
lower bound of the duration of each task. In particular, the latter represents a reliable
estimate of the likely duration of a task obtained through automatically filtering out all the
contributions of the workers with a low propensity to valid labelling. We demonstrate the
efficacy of our method using two commonlyused public datasets that relate to an important Natural Language Processing (NLP) application of crowdsourcing entity linking tasks.
In these datasets, our method achieves up to 11% more accurate classifications compared
to seven state-of-the-art methods. Further, we show that our tasks duration estimates are
up to 100% more informative than the common heuristics that do not consider the workers
completion time as correlated to the quality of their judgments.
Against this background, we make the following contributions to the state of the art.
 Through an analysis on two real-world datasets for crowdsourcing entity-linking tasks,
we show the existence of different types of taskspecific qualitytime trends, e.g.,
increasing, decreasing or invariant trends, between the quality of the judgments and
the time spent by the workers to produce them. We also re-confirm existing results
showing that the workers who submit judgments too quickly or too slowly over the
entire task set typically provide lower quality judgments.
 We develop BCCTime: The first time-sensitive Bayesian aggregation model that leverages observations of a workers completion time to simultaneously aggregate crowd
judgments and infer the duration of each task as well as the reliability of each worker.
 We show that BCCTime outperforms seven of the most competitive stateoftheart
data aggregation methods for crowdsourcing, including BCC, CBCC, one coin and
majority voting, by providing up to 11% more accurate classifications and up to 100%
more informative estimates of the tasks duration.
The rest of the paper unfolds as follows. Section 2 describes our notation and the preliminaries of the Bayesian aggregation of crowd judgments. Section 3 details our time analysis
of real-world datasets. Then, Section 4 formally introduces BCCTime and details its probabilistic inference. Section 5 presents its evaluation against the state of the art. Section 6
summarises the rest of the related work in the areas of data aggregation and time analysis
of crowd generated content and Section 7 concludes.
520

fiTime-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems

Table 1: List of symbols.
Symbol
N
K
C
T
J
ti
t
(k)
ci
(k)
i
 (k)
p
k
s

vik
i
i
0
0
0
0
0
0
s0
p0
(k)
0

Definition
Number of tasks
Number of workers
Number of true label values
Set of observed workers completion time
Set of observed judgments
True label of the task i
Vector of all ti
Judgment of k for task i
Time spent by k for judging the task i
Confusion matrix of k
Class proportions of all the tasks
Propensity of k for making valid labelling attempts
Labelling probabilities of a general low-propensity worker
Vector of k , k = 1, . . . , K
(k)
Boolean variable signalling if ci is a valid labelling attempt
Lower-bound threshold of the duration of task i
Upper-bound threshold of the duration of task i
Mean for the Gaussian prior over i
Precision hyperparameter of the Gaussian prior over i
Mean hyperparameter of the Gaussian prior over i
Precision hyperparameter of the Gaussian prior over i
True count hyperparameter of the Beta prior over k
False count hyperparameter of the Beta prior over k
Hyperparameter of the Dirichlet prior over s
Hyperparameter of the Dirichlet prior over p
Hyperparameter of the Dirichlet prior over  (k)

2. Preliminaries
Consider a crowd of K workers labelling N objects into C possible classes  all our symbols
(k)
are listed in Table 1. Assume that k submits a judgment ci  {1, . . . , C} for classifying
(k)
an object i. Let ti be the unobserved true label of i. Then, suppose that i  R+ is
(k)
(k)
the time taken by k to produce ci . Let J = {ci |i = 1, . . . , N, k = 1, . . . , K} and
(k)
T = {i |i = 1, . . . , N, k = 1, . . . , K} be the set containing all the judgments and the
time spent by the workers, respectively.
We now introduce the key features of the BCC model that are relevant to our method.
First introduced by Kim and Ghahramani (2012), BCC is a method that combines multiple
judgments produced by independent classifiers (i.e., crowd workers) with unknown accuracy. Specifically, this model assumes that, for each task i, ti is drawn from a categorical
distribution with parameters p:

ti |p  Cat(ti |p)
521

(1)

fiVenanzi, Guiver, Kohli & Jennings

where p denotes the class proportions for all the objects. Then, a workers accuracy is
represented through a confusion matrix  (k) comprising the labelling probabilities of k for
(k)
(k)
(k)
each possible true label value. Specifically, each row of the matrix  c = {c,1 , . . . , c,C } is
(k)

the vector where c,j is the probability of k producing the judgment j for an object of class
c. Importantly, this confusion matrix expresses both the accuracy (diagonal values) and the
biases (off-diagonal values) of a worker. It can recognise workers who are particularly accurate (inaccurate) or have a bias for a specific class of objects. In fact, accurate (inaccurate)
workers are represented through high (low) probabilities on the diagonal of the confusion
matrix, whilst workers with a bias towards a particular class will have high probabilities on
the corresponding column of the matrix. For example, in the galaxy zoo domain in which
the workers classify images of celestial galaxies, the confusion matrices can detect workers
who have low accuracy in classifying spiral galaxies or those who systematically classify
every object as elliptical galaxies (Simpson et al., 2013).
To relate the workers confusion matrix to the quality of a judgment, BCC assumes that
(k)
ci is drawn from a categorical distribution with parameters corresponding to the ti -th row
of  (k) :
(k)

(k)

(k) 

ci | (k) , ti  Cat ci | ti

(2)
(k)

This is equivalent to having a categorical mixture model over ci with ti as the mixture
parameter and  c as the parameter of the c-th categorical component. Then, assuming that
the judgments are independent and identically distributed (i.i.d.), the joint likelihood can
be expressed as:
p(C, t|, p) =

N
Y

Cat(ti |p)

i=1

K
Y

(k)

(k) 

Cat ci | ti

k=1

Using conjugate Dirichlet prior distributions for the parameters p and  and applying
Bayes rule, the joint posterior distribution can be derived as:
p(, p|C, t) Dir(p|p0 )

N n
Y
Cat(ti |p)
i=1

K
Y

(k)

(k) 

Cat ci | ti

o
(k) (k)
Dir( ti | ti ,0 )

(3)

k=1

From this expression, it is possible to derive the predictive posterior distributions of each
unobserved (latent) variable using standard integration rules for Bayesian inference (Bishop,
2006). Unfortunately, the exact derivation of these posterior distributions is intractable for
BCC due to the non-conjugate form of the model (Kim & Ghahramani, 2012). However, it
has been shown that, particularly for BCC models, it is possible to compute efficient approximations of these distributions using standard techniques such as Gibbs sampling (Kim and
Ghahramani), variational Bayes (Simpson, 2014) and Expectation-Propagation (Venanzi
et al., 2014). Building on this, several extensions of BCC have been proposed for various
522

fiTime-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems

crowdsourcing domains (Venanzi et al., 2014; Simpson et al., 2015, 2013). In particular,
CBCC applies communitybased techniques to represent groups of workers with similar
confusion matrices in the classifier combination process (Venanzi et al.). This mechanism
enables the model to transfer learning of a workers reliability through the communities and
so improve the quality of the inference.
However, a drawback of all these BCC based models is that they do not learn the tasks
duration nor do they consider any extra features other than the workers judgments. As a
result, they perform the full learning of the confusion matrices and task labels using only the
judgments produced by the workers. But, as mentioned earlier, this strategy is challenged
by sparse datasets where each worker only labels a few tasks. This is the case, for instance,
in the Crowdflower dataset used in the 2013 CrowdScale Shared Task challenge6 where the
sentiment of 98,980 tweets was classified by 1,960 workers over five sentiment classes. In
this dataset, 30% of the workers judged only 15 tweets, i.e., 0.015% of the total samples,
and there is a long tail of workers with less than 3 judgments.

3. Analysis of Workers Time Spent on Judgments
Having discussed the basic concepts of non-time based data aggregation, we now turn to
the analysis of the relationship between the time that workers spend on the task and the
quality of the judgments they produce. In contrast to previous works in this area (Demartini, Difallah, & Cudre-Mauroux, 2012; Wang, Faridani, & Ipeirotis, 2011), we extend our
analysis of qualitytime responses to both specific task instances, as well as for the entire
task set. By so doing, we provide key insights to inform the design of our timesensitive
aggregation model. To this end, we consider two public datasets generated from a widely
used NLP application of crowdsourcing entity linking tasks.
3.1 The Datasets
ZenCrowd - India (ZC-IN): contains a set of links between the names of entities
extracted from news articles and uniform resource identifiers (URIs) describing the entity
in Freebase7 and DBpedia8 (Demartini et al., 2012). The dataset was collected using AMT,
with each worker being asked to classify whether a single URI was either irrelevant (0)
or relevant (1) to a single entity. It contains the timestamps of the acceptance and the
submission of each judgment. Moreover, gold standard labels were collected from expert
editors for all the tasks. No information was released regarding the restrictions on the
worker pool, although all workers are known to be living in India, and each worker was
paid $0.01 per judgment. A total of 11,205 judgements were collected from a small pool
of 25 workers, giving this dataset a moderately high number of judgements per worker, as
detailed in Table 2. In particular, Figure 1a shows that the vast majority of tasks receive
5 judgements, while Figure 1c shows a skewed distribution of gold labels, in which 78% of
links between entities and URIs were classified by workers as irrelevant (0). As such, it
is worth noting that any binary classifiers with a bias towards unrelated classification will
correctly classify the majority of tasks and thus receive a high accuracy. Therefore, as we
6. www.crowdscale.org/shared-task
7. www.freebase.com
8. www.dbpedia.org

523

fiVenanzi, Guiver, Kohli & Jennings

Table 2: Crowdsourcing datasets for entity linking tasks.
Dataset:

Judgements

Workers

Tasks

Labels

11205
12190
6000

25
74
110

2040
2040
300

2
2
5

ZC-IN
ZC-US
WS-AMT

Judgement
accuracy
0.678
0.770
0.704

1800

1000

1400

800

# tasks

# tasks

1200
1000
800
600

600
400

400

200

200
2

3

4

5

7

9

10

# judgements

14

15

19

0

20

2 3 4 5 6 7 8 9 101112131516192225
# judgements

(a) ZC - IN

(b) ZC - US

1600

100

1400

80

1200
1000

# tasks

# tasks

Judgements
per worker
448.200
164.730
54.545

1200

1600

0

Judgements
per task
5.493
5.975
20.000

800
600
400

60
40
20

200
0

0
0

Gold label

1

0

1

2

3

4

Gold label

(c) ZC

(d) WS - AMT

Figure 1: Histograms of the number of judgments per task for ZC-IN (a) and ZC-US (b) 
WS - AMT is not shown because the tasks received exactly 20 judgments  and the number
of tasks per gold label for ZC (c) and WS - AMT (d).

will detail in Section 5, it is important to select accuracy metrics that evaluate the classifier
across the whole spectrum of possible discriminant thresholds.
ZenCrowd - USA (ZC-US): This dataset was also provided by Demartini et al. (2012)
and contains judgements for the same set of tasks as ZC-IN, although the judgements were
collected from AMT workers in the US. The same payment of $0.01 per judgement was
used. However, a larger pool of 74 workers was involved, and as such a lower number of
judgements were collected from each worker, as shown in Table 2. Furthermore, Figure 1b
shows a similar distribution of judgements per task as the India dataset, although slightly
fewer tasks received 5 judgements, with most of the remaining tasks receiving 3-4 judgements
524

fiTime-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems

or 9-11 judgements. The judgement accuracy of the US dataset is higher than the India
dataset despite an identical crowdsourcing system and reward mechanism being used.
Weather Sentiment - AMT (WS-AMT): The Weather Sentiment dataset was provided by CrowdFlower for the 2013 Crowdsourcing at Scale shared task challenge.9 It
includes 300 tweets with 1,720 judgements from 461 workers and has been used in several
experimental evaluations of crowdsourcing models (Simpson et al., 2015; Venanzi et al.,
2014; Venanzi, Teacy, Rogers, & Jennings, 2015b). In detail, the workers were asked to
classify the sentiment of tweets with respect to the weather into the following categories:
negative (0), neutral (1), positive (2), tweet not related to weather (3) and cant tell (4). As
a result, this dataset pertains to a multi-class classification problem. However, the original
dataset used in the Share task challenge did not contain any time information about the
collected judgments. Therefore, a new dataset (WS-AMT), was recollected for the same
tasks as in the CrowdFlower shared task dataset using the AMT platform, acquiring exactly 20 judgements and recording the elapsed time for each judgment (Venanzi, Rogers,
& Jennings, 2015a). As a result, WS-AMT contains 6,000 judgements from 110 workers,
as shown in Table 2. No restrictions were placed on the worker pool and each worker was
paid $0.03 per judgement. Furthermore, Figure 1d shows that, as per the original dataset,
the most common gold label is unrelated, while only five tasks were assigned the gold label
cant tell.
3.2 Time Spent on Task versus Judgment Accuracy
We wish to analyse the distribution of the workers completion time and the judgments
accuracy. To do so, we focus on the two datasets, ZC-US and ZC-IN with binary labels. In
fact, the binary nature of these two datasets allow us to analyse accuracy at a higher level of
detail, i.e., in terms of precision and recall of the workers judgments and the time spent to
produce them. Specifically, Figure 2 shows the cumulative distribution of the precision and
the recall of the set of judgments selected by a specific time threshold (x-axis) with respect
to the gold standard labels. Here, the precision is the fraction of true positive classifications
over all the returned positive classifications (true positives + false positives) and the recall is
the number of true positive classifications divided by the number of all the positive samples.
Similarly to Demartini et al. (2012), we find that the accuracy is lower at the extremes of
the time distributions. In ZC-US, both the precision and recall are higher for the sub-set
of judgments that were produced in more than 80 seconds and less than 1500 seconds. In
ZC-IN, the precision and recall are higher for judgments produced in more than 80 seconds
and less than 600 seconds.
In addition, Figure 3 shows the distribution of the recall and execution time for a sample
set of six positive task instances (i.e., entities with positive gold standard labels) with at
least ten judgments. For example, Figure 3b shows the time distribution of the judgments
for the URI: freebase.com/united states associated to the entity American. In these
graphs, some samples have an increasing quality-time curve, i.e., workers spending more
time produce better judgments, (Figure 3a and Figure 3b). Other samples have a decreasing
quality-time curve, i.e., workers spending more time produce worse judgments (Figure 3c
and Figure 3d). Finally, the last two samples have an approximately constant quality-time
9. www.kaggle.com/c/crowdflower-weather-twitter

525

fi0.9

0.8

0.8

0.7

0.7

0.6

0.6

Recall

Precision

Venanzi, Guiver, Kohli & Jennings

0.5
0.4

0.5
0.4
0.3

0.3
0.2

0.2

0.1

0.1

0

5

80

620

320

0

1520

5

80

620

320

1520

Time spent(sec)

Time spent (sec)

(b) ZC - US

(a) ZC - US
0.5

0.7
0.6

0.4

Recall

Precision

0.5
0.3
0.2

0.4
0.3
0.2

0.1
0

0.1
5

85

350

Time spent (sec)

600

0

1400

5

85

350

Time spent (sec)

600

1400

(d) ZC - IN

(c) ZC - IN

Figure 2: Histograms of the precision and recall binned by the time spent by the US workers
(a, b) and Indian workers (c, d) in the ZenCrowd datasets.
curve, i.e., workers quality is invariant to the time spent (Figure 3e and Figure 3f). It can
also be seen that these trends naturally correlate to the difficulty of each task instance.
For instance, URI: freebase.com/m/03hkhgs linked to the entity Southern Avenue is
more difficult to judge than the URI: dbpedia.org/page/Switzerland linked to the entity
Switzerland. In fact, Southern Avenue is more ambiguous as an entity name, which
may lead the worker to open the URI and check its content to be able to issue a correct
judgment. Instead, the relevance for the second entity Switzerland can be judged more
easily through visual inspection of the URI. In addition, each task has a specific time interval
that includes the sub-set of judgments with the highest precision. For example, in ZC-IN,
the judgments with the highest precision for the URI: dbpedia.org/page/Switzerland
and the entity Switzerland were submitted between 5 sec. and 20 sec. (Figure 2d).
Instead, in ZC-US, the best judgments for the URI: dbpedia.org/page/European linked
to the entity European were submitted in the interval of 2 sec. and 16 sec. (Figure 2c).
As a result, it is clear that each task instance has specific qualitytime profile that relates
to the difficulty of labelling that instance.
To better analyse these trends, Figure 4 shows the Pearsons correlation coefficient ()
(i.e., a standard measure of the degree of linear correlation between two variables) for all the
13 entities with positive links and more than ten judgments across the two datasets. The
time spent by the worker is not always (linearly) correlated to the quality of the judgment
526

fi1

1

0.8

0.8

Recall

Recall

Time-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems

0.6
0.4
0.2
3

10

18

0

41

Time spent (sec)

11

23

48

Time spent (sec)

(a) Time-increasing task

(b) Time-increasing task

ZC - US
Entity: SouthernAvenue - Link:freebase.com/m/03hkhgs

ZC - IN
Entity: American - Link: freebase.com/united_states

1

1

0.8

0.8

0.6
0.4

0

0.6
0.4
0.2

0.2
2

12

Time spent (sec)

0

25

19

36

Execution time (sec)

45

(d) Time-decreasing task

ZC - US
Entity: European - Link: dbpedia.org/page/European

ZC - IN
Entity: Swiss - Link: dbpedia.org/page/Switzerland

1

1

0.8

0.8

0.6

0.6

0.4
0.2
0

5

(c) Time-decreasing task

Recall

Recall

0.4
0.2

Recall

Recall

0

0.6

0.4
0.2

2

8

14

Time spent (sec)

0

71

2

7

10

Time spent(sec)

20

60

(e) Time-decreasing task

(f) Time-constant task

ZC - US
Entity: Switzerland - Link: dbpedia.org/page/Switzerland

ZC - US
Entity: GMT - Link: dbpedia.org/page/Greenwich_Mean_Time

Figure 3: Histograms of the recall for six entity linking tasks with positive gold standard
labels and at least ten judgments in the ZenCrowd datasets. They show the different trends
of recall-time curves for various tasks.

across all the task instances. Some tasks have a significantly positive correlation (i.e., task
index = 6, 8, 13 with  > 0.7, p < 0.05), others have a significantly negative correlation
(i.e., task index = 9, 12 with  < 0.7, p < 0.05), whilst the other tasks have a less significant
correlation between the accuracy of their judgments and the time spent by the workers. This
confirms that different task instances have substantially different quality-time responses
based on the difficulty of each sample. Thus, this insight significantly extends the previous
findings reported by Demartini et al. (2013) in which such a qualitytime trend was only
observed across the entire task set. Moreover, it empirically supports the theory of several
existing data aggregation models (Kamar, Kapoor, & Horvitz, 2015; Whitehill et al., 2009;
Bachrach, Graepel, Minka, & Guiver, 2012) that make use of these taskspecific features to
527

fiVenanzi, Guiver, Kohli & Jennings

0.45

1
0.8

Strong positive
correlation

0.4

0.6

0.35
0.3

0.2

p value

Pearson's 

0.4
0
-0.2

0.2
0.15

-0.4

0.1

-0.6
-0.8
-1

0.25

Significant correlation

0.05

Strong negative
correlation

0

1 2 3 4 5 6 7 8 9 10 11 12 13
Task index
(a)

1 2 3 4 5 6 7 8 9 10 11 12 13
Task index
(b)

Figure 4: The Pearsons correlation coefficient (a) and the p-value (b) of the linear correlation between the workers completion time and the judgments accuracy for the 13 entity
linking tasks with positive gold standard labels and more than the judgments in the ZenCrowd datasets.
achieve more accurate classifications in a number of crowdsourcing applications concerning,
among others, galaxy classification (Kamar et al.), image labelling (Whitehill et al., 2009)
and problem solving (Bachrach et al., 2012).

4. The BCCTime Model
Based on the above results of the time analysis of workers judgments, we observed that
different types of qualitytime trends occur for specific task instances. However, the standard BCC, as well as all the other existing aggregation models that do not consider this
information, are unable to perform inference over the likely duration of a task. To rectify
this, there is a need to extend BCC to be able to include these trends in the aggregation
of crowd judgments. To this end, the model must be flexible enough to identify workers
who, in addition to having imperfect skills, may also not have the intention to make a valid
attempt to complete a task. This further increases the uncertainty about data reliability.
In this section, we describe our Bayesian Classifier Combination model with Time (BCCTime). In particular, we describe the three components of the model concerning (i) the
representation of the unknown workers propensity to valid labelling, (ii) the reliability of
workers judgments and (iii) the uncertainty in the workers completion time, followed by
the details of its probabilistic inference.
4.1 Modelling Workers Propensity To Valid Labelling
Given the uncertainty about the intention of a worker to submit valid judgments, we introduce the latent variable k  [0, 1] representing the propensity of k towards making
a valid labelling attempt for any given task. In this way, the model is able to naturally
explain the unreliability of a worker based not only on her imperfect skills but also on her
attitude towards approaching a task correctly. In particular, k close to one means that
528

fiTime-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems

the worker has a tendency to exert her best effort to provide valid judgments, even though
her judgments might be still noisy as a consequence of the imperfect skills she possesses.
In contrast, k close to zero means that the worker tends to not provide valid judgments
for her tasks, which means that she behaves similarly to a spammer. Specifically, only the
workers with high propensity to valid labelling will provide inputs that are meaningful to
the tasks true label and the tasks duration. To capture this, we define a per-judgment
(k)
(k)
boolean variable vi  {0, 1} with vi = 1 meaning that k has made a valid labelling
(k)
(k)
(k)
attempt when submitting ci and vi = 0 meaning that ci is an invalid annotation. In
this setting, the number of valid labelling attempts made by the worker derives from her
(k)
propensity to valid labelling. Thus, we can model this by assuming that vi is a random
draw from a Bernoulli distribution parametrised by k :
(k)

vi

 Bernoulli(k )

(4)

That is, workers with high propensity to valid labelling are more likely to make more valid
labelling attempts, whilst workers with low propensity are more likely to submit more spam
annotations.
4.2 Modelling Workers Judgments
Here we describe the part of the model concerned with the generative process of crowd
judgments from the confusion matrix and the propensity of the workers. Intuitively, only
those judgments associated with valid labelling attempts should be considered to estimate
the final labels. This means that each judgment may be generated from two different
processes depending on whether or not it comes from a valid labelling attempt. To capture
this in the generative model of BCCTime, a mixture model is used to switch between these
(k)
(k)
two cases conditioned on vi . For the first case of a valid labelling attempt, i.e., vi = 1,
the judgment is generated through the workers confusion matrix as per the standard BCC
(k)
model. Therefore, we assume that ci is generated for the same model described for BCC
(k)
(Eq. 2), including vi in the conditional variables. Formally:
(k)

(k)

ci | (k) , ti , vi

(k)

(k) 

= 1  Cat ci | ti

(5)
(k)

For the second case of a judgment produced from an invalid labelling attempt, i.e., vi = 0,
it is natural to assume that the judgment does not contribute to the estimation of the true
label. Formally, this assumption can be represented through general random vote model in
(k)
which ci is drawn from a categorical distribution with a vector parameter s:
(k)

(k)

ci |s, vi

(k) 
= 0  Cat ci |s

(6)

Here s is the vector of the labelling probabilities of a general worker with low propensity
to make valid labelling attempts. Notice that the equation above does not depend on ti ,
which means that all the judgments coming from invalid labelling attempts are treated as
noisy responses uncorrelated to ti .
529

fiVenanzi, Guiver, Kohli & Jennings

4.3 Modelling Workers Completion Time
As shown in Section 3, the duration of a task may be defined as the interval in which
workers are more likely to submit high-quality judgments. However, due to the dependency
of the duration on the tasks characteristics, the requirement is that such an interval must
be non-constant across all the tasks. To model this, we define a lower-bound threshold, i ,
and an upper-bound threshold, i , for the time interval representing the duration of i. Both
these pertask thresholds are latent variables that must be learnt at training time. Then,
the tasks with a lower or higher variability in their duration can be represented based on
the values of their time thresholds. In this setting, all the valid labelling attempts made by
the workers are expected to be completed within the tasks duration interval detailed by
(k)
these thresholds. Formally, we represent the probability of i being greater than i using
the standard greaterThan probabilistic factor introduced by Herbrich, Minka, and Graepel
(2007) for the TrueSkill Bayesian ranking model:
(k)

I(i

(k)

> i |vi

= 1)

(7)

This factor defines a non-conjugate relationship over i such that the posterior distribution
(k)
of i is not in the same form as the prior distribution of i . Therefore the posterior
(k)
distribution p(i ) needs to be approximated. We do this via moment matching with a
(k)
Gaussian distribution p(i ) by matching the precision and the precision adjusted mean
(k)
(i.e., the mean multiplied by the precision) to the posterior distribution of p(i ), as shown
(k)
in Table 1 in Herbrich et al. (2007). In a similar way, we model the probability of i
being greater than i as:
(k)

(k)

I(i > i |vi

= 1)

(8)

Drawing all this together, upon observing a set of i.i.d. pairs of judgments and workers
completion times contained in J and T respectively, we can express the joint likelihood of
BCCTime as:

p(J , T , t|, p, s, ) =

N
Y
i=1

Cat(ti |p)

Y
K 

(k)

I(i

(k)

(k)


(k)  k

> i )I(i > i )Cat ci | ti

k=1

(k) (1k )
Cat ci |s


(9)

The factor graph of BCCTime is illustrated in Figure 5. Specifically, the two shaded vari(k)
(k)
ables ci and i are the observed inputs, while all the unobserved random variables are
unshaded. The graph uses the gate notation (dashed box) introduced by Minka and Winn
(2008) to represent the two mixture models of BCCTime. Specifically, the outer gate represents the workers judgments (see Section 4.2) and completion times (see Section 4.3) that
(k)
are generated from either BCC or the random vote model using vi as the gating variable.
The inner gate is the mixture model generating the workers judgments from the rows of
the confusion matrix using ti as the gating variable.
530

fiTime-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems

Dir.

Dir.

p

s

K workers

Beta

Gaussian

Gaussian

i

Cat.

k

ti

i

Bernoulli

C true label values
Cat.

Dir.

(k)

greater

smaller

(k)

(k)

N tasks

vi

Cat.

(k)
c

ci

i

Figure 5: The factor graph of BCCTime.
4.4 Probabilistic Inference
To perform Bayesian inference over all the unknown quantities, we must provide prior
distributions for the latent parameters of BCCTime. Following the structure of the model,
we can select conjugate distributions for all such parameters to enable a more tractable
inference of their posterior probabilities. Therefore, the prior of p is Dirichlet distributed
with hyperparameter p0 :
(true label prior)
(k)

The priors of s and  c
respectively:

p  Dir(p|p0 )

(10)
(k)

are also Dirichlet distributed with hyperparameter s0 and  c,0

(spammer label prior)
(confusion matrix prior)

s  Dir(s|s0 )
 (k)
c 

(k)
Dir( (k)
c | c,0 )

(11)
(12)

Then, k has a Beta prior with true count 0 and false count 0 :
(workers propensity prior) k  Beta(k |0 , 0 )

(13)

The two time thresholds i and i have Gaussian priors with mean 0 and 0 and precision
0 and 0 respectively:
(lower-bound of the tasks duration threshold prior) i  N (i |0 , 0 )

(14)

(upper-bound of the tasks duration threshold prior) i  N (i |0 , 0 )

(15)

531

fiVenanzi, Guiver, Kohli & Jennings

Collecting all the hyperparameters in the set  = {p0 , s0 , 0 , 0 , 0 , 0 , 0 , 0 }, we find by
applying Bayes theorem that the joint posterior distribution is proportional to:

p(, p, s, t, |J , T , )  Dir(s|s0 )Dir(p|p0 )

N 
Y

Cat(ti |p)N (i |0 , 0 )N (i |0 , 0 )

i=1
K 
Y

(k)

I(i

(k)

(k)

(k) 

> i )I(i < i )Cat ci | ti

(k)

(k)

Dir( ti | ti ,0 )

k

k=1



Cat

(k) (1k )
ci |s
Beta(k |0 , 0 )

(16)

From this expression, we can compute the marginal posterior distributions of each latent
variable by integrating out all the remaining variables. Unfortunately, these integrations
are intractable due to the nonconjugate form of our model. However, we can still compute
approximations of such posterior distributions using standard techniques from the family of approximate Bayesian inference methods (Minka, 2001). In particular, we use the
well-known EP algorithm (Minka, 2001) that has been shown to provide good quality approximations for BCC models (Venanzi et al., 2014)10 . This method leverages a factorised
distribution of the joint probability to approximate the marginal posterior distributions
through an iterative message passing scheme implemented on the factor graph. Specifically,
we use the EP implementation provided by Infer.NET (Minka, Winn, Guiver, & Knowles,
2014), which is a standard framework for running Bayesian inference in probabilistic models.
Using Infer.NET, we are able to train BCCTime on our largest dataset of 12,190 judgments
within seconds using approximately 80MB of RAM on a standard laptop.

5. Experimental Evaluation
Having described our model, we test its performance in terms of classification accuracy and
ability to learn the tasks duration in real crowdsourcing experiments. Using the datasets
described in Section 3, we conduct experiments in the following experimental setup.
5.1 Benchmarks
We consider a set of benchmarks consisting of three popular baselines (Majority voting,
Vote distribution and Random) and three stateoftheart aggregation methods (One coin,
BCC and CBCC) that are commonly employed in crowdsourcing applications. In more
detail:
 One coin: This method represents the accuracy of a worker with a single reliability
parameter (or workers coin) assuming that the worker will return the correct answer
with probability specified by the coin, and the incorrect answer with inverse probability. As a result, this method is only applicable to binary datasets. Crucially, this
model represents the core mechanism of several existing methods including (Whitehill
10. Alternative inference methods such as Gibbs sampling or Variational Bayes can be trivially applied to
our model in the Infer.NET framework.

532

fiTime-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems

et al., 2009; Demartini et al., 2012; Liu, Peng, & Ihler, 2012; Karger et al., 2011; Li,
Zhao, & Fuxman, 2014)11 .
 BCC: This is the closest benchmark to our method that was described in Section 2.
It learns the confusion matrices and the aggregated labels without considering the
workers completion time as an input feature. It has been used in several crowdsourcing contexts including galaxy classification (Simpson et al., 2013), image annotation
(Kim & Ghahramani, 2012) and disaster response (Ramchurn et al., 2015).
 BCCPropensity: This is equivalent to BCCTime where only the workers propensity
is learnt. This benchmark is used to assess the contribution of inferring only the
workers propensity, versus their joint learning with the tasks time thresholds, to the
quality of the final labels. Note that BCCPropensity is easy to obtain from BCCTime
by setting the time thresholds to static observations with  = 0.0 and  = max.value.
 CBCC: An extension of BCC that learns the communities of workers with similar
confusion matrices as described in Section 2. Given a judgment set, CBCC is able to
learn the confusion matrix of each community and each worker, as well as the task
label. This method has also been used in a number of crowdsourcing applications
including web search evaluation and sentiment analysis (Venanzi et al., 2014). In our
experiments, we ran CBCC with the number of worker types set to two communities
in order to infer the two groups of more reliable workers and less reliable workers 
similar results were observed for higher number of communities.
 Majority Voting: This is a simple yet very popular algorithm that estimates the
aggregated label as the one that receives the most votes (Littlestone & Warmuth,
1989; Tran-Thanh et al., 2013). It assigns a point mass to the label with the highest
consensus among a set of judgments. Thus, the algorithm does not represent its
uncertainty around a classification and it considers all judgments as coming from
reliable workers.
 Vote Distribution: This method estimates the true label based on the empirical probabilities of each class observed in the judgment set (Simpson et al., 2015). Specifically,
it assigns the probability of a label as the fraction of judgments corresponding to that
label.
 Random: This is a baseline method that assigns random class labels to all the tasks,
i.e., it assigns uniform probabilities to all the labels.
Note that the alternative variant of BCCTime that captures only the time spent is redundant. In fact, when the workers propensity is not modelled together with the time spent,
the workers accuracy is only captured by their confusion matrices. This means that the
model is equivalent to BCC, which is already included in the benchmarks. All these benchmarks were also implemented in Infer.NET and trained using the EP algorithm. In our
11. In particular, we refer to One coin as the unconstrained version of ZenCrowd (Demartini et al., 2012)
without the two unicity and SameAs constraints defined in the original method. This suggests that this
version is more suitable for a fair comparison with the other methods.

533

fiVenanzi, Guiver, Kohli & Jennings

experiments, we set the hyperparameters of BCCTime to reproduce the typical situation in
which the task requester has no prior knowledge of the true labels and the labelling probabilities of the workers, and only a basic prior knowledge about the accuracy of workers
representing that, a priori, they are assumed to be better than random annotators (Kim
& Ghahramani, 2012). Therefore, the workers confusion matrices are initialised with a
slightly higher value on the diagonal (0.6) and lower values on the rest of the matrix. Then,
the Dirichlet priors for p and s are set uninformatively with uniform counts12 . The priors
of the confusion matrices were initialised with a higher diagonal value (0.7) meaning that a
priori the workers are assumed to be better than random. The Gaussian priors for the tasks
time durations are set with means 0 = 10 and 0 = 50 and precisions 0 = 0 = 101 ,
meaning that a priori each entity linking task is expected to be completed within 10 and 50
seconds. Furthermore, we initialise the Beta prior of k as a function of the number of tasks
with 0 = 0.7N and 0 = 0.3N to represent the fact that a priori the worker is considered
as a reliable if she makes valid labelling attempts for 70% of the tasks. Importantly, given
the shape distribution of the workers time completion data observed in the datasets (see
(k)
Figure 2), we apply a logarithmic transformation to i in order to obtain a more uniform
distribution of workers completion time in the training data. Finally, the priors of all the
benchmarks were set equivalently to BCCTime.
5.2 Accuracy Metrics
We evaluate the classification accuracy of the tested methods as measured by the Area
Under the ROC Curve (AUC) for ZC-US and ZC-IN and the average recall for WS-AMT.
In particular, the former is a standard accuracy metric to evaluate the performance of
binary classifiers over a range of discriminant thresholds applied to their predictive class
probabilities (Hanley & McNeil, 1982), which is well suited for the two ZenCrowd binary
datasets. The latter is the recall averaged over the class categories (Rosenberg, 2012),
which is the main metric used to score the probabilistic methods that competed in the 2013
CrowdFlower shared task challenge on a dataset equivalent to WS-AMT (see Section 3.1).
5.3 Results
Table 3 reports the AUC of the seven algorithms on the ZenCrowd datasets. Specifically, it
shows that BCCTime and BCCPropensity have the highest accuracy in both the datasets:
Their AUC is 11% higher in ZC-IN and 8% higher in ZC-US, respectively, compared to the
other methods. Among the two, BCCTime is the best method with an improvement of 13%
in ZC-IN and 1% in ZC-US. Similarly, Table 4 reports the average recall of the methods in
WS-AMT showing that BCCTime has the highest average recall, which is 2% higher than
the second best benchmark (Vote distribution) and 4% higher than BCCPropensity13 . This
means that the inference of the time thresholds, which already provides valuable information
about the tasks extracted from the judgments, also adds an extra quality improvement to
aggregated labels in addition to the modelling of the workers propensities. This is an
12. It should be noted that in cases where a different type of knowledge is available about the workers, this
information can be plugged into our method by selecting the appropriate prior distributions.
13. In this dataset, majority vote performs very similarly to BCCTime due to the higher quality of the
workers

534

fiTime-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems

Table 3: The AUC of the tested methods measured on the ZenCrowd datasets.
The highest AUC in each dataset is highlighted in bold.
Dataset:
Majority vote
Vote distribution
One coin
Random
BCC
CBCC
BCCPropensity
BCCTime

ZC-US
0.3820
0.2101
0.7204
0.5000
0.6418
0.6730
0.7740
0.7800

Table 4: The average recall of the tested
methods measured on WS-AMT. The
highest average recall is highlighted in
bold.

ZC-IN
0.3862
0.3080
0.6263
0.5000
0.5407
0.5544
0.6177
0.6925

Dataset:
Majority vote
Vote distribution
One coin
Random
BCC
CBCC
BCCPropensity
BCCTime

WS-AMT
0.727
0.728
N/A
0.183
0.705
0.711
0.703
0.730

important observation because it proves that the information of workers completion time
can be effectively for data aggregation. Altogether, this information allows the model to
correctly filter unreliable judgments and consequently provide more accurate classifications.
Figure 6 shows the ROC curve of the methods for the ZenCrowd (binary) datasets,
namely the plot of the false positive rate and the true positive rate obtained for different
discriminant thresholds. The graph shows that the true positive rate of BCCTime is generally higher than that of the benchmarks at the same false positive rate. In detail, Majority
vote, and Vote distribution perform worse than Random in these datasets as these methods
are clearly penalised by the presence of less reliable workers as they treat all the workers
as equally reliable. Interestingly, One coin performs better than BCC and CBCC meaning
that the confusion matrix is better approximated by a single (one coin) parameter for these
two datasets. Also, looking at the percentages of the workers propensities inferred by BCCTime reported in Table 5, we found that 93.2% of the workers in ZC-US, 60% of the workers
in ZC-IN and 97.3% of the workers in WS-AMT have a propensity greater than 0.5. This
means that, in ZC-US and WS-AMT, only a few workers were identified as suspected spammers while the majority of them were estimated as more reliable with different propensity
values. In ZC-IN, the percentage of suspected spammers is higher and this is also reflected
in the lower accuracy of the judgments with respect to the gold standard labels.
Figure 7 shows the mean value of the inferred upper-bound time threshold i (blue
cross points) and the workers maximum completion time (green asterisked points) for each
Table 5: The propensity of workers learnt from BCCTime in each dataset.
Dataset:
ZC-US
ZC-IN
WS-AMT

% high propensity
workers (p(k ) > 0.5)
93.2%
60%
97.3%

535

% low propensity
workers (p(k )  0.5)
6.8%
30%
2.7%

fiVenanzi, Guiver, Kohli & Jennings

Majority vote

Vote distribution

One coin

Random

BCC

BCCPropensity

CBCC

BCCTime

1

1
0.9

0.8

0.7

True positive rate

True positive rate

0.8
0.6
0.5
0.4
0.3

0.6
0.4
0.2

0.2
0.1
0

0

0.2

0.4

0.6

False positive rate

0.8

0

1

(a) ZC - US

0

0.2

0.4
0.6
False positive rate

0.8

1

(b) ZC - IN

Figure 6: The ROC curve of the aggregation methods for ZC-US (a) and ZC-IN (b).

task of the three datasets. Looking at the raw data in the ZenCrowd datasets, the average
maximum time spent by the US workers is higher (approx. 1.7 minutes) than that of the
Indian workers (approx. 1 minute). It can also be seen that in both datasets there is a
significant portion of outliers that reach up to 50 minutes. However, as discussed in Section
3, we know that many of these entity linking tasks are fairly simple  some of them can
easily be solved through visual inspection of the candidate URI. This does not imply that
a normal worker who completes the task in a single session (i.e., no interrupts) should
take such a long time to issue her judgment. Interestingly, BCCTime efficiently removes
these outliers and recovers more realistic estimates of the maximum duration of an entity
linking task. In fact, its estimated upper-bound time thresholds lie within a smaller time
band, i.e., around 10 seconds in ZC-US and 40 seconds in ZC-IN. Similar results are also
observed in WS-AMT where the average observed maximum time is significantly higher
than the average inferred maximum time, thus suggesting that the BCCTime estimates
are also more realistic in this dataset. In addition, Figure 7 shows the same plot for the
average duration as estimated by BCCTime (i.e, (E[i ]  E[i ])/2 i) and the average
workers completion time for each task. The graphs show that the BCCTime estimates are
similar between the micro-tasks of the three datasets, i.e., between 3 and 5, while the same
estimates obtained from the workers completion time data are much higher: 53 seconds
for ZC-US, 45 seconds for ZC-IN and 80 seconds in WS-AMT. Again, this is due to the
presence of outliers in the original data that significantly bias the empirical average times
towards high values. Moreover, measuring the variability in the two sets of estimates, the
BCCTime estimates have a much smaller standard deviation that is up to 100% lower than
that of the empirical averages. This means that our estimates are more informative when
compared to the normal average times obtained from the raw workers completion time
data.
536

fiTime-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems

10

104
Inferred max. time
Observed max. time

3

Time spent (sec)

Time spent (sec)

104

102
101
10

500

1000 1500
Task index

10

2

100

0

0

Inferred avg. time
Observed avg. time

2000

0

500

(a) ZC - US

Time spent (sec)

Time spent (sec)

10
Inferred max. time
Observed max. time

3

102

10

4
Inferred avg. time
Observed avg. time

102

100

1

0

500

1000 1500
Task index

2000

0

500

1000 1500
Task index

4

10

4
Inferred avg. time
Observed avg. time

Time spent (sec)

Time spent (sec)

Inferred max. time
Observed max. time

103

102

10

2000

(d) ZC - IN

(c) ZC - IN

10

2000

(b) ZC - US

104

10

1000 1500
Task index

102

100

1

0

100

200

300

Task index

0

100

200

300

Task index

(e) WS - AMT

(f) WS - AMT

Figure 7: The plot of the inferred (+) and observed (*) maximum time spent on the tasks
in ZC-US (a), ZC-IN (c) and WS-AMT (e), and the average time spent on the tasks in
ZC-US (b), ZC-IN (d) and WS-AMT (f).

537

fiVenanzi, Guiver, Kohli & Jennings

0

0

10

10

1

AUC

AUC

10

2

10
1

10

3

0

2000

4000

6000

8000

10

10000

0

2000

4000

6000

Num. of judgments

Num. of judgments

(a) ZC  US

(b) ZC  IN

0.8

8000

Majority vote

Average recall

Vote distribution
0.6

BCC
One coin

0.4

CBCC
BCCPropensity

0.2

BCCTime
0

Random
1000 2000 3000 4000 5000 6000
Num. of judgments
(c) WS  AMT

Figure 8: The AUC in ZC-US (a) and ZC-IN (b) and the average recall in WS-AMT (c) of
the methods trained over increasingly large sub-sets of judgments.

To evaluate the performance of the methods against data sparsity, Figure 8 shows the
accuracy measured over sub-samples of judgments in each dataset. In more detail, one coin
is more accurate over sparse judgments in ZC-IN and ZC-US, while in WS-AMT there is no
clear winner since all the methods except Random have a similar average recall when trained
on sparse judgments. This shows that BCCTime in the current form does not necessarily
outperform the other methods with sparse data. This can be explained by the fact that
the extra latent variables (i.e., workers propensity and time thresholds) used to improve
the quality of the final labels also require a larger set of judgments to be accurately learnt.
However, to address this issue, it is possible to draw from community-based models (e.g.,
CBCC) to design a hierarchical extension for BCCTime over, for example, the workers
confusion matrices and so improve its robustness on sparse data. Here, for simplicity, BCCTime is presented based on simpler instance of Bayesian classifier combination framework
(i.e., the BCC model), and its community-based version is considered as a trivial extension.
538

fiTime-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems

Table 6: Comparison of 21 existing methods for computing aggregated labels from crowdsourced judgments classified according to their classification models (binary class and multiclass) and learning features (worker accuracy, worker confusion matrix, task difficulty, task
duration and workers type).

Majority voting
DS - Dawid & Skene (1979)
GLAD - Whitehill et al. (2009)
RY - Raykar et al. (2010)
CUBAM - Welinder et al. (2010)
YU - Yan et al. (2010)
LDA - Wang et al. (2011)
KJ - Kajino et al. (2012)
ZenCrowd - Demartini et al. (2012)
DARE - Bachrach et al. (2012)
MinMaxEntropy - Zhou et al. (2012)
BCC - Kim & Ghahramani (2012)
MSS - Qi et al. (2013)
MLNB - Bragg et al. (2013)
BM - Bi et al. (2014)
GP - Rodriguez et al. (2014)
LU - Liu et al. (2014)
WM -Li et al. (2014)
CBCC - Venanzi et al. (2014)
APM - Nushi et al. (2015)
BCCTime - Proposed method

binary
class
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

multi
class
X
X
X
X
X
X
X
X
X
X
X
X

worker
acc.
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

worker
CF
X
X
X
X
X

task
diff.
X
X
X
X

task
duration
X
X

worker
type
X
X
X
X
X
X

6. Related Work
Here we review the rest of previous work relating to aggregation models and time analysis
in crowdsourcing contexts extending the background of the methods already considered in
our experimental evaluation. In recent years, a large body of literature has focussed on the
development of smart data aggregation methods to aid requesters in combining judgments
from multiple workers. In general, existing methods vary by assumptions and complexity
in modelling the different aspects of labelling noise. The interested reader may refer to the
survey by Sheshadri and Lease (2013), as well as to the summary in Table 6 that lists the
most popular methods and their comparison with our approach.
In particular, some of these methods are able to handle both binary classification problems, i.e., when workers have to vote on objects between two possible classes, and multi-class
classification problems, i.e., when workers have to vote on objects between more than two
classes. Among these, many approaches use the one coin model introduced in our benchmarks. In more detail, this model represents the workers reliability with a single parameter
defined within the range of [0, 1] (0 = unreliable worker, 1 = reliable worker) (Karger et al.,
2011; Liu et al., 2012; Demartini et al., 2012; Li et al., 2014; Nushi, Singla, Gruenheid, Zamanian, Krause, & Kossmann, 2015). Specifically, Karger et al. combines this model with
a budgetlimited task allocation framework and provides strong theoretical guarantees on
539

fiVenanzi, Guiver, Kohli & Jennings

the asymptotical optimality of the inference of the workers reliability and the worker-task
matching. Liu et al. uses a more general variational inference model that reduces to Karger
et al.s method, as well as other algorithms under special conditions. Other methods use
a two coin model that represents the bias of a worker towards the positive labelling class
(specificity) and towards the negative class (sensitivity) (Raykar, Yu, Zhao, Valadez, Florin,
Bogoni, & Moy, 2010; Rodrigues, Pereira, & Ribeiro, 2014; Bragg, Mausam, & Weld, 2013).
Then, these quantities may be inferred using logistic regression as in the work by Raykar
et al. or maximumaposteriori approaches as in the work by Bragg et al. Alternatively,
Rodrigues et al. uses the two coin model embedded in a Gaussian process classification
framework to compute the predictive probabilities of the aggregated labels and the workers
reliability using EP. Along the same lines, other models reason about the difficulty of a task
that affects the quality of a judgment to improve the reliability of aggregated labels (Whitehill et al., 2009; Bachrach et al., 2012; Kajino & Kashima, 2012). In this area, Whitehill
et al. use a logistic regression model to incorporate the tasks difficulty, together with the
expertise of the worker for labelling images. In contrast, Bachrach et al. use the difference
between these two quantities to quantify the advantage that the worker may have in classifying the object within a joint difficulty-ability-response model. In a similar setting, Kajino
and Kashima exploit a convex problem formulation of this model to improve the efficiency
of inferring these quantities through a numerical optimisation method. Additional factors,
such as the workers motivation or propensity for a particular task, are taken into account
in more sophisticated models (Welinder et al., 2010; Yan, Rosales, Fung, Schmidt, Valadez,
Bogoni, Moy, & Dy, 2010; Bi, Wang, Kwok, & Tu, 2014). More recently, Nushi et al. (2015)
devised a method that leverage the fact that the error rates of the workers are directly affected by the access path they follow, where the access path represents several contextual
features of the task (e.g., task design, information sources and task composition). However,
unlike our work, none of these methods learn the confusion matrix of each worker. As a
result, they do not represent reliability considering the accuracy and the potential biases of
a worker with a single data structure.
Alternative models that do learn the confusion matrices of the workers have been presented, among others, in the works by Dawid and Skene (1979), Zhou, Basu, Mao, and Platt
(2012), Kim and Ghahramani (2012) and Venanzi et al. (2014). In particular, Dawid and
Skene introduced the first confusion matrix-based model in which the confusion matrices
are inferred using expectation-maximisation in an unsupervised manner. Then, Zhou et al.
extended this work to include a taskspecific latent matrix representing the confusability of
a task as perceived by the workers. However, neither of these methods consider the uncertainty over the workers reliability and the other parameters of their models. For example,
when only one label is obtained from a worker, these methods may infer that the worker
is perfectly reliable or totally incompetent when, in reality, the worker is neither. To overcome this limitation, other methods such as BCC and CBCC capture the uncertainty in
the workers expertise and the true labels using a Bayesian learning framework. These two
methods were extensively discussed earlier (see Sections 2 and 5) and are included as benchmarks in our experiments. Similarly to CBCC, other methods leverage groups of workers
with equivalent reliability to improve the quality of the aggregated labels with limited data
(Li et al., 2014; Bi et al., 2014; Kajino & Kashima, 2012; Yan et al., 2010). However, as
already noted, all these methods do not use any extra information other than the workers
540

fiTime-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems

judgments to learn their probabilistic models. As a result, unlike our approach, they cannot take full advantage of the time information provided by the crowdsourcing platform to
improve the quality of their inference results.
Now we turn to the problem of time analysis in crowd generated content. Recently
introduced a metric for measuring the effort required to complete a crowdsourced task
based on the area under the error-time curve (ETA). As such, this metric supports the
idea of considering time as an important factor a crowdsourcing effort. In this regard, a
closely related work on the analysis of the ZenCrowd datasets (see Section 3) was presented
in the work by Difallah, Demartini, and Cudre-Mauroux (2012). Their work showed that
workers who complete their tasks too fast or too slow are typically less accurate than the
others. These findings were also confirmed in our work. However, in addition, we extended
their analysis by showing the judgments quality is correlated to the time spent by the
workers in different ways for specific task instances. This is the intuition that our method
exploits to efficiently combine the workers completion time features in the data aggregation
process. Furthermore, earlier work introducing a method that predicts the duration of the
task based on a number of available features (including the tasks price, the creation time
and the number of assignments) using a survival analysis model was presented in the paper
by Wang et al. (2011). However, their method does not deal with aggregating labels, nor
learning the accuracy of the workers, as we do in our approach.

7. Conclusions
We presented and evaluated BCCTime, a new timesensitive aggregation method that simultaneously merges crowd labels and estimates the duration of individual task instances
using principled Bayesian inference. The key innovation of our method is to leverage an extended set of features comprising the workers completion time and the judgment set. When
appropriately correlated together, these features become important indicators of the reliability of a worker that, in turn, allow us to estimate the final labels, the tasks duration and
the workers reliability more accurately. Specifically, we introduced a new representation
of the accuracy profile of a worker consisting of both the workers confusion matrix, which
accounts for the workers labelling probabilities in each class, and the workers propensity
to valid labelling, which represents the workers intention to meaningfully participate in the
labelling process. Furthermore, we used latent variables to represent the duration of each
task using pairs of latent thresholds to capture the time interval in which the best judgments for that task are likely to be submitted by honest workers. In this way, the model can
deal with the differences in the time length of each task instance relating to the different
type of correlation between quality of the received judgments and the time spent by the
workers. In fact, such taskspecific correlations have been observed in our experimental
analysis of crowdsourced datasets in which various task instances showed different types of
qualitytime trends. Thus, the main idea behind BCCTime is to model these trends in the
aggregation of crowd judgments to make more reliable inference about all the quantities of
interest. Through an extensive experimental validation on real-world datasets, we showed
that BCCTime produces significantly more accurate classifications and its estimates of the
tasks duration are considerably more informative than common heuristics obtained from
the raw workers completion time data.
541

fiVenanzi, Guiver, Kohli & Jennings

Against this background, there are several implications of this work concerning various
aspects of reliable crowdsourcing systems. Firstly, the process of designing the task can
take exploit the unbiased tasks duration estimated by BCCTime. As we have shown, this
information is a valid proxy to assess the difficulty of a task and therefore supports a number
of decisionmaking problems such as fair pricing for more difficult tasks and defining fair
bonuses to honest workers. Secondly, the workers propensity to valid labelling uncovers
an additional dimension of the workers reliability that enables us to score their attitude
towards correctly approaching a given task. This information is useful to select different task
designs or more engaging tasks for workers who systematically approach a task incorrectly.
Thirdly, our method uses only features that are readily available in common crowdsourcing
systems, which allows for a faster take up of this technology in real applications.
Building on these advances, there are several aspects of our current model that indicate
promising directions for further improvements. For example, we can consider that time
dependencies in the accuracy profile of a worker capture the fact that workers typically
improve their skills over time by performing a sequence of tasks. By so doing, it is possible
to take advantage of these temporal dynamics to potentially improve the quality of the final
labels. In addition, some crowdsourcing settings involve continuous-valued judgments that
are currently not supported by our method. To deal with these cases, a number of non
trivial extensions to our generative model and, in turn, a new treatment of its probabilistic
inference are required.

8. Acknowledgments
The authors gratefully acknowledge all the funding bodies, Microsoft and the UK Research
Council for the ORCHID project, grant EP/I011587/1ORCHID, and Bhaskar Mitra (Microsoft) for proofreading this manuscript.

References
Alonso, O., Rose, D. E., & Stewart, B. (2008). Crowdsourcing for relevance evaluation. In
ACM SigIR Forum, Vol. 42, pp. 915, New York, NY, USA. ACM.
Bachrach, Y., Graepel, T., Minka, T., & Guiver, J. (2012). How to grade a test without
knowing the answersa Bayesian graphical model for adaptive crowdsourcing and
aptitude testing. In Proceedings of the 29th International Conference on Machine
Learning (ICML), pp. 11831190.
Bernstein, M., Little, G., Miller, R., Hartmann, B., Ackerman, M., Karger, D., Crowell, D.,
& Panovich, K. (2010). Soylent: a word processor with a crowd inside. In Proceedings
of the 23nd annual ACM symposium on User interface software and technology, pp.
313322. ACM.
Bi, W., Wang, L., Kwok, J. T., & Tu, Z. (2014). Learning to predict from crowdsourced
data. In Proceedings of the 30th International Conference on Uncertainty in Artificial
Intelligence (UAI).
Bishop, C. (2006). Pattern recognition and machine learning, Vol. 4. Springer New York.
542

fiTime-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems

Bragg, J., Mausam, & Weld, D. (2013). Crowdsourcing multi-label classification for taxonomy creation. In First AAAI Conference on Human Computation and Crowdsourcing,
pp. 2533.
Dawid, A., & Skene, A. (1979). Maximum likelihood estimation of observer error-rates using
the em algorithm. Applied statistics, 2028.
Demartini, G., Difallah, D. E., & Cudre-Mauroux, P. (2012). Zencrowd: Leveraging probabilistic reasoning and crowdsourcing techniques for large-scale entity linking. In
Proceedings of the 21st international conference on World Wide Web (WWW), pp.
469478.
Difallah, D. E., Demartini, G., & Cudre-Mauroux, P. (2012). Mechanical cheat: Spamming
schemes and adversarial techniques on crowdsourcing platforms. In CrowdSearch, pp.
2630.
Faradani, S., Hartmann, B., & Ipeirotis, P. G. (2011). Whats the right price? pricing tasks
for finishing on time. In Human Computation, Vol. WS-11-11 of AAAI Workshops,
pp. 2631. AAAI.
Hanley, J. A., & McNeil, B. J. (1982). The meaning and use of the area under a receiver
operating characteristic (roc) curve. Radiology, 143 (1), 2936.
Herbrich, R., Minka, T., & Graepel, T. (2007). Trueskill(tm): A Bayesian skill rating
system. In Advances in Neural Information Processing Systems (NIPS), pp. 569576.
MIT Press.
Huff, C., & Tingley, D. (2015). Who are these people? evaluating the demographic characteristics and political preferences of mturk survey respondents. Research & Politics,
2 (3), 2053168015604648.
Kajino, H., & Kashima, H. (2012). Convex formulations of learning from crowds. Transactions of the Japanese Society for Artificial Intelligence, 27, 133142.
Kamar, E., Hacker, S., & Horvitz, E. (2012). Combining human and machine intelligence
in large-scale crowdsourcing. In Proceedings of the 11th International Conference on
Autonomous Agents and Multiagent Systems (AAMAS), pp. 467474.
Kamar, E., Kapoor, A., & Horvitz, E. (2015). Identifying and accounting for task-dependent
bias in crowdsourcing. In Third AAAI Conference on Human Computation and
Crowdsourcing.
Karger, D., Oh, S., & Shah, D. (2011). Iterative learning for reliable crowdsourcing systems.
In Advances in Neural Information Processing Systems (NIPS), pp. 19531961. MIT
Press.
Kazai, G. (2011). In search of quality in crowdsourcing for search engine evaluation. In
Advances in information retrieval, pp. 165176. Springer.
Kim, H., & Ghahramani, Z. (2012). Bayesian classifier combination. In International
Conference on Artificial Intelligence and Statistics, pp. 619627.
Li, H., Zhao, B., & Fuxman, A. (2014). The wisdom of minority: discovering and targeting
the right group of workers for crowdsourcing. In Proceedings of the 23rd International
Conference on World Wide Web (WWW), pp. 165176.
543

fiVenanzi, Guiver, Kohli & Jennings

Littlestone, N., & Warmuth, M. K. (1989). The weighted majority algorithm. In 30th
Annual Symposium on Foundations of Computer Science, pp. 256261. IEEE.
Liu, Q., Peng, J., & Ihler, A. (2012). Variational inference for crowdsourcing. In Advances
in Neural Information Processing Systems (NIPS), pp. 692700. MIT Press.
Minka, T. (2001). Expectation propagation for approximate Bayesian inference. In Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence (UAI), pp.
362369.
Minka, T., & Winn, J. (2008). Gates. In Advances in Neural Information Processing Systems
(NIPS), pp. 10731080. MIT Press.
Minka, T., Winn, J., Guiver, J., & Knowles, D. (2014). Infer.NET 2.6. Microsoft Research
Cambridge.
Minka, T. P. (2001). A family of algorithms for approximate Bayesian inference. Ph.D.
thesis, Massachusetts Institute of Technology.
Nushi, B., Singla, A., Gruenheid, A., Zamanian, E., Krause, A., & Kossmann, D. (2015).
Crowd access path optimization: Diversity matters. In Third AAAI Conference on
Human Computation and Crowdsourcing, pp. 130139.
Ramchurn, S. D., Huynh, T. D., Ikuno, Y., Flann, J., Wu, F., Moreau, L., Jennings, N. R.,
Fischer, J. E., Jiang, W., Rodden, T., et al. (2015). Hac-er: a disaster response system
based on human-agent collectives. In 2015 International Conference on Autonomous
Agents and Multiagent Systems, pp. 533541.
Raykar, V., Yu, S., Zhao, L., Valadez, G., Florin, C., Bogoni, L., & Moy, L. (2010). Learning
from crowds. The Journal of Machine Learning Research, 11, 12971322.
Rodrigues, F., Pereira, F., & Ribeiro, B. (2014). Gaussian process classification and active
learning with multiple annotators. In Proceedings of the 31st International Conference
on Machine Learning (ICML), pp. 433441.
Rosenberg, A. (2012). Classifying skewed data: Importance weighting to optimize average
recall. In INTERSPEECH, pp. 22422245.
Sheng, V., Provost, F., & Ipeirotis, P. (2008). Get another label? Improving data quality and
data mining using multiple, noisy labelers. In Proceedings of the 14th International
Conference on Knowledge Discovery and Data Mining (SIGKDD), pp. 614622. ACM.
Sheshadri, A., & Lease, M. (2013). Square: A benchmark for research on computing crowd
consensus. In Proceedings of the 1st AAAI Conference on Human Computation and
Crowdsourcing (HCOMP), pp. 156164.
Simpson, E., Roberts, S., Psorakis, I., & Smith, A. (2013). Dynamic bayesian combination
of multiple imperfect classifiers. In Decision Making and Imperfection, pp. 135.
Springer.
Simpson, E., Venanzi, M., Reece, S., Kohli, P., Guiver, J., Roberts, S., & Jennings, N. R.
(2015). Language understanding in the wild: Combining crowdsourcing and machine
learning. In 24th International World Wide Web Conference (WWW), pp. 9921002.
ACM.
544

fiTime-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems

Simpson, E. (2014). Combined Decision Making with Multiple Agents. Ph.D. thesis, University of Oxford.
Tran-Thanh, L., Venanzi, M., Rogers, A., & Jennings, N. R. (2013). Efficient Budget Allocation with Accuracy Guarantees for Crowdsourcing Classification Tasks. In The
12th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pp. 901908.
Venanzi, M., Guiver, J., Kazai, G., Kohli, P., & Shokouhi, M. (2014). Community-based
bayesian aggregation models for crowdsourcing. In 23rd International Conference on
World Wide Web (WWW), pp. 155164. ACM.
Venanzi, M., Rogers, A., & Jennings, N. R. (2015a). Weather Sentiment - Amazon Mechanical Turk dataset. University of Southampton.
Venanzi, M., Teacy, W., Rogers, A., & Jennings, N. R. (2015b). Bayesian modelling of
community-based multidimensional trust in participatory sensing under data sparsity.
In Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI15), pp. 717724.
Wang, J., Faridani, S., & Ipeirotis, P. (2011). Estimating the completion time of crowdsourced tasks using survival analysis models. In Crowdsourcing for Search and Data
Mining (CSDM), Vol. 31, pp. 3134.
Welinder, P., Branson, S., Belongie, S., & Perona, P. (2010). The multidimensional wisdom
of crowds. In Advances in Neural Information Processing Systems (NIPS), Vol. 10,
pp. 24242432. MIT Press.
Whitehill, J., Ruvolo, P., Wu, T., Bergsma, J., & Movellan, J. R. (2009). Whose vote
should count more: Optimal integration of labels from labelers of unknown expertise.
In Advances in Neural Information Processing Systems (NIPS), Vol. 22, pp. 2035
2043. MIT Press.
Yan, Y., Rosales, R., Fung, G., Schmidt, M., Valadez, G. H., Bogoni, L., Moy, L., & Dy,
J. (2010). Modeling annotator expertise: Learning when everybody knows a bit of
something. In International Conference on Artificial Intelligence and Statistics, pp.
932939.
Zhou, D., Basu, S., Mao, Y., & Platt, J. (2012). Learning from the wisdom of crowds by
minimax entropy. In Advances in Neural Information Processing Systems (NIPS), pp.
21952203. MIT Press.
Zilli, D., Parson, O., Merrett, G. V., & Rogers, A. (2014). A hidden markov model-based
acoustic cicada detector for crowdsourced smartphone biodiversity monitoring. Journal of Artificial Intelligence Research, 805827.

545

fiJournal of Artificial Intelligence Research 56 (2016) 613-656

Submitted 03/16; published 08/16

Datalog  Ontology Consolidation
Cristhian Ariel D. Deagustini
Mara Vanina Martnez
Marcelo A. Falappa
Guillermo R. Simari

cadd@cs.uns.edu.ar
mvm@cs.uns.edu.ar
mfalappa@cs.uns.edu.ar
grs@cs.uns.edu.ar

AI R&D Lab., Institute for Computer Science and Engineering (ICIC)
Consejo Nacional de Investigaciones Cientficas y Tecnicas (CONICET)
Universidad Nacional del Sur (UNS), Alem 1253,
(B8000CPB) Baha Blanca, Argentina.

Abstract
Knowledge bases in the form of ontologies are receiving increasing attention as they
allow to clearly represent both the available knowledge, which includes the knowledge in itself and the constraints imposed to it by the domain or the users. In particular, Datalog 
ontologies are attractive because of their property of decidability and the possibility of
dealing with the massive amounts of data in real world environments; however, as it is the
case with many other ontological languages, their application in collaborative environments
often lead to inconsistency related issues. In this paper we introduce the notion of incoherence regarding Datalog  ontologies, in terms of satisfiability of sets of constraints, and
show how under specific conditions incoherence leads to inconsistent Datalog  ontologies.
The main contribution of this work is a novel approach to restore both consistency and
coherence in Datalog  ontologies. The proposed approach is based on kernel contraction
and restoration is performed by the application of incision functions that select formulas to
delete. Nevertheless, instead of working over minimal incoherent/inconsistent sets encountered in the ontologies, our operators produce incisions over non-minimal structures called
clusters. We present a construction for consolidation operators, along with the properties
expected to be satisfied by them. Finally, we establish the relation between the construction and the properties by means of a representation theorem. Although this proposal is
presented for Datalog  ontologies consolidation, these operators can be applied to other
types of ontological languages, such as Description Logics, making them apt to be used in
collaborative environments like the Semantic Web.

1. Introduction
The integration of different systems, and the interaction resulting from this integration, led
to a host of pervasive practical problems and challenging research opportunities; some of
the most interesting ones occurs in the Webs collaborative environments, e.g., e-commerce,
and with the arrival of the Semantic Web, such as ontology engineering. However, the
collaboration among systems brings along the problem of conflicting pieces of information
that are likely to appear as knowledge repositories evolve. Admittedly, the management of
conflicting information is an important and challenging issue that has to be faced (Gomez,
Chesnevar, & Simari, 2010; Haase, van Harmelen, Huang, Stuckenschmidt, & Sure, 2005;
Huang, van Harmelen, & ten Teije, 2005; Bell, Qi, & Liu, 2007), specially when integrating
c
2016
AI Access Foundation. All rights reserved.

fiDeagustini, Martinez, Falappa & Simari

knowledge coming from different sources (Black, Hunter, & Pan, 2009; Baral, Kraus, &
Minker, 1991; Amgoud & Kaci, 2005), or when such knowledge is expected to be exploited
by a reasoning process. In this context, knowledge bases in the form of ontologies are becoming a useful device that provide a convenient way to represent both the intensional and
extensional knowledge of the application domain. Moreover, the expressive power of ontologies allows to perform important tasks on data integration (Lenzerini, 2002), and also plays
a role of great importance in the aforementioned Semantic Web (Berners-Lee, Hendler, &
Lassila, 2001). In this work we adopt Datalog  ontologies, a family of rule-based ontology
languages (Cal, Gottlob, & Lukasiewicz, 2012). Datalog  enables a modular rule-based
style of knowledge representation, and it can represent syntactical fragments of first-order
logic (FOL) so that answering a Boolean Conjunctive Query (BCQs) Q under a set 
of Datalog  rules for an input database D is equivalent to the classical entailment check
D   |= Q. Tractable fragments of Datalog  guarantee termination of query answering
procedures in polynomial time in the data complexity and first-order rewritability. Moreover, ontologies described using existential rules generalize several well-known Description
Logics (DLs); in particular, linear and guarded Datalog  (two basic tractable fragments of
this family) are strictly more expressive than the whole DL-Lite family (Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2005), and guarded Datalog  is strictly more expressive
than EL (Brandt, 2004; Baader, Brandt, & Lutz, 2005). Therefore, the results presented in
this paper extend directly to these DLs as well. These properties of Datalog  together with
its expressive power, and the fact that it keeps a syntax closer to that used in relational
databases for greater readability, make it very useful in modeling real applications, such as
ontology querying, Web data extraction, data exchange, ontology-based data access, and
data integration.
We focus on two particular problems that arise from the integration and/or evolution
of information systems: inconsistency and incoherence. Inconsistency refers to the lack of
models of a theory. On the other hand, in ontological settings, incoherence refers to a set
of ontological rules that cannot be applied without leading to violations of the constraints
imposed on the knowledge, making them unsatisfiable. Incoherence and inconsistency, which
can arise from automated procedures such as data integration and ontology matching, may
be serious issues in real world applications. Since standard ontology languages adhere to
the classical FOL semantics, classical inference semantics fails in the presence of this kind
of problems. Thus, it is important to focus on the formalization of methods to address both
inconsistency and incoherence in ontologies that are able to cope with the users expectations
in terms of effectiveness of the procedures for query answering and the meaning of these
answers when potential conflict exists.
This paper addresses the problem of handling inconsistencies and incoherences that
may appear in Datalog  ontologies. In this regard, we propose a general framework that
aims at the consolidation of Datalog  ontologies (i.e., solving every conflict of coherence
and consistency in them). That is, a consolidation operator takes as input a (possibly)
incoherent and inconsistent Datalog  ontology and returns another Datalog  ontology
where all conflicts are amended, thus ensuring that it is both coherent and consistent. As
it is usual in this setting, an assumption of minimal change is made, that is to say, it is
expected that the consolidation process changes the original ontology as little as possible.
The approach presented is based on the use of incision functions (Hansson, 1993, 1994, 1997,
614

fiDatalog  Ontology Consolidation

2001) from the Belief Revision literature. Instead of operators that only account for the
information included in the conflicts in a knowledge base, in this work we aim to capture
consolidation operators that can consider all the information included in a KB when solving
conflicts. The main contributions of this work are the following:
 We introduce a notion of incoherence tailored for Datalog  . To achieve this we adapt
to this setting similar notions from Description Logics. Also, we look into the relationship of incoherence and inconsistency and how it impacts the consolidation process.
 We provide a set of properties expected to be satisfied by consolidation operators
for Datalog  ontologies by means of postulates. These postulates provide a formal
characterization of a consolidation operator without focusing on how the consolidation
process is actually performed, thus providing a formal comparison framework for
consolidation operators. The postulates consider some intuitions in classic Belief
Revision; nevertheless, they are adapted to the Datalog  ontological setting (and
could be also adapted to suit other ontological languages), meaning that they have
two versions (one addressing incoherence and another one for inconsistency).
 We present a complete construction of consolidation operators that take a (possibly)
incoherent and inconsistent Datalog  ontology and gives as a result a consistent and
coherent one. A noteworthy characteristic of such operators is that it involves a two
steps approach, first considering incoherence conflicts, and then solving inconsistency
conflicts as a latter step, helping to improve the final result in terms of the information
that needs to be deleted to solve conflicts.
 We study the relationship between the formal properties of the operator and the
construction we propose, demonstrating that they are equivalent; thus, this shows that
any consolidation operator satisfying the properties corresponds to the construction
introduced in this work.
The paper is organized as follows: in Section 2 we introduce the necessary notions from
Datalog  and Belief Revision. Next, though inconsistency and incoherence are related,
they are also two very distinct problems in the setting of ontological knowledge bases in
particular, where there is a clear separation of the intensional and the extensional knowledge.
Therefore, in Section 3, we discuss the two notions in Datalog  ontologies, how they relate
to each other, and the reasons why they need to be treated in combination but separately.
Then, in Section 4 we present the properties that an ontology consolidation operator must
satisfy, and in Section 5 we introduce the process used to restore consistency and coherence
of Datalog  ontologies, and relate the presented process to the given properties by means
of a representation theorem. Next, we present a complete example depicting the entire
consolidation process. Finally, in Sections 7 and 8 we discuss related work from different
areas in Artificial Intelligence and Database Theory, and provide conclusions and future
lines of research, respectively.

2. Preliminaries and Background
To facilitate the reading, we begin by introducing the notions from Datalog  and Belief
Revision that will be needed in the rest of the paper.
615

fiDeagustini, Martinez, Falappa & Simari

2.1 Preliminaries on Datalog 
First, we recall the basic notions of Datalog  ontologies that will be used in the paper
(see Cal et al., 2012 for more details). Datalog  extends Datalog by allowing existential
quantification in the rule heads, together with other extensions that we enumerate below,
but limiting the interaction of these elements in order to achieve tractability.
We will assume that the domain of discourse of a Datalog  ontology consists of a
countable set of data constants , a countable set of nulls N (as place holders for unknown
values), and a countable set of variables V. We also assume that different constants represent
different values (unique names assumption). To distinguish constants from variables, we
adopt the standard notation from logic programming, where variable names begin with
uppercase letters, while constants and predicate symbols begin with lowercase letters.
We assume a relational schema R that is a finite set of predicate symbols (or simply predicates). A term t is a constant, a null, or a variable. An atom a has the form p(t1 , . . . , tn ),
where p is an n-ary predicate and t1 , . . . , tn are terms; an atom is ground iff all terms in
it are constants. Let L be a first-order language such that R  L; then LR denotes the
sublanguage generated by R. A database (instance) of R is a finite set of atoms with predicates in R and terms in   N . A homomorphism on constants, nulls and variables is
a mapping h :   N  V    N  V such that (i) c   implies h(c) = c, (ii)
c  N implies h(c)    N , and (iii) h is naturally extended to atoms, sets of atoms,
and conjunctions of atoms.
Given a relational schema R, a tuple-generating dependency (TGD)  is a first-order
formula of the form XY(X, Y)  Z(X, Z) where (X, Y) and (X, Z) are conjunctions of atoms over R called the body (denoted body()) and the head (denoted head()),
respectively. Consider a database D for a relational schema R, and a TGD  on R of the
form (X, Y)  Z (X, Z). Then,  is applicable to D if there exists a homomorphism
h that maps the atoms of (X, Y) to atoms in D. Let  be applicable to D, and h0 be
a homomorphism that extends h as follows: for each Xi  X, h0 (Xi ) = h(Xi ); for each
Zj  Z, h0 (Zj ) = zj , where zj is a fresh null, i.e., zj  N , zj does not occur in D, and zj
lexicographically follows all other nulls already introduced. The application of  on D adds
to D the atom h0 ((X, Z)) if it is not already in D. After the application we say that  is
satisfied by D. The Chase for a database D and a set of TGDs T , denoted chase(D, T ),
is the exhaustive application of the TGDs (Cal et al., 2012) in a breadth-first (level-saturating) fashion, which leads to a (possibly infinite) chase for D and . It is important
to remark that BCQs Q over D and T can be evaluated on the chase for D and T , i.e.,
D  T |= Q is equivalent to chase(D, T ) |= Q (Cal et al., 2012).
Negative constraints (NCs) are first-order formulas of the form X(X)  , where
(X) is a conjunction of atoms (without nulls) and the head is the truth constant false,
denoted . An NC  is satisfied by a database D under a set of TGDs T iff there does not
exist a homomorphism h that maps the atoms of (X) to D, where D is such that every
TGD in T is satisfied, i.e., the atoms in the body cannot all be true together.
Equality-generating dependencies (EGDs) are first-order formulas of the form
X(X)  Xi = Xj , where (X) is a conjunction of atoms, and Xi and Xj are variables from X. An EGD  is satisfied in a database D for R iff, whenever there exists a
homomorphism h such that h((X))  D, it holds that h(Xi ) = h(Xj ). In this work we
616

fiDatalog  Ontology Consolidation

will focus on a particular class of EGDs, called separable (Cal et al., 2012); intuitively,
separability of EGDs w.r.t. a set of TGDs states that, if an EGD is violated, then atoms
contained in D are the reason of the violation (and not the application of TGDs); i.e., if an
EGD in E is violated when we apply the TGDs in T for a database D, then the EGD
is also violated in D. Separability is a standard assumption in Datalog  ontology, as one
of the most important features of this family of languages is the focus on decidable (Cal,
Lembo, & Rosati, 2003) (actually tractable) fragments of Datalog  .
NCs and EGDs play an important role in the matter of conflicts in Datalog  ontologies.
In fact, the approach that we present in this work ensure that neither NCs nor EGDs are
violated in the resulting ontology. Also, as an important remark, note that the restriction
of using only separable EGDs makes that certain cases of conflicts are not considered in our
proposal. The treatment of such cases, though interesting from a technical point of view,
are outside the scope of this work since we focus on tractable fragments of Datalog  .
As is the usual case in the literature, in general the universal quantifiers in TGDs,
negative constraints and EGDs are omitted, and the sets of dependencies and constraints
are assumed to be finite. Now that we have presented the different ways of expressing
knowledge in Datalog  , we are ready to formally define Datalog  ontologies.
Definition 1 (Datalog  Ontology) A Datalog  ontology KB = (D, ), where  = T 
E  NC , consists of a database instance D that is a finite set of ground atoms (without
nulls), a set of TGDs T , a set of separable EGDs E and a set of NCs NC .
Otherwise explicitly said, through the paper when it is clear from context we will refer
to the component  in KB as the set of constraints in the ontology, without distinguishing
between dependencies and constraints. Given a database D for R and a set of constraints
 = T  E  NC , the set of models of D and , denoted mods(D, ), is the set of
all databases B such that D  B and every formula in  is satisfied. The following
example shows a simple Datalog  ontology; the ontology describes knowledge about the
therapy/psychology domain.
Example 1 (Datalog  Ontology)

D: {a1 : in therapy(charlie), a2 : dating(kate, charlie),




a3 : therapist(kate), a4 : belongs to(g1 , charlie),




a

5 : in therapy(patrick ), a6 : belongs to(g2 , ed ),



a

7 : belongs to(g1 , kate)}







 NC : {1 : treating(T , P )  dating(T , P )  }
KB =


E : {1 : treating(T , P )  treating(T 0 , P )  T = T 0 }








T : {1 : in therapy(P )  patient(P ),




2 : therapist(T )  belongs to(G, T )  leads(T , G),




3 : leads(T , G)  belongs to(G, P )  treating(T , P ),



4 : treating(T , P )  therapist(T )}













































The set T of TGDs expresses dependencies such as: TGD 1 states that if a person P
is in therapy then P is a patient, 2 establishes that a therapist T that belongs to a group
617

fiDeagustini, Martinez, Falappa & Simari

G is the leader of that group. The only NC 1 states that a patient cannot be dating his
therapist, and EGD 1 states that every patient is in treatment with at most one therapist.
Following the classical notion of consistency, we say that a consistent Datalog  ontology
has a non-empty set of models.
Definition 2 (Consistency) A Datalog  ontology KB = (D, ) is consistent iff
mods(D, ) 6= . We say that KB is inconsistent otherwise.
Example 2 Consider the Datalog  ontology from the example above; this ontology is clearly

inconsistent. Database instance D is clearly not a model in itself since at least TGD 2 is
applicable to D, but there is no superset of D such that it satisfies all TGDs and constraints in  at the same time. For instance TGDs 2 is applicable in D creating the atom
leads(kate, g1 ) making now 3 applicable and resulting in the new atom treating(kate, charlie),
which together with dating(kate, charlie) (that was already in D) violate the NC 1 , as a
therapist is dating one of her patients.
For the rest of the paper, otherwise explicitly stated KB = (D, ) will denote a Datalog 
ontology with  = T  E  NC , where D is a database instance, T is the set of all
TGDs, E the set of all separable EGDs and NC being the set of all NCs in .
2.2 Background in Belief Revision
Establishing the origins of scientific ideas is a difficult task that sometimes can be controversial; nevertheless, it could be argued that the origins of belief change theory go back to
the work of Isaac Levi (1977), who discussed the problems concerning this field of research,
and to William Harpers proposal of a rational way to interrelate belief change operators (Harper, 1975). However, the main advances on belief change theory came during the
1980s when Carlos Alchourron and David Makinson studied changes in legal codes (Alchourron & Makinson, 1981), and Peter Gardenforss introduced rational postulates for
change operators (Gardenfors, 1982). After that, the three authors produced a foundational paper containing what became known as the AGM model (Alchourron, Gardenfors,
& Makinson, 1985). The core contribution of the AGM model is the presentation of a
new and more general formal framework for the study of belief change; today, this work is
considered as the cornerstone from which belief change theory evolved.
Since the introduction of the AGM model, different frameworks for belief dynamics and
their respective epistemic models have been proposed. The epistemic model corresponds to
the formalism in which beliefs are represented, providing the framework in which different
kinds of operators can be defined. The AGM model is conceived as an idealistic theory of
rational change in which epistemic states are represented by belief sets (sets of sentences
closed under logical consequence, commonly denoted in boldface), and the epistemic input
is represented by a sentence. In the AGM model, three basic change operators are defined:
expansion, contraction, and revision. In the rest of this section, whenever we use the term
consistent or inconsistent, we refer to the traditional notion of inconsistency of a knowledge
base that has no models. Let K be a belief set, the change operations are as follows:
618

fiDatalog  Ontology Consolidation

 Expansions: the result of expanding K by a sentence  is a possibly larger set which
infers ; intuitively, belief , hopefully consistent with the given epistemic state, is
directly added to K.
 Contractions: the result of contracting K by  is a possibly smaller set which does
not infer , unless  is a tautology;
 Revisions: the result of revising K by  is a set that neither extends nor is part of
the set K. In general, if  is not a fallacy then  is consistently inferred from the
revision of K by .
The great importance of AGM comes from providing axiomatic characterizations of contraction and revision in terms of rationality postulates. Such rationality postulates regard
the operators as black boxes, characterizing what they do, but not explaining how they
do it. In other words, their behavior is constrained with regard to inputs in basic cases,
without describing the internal mechanisms used for achieving that behavior, so it is crucial
to say that contraction and revision operators can also be obtained via more constructive
approaches. AGM contractions can be realized by partial meet contractions, which are
based on a selection among (maximal) subsets of K that do not imply . Via the Levis
identity (Gardenfors, 1988), associated revision operations called partial meet revisions are
obtained. Another possible approach for contraction is based on a selection among the
(minimal) subsets of K that contribute to make K imply , as in safe contraction (Alchourron & Makinson, 1985). A more general variant of the same approach, known as
kernel contraction, was introduced later (Hansson, 1994). It has been shown that both safe
contractions and kernel contractions are equivalent to partial meet contractions, and hence
to the AGM approach to contraction (Hansson, 1994, 2001).
A particularly interesting characteristic of kernel contraction is that it may be concerned
with changes at the symbolic level since it is suitable of being applied to belief bases (set of
sentences not closed under a consequence relation) as well as belief sets. Thus, it matters
how the beliefs are actually represented. This does not happen in the AGM approach, as it
studies the changes at the knowledge level since it uses belief sets. The distinction between
knowledge and symbolic level was proposed by Allen Newell (1982). According to Newell,
the knowledge level lies above the symbolic level, and the latter is used to somehow represent
the former. Because of this, belief bases with different symbolic content may represent the
same knowledge. The importance of this is that, although they are statically equivalent
(they represent the same beliefs), equivalent belief bases could be dynamically different if
we choose to use an approach working directly with them, as with kernel contraction.
Besides the three basic operations mentioned, through the years additional operations
where developed in Belief Revision to achieve different behaviors. For instance, when a
belief base is inconsistent, the removal of enough sentences from it can lead to a consistent
state. This additional operation is called consolidation, and the consolidation of a belief base
K is denoted K ! (see Hansson, 1991, 2001). Here we will focus on this last operation, which
is inherently different from contraction and revision, since its ultimate goal is to obtain a
consistent belief base from a possibly inconsistent one (without being given any epistemic
input), rather than revising the knowledge base by a specific formula or by removing a
particular formula from it. The consolidation of K can be obtained in a natural way in belief
619

fiDeagustini, Martinez, Falappa & Simari

bases by contracting them by falsum, i.e., K ! = K  , where  represents a contraction
operator; this process restores consistency attending every conflict in K (Hansson, 1991).

3. Incoherence and Inconsistency Problems Related to Datalog 
Ontology Consolidation
The problem of obtaining consistent knowledge from an inconsistent knowledge base is
natural in many computer science fields. As knowledge evolves, contradictions are likely to
appear, and these inconsistencies have to be handled in a way that they do not affect the
quality of the information obtained from the database.
In the setting of Consistent Query Answering (CQA), repairing of relational databases,
and inconsistency-tolerant query answering in ontological languages (Arenas, Bertossi, &
Chomicki, 1999; Lembo, Lenzerini, Rosati, Ruzzi, & Savo, 2010; Lukasiewicz, Martinez, &
Simari, 2012), often the assumption is made that the set  expresses the semantics of the
data in the component D, and as such there is no internal conflict on the set of constraints
and these constraints are not subject to changes over time. This means first, that the set 
is always satisfiable, in the sense that their application do not inevitably yield a consistency
problem. Second, as a result of this assumption, it must be the case that the conflicts come
from the data contained in the database instance, and that is the part of the ontology that
must be modified in order to restore consistency. Although this is a reasonable assumption
to make, specially in the case of a single ontology, in this work we will focus on a more
general setting, and consider that both data and constraints can change through time and
become conflicting. In this more general scenario, as knowledge evolves (and so the ontology
that represents it) not only data related issues can appear, but also constraint related ones.
We argue that it is also important to identify and separate the sources of conflicts
in Datalog  ontologies. In the previous section we defined inconsistency of a Datalog 
ontology based on the lack of models. From an operational point of view, conflicts appear
in a Datalog  ontology whenever a NC or an EGD is violated, that is, whenever the body
of one such constraint can be mapped to either atoms in D or atoms that can be obtained
from D by the application of the TGDs in T  . Beside these conflicts, we will also
focus on the relationship between the set of TGDs and the set of NCs and EGDs, as
it could happen that (a subset of) the TGDs in T cannot be applied without leading
always to the violation of the NCs or EGDs. Note that in this case clearly the data in the
database instance is not the problem, as any database in which these TGDs are applicable
will inevitable produce an inconsistent ontology. This issue is related to the unsatisfiability
problem of a concept in an ontology, and it is known in the Description Logics community as
incoherence (Flouris, Huang, Pan, Plexousakis, & Wache, 2006; Schlobach & Cornet, 2003;
Borgida, 1995; Beneventano & Bergamaschi, 1997; Kalyanpur, Parsia, Sirin, & Hendler,
2005; Schlobach, Huang, Cornet, & van Harmelen, 2007; Qi & Hunter, 2007). Incoherence
can be particularly important when combining multiple ontologies since the constraints
imposed by each one of them over the data could (possibly) represent conflicting models of
the application at hand. Clearly, the notions of incoherence and inconsistency are highly
related; in fact, Flouris et al.s (2006) work establish a relation between incoherence and
inconsistency, considering incoherence as a particular form of inconsistency.
620

fiDatalog  Ontology Consolidation

Later in this section we present a complete definition of incoherence in Datalog  , based
on the concept of unsatisfiability of sets of TGDs. Nevertheless, for now it is sufficient
to know that our proposed notion of incoherence states that given a set of unsatisfiable
constraints  it is not possible to find a set of atoms D such that KB = (D, ) is a consistent
ontology and at the same time all TGDs in T   are applicable in D. This means that
a Datalog  ontology can be consistent even if the set of constraints is incoherent, as long
as the database instance does not make those dependencies applicable. On the other hand,
a Datalog  ontology can be inconsistent even when the set of constraints is satisfiable,
e.g., KB = ({tall(peter), small(peter)}, {tall(X)  small(X)  }), where the (empty)
set of dependencies is trivially satisfiable and thus the ontology coherent; the ontology is,
nevertheless, inconsistent.
Before formalizing the notion of incoherence that we use in our Datalog  setting we
need to identify the set of atoms relevant to a given set of TGDs. Intuitively, we say that
a set of atoms A is relevant to a set T of TGDs if the atoms in the set A are such that the
application of T over A generates the atoms that are needed to apply all TGDs in T , i.e.,
A triggers the application of every TGD in T .
Definition 3 (Relevant Set of Atoms for a Set of TGDs) Let R be a relational
schema, T be a set of TGDs, and A a (possibly existentially closed) non-empty set of
atoms, both over R. We say that A is relevant to T iff for all   T of the form
XY(X, Y)  Z(X, Z) it holds that chase(A, T ) |= XY(X, Y).
When it is clear from the context, if a singleton set A = {a} is relevant to T  T we
just say that atom a is relevant to T .
Example 3 (Relevant Set of Atoms) Consider the following constraints:

T = {1 : supervises(X , Y )  supervisor (X ),
2 : supervisor (X )  makes decisions(X )  leads department(X , D),
3 : employee(X )  works in(X , D)}
Consider set A1 = {supervises(walter , jesse), makes decisions(walter ), employee(jesse)}.
This set is a relevant set of atoms to the set of constraints T = {1 , 2 , 3 }, since 1
and 3 are directly applicable to A1 and 2 becomes applicable when we apply 1 (i.e., the
chase entails the atom supervisor (walter ), which together with makes decisions(walter )
triggers 2 ).
However, the set A2 = {supervises(walter , jesse), makes decisions(gus)} is not relevant
to T . Note that even though 1 is applicable to A2 , the TGDs 2 and 3 are never applied
in chase(A2 , T ), since the atoms in their bodies are never generated in chase(A2 , T ).
For instance, consider the TGD 2  T . In the chase of T over D we create the atom
supervisor(walter), but nevertheless we still cannot trigger 2 since we do not have and
cannot generate the atom makes decisions(walter ), and the atom makes decisions(gus) that
is already in A2 does not match the constant value.
We now present the notion of coherence for Datalog  , which adapts efforts made for
DLs such as Schlobach and Cornets (2003) and Flouris et al.s (2006). Our conception
621

fiDeagustini, Martinez, Falappa & Simari

of (in)coherence is based on the notion of satisfiability of a set of TGDs w.r.t. a set of
constraints. Intuitively, a set of dependencies is satisfiable when there is a relevant set of
atoms that triggers the application of all dependencies in the set and does not produce the
violation of any constraint in NC  E , i.e., the TGDs can be satisfied along with the NCs
and EGDs in the KB .
Definition 4 (Satisfiability of a Set of TGDs w.r.t. a Set of Constraints) Let R be
a relational schema, T  T be a set of TGDs, and N  NC  E , both over R. The set
T is satisfiable w.r.t. N iff there is a set A of (possibly existentially closed) atoms over R
such that A is relevant to T and mods(A, T  N ) 6= . We say that T is unsatisfiable w.r.t.
N iff T is not satisfiable w.r.t. N . Furthermore, T is satisfiable w.r.t. NC  E iff there
is no T  T such that T is unsatisfiable w.r.t. some N with N  NC  E .
In the rest of the paper sometimes we write that a set of TGDs is (un)satisfiable omitting
the set of constraints, we do this in the context of a particular ontology where we have a
fixed set of constraints NC  E since any set of TGDs that is satisfiable w.r.t. NC  E is
satisfiable w.r.t. any subset of it and, on the other hand, any set of TGDs that is unsatisfiable
w.r.t. a subset of NC  E is also unsatisfiable w.r.t. the whole set of constraints.
Example 4 (Unsatisfiable Sets of Dependencies) Consider the following constraints.

1NC = { : risky job(P )  unstable(P )  }
1T = {1 : dangerous work (W )  works in(W, P )  risky job(P ),
2 : in therapy(P )  unstable(P )}
The set 1T is a satisfiable set of TGDs, and even though the simultaneous application of
1 and 2 may violate some formula in 1NC  1E , that does not hold for every relevant
set of atoms. Consider as an example the relevant set D1 = {dangerous work (police),
works in(police, marty), in therapy(rust)}; D1 is a relevant set for 1T , however, as we
have that mods(D1 , 1T  1NC  1E ) 6=  then 1T is satisfiable.
On the other hand, as an example of unsatisfiability consider the following constraints:
2NC = {1 : sore throat(X)  can sing(X)  }
2T = {1 : rock singer (X)  sing loud (X), 2 : sing loud (X)  sore throat(X),
3 : rock singer (X)  can sing(X)}
The set 2T is an unsatisfiable set of dependencies, as the application of TGDs {1 , 2 , 3 }
on any relevant set of atoms will cause the violation of 1 . For instance, consider the
relevant atom rock singer (axl): we have that the application of 2T over {rock singer (axl)}
causes the violation of 1 when considered together with 2T , and therefore we have that
mods({rock singer (axl)}, 2T  2NC  2E ) = . Note that any set of relevant atoms will
cause the violation of 1 .
We are now ready to formally define coherence for a Datalog  ontology. Intuitively,
an ontology is coherent if there is no subset of their TGDs that is unsatisfiable w.r.t. the
constraints in the ontology.
622

fiDatalog  Ontology Consolidation

Definition 5 (Coherence) An ontology KB is coherent if and only if T is satisfiable
w.r.t. NC  E . Also, KB is said to be incoherent iff it is not coherent.
Example 5 (Coherence) Consider the sets of dependencies and constraints defined in Ex-

ample 4 and an arbitrary database instance D. We can see that the Datalog  ontology
KB 1 = (D, 1T  1NC  1E ) is coherent, while KB 2 = (D, 2T  2NC  2E ) is incoherent.
Considering incoherence of a set of TGDs is important in the consolidation process of
Datalog  ontologies, since if not treated appropriately within the consolidation process, an
incoherent set of TGDs may lead to the trivial solution of removing every single relevant
atom in D (in the worst case, the entire database instance). This may be adequate for some
particular domains, but does not seem to be a desirable outcome in the general case.
Looking into Definitions 4 and 5 we can see that there is a close relationship between the
concepts of incoherence and inconsistency. In fact, it can be inferred from those definitions
that an incoherent KB will induce an inconsistent KB when the database instance contains
any set of atoms that is relevant to the unsatisfiable sets of TGDs. This result is captured
in the following proposition (proofs of results are presented in Appendix A).
Proposition 1 If KB is incoherent and there exists A  D such that A is relevant to some
unsatisfiable set U  T then KB = (D, ) is inconsistent.
As an instance of this relationship, consider the following representative example.
Example 6 (Relating Incoherence and Inconsistency) Consider the following ontology.

KB =


D : {a1 : can sing(simone), a2 : rock singer (axl ), a3 : sing loud (ronnie),




a4 : has fans(ronnie), a5 : rock singer (ronnie), a6 : rock singer (roy),




a7 : manage(band1 , richard )}








NC : {1 : sore throat(X)  can sing(X)  ,




2 : has private life(X)  famous(X)  }



E :








T :
















{1 : manage(X, Y )  manage(X, Z)  Y = Z}
{1
2
3
4
5

: rock singer (X)  sing loud (X),
: sing loud (X)  sore throat(X),
: has fans(X)  famous(X),
: rock singer (X)  can sing(X),
: has fans(X)  has private life(X)}

















































As hinted previously in Example 4, there we have the set A  D = {rock singer (axl)}
and the unsatisfiable set of TGDs U  T = {1 : rock singer (X)  sing loud (X), 2 :
sing loud (X)  sore throat(X), 4 : rock singer (X)  can sing(X)}. Since A is relevant
to U the conditions in Proposition 1 are fulfilled, and indeed the ontology KB = (D, ) is
inconsistent since 1  T is violated.
A set of constraints such as the one presented in Example 6 may appear when we consider scenarios where both components of an ontology evolve (perhaps being collaboratively
623

fiDeagustini, Martinez, Falappa & Simari

maintained by a pool of users). As long as new constraints are added, incoherence problems
may arise. In this particular scenario it would seem more sensible to identify and modify,
somehow, the set of incoherent constraints to make them satisfiable, instead of deleting all
the information from the ontology; and only then proceed to solve remaining inconsistencies, if any. That is, it could be beneficial to define consolidation processes in which the
changes performed to achieve coherence are given higher priority than the changes needed
for consistency when possible. To address this we present a twofold proposal for consolidation of Datalog  ontologies: that is, to obtain the new KB 0 we begin by addressing issues in
the component T w.r.t. the components E and NC in the original ontology to obtain a
new coherent set of constraints, giving up some TGDs in T if necessary. Then, we address
the problems arising from the component D, obtaining a new one D0 that is consistent with
0T  E  NC . In the next section we characterize, by means of a set of postulates, a
consolidation operator that takes into account these considerations.

4. Characterizing the Consolidation: Postulates for Datalog  Ontology
Consolidation Operators
Belief Revision is one of the main areas that deals with defined principled methods to solve
incoherences and inconsistencies; as explained in Section 2, it is common to characterize
change operators by means of postulates, which are properties that the operators must satisfy. In this section we introduce a set of postulates with the objective of characterizing
consolidation operators for Datalog  ontologies. We start by briefly defining the scenario
underlying the consolidation process and introducing the characteristics of the sets of formulas that we focus on (Friedman & Halpern, 2001).

4.1 Defining the Consolidation Environment
Depending on the type of knowledge base, we find two main streams of work in Belief
Revision. On one hand, some works are based on sets of formulas that are closed under
some consequence relation, called belief sets (Alchourron et al., 1985). This is known in
the Belief Revision literature as the coherence model. On the other hand, the option is to
choose belief bases (Katsuno & Mendelzon, 1991, 1992; Fuhrmann, 1991; Hansson, 1994,
1997, 2001; Falappa, Kern-Isberner, & Simari, 2002), i.e., non-closed sets of formulas; this
is referred to as the foundational model.
Opposite to the traditional closed world assumption found in other established areas
like relational databases, one important characteristic of Datalog  is that of an open world
assumption, unknown data is represented by means of null values. As a consequence, the
generation of new information in the language by the application of rules is susceptible of
being infinite (Cal, Gottlob, & Kifer, 2008, 2013), which seems to make the foundational
model a more appealing choice when working in this setting. Therefore, for the consolidation
of Datalog  ontologies we have chosen to follow the foundational model. In this model, the
epistemic state is a (possibly incoherent and inconsistent) Datalog  ontology.
624

fiDatalog  Ontology Consolidation

4.2 Expected Properties for the Consolidation Operator: Postulates
We present now the set of properties that a consolidation operator for Datalog  ontologies
must satisfy. We use the following notation through the rest of the paper. Let KB = (D, )
be the original Datalog  ontology being consolidated, where  = T  E  NC . Also,
KB ! denotes the Datalog  ontology KB ! = (D!, !) resulting from the consolidation of KB ,
with D! and ! being the consolidated components D and  in KB !, respectively. When
necessary we will differentiate KBs by using subscripts. In such cases, given KB i we denote
its consolidation by KB i ! = (Di !, i !).
We are ready now to introduce the Ontology Consolidation Postulates (OCP) expected
to be satisfied by the consolidation operators. Let  be the set of all Datalog  ontologies.
Then, a Datalog  ontology consolidation operator ! :    is a function that must
satisfy the following properties:
OCP 1. (Inclusion) !   and D!  D.
The consolidation process only includes in the resulting ontology formulas belonging to the original ontology.
OCP 2. (Consistency) KB ! is consistent.
The ontology obtained by the consolidation process must be consistent, i.e.,
there are no negative constraints or equality-generating dependencies that are
violated when we apply all TGDs in ! to the atoms in D!, and therefore
mods({D!, !}) 6= .
OCP 3. (Coherence) KB ! is coherent.
The ontology obtained by the consolidation process must be coherent, i.e., T
in ! must be satisfiable with respect to NC  E in !.
OCP 4. (Minimality): If KB 0  KB is coherent and consistent, then it holds that
KB ! 6 KB 0 .
There is no coherent and consistent ontology obtained from the original ontology that strictly contains the consolidated ontology.
Some of the postulates presented are inspired by the properties proposed by Hansson (1994) and by Konieczny and Pino-Perez (2002). Nevertheless, they are adapted to
suit the particularities of the ontological setting of Datalog  ; in particular, they take into
account the distinction between incoherence and inconsistency. For instance, Inclusion
is a direct adaptation of Hanssons homonymous postulate (Hansson, 1994), which states
that the contraction of a knowledge base should be a (not necessarily proper) subset of the
original one. Consistency and Coherence, on the other hand, result from adapting to
our setting Konieczny and Pino-Perezs postulate IC1 (2002), which intuitively ask that
the resulting merging must be consistent; in here we ask that the resulting consolidation
is not only consistent but also coherent. Minimality is a postulate added to ensure the
quality of the consolidation (w.r.t. a loss of information aspect), and is not adapted from
any particular work, but rather as a general notion in Belief Revision, where as noted by
625

fiDeagustini, Martinez, Falappa & Simari

Hansson (2001) it has been given many names such as conservatism (Harman, 2008), conservativity (Gardenfors, 1988), minimum mutilation (Quine, 1986) and minimal change (Rott,
1992).
The proposed postulates capture the notion that changes made with respect to the
original ontology are those that are necessary, and that the resulting ontology is, as expected,
both coherent and consistent. That is, given the original ontology the consolidation process
only removes constraints (TGDs) and atoms if they are somehow involved in making the
original ontology incoherent/inconsistent, and makes it in such a way that no unnecessary
removal is made.

5. A Datalog  Ontology Consolidation Operator
In previous sections we have presented examples of incoherences and inconsistencies that can
arise in Datalog  ontologies. Additionally, we stated the properties that the consolidation
operator should satisfy in order to make adequate changes in the original ontology regaining
coherence and consistency. Now, we propose a construction for the consolidation operator
that addresses such incoherence and inconsistency problems in Datalog  ontologies.
5.1 A Possible Construction for the Consolidation Operator
In the literature of Belief Revision several constructions for revision and contraction operators have been studied. Hansson (1994) presents how a contraction operation on belief
bases can be modeled by means of the application of incision functions. These functions
contract a belief base by a formula  by taking minimal sets that entail  (called -kernels)
and producing incisions on these sets so they no longer entail . The resulting belief base
is then conformed by the union of all formulas that are not removed by any function. This
approach is known as kernel contraction; the task of restoring consistency is also known
in the belief revision literature as contraction by falsum (Hansson, 1991). In this work,
we define the consolidation process as the application of incision functions. Nevertheless,
instead of directly considering minimal inconsistent subsets of formulas from the different
components of the ontology (which are equivalent to -kernels), in this work we perform incisions over structures called clusters (Martinez, Pugliese, Simari, Subrahmanian, & Prade,
2007; Lukasiewicz et al., 2012) that groups together related kernels. More specifically, to
solve incoherence we begin by establishing the dependency kernels; in an analogous way,
we define the data kernels to solve inconsistencies in D w.r.t. , then, based on them, we
obtain the dependency clusters and data clusters by exploiting an overlapping relation.
5.1.1 Identifying the Relation among Conflicts
The first step towards conflict resolution in our framework is to calculate the minimal coherence and consistency conflicts, and identify possible relations among such conflicts, if any.
Dependency kernels are sets of TGDs which are unsatisfiable w.r.t. the set of NCs and EGDs
in a Datalog  ontology and are minimal under set inclusion. These sets are known as Minimal unsatisfiability-preserving sub-TBoxes (MUPS) and Minimal incoherence-preserving
sub-TBoxes (MIPS) (Schlobach & Cornet, 2003) in the DL community.

626

fiDatalog  Ontology Consolidation

Definition
6 (Dependency Kernels) The set of dependency kernels of KB , denoted
Q
with KB , is the set of all X  T such that X is an unsatisfiable set of dependencies
w.r.t. NC  E and every proper subset X 0 of X (X 0 ( X) is satisfiable w.r.t. NC  E .
Example 7 (Dependency Kernels) Consider the following sets of constraints in a

Datalog  ontology KB :

NC : {1 : counselor (X )  regent(X )  ,




2 : cannot rule(X )  heir (X )  }








E : {1 : advise(X , K )  advise(X , K 0 )  K = K 0 }







T : {1 : advise(X , K )  counselor (X ),
KB =
2 : propose law (X , K )  regent(X ),




3 : prince(P )  heir (P ),




4 : son(P , K )  king(K )  prince(P ),




5 : counselor (C )  regent(C ),




6 : bastard son(X , Y )  son(X , Y ),



7 : bastard son(X , K )  king(K )  cannot rule(X )}









































For this KB there exist two dependency kernels, i.e.,
Q
KB

= {{3 , 4 , 6 , 7 }, {5 }}.

It is easy to show that the dependency kernels for a Datalog  ontology are independent
from the particular component D in the ontology, and thus they can be obtained by looking
only into the component . That is, even if we replace the component D in an ontology with
an empty set of atoms, the dependency kernels for the ontology with the empty database
are the same than those in the original one.


Lemma 1 Let KB
Q 1 = (D
Q1 , 1 ) and KB 2 = (, 2 ) be two Datalog ontologies such that
1 = 2 . Then, KB 1 = KB 2 .

In addition to the removal of the TGDs that make a set  unsatisfiable (thus making
an ontology incoherent), to solve inconsistencies we may need to remove atoms from components D in order to address data inconsistency as well. Analogously to the definition of
the dependency kernels, we define now data kernels as the minimal subset of atoms in D
that makes a KB = (D, ) inconsistent.

`
Definition 7 (Data Kernels) The set of data kernels of KB , denoted with KB , is the set
of all X  D such that mods(X, ) =  and for every X 0 ( X it holds that mods(X 0 , ) 6= .
627

fiDeagustini, Martinez, Falappa & Simari

Example 8 (Data Kernels) Consider the following coherent but inconsistent KB , pro-

posed by Lukasiewicz et al. (2012).

KB =


D : {directs(john, d1 ), directs(tom, d1 ), directs(tom, d2 ),




supervises(tom, john), works in(john, d1 ), works in(tom, d1 )}








NC : {supervises(X , Y )  manager (Y )  ,




supervises(X , Y )  works in(X , D)  directs(Y , D)  }



E :








T :








{directs(X , D)  directs(X , D 0 )  D = D 0 }
{1 : works in(X , D)  employee(X ),
2 : directs(X , D)  employee(X ),
3 : directs(X , D)  works in(X , D)  manager (X )}





































For this KB , the set of data kernels is
`
KB



 {supervises(tom, john), directs(john, d1 ), works in(john, d1 )}, 
{supervises(tom, john), directs(john, d1 ), works in(tom, d1 )},
=


{directs(tom, d1 ), directs(tom, d2 )}

Once we know the minimal conflicts in the ontology we identify relations among them, if
such relation exists. To do this, we group related kernels together in a new structure called
cluster, which makes possible to achieve an optimal solution in related kernels. Clusters are
obtained through an overlapping relation defined as follows.
Definition 8 (Overlapping, Equivalence) Let L be a first order language, R  L be a
relational schema, and LR the sublanguage
T generated by R. Given A  LR and B  LR ,
we say they overlap, denoted A B, iff A B 6= . Furthermore, given a multi-set of first
 the equivalence relation obtained over M through
order formulas M  2LR we denote as M
the reflexive and transitive closure of .
By exploiting the overlapping among dependency kernels and data kernels we can define
dependency clusters and data clusters, respectively.
Q
Definition 9 (Dependency Clusters) Let KB
set of Dependency Kernels for
Q be the

KB . Let  be the overlapping relation, and K = KB /Q
the quotient set for the equivKB
Q
S
alence relation obtainedQ
Qover KB . A Constraint Cluster is a set C = [] , where
[]  K. We denote by KB the set of all Constraint Clusters for KB .
`
Definition 10 (Data Clusters) `
Let KB be the set of Data Kernels for KB . Let  be

the overlapping relation, and K = KB /`
the quotient set for the equivalence relation
KB
`
S
obtained
over KB . A Data Cluster is a set C = [] , where []  K. We denote by
`
`
the set of all Data Clusters for KB .
KB
Intuitively, a dependency cluster groups dependency kernels that have some TGD in
common, in a transitive fashion; data clusters groups data kernels in an analogous way.
628

fiDatalog  Ontology Consolidation

Example 9 (Dependency Clusters and Data Clusters) Assume we have KB such that

Q

`
= {{1 , 2 }, {1 , 3 }, {4 , 5 }} and KB = {{a1 , a2 }, {a1 , a3 }, {a2 , a4 , a5 }}. Then, we
have two dependency clusters based on those kernels, grouping the first two kernels (due to
1 ) and the remaining kernel in another cluster; i.e.,
KB

Q
Q
KB

= {{1 , 2 , 3 }, {4 , 5 }}.

On the other hand, for the case of data clusters we have that
`
`
KB

= {{a1 , a2 , a3 , a4 , a5 }}.

The following proposition states that, since clusters are based on equivalence classes, every
kernel is included in one and only one cluster.
Q
Q
Q
PropositionQ
2 Y  KB is such that Y  X for some`X  KB if and only if Y * X 0
Q
0
for all
X 0 . Analogously,
Y  KB is such that Y  X for some
`
`
`
`X  KB such that X 6=
0
0
X  KB if and only if Y * X for all X  KB such that X 6= X 0 .
As a corollary of Proposition 2 we have that a formula in a kernel is included in only
one cluster.
Q
Corollary 1 (Corollary
from
Proposition
2)
Let


Y
for
some
Y

and  
KB
`
Q
Q
0
0
Y forQ
  X for some X  KB if and only if  
/ X 0 for
`
`all
Qsome Y  KB . Then,
0
0
0


Y
is
such
that


X
for
some
X

X  KB such that X 6= X . Analogously,
KB
`
`
if and only if  
/ X 0 for all X 0  KB such that X 6= X 0 .
The following lemma that we shall use further in the paper shows an example of how,
in the ontological setting of Datalog  , Leibnizs indiscernibility of identicals (von Leibniz,
1976) holds w.r.t. the clusters in Datalog  ontologies, as when two KBs are equivalent they
have the same set of clusters.
 ontologies such that KB = KB . Then,
Lemma
2 Let KB 1Q
and KBQ
2 be two Datalog
1
2
`
`
`
`
Q
Q
=
and
=
.
KB 1
KB 2
KB 1
KB 2

5.1.2 Solving Conflicts: Incision Functions
Once we have identified the clusters, we have to establish how the incoherences and inconsistencies are solved. An incision function selects which formulas should be deleted from
the data and dependency clusters.
Definition 11 (General Incision Function) A General Incision Function for KB is a
function  : (2LR , 2LR )  2LR such that all following conditions holds:
SQ
Q
S`
`
1. (KB )  ( KB )  ( KB ).
Q
Q
Q
2. For all X  KB and Y  KB such that Y  X it holds that (Y  (KB )) 6= .
`
`
`
3. For all X  KB and Y  KB such that Y  X it holds that (Y  (KB )) 6= .
629

fiDeagustini, Martinez, Falappa & Simari

Q
Q
4. For all X  KB it holds that T = (X  (KB )) is such that there not exists R  X
where R satisfies conditions 1 and 2, and R ( T .
`
`
5. For all X  KB it holds that T = (X  (KB )) is such that there not exists R  X
where R satisfies conditions 1 and 3, and R ( T .
Definition 11 states that a general incision function selects from each dependency (data,
respectively) cluster TGDs (atoms, respectively) for deletion in order to restore coherence
(consistency). Any incision function that complies with Definition 11 can be used as a base
for a consolidation operator. However, note that such an operator may not differentiate
between restoring coherence and consistency. This is not a problem in the classic literature
of Belief Revision since there is no notion of incoherence, and there is no distinction between
rules and facts in languages like propositional logic; thus, only consistency conflicts can
appear, avoiding the need to treat incoherences. Nevertheless, in the ontological setting of
Datalog  we have the opportunity of exploiting the fact that we have two different although
related kinds of conflicts to address them separately with the goal of finding a solution that
better suits the needs of applications that rely on this kind of knowledge bases.
The point this paper is trying to make is that, for knowledge bases in the form of
Datalog  ontologies it is important to differentiate, and adequately handle, incoherence
from inconsistency as the quality of the consolidated ontology heavily depends on that 
assuming we strive for minimal loss in the process. This, more complex setting, needs a
careful definition of what constitutes a kernel. To see what could happen if this is not done
properly, consider the following example.
Example 10 (Influence of Incoherence on Consolidation) Consider KB from Exam-

ple 6. There we have  = 2T  2E  2NC such that 2T is unsatisfiable. As explained in Example 6, for the singleton set {rock singer (axl)} we have that the NC 1 : sore throat(X) 
can sing(X)   is violated, making {rock singer (axl)} inconsistent with . Then,
{rock singer (axl)} is a data kernel (and a data cluster, since it cannot overlap with any
other kernel) and the same is verifiable for every singleton set in D relevant to some dependency cluster. Thus, we have that

`
`
KB


{rock singer (axl)},



{rock singer (ronnie)},
=
{rock singer (roy)},



{has fans(ronnie)}









Consider the cluster {rock singer (axl)}; for this cluster we have that
({rock singer (axl)}) = rock singer (axl).
`
`
`
`
S`
`
This same situation holds for every cluster in KB , and thus ( KB ) = ( KB ).
The problem in this example is that the data kernels (and hence so the data clusters) are
computed w.r.t. the original  component, which, in this case, contain unsatisfiable sets
of constraints. As can be seen in Example 10, this becomes of utter importance when we
have atoms relevant to unsatisfiable sets: in that case, any general incision function (and
any inconsistency management technique based on deletion that does not treat incoherence
conflicts) will necessarily delete such atoms.
630

fiDatalog  Ontology Consolidation

Proposition 3 Let  be a general incision function. If   D is relevant to some X 
then   (KB ).

Q
KB

Clearly, as a corollary of Proposition 3 we have that if every atom in D is relevant to
some unsatisfiable set then we have to remove every atom in D to restore consistency.
Corollary 2 (Corollary from Proposition 3) LetQ be a general incision function. If
for all   D it holds that  is relevant to some X  KB then D  (KB ).
As seen, incoherence can have great influence in consolidation if not treated properly
(that is, previously to the consistency restoration). It would seem better to compute the
data clusters based only on the retained satisfiable part of the  components. In Lemma 1
we show that the dependency kernels can be obtained independently of the D component
from the original ontology, because unsatisfiable sets are such that they violate a negative
constraint or equality-generating
dependency for any relevant set of atoms. Therefore, we
Q
Q
can first obtain
,
and
use
the
incision function
KB
`
` on the dependency clusters to select
which TGDs will be deleted.Q
QThen, we calculate KB based on the result of the application
of the incision function on KB , in this way only paying attention to the constraints that
will prevail in the consolidation process.
Next, we define both constraint incision functions and data incision functions which
are used to select candidates for deletion (from the original ontology) to restore coherence
and consistency, respectively. First, we define an incision function on dependency clusters
that helps to solve incoherence on the constraints.
Definition 12 (Constraints Incision Function) A Constraint Incision Function for KB
is a function  : (2LR , 2LR )  2LR such that all following conditions hold:
SQ
Q
1. (KB )  ( KB ).
Q
Q
Q
2. For all X  KB and Y  KB such that Y  X it holds that (Y  (KB )) 6= .
Q
Q
3. For all X  KB it holds that T = (X  (KB )) is such that there not exists R  X
where R satisfies conditions 1 and 2, and R ( T .
Intuitively, a constraint incision function takes dependency clusters and removes TGDs
from each of them in such a way that the resulting KB is coherent. Analogously to the
constraints
incision functions, we define data incision functions that solve inconsistencies in
`
`
.
KB
Definition 13 (Data Incision Function) A Data Incision Function for D is a function
% : (2LR , 2LR )  2LR such that all following conditions hold:
S`
`
 %(KB )  ( KB ).
`
`
`
 For all X  KB and Y  KB such that Y  X it holds that (Y  %(KB )) 6= .
`
`
 For all X  KB it holds that T = (X  %(KB )) is such that there not exists R  X
where R satisfies conditions 13 and 13, and R ( T .
631

fiDeagustini, Martinez, Falappa & Simari

Finally, it is necessary to make a significant remark regarding our usage of incision
functions. For that, let us first consider the following excerpt quoted from Hanssons (2001,
cf. p. 122) regarding the possible parameters passed to selection functions (which in our
case are incision functions) and how this choice affects the possible outcomes.
[. . . ] the proof of uniformity makes essential use of the fact that selection functions have been defined on remainder sets of the form A, not on pairs of the
form hA, i. If we had instead defined selection functions as follows:
 (A, ) is a non-empty subset of (A, ) if A is non-empty.
 (A, ) = {A} if A is empty.
T
then (A, ) would have been an operation very similar to partial meet contraction in other respects, but it would have been possible for (A, ) 6= (A, )
to hold if A = A, which the standard definition does not allow [. . . ]
Thus, extending Hanssons observation to incision functions and their use on consolidation,
we have that if we take only the sets of conflicts as arguments for incisions then the formulas
to be removed from two different ontologies having the same set of conflicts by an operator
using the incision function are identical. The reason for this is that the operator could not
tell the difference between the ontologies since its parameter is only the conflicts, which are
exactly the same. However, here we have chosen not to restrict our family of operators to
such behaviors; instead, we model operators whose behavior could select for removal the
same formula from equal conflicts, but that are not restricted to that choice. To achieve
this, we have chosen to take ontologies as parameters; so, if it fits the application domain
in which the operators are exploited, formulas that are not in any conflict could affect the
outcome of the consolidation.
In the approach presented here, an incision function not only should consider the TGDs
effect over a cluster, but its global effect over the whole knowledge base. The reason for
this requirement is that unlike the classic models of belief revision, the language used has
greater expressivity and the fact that a TGD generates multiple inferences. For instance,
in our framework from a TGD of the form XY(X, Y)  Z(X, Z) it is possible to
infer multiple instances of (X, Z).
To see the reason behind our choice more clearly consider the following example.
Example 11 Consider the following ontologies.


D : {p(a), q(a)}





NC : {p(X )  r (X )  }
KB 1 =





T : {1 : q(X )  r (X )}








KB 2 =








D : {p(a), q(a)}








 NC : {p(X )  r (X )  }












{1 : q(X )  r (X ),
2 : p(X )  s(X ),
3 : p(X )  t(X )}













T :








For these KB , the set of data clusters are equal, as we have
632

fiDatalog  Ontology Consolidation

`
`
KB 1

=

`
`
KB 2

=



{p(a), q(a)}

	

If we use the standard approach and take clusters as arguments for incisions, then we must
remove the same formula in both ontologies, because as it was explained the incision is a
function and therefore it cannot choose differently for the same argument.
Nevertheless, suppose that in our particular scenario we want to remove the atoms based
on the information they help to infer. If that is the case, then from KB 1 we should remove
p(a), but from KB 2 we should take out q(a), since in KB 2 the formula p(a) triggers more
TGDs, thus inferring more atoms. To achieve this type of behavior, it is necessary to pass
ontologies as parameters, since it is that what provides the adequate context.
5.1.3 Cluster Contraction-Based Consolidation Operator
Lastly, we define the consolidation operator for Datalog  ontologies that represents the two
different parts in the consolidation. First, the coherence restoration of the component 
is obtained based on the dependency clusters in the D component in the original ontology.
Second, the restoration of consistency in the D component is obtained based on the data
clusters w.r.t. the ! component obtained by applying a constraint incision function on the
original . In this way we achieve the behavior stated earlier in the paper; in a sense, we
give the incoherence resolution higher priority, since if we can retain atoms by addressing
unsatisfiable sets of TGDs instead, we choose to follow that path. The cluster contractionbased consolidation operator is formally defined as follows:
Definition 14 (Cluster Contraction-based Consolidation Operator)
Let KB be a Datalog  ontology,  be a Constraint Incision Function and % a Data Incision
Function. Also, let KB ? = (D,  \ (KB )) be the Datalog  ontology resulting from deleting
from KB the TGDs selected by . The Cluster contraction-based consolidation operator
KB !, is defined as follows:
KB ! = (D \ %(KB ? ),  \ (KB ))
The result of KBQ
! is the Datalog  ontology obtained by removing,
first, the TGDs
`
`
Q
(selected by  from
) and then atoms (selected by % from
) from the original
KB ?
KB
ontology KB . It is important to note that, on one hand only TGDs are removed from
, as dependency clusters do not contain EGDs or NCs. On the other hand, as the Data
Incision Function uses KB ? instead of KB then only atoms from D that are in conflicts with
 \ (KB ) are removed; this is because data clusters are calculated based on the constrains
obtained after the consolidation of .
5.2 Relation between Postulates and Construction: Representation Theorem
In Section 4 we have introduced the properties that a Datalog  consolidation operator
must satisfy. By means of the following representation theorem we can now establish the
relationship between the set of postulates for a Datalog  ontology consolidation operator
and the cluster contraction-based consolidation operator that we proposed in the previous
section. In what follows we denote with ! a consolidation operator defined as in Definition 14
where  and % correspond to arbitrary constraint and data incision functions, respectively.
633

fiDeagustini, Martinez, Falappa & Simari

Theorem 1 (Representation Theorem) The operator consolidation ! is a Cluster
Contraction-based Datalog  Ontology Consolidation Operator for a Datalog  ontology KB
iff it satisfies Inclusion, Coherence, Consistency, and Minimality.

6. A Complete Example of Datalog  Ontologies Consolidation
We have introduced an operator that allows us to consolidate Datalog  ontologies that
satisfies the set of expected properties expressed by the postulates in Section 4. In this
section, the complete process for the consolidation of Datalog  ontologies is depicted in the
following example.
Example 12 (Consolidation of Datalog  Ontologies) Suppose that we have the (in-

coherent and inconsistent) ontology KB shown in Figure 1, which expresses the information
we have collected about certain company.

D:




























NC :













E :
KB =






T :




































{a1
a3
a5
a7
a8
a9

: boss(walter ), a2 : supervises(walter , jesse),
: makes decisions(walter ),a4 : makes decisions(jesse),
: supervises(skyler , walter ), a6 : employee(walter ),
: in charge of (jesse, distribution),
: in charge of (walter , cooking),
: on strike(mike)}

{1 : follows orders(X )  makes decisions(X )  ,
2 : supervises(Y , X )  supervisor (X )  ,
3 : absent(X )  on strike(X )  }
{1 : in charge of (X , Y )  in charge of (X , Y 0 )  Y = Y 0 }
{1 : employee(X )  is supervised (X ),
2 : is supervised (X )  follows orders(X ),
3 : boss(X )  makes profit(X ),
4 : supervises(Y , X )  supervisor (Y ),
5 : supervises(Y , X )  employee(X ),
6 : is supervised (X )  makes decisions(X ),
7 : is supervised (X )  has work (X ),
8 : has work (X )  get paid (X ),
9 : has work (X )  Y in charge of (X , Y ),
10 : on strike(X )  absent(X )}





















































































Figure 1: The original ontology to be consolidated.
Now, to begin with the first part of the consolidation process (i.e., solving incoherences by making the set T satisfiable) we obtain, as the first step towards obtaining the
dependency clusters, the dependency kernels for KB :
Q
KB

= {{2 , 6 }, {10 }},

and based on the kernels, we calculate the set of dependency clusters for KB
Q
Q
= {{2 , 6 }, {10 }}.
KB
634

fiDatalog  Ontology Consolidation

Q
Q
Q
Note that, as there is no overlap among dependency kernels, we have KB = KB . Next,
we use a cluster incision function to solve incoherency problems. For the sake of the example
assume that we will guide the contraction process by means of a quantitative criterion, i.e.,
choosing among the possible incisions the ones that removes fewer formulas, and using
the plausibility among formulas when the cardinality of the incisions is the same. In the
following we show the possible incisions, i.e., those satisfying the conditions in Definition 12.
These sets are
 For cluster {2 , 6 } we could either remove 2 or 6 . Since the two incisions remove
the same number of atoms assume for this example that 2 is more plausible than 6 ,
and thus we prefer to retain the former.
 For cluster {10 } we can only remove 10 .
Then, the particular incision in this example will be as follows:
({2 , 6 }) = {6 }
({10 }) = {10 }
Now, we move on to the next part in the consolidation process: the consistency recovery.
As explained before, for this part the operator only considers TGDs that will effectively be
included in the consolidation. In this particular example this is T ! = T \ {6 , 10 }. From
now on then let KB ? = (D, !); based on KB ? we calculate the data kernels.
Q
= {{a2 , a4 }, {a3 , a5 }, {a3 , a6 }, {a2 , a5 }}
KB ?
Then, we obtain the data clusters, which are:
Q
Q
= {{a2 , a3 , a4 , a5 , a6 }}
KB ?
Now, to solve inconsistencies we need to consider those sets such that the intersection
with all kernels included in the clusters is not empty, using T ! instead of T when doing
this. Once again, we analyze the possible incisions (the sets respecting all conditions in
Definition 13) in the light of the number of atoms deleted and the plausibility of the formulas
in them. The different possible incisions for the cluster are:
- To remove {a2 , a3 }.
- To remove {a2 , a3 , a6 }.
- To remove {a2 , a5 , a6 }.
- To remove {a4 , a5 , a6 }.
Once again, each of the sets presented is such that its removal induce an operator that
satisfies the postulates, and thus is captured by our framework. Nonetheless, as explained
before for this example we will choose to remove as few atoms as possible. That is, we
choose to remove {a2 , a3 }), and so we have
%({{a2 , a3 , a4 , a5 , a6 }}) = {a2 , a3 })
Then, using a Datalog  ontology consolidation operator based on the contraction of
clusters like the one introduced in Definition 14 we can obtain the coherent and consistent
ontology shown in Figure 2.
635

fiDeagustini, Martinez, Falappa & Simari

KB ! =


D! : {boss(walter ), makes decisions(jesse),




supervises(skyler , walter ), employee(walter ),




in charge of (jesse, distribution),




in charge of (walter , cooking),




on strike(mike)}








NC ! : {follows orders(X )  makes decisions(X )  ,




supervises(Y , X )  supervisor (X )  ,




absent(X )  on strike(X )  }



E ! :








T ! :

































































{in charge of (X , Y )  in charge of (X , Y 0 )  Y = Y 0 } 







{employee(X )  is supervised (X ),




is supervised (X )  follows orders(X ),




boss(X )  makes profit(X ),




supervises(Y , X )  supervisor (Y ),




supervises(Y , X )  employee(X ),




is supervised (X )  has work (X ),




has work (X )  get paid (X ),



has work (X )  Y in charge of (X , Y )}

Figure 2: The ontology resulting from the consolidation.

7. Related Work
The most closely related work to ours is the work by Croitoru and Rodriguez (2015). In that
work the authors present consolidation operators that are used as the basis for the definition
of semantics for inconsistency tolerant ontology query answering for Datalog+ (a more
expressive language than Datalog  , Cal et al., 2012). As for the case with our work, the
work by Croitoru and Rodriguez (2015) is based on the use of Hanssons incision functions
(Hansson, 1994) to solve conflicts. Nevertheless, there are some remarkable differences
between the works as well. Among the most important ones is that the operators presented
by Croitoru and Rodriguez only deal with inconsistent ontologies, but no acknowledgment
of the incoherence problem is made. As we have shown through this work, this can have a
significant impact in the quality of the consolidation when analysed with respect to minimal
loss of information. Moreover, this fact makes that, even though the set of postulates in
both works are similar in spirit, the family of operators characterized by Croitoru and
Rodriguez is a subset of the ones characterized here. This is due to the fact that the setting
that we consider here (i.e., both inconsistent and incoherent ontologies) is more general,
since for instance our operators remove not only facts but also TGDs, which Croitoru and
Rodriguezs operators do not since they only focus on inconsistency.
Another closely related work is the one by Lukasiewicz et al. (2012). There, the authors define a general framework for inconsistency-tolerant query answering in Datalog 
ontologies based on the notion of incision functions. Nevertheless, their work is focused on
enforcing consistency at query time obtaining (lazy) consistent answers from an inconsistent ontology instead of consolidating one. Clearly, such process must be carried on for
every query posed to the system, while with our approach we obtain a new knowledge base
636

fiDatalog  Ontology Consolidation

in an offline manner, and such knowledge base can be queried without considering inconsistency issues; both approaches can prove useful, depending on the application domain.
Additionally, as only one KB is used the rational assumption that there are no conflicts
in the constraints in  is also made, and therefore there are no notions of unsatisfiability
and incoherence. As stated before, in order to gain generality we have chosen to drop that
assumption, and treat incoherence problems as well as inconsistency ones. In addition to
the works by Croitoru and Rodriguez and Lukasiewicz et al., there are several other works
that solve inconsistency or incoherence by means of adapting approaches based on Belief
Revision techniques in other knowledge representation formalism.
7.1 Propositional Knowledge Bases
There are numerous works in revision and merging of propositional knowledge bases (see, for
instance, Konieczny & Perez, 2002; Katsuno & Mendelzon, 1992; Lin & Mendelzon, 1999;
Liberatore & Schaerf, 1998; Everaere, Konieczny, & Marquis, 2008; Konieczny & Perez,
2011; Delgrande, Dubois, & Lang, 2006; Booth, Meyer, Varzinczak, & Wassermann, 2010;
Delgrande, 2011; Delgrande & Jin, 2012; Falappa, Kern-Isberner, Reis, & Simari, 2012),
which had provided the foundations to further work on (fragments of) first order logics.
As expected, those works have deep connections with ours, but also has some remarkable
differences, as we shall see.
As we have mentioned throughout the paper, the work by Sven Ove Hansson (1994)
provides the inspiration and foundations for our work: we follow an approach akin to Kernel
Contraction and several intuitions from it, adapted to an ontological language, Datalog  .
As a consequence, besides treating incoherence we also provide a complete inconsistency
resolution process which takes advantage of the ontological setting, exploiting the relation
between the components of the ontology to define how coherence and consistency should be
restored. Also, the classic incision functions introduced by Hansson produce their incision
over minimal conflicts. In our approach, however, we work over clusters, which are groupings
of kernels, and thus they are not always minimal. Then, we propose a particularization over
Hanssons incision functions, focusing on those incision functions that successfully work over
clusters.
Konieczny and Pino-Perez (2002) made one of the main contributions on the merging
of conflicting information. In our work we follow some of the intuitions proposed by them.
Nevertheless, the main difference between their approach and ours (besides the obvious one
on the aims of the works, merging vs. consolidation) is that they state that the final merging
will be consistent only in presence of a consistent (or, in our terminology, coherent) set of
integrity constraints, and they do not analyze the alternative case.
With respect to the work by Lin and Mendelzon (1999), besides the difference in the
focus (once again merging vs. consolidation), the main difference in the inconsistency
management strategy chosen with our work is that their conflict solving strategy relies on
the votes of the majority to establish which formulas is retained in the merging. Instead,
we have chosen not to introduce a particular strategy. Nevertheless, it is possible to adapt
our framework to the use of preference relations to choose between possible incisions (in a
similar way to what we have shown in Example 12). Such relations can indeed be designed
to comply with the majority intuition (providing that we have the votes, which does not
637

fiDeagustini, Martinez, Falappa & Simari

apply to an ontology consolidation environment since there is only one ontology), thus
obtaining a similar strategy.
In the work by Katsuno and Mendelzon (1992) the problem of knowledge base revision
for the propositional case is addressed. As in our approach, the same language is used to
express both the facts about the world and the constraints imposed to them in some KB .
Nevertheless, once again the difference between the (in this case) update of a KB and the
consolidation of a KB arises in the treatment of the integrity constraints: in their work
integrity constraints are considered invariant and the updates to restore consistency are
restricted to facts.
In they works by Delgrande (2011), Delgrande and Jin (2012) the authors present an
approach for revising a propositional knowledge base by a set of sentences, where every
sentence in the set can be independently accepted but there can be inconsistencies when
considering the whole set. The main idea follows from the AGM theory, but differs in
that, it is necessary to alter the Success postulate so it suits the intuition that not every
sentence in the set have to be in the final revision (since this set can be inconsistent).
Guided by the principle of informational economy, they characterize the revision as the most
plausible worlds among the various maximally consistent subsets of the set of sentences. In
a parallel with our Datalog  ontology environment, this is as revising the D component in
the ontology to solve inconsistencies. Being the set of sentences inconsistent, the union of
it with the original KB will be inconsistent. Nonetheless, there is an important difference
between these works and ours. In those works the authors first solve inconsistencies in the
set of sentences, so they can decide which subset of it will characterize the revision. Our
approach is different, as we directly consider an inconsistent KB . Then, in order to solve
the same problem in our setting, it is necessary to consider the union of the KB with the
entire set of sentences, and then apply a consolidation operator.
7.2 Knowledge Expressed in Description Logics Ontologies, Logic Programs
and Relational Databases
We now focus on other knowledge representation formalisms that are more closely related
to Datalog  , mainly in the family of Description Logics (Baader, Calvanese, McGuinness,
Nardi, & Patel-Schneider, 2003) and Logic Programming (Lloyd, 1987; Nilsson & Maluszynski, 1995; Gelfond, 2008). A remarkable work using belief revision to solve conflicts in DLs
is the one by Qi, Liu, and Bell (2006), which is based on the AGM theory (Alchourron
et al., 1985; Gardenfors, 1988). What makes this work stand out is that they do not only
introduce the generalizations of the AGM postulates to the case of DLs, but also define two
operators on knowledge bases, based on formulas weakening, that satisfy such postulates.
The main difference with our approach is that they only take into account consistency problems in the ontologies, and no incoherence treatment is provided. As we pointed out earlier,
incoherence can lead to extreme a weakening of the information, where they may have to
take out every individual name from some general concept inclusion.
As we have previously mentioned, our notion of incoherence was inspired by Schlobach
and Cornets work (2003), among others. In that paper the authors focus on the definition
of processes capable of detecting unsatisfiabilities and incoherences in DLs ontologies, introducing complete algorithms along with an empirical analysis of the approach. Nevertheless,
638

fiDatalog  Ontology Consolidation

as it is not in the main focus of their work, the authors set aside the issue of how to recover
coherence once a conflict has been detected, and also do not consider inconsistencies. In our
work we presented a consolidation process that treats both incoherence and inconsistency,
based on the use of Belief Revision techniques. Thus, the approach presented by Schlobach
and Cornet could potentially be useful regarding the implementation of the operators presented in this work, providing an effective way of obtaining the set of kernels in which the
set of clusters will be based.
Black et al. (2009) propose an approach that is capable of using information coming from
several DL ontologies in order to answer queries, taking care in the process of both incoherence and inconsistency. Their approach is based on agents with argumentative capabilities,
each one with a personal knowledge base in the form of a DL ontology. These agents use
dialogue games to interchange arguments until they reach an agreement about the answer
to a certain query. Thus, the agents can use the (possible incoherent/inconsistent) union
of the ontologies without merging them, and still obtain an answer influenced by every
ontology in play. Moreover, this approach has the advantage that no information is lost,
as no formula is deleted from the ontologies, and as a result the inferences obtained by
this approach are a superset of those that can be obtained in the ontology resulting from
the consolidation of the union of DL ontologies. Even though the authors argue that one
advantage of the proposed approach is that they do not need to waste time and effort in
performing the consolidation of the KB , one disadvantage is the computational complexity
associated with argumentative reasoning (Parsons, Wooldridge, & Amgoud, 2003; Dunne &
Wooldridge, 2009; Cecchi, Fillottrani, & Simari, 2006) as this process has to be conducted
for each query issued and in an online manner. Even though a consolidation process can
also be computationally expensive, it is only necessary to perform it once and it can be
done offline before the query answering system becomes available. The choice of one approach over the other depends highly on the environment in which they will be used, i.e.,
on the size of the ontologies that will be used, how often updates are issued over the KB
or how critical time consumption is for the system, among other considerations; of course
the set of inferences that can be obtained from every approach may differ and this should
also be taken into account. A consolidation-based approach could be more suitable for
time-dependant systems like real-time systems or query intensive systems where the data
tractability associated with (a consolidated) Datalog  ontology may be proven handy.
Another work worth mentioning is that by Kalyanpur, Parsia, Horridge, and Sirins
(2007). This work verses on how to find all justifications of entailments over a Description
Logics ontology. A justification is simply the precise set of axioms in an ontology responsible
for a particular entailment (Kalyanpur et al., 2007). In other words, it is a minimal set of
axioms sufficient to produce an entailment, which is related to our use of kernels as a mean to
obtain clusters as part of the consolidation strategy used. Moreover, Horridge, Parsia, and
Sattler (2009) state that justifications are important for repairing inconsistent ontologies.
Thus, they could be important for the definition of consolidation processes similar to our
cluster-based consolidation, as If at least one of the axioms in each of the justifications for
an entailment is removed from the ontology, then the corresponding entailment no longer
holds.(Kalyanpur et al., 2007, p. 269). One of the main contributions of that work is the
definition of practical black-box (i.e.,, reasoner independent) techniques that allows us to
find justifications for entailments in the ontology in an efficient way. As such, is evident
639

fiDeagustini, Martinez, Falappa & Simari

that while our work verses in a different direction we still can benefit from those findings. In
particular, it may be possible to use the developed algorithms as part of our implementation
strategy for our consolidation operators, adapting them to be used in Datalog  and our
dual incoherence/inconsistency setting.
Regarding Logic Programming, there are also several works that address the problem
of merging knowledge bases expressed as logic programs, solving inconsistency issues in
the process. For instance, Hue, Papini, and Wurbel (2009) introduce a merging process
based on stable model semantics, using the logic of Here-and-There (Turner, 2003). Hue
et al. consider the merging strategy based on pre-orders among deletion candidates called
potential removed sets and they do not establish any particular way to obtain these preorders. Instead, they assume that for any strategy P there is a given pre-order that defines
P . As for the case with Lin and Mendelzons work (1999), although it falls out of the scope
of the present work we certainly can adapt our framework to use similar techniques when
choosing which incision prevails.
Another notorious work in the Logic Programming field is the one by Delgrande, Schaub,
Tompits, and Woltran (2009). In that work two different approaches are proposed. The first
one follows an arbitration approach, selecting the models of a program that differs the least
w.r.t. the models of the other programs. In this work the case of unsatisfiable programs is
studied, similar to the way we consider incoherence leaded by unsatisfiable sets of TGDs.
Nevertheless, they consider unsatisfiability of a certain program, and not of some concept
in the union of the programs. Furthermore, the strategy to solve unsatisfiability is simply
leaving the unsatisfiable program out of consideration for the merging, instead of trying to
solve the conflict somehow. The second approach is based on the selection of the models of
a special program P0 , which can be thought as the constraints guiding the merging process,
that has the least variations w.r.t. the programs for the merging. This approach can be
seen as a particular instance of the approach proposed by Konieczny and Perez (2002).
In the area of databases, one of the most influential works is the one by Arenas et al.
(1999) on Consistent Query Answering, there the authors propose a model theoretic definition of consistent answers to a query in a relational database potentially inconsistent with a
set of integrity constraints. Intuitively, the consistent answers to a query is the set of atoms
that are (classical) answers to the query in every repair of the inconsistent database; a
repair is a set of atoms that satisfy the set of constraints and is as close as possible to the
original database. Different notions of repairs have been studied in the literature, as well as
different notions of what it means for a set of atoms to be as close as possible to the original
database. Most of the proposals are based on repairing by inserting and/or deleting tuples
to/from the database (actually, the possible actions depend on the form of the integrity
constraints and their expressiveness) and the notion of closeness is defined via set inclusion
or cardinality. The work by Arieli, Denecker, and Bruynooghe (2007), however, proposes
a uniform framework for representing and implementing different approaches to database
repairing based on minimizing domain dependent distances. The main idea of that work is
to show how thinking in terms of (different) distances to express preferences among repairs
leads to different preferences that can be applied in different scenarios. The authors show
that the set of repairs obtained using the proposed distance functions deviate from those
that can be obtained using set-inclusion. Furthermore, besides insertion and deletion of entire tuples there are other several domain independent approaches, e.g., based on cardinality
640

fiDatalog  Ontology Consolidation

or more complex objective functions. In the approach proposed by Wijsen (2005) updates
are considered a primitive in the theoretical framework; Bohannon et al. (2005) present a
cost-based framework that allows finding good repairs for databases that exhibit inconsistencies in the form of violations to either functional or inclusion dependencies, allowing
also updates to attribute values. In that work, two heuristics are defining for constructing repairs both based on equivalence classes of attribute values; the algorithms presented
are based on greedy selection of least repair cost, and a number of performance optimizations are also explored. A quite different semantics for repairing is proposed by Caroprese,
Greco, and Zumpano (2009), Caroprese and Truszczynski (2011) through Active Integrity
Constraints (AICs for short); an AIC is a production rule where the body is a conjunction
of literals, which should be false for the database to be consistent, whereas the head is a
disjunction of update atoms that have to be performed if the body is true (that is the constraint is violated). Repairs are then defined as minimal sets (under set inclusion) of update
actions (tuple deletions/insertions) and AICs specify the set of update actions that are used
to restore data consistency. Hence, among the set of all possible repairs, only the subset
of founded repairs consisting of update actions supported by AICs is considered. Other
works in this area propose different semantics for repairing by either explicitly or implicitly
considering a preference relation among the set of repairs (cf. Andritsos, Fuxman, & Miller,
2006; Staworko, Chomicki, & Marcinkowski, 2012; Greco & Molinaro, 2012).
More recently, in the area of ontology-based data access (OBDA), Lembo et al. (2010)
study the adaptation of CQA for DL-Lite ontologies, called AR (ABox semantics). In that
work, also the intersection (IAR) semantics is presented as a sound approximation of consistent answers; this semantics consists of computing the intersection of all repairs and answers
are obtained from there, though (possibly many) AR answers cannot be obtained from under the IAR semantics, the latter are computationally easy to obtain for the DL-Lite family,
i.e., it is not necessary to compute the whole set of repairs in order to compute their intersection. The data and combined complexity of these and other semantics were studied (Rosati,
2011) for a wider spectrum of DLs. Also, Rosati (2011) presents intractability results for
query answering for EL under the intersection semantics, and a non-recursive segment of
that language was proved to be computable in polynomial time. More recently, Bienvenu
and Rosati (2013) propose another family of approximations to CQA, also for the DL-Lite
family. The k-support semantics allows to (soundly) approximate the set of queries entailed
under the CQA semantics, based on k subsets of the database that consistently entail q;
on the other hand, the k-defeater semantics approximates complete approximations seeking
sets that contradict the supporters for q. Both semantics are FO-rewritable for any ontological language for which standard CQ answering is FO-rewritable as well, and can be used
in conjunction to over- and under-approximate consistent answers.
Much like Black et al. (2009), the treatment of inconsistencies proposed by all these
semantics is related to particular queries instead of the inconsistency of the whole database.
Thus, they do not attempt to obtain a final consistent database that can be queried without
considering restrictions. Furthermore, they do not address the issues of incoherence and
inconsistency together; instead most of the approaches either assume that the set of integrity
constraint correctly defines the semantics of the database instance, so there is no room for
incoherence, or they treat constraints and data alike at the moment of removing or ignoring
information, which leads to the type of problems that we discuss in Example 10. While
641

fiDeagustini, Martinez, Falappa & Simari

these techniques may be suitable for the case of one single database, in the presence of
incoherence in the set of ICs, as can be the case when we consider several databases together,
this approach would lead to meaningless empty answers, since no subset of the database
could satisfy the constraints as would also be the case for the approach by Lukasiewicz
et al. (2012).
Also related to the databases field is the work by Lin and Mendelzon (1998). There,
a database is viewed as a first-order theory without rules, and ICs are used to ensure
consistency in the final result as in the work by Konieczny and Perez (2002), presenting ways
to solve known database merging problems like synonyms and homonyms. Nonetheless, like
Konieczny and Pino-Perezs work, they do not consider problems related to the set of ICs.
Instead, the set of ICs used in the merging process is unique, and the choice of such set is
expected to be performed by a merge designer. Unlike Lin and Mendelzon, we do not made
an assumption for our consolidation environment on the set of ICs being conflict-free.
Cholvy (1998) introduces another approach that can be used to reason with contradictory information. The framework is represented through a set of axioms and inference
rules. Additionally, in the paper several applications for the framework are introduced,
e.g., the solving of conflicts among beliefs represented by first order databases, where facts
are ground literals and there are rules that can be integrity constraints or deduction rules.
In that scenario, contradiction is obtained by the application of the constraints when considering several databases together. This establishes a certain parallel with the case of
inconsistency in a Datalog  ontology. However, the main difference with our work lies in
how the strategy for the inconsistency management process is defined. In that work, a
preference order between databases is assumed. Instead, we have chosen not to restrict how
to achieve the consolidation, thus presenting a general approach. Nevertheless, as stated
before we can adapt incision functions to suit the intuition that no every formula is equally
desirable, choosing for instance preferences between ontologies as a guideline (if we are using
the approach for other tasks rather than consolidation of a single ontology), obtaining an
inconsistency management strategy akin to the one introduced by Cholvy.
Finally, Meyer, Lee, and Booth (2005) use two well-known techniques for knowledge
integration for the propositional case, adapted and refined to the expressiveness of DLs.
The proposed approach takes the knowledge bases and produces a disjunctive knowledge
base (DKB) as the result of the integration. One disadvantage of DKBs is that they state the
possible options we can take when the conflicting knowledge is expected to be exploited by a
reasoning process rather than choosing one of them. Thus, contrary to our approach where
a final consolidated ontology is given, in theirs there is no definitive final merging; moreover,
they set aside for further research problems related to incoherence in the integration process.

8. Conclusions and Future Work
Collaborative work and information exchange are becoming key aspects of almost any system; thus, it is of the uttermost importance to have automatic and adequate ways to solve
conflicts: as knowledge evolves in a collaborative environment incoherence and inconsistency are prone to arise. This knowledge is often represented by ontologies that can be
collaboratively built, and often shared among entities that use and modify them. One particular way to deal with the conflicts that can appear in such application environments is
642

fiDatalog  Ontology Consolidation

to try to modify the information contained in the ontology in order to regain coherence and
consistency. In this paper we have shown how to achieve the consolidation of Datalog 
ontologies. We introduced the concept of incoherence in Datalog  ontologies in terms of
unsatisfiability of sets of TGDs, and showed the relationship with the classical notion of
inconsistency of a logical theory that lacks models.
We also proposed a construction for consolidation operators. The construction is inspired by kernel contraction, and uses incision functions on groupings of minimal unsatisfiable/inconsistent sets called clusters to solve conflicts. Finally, we stated the properties
that the Datalog  ontology consolidation operator is expected to satisfy. We showed that
our operators satisfy the respective properties, obtaining as the result of the consolidation a
new Datalog  ontology that is always coherent and consistent while minimizing the changes
that are made in the conflict resolution.
As a final remark, notice that these operators take care of all incoherences in the ontology. However, there are rare cases when the ontology designer introduce some unsatisfiable
concepts in an ontology on purpose, to model some particular feature of the application domain. If that is the case then we should not remove the incoherence, and rather we have to
delete the atoms triggering it, if any. Clearly, since they were not defined with that setting
in mind this behavior cannot be achieved by the operators presented here. Nevertheless, to
modify our present approach to suit such setting is almost straightforward, provide that we
can identify whether or not some unsatisfiable set of TGDs is made on purpose or not.
As for future work, we intend to study new constructions for Datalog  consolidation
operators. To do this, first we plan to change the general approach, i.e., operators based
on formalisms other than kernel contraction, mainly from the AGM theory (Alchourron
& Makinson, 1985; Alchourron et al., 1985); and then, while the proposed framework for
cluster contraction based consolidation operators is fully constructive, depending on the
application domain it may certainly be difficult to asses how to effect the incisions, i.e., it
may be hard to decide among the family of possible incisions which one to select. From
a design point of view, it may be easier to select how to perform the consolidation if we
have some additional information about the formulas in the knowledge base, such as a
preference relation that can, for example, be elicited from domain experts. In general, it
could be easier for an expert to provide guidelines and information about the application
domain at hand that could be then modeled into a preference relation on the formulas in
an ontology rather than trying to single out the desired incisions. In this direction we want
to explore constructions based on exploiting preference relations among the formulas in the
ontologies to define different strategies to choose which formulas to delete, possibly tailored
for particular scenarios. Mainly, we plan to analyze two different aspects: the relation
between these operators based on preference relations with respect to the ones presented in
this work, and how different strategies affect their behavior.
Also, in this work we make a point in differentiating the concept of inconsistency from
that of incoherence; therefore, we need to focus on languages that separate extensional
from intensional knowledge, otherwise the two notions are indistinguishable (as it is the
case in propositional logic). In that sense, the choice of Datalog  is due to its desirable
property of generalizing several popular languages such as classical Datalog, DL-Lite, ELH,
F-Logic-Lite, etc. Even though in this paper we do not perform a particular analysis of the
effects of nulls in the proposed solutions to consolidation, the Datalog  family of languages
643

fiDeagustini, Martinez, Falappa & Simari

was chosen because it offers a wide variety of languages with high computational tractability
(some are FO rewritable and others have PTIME inference algorithms). The results in this
work pave the way to continue the research line into the next natural step, which is to show
how (or whether) the different syntactic and semantic properties that yield tractability for
query answering allow us to obtain tractability results also for the consolidation problem,
much in the same way as it has happened already in the area of consistent query answering
(where only repairs over the extensional part of the KB are considered). It is, for example, in
the rewriting algorithms where the capability of value invention plays an important role: the
value invention process should be controlled (in general with syntactic restrictions) in order
to keep a low complexity for the reasoning tasks. With this in mind, in the future we will
further look into the role of processes like value invention in the consolidation of Datalog 
ontologies, and their impact both on how conflicts should be solved and computational
efficiency.
We are currently working on the implementation of our operators; we plan to study different techniques that can be used in order to produce an efficient implementation, possibly
tailored for specific fragments of Datalog  . As explained before, the algorithms introduced
by Schlobach and Cornet (2003) can be proven useful regarding this aspect since they may
provide a way to calculate the kernels in a Datalog  ontology, thus providing the first step
towards incoherence resolution. Another important work regarding the implementation of
our consolidation operators is the one by Wassermann (2000), where the author shows that
the minimal incision functions of a knowledge base can be obtained from the kernels of a KB
by using any algorithm for finding minimal hitting sets (Reiter, 1987). Several works in the
area of ontology debugging and repairs, (e.g., Halaschek-Wiener & Katz, 2006, or Horridge
et al., 2009 as a way to find the justifications for an inconsistency) have exploited Reiters
algorithms in order to implement their frameworks. Among others, we plan to study how
adequate this techniques are for our operators, as there is an almost direct relation between
minimal incision functions and Reiters minimal hitting sets; in this way, it may be possible to adapt Reiter techniques to attend incoherences and inconsistencies, moreover, as we
already discussed, we plan to analyze the relation between cluster incision functions and
preference relations. Regarding implementation, we hold the conjecture that such relations
can be exploited to further refine the implementation of the operators: Reiters algorithm
is based on the expansion of a directed acyclic graph, and such expansion is made in a
breadth first fashion, which in the end generates all possible values for minimal incision
functions. As acknowledged by Wassermann, if some kind of ordering among the formulas
is present, this ordering can be used to choose which branch to expand; in other words, not
only it may be possible to implement the construction for operators proposed in this work
by means of exploiting Reiters hitting sets algorithm, but also we can use the preference
relation equivalent to the incision (if any) to guide the consolidation process. That is, it
may be possible to adapt the algorithm so it chooses to expand the branch that has the less
preferred set of formulas, thus guiding the graph expansion process.

Appendix A. Proofs
Proof for Proposition 1
644

fiDatalog  Ontology Consolidation

Proof Consider some U  T such that U is an unsatisfiable set of dependencies w.r.t.
NC  E , and A  D a set of atoms relevant to U .
It follows from the definition of satisfiability of a set of dependencies w.r.t. a set of
constraints that if U is unsatisfiable then there does not exist a relevant set of atoms A0
that makes mods(A0 , U  E  NC ) 6= , because otherwise U is satisfiable.
Then, mods(A, U  E  NC ) = . Moreover, since A  D and U  T we have that
chase(A, U )  chase(D, T ), and thus any NC or EGD that is violated in chase(A, U ) is
also violated in chase(D, T ). Thus, mods(D, T  E  NC ) = , i.e., KB is inconsistent.
Proof for Lemma 1
Proof Let KB 1 = (D1 , 1 ) with 1 = 1T  1E  1NC , and KB 2 = Q
(, 2 ) with

2 = 2T  2E  2NC be twoQDatalog ontologies such that 1 = 2 , KB 1 be the
dependency kernels of KB 1 , and KB 2 be the dependency kernels of KB 2 , respectively.
Q
Consider any X  KB 1 . Then, by Definition 6 we have that X  1T is an unsatisfiable
set of dependencies w.r.t. 1E  1NC and every X 0 ( X is satisfiable w.r.t. 1E  1NC .
Since 1 = 2 , then 1T = 2T , 1E = 2E and 1NC = 2NC , and thus it holds that
X  2T is an unsatisfiable set of dependencies w.r.t. 2E  2NC and every X 0 ( X is
satisfiable w.r.t. 2E  2NC .
Q
Then,Q
by Definition 6 we Q
have that
Q X  KB 2 , and since this holds for any arbitrary
kernel in KB 1 we have that KB 1 = KB 2 .
Proof for Proposition 2
Proof We will focus on the case of dependency clusters, omitting theQproof for data
clusters, as they are analogous to each other. Consider any arbitrary Y  KB .
) We begin by showing that if a kernel
then it is not
Q
Q part of any
Q
Qis part of a cluster
0
0
other cluster, i.e., if Y  X for some X  KB then Y * X for all X  KB such that
X 6= X 0 .
S
This is obtained directly from the definition of clusters: we have that X Q
= Y [] Y
where [] is an equivalence class in the equivalence relation  obtained from KB . Then,
clearly for Y we have that if Y  X then Y  []. Therefore, since by definition two
equivalence
classes are either equal or disjoint then it holds that Y 
/ [0 ] for all [0 ]. Let
S
0
0
0
0
X = Y 0 [0 ] Y . Then it holds that X 6= X and that Y * X . Since this holds for any
Q
Q
arbitrary equivalence
class [0 ] then it holds that if Y  X for some X  KB then Y * X 0
Q
Q
for all X 0  KB such that X 6= X 0 .
) Now we show that
to a cluster, i.e.,
Q
Q there not exist any0 kernel that does not belong
Q
Q
0
if Y * X for all X  KB such that X 6= X then Y  X for X  KB . Again,Q
this arise
Q
from the use of equivalence classes in Definitions 9 and 10. If Y * X 0 for all X 0  KB such
that X 6= X 0 , then it holds that Y 
/ [0 ] for all [0 ] 6= []. So,
Ssince equivalence classes form0
a partition it must holds that Y  []. Therefore, as X = Y [] Y we have that Y * X
Q
Q
for all X 0  KB such that X 6= X 0 then Y  X.
Proof for Corollary 1
645

fiDeagustini, Martinez, Falappa & Simari

Q
Proof Consider
  Y for some Y  KB . By Q
Proposition
2 we have that Y  X for
Q
Q
Q
0 for all X 0 
some X  KB if and onlyQ
if
Y
*
X
such
that
X 6= X 0 . Thus, we have
KB
Q
Q
Q
that   X for some X  KB if and only if  
/ X 0 for all X 0  KB such that X 6= X 0 .
`
0
0 
Analogously,
we
can
show
that


Y
for
some
Y
is such that   X for some
KB
`
`
`
`
0
0
X  KB if and only if  
/ X for all X  KB such that X 6= X 0 .
Proof for Lemma 2
Q
Proof Consider any X  KB 1 . Then, X is a minimal unsatisfiable set of TGDs w.r.t.
1NC  1E . Since KB 1 = KB 2 , then it holds that X  KB 2 , 1E = 2E , 1NC = 2NC and
X is an unsatisfiable set of TGDs w.r.t. 2NC  2E . Also, there does not exist X 0 ( X such
that X 0 is an unsatisfiable set
contradict
Q of TGDs w.r.t. 2NC 2E , since otherwise we wouldQ
our hypothesis that X  KB 1 , as 1NC
E . Then,
Q = 2NC and 1E = 2Q
QX  KB 2 ; and
since this holds for any arbitrary X  KB 1 , then we have that KB 1 = KB 2 .
Q
Q
Q
Consider
any arbitrary X, Y  KB 1 such that XY . Since
= KB 2 , then
KB
1
Q
Q
Q
Q
Q


X, Y  KB 2 . Thus, Q
is equivalent to Q
, and then KB 1 = KB 2 .
KB 1
KB 2
`
`
`
0 Y 0 . Since
0, Y 0 
=
such
that
X
Likewise, consider
any
arbitrary
X
KB
KB
1 `
1
` KB 2 ,
`
`
`


0
0
`
`
, and thus KB 1 = KB 2 .
is equivalent to 
then X , Y  KB 2 . Therefore, 
KB 2

KB 1

Proof for Proposition 3
Q
Proof Consider   D and X  KB such that  is relevant to X. From Definition 6
we have that X is unsatisfiable w.r.t. N  E  NC , and then from Definition 4 and the
fact that  is relevant to X we have that mods({}, X  N ) =  (1). Also, since {} is
singleton then the only A ( {} is A = , and clearly
mods(, X  N ) 6=  (2). Then, from
`
(1), (2)`
`and Definition 7 it follows that {}  KB . Also, from Definition 9 we have that
{}  KB , since {} cannot overlap with any other kernel, being singleton.
Consider the incision over {}. From Definition 11 it follows that (KB )  {} 6= .
Then, we have that (KB )  {} = , and thus   (KB ).
Proof for Corollary 2
Q
Proof Consider any arbitrary   D. Since  is relevant to some X  KB , then by
Proposition 3 it holds that   (KB ). Thus, since this holds for any arbitrary   D we
have that D  (KB ).
Proof for Theorem 1
Proof Let KB 1 = (D1 , 1 ) and KB 2 = (D2 , 2 ) be two Datalog  ontologies such that
KB 1 = KB 2 .
) Construction to postulates
Consider an operator ! defined as in Definition 14; we have to prove that ! satisfies every
postulate in Theorem 1. Let KB 1 ! = (D1 !, 1 !) and KB 2 ! = (D2 !, 2 !) be the two Datalog 
ontologies resulting from the consolidation of KB 1 and KB 2 by means of !, respectively.
be the ontology
Furthermore, let KB ?1 = (D1 , 1 \ (KB 1 )) and KB ?2 = (D2 , 2 \ (KB 2 )) Q
`
resulting from removing the TGDs selected by  from KB 1 and KB 2 . Let KB 1 and KB ?
1

646

fiDatalog  Ontology Consolidation

Q
`
be the set of dependency and data kernels for KB 1 and KB ?1 respectively, KB 2 and KB ?
Q
Q
`
` 2
be the sets of dependency and data kernels for KB 2 and KB ?2 . Finally, let KB 1 and KB ?
Q
Q
`
` 1
be the set of dependency and data clusters for KB 1 and KB ?1 respectively, KB 2 and KB ?
2
be the sets of dependency and data clusters for KB 2 and KB ?2 .
 Inclusion: 1 !  1 and D1 !  D1 .
By definition of KB 1 ! we have that D1 ! = D1 \ %(KB ?1 ), and thus D1 !  D1 .
In a similar way, by definition of KB 1 ! we have that 1 ! = 1 \ (KB 1 ), and thus
1 !  1 .
 Coherence: KB 1 ! is coherent.
To prove that KB 1 ! is coherent we have to show that T  1 ! is satisfiable for
E  NC  1 !. To do this it is sufficient to show that all minimal conflicts are
attended to by the operator, i.e., that no dependency kernel is included in 1 !,
Q
exists
Consider
Q
Q any arbitrary X  KB 1 . From Proposition 2 we have that there Q
Q
Y 
such that X  Y . By definition of  it holds that for all Y 
KBQ
KB 1
1
and X  KB 1 where X  Y it holds that ((KB 1 )  X) 6= . Then, there exists
some   X such that   ((KB 1 )  X), and thus  
/ 1 !. Therefore,
X * 1 !,
Q
i.e., the conflict was solved. Since this holds for any arbitrary X  KB 1 then every
unsatisfiable set in 1 is not included in 1 !, and thus T  1 ! is satisfiable for
E  NC  1 !, i.e., KB 1 ! is coherent.
 Consistency: Proof is analogous to that for Coherence.
 Minimality: If KB 0  KB 1 is coherent and consistent, then it holds that KB 1 ! 6
KB 0 .
0
LetQ
KB 0 is coherent and consistent, and let CF 1 = 1 \
S`
`
S
QKB  KB 1 be such that
( KB ) and CF D1 = D1 \ ( KB ) be the set of formulas that do not belong to any
kernel in 1 and D1 , respectively.
0
Suppose
by reductio that KB 1 ! SKB
`
` . By definition of KB 1 ! we have that (KB 1 ) 
S
Q
Q
( KB ) , and that %(KB 1 )  ( KB ). Then, CF 1  KB 1 ! and CF D1  KB 1 !.
Therefore, we have that CF 1  KB 0 and that CF D1  KB 0 .
Q
Q
`
`
Then, since KB 1 !  KB 0 there must exist   KB  KB such that   KB 0 but

/ KB 1 !, while KB 0 is coherent and consistent all the same. That is, there exists a
dependency cluster or a data cluster where the removal is not optimal, since  could
be included in the consolidation. For the rest of the proof and for simplicity reasons,
we consider the case where  belongs to a dependency cluster. This is made without
loss of generality, since the proof for the case where  is included in a data cluster is
analogous to the one presented here.
Q
Q
Let us consider then
such that   KB 0 . By Corollary 1 we have that
KB
Q
Q 
  X where X  KB . Let T = (X  (KB )) be the incision performed over the
cluster, and let R = (X  {KB \ KB 0 }) be those formulas removed from XQwhen
obtaining KB 0 . Clearly, since KB 0 is coherent then for all Y  X where Y  KB it

647

fiDeagustini, Martinez, Falappa & Simari

holds that R  Y 6= , because otherwise
Y  KB 0 , which will make KB 0 incoherent.
Q
Q
Besides, since R  Y then R  KB , and thus R satisfies the first two conditions in
Definition 12.
By Definition 12 we have that T is such that there not exists a set of TGDs that
satisfies the first two conditions in the definition and at the same time it holds that
(1) T  R.
Since  
/ KB 1 ! and   X then   (KB ), and thus   T . However, we know that
  X and   KB 0 , and thus  
/ R. Therefore we have that (2) T 6 R.
From (1) and (2) we have that T  R and that T 6 R, an absurd coming from our
original assumption that KB 1 !  KB 0 , and it holds that if KB 0  KB 1 is coherent
and consistent then KB 1 ! 6 KB 0 .
) Postulates to Construction
For the second part of the proof, consider an operator ! that satisfies all postulates in
Theorem 1. Let (!) be a function based on ! defined as follows:
Q
Q
(!) (KB 1 ) = {x | x  X for some X  KB 1 and x 
/ {1  KB 1 !}}
Let KB ?1 = (D1 , 1 \ (!) (KB 1 )) be the ontology resulting of removing from KB 1 the
TGDs selected by (!) . Then, let %(!)D be another function based on ! defined as follows:
`
`
/ {D1  KB 1 !}}
%(!)D (KB ?1 ) = {x | x  X for some X  KB ? and x 
1

Based on %(!)D and (!) we define a new operator as follows:
KB 1 !0 = (D1 \ %(!)D (KB ?1 ), 1 \ (!) (KB 1 ))
We have to show that !0 is a Datalog  ontology consolidation operator based on Cluster
Contraction. To do this, we first prove that %(!)D is a well-defined data incision function
and that (!) is a well-defined constraint incision function. That is, given (!) we have to
prove that:
- (!) is well-defined, i.e., if KB 1 = KB 2 , then (!) (KB 1 ) = (!) (KB 2 ).
Q
Q
By definition of (!) we have that (!) (KB 1 ) = {x | x  X for some X 
KB 1
and x 
/ 1  KB 1 !}.
Consider
arbitrary
x  (!) (KB 1 ). Since KB 1 = KBQ
, then by Lemma 2 we have
2Q
Q
Q any Q
Q
that KBQ
=
.
Since
x


(KB
),
then
x

X

, and thus it holds that
1
(!)
KB 2
KB 1

1
Q
x  X  KB 2 (1).
Q
Q
Besides, since x  X  KB 1 then x  1 . Thus, since x 
/ 1  KB 1 !, then x 
/ KB 1 !.
Since KB 1 = KB 2 , from the fact that ! is a function we have that KB 1 ! = KB 2 !, and
then it also holds that x 
/ KB 2 !. Thus, x 
/ 2  KB 2 !(2).
From (1) and (2)Q
Qit follows that for any x  (!) (KB 1 ) it holds that x  {y | y 
Y for some Y 
and y 
/ 2  KB 2 !}. By definition of (!) this is (!) (KB 2 ),
KB 2
and thus we have that if KB 1 = KB 2 , then (!) (KB 1 ) = (!) (KB 2 ).
648

fiDatalog  Ontology Consolidation

- (!) (KB 1 ) 

SQ
Q
( KB 1 ).

This follows directly from Q
the definition of (!) , since for every x  (!) (KB 1 ) it holds
Q
that x  X for some X  KB 1 because of the first condition in the definition.
- If X 

Q
Q

eY 

Q

are such that Y 6=  and Y  X, then (Y  (!) (KB 1 )) 6= .
Q
Q
Q
Suppose by reductio
that there exists some X  KB 1 and Y  KB 1 such that Y 6= ,
T
Y  X and (Y (!) (KB 1 )) = .
Q
Q
Then, for all   Y it holds that  
/ (!) (KB
),
i.e.,


/
X

or   {1 KB 1 !}.
1
KB 1
Q
By our hypothesis we have that   Y  KB 1 and Y  X. Thus   X, and therefore
it must hold that   {1  KB 1 !}, and by extension   KB 1 !.
KB 1

KB 1

Since this holds for any arbitrary   Y then we have that Y  1 !. From Definition 6
it holds that Y is a minimal unsatisfiable set of TGDs w.r.t. E  NC  1 . Then,
for any relevant set of atoms A it holds that mods(A, Y  E  NC ) = . Then, since
Y  1 ! then for any relevant set A0 it holds that mods(A0 , 1 !  E  NC ) = , because
the TGDs in Y are triggered by A0 . Then, 1 ! is an unsatisfiable set of TGDs w.r.t.
E  NC  1 .
However, from Coherence we have that KB 1 ! is coherent, and thus 1 ! is satisfiable
w.r.t. E  NC  1 .
Then we have that 1 ! is satisfiable w.r.t. E  NC  1 and 1 ! is unsatisfiable
w.r.t. E Q
NC  1 , an absurd
coming from our initial supposition
Q
Q
T that there exists
some X  KB 1 and Y  KB
such
that
Y
=
6
,
Y

X
and
(Y
(!) (KB 1 )) = ,
1
Q
Q
Q
andTit holds that for all X  KB 1 and Y  KB 1 such that Y  X, if Y 6=  then
(Y (!) (KB 1 )) 6= .
Q
Q
it holds that T = (X  (!) (KB 1 )) is such that there not exists
- For all X 
KB 1
R  X where R satisfies the two previous conditions and R ( T .
To prove this is sufficient to show that, being clusters disjoint sets, the election in each
cluster is optimal, because otherwise if should exists any cluster where the incision
function does not choose in an optimal way then
Q
Q Minimality would not be satisfied.
So, suppose by reductio that there exists X  KB 1 where T = (X  (!) (KB 1 )) is such
that there does exist R  X where R satisfies the two previous conditions and R ( T .
Q
Q
Let us consider KB 0 = (0 , D0 ) such that for all Y  KB 1 such that Y 6= X it holds
that T 0 = (Y  (!) (KB 1 )) and R0 = (Y  {KB \ KB 0 }) (those formulas removed from
Y when obtaining KB 0 ) are such that T 0 = R0 . Since T 0 = R0 then R0 is Q
such
Q that
the two conditions
in
Definition
12
are
satisfied.
Besides,
let
CF
=

\
and
1
1
KB
`
`
CF D1 = D1 \ KB be the set of formulas that do not belong to any kernel in 1 and
D1 , respectively; and let KB 0 be such that CF 1  0 and CF D1  D0 .
The fact that every formula that is not in any conflict belongs to KB 0 and that KB 0 is
built in such a way that the election in each cluster different than X is the same both
in KB 0 and (!) (KB 1 ) makes that KB 0 \ (X  {KB \ KB 0 }) = KB 1 ! \ (X  (!) (KB 1 )).
This is, if there is any difference between KB 0 and KB 1 ! that difference arise from the
election on which formulas to remove from X.
649

fiDeagustini, Martinez, Falappa & Simari

Finally, from our supposition we have that there exists R  X where R satisfies the two
previous conditions and R ( T . Let KB 0 and R ( T be such that R = (X {KB \KB 0 })
is the set of formulas removed from X when obtaining KB 0 . Then, we have that KB 0 is
coherent and consistent, since every conflict in clusters in KB 1 where solved, whether by
removing R (for cluster X) or the sets R0 (for every cluster different than X). Besides,
since we have that KB 0 \ (X  {KB \ KB 0 }) = KB 1 ! \ (X S(!) (KB 1 )) and that R ( T ,
Q
then for KB 1 ! = KB 1 \ (!) (KB 1 ) and KB 0 = KB 1 \ { Y {Q
R0  R} where
\X}
KB 1

(R0 = Y  {KB \ KB 0 }) and R = (X  {KB \ KB 0 }) it holds that KB 1 !  KB 0 (1).
That is, if all formulas that are not involved in conflicts belong to both KB 1 ! and KB 0 ,
in each cluster different than X the same formulas are removed, and the set of formulas
removed from X to obtain KB 0 are an strict subset of those removed by (!) (KB 1 ) to
obtain KB 1 !, then KB 1 ! is an strict subset of KB 0 , i.e., we have removed more formulas
by deleting T than by deleting R.
On the other hand, since KB 0 is coherent and consistent, then by Minimality we have
that KB 1 ! 6 KB 0 (2).
0
Therefore, from (1) and (2) we have that KB 1 !  KB 0 and KB
Q
Q 1 ! 6 KB , and absurd
where T = (X 
coming from our initial supposition that there exists X 
KB 1
(!) (KB 1 )) is such that there exists R  X where
R
satisfies
the
two
previous
conditions
Q
Q
and R ( T , and we have that for all X 
it
holds
that
T
=
(X


(!) (KB 1 ))
KB 1
is such that there not exists R  X where R satisfies the two previous conditions and
R ( T.
We omit the proof that %(!)D is a well-defined data incision function using Consistency
and Minimality since it is analogous to the proof that (!) is a well-defined constraint
incision function using Coherence and Minimality.
Now that we have shown that %(!)D and (!) are well-defined data incision functions
and constraint incision functions, respectively, to conclude this second part of the proof
we have to show that !0 coincides with !. From Inclusion it follows that D1 !  D1 and
1 !  1 (1). Also, from our definition of %(!)D it follows that %(!)D (KB ?1 ) = D1 \ D1 !,
and from our definition of (!) it follows that (!) (KB 1 ) = 1 \ 1 ! (2). Then, from
(1) and (2) we have that D1 ! = D1 \ %(!)D (KB ?1 ) and 1 ! = 1 \ (!) (KB 1 ). Thus, ! =
(D1 \ %(!)D (KB ?1 ), 1 \ (!) (KB 1 )), and therefore !0 coincides with !.

References
Alchourron, C., Gardenfors, P., & Makinson, D. (1985). On the logic of theory change:
Partial meet contraction and revision functions. Journal of Symbolic Logic, 50 (2),
510530.
Alchourron, C., & Makinson, D. (1981). Hierarchies of Regulation and Their Logic. New
Studies in Deontic Logic, 125148.
Alchourron, C., & Makinson, D. (1985). On the Logic of Theory Change: Safe Contraction.
Studia Logica, 44, 405422.
Amgoud, L., & Kaci, S. (2005). An argumentation framework for merging conflicting knowledge bases: The prioritized case. In Proc. of 8th European Conferences on Symbolic
650

fiDatalog  Ontology Consolidation

and Quantitative Approaches to Reasoning with Uncertainty (ECSQUARU 05), pp.
527538.
Andritsos, P., Fuxman, A., & Miller, R. J. (2006). Clean answers over dirty databases: A
probabilistic approach. In Proc. of 22nd International Conference on Data Engineering (ICDE 06), p. 30.
Arenas, M., Bertossi, L. E., & Chomicki, J. (1999). Consistent query answers in inconsistent databases. In Proc. of 18th ACM SIGACT-SIGMOD-SIGART Symposium on
Principles of Database Systems (PODS 99), pp. 6879.
Arieli, O., Denecker, M., & Bruynooghe, M. (2007). Distance semantics for database repair.
Annals of Mathematics and Artificial Intelligence, 50 (3-4), 389415.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing the EL envelope. In Proc. of the 19th
International Joint Conference on Artificial Intelligence (IJCAI 05), pp. 364369.
Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2003). The Description Logic Handbook: Theory, Implementation, and Applications.
Cambridge University Press.
Baral, C., Kraus, S., & Minker, J. (1991). Combining multiple knowledge bases. Transactions on Knowledge and Data Engineering, 3 (2), 208220.
Bell, D. A., Qi, G., & Liu, W. (2007). Approaches to inconsistency handling in descriptionlogic based ontologies. In Proc. of On the Move to Meaningful Internet Systems
(OTM) Workshops (2), pp. 13031311.
Beneventano, D., & Bergamaschi, S. (1997). Incoherence and subsumption for recursive
views and queries in object-oriented data models. Data and Knowledge Engineering,
21 (3), 217252.
Berners-Lee, T., Hendler, J., & Lassila, O. (2001). The semantic web. Scientific American,
284(5):3443.
Bienvenu, M., & Rosati, R. (2013). Tractable approximations of consistent query answering
for robust ontology-based data access. In Proc. of 23rd International Joint Conference
on Artificial Intelligence (IJCAI 13), pp. 775781.
Black, E., Hunter, A., & Pan, J. Z. (2009). An argument-based approach to using multiple ontologies. In Proc. of 3rd International Conference on Scalable Uncertainty
Management (SUM 09), pp. 6879.
Bohannon, P., Flaster, M., Fan, W., & Rastogi, R. (2005). A cost-based model and effective heuristic for repairing constraints by value modification. In Proc. of 24th ACM
SIGMOD International Conference on Management of Data / Principles of Database
Systems (PODS 05), pp. 143154.
Booth, R., Meyer, T. A., Varzinczak, I. J., & Wassermann, R. (2010). Horn belief change:
A contraction core. In Proc. of 19th European Conference on Artificial Intelligence
(ECAI 10), pp. 10651066.
Borgida, A. (1995). Description logics in data management. Transactions on Knowledge
and Data Engineering, 7 (5), 671682.
651

fiDeagustini, Martinez, Falappa & Simari

Brandt, S. (2004). Polynomial time reasoning in a description logic with existential restrictions, GCI axioms, and - what else?. In Proc. of 16th European Conference on
Artificial Intelligence (ECAI 04), pp. 298302.
Cal, A., Gottlob, G., & Kifer, M. (2008). Taming the infinite chase: Query answering under
expressive relational constraints. In Brewka, G., & Lang, J. (Eds.), Proc. of 11th
International Conference on Principles of Knowledge Representation and Reasoning
(KR 08), pp. 7080. AAAI Press.
Cal, A., Gottlob, G., & Kifer, M. (2013). Taming the infinite chase: Query answering
under expressive relational constraints. Journal of Artificial Intelligence Research, 48,
115174.
Cal, A., Gottlob, G., & Lukasiewicz, T. (2012). A general Datalog-based framework for
tractable query answering over ontologies. Journal of Web Semantic, 14, 5783.
Cal, A., Lembo, D., & Rosati, R. (2003). On the decidability and complexity of query answering over inconsistent and incomplete databases. In Proc. of 22nd ACM SIGMOD
Symposium on Principles of database systems (PODS 03), pp. 260271. ACM.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2005). DL-Lite:
Tractable description logics for ontologies. In AAAI, pp. 602607.
Caroprese, L., Greco, S., & Zumpano, E. (2009). Active integrity constraints for database
consistency maintenance. Transactions on Knowledge and Data Engineering, 21 (7),
10421058.
Caroprese, L., & Truszczynski, M. (2011). Active integrity constraints and revision programming. Theory and Practice of Logic Programming, 11 (6), 905952.
Cecchi, L., Fillottrani, P., & Simari, G. R. (2006). On the Complexity of DeLP through
Game Semantics. In Dix, J., & Hunter, A. (Eds.), Proc. of 11th International Workshop on Non-Monotonic Reasoning (NMR 06), pp. 386394.
Cholvy, L. (1998). Reasoning about merged information. In Belief Change, Vol. 3, pp.
233263. Springer Netherlands.
Croitoru, M., & Rodriguez, R. O. (2015). Using kernel consolidation for query answering in
inconsistent OBDA. In Proc. of the Joint Ontology Workshops 2015 Episode 1: The
Argentine Winter of Ontology.
Delgrande, J. P. (2011). Revising by an inconsistent set of formulas. In Proc. of 22nd
International Joint Conference on Artificial Intelligence (IJCAI 11), pp. 833838.
Delgrande, J. P., Dubois, D., & Lang, J. (2006). Iterated revision as prioritized merging.
In Proc. of 10th International Conference on Principles of Knowledge Representation
and Reasoning (KR 06), pp. 210220.
Delgrande, J. P., & Jin, Y. (2012). Parallel belief revision: Revising by sets of formulas.
Artificial Intelligence, 176 (1), 22232245.
Delgrande, J. P., Schaub, T., Tompits, H., & Woltran, S. (2009). Merging logic programs
under answer set semantics. In Proc. of 25th International Conference on Logic Programming (ICLP 09), pp. 160174.
652

fiDatalog  Ontology Consolidation

Dunne, P., & Wooldridge, M. (2009). Argumentation in Artificial Intelligence, chap. Complexity of Abstract Argumentation, pp. 85104. Springer.
Everaere, P., Konieczny, S., & Marquis, P. (2008). Conflict-based merging operators. In
Proc. of 11th International Conference on Principles of Knowledge Representation
and Reasoning (KR 08), pp. 348357.
Falappa, M. A., Kern-Isberner, G., Reis, M. D. L., & Simari, G. R. (2012). Prioritized and
non-prioritized multiple change on belief bases. Journal of Philosophical Logic, 41 (1),
77113.
Falappa, M. A., Kern-Isberner, G., & Simari, G. R. (2002). Belief Revision, Explanations
and Defeasible Reasoning. Artificial Intelligence, 141, 128.
Flouris, G., Huang, Z., Pan, J. Z., Plexousakis, D., & Wache, H. (2006). Inconsistencies,
negations and changes in ontologies. In Proc. of 21st National Conference on Artificial
Intelligence (AAAI 06), pp. 12951300.
Friedman, N., & Halpern, J. Y. (2001). Belief revision: A critique. Computer Research
Repository (CoRR), cs.AI/0103020.
Fuhrmann, A. (1991). Theory contraction through base contraction. Journal of Philosophical Logic, 20, 175203.
Gardenfors, P. (1982). Rule for rational changes of belief. Philosophical Essay Dediccated
To Lennart Aqvist on his Fiftieth Birthday, 88101.
Gardenfors, P. (1988). Knowledge in Flux: Modeling the dynamics of epistemic states. MIT
Press.
Gelfond, M. (2008). Answer sets. In Handbook of Knowledge Representation, chap. 7, pp.
285316. Elsevier.
Gomez, S. A., Chesnevar, C. I., & Simari, G. R. (2010). Reasoning with inconsistent
ontologies through argumentation. Applied Artificial Intelligence, 24 (1&2), 102148.
Greco, S., & Molinaro, C. (2012). Probabilistic query answering over inconsistent databases.
Annals of Mathematics and Artificial Intelligence (AMAI), 64 (2-3), 185207.
Haase, P., van Harmelen, F., Huang, Z., Stuckenschmidt, H., & Sure, Y. (2005). A framework for handling inconsistency in changing ontologies. In Proc. of 4th International
Semantic Web Conference (ISWC 05), pp. 353367.
Halaschek-Wiener, C., & Katz, Y. (2006). Belief base revision for expressive description
logics. In Proc. of International Workshop on OWL: Experiences and Directions
(OWLED 06).
Hansson, S. O. (1991). Belief Base Dynamics. Ph.D. thesis, Uppsala University, Department
of Philosophy, Uppsala, Sweden.
Hansson, S. O. (1993). Theory contraction and base contraction unified. Journal of Symbolic
Logic, 58 (2), 602625.
Hansson, S. O. (1994). Kernel contraction. Journal of Symbolic Logic, 59 (3), 845859.
Hansson, S. O. (1997). Semi-revision. Journal of Applied Non-Classical Logics, 7 (1-2),
151175.
653

fiDeagustini, Martinez, Falappa & Simari

Hansson, S. O. (2001). A Textbook of Belief Dynamics: Solutions to Exercises. Kluwer
Academic Publishers, Norwell, MA, USA.
Harman, G. (2008). Change in view: Principles of reasoning. Cambridge University Press.
Harper, W. (1975). Rational Belief Change, Popper Functions and Counterfactuals. Synthese, 30, 221262.
Horridge, M., Parsia, B., & Sattler, U. (2009). Explaining inconsistencies in OWL ontologies.
In Scalable Uncertainty Management, pp. 124137. Springer.
Huang, Z., van Harmelen, F., & ten Teije, A. (2005). Reasoning with inconsistent ontologies.
In Proc. of 19th International Joint Conference on Artificial Intelligence (IJCAI 05),
pp. 454459.
Hue, J., Papini, O., & Wurbel, E. (2009). Merging belief bases represented by logic programs. In Proc. of 10th European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU 09), pp. 371382.
Kalyanpur, A., Parsia, B., Horridge, M., & Sirin, E. (2007). Finding all justifications of
OWL DL entailments. Springer.
Kalyanpur, A., Parsia, B., Sirin, E., & Hendler, J. A. (2005). Debugging unsatisfiable classes
in owl ontologies. Web Semantics: Science, Services and Agents on the World Wide
Web, 3 (4), 268293.
Katsuno, H., & Mendelzon, A. O. (1991). On the difference between updating a knowledge base and revising it. In Proc. of 2nd International Conference on Principles of
Knowledge Representation and Reasoning (KR91), pp. 387394.
Katsuno, H., & Mendelzon, A. O. (1992). Propositional knowledge base revision and minimal change. Artificial Intelligence, 52 (3), 263294.
Konieczny, S., & Perez, R. P. (2002). Merging information under constraints: A logical
framework. Journal of Logic and Computation, 12 (5), 773808.
Konieczny, S., & Perez, R. P. (2011). Logic based merging. Journal of Philosophical Logic,
40 (2), 239270.
Lembo, D., Lenzerini, M., Rosati, R., Ruzzi, M., & Savo, D. F. (2010). Inconsistencytolerant semantics for description logics. In Proc. of 4th International Conference on
Web Reasoning and Rule Systems (RR 10), pp. 103117.
Lenzerini, M. (2002). Data integration: A theoretical perspective. In Proc, of 21st ACM
SIGMOD Symposium on Principles of Database Systems (PODS 02), pp. 233246.
Levi, I. (1977). Subjunctives, Dispositions and Chances. Synthese, 34 (4), 423455.
Liberatore, P., & Schaerf, M. (1998). Arbitration (or how to merge knowledge bases).
Knowledge and Data Engineering, 10 (1), 7690.
Lin, J., & Mendelzon, A. O. (1998). Merging databases under constraints. International
Journal of Cooperative Information Systems, 7 (1), 5576.
Lin, J., & Mendelzon, A. O. (1999). Knowledge base merging by majority. Applied Logic
Series, 18, 195218.
654

fiDatalog  Ontology Consolidation

Lloyd, J. W. (1987). Foundations of Logic Programmming. Springer-Verlag.
Lukasiewicz, T., Martinez, M. V., & Simari, G. I. (2012). Inconsistency handling in
datalog+/- ontologies. In Proc. of 20th European Conference on Artificial Intelligence (ECAI 12), pp. 558563.
Martinez, M., Pugliese, A., Simari, G., Subrahmanian, V., & Prade, H. (2007). How dirty
is your relational database? an axiomatic approach. In Mellouli, K. (Ed.), Proc. of
9th European Conference on Symbolic and Quantitative Approaches to Reasoning with
Uncertainty (ECSQARU 07), Vol. 4724 of Lecture Notes in Computer Science, pp.
103114. Springer.
Meyer, T., Lee, K., & Booth, R. (2005). Knowledge integration for description logics. In
Veloso, M., & Kambhampati, S. (Eds.), Proceedings of AAAI05, Twentieth National
Conference on Artificial Intelligence, pp. 645650. AAAI Press.
Newell, A. (1982). The Knowledge Level. Artificial Intelligence, 18, 87127.
Nilsson, U., & Maluszynski, J. (1995). Logic, Programming and Prolog (2ed). John Wiley
& Sons Ltd.
Parsons, S., Wooldridge, M., & Amgoud, L. (2003). Properties and complexity of some
formal inter-agent dialogues. Journal of Logic and Computation, 13 (3), 347376.
Qi, G., & Hunter, A. (2007). Measuring incoherence in description logic-based ontologies.
In Proc. of 6th International Semantic Web Conference and the 2nd Asian Semantic
Web Conference (ISWC/ASWC 07), pp. 381394.
Qi, G., Liu, W., & Bell, D. A. (2006). Knowledge base revision in description logics. In
Proc. of 10th European Conference in Logics in Artificial Intelligence (JELIA 06),
pp. 386398.
Quine, W. V. O. (1986). Philosophy of logic. Harvard University Press.
Reiter, R. (1987). A theory of diagnosis from first principles. Artificial Intelligence, 32(1),
5795.
Rosati, R. (2011). On the complexity of dealing with inconsistency in description logic
ontologies. In Proc. of International Joint Conference on Artificial Intelligence (IJCAI
11), pp. 10571062.
Rott, H. (1992). Modellings for belief change: Prioritization and entrenchment. Theoria,
58 (1), 2157.
Schlobach, S., & Cornet, R. (2003). Non-standard reasoning services for the debugging of
description logic terminologies.. In Proceedings of the Eighteenth International Joint
Conference on Artificial Intelligence (IJCAI 03), pp. 355362.
Schlobach, S., Huang, Z., Cornet, R., & van Harmelen, F. (2007). Debugging incoherent
terminologies. Journal of Automated Reasoning, 39 (3), 317349.
Staworko, S., Chomicki, J., & Marcinkowski, J. (2012). Prioritized repairing and consistent query answering in relational databases. Annals of Mathematics and Artificial
Intelligence, 64 (2-3), 209246.
655

fiDeagustini, Martinez, Falappa & Simari

Turner, H. (2003). Strong equivalence made easy: nested expressions and weight constraints.
Theory and Practice of Logic Programming, 3 (4-5), 609622.
von Leibniz, G. W. F. (1976). Philosophical Papers and Letters: a selection, Vol. 1. Springer.
Wassermann, R. (2000). An algorithm for belief revision. In Proc. of International Conference on Principles of Knowledge Representation and Reasoning (KR 00), pp. 345
352.
Wijsen, J. (2005). Database repairing using updates. ACM Transaction on Database Systems, 30 (3), 722768.

656

fiJournal of Artificial Intelligence Research 56 (2016) 153-195

Submitted 03/15; published 06/16

Global Continuous Optimization with Error Bound
and Fast Convergence
Kenji Kawaguchi

kawaguch@mit.edu

Massachusetts Institute of Technology
Cambridge, MA, USA

Yu Maruyama

maruyama.yu@jaea.go.jp

Nuclear Safety Research Center
Japan Atomic Energy Agency
Tokai, Japan

Xiaoyu Zheng

zheng.xiaoyu@jaea.go.jp

Nuclear Safety Research Center
Japan Atomic Energy Agency
Tokai, Japan

Abstract
This paper considers global optimization with a black-box unknown objective function
that can be non-convex and non-differentiable. Such a difficult optimization problem arises
in many real-world applications, such as parameter tuning in machine learning, engineering
design problem, and planning with a complex physics simulator. This paper proposes a new
global optimization algorithm, called Locally Oriented Global Optimization (LOGO), to aim
for both fast convergence in practice and finite-time error bound in theory. The advantage
and usage of the new algorithm are illustrated via theoretical analysis and an experiment
conducted with 11 benchmark test functions. Further, we modify the LOGO algorithm
to specifically solve a planning problem via policy search with continuous state/action
space and long time horizon while maintaining its finite-time error bound. We apply the
proposed planning method to accident management of a nuclear power plant. The result
of the application study demonstrates the practical utility of our method.

1. Introduction
Optimization problems are prevalent and have held great importance throughout history
in engineering applications and scientific endeavors. For instance, many problems in the
field of artificial intelligence (AI) can be viewed as optimization problems. Accordingly,
generic local optimization methods, such as hill climbing and the gradient method, have
been successfully adopted to solve AI problems since early research on the topic (Kirk, 1970;
Gullapalli, Franklin, & Benbrahim, 1994; Deisenroth & Rasmussen, 2011). On the other
hand, the application of global optimization to AI problems has been studied much less
despite its practical importance. This is mainly due to the lack of necessary computational
power in the past and the absence of a practical global optimization method with a strong
theoretical basis. Of these two obstacles, the former is becoming less serious today, as
evidenced by a number of studies on global optimization in the past two decades (Horst
& Tuy, 1990; Ryoo & Sahinidis, 1996; He, Verstak, Watson, Stinson, et al., 2004; Rios &
Sahinidis, 2013). The aim of this paper is to partially address the latter obstacle.
c
2016
AI Access Foundation. All rights reserved.

fiKawaguchi, Maruyama, & Zheng

The inherent difficulty of the global optimization problem has led to two distinct research
directions: development of heuristics without theoretically guaranteed performance and
advancement of theoretically supported methods regardless of its difficulty. A degree of
practical success has resulted from heuristic approaches such as simulated annealing, genetic
algorithms (for a brief introduction on the context of AI, see Russell & Norvig, 2009), and
swarm-based optimization (for an interesting example of a recent study, see Daly & Shen,
2009). Although these methods are heuristics without strong theoretical supports, they
became very popular partly because their optimization mechanisms aesthetically mimic
natures physical or biological optimization mechanism.
On the other hand, the Lipschitzian approach to global optimization aims to accomplish the global optimization task in a theoretically supported manner. Despite its early
successes in theoretical viewpoints (Shubert, 1972; Mladineo, 1986; Pinter, 1986; Hansen,
Jaumard, & Lu, 1991), the early studies were based on an assumption that is impractical in
most applications: the Lipschitz constant, which is the bound of the slope on the objective
function, is known. The relaxation of this crucial assumption resulted in the well-known
DIRECT algorithm (Jones, Perttunen, & Stuckman, 1993) that has worked well in practice,
yet guarantees only consistency property. Recently, the Simultaneous Optimistic Optimization (SOO) algorithm (Munos, 2011) achieved the guarantee of a finite-time error bound
without knowledge of the Lipschitz constant. However, the practical performance of the
algorithm is unclear.
In this paper, we propose a generic global optimization algorithm that is aimed to achieve
both a satisfactory performance in practice and a finite-loss bound as the theoretical basis
without strong additional assumption1 (Section 2), and apply it to an AI planning problem
(Section 6). For AI planning problem, we aim at solving real-world engineering problem
with a long planning horizon and with continuous state/action space. As an illustration of
the advantage of our method, we present the preliminary results of an application study
conducted on accident management of a nuclear power plant as well. Note that the optimization problems discussed in this paper are practically relevant yet inherently difficult to
scale up for higher dimensions, i.e., NP-complete (Murty & Kabadi, 1987). Accordingly,
we discuss possible extensions of our algorithm for higher dimensional problems, with an
experimental illustration with a 1000-dimensional problem.

2. Global Optimization on Black-Box Function
The goal of global optimization is to solve the following very general problem:
maxx f (x)
subject to x  
where f is the objective function defined on the domain   RD . Since the performance of
our proposed algorithm is independent of the scale of , we consider the problem with the
rescaled domain 0 = [0, 1]D . Further, in this paper, we focus on a deterministic function f .
1. In this paper, we use the term strong additional assumption to indicate the assumption that the tight
Lipschitz constant is known and/or the main assumption of many Bayesian optimization methods that
the objective function is a sample from Gaussian process with some known kernel and hyperparameters.

154

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

For global optimization, the performance of an algorithm can be assessed by the loss rn ,
which is given by
rn = max0 f (x)  f (x+ (n)).
x

x+ (n)

Here,
is the best input vector found by the algorithm after n trials (more precisely,
we define n to denote the total number of divisions in the next section).
A minimal assumption that allows us to solve this problem is that the objective function f can be evaluated at all points of 0 in an arbitrary order. In most applications, this
assumption is easily satisfied, for example, by having a simulator of the world dynamics
or an experimental procedure that defines f itself. In the former case, x corresponds to
the input vector for a simulator f , and only the ability to arbitrarily change the input
and run the simulator satisfies the assumption. A possible additional assumption is that
the gradient of function f can be evaluated. Although this assumption may produce some
effective methods, it limits the applicability in terms of real-world applications. Therefore,
we assume the existence of a simulator or a method to evaluate f , but not an access to
the gradient of f . The methods in this scope are often said to be derivative-free and the
objective function is said to be a black-box function.
However, if no further assumption is made, this very general problem is proven to be
intractable. More specifically, any number of function evaluations cannot guarantee getting
close to the optimal (maximum) value of f (Dixon, 1978). This is because the solution may
exist in an arbitrary high and narrow peak, which makes it impossible to relate the optimal
solution to the evaluations of f at any other points.
One of the simplest additional assumptions to restore the tractability would be that the
slope of f is bounded. The form of this assumption studied the most is Lipschitz continuity
for f :
|f (x1 )  f (x2 )|  bkx1  x2 k, x1 , x2  0 ,
(1)
where b > 0 is a constant, called the Lipschitz constant, and k  k denotes the Euclidean
norm. The global optimization with this assumption is referred to as Lipschitz optimization, and has been studied for a long time. The best-known algorithm in the early days of
its history was the Shubert algorithm (Shubert, 1972), or equivalently the Piyavskii algorithm (Piyavskii, 1967) as the same algorithm was independently developed. Based on the
assumption that the Lipschitz constant is known, it creates an upper bound function over
the objective function and then chooses a point of 0 that has the highest upper bound at
each iteration. For problems with higher dimension D  2, finding the point with the highest upper bound becomes difficult and many algorithms have been proposed to tackle the
problem (Mayne & Polak, 1984; Mladineo, 1986). These algorithms successfully provided
finite-loss bounds.
However appealing from a theoretical point of view, a practical concern was soon raised
regarding the assumption that the Lipschitz constant is known. In many applications, such
as with a complex physics simulator as an objective function f , the Lipschitz constant is
indeed unknown. Some researchers aimed to relax this somewhat impractical assumption
by proposing procedures to estimate the Lipschitz constant during the optimization process
(Strongin, 1973; Kvasov, Pizzuti, & Sergeyev, 2003). Similarly, the Bayesian optimization
method with upper confidence bounds (Brochu, Cora, & de Freitas, 2009) estimates the objective function and its upper confidence bounds with a certain model assumption, avoiding
155

fiKawaguchi, Maruyama, & Zheng

the prior knowledge on the Lipschitz constant. Unfortunately, this approach, including
the Bayesian optimization method, results in mere heuristics unless the several additional
assumptions hold. The most notable of these assumptions are that an algorithm can maintain the overestimate of the upper bound and that finding the point with the highest upper
bound can be done in a timely manner. As noted by Hansen and Jaumard (1995), it is
unclear if this approach provides any advantage, considering that other successful heuristics
are already available. This argument still applies to this day to relatively recent algorithms
such as those by Kvasov et al. (2003) and Bubeck, Stoltz, and Yu (2011).
Instead of trying to estimate the unknown Lipschitz constant, the well-known DIRECT
algorithm (Jones et al., 1993) deals with the unknowns by simultaneously considering all
the possible Lipschitz constants, b: 0 < b < . Over the past decade, there have been many
successful applications of the DIRECT algorithm, even in large-scale engineering problems
(Carter, Gablonsky, Patrick, Kelly, & Eslinger, 2001; He et al., 2004; Zwolak, Tyson, &
Watson, 2005). Although it works well in many practical problems, the DIRECT algorithm
only guarantees consistency property, limn rn = 0 (Jones et al., 1993; Munos, 2013).
The SOO algorithm (Munos, 2011) expands the DIRECT algorithm and solves its major
issues, including its weak theoretical basis. That is, the SOO algorithm not only guarantees
the finite-time loss bound without knowledge of the slopes bound, but also employs a
weaker assumption. In contrast to the Lipschitz continuity assumption used by the DIRECT
algorithm (Equation (1)), the SOO algorithm only requires the local smoothness assumption
described below.

Assumption 1 (Local smoothness). The decreasing rate of the objective function f around
at least one global optimal solution {x  0 : f (x ) = supx0 f (x)} is bounded by a semimetric `, for any x  0 as
f (x )  f (x)  `(x, x ).

Here, semi-metric is a generalization of metric in that it does not have to satisfy the
triangle inequality. For instance, `(x, x ) = bkx  xk is a metric and a semi-metric. On
the other hand, whenever  > 1 or p < 1, `(x, x ) = bkx  xkp is not a metric but
only a semi-metric since it does not satisfy the triangle inequality. This assumption is much
weaker than the assumption described by Equation (1) for two reasons. First, Assumption 1
requires smoothness (or continuity) only at the global optima, while Equation (1) does so
for any points in the whole input domain, 0 . Second, while Lipschitz continuity assumption
in Equation (1) requires the smoothness to be defined by a metric, Assumption 1 allows
a semi-metric to be used. To the best of our knowledge, the SOO algorithm is the only
algorithm that provides a finite-loss bound with this very weak assumption.
Summarizing the above, while the DIRECT algorithm has been successful in practice,
concern about its weak theoretical basis led to the recent development of its generalized
version, the SOO algorithm. We further generalize the SOO algorithm to increase the
practicality and strengthen the theoretical basis at the same time. This paper adopts a very
weak assumption, Assumption 1, to maintain the generality and the wide applicability.
156

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

Iteration 1

Iteration 2

Iteration 4

(N = 9)

w=1

(N = 3)

Iteration 3

w=4

(N = 9)

Figure 1: Illustration of SOO (w = 1) and LOGO (w = 1 or 4) at the end of each iteration

3. Locally Oriented Global Optimization (LOGO) Algorithm
In this section, we modify the SOO algorithm (Munos, 2011) to accelerate the convergence
while guaranteeing theoretical loss bounds. The new algorithm with this modification,
the LOGO (Locally Oriented Global Optimization) algorithm, requires no additional assumption. To use the LOGO algorithm, one needs no prior knowledge of the objective
function f ; it may leverage prior knowledge if it is available. The algorithm uses two parameters, hmax (n) and w, as inputs where hmax (n)  [1, ) and w  Z+ . hmax (n) and w
act in part to balance the local and global search. hmax (n) biases the search towards a
global search whereas w orients the search toward the local area.
The case with w = 1 or 4 (top or bottom diagrams) in Figure 1 illustrates the functionality of the LOGO algorithm in a simple 2-dimensional objective function. In this view,
the LOGO algorithm is a generalization of the SOO algorithm with the local orientation
parameter w in that SOO is a special case of LOGO with a fixed parameter w = 1.
3.1 Predecessor: SOO Algorithm
Before we discuss our algorithm in detail, we briefly describe its direct predecessor, the
SOO algorithm2 (Munos, 2011). The top diagrams in Figure 1 (the scenario with w =
1) illustrates the functionality of the SOO algorithm in a simple 2-dimensional objective
function. As illustrated in Figure 1, the SOO algorithm employs hierarchical partitioning
to maintain hyperintervals, each center of which is the evaluation point of the objective
function f . That is, in Figure 1, each rectangle represents the hyperintervals at the end
of each iteration of the algorithm. Let h be a set of rectangles of the same size that are
divided h times. The algorithm uses a parameter, hmax (n), to limit the size of rectangle so
as to be not overly small (and hence restrict the greediness of the search). In order to select
and refine intervals that are likely to contain a global optimizer, the algorithm executes the
following procedure:
2. We describe SOO with the simple division procedure that LOGO uses. SOO itself does not specify a
division procedure.

157

fiKawaguchi, Maruyama, & Zheng

(i)
(ii)
(iii)
(iv)

Initialize 0 = {0 } and for all i > 0, i = {}
Set h = 0
Select the interval with the maximum center value among the intervals in the set h
If the interval selected by (iii) has a center value greater than that of any larger
interval (i.e., intervals in l for all l < h), divide it and adds the new intervals to
h+1 . Otherwise, reject the interval and skip this step.

(v) Set h = h + 1
(vi) Repeat (iii)(v) until no smaller interval exists (i.e., until l = {} for all l  h) or
h > hmax (n)
(vii) Delete all the intervals already divided in (iv) from  and repeat (ii)(vi)
We now explain this procedure using the example in Figure 1. For brevity, we use the
term, iteration, to refer to the iteration of step (ii)(vii). In Figure 1, the center point
is shown as a (black) dot in each rectangle and each rectangle with a (red) circle around a
(black) dot is the one that was divided (into three smaller rectangles) during an iteration.
At the beginning of the first iteration, there is only one rectangle in 0 , which is the entire
search domain 0 . Thus, step (iii) selects this rectangle and step (iv) divides it, resulting
in the leftmost diagram with N = 3 (the rectangle with the center point with a red circle is
the one divided during the first iteration and the other two are created as a result). At the
beginning of the second iteration, there are three rectangles in 1 (i.e., the three rectangles
in the leftmost diagram with N = 3) but none in 0 (because step (vii) in the previous
iteration deleted the interval in 0 ). Hence, steps (iii)(iv) are not executed for 0 and
we begin with 1 . Step (iii) selects the top rectangle from the three rectangles because
it has the maximum center point among these. Step (iv) divides it because there is no
larger interval, resulting in the second diagram on the top (labeled with w = 1). Iteration
2 continues by conducting steps (iii)(iv) for 2 because there are three smaller rectangles
in 2 . Step (iii) selects the center rectangle on the top (in the second diagram on the top
labeled with w = 1). However, step (iv) rejects it because its center value is not greater
than that of the larger rectangle in l with l < h = 2. There is no smaller rectangle in 
and iteration 2 ends. At the beginning of iteration 3, there are two rectangles in 1 and
three rectangles in 2 (as shown in the second diagram on the top labeled with w = 1).
Iteration 3 begins by conducting steps (iii)(iv) for 1 . Steps (iii)(iv) select and divide the
top rectangle. For rectangles in 2 , steps (iii)(iv) select and divides the middle rectangle.
Here, the middle rectangle was rejected in iteration 2 because of a larger rectangle with
a larger center value that existed in iteration 2. However, that larger rectangle no longer
exists in iteration 3 due to step (vii) in the end of iteration 2, and hence it is not rejected.
The result is the third diagram (on the top labeled with w = 1). Iteration 3 continues for
the newly created rectangles in 3 . It halts, however, for the same reason as iteration 2.
3.2 Description of LOGO
Let k be the superset that is the union of the w sets as k = kw  kw+1      kw+w1
for k = 0, 1, 2, . . . . Then, similar to the SOO algorithm, the LOGO algorithm conducts
the following procedure to select and refine the intervals that are likely to contain a global
optimizer:
158

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

(i) Initialize 0 = {0 } and for all i > 0, i = {}
(ii) Set k = 0
(iii) Select the interval with the maximum center value among the intervals in the superset k
(iv) If the interval selected by (iii) has a center value greater than that of any larger interval
(i.e., intervals in l with l < k), divide it and adds the new intervals to . Otherwise,
reject the interval and skip this step.
(v) Set k = k + 1
(vi) Repeat (iii)(v) until no smaller interval exists (i.e., until l = {} for all l  k) or
k > bhmax (n)/wc.
(vii) Delete all the intervals already divided in (iv) from  and repeat (ii)(vi)
When compared with the SOO algorithm, the above steps are identical except that
LOGO processes the superset k instead of set h . The superset k is reduced to h with
k = h when w = 1 and thus LOGO is reduced to SOO.
We now explain this procedure using the example in Figure 1. With w = 1, the LOGO
algorithm functions in the same fashion as the SOO algorithm. See the last paragraph in
the previous section for the explanation as to how the SOO and LOGO algorithms function
in this example. For the case with w = 4, the difference arises during iteration 3 when
compared to the case with w = 1. At the beginning of iteration 3, there are two sets 1 and
2 (i.e., there are two sizes of rectangles in the second diagram on the bottom with w = 4).
However, there is only one superset consisting of the two sets 0 = 0  0+1  0+41 .
Therefore, step (iii)(iv) is conducted only for k = 0 and the LOGO algorithm divides
only the one rectangle with the highest center value among those in 0 . Consequently,
the algorithm has one additional iteration (iteration 4) using the same number of function
evaluations (N = 9) as the case with w = 1. It can be seen that as w increases, the
algorithm is more biased to the local search and, in this example, this strategy turns out
to be beneficial as the algorithm divides the rectangle near the global optima more when
w = 4 than when w = 1.
The pseudocode for the LOGO algorithm is provided in Algorithm 1. Steps (ii), (v), and
(vi) correspond to the for-loop in lines 1019. Steps (iii)(iv) correspond to line 11 and line
1214, respectively. We use following notation. Each hyperrectangle,   0 , is coupled
with a function value at its center point f (c ), where c indicates the center point of the
rectangle. As explained earlier, we use h to denote the number of divisions and the index
of the set as in h . We define h,i to be the ith element of a set h (i.e., h,i  h ). Let
xh,i and ch,i be an arbitrary point and the center point in the rectangle h,i , respectively.
We denote val[h,i ] to indicate a stored function value of the center point in the rectangle
h,i . As it can be seen in line 14, this paper considers a simple division procedure with
a rescaled domain 0 . If we have prior knowledge about the domain of the function, we
should leverage the information. For example, we could map the original input space to
another so that we can obtain a better ` based on the theoretical results in Section 4, or we
could employ a more elaborate division procedure based on the prior knowledge.
Having discussed how the LOGO algorithm functions, we now consider the reason why
the algorithm might work well. The key mechanism of the DIRECT and SOO algorithms
159

fiKawaguchi, Maruyama, & Zheng

Algorithm 1: LOGO algorithm
0:
1:
2:
3:
4:
5:

6:
7:
8:
9:
10:
11:
12:
13:
14:

15:

16:

17:
18:
19:
20:

Inputs (problem): an objective function f : x  RD  R, the search domain : x  
Inputs (parameter): the search depth function hmax : Z+  [1, ), the local weight
w  Z+ , stopping condition
Define the set h as a set of hyperrectangles divided h times
Define the superset k as the union of the w sets: k = kw  kw+1      kw+w1
Normalize the domain  to 0 = [0, 1]D
Initialize the variables: the set of hyperrectangles: h = {}, h = 0, 1, 2, . . . ,
the current maximum index of the set: hupper = 0
the number of total divisions: n = 1
Adds the initial hyperrectangle 0 to the set: 0  0  {0 } (i.e., 0,0 = 0 )
Evaluate the function f at the center point of 0 , c0,0 : val [0,0 ]  f (c0,0 )
for iteration = 1, 2, 3, . . .
val max  , hplus  hupper
for k = 0, 1, 2, . . . , max(bmin(hmax (n), hupper )/wc, hplus )
Select a hyperrectangle to be divided: (h, i)  arg maxh,i val [h,i ] for h, i : h,i  k
if val [h,i ] > val max then
val max  val [h,i ], hplus  0, hupper  max(hupper , h + 1), n  n + 1
Divide this hyperrectangle h,i along the longest coordinate direction
- three smaller hyperrectangles are created  left , center , right
- val [center ]  val [h,i ]
Evaluate the function f at the center points of the two new hyperrectangles:
val [left ]  f (cleft ), val [right ]  f (cright )
Group the new hyperrectangles into the set h+1 and remove the original rectangle:
h+1  h+1  {center , left , right }, h  h \ h,i
end if
if stopping condition is met then Return (h, i) = arg maxh,i val [h,i ]
end for
end for

is to divide all the hyperintervals with potentially highest upper bounds w.r.t. unknown
smoothness at each iteration. The idea behind the LOGO algorithm is to reduce the number
of divisions per iteration by biasing the search toward the local area with the concept of
the supersets. Intuitively, this can be beneficial for two reasons. First, by reducing the
number of divisions per iteration, more information can be utilized when selecting intervals
to divide. For example, one may simultaneously divide five or ten intervals per iteration.
In the former, when selecting the sixth to the tenth interval to divide, one can leverage
information gathered by the previous five divisions (evaluations), whereas the latter makes
it impossible. Because the selection of intervals depends on the information, which in turn
provides the new information to the next selection, the minor difference in availability of
the information may make the two sequences of the search very different in the long run.
Second, by biasing the search toward the local area, the algorithm likely converges faster in
a certain type of problem. In many practical problems, we do not aim to find a position of
160

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

global optima, but a position with a function value close to global optima. In that case, the
local bias is likely beneficial unless there are too many local optima, the value of which is far
from that of global optima. Even though our local bias strategy is motivated to solve the
problem of the impractically slow convergence rate of most global optimization methods,
the algorithm maintains guaranteed loss bounds w.r.t. global optima, as discussed below.

4. Theoretical Results: Finite-Time Loss Analysis
We first derive the loss bounds for the LOGO algorithm when it uses any division strategy
that satisfies certain assumptions. Then, we provide the loss bounds for the algorithm with
the concrete division strategy provided in Algorithm 1 and with the parameter values that
we use in the rest of this paper. The motivation in the first part is to extend the existing
framework of the theoretical analysis and thus produce the basis for future work. The
second part is to prove that the LOGO algorithm maintains finite-time loss bounds for the
parameter settings that we actually use in the experiments.
4.1 Analysis for General Division Method
In this section, we generalize the result obtained by Munos (2013) in that the previous result
is now seen as a special case of the new result when w = 1. The previous work provided
the loss bound of the SOO algorithm with any division process that satisfied the following
two assumptions, which we adopt in this section.
Assumption A1 (Decreasing diameter). There exists a function (h) > 0 such that for
any hyperinterval h,i  0 , we have (h)  supxh,i `(xh,i , ch,i ), while (h  1)  (h) holds
for h  1.
Assumption A2 (Well-shaped cell). There exists a constant  > 0 such that any hyperinterval h,i contains a `-ball of radius (h) centered in h,i .
Intuitively, Assumption A1 states that the unknown local smoothness ` is upper-bounded
by a monotonically decreasing function of h. This assumption ensures that each division
does not increase the upper bound, (h). Assumption A2 ensures that every interval covers
at least a certain amount of space in order to relate the number of intervals to the unknown
smoothness ` (because ` is defined in terms of space). To present our analysis, we need to
define the relevant terms and variables. We define -optimal set X as
X := {x  0 : f (x) +   f (x )}.
That is, the set of -optimal set X is the set of input vectors whose function value is at least
-close to the value of the global optima. In order to bound the number of hyperintervals
relevant to the -optimal set X , we define near-optimality dimension as follows.
Definition 1 (Near-optimality dimension). The near-optimality dimension is the smallest
d  0 such that there exists C > 0, for all  > 0, the maximum number of disjoint `-balls
of radius  centered in the -optimal set X is less than or equal to Cd .
The near-optimality dimension was introduced by Munos (2011) and is closely related
to a previous measure used by Kleinberg, Slivkins, and Upfal (2008). The value of the
161

fiKawaguchi, Maruyama, & Zheng

near-optimality dimension d depends on the objective function f , the semi-metric ` and the
division strategy (i.e., the constant  in Assumption A2). If we consider a semi-metric `
that satisfies Assumptions 1, A1, and A2, then the value of d depends only on such a semimetric ` and the division strategy. In Theorem 2, we show that the division strategy of the
LOGO algorithm can let d = 0 for a general class of semi-metric `.
Now that we have defined the relevant terms and variables used in previous work, we
introduce new concepts to advance our analysis. First, we define the set of -optimal
hyperinterval h (w) as
h (w) := {h,i  0 : f (ch,i ) + (h  w + 1)  f (x )}.
The -optimal hyperinterval h (w) is used to relate the hyperintervals to -optimal set X .
Indeed, the -optimal hyperinterval h (w) is almost identical to the (h  w + 1)-optimal
set X(hw+1) (-optimal set X with  being (hw+1)), except that h (w) only considers
the hyperintervals and the values of their center points while X(hw+1) is about the whole
input vector space. In order to relate h (w) to h (1) , we define `-ball ratio as follows.
Definition 2 (`-ball ratio). For every h and w, the `-ball ratio is the smallest h (w) > 0
such that the volume of a `-ball of radius (h  w + 1) is no more than the volume of h (w)
disjoint `-balls of radius (h).
In the following lemma, we bound the maximum cardinality of h (w) . We use |h (w) |
to denote the cardinality.
Lemma 1. Let d be the near-optimality dimension and C denote the corresponding constant in Definition 1. Let h (w) be the `-ball ratio in Definition 2. Then, the -optimal
hyperinterval is bounded as
|h (w) |  Ch (w)(h  w + 1)d .
Proof. The proof follows the definition of -optimal set X , Definition 1, Definition 2, and
Assumption A2. From the definition of -optimal space X , we can write (hw+1)-optimal
set as
X(hw+1) = {x  0 : f (x) + (h  w + 1)  f (x )}.
The definition of the near-optimality dimension (Definition 1) implies that at most C(h 
w + 1)d centers of disjoint `-balls of radius (h  w + 1) exist within space X(hw+1) .
Then, from the definition of the `-ball ratio (Definition 2), the space of C(h  w + 1)d
disjoint `-balls of radius (h  w + 1) is covered by at most Ch (w)(h  w + 1)d disjoint
`-balls of radius (h). Notice that the set of space covered by C(h  w + 1)d disjoint
`-balls of radius (h  w + 1) is a superset of X(hw+1) . Therefore, we can deduce that
there are at most Ch (w)(h  w + 1)d centers of disjoint `-balls of radius (h) within
X(hw+1) . Now, recall the definition of the h-w-optimal interval,
h (w) := {h,i  0 : f (ch,i ) + (h  w + 1)  f (x )}
and notice that the number of intervals is equal to the number of centers ch,i that satisfy
the condition f (ch,i ) + (h  w + 1)  f (x ). Assumption A2 causes this number to be
equivalent to the number of centers of disjoint `-balls with radius (h), which we showed
to be upper bounded by Ch (w)(h  w + 1)d .
162

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

Next, we bound the maximum size of the optimal hyperinterval, which contains a global
optimizer x . In the following analysis, we use the concept of the set and superset of
hyperintervals. Recall that set h contains all the hyperintervals that have been divided h
times thus far, and superset k is a union of the w sets, given as k = kw  kw+1     
kw+w1 for k = 0, 1, 2, . . . . We say that a hyperinterval is dominated by other intervals
when the hyperinterval is not divided because its center value is at most that of other
hyperintervals in any set.
Lemma 2. Let kn be the highest integer such that the optimal hyperinterval, which contains
a global optimizer x , belongs to the superset kn after n total divisions (i.e., kn  n
determines the size of the optimal hyperinterval, and hence the loss of the algorithm). Then,
kn is lower bounded as kn  K with any K that satisfies 0  K  bhmax (n)/wc and
 K 

w1
X
hmax (n) + w X


n
|kw (1) | +
|kw+l (l + 1) | .
w


k=0

l=1

Proof. Let  (k ) be the number of divisions, with which the algorithm further divides the
optimal hyperinterval in superset k and places it into k+1 . In the example in Figure 1
with w = 1, the optimal hyperinterval is initially the whole domain 0  0 . It is divided
with the first division and the optimal hyperinterval is placed into 1 . Therefore,  (0 ) = 1.
Similarly,  (1 ) = 2. A division of non-optimal interval occurs before that of the optimal
one for  (2 ) and hence  (2 ) = 4. In other words,  (k ) is the time when the optimal
hyperinterval in superset k is further divided and escapes the superset k , entering into
k+1 . Let ckw+l,i be the center point of the optimal hyperinterval in a set kw+l  k .
We prove the statement by showing that the quantity  (k )   (k1 ) is bounded
by the number of -optimal hyperintervals h (w) . To do so, let us consider the possible
hyperintervals to be divided during the time [ (k1 ),  (k )  1]. For the hyperintervals
in the set kw , the ones that can possibly further be divided during this time must satisfy f (ckw,i )  f (ckw,i )  f (x )  (kw). The first inequality is due to the fact that the
algorithm does not divide an interval that has center value less than the maximum center value of an existing interval for each set, and there exists f (ckw,i ) during the time
[ (k1 ),  (k )  1]. The second inequality follows Assumption 1 and the definition of the
optimal interval. Then, from the definition of h (w) , the hyperintervals that can possibly
be divided during this time belong to kw (1)  k .
In addition to set kw , in superset k , there are sets kw+l with l : w  1  l  1.
For these sets, we have f (ckw+l,i )  f (clw+l,i )  f (x )  (kw) with similar deductions.
Here, notice that during the time [ (k1 ),  (k )  1], we can be sure that the center value
in the superset is lower bounded by f (ckw,i ) instead of f (ckw+l,i ). In addition, we have
(kw) = (kw + l  l). Thus, we can conclude that the hyperintervals in set kw+l that can
be divided during time [ (k1 ),  (k )  1] belongs to kw+l (l + 1) where (w  1)  l  1.
No hyperinterval in superset k may be divided at iteration since the intervals can be
dominated by those in other supersets. In this case, we have f (cjw+l,i )  f (ckw,i ) 
f (x )  (kw) for some j < k and l  0. With similar deductions, it is easy to see
f (x )  (kw)  f (x )  (jw + l). Thus, the hyperintervals in a superset j with j < k
that can dominate those in superset k during [ (k1 ),  (k )  1] belongs to jw+l (1) .
163

fiKawaguchi, Maruyama, & Zheng

Putting the above results together and noting that the algorithm divides at most
bhmax (n)/wc + 1 intervals during any iteration (hplus plays its role only when the algorithm divides at most one interval), we have

 (k )   (k1 ) 


 
w1
k1 w1
X
X
X
hmax (n)



+ 1 |kw (1) | +
|kw+l (l + 1) | +
|jw+l (1) | .
w
j=1 l=0

l=1

Then,


kn
X
k=1

 kn 

w1
X
hmax (n) + w X


 (k )   (k1 ) 
|kw (1) | +
|kw+l (l + 1) |
w


k=1

l=1

since the last term for a superset j with j  k  1 in the previous inequality contains
only the optimal
Pkn intervals that are subsets of the optimal intervals covered by the new
summation k=1 .
If kn  bhmax (n)/wc, then the statement always holds true for any 0  K  bhmax (n)/wc
since kn  bhmax (n)/wc  K. Accordingly, we assume kn < bhmax (n)/wc in the following.
Since  (0 ) is upper bounded by the term in the previous summation on the right hand of
the above inequality with k = 0,


hmax (n) + w
 (kn +1 ) 
w



 kX
w1
n +1
X


|kw+l (l + 1) | .
|kw (1) | +

l=1

k=0

By the definition of kn , we have n <  (kn +1 ). Therefore, for any K  bhmax (n)/wc such
that


 K 

w1
X
hmax (n) + w X


|kw (1) | +
|kw+l (l + 1) |
w
k=0

l=1



hmax (n) + w
n<
w


 kX
n +1



|kw (1) | +

w1
X

k=0


|kw+l (l + 1) | ,


l=1

we have kn  K.
With Lemmas 1 and 2, we can now present the main result in this section that provides
the finite-time loss bound of the LOGO algorithm.
Theorem 1. Let ` be a semi-metric such that Assumptions 1, A1, and A2 are satisfied.
Let h(n) be the smallest integer h such that


hmax (n) + w
nC
w

 bh/wc

w1
X
X
d
d
(kw) +
kw+l (l + 1)(kw)
.
k=0

l=1

Then, the loss of the LOGO algorithm is bounded as

rn   min(wbh(n)/wc  w, wbhmax (n)/wc) .
164

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

Proof. From Lemma 1 and the definition of h(n),


hmax (n) + w
n>C
w


hmax (n) + w

w


 bh(n)/wc1
w1
X 
X
d
d
(kw) +
kw+l (l + 1)(kw + l  l)
k=0

l=1

 bh(n)/wc1
X 



|kw (1) | +

k=0

w1
X


|kw+l (l + 1) | .


l=1

Therefore, we set K as K = bh(n)/wc  1 in the following to apply the result of Lemma 2.
Then, it follows that kn  K(n) when K < bhmax (n)/wc. Here, the number of divisions
that an interval in the superset K is at least Kw = wbh(n)/wc  w. Therefore, from
Assumptions 1, A1, and A2, we can deduce that rn  (wbh(n)/wc  w).
When K  bhmax (n)/wc, we have b(hupper )/wc  kn  bhmax (n)/wc. Thus, in this
case, we have kn being equal to at least bhmax (n)/wc. From Assumptions 1, A1, and A2,
we can similarly deduce that rn  (wbhmax (n)/wc).
The loss bound stated by Theorem 1 applies to the LOGO algorithm with any division
strategy that satisfies Assumptions A1 and A2. We add the following assumption about
the division process to derive more concrete forms of the loss bound.
Assumption A3 (Decreasing diameter revisit). The decreasing diameter defined in Assumption 1 can be written as (h) = c h/D for some c > 0 and  < 1, and accordingly the
corresponding `-ball ratio is h (w) = ((h  w + 1)/(h))D .
Assumption A3 is similar to an assumption made by Munos (2013), which is that
(h) = c h . In contrast to the previous assumption, our assumption explicitly reflects
the fact that the size of a hyperinterval decreases at a slower rate for higher dimensional
problems. For the LOGO algorithm, the validity of Assumptions A1, A2, and A3 is confirmed in the next section.
We now present the finite-loss bound for the LOGO algorithm in the case of the general
division strategy with the above additional assumption and with d = 0.
Corollary 1. Let ` be a semi-metric such that Assumptions 1, A1, A2, and A3 are satisfied.

If the near-optimality dimension d = 0 and hmax (n) is set to n  w, then the loss of the
LOGO algorithm is bounded for all n as
!




 w  w  1 1

w
1
rn  c exp  min
n
 2, n  w
ln
.
C  1  1
D 
Proof. Based on the definition of h(n) in Theorem 1, we first relate h(n) to n as
hmax (n) + w
nC
w
hmax (n) + w
=C
w

bh(n)/wc

X

(kw)

d

+

k=0

k=0

d



kw+l (l + 1)(kw + l  l)

l=1

bh(n)/wc

X

w1
X

1+

w1
X
l=1



w



hmax (n) + w
C
w

165




 w1
X
h(n)
+1
 w .
w
l=0

fiKawaguchi, Maruyama, & Zheng

0.9
0.8
0.7
0.6



0.5
0.4
0.3
0.2
0.1
1

2

3

4

5

w
Figure 2: Effect of local bias w on loss bound in the case of d = 0 := w2 ( 1  1)/( w  1)
The first line follows the definition of h(n), and the second line is due to d = 0 and
Assumption A3. By algebraic manipulation,


Here, we use hmax (n) =


 w

h(n)
n
w

 1 1

 1.
w
C hmax (n) + w  1  1


n  w, and hence





 w  w  1 1
h(n)
 n
 1.
w
C  1  1

By substituting these results into the statement of Theorem 1,



!
 w2  w  1 1

2
n
 2w, w n  w
.
rn   min
C  1  1
From Assumption A3, (h) = c h/D . By using (h) = c h/D in the above inequality, we
have the statement of this corollary.
Corollary 1 shows that the LOGO algorithm guarantees an exponential bound on the

loss in terms of n (a stretched exponential bound in terms of n). The loss bound in
Corollary 1 becomes almost identical to that of the SOO algorithm with w = 1. Accordingly,

we illustrate the effect of w, when n is large enough to let us focus on the coefficient of n,
in Figure 2. The (red) bold line with label 1 indicates the area where w has no effect on the
bound. The area with lines having labels greater than one is where w improves the bound,
and the area with labels less than one is where w diminishes the bound. More concretely, in

the figure, we consider the ratio of the coefficient of n in the loss bound with the various
value of w to that with w = 1. The ratio is w2 ( 1  1)/( w  1) or w, depending on which
element of the min in the bound is smaller. Since w2 ( 1 1)/( w 1) is at most w (in the
domain we consider), we plotted w2 ( 1  1)/( w  1) to avoid overestimating the benefit
of w. Thus, this is a rather pessimistic illustration of the advantage of our generalization
166

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

regarding w. For instance, if the second element of the min in the bound is smaller and n is
large enough, increasing w always improves the bound, regardless of the values in Figure 2.
The next corollary presents the finite-loss bound for the LOGO algorithm in the case of
d 6= 0.
Corollary 2. Let ` be a semi-metric such that Assumptions 1, A1, A2, and A3 are satisfied.
If the near-optimality dimension d > 0 and hmax (n) is set to be ((ln n)c1 )  w for some
c1 > 1, the loss of the LOGO algorithm is bounded as
!

 w
 

 1 1 1/d
2 wd/D
2wd/D
1/d
w (

)
rn  O n
.
 1  1
Proof. In the same way as the first step in the proof of Corollary 1, except for d > 0,
n  Ccd

hmax (n) + w
w

bh(n)/wc w1

X

X

k=0

l=0

 lkwd/D .

The reason why we couldP
not bound the loss in a similar rate as in the case d = 0 is that
the last summation term w1
l=0 is no longer independent of k. Since
w1
X



lkwd/D

=

w

1
,
1
 1

kwd/D 

l=0

bh(n)/wc

X
k=0

 kwd/D =

 (bh(n)/wc+1)wd/D  1
,
 wd/D  1

with algebraic manipulation,
d

c

(

(bh(n)/wc+1)wd/D

 w

w

 1 1 wd/D
n
(
 1).
 1) 
C hmax (n) + w  1  1

Therefore,
c (wbh(n)/wcw)/D 

!1/d
 w

n
w

 1 1 wd/D
.
(
 1) 2wd/D
C hmax (n) + w  1  1

From Theorem 1 and Assumption A3,

rn  max

!
 w

1/d
n
w

 1 1 wd/D
(
  2wd/D )
, c (wbhmax (n)/wcw)/D .
C hmax (n) + w  1  1

For hmax (n) = ((ln n)c1 )  w and for a sufficiently large n, the first element of the previous
max becomes larger than the second one, and its order is equivalent to the one in the
statement.
We derived the loss bound for the SOO algorithm with Assumption A3 in the case of
d 6= 0 as well. The SOO version of the loss bound is rn  O(n1/d ( d/D   2d/D )1/d ),
which is equivalent to the loss bound of the LOGO algorithm with w = 1 in Corollary 2.
In Figure 3, we thereby illustrate the effect of w on the loss bound in the O form. In the
figure, we plotted the ratio of the elements inside O of the loss bounds. From Figure 2 and
167

fiKawaguchi, Maruyama, & Zheng



1

1

1

0.8

0.8

0.8

0.6



0.6



0.6

0.4

0.4

0.4

0.2

0.2

0.2

1

2

3

4

5

w
(a) d = 0.01

1

2

3

w
(b) d = 0.5

4

5

4

1

2

3

4

5

w
(c) d = 1.0

Figure 3: Effect of local bias w on loss bound in the case of d 6= 0 := (w2 ( wd/D 
w 1
)1 )1/d /( d/D   2d/D )1/d
 2wd/D (  1 1
Figure 3, we can infer that the loss bound is improved with w > 1 if  is large and d is
small (when n is sufficiently large). Intuitively, this makes sense, since there are more of
the different yet similar sizes of hyperintervals w.r.t. ` if  is larger and d is smaller. In that
case, dividing all the hyperintervals in the marginally different sizes would be redundant
and a waste of computational resources. Note that our discussion here is limited to the loss
bound that we have now, which may be tightened in future work. We would then see the
different effects of w on such tightened bounds.
4.2 Basis of Practical Usage
In this section, we derive the loss bound of the LOGO algorithm with the concrete division
strategy presented in Section 3.1. The purpose of this section is to analyze the LOGO
algorithm with the division process and the parameter settings that are actually used in the
rest of this paper. The results of this section are directly applicable to our experiments. In
this section, we discard Assumptions A1, A2, and A3. We consider the following assumption
to present the loss bound in a concrete form.
Assumption B1. There exists a semi-metric ` such that that it satisfies Assumption 1 and
both of the following conditions hold:
 there exist b > 0,  > 0 and p  1 such that for all x, y  0 , `(x, y) = bkx  ykp
 there exist   (0, 1) such that for all x  0 , f (x )  f (x) + ` (x, x ).
First, we state that the loss bound of the algorithm with the practical division process
and parameter settings decreases at a stretched exponential rate.
Theorem 2 (worst-case analysis). Let ` be a semi-metric such that Assumptions 1 and B1
are satisfied. The loss of the LOGO algorithm is bounded as
!

 w


 w

 1 1
w
1
0
rn  c exp  min
n 0
 2, w n  w
ln
w C  1  1
D 
168

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

where  = 3 and c = b3 D/p . Here, w0 = 1 if we set the parameter as hmax (n) =

On the other hand, w0 = w if we set the parameter as hmax (n) = w n  w.



nw.

Proof. From Assumption B1 and the division strategy,
supxh,i `(xh,i , ch,i )  b(3bh/Dc D1/p ) = bD/p 3bh/Dc
which corresponds to the diagonal length of each rectangle, while 3bh/Dc corresponds
to the length of the longest side. This quantity is upper bounded by 3h/D+ . Thus,
we consider the case where (h) = b3 D/p 3h/D , which satisfies Assumption A1. Also,
Assumption A3 is satisfied for (h) with  = 3 and c = b3 D/p .
Every rectangle contains at least a `-ball of radius corresponding to the length of the
shortest side for the rectangle. Consequently, we have at least a `-ball of radius (h) =
b3 3h/D for any rectangle where  = 32 D/p , which satisfies Assumption A2.
From Assumption B1, the volume V of a `-ball of radius (h) is proportional to ((h))D
as the following: VDp ((h)) = (2(h)(1 + 1/p))D /(1 + D/p). Therefore, Assumption A3
is satisfied for the `-ball ratio h (w). In addition, the (h)-optimal set X(h) is covered by a
`-ball of radius (h) by Assumption B1, and thereby contains at most ((h)/(h))D =  D
disjoint `-balls of radius (h). Hence, the number of the `-balls does not depend on (h),
which means d = 0.
Now that we have satisfied Assumptions A1, A2, and A3 with  = 3 , c = b3 D/p ,
and d = 0, we obtain the statement by following the proof of Corollary 1.
Regarding the effect of local orientation w, Theorem 2 presents the worst-case analysis.
Recall that w is introduced in this paper to restore the practicality of global optimization
methods. Thus, focusing on the worst case is likely too pessimistic. To mitigate this
problem, we present the following optimistic analysis.
Theorem 3 (best-case analysis in terms of w). Let ` be a semi-metric such that Assumptions 1 and B1 are satisfied. For 1  l  w, let h+l1,i0 be any hyperinterval that
may dominate other intervals in the set h during the algorithms execution. Assume that
h+l1,i0  h (1) . Then, the loss of the LOGO algorithm is bounded as




 1
w
1
0
rn  c exp  min
n 0  2, w n  w
ln
wC
D 

where  = 3 and c = b3 D/p . Here, w0 = 1 if we set the parameter as hmax (n) = nw.

On the other hand, w0 = w if we set the parameter as hmax (n) = w n  w.
Proof. The statement of Lemma 2 is modified as

 K 

w1
X
hmax (n) + w X


n
|kw (1) | +
|kw (1) | .
w
k=0

l=1

The statement of Theorem 1 is modified as

 bh/wc
hmax (n) + w X
nC
(w(kw)d ),
w
k=0


rn   min(wbh(n)/wc  w, wbhmax (n)/wc) .
169

fiKawaguchi, Maruyama, & Zheng

25
25

4

5

1

w
(a) Pessimistic w2 ( 1  1)/( w  1)

5

3

1

2

25

0.5

2.5
1

20

0.5

20

1

15

1

15

 1.5

10

 1.5

10

2

10

2

5

2.5

5

2.5

1

3
1

3

2

w

3

4

5

(b) Optimistic w2

Figure 4: Effect of local bias w on loss bound with practical setting. The real effect would
exist somewhere in-between.
Then, we can follow the proof of Theorem 2 and Corollary 1, obtaining


n
1
h(n)

 1.
w
C hmax (n) + w
With hmax (n) =
of this theorem.



nw, from the modified statement of Theorem 1, we obtain the statement

As Theorem 3 makes a strong assumption to eliminate the negative effect of the local
orientation in the bound, increasing w always improves the loss bound in the theorem when
n is sufficiently large. This may seem to be overly optimistic, but we show an instance of
this case in our experiment.
More realistically, the effect of w with large n would exist somewhere between the left
and the right diagrams in Figure 4. As in the previous figures, the (red) bold line with
label 1 is where w has no effect on the bound, the area with labels greater than one is where
w improves the bound, and the area with labels less than one is where w diminishes the
bound. The left diagram shows the effect of w in the worst case of Theorem 2 by plotting
w2 ( 1  1)/( w  1) with  = 3 . The reason why plotting w2 ( 1  1)/( w  1)
represents the worst case is discussed in the previous section. The right diagram presents
the effect of w in the best case of Theorem 2 or Theorem 3 by simply plotting w2 . Notice
that in both Theorem 2 and Theorem 3, the best scenario for the effect of w is when we use

hmax (n) = w n  w and the second element of the min dominates the bound. In this case,

the coefficient of n is w2 , which is the effect of w on the bound when n is large enough to
ignore the other term.
In conclusion, we showed that the LOGO algorithm provides a stretched exponential
bound on the loss with the algorithms division strategy, which is likely more practical
than the one used in the analysis of the SOO algorithm, and with the parameter setting


hmax (n) = n  w or hmax (n) = w n  w. We also discussed how the local bias w
170

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

affects the loss bound. Based on these results, we use the LOGO algorithm in the following
experiments.

5. Experimental Results
In this section, we test the LOGO algorithm with a series of experiments. In the main
part of the experiments, we compared the LOGO algorithm with its direct predecessor,
the SOO algorithm (Munos, 2011) and its latest powerful variant, the Bayesian Multi-Scale
Optimistic Optimization (BaMSOO) algorithm (Wang, Shakibi, Jin, & de Freitas, 2014).
The BaMSOO algorithm combines the SOO algorithm with a Gaussian Process (GP) to
leverage the GPs estimation of the upper confidence bound. It was shown to outperform
the traditional Bayesian Optimization method that uses a GP and the DIRECT algorithm
(Wang et al., 2014). Accordingly, we omitted the comparison with the traditional Bayesian
Optimization method. We also compare LOGO with popular heuristics, simulated annealing
(SA) and genetic algorithm (GA) (see Russell & Norvig, 2009 for a brief introduction).
In the experiments, we rescaled the domains to the [0, 1]D hypercube. We used the same
division process for SOO, BaMSOO and LOGO, which is the one presented in Section 3.2
and proven to provide stretched exponential bounds on the loss in Section 4.2. Previous
algorithms have also been used with this division process in experiments (Jones et al., 1993;
Gablonsky, 2001; Munos, 2013; Wang et al., 2014). For the SOO and LOGO algorithms, we

set hmax (n) = w n  w. This setting guarantees a stretched exponential bound for LOGO,
as proven in Section 4.2, and for SOO (Munos, 2013). For the LOGO algorithm, we used
a simple adaptive procedure to set the parameter w. Let f (x+
i ) be the best value found
thus far in the end of iteration i. Let W = {3, 4, 5, 6, 8, 30}. The algorithm begins with
w = W1 = 3. At the end of iteration i, the algorithm set w = Wk with k = min(j + 1, 6)
+
if f (x+
i )  f (xi1 ), and k = max(j  1, 1) otherwise, where Wj is the previous parameter
value w before this adjustment occurs. Intuitively, this adaptive procedure is to encourage
the algorithm to be locally biased when it seems to be making progress, forcing it to explore
a more global region when this does not seem to be the case. Although the values in the
set W = {3, 4, 5, 6, 8, 30} are arbitrary, this simple setting was used in all the experiments
in this paper, including the real-world application in Section 6.4. The results demonstrate
the robustness of this setting. As discussed later, a future work would be to replace this
simple adaptive mechanism to improve the performance of the proposed algorithm. For the
BaMSOO algorithm, the previous work of Wang et al. (2014) used a pair of a good kernel
and hyperparameters that were handpicked for each test function. In our experiments, we
assumed that such a handpicking procedure was unavailable, which is typically the case
in practice. We tested several pairs of a kernel and hyperparameters; however, none of
the pairs performed robustly well for all the test functions (e.g., one pair performed well
for one test function, although not others). Thus, we used the empirical Bayes method
3
to adaptively update the hyperparameters
p . We selected the isotropic Matern kernel with
0
 = 5/2, which is given by (x, x ) = g( 5kx  x0 k2 /l), where the function g is defined to
be g(z) =  2 (1 + z + z 3 /3). The hyperparameters were initialized to  = 1 and l = 0.25. We
updated the hyperparameters every iteration until 1,000 function evaluations were executed
3. We implemented BaMSOO by ourselves to use the empirical Bayes method, which was not done in the
original implementation. The original implementation of BaMSOO was not available for us as well.

171

fiKawaguchi, Maruyama, & Zheng

f

D

SOO


N

Time (s)

BaMSOO
Error

N

Time (s)

LOGO
Error

N

Time (s)

Error

Sin 1

1

[0, 1]

57

5.3 E02 2.3 E06

30

2.0 E+00 2.3 E06 17 3.9 E02 2.3 E06

Sin 2

2

[0, 1]2

271

1.7 E01 4.6 E06

181

7.5 E+00 4.6 E06 45 5.4 E02 4.6 E06

Peaks

2

[3, 3]2

141

1.0 E01 9.0 E05

37

3.5 E+00 9.0 E05 35 6.1 E02 9.0 E05

Branin

2 [5, 10]  [0, 15]

339

2.1 E01 9.0 E05

121

8.1 E+00 9.0 E05 85 7.0 E02 8.7 E05

Rosenbrock 2

2

[5, 10]2

491

3.1 E01 9.7 E06 >4000 5.8 E+04 5.5 E03 137 1.3 E01 9.7 E06

Hartman 3

3

[0, 1]3

359

2.3 E01 7.91 E05

126

8.9 E+00 7.9 E05 65 7.1 E02 5.1 E05

Shekel 5

4

[0, 10]4

1101 6.6 E01 8.4 E05

316

3.1 E+01 8.4 E05 157 1.2 E01 8.4 E05

Shekel 7

4

[0, 10]4

1117 7.1 E01 9.4 E05

95

1.2 E+01 9.4 E05 157 1.2 E01 9.4 E05

Shekel 10

4

[0, 10]4

1117 6.4 E0.1 9.68 E05 >4000 4.5 E+04 8.1 E+00 197 1.5 E01 9.7 E05

Hartman 6

6

[0, 1]6

1759 1.2 E+00 7.51 E05 >4000 4.0 E+04 2.3 E03 161 1.3 E01 6.8 E05

[5, 10]10

>8000 7.8 E+00 3.83 E03 >8000 5.8 E+04 9.6 E+00 1793 1.7 E+00 4.8 E05

Rosenbrock 10 10

Table 1: Performance comparison in terms of the number of evaluations (N ) and CPU time
(Time) to achieve Error < 104 . The grayed cells indicate the experiments where we could
not achieve Error < 104 even with a large number of function evaluations (4000 or 8000).
and then per 1,000 iterations afterward (to reduce the computational cost). For SA and GA,
we used the same settings as those of the Matlab standard subroutines simulannealbnd
and ga, except that we specified the domain bounds.
Table 1 shows the results of the comparison with 11 test functions in terms of the
number of evaluations and CPU time to achieve a small error. The first two test functions,
Sin 1 and Sin 2, were used to test the SOO algorithm (Munos, 2013), and have the form
f (x) = (sin(13x) sin(27x) + 1)/2 and f (x1 , x2 ) = f (x1 )f (x2 ) respectively. The form of the
third function, Peaks, is given in Equation (16) and illustrated in Figure 2 of McDonald,
Grantham, Tabor, and Murphys paper (2007). The rest of the test functions are common
benchmarks in global optimization literature; Surjanovic and Bingham present detailed
information about the functions (2013). In the table, Time (s) indicates CPU time in
second and Error is defined as
(
|(f (x )  f (x+ ))/f (x )| if f (x ) 6= 0,
Error =
|f (x )  f (x+ )|
otherwise.
In the table, N = 2n is the number of function evaluations needed to achieve Error < 104 ,
where n is the total number of divisions and is the one used as the main measure in the analyses in the previous sections. Here, N is equal to 2n because of the adopted division process.
Thus, the lower the value of N becomes, the better the algorithms performance is. We
continued iterations until 4000 function evaluations for all the functions with dimensionality
less than 10, and 8000 for the function with dimensionality equal to 10.
As can be seen in Table 1, the LOGO algorithm outperformed the other algorithms.
The superior performance of the LOGO algorithm with the small number of function evaluations is attributable to its focusing on the promising area discovered during the search.
Conversely, the SOO algorithm continues to search the global domain and tends to be similar to a uniform grid search. The BaMSOO algorithm also follows the tendency toward a
172

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

grid search as it is based on the SOO algorithm. The BaMSOO algorithm chooses where to
divide based on the SOO algorithm; however, it omits function evaluations when the upper
coincidence bound estimated by the GP indicates that the evaluation is not likely to be
beneficial. Although this mechanism of the BaMSOO algorithm seems to be beneficial to
reduce the number of function evaluations in some cases, it has two serious disadvantages.
The first disadvantage is computational time due to the use of GP. Notice that it requires
O(N 3 ) every time to re-compute the upper confidence bound.4 A more serious disadvantage is the possibility of not determining a solution at all. From Table 1, we can see that
BaMSOO improves the performance of SOO in 7/11 cases; however, it severely degrades
the performance in 4/11 cases. Moreover, not only may BaMSOO reduce the performance
but also it may not guarantee convergence even in the limit in practice. The BaMSOO
algorithm reduces the number of function evaluations by relying on the estimation of the
upper confidence bound. However, the estimation can be wrong, and if it is wrong, it may
never explore the region where the global optimizer exists. Notice that these limitations
are not unique to BaMSOO but also apply to many GP-based optimization methods. In
terms of the first limitation (computational time), BaMSOO is a significant improvement
when compared to other traditional GP-based optimization methods (Wang et al., 2014).
Although the LOGO algorithm has a bias toward local search, it maintains a strong
theoretical guarantee, similar to the SOO algorithm, as proven in the previous sections.
In terms of theoretical guarantee, the SOO algorithm and the LOGO algorithm share a
similar rate on the loss bound and base their analyses on the same set of assumptions that
hold in practice. On the other hand, the BaMSOO algorithm has a worse rate on the loss
bound (an asymptotic loss of the order n(1)/d ) and its bound only applies to a restricted
class of a metric ` (the Euclidean norm to the power  = {1, 2}). It also requires several
additional assumptions to guarantee the bound. Some of the additional assumptions would
be impractical, particularly the assumption of the objective function being always wellcaptured by the GP with a chosen kernel and hyperparameters. As discussed above, this
assumption would cause BaMSOO to not only lose the loss bound but also the consistency
guarantee (i.e., convergence in the limit) in practice.
Figure 5 presents the performance comparison for each number of function evaluations
and Figure 6 plots the corresponding computational time. In both figures, a lower plotted
value along the vertical axis indicates improved algorithm performance. For SA and GA,
each figure shows the mean over 10 runs. We report the mean of the standard deviation
over time in the following. For SA, it was 1.19 (Sin 1), 1.32 (Sin 2), 0.854 (Peaks), 0.077
(Branin), 1.06 (Rosenbrock 2), 0.956 (Hartman 3), 0.412 (Shekel 5), 0.721 (Shekel 7), 1.38
(Shekel 10), 0.520 (Hartman 6), and 0.489 (Rosenbrock 10). For GA, it was 0.921 (Sin 1),
0.399 (Sin 2), 0.526 (Peaks), 0.045 (Branin), 1.27 (Rosenbrock 2), 0.493 (Hartman 3), 0.216
(Shekel 5), 0.242 (Shekel 7), 1.19 (Shekel 10), 0.994 (Hartman 6), and 0.181 (Rosenbrock
10).
As illustrated in Figure 5, the LOGO algorithm generally delivered improved performance compared to the other algorithms. A particularly impressive result for the LOGO
algorithm was its robustness for the more challenging functions, Shekel 10 and Rosenbrok 10.
4. Although there are several methods to mitigate its computational burden by approximation, the effect
of the approximation on the performance of the BaMSOO algorithm is unknown and left to a future
work.

173

fiKawaguchi, Maruyama, & Zheng

0

4
6

SA
GA
SOO
BaMSOO
LOGO

8
10
12
14
16

1

2
4
6

SA
GA
SOO
BaMSOO
LOGO

8
10
12
14
16

1

N

10

1

10

7
9

N

100

N

5

SA
GA
SOO
BaMSOO
LOGO

15
20
25
1

10

100

N

5
6
1

10

100

N

3

SA
GA
SOO
BaMSOO
LOGO

5
7
9
11
13

LogDistancetoOptima

6
4
2
0
2
4
6
8
10
12

3

SA
GA
SOO
BaMSOO
LOGO

7
9

11
13
1

10

N

100

(j) Hartman 6

7
9

1000

10

N

100

1000

0
2

SA
GA
SOO
BaMSOO
LOGO

4
6
8
10
12

1

10

100

N

1000

(h) Shekel 7

1

5

5

2

(g) Shekel 5
1

1000

(f) Hartman 3

1

1000

100

SA
GA
SOO
BaMSOO
LOGO

3

1

15

7

N

1

1000

LogDistancetoOptima

LogDistancetoOptima

SA
GA
SOO
BaMSOO
LOGO

10

11

1

4

13

(e) Rosenbrock 2

0

3

11

1

10

1000

1

2

9

(c) Peaks

0

(d) Branin

1

SA
GA
SOO
BaMSOO
LOGO

7

1

30

11
10

5

1000

LogDistancetoOptima

LogDistancetoOptima

LogDistancetoOptima

SA
GA
SOO
BaMSOO
LOGO
1

LogDistancetoOptima

100

5

1

5

3

(b) Sin 2

1
3

1

15

100

(a) Sin 1

LogDistancetoOptima

LogDistancetoOptima

LogDistancetoOptima

LogDistancetoOptima

0
2

1

10

N

100

1000

(i) Shekel 10

SA
GA
SOO
BaMSOO
LOGO
1

10

N

100

1000

(k) Rosenbrock 10

Figure 5: Performance comparison: the number of evaluations N vs. the log error computed
as log10 |f (x )  f (x+ )|. f (x ) indicates the true optimal value of the objective function
and f (x+ ) is the best value determined by each algorithm.

The function Shekel m has m local optimizers and the slope of the surface generally becomes
larger as m increases. Therefore, Shekel 10 and Rosenbrok 10, which have 10-dimensionality,
are generally more difficult functions when compared with the others in our experiment.
Indeed, only the LOGO algorithm achieved acceptable performance on these. From Figure 6, we can see that the LOGO algorithm and the SOO algorithm were fast. The LOGO
algorithm was often marginally slower than the SOO algorithm owing to the additional
computation required to maintain the supersets. The reason why the BaMSOO algorithm
required a large computational cost at some horizontal axis points is that it continued skipping to conduct the function evaluations (because the evaluations were judged to be not
174

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

1000

1000

100

CPUtime(s)

1
0.1

0.01

SOO
SA

0.001
0

20

BaMSOO
GA
40

60

N

CPUtime(s)

100
10
1
0.1

SA
BaMSOO

0.01

LOGO

0.001

80

0

100

200

(a) Sin 1

400

600

N

SOO

1
0.1

800

0

1000

0.01
0.001
0

200

SA
SOO
LOGO
400 N 600

GA
BaMSOO

100
10

GA
BaMSOO

1

1000

N

2000

3000

4000

0

1000

SA
SOO
LOGO

0.001
0

200

400

600

N

GA
BaMSOO

1
0.1

SA
SOO
LOGO

0.01
0.001

800

(g) Shekel 5

200

400

600

N

800

1000

(h) Shekel 7

400

600

N

800

1000

SA
SOO
LOGO

100
10

GA
BaMSOO

1
0.1
0

1000

N

2000

3000

4000

(i) Shekel 10

100000

100000

10000

1000

SA
SOO
LOGO

100
10

GA
BaMSOO

1
0.1

1000

CPUtime(s)

10000

CPUtime(s)

200

GA
BaMSOO

0.01

0

1000

GA
BaMSOO

CPUtime(s)

10000

10

CPUtime(s)

100000

100

CPUtime(s)

1000

100

1

1000

(f) Hartman 3

1000

0.1

SA
SOO
LOGO

(e) Rosenbrock 2

10

800

1
0.1

0.001

0

1000

600

N

0.01

0.01

800

400

10

0.1

(d) Branin

0.01

200

100

SA
SOO
LOGO

1000

CPUtime(s)

1

SOO

1000

10000

10

GA
LOGO

(c) Peaks

100000

100

0.1

SA
BaMSOO

0.01
0.001

(b) Sin 2

1000

CPUtime(s)

GA
LOGO

10

CPUtime(s)

CPUtime(s)

10

SA
SOO
LOGO

100
10

GA
BaMSOO

1
0.1

0.01

0.01
0

1000

N 2000

(j) Hartman 6

3000

4000

0

2000

N 4000

6000

8000

(k) Rosenbrock 10

Figure 6: CPU time comparison: CPU time required to achieve the performance indicated
in Figure 5

beneficial based on GP). This is an effective mechanism of BaMSOO to avoid wasteful function evaluations; however, one must be careful to make sure that the function evaluations
are costly, relative to this mechanism.
In summary, compared to the BaMSOO algorithm, the LOGO algorithm was faster and
considerably simpler (in both implementation and parameter selection) and had stronger
theoretical bases while delivering superior performance in the experiments. When compared
with the SOO algorithm, the LOGO algorithm decreased the theoretical convergence rate
in the worst case analysis, but exhibited significant improvements in the experiments.
Now that we have confirmed the advantages of the LOGO algorithm, we discuss its possible limitations: scalability and parameter sensitivity. The scalability for high dimensions
175

fiKawaguchi, Maruyama, & Zheng

Log Distance to Optimal

LogDistancetoOptima

0
1
2
3

SA
GA
SOO
BaMSOO
REMBOLOGO

4
5
6
7
8

1
-1
-3
-5

w=1
w=2
w = 20
adaptive w

-7
-9

-11

1

10

N

100

1000

1

10

N

100

1000

(b) Sensitivity to local bias parameter w

(a) Scalability: a 1000-dimensional function

Figure 7: On the current possible limitations of LOGO
is a challenge for non-convex optimization in general as the search space grows exponentially
in space. However, we may achieve the scalability by leveraging additional structures of the
objective function that are present for some applications. For example, Kawaguchi (2016b)
showed an instance of deep learning models, in which the objective function has such an
additional structure: the nonexistence of poor local minima. As an illustration, we combine
LOGO with a random embedding method, REMBO (Wang, Zoghi, Hutter, Matheson, &
De Freitas, 2013), to account for another structure: a low effective dimensionality. In Figure 7 (a), we report the algorithms performances for a 1000 dimensional function: Sin 2
embedded in 1000 dimensions in the same manner described in Section 4.1 in the previous
study (Wang et al., 2013).
Another possible limitation of LOGO is the sensitivity of its performance to the free
parameter w. Even though we provided theoretical analysis and insight on the effect of
the parameter value in the previous section, it is yet unclear how to set w in a principle
manner. We illustrate this current limitation in Figure 7 (b). The result labeled with
adaptive w indicates the result with the fixed adaptive mechanisms of w that we use in
all the other experiments except ones in Figure 7 (b) and 8. In the illustration, we use the
Branin function because the experiment conducted with it clearly illustrated the limitation.
As can be seen in the figure, the performance in the early stage is always improved as w
increases because the algorithm finds a local optimum faster with higher w. However, if w
is too large, such as w = 20 in the figure, the algorithm gets stuck at the local optimum for
a long time. Thus, the best value (or sweet spot) exists between too large and too small
values of w. In the results of this experiment, it can be seen that the choice of w = 2 is the
best, which finds the global optima with high precision within only 200 function evaluations.
However, this limitation would not be a serious problem in practice for the following four
reasons. First, a similar limitation exists, to the best of our knowledge, for any algorithms
that are successfully used in practice (e.g., simulated annealing, genetic algorithm, swarmbased optimization, the DIRECT algorithm, and Bayesian optimization). Second, unlike
any other previous algorithm, the finite-time loss bound always applies even for a bad choice
of w. Third, we demonstrated in the previous experiments that a very simple adaptive
rule may suffice to produce a good result. Also, future work may further mitigate this
limitation by developing different methods to adaptively determine the value of w. Also,
176

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

another possibility would be to conduct optimization over w with a cheaper surrogate model.
Finally, the limitation may not apply to some of the target objective functions at all.
For the fourth and final reason, recall that we speculated in the algorithms analysis
that increasing w would always have beneficial effects in some problems, as illustrated in
Figure 4. Clearly, any problems within the scope of local optimization fall into this category.
In Figure 8, we show a rather unobvious instance of such problems, and thus an example,
for which the limitation of the parameter sensitivity does not apply. As can be seen in the
diagram on the left in Figure 8, this test function has many local optima, only one of which
is the global optimum. Nevertheless, as in the diagram on the right, the performance of the
LOGO algorithm improves as w increases, with no harmful effect.

Log Distance to Optimal

0

f

x1

-2

N = 50

-4

N = 100

-6
-8
-10
-12
-14
-16

x2

1

10

100

Local Bias Parameter: w

Figure 8: An example of problems where increasing w is always better. The diagram on the
left shows the objective function, and the diagram on the right presents the performance at
N = 50 and 100 for each w.

6. Planning with LOGO via Policy Search
We now apply the LOGO algorithm to planning, which is an important area in the field
of AI. The goal of our planning problem is to find an action sequence that maximizes the
total return over the infinite discounted horizon or finite horizon (unlike classical planning
problem, we do not consider constraints that specify the goal state). In this paper, we discuss
the formulations for the case of the infinite discounted horizon, but all the arguments are
applicable to the case of a finite horizon with straightforward modifications. We consider
the case where the state/action space is continuous, the planning horizon is long, and the
transition and reward functions are known and deterministic.
The planning problem can be formulated as follows. Let S  RDS be a set of states,
A  RDA be a set of actions, T : RDS  RDS be a transition function, R : RDS  RDA  R
be a return or reward function, and   1 be a discount factor. A planner considers to
take an action a  A in a state s  S, which triggers a transition to another state based
on the transition function T , while receiving a return based on the reward function R.
The discount factor  discounts the future rewards to fulfill either or both of the following
two roles: accounting for the relative importance of immediate rewards compared to future
177

fiKawaguchi, Maruyama, & Zheng

rewards, and obviating the need to think ahead toward the infinite horizon. An action
sequence can be represented by a policy  that maps the state space to the action space:
 : RDS  RDA .
The value of an action sequence or a policy , V  , is the sum of the rewards over the
infinite discounted horizon, which is


V (s0 ) =


X

 t R(sj , (sj )).

j=0

The value of a policy can be also written with a recursive form as

V  (s) = R(s, (s)) + V  T (s, (s)) .

(2)

Here, we are interested in finding the optimal policy   . In the dynamic programing approach, we can compute the optimal policy, by solving the following Bellmans optimality
equation:
V  (s) = max R(s, a) + V  (T (s, a))
(3)
a



where V is the value of the optimal policy. In Equation (3), the optimal policy   is the set
of the actions defined by the max. A major problem with this approach is that the efficiency
of the computation depends on the size of the state space. In a real-world application, the
state space is usually very large or continuous, which often makes it impractical to solve
Equation (3).
A successful approach to avoid the state size dependency is to focus only on the state
space that is reachable from the current state within the planning time horizon. In this
way, even with an infinitely large state space, a planner only needs to consider a finitely
sized subset of the space. This approach is called local planning. Unlike local optimization
vs. global optimization, the optimal solution of local planning is indeed globally optimal,
given the initial state. It is called local planning because it does not cover all the states and
its solution changes for different initial states. Accordingly, as the initial state changes, a
planner may need to conduct re-planning.
A natural way to solve local planning is to use tree search methods, which construct a
tree rooted in an initial state toward the future possible states in the depth of the planning
horizon. This tree search can be conducted using any traditional search method, including
both uninformed search (e.g., breadth-first and depth-first search) and informed (heuristic)
search (e.g., A search). Also, recent studies have developed several tree-based algorithms
that are specialized to local planning. Among those, the SOO algorithm, the direct predecessor of the LOGO algorithm, was applied to local planning with the tree search approach
(Busoniu, Daniels, Munos, & Babuska, 2013). Most of the new algorithms, for example,
HOLOP (Bubeck & Munos, 2010; Weinstein & Littman, 2012), operate with stochastic
transition functions.
However efficient these proposed algorithms are, the search space in the tree search
approach grows exponentially in the planning time horizon, H. Therefore, local planning
with the tree search approach would not work well with a very long time horizon. In some
applications, a small H is justified, but in other applications, it is not. If an application
problem requires a long time tradeoff between immediate and future rewards, then the tree
178

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

search approach would be impractical. Here, we are motivated to solve such a real-world
application, and therefore need another approach.
In this paper, we consider policy search (Deisenroth, Neumann, & Peters, 2013) as an
effective alternative to solve the planning problem with continuous state/action space and
with a long time horizon. Policy search is a form of local planning. Thus, like the tree search
approach, it operates even with infinitely large or continuous state space. In addition, unlike
the tree search approach, policy search significantly reduces the search space by naturally
integrating the domain-specific expert knowledge into the structure of the policy. More
concretely, the search space of policy search is a set of policies {x : x  }, which are
parameterized by a vector x in   RD . Therefore, the search space is no longer dependent
on the planning time horizon H, the state space S, nor the action space A, but only on
the parameter space . Here, parameter space  can be determined by expert knowledge,
which can significantly reduce the search space.
We use the regret rm as the measure of the policy search algorithms performance:
+



rm = V x (s0 )  V x (m) (s0 )
where x is the optimal policy in the given set of policies {x : x  }, and x+ (m) is the
best policy found by an algorithm after the m steps of planning. An evaluation of each
policy takes mH steps if we consider a fixed planning horizon H. Here, x may differ from
the optimal policy   when   is not covered in the set {x : x  }.
The policy search approach is usually adopted with gradient methods (Baxter & Bartlett,
2001; Kober, Bagnell, & Peters, 2013; Weinstein, 2014). While a gradient method is fast,
it converges to local optima (Sutton, McAllester, Singh, & Mansour, 1999). Further, it
has been observed that it may result in a mere random walk when large plateaus exist in
the surface of the policy space (Heidrich-Meisner & Igel, 2008). Clearly, these problems
can be resolved using global optimization methods at the cost of scalability (Brochu et al.,
2009; Azar, Lazaric, & Brunskill, 2014). Unlike previous policy search methods, our method
guarantees finite-time regret bounds w.r.t. global optima in {x : x  } without strong
additional assumption, and provides a practically useful convergence speed.
6.1 LOGO-OP Algorithm: Leverage (Unknown) Smoothness in Both Policy
Space and Planning Horizon
In this section, we present a simple modification of the LOGO algorithm to leverage not only
the unknown smoothness in policy space but also the known smoothness over the planning
horizon. The former is accomplished by the direct application of the LOGO algorithm to
policy search, and the latter is what the modification in this section aims to do without
losing the advantage of the original LOGO algorithm. We call the modified version, Locally
Oriented Global Optimization with Optimism in Planning horizon (LOGO-OP). As a result
of this modification, we add a new free parameter L.
The pseudocode for the LOGO-OP algorithm is provided in Algorithm 2. By comparing
Algorithms 1 and 2, it can be seen that the LOGO-OP algorithm functions in the same
manner as the LOGO algorithm, except for line 15 (the function evaluation or, equivalently,
the policy evaluation in the policy search) and line 20. Notice that the LOGO algorithm is
directly applicable to the policy search by considering V to be f in Algorithm 1. While the
179

fiKawaguchi, Maruyama, & Zheng

LOGO algorithm does not assume the structure of the function f , the LOGO-OP algorithm
functions with and exploits the given structure of the value function V (i.e., MDP model).
The algorithm functions as follows. The policy evaluation is performed for each policy
x with a parameter x specified by each of the two new hyperrectangles (from line 15-1 to
15-11). Given the initial condition s0  S, the transition function T , the reward function R,
a discount factor   1, and the policy x , the algorithm computes the value of the policy
as in Equation (2) (from line 15-2 to line 15-10, except line 15-6).
The main modification appears in line 15-6 where the algorithm leverages the known
smoothness over the planning horizon. Remember that the unknown smoothness in policy
space (or input space x) is specified as f (x )f (x)  `(x, x ) (from Assumption 1) and thus
it infers the upper bound of the value of a policy that is not yet evaluated but similar (close
in policy space w.r.t. `) to already evaluated polices. Conversely, the known smoothness over
the planning horizon renders the upper bound on the value of a policy while the particular
policy is being evaluated. That is, the known smoothness over the planning horizon can be
written as

t
X
X
 t+1
 j R(sj , x (sj )) 
 j R(sj , x (sj )) 
Rmax
1
j=0

j=0

where 0  t   is a arbitrary point in the planning horizon as in line 15-3 and Rmax is
the maximum reward. This known smoothness is due to the definition of Rmax and the sum
of a geometric series. In the case of the finite horizon with H, we have the same formula
with ( t /(1  ))Rmax being replaced by (H  t)Rmax . In line 15-6, unlike the original
LOGO algorithm, the LOGO-OP algorithm terminates the evaluation of a policy when the
continuation of evaluating the policy is judged to be a misuse of the computational resources
based on the known smoothness over the planning horizon. Concretely, it terminates the
evaluation of a policy when the upper bound of the value of the policy becomes less than
(V +  L), where V + is the value of the best policy found thus far and L is the algorithms
parameter.
When the upper bound of the value of policy becomes less than V + , the planner can
know that the policy is not the best policy. Thus, it is tempting to simply terminate the
policy evaluation with this criterion. However, the essence of the LOGO algorithm is the
utilization of the unknown smoothness embedded in the surface of the value function in the
policy space. In other words, the algorithm makes use of the result of each policy evaluation,
whether the policy is the best one or not. Any interruption of the policy evaluation changes
the shape of the surface of the value function, which interferes with the mechanism of the
LOGO algorithm. Nevertheless, the some degree of the interruption is likely to be beneficial
since our goal is to find the optimal policy instead of surface analysis.
The LOGO-OP algorithm uses L to determine the degree of the interruption. Because
+
V is monotonically increasing along the execution, the value of a policy that is not fully
evaluated owing to line 15-6 in early iterations tends to be greater than the value of a policy
that is not fully evaluated in the later iterations. The algorithm resolves this problem in
line 20 such that it is not biased to divide the interval evaluated in an early iteration.
With smaller L, the LOGO-OP algorithm can stop the evaluation of a non-optimal
policy earlier, at the cost of accuracy in the evaluation of the value functions surface. With
larger L, the algorithm needs to spend more time on the evaluation of a non-optimal policy,
180

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

Algorithm 2: LOGO-OP algorithm
Inputs (problem): the initial condition s0  S, the transition function T , the reward
function R, a discount factor   1 with convergence criteria (or finite horizon H), the
policy space x : x    RD .
1: Inputs (parameter): the search depth function hmax : Z+  [1, ), the local weight
w  Z+ , stopping condition, the maximum reward Rmax , a parameter L.
25: lines 25 are exactly the same as lines 25 in Algorithm 1
6: Adds the initial hyperrectangle 0 to the set: 0  0  {0 } (i.e., 0,0 = 0 )
7: Evaluate the value function V at the center point of 0 , c0,0 : val [0,0 ]  V (c0,0 ),
V +  val [0,0 ]
8: for iteration = 1, 2, 3, . . .
9: val max  , hplus  hupper
10: for k = 0, 1, 2, . . . , max(bmin(hmax (n), hupper )/wc, hplus )
11:
Select a hyperrectangle to be divided: (h, i)  arg maxh,i val [h,i ] for h, i : h,i  k
12:
if val [h,i ] > val max then
13:
val max  val [h,i ], hplus  0, hupper  max(hupper , h + 1), n  n + 1
14:
Divide this hyperrectangle h,i along the longest coordinate direction
- three smaller hyperrectangles are created  left , center , right
- val [center ]  val [h,i ]
15:
Evaluate the value function V at the center points of the two new hyperrectangles:
151:
for each policy x corresponding cleft and cright
152:
z1  0, z2  1, s  s0
153:
for t = 0, 1, 2, . . . ,
154:
z1  z1 + z2 R(s, x (s))
155:
z2  z2 , s  T (s, x (s)) ,
156:
if z1 + ( t+1 /(1  ))Rmax < (V +  L) then Exit loop
157:
if convergence criteria is met then Exit loop
158:
end for
159:
save z1 as the value of the corresponding rectangle
1510:
val [left ]  z1 or val [right ]  z1
1511:
end for
1512:
V +  max(V + , val [left ], val [center ], val [right ])
16:
Group the new hyperrectangles into the set h+1 and remove the original rectangle:
h+1  h+1  {center , left , right }, h  h \ h,i
17:
end if
18:
if stopping condition is met then Return (h, i) = arg maxh,i val [h,i ]
19: end for
20: for all intervals  with val [] < (V +  L) do val []  (V +  L)
21: end for
0:

but can obtain a more accurate estimate of the value functions surface. In the regret
analysis, we show that a certain choice of L ensures a tighter regret bound when compared
to the direct application of the LOGO algorithm.
181

fiKawaguchi, Maruyama, & Zheng

6.2 A Parallel Version of the LOGO-OP Algorithm
The LOGO-OP algorithm presented in Algorithm 2 has four main procedures: Select (line
11), Divide (line 14), Evaluate (line 15), and Group (line 16). A natural way to parallelize the algorithm is to decouple Select from the other three procedures. That is, let the
algorithm first Select z hyperrectangles to be divided, and then allocate the z number of
Divide, Evaluate, and Group to z parallel workers. However, this natural parallelization
has data dependency from one Select to another Select. In other words, the procedure of
the next Select cannot start before Divide, Evaluate, and Group for the previous Select
are finalized. As a result, the parallel overhead tends to be non-negligible. In addition, if
Select chooses less hyperrectangles than parallel workers, then the available resources of the
parallel workers are wasted. Indeed, the latter problem was tackled by creating multiple
initial rectangles in a recent parallelization study of the DIRECT algorithm (He, Verstak,
Sosonkina, & Watson, 2009). While the use of multiple initial rectangles can certainly
mitigate the problem, it still allows the occasional occurrence of the resource wastage, in
addition to requiring the user to specify the arrangement of the initial rectangles.
To solve these problems, we instead decouple the Evaluate procedure from the other
three procedures and allocate only the Evaluate task to each parallel worker. We call the
parallel version, the pLOGO-OP algorithm. The algorithm uses one master process to
conduct Select, Divide, and Group operations and an arbitrary number of parallel workers
to execute Evaluate. The main idea is to temporarily use the artificial value assignment
to the center point of a hyperrectangle in the master process, which is overwritten by the
true value when the parallel worker finishes evaluating the center point. With this strategy,
there is no data dependency and all the parallel workers are occupied with tasks almost
all the time. In this paper, we use the center value of the original hyperrectangle before
division as the temporary artificial value, but the artificial value may be computed using a
more advanced method (e.g., methods in surface analysis) in future work. For the center
point of the initial hyperrectangle, we simply assign the worst possible value (if we have no
knowledge regarding the worst value, we can use ).
The master process keeps selecting new hyperrectangles unless all the parallel workers
are occupied with tasks. This logic ensures that all the parallel workers always have tasks
assigned by the master process, but the master process does not select too many hyperrectangles based on the artificial information. Note that this parallelization makes sense only
when Evaluate is the most time consuming procedure, and it is very likely true for policy
evaluation.
6.3 Regret Analysis
Under a certain condition, all the finite-loss bounds of the LOGO algorithm are directly
translated to the regret bound of the LOGO-OP algorithm. The condition that must be
met is that (V +  L) is less than the center value of the optimal hyperinterval during the
algorithms execution. We state the regret bound more concretely below. For simplicity,
we use the notion of a planning horizon H, which is the effective (non-negligible) planning
horizon for LOGO in accordance with the discount factor, . Let H 0 be the effective
planning horizon of the LOGO-OP algorithm. Then, the planning horizon for LOGO-OP,
H 0 , becomes smaller than that for LOGO, H, as the algorithm finds improved function
182

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

values. This is because the LOGO-OP algorithm terminates each policy evaluation at line
15-6 when the upper bound on the policy value is determined to be lower than (V +  L).
Corollary 3. Let H 0  H be the planning horizon used by the LOGO-OP algorithm at
each policy evaluation. Let V + be the value of the best policy found by the algorithm at any
iteration. Assume that the value function of the policy satisfies Assumptions 1 and B1. If
(V +  L) is maintained to be less than the center value of the optimal hyperinterval, then
the algorithm holds the finite-time loss bound of Theorem 2 with


m
n
.
2H 0
Proof. As the policy search is just a special case of the optimization problem, it is trivial
that the loss bound of Theorem 2 holds for the LOGO algorithm when it is applied to policy
search. Because every function evaluation takes H steps in the planning horizon, we have
n  bm/2Hc in this case. For the LOGO-OP algorithm, only the effect that new parameter
L has in the loss analysis takes place in the proof of Lemma 2. If (V +  L) is maintained
to be less than the center value of the optimal interval, then all the statements in the proof
hold true for the LOGO-OP algorithm as well. Here, due the effect of L, function evaluation
may take less than H steps in the planning horizon. Therefore, we have the statement of
this corollary.
We can tighten the regret bound of the LOGO-OP algorithm by decreasing L, since
the algorithm can then terminate evaluations of unpromising policies earlier, which means
that the value of H 0 in the bound is reduced. However, using a too small value of L that
violates the condition in Corollary 3 leads us to discard the theoretical guarantee. Even
in that case, because the too small value of L only results in a more global search, the
consistency property, limn rn = 0, is still trivially maintained. On the other hand, if we
set L = , the LOGO-OP algorithm becomes equivalent to the direct application of the
LOGO algorithm to policy search, and thus, we have the regret bound of Corollary 3 with
H 0 = H.
The pLOGO-OP algorithm also maintains the same regret bound with n = np where
np counts the number of the total divisions that are devoted to the set of -optimal hyperinterval kw+l (l + 1) , where (w  1)  l  0. While non-parallel versions ensure the
devotion to kw+l (l + 1) , the parallelization makes it possible to conduct division on other
hyperintervals. Thus, considering the worst case, the pLOGO-OP may not improve the
bound in our proof procedure, although the parallelization is likely beneficial in practice.
6.4 Application Study on Nuclear Accident Management
The management of the risk of potentially hazardous complex systems, such as nuclear
power plants, is a major challenge in modern society. In this section, we apply the proposed
method to accident management of nuclear power plants and demonstrate the potential
utility and usage of our method in a real-world application. Our focus is on assessing the
efficiency of containment venting as an accident management measure and on obtaining
knowledge about its effective operational procedure (i.e., policy ). This problem requires
planning with continuous state space and with a very long planning horizon (H  86400),
183

fiKawaguchi, Maruyama, & Zheng

for which dynamic programming (e.g., value iteration), tree-based planning (e.g., A search
and its variants) would not work well (dynamic programming suffers from the curse of
dimensionality for the state space, and the search space of tree-based methods grows exponentially in the planning horizon).
Containment venting is an operation that is used to maintain the integrity of the containment vessel and to mitigate accident consequences by releasing gases from the containment vessel to the atmosphere. In the accident at the Fukushima Daiichi nuclear power
plant in 2011, the containment venting was activated as an essential accident management
measure. As a result, in 2012, the United States Nuclear Regulatory Commission (USNRC) issued an order for 31 nuclear power plants to install the containment vent system
(USNRC, 2013). Currently, many countries are considering the improvement of the containment venting system and its operational procedures (OECD/NEA/CSNI, 2014). The
difficulty of determining its actual benefit and effective operation comes from the fact that
the containment venting also releases fission products (radioactive materials) into the atmosphere. In other words, the effective containment venting must trade off the future risk
of containment failure against the immediate release of fission products (radioactive materials). In our experiments, we use the release amount of the main fission product compound,
cesium iodide (CsI), as a measure of the effectiveness of the containment venting.
In the nuclear accident management literature, an integrated physics simulator is used
as the model of world dynamics or the transition function T and the state space S. The
simulator that we adopt in this paper is THALES2 (Thermal Hydraulics and radionuclide
behavior Analysis of Light water reactor to Estimate Source terms under severe accident
conditions) (Ishikawa, Muramatsu, & Sakamoto, 2002). Thus, the transition function T and
the state space S are fully specified by THALES2. The initial condition s0  S is designed
to approximately simulate the accident at the Fukushima Daiichi nuclear power plant. In
this experiment, we focus on a single initial condition with the deterministic simulator, the
relaxation of which is discussed in the next section. The reward function R is the negative
of the amount of CsI being released in the atmosphere as a result of a state-action pair. We
use the finite-time horizon H = 86400 seconds (24 hours), which is a traditional first phase
time-window considered in risk analysis with nuclear power plant simulations (owing to the
assumption that after 24 hours, many highly uncertain human operations are expected).
We use the following policy structure based on our engineering judgment.
(
1 if ((FP  x1 )  (Press  x2 ))  (Press > 100490),
x =
0 otherwise,
where x = 1 indicates the implementation of the containment venting, FP (g) represents
the amount of CsI in the gas phase of the suppression chamber, and Press (kgf/m2 ) is
the pressure of the suppression chamber. Here, the suppression chamber is the volume in
the containment vessel that is connected to the atmosphere via the containment venting
system. This policy structure reflects our engineering knowledge that the venting should
be done while the fission products exist under a certain amount in the suppression chamber, but should not be operated before the pressure gets larger than a specific value. We
consider x1 = [0, 3000] and x2 = [10810, 100490]. We let x = 1 whenever the pressure
exceeds 100490 kgf/m2 , since the containment failure is considered to probably occur af184

fiCsI release by the Computed Policy (g)

Global Continuous Optimization with Error Bound and Fast Convergence

10000

1000

100
SOO
LOGO
LOGO-OP
pLOGO-OP

10

1
0

10000

20000

30000

40000

Wall time (s)

Figure 9: Performance of the computed policy (CsI release) vs. Wall Time.

ter the pressure exceeds this point. The detail of the experimental setting is outlined in
Appendix A.
We first compare the performance of various algorithms in this problem. For all the
algorithms, we used the same parameter settings as in the benchmark tests in Section 5.

That is, we used hmax (n) = w n  w and a simple adaptive procedure for the parameter w
with W = {3, 4, 5, 6, 8, 30}. For the LOGO-OP algorithm and the pLOGO-OP algorithm,
we blindly set L = 1000 (i.e., there is likely a better parameter setting for L). We used only
eight parallel workers for the pLOGO-OP algorithm.
Figure 9 shows the result of the comparison with wall time  12 hours. The vertical axis
is the total amount of CsI released into the atmosphere (g), which we want to minimize.
Since we conducted containment venting whenever the pressure exceeded 100490 kgf/m2 ,
containment failure was prevented in all the simulation experiments. Thus, the lower the
value along the vertical axis gets, the better the algorithms performances is. As can be seen,
the new algorithms performed well compared to the SOO algorithm. It is also clear that the
two modified versions of the LOGO algorithm improved the performance of the original. For
the LOGO-OP algorithm, the effect of L on the computational efficiency becomes greater
as the found best policy improves. Indeed, the LOGO algorithm required 10798 seconds
for ten policy evaluations and 52329 seconds for 48 evaluations. The LOGO-OP algorithm
required 9297 seconds for ten policy evaluations, and 44678 seconds for 48 evaluations. This
data in conjunction with Figure 9 illustrates the property of the LOGO-OP algorithm that
the policy evaluation becomes faster as the found best policy improves. For the pLOGOOP algorithm, the number of function evaluations performed by the algorithm increased
by a factor of approximately eight (the number of parallel workers) compared to the nonparallel versions. Notice that the parallel version tends to allocate the extra resources to
the global search (as opposed to the local search). We can focus more on the local search by
utilizing the previous results of the policy evaluations; however, the parallel version must
initiate several policy evaluations without waiting for the previous evaluations, resulting in
a tendency for global search. This tendency forced the improvement, in terms of reducing
185

fiKawaguchi, Maruyama, & Zheng

(1)

(2)

(4)

(3)

(5)

(6)

Venting (-) / CsI (g)

Venting (-) / CsI (g)

1
Venting = 1: yes, 0: no

0.8

CsI in the atmosphere

0.6
0.4
0.2
0
0

10000

20000

30000
40000
50000
60000
70000
Time along Accident Progression (s)

80000

Time along Accident Progression (s)

Figure 10: Action sequence generated by found policy and CsI release

Figure 10: Action sequence generated by found policy and CsI release
the amount of CsI, to be moderate relative to the number of policy evaluations in this
particular experiment. However, such a tendency may have a more positive effect in different
problems where increased global search is beneficial. The CPU time per policy evaluation
varied significantly for different policies owing to the different phenomenon computed in the
simulator. On the average, for the LOGO-OP algorithm, it took approximately 930 seconds
per policy evaluation.
Now that we partially confirmed the validity of the pLOGO-OP algorithm, we attempt
to use it to provide meaningful information to this application field. Based on the examination of the results in the above comparison, we narrowed the range of the parameter values
as x1 = [0, 1.2] and x2 = [10330, 10910]. After the computation with CPU time of 86400 (s)
and with eight workers for the parallelization, the pLOGO-OP algorithm found the policy
with x1  0.195 (g) and x2  10880 (kgf/m2 ). With the policy determined, containment
failure was prevented and the total amount of CsI released into the atmosphere was limited
to approximately 0.5 (g) (approximately 0.002% of the total CsI) in the 24 hours after
the initiation of the accident. This is a major improvement because this scenario with our
experimental setting is considered to result in a containment failure or at best, in a large
amount of CsI release, more than 2000 (g) (about 10% of total CsI) in our setting. The
computational cost of CPU time of 86400 (s) is likely acceptable in the application field.
In terms of computational cost, we must consider two factors: the offline computation and
the variation of scenarios. The computational cost with CPU time of 86400 (s) for a phenomenon that requires 86400 (s) is not acceptable for online computation (i.e., determining
a satisfactory policy while the accident is progressing). However, such computational cost
is likely acceptable if we consider preparing acceptable policies for various scenarios in an
offline manner (i.e., determining satisfactory polices before the accident). Such information
regarding these polices can be utilized during an accident by first identifying the accident
scenario with heuristics or machine learning methods (Park & Ahn, 2010). For offline preparation, we must determine policies for various major scenarios and thus if each computation
takes, for example, one month, it may not be acceptable.
Note that the policy found by our method is both novel and nontrivial in the literature,
and yet worked very well. Accordingly, we explain why the policy performed as well as it
did. Figure 10 shows the action sequence generated by the policy found and the amount
186

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

of CsI (g) released versus accident progression time (s). We analyze the action sequence
by dividing it into six phases, as indicated in Figure 10, with the six numbers inside the
parentheses. In the first phase (1), the venting is conducted intermittently in order to
keep the pressure around x2  10880 (kgf/m2 ). In this phase, no fission product has yet
been released from the nuclear fuels. Reducing the pressure and the heat should be done
preferably without releasing fission products, and the actions in this phase accomplish this.
One may wonder why the venting should be done intermittently, instead of continuing to
conduct venting to reduce the pressure as much as possible, which can be done without the
release of fission products only in this phase. This is because reducing the pressure too much
leads to a large difference between the pressures in the suppression chamber and the reactor
pressure vessel, which in turn results in a large mass flow and fission product transportation
from the reactor pressure vessel to the suppression chamber (see Figure 11 in Appendix A
for information about the mass flow paths). The increase in the amount of fission products
in the suppression chamber will likely result in a large release of fission products into the
atmosphere when venting is conducted. Therefore, this specific value of x2 that generates
the intermittent venting works well in the first phase. In the second phase (2), containment
venting is executed all the time since the pressure in the suppression chamber increases
rapidly in this phase (due to the operation of depressurizing the reactor pressure vessel via
the SRV line), and thus, the criterion (Press  x2 ) in the policy is satisfied all the time from
this point. In the beginning of the third phase (3), the amount of CsI in the suppression
chamber exceeds x1  0.195 (g) and thereby no venting is conducted. In the fourth phase
(4), the pressure reaches 100490 (kgf/m2 ) and containment venting is intermittently done
in order to keep the pressure under the point to avoid catastrophic containment failure. In
the fifth phase (5), the containment vent is kept open because the amount of CsI in the gas
phase of the suppression chamber decreases to below x1 (due to the phenomenon illustrated
in Figure 12 in Appendix A). This continuous containment venting decreases the pressure
such that no venting is required in terms of the pressure in the final phase (6), where venting
is not conducted also because the amount of CsI becomes larger than x1 .
Thus, it is clear that the policy found by this AI-related method also has a basis in
terms of physical phenomenon. In addition, the generated action sequence is likely not
simple enough for an engineer to discover with several sensitivity analyses. In particular,
not only did our method solve the known tradeoff between the immediate CsI release and the
risk of future containment failure, the method also discovered the existence of a new tradeoff
between the immediate reduction of the pressure without CsI release and future increase in
the mass flow. Although there is no consensus as to how to operate the containment venting
system at the moment, the tendency is to use it only when the pressure exceeds a certain
point in order to prevent immediate sever damage of containment vessel, which corresponds
only to the fourth phase (4) in Figure 10. In our experiment, such a myopic operation
resulted in containment failure, or a significantly large amount of CsI being released into
the atmosphere (at least more than 4800 (g)).
In summary, we successfully applied the proposed method to investigate the containment
venting policy in nuclear power plant accidents. As a preliminary application study, several
topics are left to future work. From a theoretical viewpoint, future work should consider
a way to mitigate the simulation bias due to model error and model uncertainty. For the
model error, the robotics community is already cognizant that a small error in a simulator
187

fiKawaguchi, Maruyama, & Zheng

can result in poor performance of the derived policy (i.e., simulation bias) (Kober et al.,
2013). We can mitigate this problem by adding a small noise to the model, since the
noise works as regularization to prevent over-fitting as demonstrated by Atkeson (1998).
For the model uncertainty, recent studies in the field of nuclear accident analysis provide
possible directions for the treatment of uncertainty in accident phenomena (Zheng, Itoh,
Kawaguchi, Tamaki, & Maruyama, 2015) and accident scenarios (Kawaguchi, Uchiyama, &
Muramatsu, 2012). As a result of either or both of these countermeasures, the objective
function becomes stochastic, and thereby we may first expand the pLOGO-OP algorithm to
stochastic case. On the other hand, from the phenomenological point of view, future work
should consider other fission products as well as CsI. Such fission products include, but are
not limited to, Xe, Cs, I, Te, Sr, and Ru. In particular, a noble gas element, such as Xe, can
be a major concern in an accident (it tends to be released a lot and is easily diffused into
the atmosphere), but its property is different from CsI (its half-life is much smaller). Thus,
if Xe is identified as a major concern, one may consider a significantly different policy from
ours (considering its half-life, one may delay conducting the containment venting).

7. Conclusions
In this paper, we proposed the LOGO algorithm, the global optimization algorithm that is
designed to operate well in practice while maintaining a finite-loss bound with no strong
additional assumption. The analysis of the LOGO algorithm generalized previous finite-loss
bound analysis. Importantly, the analysis also provided several insights regarding practical
usage of this type of algorithm by showing the relationship among the loss bound, the
division strategy, and the algorithms parameters.
We applied the LOGO algorithm to an AI planning problem with the policy search
framework, and showed that the performance of the algorithm can be improved by leveraging not only the unknown smoothness in policy space, but also the known smoothness
in the planning horizon. As our study is motivated to solve real-world engineering applications, we also discussed a parallelization design that utilizes the property of AI planning in
order to minimize the overhead. The resulting algorithm, the pLOGO-OP algorithm, was
successfully applied to a complex engineering problem, namely, policy derivation for nuclear
accident management.
Aside from the planning problem that we considered, the LOGO algorithm can be also
used, for example, to optimize parameters of other algorithms (i.e., algorithm configuration). In the AI community, the algorithm configuration problem has been addressed by
several methods, including a genetic algorithm (Ansotegui, Sellmann, & Tierney, 2009), discrete optimization with convergence guarantee in the limit (Hutter, Hoos, Leyton-Brown, &
Stutzle, 2009), the racing approach originated from the machine learning community (Hoeffding Races) (Birattari, Yuan, Balaprakash, & Stutzle, 2010), model-based optimization
with convergence guarantee in the limit (Hutter, Hoos, & Leyton-Brown, 2011), a simultaneous use of several randomized local optimization methods (Gyorgy & Kocsis, 2011),
and Bayesian optimization (Snoek, Larochelle, & Adams, 2012). Compared to the previous
parameter tuning methods, the LOGO algorithm itself is limited to optimizing continuous
deterministic functions. To apply it to stochastic functions, a future work would modify
the LOGO algorithm as was done for the SOO algorithm in a previous study (Valko, Car188

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

pentier, & Munos, 2013). To consider categorical and/or discrete parameters in addition to
continuous parameters, a possibility could be to use the LOGO algorithm as a subroutine
to deal with the continuous variables in one of the previous methods.
The promising results presented in this paper suggest several interesting directions for
future research. An important direction is to leverage additional assumptions. Since LOGO
is based on a weak set of assumptions, it would be natural to use LOGO as a main subroutine but add other mechanisms to account for additional assumptions. As a example,
we illustrated that LOGO would be able to scale up for a higher dimension with additional
assumptions in Section 5. Another possibility is to add a GP assumption based on the
idea presented in a recent paper (Kawaguchi, Kaelbling, & Lozano-Perez, 2015). Future
work also would design an autonomous agent by integrating our planning algorithm with
a learning/exploration algorithm (Kawaguchi, 2016a). One remaining challenge of LOGO
is to derive a series of methods that adaptively determine the algorithms free parameter
w. As illustrated in our experiment, the achievement in this topic not only mitigates the
problem of parameter sensitivity, but also would improve the algorithms performance.

Acknowledgments
This work was carried out while the first author was at the Japan Atomic Energy Agency.
The authors would like to thank Dr. Hiroto Itoh and Mr. Jun Ishikawa at JAEA for several
discussions on the related topics. The authors would like to thank Mr. Lawson Wong at
MIT for his insightful comments. The authors would like to thank anonymous reviewers
for their insightful and constructive comments.

Appendix A. Experimental Design of Application Study on Nuclear
Accident Management
In this appendix, we present the experimental setting for the Application Study on Nuclear
Accident Management in Section 6.4. With THALES2, we consider the volume nodalization as shown in Figure 11. The reactor pressure vessel was divided into seven volumes,
consisting of core, upper plenum, lower plenum, steam dome, downcomer, and recirculation
loops A and B. The containment vessel consists of drywell, suppression chamber, pedestal
and vent pipes. The atmosphere and suppression chamber are connected via the containment venting system (S/C venting). The plant data and initial conditions were determined
based on the data of the Unit 1 of the Browns Ferry nuclear power plant (BWR4/Mark-I)
and the construction permit application forms of BWR plants in Japan. The failure of
the containment vessel is assumed to occur when the pressure of the vessel becomes 2.5
times greater than the design pressure. Here, the design pressure is 3.92 (kgf/cm2 g) and
the criterion for the containment failure is 108330 kgf/m2 . The degree of opening for the
containment venting was fixed at 25% and no filtering was considered.
We consider the TQUV sequence as the accident scenario. In the TQUV sequence, no
Emergency Core Cooling Systems (ECCSs) functions, similar to the case of the accident
at the Fukushima Daiichi nuclear power plant. The TQUV sequence is one of the major
scenarios considered in Probabilistic Risk Assessment (PRA) of nuclear plants. Therefore,
189

fiFigure 11: Nodalization of physical space
Kawaguchi, Maruyama, & Zheng

Reactor building
Containment

CST
Steam
dome

RCIC
HPCI

Plenum

SRV

DC

Core

LPCI

Lower
plenum
Loop A

Loop B

Vacuum
breaker

Pedestal

Vent
pipe
Suppression chamber

Containm
ent Vent

Figure 11: Nodalization of physical space

FP: Fission Products

FP: Fission Products

12: Phenomenon
considered
for fission
producttransportation
transportation
Figure 1:Figure
Phenomenon
considered
for fission
product
190



fiGlobal Continuous Optimization with Error Bound and Fast Convergence

our results show a promising benefit of containment venting, as long as we use it with a
good policy.
The simulator we developed at the Japan Atomic Energy Agency and adopted in this
experiment (THALES2) computes the transportation of fission products as well as thermal
hydraulics in each volume of Figure 11 and core melt progression in the Core volume. The
transportation of fission products considered in the experiment is shown in Figure 12. The
details of the computation of the THALES2 code are found in the paper by Ishikawa et al.
(2002). Figure 11 and Figure 12 are the modified versions of the graphs used in a previous
presentation about the ongoing development of the THALES2 code, which was given at the
USNRCs 25th Regulatory Information Conference (Maruyama, 2013).

References
Ansotegui, C., Sellmann, M., & Tierney, K. (2009). A gender-based genetic algorithm for
the automatic configuration of algorithms. In Proceedings of the 15th International
Conference on Principles and Practice on Constraint Programing (CP 2009).
Atkeson, C. G. (1998). Nonparametric model-based reinforcement learning. In Advances in
Neural Information Processing Systems (NIPS), pp. 10081014.
Azar, M. G., Lazaric, A., & Brunskill, E. (2014). Stochastic optimization of a locally
smooth function under correlated bandit feedback. In 31st International Conference
on Machine Learning (ICML).
Baxter, J., & Bartlett, P. L. (2001). Infinite-horizon policy-gradient estimation. Journal of
Artificial Intelligence Research (JAIR), 15, 319350.
Birattari, M., Yuan, Z., Balaprakash, P., & Stutzle, T. (2010). F-race and iterated F-race:
An overview. In Experimental Methods for the Analysis of Optimization Algorithms,
pp. 311336. Springer-Verlag.
Brochu, E., Cora, V. M., & de Freitas, N. (2009). A tutorial on Bayesian optimization of
expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. Technical report No. UBC TR-2009-23 and arXiv:1012.2599v1,
Dept. of Computer Science, University of British Columbia.
Bubeck, S., & Munos, R. (2010). Open loop optimistic planning. In Conference on Learning
Theory.
Bubeck, S., Stoltz, G., & Yu, J. Y. (2011). Lipschitz bandits without the Lipschitz constant.
In Proceedings of the 22nd International Conference on Algorithmic Learning Theory.
Busoniu, L., Daniels, A., Munos, R., & Babuska, R. (2013). Optimistic planning for
continuous-action deterministic systems. In 2013 Symposium on Adaptive Dynamic
Programming and Reinforcement Learning.
Carter, R. G., Gablonsky, J. M., Patrick, A., Kelly, C. T., & Eslinger, O. J. (2001). Algorithms for noisy problems in gas transmission pipeline optimization. Optimization
and Engineering, 2 (2), 139157.
Daly, R., & Shen, Q. (2009). Learning Bayesian network equivalence classes with ant colony
optimization. Journal of Articial Intelligence Research, 35 (1), 391447.
191

fiKawaguchi, Maruyama, & Zheng

Deisenroth, M. P., Neumann, G., & Peters, J. (2013). A survey on policy search for robotics.
Foundations and Trends in Robotics, 2, 1142.
Deisenroth, M. P., & Rasmussen, C. E. (2011). PILCO: A model-based and data-efficient
approach to policy search. In 28th International Conference on Machine Learning
(ICML).
Dixon, L. C. W. (1978). Global Optima Without Convexity. Hatfield, England: Numerical
Optimization Centre, Hatfield Polytechnic.
Gablonsky, J. M. (2001). Modifications of the direct algorithm. Ph.D. thesis, North Carolina
State University, Raleigh, North Carolina.
Gullapalli, V., Franklin, J., & Benbrahim, H. (1994). Acquiring robot skills via reinforcement
learning. Control Systems Magazine, IEEE, 14 (1), 1324.
Gyorgy, A., & Kocsis, L. (2011). Efficient multi-start strategies for local search algorithms.
Journal of Artificial Intelligence Research (JAIR), 41, 407444.
Hansen, P., & Jaumard, B. (1995). Lipschitz optimization. In Horst, R., & Pardalos,
P. M. (Eds.), Handbook of Global Optimization, pp. 407493. The Netherlands: Kluwer
Academic Publishers.
Hansen, P., Jaumard, B., & Lu, S. H. (1991). On the number of iterations of Piyavskiis
global optimization algorithm. Mathematics of Operations Research, 16, 334350.
He, J., Verstak, A., Sosonkina, M., & Watson, L. (2009). Performance modeling and analysis
of a massively parallel DIRECT, Part 1. Journal of High Performance Computing
Applications, 23, 1428.
He, J., Verstak, A., Watson, L. T., Stinson, C. A., et al. (2004). Globally optimal transmitter
placement for indoor wireless communication systems. IEEE Transactions on Wireless
Communications, 3 (6), 19061911.
Heidrich-Meisner, V., & Igel, C. (2008). Evolution strategies for direct policy search. In
Proceedings of the 10th International Conference on Parallel Problem Solving from
Nature: PPSN X, pp. 428437. Springer-Verlag.
Horst, R., & Tuy, H. (1990). Global Optimization: Deterministic Approaches. Berlin:
Springer.
Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2011). Sequential model-based optimization
for general algorithm configuration. In Learning and Intelligent Optimization, 5, 507
523.
Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: An automatic
algorithm configuration framework. Journal of Artificial Intelligence Research (JAIR),
36, 267306.
Ishikawa, J., Muramatsu, K., & Sakamoto, T. (2002). Systematic source term analysis for
level 3 PSA of a BWR with Mark-II containment with THALES-2 code. In Proceedings
of 10th International Conference of Nuclear Engineering, ICONE-10-22080.
Jones, D. R., Perttunen, C. D., & Stuckman, B. E. (1993). Lipschitzian optimization without
the Lipschitz constant. Journal of Optimization Theory and Applications, 79 (1), 157
181.
192

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

Kawaguchi, K., Uchiyama, T., & Muramatsu, K. (2012). Efficiency of analytical methodologies in uncertainty analysis of seismic core damage frequency. Journal of Power
and Energy Systems, 6 (3), 378393.
Kawaguchi, K. (2016a). Bounded optimal exploration in MDP. In Proceedings of the 30th
AAAI Conference on Artificial Intelligence (AAAI).
Kawaguchi, K. (2016b). Deep learning without poor local minima. Massachusetts Institute
of Technology, Technical Report, MIT-CSAIL-TR-2016-005.
Kawaguchi, K., Kaelbling, L. P., & Lozano-Perez, T. (2015). Bayesian optimization with
exponential convergence. In Advances in Neural Information Processing (NIPS).
Kirk, D. E. (1970). Optimal Control Theory. Englewood Cliffs, NJ: Prentice-Hall.
Kleinberg, R. D., Slivkins, A., & Upfal, E. (2008). Multi-armed bandit problems in metric
spaces. In Proceedings of the 40th ACM Symposium on Theory of Computing, pp.
681690.
Kober, J., Bagnell, J. A. D., & Peters, J. (2013). Reinforcement learning in robotics: A
survey. International Journal of Robotics Research, 32.
Kvasov, D. E., Pizzuti, C., & Sergeyev, Y. D. (2003). Local tuning and partition strategies
for diagonal GO methods. Numerische Mathematik, 94 (1), 93106.
Maruyama, Y. (2013). Development of THALES2 code and application to analysis of the
accident at Fukushima Daiichi Nuclear Power Plant. In The NRCs 25th Regulatory
Information Conference.
Mayne, D. Q., & Polak, E. (1984). Outer approximation algorithm for nondifferentiable
optimization problems. Journal of Optimization Theory and Applications, 42 (1), 19
30.
McDonald, D. B., Grantham, W. J., Tabor, W. L., & Murphy, M. J. (2007). Global and
local optimization using radial basis function response surface models. Applied Mathematical Modelling, 31 (10), 20952110.
Mladineo, R. H. (1986). An algorithm for finding the global maximum of a multimodal,
multivariate function. Mathematical Programming, 34, 188200.
Munos, R. (2011). Optimistic optimization of deterministic functions without the knowledge
of its smoothness. In Advances in Neural Information Processing Systems (NIPS), pp.
783791.
Munos, R. (2013). From bandits to Monte-Carlo tree search: The optimistic principle applied
to optimization and planning. Foundations and Trends in Machine Learning, 7 (1),
1130.
Murty, K. G., & Kabadi, S. N. (1987). Some np-complete problems in quadratic and
nonlinear programming. Mathematical programming, 39 (2), 117129.
OECD/NEA/CSNI (2014). Status report on filtered containment venting. Technical report
NEA/CSNI/R(2014)7, JT03360082.
Park, Y., & Ahn, I. (2010). SAMEX: A severe accident management support expert. Annals
of Nuclear Energy, 37 (8), 10671075.
193

fiKawaguchi, Maruyama, & Zheng

Pinter, J. (1986). Globally convergent methods for n-dimensional multiextremal optimization. Optimization, 17, 187202.
Piyavskii, S. A. (1967). An algorithm for finding the absolute minimum of a function.
Theory of Optimal Solutions, 2, 1324. Kiev, IK AN USSR.
Rios, L. M., & Sahinidis, N. V. (2013). Derivative-free optimization: A review of algorithms
and comparison of software implementations. Journal of Global Optimization, 56,
12471293.
Russell, S. J., & Norvig, P. (2009). Articial intelligence: A modern approach (3rd edition).
Prentice-Hall.
Ryoo, H. S., & Sahinidis, N. V. (1996). A branch-and-reduce approach to global optimization. Journal of Global Optimization, 8 (2), 107138.
Shubert, B. O. (1972). A sequential method seeking the global maximum of a function.
SIAM Journal on Numerical Analysis, 9, 379388.
Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems
(NIPS).
Strongin, R. G. (1973). On the convergence of an algorithm for finding a global extremum.
Engineering Cybernetics, 11, 549555.
Surjanovic, S., & Bingham, D. (2013). Virtual library of simulation experiments: Test
functions and datasets. Retrieved July 2, 2014, from http://www.sfu.ca/~ssurjano.
Sutton, R. S., McAllester, D., Singh, S., & Mansour, Y. (1999). Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural
Information Processing Systems, 12, 10571063.
USNRC (2013). Hardened Vents and Filtration for Boiling Water Reactors with Mark I and
Mark II containment designs . Retrieved in August 2015 from http://www.nrc.gov/
reactors/operating/ops-experience/japan-dashboard/hardened-vents.html .
Valko, M., Carpentier, A., & Munos, R. (2013). Stochastic simultaneous optimistic optimization. In Proceedings of the 30th International Conference on Machine Learning
(ICML).
Wang, Z., Shakibi, B., Jin, L., & de Freitas, N. (2014). Bayesian multi-scale optimistic
optimization. In AI and Statistics, pp. 10051014.
Wang, Z., Zoghi, M., Hutter, F., Matheson, D., & De Freitas, N. (2013). Bayesian optimization in high dimensions via random embeddings. In Proceedings of the Twenty-Third
international joint conference on Artificial Intelligence, pp. 17781784. AAAI Press.
Weinstein, A. (2014). Local planning for continuous Markov decision processes. Ph.D.
thesis, Rutgers, The State University of New Jersey.
Weinstein, A., & Littman, M. L. (2012). Bandit-based planning and learning in continuousaction Markov decision processes. In International Conference on Automated Planning
and Scheduling, pp. 306314.
194

fiGlobal Continuous Optimization with Error Bound and Fast Convergence

Zheng, X., Itoh, H., Kawaguchi, K., Tamaki, H., & Maruyama, Y. (2015). Application of
Bayesian nonparametric models to the uncertainty and sensitivity analysis of source
term in a BWR severe accident. Journal of Reliability Engineering & System Safety,
138, 253262.
Zwolak, J. W., Tyson, J. J., & Watson, L. T. (2005). Globally optimized parameters for a
model of mitotic control in frog egg extracts. IEE Systems Biology, 152 (2), 8192.

195

fiJournal of Artificial Intelligence Research 56 (2016) 403428

Submitted 11/15; published 07/16

On the Satisfiability Problem for SPARQL Patterns
Xiaowang Zhang

xiaowangzhang@tju.edu.cn

School of Computer Science and Technology,
Tianjin University, China
Tianjin Key Laboratory of
Cognitive Computing and Application,
Tianjin, China

Jan Van den Bussche

jan.vandenbussche@uhasselt.be

Hasselt University, Belgium

Francois Picalausa

fpicalausa@gmail.com

Abstract
The satisfiability problem for SPARQL 1.0 patterns is undecidable in general, since the
relational algebra can be emulated using such patterns. The goal of this paper is to delineate
the boundary of decidability of satisfiability in terms of the constraints allowed in filter
conditions. The classes of constraints considered are bound-constraints, negated boundconstraints, equalities, nonequalities, constant-equalities, and constant-nonequalities. The
main result of the paper can be summarized by saying that, as soon as inconsistent filter
conditions can be formed, satisfiability is undecidable. The key insight in each case is to find
a way to emulate the set difference operation. Undecidability can then be obtained from
a known undecidability result for the algebra of binary relations with union, composition,
and set difference. When no inconsistent filter conditions can be formed, satisfiability is
decidable by syntactic checks on bound variables and on the use of literals. Although the
problem is shown to be NP-complete, it is experimentally shown that the checks can be
implemented efficiently in practice. The paper also points out that satisfiability for the
so-called well-designed patterns can be decided by a check on bound variables and a check
for inconsistent filter conditions.

1. Introduction
The Resource Description Framework is a popular data model for information on the Web.
RDF represents information in the form of directed, labeled graphs. The standard query
language for RDF data is SPARQL (Harris & Seaborne, 2013). The current version 1.1 of
SPARQL extends SPARQL 1.0 (Prudhommeaux & Seaborne, 2008) with important features such as aggregation and regular path expressions (Arenas, Conca, & Perez, 2012).
Other features, such as negation and subqueries, have also been added, but mainly for efficiency reasons, as they were already expressible, in a more involved manner, in version 1.0.
Hence, it is still relevant to study the fundamental properties of SPARQL 1.0. In this paper,
we follow the elegant formalization of SPARQL 1.0 by Arenas, Gutierrez, & Perez (2009)
which is eminently suited for theoretical investigations.
The fundamental problem that we investigate is that of satisfiability of SPARQL patterns. A pattern is called satisfiable if there exists an RDF graph under which the pattern
evaluates to a nonempty set of mappings. For any query language, satisfiability is clearly one
c
2016
AI Access Foundation. All rights reserved.

fiZhang, Van den Bussche, & Picalausa

of the essential properties one needs to understand if one wants to do automated reasoning.
Since SPARQL patterns can emulate relational algebra expressions (Angles & Gutierrez,
2008; Polleres, 2007; Arenas & Perez, 2011), and satisfiability for relational algebra is undecidable (Abiteboul, Hull, & Vianu, 1995), the general satisfiability problem for SPARQL
is undecidable as well.
Whether or not a pattern is satisfiable depends mainly on the filter operations appearing
in the pattern; without filter operations, a pattern is always satisfiable except for trivial cases
where a literal occurs in the wrong place. The goal of this paper is to precisely delineate
the decidability of SPARQL fragments that are defined in terms of the constraints that can
be used as filter conditions. The six basic classes of constraints we consider are boundconstraints; equalities; constant-equalities; and their negations. In this way, fragments of
SPARQL can be constructed by specifying which kinds of constraints are allowed as filter
conditions. For example, in the fragment SPARQL(bound, 6=, 6=c ), filter conditions can only
be bound constraints, nonequalities, and constant-nonequalities.
Our main result states that the only fragments for which satisfiability is decidable are
the two fragments SPARQL(bound, =, 6=c ) and SPARQL(bound, 6=, 6=c ) and their subfragments. Consequently, as soon as either negated bound-constraints, or constant-equalities, or
combinations of equalities and nonequalities are allowed, the satisfiability problem becomes
undecidable. Each undecidable case is established by showing how the set difference operation can be emulated. This was already known using negated bound-constraints (Angles
& Gutierrez, 2008; Arenas & Perez, 2011); so we show it is also possible using constantequalities, and using combinations of equalities and nonequalities, but in no other way.
Undecidability can then be obtained from a known undecidability result for the algebra
of binary relations with union, composition, and set difference (Tan, Van den Bussche, &
Zhang, 2014).
In the decidable cases, satisfiability can be decided by syntactic checks on bound variables and the use of literals. Although the problem is shown to be NP-complete, it is
experimentally shown that the checks can be implemented efficiently in practice.
At the end of the paper we look at a well-behaved class of patterns known as the
well-designed patterns (Perez et al., 2009). We observe that satisfiability of well-designed
patterns can be decided by combining the check on bound variables with a check for inconsistent filter conditions.
This paper is further organized as follows. In the next section, we introduce syntax
and semantics of SPARQL patterns and introduce the different fragments under consideration. Section 3 introduces the satisfiability problem and shows satisfiability checking for the
fragments SPARQL(bound, =, 6=c ) and SPARQL(bound, 6=, 6=c ). Section 4 shows undecidability for the fragments SPARQL(bound), SPARQL(=c ), and SPARQL(=, 6=). Section 5
considers well-designed patterns.
Section 6 reports on experiments that test our decision methods in practice. In Section 7
we briefly discuss how our results extend to the new operators that have been added to
SPARQL 1.1. We conclude in Section 8.
404

fiSatisfiability Problem for SPARQL

2. SPARQL and Fragments
In this section we recall the syntax and semantics of SPARQL patterns, closely following the
core SPARQL formalization given by Arenas, Gutierrez, & Perez (2009).1 The semantics
we use is set-based, whereas the semantics of real SPARQL is bag-based. However, for
satisfiability (the main topic of this paper), it makes no difference whether we use a set or
bag semantics (Schmidt, Meier, & Lausen, 2010, Lemma 1).
In this section we will also define the language fragments defined in terms of allowed
filter conditions, which will form the object of this paper.
2.1 RDF Graphs
Let I, B, and L be infinite sets of IRIs, blank nodes and literals, respectively. These three
sets are pairwise disjoint. We denote the union I  B  L by U , and elements of I  L will
be referred to as constants. Note that blank nodes are not constants.
A triple (s, p, o)  (I  B)  I  U is called an RDF triple. An RDF graph is a finite set
of RDF triples.
2.2 Syntax of SPARQL Patterns
Assume furthermore an infinite set V of variables, disjoint from U . The convention in
SPARQL is that variables are written beginning with a question mark, to distinguish them
from constants. We will follow this convention in this paper.
SPARQL patterns are inductively defined as follows.
 Any triple from (I  L  V )  (I  V )  (I  L  V ) is a pattern (called a triple pattern).
 If P1 and P2 are patterns, then so are the following:
 P1 UNION P2 ;
 P1 AND P2 ;
 P1 OPT P2 .
 If P is a pattern and C is a constraint (defined next), then P FILTER C is a pattern;
we call C the filter condition.
Here, a constraint can have one of the six following forms:
1. bound-constraint: bound(?x)
2. negated bound-constraint: bound(?x)
3. equality: ?x = ?y
4. nonequality: ?x 6= ?y with ?x and ?y distinct variables
5. constant-equality: ?x = c with c a constant
6. constant-nonequality: ?x 6= c
1. Arenas, Perez, and Guttierez (2009) discuss minor deviations between the formalization and real
SPARQL, and why these differences are inessential for the purpose of formal investigation.

405

fiZhang, Van den Bussche, & Picalausa

We do not need to consider conjunctions and disjunctions in filter conditions, since
conjunctions can be expressed by repeated application of filter, and disjunctions can be
expressed using UNION. Hence, by going to disjunctive normal form, any predicate built
using negation, conjunction, and disjunction is indirectly supported by our language.
Moreover, real SPARQL also allows blank nodes in triple patterns. This feature has been
omitted from the formalization because blank nodes in triple patterns can be equivalently
replaced by variables.
2.3 Semantics of SPARQL Patterns
The semantics of patterns is defined in terms of sets of so-called solution mappings, hereinafter simply called mappings. A solution mapping is a total function  : S  U on some
finite set S of variables. We denote the domain S of  by dom().
We make use of the following convention.
Convention. For any mapping  and any constant c  I  L, we agree that (c) equals c
itself.
In other words, mappings are by default extended to constants according to the identity
mapping.
Now given a graph G and a pattern P , we define the semantics of P on G, denoted by
JP KG , as a set of mappings, in the following manner.
 If P is a triple pattern (u, v, w), then

JP KG := { : {u, v, w}  V  U | ((u), (v), (w))  G}.
This definition relies on Convention 2.3 formulated above.
 If P is of the form P1 UNION P2 , then
JP KG := JP1 KG  JP2 KG .
 If P is of the form P1 AND P2 , then
JP KG := JP1 KG o
n JP2 KG ,
where, for any two sets of mappings 1 and 2 , we define
1 o
n 2 = {1  2 | 1  1 and 2  2 and 1  2 }.
Here, two mappings 1 and 2 are called compatible, denoted by 1  2 , if they agree
on the intersection of their domains, i.e., if for every variable ?x  dom(1 )dom(2 ),
we have 1 (?x) = 2 (?x). Note that when 1 and 2 are compatible, their union 1 2
is a well-defined mapping; this property is used in the formal definition above.
 If P is of the form P1 OPT P2 , then
JP KG := (JP1 KG o
n JP2 KG )  (JP1 KG r JP2 KG ),
where, for any two sets of mappings 1 and 2 , we define
1 r 2 = {1  1 | 2  2 : 1  2 }.
406

fiSatisfiability Problem for SPARQL

 Finally, if P is of the form P1 FILTER C, then
JP KG := {  JP1 KG |  |= C}
where the satisfaction of a constraint C by a mapping , denoted by  |= C, is defined
as follows:
1.  |= bound(?x) if ?x  dom();
2.  |= bound(?x) if ?x 
/ dom();
3.  |= ?x = ?y if ?x, ?y  dom() and (?x) = (?y);
4.  |= ?x 6= ?y if ?x, ?y  dom() and (?x) 6= (?y);
5.  |= ?x = c if ?x  dom() and (?x) = c;
6.  |= ?x 6= c if ?x  dom() and (?x) 6= c.
Note that  |= ?x 6= ?y is not the same as  6|= ?x = ?y, and similarly for  |= ?x 6= c.
This is in line with the three-valued logic semantics for filter conditions used in the official
semantics (Arenas et al., 2009). For example, if ?x 
/ dom(), then in three-valued logic
?x = c evaluates to error under ; consequently, also ?x = c evaluates to error under .
Accordingly, in the semantics above, we have both  6|= ?x = c and  6|= ?x 6= c.
2.4 SPARQL Fragments
We can form fragments of SPARQL by specifying which of the six classes of constraints are
allowed as filter conditions. We denote the class of bound-constraints by bound, negated
bound-constraints by bound, equalities by =, nonequalities by 6=, constant-equalities
by =c , and constant-nonequalities by 6=c . Then for any subset F of {bound, bound, =
, 6=, =c , 6=c } we can form the fragment SPARQL(F ). For example, in SPARQL(bound, =,
6=c ), filter conditions can only be bound constraints, equalities, and constant-nonequalities.

3. Satisfiability: Decidable Fragments
A pattern P is called satisfiable if there exists a graph G such that JP KG is nonempty.
In general, checking satisfiability is a very complicated, indeed undecidable, problem. But
for the two fragments SPARQL(bound, =, 6=c ) and SPARQL(bound, 6=, 6=c ), it will turn out
that there are essentially only two possible reasons for unsatisfiability.
The first possible reason is that the pattern specifies a literal value in the first position
of some RDF triple, whereas RDF triples can only have literals in the third position. For
example, using the literal 42, the triple pattern (42, ?x, ?y) is unsatisfiable. Note that literals
in the middle position of a triple pattern are already disallowed by the definition of triple
pattern, so we only need to worry about the first position.
This discrepancy between triple patterns and RDF triples is easy to sidestep, however.
In the Appendix we show how, without loss of generality, we may assume from now on that
patterns do not contain any triple pattern (u, v, w) where u is a literal.
The second and main possible reason for unsatisfiability is that filter conditions require
variables to be bound together in a way that cannot be satisfied by the subpattern to which
407

fiZhang, Van den Bussche, & Picalausa

the filter applies. For example, the pattern
((?x, a, ?y) UNION (?x, b, ?z)) FILTER (bound(?y)  bound(?z))
is unsatisfiable. Note that bound constraints are not strictly necessary to illustrate this
phenomenon: if in the above example we replace the filter condition by ?y = ?z the resulting
pattern is still unsatisfiable.
We next prove formally that satisfiability for patterns in SPARQL(bound, =, 6=c ) and
SPARQL(bound, 6=, 6=c ) is effectively decidable, by catching the reason for unsatisfiability
described above. Note also that the two fragments can not be combined, since satisfiability
for SPARQL(=, 6=) is undecidable as we will see in the next Section.
3.1 Checking Bound Variables
To perform bound checks on variables, we associate to every pattern P a set (P ) of schemes,
where a scheme is simply a set of variables, in the following way.2
 If P is a triple pattern (u, v, w), then (P ) := {{u, v, w}  V }.
 (P1 UNION P2 ) := (P1 )  (P2 ).
 (P1 AND P2 ) := {S1  S2 | S1  (P1 ) and S2  (P2 )}.
 (P1 OPT P2 ) := (P1 AND P2 )  (P1 ).
 (P1 FILTER C) := {S  (P1 ) | S ` C}, where S ` C is defined as follows:
 If C is of the form bound(?x) or ?x = c or ?x 6= c, then S ` C if ?x  S;
 If C is of the form ?x = ?y or ?x 6= ?y, then S ` C if ?x, ?y  S;
 S ` bound(?x) if ?x 
/ S.
Example 1. Consider the pattern
P = (?x, p, ?y) OPT ((?x, q, ?z) UNION (?x, r, ?u)).
For the subpattern P1 = (?x, q, ?z) UNION (?x, r, ?u) we have (P1 ) = {{?x, ?z}, {?x, ?u}}.
Hence, ((?x, p, ?y) AND P1 ) = {{?x, ?y, ?z}, {?x, ?y, ?u}}. We conclude that (P ) =
{{?x, ?y}, {?x, ?y, ?z}, {?x, ?y, ?u}}.
Example 2. For another example, consider the pattern
P = ((?x, p, ?y) OPT ((?x, q, ?z) FILTER ?y = ?z)) FILTER ?x 6= c.
We have (?x, q, ?z) = {{?x, ?z}}. Note that {?x, ?z} 6` ?y = ?z, because ?y 
/ {?x, ?z}.
Hence, for the subpattern P1 = (?x, q, ?z) FILTER ?y = ?z we have (P1 ) = . For the
subpattern P2 = (?x, p, ?y) OPT P1 we then have (P2 ) = (?x, p, ?y) = {{?x, ?y}}. Since
{?x, ?y} ` ?x 6= c, we conclude that (p) = {{?x, ?y}}.
2. We define (P ) for general patterns, not only for those belonging to the fragments considered in this
Section, because we will make another use of (P ) in Section 5.

408

fiSatisfiability Problem for SPARQL

We now establish the main result of this Section.
Theorem 3. Let P be a SPARQL(bound, =, 6=c ) or SPARQL(bound, 6=, 6=c ) pattern. Then
P is satisfiable if and only if (P ) is nonempty.
The only-if direction of Theorem 3 is the easy direction and is given by the following
Lemma 4. Note that this lemma holds for general patterns; it can be straightforwardly
proven by induction on the structure of P .
Lemma 4. Let P be a pattern and G a graph. If   JP KG then there exists S  (P ) such
that dom() = S.
The if direction of Theorem 3 for SPARQL(bound, =, 6=c ) is given by the following
Lemma 5.
In the following we use var(P ) to denote the set of all variables occurring in a pattern
P .3
Lemma 5. Let P be a pattern in SPARQL(bound, =, 6=c ). Let c  I be a constant that does
not appear in any constant-nonequality filter condition in P . With the constant mapping
 : var(P )  {c}, let G be the RDF graph consisting of all possible triples ((u), (v), (w))
where (u, v, w) is a triple pattern in P .
Then for every S  (P ) there exists S 0  S such that |S 0 belongs to JP KG .
Proof. By induction on the structure of P . If P is a triple pattern (u, v, w) then S =
{u, v, w}  V . Since (|S (u), |S (v), |S (w)) = ((u), (v), (w))  G, we have |S  JP KG
and we can take S 0 = S.
If P is of the form P1 UNION P2 , then the claim follows readily by induction.
If P is of the form P1 AND P2 , then we have S = S1  S2 with Si  (Pi ) for i = 1, 2.
By induction, there exists Si0  Si such that |Si0  JPi KG . Clearly |S10  |S20 since they
are restrictions of the same mapping. Hence |S10  |S20 = S10 S20  JP KG and we can take
S 0 = S10  S20 .
If P is of the form P1 OPT P2 , then there are two possibilities.
 If S  (P1 AND P2 ) then we can reason as in the previous case.
 If S  (P1 ) then by induction there exists S10  S so that |S10  JP1 KG . Now there
are two further possibilities:
 If (P2 ) is nonempty then by induction there exists some S20 so that |S20  JP2 KG .
We can now reason again as in the case P1 AND P2 .
 Otherwise, by Lemma 4 we know that JP2 KG is empty. But then JP KG = JP1 KG
and we can take S 0 = S10 .
Finally, if P is of the form P1 FILTER C, then we know that S  (P1 ) and S ` C.
By induction, there exists S 0  S such that |S 0  JP1 KG . We show that |S 0  JP KG by
showing that |S 0 |= C. There are three possibilities for C.
3. We also use the following standard notion of restriction of a mapping. If f : X  Y is a total function
and Z  X, then the restriction f |Z of f to Z is the total function from Z to Y defined by f |Z (z) = f (z)
for every z  Z. That is, f |Z is the same as f but is only defined on the subdomain Z.

409

fiZhang, Van den Bussche, & Picalausa

 If C is of the form bound(?x), then we know by S ` C that ?x  S 0 . Hence |S 0 |= C.
 If C is of the form ?x = ?y, then we again know ?x, ?y  S 0 , and certainly |S 0 |= C
since  maps everything to c.
 If C is of the form ?x 6= d, then we have d 6= c by the choice of c, so |S 0 |= C since
(?x) = c.
Example 6. To illustrate the above Lemma, consider the pattern
P = ((?x, p, ?y) FILTER ?x 6= a) OPT ((?x, q, ?z) UNION (?x, r, ?u))
which is a variant of the pattern from Example 1. As in that example, we have (P ) =
{{?x, ?y}, {?x, ?y, ?z}, {?x, ?y, ?u}}. In this case, the mapping  from the Lemma maps ?x,
?y, ?z and ?u to c. The graph G from the Lemma equals {(c, p, c), (c, q, c), (c, r, c)}, and
JP KG = {1 , 2 } where 1 = |{?x,?y,?z} and 2 = |{?x,?y,?u} . Now consider S = {?x, ?y} 
(P ). Then for S 0 = {?x, ?y, ?z} we indeed have S 0  S and |S 0 = 1  JP KG . Note that
in this example we could also have chosen {?x, ?y, ?u} for S 0 .
The counterpart to Lemma 5 for the fragment SPARQL(bound, 6=, 6=c ) is given by the
following Lemma, thus settling Theorem 3 for that fragment.
Lemma 7. Let P be a pattern in SPARQL(bound, 6=, 6=c ). Let W be the set of all constants
appearing in a constant-nonequality filter condition in P . Let Z  I be a finite set of
constants of the same cardinality as var(P ), and disjoint from W . With  : var(P )  Z
an arbitrary but fixed injective mapping, let G be the RDF graph consisting of all possible
triples ((u), (v), (w)) where (u, v, w) is a triple pattern in P .
Then for every S  (P ) there exists S 0  S such that |S 0 belongs to JP KG .
Proof. We prove for every subpattern Q of P that for every S  (Q) there exists S 0  S
such that |S 0  JQKG . The proof is by induction on the height of Q. The reasoning is
largely the same as in the proof of Lemma 5. The only difference is in the case where Q is
of the form Q1 FILTER C. In showing that S 0 |= C, we now argue as follows for the last
two cases:
 If C is of the form ?x 6= ?y, then |S 0 |= C since  is injective.
 If C is of the form ?x 6= c, then |S 0 |= C since Z and W are disjoint.
3.2 Computational Complexity
In this section we show that satisfiability for the decidable fragments is NP-complete. Note
that this does not immediately follow from the NP-completeness of SAT, since boolean
formulas are not part of the syntax of the decidable fragments.
Theorem 3 implies the following complexity upper bound:
Corollary 8. The satisfiability problem for SPARQL(bound, =, 6=c ) patterns, as well as for
SPARQL(bound, 6=, 6=c ) patterns, belongs to the complexity class NP.
410

fiSatisfiability Problem for SPARQL

Proof. By Theorem 3, a SPARQL(bound, =, 6=c ) or SPARQL(bound, 6=, 6=c ) pattern P is
satisfiable if and only if there exists a scheme in (P ). Following the definition of (P ),
it is clear that there is a polynomial-time nondeterministic algorithm such that, on input
P , each accepting possible run computes a scheme in (P ), and such that every scheme in
(P ) is computed by some accepting possible run.
Specifically, the algorithm works bottom-up on the syntax tree of P and computes a
scheme for every subpattern. At every leaf Q, corresponding to a triple pattern in P , we
compute the unique scheme in (Q). At every UNION operator we nondeterministically
choose between continuing with the scheme from the left or from right child. At every
AND operator we continue with the union of the left and right child schemes. At every
OPT operator, we nondeterministically choose between treating it as an AND, or simply
continuing with the scheme from the left. At every FILTER operation with constraint C
we check for the child scheme S whether S ` C. If the check succeeds, we continue with
S; if the check fails, the run is rejected. When the computation has reached the root of
the syntax tree and we can compute a scheme for the root, the run is accepting and the
computed scheme is the output.
Remark 9. In our presentation of the syntax of SPARQL, we do not consider conjunction
and disjunction in filter conditions. Extending the syntax to allow this would not ruin the
NP upper bound. Allowing conjunctions and disjunctions, we would need to extend the
definition of (P ) in the obvious manner, defining S ` C1  C2 if S ` C1 or S ` C2 , and
similarly for the definition of S ` C1  C2 . The results would then carry through.
We next show that satisfiability is actually NP-hard, even for patterns not using any
OPT operators and using only bound constraints in filter conditions.
Proposition 10. The satisfiability problem for OPT-free patterns in SPARQL(bound) is
NP-hard.
Proof. We define the problem Nested Set Cover as follows:
Input: A finite set T and a finite set E of sets of subsets of T . (So, every element of E is
a set of subsets of T .)
S
Decide: Whether for each element e of E we can choose a subset Se in e, so that eE Se =
T.
We will show later that the above problem is NP-hard; let us first describe how it can be
reduced in polynomial time to the satisfiability problem at hand. Consider an input (T, E)
for Nested Set Cover. Without loss of generality we may assume that T is a set of variables
{?x1 , ?x2 , . . . , ?xn }. Fix some constant c. For any subset S of T , we can make a pattern PS
by taking the AND of all (x, c, c) for x  S. Now for a set e of subsets of T , we can form
the pattern Pe by taking the UNION of all PS for S  e. Finally, we form the pattern PE
by taking the AND of all Pe for e  E.
Now consider the following pattern which we denote by P(T,E) :
PE FILTER bound(?x1 ) FILTER bound(?x2 ) . . . FILTER bound(?xn )
411

fiZhang, Van den Bussche, & Picalausa

We claim that P(T,E) is satisfiable if and only if (T, E) is a yes-instance for Nested Set
Cover. To see the only-if direction, let G be a graph such that JP(T,E) KG is nonempty, i.e.,
has as an element some solution mapping . Then
S in particular   JPE KG . Hence, for every
e  E there exists e  JPe KG such that  = eE e . Since Pe is the UNION of all PS for
S  e, for each e  E there exists Se  e such that e  JPSe KG . Since PSeSis the AND of all
(x, c, c) for x  SSe , it follows that dom(e ) = Se . Hence, since dom() = eE dom(e ), we
have dom() = eE Se . However, by the bound constraints in the filters
applied in P(T,E) ,
S
we also have dom() = {?x1 , . . . , ?xn } = T . We conclude that T = eE Se as desired.
S For the if-direction, assume that for each e  E there exists Se  e such that T =
eE Se . Consider the singleton graph G = {(c, c, c)}. For any subset S of T , let S : S 
{c} be the constant solution mapping with domain S. Clearly, S  JPS KG , so Se  JPe KG
for everySe  E. All the S map to the same constant, so
Hence,
S they are all compatible.
S
for  = eE Se , we have   JPE KG . Since dom() = eE dom(Se ) = eE Se = T =
{?x1 , . . . , ?xn }, the mapping  satisfies every constraint bound(?xi ) for i = 1, . . . , n. We
conclude that   JP(E,T ) KG as desired.
It remains to show that Nested Set Cover is NP-hard. Thereto we reduce the classical
CNF-SAT problem. Assume given a boolean formula  in CNF, so  is a conjunction of
clauses, where each clauses is a disjunction of literals (variables or negated variables). We
construct an input (T, E) for Nested Set Cover as follows. Denote the set of variables used
in  by W .
For T we take the set of clauses of . For any variable x  W , consider the set Posx
consisting of all clauses that contain a positive occurrence of x, and the set Negx consisting
of all clauses that contain a negative occurrence of x. Then we define ex as the pair
{Posx , Negx }.
Now E is defined as the set {ex | x  W }. It is clear that  is satisfiable if and only if
the constructed input is a yes-instance for Nested Set Cover. Indeed, truth assignments to
the variables correspond to selecting either Posx or Negx from ex for each x  W .

4. Undecidable Fragments
In this Section we show that the two decidable fragments SPARQL(bound, =, 6=c ) and
SPARQL(bound, 6=, 6=c ) are, in a sense, maximal. Specifically, the three minimal fragments
not subsumed by one of these two fragments are SPARQL(bound), SPARQL(=, 6=), and
SPARQL(=c ). The main result of this Section is:
Theorem 11. Satisfiability is undecidable for SPARQL(bound) patterns, for SPARQL(=,
6=) patterns, and for SPARQL(=c ) patterns.
We will prove this theorem by reducing from the satisfiability problem for the algebra
of finite binary relations with union, composition, and difference (Tan et al., 2014). This
algebra is also called the Downward Algebra and denoted by DA. The expressions of DA
are defined as follows. Let R be an arbitrary fixed binary relation symbol.
 The symbol R is a DA-expression.
 If e1 and e2 are DA-expressions, then so are e1  e2 , e1  e2 , and e1  e2 .
412

fiSatisfiability Problem for SPARQL

Semantically, DA-expressions represent binary queries on binary relations, i.e., mappings
from binary relations to binary relations. Let J be a binary relation. For DA-expression e,
we define the binary relation e(J) inductively as follows:
 R(J) = J;
 (e1  e2 )(J) = e1 (J)  e2 (J);
 (e1  e2 )(J) = e1 (J)  e2 (J) (set difference);
 (e1  e2 )(J) = {(x, z) | y : (x, y)  e1 (J) and (y, z)  e2 (J)}.
A DA-expression is called satisfiable if there exists a finite binary relation J such that
e(J) is nonempty.
Example 12. An example of a DA-expression is e = (R  R)  R. If J is the binary relation
{(a, b), (b, c), (a, c), (c, d)} then e(J) = {(b, d), (a, d)}. An example of an unsatisfiable DA
expression is ((R  R  R)  R)  (R  R  R).
We recall the following result. It is actually well known (Andreka, Givant, & Nemeti,
1997) that relational composition together with union and complementation leads to an
undecidable algebra; the following result simplifies matters by showing that undecidability
already holds for expressions over a single relation symbol and using set difference instead of
complementation. The following result has been proven by reduction from the universality
problem for context-free grammars.
Theorem 13 (Tan et al., 2014). The satisfiability problem for DA-expressions is undecidable.
4.1 Expressing MINUS
The main problem we face in reducing from DA to the SPARQL fragments stated in Theorem 11, is to emulate the difference operator. We review here more generally how to emulate
the MINUS operator, which is the most meaningful counterpart of the relational difference
operator in the SPARQL context.
The MINUS operator is defined as follows. For two patterns P1 and P2 and a graph G,
we define
JP1 MINUS P2 KG = JP1 KG r JP2 KG ,
where we reuse the r operation on sets of mappings, already seen in the definition of OPT
in Section 2.3.
For the fragment SPARQL(bound), expressibility of MINUS is already known:
Lemma 14 (Arenas & Perez, 2011). MINUS is expressible in SPARQL(bound). More
precisely, for any two patterns P1 and P2 and any graph G, we have JP1 MINUSP2 KG = JP KG
where P is the pattern

P1 OPT (P2 AND (?u, ?v, ?w)) FILTER bound(?u).
Here, ?u, ?v and ?w are fresh variables not used in P1 or P2 .
413

fiZhang, Van den Bussche, & Picalausa

Our task is to find similar expressions in the two other fragments SPARQL(=, 6=) and
SPARQL(=c ). We will actually only be able to express MINUS up to projection, and under
some mild assumptions on the graph G.
As for projection, its counterpart in SPARQL is the operation SELECT, defined as
follows. Let P be a pattern and let S be a finite set of variables. Then SELECTS P
restricts the solution mappings coming from P to the variables listed in S. Formally, for
any graph G, we define
JSELECTS P KG = {|Sdom() |   JP KG }.
The assumptions on the graph G we need to make have to do with its active domain.
Intuitively, the active domain of a graph is the set of all entries of triples in the graph.
Formally, we define
adom(G) = {s | p, o : (s, p, o)  G}  {p | s, o : (s, p, o)  G}  {o | s, p : (s, p, o)  G}.
We can easily express the active domain in SPARQL, in the following sense. Using three
variables ?u, ?v, ?w, consider the pattern
adom = (?u, ?v, ?w) UNION (?w, ?u, ?v) UNION (?v, ?w, ?u).
Then for any graph, we have
adom(G) = {(?u) |   JadomKG }
= {(?v) |   JadomKG }

= {(?w) |   JadomKG }.
We are now ready to state the counterpart of Lemma 14 for SPARQL(=, 6=).
Lemma 15. MINUS is expressible in SPARQL(=, 6=), up to projection and on graphs
with at least two distinct elements. More precisely, for any two patterns P1 and P2 and
any graph G such that adom(G) has at least two distinct elements, we have the equality
JP1 MINUS P2 KG = JSELECTvar(P1 ) P KG , where P is the pattern



P1 OPT ((P2 AND adom AND adom 0 ) FILTER ?u 6= ?u0 )

AND adom AND adom 0 FILTER ?u = ?u0 .

Here, adom 0 is a copy of the adom pattern with different variables ?u0 , ?v 0 and ?w0 . These
variables, and the variables ?u, ?v and ?w used in adom, are fresh variables not used in P1
or P2 .
Proof. To prove the equality stated in the Theorem we are going to consider both inclusions.
For easy reference we name some subpatterns of P as follows.
 P20 denotes (P2 AND adom AND adom 0 ) FILTER ?u 6= ?u0 ;
 P3 denotes P1 OPT P20 .
414

fiSatisfiability Problem for SPARQL

 Thus, P is (P3 AND adom AND adom 0 ) FILTER ?u = ?u0 .
To prove the inclusion from right to left, let   JP KG . Then  = 3  , where
3  JP3 KG and  is a mapping defined on {?u, ?v, ?w, ?u0 , ?v 0 , ?w0 } such that (?u) = (?u0 ).
In particular, 3  . Since P3 = P1 OPT P20 , there are two possibilities for 3 :
 3  JP1 KG and there is no 02  JP20 KG such that 3  02 . Then 3 = |var(P1 ) , so it
remains to show that there does not exist 2  JP2 KG such that 3  2 . Assume the
contrary. Since adom(G) has at least two distinct elements, 2 can be extended to a
mapping 02  JP20 KG . Then 2  02  3 , which is a contradiction.

 3 = 1  02 with 1  JP1 KG and 02  JP20 KG . In particular, 3 is defined on ?u
and ?u0 and 3 (?u) 6= 3 (?u0 ). On the other hand, since 3  , and (?u) = (?u0 ),
also 3 (?u) = 3 (?u0 ). This is a contradiction, so the possibility under consideration
cannot happen.

To prove the inclusion from left to right, let 1  JP1 MINUS P2 KG . Assume, for the sake
of argument, that there would exist 02  JP20 KG such that 1  02 . Mapping 02 contains a
mapping 2  JP2 KG , by definition of P20 . Since 1  02 , also 1  2 which is not possible.
So, we now know that there does not exist 02  JP20 KG such that 1  02 . Hence,
1  JP3 KG . Note that the six variables ?u, ?u0 , ?v, ?v 0 , ?w, and ?w0 do not belong to
var(P1 ). Since G is nonempty, 1 can thus be extended to a mapping   JP KG . We
conclude 1  JSELECTvar(P1 ) P KG as desired.
The analogous result for the fragment SPARQL(=c ) is as follows. Fix two distinct
constants a and b arbitrarily.
Lemma 16. MINUS is expressible in SPARQL(=c ), up to projection and on graphs in which
a and b appear. More precisely, for any two patterns P1 and P2 and any graph G such that
a and b belong to adom(G), we have the equality JP1 MINUS P2 KG = JSELECTvar(P1 ) P KG ,
where P is the pattern



Pe1 OPT ((Pe2 AND adom ?u ) FILTER ?u = a) AND adom ?u FILTER ?u = b.
As always, in the above expression, the variables ?u, ?v and ?w used in adom are taken
to be fresh variables not used in P1 or P2 .
The correctness proof of the above Lemma is analogous to the proof given for Lemma 15;
instead of exploiting the inconsistency between ?u 6= ?u0 and ?u = ?u0 as done in that proof,
we now exploit the inconsistency between ?u = a and ?u = b.
4.2 Reduction from the Downward Algebra
We are now ready to formulate the reduction from the satisfiability problem for DA to
the satisfiability problem for the three fragments mentioned in Theorem 11. We precisely
formulate the reduction and prove the Theorem for the fragment SPARQL(bound) first.
After that we will discuss how the reduction must be adapted for the other two fragments.
We say that an RDF graph G represents a binary relation J if J = {(s, o) | p : (s, p, o) 
G}. Intuitively, we view an RDF graph as a binary relation by ignoring the middle column.
415

fiZhang, Van den Bussche, & Picalausa

Lemma 17. For every DA-expression e there exists a SPARQL(bound) pattern Pe with
the following properties:
1. there exist two distinct fixed variables ?x and ?y such that for every RDF graph G
and every   JPe KG , ?x and ?y belong to dom();
2. for every binary relation J and RDF graph G that represents J, we have
e(J) = {((?x), (?y)) |   JPe KG };
Proof. By induction on the structure of e. If e is R then Pe is the triple pattern (?x, ?z, ?y).
If e is of the form e1  e2 , then Pe is Pe1 UNION Pe2 .
If e is of the form e1  e2 , then Pe is Pe01 AND Pe02 , where Pe01 and Pe02 are obtained as
follows. First, by renaming variables, we may assume without loss of generality that Pe1
and Pe2 have no variables in common other than ?x and ?y. Let ?z be a fresh variable.
Now in Pe1 , rename ?y to ?z, yielding Pe01 , and in Pe2 , rename ?x to ?z, yielding Pe02 .
Finally, if e is of the form e1  e2 , we use the expression P from Lemma 14 applied to
Pe1 and Pe2 . As before we may assume without loss of generality that Pe1 and Pe2 have no
variables in common other than ?x and ?y.
From the above lemma we clearly have that e is satisfiable if and only if Pe is satisfiable.
We thus have a reduction from satisfiability for DA to satisfiability for SPARQL(bound),
showing undecidability of the latter problem.
We now discuss the two remaining fragments.
4.3 SPARQL(=, 6=)
For this fragment we consider a minor variant of satisfiability for DA-expressions where we
restrict attention to binary relations over at least two elements. Formally, the active domain
of a binary relation J is the set of all entries in pairs belonging to J, so adom(J) := {x |
y : (x, y)  J or (y, x)  J}. Then a DA-expression e is called two-satisfiable if e(J) is
nonempty for some J such that adom(J) has at least two distinct elements.
Clearly, two-satisfiability is undecidable as well, for if it were decidable, then satisfiability
would be decidable too. Indeed, e is satisfiable if and only if it is two-satisfiable, or satisfiable
by a binary relation J over a single element. Up to isomorphism there is only one such J
(the singleton {(x, x)}), so that case could be checked separately.
Lemma 17 can now be adapted by claiming the second property only for binary relations
J over at least two distinct elements. In the proof for the case where e is e1  e2 , we can
then use Lemma 15.
Using the adapted lemma, we can now reduce two-satisfiability for DA to satisfiability
for SPARQL(=, 6=). All we need extra is a test whether the graph represents a binary
relation over at least two distinct elements. We can use the following pattern test (using
fresh variables ?u, . . . , ?w0 as usual):
(((?u, ?v, ?w) AND (?u0 , ?v 0 , ?w0 )) FILTER ?u 6= ?u0 )
UNION (((?u, ?v, ?w) AND (?u0 , ?v 0 , ?w0 )) FILTER ?w 6= ?w0 )
UNION (((?u, ?v, ?w) AND (?u0 , ?v 0 , ?w0 )) FILTER ?u 6= ?w0 )
Then, e is two-satisfiable if and only if Pe AND test is satisfiable.
416

fiSatisfiability Problem for SPARQL

4.4 SPARQL(=c )
For this fragment we consider a further variant of two-satisfiability, called ab-satisfiability,
for two arbitrary fixed constants a, b  I. A DA-expression is called ab-satisfiable if e(J) is
nonempty for some binary relation J where a, b  adom(J).
DA-expressions do not distinguish between isomorphic binary relations. Hence, absatisfiability is equivalent to two-satisfiability, and thus still undecidable.
We now again adapt Lemma 17, as follows. The second property is now claimed only
for binary relations J where a, b  adom(J). In the proof for the case e = e1  e2 , we now
use Lemma 16.
We then obtain that e is ab-satisfiable if and only if Pe AND test ab is satisfiable, where
test ab is the following pattern which tests whether the graph represents a binary relation
with a and b in its active domain:
(((?u, ?v, ?w) UNION (?w, ?v, ?u)) AND ((?u0 , ?v 0 , ?w0 ) UNION (?w0 , ?v 0 , ?u0 )))
FILTER ?u = a FILTER ?u0 = b
Remark 18. Recall that literals cannot appear in first or second position in an RDF triple.
Patterns using constant-equality predicates can be unsatisfiable because of that reason. For
example, using the literal 42, the pattern (?x, ?y, ?z) FILTER ?y = 42 is unsatisfiable. However, we have seen here that the use of constant-equality predicates leads to undecidability
of satisfiability for a much more fundamental reason, that has nothing to do with literals,
namely, the ability to emulate set difference.

5. Satisfiability of Well-Designed Patterns
The well-designed patterns (Perez et al., 2009) have been identified as a well-behaved class of
SPARQL patterns, with properties similar to the conjunctive queries for relational databases
(Abiteboul et al., 1995). Standard conjunctive queries are always satisfiable, and conjunctive
queries extended with equality and nonequality constraints, possibly involving constants,
can only be unsatisfiable if the constraints are inconsistent. An analogous behavior is present
in what we call AF-patterns: patterns that only use the AND and FILTER operators. We
will formalize this in Proposition 19. We will then show in Theorem 21 that a well-designed
pattern is satisfiable if and only if its reduction to an AF-pattern is satisfiable. In other
words, as far as satisfiability is concerned, well-designed patterns can be treated like AFpatterns.
5.1 Satisfiability of AF-Patterns
In Section 3.1 we have associated a set of schemes (P ) to every pattern P . When (P ) is
empty, P is unsatisfiable (Lemma 4).
Now when P is an AF-pattern and (P ) is nonempty, the satisfiability of P will turn
out to depend solely on the equalities, nonequalities, constant-equalities, and constantnonequalities occurring as filter conditions in P . We will denote the set of these constraints
by C(P ).
Any set  of constraints is called consistent if there exists a mapping that satisfies every
constraint in .
417

fiZhang, Van den Bussche, & Picalausa

We establish:
Proposition 19. An AF-pattern P is satisfiable if and only if (P ) is non-empty and
C(P ) is consistent.
Proof. The only-if direction of this proposition is given by Lemma 4 together with the
observation that if   JP KG , then  satisfies every constraint in C(P ). Since P is satisfiable,
such G and  exist, so C(P ) is consistent.
For the if direction, since P does not have the UNION and OPT operators, (P ) is a
singleton {S}. Since C(P ) is consistent, there exists a mapping  : S  U satisfying every
constraint in C(P ). Let G be the graph consisting of all triples ((u), (v), (w)) where
(u, v, w) is a triple pattern in P . It is straightforward to show by induction on the height
of Q that for every subpattern Q of P , we have |S 0  JQKG , where (Q) = {S 0 }. Hence
  JP KG and P is satisfiable.
Note that (P ) can blow up only because of possible UNION and OPT operators,
which are missing in an AF-pattern. Hence, for an AF-pattern P , we can efficiently compute
(P ) by a single bottom-up pass over P . Morever, C(P ) is a conjunction of possibly negated
equalities and constant equalities. It is well known that consistency of such conjunctions
can be decided in polynomial time (Kroening & Strichman, 2008). Hence, we conclude:
Corollary 20. Satisfiability for AF-patterns can be checked in polynomial time.
5.2 AF-Reduction of Well-Designed Patterns
A well-designed pattern is defined as a union of union-free well-designed patterns. Since a
union is satisfiable if and only if one of its terms is, we will focus on union-free patterns in
what follows. Formally, a union-free pattern P is called well-designed (Perez et al., 2009) if
1. for every subpattern of P of the form Q FILTER C, all variables mentioned in C also
occur in Q; and
2. for every subpattern Q of P of the form Q1 OPT Q2 , and every ?x  var(Q2 ), if ?x
also occurs in P outside of Q, then ?x  var(Q1 ).
We associate to every union-free pattern P an AF-pattern (P ) obtained by removing all
applications of OPT and their right operands; the left operand remains in place. Formally,
we define the following:
 If P is a triple pattern, then (P ) equals P .
 If P is of the form P1 AND P2 , then (P ) = (P1 ) AND (P2 ).
 If P is of the form P1 FILTER C, then (P ) = (P1 ) FILTER C.
 If P is of the form P1 OPT P2 , then (P ) = (P1 ).
The announced result is now given by the following theorem, which is proved directly
from results by Perez et al. (2009).4
4. We thank an anonymous referee for offering the given proof of the only-if direction.

418

fiSatisfiability Problem for SPARQL

Theorem 21. Let P be a union-free well-designed pattern. Then P is satisfiable if and
only if (P ) is.
Proof. We are going to refer to Lemma 4.3 and Proposition 4.5 by Perez et al. (2009).
Indeed, Lemma 4.3 gives us the if-direction of Theorem 21. The cited paper introduced the
notion of a reduction P 0 E P . Whenever P 0 E P , also (P ) E P 0 and (P ) = (P 0 ).
Now for the only-if direction, assume P is satisfiable, so there exists G and  so that
  JP KG . Then there exists P 0 E P such that   Jand(P 0 )KG (Proposition 4.5). Here,
and(P 0 ) denotes the pattern obtained from P 0 by replacing every OPT by AND. By the
above we have (P ) = (P 0 ).
Now the following claim is easy to verify for every union-free pattern P 0 : If  
Jand(P 0 )KG then |var((P 0 ))  J(P 0 )KG . By that claim, we obtain that J(P 0 )KG is nonempty
so (P 0 ) = (P ) is satisfiable, as desired.
Since (P ) can be efficiently computed from P , the above Theorem and Corollary 20
imply:
Corollary 22. Satisfiability of union-free well-designed patterns can be tested in polynomial
time.

6. Experimental Evaluation
We want to evaluate experimentally the positive results presented so far:
1. Wrong literal reduction (Proposition 24);
2. Satisfiability checking for SPARQL(bound, =, 6=c ) and SPARQL(bound, 6=, 6=c ) by computing (P ) (Theorem 3);
3. Satisifiability checking for well-designed patterns, by reduction to AF-patterns (Proposition 19 and Theorem 21).
Our experiments follow up on those reported earlier by Picalausa and Vansummeren
(2011). As test datasets of real-life SPARQL queries, we use logs of the SPARQL endpoint
for DBpedia.5 This data source contains the query dumps from the year 2012, divided
into 14 logfiles. Out of these we chose the three logs 20120913, 20120929 and 20121031 to
obtain a span of roughly three months; we then took a sample of 100 000 queries from each
of them. A typical query in the log has size between 75 and 125 (size measured as number
of nodes in the syntax tree). About 10% of the queries in each log is not usable because
they have syntax errors or because they use features not covered by our analysis.
The implementation of the tests was done in Java 7 under Windows 7, on an Intel Core 2
Duo SU94000 processor (1.40GHz, 800MHz, 3MB) with 3GB of memory (SDRAM DDR3
at 1067MHz).
Our tests measure the time needed to perform the analyses of SPARQL queries presented
above. The timings are averaged over all queries in a log, and each experiment is repeated
five times to smooth out accidental quirks of the operating system. Although we give
5. ftp://download.openlinksw.com/support/dbpedia/

419

fiZhang, Van den Bussche, & Picalausa

Table 1: Timings of experiments (averaged over five repeats). Times are in ms. Baseline
is time to read and parse 1000 000 queries; WL stands for baseline plus time for
wrong-literal reduction. (P ) stands for WL plus time for computing (P ). AF
stands for WL, plus testing well-designedness, plus doing AF-reduction and testing
satisfiability (Proposition 19). The percentages show the increases relative to the
baseline.
logfile
20120913
20120929
20121031

baseline
39 422
34 281
32 286

WL
41 254
35 868
33 186

5%
5%
3%

(P )
44 395
38 102
34 419

8%
7%
4%

AF
48 329
41 087
36 993

10%
9%
8%

absolute timings, the main emphasis is on the percentage of the time needed to analyse
a query, with respect to the time needed simply to read and parse that query. If this
percentage is small this demonstrates efficient, linear time complexity in practice. It will
turn out that this is indeed achieved by our experiments, as shown in Table 1.
In the following subsections we discuss the results in more detail.
6.1 Wrong Literal Reduction
Testing for and removing triple patterns with wrong literals in a pattern P is performed by
the reduction (P ) defined in the Appendix. From the definition of (P ) it is clear that it
can be computed by a single bottom-up traversal of P and this is indeed borne out by our
experiments. Table 1 shows that on average, wrong-literal reduction takes between 3 and
5% of the time needed to read and parse the input.
Interestingly, some real-life queries with literals in the wrong position were indeed found;
one example is the following:
SELECT DISTINCT *
WHERE { 49 dbpedia-owl:wikiPageRedirects

?redirectLink .}

6.2 Computing (P )
In Section 3 we have seen that satisfiability for the decidable fragments can be tested
by computing (P ), but that the problem is NP-complete. Intuitively, the problem is
intractable because (P ) may be of size exponential in the size of P . This actually occurs
in real life; a common SPARQL query pattern is to use many nested OPTIONAL operators
to gather additional information that is not strictly required by the query but may or may
not be present. We found in our experiments queries with up to 50 nested OPT operators,
which naively would lead to a (P ) of size 250 . A shortened example of such a query is
shown in Figure 1.
In practice, however, the blowup of (P ) can be avoided as follows. Recall that Theorem 3 states that P is satisfiable if and only if (P ) is nonempty. The elements of (P ) are
sets of variables. Looking at the definition of (P ), a set may be removed from (P ) only
420

fiSatisfiability Problem for SPARQL

SELECT DISTINCT *
WHERE {
?s a <http://dbpedia.org/ontology/EducationalInstitution>,
<http://dbpedia.org/ontology/University> .
?s <http://dbpedia.org/ontology/country> <http://dbpedia.org/resource/Brazil> .
OPTIONAL {?s <http://dbpedia.org/ontology/affiliation> ?ontology_affiliation .}
OPTIONAL {?s <http://dbpedia.org/ontology/abstract> ?ontology_abstract .}
OPTIONAL {?s <http://dbpedia.org/ontology/campus> ?ontology_campus .}
OPTIONAL {?s <http://dbpedia.org/ontology/chairman> ?ontology_chairman .}
OPTIONAL {?s <http://dbpedia.org/ontology/city> ?ontology_city .}
OPTIONAL {?s <http://dbpedia.org/ontology/country> ?ontology_country .}
OPTIONAL {?s <http://dbpedia.org/ontology/dean> ?ontology_dean .}
OPTIONAL {?s <http://dbpedia.org/ontology/endowment> ?ontology_endowment .}
OPTIONAL {?s <http://dbpedia.org/ontology/facultySize> ?ontology_facultySize .}
OPTIONAL {?s <http://dbpedia.org/ontology/formerName> ?ontology_formerName .}
OPTIONAL {?s <http://dbpedia.org/ontology/head> ?ontology_head .}
OPTIONAL {?s <http://dbpedia.org/ontology/mascot> ?ontology_mascot .}
OPTIONAL {?s <http://dbpedia.org/ontology/motto> ?ontology_motto .}
OPTIONAL {?s <http://dbpedia.org/ontology/president> ?ontology_president .}
OPTIONAL {?s <http://dbpedia.org/ontology/principal> ?ontology_principal .}
OPTIONAL {?s <http://dbpedia.org/ontology/province> ?ontology_province .}
OPTIONAL {?s <http://dbpedia.org/ontology/rector> ?ontology_rector .}
OPTIONAL {?s <http://dbpedia.org/ontology/sport> ?ontology_sport .}
OPTIONAL {?s <http://dbpedia.org/ontology/state> ?ontology_state .}
OPTIONAL {?s <http://dbpedia.org/property/acronym> ?property_acronym .}
OPTIONAL {?s <http://dbpedia.org/property/address> ?property_address .}
OPTIONAL {?s <http://www.w3.org/2003/01/geo/wgs84_pos#lat> ?property_lat .}
OPTIONAL {?s <http://www.w3.org/2003/01/geo/wgs84_pos#long> ?property_long .}
OPTIONAL {?s <http://dbpedia.org/property/established> ?property_established .}
OPTIONAL {?s <http://dbpedia.org/ontology/logo> ?ontology_logo .}
OPTIONAL {?s <http://dbpedia.org/property/website> ?property_website .}
OPTIONAL {?s <http://dbpedia.org/property/location> ?property_location .}
FILTER ( langMatches(lang(?ontology_abstract), "es") ||
langMatches(lang(?ontology_abstract), "en") )
FILTER ( langMatches(lang(?ontology_motto), "es") ||
langMatches(lang(?ontology_motto), "en") )
}

Figure 1: A real-life query with many nested OPTIONAL operators, retrieving as much
information as possible about universities in Brazil.

421

fiZhang, Van den Bussche, & Picalausa

by the application of a FILTER. Hence, only variables that are mentioned in FILTER conditions can influence the emptiness of (P ); other variables can be ignored. For example,
in the query in Figure 1, only two variables appear in a filter, namely ?ontology abstract
and ?ontology motto, so that the maximal size of (P ) is reduced to 22 .
In our experiments, it turns out that typically few variables are involved in filter conditions. Hence, the above strategy works well in practice.
Another practical issue is that, in this paper, we have only considered filter conditions that are bound checks, equalities, and constant-equalities, possibly negated. In practice, filter conditions typically apply built-in SPARQL predicates such as the predicate
langMatches in Figure 1. For the experimental purpose of testing the practicality of computing (P ), however, such predicates can simply be treated as bound checks. In this way
we can apply our experiments to 70% of the queries in the testfiles.
With the above practical adaptations, our experiments show that computing (P ) is
efficient: Table 1 shows that it requires, on average, between 4 and 8% of the time needed
to read and parse the input, and these timings even include the wrong-literal reduction
All in all, our experiments encountered very few unsatisfiable queries. This observation
is corrobated by the findings of a recent new statistical analysis of practical SPARQL usage
(Han, Feng, Zhang, Wang, Rao, & Jiang, 2016). Of course, that users in practice do not
write unsatisfiable expressions is only good news. Satisfiability remains a basic problem
that we need to understand, because many other problems can be reduced to it.
6.3 Satisfiability Testing for Well-Designed Patterns
In Section 5 we have seen that testing satisfiability of a well-designed pattern can be done by
testing satisfiability of the AF-reduction (Theorem 21). The latter can be done by testing
nonemptiness of (P ) and testing consistency of the filter conditions (Proposition 19).
Computing the AF-reduction can be done by a simple bottom-up traversal of the pattern.
Moreover, for an AF-pattern P , computing (P ) poses no problems since it is either empty
or a singleton. As far as testing consistency of filter conditions is concerned, our experiments
yield a rather baffling observation: almost all well-designed patterns in the test sets have
no filters at all. We cannot explain this phenomenon, but it implies that we have not been
able to test the performance of the consistency checks on real-life SPARQL queries.
Anyhow, Table 1 shows that doing the entire analysis of wrong-literal reduction, testing
well-designedness, AF-reduction, computing (P ), and consistency checking (in the few
cases where the latter was necessary), incurs at most a 10% increase relative to reading and
parsing the input.
6.4 Scalability
The experiments described above were run on sets of 100 000 queries each. We also did a
modest scaling experiment where we varied the number of queries from 5 000 to 200 000.
Table 2 shows that the performance scales linearly.
422

fiSatisfiability Problem for SPARQL

Table 2: Scalability experiment (times in ms). Timings clearly scale linearly for increasing
input size.
input size
baseline
WL
(P )
AF

200 000
74 168
77 800
81 730
91 470

100 000
39 422
41 253
44 395
48 329

50 000
21 315
21 876
23 552
26 023

10 000
3 596
3 762
4 016
4 463

5 000
1 851
1 942
2 036
2 254

Pearson coeficient
0.999924005
0.999989454
0.999900948
0.999044542

7. Extension to SPARQL 1.1
As already mentioned in the Introduction, SPARQL 1.0 has been extended to SPARQL 1.1
with a number of new operators for building patterns. The main new features are property
paths; grouping and aggregates; BIND; VALUES; MINUS; EXISTS and NOT EXISTSsubqueries; and SELECT. A complete analysis of SPARQL 1.1 goes beyond the scope of
the present paper. Nevertheless, in this section, we briefly discuss how our results may be
extended to this new setting.
Property paths provide a form of regular path querying over graphs. This aspect of graph
querying has already been extensively investigated, including questions of satisfiability and
other kinds of static analysis such as query containment (Kostylev, Reutter, & Vrgoc, 2014;
Kostylev, Reutter, Romero, & Vrgoc, 2015). Therefore we do not discuss property paths
any further here.
The SPARQL 1.1 features that we discuss can be grouped in two categories: those that
cause undecidability, and those that are harmless as far as satisfiability is concerned. We
begin with the harmless category.
7.1 SELECT Operator and EXISTS-Subqueries
SPARQL 1.1 allows patterns of the form SELECTS P , where S is a finite set of variables and
P is a pattern. The novelty compared to 1.0 is that this can be applied to subexpressions.
The semantics is that of projection; we have already seen it in Section 4.1.
This feature in itself does not influence the satisfiability of patterns. Indeed, patterns
extended with SELECT operators can be reduced to patterns without said operators. The
reduction amounts simply to rename the variables that are projected out by fresh variables
that are not used anywhere else in the pattern; then the SELECT operators themselves
can be removed. The resulting, SELECT-free, pattern is equivalent to the original one if
we omit the fresly introduced variables from the solution mappings in the final result. In
particular, the two patterns are equisatisfiable.
Example 23. Rather than giving the formal definition of SELECT-reduction and formally
stating and proving the equivalence, we give an example. Consider the pattern P :
(c, p, ?x) OPT ((?x, p, ?y) AND SELECT?y (?y, q, ?z) AND SELECT?y (?y, r, ?z))
423

fiZhang, Van den Bussche, & Picalausa

Renaming projected-out variables by fresh variables and omitting the SELECT operators
yields the following pattern P 0 :
(c, p, ?x) OPT ((?x, p, ?y) AND (?y, q, ?z1 ) AND (?y, r, ?z2 ))
Pattern P 0 is equivalent to P in the sense that for any graph G, we have JP KG = { |  
JP 0 KG }, where  denotes the mapping obtained from  by omitting the values for ?z1 and
?z2 (if at all present in dom()).
Now that we know how to handle SELECT operators, we can also handle EXISTSsubqueries. Indeed, a pattern P FILTER EXISTS(Q) (with the obvious SQL-like semantics)
is equivalent to SELECTvar(P ) (P AND Q).
7.2 Features Leading to Undecidability
In Section 4 we have seen that as soon as one can express the union, composition and
difference of binary relations, the satisfiability problem becomes undecidable. Since union
and composition are readily expressed in basic SPARQL (UNION and AND), the key lies
in the expressibility of the difference operator. In this subsection we will see that various
new features of SPARQL 1.1 indeed allow expressing difference.
7.2.1 MINUS Operator and NOT EXISTS Subqueries
Each of these two features can quite obviously be used to express difference, so we do not
dwell on them any further.
7.2.2 Grouping and Aggregates
A known trick for expressing difference using grouping and counting (Celko, 2005) can be
emulated in the extension of SPARQL 1.0 with grouping. We illustrate the technique with
an example.
Consider the query (?x, p, ?y) MINUS (?x, q, ?y) asking for all pairs (a, b) such that
(a, p, b) holds but (a, q, b) does not. We can express this query (with the obvious SQL-like
semantics) as follows:

SELECT?x,?y (?x, p, ?y) OPT ((?x, q, ?y) AND (?xx, p, ?yy))
GROUP BY ?x, ?y
HAVING count(?xx) = 0
Note that this technique of looking for the (?x, ?y) groups with a zero count for ?xx is very
similar to the technique used to express difference using a negated bound constraint (seen
in the proof of Lemma 17).
7.2.3 BIND and VALUES
We have seen in Section 4.4 that allowing constant equalities in filter constraints allows us
to emulate the difference operator. Two mechanisms introduced in SPARQL 1.1, BIND and
VALUES, allow the introduction of constants in solution mappings. Together with equality
constraints this allows us to express constant equalities, and hence, difference.
424

fiSatisfiability Problem for SPARQL

Specifically, using VALUES, we can express P FILTER ?x = c as
SELECTvar(P ) (P AND VALUES?x (c)).
Using BIND, it can be expressed as
SELECTvar(P ) ((P BIND?x0 (c)) FILTER ?x = ?x0 )
where ?x0 is a fresh variable. Note the use of SELECT, which, however, does not influence
satisfiability as discussed above. We conclude that SPARQL(=) extended with BIND, or
SPARQL(=) extended with VALUES, have an undecidable satisfiability problem.

8. Conclusion
The results of this paper may be summarized by saying that, as long as the kinds of
constraints allowed in filter conditions cannot be combined to yield inconsistent sets of
constraints, satisfiability for SPARQL patterns is decidable; otherwise, the problem is undecidable. Moreover, for well-designed patterns, satisfiability is decidable as well. All our
positive results yield straightforward bottom-up syntactic checks that can be implemented
in practice.
We thus have attempted to paint a rather complete picture of the satisfiability problem
for SPARQL 1.0. Of course, satisfiability is only the most basic automated reasoning
task. One may now move on to more complex tasks such as equivalence, implication,
containment, or query answering over ontologies. Indeed, investigations along this line for
limited fragments of SPARQL are already happening (Letelier, Perez, Pichler, & Skritek,
2013; Wudage, Euzenat, Geneves, & Layada, 2012; Kollia & Glimm, 2013; Cuenca Grau,
Motik, Stoilos, & Horrocks, 2012) and we hope that our work may serve to provide some
additional grounding to these investigations.
We also note that in query optimization it is standard to check for satisfiability of
subexpressions, to avoid executing useless code. Some specific works on SPARQL query
optimization (Sequeda & Miranker, 2013; Groppe, Groppe, & Kolbaum, 2009) do mention
that inconsistent constraints can cause unsatisfiability, but they have not provided sound
and complete characterizations of satisfiability, like we have offered in this paper. Thus,
our results will be useful in this direction as well.

Acknowledgment
We thank the anonymous referees, both on the original submission and on the revised
submission, for their critical comments, which encouraged us to significantly improve the
paper. This work has been funded by grant G.0489.10 of the Research Foundation Flanders
(FWO).

Appendix A.
Literals in the wrong place in triple patterns are easily dealt with in the following manner.
We define the wrong-literal reduction of a pattern P , denoted by (P ), as a set that is either
empty or is a singleton containing a single pattern P 0 :
425

fiZhang, Van den Bussche, & Picalausa

 If P is a triple pattern (u, v, w) and u is a literal, then (P ) := ; else (P ) := {P }.
 (P1 UNION P2 ) := (P1 )  (P2 ) if (P1 ) or (P2 ) is empty;
 (P1 UNION P2 ) := {P10 UNION P20 | P10  (P1 ) and P20  (P2 )} otherwise.
 (P1 AND P2 ) := {P10 AND P20 | P10  (P1 ) and P20  (P2 )}.
 (P1 OPT P2 ) :=  if (P1 ) is empty;
 (P1 OPT P2 ) := (P1 ) if (P2 ) is empty but (P1 ) is nonempty;
 (P1 OPT P2 ) := {P10 OPT P20 | P10  (P1 ) and P20  (P2 )} otherwise.
 (P1 FILTER C) := {P10 FILTER C | P10  (P1 )}.
Note that the wrong-literal reduction never has a literal in the subject position of a triple
pattern. The next proposition shows that, as far as satisfiability checking is concerned, we
may always perform the wrong-literal reduction.
Proposition 24. Let P be a pattern. If (P ) is empty then P is unsatisfiable; if (P ) =
{P 0 } then P and P 0 are equivalent, i.e., JP KG = JP 0 KG for every RDF graph G. Moreover,
if (P ) = {P 0 } then P 0 does not contain any triple pattern (u, v, w) where u is a literal.
Proof. Assume P is a triple pattern (u, v, w) and u is a literal, so that (P ) = . Since u
is a constant, (u) equals the literal u for every solution mapping . Since no triple in an
RDF graph can have a literal in its first position, JP KG is empty for every RDF graph G,
i.e., P is unsatisfiable. If u is not a literal, (P ) = {P } and the claims of the Proposition
are trivial.
If P is of the form P1 UNION P2 , or P1 AND P2 , or P1 FILTER C, the claims of the
Proposition follow straightforwardly by induction.
If P is of the form P1 OPT P2 , there are three cases to consider.
 If (P1 ) is empty then so is (P ). In this case, by induction, P1 is unsatisfiable,
whence so is P .
 If (P1 ) = {P10 } is nonempty but (P2 ) is empty, then (P ) = {P10 }. By induction,
P2 is unsatisfiable. Hence, P is equivalent to P1 , which in turn is equivalent to P10 by
induction. That P10 does not contain any triple pattern with a literal in first position
again follows by induction.
 If (P1 ) = {P10 } and (P2 ) = {P20 } are both nonempty, then (P ) = P10 OPT P20 .
By induction, P1 is equivalent to P10 and so is P2 to P20 . Hence, P is equivalent to
P10 OPT P20 as desired. By induction, neither P10 nor P20 contain any triple pattern with
a literal in first position, so neither does P10 OPT P20 .

426

fiSatisfiability Problem for SPARQL

References
Abiteboul, S., Hull, R., & Vianu, V. (1995). Foundations of Databases. Addison-Wesley.
Andreka, H., Givant, S., & Nemeti, I. (1997). Decision problems for equational theories of
relational algebras, Vol. 126 of Memoirs. AMS.
Angles, R., & Gutierrez, C. (2008). The expressive power of SPARQL. In Sheth, A., Staab,
S., et al. (Eds.), Proceedings 7th International Semantic Web Conference, Vol. 5318
of Lecture Notes in Computer Science, pp. 114129. Springer.
Arenas, M., Conca, S., & Perez, J. (2012). Counting beyond a Yottabyte, or how SPARQL
1.1 property paths will prevent adoption of the standard. In Mille, A., et al. (Eds.),
Proceedings 21st World Wide Web Conference, pp. 629638. ACM.
Arenas, M., & Perez, J. (2011). Querying semantic web data with SPARQL. In Proceedings
30st ACM Symposium on Principles of Databases, pp. 305316. ACM.
Arenas, M., Perez, J., & Gutierrez, C. (2009). On the semantics of SPARQL. In De Virgilio,
R., Giunchiglia, F., & Tanca, L. (Eds.), Semantic Web Information ManagementA
Model-Based Perspective, pp. 281307. Springer.
Celko, J. (2005). SQL for Smarties: Advanced SQL Programming (Third edition). Elsevier.
Cuenca Grau, B., Motik, B., Stoilos, G., & Horrocks, I. (2012). Completeness guarantees for
incomplete ontology reasoners: Theory and practice. Journal of Artificial Intelligence
Research, 43, 419476.
Groppe, J., Groppe, S., & Kolbaum, J. (2009). Optimization of SPARQL by using coreSPARQL. In Cordeiro, J., & Filipe, J. (Eds.), Proceedings 11th International Conference
on Enterprise Information Systems, pp. 107112.
Han, X., Feng, Z., Zhang, X., Wang, X., Rao, G., & Jiang, S. (2016). On the statistical
analysis of practical SPARQL patterns. In Proceedings 19th International Workshop
on the Web and Databases.
Harris, S., & Seaborne, A. (2013). SPARQL 1.1 query language. W3C Recommendation.
Kollia, I., & Glimm, B. (2013). Optimizing SPARQL query answering over OWL ontologies.
Journal of Artificial Intelligence Research, 48, 253303.
Kostylev, E., Reutter, J., Romero, M., & Vrgoc, D. (2015). SPARQL with property paths.
In Arenas, M., Corcho, O., Simperl, E., Strohmaier, M., et al. (Eds.), Proceedings
14th International Semantic Web Conference, Vol. 9366 of Lecture Notes in Computer
Science, pp. 318. Springer.
Kostylev, E., Reutter, J., & Vrgoc, D. (2014). Containment of data graph queries. In
Proceedings 17th International Conference on Database Theory. ACM.
Kroening, D., & Strichman, O. (2008). Decision Procedures. Springer.
Letelier, A., Perez, J., Pichler, R., & Skritek, S. (2013). Static analysis and optimization of
semantic web queries. ACM Transactions on Database Systems, 38 (4), article 25.
Perez, J., Arenas, M., & Gutierrez, C. (2009). Semantics and complexity of SPARQL. ACM
Transactions on Database Systems, 34 (3), article 16.
427

fiZhang, Van den Bussche, & Picalausa

Picalausa, F., & Vansummeren, S. (2011). What are real SPARQL queries like?. In De Virgilio, R., Giunchiglia, F., & Tanca, L. (Eds.), Proceedings International Workshop on
Semantic Web Information Management, No. 7. ACM Press.
Polleres, A. (2007). From SPARQL to rules (and back). In Williamson, C., Zurko, M., et al.
(Eds.), Proceedings 16th World Wide Web Conference, pp. 787796. ACM.
Prudhommeaux, E., & Seaborne, A. (2008). SPARQL query language for RDF. W3C
Recommendation.
Schmidt, M., Meier, M., & Lausen, G. (2010). Foundations of SPARQL query optimization.
In Proceedings 13th International Conference on Database Theory, pp. 433. ACM.
Sequeda, J., & Miranker, D. (2013). Ultrawrap: SPARQL execution on relational data. Web
Semantics, 22, 1939.
Tan, T., Van den Bussche, J., & Zhang, X. (2014). Undecidability of satisfiability in the algebra of finite binary relations with union, composition, and difference. arXiv:1406.0349.
Wudage, M., Euzenat, J., Geneves, P., & Layada, N. (2012). SPARQL query containment
under SHI axioms. In Proceedings 26th AAAI Conference on Artificial Intelligence,
pp. 1016.

428

fiJournal of Artificial Intelligence Research 56 (2016) 547-571

Submitted 01/16; published 08/16

Research Note
Time-Bounded Best-First Search for Reversible and Non-reversible
Search Graphs
Carlos Hernandez

CARLOS . HERNANDEZ . U @ UNAB . CL

Departamento de Ciencias de la Ingeniera,
Universidad Andres Bello,
Santiago, Chile

Jorge A. Baier

JABAIER @ ING . PUC . CL

Departamento de Ciencia de la Computacion
Pontificia Universidad Catolica de Chile
Santiago, Chile

Roberto Asn

RASIN @ UCSC . CL

Departamento de Ingeniera Informatica
Universidad Catolica de la Santsima Concepcion
Concepcion, Chile

Abstract
Time-Bounded A* is a real-time, single-agent, deterministic search algorithm that expands
states of a graph in the same order as A* does, but that unlike A* interleaves search and action execution. Known to outperform state-of-the-art real-time search algorithms based on Korfs Learning
Real-Time A* (LRTA*) in some benchmarks, it has not been studied in detail and is sometimes not
considered as a true real-time search algorithm since it fails in non-reversible problems even it
the goal is still reachable from the current state. In this paper we propose and study Time-Bounded
Best-First Search (TB(BFS)) a straightforward generalization of the time-bounded approach to any
best-first search algorithm. Furthermore, we propose Restarting Time-Bounded Weighted A* (TBR
(WA*)), an algorithm that deals more adequately with non-reversible search graphs, eliminating
backtracking moves and incorporating search restarts and heuristic learning. In non-reversible
problems we prove that TB(BFS) terminates and we deduce cost bounds for the solutions returned
by Time-Bounded Weighted A* (TB(WA*)), an instance of TB(BFS). Furthermore, we prove TBR
(WA*), under reasonable conditions, terminates. We evaluate TB(WA) in both grid pathfinding and
the 15-puzzle. In addition, we evaluate TBR (WA*) on the racetrack problem. We compare our
algorithms to LSS-LRTWA*, a variant of LRTA* that can exploit lookahead search and a weighted
heuristic. A general observation is that the performance of both TB(WA*) and TBR (WA*) improves as the weight parameter is increased. In addition, our time-bounded algorithms almost
always outperform LSS-LRTWA* by a significant margin.

1. Introduction
In many search applications, time is a very scarce resource. Examples range from video game path
finding, where a handful of milliseconds are given to the search algorithm controlling automated
characters (Bulitko, Bjornsson, Sturtevant, & Lawrence, 2011), to highly dynamic robotics (Schmid,
Tomic, Ruess, Hirschmuller, & Suppa, 2013). In those settings, it is usually assumed that a standard
search algorithm will not be able to compute a complete solution before an action is required, and
thus execution and search must be interleaved.
c
2016
AI Access Foundation. All rights reserved.

fiH ERN ANDEZ , BAIER , & A S IN

Time-Bounded A* (Bjornsson, Bulitko, & Sturtevant, 2009) is an algorithm suitable for searching under tight time constraints. In a nutshell, given a parameter k, it runs a standard A* search
towards the goal rooted in the initial state, but after k expansions are completed, a move is performed and then search, if still needed, is resumed. The move is computed as follows. If the agent
is in the path  found by A* from the root node to the best node b in the search frontier then the
agent is moved towards b following the path . Otherwise, it performs backtracking move, returning the agent to its previous state. The algorithm always terminates with the agent at the goal
state, if the problem has a solution.
Time-Bounded A* is an algorithm that is relevant to the real-time search community. It is
significantly superior to well-known real-time heuristic search algorithms in some applications.
Indeed Hernandez, Baier, Uras, and Koenig (2012) showed it significantly outperforms state-of-theart real-time heuristic search algorithms such as RTAA* (Koenig & Likhachev, 2006) and daRTAA*
(Hernandez & Baier, 2012) in pathfinding.
Being a relatively new algorithm, Time-Bounded A* has not been studied deeply in the literature. One of the reasons for this is perhaps its inability to adequately deal with non-reversible
problems. Indeed, in non-reversible problems any real-time search algorithm will fail as soon the
algorithm has led the agent to a dead-end state; i.e., one from which the goal is unreachable. TimeBounded A*, however, has an additional failure condition: it will always fail as soon as a backtrack
move is required over an unreversible action. Thus the class of problems it cannot solve is more limited compared to other real-time search algorithms, like, for example, the well-known LRTA* (Korf,
1990). For this reason, Time-Bounded A* is sometimes excluded from experimental comparisons
with real-time search algorithms (see e.g. Burns, Ruml, & Do, 2013, p. 725).
In this paper we extend the time-bounded search approach in two directions. As already noted
by their authors (Bjornsson et al., 2009), the time-bounded approach is not limited just to A*. A first
contribution of this paper is a study of what are the implications of using other search algorithms
instead of A*. Specifically, we generalize Time-Bounded A* to Time-Bounded Best-First Search.
In general, if A is an instance of Best-First Search, we call TB(A) the algorithm that results from
applying the time-bounded approach to A. A second contribution of this paper is an extension to the
time-bounded search approach that allows the algorithm to deal more adequately with non-reversible
problems. The algorithm we propose here, Restarting Time-Bounded Weighted A*which we call
TBR (WA*), can be seen as lying in the middle ground between time-bounded algorithms and
learning-based real-time search algorithms like Korfs Learning Real-Time A* (LRTA*) (1990). In
fact, TBR (WA*) restarts search from the current state when a backtracking move is not available
and updates the heuristic function.
We carry out a theoretical analysis of both Time-Bounded Weighted A* (TB(WA*)), an instance
of TB(BFS), and of TBR (WA*). For TB(WA*) we establish upper and lower bounds for the
solution cost. Our cost bound establishes that, in some domains, the solution cost may be reduced
significantly by increasing w without increasing search time; hence, in contrast to what is wellknown about Weighted A* when solving offline search problems, we might obtain better solutions
by increasing the weight. This result is important since it suggests that TB(WA*) (with w > 1)
should be preferred to TB(A*) in domains in which WA* runs faster than A*. While WA* does not
always run faster than A* (see e.g., Wilt & Ruml, 2012), it is known that it does in many situations.
Experimentally, we evaluate TB(WA*) on pathfinding benchmarks and in the 15-puzzle, and
TBR (WA*) on the racetrack problem. In all three benchmarks we observe performance improvement as w is increased. In addition, we observe TB(WA*) is significantly superior to both TB(A*)
548

fiT IME -B OUNDED B EST-F IRST S EARCH FOR R EVERSIBLE AND N ON - REVERSIBLE S EARCH G RAPHS

and LSS-LRTWA* (Rivera, Baier, & Hernandez, 2015), a real-time search algorithm that can use
weighted heuristics.
This paper extends work that appears in conference proceedings (Hernandez, Asn, & Baier,
2014), by including an empirical analysis on new benchmarks (Counter Strike Maps, the racetrack,
and the 15-puzzle), by extending pathfinding experiments with 16-neighbor connectivity, by providing a lower bound for the cost of the solution returned by TB(WA*) (Theorem 2, below), and by
introducing, analyzing, and evaluating TBR (WA*).
The rest of the paper is organized as follows. We start by describing the background needed
for the rest of the paper. Then we describe TB(BFS) and TBR (BFS), including a formal analysis
of their properties. Then we describe the experimental results, and finish with a summary and
perspectives for future research.

2. Background
Below we describe the background for the rest of the paper.
2.1 Search in Reversible and Non-reversible Environments
A search graph is a tuple G = (S, A), where S is a finite set of states, A  S  S is a set of edges
which represent the actions available to the agent in each state. A path over graph (S, A) from s to
t is a sequence of states  = s0 s1    sn , where (si , si+1 )  A, for all i  {0, . . . , n  1}, s0 = s,
and sn = t. We say that t is a successor of s if (s, t) is an edge in A. Moreover, for every s  S we
define Succ(s) = {t | (s, t)  A}.
A cost function c for a search graph (S, A) is such that c : A  P
R+ ; i.e., it associates an action
with a positive cost. The cost of a path  = s0 s1    sn is c() = n1
i=0 c(si , si+1 ), i.e. the sum
of the costs of each edge considered in the path. A cost-optimal path from s to t is one that has
lowest cost among all paths between s and t; we denote this cost by c (s, t). In addition, we denote
by cT (s, t) the cost of a cost-optimal path between s and t that visits states only in T , that is, a
cost-optimal path  = s1 s2 . . . sn such that s = s1 , sn = t, and si  T , for all i  {2, . . . , n  1}.
A search problem is a tuple (S, A, c, sstart , sgoal ) where G = (S, A) is a search graph, sstart
and sgoal are states in S, and c is a cost function for G. A search graph G = (S, A) is reversible if
A is symmetric; that is, whenever (s, t)  A then (t, s)  A. A search problem is reversible if and
only if its search graph is reversible. Consequently, problem is non-reversible if its search graph
contains an action (s, t) but does not contain an action (t, s).
A solution to a search problem is a path from sstart to sgoal .
2.2 Best-First Search
Best-First Search (BFS) (Pearl, 1984) encompasses a family of search algorithms for static environments which associate an evaluation function f (s) with every state s. The priority is such that
f (s) < f (t) when s is viewed as a more promising node than t. BFS starts off by initializing the
priority of all states in the search space to infinity, except for sstart , for which the priority is set to
f (sstart ). A priority queue Open is initialized as containing sstart . In each iteration, the algorithm
extracts from Open the state with lowest priority, s. For each successor t of s it computes the evaluation fs (t), considering the path that has been found to t from s. If fs (t) is lower than f (t), then t
549

fiH ERN ANDEZ , BAIER , & A S IN

is added to Open and f (t) is set to fs (t). The algorithm repeats this process until sgoal is in Open
with the lowest priority.
A pseudo code is presented in Algorithm 1. The f -value of state s is usually implemented as
an attribute of s, and the Open list is implemented as a priority list. Furthermore, we assume the
cost fs (t) computed in Line 13 is a function of the path to t via s. Thus fs (t) can only take a finite
number of values during an execution of BFS, because it depends on the (finite) number of simple
paths that connect the initial state with s.
Algorithm 1: Best-First Search

16
17

sroot  scurrent
Open  
foreach s  S do
f (s)  
f (sroot )  evaluation for sroot
Insert sroot in Open
while Open 6=  do
Let s be the state with minimum f -value in Open
if s = sgoal then
return s
Remove s from Open
foreach t  Succ(s) do
fs (t)  evaluation function for t considering that t is discovered from s
if fs (t) < f (t) then
f (t)  fs (t)
parent(t)  s
Insert t in Open

18

return no solution

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

An instance of Best-First Search is Weighted A* (WA*) (Pohl, 1970). WA* computes the evaluation function in terms of two other functions, g and h. The g-value corresponds to the cost of the
lowest-cost path found so far towards s, and it is implemented as an attribute of s. WA*s evaluation function is defined as f (s) = g(s) + wh(s), where g(s) is the cost of the lowest-cost path
found from sstart to s. In addition, h is a non-negative, user-given heuristic function such that h(s)
estimates the cost of a path from s to sgoal . Finally, w is a real number greater than or equal to 1.
The pseudo-code for WA* can be obtained from Algorithm 1 by storing the g-value as an attribute of the state, while the h value is computed by an external function. The resulting pseudo-code
appears in Algorithm 2.
A heuristic function h is admissible if and only if h(s)  c (s, sgoal ), for all s  S. Function
h is consistent if h(sgoal ) = 0, and h(s)  c(s, t) + h(t) for every edge (s, t) of the search graph.
Consistency implies that if  is a path from s to t then h(s)  c() + h(t), which, in turn, implies
admissibility.
BFSs closed listdenoted henceforth by Closed is defined as the set of states that are not in
Open and that are such that g(s) is not infinity.1 In other words, it contains the states for which a
path is known but that are not being considered for re-expansion.
1. BFS initially sets f (s) to infinity for every s that is not the start node. In WA* this translates to setting g(s) to infinity
for all s except from sstart .

550

fiT IME -B OUNDED B EST-F IRST S EARCH FOR R EVERSIBLE AND N ON - REVERSIBLE S EARCH G RAPHS

Algorithm 2: Weighted A*

19
20

sroot  scurrent
Open  
foreach s  S do
g(s)  
f (s)  
g(sroot )  0
f (sroot )  wh(sroot )
Insert sroot in Open
while Open 6=  do
Let s be the state with minimum f -value in Open
if s = sgoal then
return s
Remove s from Open
foreach t  Succ(s) do
gs,t = min{g(t), g(s) + c(s, t)}
if gs,t < g(t) then
g(t)  gs,t
f (t)  g(t) + wh(t)
parent(t)  s
Insert t in Open

21

return no solution

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

If h is admissible, WA* is known to find a solution whose cost cannot exceed wc (sstart , sgoal ).
As such, WA* may return increasingly worse solutions as w is increased. The advantage of increasing w is that search time is usually decreased because fewer states are expanded. When w = 1,
WA* is equivalent to A* (Hart, Nilsson, & Raphael, 1968). Another interesting result generalizes a
well-known property of consistent heuristics of the A* algorithm. It is formally stated as follows:
Lemma 1 (Ebendt & Drechsler, 2009) At every moment during the execution of Weighted A* from
state sroot , if h is consistent, upon expansion of a state s (Line 14 of Algorithm 2), it holds that
g(s)  wc (sroot , s).
Another instance of Best-First Search is Greedy Best-First Search (GBFS). Here f is equal to
the user-given heuristic function h. When WA* is used with a sufficiently large value of w, both
WA* and GBFS rank nodes in a similar way. Indeed, let fGBFS and fWA* denote, respectively, the f
function for GBFS and WA*. If w is such that it exceeds the g-value of every node ever generated
and two nodes s1 and s2 have been generated with the same g-value by both algorithms such that
fGBFS (s1 ) = h(s1 ) > h(s2 ) = fGBFS (s2 ), then it will hold that fWA* (s1 ) > fWA* (s2 ). However,
even if w is sufficiently large, the reverse is not always true since fWA* (s1 ) > fWA* (s2 ) can hold
true when h(s1 ) = h(s2 ), because the g-value in fWA* acts in practice as a tie breaker.
2.3 Real-Time Heuristic Search
In real-time search the objective is to solve a search problem subject to an additional real-time
constraint. Under this constraint, a constant amount of time (independent of problem size) is given
to the search algorithm, by the end of which it is expected to perform one or more actions in a
sequence. Such a constant is very small in relation to the time that would be required by an offline
551

fiH ERN ANDEZ , BAIER , & A S IN

search algorithm to solve search problem. If after performing actions the agent has not reached
the goal, the process repeats. Each iteration of the algorithm can be understood as two consecutive
episodes: (1) a search episode, in which a path is computed, and (2) an execution episode, in which
the actions in such a path are performed.
Rather than receiving a time limit in seconds, most real-time search algorithms receive a parameter, say k, and guarantee that the computational time taken by the search episode is bounded by a
non-decreasing function of k. An example of a real-time search algorithm is Local Search-Space,
Learning Real-Time A* (LSS-LRTA*; Algorithm 3) (Koenig & Sun, 2009). It receives a search
problem P and a parameter k. In its search episode, it runs a bounded execution of A* rooted at
the current state which expands at most k states. Following, it updates the heuristic values of those
states in the closed list of the A* run. This update, usually referred to as learning step, makes h
more informed, and guarantees that the following holds for every s in A*s closed list:
h(s) = min {cClosed (s, t) + h(t)}.
tOpen

(1)

The execution episode performs the actions that appear in the path found by A* from the current
state towards the state which has lowest f -value in the open list. In reversible search spaces if h
Algorithm 3: LSS-LRTA*
1
2
3
4
5
6
7
8

Input: A search problem P and a natural number k
s  sstart
while s is not a goal state do
run A* from s until k states are expanded or a goal node the best state in Open
best  state in A*s closed list with lowest f -value
for each s  Closed do
update the h-value of s such that Equation 1 holds
move along the path found by A* between s and best
s  best,

is initially consistent it can be shown that LSS-LRTA* terminates when the search problem has a
solution (Koenig & Sun, 2009). If the search space is non-reversible, however, termination cannot
be guaranteed. As we see later, time-bounded algorithms (without restarts) can prove a solution
does not exist as well. This property does not hold for algorithms whose search expands nodes
whose distance from the current state is bounded, like LSS-LRTA*.
2.4 Comparing Two Real-Time Search Algorithms
One way frequently used in the literature to compare two real-time search algorithms A and B is by
comparing the cost of the returned paths when the algorithms are configured is such a way that their
search episodes have approximately the same duration. Assume that a real-time search algorithm
requires n search episodes to solve a search problem and that its runtime is T . Then we say that the
average time per search episode for that run is T /n.
To evaluate the relative performance of two algorithms A and B we use a set of benchmark
problems P and a set of algorithm parameters. For each parameter of algorithm A, we obtain and
record the average solution cost for all problems in P and the average time per episode. We do
likewise with B and then plot the average solution cost versus the average time per episode for each
algorithm. If the curve for algorithm A is always on top of the curve for algorithm B we can clearly
552

fiT IME -B OUNDED B EST-F IRST S EARCH FOR R EVERSIBLE AND N ON - REVERSIBLE S EARCH G RAPHS

state that B is superior to A, because B returns better-quality solutions for a comparable search time
per episode.
Another approach that has been used to compare real-time search algorithms is the Game Time
Model (Hernandez et al., 2012). In this model, time is partitioned into uniform time intervals. An
agent can execute one movement during each time interval, and search and movements are done
in parallel. The objective is to move the agent from its start location to its goal location in as few
time intervals as possible. The game time model is motivated by video games. Video games often
partition time into game cycles, each of which is only a couple of milliseconds long (Bulitko et al.,
2011). When using the Game Time Model, the implementation of the real-time search algorithm is
modified to stop search as soon as T units of timewhere T is a parameterhave passed.

3. Time-Bounded Best-First Search
Time-Bounded A* (TB(A*), Bjornsson et al., 2009) is a real-time search algorithm based on A*.
Intuitively, TB(A*) can be understood as an algorithm that runs an A* search from sstart to sgoal
that alternates a search phase with an execution phase until the goal is reached. In each search phase
a bounded number of states are expanded using A*. In the execution phase there are two cases.
If the agent is on the path from sstart to the best state in Open, then a forward movement on that
path is performed. Otherwise, the algorithm performs backtracking moves in which the agent is
moved to the state from where it came from. The search phase does not execute if a path connecting
sstart and sgoal has already been found. The algorithm terminates when the agent has reached the
goal.
Our generalization of TB(A*) is Time-Bounded Best-First Search, which simply replaces A*
in TB(A*) by a Best-First Search. Its pseudo code is shown in Algorithm 4. The parameters
are a search problem (S, A, c, sstart , sgoal ), and an integer k which we refer to as the lookahead
parameter.
TB(BFS) uses a variable scurrent to store the current state of the agent. Its MoveToGoal procedure (called from Main) implements the loop that alternates search and execution. At initialization
(Lines 2728) scurrent is initialized to sstart , and, among other things, BFSs Open list is set to
contain sstart only. If the goal state has not been reached (represented by the fact that variable
goalF ound is false), a bounded version of BFS is called (Line 31) that expands k states, and then
computes a path from sstart to the state in Open that minimizes the evaluation function f . The
path is built quickly by following parent pointers, and it is stored in variable path. In the execution phase (Lines 3236), if the current position of the agent, scurrent , is on path, then the agent
performs the action determined by the state immediately following scurrent on path. Otherwise, a
backtracking move is implemented by moving the agent to the parent of s in the search tree of BFS,
parent(scurrent ). The use of backtracking moves is a mechanism that guarantees that the agent will
eventually reach a state in variable path because, in the worst case, the agent will eventually reach
sstart . As soon as such a state is reached the agent will start moving towards the state believed to be
closest to the goal.
Algorithm 4 is equivalent to TB(A*) when BFS is replaced by A*. Finally, we call TimeBounded Greedy Best-First Search (TB(GBFS)) the algorithm that results when we use Greedy
Best-First Search instead of BFS.
Note that the length of the path cannot in general be bounded by a constant on the size of the
problem. To bound the computation of each search episode we can use the same technique described
553

fiH ERN ANDEZ , BAIER , & A S IN

Algorithm 4: Time-Bounded Best-First Search
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

procedure InitializeSearch()
sroot  scurrent
Open  
foreach s  S do
f (s)  
f (sroot )  evaluation for sroot
Insert sroot in Open
goalFound  false
function Bounded-Best-First-Search()
expansions  0
while Open 6=  and expansions < k and f (sgoal ) > mintOpen f (t) do
Let s be the state with minimum f -value in Open
Remove s from Open
foreach t  Succ(s) do
Compute fs (t) considering that t is discovered from s.
if fs (t) < f (t) then
f (t)  fs (t)
parent(t)  s
Insert t in Open
expansions  expansions + 1
if Open =  then return false
Let sbest be the state with minimum priority in Open.
if sbest = sgoal then goalFound  true
path  path from sroot to sbest
return true
function MoveToGoal()
scurrent  sstart
InitializeSearch()
while scurrent 6= sgoal do
if goalFound = false then
if Bounded-Best-First-Search() = false then return false
if scurrent is on path then
scurrent  state after scurrent on path
else
scurrent  parent(scurrent );
Execute movement to scurrent
return true
procedure Main
if MoveToGoal() = true then
print(the agent is now at the goal state)
else
print(no solution)

554

fiT IME -B OUNDED B EST-F IRST S EARCH FOR R EVERSIBLE AND N ON - REVERSIBLE S EARCH G RAPHS

by Bjornsson et al. (2009), whereby an additional counter (analogous to k is used to measure the
effort for path extraction). This is omitted from the pseudocode for clarity.
3.1 Properties
Now we analyze a few interesting properties of the algorithms we have just proposed. First, just
like TB(A*), TB(BFS) always terminates and finds a solution if one exists. This is an important
property since many real-time heuristic search algorithms (e.g., LSS-LRTA*) enter an infinite loop
on unsolvable problems. Second, we prove an upper and a lower bound on the cost of solutions
returned by TB(WA*). This bound is interesting since it suggests that by increasing w one might
obtain better solutions rather than worse.
Theorem 1 TB(BFS) will move an agent to the goal state given a reversible search problem P if a
solution to P exists. Otherwise, it will eventually print no solution.
Proof: Follows from the fact that Best-First Search eventually finds a path towards the goal. This is
because of the fact that the search space is finite and that each state can only be inserted into Open a
finite number of times. In addition, all moves carried out by the algorithm (including moving from
s to parent(s)) are executable in a reversible search space.

It is important to note that the reason why TB(BFS) will eventually print no solution in an
unsolvable problem is dependent on the fact that an Open list is used. LSS-LRTA* cannot always
detect unsolvable problems because search can only expand a locality around the current state. This
is a characteristic of agent-centered search algorithms (Koenig, 2001), a class of algorithms that
TB(BFS) is not a member of.
The following two lemmas are intermediate results that allow us to prove an upper bound on the
cost of solutions obtained with TB(WA*). The results below apply to TB(A*) but to our knowledge
Lemma 2 and Theorem 2 had not been proven before for TB(A*).
In the results below, we assume that P = (S, A, c, sstart , sgoal ) is a reversible search problem,
that TB(WA*) is run with a parameter w  1 and that h is an admissible heuristic. Furthermore, we
assume that c+ = max(u,v)A c(u, v), that c = min(u,v)A c(u, v), and that N (w) is the number
of expansions needed by WA* to solve P . Finally, we assume k  N (w) which is a reasonable
assumption given that we are in a real-time setting.
Lemma 2 The cost of the moves incurred by an agent controlled by TB(WA*) before goalFound
becomes true is bounded from below by b N (w)1
cc and bounded from above by b N (w)1
cc+ .
k
k
Proof: N (w)  1 states are expanded before goalFound becomes true. If k states are expanded
per call to the search procedure, then clearly b N (w)1
c is the number of calls for which Best-Firstk
Search terminates without setting goalFound to true. Each move costs at least c and at most c+ ,
from where the result follows.

Now we focus on the cost that is incurred after a complete path is found. The following Lemma
is related to a property enjoyed by TB(A*) and stated in Theorem 2 by Hernandez et al. (2012).
Lemma 3 The cost of the moves incurred by an agent controlled by TB(WA*) after goalFound has
become true cannot exceed 2wc (sstart , sgoal ).
555

fiH ERN ANDEZ , BAIER , & A S IN

Proof: Assume goalFound has just become true. Let  be the path that starts in sstart , ends in
scurrent and that is defined by following the parent pointers back to sstart . Path  is the prefix of a
path to the lowest f -value state in a previous run of WA* and therefore, by Lemma 1, is such that
c() < wc (sstart , sgoal ). Now the worst case in terms of number of movements necessary to reach
the goal is that path and  coincide only in sstart . In this case, the agent has to backtrack all the
way back to sstart . Once sstart is reached, the agent has to move to the goal through a path of cost at
most wc (sstart , sgoal ). Thus the agent may not incur a cost higher than 2wc (sstart , sgoal ) to reach
the goal.

Now we obtain a lower bound and an upper bound on the solution cost for TB(WA*) which
follows straightforwardly from the two previous lemmas.
Theorem 2 Let C be the solution cost obtained by TB(WA*). Then,
b

N (w)  1 
N (w)  1 +
cc  C  b
cc + 2wc (sstart , sgoal ).
k
k

Proof: We put together the inequalities implied by Lemmas 2 and 3.



A first observation about this result is that it has been shown empirically that in some domains,
when w is increased, N (w) may decrease substantially. Gaschnig (1977), for example, reports that
in the 8-puzzle N (1) is exponential in the depth d of the solution whereas N (w), for a large w is
subexponential in d. In other domains like grid pathfinding, it is well known that using high values
for w results in substantial reductions in expanded nodes (see e.g., Likhachev, Gordon, & Thrun,
2003). Thus, when increasing w, both the lower bound and the first term of the upper bound may
decrease substantially. The second term of the upper bound, 2wc (sstart , sgoal ), when increasing
w, may increase only linearly with w. This suggests that there are situations in which better- rather
than worse-quality solutions may be found when w is increased. As we see later, this is confirmed
by our experimental evaluation.
A second observation about the bounds is that the factor b(N (w)  1)/kc decreases as k increases. This suggests that when k is large (i.e., close to N (w)), increasing w may actually lead to
decreased performance.
Putting both observations together, Theorem 2 suggests that TB(WA*) will produce better solutions than TBA* when k is relatively small in problems in which WA* expands fewer nodes than
A* in offline mode. Problems in which WA* does not expand fewer nodes than A* exist (Wilt &
Ruml, 2012).
Finally, it is not hard to see that Theorem 2 can be generalized to other algorithms that provide
optimality guarantees. Given two search algorithms A and B that provide such bounds and whose
relative performance is known, the theorem can be used as a predictor of the relative performance
of TB(A) versus TB(B).
3.2 Non-reversible Search Problems via Restarting
In non-reversible problems, well-known real-time heuristic search algorithms such as LSS-LRTA*
will fail when, in the execution episode, a state from which there is no path to the goal is visited.
Time-bounded algorithms like TB(BFS) will fail under that very same condition but they will also
fail as soon as a physical backtrack is required over a non-reversible action. This second condition
556

fiT IME -B OUNDED B EST-F IRST S EARCH FOR R EVERSIBLE AND N ON - REVERSIBLE S EARCH G RAPHS

for failure is the reason why sometimes time-bounded algorithms are discarded for use in nonreversible domains. The objective of this section is to propose a time-bounded algorithm that, when
used in non-reversible problems, will not fail due to the latter condition, but only due to the former.
Our modification of TB(WA*) for non-reversible problems comes from incorporating into it the
two key characteristics of real-time search algorithms like LSS-LRTA*: search restarts and heuristic
updates. Indeed, whenever physical backtracking is not available, or, more generally, when some
predefined restart condition holds, our algorithm restarts search. In addition, to avoid getting
trapped in infinite loops, our algorithm updates the heuristic using the same update rule of LSSLRTA*. We call the resulting algorithm Restarting Time-Bounded Weighted A* (TBR (WA*)).
Algorithm 5 shows the details of TBR (WA*). Lines 1012 are the most relevant difference
with the previous algorithm. The algorithm restarts search when the agent is not in path and certain
restart condition, which must become true when there is no action leading from the current state
(scurrent ) to its parent (parent(scurrent )).
Algorithm 5: Restarting Time-Bounded Weighted A*
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

function MoveToGoal()
scurrent  sstart
InitializeSearch()
while scurrent 6= sgoal do
if goalFound = false then
if Bounded-WA*() = false then return false;
if scurrent is on path then
scurrent  state after scurrent on path
Execute movement to scurrent
else if restart condition holds then
Update heuristic function h using LSS-LRTA* update rule (Equation 1)
InitializeSearch()
else
scurrent  parent(scurrent );
Execute movement to scurrent
return true
procedure Main
if MoveToGoal() = true then
print(the agent is now at the goal state)
else
print(no solution)

Note that prior to restarting the algorithm updates the heuristic just as LSS-LRTA* would. This
can be implemented with a version of Dijkstras algorithm. Note that the number of states that may
need to be updated may not be bounded by a constant. If needed, we can compute the update in
an incremental manner, across several episodes. We refer the reader to the analysis of Koenig and
Sun (2009), and Hernandez and Baier (2012) for details about the implementation and proofs of
correctness.
3.2.1 T ERMINATION OF TBR (WA*)
TBR (WA*) can be used in both reversible and non-reversible domains. If the heuristic function h
is initially consistent and the search graph is strongly connected, the algorithm terminates.
557

fiH ERN ANDEZ , BAIER , & A S IN

Theorem 3 Let P be a search problem with a strongly connected search graph. Then TBR (WA*),
run with a consistent heuristic h, finds a solution for P .
The proof for Theorem 3 depends on some intermediate results, some of which have proofs that
appear elsewhere. The following result establishes that if h is consistent, then it remains consistent
after being updated.
Lemma 4 (Koenig & Sun, 2009) If h is consistent it remains consistent after h is updated with
Equation 1.
Another intermediate results says that h cannot decrease after an update following Equation 1.
Lemma 5 (Koenig & Sun, 2009) If h is initially consistent then h(s), for every s, cannot decrease
if h is updated following Equation 1.
Another intermediate result says that h(s) finitely converges, which intuitively means that even
if we wanted to apply an infinite number of updates to h, then from some point on, h will not change
anymore.
Definition 1 (Finite Convergence) A series of functions {fi }i0 finitely converges to function f if
there exists an n such that for every m  n, it holds that fm = f . In addition, we say that a series
of functions {fi }i0 finitely converges if there exists a function f to which it finitely converges.
Lemma 6 Let h0 be a consistent heuristic function and P be a strongly connected graph. Let
 = {hi }i0 be such that hk+1 is the function that results from (1) assigning hk to hk+1 then (2)
updating hk+1 using Equation 1, for some set Closed and Open generated by a bounded Weighted
A* run rooted at an arbitrary state. Then  finitely converges.
Proof: A first observation is that hk (s) is bounded from above by a positive number for every s
and every k. Indeed, because of Lemma 4, hk is consistent, and thus admissible, for every k. In
addition, because the problem has a solution, hk (s)  c (s, sgoal ), for every s and every k.
A second observation is that the set of h-values that any state s can take is finite, even if  is
infinite. Formally we prove H(s) = {hk (s) | k  0} is a finite set. Indeed, it is not hard to verify
by induction (we leave it as an exercise to the reader) that by using Equation 1, for every k  0, it
holds that hk (s) = c(ks ) + h0 (s0 ) for some, possibly empty path ks originating in s and finishing
in s0 . Now recall that hk (s) is bounded from above and observe there are only finitely many paths
in the graph whose cost is bounded. We conclude that H(s) is a finite set, for every s.
Now the proof follows by contradiction, assuming  does not finitely converge. Because  is
non-decreasing (Lemma 5), the only possibility is that  increases infinitely often. This implies
that there is at least one state s such that H(s) is infinite: a contradiction. We conclude  finitely
converges.

Note that the previous lemma is not saying anything about the function that  converges to; we
do not need to know which function is it for the rest of the proof. The last intermediate result is
related to the result by Ebendt and Drechsler (2009) that was stated in Section 2.2 (Lemma 1).
Lemma 7 At every moment during the execution of Weighted A* from state sroot , if h is consistent,
for every state s in the open list, it holds that g(s)  wcClosed (sroot , s).
558

fiT IME -B OUNDED B EST-F IRST S EARCH FOR R EVERSIBLE AND N ON - REVERSIBLE S EARCH G RAPHS

Proof: Let  be a cost-optimal path from sroot to s that visits only states in Closed. Let s0 be the
state that precedes s in . Because s0 is part of an optimal path we have:
cClosed (sroot , s0 ) + c(s0 , s) = cClosed (sroot , s).

(2)

Because s is a successor of s0 , it holds that:
g(s)  g(s0 ) + c(s0 , s).

(3)

g(s0 )  wc (sroot , s0 ),

(4)

g(s)  wc (sroot , s0 ) + c(s0 , s).

(5)

g(s)  wcClosed (sroot , s0 ) + wc(s0 , s) = w(cClosed (sroot , s0 ) + c(s0 , s)).

(6)

Because of Lemma 1, we have that:

Inequalities 3 and 4 imply:
Because w > 0 and cClosed  c :

Substituting with Equation 2 we have that:
g(s)  wcClosed (sroot , s),

(7)


which finishes the proof.
Now we provide a proof of the main result of this section.

Proof (of Theorem 3) : Let us assume the algorithm does not terminate and thus enters an infinite
loop. Note this means the algorithm restarts an infinite number of times (otherwise, Weighted A*
would eventually find the goal state, allowing the agent to reach the goal). Assume a moment
during this infinite execution after h has converged (we know this by Lemma 6), and let s1 s2 . . . be
an infinite sequence of states such that si is a state where search was restarted. We now prove that
for every i, h(si ) > h(si+1 ).
Let O denote the contents of the open list exactly when the algorithm expanded si+1 , and Closed
denote the contents of the closed list immediately before the heuristic is updated. From Equation 1,
the following holds:
h(si ) = cClosed (si , sO ) + h(sO ),

for some sO  O

(8)

We can rewrite Equation 8 as:
wh(si ) = wcClosed (si , sO ) + wh(sO ),

(9)

Let g(sO ) denote the g-value of sO exactly when si+1 is preferred for expansion over sO . Now,
we prove that wcClosed (si , sO )  g(sO ). Indeed, if sO  Closed this follows from Lemma 1 and
from the fact that cClosed  c and w  1. On the other hand, if sO  Open, then we obtain
wcClosed (si , sO )  g(sO ) from Lemma 7. Now we use this fact to write:
wh(si )  g(sO ) + wh(sO ).
559

(10)

fiH ERN ANDEZ , BAIER , & A S IN

Because the algorithm preferred to expand si+1 instead of sO , then g(sO ) + wh(sO )  g(si+1 ) +
wh(si+1 ), and hence:
wh(si )  g(si+1 ) + wh(si+1 ).
(11)
Finally, because w > 0 and g(si+1 ) > 0 we obtain h(si ) > h(si+1 ).
This implies that the sequence of states s1 s2 . . . has strictly decreasing h-values. But because
the state space is finite, it must be the case that si = sj , for some i and j with i 6= j, which would
lead to conclude that h(si ) > h(si ), a contradiction.


4. Experimental Results
This section presents our experimental results. The objective of our experimental evaluation was to
understand the effect of the weight configuration on the performance of both TB(WA*) and TBR
(WA*). To that end, we evaluate TB(WA*) in reversible search problems (grid pathfinding and the
15-puzzle), and TBR (WA*) in a non-reversible problem (the racetrack). For reference, we compare
against LSS-LRTWA* (Rivera et al., 2015), a version of LSS-LRTA* that uses Weighted A* rather
than A* in the search phase. We used this algorithm since it is among the few real-time search
algorithms that are able to exploit weights during search. LSS-LRTWA* is configured to perform a
single action in each execution phase.
We decided not to include results for WLSS-LRTA* (Rivera et al., 2015), another real-time
search algorithm that exploits weights, for two reasons. First, our new results are focused on relatively large lookahead values (over 128). With these lookahead values, Rivera et al. (2015), in
grid-like terrain, observe improvements but not very significant. Second, we observed that, on the
15-puzzle, WLSS-LRTA* yields worse performance as w is increased.
In Section 4.1 we report results in 8- and 16-neighbor grids in a similar manner as was reported
in an earlier publication (Hernandez et al., 2014). Section 4.2 reports results for 8- and 16-neighbor
grids using the Game Time Model (cf. Section 2.4). Section 4.3 reports results on non-reversible
maps in a deterministic version of a setting used to evaluate algorithms for the Stochastic ShortestPath problem (Bonet & Geffner, 2003). Finally, Subsection 4.4 reports results on the 15-puzzle.
The path-finding tasks of Section 4.1 and Section 4.2 are evaluated using 8-neighbor (Bulitko
et al., 2011; Koenig & Likhachev, 2005) and16-neighbor
 grids (Aine & Likhachev, 2013) (see
Figure 5). The costs of the movements are 1, 2, and 5 for, respectively, orthogonal, diagonal,
chess-knight movements. In our implementation the agent cannot jump over obstacles. In addition, a diagonal movement (d, d) (for d  {1, 1}) is illegal in (x, y) if either (x+d, y) or (x, y +d)
is an obstacle. For 8-neighbor and 16-neighbor grids we use the octile distance and the Euclidean
distance as heuristic values, respectively. All experiments were run on an Intel(R) Core(TM) i72600 @ 3.4Ghz machine, with 8Gbytes of RAM running Linux. All algorithms have a common
code base and use a standard binary heap for Open. Ties in Open are broken in favor of larger
g-values; we do not have a rule for breaking further ties.
4.1 Results in 8-Neighbor and 16-Neighbor Grid Maps
We evaluated the algorithms considering solution cost and runtime, as measures of solution quality
and efficiency, respectively, for several lookahead and weight values.
We used all 512  512 maps from the video game Baldurs Gate (BG), all the Room maps
(ROOMS), and all maps of different size from the Starcraft (SC) available from N. Sturtevants
560

fiT IME -B OUNDED B EST-F IRST S EARCH FOR R EVERSIBLE AND N ON - REVERSIBLE S EARCH G RAPHS

8-neighbor BG Maps

6000
4000
2000

Algorithm

Cost (log scale)

Cost (log scale)

)
)(3
.0

)
TB

(W
A*

)(2
.6

)
(W
A*

8-neighbor Counter Strike Maps

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

20000
10000

TB

Algorithm

8-neighbor Starcraft Maps

100000

)(2
.2

)
TB

(W
A*

)(1
.8

)
(W
A*
TB

TB

(W
A*

)(1
.0

)
)(3
.0

)
TB

(W
A*

)(2
.6

)
)(2
.2

(W
A*
TB

)(1
.8

(W
A*
TB

TB

(W
A*

)(1
.4

(W
A*

)(1
.0
TB

(W
A*
TB

)

300
)

500

300
)

500

)

1000

(W
A*

1000

10000

)(1
.4

2000

Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

TB

6000
4000

Cost (log scale)

Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

10000
Cost (log scale)

8-neighbor Room Maps
30000

2000
1000
500

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

1e+06

100000
20000
10000

Algorithm

0)
A*
)

(3
.

6)
TB

(W

A*
)

(2
.

2)
TB

(W

A*
)

(2
.

8)
(W
TB

(W

A*
)

(1
.

4)
TB

(W

A*
)

(1
.

0)
TB

TB

(W

A*
)

(1
.

0)
A*
)

(3
.

6)
TB

(W

A*
)

(2
.

2)
TB

(W

A*
)

(2
.

8)
TB

(W

A*
)

(1
.

4)
(1
.
TB

(W

A*
)
(W

TB

TB

(W

A*
)

(1
.

0)

2000

Algorithm

Figure 1: In the 8-neighbor results, solution cost tends to decrease as w or the lookahead parameter
is increased.

path-finding repository (Sturtevant, 2012). In addition, we used 7 large maps from Counter Strike
(CS), whose sizes range between between 4259  4097 and 4096  5462.
We evaluated six lookahead values (1, 4, 16, 64, 128, 256) for the 512  512 maps and six
lookahead values (1, 32, 128, 256, 512, 1024) for SC and CS maps. We used six weight values
(1.0, 1.4, 1.8, 2.2, 2.6, 3.0). For each map we generated 50 random solvable search problems, resulting in 1800 problems for BG, 2000 problems for ROOMS, 3250 problems for SC, and 350
problems for CS.
Figures 1 and 2 show performance measures for the 8-neighbor grid maps. Note here that the
average search time per episode is the same across all algorithms when using the same lookahead
parameter. This is because search time per episode is proportional to the lookahead parameter and
depends on no other variable (in particular, it does not depend on the weight). Thus fair conclusions
can be drawn when comparing two configurations if their lookahead parameter is set to the same
value.
561

fiH ERN ANDEZ , BAIER , & A S IN

8-neighbor BG Maps

)
)(3
.0

)
TB

(W
A*

)(2
.6

)
(W
A*

8-neighbor Counter Strike Maps

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

Runtime (ms)

Runtime (ms)

)(2
.2

)
)(1
.8

Algorithm

8-neighbor Starcraft Maps

50

(W
A*

TB

TB

(W
A*

)(1
.0
(W
A*

A*
)(

Algorithm

TB

)

0)
3.

6)
TB
(

W

A*
)(

2.

2)
TB
(

W

A*
)(

2.

8)
W
TB
(

TB
(

W

A*
)(

1.

4)
1.
A*
)(
W

TB
(

TB
(

W

A*
)(

1.

0)

0

)

1

10
8
6
4
2
0
(W
A*

2

)(1
.4

3

20

TB

Runtime (ms)

4

Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

25

TB

Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

5
Runtime (ms)

8-neighbor Room Maps

5
3
1

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

20000
10000

1000
200

Algorithm

)

)

(3.0
A*)
TB
(W

TB
(W

A*)

(2.6

)
(2.2
A*)
TB
(W

)
(1.8
A*)
TB
(W

)
(1.4

(1.0

A*)
TB
(W

(W

TB
(W

A*)

0)
A*
)

(3
.

6)
TB

TB

(W

A*
)

(2
.

2)

(W

A*
)

(2
.

8)
TB

TB

(W

A*
)

(1
.

4)
(1
.
A*
)

(W
TB

TB

(W

A*
)

(1
.

0)

)

60

Algorithm

Figure 2: In the 8-neighbor results, search time typically decreases as w or the lookahead parameter
is increased.

We observe the following relations hold for all maps regarding solution cost and search time.

Solution Cost For most lookahead values, solution cost decreases as w is increased. More significant improvements are observed for lower lookahead values. This is not surprising in the light
of our cost bound (Theorem 2) . For large lookahead parameters ( 256), the value of w does
not affect solution cost significantly. When the lookahead parameter increases, fewer search
episodes are needed and less physical backtracks (back moves) are needed (Hernandez et al.,
2014). Back moves strongly influence the performance of the algorithms. In TB(WA*), when
w is increased the number of back moves decreases, which explains the improvement in solution quality. For example, in the BG maps, when using lookahead 1, the average reduction of
back moves is 1,960.5, when comparing w = 1 and w = 3, whereas when lookahead is 512
this reduction is only 2.4, when comparing w = 1 and w = 3.
562

fiT IME -B OUNDED B EST-F IRST S EARCH FOR R EVERSIBLE AND N ON - REVERSIBLE S EARCH G RAPHS

16-neighbor BG Maps
Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

6000
4000
2000
1000

10000
6000
4000
2000
1000

500

Algorithm

Cost (log scale)

Cost (log scale)

)
)(3
.0

)
TB

(W
A*

)(2
.6

)
(W
A*

16-neighbor Counter Strike Maps

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

20000
10000

TB

Algorithm

16-neighbor Starcraft Maps

100000

)(2
.2

)
TB

(W
A*

)(1
.8

(W
A*

)(1
.4
TB

)
(W
A*
TB

TB

(W
A*

)(1
.0

)
)(3
.0

)
TB

(W
A*

)(2
.6

)
TB

(W
A*

)(2
.2

)
)(1
.8

(W
A*
TB

)
(W
A*
TB

)

)(1
.4

(W
A*

)(1
.0
TB

(W
A*

)

500
300

300

TB

Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

30000
Cost (log scale)

10000
Cost (log scale)

16-neighbor Room Maps

2000
1000
500

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

1e+06

100000
20000
10000

Algorithm

0)
A*
)

(3
.

6)
TB

(W

A*
)

(2
.

2)
TB

(W

A*
)

(2
.

8)
(W
TB

(W

A*
)

(1
.

4)
TB

(W

A*
)

(1
.

0)
TB

TB

(W

A*
)

(1
.

0)
A*
)

(3
.

6)
TB

(W

A*
)

(2
.

2)
TB

(W

A*
)

(2
.

8)
TB

(W

A*
)

(1
.

4)
(1
.
TB

(W

A*
)
(W

TB

TB

(W

A*
)

(1
.

0)

2000

Algorithm

Figure 3: In the 16-neighbor results, solution cost tends to decrease as w or the lookahead parameter
is increased.

Search Time As w is increased, search time decreases significantly for lower lookahead values
and decreases moderately for higher lookahead values. In ROOMS we observe the largest
improvements when w is increased. This behavior in ROOMS is explained because WA*
performs very well in this type of map for w > 1.
Figures 3 and 4 show performance measures for the 16-neighbor grid maps. We observe the
same relations observed in 8-neighbor grid maps regarding solution cost and search time.
4.1.1 8-N EIGHBOR VERSUS 16-N EIGHBOR G RID M APS
Lower cost solutions are obtained with 8-neighbor grids than with 16-neighbor grids for the lookahead values 1, 4, and 16 in BG. Note that there exist some 16-neighbor movements which are more
expensive than any 8-neighbor moves, so for small lookaheads, 16-neighbor solutions may have
a similar number of moves, but a worse quality than 8-neighbor solutions. On the other hand, a
563

fiH ERN ANDEZ , BAIER , & A S IN

16-neighbor BG Maps

)
)(3
.0

)
TB

(W
A*

)(2
.6

)

16-neighbor Counter Strike Maps

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

Runtime (ms)

Runtime (ms)

(W
A*

Algorithm

16-neighbor Starcraft Maps

50

)(2
.2

)

Algorithm

(W
A*

TB

TB

(W
A*

)(1
.0
(W
A*

A*
)(

)(1
.8

)

0)
3.

6)
TB
(W

A*
)(

2.

2)
TB
(W

A*
)(

2.

8)
TB
(W

A*
)(

1.

4)
1.
TB
(W

A*
)(
TB
(W

TB
(W

A*
)(

1.

0)

0

10
8
6
4
2
0

TB

1

)

2

(W
A*

3

20

)(1
.4

4

25

TB

Runtime (ms)

5

Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

30

TB

Lookahead 1
Lookahead 4
Lookahead 16
Lookahead 64
Lookahead 128
Lookahead 256

7
Runtime (ms)

16-neighbor Room Maps

5
3
1

Lookahead 1
Lookahead 32
Lookahead 128
Lookahead 256
Lookahead 512
Lookahead 1024

20000
10000

1000
200

Algorithm

)

)

(3.0
A*)
TB
(W

TB
(W

A*)

(2.6

)
(2.2
A*)
TB
(W

)
(1.8
A*)
TB
(W

)
(1.4

(1.0

A*)
TB
(W

(W

TB
(W

A*)

0)
A*
)

(3
.

6)
TB

TB

(W

A*
)

(2
.

2)

(W

A*
)

(2
.

8)
TB

TB

(W

A*
)

(1
.

4)
(1
.
A*
)

(W
TB

TB

(W

A*
)

(1
.

0)

)

60

Algorithm

Figure 4: In the 16-neighbor results, search time typically decreases as w or the lookahead parameter is increased.

(a)

(b)

Figure 5: 8-neighborhoods (a) and 16-neighborhoods (b).

564

fiT IME -B OUNDED B EST-F IRST S EARCH FOR R EVERSIBLE AND N ON - REVERSIBLE S EARCH G RAPHS

8-neighbor Counter Strike Maps
10000

TB(WA*)(1.0)
TB(WA*)(1.4)
TB(WA*)(1.8)
TB(WA*)(2.2)
TB(WA*)(2.6)
TB(WA*)(3.0)

9000
8000
7000
6000
5000
4000
3000
2000
1000

TB(WA*)(1.0)
TB(WA*)(1.4)
TB(WA*)(1.8)
TB(WA*)(2.2)
TB(WA*)(2.6)
TB(WA*)(3.0)

9000
Number of Time Intervals

Number of the Time Intervals

10000

16-neighbor Counter Strike Maps

8000
7000
6000
5000
4000
3000
2000

0.1

0.3

0.5

0.7

0.9

1.1

1000

Duration of Time Interval (ms)

0.1

0.3

0.5

0.7

0.9

1.1

Duration of the Time Interval (ms)

Figure 6: Results in the Game Time Model.
similar quality is observed for other lookahead values. TB(WA*), for almost all values of w and
lookahead configurations, on 16-neighbor grids performs fewer moves than on 8-neighbor grids.
For example, in SC when w = 2.6 and the lookahead parameter is 1024, 8-neighbor grids need
a factor of 1.6 more moves than 16-neighbor grids. Note however that some 16-neighbor moves
have a higher cost than 8-neighbor moves. Regarding runtime, TB(WA*) in 8-neighbor connectivity runs faster than TB(WA*) in 16-neighbor connectivity. This happens because the expansion of a
state with 16-neighbor connectivity takes more time than expanding the same state with 8-neighbor
connectivity.
4.2 Results on the Game Time Model
We report results for TB(WA*) using the Game Time Model in the Counter Strike maps for 8and 16-neighbor grids. We use 0.1, 0.3, 0.5, 0.7, 0.9, 1.1 milliseconds as the duration of the time
intervals. In this setting, the quality of the solution is measured as the number of time intervals
required to solve the problem, so the fewer the intervals that are used, the better the solution quality
is.
Figure 6 shows average performance. We observe that when the length of the time interval increases, TB(WA*) yields solutions of better quality. On the other hand, as w is increased, TB(WA*)
obtains better solutions. This can be observed more clearly when the duration of the intervals is
small (e.g., 0.1ms). We also observe that better-quality solutions with 16- rather that with 8-neighbor
connectivity. This is because with 16-neighbor connectivity the agent can perform a knight move in
a single interval.
4.3 Results on Non-reversible Search Graphs: The Racetrack
In this section we compare TBR(WA*) and LSS-LRTWA* on a deterministic version of the racetrack problem (Barto, Bradtke, & Singh, 1995; Bonet & Geffner, 2003). In this problem the race565

fiH ERN ANDEZ , BAIER , & A S IN

Extended Hansen Racetrack
400

TBR(WA*)(1.0)
TBR(WA*)(3.0)
TBR(WA*)(5.0)
TBR(WA*)(7.0)
LSS-LRT(WA*)(1.0)
LSS-LRT(WA*)(3.0)
LSS-LRT(WA*)(5.0)
LSS-LRT(WA*)(7.0)

300
250

TBR(WA*)(1.0)
TBR(WA*)(3.0)
TBR(WA*)(5.0)
TBR(WA*)(7.0)
LSS-LRT(WA*)(1.0)
LSS-LRT(WA*)(3.0)
LSS-LRT(WA*)(5.0)
LSS-LRT(WA*)(7.0)

450
400
Number of Actions

350
Number of Actions

Game Map Racetrack
500

200
150

350
300
250
200
150

100

100

Average Time per Search (ms)

2

8

6

1.

4

1.

1.

1

2
1.

8

6

0.

4

0.

0.

2
0.

2

2
2.

8

6

1.

4

1.

1

2

1.

1.

8

6

0.

0.

4

50
0.

0.

2

50

Average Time per Search (ms)

Figure 7: Results on the Racetrack Grids.
track is represented as a grid where some cells are marked as obstacles. Similar to grid pathfinding,
the problem is to move an agent from a set of initial positions to any of the cells marked as a final
position. Nevertheless, in this problem the agent has an associated velocity, and the set of actions
involve accelerating (vertically or horizontally), or performing a no-op action which maintains the
current velocity.
A state in the racetrack is a tuple (x, y, vx , vy ), where (x, y) is the position of the vehicle, and
(vx , vy ) is the velocity vector. The actions are represented as tuples of the form (ax , ay ), where
ax , ay  {1, 0, 1}, which correspond to an acceleration vector. Unlike the original version (Barto
et al., 1995), in ours actions are deterministic and we have only one initial and one destination cell.
Because actions are deterministic, when (ax , ay ) is performed in (x, y, vx , vy ), the new state is given
by (x0 , y 0 , vx0 , vy0 ), where vx0 = vx +ax and vy0 = vy +ay , and where (x0 , y 0 ) is computed considering
the vehicle changes its velocity to (vx0 , vy0 ) before moving. When the movement towards (x0 , y 0 )
would lead to crashing into an obstacle, like Bonet and Geffner (2003) do, we leave the vehicle next
to such an obstacle with velocity (0, 0).
In our experiments, we used two racetracks. The firstwhich we refer to as HRTis a 33 
207 grid which corresponds to an extended version of the racetrack used by Hansen and Zilberstein (2001) (which is a 33  69 grid). We also use the game map AR0205SR from Baldurs Gate,
whose size is 214x212. Below we refer to such a map by GRT.
We generated 50 random test cases for HRT and GRT that were such that the Manhattan distance
between the initial state and goal state was greater than half of the width of the map. The absolute
value of each of the components of the velocity vector is restricted to be at most 3. As a heuristic
we use the Euclidean distance divided by the maximum speed.
We evaluated TBR (WA*) and LSS-LRTAWA* with four weight values (1.0, 3.0, 5.0, 7.0). Figure 7 shows a plot of the number of actions versus average time per search episode. For TBR (WA*)
the number of actions corresponds to the sum of the number of moves plus the number of times the
566

fiT IME -B OUNDED B EST-F IRST S EARCH FOR R EVERSIBLE AND N ON - REVERSIBLE S EARCH G RAPHS

TBWA*(2)
TBWA*(3)
TBWA*(4)
TBWA*(5)
LSS-LRTWA*(2)
LSS-LRTWA*(3)
LSS-LRTWA*(4)
LSS-LRTWA*(5)

0
50
10
0
15
0
20
0
25
0
30
0
35
0
40
0
45
0
50
0
55
0

100

Lookahead

100000

TBWA*(2)
TBWA*(3)
TBWA*(4)
TBWA*(5)
LSS-LRTWA*(2)
LSS-LRTWA*(3)
LSS-LRTWA*(4)
LSS-LRTWA*(5)

10000

0
50
10
0
15
0
20
0
25
0
30
0
35
0
40
0
45
0
50
0
55
0

1000

15-puzzle
Number of Expansions (log scale)

Cost (log scale)

15-puzzle

Lookahead

Figure 8: Cost and time comparison between TB-WA and LSS-LRTWA*

vehicle did not move. We do this because TBR (WA*) does not make any movements when search
is restarted.
It is important to note that the time spent updating the heuristic is proportional to the number
of states being updated. As such an update by TBR (WA*) may take more time than an update
by LSS-LRTWA* because the Closed list may contain more states for the former algorithm. For
this reason we use in our comparison the average time per search, which considers both search and
update time.
In HRT (Figure 7) we observe that the worst behavior is the one obtained with TBR (WA*)(1.0).
Both algorithms improve performance when increasing w, but TBR (WA*), used with a weight
greater than 1.0, is the algorithm that clearly yields the best performance. In GRT, the worst algorithms are TBR (WA*)(1.0) and LSS-LRTA(1.0). Here, both algorithms improve when increasing
the weight.
Because in this benchmark we used fewer problems than on the game maps, we carried out
a 95% confidence analysis on for the cost of the solutions. In the HRT, this showed that costs
for our best configuration TBR (WA*)(5.0) could be 10% away from the true mean, while for
LSS-LRTA*(3.0) costs could be 11% away from the true mean. In the GRT, on the other hand,
the difference in performance of the two best configurations TBR (7) and LSS-LRTWA*(7) is not
statistically significant.
Finally, our experiments showed that the computational cost of learning phase of TB(WA*) is
not higher than that of LSS-LRTA(WA*). Indeed, the number of updates carried out by TB(WA*)
is 3.4 times less than the number of updates carried out by LSS-LRTA(WA*) in HRT and 1.6 time
less in GRT. This explains the better performance in terms of runtime.
567

fiH ERN ANDEZ , BAIER , & A S IN

4.4 Results on the 15-Puzzle
We chose the 15-puzzle as a another domain for evaluating the time-bounded algorithms. We
build our 15-puzzle implementation extending Richard Korfs implementation available from Carlos Linaress homepage.2 We present results for TB(WA*), and LSS-LRT(WA*) algorithms. We
use the 100 test cases presented by Korf (1993), which uses the Manhattan distance as a heuristic.
In this domain we report the results in a slightly different way. First, we omit results for TB(A*)
(TB(WA*) with w = 1) because it does not terminate in a reasonable time. This is due to the fact
that A* needs too many expansions for solving the hardest test cases. Second, we use the number
of expansions instead of runtime as an efficiency measure. In this domain, we found this measure
to be more stable since, in general, solving all 100 problems does not take too much time when
w > 1 (0.3s for w = 2; 0.08s for w = 3), and thus time is prone to be affected by external factors
controlled by the operating system.
Figure 8 shows the performance of TB(WA*) and LSS-LRT(WA*). We use lookahead values
in {16, 32, 64, 128, 256, 512} and weights in {2, 3, 4, 5}. We observe the following relations.
Solution Cost The solution cost of TB(WA*) decreases as w is increased for almost all lookahead
values. TB(WA*) obtains better results than LSS-LRTWA* for all lookahead values when
w > 2. With w < 2 the performance of TB(WA*) is worse than the performence of LSSLRTA*. On the other hand, TB(WA*) with w = 5 obtains a solution 2.0 times better on
average than the solution obtained by LSS-LRTA* (LSS-LRTWA* with w = 1).
Number of Expansions The number of expansions of TB(WA*) decreases as w is increased.
TB(WA*) is more efficient than LSS-LRTWA* for all lookahead values and w > 2. The
worst performing configuration for TB(WA*) is w = 1.
Note that the curve remains flat for several of the configurations. This is because a small
number of expansions are needed to solve the problem.
In conclusion, considering solution cost and number of expansions, in 15-puzzle TB(WA*) is
the better algorithm. For instance, the average solution cost of TB(WA*) is 1.6 times better on
average than the average solution cost of LSS-LRTA*.
We did not compare to the greedy algorithm of (Parberry, 2015), which is real-time but domainspecific, unlike our.

5. Summary and Conclusions
This paper introduced Time-Bounded Best-First Search, a generalization of the real-time search
algorithm Time-Bounded A*. In addition, it introduced a restarting version of the time-bounded
approach, TBR (WA*), which unlike TB(BFS), has a better coverage of non-reversible domains.
We carried out a theoretical analysis of both TB(WA*) and TBR (WA*), including termination
results and a cost bound for TB(WA*). Given a weight w, our bound suggests that TB(WA*) will be
significantly superior to TB(A*) precisely on search problems in which WA* expands significantly
fewer states than A*. In addition, our bound suggests that TB(WA*) may not yield benefits in
domains in which WA*, run offline, will not yield any improvements over A*. Our theoretical
bounds can be easily adapted to other instances of Best-First Search that offer guarantees on solution
2. http://scalab.uc3m.es/clinares/download/source/ida/ida.html

568

fiT IME -B OUNDED B EST-F IRST S EARCH FOR R EVERSIBLE AND N ON - REVERSIBLE S EARCH G RAPHS

quality. For TBR (WA*), we proved termination in strongly connected graphs, even if they contain
non-reversible actions. This property is also enjoyed by real-time search algorithms of the LRTA*
family but is not enjoyed by TB(BFS).
In our experimental evaluation, that focused on pathfinding, the 15-puzzle, and the racetrack
problem, we found both TB(WA*) and TBR (WA*) to be significantly superior to some real-time
search algorithms of the LRTA* family. In addition, we found that performance tends to improve as
the weight parameter is increased, without increasing the time per search episode. This finding is
interesting because although quality can also be improved by increasing the lookahead parameter,
this increases the time spent on each search episode.
It is well known that in many search benchmarks, WA* may expand significantly fewer nodes
than A*. Consistent with this, in our experiments, time-bounded versions of suboptimal algorithms
like Weighted A* produce significantly better solutions than those obtained by TB(A*). Improvements are less noticeable when the lookahead parameter is large, as is also predicted by theory.
We are not the first to observe performance gains when using weights in a real-time setting.
Indeed, our findings are consistent with those of Rivera et al. (2015), who also obtain better solutions
by using weighted heuristics. Our work adds another piece of evidence that justifies studying the
incorporation of weights into other real-time algorithms (e.g., RIBS and EDA;* Sturtevant, Bulitko,
& Bjornsson, 2010; Sharon, Felner, & Sturtevant, 2014). Finally, SLA* (Shue & Zamani, 1993)
and LRTS (Bulitko & Lee, 2006) are two algorithms that also perform backtracking moves. An
investigation of whether or not restarts could provide benefits for those algorithms is left for future
work.

Acknowledgements
We thank Vadim Bulitko for providing the Counter Strike maps. This research was partly funded by
Fondecyt grant number 1150328.

References
Aine, S., & Likhachev, M. (2013). Truncated incremental search: Faster replanning by exploiting
suboptimality. In Proceedings of the 27th AAAI Conference on Artificial Intelligence (AAAI),
Bellvue, Washington, USA.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamic programming. Artificial Intelligence, 72(1-2), 81138.
Bjornsson, Y., Bulitko, V., & Sturtevant, N. R. (2009). TBA*: Time-bounded A*. In Proceedings
of the 21st International Joint Conference on Artificial Intelligence (IJCAI), pp. 431436.
Bonet, B., & Geffner, H. (2003). Labeled rtdp: Improving the convergence of real-time dynamic
programming.. In ICAPS, Vol. 3, pp. 1221.
Bulitko, V., & Lee, G. (2006). Learning in real time search: a unifying framework. Journal of
Artificial Intelligence Research, 25, 119157.
Bulitko, V., Bjornsson, Y., Sturtevant, N., & Lawrence, R. (2011). Real-time Heuristic Search for
Game Pathfinding. Applied Research in Artificial Intelligence for Computer Games. Springer.
Burns, E., Ruml, W., & Do, M. B. (2013). Heuristic search when time matters. Journal of Artificial
Intelligence Research, 47, 697740.
569

fiH ERN ANDEZ , BAIER , & A S IN

Ebendt, R., & Drechsler, R. (2009). Weighted A* search - unifying view and application. Artificial
Intelligence, 173(14), 13101342.
Gaschnig, J. (1977). Exactly how good are heuristics?: Toward a realistic predictive theory of bestfirst search. In Reddy, R. (Ed.), Proceedings of the 5th International Joint Conference on
Artificial Intelligence (IJCAI), pp. 434441. William Kaufmann.
Hansen, E. A., & Zilberstein, S. (2001). Lao: A heuristic search algorithm that finds solutions with
loops. Artificial Intelligence, 129(1), 3562.
Hart, P. E., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination of
minimal cost paths. IEEE Transactions on Systems Science and Cybernetics, 4(2).
Hernandez, C., Asn, R., & Baier, J. A. (2014). Time-bounded best-first search. In Proceedings of
the 7th Symposium on Combinatorial Search (SoCS).
Hernandez, C., & Baier, J. A. (2012). Avoiding and escaping depressions in real-time heuristic
search. Journal of Artificial Intelligence Research, 43, 523570.
Hernandez, C., Baier, J. A., Uras, T., & Koenig, S. (2012). TBAA*: Time-Bounded Adaptive A*.
In Proceedings of the 10th International Joint Conference on Autonomous Agents and Multi
Agent Systems (AAMAS), pp. 9971006, Valencia, Spain.
Koenig, S. (2001). Agent-centered search. Artificial Intelligence Magazine, 22(4), 109131.
Koenig, S., & Likhachev, M. (2005). Fast replanning for navigation in unknown terrain. IEEE
Transactions on Robotics, 21(3), 354363.
Koenig, S., & Likhachev, M. (2006). Real-time adaptive A*. In Proceedings of the 5th International
Joint Conference on Autonomous Agents and Multi Agent Systems (AAMAS), pp. 281288.
Koenig, S., & Sun, X. (2009). Comparing real-time and incremental heuristic search for real-time
situated agents. Autonomous Agents and Multi-Agent Systems, 18(3), 313341.
Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42(2-3), 189211.
Korf, R. E. (1993). Linear-space best-first search. Artificial Intelligence, 62(1), 4178.
Likhachev, M., Gordon, G. J., & Thrun, S. (2003). ARA*: Anytime A* with Provable Bounds on
Sub-Optimality. In Proceedings of the 16th Conference on Advances in Neural Information
Processing Systems (NIPS), Vancouver, Canada.
Parberry, I. (2015). Memory-efficient method for fast computation of short 15-puzzle solutions.
IEEE Trans. Comput. Intellig. and AI in Games, 7(2), 200203.
Pearl, J. (1984). Heuristics: Preintelligent Search Strategies for Computer Problem Solving.
Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.
Pohl, I. (1970). Heuristic search viewed as path finding in a graph. Artificial Intelligence, 1(3),
193204.
Rivera, N., Baier, J. A., & Hernandez, C. (2015). Incorporating weights into real-time heuristic
search. Artificial Intelligence, 225, 123.
Schmid, K., Tomic, T., Ruess, F., Hirschmuller, H., & Suppa, M. (2013). Stereo vision based indoor/outdoor navigation for flying robots. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 39553962.
570

fiT IME -B OUNDED B EST-F IRST S EARCH FOR R EVERSIBLE AND N ON - REVERSIBLE S EARCH G RAPHS

Sharon, G., Felner, A., & Sturtevant, N. R. (2014). Exponential deepening a* for real-time agentcentered search. In Proceedings of the 7th Symposium on Combinatorial Search (SoCS), pp.
871877.
Shue, L., & Zamani, R. (1993). An admissible heuristic search algorithm. In Komorowski, H. J.,
& Ras, Z. W. (Eds.), Proceedings of the 7th International Symposium Methodologies for
Intelligent Systems (ISMIS), Vol. 689 of LNCS, pp. 6975. Springer.
Sturtevant, N. (2012). Benchmarks for grid-based pathfinding. Transactions on Computational
Intelligence and AI in Games, 4(2), 144  148.
Sturtevant, N. R., Bulitko, V., & Bjornsson, Y. (2010). On learning in agent-centered search. In
Proceedings of the 9th International Joint Conference on Autonomous Agents and Multi Agent
Systems (AAMAS), pp. 333340, Toronto, Ontario.
Wilt, C. M., & Ruml, W. (2012). When does weighted A* fail?. In Proceedings of the 5th Symposium on Combinatorial Search (SoCS), Niagara Falls, Ontario, Canada.

571

fiJournal of Artificial Intelligence Research 56 (2016) 379-402

Submitted 09/15; published 06/16

Generating Models of a Matched Formula
With a Polynomial Delay
Petr Savicky

savicky@cs.cas.cz

Institute of Computer Science, The Czech Academy of Sciences
Pod Vodarenskou Vez 2, 182 07 Praha 8, Czech Republic

Petr Kucera

kucerap@ktiml.mff.cuni.cz
Department of Theoretical Computer Science and Mathematical Logic
Faculty of Mathematics and Physics, Charles University in Prague,
Malostranske nam. 25, 118 00 Praha 1, Czech Republic

Abstract
A matched formula is a CNF formula whose incidence graph admits a matching which
matches a distinct variable to every clause. Such a formula is always satisfiable. Matched
formulas are used, for example, in the area of parametrized complexity. We prove that
the problem of counting the number of the models (satisfying assignments) of a matched
formula is #P-complete. On the other hand, we define a class of formulas generalizing the
matched formulas and prove that for a formula in this class one can choose in polynomial
time a variable suitable for splitting the tree for the search of the models of the formula.
As a consequence, the models of a formula from this class, in particular of any matched
formula, can be generated sequentially with a delay polynomial in the size of the input.
On the other hand, we prove that this task cannot be performed efficiently for linearly
satisfiable formulas, which is a generalization of matched formulas containing the class
considered above.

1. Introduction
In this paper, we consider the problem of counting the models (satisfying assignments) and
generating subsets of the models of a given formula in conjunctive normal form (CNF). It is
well known that the problem of counting the models of a general CNF is #P-complete (Sipser,
2006). The problem of generating the models of a general CNF formula is clearly also hard,
because checking whether there is at least one satisfying assignment of the formula, the
SAT problem, is NP-complete (Garey & Johnson, 1979).
In this paper, we mostly deal with the problem of enumerating models of a formula.
This problem is important in areas of research and applications, such as unbounded model
checking (Kang & Park, 2005; McMillan, 2002) or data mining (Coquery, Jabbour, Sais,
Salhi, et al., 2012). The success of modern SAT solvers inspired design of model counting
and enumeration algorithms as well (see e.g. Jabbour, Lonlac, Sais, & Salhi, 2014; Morgado
& Marques-Silva, 2005a, 2005b). In addition to the basic enumeration problem in which we
do not require the models to be generated in any prescribed order, other versions have been
considered, e.g. generating models by non-decreasing weight (Creignou, Olive, & Schmidt,
2011).
Another line of research concentrated on studying special classes of boolean formulas
for which an enumeration algorithm with guaranteed complexity could be devised. One can
c
2016
AI Access Foundation. All rights reserved.

fiSavicky & Kucera

easily find an example of a formula for which the set of models is exponentially larger than
the size of the formula itself. In such a case it is reasonable to include the size of the output
into the bound on the running time of an enumeration algorithm. More specifically we say
that an algorithm which enumerates models of a formula runs in output polynomial time
if its running time can be bounded by a polynomial in two variables, the size of the input
(i.e. the input formula ) and the size of the output (i.e. the number of models of ). In
this paper, we consider more restrictive setting as follows. The algorithm receives as input
a formula and generates a sequence of all its models in such a way that the time needed for
generating the first model and the time between generating any two consecutive models in
the sequence is polynomial in the length of the formula. This type of complexity bound is
called a polynomial delay. It should be clear that if we can enumerate models of a formula
with a polynomial delay, then we can construct an output polynomial algorithm for this
task as well. On the other hand, it can be much harder to get an enumeration algorithm
with polynomial delay than an output polynomial algorithm. For an overview of various
notions of enumeration complexity (see Johnson, Yannakakis, & Papadimitriou, 1988).
There are special classes of formulas for which polynomial delay enumeration algorithms
have been described, this includes 2-CNF formulas, Horn formulas, generalized satisfiability
problems and others (see e.g. Aceto, Monica, Ingolfsdottir, Montanari, & Sciavicco, 2013;
Creignou & Hebrard, 1997; Dechter & Itai, 1992; Kavvadias, Sideri, & Stavropoulos, 2000).
In this paper, we describe another class of formulas for which a polynomial delay enumeration algorithm based on backtrack-free search can be described. On the contrary to such
algorithms known for 2-CNF or Horn formulas, the splitting variable in each step cannot
be chosen arbitrarily, however, the existence of a suitable variable is guaranteed and it can
be efficiently identified.
In particular we consider the class of matched formulas introduced by Franco and
Van Gelder (2003). Given a CNF formula , we consider its incidence graph I() defined as follows. I() is a bipartite graph with one part consisting of clauses of  and the
other part containing the variables of . An edge {x, C} for a variable x and a clause C is
in I() if x or x appears in C. It was observed by Aharoni and Linial (1986) and Tovey
(1984) that if I() admits a matching (i.e. a set of pairwise disjoint edges) of size m (where
m is the number of clauses in ), then  is satisfiable. Later the formulas satisfying this
condition were called matched formulas by Franco and Van Gelder. Since a matching of
maximum size in a bipartite graph can be found in polynomial time (see e.g. Lovasz &
Plummer, 1986), one can check efficiently whether a given formula is matched.
Given a general formula , we can measure how far it is from being matched by considering its maximum deficiency  (), the number of clauses which remain unmatched in a
maximum matching of I(). A formula  is thus matched iff  () = 0. A weaker notion
of deficiency () = m  n, where m is the number of clauses and n the number of the
variables in , is also often being considered.
Matched formulas play a significant role in the theory of satisfiability solving. Since
their introduction matched formulas have been considered as a base class in parameterized
algorithms for satisfiability, see e.g. the book of Flum and Grohe (2006) for an overview of
parameterized algorithms theory. In particular, Fleischner, Kullmann, and Szeider (2002)
show that satisfiability of formulas with maximum deficiency bounded by a constant k can
be decided in time O(kknO(k) ) where kk is the length of the input formula  and n
380

fiGenerating Models of a Matched Formula

denotes the number of its variables. This result was later improved by Szeider (2003) to
an algorithm for satisfiability parameterized with maximum deficiency of a formula with
complexity O(2k n3 ). Parameterization based on backdoor sets with respect to matched
formulas were considered by Szeider (2007).
Since all matched formulas are trivially satisfiable, we ask a stronger question: How
hard is it to count or enumerate the models of a matched formula? We prove that counting
the models of a matched formula is a #P-complete problem, and turn our attention to
generating models of a matched formula. The main result of the paper is an algorithm
which generates models of a matched formula with a polynomial delay. The algorithm
constructs a splitting tree whose nodes correspond to either a matched or an unsatisfiable
formula. However, in some cases this strategy is not sufficient since some nodes of the tree
cannot be split in this way. We prove that such a node corresponds to a formula which
can be satisfied by iterated elimination of pure literals. Formulas with this property will
be called pure literal satisfiable. These formulas were studied by Kullmann (2000) as a
subclass of linearly satisfiable formulas. If a node with a pure literal satisfiable formula is
reached, the algorithm switches to a simpler strategy. We prove that the models of a pure
literal satisfiable formula can be generated with a delay linear in the length of the formula.
On the other hand, the #SAT problem for pure literal satisfiable formulas is #P-complete,
because this problem is #P-complete for monotone 2CNFs (Valiant, 1979a, 1979b), which
are pure literal satisfiable.
Several generalizations of matched formulas have also been considered in the literature. Kullmann (2000) generalized matched formulas into the class of linearly satisfiable
formulas. Autarkies based on matchings were studied by Kullmann (2003). Szeider (2005)
considered another generalization of matched formulas, the classes of biclique satisfiable
and var-satisfiable formulas. Unfortunately, for both biclique satisfiable and var-satisfiable
formulas it is hard to check if a formula falls into one of these classes (Szeider, 2005).
We show in this paper that our result does not transfer to the class of linearly satisfiable
formulas by demonstrating that it is not possible to generate models of a linearly satisfiable
formula with a polynomial delay unless P=NP.
The paper is organized as follows. After giving basic definitions in Section 2, we describe
in Section 3 a specific simple splitting property of a class of formulas, which allows to
generate the models of a formula from the class efficiently. In Section 4, we consider pure
literal satisfiable formulas and prove that this class has the required splitting property. In
Section 5, we consider the matched formulas and prove the required splitting property of a
class of formulas, which generalizes both the matched and pure literal satisfiable formulas
in a natural way. This implies an algorithm for generating all models of a matched formula
or a formula from the more general class with a polynomial delay. In Section 6, we present
complexity bounds for efficient versions of the algorithms from the previous sections. In
Section 7, we show the negative result concerning linearly satisfiable formulas. Section 8
contains concluding remarks and some directions for further research.

2. Definitions
In this section, we give the necessary definitions and summarize the results we use in this
paper.
381

fiSavicky & Kucera

2.1 Boolean Functions
A Boolean function of n variables is a mapping f : {0, 1}n  {0, 1}. A literal is either
a variable, called positive literal, or its negation, called negative literal. The negation of
the variable x will be denoted x or x. A clause is a disjunction of a set of literals, which
contains at most one literal for each variable. Formula  is in conjunctive normal form
(CNF) or, equivalently,  is a CNF formula, if it is a conjuction of clauses. We often treat
a clause as a set of its literals and a CNF formula as a set of its clauses. It is a well known
fact that every Boolean function can be represented by a CNF formula (see e.g. Genesereth
& Nilsson, 1987). The size of a formula  is the number of the clauses in  and will be
denoted as ||. The length of a formula  is the total number of occurrences of literals in ,
i.e. the sum of the sizes of the clauses in , and will be denoted as kk. Given a variable x
and a value a  {0, 1}, [x = a] denotes a formula originating from  by substituting x with
value a and the obvious simplifications consisting in removing falsified literals and satisfied
clauses. We extend this notation to negative literals as well by setting [x = a] = [x = a].
The formula obtained from  by assigning the values a1 , . . . , ak  {0, 1} to the variables
x1 , . . . , xk is denoted as [x1 = a1 , x2 = a2 , . . . , xk = ak ]. We say that a literal l is pure in
a CNF formula, if it occurs in the formula and the negated literal l does not. A literal
is irrelevant in a formula, if neither the literal nor its negation occurs in the formula. A
variable is pure, if it appears only positively, or only negatively in , i.e. it appears in a
literal, which is pure in .
Let  be a formula defining a Boolean function f on n variables. An assignment of
values v  {0, 1}n is a model of  (also a satisfying assignment, or a true point of ), if
it satisfies f , i.e. if f (v) = 1. The set of models of  is denoted as T (). The models in
T () are defined on the variables which have an occurrence in . The set of the variables
of the function defined by a formula can be larger, however, we do not introduce a special
notation for this more general case. For algorithmic purposes, this is also not necessary,
since adding an irrelevant variable to a formula changes the set of the models by adding
this variable with both possible values to each element of the original set of models.
A partial assignment assigns values only to a subset of the variables. For a formula
of the variables x1 , . . . , xn , it can be represented as a ternary vector v  {0, 1, }n , where
vi =  denotes the fact that xi is not assigned a value by v.
Note that an empty clause does not admit a satisfying assignment and an empty CNF
is satisfied by any assignment.
2.2 Matched Formulas
In this paper we use standard graph terminology, (see e.g. Bollobas, 1998). Given an
undirected graph G = (V, E), a subset of edges M  E is a matching in G if the edges
in M are pairwise disjoint. A bipartite graph G = (A, B, E) is an undirected graph with
disjoint sets of vertices A and B, and the set of edges E satisfying E  A  B. For a set
W of vertices of G, let (W ) denote the neighborhood of W in G, i.e. the set of all vertices
adjacent to some element of W . We shall use the following well-known result on matchings
in bipartite graphs:
Theorem 2.1 (Halls Theorem). Let G = (A, B, E) be a bipartite graph. A matching M
of size |M | = |A| exists if and only if for every subset S of A we have that |S|  |(S)|.
382

fiGenerating Models of a Matched Formula

Let  = C1  . . .  Cm be a CNF formula on n variables X = {x1 , . . . , xn }. We associate
a bipartite graph I() = (, X, E) with  (also called the incidence graph of ), where
the vertices correspond to clauses in  and the variables X. A clause Ci is connected to a
variable xj (i.e. {Ci , xj }  E) if Ci contains xj or xj . A CNF formula  is matched if I()
has a matching of size m, i.e. if there is a matching which pairs each clause with a unique
variable, we shall call such matching as clause saturated matching. Note that a matched
CNF is trivially satisfiable, since each clause can be satisfied by the literal containing the
variable matched to the given clause. A variable, which is matched to some clause in a
given matching M , is called matched in M , it is free in M otherwise.
2.3 Generating Models With a Polynomial Delay
The main goal of this paper is to describe an algorithm which, given a matched formula ,
generates the set T () of models of  with a polynomial delay. Let us state more formally
what we require of such an algorithm.
We say that an algorithm generates the models of a Boolean formula  with a polynomial
delay, if there is a polynomial p, such that the algorithm, given a formula  as an input,
satisfies the following properties.
1. It works in steps, each of which takes time O(p(kk)).
2. In each step, it either finds a model of  different from the models obtained in the
previous steps (in particular, any model in the first step) or determines that that there
is no such model, so the previous steps already found all the models of .
If an algorithm with the properties above exists, it follows that we can construct the
set T () of all models in time O((|T ()| + 1)  p(kk)), which means that the algorithm is
output polynomial. Note that since T () may be of exponential size with respect to kk,
efficiency with respect to the size of the input and output is the best we can hope for when
constructing T ().

3. Efficient Splitting Tree Algorithm
The idea of the algorithm is to construct a decision tree for the function represented by a
given satisfiable CNF, such that every subtree larger than a single leaf contains a 1-leaf.
The depth of the tree is at most the number of the variables. If this tree is searched in
a DFS order, then the time needed in an arbitrary moment to reach a 1-leaf is at most n
times the time needed to split a node. In the following, we show that for some classes of
formulas including the matched formulas it is possible to find a splitting procedure which
yields a tree as described above.
A decision tree for a Boolean function f is a labeled binary tree, where each inner node is
labeled with a variable, while leaves and edges have labels 0 or 1. A decision tree computes
f (x) for a given assignment x by a process which starts at the root and in each visited
node follows the edge labeled by the value of the variable, which is the label of the node.
The output is the label of the leaf reached by this process. If a computation path tests a
variable, which was tested in the previous part of the path, then this test is redundant. We
consider only trees without such redundant tests.
383

fiSavicky & Kucera

A decision tree representing the same function as a given CNF formula  can be constructed top down as follows. The root of the tree is assigned to . For each non-leaf node
of the tree assigned to a formula , we choose an arbitrary split variable x which has an
occurrence in  and assign the restricted formulas [x = 0] and [x = 1] to the successors.
A node assigned to an empty formula becomes a 1-leaf and a node assigned to a formula,
which contains an empty clause, becomes a 0-leaf. The resulting decision tree represents
the function given by , although it can be too large for practical purposes. Each path from
the root to an inner node u of the tree corresponds to a partial assignment which changes
 to a formula representing the function computed by the subtree whose root is u. The
depth of a tree for a function of n variables is at most n.
Each leaf node labeled with 1 represents a set of models of , more precisely, a leaf
in depth d represents 2nd models of . Moreover, different leaves of the tree represent
disjoint sets of models. Given a decision tree for the function represented by , we can, by
traversing it, generate all models of  in time proportional to its size. This process leads to
a large delay between generating successive models, if the tree contains large subtrees with
only 0-leaves. The following condition on a class of formulas describes a situation when this
can be avoided.
Definition 3.1. Let U be a class of formulas, let   U and let x be a variable with
an occurrence in . We say that x is a splitting variable for  relative to U , if for every
a  {0, 1}, such that [x = a] is satisfiable, we have [x = a]  U .
A class of formulas U has the splitting property, if every formula in U containing a
variable contains a splitting variable relative to U .
We shall associate a splitting problem with a class of formulas U having splitting property.
Definition 3.2. Let U be a class of formulas with splitting property. The splitting problem
relative to U is the following problem: Given a formula   U , find a splitting variable for
 relative to U and the results of satisfiability tests for the formulas [x = 0] and [x = 1].
Note that the complexity of the splitting problem relative to U is also an upper bound on
the time of a satisfiability test for formulas in U . This is because a formula  is satisfiable,
if and only if for any variable x we have that at least one of the formulas [x = 0] and
[x = 1] is satisfiable. The result of these satisfiability checks for a splitting variable x is a
required part of solution to the splitting problem.
Theorem 3.3. If a class of formulas U has the splitting property and the splitting problem
relative to U can be solved in time c(), where c()  kk for each formula   U , then
the models of a formula   U with n variables can be generated with a delay O(n  c()).
Proof. Construct a tree for  in a DFS order using a splitting variable for every formula
assigned to a non-leaf node. If a non-leaf node is labeled by  and x is the splitting
variable, the successors are labeled by [x = 0] and [x = 1]. If some of these formulas
is unsatisfiable, the corresponding successor becomes a 0 leaf. If some of these formulas is
empty, the corresponding successor becomes a 1 leaf. The root of the tree is split even if
 is unsatisfiable, however, other nodes labeled by an unsatisfiable formula are not split.
384

fiGenerating Models of a Matched Formula

Hence, except possibly of the root, there is no other node with two 0-leaves as successors.
Since the length of every formula in the tree is at most kk, in each node, time O(c())
is sufficient to choose a splitting variable, determine which of the successors is a leaf, and
construct the formulas for the successors of the node.
Let us assume that u is a non-leaf node of the constructed tree different from the root.
One of the successors of u can be labeled by an unsatisfiable formula. This is recognized
by the splitting algorithm and this successor is a 0-leaf. Consequently, in time at most
O(c()), the construction of the tree continues at a satisfiable successor of u. Hence, in at
most n splitting steps and time at most O(n  c()), a 1-leaf is reached. 2
Remark 3.4. If  contains a unit clause and U is closed under unit propagation, then a
variable x contained in a unit clause is a splitting variable which can be identified efficiently.
The reason is that if  is known to be satisfiable, then one of the formulas [x = a] contains
an empty clause and, hence, the other is satisfiable.
Remark 3.5. If a class U satisfies that
1. the satisfiability of formulas in U can be tested in polynomial time, and
2. U is closed under partial assignments,
then the splitting problem relative to U has polynomial complexity. Indeed, in this case
any variable in a formula  from U is a splitting variable and the satisfiability tests for the
corresponding restrictions can be obtained in polynomial time. Class U with this property
is sometimes also conservative. We can also say that this property is a particular form
of self-reducibility (in a sense considered e.g. by Khuller & Vazirani, 1991). All classes
of generalized satisfiability problem described by Creignou and Hebrard (1997) have this
property in addition to other classes, consider, for instance, Horn formulas, SLUR formulas,
2CNFs, q-Horn formulas, etc. As an immediate corollary of Theorem 3.3, it is possible to
generate the models of formulas in these classes with a polynomial delay.
The main result of this paper is that the splitting problem relative to a slight generalization of matched formulas also has polynomial complexity although the class of matched
formulas is not closed under partial assignments.

4. Pure Literal Satisfiable Formulas
Before considering matched formulas, let us make a small detour to the class of formulas
which are satisfiable by iterated elimination of pure literals, which we call pure literal
satisfiable. These formulas have already been considered by Kullmann (2000) as a special
case of linearly satisfiable formulas.
A set of literals is called consistent, if it does not contain contradictory literals. If l
is a literal, let assign(l) be the assignment to the variable contained in l, which satisfies
l. For a consistent set or sequence of literals L, let assign(L) be the partial assignment
of the variables satisfying the literals in L. For a formula , [L] is an abbreviation for
[assign(L)].
385

fiSavicky & Kucera

Definition 4.1. A pure literal sequence for a formula  is a consistent sequence of literals
(l1 , . . . , lk ), such that for every i = 1, . . . , k, the literal li is either pure or irrelevant in the
formula [l1 , . . . , li1 ]. In particular, l1 is pure or irrelevant in . A pure literal sequence is
called strict, if each of the literals li is pure in [l1 , . . . , li1 ].
If L is a pure literal sequence for , the formula [L] will be called the reduced formula
corresponding to  and L. If [L] does not contain a pure literal, L will be called a maximal
pure literal sequence for .
Definition 4.2. A formula  is pure literal satisfiable, if there is a pure literal sequence L
for , such that the reduced formula [L] is empty or, equivalently, assign(L) is a satisfying
assignment of .
An autarky for a formula  is a partial assignment v of the variables, such that every
clause is either satisfied or unchanged by v. Autarkies were studied e.g. by Kullmann
(2000). Note that every initial segment of a pure literal sequence defines an assignment
to the variables, which is an autarky. Moreover, one can easily verify that this property
characterizes pure literal sequences.
Let us note that pure literal satisfiable formulas are not closed under partial assignments.
Consider a formula , which does not contain a pure literal. Let  be the formula obtained
from  by adding a new variable x as a positive literal to every clause. Formula  is
pure literal satisfiable, but [x = 0] =  is not pure literal satisfiable. It follows that
pure literal satisfiable formulas do not satisfy the second property required in Remark 3.5
and we have to put more effort into showing that pure literal satisfiable formulas have the
splitting property and that the splitting problem relative to pure literal satisfiable formulas
has polynomial complexity.
For every CNF formula, it may be tested in polynomial time, whether it is pure literal
satisfiable. In order to find a pure literal sequence witnessing this fact, the procedure
FindPLS in Algorithm 1 uses a greedy approach, which at each step chooses and satisfies
any pure literal in the current formula. This approach is meaningful, since if a literal
is pure at some stage of the procedure, it either remains pure or becomes irrelevant in
the following stages. The pure literal sequence obtained by the procedure depends on the
nondeterministic choices made by the procedure, however, by Corollary 4.4, the resulting
reduced formula is uniquely determined by the input.
Lemma 4.3. If a clause C of a CNF  is removed by some run of FindPLS, then it is
removed by every run of FindPLS with input .
Proof. Let L and K be pure literal sequences produced by different runs of FindPLS for
. The formulas [L] and [K] are the corresponding reduced formulas and let C be a
clause of  not contained in [L]. Hence, L contains some of the literals of C. Since [K]
is a subset of , L is a pure literal sequence for [K]. If some literal of L is contained in
[K], then the first of such literals is pure in [K]. Since [K] does not contain a pure
literal, no literal of L is contained in [K]. In particular, C is not contained in [K]. 2
The following is an immediate corollary.
386

fiGenerating Models of a Matched Formula

Algorithm 1 Constructing pure literal sequence
Require: A CNF formula .
Ensure: A maximal strict pure literal sequence L for  and the corresponding reduced
formula.
1: procedure FindPLS()
2:

3:
Initialize a new empty list of literals L.
4:
Initialize Pure() as a set of pure literals in .
5:
while Pure() 6=  do
6:
Choose a literal l from Pure().
7:
Add l to L.
8:
  [l].
9:
Update Pure() to consist of pure literals in .
10:
end while
11: end procedure

Corollary 4.4. Let  be a CNF formula and let L be a pure literal sequence obtained by
FindPLS for .
1. The formula [L] is uniquely determined by .
2. The formula  is pure literal satisfiable, if and only if [L] is empty.
Since the running time of procedure FindPLS is polynomial in the length of the input
formula, a maximal pure literal sequence for a formula can be constructed in polynomial
time. The complexity of constructing a maximal pure literal sequence for a formula  is, in
fact, O(kk) by Lemma 6.1.
Lemma 4.5. Let L = (l1 , . . . , ln ) be a pure literal sequence for a formula , which contains
a literal for each variable of . For i = 1, . . . , n, denote by xi the variable contained in li .
If xi is the variable with the largest index i among the variables, which have an occurence in
, then xi is a splitting variable for  relative to pure literal satisfiable formulas and each
of the formulas [xi = 0] and [xi = 1] is satisfiable, if and only if it does not contain an
empty clause.
Proof. Let  be one of the formulas [xi = 0] and [xi = 1] and let L = (l1 , . . . , li1 ).
Clearly, L is a pure literal sequence for . Moreover, if  does not contain an empty clause,
then L assigns a value to some of the literals in every clause of  and hence, satisfies it. 2
For now it is sufficient to show that the splitting problem relative to class of pure literal
satisfiable formulas has polynomial complexity. Later in Theorem 6.2 we shall show that
the splitting problem can in this case be solved in time O(kk).
Lemma 4.6. The splitting problem relative to class of pure literal satisfiable formulas has
polynomial complexity.
387

fiSavicky & Kucera

Proof. If  is pure literal satisfiable, then a pure literal sequence, which satisfies it, can be
obtained by FindPLS in polynomial time. If the sequence does not contain literals for all
variables, it is extended in polynomial time by appending arbitrary literals for the missing
variables to obtain a pure literal sequence satisfying the assumption of Lemma 4.5. Then,
this lemma implies a method to select a splitting variable and obtain the results of the
satisfiability test for the corresponding restrictions in polynomial time. 2
If a pure literal sequence satisfies the assumption of Lemma 4.5 for a formula , then
the same sequence can be used to find a splitting variable for all formulas in a splitting tree
for . Using this, the models of a pure literal satisfiable formula can be generated with a
delay smaller than the general bound from Theorem 3.3, see Corollary 6.2.
Remark 4.7. The sign of a literal for a given variable, which occurs in a strict pure literal
sequence, is not uniquely determined. Each of the variables y1 and y2 can occur both
positively and negatively in a strict pure literal sequence for the formula
(x1  y1 )  (x2  y1 )  (x3  y2 )  (x4  y2 )  (y1  y2 ) .
For example, (x2 , y1 , x3 , y2 ) and (x4 , y2 , x1 , y1 ) are strict pure literal sequences for this formula.

5. Matched Formulas
In this section we concentrate on matched formulas. Let us start with showing that the
problem of determining the number of models of a matched formula , i.e. the size |T ()|,
is as hard as a general #SAT problem.
Theorem 5.1. The problem of determining |T ()| given a matched formula  is #Pcomplete.
Proof. Let  = C1  C2  . . .  Cm be an arbitrary CNF formula on n variables. Let
y1 , . . . , ym be new variables not appearing in  and let D = (y1  y2  . . .  ym ) be a clause.
Let us define a CNF formula of n + m variables  equivalent to   D as
 = (C1  D)  (C2  D)  . . .  (Cm  D) .
Clearly,  is a matched formula and one can also observe that |T ()| = |T ()|  2n (2m  1).
We have thus reduced the problem of counting the models of a general CNF formula 
(i.e. the general #SAT problem) to the problem of counting the models of a matched CNF
formula  (i.e. the #SAT problem restricted to the matched formulas). 2
Our goal is to show that we can generate the models of a matched formula with a
polynomial delay. Theorem 3.3 cannot be used for this directly, since the class of the
matched formulas does not have the splitting property as can be seen from the following
example. Consider the formula
(x1  x2 )  (x1  x3 )  (x2  x3 ) .
388

fiGenerating Models of a Matched Formula

This formula is matched, but it has no splitting variable. Indeed, setting x1 to 0 leads to
a satisfiable, yet not matched formula (x2 )(x3 )(x2  x3 ) and by symmetry this is true for
variables x2 and x3 as well. In order to achieve our objective, we have to consider a richer
class of formulas. The class we consider generalizes matched and pure literal satisfiable
formulas as follows. Note that an empty formula is matched, since it corresponds to an
empty graph and we can formally assume that an empty graph possesses the required
matching.
Definition 5.2. A formula  is called pure literal matched, if the reduced formula obtained
by procedure FindPLS for  is matched.
Elimination of a pure literal preserves the property of being matched, since a pure literal
is an autarky. Hence, a matched formula is pure literal matched. Clearly, every pure literal
satisfiable formula is pure literal matched, since its reduced formula is empty and, hence,
matched.
The basic idea of an efficient splitting algorithm for the matched formulas is presented
in the following theorem. Later we shall show in Corollary 6.4 that the splitting problem
relative to pure literal matched formulas can be solved in time O(n2  kk).
Theorem 5.3. The class of pure literal matched formulas has the splitting property and
the splitting problem relative to pure literal matched formulas has polynomial complexity.
In order to prove Theorem 5.3, we have to show several statements concerning the
structure of a matched formula. If V is a set of variables, we say that a clause is limited to
V , if it contains only literals with variables from V .
Definition 5.4. Let V be a subset of the variables of a matched formula  and let C denote
the set of clauses in  which are limited to V . The set V will be called a critical block , if
|C| = |V |. Formally, if V is empty, it is also a critical block.
Note that if  is a matched formula, V is a subset of its variables, and C is the set
of the clauses in  limited to V , then by Halls theorem (Theorem 2.1 above) we have
|C|  (C)  |V |. Critical blocks are those achieving the equality. These blocks have the
following property.
Lemma 5.5. Let V be a critical block of a matched formula . Then, in every clause
saturated matching of I(), the variables from V are matched to clauses limited to V .
Proof. Let  be a matched formula with a fixed clause saturated matching between the
variables and the clauses of . If V is a critical block, then there are |V | clauses limited
to V and these clauses are matched to the variables in V . Since the variables matched to
these clauses are different, each of the variables in V is matched to one of these clauses. 2
Another useful property of the set of the critical blocks is as follows.
Lemma 5.6. The set of the critical blocks of a matched formula is closed under intersection.
Proof. Let  be a matched formula and let V1 , V2 be critical blocks. If the intersection
V1  V2 is empty, the conclusion of the lemma is satisfied. If there is a variable x  V1  V2 ,
389

fiSavicky & Kucera

then by Lemma 5.5, in every clause saturated matching of I(), this variable is matched to
a clause, which is limited to V1 and also to V2 . Hence, the number of the clauses, which are
limited to V1  V2 , is at least |V1  V2 |. Since  is matched, the number of these clauses is
equal to |V1  V2 | by Halls theorem. Hence, V1  V2 is a critical block as required. 2
If  is a formula and x is a variable contained in at least one critical block, then
Lemma 5.6 implies that there is a unique inclusion minimal critical block of  containing
x, which is equal to the intersection of all critical blocks of  containing x. If a matched
formula has the same number of clauses and variables, then every variable is contained in
a critical block, since the set of all the variables of the formula is a critical block.
Definition 5.7. If  is a matched formula with the same number of clauses and variables and x is one of its variables, then let Bx denote the inclusion minimal critical block
containing x.
The notation Bx does not specify the formula, since it will always be clear from the
context. Our aim is to show that if a formula  is matched, then either we can find a
splitting variable for  relative to matched formulas, or  is actually pure literal satisfiable.
In order to show this property which is at the basis of our algorithm, we shall first investigate
the structure of critical blocks with respect to matchings.
Lemma 5.8. Let  be a matched formula with the same number of clauses and variables.
Let l be a literal containing a variable x and let us assume that the formula [l] is not
matched. Then
1. the literal l is pure or irrelevant in the clauses of  limited to Bx ,
2. if a clause C of  contains l, then in every matching for , C is matched to a
variable y, such that Bx  By (where  denotes strict inclusion).
Proof. By symmetry, we shall consider only the case l = x. Hence, by the assumptions,
[x = 0] is not a matched formula.
1. The critical block Bx is a subset of every critical block containing x. Hence, in order
to prove the first part of the lemma, it is sufficient to show that there is at least one
critical block B containing x, such that x does not occur negatively in the clauses
limited to B. Let C be any set of the clauses for which the Halls condition for the
formula [x = 0] is not satisfied so we have |(C)| < |C|. Let V = (C) be the set of
the variables, which have an occurrence in some of the clauses of C, and let k = |V |.
There are at least k + 1 clauses in C. Since every clause of C is limited to V , there
are at least k + 1 clauses of [x = 0] limited to V . Each of these clauses is either a
clause of  or is obtained from a clause of  by removing the literal x. Consider the
set of the clauses of  limited to V  {x}. Since  is matched, the Halls condition
is satisfied for this set. Hence,  contains at most k + 1 clauses limited to V  {x}.
Setting x = 0 leads to at least k + 1 clauses limited to V . Hence,  contains precisely
k + 1 clauses limited to V  {x} and none of them contains the literal x. Hence,
V  {x} is a critical block with the required property and the proof of the first part
of the lemma is finished.
390

fiGenerating Models of a Matched Formula

2. Let us fix a clause saturated matching M of clauses to variables in I() and let D
be a clause in  which is matched to x. Since [x = 0] is not matched, it follows
that D contains the positive literal x, otherwise the same matching would work for
[x = 0] as well. Let C be a clause containing x and let y be a variable to which C
is matched in M . Since C is different from D, we have y 6= x. By the assumptions,
the set of all variables is a critical block for  and, hence, the critical block By is
well-defined. Since C is matched to y, we have that C is limited to By by Lemma 5.5.
This implies x  By , because x  C. Since By is a critical block containing x and
Bx is the inclusion minimal critical block containing x, Bx  By . By the first part of
the lemma, no clause limited to Bx contains x which implies that C is not limited
to Bx and thus Bx 6= By . Together we get that Bx  By .
2
The structure of the critical blocks will be used to show the following proposition needed
to prove Theorem 5.3.
Theorem 5.9. Let  be a matched formula. If for every variable x, which has an occurence
in , there is a  {0, 1}, such that [x = a] is not matched, then  is pure literal satisfiable.
Proof. Let  be a matched formula satisfying the assumptions and let us fix a clause
saturated matching M of I(). If there is a variable x in  which is not matched to a
clause, then assigning any value to x yields a matched formula. By assumption we can
therefore suppose that such variable does not exist in  and that each variable is matched
to a clause. In this case, the numbers of clauses and variables of  are equal and for each
variable x in , Bx is well-defined.
Let n be the number of the variables and the clauses of . For each i = 1, . . . , n, let li
be the literal containing the variable xi in the clause matched to this variable. For every
i = 1, . . . , n, the formula [li ] is matched and the formula [li ] is not matched. Consider
the strict partial order on the variables defined by
x < y  Bx  By

(1)

where  means a strict inclusion. By Lemma 5.8, the variables which are maximal in this
partial order are pure in . Let us consider a total ordering of the variables, which is
consistent with the strict partial order (1). Using an appropriate renaming of the variables,
we may assume that this ordering is x1 , . . . , xn , so for every i, j, if xi < xj , then i < j.
Let us verify that using this ordering, the sequence ln , ln1 , . . . , l1 is a satisfying pure literal
sequence for . Let us show by induction on i = n, . . . , 1 that xi is pure or irrelevant in
the formula [ln , . . . , li+1 ]. It is true for i = n by Lemma 5.8, because xn is maximal in the
order of variables induced by the inclusion of their critical blocks. Let us now fix some i and
consider the partial assignment assign(ln , . . . , li+1 ). By Lemma 5.8, each clause containing
li is matched to a variable xj satisfying xi < xj . Hence, these clauses are eliminated by
the considered partial assignment and the variable xi is pure or irrelevant in the formula
[ln , . . . , li+1 ]. 2
Proof of Theorem 5.3. Assume,  is a pure literal matched formula. Let L be a pure
literal sequence obtained by FindPLS procedure for  and let  = [L], which is, by the
391

fiSavicky & Kucera

assumption, a matched formula. Since L is maximal,  does not contain a pure literal. If 
is empty, then  is itself a pure literal satisfiable formula and we can find a splitting variable
for  by the method from Lemma 4.6. If  is not empty, then it is matched and not pure
literal satisfiable. Hence, by Theorem 5.9, there is a variable x of , such that [x = 0]
and [x = 1] are both matched. Since L does not contain a literal with the variable x, the
application of assign(L) and x = a commute for each a  {0, 1}. Hence, L is a pure literal
sequence for the formula [x = a] and the application of assign(L) to [x = a] leads to
[x = a], which is matched. Hence, for each a  {0, 1}, the formula [x = a] is pure literal
matched and the variable x is a splitting variable for the formula .
A time polynomial in the length of the formula is sufficient to select a splitting variable
x as in the proof above. If  is nonempty, the satisfiability of [x = 0] and [x = 1] is
guaranteed by the choice of x. If  is empty,  is pure literal satisfiable and the method from
Lemma 4.6 is used. Hence, a splitting variable and the results of the required satisfiability
tests can be obtained in polynomial time. 2
Similarly as the class of matched formulas, also the class of pure literal matched formulas
is closed under unit propagation. This implies that unit propagation can be used as part
of the construction of the splitting tree, in particular by Remark 3.4 we can always select a
variable in a unit clause as a splitting variable.
Proposition 5.10. The class of pure literal matched formulas is closed under unit propagation.
Proof. Assume,  is a pure literal matched formula containing a unit clause C = (l) where
l is a literal. Let us prove that [l] is a pure literal matched formula.
Let L be a pure literal sequence for . Observe that l cannot be contained in L, because
[l] is unsatisfiable. In the rest of the proof, we distinguish, whether l is contained in L
or not.
If l is contained in L, let L1 denote the sequence of literals in L before l and let L2 be the
sequence of literals in L after l. For simplicity, this can be written as L = (L1 , l, L2 ). Some
of the clauses of  are missing in [l] and some are changed by removing l. Since l is not
contained in L1 , the sequence L1 is a pure literal sequence for [l]. Since any assignments
to disjoint sets of variables commute, we have [L1 , l] = [l, L1 ] and, hence, the sequence
L2 is a pure literal sequence for both these formulas. Hence, the sequence L = (L1 , L2 ) is
a pure literal sequence for [l]. Since, moreover, [L1 , l, L2 ] = [l, L1 , L2 ], the application
of L to [l] leads to a matched formula. Consequently, [l] is pure literal matched.
Let us now consider the case when l is not contained in L. In this case [L] is a matched
formula which contains a unit clause C = (l), since this clause cannot be eliminated by
satisfying any of the literals in L. In every maximum matching of [L], clause C is matched
to l. Thus satisfying l gives a matched formula [L, l]. Since [L, l] = [l, L] and L is a
pure literal sequence for [l], this formula is pure literal matched. 2

6. Algorithms and Complexity
In this section, we prove specific complexity bounds for the algorithms presented in the
previous sections. The complexity bounds are derived for the RAM model with the unit cost
392

fiGenerating Models of a Matched Formula

measure and the word size O(log kk), where  is the input formula. The data structures
used in the algorithms are similar to those described by Minoux (1988) or Murakami and
Uno (2014). Let us first concentrate on the pure literal satisfiable formulas.
Lemma 6.1. A maximal pure literal sequence L for a CNF formula  can be constructed
in time O(kk).
Proof. We use the approach presented in the linear time algorithm for unit propagation
by Minoux (1988) to obtain an efficient version of procedure FindPLS in Algorithm 1. In
addition to the initializations in Algorithm 1, we initialize some auxiliary data structures.
These data structures are similar to those described by Murakami and Uno (2014). In
particular, the occurences of the literals in the formula are represented as nodes arranged as
a sparse matrix, whose rows correspond to literals and columns correspond to clauses. Each
node contains an identification of the clause and the literal, whose occurence it represents.
All the auxiliary data structures and their names are as follows:
 For each literal l we denote cl(l) the row of the matrix, which is a doubly-linked list
of nodes representing the occurences of l in .
 For each clause C   we denote lit(C) the column of the matrix, which is a doublylinked list of nodes corresponding to the occurences of literals in C.
 For each literal l we denote cnt(l) a counter, which contains the size of list lit(C) that
is the number of clauses in which l appears.
 We initialize the set Pure() as a queue which always contains pure literals in .
These are the literals l for which cnt(l) > 0 and cnt(l) = 0.
All these data structures can be initialized by traversing  in linear time. It is important
to note that each node represents an occurence of a literal l in a clause C. As such the
structure representing the node contains four pointers, two for doubly-linked list lit(C) and
two for double-linked list cl(l). Thus removing this node from any of these lists can be
performed in constant time.
In procedure FindPLS we repeat the following steps  find a pure literal l in , add l to
L and apply assign(l) to . Finding a pure literal amounts to dequeueing it from Pure().
When applying assign(l) we remove all clauses containing l (these are now satisfied) and
remove l from the remaining clauses. Let 1 consist of clauses in  which contain l and
let 0 consist of clauses in  which contain l. We claim that assign(l) can be applied to
 in time O(k1 k + |0 |).
1. Removing clauses in 1 means going through the list cl(l) and for each clause C in
this list and each literal l in lit(C) (including l), remove the corresponding node from
cl(l ) and make the list lit(C) inaccessible. This requires time O(1) for each literal l .
During this operation we also decrement the counters cnt(l ) of literals in lit(C) and
if any of their negated counterparts becomes pure, we add it to queue Pure().
2. Removing all occurrences of l means going through the list cl(l) and for each clause
C in this list, remove the corresponding node from cl(l) and from lit(C). This can
be done in time O(1) for each occurrence of l.
393

fiSavicky & Kucera

Repeating these steps for all literals which are included into L requires a constant number
of operations on each occurrence of a literal in the input formula  which implies the total
time O(kk). 2

Theorem 6.2. The splitting problem relative to pure literal satisfiable formulas can be
solved in time O(kk) where  is the input pure literal satisfiable formula. Moreover, the
set T () of the models of a pure literal satisfiable formula  can be generated with a delay
of O(kk).
Proof. Using the efficient version of FindPLS guaranteed by Lemma 6.1, all the operations
used in the proof of Lemma 4.6 can be done in time O(kk). This implies the first statement
of the theorem. The same procedure will be used as a preprocessing step for the algorithm
proving the second statement. In time O(kk), the preprocessing produces a pure literal
sequence L = (l1 , . . . , ln ), which contains a literal for each variable of . The auxiliary data
structures cl(l), lit(C) and cnt(l) used in the preprocessing will be used also later, so they
can be stored or reconstructed when needed.
By construction of L, the assumption of Lemma 4.5 is satisfied for  and L. If the method
from Lemma 4.5 is used to find a splitting variable for such a formula, then each of the
corresponding restrictions either contains an empty clause or also satisfies the assumption
of Lemma 4.5 with L. Hence, the sequence L can be used for selecting a splitting variable
in all nodes of a splitting tree for .
The DFS search is controlled by a stack of postponed nodes, which is initialized with
the root before the search starts. The search is split into a sequence of descending branches.
Each of the descending branches starts by removing a node from the stack and resuming
the search from this node. If a visited node has two satisfiable successors, DFS continues to
one of them and the other is put onto the stack. If a node has a single satisfiable successor,
then the stack is not modified. Each descending branch ends when a 1-leaf is found. For an
estimate of the delay, we estimate the total time needed to construct nodes in one of these
descending branches as follows.
The indices in L of the splitting variables chosen in a descending branch are monotonically decreasing. Hence, the total time needed to search for all the splitting variables in
one descending branch is O(n) and, hence, O(kk).
The time needed for manipulations with the formula in one descending branch is as
follows. When a node is removed from the stack, the auxiliary data structures cl(l), lit(C)
and cnt(l) are computed for the original formula  and then modified according to the
sequence of settings of the variables along the path from the root to the current node. This
can be done in time O(kk). Then, in each node of the descending branch, both assignments
of the chosen variable are computed and a satisfiable successor is selected. In one node, this
can be done in time O(k), where k is the number of the occurrences of the chosen variable in
. When a satisfiable successor is selected, the auxiliary structures are updated according
to it. The total time needed for these operations in one descending branch is O(kk) using
a similar argument as in the proof of Corollary 6.1.
By combining the above estimates, the total time for constructing a descending branch
and, hence, the delay between generating two consecutive models, is O(kk). 2
394

fiGenerating Models of a Matched Formula

Now let us concentrate on the time complexity of selecting a splitting variable of a pure
literal matched formula.
Lemma 6.3. The splitting problem relative to pure literal matched formulas can be solved
in time O(n  kk) where  is the input formula on n variables.
Proof. Following the proof of Theorem 5.3, we first find a pure literal sequence L for 
which can be done in time O(kk) by Lemma 6.1. If  = [L] is an empty formula, the
last variable in L is a splitting variable. Otherwise  is matched and we find a maximum

matching M for . This step can be performed in time O(kk  n) (see Hopcroft & Karp,
1973). Then, we search for a variable x in , such that both [x = 0] and [x = 1] are
matched. Such a variable exists by Theorem 5.9. If the number of the clauses of  is less
than n, any variable not used in the matching has this property. Otherwise, we check for
every variable, whether [x = a] is matched for a  {0, 1}. If the assignment x = a satisfies
the matched literal containing x, then [x = a] is matched. In the rest of the proof, we
estimate the complexity of each of the at most n checks for the assignments falsifying a
matched literal.
Partial assignment can be performed in time O(kk) = O(kk). During partial assignment the satisfied clauses are removed and the occurences of variable x are removed from
the remaining clauses. We modify matching M into a matching N for [x = a] accordingly, that is we remove pairs containing a satisfied clause and the pair containing x. If in
|N | = m (where m is the number of clauses in [x = a]), we are done. Otherwise we know
that |N | = m  1, since at most one pair containing a clause of [x = a], specifically, the
pair containing a literal on x, was removed from M when forming N . It remains to check
whether N is already a maximum matching or whether there is a better matching. This
can be tested by looking for a single augmentating path in I([x = a]) for matching N . An
augmentating path can be found using a breadth first search in time linear in the size of
the graph I([x = a]) (see e.g. Hopcroft & Karp, 1973; Lovasz & Plummer, 1986). Hence,
the test, whether [x = a] is matched can be done in time O(kk) = O(kk). 2
As a corollary of Lemma 6.3 and the general bound from Theorem 3.3, we get the
following.
Corollary 6.4. Models of a pure literal matched formula  on n variables can be generated
with a delay O(n2  kk).
Proof. By Lemma 6.3 we can find a splitting variable for a pure literal matched formula 
in time O(n  kk), in the same time we can determine the satisfiability of formulas [x = 0]
and [x = 1] as well. By Theorem 3.3 we thus get that the delay is O(n2  kk). 2

7. Linearly Satisfiable Formulas
In this section we consider the class of linearly satisfiable formulas. By results of Kullmann
(2000), this class generalizes both the matched formulas and the pure literal satisfiable
formulas and, by combining the proofs, also the class of pure literal matched formulas. In
this section, we show that it is not possible to generate models of linearly satisfiable formulas
with a polynomial delay unless P=NP.
395

fiSavicky & Kucera

As a consequence, the splitting problem relative to linearly satisfiable formulas does not
have polynomial complexity unless P=NP. This consequence follows also unconditionally
from Example 7.10, which presents a linearly satisfiable formula of 4 variables, which does
not have a splitting variable with respect to the class of linearly satisfiable formulas.
Let us recall the notation introduced by Kullmann, which is used below to present the
definition and basic facts concerning the linearly satisfiable formulas. If l is a literal, then
var(l) is the variable in this literal. If v is a partial assignment, then v(l) is the value of this
assignment on literal l.
Definition 7.1 (Kullmann, 2000). Let  be a CNF formula and let v be a non-empty
partial assignment of the variables of . We say that v is a simple linear autarky, if there
is an associated weight function w which assigns each variable x evaluated by v a positive
real number w(x) such that for all clauses C of  we have
X
X
w(var(l)) 
w(var(l)) .
(2)
lC,v(l)=1

lC,v(l)=0

Clearly, if any literal in C is falsified by v, then there must be a literal satisfied by v as
well. Therefore a simple linear autarky is an autarky. Kullmann showed that we can check
whether there is a simple linear autarky v for a CNF formula  and find one, if it exists,
by solving several linear programs.
If a literal l is pure in a formula, then the partial assignment v(l) = 1 with the weight
w(var(l)) = 1 is a simple linear autarky for the formula. As another example, consider any
satisfying assignment of a satisfiable 2-CNFs. Such an assignment with the same weight
for all variables forms a simple linear autarky. Similarly, pure Horn CNFs without unit
clauses are satisfiable by a simple linear autarky which assigns value 0 and equal weight to
all variables. On the other hand, if a pure Horn CNF formula contains a unit clause, it can
be satisfiable and have no simple linear autarky. An example is the formula
(x1 )  (x1  x2 )  (x1  x3 )  (x1  x2  x3 ) ,
which has no simple linear autarky by Theorem 7.8 and Lemma 7.9 below.
By considering iterative application of simple linear autarkies to a formula we can get
the class of linearly satisfiable formulas defined as follows.
Definition 7.2 (Kullmann, 2000). The class of linearly satisfiable formulas is defined as
the smallest class satisfying the following two properties:
1. An empty CNF is linearly satisfiable.
2. Let  be a CNF, such that there is a simple linear autarky v for  and [v] is linearly
satisfiable. Then so is .
In other words, a CNF formula  is linearly satisfiable if by subsequent applications
of linear autarkies we obtain an empty formula. A composition of simple linear autarkies
is called linear autarky by Kullmann (2000) and the class of linearly satisfiable formulas
therefore consists of formulas which are satisfiable by a linear autarky. Kullmann showed
that all matched formulas are linearly satisfiable. Since a pure literal is a simple linear
396

fiGenerating Models of a Matched Formula

autarky, any pure literal satisfiable formula is linearly satisfiable. Similarly, any pure literal
matched formula defined in Section 5 is linearly satisfiable by simple linear autarkies for
the pure literals concatenated with the linear autarky for the resulting matched formula.
While for matched and pure literal satisfiable formulas we have presented algorithms
which generate models of these formulas with polynomial delay, it is not possible to extend
this result to linearly satisfiable formulas unless P=NP. Let us first present a construction,
which is used in a reduction argument.
Let  be an arbitrary 3-CNF formula with variables x1 , . . . , xn and clauses c1 , . . . , cm .
Consider new variables y1 , y2 , y3 and let  be the formula consisting of the clauses
(y1  y2 ), (y2  y3 ), (y3  y1 ),
(cj  y1  y2  y3 ),
j = 1, . . . , m
(xi  y1 )
i = 1, . . . , n .
Recall that the number of the models of a formula is the number of the satisfying assignments
of the variables, which have an occurrence in it. Hence, in the next lemma, T () and T ()
are defined on different sets of the variables.
Lemma 7.3. Formula  is linearly satisfiable and the number of its models is |T ()| =
|T ()| + 1.

Proof. Each clause ci of  contains three literals. Hence, in each clause of , the number
of the positive literals is at least the number of the negative literals. It follows that the
assignment of all variables to 1 with equal weight for all variables defines a simple linear
autarky, which satisfies . Hence, this formula is linearly satisfiable.
Any model of  satisfies y1 = y2 = y3 . An assignment containing y1 = y2 = y3 = 1 is a
model of  if and only if xi = 1 for i = 1, . . . , n. An assignment containing y1 = y2 = y3 = 0
is a model of  if and only if the assignment of the variables xi is a model of . This implies
the second part of the statement of the lemma. 2
Since the formula  can be constructed for every 3-CNF formula , the lemma implies
the following immediate corollary.

Corollary 7.4. It is an NP-complete problem to determine, whether a general linearly
satisfiable formula has at least 2 models.
Note that this implies NP-hardness of #SAT problem restricted to the linearly satisfiable formulas. This problem is, in fact, also #P-complete, since it is #P-complete to
count models of monotone formulas, which are pure literal satisfiable and, hence, linearly
satisfiable.
In Example 7.10 below, we present a linearly satisfiable formula, which has no splitting
variable relative to the class of linearly satisfiable formulas. For analysis of this example, we
use a characterization of simple linear autarkies obtained using the clause-variable matrix.
Definition 7.5. Let  be a CNF formula with clauses c1 , . . . , cm and variables x1 , . . . , xn .
The clause-variable matrix of this formula is the matrix A = {aj,i } of the dimension m  n
defined as

 1 xi  cj
1 xi  cj
aj,i =

0 otherwise .
397

fiSavicky & Kucera

If u  Rm , then u  0 means uj  0 for all j = 1, . . . , m. Kullmann showed the following
proposition.
Lemma 7.6 (Kullmann, 2000). A formula  with the clause-variable matrix A has a simple
linear autarky, if and only if there is a nonzero z  Rn , such that Az  0. Moreover, a
linear autarky can be obtained from such a vector z using the assignment

 1 if zi > 0
v(xi ) =
0 if zi < 0

 if zi = 0
and the weight function w(xi ) = |zi |.
Let us present the well-known Farkas lemma in the form used in the proof of Theorem
7.8.
Theorem 7.7 (Farkas lemma). Let A be an m  n real matrix and b  Rn . Then, exactly
one of the following statements is true.
1. There is a vector y  Rm , such that y  0 and y t A = bt .
2. There is a vector z  Rn , such that Az  0 and bt z < 0.
A linear combination of real vectors with non-negative coefficients will be called, for
simplicity, a non-negative combination.
Theorem 7.8. Assume,  is a formula of n variables and m clauses and A is its clausevariable matrix of the dimension m  n. Then, exactly one of the following statements is
true:
(a)  has a linear autarky,
(b) every vector in Rn is a non-negative combination of the rows of A.
Proof. First, assume, both (a) and (b) are satisfied. Lemma 7.6 implies that there is a
non-zero z  Rn , such that Az  0. By (b), there is a non-negative vector y  Rm , such
that y t A = z t . Multiplying this by z from the right, we get
y t Az = z t z < 0 .
This is a contradiction, since both y and Az are non-negative.
Assume, (b) is not satisfied. Hence, there is a vector b  Rn , which is not a non-negative
combination of the rows of A. By Farkas lemma, there is a vector z  Rn , such that Az  0
and bt z < 0. Since the latter condition implies that z is non-zero, there is a simple linear
autarky for  by Lemma 7.6 which means that (a) is satisfied. 2
Lemma 7.9. Assume, A is a matrix of dimension m  n, such that rank(A) = n and there
is a vector u  Rm with all components positive, such that ut A = 0. Then, every vector in
Rn is a non-negative combination of the rows of A.
398

fiGenerating Models of a Matched Formula

Proof. By the assumption, the linear space generated by the rows of A is Rn . Hence, for
every z  Rn , there is v  Rm , such that v t A = z. For a sufficiently large real number s,
the vector v + su has all components non-negative and (v + su)t A = z. 2
Note that every linearly satisfiable CNF formula of at most 3 variables has a splitting
variable relative to the class of linearly satisfiable CNF formulas, since setting any variable
to a constant leads to a formula of at most 2 variables, which is at most quadratic and,
hence, is satisfiable if and only if it is linearly satisfiable.
Example 7.10. Denote E = {a  {0, 1}4 | 2  a1 + a2 + a3 + a4  3} and for every Boolean
variable x, let x1 = x and x0 = x. The formula
(x1 , x2 , x3 , x4 ) =

4
^ _

xai i

aE i=1

is linearly satisfiable, but has no splitting variable relative to the class of linearly satisfiable
formulas.
Proof. In every clause, the number of positive literals is at least the number of negative
literals. Hence, the formula  is linearly satisfiable by Lemma 7.6 with z = (1, 1, 1, 1).
Since  is invariant under any permutation of the variables, it is sufficient to prove that
x4 is not a splitting variable. Since every clause of  contains a negative literal, we have
(0, 0, 0, 0) = 1. It follows that the formula [x4 = 0] is satisfiable. One can verify that
[x4 = 0] =

3
^ _

xai i ,

aE  i=1

where E  = {a  {0, 1}3 | 1  a1 + a2 + a3  2}. In order to prove that [x4 = 0] is not
linearly satisfiable, consider its clause-variable matrix with the columns corresponding to
x1 , x2 , x3 , which is


1
1 1
 1 1
1 


 1

1
1


 1 1 1  .


 1
1 1 
1 1

1

This matrix has rank 3, since each of the vectors (2, 0, 0), (0, 2, 0), (0, 0, 2) is a sum of two
rows out of the first three. Moreover, the sum of all the rows of this matrix is the zero
vector. Hence, the formula [x4 = 0] does not have a linear autarky by Lemma 7.9 and
Theorem 7.8. 2

8. Conclusion and Directions for Further Research
In this paper, we have shown that it is possible to generate the models of a matched formula
 of n variables with delay O(n2  kk). As a byproduct we have shown that the models
399

fiSavicky & Kucera

of a pure literal satisfiable formula  (i.e. a formula satisfiable by iterated pure literal
elimination) can be generated with delay O(kk). We have also shown that this result
cannot be generalized for the class of linearly satisfiable formulas since it is not possible to
generate models of linearly satisfiable formulas with a polynomial delay unless P=NP.
Let us mention that the procedure for generating the models with a bounded delay can
be extended to formulas for which a small strong backdoor set with respect to the class
of matched formulas with empty clause detection can be found. Let us assume that B is
such a backdoor set for a formula , i.e. B is a set of variables satisfying that any partial
assignment to variables in B leads to a matched formula, or to a formula containing an
empty clause. Then we can generate the decision tree for  (and thus generate its models)
in time O(2|B| kk + T (f ) n2 kk). Unfortunately, searching for strong backdoor sets with
respect to the class of matched formulas is hard (Szeider, 2007).
The algorithms described in this paper for the cases of pure literal satisfiable and pure
literal matched formulas can be used in a general algorithm for model enumeration which
is based on splitting tree. This, in turn, is any DPLL based enumeration algorithm. To
this end, a similar approach to the one described by Stefan Szeider (2003) can be used.
Together with a formula  we would keep a maximum matching M of I(). This maximum
matching can then be maintained through the reduction and assignment steps performed
in the enumeration algorithm. Once the algorithm arrives at a matched formula, it can
select splitting variables in the way we have described in this paper which has guaranteed
polynomial delay.
An interesting question is whether our approach could be used with the parameterized
satisfiability algorithm based on maximum deficiency (see Szeider, 2003) in order to get a
parameterized algorithm for generating the models of a general formula.

Acknowledgments
Petr Savicky was supported by CE-ITI and GACR under the grant number GBP202/12/G061
and by the institutional research plan RVO:67985807. Petr Kucera was supported by the
Czech Science Foundation (grant GA15-15511S).

References
Aceto, L., Monica, D., Ingolfsdottir, A., Montanari, A., & Sciavicco, G. (2013). Logic for
Programming, Artificial Intelligence, and Reasoning: 19th International Conference,
LPAR-19, Stellenbosch, South Africa, December 14-19, 2013. Proceedings, chap. An
Algorithm for Enumerating Maximal Models of Horn Theories with an Application
to Modal Logics, pp. 117. Springer Berlin Heidelberg, Berlin, Heidelberg.
Aharoni, R., & Linial, N. (1986). Minimal non-two-colorable hypergraphs and minimal
unsatisfiable formulas. Journal of Combinatorial Theory, Series A, 43 (2), 196  204.
Bollobas, B. (1998). Modern Graph Theory, Vol. 184 of Graduate Texts in Mathematics.
Springer.
400

fiGenerating Models of a Matched Formula

Coquery, E., Jabbour, S., Sais, L., Salhi, Y., et al. (2012). A SAT-based approach for
discovering frequent, closed and maximal patterns in a sequence. In Proceedings of
ECAI.
Creignou, N., & Hebrard, J.-J. (1997). On generating all solutions of generalized satisfiability
problems. Informatique theorique et applications, 31 (6), 499511.
Creignou, N., Olive, F., & Schmidt, J. (2011). Theory and Applications of Satisfiability
Testing - SAT 2011: 14th International Conference, SAT 2011, Ann Arbor, MI, USA,
June 19-22, 2011. Proceedings, chap. Enumerating All Solutions of a Boolean CSP by
Non-decreasing Weight, pp. 120133. Springer Berlin Heidelberg, Berlin, Heidelberg.
Dechter, R., & Itai, A. (1992). Finding all solutions if you can find one. In AAAI-92
Workshop on Tractable Reasoning, pp. 3539.
Fleischner, H., Kullmann, O., & Szeider, S. (2002). Polynomial-time recognition of minimal unsatisfiable formulas with fixed clause-variable difference. Theoretical Computer
Science, 289 (1), 503  516.
Flum, J., & Grohe, M. (2006). Parameterized complexity theory (1st edition)., Vol. 3 of
Texts in Theoretical Computer Science. An EATCS Series. Springer-Verlag Berlin
Heidelberg.
Franco, J., & Van Gelder, A. (2003). A perspective on certain polynomial-time solvable
classes of satisfiability. Discrete Appl. Math., 125 (2-3), 177214.
Garey, M., & Johnson, D. (1979). Computers and Intractability: A Guide to the Theory of
NP-Completeness. W.H. Freeman and Company, San Francisco.
Genesereth, M., & Nilsson, N. (1987). Logical Foundations of Artificial Intelligence. Morgan
Kaufmann, Los Altos, CA.
Hopcroft, J. E., & Karp, R. M. (1973). An n5/2 algorithm for maximum matchings in
bipartite graphs. SIAM Journal on computing, 2 (4), 225231.
Jabbour, S., Lonlac, J., Sais, L., & Salhi, Y. (2014). Extending modern sat solvers for
models enumeration. In IEEE 15th International Conference on Information Reuse
and Integration (IRI), 2014, pp. 803810. IEEE.
Johnson, D. S., Yannakakis, M., & Papadimitriou, C. H. (1988). On generating all maximal
independent sets. Information Processing Letters, 27 (3), 119  123.
Kang, H.-J., & Park, I.-C. (2005). Sat-based unbounded symbolic model checking.
Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on,
24 (2), 129140.
Kavvadias, D. J., Sideri, M., & Stavropoulos, E. C. (2000). Generating all maximal models
of a Boolean expression. Information Processing Letters, 74 (34), 157162.
Khuller, S., & Vazirani, V. V. (1991). Planar graph coloring is not self-reducible, assuming
P 6= N P . Theoretical Computer Science, 88 (1), 183  189.
Kullmann, O. (2000). Investigations on autark assignments. Discrete Applied Mathematics,
107 (13), 99  137.
401

fiSavicky & Kucera

Kullmann, O. (2003). Lean clause-sets: generalizations of minimally unsatisfiable clausesets. Discrete Applied Mathematics, 130 (2), 209  249. The Renesse Issue on Satisfiability.
Lovasz, L., & Plummer, M. D. (1986). Matching Theory. North-Holland.
McMillan, K. L. (2002). Computer Aided Verification: 14th International Conference,
CAV 2002 Copenhagen, Denmark, July 2731, 2002 Proceedings, chap. Applying SAT
Methods in Unbounded Symbolic Model Checking, pp. 250264. Springer Berlin Heidelberg, Berlin, Heidelberg.
Minoux, M. (1988). LTUR: A simplified linear time unit resolution algorithm for Horn
formulae and computer implementation. Information Processing Letters, 29, 1  12.
Morgado, A., & Marques-Silva, J. (2005a). Algorithms for propositional model enumeration and counting. Tech. rep., Instituto de Engenharia de Sistemas e Computadores,
Investigacao e Desenvolvimento, Lisboa.
Morgado, A., & Marques-Silva, J. (2005b). Good learning and implicit model enumeration. In Tools with Artificial Intelligence, 2005. ICTAI 05. 17th IEEE International
Conference on, pp. 6 pp.136.
Murakami, K., & Uno, T. (2014). Efficient algorithms for dualizing large-scale hypergraphs.
Discrete Applied Mathematics, 170, 8394.
Sipser, M. (2006). Introduction to the Theory of Computation, Vol. 2. Thomson Course
Technology Boston.
Szeider, S. (2003). Minimal unsatisfiable formulas with bounded clause-variable difference
are fixed-parameter tractable. In Warnow, T., & Zhu, B. (Eds.), Computing and
Combinatorics, Vol. 2697 of Lecture Notes in Computer Science, pp. 548558. Springer
Berlin Heidelberg.
Szeider, S. (2005). Generalizations of matched CNF formulas. Annals of Mathematics and
Artificial Intelligence, 43 (1-4), 223238.
Szeider, S. (2007). Matched formulas and backdoor sets. In Marques-Silva, J., & Sakallah,
K. A. (Eds.), Theory and Applications of Satisfiability Testing  SAT 2007, Vol. 4501
of Lecture Notes in Computer Science, pp. 9499. Springer Berlin Heidelberg.
Tovey, C. A. (1984). A simplified NP-complete satisfiability problem. Discrete Applied
Mathematics, 8 (1), 85  89.
Valiant, L. (1979a). The complexity of computing the permanent. Theoretical Computer
Science, 8 (2), 189  201.
Valiant, L. (1979b). The complexity of enumeration and reliability problems. SIAM Journal
on Computing, 8 (3), 410421.

402

fiJournal of Artificial Intelligence Research 56 (2016) 247-268

Submitted 12/15; published 06/16

Association Discovery and Diagnosis of Alzheimers Disease
with Bayesian Multiview Learning
Zenglin Xu

zlxu@uestc.edu.cn

Big Data Research Center
School of Computer Science & Engineering
University of Electronic Science & Technology of China
Chengdu, Sichuan, 611731 China

Shandian Zhe

szhe@purdue.edu

Department of Computer Science, Purdue University
West Lafayette, IN 47906 USA

Yuan(Alan) Qi

alanqi@cs.purdue.edu

Department of Computer Science & Department of Statistics
Purdue University
West Lafayette, IN 47906 USA

Peng Yu

yu peng py@lilly.com

Eli Lilly and Company, Indianapolis, IN 46225, USA

Abstract
The analysis and diagnosis of Alzheimers disease (AD) can be based on genetic variations, e.g., single nucleotide polymorphisms (SNPs) and phenotypic traits, e.g., Magnetic
Resonance Imaging (MRI) features. We consider two important and related tasks: i) to
select genetic and phenotypical markers for AD diagnosis and ii) to identify associations
between genetic and phenotypical data. While previous studies treat these two tasks separately, they are tightly coupled because underlying associations between genetic variations
and phenotypical features contain the biological basis for a disease. Here we present a new
sparse Bayesian approach for joint association study and disease diagnosis. In this approach, common latent features are extracted from different data sources based on sparse
projection matrices and used to predict multiple disease severity levels; in return, the
disease status can guide the discovery of relationships between data sources. The sparse
projection matrices not only reveal interactions between data sources but also select groups
of biomarkers related to the disease. Moreover, to take advantage of the linkage disequilibrium (LD) measuring the non-random association of alleles, we incorporate a graph
Laplacian type of prior in the model. To learn the model from data, we develop an efficient
variational inference algorithm. Analysis on an imaging genetics dataset for the study of
Alzheimers Disease (AD) indicates that our model identifies biologically meaningful associations between genetic variations and MRI features, and achieves significantly higher
accuracy for predicting ordinal AD stages than the competing methods.

1. Introduction
Alzheimers disease (AD) is the most common neurodegenerative disorder (Khachaturian,
1985). In order to predict the onset and progression of AD, NIH funded the Alzheimers
Disease Neuroimaging Initiative (ADNI) to facilitate the evaluation of genetic variations,
e.g., Single Nucleotide Polymorphisms (SNPs) and phenotypical traits, e.g., Magnetic Resoc
2016
AI Access Foundation. All rights reserved.

fiXu, Zhe, Qi, & Yu

nance Imaging (MRI). In addition to progression study, it is becoming important in medical
studies to identify the relevant pathological genotypes and phenotypic traits, and to discover
their associations. Although found in many bioinformatics applications (Consoli, Lefevre,
Zivy, de Vienne, & Damerval, 2002; Hunter, 2012; Gandhi & Wood, 2010; Liu, Pearlson,
Windemuth, Ruano, Perrone-Bizzozero, & Calhoun, 2009), association studies are scarce
and especially in need in the AD study.
Many statistical approaches have been developed to discover associations or select features (or variables) for prediction in a high dimensional problem. For association studies, representative approaches are canonical correlation analysis (CCA) and its extensions (Harold, 1936; Bach & Jordan, 2005). These approaches have been widely used
in expression quantitative trait locus (eQTL) analysis (Parkhomenko, Tritchler, & Beyene,
2007; Daniela & Tibshirani, 2009; Chen, Liu, & Carbonell, 2012). For disease diagnosis based on high dimensional biomarkers, popular approaches include lasso (Tibshirani,
1994), elastic net (Zou & Hastie, 2005), and group lasso (Yuan & Lin, 2007), and Bayesian
automatic relevance determination (MacKay, 1991; Neal, 1996). Despite their wide success
in many applications, these approaches are limited by the following reasons:
 Most association studies neglect the supervision from the disease status. Because
many diseases, such as AD, are a direct result of genetic variations and often highly
correlated to clinical traits, the disease status provides useful yet currently unutilized
information for finding relationships between genetic variations and clinical traits.
 For disease diagnosis, most sparse approaches use classification models and do not
consider the order of disease severity. For subjects in AD studies, there is a natural
severity order from being normal to mild cognitive impairment (MCI) and then from
MCI to AD. Classification models cannot capture the order in ADs severity levels.
 Most previous methods are not designed to handle heterogeneous data types. The
SNPs values are discrete (and ordinal based on an additive genetic model), while the
imaging features are continuous. Popular CCA or lasso-type methods simply treat
both of them as continuous data and overlook the heterogeneous nature of the data.
 Most previous methods ignore or cannot utilize the valuable prior knowledge. For
example, the occurrence of some combinations of alleles or genetic markers in a population are more often or less often than that would be expected from a random
formation of haplotypes from alleles based on their frequencies, which is known as
Linkage Disequilibrium (LD) (Falconer & Mackay, 1996). To our knowledge, this
structure has not been utilized in association discovery.
To address these problems, we propose a new Bayesian approach that unifies multiview
learning with sparse ordinal regression for joint association study and disease diagnosis. It
can also conduct nonlinear classification over latent variables (Zhe, Xu, Qi, & Yu, 2014) and
find associations by incorporating the LD information as an additional prior for the SNPs
data (Zhe, Xu, Qi, & Yu, 2015) . In more detail, genetic variations and phenotypical traits
are generated from common latent features based on separate sparse projection matrices and
suitable link functions, and the common latent features are used to predict the disease status
(See Section 2). To enforce sparsity in projection matrices, we assign spike and slab priors
248

fiSimultaneous Association Discovery and Diagnosis

(George & McCulloch, 1997) over them; these priors have been shown to be more effective
than l1 penalty to learn sparse projection matrices (Goodfellow, Couville, & Bengio, 2012;
Mohamed et al., 2012). In order to take advantage of the linkage disequilibrium, which
describes the non-random association of alleles at different loci, we employ an additional
graph Laplacian type of prior for the SNPs view. The sparse projection matrices not only
reveal critical interactions between the different data sources but also identify biomarkers in
data relevant to disease status. Meanwhile, via its direct connection to the latent features,
the disease status influences the estimation of the projection matrices so that it can guide
the discovery of associations between heterogeneous data sources relevant to the disease.
To learn the model from data, we develop a variational inference approach (See Section
3). It iteratively minimizes the Kullback-Leibler divergence between a tractable approximation and exact Bayesian posterior distributions. We extend the proposed sparse multiview
learning model by incorporating the linkage disequilibrium information about SNPs in Section 4. We then employ our model to the real study of AD in Section 5. The results show
that our model achieves the highest prediction accuracy among all the competing methods. Furthermore, our model finds biologically meaningful predictive relationships between
SNPs, MRI features, and AD status.

2. Sparse Heterogeneous Multiview Learning Models
In this section, we first present the notations and assumptions, and then present the sparse
heterogeneous multiview learning model.
2.1 Notations and Assumptions
First, let us describe the data. We assume there are two heterogeneous data sources: one
contains continuous data  for example, MRI features  and the other contains discrete
ordinal data  for instance, SNPs. Note that we can easily generalize our model below to
handle more views and other data types by adopting suitable link functions (e.g., a Poisson
model for count data). Given data from n subjects, p continuous features and q discrete
features, we denote the continuous data by a p  n matrix X = [x1 , . . . , xn ], the discrete
ordinal data by a q  n matrix Z = [z1 , . . . , zn ] and the labels (i.e., the disease status) by a
n  1 vector y = [y1 , . . . , yn ]> . For the AD study, we let yi = 0, 1, and 2 if the i-th subject
is in the normal, MCI or AD condition, respectively.
2.2 Spare Heterogeneous Multiview Learning Model
To link the two data sources X and Z together, we introduce common latent features
U = [u1 , . . . , un ] and assume X and Z are generated from U by sparse projection. The
common latent feature assumption is sensible for association studies because both SNPs and
MRI features are biological measurements of the same subjects. Note that ui is the latent
feature for the i-th subject with dimension k. We denote the proposed Spare Heterogeneous
Multiview Learning
Q Model by SHML. In a Bayesian framework, we assign a Gaussian prior
over U, p(U) = i N (ui |0, I), and specify the rest of the model (see Figure 1) as follows.
249

fiXu, Zhe, Qi, & Yu

Sh

H

y

w

Sw

U

G

Sg



X

Z

Figure 1: The graphical representation of SHML, where X is the continuous view, Z is the ordinal
view, y are the labels.

2.2.1 Continuous Data Distribution
Given U, X is generated from
p(X|U, G, ) =

n
Y

N (xi |Gui ,  1 I)

i=1

where G = [g1 , g2 , ...gp ]> is a p  k projection matrix, I is an identity matrix, and  1 I
is the precision matrix of the Gaussian distribution. For the precision parameter , we
assign a conjugate prior Gamma prior, p(|r1 , r2 ) = Gamma(|r1 , r2 ) where r1 and r2 are
the hyperparameters and set to be 103 in our experiments.
2.2.2 Ordinal Data Distribution
For an ordinal variable z  {0, 1, . . . , R1}, its value is decided by which region an auxiliary
variable c falls in
 = b0 < b1 < . . . < bR = .
If c falls in [br , br+1 ), z is set to be r. For the AD study, the SNPs Z take values in {0, 1, 2}
and therefore R = 3. Given a q  k projection matrix H = [h1 , h2 , ...hq ]> , the auxiliary
variables C = {cij } and the ordinal data Z are generated from
p(Z, C|U, H) =

q Y
n
Y

p(cij |hi , uj )p(zij |cij )

i=1 j=1

where
p(cij |hi , uj ) = N (cij |h>
i uj , 1)
p(zij |cij ) =

2
X

(zij = r)(br  cij < br+1 ).

r=0

Here (a) = 1 if a is true and (a) = 0 otherwise.
250

fiSimultaneous Association Discovery and Diagnosis

2.2.3 Label Distribution
The disease status labels y are ordinal variables too. To generate y, we use the ordinal
regression model based the latent representation U,
p(y, f |U, w) = p(y|f )p(f |U, w),
where f is the latent continuous values corresponding to y, w is the weight vector for the
latent features and
p(fi |ui , w) = N (fi |u>
i w, 1),
p(yi |fi ) =

2
X

(yi = r)(br  fi < br+1 ).

r=0

Note that the labels y are linked to the data X and Z via the latent features U and the
projection matrices H and G. Due to the sparsity in H and G, only a few groups of
variables in X and Z are selected to predict y.
2.2.4 Sparse Priors for Projection Matrices and Weights Vector
Because we want to identify a few critical interactions between different data sources, we
use spike and slab prior (George & McCulloch, 1997) to sparsify the projection matrices G
and H. The spike and slab priors are continuous bimodal priors to model hypervariance parameters, which controls both the selection of the variable and the effective scale of choosing
this variable. We apply the spike and slab prior over the weight vector w. Specifically, we
use a p  k matrix Sg to represent the selection of elements in G: if sgij = 1, gij is selected
and follows a Gaussian prior distribution with variance 12 ; if sgij = 0, gij is not selected and
forced to almost zero (i.e., sampled from a Gaussian with a very small variance 22 ). We
have the following prior over G:
p(G|Sg , g ) =

p Y
k
Y

ij ij
p(gij |sij
g )p(sg |g )

i=1 j=1

where
ij
2
ij
2
p(gij |sij
g ) = sg N (gij |0, 1 ) + (1  sg )N (gij |0, 2 ),
ij
ij
p(sij
g |g ) = g

sij
g

ij

(1  gij )1sg ,

2
2
where gij in g is the probability of sij
g = 1, and 1  2 (in our experiment, we set
2
2
6
1 = 1 and 2 = 1o ). To reflect our uncertainty about g , we assign a Beta hyperprior
distribution:
p Y
k
Y
p(g |l1 , l2 ) =
Beta(gij |l1 , l2 ),
i=1 j=1

where l1 and l2 are hyperparameters. We set a diffuse and non-informative hyperprior, i.e.,
l1 = l2 = 1 in our experiments. Similarly, H is sampled from
p(H|Sh , h ) =

q Y
k
Y
i=1 j=1

251

ij ij
p(hij |sij
h )p(sh |h ),

fiXu, Zhe, Qi, & Yu

sij

ij

ij
ij
ij ij
ij h
2
2
where p(hij |sij
(1hij )1sh .
h ) = sh N (hij |0, 1 )+(1sh )N (hij |0, 2 ) and p(sh |h ) = h
ij
ij
Sh are binary selection variables and h in h is the probability of sh = 1. We assign Beta
hyperpriors for h :
q Y
k
Y
p(h |d1 , d2 ) =
Beta(hij |d1 , d2 ),
i=1 j=1

where d1 and d2 are hyperparameters. We set d1 = d2 = 1 in our experiments since we have
found that they are not sensitive to the final performance. Similarly for weights vector w,
p(w|sw ,  w ) =

k
Y

j
p(wj |sjw )p(sjw |w
)

j=1
sjw

j

j
j
j 1sw
where p(wj |sjw ) = sjw N (wj |0, 12 ) + (1  sjw )N (wj |0, 22 ) and p(sjw |w
) = w
(1  w
)
.
j
j
sw are binary selection variables and w in  w is the probability of sw = 1. We assign
Beta hyperpriors for  w :

p( w ) =

k
Y

i
Beta(w
|e1 , e2 ),

i=1

where e1 and e2 are hyperparameters. We similarly set e1 = e2 = 1 in our experiments.
2.2.5 Joint Distribution
Based on all these specifications, the joint distribution of our model is
p(X, Z, y, U, G, Sg , g , , C, H, H, Sh , h , Sw , w , f )
= p(X|U, G, )p(G|Sg )p(Sg |g )p(g |l1 , l2 )p(|r1 , r2 )
p(Z, C|U, H)p(H|Sh )p(Sh |h )p(h |d1 , d2 )
p(y|f )p(f |U, w)p(w|Sw )p(Sw |w )p(U).

(1)

Different from Figure 1, we put the conjugate prior for Sw and Sg into the joint distribution. Then the next step is to estimate the distributions of the latent variables and their
hyperparemeters.

3. Model Inference
Given the model specified in the previous section, now we present an efficient method to
estimate the latent features U, the projection matrices H and G, the selection indicators Sg
and Sh , the selection probabilities g and h , the variance , the auxiliary variables C for
generating ordinal data Z, the auxiliary variables f for generating the labels y, the weights
vector w for generating f and the corresponding selection indicators and probabilities sw and
 w . In a Bayesian framework, this estimation task amounts to computing their posterior
distributions.
However, computing the exact posteriors turns out to be infeasible since we cannot
calculate the normalization constant of the posteriors based on Equation (1). Thus, we
252

fiSimultaneous Association Discovery and Diagnosis

resort to a mean-field variational approach. Specifically, we approximate the posterior
distributions of U, H, G, Sg , Sh , g , h , , w, C and f by a factorized distribution
Q() = Q(U)Q(H)Q(G)Q(Sg )Q(Sh )Q(g )Q(h )Q()Q(w)Q(C)Q(f )

(2)

where  denotes all the latent variables.
Variational inference minimizes the Kullback-Leibler (KL) divergence between the approximate and the exact posteriors
min KL (Q()kp(|X, Z, y))

(3)

Q()

More specifically, using a coordinate descent algorithm, the variational approach updates
one approximate distribution, e.g, q(H), in Equation (2) at a time while having all the
others fixed. The detailed updates are given in the following paragraphs.
3.1 Updating Variational Distributions for Continuous Data
For the continuous data X, the approximate distributions of the projection matrix G, the
noise variance , the selection indicators Sg and the selection probabilities g are
Q(G) =

Q(Sg ) =

p
Y

N (gi ; i , i ),

i=1
p Y
k
Y

sij

(4)
ij

ijg (1  ij )1sg ,

(5)

Beta(gij |l1ij , l2ij ),

(6)

i=1 j=1

Q(g ) =

p Y
k
Y
i=1 j=1

Q() = Gamma(|r1 , r2 ).

(7)

The mean and covariance of gi are calculated as follows:
i = hihUU> i +

1
1
1
diag(hsig i) + 2 diag(1  hsig i) ,
2
1
2

i = i (hihUixi ),
where hi means expectation over a distribution, xi and sig are the transpose of the i-th
2 i is the j-th diagonal element in  . The
rows of X and Sg , hsig i = [i1 , . . . , ik ]> , and hgij
i
computation of parameters ij and Q(gij ) can be found in Appendices A.
3.2 Updating Variational Distributions for Ordinal Data
For the ordinal data Z, we update the approximate distributions of the projection matrix H,
the auxiliary variables C, the sparse selection indicators Sh and the selection probabilities
h . To make the variational distributions tractable, we update Q(H) in a column-wise
253

fiXu, Zhe, Qi, & Yu

way and re-denote H = [h1 , h2 , ...hk ], Sh = [s1h , s2h , ...skh ] and U = [u1 , u2 , ...uk ]> . The
variational distributions of C and H are
Q(C) =

q Y
k
Y

Q(cij ),

(8)

i=1 j=1

Q(cij )  (bzij  cij < bzij +1 )N (cij |cij , 1),
Q(H) =

k
Y

N (hi ;  i , i ),

(9)
(10)

i=1

1
where cij = (hHihUi)ij , i = hui > ui iI+ 12 diag(hsih i)+ 12 diag(h1sih i) ,  i = i Ci hui i
1
2
P
and Ci = C  j6=i  j huj i> . The computation of parameters in the distributions of Sh
and h is given in Appendices B.
3.3 Updating Variational Distributions for Labels
For the ordinal labels y, we update the approximation distributions of the auxiliary variables
f , the weights vector w, the sparse selection indicators sw and the selection probabilities
 w . The variational distributions of f and w are
Q(f ) =

n
Y

Q(fi ),

(11)

i=1

Q(fi )  (byi  fi < byi +1 )N (fi |fi , f2i ),

(12)

Q(w) = N (w; m, w ),

(13)

1
and m = w hUihf i.
where fi = (hUi> m)i , w = hUU> i+ 12 diag(sw )+ 12 diag(1sw )
1
2
The computation of parameters in the variational distributions of sw and w can be found
in Appendices C.
3.4 Updating Variational Distributions for Latent Representation U
The variational distribution for U is given by
Y
Q(U) =
N (ui |i , i )

(14)

i

where
1

(15)

i = i (hwihfi i + hihGi xi + hHi hci i).

(16)

i = hww> i + hihG> Gi + hH> Hi + I
>

>

The required moments are given in Appendices D.
3.5 Label Prediction
Let us denote the training data as Dtrain = {Xtrain , Ztrain , ytrain } and the test data as
Dtest = {Xtest , Ztest }. The prediction task needs the latent representation Utest for Dtest .
254

fiSimultaneous Association Discovery and Diagnosis

We carry out variational inference simultaneously on Dtrain and Dtest . After both Q(Utest )
and Q(Utrain ) are obtained, we predict the labels for test data as follows:
ftest = hUtest i> m,
i
ytest
=

R1
X

i
r  (br  ftest
< br+1 ),

(17)
(18)

r=0
i
where ytest
is the prediction for i-th test sample.

4. Sparse Heterogeneous Multiview Learning Model with Linkage
Disequilibrium Priors
In population genetics, lLinkage Disequilibrium (LD) refers to the non-random association
of alleles at different loci, i.e., the presence of statistical associations between alleles at
different loci that are different from what would be expected if alleles were independently,
randomly sampled based on their individual allele frequencies (Slatkin, 2008). If there is
no linkage disequilibrium between alleles at different loci they are said to be in linkage
equilibrium.
Linkage Disequilibrium also appears in the SNPs, which is a measure between pairs of
SNPs and can be regarded as a natural indicator for the correlation between SNPs. This
information can be publicly retrieved from www.ncbi.nlm.nih.gov/books/NBK44495/. To
incorporate such correlation as a prior in our model, we first introduce a latent q  k matrix
H, which is tightly linked to H as explained later. Each column hj of H is regularized by
the graph Laplacian of the LD structure, i.e.,
Y
p(H|L) =
N (hj |0, L1 )
j

=

Y

N (0|hj , L1 )

j

= p(0|H, L),
where L is the graph Laplacian matrix of the LD structure. As shown above, the prior
p(H|L) has the same form as p(0|H, L), which can be viewed as a generative model  in
other words, the observation 0 is sampled from H. This view enables us to combine the
generative model for graph Laplacian regularization with the sparse projection model via a
principled hybrid Bayesian framework (Lasserre et al., 2006).
To link the two models together, we introduce a prior over H:
Y
p(H|H) =
N (hj |hj , I)
j

where the variance  controls how similar H and H are in our model. For simplicity, we
set  = 0 so that p(H|H) = Dirac(H  H) where Dirac(a) = 1 if a = 1 and Dirac(a) = 0 if
a = 0.
Adopting this additional information, the new graphical model is designed as shown in
Fig. 2.
255

fiXu, Zhe, Qi, & Yu

L

Sh

y

w

Sw

hj

H

U

G

Sg

0

Z



X

Figure 2: The graphical representation of our model, where X is the continuous view, Z is the
ordinal view, y are the labels and L is the graph laplacian generated by the LD structure.

Based on all these specifications, the joint distribution of our model is
p(X, Z, y, U, G, Sg , g , , C, H, H, Sh , h , Sw , w , f )
= p(X|U, G, )p(G|Sg )p(Sg |g )p(g |l1 , l2 )p(|r1 , r2 )
 p(Z, C|U, H)p(H|Sh )p(Sh |h )p(h |d1 , d2 )p(H|H)
 p(0|H, L)p(y|f )p(f |U, w)p(w|Sw )p(Sw |w )p(U).

(19)

The inference is almost the same with the original model described in Section 2, except
the updating of the sparse projection matrix H. Given the ordinal data Z and the updates
of other variables, we update the approximate distributions of the projection matrix H, the
auxiliary variables C, the sparse selection indicators Sh and the selection probabilities h .
The variational distributions of C and H are
Q(C) =

q Y
k
Y

Q(cij ),

(20)

i=1 j=1

Q(cij )  (bzij  cij < bzij +1 )N (cij |cij , 1),
Q(H) =

k
Y

N (hi ;  i , i ),

(21)
(22)

i=1

1
where cij = (hHihUi)ij , i = hui > ui iI + L + 12 diag(hsih i) + 12 diag(h1  sih i) ,  i =
1
2
P
i Ci hui i and Ci = C  j6=i  j huj i> . The updating of other variables remains the same.

5. Experimental Results and Discussion
In order to examine the performance of the proposed method , we design a simulation study
and a realworld study for Alzheimers Disease.
256

fiSimultaneous Association Discovery and Diagnosis

5.1 Simulation Study
We first design a simulation study to examine the basic model, i.e., our model, in terms of
(i) estimation accuracy on finding associations between the two views and (ii) prediction
accuracy on the ordinal labels. Note that a similar study can be conducted on the model
with LD priors.
5.1.1 Simulation Data
To generate the ground truth, we set n = 200 (200 instances), p = q = 40, and k = 5. We
designed G, the 40  5 projection matrix for the continuous data X, to be a block diagonal
matrix; each column of G had 8 elements being ones and the rest of them were zeros,
ensuring each row with only one nonzero element. We designed H, the 40  5 projection
matrix for the ordinal data Z, to be a block diagonal matrix; each of the first four columns
of H had 10 elements being ones and the rest of them were zeros, and the fifth column
contained only zeros. We randomly generated the latent representations U  Rkn with
each column ui  N (0, I). To generate Z, we first sampled the auxiliary variables C with
each column ci  N (Hui , 1), and P
then decided the value of each element zij by the region
cij fell inin other words, zij = 2r=0 r(br  cij < br+1 ). Similarly, to generate y, we
sampled the auxiliary variables f from N (0, U> U + I) and then each yi was generated by
p(yi |fi ) = (yi = 0)(fi  0) + (yi = 1)(fi > 0).
5.1.2 Comparative Methods
We compared our model with several state-of-the-art methods including (1) CCA (Bach &
Jordan, 2005), which finds the projection direction that maximizes the correlation between
two views, (2) sparse CCA (Sun, Ji, & Ye, 2011; Daniela & Tibshirani, 2009), where
sparse priors are put on the CCA directions, and (3) multiple-response regression with lasso
(MRLasso) (Kim, Sohn, & Xing, 2009) where each column of the second view (Z) is regarded
as the output of the first view (X). We did not include results from the sparse probabilistic
projection approach (Archambeau & Bach, 2009) because it performed unstably in our
experiments. Regarding the software implementation, we used the built-in Matlab routine
for CCA and the code by (Sun et al., 2011) for sparse CCA. We implemented MRLasso
based on the Glmnet package (cran.r-project.org/web/packages/glmnet/index.html).
To test prediction accuracy, we compared the proposed SHML model based on the
Gaussian process prior with the following ordinal or multinomial regression methods: (1)
lasso for multinomial regression (Tibshirani, 1994), (2) elastic net for multinomial regression
(Zou & Hastie, 2005), (3) sparse ordinal regression with the spike and slab prior, (4) CCA
+ lasso, for which we first ran CCA to obtain the latent features H and then applied lasso
to predict y, (5) CCA + elastic net, for which we first ran CCA to obtain the projection
matrices and then applied elastic net on the projected data, (6) Gaussian Process Ordinal
Regression (GPOR) (Chu & Ghahramani, 2005), and (7) Laplacian Support Vector Machine
(LapSVM) (Melacci & Mikhail, 2011), a semi-supervised SVM classification method. We
used the published code for lasso, elastic net, GPOR and LapSVM. For all the methods,
we used 10-fold cross validation on the training data for each run to choose the kernel form
(Gaussian or linear or Polynomials) and its parameters (the kernel width or polynomial
orders) for our model, GPOR, and LapSVM.
257

fiXu, Zhe, Qi, & Yu

Because alternative methods cannot learn the dimension automatically for simple comparison, we provided the dimension of the latent representation to all the methods we tested
in our simulations. We partitioned the data into 10 subsets and used 9 of them for training
and 1 subset for testing; we repeated the procedure 10 times to generate the averaged test
results.
5.1.3 Results
To estimate linkage (i.e., interactions) between X and Z, we calculated the cross covariance
matrix GH> . We then computed the precision and the recall based on the ground truth.
The precision-recall curves are shown in Figure 3. Clearly, our method successfully recov1

SHML
0.9
0.8

Precision

0.7

Sparse CCA

0.6

MRLasso

0.5
0.4
0.3

CCA

0.2
0.1
0

0

0.2

0.4

0.6

0.8

1

Recall

Figure 3: The precision-recall curves for association discovery.

ered almost all the links and significantly outperformed all the competing methods. This
improvement may come from i) the use of the spike and slab priors, which not only remove
irrelevant elements in the projection matrices but also avoid over-penalizing the active association structures (the Laplace prior used in sparse CCA does over penalize the relevant
ones) and ii) more importantly, the supervision from the labels y, which is probably the
biggest difference between ours and the other methods for the association study. The failing
of CCA and sparse CCA may be due to the insufficient representation of all sources of data
caused by using only one projection direction. The prediction accuracies on unknown y and
their standard errors are shown in Figure 4a and the AUC and their standard errors are
shown in Figure 4b. Our proposed SHML model achieves significant improvement over all
the other methods. It reduces the prediction error of elastic net (which ranks the second
best) by 25%, and reduces the error of LapSVM by 48%.
5.2 Real-World Study on Alzheimers Disease
Alzheimers Disease is the most common form of dementia with about 30 million patients
worldwide and payments for care are estimated to be $200 billion in 2012 (Alzheimers
258

fiSimultaneous Association Discovery and Diagnosis

0.95

Area Under Curve

0.9

Precision

0.85

0.8

LapSVM
Lasso
ElasticNet
SparseOR
GPOR
CCA + Lasso
CCA + ElasticNet
SHML

0.9

0.85

0.75
0.8

(a) Precision on simulation

(b) AUC on simulation

Figure 4: The prediction results on simulated and real datasets. The results are averaged over 10
runs. The error bars represent standard errors.

Association, 2012). We conducted association analysis and diagnosis of AD based on a
dataset from Alzheimers Disease Neuroimaging Initiative(ADNI) 1 . The ADNI study is a
longitudinal multisite observational study of elderly individuals with normal cognition, mild
cognitive impairment, or AD. We applied the proposed method to study the associations
between genotypes and brain atrophy measured by MRI and to predict the subject status
(normal vs MCI vs AD). Note that the statuses are ordinal since they represent increasing
severity levels.
After removing missing values, the data set consists of 625 subjects including183 normal,
308 MCI and 134 AD cases, and each subject contains 924 SNPs and 328 MRI features.
The selected SNPs are those top SNPs separating normal subjects from AD in ADNI.
The MRI features measure the brain atrophies in different brain regions based on cortical
thickness, surface areas or volumes, which are obtained from FreeSurfer software 2 . To test
the diagnosis accuracy, we compared our method with the previously mentioned ordinal or
multinomial regression methods. We employ the extended model with linkage disequilibrium
priors, denoted as SHML-LD, to discover the associations.
We compare both SHML and SHML-LD with the state-of-the-art classification methods.
And we used the 10-fold cross validation for each run to tune free parameters on the training
data. To determine the dimension k for the latent features U in our method, we computed
the variational lower bounds as an approximation to the model marginal likelihood (i.e.,
evidence), with various k values {10, 20, 40, 60}. We chose the value with the largest approximate evidence, which led to k = 20 (see Figure 5). Our experiments confirmed that
with k = 20, our model achieved the highest prediction accuracy, demonstrating the benefit
of evidence maximization.
As shown in Figure 6, our method achieved the highest prediction accuracy, higher than
that of the second best method, GP ordinal Regression, by 10% and than that of the worst
method, CCA+lasso, by 22%. The two-sample t test shows our model outperforms the
alternative methods significantly (p < 0.05).
1. http://adni.loni.ucla.edu/
2. http://surfer.nmr.mgh.harvard.edu

259

fiXu, Zhe, Qi, & Yu

5

x 10

Evidence Lower Bound

7.4

7.42

7.44

7.46

7.48

10

20
40
Dimensions

60

Figure 5: The variational lower bound for the model marginal likelihood.

0.64

Precision

0.6

0.55

0.5

D

L
M

LL

M
SH

C

C

A

C

+

SH

et
N
ic

El

as
t

La
s

so

R
+
C
A

R

PO
G

se
O

ic

N
et

Sp
ar

so

as
t

La
s

El

La
pS

VM

0.46

Figure 6: The prediction accuracy with standard errors on the real data.
We also examined the strongest associations discovered by our model. Firstly, the ranking of MRI features in terms of prediction power for the three different disease populations
(normal, MCI and AD) demonstrate that most of the top ranked features are based on
the cortical thickness measurement. On the other hand, the features based on volume and
260

fiSimultaneous Association Discovery and Diagnosis

CT std of R. CaudalAnteriorCingulate
CT std of L. SuperiorParietal
CT std of R. Postcentral
CT std of R. SuperiorParietal
CT std of L. Precentral
Vol (WMP) of CorpusCallosumMidPosterior
Vol (WMP) of CorpusCallosumCentral
Vol (WMP) of CorpusCallosumPosterior
Vol (WMP) of CorpusCallosumMidAnterior
Vol (WMP) of L. CerebellumWM
Vol (WMP) of R. CerebellumWM
Vol (WMP) of FifthVentricle
Vol (WMP) of NonWMHypoIntensities
Surf Area of L. Unknown
Vol (WMP) of ThirdVentricle
Vol (WMP) of R. LateralVentricle
Vol (WMP) of L. LateralVentricle
Vol (WMP) of R. Caudate

2

1

0

1

CAP
CAPZB(rs7
CA ZB(rs 04415
CAPPZB(rs43692 0)
52
8
Z
CAP B(rs1605023 )
2
CAPZB(rs1936880)
CAPZB(rs732472 )
7
BCAZB(rs914550 )
9
BCAR3(rs188785 )
9)
5
R
5
MA
P3K3(rs38 3833)
NCO 1(rs1 5803
NCO A2(r 1318 8)
s80
77)
A
NC 2(rs12 14818
TRAOA2(rs588339)
TRA F3(rs 76130 )
7
8
F
TR 3(rs1252178 )
TRAAF3(rs 896382)
F3( 2533 1)
rs13
0
260 59)
060
)

2

(a)

CAP
Z
CAP B(rs70
44
Z
CAP B(rs80 150)
5
CAP ZB(rs4 0232)
369
ZB
CAP (rs169 252)
368
ZB(
80)
rs7
CAP
ZB( 14550
rs13
9)
CAP
24
Z
TRA B(rs98 727)
878
F3(
TRA rs1326 59)
0
F
TRA 3(rs25 060)
330
F3(
TRA rs1289 59)
6
F
BCA 3(rs75 381)
21
R
BCA 3(rs15 782)
5
MA R3(rs3 3833)
P3K
858
NCO 1(rs11 038)
3
NCO A2(rs8 1877)
014
A2(
8
r
NCO s1258 18)
83
A2(
rs76 39)
130
8)

Vol (WMP) of R. Hippocampus
Vol (WMP) of L. Hippocampus
Vol (CP) of R. Parahippocampal
CT std of L. Unknown
CT std of R. Unknown
Vol (CP) of L. Entorhinal
Vol (CP) of R. Entorhinal
CT Avg of R. Entorhinal
CT Avg of L. Entorhinal
Vol (CP) of L. Parahippocampal
CT Avg of R. Parahippocampal
CT Avg of L. Parahippocampal
CT Avg of R. Unknown
CT Avg of L. Unknown
Vol (WMP) of L. Amygdala
Vol (CP) of R. Unknown
Vol (WMP) of R. Amygdala
Vol (CP) of L. Unknown

(b)

Figure 7: The estimated associations between MRI features and SNPs. In each sub-figure,
the MRI features are listed on the right and the SNP names are given at the
bottom.

surface area estimation are less predictive. Particularly, thickness measurements of middle
temporal lobe, precuneus, and fusiform were found to be most predictive compared with
other brain regions. These findings are consistent with the memory-related function in
these regions and findings in the literature for their prediction power of AD. We also found
that measurements of the same structure on the left and right sides have similar weights,
indicating that the algorithm can automatically select correlated features in groups, since
no asymmetrical relationship has been found for the brain regions involved in AD.
Secondly, the analysis of associating genotype to AD prediction also generated interesting results. Similar to the MRI features, SNPs that are in the vicinity of each other are often
selected together, indicating the group selection characteristics of the algorithm. For example, the top ranked SNPs are associated with a few genes including CAPZB (F-actin-capping
261

fiXu, Zhe, Qi, & Yu

protein subunit beta), NCOA2 (The nuclear receptor coactivator 2) and BCAR3(Breast
cancer anti-estrogen resistance protein 3).
At last, biclustering of the gene-MRI associations, as shown in Figure 7, reveals interesting patterns in terms of the relationship between genetic variations and brain atrophy
measured by structural MRI. For example, the top ranked SNPs are associated with a few
genes including BCAR3 (Breast cancer anti-estrogen resistance protein 3) and NCOA2, and
MAP3K1 (mitogen-activated protein kinase kinase kinase 1) which have been studied more
carefully in cancer research. The set of SNPs are associated with cingulate in negative
directions, which is part of the limbic system and involves in emotion formation and processing. Compared with other structures such as temporal lobe, it plays a more important
role in the formation of long-term memory. For example, the association between MAP3K1
and the caudate anterior cingulate cortex has been identified. Literature has shown that
MAP3K1 is associated with biological processes such as apoptosis, cell cycle, chromatin
binding and DNA binding3 , and cingulate cortex has been shown to be severely affected
by AD (Jones et al., 2006). The strong association discovered in this work might indicate
potential genetic effects in the atrophy pattern observed in this cingulate subregion.

6. Related Work
The proposed our model model is related to a broad family of probabilistic latent variable
models, including probabilistic principle component analysis (Tipping & Bishop, 1999),
probabilistic canonical correlation analysis (Bach & Jordan, 2005) and their extensions (Yu,
Yu, Tresp, Kriegel, & Wu, 2006; Archambeau & Bach, 2009; Guan & Dy, 2009; Virtanen,
Klami, & Kaski, 2011). They all learn a latent representation whose projection leads to the
observed data. Recent studies on probabilistic factor analysis methods put more focus on
the sparsity-inducing priors to the projection matrix. Among them, Guan and Dy (2009)
used the Laplace prior, the Jeffreys prior, and the inverse-Gaussian prior; Archambeau and
Bach (2009) employed the inverse-Gamma prior; and Virtanen et al. (2011) used the Automatic Relevance Determination(ARD) prior. Despite their success, these sparsity-inducing
priors have their own disadvantages  they confound the degree of sparsity with the degree
of regularization on both relevant and irrelevant variables, while in practical settings there
is little reason that these two types of complexity control should be so tightly bounded
together. Although the inverse-Gaussian prior and the inverse-Gamma prior provide more
flexibility of controlling the sparsity, they suffer from being highly sensitive to the controlling parameters and thus lead to unstable solutions. In contrast, our model adopts the spike
and slab prior, which has been recently used in multi-task multiple kernel learning (Titsias
& Lazaro-Gredilla, 2011), sparse coding (Goodfellow et al., 2012), and latent factor analysis (Carvalho, Chang, Lucas, Nevins, Wang, & West, 2008). Note that while our Beta priors
over the selection indicators lead to simple yet effective variational updates, the hierarchical prior in the work of Carvalho et al.(2008) can better handle the selection uncertainty.
Regardless what priors are assigned to the spike and slab models, they generally avoid the
confounding issue by separately controlling the projection sparsity and the regularization
effect over selected elements.
3. https://portal.genego.com/

262

fiSimultaneous Association Discovery and Diagnosis

SHML is also connected with many methods on learning from multiple sources or
views (Hardoon, Leen, Kaski, & Shawe-Taylor, 2008). Multiview learning methods are
often used to learn a better classifier for multi-label classification  usually in text mining
and image classification domains  based on correlation structures among the training data
and the labels (Yu et al., 2006; Virtanen et al., 2011; Rish, Grabarnik, Cecchi, Pereira, &
Gordon, 2008). However, in medical analysis and diagnosis, we meet two separate tasks 
the association discovery between genetic variations and clinical traits, and the diagnosis
on patients. Our proposed SHML conducts these two tasks simultaneously: it employs the
diagnosis labels to guide association discovery, while leveraging the association structures to
improve the diagnosis. In particular, the diagnosis procedure in SHML leads to an ordinal
regression model based on latent Gaussian process models. The latent Gaussian process
treatment differentiates ours from multiview CCA models (Rupnik & Shawe-Taylor, 2010).
Moreover, most multiview learning methods do not model the heterogeneous data types
from different views, and simply treat them as continuous data. This simplification can
degrate the predictive performance. Instead, based on a probabilistic framework , SHML
uses suitable link functions to fit different types of data.

7. Conclusions
We have presented a new Bayesian multiview learning framework to simultaneously find
key associations between data sources (i.e., genetic variations and phenotypic traits) and to
predict unknown ordinal labels. We have shown that the model can also employ background
information, e.g., the Linkage Disequilibrium information, via an additional graph Laplacian
type of prior. Our proposed approach follows a generative model: it extracts a common
latent representation which encodes the structural information within all the data views,
and then generates data via sparse projections. The encoding of knowledge from multiple
views via the latent representation makes it possible to effectively detect the associations
with high sensitivity and specificity.
Experimental results on the ADNI data indicate that our model found biologically meaningful associations between SNPs and MRI features and led to significant improvement on
predicting the ordinal AD stages over the alternative classification and ordinal regression
methods. Despite the drawbacks of the proposed framework in slow training speed and requirement of careful tuning parameters, it has strong modeling power due to the Bayesian
nature. Although we have focused on the AD study, we expect that our model, as a powerful extension of CCA, can be applied to a wide range of applications in biomedical research
 for example, eQTL analysis supervised by additional labeling information.

Acknowledgments
Data used in preparation of this article were obtained from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (adni.loni.ucla.edu). As such, the investigators within
the ADNI contributed to the design and implementation of ADNI and/or provided data but
did not participate in analysis or writing of this report. A complete listing of ADNI investi263

fiXu, Zhe, Qi, & Yu

gators can be found at: http://adni.loni.ucla.edu/wp-content/uploads/how to apply/ADNI
Acknowledgement List.pdf.
This work was supported by NSF IIS-0916443, IIS-1054903, CCF-0939370, NSF China
(Nos. 61572111, 61433014, 61440036), a 973 project of china (No.2014CB340401), a 985
Project of UESTC (No.A1098531023601041) and a Basic Research Project of China Central
University ( No. ZYGX2014J058).
Zenglin Xu and Shandian Zhe have equal contributions to this article. Yuan Qi is the
Principle corresponding author.

Appendix A. Parameter Update for Continuous Data
The parameter ij in Q(sij
g ) introduced in Section 3.1 is calculated as ij = 1/ 1 +

2
ij
ij
2 i( 1  1 )) . The parameters of the Beta
exp(hlog(1  g )i  hlog(g )i + 21 log( 12 ) + 12 hgij
2
2


2
1
2
distribution Q(gij ) is given by lij = ij + l1 and lij = 1  ij + l2 . The parameters of
1

2

1
>
the Gamma distribution Q() are updated as r1 = r1 + np
2 and r2 = r2 + 2 tr(XX ) 
1
>
>
>
tr(hGihUiX ) + 2 tr(hUU ihG Gi).
The moments required in the above distributions are calculated as hi = rr12 and

hlog(gij )i = (l1ij )  (l1ij + l2ij ),
hlog(1  gij )i = (l2ij )  (l1ij + l2ij ),
>

hG Gi =

p
X

i + i >
i ,

i=1

hGi = [1 , . . . , p ]> ,
where (x) =

d
dx

(23)

ln (x).

Appendix B. Parameter Update for Ordinal Data
The variational distributions of Sh and h introduced in Section 3.2 are given by
Q(Sh ) =

q Y
k
Y

sij

ij

ijh (1  ij )1sh ,

(24)

Beta(hij |dij1 , dij2 ),

(25)

i=1 j=1

Q(h ) =

q Y
k
Y
i=1 j=1


2
where ij = 1/ 1+exp(hlog(1hij )ihlog(hij )i+ 12 log( 12 )+ 12 hh2ij i( 12  12 )) , dij
1 = ij +d1 ,
2
1
2
ij
i
>
2
d2 = 1  ij + d2 , hsh i = [1i , . . . , qi ] , and hhij i is the i-th diagonal element in j .
The required moments for updating the above distributions can be calculated as follows:
hlog( ij )i = (dij )  (dij + dij ),
h
hij )i

1

1

2

ij ij
hlog(1 
= (dij
2 )  (d1 + d2 ),
N (bzij +1 |cij , 1)  N (bzij |cij , 1)
,
hcij i = cij 
(bzij +1  cij )  (bzij  cij )
264

fiSimultaneous Association Discovery and Diagnosis

where () is the cumulative distribution function of a standard Gaussian distribution. Note
that in Equation (26), Q(cij ) is a truncated Gaussian and the truncation is controlled by
the observed ordinal data zij .

Appendix C. Parameter Update for Labels
The variational distributions of sw and w in Section 3.3 are given by
Q(sw ) =
Q( w ) =

k
Y
i=1
k
Y

si

i

i w (1  i )1sw ,

(26)

i
Beta(w
; ei1 , ei2 ),

(27)

i=1
i )i  hlog( i )i + 1 hw 2 i( 1 
where i = 1/ 1 + exp(hlog(1  w
w
i 2
2
1

1
))
22



, ei1 = i + e1 and

ei2 = 1  i + e2 .
The required moments for updating the above distributions can be calculated as follows:
i
hlog(w
)i = (ei1 )  (ei1 + ei2 ),
i
hlog(1  w
)i = (ei2 )  (ei1 + ei2 ),
N (byi +1 |fi , 1)  N (byi |fi , 1)
hfi i = fi 
.
(by +1  fi )  (by  fi )
i

i

Note that Q(fi ) is also a truncated Gaussian and the truncated region is decided by the ordinal label yi . In this way, the supervised information from y is incorporated into estimation
of f and then estimation of the other quantities by the recursive updates.

Appendix D. Parameter Update for Latent Representation U
>
The required moments hww> i, hG> Gi
in Section 3.4 are calculated
Ppand hH Hi introduced
>
>
>
as hww i = w + mm , hG Gi = i=1 i + i >
and
i
(
trace(i +  i  >
i ) i=j
(hH> Hi)ij =
.
>
i j
i 6= j

The other required moments have already been listed in the previous sections. The moments regarding U required
variational distributionsPare hUi =
Pn in the> updates of other
n
>
i
i
i 2
[1 , 2 , ...n ], hUU i = i=1 i i +i , hui i = [1 , 2 , ...in ]> and hu>
i ui i =
j=1 (j ) +
(j )ii .

References
Alzheimers Association (2012). 2012 facts and figures alzheimers disease facts and figures.
Tech. rep..
Archambeau, C., & Bach, F. (2009). Sparse probabilistic projections. In Advances in Neural
Information Processing Systems 21, pp. 7380.
265

fiXu, Zhe, Qi, & Yu

Bach, F., & Jordan, M. (2005). A probabilistic interpretation of canonical correlation
analysis. Tech. rep., UC Berkeley.
Carvalho, C., Chang, J., Lucas, J., Nevins, J., Wang, Q., & West, M. (2008). Highdimensional sparse factor modeling: applications in gene expression genomics. Journal
of the American Statistical Association, 103 (484), 14381456.
Chen, X., Liu, H., & Carbonell, J. (2012). Structured sparse canonical correlation analysis..
In AISTATS12, Vol. 22, pp. 199207.
Chu, W., & Ghahramani, Z. (2005). Gaussian processes for ordinal regression. Journal of
Machine Learning Research, 6, 10191041.
Consoli, L., Lefevre, A., Zivy, M., de Vienne, D., & Damerval, C. (2002). QTL analysis
of proteome and transcriptome variations for dissecting the genetic architecture of
complex traits in maize. Plant Mol Biol., 48 (5), 575581.
Daniela, M., & Tibshirani, R. (2009). Extensions of sparse canonical correlation analysis,
with applications to genomic data. Stat Appl Genet Mol Biol., 383 (1).
Falconer, D., & Mackay, T. (1996). Introduction to Quantitative Genetics (4th ed.). Addison
Wesley Longman.
Gandhi, S., & Wood, N. (2010). Genome-wide association studies: the key to unlocking
neurodegeneration?. Nature Neuroscience, 13, 789794.
George, E., & McCulloch, R. (1997). Approaches for bayesian variable selection.. Statistica
Sinica, 7 (2), 339373.
Goodfellow, I., Couville, A., & Bengio, Y. (2012). Large-scale feature learning with spikeand-slab sparse coding. In Proceedings of International Conference on Machine Learning.
Guan, Y., & Dy, J. (2009). Sparse probabilistic principal component analysis. Journal of
Machine Learning Research - Proceedings Track, 5, 185192.
Hardoon, D., Leen, G., Kaski, S., & Shawe-Taylor, J. (Eds.). (2008). NIPS Workshop on
Learning from Multiple Sources.
Harold, H. (1936). Relations between two sets of variates. Biometrika, 28, 321377.
Hunter, D. (2012). Lessons from genome-wide association studies for epidemiology. Epidemiology, 23 (3), 363367.
Jones, B. F., et al. (2006). Differential regional atrophy of the cingulate gyrus in Alzheimer
disease: a volumetric MRI study. Cereb. Cortex, 16 (12), 17011708.
Khachaturian, S. (1985). Diagnosis of Alzheimers disease. Archives of Neurology, 42 (11),
10971105.
Kim, S., Sohn, K., & Xing, E. (2009). A multivariate regression approach to association
analysis of a quantitative trait network. Bioinformaics, 25 (12), 204212.
Lasserre, J., et al. (2006). Principled hybrids of generative and discriminative models. In
IEEE Computer Society Conference on Computer Vision and Pattern Recognition,
Vol. 1, pp. 8794.
266

fiSimultaneous Association Discovery and Diagnosis

Liu, J., Pearlson, G., Windemuth, A., Ruano, G., Perrone-Bizzozero, N., & Calhoun, V.
(2009). Combining fMRI and SNP data to investigate connections between brain
function and genetics using parallel ICA. Hum Brain Mapp, 30 (1), 241255.
MacKay, D. (1991). Bayesian interpolation. Neural Computation, 4, 415447.
Melacci, S., & Mikhail, B. (2011). Laplacian support vector machines trained in the primal.
Journal of Machine Learning Research, 12, 11491184.
Mohamed, S., et al. (2012). Bayesian and L1 approaches for sparse unsupervised learning.
In Proceedings of International Conference on Machine Learning.
Neal, R. M. (1996). Bayesian Learning for Neural Networks. Springer-Verlag New York,
Inc.
Parkhomenko, E., Tritchler, D., & Beyene, J. (2007). Genome-wide sparse canonical correlation of gene expression with genotypes. BMC Proc.
Rish, I., Grabarnik, G., Cecchi, G., Pereira, F., & Gordon, G. (2008). Closed-form supervised dimensionality reduction with generalized linear models. In Proceedings of
International Conference on Machine Learning08, pp. 832839.
Rupnik, J., & Shawe-Taylor, J. (2010). Multi-view canonical correlation analysis. In Proceedings of SIG Conference on Knowledge Discovery and Mining10.
Slatkin, M. (2008). Linkage disequilibrium  understanding the evolutionary past and
mapping the medical future. In Nature Reviews Genetics, Vol. 6, pp. 477485.
Sun, L., Ji, S., & Ye, J. (2011). Canonical correlation analysis for multi-label classification:
A least squares formulation, extensions and analysis. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 33 (1), 194200.
Tibshirani, R. (1994). Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society, Series B, 58, 267288.
Tipping, M., & Bishop, C. (1999). Probabilistic principal component analysis. Journal of
The Royal Statistical Society Series B-statistical Methodology, 61, 611622.
Titsias, M., & Lazaro-Gredilla, M. (2011). Spike and slab variational inference for multitask and multiple kernel learning. In Advances in Neural Information Processing
Systems11, pp. 23392347.
Virtanen, S., Klami, A., & Kaski, S. (2011). Bayesian CCA via group sparsity. In Proceedings
of International Conference on Machine Learning11, pp. 457464.
Yu, S., Yu, K., Tresp, V., Kriegel, H., & Wu, M. (2006). Supervised probabilistic principal
component analysis. In Proceedings of SIG Conference on Knowledge Discovery and
Mining06, pp. 464473.
Yuan, M., & Lin, Y. (2007). Model selection and estimation in regression with grouped
variables.. Journal of the Royal Statistical Society, Series B, 68 (1), 4967.
Zhe, S., Xu, Z., Qi, Y., & Yu, P. (2014). Supervised heterogeneous multiview learning for
joint association study and disease diagnosis. Pacific Symposium on Biocomputing,
19.
267

fiXu, Zhe, Qi, & Yu

Zhe, S., Xu, Z., Qi, Y., & Yu, P. (2015). Sparse bayesian multiview learning for simultaneous association discovery and diagnosis of alzheimers disease. In Proceedings of
the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015,
Austin, Texas, USA., pp. 19661972.
Zou, H., & Hastie, T. (2005). Regularization and variable selection via the elastic net.
Journal of the Royal Statistical Society, Series B, 67, 301320.

268

fiJournal of Artificial Intelligence Research 56 (2016) 463-515

Submitted 12/15; published 07/16

Computing Repairs of Inconsistent DL-Programs over EL Ontologies
EITER @ KR . TUWIEN . AC . AT

Thomas Eiter
Michael Fink
Daria Stepanova

FINK @ KR . TUWIEN . AC . AT
DASHA @ KR . TUWIEN . AC . AT

Institut fr Informationssysteme, TU Wien,
Favoritenstrae 9-11, 1040 Vienna, Austria

Abstract
Description Logic (DL) ontologies and non-monotonic rules are two prominent Knowledge
Representation (KR) formalisms with complementary features that are essential for various applications. Nonmonotonic Description Logic (DL) programs combine these formalisms thus providing support for rule-based reasoning on top of DL ontologies using a well-defined query interface
represented by so-called DL-atoms. Unfortunately, interaction of the rules and the ontology may
incur inconsistencies such that a DL-program lacks answer sets (i.e., models), and thus yields no
information. This issue is addressed by recently defined repair answer sets, for computing which
an effective practical algorithm was proposed for DL-Lite A ontologies that reduces a repair computation to constraint matching based on so-called support sets. However, the algorithm exploits
particular features of DL-Lite A and can not be readily applied to repairing DL-programs over
other prominent DLs like EL. Compared to DL-Lite A , in EL support sets may neither be small
nor only few support sets might exist, and completeness of the algorithm may need to be given
up when the support information is bounded. We thus provide an approach for computing repairs
for DL-programs over EL ontologies based on partial (incomplete) support families. The latter are
constructed using datalog query rewriting techniques as well as ontology approximation based on
logical difference between EL-terminologies. We show how the maximal size and number of support sets for a given DL-atom can be estimated by analyzing the properties of a support hypergraph,
which characterizes a relevant set of TBox axioms needed for query derivation. We present a declarative implementation of the repair approach and experimentally evaluate it on a set of benchmark
problems; the promising results witness practical feasibility of our repair approach.

1. Introduction
Description Logics (DLs) are a powerful formalism for Knowledge Representation (KR) that is
used to formalize domains of interest by describing the meaning of terms and relationships between
them. They are well-suited for terminological modelling in contexts such as, the Semantic Web, data
integration and ontology-based data access (Calvanese, De Giacomo, Lenzerini, Lembo, Poggi, &
Rosati, 2007b; Calvanese, De Giacomo, Lembo, Lenzerini, Poggi, & Rosati, 2007a), reasoning
about actions (Baader, Lutz, Milicic, Sattler, & Wolter, 2005), spatial reasoning (zccep & Mller,
2012), or runtime verification and program analysis (Baader, Bauer, & Lippmann, 2009; Kotek,
Simkus, Veith, & Zuleger, 2014), to mention a few.
As most DLs are fragments of classical first-order logic, they have some shortcomings for modelling application settings, where nonmonotonicity or closed-world reasoning needs to be expressed.
Rules as in nonmonotonic logic programming offer these features. In addition, they serve well as
a tool for declaring knowledge and reasoning about individuals, and for modelling nondeterminism in model generation as possible in Answer Set Programming. To get the best out of the two
c
2016
AI Access Foundation. All rights reserved.

fiE ITER , F INK & S TEPANOVA



(1) Blacklisted  Staff




 (2) StaffRequest  hasAction.Action  hasSubject.Staff  hasTarget.Project
O = (3) BlacklistedStaffRequest  StaffRequest  hasSubject.Blacklisted



(4) StaffRequest(r1 ) (5) hasSubject(r1 , john) (6) Blacklisted (john)



(7) hasTarget(r1 , p1 ) (8) hasAction(r1 , read ) (9) Action(read )


(10) projfile(p1 ); (11) hasowner (p1 , john);






 (12) chief (Y )  hasowner (Z , Y ), projfile(Z );









(13) grant(X)  DL[Project  projfile; StaffRequest](X), not deny(X);
P=




 (14) deny(X)  DL[Staff  chief ; BlacklistedStaffRequest](X);





(15)


hasowner
(Y,
Z),
not
grant(X),






DL[; hasTarget](X, Y ), DL[; hasSubject](X, Z).















Figure 1: DL-program  over a policy ontology
worlds of DLs and nonmonotonic rules, the natural idea of combining them led to a number of
approaches for such a combination, which are often called hybrid knowledge bases; see the work
of Motik and Rosati (2010) and references therein. Among them, Nonmonotonic Description Logic
(DL-)programs (Eiter, Ianni, Lukasiewicz, Schindlauer, & Tompits, 2008) are a prominent approach
in which so-called DL-atoms serve as query interfaces to the ontology in a loose coupling and enable a bidirectional information flow between the rules and the ontology. The possibility to add
information from the rules part prior to query evaluation allows for adaptive combinations. However, the loose interaction between rules and ontology can easily lead to inconsistency, that is to a
lack of models or answer sets.
Example 1 Consider the DL-program  = hO, Pi in Figure 1 formalizing an access policy over
an ontology O = hT , Ai (Bonatti, Faella, & Sauro, 2010), whose taxonomy (TBox) T is given
by (1)-(3), while (4)-(9) is a sample data part (ABox) A. Besides facts (10), (11) and a simple
rule (12), the rule part P contains defaults (13), (14) expressing that staff members are granted
access to project files unless they are blacklisted, and a constraint (15), which forbids that owners of project information lack access to it. Both parts, P and O, interact via DL-atoms such as
DL[Project  projfile; StaffRequest](X). The latter specifies a temporary update of O via the operator , prior to querying it; i.e. additional assertions Project(c) are considered for each individual c, such that projfile(c) is true in an interpretation of P, before all instances X of StaffRequest
are retrieved from O. Inconsistency arises as john, the chief of project p1 and owner of its files,
has no access to them.
Inconsistency is a well-known problem in logic-based and data intensive systems, and the problem of treating logically contradicting information has been studied in various fields, e.g. belief
revision (Alchourrn, Grdenfors, & Makinson, 1985; Grdenfors & Rott, 1995), knowledge base
updates (Eiter, Erdem, Fink, & Senko, 2005), diagnosis (Reiter, 1987), ontology based data access (Lembo, Lanzerini, Rosati, Ruzzi, & Savo, 2015), nonmonotonic reasoning (Brewka, 1989;
Sakama & Inoue, 2003), and many others; (cf. Bertossi, Hunter, & Schaub, 2005; Nguyen, 2008;
Martinez, Molinaro, Subrahmanian, & Amgoud, 2013; Bertossi, 2011). In hybrid formalisms so
far inconsistency management has concentrated mostly on inconsistency tolerance. For instance,
464

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

for MKNF knowledge bases paraconsistent semantics was developed by Knorr, Alferes, and Hitzler
(2008), Huang, Li, and Hitzler (2013) and Kaminski, Knorr, and Leite (2015). For DL-programs inconsistency tolerance issues were targeted by Fink (2012), where a paraconsistent semantics based
on the Logic of Here and There was introduced. Furthermore, Phrer, Heymans, and Eiter (2010)
considered suppressing certain problematic DL-atoms. These approaches aimed at reasoning in
an inconsistent system rather then making required changes to the system to arrive at a consistent
state. This is in contrast to repair techniques that have been recently developed by Eiter, Fink, and
Stepanova (2013, 2014d).
In the theoretical framework for repairing inconsistent DL-programs proposed by Eiter et al.
(2013), the ontology ABox (a likely source of errors) is changed such that the modified DL-program
has answer sets, called repair answer sets. Different repair options including deletion of ABox formulas and various restricted forms of addition have been considered together with a naive algorithm
for computing repair answer sets which lacked practicality.
An effective repair algorithm in which all DL-atoms can be decided without dynamic ontology
access was presented by Eiter, Fink, and Stepanova (2015). It is based on support sets (Eiter, Fink,
Redl, & Stepanova, 2014b) for DL-atoms, the portions of the input that together with the ABox
determine the truth value of the DL-atom. The algorithm exploits complete support families, i.e.
stocks of support sets from which the value of a DL-atom under every interpretation can be determined, such that an (repeated) ontology access can be avoided. The approach works well for
DL-Lite A , which is a prominent tractable DL, since complete support families are small and easy
to compute.
However, unfortunately, for other DLs this approach is not readily usable, because in general
there can be large or infinite support families. This applies even for EL, which is another wellknown important DL that offers tractable reasoning and is widely applied in many domains, including biology, (cf. e.g., Schulz, Cornet, & Spackman, 2011; Aranguren, Bechhofer, Lord, Sattler, &
Stevens, 2007), medicine (Steve, Gangemi, & Mori, 1995), chemistry, policy management, etc. Due
to the features of EL that include range restrictions and concept conjunctions on the left-hand side
of inclusion axioms, a DL-atom accessing an EL ontology can have arbitrarily large and infinitely
many support sets in general. While the latter is excluded for acyclic TBoxes, often occurring in
practice (Gardiner, Tsarkov, & Horrocks, 2006), complete support families can be still very large,
and constructing as well as managing them might be impractical. This obstructs the deployment of
the approach proposed by Eiter et al. (2014d) to EL ontologies. In this paper we tackle this issue and
develop repair computation techniques for DL-programs over ontologies in EL. We focus on EL,
since apart from being simple and widely used, this DL is well-researched, and available effective
algorithms for query rewriting and other important reasoning can be readily used.
More specifically, we introduce here a more general algorithm for repair answer set computation that operates on partial (incomplete) support families along with techniques how such families
can be effectively computed. The problem of computing repair answer sets for DL-programs over
EL ontologies is P2 -complete (in its formulation as a decision problem; we refer to the work of
Stepanova (2015) for details on the complexity).
Our contributions and advances over previous works by Eiter et al. (2014b, 2014d, 2015) are
summarized as follows:
 For effective computation of repair answer sets we exploit the support sets of Eiter et al.
(2014d). In contrast to the approaches of Eiter et al. (2014d, 2015), however, where TBox
classification is invoked, we use datalog rewritings of queries for computing support sets
465

fiE ITER , F INK & S TEPANOVA

(see also Hansen, Lutz, Seylan, & Wolter, 2014). We introduce the notion of partial support
families, with which ontology reasoning access can be completely eliminated.
 As in general constructing complete support families is not always feasible for EL ontologies, we provide novel methods for computing partial support families by exploiting ontology
approximation techniques based on the logical difference between EL-terminologies as considered by Konev, Ludwig, Walther, and Wolter (2012) and Ludwig and Walther (2014).
 To capture restricted classes of TBoxes, for which complete support families can still be
effectively computed, we consider a support hypergraph for DL-atoms, which is inspired
by ontology hypergraphs (Nortje, Britz, & Meyer, 2013; Ecke, Ludwig, & Walther, 2013).
The support hypergraph serves to characterize the TBox parts that are relevant for deriving
the query. The analysis of support hypergraphs allows us to estimate the maximal size and
number of support sets that is needed to form a complete support family.
 We generalize the algorithm for repair answer set computation proposed by Eiter et al. (2014d)
such that EL ontologies can be handled. The novel algorithm operates on partial support
families, and in principle can be applied to the ontologies in any DLs beyond EL. It uses
hitting sets to disable known support sets of negative DL-atoms and performs evaluation
postchecks if needed to compensate incompleteness of support families. Moreover, it trades
answer completeness for scalability by using minimal hitting sets; however completeness may
be ensured by a simple extension.
 We provide a system prototype with a declarative realization of the novel algorithm for repair
answer set computation. Our repair approach has been evaluated using some novel benchmarks; the results show very promising potential of the proposed approach.
Organization. The rest of the paper is organized as follows. In Section 2, we recall basic notions
and preliminary results. Section 3 deals with support sets and their computation, while Section 4
discusses partial support family construction based on TBox approximation techniques. In Section 5
we analyze properties of a support hypergraph for estimating the maximal size and number of support sets in a complete support family for a DL-atom. In Section 6, the algorithm for repair answer
set computation and its declarative implementation are presented. Experiments are presented in Section 7, followed by a discussion of related work in Section 8 and concluding remarks in Section 9.

2. Preliminaries
In this section, we recall basic notions of Description Logics, where we focus on EL (Baader,
Brandt, & Lutz, 2005), and DL-programs (Eiter et al., 2008). For more background on Description
Logics, (see Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003).
2.1 Description Logic Knowledge Bases
We consider Description Logic (DL) knowledge bases (KBs) over a signature O = hI, C, Ri with
a set I of individuals (constants), a set C of concept names (unary predicates), and a set R of role
names (binary predicates) as usual. A DL knowledge base (or ontology) is a pair O = hT , Ai
of a TBox T and an ABox A, which are finite sets of formulas capturing taxonomic resp. factual
466

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

Tnorm


(1) StaffRequest  hasAction.Action




(2)
StaffRequest  hasSubject.Staff




(3)
StaffRequest
 hasTarget.Project



(4) hasAction.Action  ChasA.A
=
(5) hasSubject.Staff  ChasS .St




(6) hasTarget.Project  ChasT .P




(7) ChasA.A  ChasS .St  ChasA.AhasS .St



(8) ChasA.AhasS .St  ChasT .P  StaffRequest

Figure 2: Normalized TBox

























knowledge, whose form depends on the underlying DL. In abuse of notation, we also write O =
T  A viewing O as a set of formulas.
Syntax. In EL, concepts C, denoting sets of objects, and roles R, denoting binary relations
between objects, obey the following syntax, where A  C is an atomic concept and R  R an
atomic role:
C  A |  | C  C | R.C
In EL, TBox axioms are of the form C1  C2 (also called generalized concept inclusion axioms,
GCIs), where C1 , C2 are EL-concepts. ABox formulas are of the form A(c) or R(c, d), where
A  C, R  R, and c, d  I. In the sequel, we use P as a generic predicate from C  R (if the
distinction is immaterial).
An example of an EL ontology is given in Figure 1.
Definition 2 (normalized TBox) A TBox is normalized, if all of its axioms have one of the following forms:
A1  A2
A1  A2  A3
R.A1  A2
A1  R.A2 ,
where A1 , A2 , A3 are atomic concepts.
E.g., the axiom (1) in Example 1 is in normal form, while the axioms (2) and (3) are not. For
any EL TBox, an equivalent TBox in normal form is constructible in linear time (Stuckenschmidt,
Parent, & Spaccapietra, 2009) (over an extended signature)1 (Baader et al., 2005).
A special class of TBoxes widely studied in literature are EL-terminologies, defined as follows:
Definition 3 (EL-terminology) An EL-terminology is an EL TBox T , satisfying the following conditions:
(1) T consists of axioms of the forms A  C and A  C, where A is atomic and C is an
arbitrary EL concept;
(2) no concept name occurs more then once on the left hand side of axioms in T .
For example, the TBox of the ontology in Figure 1 is an EL-terminology.
Semantics. The semantics of DL ontologies is based on first-order interpretations (Baader et al.,
2005). An interpretation is a pair I = hI , I i of a non-empty domain I and an interpretation
1. Linear complexity results are obtained under the standard assumption in DLs that each of the atomic concepts is of
constant size, i.e., the length of a binary string representing an atomic concept does not depend on the particular
knowledge base.

467

fiE ITER , F INK & S TEPANOVA

function I that assigns to each individual c  I an object cI  I , to each concept name C a subset
C I of I , and to each role name R a binary relation RI over I . The interpretation I extends
inductively to non-atomic concepts C and roles R according to the concept resp. role constructors;
as for EL, (R.C)I = {o1 | ho1 , o2 i  RI , o2  C I } and (C  D)I = {o1 | o1  C I , o1  DI }.
Satisfaction of an axiom resp. assertion  w.r.t. an interpretation I, i.e. I |= , is as follows:
(i) I |= C  D, if C I  DI ; (ii) I |= C(a), if aI  C I ; (iii) I |= R(a, b), if (aI , bI )  RI .
Furthermore, I satisfies a set of formulas , denoted I |= , if I |=  for each   .
A TBox T (respectively an ABox A, an ontology O) is satisfiable (or consistent), if some
interpretation I satisfies it. We call an ABox A consistent with a TBox T , if T  A is consistent.
Since negation is neither available nor expressible in EL, all EL ontologies are consistent.
Example 4 The ontology O in Figure 1 is consistent; a satisfying interpretation I = hI , I i
exists, where I = {john, read , p1 , r1 }, Action I = {read }, Blacklisted I = Staff I = {john},
hasSubject I = {r1 , john}, StaffRequest I = BlacklistedStaffRequest I = {r1 }, hasAction I =
{r1 , read }, hasTarget I = {r1 , p1 }.
Throughout the paper, we consider ontologies in EL under the unique name assumption (UNA),
i.e., o1 I 6= o2 I whenever o1 6= o2 holds in any interpretation. However, our results carry over
to ontologies without UNA, as it is not hard to see that the UNA has for EL no effect on query
answering, (cf. Lutz, Toman, & Wolter, 2009).
2.2 DL-Programs
A DL-program  = hO, Pi is a pair of a DL ontology O and a set P of DL-rules, which extend
rules in non-monotonic logic programs with special DL-atoms. They are formed over a signature
 = hC, P, I, C, Ri, where P = hC, Pi is a signature of the rule part P with a set C of constant
symbols and a (finite) set P of predicate symbols (called lp predicates) of non-negative arities, and
O = hI, C, Ri is a DL signature. The set P is disjoint with C, R. For simplicity, we assume
C = I.
Syntax. A (disjunctive) DL-program  = hO, Pi consists of a DL ontology O and a finite set P
of DL-rules r of the form
a1  . . .  an  b1 , . . . , bk , not bk+1 , . . . , not bm

(1)

where not is negation as failure (NAF)2 and each ai , 0  i  n, is a first-order atom p(~t) with
predicate p  P (called ordinary or lp-atom) and each bi , 1  i  m, is either an lp-atom or a DLatom. The rule is a constraint, if n = 0, and normal, if n  1. We call H(r) = {a1 , . . . , an } the
head of r, and B(r) = {b1 , . . . , bk , not bk+1 , . . . , not bm } the body of r. B + (r) = {b1 , . . . , bk }
and B  (r) = {bk+1 , . . . , bm } denote the positive and the negative parts of B(r) respectively.
A DL-atom d(~t) is of the form
DL[; Q](~t),
(2)
where
(a)  = S1 op 1 p1 , . . . , Sm op m pm , m  0 is the input list and for each i, 1  i  m, Si 
C  R, op i  {} is an update operator, and pi  P is an input predicate of the same arity
as Si ; intuitively, op i =  increases Si by the extension of pi ;
2. Strong negation a can be added resp. emulated as usual (Eiter et al., 2008).

468

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

(b) Q(~t) is a DL-query, which has one of the forms (i) C(t), where C is a concept and t is a
term; (ii) R(t1 , t2 ), where R is a role and t1 , t2 are terms; (iii) C1  C2 and ~t = .
Note that inclusion DL-queries of the form C1  C2 can be easily reduced to instance queries.3
Thus for simplicity, we consider in this work only instance DL-queries.
Example 5 Consider a DL-atom DL[Project  projfile; StaffRequest](X ) in the rule (13) of 
in Figure 1 for X = r1 . It has a DL-query StaffRequest(r1 ); its list  = Project  projfile
contains an input predicate projfile which extends the ontology predicate Project via an update
operator .
Semantics. The semantics of a DL-program
S  = hO, Pi is given in terms of its grounding
gr() = hO, gr(P)i over C, i.e., gr(P) = rP gr(r) contains all possible ground instances of
all rules r in P over C. In the remainder, by default we assume that  is ground.
A (Herbrand) interpretation of  is a set I  HB  of ground atoms, where HB  is the Herbrand base for P =hC, Pi, i.e. the set of all ground atoms over P ; I satisfies an lp- or DL-atom a,
if
(i) a  I, if a is an lp-atom, and
(ii) O  I (a) |= Q(~t) where O = hT , Ai, if a is a DL-atom of form (2), where
I (d) =

m
[

Ai (I) and Ai (I) = {Si (~t) | pi (~t)  I}, 1  i  m.

(3)

i=1

Satisfaction of a DL-rule r (resp. a set P of rules) by a Herbrand interpretation I of  = hP, Oi
is then as usual, where I satisfies not bj , if I does not satisfy bj ; I satisfies , if it satisfies each
r  P. By I |=O  we denote that I satisfies (is a model of) an object , where  can be an (DL)atom, a rule or a set of rules; the superscript O of |= specifies the ontology on which DL-atoms are
evaluated. A model I of  is minimal, if no model I  of  exists such that I   I.
Example 6 The DL-atom d = DL[Project  projfile; StaffRequest](r1 ) is satisfied by the interpretation I = {projfile(p1 ), hasowner (p1 , john)}, since O |= StaffRequest(r1 ). For O =

O\{StaffReqeust(r1 )} it still holds that I |=O d, as O  I (d) |= StaffRequest(r1 ).
Repair Answer Sets. Various semantics for DL-programs extend the answer set semantics of logic
programs (Gelfond & Lifschitz, 1991) to DL-programs, (e.g., Eiter et al., 2008; Lukasiewicz, 2010;
Wang, You, Yuan, & Shen, 2010; Shen, 2011). We concentrate here on weak answer sets (Eiter
et al., 2008), which treat DL-atoms like atoms under NAF, and flp-answer sets (Eiter, Ianni, Schindlauer, & Tompits, 2005), which obey a stronger foundedness condition. Both are like answer sets
of an ordinary logic program interpretations that are minimal models of a program reduct, which
intuitively captures that assumption-based application of the rules can reconstruct the interpretation.
I,O
of P relative to O and to I  HB results from gr(P) by deleting
The weak -reduct Pweak
(i) all rules r such that either I 6|=O d for some DL-atom d  B + (r), or I |=O l for some l  B  (r);
(ii) all DL-atoms in B + (r) and all literals in B  (r).
3. Evaluating d = DL[; C1  C2 ]() over O = T  A reduces to evaluating d = DL[; AC2 ](a) over O =
T  {AC1  C1 , C2  AC2 }  A  {AC1 (a)}, where a is a fresh constant and AC1 , AC2 are fresh concepts (similar
as in TBox normalization).

469

fiE ITER , F INK & S TEPANOVA

I,O
The flp-reduct Pflp
of P results from gr(P) by deleting all rules r, whose bodies are not
O
satisfied by I, i.e. I 6|= bi , for some bi , 1  i  k or I |=O bj , for some bj , k < j  m. We
illustrate the notions on an example.

Example 7 Let O be as in Figure 1, and let the rule set P contain the facts (10), (11) and the rules
(12), (13) with X, Y, Z instantiated to r1 , john, p1 respectively. Consider the interpretation I =
I,O
{projfile(p1 ), hasowner (p1 , john), chief (john), grant(r1 )}. While the flp-reduct Pflp
contains
I,O
all rules of P, in the weak -reduct Pweak
the rule (13) is replaced by the fact grant(r1 ).

Definition 8 (x-deletion repair answer set) An interpretation I is an x-deletion repair answer set

of  = hT  A, Pi for x  {flp, weak }, if it is a minimal model of PxI,T A , where A  A; any
such A is called an x-deletion repair of . If A = A, then I is a standard x-answer set.
Example 9 I = {projfile(p1 ), chief (john), hasowner (p1 , john), grant(john)} is both a weak
and flp-repair answer set of  in Example 1 with a repair A = A\{Blacklisted (john)}.
Notation. We denote for any normal logic program P by AS (P) the set of all answer sets of P,
and for any DL-program  by AS x () (resp. RAS x ()) the set of all x-answer sets (resp. x-repair
answer sets) of .
In general an flp-answer set is a weak -answer set, but not vice versa, i.e. flp-answer sets are
a more restrictive notion; however, in many cases weak and flp answer sets coincide. For more
information on the reducts, see works by Eiter et al. (2008) and Wang et al. (2010).
Shifting Lemma. To simplify matters and avoid dealing with the logic program predicates separately, we shall shift as Eiter et al. (2014d) the lp-input of DL-atoms to the ontology. Given a
DL-atom d = DL[; Q](~t) and P  p  , we call Pp (c) an input assertion for d, where Pp
is a fresh ontology predicate and c  C; Ad is the set of all such assertions. For a TBox T and
a DL-atom d, we let Td = T  {Pp  P | P  p  }, and for an interpretation I, we let
OdI = Td  A  {Pp (~t)  Ad | p(~t)  I}. We then have:
Proposition 10 (Eiter et al., 2014d) For every O = T  A, DL-atom d = DL[; Q](~t) and interI
pretation I, it holds that I |=O d iff I |=Od DL[; Q](~t) iff OdI |= Q(~t).
Unlike OI (d), in OdI there is a clear distinction between native assertions and input assertions
for d w.r.t. I (via facts Pp and axioms Pp  P ), mirroring its lp-input. Note that if T is in normal
form, then also Td is in normal form.

3. Support Sets for DL-Atoms
In this section, we recall support sets for DL-atoms by Eiter et al. (2014b), which are an effective
optimization means for (repair) answer set computation (Eiter et al., 2014d). Intuitively, a support
set for a DL-atom d = DL[; Q](~t) is a portion of its input that, together with ABox assertions,
is sufficient to conclude that the query Q(~t) evaluates to true; i.e., given a subset I   I of an
interpretation I and a set A  A of ABox assertions from the ontology O, we can conclude
that I |=O Q(~t). Basically, our method suggests precomputing support sets for each DL-atom at the
nonground level. During DL-program evaluation, for each candidate interpretation ground instances
of support sets are computed, which help to prune the search space for (repair) answer sets.
470

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

Exploiting Proposition 10 we have the following definition of support sets using only ontology
predicates.
Definition 11 (ground support sets) Given a ground DL-atom d = DL[; Q](~t), a set S  AAd
is a support set for d w.r.t. an ontology O = hT , Ai, if Td  S |= Q(~t). By Supp O (d) we denote the
set of all support sets S for d w.r.t. O.
Support sets are grouped into families of support sets or simply support families. More formally,
Definition 12 (support family) Any collection S  Supp O (d) of support sets for a DL-atom d
w.r.t. an ontology O is a support family of d w.r.t. O.
In general and for EL in particular, even -minimal support sets can be arbitrarily large and
infinitely many support sets may exist (not for acyclic TBoxes T , where still exponentially many
support sets are possible). However, we nonetheless can exploit them for the repair answer set
computation algorithms in Section 6.
Support sets are linked to interpretations by the following notion.
Definition 13 (coherence) A support set S of a DL-atom d is coherent with an interpretation I, if
for each Pp (~c)  S it holds that p(c)  I.
Example 14 The DL-atom d = DL[Project  Projfile; StaffRequest](r1 ) from Figure 1 has two
support sets: S1 = {StaffRequest(r1 )} and S2 = {hasSubject(r1 , john),Projectprojfile (p1 ),
Staff (john),hasAction(r1 , read ), Action(read )}. S1 is coherent with any interpretation, while
S2 is coherent only with interpretations I  {projfile(p1 )}.
The evaluation of d w.r.t. I then reduces to the search for coherent support sets.
Proposition 15 Let d = DL[; Q](~t) be a ground DL-atom, let O = hT , Ai be an ontology, and
let I be an interpretation. Then, I |=O d iff some S  Supp O (d) exists s.t. S is coherent with I.
Using a sufficient portion of support sets, we can completely eliminate the ontology access for
the evaluation of DL-atoms. In a naive approach, one precomputes all support sets for all ground DLatoms with respect to relevant ABoxes, and then uses them during the repair answer set computation.
This does not scale in practice, since support sets may be computed that are incoherent with all
candidate repair answer sets.
An alternative is to fully interleave the support set computation with the search for repair answer
sets. Here we construct coherent ground support sets for each DL-atom and interpretation on the
fly. As the input to a DL-atom may change in different interpretations, its support sets must be
recomputed, however, since reuse may not be possible; effective optimizations are not immediate.
A better solution is to precompute support sets at the nonground level, that is, schematic support
sets, prior to repair computation. Furthermore, in that we may leave the concrete ABox open; the
support sets for a DL-atom instance are then easily obtained by syntactic matching.
~ = DL[; Q](X)
~ be a
Definition 16 (nonground support sets) Let T be a TBox, and let d(X)
~
nonground DL-atom. Suppose V  X is a set of distinct variables and C is a set of constants. A
nonground support set for d w.r.t. T is a set S = {P1 (Y~1 ), . . . , Pk (Y~k )} of atoms such that
471

fiE ITER , F INK & S TEPANOVA

(i) Y~1 , . . . , Y~k  V and
~k )} is a support set
(ii) for each substitution  : V  C, the instance S = {P1 (Y~1 ), . . . , Pk (Y
~
for d(X) w.r.t. OC = T  AC , where AC is the set of all possible ABox assertions over C.
For any ontology O = T  AC , we denote by SuppO (d) the set of all nonground support sets for
d w.r.t. T .
Here AC takes care of any possible ABox, by considering the largest ABox (since O  O
implies that Supp O (d)  Supp O (d)).
Example 17 For d = DL[Project  projfile; StaffRequest](X ) the set S1 = {StaffRequest(X )}
is a nonground support set, and likewise the set S2 = {Action(W ), Staff (Y ), hasSubject(X , Y ),
hasTarget(X , Z ), Projectprojfile (Z ), hasAction(X , W )}.
If a sufficiently large portion of nonground support sets is precomputed, then the ontology access
can be fully avoided. We call such a portion a complete support family.
Definition 18 (complete support family) A family S  SuppO (d) of nonground support sets for
~ w.r.t. an ontology O is complete, if for every support set S 
a (non-ground) DL-atom d(X)
~
~
~ exist
Supp O (d(X)), where : X  C, some S   S and an extension  : V  C of  to V  X


such that S = S  .
Example 19 Consider the DL-atom d(X) = DL[Project  projfile; StaffRequest](X) from Figure 1. The family S = {S1 , S2 , S3 , S4 , S5 , S6 } is complete for d w.r.t. O, where hT = hasTarget,
hS = hasSubject and hA = hasAction:







S1
S2
S3
S4
S5
S6

= {StaffRequest(X )};
= {Project(Y ), hT (X , Y ), hS (X , Z ), Staff (Z ), hA(X , Z  ), Action(Z  )};
= {Projectprojfile (Y ), hT (X , Y ), hS (X , Z ), Staff (Z ), hA(X , Z  ), Action(Z  )};
= {Project(Y ), hT (X , Y ), hS (X , Z ), Blacklisted (Z ), hA(X , Z  ), Action(Z  )};
= {Projectprojfile (Y ), hT (X , Y ), hS (X , Z ), Blacklisted (Z ), hA(X , Z  ), Action(Z  )};
= {BlacklistedStaffRequest(X )}.


We say that two nonground support sets (resp. support families) are ground-identical, if their
groundings coincide. E.g., the support sets S1 = {P (X), r(X, Y )} and S2 = {P (X), r(X, Z)} are
ground-identical for a DL-atom d(X) = DL[; Q](X), and so are the respective support families
{S1 } and {S2 }.
Definition 20 (subsumption) A nonground support set S is subsumed by S  , denoted by S   S,
if for every ground instance S of S some ground instance S   of S  exists such that S    S.
For nonground support families, we say that S1 is subsumed by S2 , denoted S2  S1 , if for each
instance S of S  S1 some instance S   of S  in S2 exists such that S    S holds.
Example 21 S = {BlacklistedStaffRequest(X ),hasSubject(X , Y ),Blacklisted (Y )} is a support
set for the DL-atom d(X) = DL[Staff  chief ; BlacklistedStaffRequest](X) w.r.t. T from Figure 1, which is subsumed by S  = {BlacklistedStaffRequest(X )}, i.e. S   S. Moreover,
S  S, where S = {S  } and S ={S}, while the support families S = {S, S  } and S = {S,
{BlacklistedStaffRequest(X ),hasSubject(X , Z ),Blacklisted (Z )}} mutually subsume each other.
472

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

Axiom
Datalog rule
A1  A2
A2 (X)  A1 (X)
A1  A2  A3 A3 (X)  A1 (X), A2 (X)
R.A2  A1 A1 (X)  R(X, Y ), A2 (Y )
A1  R.A2
R(X, oA2 )  A1 (X)
A2 (oA2 )  A1 (X)

Table 1: EL TBox Rewriting
Definition 22 (maximal support set size, maxsup) The maximal support set size of a DL-atom d
w.r.t. T , denoted by maxsup(d ), is the smallest integer n  0 such that for every complete nonground support family S for d w.r.t. T and support set S  S with |S| > n, a support set S   S
exists for d w.r.t. T in Suppd (O) with |S  |  n.
For instance, for the DL-atom d and the TBox T from Example 19, the maximal support set
size is 6, i.e., maxsup(d) = 6.
3.1 Computing Support Sets
In this section, we provide methods for constructing nonground support sets. A natural approach
for the computation of nonground support sets is to exploit (conjunctive) query answering methods
in EL (e.g., Rosati, 2007; Lutz et al., 2009; Kontchakov, Lutz, Toman, Wolter, & Zakharyaschev,
2010; Stefanoni, Motik, & Horrocks, 2012).
Suppose we are given a DL-program  = hO, Pi, where O = hT , Ai is an EL ontology, and
~ = DL[; Q](X).
~ Our method to construct nonground support sets for d(X)
~ has
a DL-atom d(X)
the following three steps.
Step 1. DL-query Rewriting over the TBox. The first step exploits the rewriting of the DL~ over the TBox Td = T  {Pp  P | P  p  } into a set of datalog rules.
query Q of d(X)
At a preprocessing stage, the TBox Td is normalized. This technique restricts the syntactic form
of TBoxes by decomposing complex into simpler axioms. For this purpose, a set of fresh concept
symbols is introduced. Once the normalized form Td norm of Td is computed, we rewrite the part of
the TBox that is relevant for the query Q into a datalog program Prog Q,Tdnorm using the translation
given in Table 1, which is a variant of a translation by Prez-Urbina, Motik, and Horrocks (2010)
and Zhao, Pan, and Ren (2009). When rewriting axioms of the form A1  R.A2 (fourth axiom
in Table 1), we introduce fresh constants (oA2 ) to represent unknown objects. A similar rewriting
is exploited in the R EQUIEM system (Prez-Urbina et al., 2010), where function symbols are used
instead of fresh constants. As a result we obtain:
Lemma 23 For every data part, i.e., ABox A, and every ground assertion Q(~c), deciding whether
Prog Q,Tdnorm  A |= Q(~c) is equivalent to checking Td norm  A |= Q(~c).
Step 2. Query Unfolding. The second step proceeds with the standard unfolding of the rules of
Prog Q,Td norm w.r.t. the target DL-query Q. We start with a rule that has Q in the head and expand
its body using other rules of the program Prog Q,Tdnorm . By applying this procedure exhaustively,
we get a number of rules which correspond to the rewritings of the query Q over Td norm . Note
that it is not always possible to obtain all of the rewritings effectively, since in general there might
473

fiE ITER , F INK & S TEPANOVA

Prog Q,Td norm

 
(4 ) ChasA.A (X )  hasAction(X , Y ), Action(Y ).




(5 ) ChasS .St (X )  hasSubject(X , Y ), Staff (Y ).


 
(6 ) ChasT .P (X )  hasTarget(X , Y ), Project(Y ).
=

(7
) ChasA.AhasS .St (X )  ChasA.A (X ), ChasS .St (X ).





(8
)
StaffRequest(X )  ChasA.AhasS .St (X ), ChasT .P (X ).



(9) Project(X )  Projectprojfile (X ).

















Figure 3: DL-query Rewriting for DL[Project  projfile; StaffRequest](X) over Td norm
be infinitely many of them if T is cyclic, and still exponentially many for acyclic T ; we discuss
techniques for computing partial support families in the next section.
Step 3. Support Set Extraction. The last step extracts nonground support sets from the rewritings
of Step 2. We select those containing only predicates from Td and obtain rules r of the form
~  P1 (Y~1 ), . . . , Pk (Y~k ), Pk+1
~k+1 ), . . . , Pnp (Y~n ),
Q(X)
(Y
(4)
pk+1

n

where each Pi is a native ontology predicate if 1  i  k, and a predicate mirroring lp-input of d
otherwise. The bodies of such rules correspond to the support sets for a given DL-atom, i.e.
~k+1 ), . . . , Pnp (Y
~n )}
S = {P1 (Y~1 ), . . . , Pk (Y~k ), Pk+1
(Y
(5)
pk+1

n

Now the following holds.
~ = DL[; Q](X)
~ be a DL-atom of a program  = hO, Pi with an EL
Proposition 24 Let d(X)
~
ontology O = hT , Ai. Every set S constructed in Steps 1-3 is a nonground support set for d(X).
By the Shifting Lemma, when working with support sets we can focus on the ontology predicates
and operate only on them. More specifically, rules of the form (4) for k  n fully reflect nonground
support sets as of Definition 16, and ground instantiations of such a rule over constants from C
implicitly correspond to ground support sets.
We now illustrate the computation of nonground support sets for DL-atoms over EL ontologies.
Example 25 Consider a DL-atom DL[Project  projfile; StaffRequest](X) accessing an EL ontology O = hT , Ai from Figure 1. The datalog rewriting for d computed at Step 1 is given in
Figure 3. In Step 2 we obtain the following query unfoldings for StaffRequest:
(1) StaffRequest(X)  StaffRequest(X);
(2) StaffRequest(X)  hasAction(X, Y ), Action(Y ), hasSubject(X, Y  ),
Staff (Y  ), hasTarget(X, Y  ), Projectprojfile (Y  );
(3) StaffRequest(X)  hasAction(X, Y ), Action(Y ), hasSubject(X, Y  ),
Staff (Y  ), hasTarget(X, Y  ), Project(Y  );
(4) StaffRequest(X)  hasAction(X, Y ), Action(Y ), hasSubject(X, Y  ),
Blacklisted (Y  ), hasTarget(X, Y  ), Project(Y  );
(5) StaffRequest(X)  hasAction(X, Y ), Action(Y ), hasSubject(X, Y  ),
Blacklisted (Y  ), hasTarget(X, Y  ), Projectprojfile (Y  ).
In Step 3 we thus get from the rule (2) S2 = {hasAction(X, Y ), Action(Y ), Staff (Y  ),
hasSubject(X, Y  ), hasTarget(X, Y  ), Projectprojfile (Y  )} and from rule (3) S3 ={Action(Y ),
hasAction(X, Y ),Staff (Y  ),hasSubject(X, Y  ), Project(Y  ),hasTarget(X, Y  )}. From (1), (4)
and (5) the remaining support sets are similarly obtained.

474

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

3.2 Partial Support Families
Finding all support sets for a DL-atom is tightly related to computing all solutions to a logic-based
abduction problem. Abduction is an important mode of reasoning widely applied in different areas
of AI including planning, diagnosis, natural language understanding and many others (Console,
Sapino, & Dupr, 1995). Various variants of this problem were actively studied, e.g. by Eiter,
Gottlob, and Leone (1997) and Bienvenu (2008). Unfortunately, most of the practically important
problems in the context of abduction are intractable even for restricted propositional theories (Eiter
& Makino, 2007). The abduction problem for EL TBoxes has been considered by Bienvenu (2008),
represented by a tuple hT , H, Oi, with a TBox T , a set of atomic concepts H and an atomic concept
O. An explanation is a set {A1 , . . . , An }  H, such that T |= A1  . . .  An  O. If the
ABox A  Ad contains only atomic concepts, then computing all nonground support sets for d =
DL[; Q](X) accessing O = hT , Ai corresponds to an abduction problem hTd , sig(A  Ad ), Qi.
If roles occur in A  Ad , then one has to introduce new fresh concepts to construct the complex
concepts as hypothesis, e.g., for R.A an inclusion CR.A  R.A can be added to Td , and CR.A
to H, where CR.A is a fresh concept.
Unlike for DL-Lite A , support families for DL-atoms over EL ontologies have no particular
structure; they can be large, and maximal support set size can be exponential in the size of T .
Example 26 Consider the following acyclic TBox T , which contains the axioms:
(1) r.B0  s.B0  B1
(2) r.B1  s.B1  B2
...
(n) r.Bn1  s.Bn1  Bn
For d1 = DL[; B1 ](X1 ), the maximal support set size is 4, which is witnessed by
S1 = {r(X1 , X2 ), B0 (X2 ), s(X1 , X3 ), B0 (X3 )}.

For the DL-atom d2 = DL[; B2 ](X1 ), we have maxsup(d2 ) = 10, due to S2 = {r(X1 , X2 ),
r(X2 , X3 ), B0 (X3 ), s(X2 , X4 ), B0 (X4 ), s(X1 , X5 ), r(X5 , X6 ), B0 (X6 ), s(X5 , X7 ), B0 (X7 )}.
Moreover, for di = DL[; Bi ](X), we have maxsup(di ) = maxsup(di1 )  2 + 2, 1  i  n.
Note that the maximal support set for dn involves n + 3 predicates. Therefore, if the TBox is
of the above form, and |sig(T )|= k, a lower bound for the worst case support set size for d is
2k1 + 2 = (2k ), which is single exponential in the size of T .

While in general many unfoldings can be produced at Step 2, according to recent results of
Hansen et al. (2014), complete support families for EL can be computed for large classes of ontologies. Therefore, we still exploit support families, but unlike Eiter et al. (2014d) we do not require
them to be complete, and develop techniques for computing partial (i.e. incomplete) support families for DL-atoms. A natural approach in this context is to aim at finding support sets of bounded
size. In general, due to cyclic dependencies such as r.C  C, which are possible in EL but not
in DL-Lite A , support sets can be arbitrary large. An analysis of a vast number of ontologies has
revealed that in many realistic cases ontologies do not contain (nor imply) cyclic axioms (Gardiner
et al., 2006); we thus assume for practical considerations that the TBox of the ontology in a given
DL-program is acyclic, i.e., it does not entail inclusion axioms of form r.C  C. However, even
under this restriction support sets can be large as Example 26 shows.
475

fiE ITER , F INK & S TEPANOVA

If computing complete support families is computationally too expensive, a natural approach
is to produce only support sets of a certain size k using e.g. limited program unfolding. When an
unfolding branch reaches the depth k, we stop and expand a different branch. Similarly, we can compute a limited number k of support sets by stopping the rule unfolding of the program Prog Q,Tdnorm
once the k-th support set is produced. An alternative approach, based on TBox approximation
techniques, is pursued in the next section.

4. Partial Support Family Construction via TBox Approximation
We now provide practical methods to construct partial support families using TBox approximation.
4.1 TBox Approximation
The approximation of DL ontologies over a source language L in a different target language L
is a well-known and important technique in ontology management. Existing approaches for such
approximation are roughly divided into syntactic approaches and semantic approaches. The former,
e.g. those by Tserendorj, Rudolph, Krtzsch, and Hitzler (2008) and Wache, Groot, and Stuckenschmidt (2005), focus on the syntactic form of the axioms of the original ontology and appropriately
rewrite the axioms that do not comply with the syntax of the target language. They are rather effective in general but can produce unsound answers (Pan & Thomas, 2007). Semantic approaches
focus on the model-based entailment from the original ontology, rather than on its syntactic structure.
They aim at preserving these entailments as much as possible while transforming the ontology into
the target language; in general they are sound, but they might be computationally more expensive
(Console, Mora, Rosati, Santarelli, & Savo, 2014).
For our task of computing partial support families, sound ontology approximation techniques
are relevant. We choose DL-Lite core as the target approximation language, as it lies in the intersection of EL and DL-Lite A , for which complete support families can be effectively identified (Eiter
et al., 2014d). Our approach for approximating a TBox in EL to DL-Lite core exploits the logical
difference between EL TBoxes considered by Konev et al. (2012). The idea behind it is to decide
whether two ontologies give the same answers to queries over a given vocabulary (called signature)
, and compute a succinct representation of the difference if it is not empty. Typical queries include
subsumption between concepts, instance queries and conjunctive queries. In our setting subsumption queries are of particular interest, as based on them nonground support families are constructed.
~ and an ontology O = hT , Ai,
Our approach is as follows. Given a DL-atom d = DL[; Q](X)
we eliminate from the TBox Td axioms outside the DL-Lite core language, and obtain a simplified
TBox Td . We then compute a succinct representation of the logical difference between Td and Td
w.r.t.  = {sig(Ad  A)  Q}; the axioms in the logical difference that fall into DL-Lite core are
then added to Td . By restricting  to predicates that can potentially appear in support sets we avoid
redundant computations and we approximate only the relevant part of the TBox. This approach
is particularly attractive, as the logical difference for EL was intensively studied, e.g. by Lutz,
Walther, and Wolter (2007) and Konev et al. (2012), and polynomial algorithms are available for
EL-terminologies; we thus confine ourselves here to the latter.
To present our approximation approach formally, we first recall some notions introduced by
Konev et al. (2012).
476

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

Definition 27 (-concept difference) The -concept difference between EL-terminologies T1
and T2 is the set cDiff (T1 , T2 ) of all EL-inclusions  over  such that T1 |=  and T2 6|= .
Example 28 For the terminologies T1 = {B  E, E  r., C  A  B} and T2 =
{C  A, D  B, D  C} it holds that cDiff (T1 , T2 ) =  for  = {A, B, C}, while

cDiff (T1 , T2 ) = {B  r.} for  = {B, r}.
If two EL-terminologies entail the same concept subsumptions over the signature , i.e. it holds
that cDiff (T1 , T2 ) = cDiff (T2 , T1 ) = , then they are called -concept inseparable, which is
C
C
denoted by T1 C
 T2 . E.g. in Example 28 we have that T1  T2 and T1 6 T2 .
The logical difference in terms of instance queries is defined as follows.
Definition 29 (-instance difference) The -instance difference between terminologies T1 and T2
is the set iDiff (T1 , T2 ) of pairs of the form (A, ), where A is a -ABox and  a -instance
assertion, such that T1  A |=  and T2  A 6|= . We say that T1 and T2 are -instance inseparable,
in symbols T1 i T2 if iDiff (T1 , T2 ) = iDiff (T2 , T1 ) = .
As easily seen, T1 i T2 implies T1 C
 T2 . The converse is not obvious but also holds.
Theorem 30 (cf. Lutz & Wolter, 2010) For any EL-terminologies T1 and T2 and signature , T1 C

T2 iff T1 i T2 .
4.2 Partial Support Family Construction
We now show that a DL-atom has the same set of support sets under -concept inseparable terminologies. Prior to that, we establish the following lemma.
Lemma 31 Let d = DL[; Q](~t) be a DL-atom, let O = hT1 , Ai be an EL ontology, and let T2 be
C
a TBox. If T1 C
 T2 , where  =sig(A)  sig(Q)  {P | P  p  }, then T1 d  T2 d , where

 =   sig(Ad ).
Armed with this, we obtain the following result on equivalence of nonground support families.
~ be a DL-atom and let T1 , T2 be EL-terminologies such that
Proposition 32 Let d = DL[; Q](X)
C
T1  T2 where  = sig(A  Ad  Q)  {P | P  p  }. If S1 and S2 are complete nonground
support families for d w.r.t. O1 = hT1 , Ai and O2 = hT2 , Ai, respectively, then S1 and S2 are
ground-identical.
Given two EL-terminologies T1 and T2 , the inclusions C  A  cDiff (T1 , T2 ) (resp. A 
C  cDiff (T1 , T2 )) are following Konev et al. (2012) called left (resp. right) witnesses and denoted
lhs
as cWTnrhs
 (T1 , T2 ) (resp. cWTn (T1 , T2 )). It was shown that every inclusion C  D in the concept difference of T1 and T2 contains either a left or a right witness.
Theorem 33 (cf. Konev et al., 2012) Let T1 and T2 be EL-terminologies and  a signature. If
  cDiff (T1 , T2 ), then either C  A or A  D is a member of cDiff (T1 , T2 ), where A  sig()
is a concept name and C and D are EL-concepts occurring in .
477

fiE ITER , F INK & S TEPANOVA

Algorithm 1: PartSupFam: compute partial support family
~ ontology O = hT , Ai
Input: DL-atom d = DL[; Q](X),
Output: Partial nonground support family S  SuppO (d) for d
(a)   {sig(A  Ad )  Q}
(b) Td  T  {Pp  P | P  p  }
(c) Td  Td \{C  D | C 6 {A, r.} or D 6 {A, r.}}
rhs
lhs
(d) lrw  cWTn (Td , Td )  cWTn (Td , Td )


(e) Td  Td  {C  D  lrw | C, D  {A, r.}}
(f) S  {ComplSupF am(d, Td )}
return S
The logical difference between two EL-terminologies in its compact representation consists
only of inclusions with an atomic concept name on either the left or the right hand side. Some may
have inclusions with atomic concepts on both sides or role restrictions of the form r., which fall
into our target language of DL-Lite core DL, and can be therefore reintroduced.
We are now ready to describe the algorithm P artSupF am (see Algorithm 1) to compute partial
~ and an ontology
families of support sets. As input we are given a DL-atom d = DL[; Q](X)
O = hT , Ai, where T is an EL-terminology. We first set the signature  in (a) to predicates
relevant for support set computation for d. We then construct the TBox Td in (b) and its simplified
version Td in (c) by removing from Td all axioms of the form C  D, where C or D is a complex
concept, i.e. all axioms that are not in the DL-Lite core fragment. In (d) we compute right-hand side
and left-hand side witnesses between Td and Td for  and store them in lrw . After that, in (e) we
construct the TBox Td by extending Td with all axioms from lrw , having concepts of the form A or
r on both sides of inclusions. Based on the support set construction method for DL-Lite A of Eiter
et al. (2014d), we then obtain a complete support family S for Td in (f), which is a partial support
family for T .
Proposition 34 The family S computed by Algorithm 1 fulfills S  SuppO (d), i.e., S is a partial
support family for a given DL-atom d w.r.t. T where O = T  A.
If lwr =  in (d) or cDiff (Td , Td ) =  in (e), then S is guaranteed to be complete by Proposition 32. While in general Algorithm 1 can be used for computing support families for DL-atoms
accessing arbitrary TBoxes4 , practically efficient procedures for (d) are available only for acyclic
EL-terminologies (Konev et al., 2012).

5. Bounded Support Sets
In this section, we analyze the size and the number of support sets that a given DL-atom can have.
With bounds on these quantities at hand, one can limit the search space of support sets. More
precisely, we aim at support set families that are sufficient for evaluating the DL-atom. As support
sets S  that are (properly) subsumed by another support set S (i.e., S  S  ) can be dropped, we
consider non-ground support families that subsume any other (in particular, any complete) support
family. More formally,
4. For computing logical difference between arbitrary TBoxes recent results by Feng, Ludwig, and Walther (2015) might
be exploited.

478

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

Definition 35 (-complete support family) We say a nonground support family S for a DL-atom
d is -complete w.r.t. an ontology O, if S  S for S  SuppO (d).
Thus the question are bounds on the size of support sets in S and the cardinality of a smallest S.
Throughout this section, we tacitly assume that TBoxes are acyclic, i.e. they do not entail inclusions of the form R.C  C.
5.1 Estimation of Support Set Size Bounds
We first consider an estimate on the maximal size of support sets in the smallest -complete support
family by analyzing the syntactic properties of a given TBox. To start with, we recall from the work
of Konev et al. (2012) that an atomic concept A is primitive in a terminology T , if it occurs in no
axiom of T on the left-hand side, and pseudo-primitive, if it is either primitive or occurs only on the
left-hand side of axioms A  C, where C is an arbitrary EL concept.
For an EL-terminology T and every pseudo-primitive A such that T |= D  A, where D =
A1  . . .  An  r1 .C1 . . . rm .Cm , some (atomic) conjunct Ai in D exists such that T |= Ai  A
(Konev et al., 2012, Lemma 15). From this we obtain:
Proposition 36 Let d = DL[; Q](~t) be a DL-atom, and let T be an EL-terminology. If Q is
pseudo-primitive in T , then maxsup(d) = 1.
Proposition 36 exploits a specific case, in which the support set size bound is 1. For providing
more liberal syntactic conditions on T that ensure bounded size of support sets, we use ontology hypergraphs (Nortje et al., 2013; Ecke et al., 2013). The latter have been widely studied for extracting
modules of ontologies (Nortje et al., 2013), determining concept difference between EL terminologies (Ecke et al., 2013), efficient reasoning in OWL 2 QL (Lembo, Santarelli, & Savo, 2013), and
other important tasks.
First let us recall the notion of a directed hypergraph, which is a natural generalization of a
directed graph, proposed by Ausiello, DAtri, and Sacc (1983) in the context of databases to represent functional dependencies.
Definition 37 (directed hypergraph) A directed hypergraph is a pair G = (V, E), where V is a
set of nodes of the graph and E is a set of directed hyperedges of the form e = (H, H  ), where
H, H   V are nonempty sets called hypernodes.
Given a hyperedge e = (H, H  ), we call H the tail of e and H  the head of e, denoted by
tail (e) and head (e), respectively. A hypernode is a singleton, if |H| = 1, and a binary hypernode,
if |H| = 2; in abuse of notation, for a singleton {v}, we also simply write v. The notion of an
ontology hypergraph for DL EL introduced by Ecke et al. (2013) is as follows.
Definition 38 (ontology hypergraph) Let T be an EL TBox in normal form, and let   C  R.
The ontology hypergraph GT of T is a directed hypergraph GT = (V, E), where
V = {xA | A  C  (  sig(T ))}  {xr | r  R  (  sig(T ))}  {x }, and
E = {({xA }, {xB }) | A  B  T } 
{({xA }, {xr , xY }) | A  r.Y  T , Y  C  {}}
{({xr , xY }, {xA }) | r.Y  A  T , Y  C  {}} 
{({xB1 , xB2 }, {xA }) | B1  B2  A  T }.
479

fiE ITER , F INK & S TEPANOVA

xr1

xr3

xA3

xA1

x C2

xr2

xA2

xA4

x C1

xD

xr4

Figure 4: Hypergraph GT from Example 39
Example 39 Consider the following TBox in normal form:

(4) C1  C2  D

 (1) r1 .A1  C1
(2) r2 .A2  C2
(5) A3  A2
T =

 (3) r .A  A
(6)
D  r4 .A4
3
3
1

The ontology hypergraph GT for =sig(T ) is depicted in Figure 4.







.



We now define the notions of directed path between two nodes and incoming path to a singleton
node in an ontology hypergraph; both are natural generalizations of a path in a standard graph.
Definition 40 (directed path, incoming path) Suppose that T is an EL TBox in a normal form,
GT = (V, E) is an ontology hypergraph, and x, y  V are singleton nodes occurring in GT . Then a
directed path between x and y in GT is a sequence  = e1 , e2 , . . . , en of (hyper) edges, such that:
(i) tail (e1 )  x;
(ii) head (en )  y;
(iii) for every ei , i < n, some successor s(ei ) = ej of ei exists in GT such that j > i, head (ei ) 
tail (ej ), and s(ei ) = s(ei ) implies head (ei ) 6= head (ei ) for i 6= i .
An incoming path to a singleton node x  V in GT = (V, E) is a directed path  = e1 , . . . , en
from any node y  V to x, such that head (en ) = x. The set of all incoming paths to a node x in a
hypergraph G is denoted by Paths(x , G).
Intuitively, hyperedges in an ontology hypergraph GT model inclusion relations between (complex)
concepts over  in T . Consequently, an incoming path to a singleton node xC in GT models a chain
of inclusions that logically follow from T , such that C is the rightmost element of the chain.
Example 41 Let us look at the ontology hypergraph GT in Figure 4. The sequence of edges
1 = ({xr3 , xA3 }, xA1 ), ({xr1 , xA1 }, xC1 )
480

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

x A 3p

xr1

3

xr3

xA3

xA1

x C2

xr2

xA2

xD

xC

xA

x C1

xB

xD

xQ


(a) Gsupp(d),T
from Example 43


(b) Gsupp(d),T
from Example 45

Figure 5: Examples of support hypergraphs
is an incoming path to xC1 in GT that reflects the inclusions r1 .A1  C1 and r1 .(r3 .A3 )  C1 ;
the sequence
2 = ({xr3 , xA3 }, xA1 ), ({xr1 , xA1 }, xC1 ), ({xr2 , xA2 }, xC2 ), ({xC1 , xC2 }, xD )
is an incoming path to the singleton xD , from which the following set of inclusions can be extracted:
(1) C1 C2 D, (2) r2 .A2 C1 D, (3) r2 .A2 r1 .A1 D, and (4) r2 .A2 r1 .(r3 .A3 )  D.

We now introduce our notion of a support hypergraph for a DL-atom.
Definition 42 (support hypergraph) A support hypergraph for a DL-atom d=DL[; Q](~t) over a

constructed as follows:
normal ontology O = hT , Ai is a hypergraph Gsupp(d),T
1. build the ontology hypergraph GTd = (V, E), where  = sig(A  Ad )  {Q};
2. leave all nodes and edges in Paths(xQ , GTd ) and remove all other nodes and edges;
3. for xC  GTd with C 6 , if in Paths(xC , GTd ) a (hyper) node N exists such that {P | xP 
N }   then leave xC , otherwise remove it and all of its corresponding edges;
4. for xr  GTd , such that r 6 , leave e = ({xr , y}, xC  ) if (xC , {xr , y}) exists in GTd , where
y  {xD , }, otherwise remove e.
Let us illustrate the notion of a support hypergraph on the following example:
~ and
Example 43 Let T from Example 39 be accessed by the DL-atom d = DL[A3  p3 ; D](X),

Td = T  {A3p3  A3 }. The support hypergraph Gsupp(d),T for d with  = sig(Td ) is shown in Figure 5a. The node xD colored in blue corresponds to the DL-query of d. The edge ({xD }, {xr4 , xA4 })

, as it does not lie on the incoming path to xD .

is not in Gsupp(d),T
481

fiE ITER , F INK & S TEPANOVA

Before describing the approach of extracting support sets for a DL-atom from a hypergraph, we
introduce the notion of tree-acyclicity. For alternative definitions we refer the reader to the works,
e.g. by Ausiello, DAtri, and Sacc (1986), Gallo, Longo, and Pallottino (1993) and Thakur and
Tripathi (2009).
Definition 44 (tree-acyclicity) A hypergraph G = (V, E) is called tree-acyclic, if (i) at most one
directed path exists in G between any singleton nodes x, y  V, and (ii) G has no paths  =
e1 , . . . , ek such that tail (e1 )  head (ek ) 6= .
We refer to hypergraphs that are not tree-acyclic as tree-cyclic.




 = T  {B 
Example 45 Gsupp(d),T
in Figure 5a is tree-acyclic, while G  = Gsupp(d),T
 with T

 = T  {A 
A3 , B  A2 } and  =   {B} is not, and neither is G  = Gsupp(d),T
 , where T
1
C2 }.

The hypergraph Gsupp(d),T
for d = DL[; Q](X), T = {D  C; C  A; C  B; A  B  Q}
and  = sig(T ) given in Figure 5b is tree-cyclic, since it contains two paths between xD and xQ ,
namely 1 = xD , xC , xA , {xA , xB }, xQ and 2 = xD , xC , xB , {xA , xB }, xQ .


The support hypergraph Gsupp(d),T
= (V, E) for a DL-atom d = DL[; Q](X) contains all
incoming paths to xQ that start from nodes corresponding to predicates in A  Ad by construction,
i.e. it reflects all inclusions with Q on the right-hand side and predicates over A  Ad on the left
hand-side that are entailed from Td . Hence, by traversing edges of all incoming paths to xQ , we can
construct sufficiently many query rewritings of Q over the TBox Td corresponding to nonground
support sets that allow to subsume every nonground support family w.r.t. O.
If a support hypergraph for a given DL-atom is tree-acyclic, then support sets can be conveniently constructed from it by annotating nodes with variables Xi , i  N in a way as described
hX i
below. We use subscripts for annotations, e.g. xC i means that the node xC is annotated with the
hX ,X i

variable Xi , while xr i j states that xr is annotated with the ordered pair of variables Xi , Xj .
The approach proceeds as follows. We start from the node xQ , which we annotate with X0 ,
hX i
i.e. xQ 0 ; then we traverse the hypergraph backwards, going from a head of an edge to its tail.
For every edge e that we encounter we annotate tail (e) based on its form and on the annotation of
head (e), with variable names that occur in annotation of head (e) and/or fresh variable names Xi ,
i  N, in the following way:
(1) if |tail (e)| = 1, then
hX i

(1.1) if head (e) = {xC1 i }, then tail (e) is annotated with hXi i;
hXi1 ,Xi2 i

(1.2) if head (e) = {xr1
hXi i

hXi i

, xC1 3 }, then tail (e) = xC2 is annotated with hXi1 i, i.e. we

obtain xC2 1 ;
hXi i

(2) if |tail (e)| = 2 and head (e) = {xC

}, then
hX i

hX i

(2.1) if tail (e)={xC1 ,xC2 }, then both xC1 and xC2 are annotated with Xi , i.e. {xC1 i ,xC2 i };
hXi ,Xi1 i

(2.2) if tail (e)={xr1 , xC1 }, then we get {xr1
482

hXi i

, xC1 1 },

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

From every annotated hypernode N , one can create a set of nonground atoms with predicate names
extracted from labels of hypernodes and variable names from their annotations. The nonground
support sets for d = DL[; Q](X0 ) are then constructed from the incoming paths to xQ .
We pick some incoming path 1 to xQ containing n edges, and start traversing it from the
edge en with head (en ) = {xQ }. The first immediate support set is S1 = {Q(X0 )}; the next
one, S2 , is extracted from the annotated tail of en by taking nonground predicates of labels and
variables. We then pick an edge ek such that head (ek )  tail (en ), and obtain further support
sets by substituting nonground atoms that correspond to head (ek )  tail (en ) in S2 with the atoms
extracted from tail (ek ); this is repeated. One can in fact construct the incoming path backwards
along with the support set extraction, until a maximal path is obtained.

Example 46 Consider the maximal incoming path to xD of Gsupp(d),T
from Figure 5a:

 = (xA3 p3 , xA3 ), ({xr3 , xA3 }, xA1 ), ({xr1 , xA1 }, xC1 ), ({xr2 , xA2 }, xC2 ), ({xC1 , xC2 }, xD ).
{z
} |
{z
} |
{z
} |
{z
}
|
{z
} |
e1

e2

e3

e4

e5

hX3 i
3i
Traversing the path backwards, i.e. edges in the order e5 , e4 , e3 , e2 , e1 , we obtain: (xhX
A3 p ,xA3 )e ,

{z
}1
hX i
hX i
hX i
hX i
hX i hX i
hX i
hX i
hX i
0 ,X2 i
0 ,X1 i
2 ,X3 i
,xA1 2 },xC1 0 ) ,({xhX
,xA2 1 },xC2 0 ) ,({xC1 0 ,xC2 0 },xD 0 ).
({xhX
,xA3 3 },{xA1 2 }) ,({xhX
r1
r2
r3
{z
} |
{z
} |
{z
} |
{z
}
|
|

e2

e3

e4

3

e5

The nonground support sets for d are extracted from the resulting annotated path as follows:

 S0 = {D(X0 )} is immediately obtained from head (xD );
 the first incoming path to consider is 1 = e5 , from which we get S1 = {C1 (X0 ), C2 (X0 )};
 next is the path 2 = e4 , e5 as head (e4 )  tail (e5 ), yielding the support set S2 = {C1 (X0 ),
r2 (X0 , X1 ), A2 (X0 , X1 )};
 then, from 3 = e3 , e5 we get S3 = {C1 (X0 ), r1 (X0 , X2 ), A1 (X2 )};
 4 = e3 , e4 , e5 yields S4 = {r2 (X0 , X1 ), A2 (X1 ), r1 (X0 , X2 ), A1 (X2 )};
 from 5 = e2 , e3 , e5 , we extract S5 = {r1 (X0 , X2 ), r3 (X2 , X3 ), A3 (X3 ), C2 (X0 )};
 6 = e2 , e3 , e4 , e5 yields S6 = {r1 (X0 , X2 ), r3 (X2 , X3 ), A3 (X3 ), r2 (X0 , X1 ), A2 (X1 )};
 from 7 = e1 , e2 , e3 , e5 , we extract S7 = {r1 (X0 , X2 ), r3 (X2 , X3 ), A3p3 (X3 ), C2 (X0 )};
 finally, from 8 = e1 , e2 , e3 , e4 , e5 we get S8 = {r1 (X0 , X2 ), r3 (X2 , X3 ), A3p3 (X3 ),
r2 (X0 , X1 ), A2 (X1 )}.

The following lemma formally asserts the correctness of the procedure.

Lemma 47 Let SG be the support family constructed from a tree-acyclic hypergraph G=Gsupp(d),T
~ Then SG is -complete for d w.r.t. O, i.e., SG  S for every S  SuppO (d).
for d = DL[; Q](X).

In particular, Lemma 47 holds for each complete S for d w.r.t. the ontology O = hT , Ai. Thus
we can determine sufficiently many nonground support sets for d by just looking at its support hypergraph. Note that the restriction to tree-acyclic TBoxes is crucial for correctness of the procedure
from above, as it ensures that every node of a hypergraph is annotated only once.
Lemma 47 allows us to reason about the structure and size of support sets by analyzing only
parameters of the support hypergraph. One such parameter, for instance, is the maximal number
n(, G) of hyperedges with a singleton head node excluding ({xr , }, xA ), occurring on some
incoming path  to xQ of a hypergraph G.
483

fiE ITER , F INK & S TEPANOVA

xQ

xL

xE
xF

xD

xM

xB

xA

xK
xC


Figure 6: Support hypergraph Gsupp(d),T
from Example 49

Proposition 48 Let O = hT , Ai be an EL ontology with T in a normal form, and let d =
~ be a DL-atom with a tree-acyclic support hypergraph G 
DL[; Q](X)
. Then
supp(d),T

maxsup(d)  maxG 

supp(d),T


(n(, Gsupp(d),T
)) + 1.

(6)

For tree-cyclic hypergraphs, the bound from above is not tight, which we illustrate next.
Example 49 Consider the DL-atom d(X) = DL[; Q](X) accessing the TBox Td :


(4) E  F  L 


 (1) A  D  F
(2) A  C  K
(5) E  K  M
.
Td =



 (3) A  B  E
(6) M  L  Q

The support hypergraph for d is depicted in Figure 6, where  = sig(Td ). There are six hyperedges with singleton head nodes, but the maximal support set size for d(X) is 4, e.g. S =
{A(X), B(X), D(X), K(X)}.

We next define out- and in-degrees of nodes in a hypergraph.
Definition 50 (hyper-outdegree and -indegree) Given a directed hypergraph G = (V, E), the
hyper-outdegree denoted by hd+ (x) (resp., hyper-indegree hd (x)) of a singleton node x  V is
the number of hyperedges e  E such that tail (e)  x (resp., head (e)  x) and either |tail (e)| = 2
or |head (e)| = 2. Similarly, the outdegree d+ (x) (resp., indegree d (x)) of x is the number of
edges e  E such that tail (e) = {x} (resp., head (e) = {x}) and |head (e)| = |tail (e)| = 1.


Example 51 All nodes X  V\{xA3p , xD } in the hypergraph Gsupp(d),T
of Figure 5a have hyper+
+
outdegree 1, while for xAp3 and xD we have hd (xAp3 ) = hd (xD ) = 0, moreover, d+ (xAp3 ) = 1.
For hyper-indegrees we have hd (xA3 ) = hd (xA1 ) = hd (xC1 ) = hd (xC2 ) = 1. In the graph

 ({xC2 , xA2 }, xD ), it holds that hd+ (xC2 ) = hd+ (xA2 ) = hd (xD ) = 2,
G  = Gsupp(d),T
moreover, d (xA3 ) = 1.


484

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

Now let us define that
smax (x, G) = maxPaths(x,G) (n(, G)  m(, G) + 1),
(7)
P
where m(, G) = xA  (hdc+ (xA )  1), and hdc+ (xA ) is the number of hyperedges of form
({xA , xB }, xC ) on .


Example 52 Consider Gsupp(d),T
in Figure 5a, where Paths(xD , Gsupp(d),T
) contains a single maximal path to xD , viz.  = (xA3p3 , xA3 ), ({xr3 , xA3 }, xA1 ), ({xr2 , xA2 }, xC2 ), ({xr1 , xA1 }, xC1 ),
({xC1 , xC2 }, xD ). We have n(, G) = 4, as four hyperedges on  have a singleton head node, and
m(, G) = 0, as all nodes have hyper-outdegree at most 1; hence smax (xQ , G) = 4  0 + 1 =
5. The hypergraph in Figure 6 has a single maximal incoming path  to xQ , and n(, G) = 6,
m(, G) = (hdc+ (xA )  1) + (hdc+ (xE )  1) = 3; thus smax (xQ , G) = 6  3 + 1 = 4.


We generalize the bound on the maximal support set size for d from Proposition 48 using the
parameter smax (xQ , G) for a node corresponding to the DL-query Q of a DL-atom d, and obtain
the following result for hypergraphs that are possibly tree-cyclic:
Proposition 53 Let O = hT , Ai be an EL ontology with T in a normal form, and let d =
~ be a DL-atom with support hypergraph G 
DL[; Q](X)
supp(d),T , such that  has no role predi
cates. Then maxsup(d )  smax (xQ , Gsupp(d),T ).

) = 4, and
Example 54 For the tree-cyclic hypergraph in Figure 6 we have smax (xQ , Gsupp(d),T
4 is indeed the maximal support set size for d = DL[; Q](X). The hypergraph in Figure 5a has

3 hyperedges, and for every node x  V, hd+ (x)  1. Thus, smax (xQ , Gsupp(d),T
) = 4, which
coincides with maxsup(d ), where d = DL[A3  p3 ; Q](X).


Note that in Proposition 53, we take in computing m(, G) only outgoing hyperedges of the
form ({xC , xD }, xE ) into account, where C, D, E are concepts, and moreover, no roles occur in .
Multiple outgoing hyperedges involving roles r with r   do not influence the support set size.
Example 55 Let a support hypergraph for d = DL[; Q](X) have the hyperedges ({xr , xC }, xD ),
({xC , xs }, xM ), ({xD , xM }, xQ ) where r  , reflecting the axioms r.C  D, s.C  M and
M  DQ. A largest minimal support set for d is S={r(X, Y ), C(Y ), s(X, Z), C(Z)}; its size is
n + 1, where n is the number of hyperedges with a singleton head node, while hd+ (xC ) = 2. 
5.2 Number of Support Sets
Orthogonal to the question considered in the previous section is under which conditions a given
number n of support sets is sufficient to obtain a -complete support family. This problem is tightly
related to counting minimal solutions for an abduction problem, which was analyzed by Hermann
and Pichler (2010) for propositional theories under various restrictions. In particular, counting minimal explanations was shown to be #  coNP-complete for general propositional theories and
#P -complete for Horn propositional theories; as EL subsumes propositional Horn logic, determining the size of a smallest -complete support family is at least #P -hard and thus intractable.
Like for the size of support sets, the support hypergraph can be fruitfully exploited for estimating
the maximal number of support sets for a given DL-atom. To provide such an estimate, we traverse
the support hypergraph forward starting at the leaves and label every node xP with the number of
rewritings for P . To conveniently compute the labels, we introduce support weight functions.
485

fiE ITER , F INK & S TEPANOVA


Definition 56 (support weight function) Let Gsupp(d),T
= (V, E) be a support hypergraph for a
DL-atom d. A support weight function ws : V  N assigns to every node xA  V the number
ws(xA ) of rewritings of A over T w.r.t. .

For every node in a tree-acyclic support hypergraph, the value of ws can be conveniently computed in a recursive manner.

Proposition 57 Let Gsupp(d),T
be a tree-acyclic support hypergraph for a DL-atom d over a (normalized) ontology O = hT , Ai. Then ws is given as follows, where VC  V is the set of nodes for
concepts:



1, P
Q
ws(x) = 1 + T T  (x) x T ws(x )

P
P

+ T T  (x),T 6VC ({x },T )E ws(x ),

if hd (x) = 0 and d (x) = 0 or x 
/ VC ,
otherwise.
(8)

where T  (x) = {T | (T, {x})  E}.

We demonstrate the usage of Proposition 57 by the following examples.

Example 58 To compute ws(x) for the nodes of Gsupp(d),T
in Figure 5a, we traverse the graph from
leaves to the root, and for x  {xr1 , xA2 , xC2 , xr2 , xA3p3 , xr3 } we obtain ws(x) = 1; furthermore,
ws(xA3 ) = ws(xC2 ) = 2, ws(xA1 ) = 3, ws(xC1 ) = 4. Finally, ws(xD ) = 1 + ws(xC1 ) 
ws(xC2 ) = 1 + 4  2 = 9, which is the number of rewritings for D(X) (and hence support sets for
d(X) = DL[A3  p3 ; D](X)) identified in Example 46.

Example 59 Consider the TBox T = {A  B  Q; C  A; D  A; E  A; F  B; G  B; H 
B; A  L} and a DL-atom d = DL[; Q](X), whose support hypergraph for  = sig(T ) is in
Figure 7. We have that ws(xQ ) = 1 + ws(xB )  ws(xA ) = 1 + 4  4 = 17, and indeed there are 17
rewritings for Q(X), namely S1 = {A(X), B(X)}, S2 = {C(X), B(X)}, S3 = {D(X), B(X)},
S4 = {E(X), B(X)}, S5 = {A(X), F (X)}, S6 = {A(X), G(X)}, S7 = {A(X), H(X)}, S8 =
{C(X), F (X)}, S9 = {C(X), G(X)}, S10 = {C(X), H(X)}, S11 = {D(X), F (X)}, S12 =
{D(X), G(X)}, S13 = {D(X), H(X)}, S14 = {E(X), F (X)}, S15 = {E(X), G(X)}, S16 =
{E(X), H(X)}, and S17 = {Q(X)}.
As an immediate corollary of Proposition 57, we obtain

= (V, E) be a tree-acyclic support hypergraph for the DL-atom d =
Corollary 60 Let Gsupp(d),T
~ over an EL ontology O = hT , Ai. If each edge e  E satisfies |tail (e)|=|head (e)|=1,
DL[; Q](X)
then
X
ws(tail (e)) + 1.
(9)
ws(v) =
eE | head(e)=v

Thus for the query node xQ , we get ws(xQ ) = |E| + 1. In fact, Proposition 57 leads to this
simple bound on the size of  -minimal complete support families in more general cases.
486

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

xC

xD

xE

xF

xA

xG

xH

xB

xQ

Figure 7: Hypergraph Gsupp(d),T


= (V, E) be a tree-acyclic support hypergraph for the DL-atom
Proposition 61 Let Gsupp(d),T
~ over an EL ontology, such that for every edge e = ({x, y}, z)  E and edges
d = DL[; Q](X)
e1 , e2  E such that head (ei )  {x, y}, i  {1, 2}, it holds that head (e1 ) = head (e2 ). Then
|SG 
| = |E| + 1.
supp(d),T


Example 62 The hypergraph Gsupp(d),T
in Figure 5a has a single maximal path of length 5, and its
hyperedges satisfy the condition of Corollary 61. As d has 6 support sets, |S| = |E| + 1 holds. 

If the condition of Proposition 61 on e and e1 , e2 is violated, then the maximal size of a  minimal complete support family can not be assessed that easily. For instance, the support hyper
from Figure 7 contains 7 edges, but d has 17 support sets. It can be shown that if k
graph Gsupp(d),T

nodes in Gsupp(d),T violate the condition, then SG 
contains at most |E|k+1 + 1 support sets;
supp(d),T

for the considered example, this yields a bound of 72 + 1 = 50, which is far from tight.
We note that Proposition 57 can not be applied for tree-cyclic support hypergraphs.

for d = DL[; Q](X), T =
Example 63 Consider a tree-cyclic support hypergraph Gsupp(d),T
{D  C; C  A; C  B; A  B  Q} and  = sig(T ), which is shown in Figure 5b. Using
Proposition 57 we get ws(xD ) = 1, ws(xC ) = 2, ws(xA ) = 3, ws(xB ) = 3, ws(xQ ) =
33+1 = 10. However, Q(X) has only 4 rewritings: (1) S1 = {Q(X)}, (2) S2 = {A(X), B(X)},
(3) S3 = {C(X)}, and (4) S4 = {D(X)}.
Intuitively, for tree-cyclic hypergraphs the support weight function ws may also account for nonminimal rewritings {B(X), C(X)}, {A(X), C(X)}, {A(X), D(X)}, {B(X), D(X)}, and some
rewritings can be counted multiple times. Thus in general, ws(x) provides only an upper bound
for the number of rewriting. Likewise, the bound in Proposition 61 is not tight even for simple treecyclic support hypergraphs; e.g., the one for the DL-atom d = DL[; Q](X) w.r.t. the TBox A  Bi ,
Bi  Q, 1  i  n, contains 2  n edges, but d has only n + 2 support sets.


6. Repair Computation Based on Partial Support Families
In this section, we present our algorithm SoundRAnsSet for computing deletion repair answer
sets. As shown by Stepanova (2015), deciding whether a given DL-program  = hT  A, Pi over
an EL ontology has some deletion repair answer set is P2 -complete in the general case, where the
membership part is established by guessing a candidate repair ABox A  A along with a candidate
487

fiE ITER , F INK & S TEPANOVA

answer set I for  = hT  A , Pi, and the suitability of the guess is checked using an NP oracle.
Clearly this is not efficient, as there are |2n | candidate repair ABoxes for n = |A|, even if finding
an answer set I of  would be cheap.
We restrict the search space of repairs in our approach as in the work of Eiter et al. (2014d) by
exploiting support families for DL-atoms; however, in contrast to the results by Eiter et al. (2014d),
the support families are not required to be complete. If the families are complete (which may
be known or asserted in their construction), then SoundRAnsSet is guaranteed to be complete;
otherwise, it may miss repair answer set, but an easy extension ensures completeness.
Our algorithm for repair answer set computation, shown as Algorithm 2, proceeds as follows.
 We start at (a) by computing a family S of nonground support sets for each DL-atom.
 Next in (b) the so-called replacement program  is constructed.
The replacement program is obtained by a simple rewriting of gr(), where each DL-atom d
is replaced by an ordinary atom ed (called replacement atom), and a disjunctive choice rule
ed  ned  is added that informally guesses the truth value of d, where ed (respectively ned )
stands for the value true (respectively false). Each repair answer set of  augmented with the
proper choice of ed resp. ned is an answer set of  (Eiter et al., 2013, Proposition 13); thus
the search can be confined to answer sets I of , which can be found using a standard ASP
solver.
 In (c) the answer sets I of  are computed one by one.
 we determine in (d) the sets Dp (resp. Dn ) of DL-atoms that are guessed true (resp.
 For I,
 A) which instantiates S for the DL-atoms in
false) in it and then use the function Gr(S, I,

Dp  Dn to relevant ground support sets, i.e., those compatible with I.
 In (e) we loop through all minimal hitting sets H  A of the support sets for DL-atoms in Dn
that only consist of ABox assertions, and in (f) we construct for each H the set Dp of atoms
from Dp that have at least one support set which is disjoint from H (thus removing H from
A does not affect the values of atoms in Dp ).

 Then in (g) we evaluate in a postcheck the atoms in Dn and Dp \Dp over T  A\H w.r.t. I.
A Boolean flag rep stores the evaluation result of a function eval n (resp. eval p ). More specifically, given Dn (resp. Dp ), I and T  A\H, the function eval n (resp. eval p ) returns true, if
all atoms in Dn (resp. Dp ) evaluate to false (resp. true).
 T  A \ H, P) succeeds, then in (h) the
 If rep is true and the foundedness check flpFND(I,


restriction I| of I to the original language of  is output as repair answer set.
We remark that in many cases, the foundedness check might be trivial or superfluous (Eiter,
Fink, Krennwallner, Redl, & Schller, 2014a), e.g., when there are no loops through DL-atoms; if
we consider weak answer sets (Eiter et al., 2013), it can be entirely skipped.
Example 64 Let  be the DL-program from Example 1 with equivalence () in the axioms (2)
and (3) weakened to , and with further assertions Project(p1 ) and BlacklistedStaffRequest(r1 )
added to the ABox A. Moreover, assume that d1 (r1 ) = DL[Project  projfile; Staffrequest](r1 ),
d2 (r1 )=DL[Staff  chief ; BlacklistedStaffRequest](r1 ), d3 (r1 ,p1 )=DL[; hasTarget](r1 ,p1 )
488

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

Algorithm 2: SoundRAnsSet: compute deletion repair answer sets
Input: =hT  A, Pi
Output: a set of repair answer sets of 
(a) compute a set S of nonground support families for the DL-atoms in 
(b) construct the replacement program 
(c) for I  AS () do

 Dn  {d | ned  I};
 SIgr
 A);
(d)
Dp  {d | ed  I};
 Gr(S, I,
S

(e)
for all minimal hitting sets H  A of d Dn SIgr (d ) do

(f)
Dp  {d  Dp | S  SIgr (d) s.t. S  H = }
 T  A\H)  evalp (Dp \Dp , I,
 T  A\H)
(g)
rep  evaln (Dn , I,
 hT  A\H, Pi) then output I|

(h)
if rep and flpFND(I,
end
end

and d4 (r1 ,john) = DL[; hasSubject](r1 ,john). Then in (b) the following replacement program
 is constructed:



(1) ed1 (r1 )  ned1 (r1 ); (2) ed2 (r1 )  ned2 (r1 ); (3) ed3 (r1 , p1 )  ned3 (r1 , p1 ); 








(4)
e
(r1
,
john)

ne
(r1
,
john);
(5)
projfile(p1
);
(6)
hasowner
(p1
,
john);


d4
d4






(7) chief (john)  hasowner (p1 , john), projfile(p1 );
 =
.


(8) grant(r1 )  ed1 (r1 ), not deny(r1 );










 (9) deny(r1 )  ed2 (r1 );





(10)   hasowner (p1 , john), not grant(r1 ), ed3 (r1 , p1 ), ed4 (r1 , john).

Suppose that I = {ed1 , ned2 , ed3 , ed4 , hasowner (p1 , john), projfile(p1 ), chief (john)} is returned
at (c) and that the following partial support families are obtained in (d):


 SIgr (d1 ) = {S1 , S2 }, where S1 = {hasAction(r1 , read ), hasSubject(r1 , john), Action(read ),
Staff (john), hasTarget(r1 , p1 ), Projectprojfile (p1 )} and S2 = {StaffRequest(r1 )};


 SIgr (d2 ) = {S1 ,S2 }, where S1 = {StaffRequest(r1 ),hasSubject(r1 , john),Blacklisted (john)}
and S2 = {BlacklistedStaffRequest(r1 )}.


 SIgr (d3 ) = {S1 }, where S1 = {hasTarget(r1 , p1 )};


 SIgr (d4 ) = {S1 }, where S1 = {hasSubject(r1 , john)}.
At (e) we get a hitting set H = {StaffRequest(r1 ), BlacklistedStaffRequest(r1 )}, which is disjoint
with S1 , S1 and S1 . Thus in (f) we obtain Dp = {d1 , d3 , d4 } and then in (g) we check whether
d2 is false under A\H. As this is not true, rep = false and we pick a different hitting set H  , e.g.
{Blacklisted (john), BlacklistedStaffRequest(r1 )}. Proceeding with H  , we get to (g), and as
 T  A  H) = true and the flp-check succeeds at (f), the interpretation I|
  is output.
eval n (d2 , I,

The following results state that our algorithm works properly.
489

fiE ITER , F INK & S TEPANOVA

Theorem 65 Algorithm SoundRAnsSet is sound, i.e., given a program  = hT  A, Pi, every
output I is a deletion repair answer set of .
If we know in addition that the support families are complete, then the postchecks at (g) are
redundant. If Dp = Dp , then we set rep = true, otherwise rep = false.
Theorem 66 Suppose that for the input program  = hT  A, Pi of Algorithm SoundRAnsSet, it
holds that for each DL-atom in  the support family in S computed in Step (a) of SoundRAnsSet
is -complete. Then Algorithm SoundRAnsSet is complete, i.e., it outputs every deletion repair
answer set of .
We can easily turn SoundRAnsSet into a complete algorithm, by modifying (e) to consider all
hitting sets, but not only minimal ones. In the worst case, this means a fallback to almost the naive
algorithm (note that all hitting sets can be enumerated efficiently relative to their number).
6.1 Optimizations and Extensions
Research in repairing databases (see the work by Bertossi, 2011, for overview) suggests several
techniques, which are of potential interest for DL-programs, and could be exploited for optimizing
and extending our repair approach. Localization of repairs proposed by Eiter, Fink, Greco, and
Lembo (2008) is one such technique, where cautiously a part of the data that is affected by inconsistency is identified and the search of repairs is narrowed down to this part. Using localization, in
our setting the ontology ABox can be split into a safe set of facts, which will not be touched by any
repair, and a set of facts that are (probably) affected. After the affected part is repaired, the result is
then combined with the safe ABox part to get the final solution. To find a suitable ABox split, meta
knowledge about the ontology (e.g. modules, additional domain information) can be used.
Another common approach for tackling an inconsistency problem, which proved to be effective
for databases, is decomposition (Eiter et al., 2008). Here, the available knowledge is decomposed
into parts, such that the reasons for inconsistency are identified in each part separately, and then
the respective repairs are conveniently merged. While for databases decomposition is natural, it
is in general unclear how an inconsistent DL-program can be effectively decomposed. One way
to approach this problem is by determining DL-atoms whose replacement atoms are guessed true
(resp. false) in all answer sets of . Given a set of such DL-atoms, one can aim at first searching
for a repair under which every such DL-atom has the desired value, and then extend the solution
to get the final result. Modules of DL-programs (as identified by the DLVHEX solver) can also be
exploited for program decomposition.
As not all repairs are equally useful for a certain setting, various filterings on repairs can be applied to get the most plausible candidates. Here, qualitative and domain-specific aspects of repairs
are of crucial importance for their practicability. These can be formulated in terms of additional local constraints that express for instance that facts involving certain predicates or constants must be
preserved (resp. should be checked for removal). Furthermore, the number of facts/predicates/constants allowed for deletion can be bounded. These filterings are incorporated in our repair approach.
Yet there are several further extensions possible like conditional predicate dependence. For example, a user might be willing to express the condition that StaffRequest(r ) can only be eliminated if
hasAction(r , read ) holds in the data part, or Blacklisted staff members can not be removed, if they
own files, for modifying which a separate StaffRequest has been issued by a non-blacklisted staff
member.
490

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

~  S P (Y
~)
(r1 ) Supd (X)
d
A,P

~ S
~)
(Y
(r2 ) Supd (X)
d

~ S P (Y
~)
(r5 )   ned (X),
d

~ )  . . .  Pnd (Y
~ )  ned (X),
~ S A,P (Y
~)
(r6 ) P1d (Y
d

~ )  rb(S p (Y
~ ))
(r3 ) SdP (Y
d
A,P ~
A,P ~

~ ))
(r4 ) Sd (Y )  rb(Sd (Y )), nd(SdA,P (Y

~  ed (X),
~ not Cd , not Supd (X)
~
(r7 ) eval d (X)

~
~
(r8 ) eval d (X)  ned (X), not Cd

~ Cd , not Supd (X)
~
(r9 )   ed (X),

Figure 8: Rules Rd for declarative implementation
6.2 Implementation
We have implemented our repair approach in C++ in a system prototype (dlliteplugin of the DLVHEX
system, 2015).
As discussed, the support sets for EL ontologies are of a rich structure, and thus for their computation, TBox classification as in the work of Eiter et al. (2014d) is insufficient. Indeed, we need
to identify not only inclusions between atomic concepts, but also all inclusions of the form C  B,
where C is an arbitrarily complex concept and B is atomic. For constructing support sets we thus exploit the R EQUIEM tool (Prez-Urbina et al., 2010), which rewrites the target query over the TBox
using datalog rewriting techniques. By limiting the number (resp. size) of the rewritings, partial
support families can be computed.
In principle some support sets may be subsumed by smaller support sets (e.g., {R(c, d),A(c)}
by {A(c)}). These support sets are redundant and thus we eliminate them in our implementation.
After the support families are constructed, we use a declarative approach for determining repair
answer sets, in which the minimal hitting set computation is accomplished by rules. To this end,
~ the three fresh predicates (i) Supd (X),
~ (ii) S P (Y
~ ), and (iii) S A,P (Y
~ ) are
for each DL-atom d(X)
d
d
~  , which intuitively say that d(X)
~ = XX
~ has (i) some support set, (ii) some
introduced, where Y
support set involving only rule predicates, and (iii) some support set involving ABox predicates (and
possibly rule predicates), called mixed support set. Furthermore, for every DL-atom d(X), the rules
Rd in Figure 8 are added to the replacement program .
~ is known to be
In these rules, the atom Cd informally says that the support family for d(X)
complete. Information about completeness of support families for certain DL-atoms can be added
to the declarative program in the form of facts Cd . The rules (r1 )-(r4 ) reflect information about
~ under a potential repair; rb(S) stands for a rule body representing a support
support sets of d(X)
~ ), . . . , not pPnd (Y
~ ),
set S, i.e. rb(S) = A1 , . . . Ak if S = {A1 , . . . , Ak }; nd(S) = not pP1d (Y
~ ), . . . , pPnd (Y
~ )}, encodes the ontology part of S and pP (Y
~ ) states that the assertion
where {pP1d (Y
id

~
~
Pi d (Y ) is marked for deletion. The constraint (r5 ) forbids that d(X), if guessed false has a matching
~ has a matching
support set that consists only of input assertions; (r6 ) means that if instead d(X)
mixed support set, then some assertion from its ontology part must be eliminated. The rule (r7 )
~ is guessed true, completeness of its support family is unknown and no matching
says that if d(X)
~
support set is available, then an evaluation postcheck is necessary (eval d (X));
rule (r8 ) is similar

~
for d(X) guessed false. The rule (r9 ) states that a DL-atom guessed true must have some support
set, if its support family is known to be complete.
The set of facts f acts(A) = {pP (~c) | P (~c)  A}, encoding the ABox assertions and COMP 
{Cd | Sd is a complete support family for d} are added to the program , and then its answer sets
491

fiE ITER , F INK & S TEPANOVA


(1) projfile(p1 ); (2) hasowner (p1 , john); (3) issued (john, r1 );





(4) chief (john)  hasowner (p1 , john), projfile(p1 );





 (5) deny(r1 )  ed (r1 );



(6)   hasowner (p1 , john), issued (john, r1 ), deny(r1 );




(7) ed (r1 )  ned (r1 );





(8) supd (X )  pBlacklistedStaffRequest (X ), not pBlacklistedStaffRequest (X);




 (9) pBlacklistedStaffRequest (X )  ned (X ), pBlacklistedStaffRequest (X );
  R = (10) supd (X )  pStaffRequest (X ), not pStaffRequest (X ), phasSubject (X , Y ),



not phasSubject (X, Y ), pBlacklisted (Y ), not pBlacklisted (Y );





(11)
p
(X
)  phasSubject (X , Y )  pBlacklisted (Y )  ned (X), pBlacklisted (Y ),
StaffRequest




pStaffRequest (X ),





phasSubject (X , Y );




(12)
eval
(X)

e
(X),
not
C
,
not
sup
(X);

d
d
d
d



(13) eval d (X)  ned (X), not Cd ;



(14)   ed (X), Cd , not supd (X).



























































Figure 9: Program   R from Example 68
 we proceed with an evaluation postcheck for all atoms
are computed. For each such answer set I,
d(~c) for which the fact eval d (~c) is in the answer set. If all evaluation postchecks succeed, then we
  of the original program  from I.
 This way one identifies
extract the repair answer set I = I|
weak repair answer sets; for flp-repair answer sets, an additional minimality check is needed. In
many cases, however, the flp and weak answer sets coincide (cf. Eiter et al., 2014a); in particular,
this holds for the example and benchmark programs that we consider.
We now formally show that the described approach indeed correctly computes weak repair answer sets.
Proposition 67 Let  = hO, Pi be a ground DL-program, where O is an EL ontology, let for each
DL-atom d of  be Sd  SuppO (d), and let Rd be the set of rules (r1 )-(r9 ) for d. Define
1 =   R  f acts(A)  COMP,
S
where R = d Rd , f acts(A) = {pP (~c) | P (~c)  A} and COMP  {Cd | Sd is -complete for
d w.r.t. O}. Suppose I  AS (1 ) is such that the evaluation postcheck succeeds for every DL-atom
   RAS weak (). Moreover, if Cd  COMP for every DL-atom d,
d with Cd 6 COMP. Then I|

then RAS weak () = {I| | I  AS (1 )}.
Let us demonstrate the usage of the declarative implementation through an example.
Example 68 Consider in Figure 9 the replacement program  and the rules R of  = hP, Oi,
where O is as in Example 1, and P is as follows:

(1) projfile(p1 ); (2) hasowner (p1 , john); (3) issued (john, r1 );




(4) chief (john)  hasowner (p1 , john), projfile(p1 );
P=

(5) deny(r1 )  DL[Staff  chief ; BlacklistedStaffRequest](r1 );



(6)   hasowner (p1 , john), issued (john, r1 ), deny(r1 ).
492











.

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

Assume that for d(X) = DL[Staff  chief ; BlacklistedStaffRequest](X) we are given an
incomplete support family Sd = {S1 , S2 }, where S1 = {BlackListedStaffRequest(X )} and S2 =
{StaffRequest(X ), hasSubject(X , Y ), Blacklisted (Y )}. Then the interpretation I  {ned (r1 ),
pStaffRequest (r1 ), pBlacklisted (john), evald } is among the answer sets of   R  facts(A). As
 a post-check is needed for d(r1 ); this test succeeds, and thus I|
  is a repair answer set.
eval d  I,


7. Evaluation
The repair answer set computation approach is implemented within the DLVHEX system; the details
can be found in the work of Stepanova (2015), and the software is freely online available (dlliteplugin, 2015). Our approach was evaluated on a multi-core Linux server running DLVHEX 2.4.0 under
the HTCondor load distribution system (HTCondor, 2012), which is a specialized workload management system for compute-intensive tasks, using two cores (AMD 6176 SE CPUs) and 8GB
RAM.
To the best of our knowledge, no similar system for repairing inconsistent DL-programs exists. The list of systems for evaluating DL-programs includes the DR E W system (DReW, 2012;
Xiao, 2014) and the dlplugin of the DLVHEX system (dlplugin, 2007). The DR E W system exploits
datalog rewritings for evaluating DL-programs over EL ontologies; however, it can not handle inconsistencies, which are the focus of our work. Thus DR E W per se could not be used as a baseline
for experiments. To facilitate a comparison, we have thus extended DR E W with a naive repair
technique, where a guess of a repair ABox is followed by a check of its suitability. However, this
immediate implementation turned out to be infeasible even on small instances, as in general the
search space for repairs is ways too large for its full exploitation; guided search is needed to ensure
scalability. The dlplugin of the DLVHEX system invokes R ACER P RO reasoner (RacerPro, 2007) as
a back-end for evaluating calls to the ontology. However, for lightweight ontologies even in the
standard evaluation mode without any repair extensions, it scales worse than the dlliteplugin (Eiter
et al., 2014b); thus we focus on the latter in our experiments.
7.1 Evaluation Workflow
The general workflow of the experimental evaluation was as follows. In the first step, we constructed benchmarks by building rules and constraints on top of existing ontologies such that for
some data parts the constructed programs become inconsistent. The instances were generated using
shell scripts (DL-program benchmark generation scripts, 2015) with the size of the conflicting data
part as a parameter. The benchmarks were then run using the HTCondor system, and the times were
extracted from the log files of the runs. In each run, we measured the time for computing the first
repair answer set, including support set computation, with a timeout of 300 seconds.
For each benchmark, we present our experimental results in tables. The first column p specifies
the size of the instance (varied according to certain parameters specific for each benchmark), and
in parentheses the number of generated instances. E.g., the value 10(20) in the first column states
that a set of 20 instances of size 10 were tested. The other columns represent particular repair
configurations, grouped into three sets.
The first set refers to the settings where -complete support families were exploited, while the
second and the third refer to the settings in which the size, respectively the number of computed sup493

fiE ITER , F INK & S TEPANOVA

port sets was restricted. For the -complete setting, we in addition limit the number of facts (lim_f ),
predicates (lim_p) and constants (lim_c) involved in facts that can be removed; e.g., lim_p = 2
states that the set of removed facts can involve at most two predicates. The parameter del _p stores
predicates that can be deleted; e.g., del _p = StaffRequest means that repairs can be obtained by
removing only facts over StaffRequest.
In the restricted configurations, the column size = n (resp. num = n) states that in the
computed partial support families the size (resp. number) of support sets is at most n; if n = ,
then in fact all support sets were computed, but the system is not aware of the -completeness. We
exploit partial -completeness for the number and size restriction cases, i.e. if no more support sets
for an atom are computed and the number/size limits were not yet reached, then the support family
for the considered atom is -complete.
In an entry t(m)[n], t is the total average running time (including support set generation and
timeouts), m is the number of timeouts and n is the number of found repair answer sets.
7.2 Benchmarks
For the evaluation of the developed algorithms, we considered the following benchmarks.
(1) The policy benchmark is a variant of Example 1, in which the rule (14) of P is changed
to deny(X )  DL[Staff chief ; UnauthorizedStaffRequest](X), and two further axioms,
namely UnauthorizedStaffRequest  StaffRequest  hasSubject.Unauthorized and
Blacklisted  Unauthorized are added to T .
(2) The OpenStreetMap benchmark contains a set of rules over the ontology for enhanced personalized route planning with semantic information (MyITS ontology, 2012) extended by an
ABox containing data from the OpenStreetMap project (OSM, 2012).
(3) The LUBM benchmark comprises rules on top of the well-known LUBM ontology (LUBM,
2005) in EL.
We now describe the benchmark results in details. All experimental data are online available
(Experimental data, 2015).
7.2.1 ACCESS P OLICY C ONTROL
We considered ABoxes An with n staff members, for n  {10, 250, 500}. Each data set has 5
projects and 3 possible actions; furthermore 20% of the staff members are unauthorized and 40% are
blacklisted. For generating instances, we used the probability p/100 (with p from column 1) that a
fact hasowner (pi , si ) is added to the rules part P for each si , pi , such that Staff (si ), Project(pi ) 
A (i.e., instances vary only on facts hasowner (pi , si ) in P.) as a parameter. Here, p ranges from 20,
30, etc. to 90 for A10 and from 5, 10 etc. to 40 for A250 and A500 . The total average running times
for these settings are shown in Tables 24, where SR stands for StaffRequest. The experiments
were performed for the ABoxes of the chosen size (i.e., A10 , A250 , A500 ) to demonstrate how our
approach works on small, medium and large data.
As regards A10 , limiting in the -complete setting the number of predicates for removal slightly
increases the running times. Restricting repairs to removing facts only over StaffRequest does
not slow down the repair computation compared to the unrestricted case, as many of the actual
repairs indeed satisfy this condition. The results for bounded number and size of support sets are
494

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

p
20 (20)
30 (20)
40 (20)
50 (20)
60 (20)
70 (20)
80 (20)
90 (19)

-complete support families
no restr .
lim_p = 2
del_p = SR
1.92 (0)[20] 2.70 (0)[20]
1.91 (0)[20]
1.94 (0)[20] 2.72 (0)[20]
1.94 (0)[20]
1.93 (0)[20] 2.71 (0)[20]
1.93 (0)[20]
1.92 (0)[20] 2.70 (0)[20]
1.92 (0)[20]
1.94 (0)[20] 2.72 (0)[20]
1.95 (0)[20]
1.95 (0)[20] 2.73 (0)[20]
1.95 (0)[20]
1.94 (0)[20] 2.72 (0)[20]
1.95 (0)[20]
1.96 (0)[19] 2.74 (0)[19]
1.96 (0)[19]

Incomplete support families
size = 3
size = 5
num = 3
38.51 (0)[20]
33.86 (0)[20] 1.93 (0)[20]
86.35 (1)[19]
80.52 (1)[19] 1.95 (0)[20]
98.69 (1)[19]
96.45 (1)[19] 1.94 (0)[20]
100.46 (2)[18]
98.06 (2)[18] 1.93 (0)[20]
182.16 (3)[17] 186.20 (3)[17] 1.96 (0)[20]
153.66 (2)[18] 152.66 (2)[18] 1.96 (0)[20]
227.81 (6)[14] 223.24 (6)[14] 1.96 (0)[20]
267.52 (11)[8] 267.89 (12)[8] 1.96 (0)[19]

num = 
1.92 (0)[20]
1.93 (0)[20]
1.93 (0)[20]
1.91 (0)[20]
1.94 (0)[20]
1.94 (0)[20]
1.95 (0)[20]
1.95 (0)[19]

Table 2: Policy benchmark, A10
p
5(20)
10(20)
15(20)
20(20)
25(20)
30(20)
35(20)
40(20)

-complete support families
no restr .
lim_p = 2
del_p = SR
6.06(0)[20]
8.28 (0)[20]
6.05 (0)[20]
6.68(0)[20]
8.90 (0)[20]
6.68 (0)[20]
8.37(0)[20] 10.56 (0)[20]
8.35 (0)[20]
9.39(0)[20] 11.61 (0)[20]
9.40 (0)[20]
11.41(0)[20] 13.62 (0)[20]
11.41 (0)[20]
14.04(0)[20] 16.24 (0)[20]
14.09 (0)[20]
15.17(0)[20] 17.32 (0)[20]
15.19 (0)[20]
17.49(0)[20] 19.64 (0)[20]
17.47 (0)[20]

Incomplete support families
size = 6
num = 3
6.06 (0)[20]
6.07 (0)[20]
6.67 (0)[20]
6.69 (0)[20]
8.33 (0)[20]
8.34 (0)[20]
9.40 (0)[20]
9.43 (0)[20]
11.46 (0)[20] 11.40 (0)[20]
14.10 (0)[20] 14.05 (0)[20]
15.12 (0)[20] 15.16 (0)[20]
17.46 (0)[20] 17.45 (0)[20]

num = 
6.05 (0)[20]
6.67 (0)[20]
8.34 (0)[20]
9.41 (0)[20]
11.40 (0)[20]
14.04 (0)[20]
15.17 (0)[20]
17.43 (0)[20]

Table 3: Policy benchmark, A250
p
5 (20)
10 (20)
15 (20)
20 (20)
25 (20)
30 (20)
35 (20)
40 (20)

-complete support families
no restr .
lim_p = 2
del_p = SR
14.99 (0)[20]
18.71 (0)[20]
14.98 (0)[20]
23.57 (0)[20]
27.14 (0)[20]
23.52 (0)[20]
35.07 (0)[20]
38.85 (0)[20]
35.09 (0)[20]
73.43 (2)[18]
53.27 (0)[20]
73.29 (2)[18]
152.29 (8)[12]
64.91 (0)[20] 152.33 (8)[12]
288.06 (19)[1]
97.32 (1)[19] 288.08 (19)[1]
300.00 (20)[0]
153.03 (5)[15] 300.00 (20)[0]
300.00 (20)[0] 206.96 (10)[10] 300.00 (20)[0]

Incomplete support families
size = 6
num = 3
15.00 (0)[20]
14.97 (0)[20]
23.50 (0)[20]
23.51 (0)[20]
35.02 (0)[20]
35.12 (0)[20]
73.50 (2)[18]
73.32 (2)[18]
164.34 (9)[11] 152.25 (8)[12]
276.11 (18)[2] 288.05 (19)[1]
300.00 (20)[0] 300.00 (20)[0]
300.00 (20)[0] 300.00 (20)[0]

num = 
14.97 (0)[20]
23.43 (0)[20]
35.13 (0)[20]
85.33 (3)[17]
164.32 (9)[11]
300.00 (20)[0]
300.00 (20)[0]
300.00 (20)[0]

Table 4: Policy benchmark, A500
almost constant, except when the size is limited to 5 or smaller (just size 3 and size 5 are shown).
Here support sets exceed the bound and post-evaluation checks often fail, which visibly impacts the
running times. While the support sets are large, there are just few of them; this can be seen from the
insignificant difference between the times for num = 3 and num = .
For the significantly larger ABox A250 , we get that for each value of p the considered settings
perform almost identical except that lim_p = 2 is a bit slower. Moreover, the running times increase
gracefully with the value of p. While bounding the support set size to 5 produces only timeouts (thus
the column is omitted), computing support sets of size 6 is always sufficient to identify repairs.
For the largest setting A500 , in the -complete case finding an arbitrary repair is faster than
under the restriction lim_p = 2 , but only up to p = 15. From p = 20 the results for lim_p = 2
495

fiE ITER , F INK & S TEPANOVA

p
10 (20)
20 (20)
30 (20)
40 (20)
50 (20)
60 (20)
70 (20)
80 (20)
90 (20)

-complete support families
no restr .
lim_f = 5
lim_c = 10
13.01 (0)[20]
13.04 (0)[20]
13.05 (0)[20]
13.10 (0)[20]
13.04 (0)[20]
13.08 (0)[20]
13.11 (0)[20]
13.07 (0)[20]
13.12 (0)[20]

16.50 (0)[20]
16.49 (0)[20]
16.54 (0)[20]
16.58 (0)[20]
16.60 (0)[20]
16.61 (0)[20]
16.68 (0)[20]
16.70 (0)[20]
16.81 (0)[20]

16.46 (0)[20]
16.48 (0)[20]
16.49 (0)[20]
16.47 (0)[20]
16.51 (0)[20]
16.55 (0)[20]
16.58 (0)[20]
16.53 (0)[20]
16.59 (0)[20]

size = 1
16.39 (0)[11]
20.98 (0)[5]
24.56 (0)[0]
59.26 (0)[1]
123.80 (0)[0]
106.63 (1)[0]
139.08 (2)[0]
211.33 (5)[0]
260.36 (11)[0]

Incomplete support families
size = 3
num = 1
13.03 (0)[20]
13.04 (0)[20]
13.06 (0)[20]
13.07 (0)[20]
13.10 (0)[20]
13.06 (0)[20]
13.07 (0)[20]
13.06 (0)[20]
13.10 (0)[20]

13.23 (0)[20]
13.35 (0)[20]
13.51 (0)[20]
13.55 (0)[20]
13.56 (0)[20]
13.60 (0)[20]
13.61 (0)[20]
13.61 (0)[20]
13.67 (0)[20]

num = 3
13.06 (0)[20]
13.01 (0)[20]
13.02 (0)[20]
13.09 (0)[20]
13.04 (0)[20]
13.08 (0)[20]
13.07 (0)[20]
13.06 (0)[20]
13.10 (0)[20]

num = 
12.99 (0)[20]
13.02 (0)[20]
13.05 (0)[20]
13.05 (0)[20]
13.06 (0)[20]
13.08 (0)[20]
13.13 (0)[20]
13.08 (0)[20]
13.08 (0)[20]

Table 5: Open Street Map benchmark results

outperform the unrestricted setting, as the posed limitation restricts the search space of repairs effectively. Removing only facts over StaffRequest is no longer always sufficient, which is witnessed
by the decreased number of identified repairs for del _p = StaffRequest compared to lim_p = 2 .
Again the time increases rather gracefully with p as long as repair answer sets are found.
7.2.2 O PEN S TREET M AP
For the second benchmark, we added rules on top of the ontology developed in the MyITS project.
The fixed ontology contains 4601 axioms, where 406 axioms are in the TBox and 4195 are in the
ABox. The fragment T  of T relevant for our scenario and the rules P are shown in Figure 10.
Intuitively, T  states that building features located inside private areas are not publicly accessible
and a covered bus stop is a bus stop with a roof. The rules P check that public stations do not lack
public access, using CWA on private areas.
We used the method introduced by Eiter, Schneider, imkus, and Xiao (2014) to extract data
from the OpenStreetMap repository (OSM, 2012). We constructed an ABox A by extracting the
sets of all bus stops (285) and leisure areas (682) of the Irish city Cork, as well as isLocatedInside
relations between them (9) (i.e., bus stops located in leisure areas). As the data has been gathered
by many volunteers, chances of inaccuracies may be high (e.g. imprecise GPS data). Since the
data about roofed bus stops and private areas was yet unavailable, we randomly made 80% of
the bus stops roofed and 60% of leisure areas private. Finally, we added for each bsi such that
isLocatedInside(bsi , laj )  A the fact busstop(bsi ) to P with probability p/100. Some instances
are inconsistent since in our data set there are roofed bus stops located inside private areas.
The results are shown in Table 5. For the -complete setting arbitrary repairs are computed
about 3.5 seconds faster than the repairs with bounded changes. For the restricted configuration
the times do not vary much except for size = 1, where a significant time increase is observed, and
repairs are found only for smaller instances. Like in the previous benchmark computing a small
number of support sets is often sufficient, but the configuration num = 1 is as expected slightly
slower than num = 3 (computing support sets is here cheap, while postchecks take some time).
496

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES



T =



(1) BuildingFeature  isLocatedInside.Private  NoPublicAccess
(2) BusStop  Roofed  CoveredBusStop




(9) publicstation(X)  DL[BusStop  busstop; CoveredBusStop](X),



not DL[; Private](X);
P=
(10)


DL[BuildingFeature
 publicstation; NoPublicAccess](X),



publicstation(X ).









Figure 10: DL-program over OpenStreetMap ontology
p
5 (20)
15 (20)
25 (20)
35 (20)
45 (20)
55 (20)
65 (20)
75 (20)
85 (20)
95 (20)

no restr .
37.14 (0)[20]
35.74 (0)[20]
35.71 (0)[20]
36.07 (0)[20]
35.98 (0)[20]
35.92 (0)[20]
36.13 (0)[20]
36.07 (0)[20]
36.11 (0)[20]
36.38 (0)[20]

-complete support families
lim_f = 5
lim_p = 2
47.77 (0)[20] 43.74 (0)[20]
34.93 (0)[11] 42.74 (0)[20]
26.94 (0)[5] 42.80 (0)[20]
20.53 (0)[0] 43.04 (0)[20]
20.50 (0)[0] 43.11 (0)[20]
20.51 (0)[0] 43.11 (0)[20]
20.43 (0)[0] 43.44 (0)[20]
20.63 (0)[0] 43.45 (0)[20]
20.30 (0)[0] 43.35 (0)[20]
20.55 (0)[0] 43.24 (0)[20]

lim_c = 20
43.88 (0)[20]
41.51 (0)[19]
41.71 (0)[19]
26.91 (0)[7]
19.54 (0)[1]
18.47 (0)[0]
18.33 (0)[0]
18.28 (0)[0]
18.04 (0)[0]
18.20 (0)[0]

Incomplete support families
size = 1
size = 3
42.57 (0)[20] 36.52 (0)[20]
42.02 (0)[20] 35.96 (0)[20]
41.91 (0)[20] 35.80 (0)[20]
42.22 (0)[20] 36.00 (0)[20]
41.94 (0)[20] 36.40 (0)[20]
42.31 (0)[20] 35.98 (0)[20]
41.81 (0)[20] 36.02 (0)[20]
42.09 (0)[20] 36.21 (0)[20]
42.22 (0)[20] 36.15 (0)[20]
42.52 (0)[20] 36.17 (0)[20]

num = 
36.26 (0)[20]
35.49 (0)[20]
35.49 (0)[20]
35.65 (0)[20]
35.66 (0)[20]
35.60 (0)[20]
35.92 (0)[20]
35.85 (0)[20]
35.83 (0)[20]
35.62 (0)[20]

Table 6: LUBM benchmark results
7.2.3 LUBM
We have also tested our approach on DL-programs  = hP, Oi built over an EL version of the
LUBM ontology, whose TBox was extended with the following axioms:
(1) GraduateStudent  assists.Lecturer  TA
(2) GraduateStudent  teaches.UndergraduateStudent  TA
The rules of  are as follows:


(3) stud (X )  not DL[; Employee](X ), DL[; TA](X );
;
P=
(4)   DL[Student  stud ; TAof ](X , Y ), takesexam(X , Y )
here (3) states that unless a teaching assistant (TA) is known to be an employee, he/she is a student,
while (4) forbids teaching assistants to take exams in the courses they teach.
The ABox contains information about one university with more then 600 students, 29 teaching
assistants, constructed by a dedicated ABox generator (LUBM data generator, 2013). For pairs of
constants t, c, such that teachingAssistantOf (t, c) is in A, the facts takesexam(t, c) are randomly
added to the rules part with probability p/100, thus the contradicting part in the DL-program is
growing with respect to p.
The results for this benchmark are provided in Table 6. Bounding in the -complete setting
the number of removed facts to 5 slows down the computation, if repairs satisfying the condition
exist. For instances with p  35 (i.e., inconsistency is more entrenched), more than 5 facts must
be dropped to obtain a repair; moreover, they often involve more than 20 constants according to
497

fiE ITER , F INK & S TEPANOVA

column 5. The absence of repairs for lim_f = 5 and lim_c = 20 is found faster than a repair in the
unrestricted mode.
Limiting the support set size to 1 allows one to find repairs for all instances with a delay of less
than 10 seconds compared to the -complete setting. However, there are many support sets for this
benchmark, and thus bounding their number is less effective.
7.3 General Results Discussion
One can observe that for -complete settings and settings where post-evaluation checks are fast, the
running times vary only slightly with growing p. This is due to our declarative implementation, in
which computing repairs is reduced to finding answer sets of the program 1 =   R  facts(A) 
COMP followed by possible evaluation postchecks. In our benchmarks the difference between
instances of size pi and pi+1 is the data part of the logic program, which is small compared to the
part facts(A) of 1 that is constant for all p. Thus as long as postchecks are not needed, the times
required for repairing  do not differ much even though the programs become more inconsistent.
As expected, using -complete support families works well in practice. Naturally, it takes more
time to compute restricted repairs rather than arbitrary repairs; however, when the imposed restrictions are too strong such that no repair can satisfy them, the solver may recognize this faster.
As reported by Hansen et al. (2014), EL-TBoxes that originate from real-world applications
admit FO-rewritings (of reasonable size) in almost all cases. This provides some evidence that realworld EL-TBoxes hardly contain involving constraints on the conceptual level, and that hence either
the size or number of support sets for DL-atoms often turn out to be limited. The novel algorithms
for deletion repair answer set computation demonstrated their applicability for DL-programs over
some real world data (Open Street Map benchmark results in Table 5).
While most of the other benchmarks that we have run are synthetic, they still vary w.r.t. TBox
and ABox sizes. The capability of our algorithms for handling such diverse DL-programs confirms
the potential of our approach.

8. Related Work
Inconsistencies in DL-programs were studied in several works (Phrer et al., 2010; Fink, 2012;
Eiter et al., 2013, 2014d). Phrer et al. proposed an inconsistency tolerant semantics. Keeping the
ontology untouched, the DL-atoms that introduce inconsistency as well as rules involving them are
deactivated. The repair problem, outlined as an open issue by Phrer et al., was formalized by Eiter
et al. (2013), where the notions of repair and repair answer sets together with a naive algorithm
for their computation were proposed. The latter was then optimized by Eiter et al. (2014d, 2015)
for DL-Lite A by effectively exploiting complete support families for DL-atoms. Our approach is
more general, and it differs from the one of Eiter et al. (2014d, 2015) in that it uses partial (not
necessarily complete) support families and can be applied to ontologies in any DL, though with a
possible impact on complexity.
In other hybrid formalisms, inconsistency management has concentrated on inconsistency tolerance rather than on repair. For instance, Huang et al. (2013) presented a four-valued paraconsistent
semantics based on Belnaps logic (Belnap, 1977) for hybrid MKNF knowledge bases (Motik &
Rosati, 2010), which are the most prominent tightly coupled combination of rules and ontologies.
Inspired by the paracoherent stable semantics of Sakama and Inoue (1995), the work of Huang
et al. (2013) was extended by Huang, Hao, and Luo (2014) to handle also incoherent MKNF KBs,
498

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

i.e. programs in which inconsistency arises as a result of the dependency of an atom on its default
negation in analogy to the work of Fink (2012). Another direction of inconsistency handling for
hybrid MKNF KBs is using the three-valued (well-founded) semantics of Knorr, Alferes, and Hitzler (2011), which avoids incoherence for disjunction-free stratified programs. Most recently, this
has been extended by Kaminski et al. (2015) with additional truth values to evaluate contradictory
pieces of knowledge. These works aim at inconsistency tolerance rather than repair, and are geared
in spirit to query answering that is inherent to well-founded semantics; as such, it is limited to
normal logic programs, while DL-programs allow for disjunctive rule heads.
In the context of Description Logics, repairing ontologies has been studied intensively, foremost
to handle inconsistency. Our DL-program repair is related to ABox cleaning (Masotti, Rosati, &
Ruzzi, 2011; Rosati, Ruzzi, Graziosi, & Masotti, 2012). However, the latter differs in various
respects: it aims at restoring consistency of an inconsistent ontology by deleting -minimal sets
of assertions (i.e., computing -maximal deletion repairs); we deal with inconsistency incurred
on top of a consistent ontology, by arbitrary (non-monotonic) rules which access it with a query
interface. Furthermore, we must consider multiple ABoxes at once (via updates), and use EL instead
of DL-Lite. Refining our algorithm to compute -maximal deletion repairs is possible.
The problem of computing support families is tightly related to finding solutions to an abduction
problem, which was considered by Bienvenu (2008) for theories T expressed in EL-terminologies.
A hypothesis H = {A1 , . . . , An } is a set of atomic concepts, and an observation is another atomic
concept. A solution to the abduction problem is any set S  H, such that T |= Ai S Ai  O.
Our setting is more general and involves also roles along with atomic concepts. Abduction has
been studied in various related areas e.g., for DL-Lite ontologies by Calvanese, Ortiz, Simkus, and
Stefanoni (2013), for propositional logic by Eiter and Makino (2007), for datalog by Eiter et al.
(1997) and Gottlob, Pichler, and Wei (2007), etc. Using incomplete support families for DL-atoms
is related in spirit to approximate inconsistency-tolerant reasoning in DLs using restricted support
sets as considered by Bienvenu and Rosati (2013); however, we focus on repair computation and
model generation while Bienvenu and Rosati target inference from all repairs.
Our methods for constructing partial support families exploit the results on the logical difference
between EL terminologies presented by Konev et al. (2012) and Ecke et al. (2013); recently they
were extended to ELHR by Ludwig and Walther (2014) and to general TBoxes by Feng et al.
(2015).
Repairing inconsistent non-monotonic logic programs has been investigated in the work of
Sakama and Inoue (2003), where an approach for deleting rules based on extended abduction was
studied; however, to restore consistency addition of rules is also possible. The latter was considered
by Balduccini and Gelfond (2003), where under Occams razor consistency-restoring rules may be
added. Methods for explaining why the inconsistency arises in a logic program were studied, e.g.,
by Syrjnen (2006), who exploited model-based diagnosis of Reiter (1987) to debug a logic program. Generalized debugging of logic programs was investigated e.g., by Gebser, Phrer, Schaub,
and Tompits (2008). Most recently, Schulz, Satoh, and Toni (2015) considered a characterization
of reasons for inconsistency in extended logic programs (i.e., disjunction-free logic programs with
both strong (classical) negation and weak negation) in terms of culprit sets of literals, based on
the well-founded and maximal partial stable model semantics, and a derivation-based method to explain such culprits has been described; however, it remains open how debugging of logic programs
based on culprit sets could be done and whether this could be fruitfully extended to debugging DLprograms. The latter has been addressed by Oetsch, Phrer, and Tompits (2012) and is related to
F

499

fiE ITER , F INK & S TEPANOVA

the challenging but, to the best of our knowledge, unexplored problem of repairing the rule part of
a DL-program.

9. Conclusion
We have considered computing repair answer sets of DL-programs over EL ontologies, for which
we generalized the support set approach of Eiter et al. (2014d, 2014b) for DL-Lite A to work with
incomplete families of supports sets; this advance is needed since in EL complete support families can be large or even infinite. We discussed how to generate support sets, by exploiting query
rewriting over ontologies to datalog (Lutz et al., 2009; Rosati, 2007; Stefanoni et al., 2012), which
is in contrast to the work by Eiter et al. (2014d), where TBox classification is invoked. Moreover,
we have developed alternative techniques for effective computation of partial support families. Our
approach is to approximate a relevant part of the TBox to DL-Lite core exploiting a notion of logical
difference between EL-terminologies, and then compute complete support families over an approximated TBox using methods of Eiter et al. (2014d). The obtained support family is complete, if the
approximated TBox is logically equivalent to the original one.
To estimate the maximal size of support sets, we have analyzed the properties of a novel support hypergraph, which corresponds to a subgraph of an ontology hypergraph (Nortje et al., 2013;
Ecke et al., 2013), where nodes encode ontology predicates (or pairs of them), while (hyper) edges
reflect TBox inclusions. We have shown how traversing a support hypergraph one can conveniently
compute an upper bound for the number of support sets for a given DL-atom. If, in addition, the
support hypergraph satisfies certain conditions (e.g. tree-acyclicity), then an exact estimate can be
obtained.
We developed a sound algorithm for computing deletion repair answer sets for DL-programs
over EL ontologies, which is complete in case all support families are also known to be complete.
The algorithm trades answer completeness for scalability (a simple variant ensures completeness).
We have implemented the novel algorithm using declarative means within a system prototype, that
invokes a R EQUIEM reasoner for partial support family computation. For an experimental assessment of our repair approach, a set of novel benchmarks has been constructed including real
world data. While the availability of complete support families adds to the scalability of the repair
computation, partial support families work surprisingly well in practice due to the structure of the
benchmark instances: the support sets are either small or there are just few of them, and thus postevaluation checks do not cause much overhead. Overall, our experimental evaluation has revealed a
promising potential of the novel repair methodology for practical applications.
9.1 Outlook
The directions for future work in the considered area are manifold. They cover both theoretical and
practical aspects of our inconsistency handling approach. On the theoretical side, a relevant open
issue are sufficient conditions under which computing all nonground support sets for a DL-atom
accessing an EL ontology becomes tractable. Like in the work of Gebser et al. (2008) bounded
tree-width might be considered, but also other parameters like density of a support hypergraph or
various acyclicity properties. Analyzing the complexity of counting support sets in a complete
support family might give hints to possible restricted settings, in which support family computation
is efficient, but such a complexity analysis is also an interesting problem as such. On the practical
500

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

side, optimization of the current implementation and extending the range of applications to real use
cases is another issue.
Repair may be intermingled with stepping techniques used for debugging DL-programs (Oetsch
et al., 2012). We considered the DL-programs as monolithic structures when applying our repair
techniques, that is the repair computation was performed on a DL-program taken as a whole. It is
an interesting and a relevant quest to extend the approach for dealing with modular DL-programs.
Splitting a program into separate components that can be individually evaluated is a well-known
programming technique, which has been studied in the context of DL-programs (Eiter et al., 2008).
It is not clear, however, to which extent and for which program classes the repair methods can be
adapted for the modular setting.
While we have considered EL in this paper, the basic algorithm and approach is applicable also
to other DLs. Extensions of our work to EL+ and EL++ are easily possible. The main difference
is negation, which is expressible via the  concept; the ontology can get inconsistent through the
updates of DL-atoms, leading to an increased number of support sets that need to be effectively
computed and appropriately handled. The extension to expressive DLs such as SHIQ, SHOIN
or even SROIQ is more challenging, as efficient methods for support set construction remain to
be developed; by the relatively high complexity of these DLs, this comes at a computational cost.
On the other hand, the computation may be done once (even offline) and can be reused; fortunately,
support families need not be complete, but we may expect a return of investment of time in support
set construction for the overall running time.
Orthogonal to other DLs, one can study various additional repair possibilities, e.g. bounded
addition; for overview of repair possibilities see the work of Eiter et al. (2013). Here we have
concentrated on repairing the data part of the ontology, but it is also natural to allow changes in
rules and interfaces. For repairing rules, the works on ASP debugging by Frhstck, Phrer, and
Friedrich (2013), Gebser et al. (2008), and Syrjnen (2006) can be used as a starting point, but
the problem is challenging as the search space of possible changes is large. Priorities on the rules
and atoms involving them might be applied to ensure high quality of rule repairs. The interfaces
similarly admit numerous modifications, which makes this type of repair as difficult; user interaction
is most probably required.
Last but not least one could develop methods for repairing other hybrid formalisms including
tightly-coupled hybrid KBs or even more general representations like HEX-programs (Eiter et al.,
2005), where instead of an ontology arbitrary sources of computation can be accessed from a logic
program. Heterogeneity of external sources in HEX-programs makes both repair and paraconsistent
reasoning a very challenging task.

Acknowledgments
We thank the anonymous reviewers for their detailed and constructive suggestions which helped to
improve this work. This article significantly extends preliminary work of Eiter, Fink, and Stepanova
(2014c). The research has been supported by the Austrian Science Fund (FWF) projects P24090 and
P27730.
501

fiE ITER , F INK & S TEPANOVA

Appendix A. Proofs for Section 3
A.1 Proof of Proposition 15
() By Proposition 10, I |=O d iff Td  A  AI |= Q(~t), where AI = {Pp (~t)  Ad | p(~t)  I}.
Thus, S = A  AI is a support set of d w.r.t. O, and it is coherent with I by construction.
() If S  Supp O (d) is coherent with I, then S is of the form S = A  AI where A  A
and AI  AI , and thus S  A  AI . As Td  S |= Q(~t), by monotonicity Td  A  AI |= Q(~t),
hence by Proposition 10 I |=O d.
A.2 Proof of Proposition 24
~ where
Consider any instance S = {P1 (Y1 ), . . . , Pk (Yk )} of a set S of form (5) for d(X),
 : V  C. We show that S is a support set w.r.t. OC = hT , AC i (recall that AC is the set of all
~
possible ABox assertions over C), i.e., S  AC  Ad (which clearly holds) and Td  S |= Q(X).
~
The latter is equivalent to Tdnorm  S |= Q(X),
which in turn by Lemma 23 is equivalent to
0
~
Prog Q,Td norm  S |= Q(X). Let Prog = Prog Q,Td norm , and let Prog i+1 , for each i  0,
~
be the program that results from Prog i by unfolding a rule w.r.t. the target query Q(X).
Then
i+1
i
~
~
Prog
 S |= Q(X) iff Prog  S |= Q(X) holds. Now by construction of S, there is a rule
~ and thus Prog i  S |= Q(X).
~
r of the form (4) in some Prog i . Clearly {r}  S |= Q(X)
It
0
~
~
~
follows that Prog  S |= Q(X) and hence Td norm  S |= Q(X) and Td  S |= Q(X).

Appendix B. Proofs for Section 4
B.1 Proof of Lemma 31
Towards a contradiction, assume T1d 6C
 T2d . Then w.l.o.g. T1d |= P1  P2 but T2d 6|= P1  P2 ,

where P1 , P2   . Observe that  and  differ only on predicates Pp , such that P  p occurs in
, and that T  = T1d \T1 = T2d \T2 consists only of axioms Pp  P where Pp does not occur in
T1 or T2 . We first show that P2   must hold. Indeed, otherwise P2   \  and thus P2 =
Pp  sig(Ad ) for some P  p from . Now let A = {P1 (c)} if P1  , and A = {P1 (c), P1p (c)}
otherwise (i.e., P1   \ ), for an arbitrary c  I. Then T1d  A has a model I in which cI  P1I
(resp. cI  P1I and cI  P1 Ip ) and Pp I =  (thus P1 6= P2 ), as EL is negation-free and Pp occurs
in axioms only on the left. As I 6|= P1  Pp , it follows T1d 6|= P1  P2 , which is a contradiction.
This proves P2   \ . Now there are two cases.
(i) P1  : T1 C
 T2 implies T2 |= P1  P2 ; by monotonicity T2d |= P1  P2 , a contradiction.
(ii) P1   \ : then P1 = Pp , where P  p occurs in , and P  . We claim that T1 |= P  P2 .
Indeed, otherwise T1 has a model I such that P I 6 P2 I . Then as easily seen the interpretation I 


that coincides with I on  and has Pp I = P I \ P2 I and Pp I =  for each Pp   \  is a model
of T1 d ; however, I  6|= Pp  P2 , which would be a contradiction. This proves the claim. Now
from the claim and T1 C
 T2 , it follows T2 |= P  P2 and by monotonicity T2d |= P  P2 . As
Pp  P  T2 d , it follows T2 d |= P1  P2 ; this is a contradiction.
B.2 Proof of Proposition 32
Suppose that S1 is a complete nonground support family w.r.t. O1 and let S be any instance of any
i
S  S1 ; then S = A  Ad  AC  Ad . By Lemma 31, T1d C
 T2d ; thus by Theorem 30, T1d 
502

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

T2 d as well. By definition of -instance inseparability, for all -ABoxes A and -assertions 
such that T1d  A |= , it holds that T2d  A |= ; hence T2d  A  Ad |= Q(~c). Consequently,
S = A  Ad is a (ground) support set of d w.r.t. O2 . If S2 is a complete nonground support family
w.r.t. O2 , it follows that S is an instance of some S   S2 . The converse membership is symmetric.
Hence, S1 and S2 are ground-identical.
B.3 Proof of Proposition 34
Towards a contradiction, assume some S   S \ SuppO (d) exists. Then a grounding  exists such
that S    Td 6|= d(X). However, S    Td |= d(X), as according to (f), S  is a nonground
support set for d w.r.t. Td = Td  lrw . Consequently, Td 6|= Td , which is a contradiction, because
Td  Td by construction in (c) and lrw = {C   D | Td |= C   D , Td 6|= C   D }  Td by
lhs
(d) and definition of cWTnrhs
 and cWTn .

Appendix C. Proofs for Section 5
C.1 Proof of Lemma 47

for d w.r.t. the ontology O =
The construction of support sets from a given hypergraph Gsupp(d),T
hT , Ai that we have presented mimics the DL-query unfolding over the TBox Td . We now formally
show that (i) each set S extracted in the described way is indeed a nonground support sets for d,
and (ii) for each ground instance S of a nonground support set S for d, a (nonground) support
set S  can be constructed following our procedure such that S    S for some suitable ground
substitution  . This proves that SG  S holds.
We first prove (i) by induction on the length n of incoming paths, from which the support sets
are extracted.

Base: n=1. Consider any path  in the hypergraph Gsupp(d),T
. Assume that there is a single
(hyper-) edge e in . By construction, this hyperedge must have xQ as a head node, i.e. head (e) =
xQ . There are four possibilities: (1) tail (e) = {xC }, (2) tail (e) = {xr , xC }, (3) tail (e) =
{xC , xD } or (4) tail (e) = {xr , }. We annotate the nodes of a path by variables as described
above, and extract the nonground atoms from labels and annotations of the nodes. As a result for
the case (1) we obtain {C(X0 )}, for (2): {r(X0 , X1 ), C(X1 )}, for (3): {C(X0 ), D(X0 )}, and for
(4): {r(X0 , X1 )}, where X1 is a fresh variable. By construction of the hypergraph the edges of the
forms (1)-(4) correspond to the TBox axioms C  Q, r.C  Q, C  D  Q and r.  Q
respectively. Therefore, the sets that have been constructed in all of the considered cases reflect the
DL-query unfoldings of d, and hence they represent nonground support sets for d by Proposition 24.
Induction step: Suppose that the statement is true for n, i.e. from a path with n edges all sets extracted in the way described above are nonground support sets for d. Consider a path  = e0 , . . . , en
with n+1 edges, and let e = e0 be the first edge of . By the induction hypothesis, all sets extracted
from the path  \ e = e1 , . . . , en following our approach are support sets for d. There are several
possibilities for the form of e: (1) tail (e) = {xC } and head (e) = {xD }, (2) tail (e) = {xr , xC }
and head (e) = {xD }, (3) tail (e) = {xC , xD } and head (e) = {xB }, (4) tail (e) = {xr , } and
head (e) = {xC }, or (5) tail (e) = {xC } and head (e) = {xr , xD }.
As for (1), by construction both xC and xD are annotated with Xi . Let S be a family of sets
extracted from \e. We pick a set S in which C(Xi ) occurs. We substitute C(Xi ) in S with D(Xi ),
and obtain a set S  . By the induction hypothesis S must be a support set for d. However, then

503

fiE ITER , F INK & S TEPANOVA

clearly S  is also a support set, as it mimics an additional unfolding step that accounts for the rule
C(X)  D(X) of the datalog rewriting of Td .
Let us look at (2). Assume a set S  D(Xi ) of nonground atoms has been constructed using
our procedure. Then Xi must be an annotation for xD . According to our construction {xr , xD } is
annotated with {hXi , Xj i, hXj i}, where Xj is a fresh variable. The sets S  that we get from  result
by substituting D(Xi ) in some S with {r(Xi , Xj ), C(Xj )}. The latter mimics the unfolding step
for Q that accounts for the rule D(Xi )  r(Xi , Xj ), C(Xj ) of the rewriting Td . As S is a support
set for d by the induction hypothesis, S  must be a support set for d as well. The cases (3)-(5) can
be analyzed analogously. Thus all sets of size n + 1 extracted from  are support sets for d.
It remains to prove (ii). Towards a contradiction, assume that some ground instance S of
some S  SuppO (d) exists, such that for each ground instance S   of every S   SuppO (d)
constructed by our procedure we have S   6 S. As S is a support set, by definition Td norm 
S |= Q(~c), thus by Lemma 23 Prog Q,Tdnorm  S |= Q(~c). This in turn means that Q(~c) has a
~ 0 and Sm = ,
backchaining proof S0 , S1 , . . . , Sm from Prog Q,Td norm S of the form S0 = Q(X)
~ 7 ~c, and Si = (Si1  Hi + Bi )i , i  1, where Hi  Bi is a
where 0 is the substitution X
rule resp. fact in Prog Q,Td norm  S and i is the most general unifier of Hi with some atom in Si1 .
Without loss of generality, we have Hi = A2 (oA2 ) if Hi1 = R2 (X, oA2 ) and all i such that Bi is
empty are at the end, i.e. at the positions k, k + 1, . . . , m. Then each Sj resp. Sj+1 , 0  j  k


amounts to an instance of a support set Sj resp. Sj+1
of d generated from Gsupp(d),T
. In particular,


Sk1 is an instance of Sk1 and consequently {Hk , Hk+1 , . . . , Hm } ( S) is an instance of Sk1





as well. But this means S   S for some instance S  of S = Sk1 , a contradiction.
C.2 Proof of Proposition 48
We prove the statement by induction on the number n of hyperedges with a singleton head node in

G = Gsupp(d),T
for the DL-atom DL[; Q](X).
Base: n = 0. We show that maxsup(d ) = 1 if no hyperedges of the required form exist in G.
Several cases are possible: (i) G contains only hyperedges of the form (xC , {xr , xD }); (ii) G has
only hyperedges of the form ({xr , }, xC ) or (xC , {xr , }); or (iii) G has no hyperedges.
(i) Consider some hyperedge in . Then some ej must exist in , such that head (ei )  tail (ej ).
The latter implies that ej is of the form ({xr , xD }, xD ) but then n 6= 0, i.e. contradiction.
For (ii) and (iii), by construction T contains only GCIs C  D such that C, D are either atomic
or of the form r.. These axioms fall into the DL-Lite core fragment, for which all  -minimal
support sets S have size at most 2; moreover, |S| = 2 reflects in DL-Lite core inconsistency arising
in the updated ontology (Eiter et al., 2014d). As negation is not available nor expressible in EL, no
such S exists and thus the maximal support set size for d is 1.
Induction Step: Suppose that the statement is true for n; we prove it for n+1. Let  = e1 , . . . , ek

with a maximal number n+1 of hyperedges with a singleton
be an incoming path to xQ in Gsupp(d),T
head node. Assume that ei is the first hyperedge of the required form occurring in . Let us
split  into two parts: e1 , . . . , ei and ei+1 , . . . , ek . Consider the hypergraph G  = (V, E  ), where
E  = E \ {e1 , . . . , ei }, and the TBox T  reconstructed from it. By the induction hypothesis,
maxsup(d ) w.r.t. O = hT  , Ai is bounded by n + 1. Now let the hypergraph G  = (V, E  ) with
E  = E   {ei } correspond to the TBox T  . By our assumption head (ei ) = xA , i.e. ei either
reflects B  C  A or r.B  A. Two cases are possible: either A = Q or A 6= Q. In the
504

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

x C n1

x Cn

x C nk

xCn

...

xCn

1

..
.

k

..
.
...
xQ

Figure 11: Fragment of a hypergraph used for illustration in the proof of Proposition 53
former case, ei is a single hyperedge on , i.e. n = 1. Support sets obtained from rewriting Q over
B  C  Q or r.B  Q are of size at most 2. The other support sets are constructed by combining
query rewritings of predicates occurring on the left hand side of GCIs reflected by ei ; each of these
rewritings has size at most 1 as shown in the base case. Thus the overall support set size for d w.r.t.
T  is bounded by 2  n + 1.
Suppose now that A 6= Q, i.e. ei reflects either B  C  A or r.B  A. By definition of an
incoming path a (hyper) edge ej must exist, such that head (ei )  tail (ej ). Moreover, note that ej
is a unique (hyper) edge connected to ei on , as otherwise the given hypergraph is tree-cyclic, i.e.
contradiction. We distinguish two cases: (1) head (ei ) = tail (ej ) and ej corresponds to A  . . . ;
(2) head (ei )  tail (ej ) and ej reflects A  B  . . . .
1. Consider a maximal support set S for d w.r.t. T  , and suppose A(Y )  S holds. By induction

hypothesis |S|  n. As G  = Gsupp(d),T
 is tree-acyclic, only a single atom over A might

occur in S. Adding the edge ei to G from S we obtain a support set S  with the atom A(Y )
substituted with atoms B(Y ) and C(Y ), or r(Z, Y ) and B(Z) as a result of an additional
query unfolding step. Hence the support set size of S  will be bounded by n + 2.
2. If ej reflects A  B  . . . , then a support set S  {A(Y ), B(Y )} must exist. By unfolding
the respective datalog rule, we get the bound n + 2 on the support set S  for d w.r.t. T  . 

C.3 Proof Sketch of Proposition 53
Observe that in tree-acyclic hypergraphs all nodes have a hyper out-degree at most 1, and hence
m(, G) = 0. Thus, if G is tree-acyclic, then by Proposition 48 the support set size for a given
DL-atom is bounded by n(, G)  0 + 1, which equals smax . We now show that the claimed bound
is also correct for tree-cyclic hypergraphs. Intuitively, m(, G) must be subtracted from n(, G) to
avoid that certain atoms in a support set are counted multiple times. Regarding the structure of the
support hypergraph we distinguish two cases: (i) no roles appear in a hypergraph; (ii) for all xr  G,
it holds that r 6 .
505

fiE ITER , F INK & S TEPANOVA

First we consider (i). Since only concepts appear in the support hypergraph by our assumption, all support sets will contain atoms in which only a single variable X0 occurs. Consider some
node xCn in  such that hdc+ (xCn ) = k, where k > 1, i.e., there are k outgoing hyperedges
from xCn containing nodes corresponding to concepts: ({xCn1 , xCn }, xCn ). . .({xCnk ,xCn }, xCn )
1
k
(see Figure 11). From support sets S  {Cn 1 (X0 ), . . . , Cn k (X0 )} we will get support sets S  
{Cn1 (X0 ), . . . , Cnk (X0 ), Cn (X0 )}. Estimating the maximal support set size as the number of hyperedges in the hypergraph, Cn (X0 ) is counted k times, but it appears only once (as its variable is
guaranteed to be X0 ). To avoid such multiple countings, m(, G) must be subtracted from n(, G).
Consider now (ii). By construction of G, for every hypernode {xr , xC }   edges e1 =
(xA , {xr , xC }) and e2 = ({xr , xC }, xB ) exist in G. Thus if xr occurs in , then consider a
support set S  {B(X)}. Rewriting the TBox axiom reflected by e2 , we get a datalog rule
B(X)  r(X, Y ), C(Y ). Then the axiom r.C  A reflected by e1 is rewritten to datalog rules
r(X, oC )  A(X); C(oC )  A(X). Unifying Y with oC we obtain an unfolding A(X). This
essentially shows that if no role occurring in a support hypergraph is in , then all support sets
involve only a single variable; in this case, as shown in (i), the provided bound is correct.
C.4 Proof of Proposition 57

The proof is by induction on the number n of (hyper) edges in G = Gsupp(d),T
. Base: n=0. If G has
no (hyper) edges, each node has one support set.
Induction step: Suppose the statement holds for n; we show it holds for G with n + 1 (hyper)
edges. Obviously, it holds for x  VR . As G is tree-acyclic and T is in normal form, G has a
node x such that hd+ (x) = d+ (x) = 0, i.e., there are no outgoing (hyper) edges, and hd (x) 6= 0
or d (x) 6= 0, i.e., there is some incoming (hyper) edge. As G is tree-acyclic, the rewriting of
the set Qx = {A(X)}, where x = xA consists of Qx and the rewritings of all sets Qtail(e) of
(hyper) nodes tail (e) such that head (e) = x. If tail (e) is {xB } (resp., {xB , xC }, {xr , xC }) these
are all rewritings of {B(X)} (resp. {B(X), C(X)}, {R(X, Y ), C(Y )}). That is, ws(xA ) is the
sum of the number of all rewritings of each Qtail(e) denoted Qtail(e) , plus 1. Consider now an
arbitrary e with head (e) = xA and let G  = G\e. As G  has n edges and is tree-acyclic, by the
induction hypothesis for each node x  V in G  , the value of ws(x), denoted wsG  (x), is as in (8).
Furthermore, ws(Qtail(e) ) and ws(x ), x 6= xA is in G  the same as in G. We thus get for x = xA :

wsG (x) = wsG  (x) + ws(Qtail(e) )
X Y
wsG  (x ) +
= 1+
T T

= 1+



(x)

x T

X

Y

wsG (x ) +

X

Y

wsG (x ) +

T T  (x) x T

X

ws(x ) + ws(Qtail(e) )

X

X

ws(x ) + ws(Qtail(e) )



(x),T 6VC ({x },T )E 

T T   (x),T 6VC ({x },T )E 

T T   (x) x T

= 1+

T T

X

X

X

ws(x )

T T  (x),T 6VC ({x },T )E



where T  (x) = {T | (T, {x})  E  } and E  = E \ {e}, and T  (x) is as above. To obtain
ws(Qtail (e)), we simply need to count the combinations of the rewritings of each node in tail (e),
and in case tail (e) = {xr , xB } (where ws(xr ) = 1), we need to add the number of rewritings of
the tail of each hyperedge (T, {xr , xB }) (as T is in normal form, T must be of the form {xC }).
506

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

C.5 Proof of Corollary 60
This Q
is immediate from Proposition 57: under the hypothesis, in (8) each T is of form {y}  VC ;
thus x T ws(x ) = ws(y), i.e., ws(tail (e)) and the rightmost term is 0.
C.6 Proof Sketch of Proposition 61
Under the condition on e and e1 , e2 , every set T  T  (x) in Equation (8)
Q such that |T| = {x, y} > 1
contains (at least) one element, say x, such that ws(x) = 1, and thus x T wsG (x ) equals ws(y)

. By an inductive argument, we then obtain that for every node xA  VC ,
in G = Gsupp(d),T
ws(xA )  1 is the number of distinct edges in G that occur on incoming paths to xA and any
xB  VC such that an edge ({xB }, {xr , xA }) is in E, plus the number of all such edges. This in
turn implies that for the query node xQ , ws(xq ) = |E| + 1 holds, as by construction each edge e  E
is among the respective edges for xQ . From this the result follows immediately.

Appendix D. Proofs for Section 6
D.1 Proof of Theorem 65
  . We can get to (h) only if I is an answer set of , and
Suppose SupRAnsSet outputs I = I|
if the foundedness check of I w.r.t. the ontology T  A , where A = A\H succeeded. It thus
remains to show that I is a compatible set for T  A , i.e., that for each DL-atom d in , d  Dp


iff I |=O d and d  Dn iff I 6|=O d. Towards a contradiction, suppose that this is not the case. In
(d) we partitioned the DL-atoms into two sets: Dp and Dn , corresponding to DL-atoms d guessed

 respectively, and set SIgr
 A). Since we assume that I is not
to be true and false in I,
to Gr(S, I,
compatible, one of the following must hold:

(1) For some DL-atom d in Dn , we have I |=O d. There are two possibilities: (i) either there is

a support set S  SIgr (d) or (ii) no support sets for d were identified. In case (i), we are guaranteed
that all support sets S for d are such that S  A 6= , since otherwise no hitting sets H are found in
(e). Hence there must exist some support set S such that S  A =
6 . According to (e) S  H 6= 
and thus S 6 Suppd (O ). Now as rep = true at (h), a post-check of d must have succeeded in (g),

i.e. I 6|=O d must hold. This is a contradiction. In case (ii), likewise post-evaluation of d must have
succeeded in (h), which again raises a contradiction.




(2) For some DL-atom d in Dp , we have I 6|=O d. Hence SIgr (d) = , d 6 Dp , and post-evaluation
is performed for d in (g). The latter, however, must have succeeded, as rep = true at (h); this is a
contradiction. Hence I is a compatible set for  , and thus a deletion repair answer set of .
D.2 Proof of Theorem 66
The following lemmas are useful to prove Theorem 66.
Lemma 69 Let I  ASx () where x  {flp, weak } and  = hT , A, Pi is a ground DL-program.
Then I = I  {ed | d  DL , I |=T A d}  {ned | d  DL , I 6|=T A d} is an answer set of ,
where DL is the set of all DL-atoms occurring in .
This lemma follows from a more general result on compatible sets as the basis of the evaluation
approach of HEX-programs in the DLVHEX-solver (cf. (Eiter et al., 2014a)).
507

fiE ITER , F INK & S TEPANOVA

 
Lemma 70 Let  = hT , A, Pi be a ground DL-program and let I  AS () such that I = I|

ASx (), where x  {flp, weak }. Suppose A  A is such that for each DL-atom d occurring in P,

it holds that I |=T A d iff I |=T A d. Then I  ASx ( ) where  = hT , A , Pi.
  , PxI,T A and PxI,T A coincide; as I  ASx (), it is a minimal
Proof. We note that for I = I|

model of PxI,T A . Consequently, I is also a model of PxI,T A . Moreover, I is minimal, as if some

J  I satisfies PxI,T A , then J |= PxI,T A ; hence I is not an answer set of PxI,T A , a contradiction.

Suppose I  RAS x (). This implies that I  ASx ( ) where  = hT  A , Pi, for some
 A. By Lemma 69 I is an answer set of  and thus is considered in (c). In (d), Dp and

Dn are set to the (correct) guess for I |=O d for each DL-atom d, where O = T  A . From
 A )(d) 6=  and
Proposition 15 and -completeness of S, we obtain for each d  Dp that Gr(S, I,


 A )(d) = . As Gr(S, I,
 A )(d)  Gr(S, I,
 A)(d) holds for each
for each d  Dn that Gr(S, I,

I

DL-atom d, it follows for each d  Dn and S  Sgr (d) that S  (A \ A ) 6= ; this means that
S

H  = A \ A is a hitting set of d Dn SIgr (d ), and hence some minimal hitting set H  H  will

A



be considered in (e). In (f), Dp will be set to Dp as for each d  Dp some S  SIgr (d) exists
such that S  H  = , and hence S  H = . Thus in (g) the call eval p (   ) yields true, and
 A \ H)(d) = ; thus rep is true. Eventually, in (h) the
likewise the call eval n (   ) as Gr(S, I,

test flpFND(I, hT  A\H, Pi) will succeed, as I is an x-answer set of  = hT  A , Pi, and by
  is output. 
Lemma 70 also of  = hT  A \ H, Pi, as A  A \ H. Thus in step (h) I = I|
D.3 Proof of Proposition 67
   RAS weak (). Towards a contradiction,
We first show that for every I  AS (1 ), it holds that I|


suppose some I  AS (1 ) exists such that I| 6 RAS weak (). Then for every A  A we have
  6 AS weak ( ) with  = hT , A , Pi. In particular, for A = A\{P (~c) | pP (~c)  I}
that I|
  6 AS weak ( ) with  = hT , A , Pi There are several possibilities: (i) no
it holds that I|
c ; (ii) no such
  with a guess for the replacement atoms ed , ned is a model of 
extension of I|
  ,O
  is a compatible set for  ; (iii) some interpretation J  I|
  is a model of P I|
.
extension of I|
weak

c and hence it follows that I|
c .
 |= 
The case (i) is impossible:  = 

 . Towards a contradiction, assume that
Assume that (ii) is true. Consider the interpretation I|

 , or
  |=O d and ned  I|
it is not compatible for  . Then for some DL-atom d either (1) I|



O
O
 holds. In case (1), as I|
  |=
d, and ed  I|
d, some support set S for d that is
(2) I 6|=

 exists. Now consider whether S  Sd or S 6 Sd . In the former case, S must
coherent with I|

contain ABox assertions SdA , as otherwise some constraint of the form (r5 ) is violated. Due to the
rule (r6 ) at least one assertion Pid in SdA must be marked for deletion. Note that then Pid is not
present in A , and S is not a relevant support set for d w.r.t. A . If Sd is known to be complete, then
we immediately arrive at a contradiction. Otherwise, the rule of the form (r8 ) is applied, and as the
evaluation postcheck for d succeeded by our assumption, we get a contradiction. If S 6 Sd , then
Sd is not known to be complete, and again the rule of the form (r8 ) is applied; due to the successful
  6|=O d, no
evaluation postcheck, a contradiction is obtained. Now suppose that (2) is true. As I|
  . If Sd is known to be complete, then the
support set for d exists w.r.t. O that is coherent with I|
508

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

constraint (r9 ) is violated; but this contradicts I |= 1 . Thus, the body of the rule (r7 ) is satisfied,
and an evaluation postcheck is issued for d that fails; hence we get a contradiction.
  ,O
  is a model of P I|
Finally, assume that (iii) holds, i.e. some interpretation J  I|
weak . The
  \J contains only atoms over the signature of . Let us consider IM = I \ M .
set M = I|



We know that I  AS (1 ); Hence some rule rI must exist in 1 I such that IM |= B(rI ),
gl

gl

gl

I
but IM 6|=
Recall that 1 = (  R  f acts(A)  COMP). Now rgl can not be in
  ,O
  ,O
  ,O

I|
I  P I|
I  P I|

(  f acts(A)  COMP)Igl , as rgl
iff rgl
weak
weak and J 6|= Pweak by construction
I must be in RI . However, the
of the GL and weak reducts, which is a contradiction. Therefore, rgl
gl

I
latter also raises a contradiction: no rule in Rgl has atoms over the signature of  in its head and IM

and I coincide on the rule head; thus it follows I 6|= B(RIgl ), which is a contradiction. Therefore,
   AS ( ) holds, and we have a global contradiction, i.e. I|
   RAS weak () follows.
I|
I ).
H(rgl

We now consider the case where each support family Sd is known to be complete, and prove
that then AS| (1 ) = RAS weak (). From what has been shown above, it remains to check that
AS| (1 )  RAS weak (). Towards a contradiction, assume some I  RAS weak () exists such
that I 
/ AS (1 ) for every extension I of I. As I  RAS weak (), some ABox A  A exists such
that I  AS ( ) with  = hT , A , Pi. We construct an extension I of I as follows:


I = I  {ed | I |=O d}  {ned | I 6|=O d} 

{pP (~c) | P (~c)  A\A }  f acts(A)  COMP 
{Supd (~c) | d(~c) has some support set from Sd coherent with I} 
{S P (~c) | I |= rb(S A,P (~c))}  {S A,P (~c) | I |= rb(S A,P (~c)), nd(S A,P (~c))}.
d

d

d

d

Since by our assumption I 6 AS (1 ), one of the following must hold:

(i) I |6 = (  R  f acts(A)  COMP)Igl or

(ii) some J  I exists, such that J |= (  R  f acts(A)  COMP)Igl .

 it satisfies  and all rules of the forms (r )-(r ).
First assume that (i) is true. By construction of I,
1
4
Moreover, constraints of the form (r5 ) can not be violated, as no DL-atom d(~c) with I 6|=O d(~c)
can have a support set that consists only of input assertions. The rules (r7 ) and (r8 ) are not present

in the reduct 1 Igl , as I |= Cd for each DL-atom d(~c).

Thus the rule r from 1 such that I 6|= rI could only be of the form (r ) or (r ). In case of
gl



6

9

form (r6 ), some DL-atom d(~c) would exist such that I 6|=O d(~c). By Proposition 15 no support set
 Hence, r must be of
for d(~c) would exist that is coherent with I, and by construction SdA,P (~c) 
/ I.

the form (r9 ); however, as I |=O d(~c) by completeness of Sd and Proposition 15, by construction
 which implies that r can not be violated.
we have Supd (~c)  I,

Now let (ii) hold, i.e. some J  I exists s.t. J |= 1 Igl . As J contains for each DL-atom
d(~c) exactly one out of ed (~c) and ned (~c) and 1 contains ed (~c)  ned (~c), the interpretations J and
I coincide on all replacement atoms ed (~c) and ned (~c). Suppose that I \ J contains some atoms
  6|= P I,O ; hence some rule rI,O  P I,O , exists such that
from the language of . Then J|
weak
weak
weak
  6|= H(rI,O ). Consider the respective rule rI in I . As J 6|= H(rJ ),
  |= B(rI,O ), but J|
J|
gl
gl
gl
weak
weak
509

fiE ITER , F INK & S TEPANOVA

I ). By construction of the weak and GL reduct, respectively, the positive
we must have J 6|= B(rgl






J ) and in B(r I,O ) are the same. Hence, some replacement atom e (~
normal atoms in B(rgl
d c) (resp.
weak

I




ned (~c)) must occur positively in B(rgl ), such that ed (~c)  I \ J (resp. ned (~c)  I \ J). As we have
already argued, the latter is not possible, leading to a contradiction.
I of
Consequently, I \ J must contain only atoms from the language of R. For every rule rgl


form (r ) or (r ) we have J |= B(rI ) iff I |= B(rI ), thus I and J agree on all atoms S P (~c) and
3
A,P
Sd (~c).

4

gl

gl

d

Similarly, via (r1 ) and (r2 ) we must have that I and J agree on all atoms Supd (~c). Finally,
 In conclusion,
the same holds for all pP (~c) and pP (~c) by the rules (r6 ) and the construction of I.


J = I holds, which violates (ii).
Thus, it follows that I  AS (1 ). Consequently, AS (1 )  RAS weak (1 ) holds; this proves
the result.

References
Alchourrn, C. E., Grdenfors, P., & Makinson, D. (1985). On the logic of theory change: Partial
meet contraction and revision functions. J. Symbolic Logic, 50(2), 510530.
Aranguren, M. E., Bechhofer, S., Lord, P. W., Sattler, U., & Stevens, R. D. (2007). Understanding
and using the meaning of statements in a bio-ontology: recasting the gene ontology in OWL.
BMC Bioinformatics, 8(1), 113.
Ausiello, G., DAtri, A., & Sacc, D. (1983). Graph algorithms for functional dependency manipulation. J. of the ACM, 30(4), 752766.
Ausiello, G., DAtri, A., & Sacc, D. (1986). Minimal representation of directed hypergraphs. SIAM
J. on Computing, 15(2), 418431.
Baader, F., Bauer, A., & Lippmann, M. (2009). Runtime verification using a temporal description
logic. In Proc. 7th Intl Symp. on Frontiers of Combining Systems, FroCoS 2009, pp. 149164.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing the EL envelope. In Proc. 19th Intl Joint Conf.
on Artificial Intelligence, IJCAI 2005, pp. 364369.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.). (2003). The
Description Logic Handbook: Theory, Implementation and Applications. Cambridge University Press, 2003.
Baader, F., Lutz, C., Milicic, M., Sattler, U., & Wolter, F. (2005). Integrating description logics and
action formalisms: First results. In Proc. 20th National Conf. Artificial Intelligence and 17th
Conf. Innovative Applications of Artificial Intelligence, pp. 572577.
Balduccini, M., & Gelfond, M. (2003). Logic programs with consistency-restoring rules. In Intl
Symp. Logical Formalization of Commonsense Reasoning, AAAI 2003 Spring Symposium Series, pp. 918.
Belnap, N. (1977). A useful four-valued logic. In Modern Uses of Multiple-Valued Logic, pp. 737.
Reidel Publishing Company, Boston.
Bertossi, L. E. (2011). Database Repairing and Consistent Query Answering. Morgan & Claypool
Publishers, Ottawa, Canada.
510

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

Bertossi, L. E., Hunter, A., & Schaub, T. (2005). Introduction to inconsistency tolerance. In Inconsistency Tolerance [result from a Dagstuhl seminar], pp. 114.
Bienvenu, M. (2008). Complexity of abduction in the EL family of lightweight description logics.
In Proc. 11th Intl Conf. on Principles of Knowledge Representation and Reasoning, KR 2008,
pp. 220230.
Bienvenu, M., & Rosati, R. (2013). New inconsistency-tolerant semantics for robust ontology-based
data access. In Proc. 26th Intl Workshop on Description Logics, pp. 5364.
Bonatti, P. A., Faella, M., & Sauro, L. (2010). EL with default attributes and overriding. In Proceedings of the 9th Intl Semantic Web Conf., ISWC 2010, pp. 6479.
Brewka, G. (1989). Preferred subtheories: An extended logical framework for default reasoning. In
Proc. 11th Intl Joint Conf. on Artificial Intelligence, IJCAI 1989, pp. 10431048.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., & Rosati, R. (2007a).
Ontology-based database access. In Proc. 15th Italian Symposium on Advanced Database
Systems, SEBD 2007, pp. 324331.
Calvanese, D., De Giacomo, G., Lenzerini, M., Lembo, D., Poggi, A., & Rosati, R. (2007b).
MASTRO-I: efficient integration of relational data through DL ontologies. In Proc. 20th
Intl Workshop on Description Logics.
Calvanese, D., Ortiz, M., Simkus, M., & Stefanoni, G. (2013). Reasoning about explanations for
negative query answers in DL-Lite. J. Artificial Intelligence Research, 48, 635669.
Console, L., Sapino, M. L., & Dupr, D. T. (1995). The role of abduction in database view updating.
J. of Intelligent Information Systems, 4(3), 261280.
Console, M., Mora, J., Rosati, R., Santarelli, V., & Savo, D. F. (2014). Effective computation of
maximal sound approximations of description logic ontologies. In Proc. 13th Intl Semantic
Web Conf., ISWC 2014, Part II, pp. 164179.
dlliteplugin of DLVHEX system (2015). https://github.com/hexhex/dlliteplugin.
Scripts for DL-program benchmark generation (2015).
dlliteplugin/benchmarks.

https://github.com/hexhex/

dlplugin of the DLVHEX system (2015). https://github.com/hexhex/dlplugin.
DR E W reasoner for DL-Programs over Datalog-rewritable Description Logics (2012). http://
www.kr.tuwien.ac.at/research/systems/drew/.
Ecke, A., Ludwig, M., & Walther, D. (2013). The concept difference for EL-terminologies using
hypergraphs. In Proc. Intl Workshop on Document Changes: Modeling, Detection, Storage
and Visualization.
Eiter, T., Erdem, E., Fink, M., & Senko, J. (2005). Updating action domain descriptions. In Proc.
19th Intl Joint Conf. on Artificial Intelligence, IJCAI 2005, pp. 418423.
Eiter, T., Fink, M., Greco, G., & Lembo, D. (2008). Repair localization for query answering from
inconsistent databases. ACM Transactions on Database Systems, 33(2).
Eiter, T., Fink, M., Krennwallner, T., Redl, C., & Schller, P. (2014a). Efficient HEX-program
evaluation based on unfounded sets. J. Artificial Intelligence Research, 49, 269321.
511

fiE ITER , F INK & S TEPANOVA

Eiter, T., Fink, M., Redl, C., & Stepanova, D. (2014b). Exploiting support sets for answer set
programs with external evaluations. In Proc. 28th Conf. Artificial Intelligence, AAAI 2014,
pp. 10411048.
Eiter, T., Fink, M., & Stepanova, D. (2013). Data repair of inconsistent DL-programs. In Proc. 23rd
Intl Joint Conf. on Artificial Intelligence, IJCAI 2013, pp. 869876.
Eiter, T., Fink, M., & Stepanova, D. (2014c). Computing repairs for inconsistent DL-programs over
EL ontologies. In Proc. 14th Joint European Conf. Logics in Artificial Intelligence, JELIA
2014, pp. 426441.
Eiter, T., Fink, M., & Stepanova, D. (2014d). Towards practical deletion repair of inconsistent
DL-programs. In Proc. 21st European Conf. Artificial Intelligence, ECAI 2014, pp. 285290.
Eiter, T., Fink, M., & Stepanova, D. (2014d). Data repair of inconsistent DL-programs. Tech. rep.
INFSYS RR-1843-15-03, Institut f. Informationssysteme, TU Wien, A-1040 Vienna, Austria.
Eiter, T., Gottlob, G., & Leone, N. (1997). Abduction from logic programs: Semantics and complexity. Theoretical Computer Science, 189(1-2), 129177.
Eiter, T., Ianni, G., Lukasiewicz, T., Schindlauer, R., & Tompits, H. (2008). Combining answer
set programming with description logics for the Semantic Web. J. Artificial Intelligence,
172(12-13), 14951539.
Eiter, T., Ianni, G., Schindlauer, R., & Tompits, H. (2005). A uniform integration of higher-order
reasoning and external evaluations in answer-set programming. In Proc. 19th Intl Joint Conf.
on Artificial Intelligence, IJCAI 2005, pp. 9096.
Eiter, T., & Makino, K. (2007). On computing all abductive explanations from a propositional Horn
theory. J. of the ACM, 54(5).
Eiter, T., Schneider, P., imkus, M., & Xiao, G. (2014). Using OpenStreetMap data to create benchmarks for description logic reasoners. In Proc. 2nd Intl Workshop on OWL Reasoner Evaluation, ORE 2014, Vol. 1207, pp. 5157.
Experimental data with inconsistent DL-programs (2015). http://www.kr.tuwien.ac.at/
staff/dasha/jair_el/benchmark_instances.zip.
Feng, S., Ludwig, M., & Walther, D. (2015). The logical difference for EL: From terminologies
towards tboxes. In Proc. 1st Intl Workshop on Sem. Technologies, IWOST 2015, pp. 3141.
Fink, M. (2012). Paraconsistent hybrid theories. In Proc. 13th Intl Conf. on Principles of Knowledge Representation and Reasoning, KR 2012, pp. 141151.
Frhstck, M., Phrer, J., & Friedrich, G. (2013). Debugging answer-set programs with ouroboros extending the sealion plugin. In Proc. 12th Intl Conf. Logic Programming and Nonmonotonic
Reasoning, LPNMR 2013, pp. 323328.
Gallo, G., Longo, G., & Pallottino, S. (1993). Directed hypergraphs and applications. Discrete
Applied Mathematics, 42(2), 177201.
Grdenfors, P., & Rott, H. (1995). Belief revision. Handbook of Logic in Artificial Intelligence and
Logic Programming, 4, 35132.
Gardiner, T., Tsarkov, D., & Horrocks, I. (2006). Framework for an automated comparison of
description logic reasoners. In Proc. 5th Intl Semantic Web Conf., ISWC 2006, pp. 654667.
512

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

Gebser, M., Phrer, J., Schaub, T., & Tompits, H. (2008). A meta-programming technique for
debugging answer-set programs. In Proc. 23rd Conf. Artificial Intelligence, AAAI 2008, pp.
448453.
Gelfond, M., & Lifschitz, V. (1991). Classical negation in logic programs and disjunctive databases.
New Generation Computing, 9, 365385.
Gottlob, G., Pichler, R., & Wei, F. (2007). Efficient datalog abduction through bounded treewidth.
In Proc. 22nd Intl Conf. on Artificial Intelligence, AAAI 2007, pp. 16261631.
Grau, B. C., Horrocks, I., Kazakov, Y., & Sattler, U. (2007). Just the right amount: extracting
modules from ontologies. In Proc. 16th Intl Conf. World Wide Web, WWW 2007, pp. 717
726.
Hansen, P., Lutz, C., Seylan, I., & Wolter, F. (2014). Query rewriting under EL TBoxes: Efficient
algorithms. In Proc. 27th Intl Workshop on Description Logics, pp. 197208.
Hermann, M., & Pichler, R. (2010). Counting complexity of propositional abduction. J. Computer
and System Sciences, 76(7), 634649.
HTCondor load distribution system, version 7.8.7 (2012). http://research.cs.wisc.edu/
htcondor/.
Huang, S., Hao, J., & Luo, D. (2014). Incoherency problems in a combination of description logics
and rules. J. Applied Mathematics, 604753:16.
Huang, S., Li, Q., & Hitzler, P. (2013). Reasoning with inconsistencies in hybrid MKNF knowledge
bases. Logic J. of the IGPL, 21(2), 263290.
Kaminski, T., Knorr, M., & Leite, J. (2015). Efficient paraconsistent reasoning with ontologies and
rules. In Proc. 24th Intl Joint Conf. on Artificial Intelligence, IJCAI 2015, pp. 30983105.
Knorr, M., Alferes, J. J., & Hitzler, P. (2008). A coherent well-founded model for hybrid MKNF
knowledge bases. In Proc. 18th European Conf. on Artificial Intelligence, ECAI 2008, pp.
99103.
Knorr, M., Alferes, J. J., & Hitzler, P. (2011). Local closed world reasoning with description logics
under the well-founded semantics. Artificial Intelligence, 175(9-10), 15281554.
Konev, B., Ludwig, M., Walther, D., & Wolter, F. (2012). The logical difference for the lightweight
description logic EL. J. Artificial Intelligence Research, 44, 633708.
Kontchakov, R., Lutz, C., Toman, D., Wolter, F., & Zakharyaschev, M. (2010). The combined
approach to query answering in DL-Lite. In Proc. 12th Intl Conf. on Principles of Knowledge
Representation, KR 2010, pp. 247257.
Kotek, T., Simkus, M., Veith, H., & Zuleger, F. (2014). Towards a description logic for program
analysis: Extending ALCQIO with reachability. In Proc. 27th Intl Workshop on Description
Logics, pp. 591594.
Lembo, D., Lenzerini, M., Rosati, R., Ruzzi, M., Savo, D. F. (2015). Inconsistency-tolerant query
answering in ontology-based data access. J. Web Sem., 33, 329.
Lembo, D., Santarelli, V., & Savo, D. F. (2013). A graph-based approach for classifying OWL 2 QL
ontologies. In Proc. 26th Intl Workshop on Description Logics, pp. 747759.
LUBM benchmark (2005). http://swat.cse.lehigh.edu/projects/lubm/.
513

fiE ITER , F INK & S TEPANOVA

LUBM data generator (2013). http://code.google.com/p/combo-obda/.
Ludwig, M., & Walther, D. (2014). The logical difference for ELHr-terminologies using hypergraphs. In Proc. 21st European Conf. Artifical Intelligence, ECAI 2014, pp. 555560.
Lukasiewicz, T. (2010). A novel combination of answer set programming with description logics
for the semantic web. IEEE Trans. Knowledge and Data Engineering, 22(11), 15771592.
Lutz, C., Toman, D., & Wolter, F. (2009). Conjunctive query answering in the description logic EL
using a relational database system. In Boutilier, C. (Ed.), Proc. 21st Joint Intl Conf. Artificial
Intelligence, IJCAI 2009, pp. 20702075.
Lutz, C., Walther, D., & Wolter, F. (2007). Conservative extensions in expressive description logics.
In Proc. 20th Intl Joint Conf. Artificial Intelligence, IJCAI 2007, pp. 453458.
Lutz, C., & Wolter, F. (2010). Deciding inseparability and conservative extensions in the description
logic EL. J. of Symbolic Computation, 45(2), 194228.
Martinez, M. V., Molinaro, C., Subrahmanian, V. S., & Amgoud, L. (2013). A General Framework
for Reasoning On Inconsistency. Springer Briefs in Computer Science. Springer, 2013.
Masotti, G., Rosati, R., & Ruzzi, M. (2011). Practical abox cleaning in DL-Lite (progress report).
In Proc. of Description Logics Workshop.
Motik, B., & Rosati, R. (2010). Reconciling Description Logics and Rules. J. of the ACM, 57(5),
162.
MyITS - Personalized Intelligent Mobility Service (2012). http://www.kr.tuwien.ac.at/
research/projects/myits/GeoConceptsMyITS-v0.9-Lite.owl/.
Nguyen, N. T. (2008). Advanced Methods for Inconsistent Knowledge Management. Advanced
Information and Knowledge Processing. Springer.
Nortje, R., Britz, A., & Meyer, T. (2013). Module-theoretic properties of reachability modules for
SRIQ. In Proc. 26th Intl Workshop on Description Logics, pp. 868884.
Oetsch, J., Phrer, J., & Tompits, H. (2012). Stepwise debugging of description-logic programs. In
J. of Correct Reasoning, pp. 492508.
Open Street Map project (2012). http://www.openstreetmap.org/.
zccep, . L., & Mller, R. (2012). Combining DL  Lite with spatial calculi for feasible geothematic query answering. In Proc. 25th Intl Workshop on Description Logics.
Pan, J. Z., & Thomas, E. (2007). Approximating OWL-DL ontologies. In Proc. 22nd Intl Conf. on
Artificial Intelligence, AAAI 2007, pp. 14341439.
Prez-Urbina, H., Motik, B., & Horrocks, I. (2010). Tractable query answering and rewriting under
description logic constraints. J. of Applied Logic, 8(2), 186209.
Phrer, J., Heymans, S., & Eiter, T. (2010). Dealing with inconsistency when combining ontologies
and rules using DL-programs . In Proc. 7th Extended Semantic Web Conf., ESWC 2010, part
I, pp. 183197.
R ACER P RO reasoner for OWL ontologies (2007). http://franz.com/agraph/racer/.
Reiter, R. (1987). A theory of diagnosis from first principles. J. Artificial Intelligence, 32(1), 5795.
514

fiC OMPUTING R EPAIRS OF I NCONSISTENT DL-P ROGRAMS OVER EL O NTOLOGIES

Rosati, R. (2007). On conjunctive query answering in EL. In proceedings of the 20th Intl Workshop
on Description Logics.
Rosati, R., Ruzzi, M., Graziosi, M., & Masotti, G. (2012). Evaluation of techniques for inconsistency handling in OWL 2 QL ontologies. In Proc. 11th Intl Semantic Web Conf., ISWC 2012,
pp. 337349.
Sakama, C., & Inoue, K. (1995). Paraconsistent stable semantics for extended disjunctive programs.
J. of Logic and Computation, 5(3), 265285.
Sakama, C., & Inoue, K. (2003). An abductive framework for computing knowledge base updates.
Theory and Practice of Logic Programming, 3(6), 671713.
Schulz, C., Satoh, K., & Toni, F. (2015). Characterising and explaining inconsistency in logic
programs. In Proc. 13th Intl Conf., LPNMR 2015, pp. 467479.
Schulz, S., Cornet, R., & Spackman, K. A. (2011). Consolidating SNOMED CTs ontological
commitment. Applied Ontology, 6(1), 111.
Shen, Y.-D. (2011). Well-supported semantics for description logic programs. In Proc. 22nd Intl
Joint Conf. on Artificial Intelligence, IJCAI 2011, pp. 10811086.
Stefanoni, G., Motik, B., & Horrocks, I. (2012). Small datalog query rewritings for EL. In Proc.
25th Intl Workshop on Description Logics.
Stepanova, D. (2015). Inconsistencies in Hybrid Knowledge Bases. PhD thesis, Vienna University
of Technology.
Steve, G., Gangemi, A., & Mori, A. R. (1995). Modelling a sharable medical concept system:
Ontological foundation in galen. In AIME, pp. 411412.
Stuckenschmidt, H., Parent, C., & Spaccapietra, S. (Eds.). (2009). Modular Ontologies: Concepts,
Theories and Techniques for Knowledge Modularization, Vol. 5445 of Lecture Notes in Computer Science. Springer.
Syrjnen, T. (2006). Debugging Inconsistent Answer-Set Programs. In Proc. 11th Intl Workshop
on Nonmonotonic Reasoning, NMR 2006, pp. 7783.
Thakur, M., & Tripathi, R. (2009). Linear connectivity problems in directed hypergraphs. Theoretical Computer Science, 410(27-29), 25922618.
Tserendorj, T., Rudolph, S., Krtzsch, M., & Hitzler, P. (2008). Approximate OWL-reasoning with
Screech. In Proc. 2nd Intl Conf. Web Reasoning and Rule Systems, RR 2008, pp. 165180.
Wache, H., Groot, P., & Stuckenschmidt, H. (2005). Scalable instance retrieval for the semantic web
by approximation. In Proc. 1st Intl Workshops on Web Information Systems Engineering,
WISE 2005, pp. 245254.
Wang, Y., You, J.-H., Yuan, L.-Y., & Shen, Y.-D. (2010). Loop formulas for description logic
programs. Theory and Practice of Logic Programming, 10(4-6), 531545.
Xiao, G. (2014). Inline Evaluation of Hybrid Knowledge Bases. Ph.D. thesis, Vienna University of
Technology, Austria.
Zhao, Y., Pan, J. Z., & Ren, Y. (2009). Implementing and evaluating a rule-based approach to
querying regular EL+ ontologies. In Proc. 9th Intl Conf. Hybrid Intelligent Systems, HIS
2009, pp. 493498.

515

fiJournal of Artificial Intelligence Research 56 (2016) 1-59

Submitted 06/15; published 05/16

Query and Predicate Emptiness in Ontology-Based Data Access
Franz Baader

FRANZ . BAADER @ TU - DRESDEN . DE

TU Dresden, Germany

Meghyn Bienvenu

MEGHYN @ LIRMM . FR

CNRS, Universite de Montpellier
& INRIA, France

Carsten Lutz

CLU @ UNI - BREMEN . DE

University of Bremen, Germany

Frank Wolter

WOLTER @ LIVERPOOL . AC . UK

Department of Computer Science
University of Liverpool, UK

Abstract
In ontology-based data access (OBDA), database querying is enriched with an ontology that
provides domain knowledge and additional vocabulary for query formulation. We identify query
emptiness and predicate emptiness as two central reasoning services in this context. Query emptiness asks whether a given query has an empty answer over all databases formulated in a given
vocabulary. Predicate emptiness is defined analogously, but quantifies universally over all queries
that contain a given predicate. In this paper, we determine the computational complexity of query
emptiness and predicate emptiness in the EL, DL-Lite, and ALC-families of description logics,
investigate the connection to ontology modules, and perform a practical case study to evaluate the
new reasoning services.

1. Introduction
In recent years, the paradigm of ontology-based data access (OBDA) has gained increased popularity. The general idea is to add an ontology to database querying to provide domain knowledge
and to enrich the vocabulary that is available for the formulation of queries. This is particularly
useful when the data to be queried is highly incomplete and when multiple data sources with diverging vocabularies are integrated (Poggi, Lembo, Calvanese, De Giacomo, Lenzerini, & Rosati,
2008). OBDA has been taken up with particular verve in the area of description logic (DL) where it
has been studied intensively both for lightweight DLs such as the members of the DL-Lite and EL
families, which are tractable regarding the data complexity of querying, and for more expressive
DLs such as those of the ALC and SHIQ families where querying is intractable in data complexity. For the use in OBDA of the former, see for example the work of Calvanese, De Giacomo,
Lembo, Lenzerini, Poggi, Rodriguez-Muro, and Rosati (2009), Lutz, Toman, and Wolter (2009),
Perez-Urbina, Motik, and Horrocks (2009), Chortaras, Trivela, and Stamou (2011) and Eiter, Ortiz,
Simkus, Tran, and Xiao (2012), and the surveys by Krotzsch (2012) and Kontchakov, RodriguezMuro, and Zakharyaschev (2013); for the latter, see the work of Glimm, Lutz, Horrocks, and Sattler
(2008), Ortiz, Calvanese, and Eiter (2008), and Bienvenu, ten Cate, Lutz, and Wolter (2014) as well
as the references given in the work of Ortiz and Simkus (2012).
c
2016
AI Access Foundation. All rights reserved.

fiBAADER , B IENVENU , L UTZ , & W OLTER

The OBDA approach is fueled by the availability of ontologies that aim at providing a standard
vocabulary for the targeted application domain. In particular, there are now many such ontologies
in the bio-medical domain such as SNOMED CT (IHTSDO, 2016), NCI (Golbeck, Fragoso, Hartel, Hendler, Oberthaler, & Parsia, 2003), and GO (Gene Ontology Consortium, 2016), which are
all formulated in a DL and allow a comparably inexpensive adoption of OBDA in bio-medical applications such as querying electronic medical records (Patel, Cimino, Dolby, Fokoue, Kalyanpur,
Kershenbaum, Ma, Schonberg, & Srinivas, 2007). Ontologies of this kind typically have a very
broad coverage and their vocabulary often contain tens or even hundreds of thousands of predicates
that embrace various subject areas such as anatomy, diseases, medication, and even social context
and geographic location. In any given application, only a small fragment of the ontologys vocabulary will actually occur in the data. Still, the remaining predicates are potentially very useful for
formulating queries as they are linked to the data vocabulary by the ontologythis is precisely how
OBDA enriches the vocabulary available for query formulation.
Due to the size and complexity of the involved ontologies and vocabularies, however, it can
be difficult to understand which of the additional predicates are useful for query formulation and
how to write meaningful queries in the extended vocabulary. Static analysis tools for analyzing these
queries would thus be very useful. In this paper, we consider the fundamental static analysis problem
of query emptiness as well as a natural variation of it called predicate emptiness. In the former,
the problem is to decide whether a given query q provides an empty answer over all databases
formulated in a given data vocabulary . Query emptiness thus helps to identify queries which are
useless due to wrong use of the ontology vocabulary. It is a standard static analysis problem in many
subareas of database theory such as XML, see e.g. the work of Benedikt, Fan, and Geerts (2008)
and references therein.
As an example, consider the following simple ontology O:
DiabetesPatient  Patient u has disease.Diabetes
DiabetesPatientwithoutMedication  Patient u has disease.Diabetes u
on medication for.Diabetes
Assume that O is used to support querying of a medical patient database which has a unary table
for the concept names Patient and Diabetes and binary tables for the role names has disease and
on medication for, distinguishing in particular between diabetes of type 1 and type 2. For example,
the database could be given by the following set A of assertions:
Patient(a),
has disease(a, type1),

Patient(b),

on medication for(a, type1),

Diabetes(type1),

has disease(b, type2),

Diabetes(type2).

Thus, a is a patient with diabetes of type 1 who is on medication for it and b is a patient with
diabetes of type 2. In OBDA, queries are interpreted under an open world assumption and thus one
is interested in the certain answers to a query q w.r.t. O and A, that is, the answers to q on which
all extension of A that satisfy the ontology O are in agreement. For the concrete query
q1 (x) = DiabetesPatient(x),
a and b are the certain answers to q1 (x) w.r.t. O and Adespite the fact that the predicate DiabetesPatient used in q1 (x) does not occur in A. Since A is formulated using only the data vocabulary
 = {Patient, has disease, on medication for, Diabetes},
2

fiQ UERY AND P REDICATE E MPTINESS

Q UERY EVALUATION

E MPTINESS

DL

IQ

CQ

IQ-query / CQ-predicate

CQ-query

EL

PT IME-c.

NP-c.

PT IME-c.

PT IME-c

EL

PT IME-c.

NP-c.

E XP T IME-c.

E XP T IME-c.

ELI

E XP T IME-c.

E XP T IME-c.

E XP T IME-c.

E XP T IME-c.

Horn-ALCIF

E XP T IME-c.

E XP T IME-c.

E XP T IME-c.

E XP T IME-c.

NL OG S PACE-c.

NP-c.

NL OG S PACE-c.

coNP-c.

PT IME-c.

NP-c.

coNP-c.

coNP-c.

ALC

E XP T IME-c.

E XP T IME-c.

NE XP T IME-c.

NE XP T IME-c.

ALCI

E XP T IME-c.

2E XP T IME-c.

NE XP T IME-c.

2E XP T IME-c.

ALCF

E XP T IME-c.

E XP T IME-c.

undecidable

undecidable

DL-Litecore|F |R
DL-Litehorn

Figure 1: Known complexity results for query evaluation and new complexity results for emptiness

we say that q1 (x) is non-empty for  given O. We regard this as evidence that q1 (x) is a potentially
useful query on databases formulated in vocabulary . As another example, consider the query
q2 (x) = DiabetesPatientwithoutMedication(x)
There is no certain answer to q2 (x) w.r.t. O and A since, under the open world assumption, the mere
absence of the information that b is on medication for diabetes of type 1 or type 2 does not imply
that its negation is true. One can even show that, whatever database A0 formulated in vocabulary 
we use, there will never be any certain answer to q2 (x) w.r.t. O and A0 . In this case, we say that
q2 (x) is empty for  given O. In contrast to q1 (x), this query is thus useless on -databases.
We also consider predicate emptiness, the problem to decide whether for a given predicate p
and data vocabulary , it is the case that all queries q which involve p yield an empty answer over
all -databases. In the example above, the predicate DiabetesPatientwithoutMedication is empty
w.r.t. O and  for the important class of conjunctive queries (queries constructed from atomic formulas using conjunction and existential quantification). Predicate emptiness can be used to identify
predicates in the ontology that are useless for query formulation, before even starting to construct
a concrete query. In a graphical user interface, for example, such predicates would not be offered
to users for query formulation. Our notion of predicate emptiness is loosely related to predicate
emptiness in datalog queries as studied e.g. by Vardi (1989) and Levy (1993).
The aim of this paper is to perform a detailed study of query emptiness and predicate emptiness
for various DLs including members of the EL, DL-Lite, and ALC families, concentrating on the
two most common query languages in DL-based OBDA: instance queries (IQs) and conjunctive
queries (CQs). For all of the resulting combinations of DLs and query languages, we determine the
(un)decidability and exact computational complexity of query emptiness and predicate emptiness.
Our results are summarized on the right side of Figure 1 and range from PT IME for basic members
of the EL and DL-Lite families via NE XP T IME for basic members of the ALC family to undecidable for ALC extended with functional roles (ALCF). We adopt the standard notion of combined
3

fiBAADER , B IENVENU , L UTZ , & W OLTER

complexity, which is measured in terms of the size of the whole input (TBox, data vocabulary, and
query or predicate symbol).
Because of the restricted data vocabulary  and the quantification over all -databases in their
definition, query emptiness and predicate emptiness do not reduce to standard reasoning problems
such as query evaluation and query containment. Formally, this is demonstrated by our undecidability result for ALCF, which should be contrasted with the decidability of query entailment and
containment in this DL, as shown by Calvanese, De Giacomo, and Lenzerini (1998). When emptiness is decidable, the complexity still often differs from that of query evaluation. To simplify the
comparison, we display in Figure 1 known complexity results for query evaluation in the considered DLs; please consult the work of Baader, Brandt, and Lutz (2005, 2008), Krotzsch, Rudolph,
and Hitzler (2007), and Eiter, Gottlob, Ortiz, and Simkus (2008) for the results concerning EL and
its Horn extensions, the work of Calvanese, De Giacomo, Lembo, Lenzerini, and Rosati (2007) and
Artale, Calvanese, Kontchakov, and Zakharyaschev (2009) for the results on DL-Lite, and the work
of Tobies (2001), Hustadt, Motik, and Sattler (2004), Lutz (2008), and Ortiz, Simkus, and Eiter
(2008) for the results on DLs from the ALC family. By comparing the two sides of Figure 1, we
observe that there is no clear relationship between the complexity of emptiness checking and the
complexity of query evaluation. Indeed, while the problems are often of similar complexity, there
are several cases in which emptiness checking is more difficult than the corresponding query evaluation problem. It can also be the other way around, and complexities can also be incomparable. Note
that for the extension EL of EL with the bottom concept  (used to express class disjointness), we
observe a particularly significant difference between the tractability of evaluating instance queries
and the E XP T IME-completeness of checking IQ-query emptiness.
A key ingredient in developing algorithms and establishing upper complexity bounds for emptiness is to show that, when searching for databases that witness non-emptiness (such as the database
A for the non-emptiness of q1 for  given O in the above example), one can often focus on a single
database constructed specifically for that purpose or on a class of databases that are easier to handle
than the class of all databases. Which single database / class of databases to consider depends on the
DL in question. For this reason, a secondary theme of this paper is to analyze the shape of witness
databases. It turns out that in ALC and its extension ALCI with inverse roles, we can consider a
single exponential-size database whose construction is reminiscent of type elimination and filtration
constructions known from the modal logic literature. For EL and its extension ELI, we may also
concentrate on a single witness candidate, but a much simpler one: it consists of all facts that can
be constructed using the data vocabulary and a single constant. For other extensions of EL, we
use a class of databases as witness candidates, namely those that have a tree or forest structure. In
DL-Lite, we may restrict our attention to the class of databases whose size is bounded polynomially
w.r.t. the input query and ontology.
To demonstrate that predicate emptiness is a useful reasoning service for static analysis, we
perform experiments using the well-known and large-scale medical ontology SNOMED CT coupled
with both a real-world data vocabulary (corresponding to terms obtained by analyzing clinical notes
from a hospital) and with randomly generated vocabularies. For the real world vocabulary, which
contains 8,858 of the 370,000 concept names and 16 of the 62 role names in SNOMED CT,
16,212 predicates turned out to be non-empty for IQs and 17,339 to be non-empty for CQs. Thus,
SNOMED CT provides a very substantial number of additional predicates for query formulation
while a large number of other predicates cannot meaningfully be used in queries over -databases;
thus, identifying the relevant predicates via predicate emptiness is potentially very helpful.
4

fiQ UERY AND P REDICATE E MPTINESS

We also consider the use of query and predicate emptiness for the extraction of modules from
an ontology. Thus, instead of using emptiness directly to support query formulation, we show how
it can be used to simplify an ontology. A -substitute of an ontology is a subset of the ontology
that gives the same certain answers to all conjunctive queries over all -databases. Replacing a
large ontology with a (potentially quite small) -substitute supports comprehension of the ontology
and thereby the formulation of meaningful queries. We show that, for ELI, one can use predicate emptiness to extract a particularly natural -substitute of an ontology, called its CQ -core,
containing exactly those axioms from the original ontology that contain only predicates which are
non-empty for -databases. Thus, all predicates in the CQ -core of an ontology can be meaningfully used in queries posed to -databases. In our example, the CQ -core of the ontology O is
O0 = {DiabetesPatient  Patient u has disease.Diabetes}.
The second axiom was removed because any CQ that contains DiabetesPatientwithoutMedication
is empty for  given O. To analyze the practical interest of CQ -cores, we carry out a case study
where we compute CQ -cores for the ontology SNOMED CT coupled with various signatures,
showing that they tend to be drastically smaller than the original ontology and also smaller than
-modules, a popular way of extracting modules from ontologies (Grau, Horrocks, Kazakov, &
Sattler, 2008).
This article is structured as follows. We begin in Section 2 by recalling the syntax and semantics of the description logics considered in this work. In Section 3, we introduce four notions of
emptiness (IQ-query, IQ-predicate, CQ-predicate, and CQ-query) and investigate the formal relationships between them. We first observe that IQ-query and IQ-predicate emptiness coincide
(so there are only three problems to consider) and that CQ-predicate emptiness corresponds to CQquery emptiness where CQs are restricted to a very simple form. We also exhibit two polynomial
reductions between predicate and query emptiness: for all DLs considered in this paper except those
from the DL-Lite family, CQ-predicate emptiness is polynomially reducible to IQ-query emptiness, and for all Horn-DLs considered in this paper, IQ-query emptiness is polynomially reducible
to CQ-predicate emptiness.
In Section 4, we investigate the computational complexity and decidability of query and predicate emptiness in the ALC family of expressive DLs. For ALC and ALCI, we provide tight
complexity bounds, showing NE XP T IME-completeness for all three emptiness problems in ALC
and for IQ-query emptiness and CQ-predicate emptiness in ALCI, and 2E XP T IME-completeness
for CQ-query emptiness in ALCI. The situation is dramatically (and surprisingly) different for
ALCF, for which all emptiness problems are proven undecidable. As previously mentioned, the
complexity upper bounds for ALC and ALCI rely on a characterization of non-emptiness in terms
of a special witness database. The complexity lower bounds and undecidability results are proven
by means of reductions from tiling problems.
In Section 5, we continue our investigation of query and predicate emptiness by considering
the DL EL and its Horn extensions. For plain EL, we provide a simple characterization of nonemptiness in terms of a maximal singleton database, which allows us to show that all three emptiness
problems can be decided in polynomial time. Using the same characterization and the fact that
standard reasoning in ELI is E XP T IME-complete, we obtain E XP T IME-completeness of emptiness
checking in ELI. For extensions of EL that allow for contradictions, the singleton database may not
be consistent with the ontology, requiring another approach. To handle such extensions, we show
5

fiBAADER , B IENVENU , L UTZ , & W OLTER

that it is sufficient to consider tree-shaped databases as witnesses for non-emptiness, and we devise
a decision procedure for emptiness checking based upon tree automata. We obtain in this manner an
E XP T IME upper bound for Horn-ALCIF, which sharply contrasts with our undecidability result
for (non-Horn) ALCF. Interestingly, we can show a matching E XP T IME lower bound for the
considerably simpler DL EL , for which standard reasoning tasks are tractable.
In Section 6, we turn our attention to the DL-Lite family of lightweight DLs, which are the most
commonly considered DLs for ontology-based data access. We show that CQ-query emptiness is
coNP-complete for all considered DL-Lite dialects. For IQ-query emptiness and CQ-predicate
emptiness, we show that the complexity depends on whether the considered dialect allows for conjunctions on the left-hand side of axioms. For standard dialects like DL-Litecore , DL-LiteR , and
DL-LiteF , which do not allow for conjunction, we show that IQ-query emptiness and CQ-predicate
emptiness are NL OG S PACE-complete. For dialects like DL-Litehorn that admits conjunctions, both
IQ-query emptiness and CQ-predicate emptiness are coNP-complete. This difference in complexity is due to the fact that for dialects allowing conjunction, we need to consider witnesses for nonemptiness that are of polynomial size, whereas in the absence of conjunction, it is sufficient to
consider databases that consist of a single assertion.
In Section 7, we apply query and predicate emptiness to extract modules of an ontology. We
introduce the notion of a -substitute and of a CQ -core of an ontology and show that for ELI the
CQ -core of an ontology is a -substitute. We also relate -substitutes to other notions of module
proposed in the literature. In particular, we observe that semantic and syntactic -modules (Grau
et al., 2008) are examples of -substitutes, and thus, algorithms for computing such modules can
also be used to compute (possibly non-minimal) -substitutes. We then demonstrate the potential
utility of -substitutes and emptiness checking by experiments based on SNOMED CT.
Finally, in Sections 8 and 9, we conclude the paper by discussing related and future work. Please
note that to improve the readability of the text, some technical proofs are deferred to the appendix.

2. Preliminaries
In DLs, concepts are inductively defined with the help of a set of constructors, starting with countably infinite sets NC of concept names and NR of role names. The constructors that are most important in this paper are summarized in Figure 2. An inverse role has the form r with r a role name
and a role is a role name or an inverse role. For uniformity, we define double inverse to be identity,
that is, (r ) := r for all role names r. Throughout the paper, we use A, B to denote concept
names, C, D to denote (possibly compound) concepts, and r and s to denote roles.
We shall be concerned with a variety of different DLs that are all well-known from the literature.
The least expressive ones are EL and DL-Lite, which are the logical underpinnings of the OWL2
profiles OWL2 EL and OWL2 QL, respectively (Motik, Grau, Horrocks, Wu, Fokoue, & Lutz,
2009). In EL, concepts are constructed according to the following grammar using the constructors
top concept, conjunction, and existential restriction:
C, D

::=

>

|

A

| C uD

|

r.C

with A ranging over concept names and r over role names. DL-Lite concepts and TBoxes will be
introduced in Section 6. The basic expressive DL we consider in this paper is ALC which is the
extension of EL with the constructors bottom concept, negation, disjunction and value restriction:
C, D

::=

>

|



| A

| C uD
6

|

C tD

|

r.C

|

r.C

fiQ UERY AND P REDICATE E MPTINESS

Name

Syntax

concept name
role name

A
r

Semantics
AI
rI

top concept
bottom concept
negation
conjunction
disjunction
existential restriction
value restriction
role inverse

>

C
C uD
C tD
r.C
r.C
r

>I = I
I = 
I \ C I
C I  DI
C I  DI
{d  I | e  C I with (d, e)  rI }
{d  I | e  I : (d, e)  rI  e  C I }
{(d, e) | (e, d)  rI }

concept inclusion
concept assertion
role assertion

CvD
A(a)
r(a, b)

C I  DI
aI  AI
(aI , bI )  rI

Figure 2: Syntax and semantics of DL constructors, TBox axioms, and ABox assertions.

The availability of additional constructors is indicated by concatenation of letters or subscripts: the
letter I stands for the addition of inverse roles (inside existential and value restrictions, if present)
and the subscript  stands for adding . This gives, for example, the extension ALCI of ALC
with inverse roles, whose constructors are exactly the ones shown in Figure 2. It also defines the
extension ELI  of EL with inverse roles in existential restrictions and the bottom concept.
A concept inclusion (CI) in a DL L takes the form C v D, where C and D are L-concepts.
We use C  D as an abbreviation for the CIs C v D and D v C. In description logic, ontologies
are formalized as TBoxes. Given any of the DLs L introduced above, an L-TBox is a finite set of
CIs in L. We use the letter F and write LF to denote the description logic in which TBoxes consist
not only of CIs in L, but also of functionality statements funct(r), where r is a role name or an
inverse role (if inverse roles are admitted in L). For example, ALCF is thus the extension of ALC
in which TBoxes can contain CIs in ALC and functionality statements for role names. We use the
term axioms to refer to concept inclusions and functionality statements in a uniform way.
In addition to the DLs introduced above, we also consider some DLs that impose restrictions
on which constructors can be used on which side of concept inclusions. A Horn-ALCI concept
inclusion (CI) is of the form L v R, where L and R are concepts defined by the syntax rules
R, R0 ::= > |  | A | A | R u R0 | L t R | r.R | r.R
L, L0 ::= > |  | A | L u L0 | L t L0 | r.L
with A ranging over concept names and r over (potentially inverse) roles. A Horn-ALCIF-TBox
T is a finite set of Horn-ALCI CIs and functionality statements funct(r). Note that different
definitions of Horn-DLs can be found in the work of Hustadt, Motik, and Sattler (2007), Eiter et al.
(2008), and Kazakov (2009). As the original definition by Hustadt, Motik, and Sattler based on
polarity is rather technical, we prefer the above (equivalent) definition.
The size |T | of a TBox T is obtained by taking the sum of the lengths of its axioms, where the
length of an axiom is the number of symbols needed to write it as a word.
7

fiBAADER , B IENVENU , L UTZ , & W OLTER

Databases are represented using an ABox, which is a finite set of concept assertions A(a) and
role assertions r(a, b), where a, b are drawn from a countably infinite set NI of individual names,
A is a concept name, and r is a role name. Note that role assertions cannot use inverse roles. As
a shortcut, though, we sometimes write r (a, b)  A for r(b, a)  A. We use Ind(A) to denote
the set of individual names used in the ABox A. A knowledge base is a pair K = (T , A) with T a
TBox and A an ABox.
The semantics of description logics is defined in terms of an interpretation I = (I , I ). The
domain I is a non-empty set and the interpretation function I maps each concept name A  NC
to a subset AI of I , each role name r  NR to a binary relation rI on I , and each individual
name a to an element aI  I . The extension of I to compound concepts is inductively defined
as shown in the third column of Figure 2. An interpretation I satisfies (i) a CI C v D if C I  DI ,
a statement funct(r) if rI is functional, (iii) an assertion A(a) if aI  AI , and (vi) an assertion
r(a, b) if (aI , bI )  rI . Then, I is a model of a TBox T if it satisfies all axioms in T , and a model
of an ABox A if it satisfies all assertions in A. A TBox is satisfiable if it has a model and an ABox
A is satisfiable w.r.t. a TBox T if T and A have a common model. We write T |= C v D if all
models of T satisfy the CI C v D.
We consider two types of queries. First, instance queries (IQs) take the form A(v), where A is a
concept name and v an individual variable taken from a set NV . Note that instance queries can only
be used to query concept names, but not role names. This is the traditional definition, which is due
to the fact that role assertions can only be implied by an ABox if they are explicitly contained in it,
and thus querying is trivial.1 The more general conjunctive queries (CQs) take the form ~u (~v , ~u)
where  is a conjunction of atoms of the form A(v) and r(v, v 0 ) with v, v 0 individual variables from
~v  ~u  NV . Variables that are not existentially quantified are called answer variables, and the
arity of q is defined as the number of its answer variables. Queries of arity 0 are called Boolean.
We use var(q), avar(q), and qvar(q) to denote the set of variables, answer variables, and quantified
variables respectively in the query q. From now on, we use IQ to refer to the set of all IQs and CQ
to refer to the set of all CQs.
Let I be an interpretation and q an (instance or conjunctive) query q of arity k with answer
variables v1 , . . . , vk . A match of q in I is a mapping  : var(q)  I such that (v)  AI for
all A(v)  q, ((v), (v 0 ))  rI for all r(v, v 0 )  q, and for every answer variable v  var(q),
there is an individual name a with (v) = aI . We write I |= q[a1 , . . . , ak ] if there is a match
 of q in I such that (vi ) = aIi for every 1  i  k. For a knowledge base (T , A), we write
T , A |= q[a1 , . . . , ak ] if I |= q[a1 , . . . , ak ] for all models I of T and A. In this case, (a1 , . . . , ak )
is a certain answer to q w.r.t. T and A. We use certT ,A (q) to denote the set of all certain answers
to q w.r.t. T and A. Note that when q is a Boolean query, we have ()  certT ,A (q) if there is a
match for q in every model of T , A, and otherwise certT ,A (q) = . The query evaluation problem
for CQs for a DL L is the problem to decide for an L-TBox T , ABox A, CQ q of arity k, and tuple
~a  Ind(A)k , whether ~a  certT ,A (q).
We use the term predicate to refer to a concept name or role name and signature to refer to a
set of predicates (in the introduction, we informally called a signature a vocabulary). Then sig(q)
denotes the set of predicates used in the query q, and similarly sig(T ) and sig(A) refer to the
signature of a TBox T and ABox A. A -ABox is an ABox that uses only predicates from the
signature , and likewise for a -concept.
1. This is no longer true in the presence of role hierarchy statements which, however, we do not consider in this paper.

8

fiQ UERY AND P REDICATE E MPTINESS

In the context of query answering in DLs, it is sometimes useful to adopt the unique name
assumption (UNA), which requires that aI 6= bI for all interpretations I and all a, b  NI with
a 6= b. The results obtained in this paper do not depend on the UNA. The following well-known
lemma shows that the UNA does not make a difference in ALCI (and all its fragments such as EL
and ALC) because the certain answers to queries do not change.
Lemma 1 Let T be an ALCI-TBox, A an ABox, and q  CQ. Then certT ,A (q) is identical with
and without the UNA.
An analogous statement fails for ALCF, e.g. because the ABox A = {f (a, b), f (a, b0 )} is satisfiable w.r.t. the TBox T = {funct(r)} without the UNA (and thus certT ,A (A(v)) = ), but
unsatisfiable with the UNA (and thus certT ,A (A(v)) = Ind(A)).

3. Query and Predicate Emptiness
We introduce the central notions and reasoning problems studied in this paper, show how they are
interrelated, and make some basic observations that are used throughout the paper. The following
definition introduces the different notions of emptiness studied in this paper.
Definition 2 Let T be a TBox,  a signature, and Q  {IQ, CQ} a query language. Then we call
 an Q-query q empty for  given T if for all -ABoxes A that are satisfiable w.r.t. T , we have
certT ,A (q) = .
 a predicate S Q-empty for  given T if every Q-query q with S  sig(q) is empty for 
given T .
In what follows, the signatures  used in ABoxes will be called ABox signatures. We quantify over
all ABoxes that are formulated in the ABox signature to address typical database applications in
which the data changes frequently, and thus deciding emptiness based on a concrete ABox is not of
much interest. As an example, assume that ABoxes are formulated in the signature
 = {Person, hasDisease, DiseaseA, DiseaseB}
where here and in the following, all upper-case words are concept names and all lower-case ones
are role names. This signature is typically fixed in the application design phase, similar to schema
design in databases. For the TBox, we take
T = {Person v hasFather.(Person u Male), DiseaseA v InfectiousDisease}.
Then both the IQ InfectiousDisease(v) and the CQ v hasFather(u, v) are non-empty for  given T
despite using predicates that cannot occur in the data, as witnessed by the -ABoxes {DiseaseA(a)}
and {Person(a)}, respectively. This illustrates how the TBox T enriches the vocabulary that is
available for query formulation. By contrast, the CQ
vv 0 (hasFather(u, v)  hasDisease(v, v 0 )  InfectiousDisease(v 0 )),
which uses the same predicates plus an additional one from the ABox signature, is empty for 
given T .
9

fiBAADER , B IENVENU , L UTZ , & W OLTER

Regarding predicate emptiness, it is interesting to observe that the choice of the query language
is important. For example, the predicate Male is IQ-empty for  given T , but not CQ-empty as
witnessed by the -ABox {Person(a)} and the CQ v Male(v). It thus makes no sense to use Male
in instance queries over -ABoxes given T , whereas it can be meaningfully used in conjunctive
queries.
As every IQ is also a CQ, a predicate that is CQ-empty must also be IQ-empty. As illustrated
by the above example, the converse does not hold. Also note that all role names are IQ-empty for 
given any T since a role name cannot occur in an instance query. By contrast, hasFather is clearly
not CQ-empty in the above example.
It follows from Lemma 1 that, in ALCI and its fragments, query emptiness and predicate emptiness are oblivious as to whether or not the UNA is made, both for IQ and CQ. As established by the
following lemma, this is also true in ALCIF despite the fact that the certain answers to queries
can differ with and without the UNA.
Lemma 3 Let T be an ALCIF-TBox. Then each CQ q is empty for  given T with the UNA iff it
is empty for  given T without the UNA.
The proof of Lemma 3 is given in the appendix. For the direction from left to right one assumes
that q is non-empty for  given T without the UNA and takes a witness -ABox A. Using a model
I satisfying A and T without the UNA and by identifying any a, b  Ind(A) with aI = bI one
can define a -ABox A0 from A that shows that q is non-empty for  given T with the UNA.
Conversely, one assumes that q is non-empty for  given T with the UNA and takes a witness
-ABox A. One can use A to show that q is non-empty for  given T without the UNA.
With the exception of a DL-Lite dialect (containing role inclusions) all DLs considered in this
paper are fragments of ALCIF. Thus, we are free to adopt the UNA or not. In the remainder of
the paper, we will choose whatever is more convenient, but be careful to always point out explicitly
whether the UNA is made or not. For the DL-Lite dialect not covered by the formulation of Lemma 3
we will observe in our discussion of DL-Lite that even Lemma 1 holds and so we are free to adopt
the UNA or not in that case as well.
It is also relevant to note that our decision to disallow individual names in query atoms is without
any loss of generality. Indeed, it is easily verified that predicate emptiness is the same whether we
admit individuals in queries or not. Moreover, there is an immediate reduction of query emptiness
for generalized CQs (which may contain individual names) to query emptiness for CQs as defined
in this paper: it suffices to replace every individual a in the query by a fresh answer variable xa , and
then test whether the resulting query (without individuals) is empty for  given T .
Definition 2 gives rise to four natural decision problems.
Definition 4 Let Q  {IQ, CQ}. Then
 Q-query emptiness is the problem of deciding, given a TBox T , a signature , and an Qquery q, whether q is empty for  given T ;
 Q-predicate emptiness means to decide, given a TBox T , a signature , and a predicate S,
whether S is Q-empty for  given T .
10

fiQ UERY AND P REDICATE E MPTINESS

IQ-query = IQ-predicate
emptiness
emptiness
Trivial

CQ-query
emptiness

Theorem 7
(materializable DLs)

Lemma 5

Theorem 6

CQ-predicate
emptiness

Figure 3: Polytime reductions between emptiness notions.
Clearly, these four problems are intimately related. In particular, IQ-query emptiness and IQpredicate emptiness are effectively the same problem since an instance query consists only of a
single predicate. For this reason, we will from now on disregard IQ-predicate emptiness and only
speak of IQ-query emptiness. In the CQ case, things are different. Indeed, the following lemma
shows that CQ-predicate emptiness corresponds to CQ-query emptiness where CQs are restricted to
a very simple form. It is an easy consequence of the fact that, since composite concepts in queries
are disallowed, CQs are purely positive, existential, and conjunctive.
Lemma 5 A  NC (resp. r  NR ) is CQ-predicate empty for  given T iff the conjunctive query
v A(v) (resp. vv 0 r(v, v 0 )) is empty for  given T .
Lemma 5 allows us to consider only queries of the form v A(v) and vv 0 r(v, v 0 ) when dealing
with CQ-predicate emptiness. From now on, we do this without further notice.
Trivially, IQ-query emptiness is a special case of CQ-query emptiness. The following observation is less obvious and applies to all DLs considered in this paper except those from the DL-Lite
family.
Theorem 6 In any DL contained in ALCIF that admits CIs r.B v B and r.> v B with B a
concept name, CQ-predicate emptiness can be polynomially reduced to IQ-query emptiness.
Proof. Let T be a TBox,  a signature, B a concept name that does not occur in T and , and s a
role name that does not occur in T and . We prove that
1. A is CQ-predicate empty for  given T iff the IQ B(v) is empty for   {s} given the TBox
T 0 = T  TB  {A v B}, where TB = {r.B v B | r = s or r occurs in T };
2. r is CQ-predicate empty for  given T iff the IQ B(v) is empty for   {s} given the TBox
T 0 = T  TB  {r.> v B}, where TB is as above.
The proofs of Points 1 and 2 are similar, and we concentrate on Point 1. First suppose that A is CQpredicate non-empty for  given T . Then there is a -ABox A such that T , A |= v A(v). Choose
an a0  Ind(A) and set A0 := A  {s(a0 , b) | b  Ind(A)}. Using the fact that T , A |= v A(v)
and the definition of A0 and T 0 , it can be shown that T 0 , A0 |= B(a0 ). For the converse direction,
suppose that B is IQ-query non-empty for {s} given T 0 . Then there is a {s}-ABox A0 such
that T 0 , A0 |= B(a) for some a  Ind(A0 ). Let A be obtained from A0 by removing all assertions
s(a, b). Using the fact that T 0 , A0 |= B(a) and the definition of A0 and T 0 , it can be shown that
T , A |= v A(v).
o

11

fiBAADER , B IENVENU , L UTZ , & W OLTER

Figure 3 gives an overview of the available polytime reductions between our four (rather: three)
problems. In terms of computational complexity, CQ-query emptiness is thus (potentially) the hardest problem, while CQ-predicate emptiness is the simplest. More precisely, if CQ-query emptiness
in a DL L belongs to a complexity class C (larger than or equal to PT IME), then IQ-query emptiness
and CQ-predicate emptiness in L are also in C. Moreover, for DLs L satisfying the conditions of
Theorem 6, C-hardness of CQ-predicate emptiness in L implies C-hardness of CQ-query emptiness
and IQ-query emptiness in L.
Under certain conditions, we can also prove the converse of Theorem 6. Following the work of
Lutz and Wolter (2012), we call a model I of a TBox T and an ABox A a materialization of T
and A if for every CQ q of arity k and tuple ~a  Ind(A)k , I |= q[~a] iff T , A |= q[~a]. A DL L is
called materializable if for every ABox A that is satisfiable w.r.t. T there exists a materialization
of T and A. Typical DL-Lite dialects, the DL EL, and Horn-extensions of EL such as ELIF  are
materializable (Lutz & Wolter, 2012).
Theorem 7 Let L be a materializable DL that admits CIs of the form A1 u A2 v A3 , where
A1 , A2 , A3  NC . Then, in L, IQ-query emptiness can be polynomially reduced to CQ-predicate
emptiness.
Proof. We claim that the IQ A(v) is empty for  given T iff B is CQ-empty for   {X} given the
TBox T 0 = T  {A u X v B}, where B and X are concept names that do not occur in T .
For the if direction, assume that A(v) is IQ non-empty for  given T , and let A be a ABox such that T , A |= A(a) for some a  Ind(A). Set A0 := A  {X(a)}. It is easy to see that
T 0 , A0 |= v B(v) and thus B is CQ-predicate non-empty for   {X} given T 0 .
For the only if direction, assume that B is CQ non-empty for   {X} given T 0 , and let A0
be a   {X}-ABox which is satisfiable with T 0 and such that T 0 , A0 |= v B(v). We may assume
that X(a)  A0 for all a  Ind(A0 ) as adding these assertions can neither result in unsatisfiability
w.r.t. T 0 nor invalidate T 0 , A0 |= v B(v). By our assumption of materializability, there exists a
materialization I 0 of T 0 and A0 and we have I 0 |= v B(v). By definition of T 0 , we may assume
0
0
0
0
that X I = Ind(A0 ) and B I = AI  X I (if this is not the case, we can take a modified version,
00
00
0
0
00
0
I 00 , of I 0 that is defined by setting X I := Ind(A0 ), B I := AI  X I , and Y I := Y I for all
0
remaining concept and role names Y ; then I 00 still satisfies T 0 and A0 since X I  Ind(A0 ) and
00
0
A u X v B is the only inclusion containing X or B and it is still a materialization since Y I  Y I
for all concept and role names Y ). But then I 0 |= v B(v) implies that there is an a  Ind(A0 ) with
I 0 |= B(a). Since I 0 is a materialization of T 0 and A0 , this implies T 0 , A0 |= B(a). By definition of
T 0 , this implies T , A |= A(a), where A is obtained from A0 by dropping all assertions of the form
X(b). Since A is a -ABox and satisfiable w.r.t. T (since A0 is satisfiable w.r.t. T 0 ), it witnesses
that A(v) is non-empty for  given T .
o
As a final observation in this section, we note that deciding query and predicate emptiness is
essentially just ABox satisfiability whenever  contains all symbols used in the TBox. By the
described reductions, it suffices to consider CQ-query emptiness. For a CQ q = ~u (~v , ~u) we
associate with every individual variable v in q an individual name av and set
Aq = {A(av ) | A(v) is a conjunct in }  {r(av , av0 ) | r(v, v 0 ) is a conjunct in }.
Theorem 8 Let T be an ALCIF-TBox,  a signature with sig(T )  , and q a CQ. Then q is
empty for  given T iff sig(q) 6  or Aq is unsatisfiable w.r.t. T .
12

fiQ UERY AND P REDICATE E MPTINESS

Proof. (If) Assume that q is non-empty for  given T . Then there is a -ABox A that is
satisfiable w.r.t. T and such that certT ,A (q) 6= . This clearly implies sig(q)   since otherwise
there is a predicate in sig(q) \  and we can find a model of T and A in which this predicate is
interpreted as the empty set, which would mean certT ,A (q) = . It thus remains to show that Aq is
satisfiable w.r.t. T . To this end, let I be a model of T and A. Since certT ,A (q) 6= , there exists a
match  of q in I. Modify I by setting aIv = (v) for all variables v used in q. It is readily checked
that the modified I is a model of Aq and T , thus Aq is satisfiable w.r.t. T as required.
(Only if) Assume that sig(q)   and Aq is satisfiable w.r.t. T . Then sig(Aq )  . Since
clearly certT ,Aq (q) 6= , this means that q is non-empty for  given T .
o

4. Expressive Description Logics
We consider query and predicate emptiness in the ALC family of expressive DLs, establishing tight
complexity results for ALC and ALCI, and undecidability for ALCF. We start with upper bound
proofs, showing that IQ-query emptiness and CQ-predicate emptiness in ALCI are in NE XP T IME, and so is CQ-query emptiness in ALC. Moreover, we establish that CQ-query emptiness
is in 2E XP T IME. We then move on to the corresponding lower bound proofs and also establish
undecidability of all considered emptiness problems in ALCF.
4.1 Upper Bounds
The first main step in our proofs is to show that, when deciding emptiness problems in ALC or
ALCI, it suffices to consider a single special -ABox. Specifically, we show how to construct
from a given satisfiable TBox T and ABox signature  the canonical -ABox AT , such that for
every CQ q, we have certT ,AT , (q) 6=  if and only if there exists a -ABox A that is satisfiable
w.r.t. T such that certT ,A (q) 6= . We then prove NE XP T IME upper bounds for IQ-query emptiness in ALCI by computing AT , (in exponential time) and then guessing a model of AT , (of
exponential size in |T | and ) that falsifies the query; an 2E XP T IME upper bound for CQ-query
emptiness in ALCI is obtained in an even simpler way by computing AT , and then checking
whether certT ,AT , (q) =  using known algorithms. Significantly more work is required to obtain
a NE XP T IME upper bound for CQ-query emptiness in ALC. We again construct AT , , but need to
exercise a lot of care to check whether certT ,AT , (q) =  without leaving NE XP T IME.
Let T be a satisfiable ALCI-TBox and  an ABox signature. To define the canonical -ABox
for T , we introduce the well-known notion of types (or Hintikka sets) (Pratt, 1979; Kaminski,
Schneider, & Smolka, 2011). The closure cl(T , ) of T and  is the smallest set that contains
  NC as well as all concepts that occur (potentially as a subconcept) in T and is closed under
single negations. A type for T and  is a set t  cl(T , ) such that for some model I of T and
some d  I , we have t = tI (d), where tI (d) = {C  cl(T , ) | d  C I } is the type for T
and  realized by d in I. Let TT , denote the set of all types for T and . For a role name r and
t, t0  TT , , we say that the pair (t, t0 ) is r-coherent and write t ;r t0 if
 C  t0 whenever r.C  t, and
 C  t whenever r .C  t0 .
It can be seen that the above implies also corresponding conditions on existential restrictions, such
as C  t0 and r.C  cl(C, T ) implies r.C  t.
13

fiBAADER , B IENVENU , L UTZ , & W OLTER

Definition 9 (Canonical -ABox) Let T be a satisfiable ALCI-TBox and  an ABox signature.
Fix a (distinct) individual name at for each t  TT , . The canonical -ABox AT , for T is defined
as follows:
AT , = {A(at ) | A  t and t  TT , , A    NC } 
{r(at , at0 ) | t ;r t0 and t, t0  TT , , r    NR }.
The cardinality of TT , is at most exponential in the size of T and the cardinality of , and the
set TT , can be computed in exponential time by making use of well-known E XP T IME procedures
for concept satisfiability w.r.t. TBoxes in ALCI (Gabbay, Kurucz, Wolter, & Zakharyaschev, 2003,
p. 72) Thus, AT , is of exponential size and can be computed in exponential time. It is interesting
to note that the ABox AT , is a finitary version of the canonical model for basic modal logic and is
essentially identical to the model constructed by Pratts type elimination procedure (Pratt, 1979); in
fact, it is exactly identical when   sig(T ). We now show that AT , is satisfiable w.r.t. T .
Lemma 10 Let T be a satisfiable ALCI-TBox and  an ABox signature. Then AT , is satisfiable
w.r.t. T .
Proof. Let the interpretation IT , be defined by setting
IT ,

= TT ,

AIT ,

= {t  TT , | A  t}

rIT ,

= {(t, t0 )  TT ,  TT , | t ;r t0 }

for all A  NC and r  NR . One can prove by induction on the structure of C that for all C 
cl(T , ), we have C  t iff t  C IT , . By definition of types, C v D  T and C  t imply D  t.
Thus, IT , is a model of T . It is an immediate consequence of the definition of AT , that IT , is
also a model of AT , ; in fact, AT , can be regarded as the reduct of IT , to signature .
o
As a crucial tool for analyzing the properties of canonical ABoxes, we introduce homomorphism
between ABoxes. Let A and A0 be ABoxes. An ABox homomorphism from A to A0 is a total
function h : Ind(A)  Ind(A0 ) such that the following conditions are satisfied:
 A(a)  A implies A(h(a))  A0 ;
 r(a, b)  A implies r(h(a), h(b))  A0 .
The next lemma identifies a central property of ABox homomorphisms regarding query answering.
Lemma 11 If T is an ALCI-TBox, q is a CQ such that T , A |= q[a1 , . . . , an ], and h is an ABox
homomorphism from A to A0 , then T , A0 |= q[h(a1 ), . . . , h(an )].
Proof. We prove the contrapositive. Thus assume that T , A0 6|= q[h(a1 ), . . . , h(an )]. Then there is a
model I 0 of T and A0 such that I 0 6|= q[h(a1 ), . . . , h(an )]. Define a model I by starting with I 0 and
0
reinterpreting the individual names in Ind(A) by setting aI = h(a)I for each a  Ind(A). Since
individual names do not occur in T , I is a model of T . It is also a model of A: if A(a)  A, then
A(h(a))  A0 by definition of ABox homomorphisms. Since I 0 is a model of A0 and by definition
of I, it follows that aI  AI . The case r(a, b)  A is analogous. Finally, I 0 6|= q[h(a1 ), . . . , h(an )]
and the definition of I yield I 6|= q[a1 , . . . , an ]. We have thus shown that T , A 6|= q[a1 , . . . , an ].
o
14

fiQ UERY AND P REDICATE E MPTINESS

The following lemma characterizes satisfiability of -ABoxes w.r.t. T by the existence of an ABox
homomorphism into AT , .
Lemma 12 Let T be a satisfiable ALCI-TBox and  an ABox signature. A -ABox A is satisfiable
w.r.t. T iff there is an ABox homomorphism from A to AT , .
Proof. Assume A is satisfiable w.r.t. T . Let I be a model of T and A. Define a homomorphism h
from A to AT , by setting h(a) = at , where t is the type for T and  realized by aI in I. Using
the definition of AT , , one can see that h is indeed an ABox homomorphism. Conversely, let h be
an ABox homomorphism from A to AT , . By Lemma 10, AT , is satisfiable w.r.t. T . The proof
of Lemma 11 shows how one can construct a model of T and A from a model of T and AT , using
the homomorphism h. Thus A is satisfiable w.r.t. T .
o
We are now ready to prove the main property of AT , regarding emptiness, as discussed at the
beginning of this section.
Theorem 13 Let T be a satisfiable ALCI-TBox and  an ABox signature. A CQ q is empty for 
given T iff certT ,AT , (q) = .
Proof. The only if direction follows directly from the fact that AT , is satisfiable w.r.t. T (by
Lemma 10). For the if direction, let certT ,AT , (q) = . To show that q is empty for  given T ,
take a -ABox A that is satisfiable w.r.t. T . By Lemmas 11 and 12, certT ,AT , (q) =  implies
certT ,A (q) = , as required.
o
We now employ Theorem 13 to prove the NE XP T IME upper bounds for IQ-query emptiness.
Theorem 14 In ALCI, IQ-query emptiness is in NE XP T IME.
Proof. Let T be a satisfiable TBox,  an ABox signature, and A(v) an IQ for which emptiness for
 given T is to be decided. We employ the following:
Fact. For any ABox A, if T , A 6|= A(a), then there exists a model I of T and A with aI 6 AI and
|I |  |Ind(A)| + 2|T | .
Proof of Fact. If T , A 6|= A(a), then there exists a model J of T and A with aJ 6 AJ . We
may assume that {aJ | a  Ind(A)} is disjoint from the domain TT ,0 of the interpretation IT ,0
defined in the proof of Lemma 10 (where we assume that 0 := ). Now define I as the union of
the restriction of J to {aJ | a  Ind(A)} and the interpretation IT ,0 expanded by adding to rI
all pairs
 (aJ , t) such that tJ (aJ ) ;r t, a  Ind(A), and t  TT ,0 ;
 (t, aJ ) such that t ;r tJ (aJ ), a  Ind(A), and t  TT ,0 .
Then I is a model of T and A with aI 6 AI of the required size. This finishes the proof of the fact.
The NE XP T IME algorithm computes the canonical ABox AT , (in exponential time) and guesses
for every a  Ind(AT , ) a model Ia with |Ia |  |Ind(AT , )| + 2|T | . The algorithm returns yes
if for all a  Ind(AT , ):
1. Ia is a model of AT , and T , and
15

fiBAADER , B IENVENU , L UTZ , & W OLTER

2. aIa 6 AIa .
Both conditions can be checked in exponential time. Thus, by Theorem 13 and the fact above, the
algorithm returns yes iff A(v) is empty for  given T .
o
Note that by Theorem 6 CQ-predicate emptiness in ALCI is in NE XP T IME as well. For CQ-query
emptiness in ALCI, we can easily derive a 2E XP T IME upper bound using AT , and results from
the work of Calvanese et al. (1998) on the complexity of query answering in DLs.
Theorem 15 In ALCI, CQ-query emptiness is in 2E XP T IME.
Proof. The 2E XP T IME algorithm is obtained by first computing the canonical ABox AT , and
certT ,AT , (q), and then checking whether the latter is empty. This can be done in 2E XP T IME since
it is shown in the work of Calvanese et al. (1998) that for all T , A, and q with T an ALCI-TBox,
p(n)
the set certT ,A (q) can be computed in time 2p(m)2
with p a polynomial, m the size of T  A,
and n the size of q.
o
We provide an improved NE XP T IME upper bound for CQ-query emptiness in ALC, which will
allow us to show that for ALC our three emptiness problems have the same complexity.
Theorem 16 In ALC, CQ-query emptiness is in NE XP T IME.
The proof is somewhat technical and reuses the machinery of fork rewritings and spoilers introduced
by Lutz (2008), who proves that the combined complexity of CQ-answering in the DL SHQ is
in E XP T IME. More concretely, we show that one can decide emptiness of a CQ q for an ABox
signature  given an ALC-TBox T by guessing an extension AeT , of the canonical ABox AT ,
with assertions that prevent any possible match of q and then checking that AeT , is satisfiable w.r.t.
T . For example, if q is A(x), then it obviously suffices to add A(a) for every individual a in AT ,
(we allow here also complex concepts to be used in an ABox). The general case requires a careful
analysis of the assertions that have to be considered as additions, and this is where the mentioned
fork rewritings and spoilers enter the picture. In fact, they are used to prove that, since there are
no inverse roles in the TBox, it suffices to consider extensions of AT , that contain no additional
individual names and where the additional assertions are taken from a candidate set whose size is
polynomial in the size of AT , and q. It remains to show that satisfiability of (T , AeT , ) can be
decided (non-deterministically) in time single exponential in the size of T and q. Full details are
given in the appendix.
4.2 Lower Bounds and Undecidability
We prove matching lower bounds for the upper complexity bounds presented above and show undecidability of IQ-query emptiness, CQ-predicate emptiness, and CQ-query emptiness for ALCF.
The undecidability proof and the NE XP T IME-lower bound proof are by reduction of two different
tiling problems, where the first asks for a tiling of a finite rectangle of any (unbounded) size and the
second asks for a tiling of the 2n  2n -square. The 2E XP T IME lower bound for CQ-query emptiness in ALCI is by a straightforward reduction to query entailment in ALCI. We begin with the
NE XP T IME lower bound.
Theorem 17 In ALC, CQ-predicate emptiness is NE XP T IME-hard.
16

fiQ UERY AND P REDICATE E MPTINESS

Proof. The proof is by reduction of a NE XP T IME-hard 2n  2n -tiling problem. An instance of this
tiling problem is given by a natural number n > 0 (coded in unary) and a triple (T, H, V ) with
T a non-empty, finite set of tile types including an initial tile Tinit to be placed on the lower left
corner, H  T  T a horizontal matching relation, and V  T  T a vertical matching relation. A
tiling for (T, H, V ) is a map f : {0, . . . , 2n  1}  {0, . . . , 2n  1}  T such that f (0, 0) = Tinit ,
(f (i, j), f (i + 1, j))  H for all i < 2n  1, and (f (i, j), f (i, j + 1))  V for all j < 2n  1. It is
NE XP T IME-complete to decide whether an instance of the 2n  2n -tiling problem has a tiling.
For the reduction, let n > 0 and (T, H, V ) be an instance of the 2n  2n -tiling problem with
T = {T1 , . . . , Tp }. We construct a signature  and a TBox T in ALC such that (T, H, V ) has a
solution if and only if a selected concept name A is CQ-predicate empty for  given T . For the
proof, it is convenient to impose the UNA.
When formulating the reduction TBox, we use role names x and y to represent the 2n  2n grid and two binary counters X and Y for counting from 0 to 2n  1. The counters use concept
names X0 , . . . , Xn1 and Y0 , . . . , Yn1 as their bits, respectively. T contains the following wellknown inclusions stating that the value of the counter X0 , . . . , Xn1 is incremented when going to
x-successors and the value of the counter Y0 , . . . , Yn1 is incremented when going to y-successors:
for k = 1, . . . , n  1,

u

0j<k

and

t

0j<k

Xj v (Xk t x.Xk ) u (Xk t x.Xk )

Xj v (Xk t x.Xk ) u (Xk t x.Xk )

and similarly for Y0 , . . . , Yn1 and y. T also states that the value of the counter X does not change
when going to y-successors and the value of the counter Y does not change when going to xsuccessors: for i = 0, . . . , n  1,
Xi v y.Xi ,

Xi v y.Xi

Yi v x.Yi ,

Yi v x.Yi .

and
In addition, T states that when the counter X is 2n  1, it does not have an x-successor and if the
counter Y is 2n  1, it does not have a y-successor:
X0 u    u Xn1 v x.,

Y0 u    u Yn1 v y..

T states that Tinit holds at (0, 0) and that the tiling is complete:
X0 u    u Xn1 u Y0 u    u Yn1 v Tinit ,
T states that if a tiling condition is violated, then A is true:
 for all 0  i < j  p: Ti u Tj v A,
 for all 0  i, j  p such that (Ti , Tj ) 6 H: Ti u x.Tj v A,
 for all 0  i, j  p such that (Ti , Tj ) 6 V : Ti u y.Tj v A.
17

>v

t

1ip

Ti ,

fiBAADER , B IENVENU , L UTZ , & W OLTER

Finally, since we cannot use negation in ABoxes, T states that concept names X 0 , . . . , X n1
and Y 0 , . . . , Y n1 are equivalent to X0 , . . . , Xn1 and Y0 , . . . , Yn1 , respectively: for i =
1, . . . , n  1:
Xi v X i , Xi v X i , Yi v Y i , Yi v X i .
We set  = {x, y, X0 , . . . , Xn1 , Y0 , . . . , Yn1 , X 0 , . . . , X n1 , Y 0 , . . . , Y n1 } and show
Claim. (T, H, V ) has no 2n  2n -tiling iff there exists a -ABox A that is satisfiable w.r.t. T such
that T , A |= v A(v).
Proof of claim. Assume first that (T, H, V ) has no 2n  2n -tiling. To construct A, we regard
the pairs (i, j) with i < 2n and j < 2n as individual names and let x((i, j), (i + 1, j))  A for
i < 2n  1 and y((i, j), (i, j + 1))  A for j < 2n  1. We also set Xk (i, j)  A if the kth bit of
i is 1, X k (i, j)  A if the kth bit of i is 0, Yk (i, j)  A if the kth bit of j is 1, and Y k (i, j)  A if
the kth bit of j is 0. It is readily checked that A is satisfiable w.r.t. T and that T , A |= v A(v).
Conversely, assume that (T, H, V ) has a 2n  2n -tiling given by f : {0, . . . , 2n  1} 
{0, . . . , 2n 1}  T. Let A be a -ABox that is satisfiable w.r.t. T . We show that T , A 6|= v A(v).
Let I be a model of T and A. If AI = , we are done. Otherwise re-define the interpretation of
T1 , . . . , Tp and A as follows. Associate with every d  I the uniquely determined pair (id , jd )
given by the values of the counters X and Y at d in I. Then set d  TkI iff f (id , jd ) = Tk and let
AI = . It is readily checked that the resulting interpretation is still a model of T and A.
o
It follows from the preceding result that IQ-query emptiness and CQ-query emptiness for
ALC and ALCI are NE XP T IME-hard. For CQ-query emptiness in ALCI, we can easily derive
a 2E XP T IME lower bound from results on the complexity of query entailment in ALCI.
Theorem 18 In ALCI, CQ-query emptiness is 2E XP T IME-hard.
Proof. It was shown by Lutz (2008) that CQ entailment in ALCI is 2E XP T IME-hard already for
ABoxes of the form {A(a)} and for Boolean CQs. This can clearly be strengthened to empty
ABoxes: replace A(a) with the empty ABox and compensate for this by adding to the TBox > v
r.A with r a fresh role name. It thus remains to observe that a Boolean CQ q is entailed by T and
the empty ABox iff q is non-empty for  =  and T .
o
We now show that the simple addition of functional roles to ALC leads to undecidability of CQpredicate emptiness, thus also of IQ-query emptiness and CQ-query emptiness. The proof is by
reduction from a tiling problem that asks for a tiling of a rectangle of finite size (which is neither
fixed nor bounded). The reduction involves a couple of technical tricks such as using concept
names that are not in  as universally quantified second-order variables. This allows us to enforce
a grid structure using standard frame axioms from modal logic (which are second-order in nature).
The reduction requires role names that are both functional and inverse functional. Since inverse
functionality cannot be expressed in ALCF, we also use a modal logic frame axiom to enforce that
a different, (forwards) functional role name is interpreted as the inverse of the role name that we
are interested in. Of course, undecidability carries over to variants of ALCF that use a concept
constructor ( 1 r) instead of functional roles as an additional sort, and to all DLs with qualified or
unqualified number restrictions.
Theorem 19 In ALCF, CQ-predicate emptiness is undecidable.
18

fiQ UERY AND P REDICATE E MPTINESS

An instance of the aforementioned tiling problem is given by a triple (T, H, V ) with T a non-empty,
finite set of tile types including an initial tile Tinit to be placed on the lower left corner and a final tile
Tfinal to be placed on the upper right corner, H  T  T a horizontal matching relation, and V 
TT a vertical matching relation. A tiling for (T, H, V ) is a map f : {0, . . . , n}{0, . . . , m}  T
such that n, m  0, f (0, 0) = Tinit , f (n, m) = Tfinal , (f (i, j), f (i + 1, j))  H for all i < n, and
(f (i, j), f (i, j + 1))  v for all i < m. It is undecidable whether an instance of the tiling problem
has a tiling.
For the reduction, let (T, H, V ) be an instance of the tiling problem with T = {T1 , . . . , Tp }. We
construct a signature  and a TBox T such that (T, H, V ) has a solution if and only if a selected
concept name A is CQ-predicate non-empty for  given T .
The ABox signature is  = {T1 , . . . , Tp , x, y, x , y  } where T1 , . . . , Tp are used as concept
names, and x, y, x , and y  are functional role names. We use the role names x and y to represent horizontal and vertical adjacency of points in the rectangle, and the role names x and y  to
simulate the inverses of x and y. In T , we use additional auxiliary concept names. In particular U
and R mark the upper and right border of the rectangle, Zc,1 , Zc,2 , Zx,1 , Zx,2 , Zy,1 , Zy,2 serve as
second-order variables, C serves as a flag which indicates that grid cells are closed at the position
where it is set, and Ix and Iy are similar flags for the intended behavior of the role names x, x and
y, y  . The concept name Y is propagated through the grid from the upper right corner to the lower
left one, ensuring that these flags are set everywhere, that every position of the grid is labeled with
at least one tile type, and that the horizontal and vertical matching conditions are satisfied. When
the lower left corner of the grid is reached, we set A as a flag, which is what the query v A(v) asks
for.
The TBox T is defined as the set of the following CIs, where (Ti , Tj , T` ) range over all triples
from T such that (Ti , Tj )  H and (Ti , T` )  V and where Be , for e  {c, x, y}, ranges over all
Boolean combinations of the concept names Ze,1 and Ze,2 , i.e., over all concepts L1 u L2 where Li
is Ze,i or Ze,i :
Tfinal v Y u U u R
x.(U u Y u Tj ) u Ix u Ti v U u Y
y.(R u Y u T` ) u Iy u Ti v R u Y
x.(Tj u Y u y.Y ) u y.(T` u Y u x.Y ) u Ix u Iy u C u Ti v Y
Y u Tinit v A
Bx u x.x .Bx v Ix
By u y.y  .By v Iy
x.y.Bc u y.x.Bc v C
U v y. R v x. U v x.U

R v y.R

t

1s<tp

Ts u Tt v 

The CIs for Ix and Iy are responsible for enforcing that x is the inverse of x and y  the inverse
of y, at least at those ABox individuals that we are interested in. In fact, if the ABox contains
assertions x(a, b) and x (b, c) and thus violates the intended interpretation of x and x , then we
can interpret Zx,1 and Zx,2 such that the left-hand sides of all possible instantiations of the CI
for Ix are violated, e.g. by making Zx,1 and Zx,2 true at a, but false at c. If the ABox contains
x(a, b), x (b, a), then this is not possible. Since x and y  are functional, we thus enforce that x
19

fiBAADER , B IENVENU , L UTZ , & W OLTER

and y are inverse functional. The CIs for C achieve in a similar way the closing of grid cells, i.e.,
that the x-y-successor and the y-x-successor of every relevant ABox individual coincide. However,
as can be seen from the proofs, this only works if x and y are inverse functional.
To establish Theorem 19, it suffices to prove the following lemma (see the appendix for details).
Lemma 20 (T, H, V ) admits a tiling iff there is a -ABox A that is satisfiable with T and such
that T , A |= v A(v).

5. EL and its Horn Extensions
We study query and predicate emptiness for the DL EL and several of its Horn extensions. First, we
show that, in plain EL, all three emptiness problems can be decided in polynomial time. The reason
is that in this case, the exponential-size canonical ABox AT , from Section 4 can be replaced with
a total -ABox A that contains only a single individual which is an instance of all -predicates.
Note that A is satisfiable w.r.t. any EL-TBox because EL cannot express unsatisfiability. The
same approach works for ELI, but in this case one obtains only an E XP T IME upper bound which
is optimal since subsumption for ELI is already E XP T IME-hard (Baader et al., 2005, 2008). As
soon as unsatisfiability can be expressed, the situation changes drastically. In fact, we show that
even in EL where subsumption and other standard reasoning tasks are still tractable, (all versions
of) emptiness are E XP T IME-hard. Nevertheless, emptiness in Horn extensions of EL turns out to
be easier than emptiness in expressive DLs. In contrast to the undecidability result for ALCF and
the NE XP T IME-hardness result for ALC, emptiness is in E XP T IME even in Horn-ALCIF. The
reason is the unraveling tolerance of Horn description logics observed in the work of Lutz and
Wolter (2012), which implies that when looking for ABoxes that witness non-emptiness, we can
restrict ourselves to tree-shaped ones. This enables the use of automata-theoretic techniques to
decide emptiness.
5.1 EL and ELI
We begin by showing that in EL, CQ-query emptiness, CQ-predicate emptiness, and IQ-query
emptiness are all in PT IME. The proofs are transparent and simple. For any signature , the total
-ABox is A := {A(a ) | A  }  {r(a , a ) | r  }.
Lemma 21 Let T be an EL-TBox and  a signature. Any CQ q is empty for  given T iff
certT ,A (q) = .
Proof. The proof is a simplified version of the proof of Theorem 13. The (contrapositive of the)
only if direction follows from the fact that A is satisfiable w.r.t. T . For the if direction, let
certT ,A (q) = . To show that q is empty for  given T , take a -ABox A. Define an ABox
homomorphism from A to A by setting h(a) := a for all a  Ind(A ). By Lemmas 11 and 12,
certT ,A (q) =  implies certT ,A (q) = , as required.
o
Lemma 21 provides a polytime reduction of CQ-query emptiness (and, therefore, IQ-query
emptiness and CQ-predicate emptiness) to the query evaluation problem for CQs over A . In the
appendix, we show that due to the simple shape of A , checking whether certT ,A (q) =  can be
done in polynomial time. In fact, we give a polytime procedure that either returns certT ,A (q) =
 or succeeds in constructing a Boolean forest-shaped query qb that is empty for  given T iff q is.
20

fiQ UERY AND P REDICATE E MPTINESS

The construction relies on the fact that, as an immediate consequence of results proved by Lutz and
Wolter (2010), emptiness of q for  given T implies the existence of a model I of T and A with
certT ,A (q) =  and such that I has the shape of a tree extended with reflexive loops at the root.
Checking T , A 6|= qb then only requires to answer concept queries in the extension ELu of EL
with the universal role, which is possible in PT IME (Lutz & Wolter, 2010). We obtain the following
result.
Theorem 22 In EL, IQ-query emptiness and CQ-query emptiness can be decided in PT IME.
A matching PT IME lower bound for Theorem 22 can be shown by a reduction of subsumption in
EL, which is PT IME-hard (Haase, 2007). Consider an EL-TBox T and EL-concepts C and D.
Then the CI C v D follows from T if, and only if, the IQ B(v) is non-empty for the signature {A}
given the TBox T  {A v C, D v B}, where A, B are concept names that do not appear in C, D
or T . Thus, we obtain
Theorem 23 In EL, IQ-query emptiness and CQ-query emptiness are PT IME-hard.
Observe that by Lemma 7 and materializability of EL we obtain that CQ-predicate emptiness is
PT IME-complete as well in EL.
Note that we need very little for the proof of Lemma 21 to go through: it suffices that the total
-ABox A is satisfiable with every TBox. We can thus reduce emptiness to query answering
over the total -ABox in any extension of EL that is unable to express contradictions. As another
important example, we consider ELI. Since CQ evaluation in ELI is E XP T IME-complete, we
only obtain an E XP T IME upper bound in this case. A matching lower bound is obtained from the
E XP T IME-hardness of subsumption in ELI and the simple reduction of subsumption to IQ-query
emptiness given above.
Theorem 24 In ELI, IQ-query emptiness and CQ-query emptiness are E XP T IME-complete.
It follows from Lemma 7 and materializability of ELI that CQ-predicate emptiness is E XP T IMEcomplete in ELI.
5.2 Horn Extensions Involving Negation or Functionality
The simplest extension of EL that can express unsatisfiability is EL . We begin by showing that
IQ-emptiness in EL is E XP T IME-hard, and thus significantly harder than subsumption and instance checking (both of which can be decided in polynomial time). To this end, we first show
that to decide IQ-query emptiness in EL it is sufficient to consider emptiness w.r.t. directed treeshaped ABoxes, where an ABox A is called directed tree-shaped if the following conditions hold:
1. the directed graph GdA = (Ind(A), {(a, b) | r(a, b)  A}) is a tree;
2. for all a, b  Ind(A), there is at most one role name r such that r(a, b)  A or r(b, a)  A
(and only one of these is the case).
Proposition 25 An instance query B(v) is non-empty for a signature  given an EL -TBox T iff
there exists a directed tree-shaped -ABox A that is satisfiable w.r.t. T such that T , A |= B(a) for
the root a of A.
21

fiBAADER , B IENVENU , L UTZ , & W OLTER

Proof. We provide a sketch only since this result also follows from the more general Proposition 30 proved below. Assume B(v) is non-empty for  given T . We find a -ABox A that is
satisfiable w.r.t. T such that T , A |= B(a). Let the potentially infinite ABox A be obtained by
unfolding A as follows: the individuals of A are the words a0 r0    rn1 an such that a0 = a and
ri (ai , ai+1 )  A for all 0  i < n; then include A(a0 r0    rn1 an ) in A iff A(an )  A and
include r(a0 r0    an , a0 r0    an rn an+1 ) in A if rn (an , an+1 )  A. One can show that A is satisfiable w.r.t. T since A is, and that T , A |= B(a) iff T , A |= B(a). By compactness of first-order
consequence, we obtain a finite ABox A0  A with T , A0 |= B(a). A0 is as required.
o
Theorem 26 In EL , IQ-query emptiness is E XP T IME-hard.
Proof. Let T , , and B(v) be given. By Proposition 25, B(v) is non-empty for  given T iff
there exists a directed tree-shaped -ABox A that is a witness for the non-emptiness of B(v) for
 given T . Directed tree-shaped -ABoxes can be viewed as EL-concepts using symbols from 
only, and vice versa. Thus, such a witness -ABox exists iff there exists an EL-concept C using
symbols from  only such that C is satisfiable w.r.t. T and T |= C v B. Now the following can
be established by carefully analyzing the reduction underlying Theorem 36 in the work of Lutz and
Wolter (2010): given an EL -TBox T , a signature , and a concept name B, it is E XP T IME-hard
to decide if there exists an EL-concept C using symbols from  only such that C is satisfiable
w.r.t. T and T |= C v B. This establishes E XP T IME-hardness of non-emptiness. Using the fact
that E XP T IME = coE XP T IME, this hardness result transfers to IQ-query emptiness.
o
Observe that by Lemma 7 and materializability of EL we obtain that CQ-predicate emptiness is
E XP T IME-hard as well in EL .
Instead of proving a matching E XP T IME upper bound only for emptiness in EL , we do this
for the expressive Horn DL Horn-ALCIF, of which EL is a fragment. In fact, the rest of this
section is devoted to the proof of the following theorem. It is interesting to contrast this result with
the undecidability of emptiness in ALCF.
Theorem 27 In Horn-ALCIF, CQ-query emptiness is in E XP T IME.
The strategy for the proof of Theorem 27 is as follows. We first exhibit a polynomial-time reduction
from CQ-query emptiness in Horn-ALCIF to CQ-query emptiness in ELIF  . Then, we show
that non-emptiness of a CQ q under an ELIF  -TBox is always witnessed by ABoxes of a certain,
forest-like shape. We then consider canonical models of forest-shaped ABoxes (and the TBox
under consideration), which can be constructed by a chase-like procedure and are a special kind of
materialization (cf. Section 3), that is, the answers returned for this model are precisely the certain
answers. A central observation is that matches of q in canonical models of forest-shaped ABoxes
can be grouped into equivalence classes that are induced by certain splittings of q. We finally
show how to construct, for each equivalence class, a tree automaton that decides the existence of a
forest-shaped witness ABox whose canonical model admits a match of q that falls into that class.
Throughout this proof, we generally impose the UNA.
We begin with the reduction to CQ-query emptiness in ELIF  . In fact, the reduction even
shows that it suffices to consider ELIF  -TBoxes that are in normal form, by which we mean that
all CIs take one of the forms
A1 u    u An v B,

A v r.B,
22

r.A v B,

fiQ UERY AND P REDICATE E MPTINESS

where A, A1 , . . . , An , B  NC  {>, } and r is a role name or inverse role.
Proposition 28 For every Horn-ALCIF TBox T , ABox signature , and CQ q, one can construct
in polynomial time an ELIF  -TBox T 0 in normal form such that q is empty for  given T iff q is
empty for  given T 0 .
The proof of Proposition 28 is standard and given in the appendix. In what follows, we assume that
all ELIF  TBoxes are in normal form.
We next define canonical models. Let (T , A) be an ELIF  KB such that A is satisfiable
w.r.t. T . To construct the (typically infinite) canonical model IT ,A of (T , A), start with A viewed
as an interpretation, that is: IT ,A = Ind(A), AIT ,A = {a | A(a)  A}, and rIT ,A = {(a, b) |
r(a, b)  A}. Then exhaustively apply the following completion rules:
I

1. If A1 u    u An v A  T and d  Ai T ,A for 1  i  n, and d 
/ AIT ,A , then add d to
AIT ,A .
2. If r.A v B  T , (d, e)  rIT ,A , e  AIT ,A , and d 
/ B IT ,A , then add d to B IT ,A ;
3. If A v r.B  T , d  AIT ,A , and either d 
/ (r.B)IT ,A and funct(r) 6 T or d 
/
I
I
I
I
I
T
,A
T
,A
T
,A
T
,A
T
,A
(r.>)
, then add (d, e) to r
and e
to 
and B
, where e is a fresh
element.
/ B IT ,A , then add e to
4. If A v r.B  T , funct(r)  T , d  AIT ,A , (d, e)  rIT ,A , and e 
I
T
,A
B
.
The construction can be rendered deterministic by using an ordering of the inclusions and domain
elements to decide among different possible rule applications. For this reason, we may speak of the
canonical model. We call a model U of T and A universal if there is a homomorphism from U to
any model I of T and A, that is, a function h : U  I such that d  AU implies h(d)  AI ,
(d, e)  rU implies (h(d), h(e))  rI , and h(aU ) = aI for all a  Ind(A). The most important
property of IT ,A is that it is universal.2 In fact, the following is standard to prove and we omit
details, see for example the work of Lutz and Wolter (2012).
Lemma 29 Let T be an ELIF  -TBox and A an ABox that is satisfiable w.r.t. T . Then IT ,A is a
universal model of (T , A).
Let T be an ELIF  TBox and A a -ABox that is satisfiable w.r.t. T . It is an easy consequence
of Lemma 29 that a -ABox A is a witness for a CQ q being non-empty for  given T if and only
if A is satisfiable w.r.t. T and there is a match of q in IT ,A .
The next step in our proof of Theorem 27 is to establish a proposition that constrains the shape
of ABoxes to be considered when deciding emptiness in ELIF  . Here and in what follows, an
ABox A is called tree-shaped if
1. the undirected graph GA = (Ind(A), {{a, b} | r(a, b)  A}) is a tree;
2. For readers wondering about the relationship between universal models and materializations as defined in Section 3,
we remark that every universal model of a TBox T and ABox A is a materialization of T and A. Conversely, if there
is a materialization of T and A, then there exists also a universal model of T and A (Lutz & Wolter, 2012).

23

fiBAADER , B IENVENU , L UTZ , & W OLTER

2. for all a, b  Ind(A), there is at most one role name r such that r(a, b)  A or r(b, a)  A,
and only one of these is the case.
When working with tree-shaped ABoxes, we often designate one of the individuals as the root. If
the root of the tree-shaped ABox A has been fixed, then we use A|a to denote the restriction of A
to those individuals b whose unique path to the root in GA contains a, and we call b  Ind(A) an
r-successor (resp. r -successor) of a  Ind(A) if r(a, b)  A|a (resp. r(b, a)  A|a ). We will
also consider (rooted) tree-shaped interpretations and tree-shaped queries, defined analogously to
tree-shaped ABoxes.
A -ABox A is forest-shaped if there are ABoxes A0 , A1 , . . . , Ak such that the following
conditions are satisfied:
1. A is the union of A0 , A1 , . . . , Ak ;
2. k  |Ind(A0 )|;
3. for 1  i < j  k: Ind(Ai )  Ind(Aj ) =  and |Ind(Ai )  Ind(A0 )| = 1;
4. for 1  i  k: Ai is a tree-shaped ABox rooted at some individual in Ind(A0 ).
We call A0 the root component of A and A1 , . . . , Ak the tree components. The width of A if k. The
degree of A is the smallest number n such that for every tree component Ai and every a  Ind(Ai ),
the number of assertions r(a, b) and r(b, a) in Ai is bounded by n. The following proposition
clarifies the role of forest-shaped ABoxes as witnesses for non-emptiness.
Proposition 30 Let T be an ELIF  -TBox,  an ABox signature, and q a CQ. If q is non-empty
for  given T , then this is witnessed by a -ABox that is forest-shaped, has width at most |q|, and
degree at most |T |.
Proposition 30 is proved in the appendix by taking a witness -ABox A, selecting a part of A of
size |q| that is identified by a match of q and that serves as the root component of the forest-shaped
ABox, then unraveling A into a infinite ABox starting from the selected part, afterwards removing
unnecessary individual names to obtain the desired degree, and finally applying compactness to
make the resulting witness finite.
Clearly, we can assume w.l.o.g. that in forest-shaped witness ABoxes according to Proposition 30, the individual names used in the root component are taken from a fixed set Ind of cardinality
|q|. We will make this assumption without further notice in what follows.
We next analyze matches in forest-shaped ABoxes, using a splitting of the query into components. These are similar to the splittings of queries used in Appendix B, but simpler. A forest
splitting of a CQ q is a tuple F = (q 0 , q0 , q1 , . . . , qn , ) where q 0 can be obtained from q by identifying variables, q0 , q1 , . . . , qn is a partition of the atoms of q 0 , and  : var(q0 )  Ind such that the
following conditions are satisfied
1. q1 , . . . , qn are tree-shaped;
2. var(qi )  var(q0 )  1 for 1  i  n;
3. var(qi )  var(qj ) =  for 1  i < j  n.
24

fiQ UERY AND P REDICATE E MPTINESS

Let T be an ELIF  -TBox, A a forest-shaped ABox with root component A0 , and  a match
of q in IT ,A . Note that IT ,A consists of A extended with (potentially infinite) trees attached
to ABox individuals that have been generated by the completion rules. Then  is of type F =
(q 0 , q0 , q1 , . . . , qn , ) if q 0 can be obtained from q by identifying those variables that  sends to the
same element, q0 consists of the atoms in q 0 that  matches to the A0 -part of IT ,A , q0 , . . . , qn are
the maximal connected components of q 0 \ q0 , and  is the restriction of  to range Ind. Note
that, no matter which match  we choose, the maximal connected components of q 0 \ q0 must be
tree-shaped because they match into a tree-shaped part of IT ,A , which consists of a tree component
of A plus the attached trees generated by the completion rules. Thus every match  has a type and
the following is immediate, where WT ,q,F denotes the set of forest-shaped -ABoxes of width at
most |q| and degree at most |T | that admit a match of q which is of type F .
Lemma 31 Let T be an ELIF  -TBox,  an ABox signature, and q a CQ. Then q is empty for 
given T if and only if WT ,q,F is empty for every forest splitting F of q.
From now on, let T be an ELIF  -TBox T in normal form,  an ABox signature, and q a CQ, and
assume that we want to decide whether q is empty for  given T . By Lemma 31, it suffices to check
whether WT ,q,F is empty for every forest splitting F of q.
Note that defining the set WT ,q,F is possible only because the definition of a forest splitting
does not refer to a particular ABox, which in turn is due to our use of the fixed set of individual
names Ind for the root components of forest ABoxes. In fact, first quantifying over forest splittings
as in Lemma 31 and then quantifying over forest-shaped ABoxes (when testing emptiness of some
WT ,q,F ) is essential for obtaining a single exponential time upper bound. Since the number of
forest splittings is single exponential in |q|, we obtain such a bound if we can test emptiness of
each WT ,q,F in time single exponential in |T | + |q|. We will achieve this by constructing, for each
forest splitting F of q, a two-way alternating parity automaton on infinite trees (TWAPA) AF that
accepts a non-empty language if and only if WT ,q,F 6= . Note that infinite trees are needed because
automata will take trees as input that represent not only a (finite) forest-shaped -ABox A, but also
a (potentially infinite) model of A and T .
We start by introducing the necessary background for TWAPAs. Let
denote the positive
integers. A tree is a non-empty (and potentially infinite) set T   closed under prefixes. The
node  is the root of T . We use standard concatenation on the words from  (nodes of trees) and,
as a convention, take x  0 = x and (x  i)  1 = x for all x   and i  . Note that   1 is
undefined. When i  1, the node x  i is said to be a child of the node x, and x is called the parent of
x  i. We will slightly depart from Vardis original definition of TWAPAs (Vardi, 1998) by working
with trees which are not full, that is, we define an m-ary tree as a tree each of whose nodes has at
most (rather than exactly) m children. W.l.o.g., we assume that all nodes in an m-ary tree are from
{1, . . . , m} . An infinite path P of T is a prefix-closed set P  T such that for every n  0, there
is a unique x  P with |x| = n.
For any set X, we use B + (X) to denote the set of all positive Boolean formulas over X, i.e.,
formulas built using conjunction and disjunction over the elements of X used as propositional variables, and where the special formulas true and false are allowed as well. For an alphabet , a
-labeled tree is a pair (T, V ) with T a tree and V : T   a node labeling function.

N

N

N
N
N

Definition 32 (TWAPA) A two-way alternating parity automaton (TWAPA) on m-ary trees is a
tuple A = (S, , , s0 , F ) where S is a finite set of states,  is a finite alphabet,  : S   
25

fiBAADER , B IENVENU , L UTZ , & W OLTER

B + (tran(A)) is the transition function with tran(A) = {hii, [i] | i  {1, 0, . . . m}}  S the set
of transitions of A, s0  S is the initial state, and F = (G1 , . . . , Gk ) is a sequence of subsets of S
satisfying G1  G2  . . .  Gh = S, called the parity condition.
Intuitively, a transition (hii, s) with i > 0 means that a copy of the automaton in state s is sent to the
i-th successor of the current node, which is then required to exist; by contrast, the transition ([i], s)
only sends a copy if the i-th successor exists. The transitions (hii, s) and ([i], s) with i  {1, 0} are
interpreted similarly where 1 indicates sending a copy to the predecessor and 0 indicates sending
a copy to the current node. Note that a transition (h1i, s) cannot be applied at the root.
Definition 33 (Run, Acceptance) A run of a TWAPA A = (S, , , s0 , F ) on a -labeled tree
(T, V ) is a T  S-labeled tree (T , ) such that () = (, s0 ) and for all y  T , (y) = (x, s)
and (s, V (x)) =  implies that there is a (possibly empty) set {(d1 , s1 ), . . . , (dn , sn )}  tran(A)
that satisfies  and is such that for 1  i  n:
1. if di = hji, then x  j is defined, x  j  T , y  i  T , and (y  i) = (x  j, si ).
2. if di = [j] and x  j is defined and belongs to T , then y  i  T and (y  i) = (x  j, si ).
Given an infinite path P  T , we denote by inf(P ) the set of all states q such that there are infinitely
many y  P such that (y) is of the form (d, q). We say that the run (T , ) is accepting if for all
infinite paths P  T , there exists an even k such that inf(P )  Gk 6=  and inf(P )  Gk1 = .
A -labeled tree (T, V ) is accepted by A if there is an accepting run of A on (T, V ). We use
L(A) to denote the set of all -labeled trees accepted by A.
We note that the original definition of TWAPAs (Vardi, 1998) only uses transitions of the form
(hii, q) with i  {1, . . . , m}, since (hii, q) and ([i], q) coincide for full m-ary trees. It is easy to
see that emptiness for our version of TWAPAs can be reduced in polynomial time to emptiness for
TWAPAs in the original definition since we can encode m-ary trees as full m-ary trees. Vardi (1998)
has shown that the emptiness problem of TWAPAs is E XP T IME-complete. More precisely, there
is an algorithm that, given a TWAPA A = (S, , , s0 , F ), decides whether L(A) =  and runs in
time exponential in the cardinality of S and polynomial in the cardinality of  and size of . We
also remind the reader that given two TWAPAs A1 and A2 with Ai = (Si , i , i , s0,i , Fi ), it is very
easy to construct (in polynomial time) a TWAPA A such that L(A) = L(A1 )  L(A2 ) and A has
state set S1  S2 .
To make them accessible to TWAPAs, we encode forest-shaped -ABoxes of width at most |q|
and degree at most |T | as m-ary trees, where m = |q|  |T |. As has already been mentioned, each
such tree additionally encodes a model of the encoded ABox. We now explain the alphabet used
and the shape of the trees in more detail. The root node is labeled with an element of the alphabet
R that consists of all sig(T )-ABoxes A such that (i) Ind(A)  Ind, (ii) r(a, b)  A implies r  ,
and (iii) A satisfies all functionality statements in T . Let sub(T ) denote the set of concepts that
occur in T and their subconcepts. Non-root nodes are labeled with elements from the alphabet N
that consists of all subsets
  (NC  sub(T )) ] {M } ] {r, r | r  NR occurs in T } ] Ind ] {A | B v r.A  T }
26

fiQ UERY AND P REDICATE E MPTINESS

such that  contains (i) exactly one role name or inverse role, (ii) at most one element of Ind, and
(iii) either M and a role name or inverse role from  or exactly one element of the form A and, in
the latter case, also A.
A tree hT, `i with ` a labeling as described above is supposed to represent a forest-shaped ABox AhT,`i together with a model IhT,`i of this ABox and of T . The individuals of AhT,`i are
those in the ABox that labels the root of T , plus all non-root nodes of T whose label contains the
marker M . All other nodes of T denote domain elements of IhT,`i that are not identified by any
ABox individual. Both the assertions in AhT,`i and the concept and role memberships in IhT,`i are
represented by the labels of hT, `i. Note that the ABox A is a sig(T )-ABox whereas AhT,`i uses
signature . In fact, only the -assertions in A will be part of AhT,`i while all assertions in A will
be part of IhT,`i .
We need to impose some additional conditions to make sure that a R  N -labeled tree hT, `i
indeed represents an ABox and model as intended. We call hT, `i proper if it satisfies the following
conditions for all x  T :
1. the root is labeled with an element A of R and all other nodes with an element of N ;
2. `(x)  N contains an element of Ind(A) if x is a child of the root of T , and no element of
Ind otherwise;
3. if we take any path in T and remove the root node (because it carries a special label), then the
nodes whose label contains M form a finite (possibly empty) prefix of the resulting path;
4. if y is a child of x and A  `(y)  N , then there is some B v r.A  T such that one of
the following is true:
(a) x is not the root, B  `(x) and r  `(y);
(b) x is the root and for some a  Ind, B(a)  `(x) and {a, r}  `(y).
The roles and individual names in element labels describe how these elements are connected to
other elements via roles in AhT,`i and in IhT,`i . In particular, if a successor of the root contains both
the role r and the individual name a  Ind, then that node represents an r-successor of a. The label
elements that are of the form A serve a special marking purpose: if A  `(x), then this means
that the element x (which is part of IhT,`i but not of AhT,`i since `(x) cannot contain M ) is there
to satisfy some concept inclusion B v r.A. We will later need these special markers to make
sure that IhT,`i is not just some model of AhT,`i , but a materialization of AhT,`i and T . With these
explanations and the subsequent definitions, the three conditions imposed on the elements of  and
the four conditions used to define properness above should make sense to the reader.
Let hT, `i be a proper R  N -labeled tree. We now define AhT,`i and IhT,`i formally. Let A
be the ABox that labels the root  of T , and let A be the restriction of A to signature . Then the
-ABox AhT,`i described by hT, `i is
AhT,`i = A  {A(x) | A  `(x)  NC   and M  `(x)}
 {r(b, x) | {b, r, M }  `(x)}  {r(x, b) | {b, r , M }  `(x)}
 {r(x, y) | y is a child of x and M  `(x)  `(y) and r  `(y)}
 {r(y, x) | y is a child of x and M  `(x)  `(y) and r  `(y)}
27

fiBAADER , B IENVENU , L UTZ , & W OLTER

and the interpretation IhT,`i is as follows:
IhT,`i

= (T \ {})  Ind(A)

A

IhT,`i

= {a | A(a)  A}  {x | A  `(x)  NC }

r

IhT,`i

= {(a, b) | r(a, b)  A}  {(a, x) | {a, r}  `(x)}  {(x, a) | {a, r }  `(x)}
 {(x, y) | y is a child of x and r  `(y)}  {(x, y) | x is a child of y and r  `(x)}

cIhT,`i

= c

for all c  Ind(AhT,`i )

Apart from being represented as an ABox instead of as an interpretation, AhT,`i is identical to the
restriction of IhT,`i to the individuals in AhT,`i and symbols in , which in particular means that
IhT,`i is a model of AhT,`i . Note that the ABox AhT,`i is a forest-shaped -ABox. Conversely,
for any forest-shaped -ABox of width at most |q| and degree at most |T |, we can define a proper
m-ary R  N -labeled trees hT, `i such that AhT,`i = A and IhT,`i = IT ,A .
Let F = (q 0 , q0 , q1 , . . . , qn , ) be a splitting for q. We now build a TWAPA AF over m-ary
R  N -labeled trees that accepts exactly those trees hT, `i such that AhT,`i  WT ,q,F . The
number of states of AF will be polynomial in |T | + |q| and since it can be checked in time singleexponential in the number of states whether L(AF ) =  , we obtain the desired E XP T IME upper
bound for deciding whether WT ,q,F = . We construct AF as the intersection of the following
TWAPAs:
1. Aprop , which makes sure that the input tree is proper;
2. AT , which ensures that the input tree hT, `i is such that IhT,`i is a model of T ;
3. Awf which ensures that hT, `i satisfies a certain well-foundedness condition;
4. Amatch which guarantees, exploiting the conditions ensured by the previous automata, that
the input tree hT, `i is such that AhT,`i  WT ,q,F .
The construction of the first automaton Aprop is straightforward, and details are left to the reader.
Note that, to enforce Condition 3 of proper trees, the automaton needs to make use of the parity
acceptance condition (a co-Buchi condition would actually be sufficient). The second TWAPA AT
ensures that the following conditions are satisfied for all non-root nodes x, x0 of the input tree:
 if r(a, b)  `() and funct(r)  T , then {a, r} 6 `(x);
 if funct(r)  T and {a, r}  `(x)  `(x0 ), then x = x0 ;
 if funct(r)  T , then x has at most one child y with r  `(y), and if additionally r  `(x),
then there is no such child y;
 if A1 u    u An v A  T and {A1 (a), . . . , An (a)}  `(), then A(a)  `();
 if A1 u    u An v A  T and {A1 , . . . , An }  `(x), then A  `(x);
 if A v r.B  T and A(a)  `(), then (i) there is some b such that {r(a, b), B(b)}  `(),
or (ii) there is a child x of the root such that {a, r, B}  `(x);
28

fiQ UERY AND P REDICATE E MPTINESS

 if A v r.B  T and A  `(x), then (i) {a, r }  `(x) and B(a)  `(), (ii) r  `(x) and
x has non-root parent y with B  `(y), or (iii) x has a child y with {r, B}  `(y);
 if r.A v B  T and (i) {r(a, b), A(b)}  `() or (ii) there is some child y of the root such
that {a, r, A}  `(y), then B(a)  `();
 if r.A v B  T and (i) {a, r }  `(x) and A(a) is in the label of the root, (ii) r  `(x)
and x has parent y with A  `(y), or (iii) x has child y with {r, A}  `(y), then B  `(x).
Working out the exact details of Aprop is again left to the reader.
Ideally, we would like the third automaton Awf to ensure that IhT,`i is the canonical model of T
and AhT,`i . However, this does not seem to be easily possible because that model is constructed by
applying completion rules in a certain order which is difficult to simulate by an automatonnote
that applying the rules in a different order might result in the construction of an interpretation that is
not isomorphic to the one obtained when following the prescribed application order. We thus define
Awf to achieve only the crucial property of canonical models that all positive information (concept
and role memberships of domain elements) is there for a reason, namely because it is contained in
AhT,`i or because it is logically implied by AhT,`i together with T . We formalize this in terms of
derivations.
I
Let hT, `i be a proper R  N -labeled tree, A0  (NC  sub(T ))  {>}, and x0  A0 hT,`i . A
derivation of A0 at x0 in hT, `i is a finite L-labeled tree hT 0 , `0 i, where L is the set of pairs (A, x)
with A  (NC  sub(T ))  {>} and x  AIhT,`i . We require that the root of T 0 is labeled with
(A0 , x0 ) and that T 0 is minimal such that for all nodes z of T 0 with `0 (z) = (A, x), one of the
following holds:
1. A    {>} and x  Ind(AhT,`i );
2. A 6 `(x) and there are a CI A1 u    u An v A  T and children z1 , . . . , zn of z in T 0 such
that `0 (zi ) = (Ai , x) for 1  i  n;
3. A 6 `(x) and there is a CI r.A0 v A  T and a child z 0 of z in T 0 with `0 (z 0 ) = (A0 , x0 )
such that (x, x0 )  rIhT,`i . Moreover, if B   `(x), then there is a child z 00 of z in T 0 with
`0 (z 00 ) = (B, x).
4. A 6 `(x) and there is a CI A0 v r.A  T with funct(r)  T and a child z 0 of z in T 0 with
`0 (z 0 ) = (A0 , x0 ) such that (x0 , x)  rIhT,`i . Moreover, if B   `(x), then there is a child z 00
of z in T 0 with `0 (z 00 ) = (B, x).
5. A = >, > 6 `(x), B   `(x), and there is a child z 0 of z in T 0 with `0 (z 0 ) = (B, x).
6. A  `(x), there is a CI A0 v r.A  T , and there is a child z 0 of z in T 0 with `0 (z 0 ) =
(A0 , x0 ) such that (x0 , x)  rIhT,`i and either (i) x is a child of x0 in T , or (ii) x is a child of
the root, x0  Ind, and {r, x0 }  `(x).
We say that hT, `i is well-founded if whenever x  AIhT,`i , with A  NC  {>}, then there is a
derivation of A at x in hT, `i. It is not hard to construct a TWAPA Awf that accepts precisely the
well-founded proper R  N -labeled trees; essentially, the automaton can verify the existence of
all required derivations by implementing the Conditions 1 to 6 above as transitions, additionally
using a co-Buchi condition to ensure finiteness of the derivation.
Next let F = (q 0 , q0 , q1 , . . . , qn , ). The automaton Amatch checks that
29

fiBAADER , B IENVENU , L UTZ , & W OLTER

1.  is a match for q0 in IhT,`i and
2. there is a match  for qi in IhT,`i such that if v  var(q0 )  var(qi ), then (v) = (v).
Amatch is easy to construct and we once more omit details. As announced, we define AF such that
it accepts the intersection of the languages accepted by Aprop , AT , Awf , and Amatch . It remains to
show that WT ,q,F is empty iff L(AF ) is empty.
To do this, we first clarify the relation between well-foundedness, canonical models, and universal models. We call a proper R  N -labeled tree hT, `i canonical if (i) IhT,`i is the canonical
model of AhT,`i and T , and (ii) for every x  T \{} with M 6 `(x), the concept A  `(x) is such
that the element x was created due to an application of the third completion rule to an inclusion of
the form B v r.A and the parent of x in T .
Lemma 34
1. If a proper R  N -labeled tree is canonical, then it is well-founded;
2. If hT, `i is a proper R  N -labeled tree that is well-founded and IhT,`i is a model of T ,
then IhT,`i is a universal model of T and AhT,`i .
A proof of Lemma 34 can be found in the appendix. Point 1 is established by tracing the applications
of the completion rules applied to construct the canonical model of AhT,`i and T and showing that
each addition that they make gives rise to a derivation. For Point 2, we first show that one can
make a certain uniformity assumption on derivations and then show how to define a homomorphism
from IhT,`i to any model of AhT,`i and T by starting at the part of IhT,`i that corresponds to the root
component of AhT,`i and then moving downwards along the tree-shaped parts of IhT,`i .
Lemma 35 WT ,q,F =  iff L(AF ) = .
Proof. First assume that WT ,q,F 6= . Then there is a forest-shaped -ABox A of width at most
|T | and degree at most |q| that is satisfiable w.r.t. T and a match  of q in IT ,A that is of type F .
Let hT, `i be an m-ary proper R  N -labeled tree that satisfies AhT,`i = A and is canonical.
Then hT, `i  L(Aprop ). Since A is satisfiable w.r.t. T , IhT,`i = IT ,A is a model of T and thus
hT, `i  L(Aprop ). By Point 1 of Lemma 34, hT, `i  L(Awf ). Finally, the match  witnesses that
Conditions 1 and 2 from the definition of Amatch are satisfied and thus hT, `i  L(Amatch ) and we
are done.
Conversely, assume that there is a tree hT, `i  L(AF ). Since hT, `i  L(Aprop ), A = AhT,`i
is defined; by definition, it is a forest-shaped -ABox of width at most |T | and degree at most |q|.
It remains to show that there is a match  of q in IT ,A that is of type F . Since hT, `i  L(AT ) 
L(Awf ), IhT,`i is a model of T and hT, `i is well-founded. By Point 2 of Lemma 34, IhT,`i is thus
a universal model of T and A. Because hT, `i  L(Amatch ), Conditions 1 and 2 from the definition
of Amatch are satisfied. It can be verified that, consequently, there is a match  of q in IhT,`i that is
of type F . Composing  with the homomorphism from IhT,`i to IT ,A , which exists since IhT,`i is
universal, yields a match of q in IT ,A that is of type F .
o

30

fiQ UERY AND P REDICATE E MPTINESS

6. The DL-Lite Family
We study query and predicate emptiness in the DL-Lite family of description logics (Calvanese
et al., 2007; Artale et al., 2009). To begin with, we introduce the dialects of DL-Lite we consider.
Basic concepts B are defined by
B

::=

>

| A

|

r.>

where A ranges over NC and r over all (possibly inverse) roles. A DL-Litecore TBox T is a finite
set of CIs of the form B1 v B2 and B1 u B2 v , where B1 and B2 are basic concepts. Thus,
DL-Litecore is included in ELI but, because it includes inverse roles, not included in EL. DL-LiteF
is the extension of DL-Litecore with functionality statements. DL-LiteR is the extension of DLLitecore with role inclusions r v s, where r, s are roles. DL-LiteR is the logical underpinning of the
OWL profile OWL2 QL (Motik et al., 2009). Finally, DL-Litehorn is the extension of DL-Litecore
with conjunctions of basic concepts on the left hand side of CIs. Alternatively, it can be defined as
the fragment of ELI  with qualified existential restrictions r.C replaced by unqualified existential
restrictions r.>. For further details, we refer readers to the work of Calvanese et al. (2007), Artale
et al. (2009), and Calvanese, De Giacomo, Lembo, Lenzerini, and Rosati (2013).
We briefly discuss the UNA for the DL-Lite dialects introduced above. First observe that DLLitehorn and DL-LiteF are fragments of ALCIF. Thus, by Lemma 3, query emptiness and predicate emptiness for DL-Litehorn and DL-LiteF are oblivious as to whether the UNA is made or not.
DL-LiteR is not a fragment of ALCIF. It is, however, straightforward to show that for DL-LiteR
the certain answers to CQs do not depend on whether one adopts the UNA or not. Thus, also for
DL-LiteR query emptiness and predicate emptiness are oblivious as to whether the UNA is made or
not. In the following proofs we make the UNA.
Our main results are as follows: CQ-query emptiness is coNP-complete for all DL-Lite dialects.
The coNP-lower bound holds already for the fragment of DL-Litecore without role names. By
contrast, the complexity of deciding IQ-query emptiness and CQ-predicate emptiness depends
on whether conjunctions are admitted on the left-hand side of concept inclusions or not. If no
conjunctions are admitted (as in DL-Litecore , DL-LiteR , and DL-LiteF ), then IQ-query emptiness
and CQ-predicate emptiness are NL OG S PACE-complete. If conjunctions are admitted (as in DLLitehorn ), then both IQ-query emptiness and CQ-predicate emptiness are coNP-complete. Again,
the lower bound holds already for the fragments of the DLs without role names.
We note that in what follows we do not use Theorem 6 which gives a polynomial reduction of
CQ-predicate emptiness to IQ-query emptiness for certain DLs but does not apply to the DL-Lite
dialects. Instead we give direct proofs. The results presented in Figure 1 for the DL-Lite dialects
are straightforward consequences of the results established in this section.
We begin by proving the coNP lower bounds. Let Lcore be the DL that admits only CIs A v B
and A u B v , and let Lhorn be the DL that admits only CIs A u A0 v B and A u B v , where
A, A0 , and B are concept names.
Theorem 36 In Lhorn , IQ-query emptiness, CQ-query emptiness, and CQ-predicate emptiness are
coNP-hard. In Lcore , CQ-query emptiness is coNP-hard.
Proof. The proofs are by reduction from the well-known coNP-complete problem of testing whether
a propositional formula in conjunctive normal form (CNF) is unsatisfiable. Let  = 1      k
31

fiBAADER , B IENVENU , L UTZ , & W OLTER

be a CNF formula, v1 , . . . , vn the variables used in , A1 , . . . , Ak concept names for representing clauses, and Av1 , Av1 , . . . , Avn , Avn concept names for representing literals. Let A be an
additional concept name, and set  = {Av1 , Av1 , . . . , Avn , Avn }. Consider the Lhorn -TBox T
consisting of the following CIs:
 Avj u Avj v  for all 1  j  n;
 A`j v Ai for all 1  i  k and each `j = ()vj that is a disjunct of i ;
 A1 u    u Ak v A .
It is straightforward to show that A(u) is empty for  given T iff u A(u) is empty for  given T
iff  is unsatisfiable. Thus, deciding IQ-query emptiness, CQ-predicate emptiness, and CQ-query
emptiness in Lhorn is coNP-hard. For the coNP-hardness result for CQ-query emptiness in Lcore ,
we drop the last CI from T and use the CQ A1 (u)      Ak (u) instead.
o
We now prove matching upper complexity bounds, considering the logics DL-Litecore , DLLiteR , DL-LiteF , and DL-Litehorn . To this end, we formulate general sufficient conditions for
deciding emptiness in PT IME and in coNP. We say that a DL L has the polysize emptiness witness property if whenever a CQ q is not empty for  given T , then there exists a -ABox A of
polynomial size in the size of T and q that is satisfiable w.r.t. T and such that certT ,A (q) 6= .
Lemma 37 Let L be any description logic with the polysize emptiness witness property such that
the query evaluation problem for CQs for L is in NP. Moreover, assume that satisfiability of ABoxes
w.r.t. L-TBoxes can be decided in polynomial time. Then CQ-query emptiness in L is in coNP.
Proof. An NP-algorithm deciding whether a CQ q is not empty w.r.t. T and  guesses (i) a -ABox
A of polynomial size in T and q, (ii) a tuple ~a of individual names from Ind(A) of the appropriate
length, and (iii) a polysize certificate that ~a  certT ,A (q); it then verifies in polynomial time that A
is satisfiable w.r.t. T and the guessed certificate is valid.
o
Theorem 38 In DL-Litecore , DL-LiteR , DL-LiteF , and DL-Litehorn , deciding CQ-query emptiness
is in coNP.
Proof. The conditions stated in Lemma 37 have been shown by Calvanese et al. (2007) and Artale
et al. (2009). We sketch a proof of the polysize emptiness witness property. Assume ~a  certT ,A (q)
for a CQ q = ~u (~v , ~u) and a TBox T in any of the DLs listed in the theorem statement and
further assume that A is satisfiable w.r.t. T . We use the canonical model IT ,A from Lemma 29
(for DL-Litecore , DL-LiteF , and DL-Litehorn this can be used without any modification since they
are fragments of ELIF  ; for DL-LiteR , one has to add the following completion rule for the
construction of IT ,A : if (x, y)  rIT ,A and r v s  T , then add (x, y) to sIT ,A ). Let  be a
match for q in IT ,A . We recall that IT ,A consists of its restriction to the individuals aIT ,A with
a  Ind(A) and tree-shaped interpretations Ia attached to aIT ,A . Let A00 be the set of assertions in
A that only use individual names a  Ind(A) such that there exists v  var(q) with (v) = aIT ,A
or (v)  Ia . Moreover, for any individual a  Ind(A00 ) selected above such that there exists a
role r and b with r(a, b)  A, select one such r(a, b ) and include it in A01 . Let A0 = A00  A01 .
Clearly the ABox A0 is as required: it is of polynomial size, it is satisfiable w.r.t. T (being a subset
of A), and by construction, it satisfies ~a  certT ,A0 (q).
o

32

fiQ UERY AND P REDICATE E MPTINESS

We say that a DL L has the singleton emptiness witness property if whenever a CQ q of the form
A(v) or v A(v) is not empty for  given T , then there exists a -ABox A containing at most one
assertion which is satisfiable w.r.t. T and such that certT ,A (q) 6= .
Lemma 39 Let L be any description logic with the singleton emptiness witness property such that
the query evaluation problem for CQs of the form A(v) and v A(v) for L is in NL OG S PACE. Moreover, assume that satisfiability of singleton ABoxes w.r.t. L-TBoxes can be decided in NL OG S PACE.
Then IQ-query emptiness and CQ-predicate emptiness in L are in NL OG S PACE.
Proof. A non-deterministic logarithmic space algorithm deciding whether a CQ of the form A(v)
or v A(v) is not empty w.r.t. T iterates over all -ABoxes A containing at most one assertion and
checks whether at least one of those ABoxes A is satisfiable w.r.t. T and satisfies T , A |= v A(v)
or, respectively, T , A |= A(a), for an individual a  Ind(A).
o
Theorem 40 In DL-Litecore , DL-LiteR , and DL-LiteF , deciding IQ-query emptiness and CQpredicate emptiness are NL OG S PACE-complete.
Proof. For the NL OG S PACE-upper bound, the conditions stated in Lemma 39 have been shown
by Calvanese et al. (2007) and Artale et al. (2009). We sketch a proof of the singleton emptiness
witness property. Assume ~a  certT ,A (q) for a CQ q of the form A(v) or v A(v) and a TBox
T in any of the DLs listed in the theorem statement. Further assume that A is satisfiable w.r.t. T .
We consider the case q = v A(v); the case q = A(v) is similar. As in the proof of Theorem 38,
we use the canonical model IT ,A . Let  be a mapping into IT ,A such that (v)  AIT ,A and
consider the uniquely determined a  Ind(A) such that (v) = aIT ,A or (v)  Ia . Using the
fact that no conjunctions occur on the left-hand side of CIs in T , one can show that there exists a
single assertion of the form B(a) or r(a, b) such that for the ABox A0 consisting of that assertion,
we have ()  certT ,A0 (q). It follows that A0 is the desired witness ABox.
The matching lower bound follows directly from the fact that deciding whether T |= A v B is
NL OG S PACE-hard for TBoxes T in DL-Litecore (Artale et al., 2009).
o

7. Case Study and Application to Modularity
We demonstrate the usefulness of emptiness in two ways. First, we carry out a case study for
predicate emptiness in the medical domain, where we find that the use of a realistic ontology adds
a significant number of non-empty predicates to the ABox signature while there is also a large
number of predicates that are empty. In static analysis, it is thus potentially non-trivial for a user to
manually distinguish the non-empty from the empty predicates. Second, we show that (predicate)
emptiness can be used to produce a smaller version of a TBox T that is tailor-made for querying
with a given ABox signature (in a sense: a module of the TBox). Replacing T with the potentially
much smaller module facilitates comprehension of the TBox, thus helping with query formulation.
We again support our claims by experiments.
In the case study, we use the comprehensive medical ontology SNOMED CT, which provides
a systematic vocabulary used for medical information interchange and to enable interoperable electronic health records. It covers diverse medical areas such as clinical findings, symptoms, diagnoses, procedures, body structures, organisms, substances, pharmaceuticals, devices and specimens.
33

fiBAADER , B IENVENU , L UTZ , & W OLTER

concepts roles

IQ
CQ axioms
axioms
non-empty non-empty -mod. CQ -core

500

16

3557

4631

8910

4597

500

31

3654

4734

8911

4696

1000

16

5827

7385

14110

7349

1000

31

6242

7762

14147

7731

5000

16

18330

21451

33469

21427

5000

31

18469

21557

33616

21532

10000

16

29519

33493

47044

33489

10000

31

30643

34645

47256

34637

Figure 4: Experimental Results
SNOMED CT is formulated in EL extended with role inclusions (which we removed for the experiments). It contains about 370,000 concept names and 62 role names. We use SNOMED CT
together with an ABox signature from a real-world application and with randomly generated ABox
signatures. The real-world signature was obtained by analyzing clinical notes of the emergency
department and the intensive care unit of two Australian hospitals, using natural language processing methods to detect SNOMED CT concepts and roles.3 It contains 8,858 concepts and 16 roles.
For this signature, 16,212 IQ-non-empty predicates and 17,339 CQ-non-empty predicates were
computed. Thus, SNOMED CT provides a substantial number of additional predicates for query
formulation, roughly identical to the number of predicates in the ABox signature. However, these
numbers also show that the majority of predicates in SNOMED CT cannot meaningfully be used in
queries over -ABoxes, and thus identifying the relevant ones via predicate emptiness is potentially
very helpful. Somewhat surprisingly, the number of CQ-non-empty predicates is only about 10%
higher than the number of IQ-non-empty symbols.
We have analyzed randomly generated signatures that contain 500, 1,000, 5,000, and 10,000
concept names and 16 or 31 role names (1/2 and 1/4 of the role names in the ontology). Every
signature contains the special role name role-group, which is used in SNOMED CT to implement
a certain modeling pattern and should be present also in ABoxes to allow the same pattern there.
For each number of concept and role names, we generated 10 signatures. The columns IQ nonempty and CQ non-empty of Figure 4 show the results, where the numbers are averages for the
10 experiments for each size. These additional experiments confirm the findings for our real-world
signature: in each case, a substantial number of additional predicates becomes available for query
formulation, but there is also a large number of predicates that are empty.
We now come to the application in modularity. Recall that our main motivation for studying
emptiness is to support query formulation: as TBoxes can be large and complex, it can be difficult
to understand whether a TBox contains sufficient background knowledge so that a given query q
can have a non-empty answer over a -ABox. If this is not the case, it clearly does not make sense
3. See Current Collaborative Projects of the Health Information Technologies Research Laboratory at the University
of Sydney (HITRL, 2016).

34

fiQ UERY AND P REDICATE E MPTINESS

to pose q to any -ABox when this TBox is used as the background ontology. Similarly, it can be
hard to find out whether a TBox is sufficiently powerful to entail that a given predicate can occur
in some query that has a non-empty answer over some -ABox. Again, if this is not the case, then
that predicate should not be used when formulating queries. Here, we go one step further: instead
of using emptiness directly to support query formulation, we use it to simplify the TBox. More
precisely, we consider the problem of extracting a (hopefully small!) subset of a given TBox that
gives exactly the same answers to all CQs (or IQs) for any -ABox. Such a subset will be called
a -substitute w.r.t. CQ (or IQ, respectively) of the original TBox and can replace the original
TBox when answering CQs (or IQs, respectively). Working with a small -substitute instead of
the original TBox supports comprehension of the TBox and thereby the formulation of meaningful
queries.
It is beyond the scope of this paper to investigate -substitutes in depth. Instead, we show that,
in the description logic ELI, predicate emptiness gives rise to a particularly natural kind of substitute that we call the CQ -core. The CQ -core is obtained by removing all concept inclusions
that contain a predicate which is CQ-predicate empty for  w.r.t. the TBox. Thus, not only does
the CQ -core give the same answers to CQs as the original TBox for -ABoxes, but it also has
the appealing property that all predicates which occur in it can be used meaningfully in a CQ when
querying -ABoxes.
We also show that the widely known semantic -modules introduced by Grau et al. (2008) are
-substitutes and that CQ -cores cannot be larger than semantic -modules (unless the original
ontology contains tautological concept inclusions). To evaluate the method in practice and compare
the size of CQ -cores and -modules, we also extend our case study based on SNOMED CT to the
extraction of CQ -cores and their comparison with -modules. We start by defining -substitutes
in a formal way.
Definition 41 Let T 0  T and Q  {IQ, CQ}. Then T 0 is a -substitute for T w.r.t. Q if for all
-ABoxes A and all q  Q, we have that certT 0 ,A (q) = certT ,A (q).
We are not aware that -substitutes according to Definition 41 have been studied before, but they are
closely related to other types of modules. For example, -modules that give the same answers to all
CQs formulated in signature  for all -ABoxes are studied in the work of Lutz and Wolter (2010),
Kontchakov, Wolter, and Zakharyaschev (2010), Konev, Ludwig, Walther, and Wolter (2012), Botoeva, Kontchakov, Ryzhikov, Wolter, and Zakharyaschev (2014, 2016), and Romero, Kaminski,
Grau, and Horrocks (2015). A stronger version of a module is provided by -modules that require the original TBox to be a model-conservative extension of the module regarding the signature
, as studied by Konev, Lutz, Walther, and Wolter (2013) and Gatens, Konev, and Wolter (2014).
However, an important difference between all those -modules and our -substitutes is that the
latter only restrict the signature of the ABox, but not of the queries. In contrast, the mentioned
-modules only guarantee the same answers to CQs formulated in signature  (and for -ABoxes).
In particular, it follows that minimal modules, as defined in the work of Kontchakov et al. (2010)
and Konev et al. (2013), can in general not be used as a -substitute.
We now show that in ELI (and, therefore, also in its fragment EL) one can use CQ-predicate
emptiness in a straightforward way to compute a -substitute w.r.t. CQ. Let T be a TBox and  an
ABox signature. The CQ -core of T , denoted TCQ , is the set of all concept inclusions   T such
that no X  sig() is CQ-predicate empty for  given T .

35

fiBAADER , B IENVENU , L UTZ , & W OLTER

Theorem 42 Let T be a TBox in ELI. Then the CQ -core of T is a -substitute for T w.r.t. CQ
(and thus also w.r.t. IQ).
Proof. Let T 0 be the CQ -core of T and assume that T 0 , A 6|= q[~a] for some -ABox A. Consider
the canonical model IT 0 ,A , introduced in Section 5.2. Then IT 0 ,A is a model of T 0 and A, and
IT 0 ,A 6|= q[~a]. It is sufficient to show that IT 0 ,A is a model of T . Let C v D  T \ T 0 and
assume that IT 0 ,A 6|= C v D. Then C IT 0 ,A 6= . Let qC (v) be the tree-shaped conjunctive
query corresponding to C, constructed in the standard way (see Appendix B for a formal definition
of a similar construction). Then IT 0 ,A |= v qC (v) and so T 0 , A |= v qC (v). Hence T , A |=
v qC (v) and all X  sig(C) are not CQ-empty for  given T . Since C v D  T , we also
obtain T , A |= v qD (v), where qD (v) is the tree-shaped conjunctive query corresponding to D.
Thus, no X  sig(D) is CQ-empty for  given T . But this means that C v D  T 0 , which is a
contradiction.
o
Note that by Theorem 22, the CQ -core can be computed in polynomial time if T is an EL-TBox.
We make some simple observations regarding CQ -cores:
1. Theorem 42 fails in DLs that admit negation. For example, for T = {A v B, B v E}
and  = {A}, any -substitute of T w.r.t. CQ coincides with T , but the CQ -core of T is
empty.
2. the CQ -core is not always a minimal -substitute w.r.t. CQ. Consider, for example, T =
{A v B1 , A v B2 , B1 v B2 } and let  = {A}. Then T 0 = {A v B1 , A v B2 } is a
-substitute w.r.t. CQ of T but the CQ -core of T coincides with T .
3. let the IQ -core of TBox T be defined in analogy to the CQ -core of T , but based on IQemptiness instead of CQ-emptiness. Then the IQ -core cannot serve as a -substitute of T
w.r.t. IQ even when T is an EL-TBox. For example, let T = {A v r.B, r.B v E} and
 = {A}. Then B is IQ-empty for  given T and so the IQ -core of T is empty. However,
the empty TBox is not a -substitute of T w.r.t. IQ since T , A |= E(a) for A = {A(a)}.
Interestingly, in contrast to the -modules discussed above, the -modules introduced by Grau
et al. (2008) turn out to be examples of -substitutes. To define -modules, let  be a signature.
0
0
Two interpretations I and I 0 coincide w.r.t.  if I = I and X I = X I for all X  . A
subset T 0 of a TBox T is called a semantic -module of T w.r.t.  if for every interpretation I the
0
interpretation I 0 that coincides with I w.r.t.   sig(T 0 ) and in which X I =  for all X 6  
sig(T 0 ) is a model of T \ T 0 . It is shown in the work of Grau et al. (2008) that extracting a minimal
semantic -module is of the same complexity as standard reasoning (that is, subsumption). In
addition, it is shown that a syntactic approximation called the syntactic -module can be computed
in polynomial time (every syntactic -module is a semantic -module, but not necessarily the
other way around). The following lemma establishes the relationship between -modules and substitutes. A concept inclusion C v D is tautological if  |= C v D.
Proposition 43 Let T be a TBox formulated in any of the DLs introduced in this paper, and let T 0
be a semantic -module of T w.r.t. . Then
1. T 0 is a -substitute of T w.r.t. CQ;
36

fiQ UERY AND P REDICATE E MPTINESS

2.   sig(T 0 ) contains all predicates that are not CQ-empty for  given T ;
3. if T is an ELI-TBox and does not contain tautological CIs, then the CQ -core of T is
contained in T 0 .
Proof. For Point 1, suppose T 0 , A 6|= q[~a], where T 0 be a semantic -module of T w.r.t.  and A
is a -ABox. Let I be a model of T 0 and A such that I 6|= q[~a], and consider the interpretation I 0
that coincides with I 0 on   sig(T 0 ) and in which X I =  for all remaining predicates X. Then
I 0 is a model of T since T 0 is a semantic -module of T w.r.t. . and I 0 is a model of A since A
is a -ABox. Since we only shrank the extension of predicates when transitioning from I to I 0 and
I 6|= q[~a], we have I 0 6|= q[~a]. Hence T , A 6|= q[~a], as required.
For Point 2, assume X is not CQ-empty for  given T , but X 6   sig(T 0 ). Suppose X = A
for a concept name A (the case X = r for a role name r is similar and left to the reader). Take a
-ABox A that is satisfiable w.r.t. T and such that T , A |= v A(v). Let I be a model of (T , A),
0
and let I 0 be the interpretation that coincides with I on   sig(T 0 ) and in which Y I =  for all
0
remaining Y (in particular we have AI = ). By definition of semantic -modules, I 0 is a model
of (T , A). We have derived a contradiction because I 0 shows that T , A 6|= v A(v).
For Point 3, assume T is formulated in ELI and contains no tautological inclusions. Let C v
D  T \ T 0 . Then, by the definition of semantic -modules, sig(C v D) contains a predicate
X 6   sig(T 0 ) (because otherwise C v D is a tautology). Thus, by Point 2, sig(C v D)
contains a predicate that is CQ-empty for  given T . But then C v D is not in the CQ -core of T ,
as required.
o
By Point 1, we can use the algorithms for computing syntactic or semantic -modules such as the
ones provided in the work of Grau et al. (2008) to find -substitutes in a large variety of DLs.
By Point 2, such modules also provide an over-approximation of the set of predicates that are not
CQ-empty. Finally, Point 3 means that, in ELI, -modules cannot be smaller than the CQ -core
unless there are tautological concept inclusions. In general, however, -modules can be larger than
the CQ -core of a TBox. The following example shows that this can be the case already for acyclic
EL-TBoxes: let
T = {A v s1 .r1 .> u s2 .r2 .>, B  r1 .> u r2 .>}
and  = {A}. The predicates that are not CQ-empty for  given T are A, s1 , s2 , r1 , r2 . Hence the
CQ -core of T contains only the first CI of T . However, T has no non-trivial semantic -modules
w.r.t.  (and thus no syntactic ones either).
We now demonstrate the potential usefulness of -substitutes and the CQ -core by extending
our case study from the medical domain. We again use the ontology SNOMED CT and the ABox
signatures described at the beginning of this section, analyzing the size of the CQ -core and comparing it to the size of the original ontology and of the syntactic -module. For the real-world
signature, CQ -core contains 17,322 of the 370,000 concept inclusions in SNOMED CT. Thus, it
is of about 5% the size of the original ontology. The -module w.r.t.  turns out to be significantly
larger than the CQ -core, containing 27,383 axioms. For the random signatures, the sizes of CQ cores and -modules are shown in the two right-most columns of Figure 4. They again confirm the
findings for the real-world signature: the CQ -core is much smaller both than the original ontology
and than the -module.
37

fiBAADER , B IENVENU , L UTZ , & W OLTER

Q UERY C ONTAINMENT

Q UERY E MPTINESS

DL

IQ

CQ

IQ

CQ

EL

E XP T IME-c.

E XP T IME-c.

PT IME-c.

PT IME-c

EL

E XP T IME-c.

E XP T IME-c.

E XP T IME-c.

E XP T IME-c

ELI, Horn-ALCIF

E XP T IME-c.

2E XP T IME-c.

E XP T IME-c.

E XP T IME-c.

in PT IME

coNP-c.

NL OG S PACE-c.

coNP-c.

coNP-c.

p2 -c.

coNP-c.

coNP-c.

NE XP T IME-c.

NE XP T IME-h.,

NE XP T IME-c.

NE XP T IME-c.

NE XP T IME-c.

2E XP T IME-c.

DL-Litecore
DL-Litehorn
ALC

in 2NE XP T IME
ALCI

NE XP T IME-c.

2NE XP T IME-c.

Figure 5: Query Containment vs Query Emptiness

8. Related Work
Query emptiness is a fundamental problem in the static analysis of database queries. It is also
called the query satisfiability problem. For XML, for example, it takes the following form: given
an XPath query p and a DTD D, does there exist an XML document T such that T conforms to
D and the answer of p on T is non-empty. The complexity of this problem ranges from tractable
to undecidable depending on the XPath fragment, see e.g. the work of Benedikt et al. (2008) and
references therein. In a DL context, query emptiness has been first considered in the work of Lubyte
and Tessaris (2008), who use it as a step to guide the enrichment of ontologies.
The query emptiness problem studied in this paper is a special case of the following query
containment problem, first considered in the work of Bienvenu, Lutz, and Wolter (2012). We can
regard a pair (T , q) which consists of a TBox T and a query q as a compound query Q, called an
ontology-mediated query (OMQ), such that the answers to Q are the certain answers to q w.r.t. T
(Bienvenu et al., 2014). Now take two OMQs Qi = (Ti , qi ), i  {1, 2}, such that q1 and q2 are IQs
or CQs of the same arity. Then Q1 is -contained in Q2 , for an ABox signature , if for all ABoxes A that are satisfiable w.r.t. T1 and T2 , we have certT1 ,A (q1 )  certT2 ,A (q2 ). In this case, we
write Q1  Q2 . This notion of containment generalizes the more traditional query containment
problem in DLs (Calvanese et al., 2007) by relativizing it to an ABox signature and admitting
distinct TBoxes T1 and T2 . Query emptiness of an IQ q for  given T can clearly be polynomially
reduced to -containment by setting T1 = T , q1 = q, T2 = , and q2 = A(x) for a fresh concept
name A, and similarly for CQs. Deciding -containment, however, is often computationally harder
than deciding query emptiness. Table 5 summarizes known results; the results about EL and DLLite are from the work of Bienvenu et al. (2012), the results about ELI from the work of Bienvenu,
Hansen, Lutz, and Wolter (2016), and the results about ALC and ALCI from the work of Bienvenu
et al. (2014) and Bourhis and Lutz (2016).
Query emptiness is also closely related to explaining negative answers to queries. This problem
was studied, for example, by Calvanese, Ortiz, Simkus, and Stefanoni (2013). Adopting an abductive reasoning approach, it can be described as follows. Assume T , A 6|= q(~a) for a TBox T , ABox
A, and query q. To explain that ~a is not an answer to q, one wants to find minimal ABoxes E over
38

fiQ UERY AND P REDICATE E MPTINESS

a certain signature  of interest such that A  E is satisfiable w.r.t. T and T , A  E |= q(~a). Such
-ABoxes E are then regarded as an explanation for the missing answer and can be used for debugging purposes. It is shown in the work of Calvanese et al. (2013) that query emptiness of IQs and
Boolean CQs reduces (under many-one logarithmic space reductions) to the problem of deciding the
existence of an explanation for T , A 6|= q(~a) with A = . For DL-LiteA , the reduction even works
for unions of conjunctive queries of any arity. Calvanese et al. (2013) use this observation to obtain
lower complexity bounds for explaining negative query answers, exploiting the results published
in the conference predecessor of this paper (Baader, Bienvenu, Lutz, & Wolter, 2010). They also
conjecture that, conversely, techniques for proving upper complexity bounds for query emptiness
(such as the ones in this paper) can be used to obtain upper bounds for explaining negative answers.

9. Conclusion
We have investigated the computational complexity of query and predicate emptiness in the EL,
DL-Lite, and ALC families of DLs, concentrating on instance queries and conjunctive queries and
showing that complexities range from NL OG S PACE to undecidable. We have also highlighted that,
for different DLs and query languages, different kinds of witness ABoxes are sufficient to establish
non-emptiness. DLs and queries that are not considered in this paper, but which would be interesting
to investigate in future work, include the following:
 DLs that include transitive roles, role inclusions, symmetric roles, or role inclusion axioms
(Horrocks, Kutz, & Sattler, 2006; Kazakov, 2010).
In some cases, straightforward reductions to results presented in this paper are possible. For
example, IQ-query emptiness in Horn-SHIF is decidable in E XP T IME since for every
Horn-SHIF TBox T , IQ A(x), and ABox signature , one can construct in polynomial
time a Horn-ALCIF TBox T 0 such that T , A |= A(a) iff T 0 , A, |= A(a) for all -ABoxes
A (Hustadt et al., 2007; Kazakov, 2009). In other cases, such as CQ-query emptiness in
Horn-SHIF, there seems to be no such reduction.
 DLs that include nominals.
 Other important classes of queries such as unions of conjunctive queries (UCQs).
For materializable DLs such as Horn-ALCIF,
W UCQ query emptiness can be reduced to CQ
query emptiness since for every UCQ q = iI qi (~x), we have T , A |= q(~a) iff there is an
i  I such that T , A |= qi (~a). Such a simple reduction does not work for non-Horn DLs such
as ALC.
It would also be interesting to develop practical algorithms for emptiness and to evaluate these algorithms on real-world ontologies and queries. Note that our algorithms for EL and DL-Lite are
easily implementable and efficient as presented in this paper. This was actually confirmed by the
case study in Section 7. More work will be required to design efficient algorithms for more expressive DLs. Finally, it would be relevant to investigate the notion of a -substitute introduced
in our application to modularity in more detail. For example, it is an open question how to compute minimal -substitutes in expressive DLs such as ALC in practice, and what are the involved
complexities.
39

fiBAADER , B IENVENU , L UTZ , & W OLTER

Acknowledgements
The first author was partially supported by cfaed (Center for Advancing Electronics Dresden) and
the second author was partially supported by the ANR project PAGODA (ANR-12-JS02-007-01).
We are grateful to Julian Mendez and Dirk Walther for supporting us in the case study. We would
like to thank the anonymous reviewers who provided excellent comments that helped us to improve
the paper.

Appendix A. Proofs for Section 3
We formulate the result to be proved again.
Lemma 3 Let T be an ALCIF-TBox. Then each CQ q is empty for  given T with the UNA iff
it is empty for  given T without the UNA.
Proof. Consider a CQ q with answer variables v1 , . . . , vn .
(Only if) Assume that q is non-empty for  given T without the UNA. Then there is a ABox A such that A is satisfiable w.r.t. T without the UNA and certT ,A (q) 6=  without the UNA.
Take a model I of A and T and suppose without loss of generality that I is infinite. Define
an equivalence relation  over Ind(A) by setting a  b whenever aI = bI . Choose a single
representative from each equivalence class, and denote by a the representative of the equivalence
class containing a. Let A0 be the ABox obtained from A by replacing each individual a by a . We
show that A0 is satisfiable w.r.t. T with the UNA and that certT ,A0 (q) 6=  with the UNA.
Regarding satisfiability, it is easy to see that I is a model of (T and) A0 and that it satisfies the
UNA for the individuals appearing in A0 . Moreover, since I is infinite, we can reinterpret the
individual names in NI \ Ind(A0 ) to obtain an interpretation that is a model of A0 and T and satisfies
the UNA.
Now for showing certT ,A0 (q) 6=  with the UNA. Take some (a1 , . . . , an )  certT ,A (q) without
the UNA. We aim to show that (a1 , . . . , an )  certT ,A0 (q). Let J 0 be any model of T and A0 that
0
satisfies the UNA. We have to show that there is a match for q in J 0 with (vi ) = (ai )J for every
0
1  i  n. Consider the interpretation J obtained from J 0 by setting aJ = (a )J for every
a  Ind(A). It is easy to see that J is a model of A and T without the UNA, so J |= q[a1 , . . . , an ]
and there is a match  for q in J such that (vi ) = aJ
i for every 1  i  n. Then  is also the
desired match for q in J 0 , which finishes the proof.
(If) Assume that q is non-empty for  given T with the UNA. Then there is a -ABox A
such that A is satisfiable w.r.t. T with the UNA and certT ,A (q) 6=  with the UNA. Clearly, A is
also satisfiable w.r.t. T without the UNA and it remains to show that certT ,A (q) 6=  without the
UNA. Let (a1 , . . . , an )  certT ,A (q) with the UNA, and let I be any model of A and T without
the UNA. We have to show that I |= q[a1 , . . . , an ]. For any a  Ind(A), let Ia be the following
unfolding of I at aI :
 the domain Ia of Ia consists of all words d0 r0 d1    rk1 dk with d0 , . . . , dk  I and
r0 , . . . , rk1 (potentially inverse) roles such that d0 = aI , (di , di+1 )  riI for all i < k,
ri 6= ri+1 for functional ri+1 and i < k, and r0 (a, b) 6 A for any b  Ind(A) if r0 is
functional.
 AIa = {d0 r0 d1    dk | dk  AI } for all A  NC ;
40

fiQ UERY AND P REDICATE E MPTINESS

 rIa = {(d0 r0 d1    dk , d0 r0 d1    dk rk+1 dk+1 ) | r = rk+1 } 
{(d0 r0 d1    dk rk+1 dk+1 , d0 r0 d1    dk ) | r = rk+1 } for all r  NR .
Assume that the Ia are mutually disjoint and let the individual name a be the root of Ia . Then we
obtain the interpretation J by taking the disjoint union of all Ia , a  Ind(A), adding (a, b) to rJ
whenever r(a, b)  A, and setting aJ = a for all a  Ind(A). One can show that J is a model of A
and T with the UNA. Thus J |= q[a1 , . . . , an ] and there is a match  for q in J with (vi ) = (ai )J
for every 1  i  n. It can be verified that  0 defined by setting  0 (vi ) = aI if (vi ) = a  Ind(A)
and  0 (vi ) = dk if (v) = d0 , . . . , dk with k  1 is a match of q in I, thus I |= q[a1 , . . . , an ], as
required.
o

Appendix B. Proofs for Section 4
We restate the first result to be proved.
Theorem 16 In ALC, CQ-query emptiness is in NE XP T IME.
The general idea for proving Theorem 16 is as follows. Given an ALC-TBox T , a signature ,
and a CQ q, by Theorem 13 it suffices to test whether T , AT , 6|= q. We thus start by computing AT , . To check whether T , AT , 6|= q, we then guess an extension of T with a set T 0 of
concept inclusions and an extension of AT , with a set A0 of ABox assertions such that T 0 and A0
that are satisfied only in models of T and AT , in which q has no match; subsequently, it remains to
test the satisfiability of AT ,  A0 w.r.t. T  T 0 . The subtlety lies in selecting the class of extensions
to be guessed from in a careful enough way so that the final satisfiability check can be carried out
in NE XP T IME.
We reuse some technical definitions and results from the work of Lutz (2008) who proves that
the combined complexity of CQ-answering in the DL SHQ is in E XP T IME. The definitions are
slightly modified since Lutz considers only CQs without answer variables and uses the DL SHQ,
of which ALC is a proper fragment. However, it is straightforward to verify that the proofs given
by Lutz also work with our modified definitions.
A CQ q can be viewed as a directed graph Gdq = (Vqd , Eqd ) with Vqd = var(q) and Eqd =
{(v, v 0 ) | r(v, v 0 )  q for some r  NR }. We call q directed tree-shaped if Gdq is a directed tree and
r(v, v 0 ), s(v, v 0 )  q implies r = s. If q is directed tree-shaped and v0 is the root of Gdq , we call v0
the root of q. For U  var(q), we write q|U to denote the restriction of q to atoms that contain only
variables from U . The set DTrees(q) of directed tree-shaped subqueries of q is defined as follows:
DTrees(q) = {q|U | U = Reachq (v), v  var(q), q|U is directed tree-shaped}
where Reachq (v) is the set of variables that are reachable from v in Gdq . We say that
 q 0 is obtained from q by performing fork elimination if q 0 is obtained from q by selecting two
atoms r(v 0 , v) and s(v 00 , v) such that v 0 , v 00 , v  qvar(q) and v 0 6= v 00 , and identifying v 0
and v 00 ;
 q 0 is a fork rewriting of q if q 0 is obtained from q by repeatedly (but not necessarily exhaustively) performing fork elimination;
41

fiBAADER , B IENVENU , L UTZ , & W OLTER

 q 0 is a maximal fork rewriting of q if q 0 is a fork rewriting and no further fork elimination is
possible in q 0 .
The following is shown in the work of Lutz (2008), and it plays a central role in the subsequent
definitions.
Lemma 44 Up to variable renaming, every CQ has a unique maximal fork rewriting.
The following definitions of splittings and spoilers are also taken from the work of Lutz. To understand a splitting of a CQ q on an intuitive level, it is useful to consider matches of q in a model I of a
TBox T and ABox A that has a special shape: I consists of a core part whose elements are exactly
(the interpretations of) the ABox individuals in A and of tree-shaped parts that are attached to the
element in the core part and disjoint from each other. In fact, it is proved by Lutz that if T , A 6|= q,
then there is a model I of T and A of the described shape such that I 6|= q. A match of q in a model
of the described shape partitions the variables in q into several sets: a set R which contains the variables that are matched to an ABox individual; sets S1 , . . . , Sn which represent disjoint tree-shaped
subqueries of q that are matched into a tree part of I and whose root is connected to some variable
in R via a role atom in q; and a set T which represents a collection of tree-shaped subqueries of
q that are disconnected from the variables in R and Si . In addition to this partitioning, splittings
record the variable in R to which each set S1 , . . . , Sn is connected and the ABox elements that the
variables in R are mapped to. We now define splittings formally.
Let K = (T , A) be an ALC-knowledge base and q a CQ. A splitting of q w.r.t. K is a tuple  =
hR, T, S1 , . . . , Sn , , i, where R, T, S1 , . . . , Sn is a partitioning of var(q),  : {1, . . . , n}  R
assigns to each set Si a variable (i) in R, and  : R  Ind(A) assigns to each variable in R an
individual in A. A splitting has to satisfy the following conditions:
1. the CQ q|T is a variable-disjoint union of directed tree-shaped queries;
2. the queries q|Si , 1  i  n, are directed tree-shaped;
3. if r(v, v 0 )  q, then one of the following holds: (i) v, v 0 belong to the same set R, T, S1 , . . . , Sn
or (ii) v  R, (i) = v, and v 0  Si is the root of q|Si ;
4. for 1  i  n, there is a unique r  NR such that r((i), v0 )  q, with v0 the root of q|Si ;
5. avar(q)  R.
Let q be a directed tree-shaped CQ. We define an ALC-concept Cq,v for each v  var(q):
 if v is a leaf in Gdq , then Cq,v =
 otherwise, Cq,v =

u

A(v)q

Au

u C;
u r.C

A(v)q

r(v,v 0 )q

q,v 0 .

If v is the root of q, we use Cq to abbreviate Cq,v .
In the following, we allow compound concepts and negated roles to be used in ABox assertions. The semantics of such assertions and corresponding KBs is defined in the expected way: an
interpretation I satisfies C(a) if aI  C I and I satisfies r(a, b) if I does not satisfy r(a, b).
Let  = hR, T, S1 , . . . , Sn , , i be a splitting of q w.r.t. K such that q1 , . . . , qk are the (directed
tree-shaped) disconnected components of q|T . An ALC-knowledge base (T 0 , A0 ) is a spoiler for q,
K, and  if one of the following conditions hold:
42

fiQ UERY AND P REDICATE E MPTINESS

1. > v Cqi  T 0 , for some i with 1  i  k;
2. there is an atom A(v)  q with v  R and A((v))  A0 ;
3. there is an atom r(v, v 0 )  q with v, v 0  R and r((v), (v 0 ))  A0 ;
4. D(((i)))  A0 for some i with 1  i  n, and where D = r.Cq|S with v0 root of q|Si
i
and r((i), v0 )  q.
We call K0 a spoiler for q and K if (i) for every fork rewriting q 0 of q, and every splitting  of q 0
w.r.t. K, K0 is a spoiler for q 0 , K, and ; and (ii) K0 is minimal with Property (i). The following
result is proved in the work of Lutz (2008).
Theorem 45 Let K = (T , A) be an ALC-knowledge base and q a CQ. Then K 6|= q iff there is a
spoiler (T 0 , A0 ) for q and K such that A  A0 is satisfiable w.r.t. T  T 0 .
The following lemma, which was observed by Lutz, plays a central role for obtaining a NE XP T IME
decision procedure.
Lemma 46 Let K = (T , A) be an ALC-knowledge base, q a CQ, q  its maximal fork rewriting,
and K0 = (T 0 , A0 ) a spoiler for q and K. Then K0 contains only concept inclusions and ABox
assertions of the following form:
1. > v Cq0 with q 0  DTrees(q  );
2. A(a) with a  Ind(A) and A occurring in q;
3. r(a, b) with a, b  Ind(A) and r occurring in q;
4. D(a) with a  Ind(A) and D = r.Cq0 , where r occurs in q and q 0  DTrees(q  ).
Note that while the definition of a spoiler for q and K refers to all fork rewritings of q, of which
there are exponentially many, Lemma 46 only refers to the unique maximal form rewriting q  . In
fact, since the cardinality of DTrees(q  ) is clearly bounded by the size of q, the number of concept
inclusions and assertions listed in Lemma 46 is only polynomial in the size of A and q.
We are now set up for the proof of Theorem 16. By Theorems 13 and 45, a CQ q is empty for a
signature  given a TBox T iff there is a spoiler (T 0 , A0 ) for q and (T , AT , ) such that AT ,  A0
is satisfiable w.r.t. T  T 0 . Given a CQ q, signature  and TBox T , we can thus decide emptiness
of q for  given T as follows:
1. compute AT , ;
2. guess a TBox T 0 and an ABox A0 that satisfy Conditions 1 to 4 from Lemma 46 for the KB
K = (T , AT , ) and such that there is no role assertion r(a, b)  AT , with r(a, b)  A0 ;
3. verify that (T 0 , A0 ) is a spoiler for q and (T , AT , );
4. verify that AT ,  A0 is satisfiable w.r.t. T  T 0 .
43

fiBAADER , B IENVENU , L UTZ , & W OLTER

It remains to argue that this yields a NE XP T IME algorithm. As already noted, Step 1 can be carried
out in (deterministic) exponential time. Due to Conditions 1 to 4 from Lemma 46 and since AT ,
is of size at most exponential in T and , the TBox T 0 and ABox A0 guessed in Step 2 are of
size at most exponential in T and , and of size polynomial in q. Step 3 can be implemented by
a straightforward iteration over all fork rewritings q 0 of q and splittings  of q 0 w.r.t. (T , AT , ),
which requires only exponential time.
It thus remains to deal with Step 4. Let  be the closure under single negations of the union of
the following sets of concepts:
   NC ;
 all concepts that occur in T (possibly as subconcepts);
 all concept names that occur in q;
 all concepts Cq and r.Cq and their subconcepts, where q  DTrees(q  ), q  the maximal fork
rewriting of q, and r occurs in q.
Based on the remark after Lemma 46, it is easy to verify (and crucial for our argument) that ||
is polynomial in , T , and q. A -type is a set t   such that for some model I of T  T 0
and some d  I , we have t = {C   | d  C I }. As in Section 4, we introduce a notion of
coherence between types: we say that the pair of -types (t, t0 ) is r-coherent, denoted t ;r t0 , if
for all r.C  , C  t0 implies r.C  t. The set T of all -types can be computed in E XP T IME.
To verify satisfiability of AT ,  A0 w.r.t. T  T 0 , we guess a map  : Ind(AT , )  T , accept
if the following two conditions are satisfied and reject otherwise:
(i) C(c)  AT ,  A0 implies C  (c) and
(ii) r(b, c)  AT ,  A0 , C  (c), and r.C   imply r.C  (b).
Clearly, checking whether these two conditions are satisfied can be done in single exponential time.
It thus remains to argue that AT ,  A0 is satisfiable w.r.t. T  T 0 just in the case that there exists a
map  verifying these conditions. First note that given a model I of the KB (T  T 0 , AT ,  A0 ),
we can define the desired map by setting (c) = {C   | cI  C I }. Conversely, given a map 
satisfying conditions (i) and (ii), we define an interpretation I as follows:
I = T

AI = {t  T | A  t}

rI = {(t, t0 )  T  T | t ;r t0 }

cI = (c)

It is readily verified that for all C   and t  T , we have C  t iff t  C I . From this, we can
show, using a similar argument to that given in Lemma 10, that I is a model of (T T 0 , AT , A0 ).
We now complete the proof of our undecidability result (Theorem 19) by proving Lemma 20.
Lemma 20 (T, H, V ) admits a tiling iff there is a -ABox A that is satisfiable w.r.t. T and such
that T , A |= v A(v).
Proof. (Only if) Straightforward. Consider a tiling f : {0, . . . , n}  {0, . . . , m}  T for
(T, H, V ). Create individuals ai,j for 0  i  n and 0  j  m, and consider the ABox A
composed of the following assertions:
44

fiQ UERY AND P REDICATE E MPTINESS

 x(ai,j , ai+1,j ) for 0  i < n and 0  j  m
 x (ai+1,j , ai,j ) for 0  i < n and 0  j  m
 y(ai,j , ai,j+1 ) for 0  j < m and 0  i  n
 y  (ai,j+1 , ai,j ) for 0  j < m and 0  i  n
 Th (ai,j ) if f (i, j) = Th .
It can easily be verified that A is satisfiable w.r.t. T and satisfies T , A |= v A(v).
(If) Let A be a -ABox satisfiable w.r.t. T and such that T , A |= v A(v). We first show that
Ix and Iy enforce that x is the inverse of x and y  is the inverse of y, respectively, and that C
forces relevant grid cells to be closed. For r  {x, y} we call a  Ind(A) an r-defect if there exists
b  Ind(A) such that r(a, b)  A and r (b, a) 6 A. We call a an inv-defect if it is an x-defect or
a y-defect. We call a  Ind(A) a cl-defect if there exist x(a, b), y(a, c), y(b, d), x(c, e)  A with
d 6= e such that a is not an inv-defect, b is not a y-defect and c is not an x-defect.
Claim 1. There exists a model I of T and A such that for all a  Ind(A):
(d1) aI 6 IrI , for all r-defects a  Ind(A) and r  {x, y};
(d2) aI 6 C I , for all cl-defects a  Ind(A).
Moreover, I satisfies the following conditions for all a  Ind(A), role names r, and h  {1, . . . , p}:
1. I = Ind(A);
2. aI = a;
3. (a, a0 )  rI implies r(a, a0 )  A;
4. a  ThI implies Th (a)  A.
Proof of Claim 1. Let r  {x, y}. Call a two-element set {a, b} a r-defect witness if there exists
c  Ind(A) such that r(a, c), r (c, b)  A. Consider the undirected graph G with nodes Ind(A)
and the set of r-defect witnesses as its edges. Note that G has degree at most two (since r and r are
functional). Hence G is three-colorable. Choose a three coloring of G with colors Br,1 = Zr,1 uZr,2 ,
Br,2 = Zr,1 u Zr,2 and Br,3 = Zr,1 u Zr,2
of the concept names
Sand choose the interpretation
I

I
Zr,1 , Zr,2 in I correspondingly. We set Ir = i=1,2,3 (Br,i u r.r .Br,i ) .
Call a two-element set {d, e} a cl-defect witness if there exist x(a, b), y(a, c), y(b, d), x(c, e) 
A such that a is not an inv-defect, b is not a y-defect and c is not an x-defect. Consider the undirected
graph G with nodes Ind(A) and the set of cl-defect witnesses as its edges. Note that G has degree at
most two (again since x, x , y, and y  are all functional). Hence G is three-colorable with colors
C1 = Zc,1 u Zc,2 , C2 = Zc,1 u Zc,2 and C3 = Zc,1 u Zc,2 and we
S can choose the interpretation of
the concept names Zc,1 , Zc,2 in I correspondingly. We set C I = i=1,2,3 (x.y.Ci u y.x.Ci )I .
Since neither existential restrictions nor the concept names Th occur in the right-hand side of
CIs in T , it is not hard to verify that we can interpret the remaining concept names in T in such a
way that the additional conditions on I are satisfied. (End of proof of claim)
45

fiBAADER , B IENVENU , L UTZ , & W OLTER

Let I be a model satisfying the conditions of Claim 1. We additionally assume w.l.o.g. that I
is A, Y -minimal: there is no model J of T and A satisfying the conditions of Claim 1 such that
AJ  AI and Y J  Y I and at least one of these inclusions is proper.
Let aA  AI . We now exhibit a grid structure in A that gives rise to a tiling for (T, H, V ). We
start by identifying a diagonal that starts at aA and ends at an instance of Tfinal .
Claim 2. There is a set G = {r1 (ai0 ,j0 , ai1 ,j1 ), . . . , rk1 (aik1 ,jk1 aik ,jk ), Tfinal (aik ,jk )}  A such
that
 i0 = 0, j0 = 0, and a0,0 = aA ;
 for 1  ` < k, we either have (i) r` = x, i`+1 = i` + 1, and j`+1 = j` or (ii) r` = y,
j`+1 = j` + 1, and i`+1 = i` .
Proof of claim. If there is no such sequence, we can convert I into a new model J of T and
A by interpreting Y as false at all points reachable in I (equivalently: A) from aA and setting
AJ = AI \ {aA }, which contradicts the A, Y -minimality of I. (End of proof of claim)
Let n be the number of occurrences of the role x in the ABox G from Claim 1 and m the number of
occurrences of y. We next show
Claim 3. We have that
I .
(a) a0,0  Tinit

(b) ai,j  RI implies i = n;
(c) ai,j  U I implies j = m;
(d) ai,j  Y I for all ai,j  Ind(G);
(e) for all ai,j  Ind(G), there is a (unique) Th with ai,j  ThI , henceforth denoted Ti,j ;
(f) (Ti,j , Ti+1,j )  H for all ai,j , ai+1,j  Ind(G) and (Ti,j , Ti,j+1 )  V for all ai,j , ai,j+1 
Ind(G).
Proof of claim. Point (a) is an easy consequence of the fact that a0,0 = aA , aA  AI , and I is
A, Y -minimal. For (b), first note that there is a unique `  k such that is = n for all s  {`, . . . , k}
and is < n for all s  {0, . . . , `  1}. Due to the CI R v x., ai`1 ,j`1 
/ RI . To show that
I
ais ,js 
/ R for all s < `  1, it suffices to use the CIs R v x. and R v y.R. The proof of
(c) is similar. We prove (d)-(f) together, showing by induction on ` that (d)-(f) are satisfied for all
initial parts
G` := {r1 (ai0 ,j0 , ai1 ,j1 ), . . . , r`1 (ai`1 ,j`1 ai` ,j` )}
of G, with `  k. For the base case, ai0 ,j0 = aA  AI clearly implies ai0 ,j0  Y I , thus (d)
is satisfied. Point (e) follows from (a) and the disjointness of tiles expressed in T . Point (f) is
vacuously true since there is only a single individual in G0 . For the induction step, assume that G`1
satisfies (d)-(f). We distinguish four cases:
 ai`1 ,j`1  (U u R)I .
46

fiQ UERY AND P REDICATE E MPTINESS

Since G`1 satisfies (d), we have ai`1 ,j`1  Y I , and the definition of T and the A, Y minimality of I together with the fact ai`1 ,j`1  (U u R)I ensure that
ai`1 ,j`1  (x.(Tg u Y u y.Y ) u y.(Th u Y u x.Y ) u Ix u Iy u C u Tf )I
for some (Tf , Tg )  H and (Tf , Th )  V . Using the functionality of x and y, it is now easy
to show that G` satisfies (d)-(f).
 ai`1 ,j`1  (U u R)I .
Since ai`1 ,j`1  RI , T ensures that there is no x-successor of ai`1 ,j`1 in I. Moreover,
ai`1 ,j`1  Y I . Together with the definition of T , we get
ai`1 ,j`1  (y.(Tg u Y u R) u Iy u Tf )I
for some (Tf , Tg )  V . We must have i` = i`1 , j` = j`1 + 1, and r`1 = y. Using the
functionality of y, it is now easy to show that G` satisfies (d)-(f).
 ai`1 ,j`1  (U u R)I .
Analogous to the previous case.
 ai`1 ,j`1  (U u R)I .
Then there is neither an x-successor nor a y-successor of ai`1 ,j`1  (U u R)I . It follows
that `  1 = k, in contradiction to `  k.
(End of proof of claim)
Next, we extend G to a full grid such that Conditions (a)-(e) from Claim 3 are still satisfied. Once
this is achieved, it is trivial to read off a solution for the tiling problem. The construction of the grid
consists of exhaustive application of the following two steps:
1. if x(ai,j , ai+1,j ), y(ai+1,j , ai+1,j+1 )  G and there is no ai,j+1  Ind(G) with y(ai,j , ai,j+1 ) 
G and x(ai,j+1 , ai+1,j+1 )  G, then identify an ai,j+1  Ind(A) such that y(ai,j , ai,j+1 )  A
and x(ai,j+1 , ai+1,j+1 )  A and add the latter two assertions to G.
2. if y(ai,j , ai,j+1 ), x(ai,j+1 , ai+1,j+1 )  G and there is no ai+1,j  Ind(G) with x(ai,j , ai+1,j ) 
G and y(ai+1,j , ai+1,j+1 )  G, then identify an ai+1,j  Ind(A) such that x(ai,j , ai+1,j )  A
and y(ai+1,j , ai+1,j+1 )  A and add the latter two assertions to G.
It is not hard to see that exhaustive application of these rules yields a full grid, i.e., for the final G
we have (i) Ind(G) = {ai,j | i  n, j  m}, (ii) x(ai,j , ai0 ,j 0 )  G iff i0 = i + 1 and j = j 0 , and
(iii) y(ai,j , ai0 ,j 0 )  G iff i = i0 and j 0 = j + 1.
Since the two steps of the construction are completely analogous, we only deal with Case 1
in detail. Thus let x(ai,j , ai+1,j ), y(ai+1,j , ai+1,j+1 )  G with ai,j+1 
/ Ind(G). Clearly, i < n
and j < m. By (b) and (c), we thus have ai,j 
/ (R t U )I . Since ai,j  Y I by (d) and I is
A, Y -minimal, we get that
ai,j  (x.(Tg u Y u y.Y ) u y.(Th u Y u x.Y ) u Ix u Iy u C u Tf )I
for some (Tf , Tg )  H and (Tf , Th )  V . This together with the minimality of I means we can
select ai,j+1 , b  Ind(A) such that y(ai,j , ai,j+1 ), x(ai,j+1 , b)  A, ai,j+1 , b  Y I , and Ti,j+1 =
47

fiBAADER , B IENVENU , L UTZ , & W OLTER

Th . With this choice, (a), (d), (e), and the second half of (f) are clearly satisfied. To get the properties
required by Step 1 above, we have to show that b = ai+1,j+1 . If we can show this, then the
satisfaction of (b) and (c) before we apply the construction step, and the CIs
R v x. R v y.R

U v y. U v x.U

ensure that (b) and (c) are still satisfied after the construction step. Showing b = ai+1,j+1 will also
give us the first half of (f). Finally, to prove that b = ai+1,j+1 it is sufficient to show that ai,j is not
a cl-defect in Ind(A). But this follows from Claim 1 since ai,j  C I , ai,j  IxI  IyI , ai+1,j  IyI ,
and ai,j+1  IxI .
We can now use the completed grid to build a solution to our tiling problem: the tile at point
(i, j) is the unique tile which is satisfied by I at ai,j  Ind(A). Property (f) of Claim 2 and the
correctness of our grid construction ensure that adjacent tiles satisfy the vertical and horizontal
constraints.
o

Appendix C. Proofs for Section 5
Theorem 22. In EL, CQ-query emptiness can be decided in PT IME.
Proof. By Lemma 21, it suffices to show that for any n-ary CQ q and alphabet , it can be decided
in PT IME whether T , A |= q[a , . . . , a ] where A is the total -ABox. First note that we have
T , A |= q[a , . . . , a ] iff T , Ab |= qb, where
 Ab is obtained from A by adding the assertion X(a ), where X is a concept name that
does not occur in , T , and q;
 qb is the Boolean CQ obtained from q by adding the conjunct X(v) for each answer variable v
and then quantifying away all answer variables.
Recall from the discussion before Lemma 44 that every CQ q can be viewed as a directed graph
Gdq . We say that a Boolean CQ q is directed forest-shaped if it is a disjoint union of directed treeshaped Boolean CQs. Every Boolean CQ q that is directed forest-shaped corresponds to a concept
Cq in the description logic ELu that extends EL with the universal role u such that T , A |= q iff
T , A |= C(a) for all a  Ind(A) (Lutz & Wolter, 2010). Checking the latter condition is possible in
PT IME (Lutz & Wolter, 2010). Thus, it is sufficient to convert qb in polynomial time into a directed
forest-shaped CQ qb0 such that T , Ab |= qb iff T , Ab |= qb0 .
To construct qb0 from qb, we exhaustively apply the following rewriting rules:
1. if r(v, v 00 ) and r(v 0 , v 00 ) are in the query, then identify v and v 0 by replacing all occurrences
of v 0 with v;
2. if r(v 0 , v) and s(v 00 , v) are in the query (with r 6= s), then identify v, v 0 , and v 00 by replacing
all occurrences of v 0 and v 00 with v;
3. if a cycle r0 (v0 , v1 ), . . . , rn1 (vn1 , vn ), vn = v0 is in the query and {v0 , . . . , vn1 } contains
at least two variables, then identify all variables v0 , . . . , vn1 by replacing all occurrences of
v1 , . . . , vn1 with v0 .
48

fiQ UERY AND P REDICATE E MPTINESS

If the resulting query contains a reflexive loop r(v, v) with r 
/ , then we immediately return
no. Otherwise, we replace in a final step each reflexive loop r(v, v) with r   with X(v). The
query resulting from this last step is qb0 . It is easy to see that the query obtained at this point is
directed forest-shaped since every variable has at most one predecessor and there are no cycles in
the corresponding directed graph.
To prove correctness of this algorithm, we first establish the following claim:
Claim. If qb0 is defined, then T , Ab |= qb iff T , Ab |= qb0 .
It suffices to prove that each rule application preserves (non)entailment of the query by T and Ab .
As a preliminary, we recall that, as shown by Lutz and Wolter (2010), there exists a materialization
JT ,Ab of (T , Ab ) which is a directed tree-shaped interpretation with the individual a as its root
and (potentially) additional reflexive loops added to this root (an interpretation is directed treeshaped if the corresponding CQ in which the domain elements of the interpretation are regarded as
variables is directed tree-shaped). Assume that rewriting rule 1 is applied to a query p resulting in
a query p0 . It is clear that T , Ab |= pb0 implies T , Ab |= pb. For the converse, assume T , Ab |= pb
and let JT ,Ab be the materialization of T and Ab introduced above. Then there is a match of p
in JT ,Ab . Since JT ,Ab does not contain domain elements d, d0 , d00 with d 6= d0 and such that for
J

J

some role name r, (d, d00 )  r T ,Ab and (d0 , d00 )  r T ,Ab , this match of p in JT ,Ab must map the
identified variables v and v 0 to the same domain element and is thus also a match of p0 . The other
two rules and the replacement of r(v, v), r  , with X(v) can be dealt with in a similar way.
By the claim, we can substitute qb with qb0 as intended. Moreover, it is easy to see that we have
T , Ab 6|= qb if the algorithm returns no due to a reflexive loop r(v, v) with r 
/ : simply use the
interpretation JT ,Ab as in the proof of the claim.
o
Proposition 28. For every Horn-ALCIF TBox T , ABox signature , and CQ q, one can construct
in polynomial time an ELIF  -TBox T 0 in normal form such that q is empty for  given T iff q is
empty for  given T 0 .
Proof. The proof is similar to reductions provided in in the work of Hustadt et al. (2007) and
Kazakov (2009). Nevertheless, because Kazakov considers reductions preserving subsumption only,
and because Hustadt, Motik, and Sattler and also Kazakov do not reduce to ELIF  TBoxes, we
give a detailed proof.
The following rules can be used to rewrite T into an ELIF  -TBox in normal form (all freshly
introduced concept names are not in sig(T )    sig(q). Assume L v R is given.
 If L is of the form L1 u L2 and R is not a concept name, then take a fresh concept name A
and replace L v R by L v A and A v R. If R is a concept name, and either L1 or L2 are
not concept names, then take fresh concept names A1 , A2 and replace L v R by L1 v A1 ,
L2 v A2 and A1 u A2 v R;
 If L is of the form L1 t L2 and R is a concept name, then replace L v R by L1 v R and
L2 v R. Otherwise take a fresh concept name A and replace L v R by L v A and A v R;
 If L is of the form r.L0 and L0 is not a concept name, then take a fresh concept name A0 and
replace L v R by L0 v A0 and r.A0 v R;
 If R is of the form A, then replace L v R by L u A v ;
49

fiBAADER , B IENVENU , L UTZ , & W OLTER

 If R is of the form R1 u R2 and L is not a concept name, then take a fresh concept name A
and replace L v R by L v A and A v R. Otherwise take fresh concept names A1 , A2 and
replace L v R by L v A1 , L v A2 , A1 v R1 , and A2 v R2 ;
 If R is of the form L0 t R0 , then replace L v R by L u L0 v R0 ;
 If R is of the form r.R0 and R0 is not a concept name, then take a fresh concept name A0 and
replace L v R by L v r.A0 and A0 v R0 ;
 If R is of the form r.R0 , then replace L v R by r .L v R.
The resulting TBox T 0 is as required. In particular, for every -ABox A and model I of A and T 0 ,
we have that I is also a model of T ; conversely, every model I of A and T can be extended
to a model of T by appropriately interpreting the fresh concept names. Consequently, we have
certT (q, A) = certT 0 (q, A) and thus q is empty for  given T iff q is empty for  given T 0 .
o
Proposition 30. Let T be an ELIF  -TBox,  an ABox signature, and q a CQ. If q is non-empty
for  given T , then this is witnessed by a -ABox that is forest-shaped, has width at most |q|, and
degree at most |T |.
Proof. Assume that q has answer variables v1 , . . . , vn and is non-empty for  given T . Then we
can find a -ABox A that is satisfiable w.r.t. T and such that certT ,A (q) 6= . To identify a forestshaped witness for the non-emptiness of q for  given T , consider the canonical model IT ,A of
(T , A). By construction, IT ,A consists of an ABox part I0 , which is the restriction of IT ,A to
Ind(A), and tree-shaped interpretations Ia , a  Ind(A), rooted at a and containing no other ABox
individuals. Since IT ,A is universal, there is a match  of q in IT ,A . Let I consist of all individuals
a  Ind(A) such that there is some v  var(q) with (v) in Ia (possibly (v) = a). Let A0 be
the ABox obtained by restricting A to the individuals in I ; this is going to be the root component
of the forest-shaped witness we are seeking to define (observe that |Ind(A0 )|  |q|). To add the
tree components, we consider, for each a  I , the (typically infinite) tree-shaped ABox Aua that is
obtained by unraveling A starting from a, as in the work of Lutz and Wolter (2012):
 Ind(Aua ) is the set of sequences  = c0 r0 c1 . . . rm1 cm with c0 , . . . , cm  Ind(A) and
r0 , . . . , rm1 (possibly inverse) roles such that (i) c0 = a, (ii) c1 6 I , (iii) rj (cj1 , cj )  A

for all 0  j < m, and (iv) (cj1 , rj1
) 6= (cj+1 , rj ) for j > 0; we say that  is a copy
of cm ;
 if A(c)  A and   Ind(Aua ) is a copy of c, then A()  Aua ;
 if   Ind(Aua ) is a copy of c, and  = rc0  Ind(Aua ), then r(, )  Aua ;
 if   Ind(Aua ) is a copy of c, and  = r c0  Ind(Aua ), then r(, )  Aua .
We let Ab be the union of A0 and the tree-shaped ABoxes {Aua | a  I }. Observe that by Conditions
b Note
(ii) and (iv) of the first item and since A satisfies all functionality statements in T , so does A.
b
that A is forest-shaped, but need neither be finite nor of degree at most |T |; we are going to fix this
later.
We next aim to show that Ab is satisfiable w.r.t. T and that certT ,Ab(q) 6= . To this end, we
construct a universal model J of Ab and T . Start with Ab viewed as an interpretation J0 , as in
50

fiQ UERY AND P REDICATE E MPTINESS

the construction of canonical models. Then take, for each a  Ind(A) and each of as copies  
b a copy I of the tree interpretation Ia such that (i) the root of I is , (ii) J0 I = {},
Ind(A),
(iii)  6=  implies disjointness of I and I , and (iv) if  = a then I is identical to the original
tree interpretation Ia (and not a copy). If d0  I is the result of renaming d  Ia , then d0 is
called a copy of d. The desired interpretation J is obtained by taking the union of J0 and all I .
Note that every element of J is the copy of an element in IT ,A , and that, by construction, J is a
b
model of A.
It is straightforward to show by induction on the structure of C that for every ELI-concept C
and every element e  J that is a copy of d  IT ,A , e  C J iff d  C IT ,A . Since IT ,A is a
model of T , it follows that J is a model of T and thus Ab is satisfiable w.r.t. T . We only sketch the
proof that J is universal. Let I be a model of Ab and T . We start to define a homomorphism h0 from
b It remains to extend h0 to the I components of
J to I by setting h0 (a) = aI for all a  Ind(A).
J . Each such I is a copy of a tree interpretation Ia in IT ,A such that  is a copy of a. It is shown
in the work of Lutz and Wolter (2012) that4
b is a copy of a  Ind(A), then a  AIT ,A implies T , Ab |= A() for all concept
() if   Ind(A)
names A.
Recall that IT ,A was generated by the derivation rules for building canonical models. Using a
straightforward induction on the number of rule applications and exploiting () and the fact that
T is in normal form, one can construct a homomorphism ha from Ia to I such that h(a) = I .
By renaming, we obtain a homomorphism h from I to I such that h () = I . The desired
homomorphism h is the union of h0 and all h . We have thus established that J is universal. Going
through the construction of J (and in particular using Point (iv)), it can be verified that the match 
of q in IT ,A is also a match of q in J . Since J is universal, this yields certT ,Ab(q) 6=  as desired.
We now want to remove individuals from Ab such that the resulting ABox is of degree at most |T |
and still witnesses non-emptiness of q for  given T . Since J is universal, there is a homomorphism h from J to the canonical model IT ,Ab. Composing the match  with h, we obtain a match 
of q in IT ,Ab that sends every variable to an individual in A0 or to an element of a tree below such an
individual. We inductively mark individuals in Ab that are relevant for the match  , starting with all
individuals in A0 and then proceeding as follows: whenever Rule 2 or 4 adds a marked individual
x to AIT ,Ab during the construction of IT ,Ab because of the presence of (x, y)  rIT ,A (please see
the formulation of the mentioned rules), then mark y. It can be verified that every individual outside
of A0 has at most one marked neighbor for each existential restriction in T . The (potentially infinite) forest-shaped ABox Abd obtained from Ab by dropping all assertions that involve at least one
unmarked individual is thus of degree at most |T |. Moreover, the marking construction ensures that
the canonical model IT ,Abd contains A0 and each interpretation Ia , a  Ind(A0 ), hence  is a match
for q in IT ,Abd .
At this point, the ABox Abd is almost the required forest witness, except that it may be infinite.
It remains to invoke compactness to obtain a finite subset Abf  Abd such that certT ,Abf (q) 6= .
Clearly, Abf contains a forest witness for the non-emptiness of q for  given T .
o
4. Lutz and Wolter (2012) actually show this for the case where the root component A0 of Ab from which we start to
unravel consists of all the individual names in A, but contains no concept and role assertions; the proof also goes
through in our case.

51

fiBAADER , B IENVENU , L UTZ , & W OLTER

The following lemmas establish the two statements in Lemma 34.
Lemma 47 Every canonical proper R  N -labeled tree is well-founded.
Proof. Let hT, `i be a canonical proper R  N -labeled tree, and let I0 , I1 , . . . be the interpretations encountered during the construction of the canonical model of T and AhT,`i . Since hT, `i is
canonical, IhT,`i is the canonical model of AhT,`i and T .
We will slightly abuse terminology by using the term concept atom to refer to statements of the
form B(e) where B is a concept name (or >) and e is a domain element. A role atom will take the
form r(e, e0 ) with r a role and e, e0 domain elements. We will say that a concept atom B(e) (resp.
role atom r(e, e0 )) is in an interpretation J if e  B J (resp. (e, e0 )  rJ ). For each atom  in
IhT,`i , the rank of  is the smallest i such that  is in Ii . We show by induction on the rank that
every concept atom in IhT,`i has a derivation, thus hT, `i is well-founded.
The induction start is straightforward as concept atoms in I0 involve a concept from   {>}
and an element x such that either x  Ind(A) with A = `() or x  T \ {} with M  `(x), and
every such atom has a derivation of depth 0. For the induction step, let B(x) be a concept atom in
Ii+1 \ Ii . We consider the rule application that resulted in the addition of B(x):
1. Assume that B(x) is in Ii+1 because of an application of Rule 1, that is, A1 u    u An v
B  T and x  AIj i for 1  j  n.
For every 1  j  n, the atom Aj (x) has rank at most i, so by the IH, there is a derivation
hTj0 , `0j i of Aj at x. We obtain a derivation hT 0 , `0 i of B at x by setting T 0 = {}  {jw | w 
Tj0 }, `0 () = (B, x), and `0 (jw) = `0j (w).
2. Assume that B(x) is in Ii+1 because of an application of Rule 2, that is, there is r.A v B 
T such that x  (r.A)Ii .
As x  (r.A)Ii , there must exist some y  Ii such that (x, y)  rIi and y  AIi .
The atom A(y) has rank at most i, so by the IH, there is a derivation hT 00 , `00 i of A at y. If
x  I0 , then x  Ind(AhT,`i ), and so we can define a derivation hT 0 , `0 i of B at x by setting
T 0 = {}  {1w | w  T 00 }, `0 () = (B, x), and `0 (1w) = `00 (w).
Next consider the case in which x 6 I0 . Then x 6 Ind(AhT,`i ), so by properness of T , there
is a concept E  NC  {>} such that E   `(x). Since x 6 I0 but x 6 Ii , there is some
0 < j < i such that x  Ij \ Ij1 . Since hT, `i is canonical, the element x was created
due to an application of Rule 3 using a concept inclusion of the form F v s.E, so x  E Ij .
Applying the IH, we obtain a derivation hT 000 , `000 i of E at x. We can thus define a derivation
hT 0 , `0 i of B at x by setting T 0 = {}  {1w | w  T 00 }  {2w | w  T 000 }, `0 () = (B, x),
`0 (1w) = `00 (w), and `0 (2w) = `000 (w).
3. Assume that B(x) is in Ii+1 because of an application of Rule 3 involving A v r.B  T ,
that is, there is some y  Ii such that y  AIi and (y, x)  rIi+1 \ rIi .
The atom A(y) has rank at most i, so by the IH, there exists a derivation hT 00 , `00 i of A at y.
Moreover, since x was created by applying the inclusion A v r.B  T to y, the second
condition of canonicity ensures that B   `(x). We can thus define a derivation of B at x by
taking the tree hT 0 , `0 i with T 0 = {}  {1w | w  T 00 }, `0 () = (B, x), and `0 (1w) = `00 (w).
52

fiQ UERY AND P REDICATE E MPTINESS

4. Assume that B(x) = >(x) is in Ii+1 because of an application of Rule 3 involving A v
r.E  T (E 6= >), that is, there is some y  Ii such that y  AIi and (y, x)  rIi+1 \ rIi .
The atom A(y) has rank at most i, so by the IH, there exists a derivation hT 00 , `00 i of A at y.
Moreover, since x was created by applying the inclusion A v r.E  T to y, the second
condition of canonicity ensures that E   `(x). We can thus define a derivation of > at x by
taking the tree hT 0 , `0 i with T 0 = {, 1}  {11w | w  T 00 }, `0 () = (>, x), `0 (1) = (E, x),
and `0 (11w) = `00 (w).
5. Assume that B(x) is in Ii+1 because of an application of Rule 4, that is, A v r.B  T ,
funct(r)  T , y  AIi , and (y, x)  rIi .
The atom A(y) has rank at most i, so by the IH, there exists a derivation hT 00 , `00 i of A at y. If
x  I0 , we obtain a derivation of B at x by taking the tree hT 0 , `0 i with T 0 = {}  {1w |
w  T 00 }, `0 () = (B, x), and `0 (1w) = `00 (w). If If x 6 I0 , then we can use the same
argument as in Point 2 to find a derivation hT 000 , `000 i of E at x. We then obtain a derivation
hT 0 , `0 i of B at x by setting T 0 = {}  {1w | w  T 00 }  {2w | w  T 000 }, `0 () = (B, x),
`0 (1w) = `00 (w), and `0 (2w) = `000 (w).
o
Lemma 48 Let hT, `i be a proper R  N -labeled tree that is well-founded and such that IhT,`i
is a model of T . Then IhT,`i is a universal model of T and AhT,`i .
Proof. Assume that hT, `i is well-founded proper R  N -labeled tree and that IhT,`i is a model
of T . An obligation is a pair (A, x) such that x  T and A  `(x). For every obligation
(A, x), choose a derivation hTA,x , `A,x i of A at x in hT, `i that is of minimal depth. For obligations (A1 , x1 ),(A2 , x2 ), we write (A1 , x1 )  (A2 , x2 ) if (A1 , x1 ) occurs as a node label in
hTA2 ,x2 , `A2 ,x2 i.
Claim. The  relation is acyclic.
Proof of claim. Assume to the contrary that there are obligations (A0 , x0 ), . . . , (An , xn ) such that
(Ai , xi )  (Ai+1 , xi+1 ) for all i  n and (An+1 , xn+1 ) := (A0 , x0 ). We may assume without loss of generality that for all 0  i < j  n, (Ai , xi ) 6= (Aj , xj ), i.e., the obligations
(A0 , x0 ), . . . , (An , xn ) are pairwise distinct. Let ki be the depth of hTAi ,xi , `Ai ,xi i and `i the
depth of the most shallow derivation of (Ai , xi ) contained in hTAi+1 ,xi+1 , `Ai+1 ,xi+1 i. Because
hTAi ,xi , `Ai ,xi i is of minimal depth, we have ki  `i . Moreover, we clearly also have `i  ki+1 .
We thus have shown that k0 = `0 =    = kn = `n . Consequently, the derivation of (A0 , x0 )
in hTA1 ,x1 , `A1 ,x1 i must start at the root of hTAi ,xi , `Ai ,xi i, which implies (A0 , x0 ) = (A1 , x1 ) in
contradiction to the fact that these obligations are distinct. This finishes the proof of the claim.
By the claim, we can assume w.l.o.g. that if in some chosen derivation hTA,x , `A,x i, a node is
labeled with (B, y), then the subtree of hTA,x , `A,x i rooted at this node is the chosen derivation
hTB,y , `B,y i (uniformity assumption).
To prove that IhT,`i is a universal model of T and AhT,`i , take a model I of AhT,`i and T . We
show that there is a homomorphism h from IhT,`i to I, constructing h in a step-by-step fashion. To
start, set h(a) = aI for all individual names a that occur in AhT,`i . Now and after each extension
of h, we argue that
53

fiBAADER , B IENVENU , L UTZ , & W OLTER

1. if x  AIhT,`i with A a concept name, h(x) is defined and hTA,x , `A,x i uses only elements
from the domain of h, then h(x)  AI ;
2. if (x, y)  rIhT,`i with r a role and h(x), h(y) are defined, then (h(x), h(y))  rI ;
3. if (x, y)  rIhT,`i , y is a child of x in T , and h(y) is defined, then h(x) is also defined.
We start by observing that for the initial mapping h, Point 2 is trivial since I is a model of AhT,`i
and all role edges in the restriction of IhT,`i to the domain of h are from AhT,`i . For Point 3, we use
the fact that if y is an individual in AhT,`i and x is the parent of y in T , then properness of T implies
that x is also an individual in AhT,`i (and hence x belongs to the domain of h).
Point 1 is proved by induction on the depth of hTA,x , `A,x i. For the induction start, consider
depth zero. Then A    {>}, x  Ind(AhT,`i ), and A(x)  AhT,`i . Since I is a model of AhT,`i
and by definition of h, we have h(x)  AI .
Now for the induction step. Assume that hTA,x , `A,x i uses only elements from the domain of h.
The definition of derivations gives rise to the following cases:
 A 6 `(x), and there is a CI A1 u    u An v A  T such that for 1  i  n, there is a child
z 0 of z in TA,x with `A,x (z 0 ) = (Ai , x).
For 1  i  n, let zi be the child of z with `A,x (zi ) = (Ai , x). Then the subderivation
of hTA,x , `A,x i rooted at zi is the chosen derivation hTAi ,x , `Ai ,x i of Ai at x. It follows that
hTAi ,x , `Ai ,x i only uses elements from the domain of h and its depth is strictly smaller than
that of hTA,x , `A,x i. We can therefore apply the induction hypothesis to get h(x)  AIi . Since
I is a model of T and A1 u    u An v A  T , we obtain h(x)  AI .
 A 6 `(x), there is a CI r.A0 v A  T and a child z 0 of z in TA,x with `A,x (z 0 ) = (A0 , x0 )
such that (x, x0 )  rIhT,`i .
The subderivation of hTA,x , `A,x i rooted at z 0 is the chosen derivation hTA0 ,x0 , `A0 ,x0 i of A0 at
x0 , and thus it contains only elements from the domain of h and has a strictly smaller depth
than hTA,x , `A,x i. We can thus use the IH to infer that h(x0 )  B I , and we can use Point 2 to
get (h(x), h(x0 ))  rI . Since I is a model of T and r.A0 v A  T , we have h(x)  AI .
 A 6 `(x), there is a CI A0 v r.A  T with funct(r)  T and a child z 0 of z in TA,x with
`A,x (z 0 ) = (A0 , x0 ) such that (x0 , x)  rIhT,`i .
As in the previous item, we can use the IH and Point 2 to get h(x0 )  B I and (h(x), h(x0 )) 
rI . Since I is a model of T and T contains both A0 v r.A and funct(r), it follows that
h(x)  AI .
 A = >, > 
/ `(x), B   `(x),and there is a child z of  in TA,x with `A,x (z) = (B, x).
This case is not applicable since if B   `(x), then M 6 `(x), hence x is not in the domain
of h.
 A  `(x), there is a CI A0 v r.A  T , and there is a child z 0 of z in TA,x with `A,x (z 0 ) =
(A0 , x0 ) such that (x0 , x)  rIhT,`i and either (i) x is a child of x0 in T , or (ii) x is a child of
the root, x0  Ind, and {r, x0 }  `(x).
This case is not applicable since if A  `(x), then M 6 `(x), hence x is not in the domain
of h.
54

fiQ UERY AND P REDICATE E MPTINESS

To extend h, we first show that if h is not yet total, then there exists an edge (b
x, yb)  rIhT,`i

and a concept name A such that h(b
x) is defined, h(b
y ) is undefined, A  `(b
y ) (and consequently
A  `(b
y )), and hTA,by , `A,by i is such that all elements in it except the root node are in the domain
of h.
Assume to the contrary that h is not total but there is no such edge, i.e., for every edge (b
x, yb) 
rIhT,`i such that h(b
x) is defined, h(b
y ) is undefined, and A  `(b
y ), the derivation hTA,by , `A,by i
contains a non-root node that is not in the domain of h. Pick one such edge (b
x, yb)  rIhT,`i such
that its associated derivation hTA,by , `A,by i is of minimal depth. Since h(b
x) is defined and h(b
y ) is
undefined, it follows from Point 3 that either x
b is the parent of yb in T , or yb is a child of the root node
and {b
x, r}  `(b
y ). Since derivation rule 6 is the only applicable rule when the node label contains

A and by the formulation of that rule, there must thus be a CI A0 v r.A  T such that the unique
child z of  in TA,by satisfies `A,by (z) = (A0 , x
b). Since x
b is in the domain of h, the non-root node that
is not in the domain of h must be somewhere below z. Consequently, we find nodes z1 , z2  TA,by
such that z2 is a successor of z1 and the domain element x
b0 in `A,by (z1 ) is in the domain of h, but
0
the domain element yb in `A,by (z2 ) is not in the domain of h. By definition of the derivation rules,
we must have (b
x0 , yb0 )  sIhT,`i for some role s, and by Point 3, either yb0 is a child of x
b0 in T , or yb0
0
0
is a child of the root node and its label contains {b
x , s}. It follows that x
b is related to yb0 by one of
the derivation rules 4 and 5. Consequently, there is a B   `(b
y 0 ) such that hTA,by , `A,by i contains the
IhT,`i
0
0
0
obligation (B, yb ). Thus, the edge (b
x , yb )  s
satisfies the above conditions and its associated
derivation hTB,by0 , `B,by0 i is of strictly smaller depth than hTA,by , `A,by i, contradicting the minimality
of hTA,by , `A,by i.
We now extend h using the edge (b
x, yb)  rIhT,`i whose existence we have just established.
By the definition of derivations, there is a CI A0 v r.A  T and a child z of  in TA,by with
`A,by (z) = (A0 , x
b). Since all elements in hTA,by , `A,by i except the root node are in the domain of h,
the subderivation of hTA,by , `A,by i rooted at z uses only elements from the domain of h. By our
uniformity assumption, this derivation is just hTA0 ,bx , `A0 ,bx i, and thus IH yields h(b
x)  A0 IhT,`i .
IhT,`i
IhT,`i
0
Since A v r.A  T , there is a (b
x, d)  r
with d  A
. Set h(b
y ) = d.
It remains to show that Points 1 and 2 are satisfied for the extended h (Point 3 obviously is).
We start with Point 2. Assume that (x, y)  sIhT,`i . If h(x) and h(y) were defined already before
the extension of h, we are done. Otherwise, by construction of h we must have (x, y, s) = (b
x, yb, r)
(or (x, y, s) = (b
y, x
b, r ), which is equivalent). By the choice of h(b
y ), we have (h(b
x), h(b
y ))  rI ,
hence (x, y)  sI . Point 1 is proved by induction on the depth of A(x) as for the initial version
of h. The induction start is exactly the same, and for the induction step, the only cases that differ
are the following ones:
 A 
/ `(x), B   `(x), A = >, and there is a child z of  in TA,x with `A,x (z) = (B, x).
Immediate since A = >.
 A  `(x), there is a CI A0 v r.A  T , and there is a child z 0 of z in TA,x with `A,x (z 0 ) =
(A0 , x0 ) such that (x0 , x)  rIhT,`i and either (i) x is a child of x0 in T , or (ii) x is a child of
the root, x0  Ind, and {r, x0 }  `(x).
Since A  `(x), we have x 6 Ind(AhT,`i ), so x must have been introduced into the domain
of h during the examination of edge (x0 , x)  rIhT,`i . Since the child z 0 is labeled (A0 , x0 ),
we will use the CI A0 v r.A  T and choose h(x) such that h(x)  AIhT,`i .
o
55

fiBAADER , B IENVENU , L UTZ , & W OLTER

References
Artale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). The DL-Lite family and
relations. Journal of Artifical Intelligence Research (JAIR), 36, 169.
Baader, F., Bienvenu, M., Lutz, C., & Wolter, F. (2010). Query and predicate emptiness in description logics. In Proceedings of the 12th International Conference on Principles of Knowledge
Representation and Reasoning (KR).
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing the EL envelope. In Proceedings of the 19th
International Joint Conference on Artificial Intelligence (IJCAI), pp. 364369.
Baader, F., Brandt, S., & Lutz, C. (2008). Pushing the EL envelope further. In Proceedings of the
Workshop on OWL: Experiences and Directions (OWLED).
Benedikt, M., Fan, W., & Geerts, F. (2008). XPath satisfiability in the presence of DTDs. Journal
of the ACM, 55(2), 179.
Bienvenu, M., Hansen, P., Lutz, C., & Wolter, F. (2016). First-order rewritability of conjunctive
queries in Horn description logics. In Proceedings of the 25th International Joint Conference
on Artificial Intelligence (IJCAI).
Bienvenu, M., Lutz, C., & Wolter, F. (2012). Query containment in description logics reconsidered.
In Proceedings of the 13th International Conference on Principles of Knowledge Representation and Reasoning (KR).
Bienvenu, M., ten Cate, B., Lutz, C., & Wolter, F. (2014). Ontology-based data access: A study
through disjunctive datalog, CSP, and MMSNP. ACM Transactions on Database System
(TODS), 39(4), 33.
Botoeva, E., Kontchakov, R., Ryzhikov, V., Wolter, F., & Zakharyaschev, M. (2014). Query inseparability for description logic knowledge bases. In Proceedings of the 14th International
Conference in the Principles of Knowledge Representation and Reasoning (KR).
Botoeva, E., Kontchakov, R., Ryzhikov, V., Wolter, F., & Zakharyaschev, M. (2016). Games for
query inseparability of description logic knowledge bases. Artificial Intelligence Journal
(AIJ), 234, 78119.
Bourhis, P., & Lutz, C. (2016). Containment in monadic disjunctive datalog, mmsnp, and expressive
description logics. In Proceedings of the 15th International Conference on Principles of
Knowledge Representation and Reasoning (KR).
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rodriguez-Muro, M., &
Rosati, R. (2009). Ontologies and databases: The DL-Lite approach. In Tutorial Lectures of
the 5th International Reasoning Web Summer School, Vol. 5689 of Lecture Notes in Computer
Science, pp. 255356. Springer.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable reasoning
and efficient query answering in description logics: The DL-Lite family. Journal of Automated
Reasoning (JAR), 39(3), 385429.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2013). Data complexity
of query answering in description logics. Artificial Intelligence Journal (AIJ), 195, 335360.
56

fiQ UERY AND P REDICATE E MPTINESS

Calvanese, D., De Giacomo, G., & Lenzerini, M. (1998). On the decidability of query containment
under constraints. In Proceedings of the 17th ACM SIGACT-SIGMOD-SIGART Symposium
on Principles of Database Systems (PODS), pp. 149158.
Calvanese, D., Ortiz, M., Simkus, M., & Stefanoni, G. (2013). Reasoning about explanations for
negative query answers in DL-Lite. Journal of Artificial Intelligence Research (JAIR), 48,
635669.
Chortaras, A., Trivela, D., & Stamou, G. B. (2011). Optimized query rewriting for OWL 2 QL.
In Proceedings of the 23rd International Conference on Automated Deduction (CADE), pp.
192206.
Eiter, T., Gottlob, G., Ortiz, M., & Simkus, M. (2008). Query answering in the description logic
Horn-SHIQ. In Proceedings of the 11th European Conference on Logics in Artificial Intelligence (JELIA), pp. 166179.
Eiter, T., Ortiz, M., Simkus, M., Tran, T., & Xiao, G. (2012). Query rewriting for Horn-SHIQ plus
rules. In Proceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI).
Gabbay, D., Kurucz, A., Wolter, F., & Zakharyaschev, M. (2003). Many-Dimensional Modal Logics:
Theory and Applications. Elsevier.
Gatens, W., Konev, B., & Wolter, F. (2014). Lower and upper approximations for depleting modules
of description logic ontologies. In Proceedings of the 21st European Conference on Artificial
Intelligence (ECAI), pp. 345350.
Gene Ontology Consortium (2016). The gene ontology. http://geneontology.org/. [Online; accessed
16-April-2016].
Glimm, B., Lutz, C., Horrocks, I., & Sattler, U. (2008). Answering conjunctive queries in the
SHIQ description logic. Journal of Artificial Intelligence Research (JAIR), 31, 150197.
Golbeck, J., Fragoso, G., Hartel, F., Hendler, J., Oberthaler, J., & Parsia, B. (2003). The national
cancer institutes thesaurus and ontology. Journal of Web Semantics: Science, Services and
Agents on the World Wide Web, 1(1), 7580.
Grau, B. C., Horrocks, I., Kazakov, Y., & Sattler, U. (2008). Modular reuse of ontologies: Theory
and practice. Journal of Artifical Intelligence Research (JAIR), 31, 273318.
Haase, C. (2007). Complexity of subsumption in extensions of EL. Masters thesis, Dresden University of Technology.
HITRL (2016). Health Information Technologies Research Laboratory. University of Sydney.
http://sydney.edu.au/engineering/it/hitru. [Online; accessed 16-April-2016].
Horrocks, I., Kutz, O., & Sattler, U. (2006). The even more irresistible SROIQ. In Proceedings of
the 10th International Conference on Principles of Knowledge Representation and Reasoning
(KR), pp. 5767.
Hustadt, U., Motik, B., & Sattler, U. (2004). A decomposition rule for decision procedures by
resolution-based calculi. In Proceedings of the 11th International Conference on Logic for
Programming Artificial Intelligence and Reasoning (LPAR), pp. 2135.
Hustadt, U., Motik, B., & Sattler, U. (2007). Reasoning in description logics by a reduction to
disjunctive datalog. Journal of Automated Reasoning (JAR), 39(3), 351384.
57

fiBAADER , B IENVENU , L UTZ , & W OLTER

IHTSDO (2016). SNOMED CT: The global language of healthcare. http://www.ihtsdo.org/snomedct. [Online; accessed 16-April-2016].
Kaminski, M., Schneider, T., & Smolka, G. (2011). Correctness and worst-case optimality of Prattstyle decision procedures for modal and hybrid logics. In Proceedings of the 20th International Conference on Automated Reasoning with Analytic Tableaux and Related Methods
(TABLEAUX), pp. 196210.
Kazakov, Y. (2009). Consequence-driven reasoning for Horn-SHIQ ontologies. In Proceedings of
the 21st International Joint Conference on Artificial Intelligence (IJCAI), pp. 20402045.
Kazakov, Y. (2010). An extension of complex role inclusion axioms in the description logic
SROIQ. In Proceedings of the 5th International Joint Conference on Automated Reasoning
(IJCAR), pp. 472486.
Konev, B., Ludwig, M., Walther, D., & Wolter, F. (2012). The logical difference for the lightweight
description logic EL. Journal of Artificial Intelligence Research (JAIR), 44, 633708.
Konev, B., Lutz, C., Walther, D., & Wolter, F. (2013). Model-theoretic inseparability and modularity
of description logic ontologies. Artificial Intelligence Journal (AIJ), 203, 66103.
Kontchakov, R., Rodriguez-Muro, M., & Zakharyaschev, M. (2013). Ontology-based data access
with databases: A short course. In Proceedings of the International Reasoning Web Summer
School, pp. 194229.
Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2010). Logic-based ontology comparison and
module extraction, with an application to DL-Lite. Artificial Intelligence, 174(15), 1093
1141.
Krotzsch, M. (2012). OWL 2 profiles: An introduction to lightweight ontology languages. In Tutorial Lectures of the 8th International Reasoning Web Summer School, Vol. 7487 of Lecture
Notes in Computer Science, pp. 112183. Springer.
Krotzsch, M., Rudolph, S., & Hitzler, P. (2007). Complexity boundaries for Horn description logics.
In Proceedings of the 22nd AAAI Conference on Artificial Intelligence (AAAI), pp. 452457.
Levy, A. (1993). Irrelevance Reasoning in Knowledge Based Systems. Ph.D. thesis, Stanford University.
Lubyte, L., & Tessaris, S. (2008). Supporting the design of ontologies for data access. In Proceedings of the 21st International Description Logic Workshop (DL).
Lutz, C. (2008). The complexity of CQ answering in expressive description logics. In Proceedings
of the 4th International Joint Conference on Automated Reasoning (IJCAR), pp. 179193.
Lutz, C., Toman, D., & Wolter, F. (2009). Conjunctive query answering in the description logic EL
using a relational database system. In Proceedings of the 21st International Joint Conference
on Artificial Intelligence (IJCAI), pp. 20702075.
Lutz, C., & Wolter, F. (2010). Deciding inseparability and conservative extensions in the description
logic EL. Journal of Symbolic Computation, 45(2), 194228.
Lutz, C., & Wolter, F. (2012). Non-uniform data complexity of query answering in description
logics. In Proceedings of the 13th International Conference on Principles of Knowledge
Representation and Reasoning (KR).
58

fiQ UERY AND P REDICATE E MPTINESS

Motik, B., Grau, B. C., Horrocks, I., Wu, Z., Fokoue, A., & Lutz, C. (2009). OWL 2 Web Ontology Language: Profiles. W3C Recommendation. Available at http://www.w3.org/TR/owl2profiles/.
Ortiz, M., Calvanese, D., & Eiter, T. (2008). Data complexity of query answering in expressive
description logics via tableaux. Journal of Automated Reasoning (JAR), 41(1), 6198.
Ortiz, M., & Simkus, M. (2012). Reasoning and query answering in description logics. In Proceedings of the 8th International Reasoning Web Summer School, Vol. 7487 of Lecture Notes in
Computer Science, pp. 153. Springer.
Ortiz, M., Simkus, M., & Eiter, T. (2008). Worst-case optimal conjunctive query answering for an
expressive description logic without inverses. In Proceedings of the 23rd AAAI Conference
on Artificial Intelligence (AAAI), pp. 504510.
Patel, C., Cimino, J. J., Dolby, J., Fokoue, A., Kalyanpur, A., Kershenbaum, A., Ma, L., Schonberg,
E., & Srinivas, K. (2007). Matching patient records to clinical trials using ontologies. In
Proceedings of the 6th International Semantic Web Conference (ISWC), pp. 816829.
Perez-Urbina, H., Motik, B., & Horrocks, I. (2009). A comparison of query rewriting techniques
for dl-lite. In Proceedings of the 22nd International Description Logic Workshop (DL).
Poggi, A., Lembo, D., Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2008). Linking
data to ontologies. Journal of Data Semantics, 10, 133173.
Pratt, V. R. (1979). Models of program logics. In Proceedings of IEEE Annual Symposium on
Foundations of Computer Science (FOCS), pp. 115122.
Romero, A. A., Kaminski, M., Grau, B. C., & Horrocks, I. (2015). Ontology module extraction
via datalog reasoning. In Proceedings of the 29th AAAI Conference on Artificial Intelligence
(AAAI), pp. 14101416.
Tobies, S. (2001). Complexity Results and Practical Algorithms for Logics in Knowledge Representation. Ph.D. thesis, RWTH Aachen.
Vardi, M. Y. (1989). Automata theory for database theoreticans. In Proceedings of the 7th ACM
SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems (PODS), pp. 83
92.
Vardi, M. Y. (1998). Reasoning about the past with two-way automata. In Proceedings of the 25th
International Colloquium on Automata, Languages and Programming (ICALP), pp. 628641.

59

fiJournal of Artificial Intelligence Research 56 (2016) 693-745

Submitted 03/16; published 08/16

Qualitative Spatial Logics for Buffered Geometries
Heshan Du

H.Du@leeds.ac.uk

University of Leeds, UK

Natasha Alechina

Natasha.Alechina@nottingham.ac.uk

University of Nottingham, UK

Abstract
This paper describes a series of new qualitative spatial logics for checking consistency
of sameAs and partOf matches between spatial objects from different geospatial datasets,
especially from crowd-sourced datasets. Since geometries in crowd-sourced data are usually not very accurate or precise, we buffer geometries by a margin of error or a level of
tolerance   R0 , and define spatial relations for buffered geometries. The spatial logics
formalize the notions of buffered equal (intuitively corresponding to possibly sameAs),
buffered part of (possibly partOf), near (possibly connected) and far (definitely disconnected). A sound and complete axiomatisation of each logic is provided with respect to
models based on metric spaces. For each of the logics, the satisfiability problem is shown
to be NP-complete. Finally, we briefly describe how the logics are used in a system for generating and debugging matches between spatial objects, and report positive experimental
evaluation results for the system.

1. Introduction
The motivation for our work on qualitative spatial logics comes from the needs of integrating
disparate geospatial datasets, especially crowd-sourced geospatial datasets. Crowd-sourced
data involves non-specialists in data collection, sharing and maintenance. Compared to
authoritative geospatial data, which is collected by surveyors or other geodata professionals, crowd-sourced data is less accurate and less well structured, but often provides richer
user-based information and reflects real world changes more quickly at a much lower cost
(Jackson, Rahemtulla, & Morley, 2010). It is in the interests of national mapping agencies,
government organisations, and all other users of geospatial data to be able to integrate and
use different geospatial data synergistically.
Geospatial data matching refers to the problem of establishing correspondences (matches)
between spatial objects represented in different geospatial datasets. It is an essential step
for data comparison, data integration or enrichment, change detection and data update.
Matching authoritative geospatial data and crowd-sourced geospatial data is a non-trivial
task. Geometry representations of the same location or place in different datasets are usually not exactly the same. Objects are also sometimes represented at different levels of
granularity. For example, consider geometries of objects in Nottingham city centre given by
Ordnance Survey of Great Britain (OSGB) (2012) and by OpenStreetMap (OSM) (2012)
in Figure 1. The position and shape of the Prezzo Ristorante are represented differently in
OSGB data (dotted) and OSM data (solid). The Victoria Shopping Centre is represented
as a whole in OSM, and as several shops in OSGB.
c
2016
AI Access Foundation. All rights reserved.

fiDu & Alechina

Figure 1: Prezzo Ristorante and Victoria Shopping Centre represented in OSGB (dotted)
and OSM (solid)

In order to integrate the datasets, we need to determine which objects are the same
(represent the same entity) and sometimes which objects in one dataset are parts of objects
in the other dataset (as in the example of Victoria Shopping Centre). The statements
representing these two types of relations are referred to as sameAs matches and partOf
matches respectively. One way to produce such matches is to use locations and geometries
of objects, although of course we also use any lexical labels associated with the objects, such
as names of restaurants etc. In our previous work (Du, Alechina, Jackson, & Hart, 2016),
we present a method which generates matches using both location and lexical information
about spatial objects. As generated matches may contain errors, they are seen as retractable
assumptions and require further validation and checking. One way is to use logical reasoning
to check the consistency of matches with respect to statements in input datasets. By
using description logic reasoning, the correctness of matches can be checked with respect
to classification information. For example, it is wrong to state that spatial objects a and
b are the same, if a is a Bank and b is a Clinic, because the concepts Bank and Clinic
are disjoint, containing no common elements. However, this is not sufficient for validating
matches between spatial objects1 . For example, two spatial objects which are close to each
other in one dataset cannot be matched to two spatial objects which are far away apart in
the other dataset, no matter whether they are of the same type or not. Therefore, spatial
reasoning is required to validate matches with regard to location information, in addition
to description logic reasoning.
Spatial logic studies relations between geometrical structures and spatial languages describing them (Aiello, Pratt-Hartmann, & van Benthem, 2007). There are a variety of
spatial relations, such as topological connectedness of regions, relations based on distances,
relations for expressing orientations or directions, etc. In a spatial logic, spatial relations
are represented in a formal language, such as first order logic or its fragments, and inter1. There are works (Lutz & Milicic, 2007) on extending description logics with concrete domains or constraint systems, such as the region connection calculus (RCC) (Randell, Cui, & Cohn, 1992) and Allens
Interval Algebra (Allen, 1983). The description logic reasoner Pellet (Sirin, Parsia, Grau, Kalyanpur,
& Katz, 2007) was extended to PelletSpatial (Stocker & Sirin, 2009), which supports qualitative spatial
reasoning in RCC. However, later we will show that it is not appropriate to use RCC in our application.

694

fiQualitative Spatial Logics for Buffered Geometries

preted over some structures based on geometrical spaces, such as topological spaces, metric
spaces and Euclidean spaces. In the field of qualitative spatial reasoning, several spatial
formalisms have been developed for representing and reasoning about topological relations,
such as the Region Connection Calculus (RCC) (Randell et al., 1992), the 9-intersection
model (Egenhofer & Franzosa, 1991) and their extensions (Clementini & Felice, 1997; Roy
& Stell, 2001; Schockaert, Cock, Cornelis, & Kerre, 2008b, 2008a; Schockaert, Cock, &
Kerre, 2009). In addition, there are formalisms for representing and reasoning about directional relations (Frank, 1991, 1996; Ligozat, 1998; Balbiani, Condotta, & del Cerro,
1999; Goyal & Egenhofer, 2001; Skiadopoulos & Koubarakis, 2004), as well as relative or
absolute distances (Zimmermann, 1995; Clementini, Felice, & Hernandez, 1997; Wolter &
Zakharyaschev, 2003, 2005). Recent comprehensive surveys on qualitative spatial representations and reasoning are provided by Cohn and Renz (2008) and Chen, Cohn, Liu, Wang,
OuYang, and Yu (2015).
Qualitative spatial reasoning has been shown to be applicable to geospatial data (Bennett, 1996; Bennett, Cohn, & Isli, 1997; Guesgen & Albrecht, 2000; Mallenby, 2007; Mallenby & Bennett, 2007; Li, Liu, & Wang, 2013), where location information of spatial
objects comes from a single data source. The application described in this paper is different, as location representations about the same spatial object come from different sources
and are usually not exactly the same. Rather than treating all the differences in geometric
representations as logical contradictions, we would tolerate slight geometric differences and
only treat qualitatively defined large differences as logical contradictions used for detecting
wrong matches. More specifically, after establishing matches between two sets of spatial
objects, if the set of matches gives rise to a contradiction, then some match must be wrong
and should be retracted. In addition, we would provide explanations to help users understand why a contradiction exists and why some matches are wrong. In the following, we
assess the appropriateness of several existing spatial formalisms for these purposes.
The Region Connection Calculus (RCC) (Randell et al., 1992) is a first order formalism
based on regions and the connection relation C, which is axiomatised to be reflexive and
symmetric. Two regions x, y are connected (i.e. C(x, y) holds), if their closures share a
point. Based on the connection relation, several spatial relations are defined for regions.
Among them, eight jointly exhaustive and pairwise disjoint (JEPD) relations are identified:
DC (Disconnected), EC (Externally Connected), P O (Partially Overlap), T P P (Tangential
Proper Part), N T P P (Non-Tangential Proper Part), T P P i (Inverse Tangential Proper
Part), N T P P i (Inverse Non-Tangential Proper Part) and EQ (Equal). They are referred
to as RCC8, which is well-known in the field of qualitative spatial reasoning.
The 9-intersection model is developed by Egenhofer and Franzosa (1991) and Egenhofer
and Herring (1991) based on the point-set interpretation of geometries. By comparing the
nine intersections between interiors, boundaries and exteriors of point-sets, it identifies 29
mutually exclusive topological relations. The 9-intersection model provides a comprehensive
formal categorization of binary topological relations between points, lines and regions. Only
a small number of these 29 relations are realisable in a particular space (Egenhofer &
Herring, 1991). Restricting point-sets to simple regions (regions homeomorphic to disks),
the 512 relations collapse to the RCC8 relations.
For the described application, we found it difficult to use spatial formalisms such as the
Region Connection Calculus (Randell et al., 1992) and the 9-intersection model (Egenhofer
695

fiDu & Alechina

& Franzosa, 1991), since they presuppose accurate geometries or regions with sharp boundaries and define spatial relations based on the connection relation. This is too strict for
crowd-sourced geospatial data. As shown in Figure 2, a1 is sameAs a2 , both representing a
Prezzo Ristorante; b1 is sameAs b2 , both referring to a Blue Bell Inn. Though the sameAs
matches are correct, a topological inconsistency still exists, since a1 and b1 are disconnected
(DC), a2 and b2 are externally connected (EC), and the spatial relations DC and EC are
disjoint. Therefore, relations based on connection are too strict for crowd-sourced geospatial
data which is possibly inaccurate and may contain errors.

Figure 2: In OSGB data, the Prezzo Ristorante (a1 ) and the Blue Bell Inn (b1 ) are disconnected, whilst in OSM data, they (a2 and b2 ) are externally connected.
The egg-yolk theory is independently developed by Lehmann and Cohn (1994), Cohn
and Gotts (1996b, 1996a), and Roy and Stell (2001) extending the RCC theory and by
Clementini and Felice (1996, 1997) extending the 9-intersection model, in order to represent and reason about regions with indeterminate boundaries. In this theory, a region with
an indeterminate boundary (an indeterminate region) is represented by a pair of regions,
an egg and a yolk, which are the maximum extension and the minimum extension of
the indeterminate region respectively (similar to the upper approximation and lower approximation in the rough set theory, Pawlak, Polkowski, & Skowron, 2007). The yolk is
not empty and it is always a proper part of the egg. The egg-yolk theory presupposes the
existence of a core part of a region and a more vague part. For the described application,
the same location can be represented using two disconnected polygons from an authoritative geospatial dataset and a crowd-sourced geospatial dataset respectively. In this case, we
could not define a certain inner region for any of the disconnected polygons, otherwise, it
is inconsistent to treat them as different representations for the same location.
We are aware that there are several approaches (Fine, 1975; Zadeh, 1975; Smith, 2008)
to representing vague concepts and relations, which have been adopted to extend classical
theories such as RCC and the 9-intersection model. A main approach is to assign a degree of
truth or a degree of membership to concepts and relations. For example, in the fuzzy region
connection calculus (fuzzy RCC) (Schockaert et al., 2008b, 2008a, 2009), the connection
relation C is defined as a reflexive and symmetric fuzzy relation. For regions a, b, C(a, b)
denotes the degree to which a and b are connected. Using C as the primitive relation,
every RCC relation R can be redefined to calculate the degree to which R holds. The
fuzzy RCC or other similar formalisms may be applied in the case shown in Figure 2. For
example, with an appropriate membership function for the relation EC, EC(a1 , b1 ) = 0.8
696

fiQualitative Spatial Logics for Buffered Geometries

Figure 3: Buffering the geometry X by a distance ; three dashed circles are buffered part
of (BPT) the solid circle; the dashed circle and the solid circle are buffered equal
(BEQ)

and EC(a2 , b2 ) = 1, then no contradiction will arise. We did not adopt this approach in the
matching problem, mainly because there is no good way to define the degree of membership,
and it is difficult to generate user-friendly explanations for why the matches are wrong if
the underlying reasoning is numerical and relatively obscure.
The logic M S(M ) was proposed and developed by Sturm, Suzuki, Wolter, and Zakharyaschev (2000), Kutz, Sturm, Suzuki, Wolter, and Zakharyaschev (2002), Kutz, Wolter,
Sturm, Suzuki, and Zakharyaschev (2003), Wolter and Zakharyaschev (2003, 2005), and
Kutz (2007) for reasoning about distances. The logic M S(M ) makes it possible to define
concepts such as an object within the distance of 100 meters from a School. M in M S(M )
is a parameter set. A typical example of M is Q0 . The satisfiability problem for a finite set
of M S(Q0 ) formulas in a metric space is EXPTIME-complete (Wolter & Zakharyaschev,
2003). M S(M ) was not developed for the problem of geospatial data matching. However, after we designed the logics introduced in this paper, we discovered that they form a
proper fragment of M S(Q0 ). To detect problematic matches, we also reason about distances between objects, but this reasoning is of a more restricted and qualitative kind. The
complexity of the satisfiability problem of our logics is NP-complete, which makes them
somewhat more suitable for automatic debugging of matches than the full M S(Q0 ). The
syntax and semantics of M S(M ) and the proofs for the proper fragment relations are
provided later in this paper (see Section 3).
In this paper, we present a series of new qualitative spatial logics developed for validating
matches between spatial objects: a logic of NEAR and FAR for buffered points (LNF) (Du,
Alechina, Stock, & Jackson, 2013), a logic of NEAR and FAR for buffered geometries
(LNFS) and a logic of part and whole for buffered geometries (LBPT) (Du & Alechina,
2014a, 2014b). The notion of buffer (ISO Technical Committee 211, 2003) is used to model
the uncertainty in geometry representations, tolerating slight differences up to a margin of
error or a level of tolerance   R0 . As shown in Figure 3, the buffer of a geometry X is
a geometry which contains exactly all the points within  distance from X. The buffer of
X is denoted as buffer (X , ). For a geometry X which is possibly represented inaccurately
within the margin of error  in one dataset, its corresponding representation in the other
dataset is assumed to be somewhere within buffer (X , ).
The spatial logics involve four spatial relations BufferedPartOf (BPT), BufferedEqual
(BEQ), NEAR and FAR. They formalize the notions of possibly partOf, possibly sameAs,
possibly connected (given a possible displacement by ) and definitely disconnected
(even if displaced by ) respectively. A geometry X is BufferedPartOf a geometry X 0 , if
X is within buffer (X 0 , ); two geometries are BufferedEqual, if they are BufferedPartOf
697

fiDu & Alechina

Figure 4: NEAR and FAR
each other (Figure 3). We assume that two geometries X and X 0 from two diferent
datasets may correspond to the same object if they are BufferedEqual. The parameter
 captures the margin of error in representation of geometries. Two geometries X, Y are
NEAR, if the corresponding geometries X 0 , Y 0 in the other dataset could be connected,
i.e. distance(X, Y )  [0, 2] (Figure 4). Clearly, if FAR(X , Y ) holds, then NEAR(X , Y )
should be false for X and Y from the same dataset. In addition, we want to exclude the
possibility that NEAR(X 0 , Y 0 ) may hold for X 0 , Y 0 (corresponding to X, Y respectively)
in the other dataset. Therefore we define FAR(X , Y ) as distance(X, Y )  (4, +) (Figure 4). It is possible that two geometries X, Y are not NEAR and not FAR, this is,
distance(X, Y )  (2, 4].
The way of defining BEQ, N EAR and F AR is similar to that for defining distance
relations between points by Moratz and Wallgrun (2012), where each point is assigned one
or more reference distances. The distance relations between two points X, Y are defined
by comparing the distance between X, Y to the reference distances of X and those of Y .
As different points can have different reference distances for indicating nearness, distance
relations may not be symmetric. Differing from the work by Moratz and Wallgrun (2012),
the relations we defined are not only for points but also for general geometries, and every
geometry has the same reference distances (, 2 and 4), which leads to the symmetric
definitions of BEQ, N EAR and F AR. We provide sound and complete sets of axioms to
support reasoning about BEQ, BP T , N EAR and F AR relations (see Section 4). This reasoning is useful for verifying matches between spatial representations from different sources.
As explained in our previous work (Du et al., 2013), though the relations are named
N EAR and F AR, we do not attempt to model human notions of nearness or proximity,
which is influenced by several factors, such as absolute distance, relative distance, frame of
reference, object size, travelling costs and reachability, travelling distance and attractiveness
of objects (Guesgen & Albrecht, 2000). In this work, we provide a strict mathematical
definition for the calculation of whether two objects are to be considered as being N EAR
or F AR, based on a margin of error . While this makes our approach less likely to be
suitable for the simulation of human notions of nearness, it provides a useful tool for verifying
consistency of matches. The following arguments are formalized for checking consistency of
sameAs and partOf matches: if spatial objects a1 , b1 are sameAs or partOf spatial objects
a2 , b2 respectively, a1 , b1 are N EAR, a2 , b2 are F AR, then a contradiction exists.
The rest of this paper is structured as follows. Section 2, Section 3 and Section 4 provide
an introduction to the new spatial logics: their syntax and semantics, their relationships
with the logic M S(M ), their axioms and theorems. Section 5 and Section 6 present the
proofs of the soundness, completeness, decidability and complexity theorems for LBPT, as
the proofs for LNF and LNFS are similar and LBPT is more expressive than LNF and
LNFS. Section 7 describes how LBPT is used for debugging matches between objects from
698

fiQualitative Spatial Logics for Buffered Geometries

different geospatial datasets. Section 8 discusses the generality and limitations of the spatial
logics. Section 9 concludes the paper.

2. Syntax and Semantics
The language L(LN F ) is defined as
,  := BEQ(a, b) | N EAR(a, b) | F AR(a, b) |  |   
where a, b are individual names.    def (  ). The language L(LN F S) is
exactly the same as L(LN F ). The language L(LBP T ) is almost the same as L(LN F ) and
L(LN F S), except that it has BP T instead of BEQ as a predicate. L(LBP T ) is defined as
,  := BP T (a, b) | N EAR(a, b) | F AR(a, b) |  |   .
L(LN F ), L(LN F S) and L(LBP T ) are all interpreted over models based on a metric
space. Every individual name involved in an LNF formula is mapped to a point, whilst
each of those involved in an LNFS/LBPT formula is mapped to an arbitrary geometry or
a non-empty set of points.
Definition 1 (Metric Space) A metric space is a pair (, d), where  is a non-empty
set (of points) and d is a metric on , i.e. a function d :     R0 , such that for
any x, y, z  , the following axioms are satisfied:
1. identity of indiscernibles: d(x, y) = 0 iff x = y;
2. symmetry: d(x, y) = d(y, x);
3. triangle inequality: d(x, z)  d(x, y) + d(y, z).
Definition 2 (Metric Model of LNF) A metric model M of LNF is a tuple (, d, I, ),
where (, d) is a metric space, I is an interpretation function which maps each individual
name to an element in , and   R0 is a margin of error. The notion of M |=  ( is
true in the model M ) is defined as follows:
M |= BEQ(a, b) iff d (I (a), I (b))  [0 , ];
M |= N EAR(a, b) iff d (I (a), I (b))  [0 , 2 ];
M |= F AR(a, b) iff d (I (a), I (b))  (4 , );
M |=  iff M 6|= ;
M |=    iff M |=  and M |= ,
where a, b are individual names, ,  are formulas in L(LN F ).
Definition 3 (Metric Model of LNFS/LBPT) A metric model M of LNFS/LBPT is
a tuple (, d, I, ), where (, d) is a metric space, I is an interpretation function which
maps each individual name to a non-empty set of elements in , and   R0 is a margin
of error. The notion of M |=  ( is true in the model M ) is defined as follows:
699

fiDu & Alechina

M |= BP T (a, b) iff pa  I (a) pb  I (b) : d (pa , pb )  [0 , ];
M |= N EAR(a, b) iff pa  I (a) pb  I (b) : d (pa , pb )  [0 , 2 ];
M |= F AR(a, b) iff pa  I (a) pb  I (b) : d (pa , pb )  (4 , );
M |=  iff M 6|= ;
M |=    iff M |=  and M |= ,
where a, b are individual names, ,  are formulas in L(LN F S)/L(LBP T ). BEQ(a, b) is
defined as BP T (a, b)  BP T (b, a).
The notions of validity and satisfiability in metric models are standard. A formula is
satisfiable if it is true in some metric model. A formula  is valid (|= ) if it is true in all
metric models (hence if its negation is not satisfiable). The logic LNF/LNFS/LBPT is the
set of all valid formulas in the language L(LN F )/L(LN F S)/L(LBP T ) respectively.

3. Relationship with the logic M S(M )
The logic M S(M ), as well as its variations, was developed by Sturm et al. (2000), Kutz
et al. (2002, 2003), Wolter and Zakharyaschev (2003, 2005), and Kutz (2007) for reasoning
about distances.
M S(M ) is a family of logics defined relative to the parameter set M  Q0 . M is
subject to the following two conditions: if a, b  M and a + b  r, then a + b  M , where
r = sup M if M is bounded, otherwise r = ; if a, b  M and a  b > 0, then a  b  M .
The alphabet of M S(M ) consists of
 an infinite list of region variables X1 , X2 ,...;
 an infinite list of location constants c1 , c2 ,...;
 a set constant {ci } for every location constant ci ;
.
 binary distance (), equality (=) and membership () predicates;
 the boolean operators u,  (and their derivatives t, > and );
 two distance quantifiers <a , a and their duals <a , a , for every a  M ;
 two universal quantifiers  and .
M S(M ) terms are defined as:
s, t := Xi | {ci } | > |  | s | s u t | <a s | a s | s.
In addition to standard description logic concept constructions, M S(M ) can define a
concept of objects which are at a distance less than a from instances of some other concept
s: <a s, and similarly for a distance at most a. <a s and a s are defined as <a (s)
and a (s) respectively.
700

fiQualitative Spatial Logics for Buffered Geometries

M S(M ) formulas are defined as
.
,  := c  s | s = t | (c1 , c2 ) < a | (c1 , c2 )  a |  |   .
.
.
.
Further, s v t is an abbreviation for (s u t) = s and s =
6 t is an abbreviation for (s = t).
(c1 , c2 ) > a and (c1 , c2 )  a are defined as ((c1 , c2 )  a) and ((c1 , c2 ) < a) respectively.
An M S(M )-model B is a structure of the form:
B
B = hW, d, X1B , X2B , ..., cB
1 , c2 , ...i

where hW, di is a metric space (Definition 1), each XiB is a subset of W , and each cB
i is
an element of W . The value of any other M S(M )-term in B is computed inductively as
follows:
 >B = W , B = ;
 {ci }B = {cB
i };
 (s)B = W  sB ;
B
 (s1 u s2 )B = sB
1  s2 ;

 (<a s)B = {x  W | y  sB : d(x, y) < a};
 (a s)B = {x  W | y  sB : d(x, y)  a};
 (s)B = {x  W | y  sB }.
<a , a and  are dual to <a , a and  respectively. For instance,
(<a s)B = {x  W | y  W : (d(x, y) < a  y  sB )}.
The truth condition of B |= , where  is an M S(M )-formula, is defined as follows:
 B |= c  s iff cB  sB ;
.
B
 B |= s1 = s2 iff sB
1 = s2 ;
 B |= (k, l) < a iff d(k B , lB ) < a;
 B |= (k, l)  a iff d(k B , lB )  a;
 B |=  iff B 6|= ;
 B |=    iff B |=  and B |= .
A set of M S(M ) formulas  is satisfiable, if there exists an M S(M )-model B such that
B |=  for every   . This is denoted as B |= .
It is proved below that LNF/LNFS/LBPT are proper fragments of the logic M S(Q0 ).
Strictly speaking, this only holds when   Q0 , but later we will show that a finite set of
LNF/LNFS/LBPT formulas is satisfiable where   R0 , if it is satisfiable when  = 1. In
other words,  acts as a scaling factor (see the proof of Lemma 43).
701

fiDu & Alechina

Lemma 1 For individual names a, b, the M S(M ) formula {a} v {b} is not expressible
in LNF.
Proof. Let M1 , M2 be metric models2 . M1 = (1 , d, I1 , ), M2 = (2 , d, I2 , ).
In M1 , 1 = {o1 , o2 }, d(o1 , o2 ) = . I1 (a) = o1 , I1 (b) = o2 . For any individual name x
differing from a, b, I1 (x) = o1 .
In M2 , 2 = {o}. I2 (a) = o, I2 (b) = o. For any individual name x differing from a, b,
I2 (x) = o. For any individual name y, Ii ({y}) = {Ii (y)}, i  {1, 2}.
By the definitions of M1 , M2 , for any individual names x, y, d(I1 (x), I1 (y))  [0, ],
d(I2 (x), I2 (y)) = 0. If  is an atomic LNF formula about x, y, then by Definition 2, M1 |= 
iff M2 |= . By an easy induction on logical connectives, for any LNF formula , M1 |= 
iff M2 |= .
Since I1 ({a}) = {o1 }, I1 ({b}) = {o2 } and I2 ({a}) = I2 ({b}) = {o}, by the truth definition of M S(M ) formulas, M1 |= ({a} v {b}), M2 6|= ({a} v {b}). Hence, {a} v {b} is
not equivalent to any LNF formula. 
Lemma 2 The logic LNF is a proper fragment of the logic M S(Q0 ).
Proof. Every atomic LNF formula is expressible in M S(Q0 ):
 BEQ(a, b)  ((a, b)  0)  ((a, b)  );
 N EAR(a, b)  ((a, b)  0)  ((a, b)  2);
 F AR(a, b)  ((a, b) > 4).
This means that all LNF formulas can be expressed in a fragment of M S(Q0 ) (the image
of LNF under the translation above) which only contains location constants, binary distance predicate and boolean connectives , . By Lemma 1, LNF is a proper fragment of
M S(M ). 
Lemma 3 For individual names a, b, the M S(M ) formula a v b is not expressible in
LNFS/LBPT.
Proof. Let M1 , M2 be metric models3 . M1 = (1 , d, I1 , ), M2 = (2 , d, I2 , ).
In M1 , 1 = {o1 , o2 }, d(o1 , o2 ) = . I1 (a) = {o1 }, I1 (b) = {o2 }. For any individual
name x differing from a, b, I1 (x) = {o1 }.
In M2 , 2 = {o}. I2 (a) = {o}, I2 (b) = {o}. For any individual name x differing from
a, b, I2 (x) = {o}.
If  is an atomic LNFS/LBPT formula about x, y, then by Definition 3, M1 |=  iff
M2 |= . By an easy induction on logical connectives, for any LNFS/LBPT formula ,
M1 |=  iff M2 |= .
2. Note that we can construct models in a one-dimensional or two-dimensional Euclidean space in a similar
way and prove the lemma.
3. Note that we can construct models in a one-dimensional or two-dimensional Euclidean space in a similar
way and prove the lemma.

702

fiQualitative Spatial Logics for Buffered Geometries

By the truth definition of M S(M ) formulas, M1 |= (a v b) and M2 6|= (a v b).
Hence, a v b is not equivalent to any LNFS/LBPT formula. 
Lemma 4 The logic LNFS/LBPT is a proper fragment of M S(Q0 ).
Proof. Every atomic LNFS/LBPT formula is expressible in M S(Q0 ):
 (For LNFS) BEQ(a, b) iff (a v ( b))  (b v ( a));
 (For LBPT) BP T (a, b) iff (a v ( b));
.
 N EAR(a, b) iff (a u (2 b) =
6 );
.
 F AR(a, b) iff (a u (4 b) = ).
Note that the formulas on the right belong to a fragment of M S(Q0 ) which is the image
of LNFS/LBPT under the translation above.
The correctness of translation of BEQ(a, b) and BP T (a, b) into M S(Q0 ) follows directly from the truth definition of BEQ and BP T (Definition 3). To show that the translation of N EAR and F AR are correct, consider that the truth definition of N EAR(a, b)
is equivalent to 0  dmin (a, b)  2 and F AR(a, b) to dmin (a, b) > 4, where dmin (a, b) =
inf{d(pa , pb ) | pa  I(a), pb  I(b)}. It was shown by Wolter and Zakharyaschev (2005) that
.
dmin (a, b)  m iff a u (m b) =
6 . This makes the translation of the formulas have the
same truth conditions as defined in Definition 3. By Lemma 3, LNFS/LBPT is a proper
fragment of M S(Q0 ). 
Wolter and Zakharyaschev (2003) proved that the satisfiability problem for a finite set
of M S(Q0 ) formulas in a metric space is EXPTIME-complete, which provides an upper
bound on the complexity of the satisfiability problems of LNF, LNFS and LBPT in a metric
space.
Kutz et al. (2002) and Kutz (2007) gave axioms or inference rules connecting M S(M )
terms (e.g. 0 s  s) for M S(M ) and its variants. However, the axiomatisation we are
going to present is for LNF, LNFS and LBPT formulas (corresponding to M S(M ) formulas
rather than M S(M ) terms).

4. Axioms and Theorems
This section presents a sound and complete axiomatisation for the logic LNF/LNFS/LBPT
respectively. The axiomatic systems have been used as a basis for a rule-based reasoner
described later in Section 7 4 .
The following calculus (which we will also refer to as LNF) is sound and complete for
LNF:
Axiom 0 All tautologies of classical propositional logic
4. It is important to have a complete axiomatisation. Otherwise, the reasoner can not detect all the
LNF/LNFS/LBPT inconsistencies caused by problematic matches.

703

fiDu & Alechina

Axiom 1 BEQ(a, a);
Axiom 2 BEQ(a, b)  BEQ(b, a);
Axiom 3 N EAR(a, b)  N EAR(b, a);
Axiom 4 F AR(a, b)  F AR(b, a);
Axiom 5 BEQ(a, b)  BEQ(b, c)  N EAR(c, a);
Axiom 6 BEQ(a, b)  N EAR(b, c)  BEQ(c, d)  F AR(d, a);
Axiom 7 N EAR(a, b)  N EAR(b, c)  F AR(c, a);
MP Modus ponens: ,    ` .
The following calculus (which we will also refer to as LNFS) is sound and complete for
LNFS:
Axiom 0 All tautologies of classical propositional logic
Axiom 1 BEQ(a, a);
Axiom 2 BEQ(a, b)  BEQ(b, a);
Axiom 3 N EAR(a, b)  N EAR(b, a);
Axiom 4 F AR(a, b)  F AR(b, a);
Axiom 5 BEQ(a, b)  BEQ(b, c)  N EAR(c, a);
Axiom 6 BEQ(a, b)  N EAR(b, c)  BEQ(c, d)  F AR(d, a);
Axiom 8 N EAR(a, b)  BEQ(b, c)  BEQ(c, d)  F AR(d, a);
MP Modus ponens: ,    ` .
Axiom 7 of the calculus LNF only holds for points, but not for arbitrary geometries,
because two points within the same line or polygon can be far from each other. Axiom 7 is
replaced by Axiom 8 in LNFS. All other axioms in LNFS are the same as those in LNF.
The following calculus (which we will also refer to as LBPT) is sound and complete for
LBPT:
Axiom 0 All tautologies of classical propositional logic
Axiom 3 NEAR(a, b)  NEAR(b, a);
Axiom 4 FAR(a, b)  FAR(b, a);
Axiom 9 BPT (a, a);
Axiom 10 BPT (a, b)  BPT (b, c)  NEAR(c, a);
704

fiQualitative Spatial Logics for Buffered Geometries

Axiom 11 BPT (b, a)  BPT (b, c)  NEAR(c, a);
Axiom 12 BPT (b, a)  NEAR(b, c)  BPT (c, d )  FAR(d , a);
Axiom 13 NEAR(a, b)  BPT (b, c)  BPT (c, d )  FAR(d , a);
MP Modus ponens: ,    ` .
The calculus LBPT is similar to the calculus LNFS. Changing predicates from BEQ to
BP T , LNFS Axioms 1, 6, 8 are replaced by Axioms 9, 12, 13 respectively in LBPT. Since
BP T is not symmetric, LNFS Axiom 2 does not have a corresponding axiom in LBPT, and
LNFS Axiom 5 is replaced by two LBPT axioms, Axiom 10 and Axiom 11.
The notion of derivability  `  in LNF/LNFS/LBPT calculus is standard. A formula
 is derivable if ` . A set  is LNF/LNFS/LBPT-inconsistent if for some formula  it
derives both  and .
We proved the following theorems for LNF, LNFS and LBPT.
Theorem 1 (Soundness and Completeness) The LNF/LNFS/LBPT calculus is sound
and complete for metric models, namely that
`   |= 
(every derivable formula is valid and every valid formula is derivable).
Theorem 2 (Decidability and Complexity) The satisfiability problem for a finite set
of LNF/LNFS/LBPT formulas in a metric space is NP-complete.
In the following sections, we give proofs of the results above for the case of LBPT. The
proofs for LNF and LNFS are similar. For LBPT, we have the following derivable formulas,
which we will refer to as facts in the completeness proof:
Fact 14 BP T (a, b)  N EAR(a, b);
Fact 15 N EAR(a, b)  F AR(a, b);
Fact 16 N EAR(a, b)  BP T (b, c)  F AR(c, a);
Fact 17 BP T (a, b)  F AR(a, b);
Fact 18 BP T (a, b)  BP T (b, c)  F AR(c, a);
Fact 19 BP T (b, a)  BP T (b, c)  F AR(c, a);
Fact 20 BP T (a, b)  BP T (b, c)  BP T (c, d)  F AR(d, a);
Fact 21 BP T (b, a)  BP T (b, c)  BP T (c, d)  F AR(d, a);
Fact 22 BP T (a, b)  BP T (b, c)  BP T (c, d)  BP T (d, e)  F AR(e, a);
Fact 23 BP T (b, a)  BP T (b, c)  BP T (c, d)  BP T (d, e)  F AR(e, a);
Fact 24 BP T (b, a)  BP T (c, b)  BP T (c, d)  BP T (d, e)  F AR(e, a).
As shown by Facts 17-24, a chain of at most four BP T s implies the negation of F AR,
because F AR is defined as being > 4 distance away in Definition 3.
705

fiDu & Alechina

5. Soundness and Completeness of LBPT
This section shows that the LBPT calculus is sound and complete for metric models.
Though several definitions and lemmas have been presented in our previous work (Du et al.,
2013; Du & Alechina, 2014b), the proofs presented here are more complete, structured, accurate (small errors are corrected) and simplified.
The proof of soundness (every LBPT derivable formula is valid: `   |= ) is by an
easy induction on the length of the derivation of . Axioms 3, 4, 9-13 are valid (by the
truth definition of BP T , N EAR and F AR) and modus ponens preserves validity.
In the rest of this section, we prove completeness (every LBPT valid formula is derivable):
|=   ` 
We will actually prove that if a finite set of LBPT formulas  is consistent, then there
is a metric model satisfying it. Any finite set of formulas  can be rewritten as a formula
 which is the conjunction of all formulas in .  is consistent, iff  is consistent (6` ).
If there is a metric model M satisfying , then M satisfies , thus 6|= . Therefore, if
we show that if  is consistent, then there exists a metric model satisfying it, then we
show that if 6` , then 6|= . This shows that 6`  6|=  and by contraposition we get
completeness.
The completeness theorem is proved by constructing a metric model for a maximal
consistent set (Definition 4) of any finite consistent set of LBPT formulas (Lemma 5).
Definition 4 (MCS) A set of formulas  in the language L(LBP T ) is maximal consistent, if  is consistent, and any set of LBPT formulas over the same set of individual names
properly containing  is inconsistent. If  is a maximal consistent set of formulas, then we
call it an M CS.
Proposition 1 (Properties of MCSs) If  is an M CS, then,
  is closed under modus ponens: if ,     , then   ;
 if  is derivable, then   ;
 for all formulas :    or   ;
 for all formulas , :      iff    and   ;
 for all formulas , :      iff    or   .
Lemma 5 (Lindenbaums Lemma) If  is a consistent set of formulas in the language
L(LBP T ), then there is an M CS + over the same set of individual names such that
  + .
Let 0 , 1 , 2 , ... be an enumeration of LBPT formulas over the same set of individual names
as that in . + can be defined as follows:
 0 = ;
 n+1 = n  {n }, if it is consistent, otherwise, n+1 = n  {n };
706

fiQualitative Spatial Logics for Buffered Geometries

 + =

S

n0 n .

For a finite consistent set of formulas , we construct a metric model satisfying a
maximal consistent set + , which contains  and is over the same set of individual names
as , as follows. Firstly, we equivalently transform + to B(+ ), which is a set of basic
quantified formulas. Then we construct a set of distance constraints D(+ ) from B(+ ).
A key concept here is path-consistency for a set of distance constraints.
Definition 5 (Non-negative Interval) An interval h is non-negative, if h  [0, +).
Definition 6 (Distance Constraint, Distance Range) A distance constraint is a statement of the form d(p, q)  g, where p, q are constants representing points, d(p, q) stands for
the distance between p, q, and g is a non-negative interval, which stands for the distance
range for p, q.
Definition 7 (Composition) If d1 , d2 are non-negative real numbers, then the composition of {d1 } and {d2 } is defined as: {d1 }  {d2 } = [|d1  d2 |, d1 + d2 ] 5 . If g1 , g2 are nonnegative intervals, then their composition is an interval which is the union of all {d1 }{d2 },
where d1  g1 , d2  g2 , this is,
S
g1  g2 = d1 g1 ,d2 g2 {d1 }  {d2 }.
It is assumed that a set of distance constraints D contains at most one distance range for
each pair of constants p, q involved in D, and D is closed under symmetry, i.e. if d(p, q)  g
is in D, then d(q, p)  g is in D.
Definition 8 (Path-Consistency) For a set of distance constraints D, for every pair of
different constants p, q involved in D, their distance range is strengthened by successively
applying the following operation until a fixed point is reached:
s : g(p, q)  g(p, q)  (g(p, s)  g(s, q))
where s is a constant in D, s 6= p, s 6= q, and g(p, q) denotes the distance range for p, q.
This process is called enforcing path-consistency on D. If at a fixed point, for every pair of
constants p, q, g(p, q) 6= , then D is called path-consistent.
In this paper, we say an interval is referred to in the process of enforcing path-consistency
on D, if it occurs in D or is involved in the enforcement of the operation g(p, q)  g(p, q) 
(g(p, s)g(s, q)). In other words, it is used as g(p, q), g(p, s) or g(s, q). A distance constraint
appears in the process of enforcing path-consistency on D, if its distance range (an interval)
is referred to in the process of enforcing path-consistency on D.
The way of enforcing path-consistency on a set of distance constraints defined above is
almost the same as that of enforcing path-consistency on a binary constraint satisfaction
problem (CSP) (Renz & Nebel, 2007; van Beek, 1992), except that the operation s :
g(p, q)  g(p, q)  (g(p, s)  g(s, q)) ( is the composition operator for non-negative intervals,
Definition 7) is applied instead of k : Rij  Rij (Rik Rkj ) ( is the composition operator
5. Based on d(x, z)  d(x, y) + d(y, z) (Property 3 of Definition 1).

707

fiDu & Alechina

for relations). The time complexity of the path-consistency algorithm for CSP is O(n3 ) (van
Beek, 1992; Mackworth & Freuder, 1985), where n is the number of variables involved in
the input set of binary constraints. The path-consistency algorithm for CSP can be adapted
easily for enforcing path-consistency on a set of distance constraints. The time complexity of
the resulting path-consistency algorithm is also O(n3 ), where n is the number of constants
involved in the input set of distance constraints. Later in this paper, we will show that
the process of enforcing path-consistency on D(+ ) terminates, and a fixed point can be
reached in O(n3 ) (see Lemma 33).
After constructing a set of distance constraints D(+ ) from + , we prove the Metric
Model Lemma, Metric Space Lemma and Path-Consistency Lemma which are stated below.
The notion of path-consistency acts as a bridge between the lemmas.
Lemma 6 (Metric Model Lemma) Let  be a finite consistent set of formulas, and +
be an M CS which contains  and is over the same set of individual names as . If a
metric space satisfies D(+ ), then it can be extended to a metric model satisfying + .
Lemma 7 (Metric Space Lemma) Let  be a finite consistent set of formulas, and +
be an M CS which contains  and is over the same set of individual names as . If D(+ )
is path-consistent, then there is a metric space (, d) such that all distance constraints in
D(+ ) are satisfied.
Lemma 8 (Path-Consistency Lemma) Let  be a finite consistent set of formulas, and
+ be an M CS which contains  and is over the same set of individual names as . Then,
D(+ ) is path-consistent.
Using these three lemmas, we prove the completeness of LBPT: if a finite set of formulas
 is LBPT-consistent, then there exists a metric model satisfying it.
Proof. If  is LBPT-consistent, by the Lindenbaums Lemma (Lemma 5), there is an M CS
+ over the same set of individual names such that   + . By the Path-Consistency
Lemma (Lemma 8) and the Metric Space Lemma (Lemma 7), there is a metric space (, d)
such that all distance constraints in D(+ ) are satisfied. By the Metric Model Lemma
(Lemma 6), the metric space can be extended to a metric model M satisfying + . Since
  + , M satisfies . 
The detailed proofs of the Metric Model Lemma, Metric Space Lemma and PathConsistency Lemma are provided in Section 5.1, Section 5.2 and Section 5.3 respectively.
Note that, in this paper, + denotes an M CS which contains a given finite consistent set
of formulas  and is over the same set of individual names as .
5.1 Metric Model Lemma
This section shows how to construct a set of distance constraints D(+ ) from + , and
presents the proof of the Metric Model Lemma.
By the definition and properties of MCSs (Definition 4 and Proposition 1), the following
lemma holds.
708

fiQualitative Spatial Logics for Buffered Geometries

Lemma 9 If + is an M CS, then for any pair of individual names a, b occurring in ,
exactly one of the following cases holds in + :
1. case(a, b) = BP T (a, b)  BP T (b, a);
2. case(a, b) = BP T (a, b)  BP T (b, a);
3. case(a, b) = BP T (a, b)  BP T (b, a);
4. case(a, b) = BP T (a, b)  BP T (b, a)  N EAR(a, b);
5. case(a, b) = N EAR(a, b)  F AR(a, b);
6. case(a, b) = F AR(a, b),
where case(a, b) denotes the formula which holds between a, b in each case.
Lemma 9 is proved using LBPT axioms and facts (such as Axiom 3, Facts 14, 15) in the
same way as proving the lemma for LNF (see Du et al., 2013). The full proof of Lemma 9
is provided in Appendix A.
The construction of a set of distance constraints D(+ ) from + has two main steps:
Step 1 For every pair of individual names a, b occurring in , we translate case(a, b) into
a set of first order formulas which is equi-satisfiable to case(a, b). The union of all
such sets of first order formulas is B(+ ) (hence, B(+ ) and  are equi-satisfiable.).
This step is described by Definition 9 and Definition 10.
Step 2 We construct a set of distance constraints D(+ ) from B(+ ). This step is described by Definitions 11-13.
For LBPT formulas, there are first order formulas corresponding to their truth definition
in Definition 3. We use formulas of the form d(p, q)  g as abbreviations of their equivalent
first order formulas. For example, d(p, q)  [0, ] abbreviates d(p, q)  0  d(p, q)  .
Observe that6
 BP T (a, b) and pa  a pb  b : d (pa , pb )  [0 , ] are equi-satisfiable ;
 N EAR(a, b) and pa  a pb  b : d (pa , pb )  [0 , 2 ] are equi-satisfiable;
 F AR(a, b) and pa  a pb  b : d (pa , pb )  (4 , ) are equi-satisfiable.
Definition 9 (Basic Quantified Formula) We refer to the first order formulas of the
following forms as basic quantified formulas:
 pa  a pb  b : d (pa , pb )  g;
 pa  a pb  b : d (pa , pb )  g;
6. Note that by pa  a pb  b : d (pa , pb )  [0 , ], we are actually quantifying over a metric space. In
such sense, it is more precise to say, for example, BP T (a, b) is satisfiable in a metric model, iff
pa  a pb  b : d (pa , pb )  [0 , ] is satisfiable over a metric space.

709

fiDu & Alechina

 pa  a pb  b : d (pa , pb )  g;
 pa  a pb  b : d (pa , pb )  g,
where g is a non-negative interval. The abbreviations of these four forms are defined
as (a, b, g), (a, b, g), (a, b, g) and (a, b, g) respectively. In other words, for example,
(a, b, g)  (pa  a pb  b : d (pa , pb )  g).
Now we translate the formula in each case listed in Lemma 9 into basic quantified
formulas, which will be used to count the number of points needed for interpreting individual
names occurring in  later.
Definition 10 (B(+ )) For an M CS + over the same set of individual names as , its
corresponding set of basic quantified formulas B(+ ) is constructed as follows. For every
pair of individual names a, b, we translate case(a, b) into basic quantified formulas:
 translate(BP T (a, b)  BP T (b, a)) = {(a, b, [0, ]), (b, a, [0, ])};
 translate(BP T (a, b)  BP T (b, a)) = {(a, b, [0, ]), (b, a, (, ))};
 translate(BP T (a, b)  BP T (b, a)) = {(a, b, (, )), (b, a, [0, ])};
 translate(BP T (a, b)  BP T (b, a)  N EAR(a, b)) = {(a, b, (, )),
(b, a, (, )), (a, b, [0, 2]), (b, a, [0, 2])};
 translate(N EAR(a, b)  F AR(a, b)) = {(a, b, (2, )), (b, a, (2, )),
(a, b, [0, 4]), (b, a, [0, 4])};
 translate(F AR(a, b)) = {(a, b, (4, )), (b, a, (4, ))},
where   R0 is a fixed margin of error. Let names() be the set of individual names
occurring in . Then,
S
B(+ ) = anames(),bnames() translate(case(a, b)).
In the following, for a set of basic quantified formulas B(+ ), we construct a set of
distance constraints D(+ ), and then show that if there is a metric space satisfying D(+ ),
then it can be extended to a model of + . In other words, we are constructing a metric
over a set of points used to interpret individual names.
The number of points needed for interpreting each individual name depends on the
numbers of different forms of formulas in B(+ ). For any individual name a, let us predict
how many particular constants in points(a) (points assigned to an individual name a) can
be specified by the finite set of formulas about a in B(+ ). points(a) contains at least
one constant. If a formula in B(+ ) says there exists a constant in points(a), then this
constant is a particular constant within points(a). For any pair of different individual names
a, b, if both (a, b, g) and (b, a, g) are in B(+ ), we only count one of them; if (a, b, g)
is in B(+ ), we map all the constants in points(a) to the same constant in points(b). By
Lemma 9 and Definition 10, in B(+ ), for any pair of different individual names a, b and
R  {, , } we never have R(a, b, g1 ) and R(a, b, g2 ), where g1 6= g2 , at the same time.
The cardinality of points(a) is specified as follows.
710

fiQualitative Spatial Logics for Buffered Geometries

Definition 11 (num(a, B(+ )) and points(a)) Let names() be the set of individual names
occurring in  7 . For any individual name a  names(),
num(a, B ( + )) = |{b  names( ) | g : (a, b, g)  B ( + )}|
num(a, B ( + )) = |{b  names( ) | g : (a, b, g)  B ( + )}|
num(a, B ( + )) = |{b  names( ) | g : (b, a, g)  B ( + )}|
Then num(a, B ( + )) = num(a, B ( + )) + num(a, B ( + )) + num(a, B ( + )).
points(a) is a set of constants {p1a , . . . , pna }, where n = num(a, B(+ )).
Definition 12 (Witness for a formula) A witness for a formula (a, b, g) is a pair of
constants pa  points(a), pb  points(b) such that d(pa , pb )  g. A witness for a formula
(a, b, g) or (b, a, g) is a constant pa  points(a), such that for any constant pb  points(b),
d(pa , pb )  g. A constant is clean for a formula, if it is not a witness for any other formula.
Definition 13 (D(+ )) Let B(+ ) be the corresponding set of basic quantified formulas
of an M CS + . For every individual name a in , we assign a fixed set of new constants points(a) to it. We construct a set of distance constraints D(+ ) as follows, by
iterating through the basic quantified formulas in B(+ ) and eliminating quantifiers on new
constants. Initially, D(+ ) = {}. For every individual name a in , for every constant
pa  points(a), we add d(pa , pa )  {0} to D(+ ). For every pair of different individual
names a, b, if
 (a, b, g)  B ( + ), then we take clean constants pa  points(a), pb  points(b), and
add d (pa , pb ) = d (pb , pa )  g to D(+ ), so pa , pb become a witness for (a, b, g);
 (a, b, g)  B ( + ), then we take a clean constant pa  points(a), for every pb  points(b),
we add d (pa , pb ) = d (pb , pa )  g to D(+ ), so pa becomes a witness for (a, b, g);
 (b, a, g)  B ( + ), then we take a clean constant pb  points(b), for every pa  points(a),
we add d (pa , pb ) = d (pb , pa )  g to D(+ ), so pb becomes a witness for (b, a, g);
 (a, b, g)  B ( + ), then we take a clean constant pb  points(b), for every pa  points(a),
we add d (pa , pb ) = d (pb , pa )  g to D(+ ), so pb becomes a witness for (a, b, g);
 (b, a, g)  B ( + ), then we take a clean constant pa  points(a), for every pb  points(b),
we add d (pa , pb ) = d (pb , pa )  g to D(+ ), so pa becomes a witness for (b, a, g);
 (a, b, g)  B ( + ), then for every pair of constants pa  points(a), pb  points(b),
we add d (pa , pb ) = d (pb , pa )  g to D(+ ).
For every pair of different constants p, q in D(+ ), we add d (p, q) = d (q, p)  [0 , )
to D(+ ), then repeatedly replace d (p, q) = d (q, p)  g1 and d (p, q) = d (q, p)  g2 with
d (p, q) = d (q, p)  (g1  g2 ), until there is only one distance range for each pair of p, q in
D(+ ).
7. By the definition of + , + contains the same set of individual names as .

711

fiDu & Alechina

In Definition 13, for every pair of different individual names a, b, we check whether
(a, b, g)  B ( + ) holds and check whether (b, a, g)  B ( + ) holds, because it is possible only one of them holds. For the same reason, we check (a, b, g)  B ( + ) and
(b, a, g)  B ( + ) separately. By Definition 10, (a, b, g)  B ( + ) iff (b, a, g)  B ( + ).
Hence we only need to check any one of them. We only check whether (a, b, g)  B ( + )
holds, as (a, b, g)  B ( + ) iff (b, a, g)  B ( + ).
Lemma 10 When constructing D(+ ), for any individual name a, the number of clean
constants needed from points(a) is no larger than num(a, B(+ )).
Proof. By Definition 10, for any individual name a, (a, a, [0, ]) is in B(+ ). By Definition 11, num(a, B(+ ))  1.
If a is not involved in any formula of the form (a, b, g), (a, b, g) or (b, a, g), for any
other individual name b, then by Definition 11, num(a, B(+ )) = 1. By Definition 13, we
need no clean constants from points(a).
Otherwise, by Lemma 9 and Definition 10, in B(+ ), for any pair of different individual
names a, b and R  {, , }, we never have R(a, b, g1 ) and R(a, b, g2 ), where g1 6= g2 , at the
same time. By Definition 13, for each (a, b, g)  B(+ ), we take one clean constant from
points(a), so num(a, B(+ )) clean constants are needed in total for all formulas of this
form. Similarly, num(a, B(+ )) and (num(a, B(+ ))  1) clean constants are needed
for formulas of forms (a, b, g) and (b, a, g) respectively, where a, b are different individual
names. We do not need any other clean constant from points(a) for formulas in other forms.
By Definition 11, num(a, B(+ )) is enough. 
D(+ ) and B(+ ) are not equi-satisfiable because of the way we assign witnesses for 
formulas. More specifically, for any pair of different individual names a, b, if (a, b, g) is in
B(+ ), we map all the constants in points(a) to the same constant in points(b). In other
words, if B 0 (+ ) is the set of formulas resulting from replacing every (a, b, g) in B(+ )
with (b, a, g), then D(+ ) and B 0 (+ ) are equi-satisfiable. Since every individual name is
interpreted as a non-empty set of constants, if a model satisfies (b, a, g), then it satisfies
(a, b, g), but not vice versa. Hence, constructing D(+ ) for B 0 (+ ) rather than B(+ )
imposes stronger restrictions (i.e. (a, b, g) in B(+ ) is replaced with (b, a, g) in B 0 (+ ))
on the metric space compared to that required by + . However, later we will show that
if + is consistent, then D(+ ) can be satisfied in a metric space by proving the Metric
Space Lemma and Path-Consistency Lemma in the following sections.
Before proving the Metric Model Lemma, let us look at some important properties of
D(+ ), as shown by Lemmas 11-13. The proof of Lemma 11 is provided in Appendix A.
Lemma 12 follows from the proof of Lemma 11.
Lemma 11 For any distance range g occurring in D(+ ),
g  {{0}, [0, ], (, ), [0, 2], (2, ), (2, 4], (4, ), [0, )}.
Lemma 12 If p  points(a), q  points(b), and a 6= b, then d(p, q)  {0} is not in D(+ ).
Lemma 13 The number of constants in D(+ ) is finite.
712

fiQualitative Spatial Logics for Buffered Geometries

Proof. It is assumed that  is a finite consistent set of formulas over n (a finite number) individual names. By Lemma 9 and Definition 10, B(+ ) contains at most f =
(n + 2n(n  1)) formulas over n individual names. By Definition 11, for any individual
name a, num(a, B(+ ))  f . By Definition 13, the number of constants in D(+ ) is at
most nf . 
The Metric Model Lemma is proved as follows.
Lemma 14 If a metric model satisfies B(+ ), then it satisfies + .
Proof. The lemma follows from two observations. First, by Lemma 9, + is entailed by
the set C(+ ) = {case(a, b) : a  names(+ ), b  names(+ )}. Second, by Definition 10,
B(+ ) is a translation of truth conditions of C(+ ) into first order logic. If a metric model
satisfies B(+ ), then it satisfies C(+ ), and hence it satisfies + . 
Lemma 6 (Metric Model Lemma) Let  be a finite consistent set of formulas, and
+ be an M CS which contains  and is over the same set of individual names as . If a
metric space satisfies D(+ ), then it can be extended to a metric model satisfying + .
Proof. Suppose a metric space satisfies D(+ ). We extend it to a metric model M by
interpreting every individual name a occurring in + as points(a), as corresponding set
of constants of size num(a, B(+ )) (Definition 11 and Definition 13). By Definition 13,
any formula of the form (a, a, [0, ]) is satisfied by M . For any pair of different individual names, every ,  or  formula has a witness, and all  formulas are also satisfied by
M . Therefore, M is a metric model of B(+ ). By Lemma 14, M is a metric model of + . 

5.2 Metric Space Lemma
As the process of enforcing path-consistency (Definition 8) involves the application of the
composition operator  (Definition 7), we present several lemmas in Section 5.2.1 to demonstrate the main calculation rules of  and the properties of intervals obtained from composition. In Section 5.2.2, we characterize distance constraints in D(+ ) and those appearing
in the process of enforcing path-consistency on D(+ ). Using the definitions and lemmas introduced in Section 5.2.1 and Section 5.2.2, the Metric Space Lemma is proved in
Section 5.2.3.
5.2.1 The Composition Operator
In this section, we present several lemmas to show the main calculation rules of the composition operator  and the properties of intervals obtained from composition. These lemmas
are important for understanding several proofs in later sections.
Lemmas 15-16 follow from Definition 7.
Lemma 15 Let g1 , g2 be non-negative intervals. If d3  g1  g2 , then there exist d1  g1 ,
d2  g2 such that d3  [|d1  d2 |, d1 + d2 ].
713

fiDu & Alechina

Lemma 16 (Calculation of Composition) If (m, n), (s, t), (m, ), (s, ), {l}, {r} are
non-negative non-empty intervals, H1 , H2 , H are non-negative intervals, then the following
calculation rules hold:
1. {l}  {r} = [l  r, l + r], if l  r;
2. {l}  (s, t) = (s  l, t + l), if s  l;
3. {l}  (s, t) = [0, t + l), if l  (s, t);
4. {l}  (s, t) = (l  t, t + l), if t  l;
5. {l}  (s, +) = (s  l, +), if s  l;
6. {l}  (s, +) = [0, +), if s < l;
7. (m, n)  (s, t) = (s  n, t + n), if s  n;
8. (m, n)  (s, t) = [0, t + n), if (m, n)  (s, t) 6= ;
9. (m, n)  (s, +) = (s  n, +), if s  n;
10. (m, n)  (s, +) = [0, +), if s < n;
11. (m, +)  (s, +) = [0, +);
12. H1   = ;
13. H1  H2 = H2  H1 ;
14. (H1  H2 )  H = (H1  H)  (H2  H);
S
S
15. ( k Hk )  H = k (Hk  H), where k  N>0 ;
16. (H1  H2 )  H = (H1  H)  (H2  H), if (H1  H2 ) 6= ;
17. (H1  H2 )  H = H1  (H2  H).
In Lemma 16, Rule 14 is a special case of Rule 15, where k = 2. Rule 16 states that
the composition operation is distributive over non-empty intersections of intervals. It is
necessary to require H1  H2 6= , otherwise the property may not hold. For example, if
H1 = [0, 1], H2 = [2, 3], H = [1, 2], then (H1  H2 )  H =  whilst (H1  H)  (H2  H) =
[0, 3]  [0, 5] 6= . A similar property is defined by Li, Long, Liu, Duckham, and Both
(2015) for RCC relations. The proofs for the last three calculation rules are provided in
Appendix A, whilst others are more obvious.
For an interval h of the form (l, u), [l, u), (l, u] or [l, u], we call l the greatest lower
bound of h, represented as glb(h), and u the least upper bound of h, represented as lub(h).
Below we show some interesting properties regarding the composition of intervals and their
greatest lower/least upper bounds.
Lemma 17 For any non-negative non-empty intervals g, h, the following properties hold:
714

fiQualitative Spatial Logics for Buffered Geometries

1. lub(g  h) = lub(g) + lub(h);
2. glb(g  h)  max(glb(g), glb(h)).
Proof. Follows from Lemma 16. 
A non-empty interval h is right-closed, iff h = [x, y] or h = (x, y]. h is right-open, iff
h = [x, y) or h = (x, y). h is right-infinite, iff h = [x, ) or h = (x, ). h is left-closed, iff
h = [x, y] or h = [x, y). h is left-open, iff h = (x, y] or h = (x, y).
Lemma 18 Let g1 , g2 , g3 be non-negative non-empty right-closed intervals, if g1  g2  g3 ,
then lub(g1 )  lub(g2 ) + lub(g3 ).
Proof. Suppose g1  g2  g3 . Since lub(g1 )  g1 , lub(g1 )  g2  g3 . By Lemma 15, there
exist d2  g2 , d3  g3 , such that lub(g1 )  d2 + d3 . Since d2  lub(g2 ), d3  lub(g3 ),
lub(g1 )  lub(g2 ) + lub(g3 ). 
Lemma 19 Let g1 , g2 , g3 be non-negative non-empty intervals, g1  g2  g3 . If g1 is rightinfinite, then g2 or g3 is right-infinite.
Proof. Suppose g1 is right-infinite. Since g1  g2  g3 , g2  g3 is right-infinite. By Definition 7 and Lemma 16, g2 or g3 is right-infinite. 
5.2.2 Distance Constraints in D(+ ) and DS(+ )
In this section, we characterize the distance constraints which appear in the process of
enforcing path-consistency on D(+ ) in two main steps:
Step 1 We characterize the intervals involved in D(+ ), as well as the composition of those
intervals. This step is described by Definition 14 and Lemmas 20-24.
Step 2 We introduce the notion of DS(+ ) as a set containing all distance constraints
appearing in the process of enforcing path-consistency on D(+ ), and characterize
the distance constraints in DS(+ ). This step is described by Definitions 15-17 and
Lemmas 25-31.
Definition 14 (Primitive, Composite, Definable Intervals) Let h be a non-negative
interval. h is primitive, if h is one of [0, ], (, ), [0, 2], (2, ), (2, 4], (4, ),
[0, ). h is composite, if it can be obtained as the composition of at least two primitive
intervals. h is definable, if it is primitive or composite.
Lemma 20 For any non-negative interval h, h  {0} = h.
Proof. Follows from Definition 7. 
Since Lemma 20 holds, we call {0} an identity interval.
715

fiDu & Alechina

Lemma 21 If an interval occurs in D(+ ), then it is an identity interval or a primitive
interval.
Proof. Follows from Definition 14 and Lemma 11. 
Lemma 22 If h is a definable interval, then h 6= .
Proof. Follows from Definition 14 and Definition 7. 
Lemma 23 If an interval h is definable, then the following properties hold:
1. glb(h) = n, n  {0, 1, 2, 3, 4};
2. lub(h) = + or lub(h) = m, m  N>0 .
Proof. Let us prove by induction on the structure of h.
Base case: h is primitive. By Definition 14, n  {0, 1, 2, 4}, lub(h) = + or m  {1, 2, 4}.
Inductive step: Suppose Properties 1, 2 hold for any interval ht which can be obtained as
the composition of t primitive intervals, where t  N>0 (induction hypothesis). We will
show Properties 1, 2 hold for any interval ht+1 which can be obtained as the composition of
(t + 1) primitive intervals.
For any ht+1 , there exist an ht and a primitive interval hp such that ht+1 = ht  hp . By
induction hypothesis, glb(ht ) = nt , nt  {0, 1, 2, 3, 4}; lub(ht ) = + or lub(ht ) = mt ,
mt  N>0 . From the base case, glb(hp ) = np , np  {0, 1, 2, 4}; lub(hp ) = + or lub(hp ) =
mp , mp  {1, 2, 4}. By Lemma 17, lub(ht+1 ) = lub(ht ) + lub(hp ). Thus, Property 2 holds.
By Lemma 16, if
 lub(ht ) < glb(hp ), then glb(ht+1 ) = glb(hp )  lub(ht );
 lub(hp ) < glb(ht ), then glb(ht+1 ) = glb(ht )  lub(hp );
 otherwise, glb(ht+1 ) = 0.
Since mt > 0 and mp > 0, for glb(ht+1 ) = nt+1 , nt+1 < 4. In each case, nt+1  {0, 1, 2, 3}
(Property 1 holds). 
Lemma 24 If h is an identity or definable interval, then:
1. lub(h) = 0, iff h = {0};
2. lub(h) = , iff h = [0, ];
3. glb(h) = 4, iff h = (4, ).
Proof. Follows from Lemma 17, Lemma 23 and its proof. 
Now we start to characterize the distance constraints which appear in the process of
enforcing path-consistency on D(+ ).
716

fiQualitative Spatial Logics for Buffered Geometries

Definition 15 (DS(+ )) DS(+ ) is a minimal set of distance constraints such that the
following holds:
 Any distance constraint in D(+ ) is in DS(+ );
 If distance constraints d(p, q)  h and d(q, s)  g are in DS(+ ), then d(p, s)  h  g
is in DS(+ );
 If distance constraints d(p, q)  h and d(p, q)  g are in DS(+ ), then d(p, q)  h  g
is in DS(+ ),
where p, q, s are constants in D(+ ).
In the definition above, DS(+ ) is required to be minimal, such that any interval
involved in DS(+ ) is either in D(+ ) or is obtained by applying composition or intersection
operations on intervals in D(+ ). For generality, we do not restrict p, q, s to be different
constants. For example, it is possible p = q.
Lemma 25 If a distance constraint appears in the process of enforcing path-consistency on
D(+ ), then it is in DS(+ ).
Proof. Follows from Definition 8 (path-consistency) and Definition 15. 
DS(+ ) covers all the distance constraints appearing in the process of enforcing pathconsistency on D(+ ). However, not every distance constraint in DS(+ ) necessarily
appears in the process of enforcing path-consistency on D(+ ). For example, if D(+ )
contains exactly one distance constraint d(p, p)  [0, ], then by Definition 15, d(p, p) 
[0, 2] is in DS(+ ) (so is d(p, p)  [0, n], for any n  N>0 ), but by Definition 8, d(p, p) 
[0, 2] does not appear in the process of enforcing path-consistency. It is easy to see that
DS(+ ) is an infinite set.
The concept of DS(+ ) is similar to the concept of distributive subalgebra defined by
Li et al. (2015), as the composition operation distributes over non-empty intersections of
intervals involved in DS(+ ) (Rule 16 in Lemma 16). However, in our work, the composition
operation is defined for intervals rather than relations.
Lemma 26 If a distance constraint d(p, q)  h is in DS(+ ), then h is a non-negative
interval.
Proof. If a distance constraint d(p, q)  h is in D(+ ), by Lemma 11, h is a non-negative
interval. By Definitions 5, 7 and the definition of intersection, applying composition or
intersection on non-negative intervals, we obtain non-negative intervals. By Definition 15,
h is a non-negative interval. 
Differing from the previous version (Du & Alechina, 2014b), the following definitions
and lemmas are restricted to non-empty intervals.
Recall that a non-empty interval h is right-closed, iff h = [x, y] or h = (x, y]. h is
right-open, iff h = [x, y) or h = (x, y). h is right-infinite, iff h = [x, ) or h = (x, ). h is
left-closed, iff h = [x, y] or h = [x, y). h is left-open, iff h = (x, y] or h = (x, y).
717

fiDu & Alechina

Lemma 27 If a distance constraint d(p, q)  h is in DS(+ ) and h 6= , then h is either
right-infinite or right-closed.
Proof. Let n denote the total number of times of applying composition or intersection to
obtain h, n  0. We prove by induction on n.
Base case: n = 0, then d(p, q)  h is in D(+ ). By Lemma 11, h is either right-infinite
or right-closed. Inductive step: Suppose the statement holds for any non-empty h which
can be obtained by applying composition or intersection no more than n times (induction
hypothesis). We will show it also holds for any non-empty h which can be obtained by
applying composition or intersection (n + 1) times.
 If the last step to obtain h is intersection, then by Definition 15, there exist non-empty
h1 , h2 such that h = h1  h2 . By induction hypothesis, for each hi , i  {1, 2}, hi is
either right-infinite or right-closed. By intersection rules, h is either right-infinite or
right-closed.
 If the last step to obtain h is composition, then by Definition 15, there exist nonempty h1 , h2 such that h = h1  h2 . By induction hypothesis, for each hi , i  {1, 2}, hi
is either right-infinite or right-closed. By composition rules (Lemma 16), h is either
right-infinite or right-closed.


Lemma 28 For a distance constraint d(p, q)  h in DS(+ ) and h 6= , if glb(h) 6= 0, then
h is left-open.
Proof. Let n denote the total number of times of applying composition or intersection to
obtain h, n  0. We prove by induction on n.
Base case: n = 0, then d(p, q)  h is in D(+ ). By Lemma 11, if glb(h) 6= 0, then
h is left-open. Inductive step: Suppose the statement holds for any non-empty h which
can be obtained by applying composition or intersection no more than n times (induction
hypothesis). We will show it also holds for any non-empty h which can be obtained by
applying composition or intersection (n + 1) times.
 If the last step to obtain h is intersection, then by Definition 15, there exist nonempty h1 , h2 such that h = h1  h2 . By induction hypothesis, for each hi , i  {1, 2},
if glb(hi ) 6= 0, then hi is left-open. By intersection rules, if glb(h) 6= 0, then h is
left-open.
 If the last step to obtain h is composition, then by Definition 15, there exist non-empty
h1 , h2 such that h = h1  h2 . If glb(h) 6= 0, then by composition rules (Lemma 16),
h1 h2 = . Suppose lub(h1 )  glb(h2 ), then glb(h) = glb(h2 )lub(h1 ). By Lemma 26
and glb(h) 6= 0, we have glb(h) > 0, thus glb(h2 ) > lub(h1 ). By Lemma 26, lub(h1 ) 
0, thus glb(h2 ) > 0. By induction hypothesis, h2 is left-open. By composition rules
(Lemma 16), h is left-open. Similarly, this also holds if lub(h2 )  glb(h1 ).
718

fiQualitative Spatial Logics for Buffered Geometries


For a distance constraint d(p, q)  h in DS(+ ), by Definition 15, h is obtained by
applying the composition and/or intersection operations n  0 times on intervals occurring
in D(+ ). As applying the intersection operation does not generate any new bound, the
greatest lower/least upper bound (and its openness) of h must be the same as that of an
interval in D(+ ) or the composition of intervals in D(+ ). We formalize this rationale
as the concepts of Left-Definable and Right-Definable to characterize the distance constraints in DS(+ ). Later, we will show that every distance constraint d(p, q)  h (h 6= ) in
DS(+ ) is left-definable and right-definable. Left-Definable and Right-Definable are key
concepts for proving the Path-Consistency Lemma, as they establish the correspondences
between a distance constraint in DS(+ ) and a sequence of distance constraints in D(+ ).
If a non-empty interval h is left-open, then its greatest lower bound is represented as
glb (h). If h is left-closed, then its greatest lower bound is represented as glb+ (h). If h is
right-open, then its least upper bound is represented as lub (h). If h is right-closed, then
its least upper bound is represented as lub+ (h).
Definition 16 (Left-Definable) A distance constraint d(p1 , pn )  hs (n > 1) is leftdefinable, iff hs 6=  and there exists a sequence of distance constraints d(pi , pi+1 )  hi
(0 < i < n) in D(+ ), such that for m = h1  ...  hn1 , the following holds:
1. If hs is left-open, then m is left-open and glb  (m) = glb  (hs );
2. If hs is left-closed, then m is left-closed and glb + (m) = glb + (hs );
3. hs  m.
Definition 17 (Right-Definable) A distance constraint d(p1 , pn )  hs (n > 1) is rightdefinable, iff hs 6=  and there exists a sequence of distance constraints d(pi , pi+1 )  hi
(0 < i < n) in D(+ ), such that for m = h1  ...  hn1 , the following holds:
1. If hs is right-open, then m is right-open and lub  (m) = lub  (hs );
2. If hs is right-closed, then m is right-closed and lub + (m) = lub + (hs );
3. hs  m.
It is important to distinguish the definition of left-definable/right-definable distance
constraints (Definitions 16 and 17) from Definition 14 (Definable Intervals). For example, if
distance constraints d(p1 , p2 )  {0} and d(p2 , p3 )  {0} are in D(+ ), then d(p1 , p3 )  {0} is
left-definable and right-definable, but {0} is not a definable interval. If distance constraints
d(p1 , p2 )  [0, ] and d(p2 , p3 )  (4, ) are in D(+ ), then d(p1 , p3 )  (3, 5] is leftdefinable, but (3, 5] is not a definable interval.
Lemma 29 Let h, g be non-negative intervals. If distance constraints d(p, q)  h and
d(q, s)  g are left-definable and right-definable, then d(p, s)  h  g is left-definable and
right-definable.
719

fiDu & Alechina

Proof. Since d(p, q)  h and d(q, s)  g are right-definable, then by Definition 17, h 6= ,
g 6= . By Definition 7, h  g 6= . By Definition 17, in D(+ ), there exist a sequence of
distance constraints d(p, x2 )  h1 , ..., d(xn1 , q)  hn1 for d(p, q)  h and a sequence of
distance constraints d(q, y2 )  g1 , ..., d(yt1 , s)  gt1 for d(q, s)  g respectively satisfying
the three properties. Let us take the union of the two sequences as a new one, this is,
d(p, x2 )  h1 , ..., d(xn1 , q)  hn1 , d(q, y2 )  g1 , ..., d(yt1 , s)  gt1 . By composition
rules (Lemma 16), the new sequence satisfies the properties in Definition 17 for d(p, s)  hg.
Hence, d(p, s)  h  g is right-definable.
By composition rules (Lemma 16), if h  g 6= , then glb+ (h  g) = 0. We can use the
same new sequence above. Let m1 = (h1  ...  hn1 ), m2 = (g1  ...  gt1 ). By Definition 17,
h  m1 , g  m2 . Then m1  m2 6= , therefore, glb+ (m1  m2 ) = 0. By Definition 7,
h  g  m1  m2 . By Definition 16, d(p, s)  h  g is left-definable.
If h  g = , let us suppose glb(h)  lub(g). Since d(p, q)  h is left-definable and
d(q, s)  g is right-definable, by Definitions 16 and 17 respectively, in D(+ ), there exist
a sequence of distance constraints for d(p, q)  h and a sequence of distance constraints
for d(q, s)  g, satisfying the corresponding properties. Then by composition rules (Lemma
16), the union of the two sequences satisfies the properties in Definition 16 for d(p, s)  hg.
Hence, d(p, s)  h  g is left-definable. Similarly, we can show d(p, s)  h  g is left-definable,
if glb(g)  lub(h). 
Lemma 30 Let h, g be non-negative intervals. If distance constraints d(p, q)  h and
d(p, q)  g are left-definable and right-definable, h  g 6= , then d(p, q)  h  g is leftdefinable and right-definable.
Proof. As applying intersections does not generate any new bound and h  g 6= , the
left/right bound of h  g is the same as that of h or g. If the left bound of h  g is the
same as that of h, then by Definition 16, the same sequence used for showing d(p, q)  h is
left-definable can be used to show d(p, q)  h  g is left-definable. Other cases are similar. 
Lemma 31 If a distance constraint d(p, q)  h is in DS(+ ) and h 6= , then it is leftdefinable and right-definable.
Proof. Let n denote the total number of times of applying composition or intersection to
obtain h, n  0. We prove by induction on n.
Base case: n = 0, then d(p, q)  h is in D(+ ). By Definitions 16 and 17, d(p, q)  h is
left-definable and right-definable.
Inductive step: Suppose the statement holds for any non-empty h which can be obtained
by applying composition or intersection no more than n times (induction hypothesis). We
will show it also holds for any non-empty h which can be obtained by applying composition
or intersection (n + 1) times. By Definition 15, the last operation to obtain h is either
composition or intersection. In the former case, there exist d(p, s)  g1 and d(s, q)  g2 in
DS(+ ), such that g1  g2 = h. As h 6= , by Definition 7, gi 6= , i  {1, 2}. Since g1
and g2 are obtained by applying composition or intersection no more than n times, then by
induction hypothesis, d(p, s)  g1 and d(s, q)  g2 are left-definable and right-definable. By
720

fiQualitative Spatial Logics for Buffered Geometries

Lemma 29, d(p, q)  h is left-definable and right-definable. In the latter case, there exist
d(p, q)  g1 and d(p, q)  g2 in DS(+ ), such that g1  g2 = h. As h 6= , by intersection
rules, gi 6= , i  {1, 2}. By induction hypothesis, d(p, q)  g1 and d(p, q)  g2 are leftdefinable and right-definable. By Lemma 30, d(p, q)  h is left-definable and right-definable.

For generality, we do not exclude the possibility that d(p, q)   is in DS(+ ). However,
it follows from the proof of the Path-Consistency Lemma in Section 5.3 that d(p, q)   is
not in DS(+ ). Alternatively, for a direct proof, see Lemmas 45 and 46 in Appendix C.
5.2.3 Proving the Metric Space Lemma
In the following, we show there is a metric space satisfying D(+ ), if D(+ ) is pathconsistent (Metric Space Lemma). Firstly, we show that the process of enforcing patchconsistency on D(+ ) terminates. By Lemma 13, the number of constants in D(+ ) is
finite. Let us suppose the number of constants in D(+ ) is t  N>0 .
Lemma 32 Let t  N>0 be the number of constants in D(+ ). For any non-empty
right-closed interval h referred to in the process of enforcing path-consistency on D(+ ),
lub+ (h)  4t.
Proof. If a non-empty right-closed interval h occurs in D(+ ), then by Lemma 11,
lub+ (h)  4  4t.
Otherwise, it is generated from the application of composition and/or intersection operators by Definition 8. Composition creates larger least upper bounds (Lemma 17), whilst
intersection does not. Since h is right-closed, lub+ (h) is obtained by composing right-closed
intervals only (Lemma 16). Over t constants, the longest chain involves (t  1) intervals.
lub+ (h) is maximal if we use all of these (t  1) intervals and the least upper bound of each
interval is 4. Thus, lub+ (h)  4t. 
Lemma 33 Let t  N>0 be the number of constants in D(+ ). Enforcing path-consistency
on D(+ ), a fixed point can be reached in O(t3 ).
Proof. By Definition 8, Lemmas 23 and the fact that intersection does not generate new
bounds, for any interval s appearing in the process of enforcing path-consistency on D(+ ),
the following properties hold:
1. glb(s) = n, n  {0, 1, 2, 3, 4};
2. lub(s) = + or lub(s) = m, m  N>0 .
For any interval h appearing in D(+ ), by enforcing path-consistency (Definition 8), h can
only become an h0  h. By Lemma 11, h 6= . By Lemma 27, h is either right-closed or
right-infinite, h0 is , right-closed or right-infinite.
 If h is right-closed, then h0 =  or h0 is right-closed. If h0 is right-closed, then by
Lemma 11, lub(h0 )  lub(h)  4. By Properties 1, 2, there are finitely many
possibilities for h0 .
721

fiDu & Alechina

 If h is right-infinite, then h0 is , right-closed or right-infinite.
 If h0 is right-closed, then by Lemma 32, lub(h0 )  4t. By Properties 1, 2, there
are finitely many possibilities for h0 .
 If h0 is right-infinite, then by Property 1, there are finitely many possibilities for
its greatest lower bound, thus for h0 .
Since in each case, there are finitely many possibilities for h0 , a fixed point is always reached.
Suppose the widest non-negative interval [0, ) appears in the process of enforcing
path-consistency on D(+ ). In the worst case, firstly, [0, ) is strengthened to [0, u], where
u  4t (by Lemma 32), then [0, u] is strengthened by  each time. Hence, [0, ) can be
strengthened at most (4t + 1) times. Over t constants, by Definition 13, there are O(t2 )
distance constraints in D(+ ). For any interval h appearing in D(+ ), h  [0, ), hence h
can be strengthened at most (4t + 1) times. Therefore, the total time of strengthening all
the distance constraints is O(t3 ). 
The following lemma shows how to construct a metric space from D(+ ). It is used to
prove the Metric Space Lemma.
Lemma 34 Let t  N>0 be the number of constants in D(+ ), Df (+ ) be a fixed point of
enforcing path consistency on D(+ ). If D(+ ) is path-consistent, Ds (+ ) is obtained from
Df (+ ) by replacing every right-infinite interval with {5t}, every right-closed interval h
with {lub(h)}, then Ds (+ ) is path-consistent.
Proof. Suppose D(+ ) is path-consistent. By Lemma 25, Df (+ )  DS(+ ). By Definition 8, for any interval h appearing in Df (+ ), h 6= . By Lemma 27, h is either right-infinite
or right-closed. To prove Ds (+ ) is path-consistent, we only need to show that for any three
distance ranges, {npq }, {nqs }, {nps } in Ds (+ ) over three constants p, q, s, we have
1. npq  nqs + nps ;
2. nqs  npq + nps ;
3. nps  npq + nqs .
Let hpq , hqs , hps denote the corresponding distance ranges of {npq }, {nqs }, {nps } respectively
in Df (+ ), by Definition 8, we have
 hpq  hqs  hps ;
 hqs  hpq  hps ;
 hps  hpq  hqs .
We prove Ds (+ ) is path-consistent by cases:
 If every hi (i  {pq, qs, ps}) is right-closed, then ni = lub(hi ). By Lemma 18, 1-3
hold.
722

fiQualitative Spatial Logics for Buffered Geometries

 Otherwise, not all of them are right-closed. By Lemma 19, at least two of them are
right-infinite.
 If all of them are right-infinite, then ni = 5t. Since 5t  5t + 5t, 1-3 hold.
 Otherwise, only one of them is right-closed. Let hpq be right-closed. Then,
npq = lub(hpq ), nqs = 5t, nps = 5t. By Lemma 32 and   R0 , lub(hpq ) 
4t < 5t. By Lemma 26, lub(hpq )  0. Since lub(hpq ) < 5t + 5t and
5t  5t + lub(hpq ), 1-3 hold.

Lemma 7 (Metric Space Lemma) Let  be a finite consistent set of formulas, and +
be an M CS which contains  and is over the same set of individual names as . If D(+ )
is path-consistent, then there is a metric space (, d) such that all distance constraints in
D(+ ) are satisfied.
Proof. Suppose D(+ ) is path-consistent. Let  be the set of constants in D(+ ), which is
used to interpret individual names occurring in , as shown in Definition 13. If  = , then
it is trivial. Let us assume  6= . The number of constants in  is denoted by t  N>0 . By
Lemma 33, a fixed point Df (+ ) can be reached by enforcing path-consistency on D(+ ).
Let Ds (+ ) be a set of distance constraints obtained from Df (+ ) by replacing every
right-infinite interval with {5t}, every right-closed interval h with {lub(h)}. Since every
distance constraint in Ds (+ ) is of the form d(p, q)  {r}, where r  R0 , and d(p, q)  {r}
is equivalent to d(p, q) = r, a metric (distance function) is defined over . By Definition 13
and Lemma 34, for any pair of constants x, y, if x = y, then d(x, y) = 0 holds in Ds (+ );
if x 6= y, then d(x, y)   > 0 holds in Ds (+ ). Thus, we have d(x, y) = 0 iff x = y in
Ds (+ ). By Definitions 13 and 8, for any pair of constants x, y, d(x, y) = d(y, x) holds
in Ds (+ ). By Lemma 34, Ds (+ ) is path-consistent. Thus, for any constants x, y, z,
d(x, z)  d(x, y) + d(y, z) holds in Ds (+ ). By Definition 1, the (, d) of Ds (+ ) is a
metric space such that all distance constraints in D(+ ) are satisfied. 

5.3 Path-Consistency Lemma
This section proves the Path-Consistency Lemma by contradiction, supposing that D(+ )
is not path-consistent. We examine every case where the first  interval is obtained by
enforcing path-consistency. In each case, we show that  is derivable from the corresponding
LBPT formulas in + using LBPT axioms. This contradicts the assumption that + is
consistent. Lemma 35 is used to generate all possible cases and make sure no duplicated ones
are generated. By using Lemma 35, the proof of the Path-Consistency Lemma is largely
simplified, compared to the previous version (Du & Alechina, 2014b).
Lemma 35 Let g, h be non-negative intervals. g  h =  iff (g  h)  {0} = .
Proof. If g  h 6= , then by Definition 7, 0  (g  h).
If 0  (gh), then by Lemma 15, there exist d1  g, d2  h such that 0  [|d1 d2 |, d1 +d2 ].
Thus, d1 = d2 . Therefore, g  h 6= .
723

fiDu & Alechina

Since g  h 6=  iff 0  (g  h), by contraposition we get g  h =  iff (g  h)  {0} = . 
Knowing a least upper bound or a greatest lower bound of a definable interval h, Lemmas 36-42 show all possible ways in which h can be obtained as the composition of primitive
intervals. Lemma 36 and Lemma 39 are proved below. Proofs of the other lemmas are similar and omitted.
Lemma 36 If an interval h is definable, lub(h) = 2, then h is a primitive interval [0, 2]
or h is obtained as the composition of two [0, ].
Proof. If h is primitive, then by Definition 14, h = [0, 2].
If h is composite, then by Definition 14, there exist two definable intervals g1 , g2 such that
g1  g2 = h. By Lemma 17, lub(g1 ) + lub(g2 ) = 2. By Lemma 23, lub(g1 )  , lub(g2 )  ,
thus lub(g1 ) = , lub(g2 ) = . By Lemma 24, h = [0, ]  [0, ]. 

Lemma 37 If an interval h is definable, lub(h) = 3, then h is obtained as the composition
of [0, ] and [0, 2] or as the composition of three [0, ].
Lemma 38 If an interval h is definable, lub(h) = 4, then h is a primitive interval (2, 4],
or h is obtained as the composition of two [0, 2], as the composition of two [0, ] and one
[0, 2] or as the composition of four [0, ].
Lemma 39 If an interval h is definable, glb(h) = 3, then h is obtained as the composition
of [0, ] and (4, ).
Proof. By Definition 14, h cannot be primitive.
Since h is composite, then by Definition 14, there exist two definable intervals g1 , g2 such
that g1  g2 = h. g1  g2 = , otherwise, by Lemma 16, glb(h) = 0.
Without loss of generality, let us suppose lub(g1 )  glb(g2 ). By Lemma 16, glb(g2 ) 
lub(g1 ) = 3. By Lemma 23, glb(g2 )  4, lub(g1 )  , thus glb(g2 ) = 4, lub(g1 ) = . By
Lemma 24, h is obtained as the composition of [0, ] and (4, ). 
Lemma 40 If an interval h is definable, glb(h) = 2, then h is a primitive interval (2, )
or (2, 4], or h is obtained as the composition of [0, 2] and (4, ) or as the composition
of two [0, ] and one (4, ).
Lemma 41 If an interval h is definable, glb(h) = , then h is a primitive interval (, ),
or h is obtained as the composition of [0, ] and (2, ), as the composition of [0, ] and
(2, 4], as the composition of one [0, ], one [0, 2] and one (4, ), or as the composition
of three [0, ] and one (4, ).
Lemma 42 If an interval h is definable and left-open, glb(h) = 0, then h is obtained in
exactly the following ways:
 as the composition of [0, ] and (, );
724

fiQualitative Spatial Logics for Buffered Geometries

 as the composition of [0, 2] and (2, );
 as the composition of two [0, ] and one (2, );
 as the composition of [0, 2] and (2, 4];
 as the composition of two [0, ] and one (2, 4];
 as the composition of (2, 4] and (4, );
 as the composition of two [0, 2] and one (4, );
 as the composition of two [0, ], one [0, 2] and one (4, );
 as the composition of four [0, ] and one (4, ).
In our previous work (Du et al., 2013; Du & Alechina, 2014b), we presented a slightly
different way to prove the Path-Consistency Lemma for LNF and LBPT respectively: the
first empty interval is obtained using the strengthening operator, this is, g1  (g2  g3 ) = 
and gi 6= , where i  {1, 2, 3}. gi may be {0} or a primitive interval, or it can be written as
xi  (yi  zi ), where each of xi , yi , zi may not be an identity or primitive internal also. In the
latter case, since gi = xi  (yi  zi ) 6= , xi , yi , zi are all not empty. Since the composition
operation is distributive over non-empty intersections of intervals (Rule 16 in Lemma 16),
we use Rule 16 repeatedly to rewrite g1  (g2  g3 ) until every interval is an identity or
primitive interval. The final form is h1  ...  hn = , n > 1, where hx (0 < x  n) is
{0} or a definable interval. Thus there exist two intervals hi , hj (0 < i  n, 0 < j  n,
i 6= j) such that hi  hj = . Then we look at all the different combinations such that
lub(hi )  glb(hj ). There are exactly 15 such combinations. In this paper, the proof of the
Path-Consistency Lemma is largely simplified. It shows that it is sufficient to examine 5
rather than 15 combinations.
Lemma 8 (Path-Consistency Lemma) Let  be a finite consistent set of formulas, and
+ be an M CS which contains  and is over the same set of individual names as . Then,
D(+ ) is path-consistent.
Proof. Suppose D(+ ) is not path-consistent. Then by Definitions 8, 15 and Lemma 25,
d(p, q)   is in DS(+ ), for some constants p, q. By Lemma 11, for any distance range g
occurring in D(+ ), g 6= . By Definitions 15, 7, and intersection rules, the last operation
to obtain the first  interval is intersection. By Definition 15, there exist d(p, q)  g1 and
d(p, q)  g2 in DS(+ ), g1 6= , g2 6= , and g1  g2 = . By Lemma 26, g1 , g2 are
non-negative intervals. By Lemma 35, g1  g2 =  iff (g1  g2 )  {0} = .
By Definition 13 and Definition 15, d(q, p)  g2 is in DS(+ ). Since d(p, q)  g1 is in
DS(+ ), by Definition 15, d(p, p)  (g1  g2 ) is in DS(+ ). By Definition 7, g1  g2 6= .
By Lemma 31, d(p, p)  (g1  g2 ) is left-definable and right-definable. Let h = g1  g2 .
Since d(p, p)  h is left-definable, then by Definition 16, there exists a sequence of distance
constraints d(pi , pi+1 )  hi (0 < i < n) in D(+ ), such that p = p1 = pn and for h0 =
h1  ...  hn1 , h and h0 have the same greatest lower bound (including both value and
725

fiDu & Alechina

openness) and h  h0 . By Definition 14, Lemmas 21 and 20, h0 is an identity or definable
interval. By Lemma 23, glb(h0 )  {0, , 2, 3, 4}. Therefore, (g1  g2 )  {0} =  iff one of
the following holds:
 glb(h)  {, 2, 3, 4};
 h is left-open and glb (h) = 0.
We will check whether  can be derived in every case using axioms (or derivable facts).
By Axiom 3 and Axiom 4, N EAR and F AR are both symmetric.
1. glb(h) = : we look at all the different ways where h0 is obtained from a sequence of
distance constraints d(pi , pi+1 )  hi (0 < i < n) in D(+ ) such that p = p1 = pn and
h0 = h1  ...  hn1 (see Definition 16). As every hi is {0} or a primitive interval (by
Lemma 21), Lemma 41 specifies all the different ways to obtain h0 :
(a) h0 is a primitive interval (, +): by Definition 16, d(p1 , pn )  (, +) is in
D(+ ) and n = 2. As p = p1 = pn , d(p, p)  (, +) is in D(+ ). Suppose
p  points(a) for an individual name a in . By the proof of Lemma 11, (, +)
only can come from formulas of the form (x, y, (, )), where x, y are individual names. By Definition 10, (x, y, (, )) only can come from BP T (x, y).
Since d(p, p)  (, +) is in D(+ ) and p  points(a), BP T (a, a)  + . By
Axiom 9, BP T (a, a)  .
(b) h0 is obtained as the composition of [0, ] and (2, ) or as the composition of
[0, ] and (2, 4]:
by the proof of Lemma 11 and Definition 10, BP T (a, b)  + or BP T (b, a)  + ,
N EAR(a, b)  + and N EAR(b, a)  + .
By Fact 14, BP T (x1 , x2 )  N EAR(x1 , x2 )  , {x1 , x2 } = {a, b}.
(c) h0 is obtained as the composition of one [0, ], one [0, 2] and one (4, ):
by the proof of Lemma 11 and Definition 10, BP T (a, b)  + or BP T (b, a)  + ,
N EAR(b, c)  + , N EAR(c, b)  + , F AR(c, a)  + , F AR(a, c)  + . By
Fact 16, BP T (x2 , x1 )N EAR(x2 , x3 )F AR(x3 , x1 )  , {x1 , x2 , x3 } = {a, b, c}.
(d) h0 is obtained as the composition of three [0, ] and one (4, +):
by the proof of Lemma 11 and Definition 10, we have three BP T and one
F AR over four individual names a, b, c, d. BP T refers to either BP T (x, y) or
BP T (y, x). Some cases (for example, in + , we have BP T (a, b), BP T (c, b),
BP T (d, c) and F AR(a, d)) are not valid, because different constants will be
taken from the same points(b), for an individual name b (by Definition 13). As
a consequence, in an invalid case, a sequence consisting of distance constraints
d(pi , pi+1 )  hi (0 < i < n, p = p1 = pn ) cannot exist in D(+ ). We only need
to consider all valid cases, which are listed below.
i. BP T (x1 , x2 ), BP T (x2 , x3 ), BP T (x3 , x4 ), F AR(x4 , x1 ), where
{x1 , x2 , x3 , x4 } = {a, b, c, d}. By Fact 20, BP T (x1 , x2 )  BP T (x2 , x3 ) 
BP T (x3 , x4 )  F AR(x4 , x1 )  .
ii. BP T (x2 , x1 ), BP T (x2 , x3 ), BP T (x3 , x4 ), F AR(x4 , x1 ), where
{x1 , x2 , x3 , x4 } = {a, b, c, d}. By Fact 21, BP T (x2 , x1 )  BP T (x2 , x3 ) 
BP T (x3 , x4 )  F AR(x4 , x1 )  .
726

fiQualitative Spatial Logics for Buffered Geometries

Cases 2-5 below use similar arguments. In the following proof, BP T refers to either
BP T (x, y) or BP T (y, x) (whichever makes the corresponding case valid). N EAR
and F AR are symmetric, thus the order of x, y does not matter.
2. glb(h) = 2: by Definition 16 and Lemma 21, Lemma 40 specifies all the different
ways to obtain h0 from a sequence of distance constraints d(pi , pi+1 )  hi (0 < i < n)
in D(+ ):
(a) h0 is a primitive interval (2, ) or (2, 4]:
N EAR(a, a), using Axiom 9 and Fact 14.
(b) h0 is obtained as the composition of [0, 2] and (4, +) :
one N EAR and one F AR, using Fact 15.
(c) h0 is obtained as the composition of two [0, ] and one (4, +):
two BP T and one F AR, using Facts 18 and 19.
3. glb(h) = 3: by Definition 16 and Lemma 21, Lemma 39 specifies all the ways to
obtain h0 from a sequence of distance constraints d(pi , pi+1 )  hi (0 < i < n) in
D(+ ). By Lemma 39, h0 is obtained as the composition of [0, ] and (4, +).
one BP T and one F AR, using Fact 17.
4. glb(h) = 4: by Definition 16 and Lemma 21, Lemma 24 specifies all the ways to
obtain h0 from a sequence of distance constraints d(pi , pi+1 )  hi (0 < i < n) in
D(+ ). By Lemma 24, h0 = (4, +). F AR(a, a), using Axiom 9 and Fact 17.
5. glb (h) = 0: by Definition 16 and Lemma 21, Lemma 42 specifies all the ways to
obtain h0 from a sequence of distance constraints d(pi , pi+1 )  hi (0 < i < n) in
D(+ ):
(a) h0 is obtained as the composition of [0, ] and (, ): by Definition 13, ensuring
no different constants taken from the same points(x),
BP T (x1 , x2 )  + and BP T (x1 , x2 )  + , {x1 , x2 } = {a, b}.
BP T (x1 , x2 )  BP T (x1 , x2 )  .
(b) h0 is obtained as the composition of [0, 2] and (2, ) or as the composition of
[0, 2] and (2, 4]:
one N EAR and one N EAR, using Axiom 3.
(c) h0 is obtained as the composition of two [0, ] and one (2, ) or as the composition of two [0, ] and one (2, 4]:
two BP T and one N EAR, using Axioms 10 and 11.
(d) h0 is obtained as the composition of (2, 4] and (4, ):
one F AR and one F AR, using Axiom 4.
(e) h0 is obtained as the composition of two [0, 2] and one (4, ):
two N EAR and one F AR.
This case is invalid. By Definition 16, D(+ ) contains d(pa , pb )  [0, 2], d(pb , pc ) 
[0, 2] and d(pa , pc )  (4, ), where pa  points(a), pb  points(b), pc 
points(c), for individual names a, b, c. By Definitions 10 and 13, d(pa , pb )  [0, 2]
and d(pb , pc )  [0, 2] cannot come from N EAR(a, b) and N EAR(b, c) in + (by
727

fiDu & Alechina

the proof of Lemma 11, it is clear that they cannot come from other formulas as
well), because two different constants will be taken from points(b) as witnesses
for (a, b, [0, 2]) and (b, c, [0, 2]) respectively.
(f) h0 is obtained as the composition of two [0, ], one [0, 2] and one (4, ):
two BP T , one N EAR and one F AR, using Axioms 12 and 13.
(g) h0 is obtained as the composition of four [0, ] and one (4, ):
four BP T and one F AR, using Facts 22-24.
In each valid case,  is derivable using the corresponding axioms or facts, which contradicts
the assumption that + is consistent. Therefore, D(+ ) is path-consistent. 
There is an alternative way to prove the Path-Consistency Lemma, which we believe is
longer and more complicated than the one presented in this paper, but since it may provide
additional intuitions to the reader, we sketch it in Appendix B.

6. Decidability and Complexity of LBPT
In this section, we establish the complexity of the LBPT satisfiability problem. The complexity of the LNF/LNFS satisfiability problem can be established in a similar way. The
complexity of these satisfiability problems is important, as it is related to the complexity
of the problem of finding inconsistencies, which is the basis of our approach to debugging
matches between geospatial datasets.
Definition 18 (Size of a Formula) The size of a LBPT formula s() is defined as follows:
 s(BP T (a, b)) = 3, s(N EAR(a, b)) = 3, s(F AR(a, b)) = 3;
 s() = 1 + s();
 s(  ) = 1 + s() + s(),
where a, b are individual names, ,  are formulas in L(LBP T ).
As a set of LBPT formulas S and the conjunction of all formulas in S are equi-satisfiable,
the combined size of LBPT formulas in a set S is defined as the size of the conjunction of
all formulas in S.
Next we prove Theorem 2 for LBPT: the satisfiability problem for a finite set of LBPT
formulas in a metric space is NP-complete.
Proof. NP-hardness of the LBPT satisfiability problem follows from NP-hardness of the
satisfiability problem for propositional logic, which is included in LBPT.
To prove that the LBPT satisfiability problem is in NP, we show that if a finite set of
LBPT formulas  is satisfiable, then we can guess a metric model for  and verify that this
model satisfies , both in time polynomial in the combined size of formulas in .
Suppose  is a finite set of LBPT formulas, and the number of individual names in  is
n. The completeness proof shows that, if  is satisfiable, it is satisfiable in a metric model
728

fiQualitative Spatial Logics for Buffered Geometries

M of size which is polynomially bounded by the number of individual names in . To recap
the construction of the metric model for , first we construct B(+ ), the corresponding set
of basic quantified formulas from an MCS + containing , and then construct a model for
B(+ ). By Definition 10, the number of formulas in B(+ ) is at most f = (n + 2n(n  1)).
By Definitions 11 and 13, to every individual name a in , we assign a fixed set of new
constants, points(a) = {p1a , . . . , pxa }, where x = num(a, B(+ )). Since x  f , the number of
constants in M is at most t = nf . By Lemma 34 and proofs of the Metric Space Lemma, in
such a model M , every value assigned by the distance function is of the form m, m  N0 ,
m  5t.
We guess a metric model M like this. Let s be the combined size of formulas in . Then
n < s. To every individual name a in , we assign {p1a , . . . , pxa }, where x < 2s2 . This results
in a set of constants , the size of which is < 2s3 . To every pair of constants p, q in , we
assign m to d(p, q), where m  N0 , m < 10s3 . To verify that (, d) is a metric space, by
Definition 1, it is in O(s9 ).
To verify that M satisfies , we need to verify that it satisfies the conjunction of all formulas in . For any R(a, b), where R  {BP T, N EAR, F AR}, a, b are individual names, to
verify that R(a, b) is satisfied, it takes time which is polynomial in |points(a)|  |points(b)|,
thus it is in O(s4 ). Hence, verifying that M satisfies  can be done in O(s5 ). 
In Section 3, we mentioned that  acts as a scaling factor of a metric model. This is
stated as Lemma 43 and follows from the proofs of the completeness theorem and Theorem 2.
The proof for LBPT is provided below. The proof for LNF/LNFS is similar.
Lemma 43 A finite set of LNF/LNFS/LBPT formulas is satisfiable in a metric model
where   R0 , iff it is satisfiable in a metric model when  = 1.
Proof.[for LBPT] Suppose  is a finite set of LBPT formulas, the number of individual
names in  is n and the combined size of formulas in  is s. By Definition 18, n < s.
The completeness proof shows that, if  is satisfiable, it is satisfiable in a metric model
M = (, d, I, ) constructed as shown in Section 5.1 and Section 5.2. By Definition 13,
to every individual name a in , we assign {p1a , . . . , pxa }, where x = num(a, B(+ )) < 2s2 .
This results in a set of constants , the size of which is < 2s3 . To every constant p in ,
we assign d(p, p) = 0. By Definition 13 and Lemma 34, to every pair of different constants
p, q in , we assign m to d(p, q), where m  N>0 , m < 10s3 . As M is a metric model, by
Definition 1 and the proof of the Metric Space Lemma, for any x, y, z  , we have
1. d(x, y) = 0, iff x = y;
2. d(x, y) = mxy , iff d(y, x) = mxy ;
3. if d(x, z) = mxz , d(x, y) = mxy , d(y, z) = myz , then mxz   mxy  + myz .
If M satisfies , by Definition 3, the following holds:
M |= BP T (a, b) iff pa  I (a) pb  I (b) : d (pa , pb )  [0 , ];
M |= N EAR(a, b) iff pa  I (a) pb  I (b) : d (pa , pb )  [0 , 2 ];
729

fiDu & Alechina

M |= F AR(a, b) iff pa  I (a) pb  I (b) : d (pa , pb )  (4 , );
M |=  iff M 6|= ;
M |=    iff M |=  and M |= ,
where a, b are individual names, ,  are formulas in L(LBP T ).
By setting  = 1, (, d) is still a metric space, as the following holds for any x, y, z  :
1. d(x, y) = 0 iff x = y;
2. d(x, y) = mxy , iff d(y, x) = mxy ;
3. if d(x, z) = mxz , d(x, y) = mxy , d(y, z) = myz , then mxz  mxy + myz .
By setting  = 1, the definitions of BP T , N EAR and F AR change accordingly as well.
One can easily see that M still satisfies  after replacing every  by 1.
Similar, if we have a metric model with  = 1, we can obtain a metric model with
  R0 by multiplying every distance value m by . One can easily see that M still satisfies  after multiplying all distance values in M by , and multiplying all greatest lower
bounds and least upper bounds of intervals in the truth definitions of BP T , N EAR and
F AR by . 

7. Validating Matches using Spatial Logic
The spatial logics LN F , LN F S and LBP T can be used to verify consistency of sameAs
and partOf matches between spatial objects from different geospatial datasets. If every
spatial object has a point geometry, then we apply LN F , otherwise, we apply LN F S or
LBP T . LBP T reasoning has been used together with description logic reasoning in a
geospatial data matching system MatchMaps (Du, Nguyen, Alechina, Logan, Jackson, &
Goodwin, 2015; Du, 2015). LBP T reasoning and description logic reasoning complement
each other in the sense that LBP T reasoning verifies matches regarding spatial information
whilst description logic reasoning verifies matches regarding classification information, the
unique name assumption and a stronger version of it. In the following, we describe how
LBPT is used for debugging matches.
A dedicated LBPT reasoner integrated with an assumption-based truth maintenance
system (ATMS) (de Kleer, 1986) was developed as a part of MatchMaps. It implements
LBPT axioms and the definition BEQ(a, b)  BP T (a, b)  BP T (b, a) as a set of inference
rules. For efficiency reasons, there is no one-to-one correspondence between rules and axioms. To speed up matching and avoid cycles, facts such as N EAR(a, b) are stored for only
one order of a and b, and symmetry axioms are removed. Each of the remaining axioms
involving any symmetric relation gives rise to several rules, to compensate for the removal
of symmetry. For example, the axiom
BPT (b, a)  NEAR(b, c)  BPT (c, d )  FAR(d , a)
also gives rise to a rule corresponding to the following implication:
BPT (b, a)  NEAR(c, b)  BPT (c, d )  FAR(d , a)
730

fiQualitative Spatial Logics for Buffered Geometries

(with N EAR(c, b) instead of N EAR(b, c)). However the set of rules is trivially equivalent
to the set of axioms.
Possible matches of the form sameAs(a, b) and partOf (a, b) (a is partOf b) are generated as assumptions, to be withdrawn if they are involved in a derivation of a contradiction
in description logic or in LBPT. In order to apply the LBPT reasoning, sameAs(a, b) is
replaced by BEQ(a, b), and partOf (a, b) is replaced by BP T (a, b). These substitutions do
not affect correctness of the matching results of MatchMaps, as MatchMaps adopts the definitions of sameAs and partOf such that sameAs(a, b) entails BEQ(a, b) and partOf (a, b)
entails BP T (a, b). N EAR(a, b) and F AR(a, b) facts are generated for those objects a, b
in the same dataset that are involved in some matches across the two datasets (there is
an object c in the other dataset such that BEQ(a, c), BP T (a, c) or BP T (c, a) holds, and
similarly for b).
The LBPT reasoner derives new formulas by applying inference rules to previously
derived formulas, and the ATMS maintains dependencies between derived consequences and
a set of assumptions (corresponding to possible matches). In particular, it maintains all
minimal sets of assumptions responsible for the derivation of  (false), referred to as nogoods
in ATMS terminology. Such minimal sets of assumptions responsible for a contradiction
are used to decide which matches are wrong and should be withdrawn.
In experiments, the LBPT reasoner with an ATMS was used to validate matches between
spatial objects from OSM data (building layer) and OSGB MasterMap data (Address Layer
and Topology Layer) (Du et al., 2015). The study areas are in city centres of Nottingham UK
and Southampton UK. The Nottingham data was obtained in 2012, and the Southampton
data in 2013. The numbers of spatial objects in the case studies are shown in Table 1.

Nottingham
Southampton

OSM spatial objects
281
2130

OSGB spatial objects
13204
7678

Table 1: Data used for Evaluation

The initial matches are generated by the matching method implemented in MatchMaps.
The detailed matching method is provided by Du et al. (2016). The method consists
of two main steps: matching geometries and matching spatial objects. A spatial object
in a geospatial dataset has an ID, location information (coordinates and geometry) and
meaningful labels, such as names or types, and represents an object in the real world. A
geometry here refers to a point, a line or a polygon, which is used to represent location
information in geospatial datasets.
The geometry matching requires a level of tolerance, as some difference in the geometry
representation of a spatial object is to be expected in different datasets. After discussing
with domain experts in geospatial science, we decided to apply the same level of tolerance
for the matching method and the spatial logic used in MatchMaps. In the experiments
described by Du et al. (2015), the level of tolerance for the geometry matching was set to
be 20 meters, based on the published estimate about the positional accuracy of OSM data.
The OSM positional accuracy was estimated to be about 20 meters in UK (Haklay, 2010).
In our more recent work (Du et al., 2016), we analysed how the level of tolerance affects
731

fiDu & Alechina

the precision and recall of matching results for the same geographic area in Nottingham
(the same data as shown in the first row of Table 1) using 12 different levels of tolerance
within a range of 1 to 80 meters. It shows that, for the Nottingham case, 20 meters is a
good estimate, though it is not the optimal value.
Following the first step of the matching method, we first aggregate adjacent single geometries, such as those of shops within a shopping center, then establish correspondences
between aggregated geometries using geometry matching. In the second step, we match
spatial objects located on these corresponding aggregated geometries by comparing the
similarity of names and types of spatial objects in several different cases (one-to-one, manyto-one and many-to-many). The most difficult case is when there is a match between
two aggregated geometries which contain objects {a1 , . . . , an } in one dataset and objects
{b1 , . . . , bk } in the other dataset (many-to-many matching case). When we cannot decide
the exact matches automatically using names and types of objects, we generate all matches
which are possibly correct between the objects in the two sets: for each pair ai , bj with similar labels, we generate sameAs(ai , bj ), partOf (ai , bj ), partOf (bj , ai ). We apply reasoning in
LBPT and description logic to verify consistency of these matches. The use of description
logic reasoning is described by Du (2015).

Figure 5: Examples of using LNFS and LBPT for validating matches
If a minimal set of statements involved in a contradiction contains more than one
retractable assumption, a domain expert is needed to decide the correctness of the retractable assumptions and remove the wrong one(s) to restore consistency. Location information is visualized and provided to help domain experts make such decisions. As
shown in Figure 5, a1 , b1 , c1 , d1 (dotted) are from OSGB data and a2 , b2 , c2 , d2 (solid) are
from OSM data. In the left example, by LNFS Axiom 6 (or by LBPT Axiom 12 and
BEQ(a, b)  BP T (a, b)  BP T (b, a)), a minimal set of statements for deriving an inconsistency consists of BEQ(a1 , a2 ), BEQ(b1 , b2 ), N EAR(a1 , b1 ), F AR(a2 , b2 ). It is clear that
BEQ(b1 , b2 ) is wrong, as N EAR(a1 , b1 ) and F AR(a2 , b2 ) are facts. In the right example,
BP T (d2 , d1 ) is wrong, because it contradicts BP T (c2 , c1 ), N EAR(c2 , d2 ), F AR(c1 , d1 ) by
LBPT Axiom 12. As a consequence, the sameAs and partOf matches corresponding to
BEQ(b1 , b2 ) and BP T (d2 , d1 ) respectively are also incorrect.
Table 2 shows the numbers of nogoods generated by the LBPT reasoner with an ATMS.
As mentioned earlier, nogoods are justifications for false: the minimal sets of statements
from which a contradiction is derivable. The number of interactions is the number of
times users are asked to take actions or use strategies to resolve problems (a strategy is
a heuristic which allows users to retract all similar statements at a time, for example,
732

fiQualitative Spatial Logics for Buffered Geometries

Nottingham
Southampton

nogoods
172
268

retracted BEQ/BPT
31
114

retracted sameAs/partOf
1325
488

interactions
3
7

Table 2: LBPT Verification of Matches

retracting partOf (o, x) for any x differing from an object o). As a result of LBPT reasoning
and removal of BEQ and BPT assumptions, we withdraw 1325 sameAs/partOf assumptions
for the Nottingham case and 488 sameAs/partOf assumptions for the Southampton case.
With the LBPT validation of matches, MatchMaps achieved high precision ( 90%) and
recall ( 84%) for both Nottingham and Southampton cases.
As described in our previous work (Du, Alechina, Hart, & Jackson, 2015), MatchMaps
was used by 12 experts from the University of Nottingham and Ordnance Survey of Great
Britain to match about 100 buildings and places in Southampton. A graphical user interface of MatchMaps is provided allowing users to take different types of actions to remove
wrong matches. The number of actions and the decision time of users are recorded. The
precision and recall of the matching results are compared to those obtained without using any user-involved verification. Experimental results showed that by using reasoning in
LBPT and description logic, the precision and recall of matches generated by MatchMaps
were improved on average by 9% and 8% respectively. The human effort is also reduced, in
the sense that the decision time required is much less than that of a fully manual matching
process.

8. Discussion
The spatial logics LNF, LNFS and LBPT are generally applicable to reason with spatial objects whose locations are represented at different levels of accuracy or granularity in different
datasets. Locations of spatial objects can be represented using vector data (coordinates)
or raster data (images). Sometimes, for spatial objects in different datasets, measuring
whether their locations are buffered equal directly is difficult or impossible, for example,
when locations are represented as images without knowing their coordinates. In such cases,
spatial objects may be matched by comparing shapes in images or using lexical information.
No matter how the matches between spatial objects are generated, the LNF/LNFS/LBPT
reasoning could be used to verify consistency of matches with regard to relative locations
(N EAR/F AR facts) between spatial objects in the same dataset, which are often reliable
and easy to capture.
Another potential application of these logics is in matching non-georeferenced volunteered spatial information or sketch data (Egenhofer, 1997; Kopczynski, 2006; Wallgrun,
Wolter, & Richter, 2010). Instead of being created by surveying or other mapping techniques, sketch data is often created by a person from memory or by schematizing authoritative geospatial data. A sketch map cannot provide precise metric information such as the
exact distance or size of a spatial object, but it roughly shows several kinds of qualitative
relations (e.g. nearness and directions) between spatial objects. In the work by Wallgrun
et al. (2010), qualitative spatial reasoning (based on the dipole relation algebra presented in
Moratz, Renz, & Wolter, 2000 for checking connectivity and the cardinal direction calculus
733

fiDu & Alechina

presented in Ligozat, 1998) is used for the task of matching a sketch map of a road network to a larger geo-referenced data set, for example, from OpenStreetMap. Endpoints and
junctions of roads are extracted and their relative directions are represented and checked
by spatial reasoning. The spatial logic LNF can be applied similarly to check the relative
distances between endpoints or junctions of roads. N EAR/F AR relations between these
points indicate the length of roads. For the task of matching a sketch map of polygonal
objects (e.g. buildings and places), the logic LNFS/LBPT can be applied. Suppose users
draw a sketch map of buildings and estimate the distances between some buildings as being
N EAR or F AR regarding an agreed level of tolerance . The N EAR and F AR relations
between buildings in a geo-referenced map can be calculated automatically. A mapping
between the sketch map and a geo-referenced map can be checked by reasoning in the logic
LNFS/LBPT. For example, two buildings which are specified as being F AR in the sketch
map cannot be matched to two buildings which are N EAR in the geo-referenced map.
The main limitation of the new spatial logics is that they require a level of tolerance 
and when using the logics, the value of  is the same for spatial objects of different sizes
and types (such as buildings, roads, rivers and lakes). For example, the margin of error
used for cities should be larger than that for buildings. Ideally, the value of  should vary
by the size and type of the spatial object being checked. This motivates the development
of new spatial logics to reason about the sizes and types of spatial objects, in addition to
their relative locations.
In this paper, the theorems were all proved with respect to a metric space. However,
models based on a metric space may not be realisable in a 2D Euclidean space, which is
more realistic for geospatial data. Suppose there are four points pi , where i  {1, 2, 3, 4}.
For each point pi , d(pi , pi ) = 0. For any pair of them, d(pi , pj ) = d(pj , pi ) = 1. It is clear
that there is a metric space satisfying all the distance constraints, but there is no such
2D Euclidean space. Wolter and Zakharyaschev (2003, 2005) proved that the satisfiability
problem for a finite set of M S(Q0 ) formulas in a 2D Euclidean space R2 is undecidable,
whilst its proper fragments may be decidable. We proved that the satisfiability problem
for a finite set of LNF formulas in a 2D Euclidean space is decidable in PSPACE (Du,
2015), but whether the satisfiability problem for a finite set of LNFS/LBPT formulas in a
2D Euclidean space is decidable is still unknown. It also remains open that whether the
LNF/LNFS/LBPT calculus is complete for models based on a 2D Euclidean space. If not,
a theoretical challenge is to design logics which are complete for 2D Euclidean spaces, and
hence provide more accurate debugging of matches than the logics of metric spaces.
Finally, the use of description logic and the new spatial logics may not be able to detect
all the wrong matches. For example, for spatial objects X, Y in one dataset and X 0 , Y 0 in
the other dataset, if sameAs(X, X 0 ) is correct, Y is N EAR and to the south of X, and Y 0
is N EAR and to the north of X 0 , then sameAs(Y, Y 0 ) is wrong but cannot be detected.
To deal with this, we could extend the logics with existing spatial formalisms for reasoning
about directional relations (Frank, 1991, 1996; Ligozat, 1998; Balbiani et al., 1999; Goyal
& Egenhofer, 2001; Skiadopoulos & Koubarakis, 2004).
734

fiQualitative Spatial Logics for Buffered Geometries

9. Conclusion and Future Work
We presented a series of new qualitative spatial logics LNF, LNFS and LBPT for validating
matches between spatial objects, especially in crowd-sourced geospatial data. For models
based on a metric space, a sound and complete axiomatisation is provided and corresponding
theorems are proved for each logic. The LNF, LNFS and LBPT satisfiability problems in
a metric space are all NP-complete. An LBPT reasoner with an ATMS was implemented
and used as a part of MatchMaps. Experimental results show that the LBPT reasoner
can be used to verify consistency of matches with respect to location information and
detect obvious logical errors effectively. As future work, we will investigate whether the
LNF/LNFS/LBPT calculus is complete for models based on a 2D Euclidean space and
develop new spatial logics (e.g. for reasoning about directional relations and object sizes in
addition to distances) to provide more accurate debugging of matches.

Acknowledgments
We would like to thank the anonymous reviewers who provided excellent comments that
helped us improve the paper.

Appendix A. Proofs
Lemma 9 If + is an M CS, then for any pair of individual names a, b occurring in ,
exactly one of the following cases holds in + :
1. case(a, b) = BP T (a, b)  BP T (b, a);
2. case(a, b) = BP T (a, b)  BP T (b, a);
3. case(a, b) = BP T (a, b)  BP T (b, a);
4. case(a, b) = BP T (a, b)  BP T (b, a)  N EAR(a, b);
5. case(a, b) = N EAR(a, b)  F AR(a, b);
6. case(a, b) = F AR(a, b),
where case(a, b) denotes the formula which holds between a, b in each case.
Proof. For any pair of individual names a, b occurring in + , we have:
` (B  B 1  N  F )  (B  B 1  N  F )  (B  B 1  N  F )  (B  B 1  N  F )
(B  B 1  N  F )  (B  B 1  N  F )  (B  B 1  N  F )  (B  B 1  N  F )
(B  B 1  N  F )  (B  B 1  N  F )  (B  B 1  N  F )  (B  B 1  N  F )
(BB 1 N F )(BB 1 N F )(BB 1 N F )(BB 1 N F )
where B, B 1 , N, F stand for BP T (a, b), BP T (b, a), N EAR(a, b), F AR(a, b) respectively.
From Table 3, we have
` (B  B 1 )  (B  B 1 )  (B  B 1 )  (B  B 1  N )  (N  F )  F .
735

fiDu & Alechina

B
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0

B 1
1
1
1
1
0
0
0
0
1
1
1
1
0
0
0
0

N
1
1
0
0
1
1
0
0
1
1
0
0
1
1
0
0

F
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0

Prime Implicant

B  B 1



B  B 1



B  B 1



B  B 1  N
F
N  F

Axiom/Fact used
Fact 15
Facts 14, 15, Axiom
Fact 14, Axiom 3
Fact 14, Axiom 3
Fact 15
Facts 14, 15, Axiom
Fact 14, Axiom 3
Fact 14, Axiom 3
Fact 15
Facts 14, 15, Axiom
Fact 14, Axiom 3
Fact 14, Axiom 3
Fact 15
Fact 15
Facts 14, 15, Axiom
Fact 14, Axiom 3

3

3

3

3

Table 3: truth table, 1 stands for true, 0 stands for false


Lemma 11 For any distance range g occurring in D(+ ),
g  {{0}, [0, ], (, ), [0, 2], (2, ), (2, 4], (4, ), [0, )}.
Proof. Suppose p, q are constants and d(p, q)  g is in D(+ ). Let us look at g in all
different cases:
 p = q: by Definition 13, g = {0}.
 p 6= q:
 p  points(a), q  points(a), for some individual name a:
by Definition 13, g = [0, ).
 p  points(a), q  points(b), for different individual names a, b:
by Lemma 9 and Definition 10, exactly one of following cases holds:
C1
C2
C3
C4
C5
C6

{(a, b, [0, ]), (b, a, [0, ])}  B(+ )
{(a, b, [0, ]), (b, a, (, ))}  B(+ )
{(a, b, (, )), (b, a, [0, ])}  B(+ )
{(a, b, (, )), (b, a, (, )), (a, b, [0, 2]), (b, a, [0, 2])}  B(+ )
{(a, b, (2, )), (b, a, (2, )), (a, b, [0, 4]), (b, a, [0, 4])}  B(+ )
{(a, b, (4, )), (b, a, (4, ))}  B(+ )

In C1:
 if exactly one of p, q is a witness of (a, b, [0, ]) or (b, a, [0, ]), then by
Definition 13, in the construction process, d(p, q)  [0, ] will be added to
736

fiQualitative Spatial Logics for Buffered Geometries

D(+ ), and then d(p, q)  [0, +) will be added to D(+ ). Since [0, ] 
[0, +) = [0, ], g = [0, ].
 else if p is a witness of (b, a, [0, ]) and q is a witness of (a, b, [0, ]), then
by Definition 13, in the construction process, d(p, q)  [0, ] will be added
to D(+ ), and then d(p, q)  [0, ] will be added to D(+ ) again, and then
d(p, q)  [0, +) will be added to D(+ ). Since [0, ]  [0, ]  [0, +) =
[0, ], g = [0, ].
 else, by Definition 13, g = [0, +).
In C2:
 if q is a witness of (a, b, [0, ]), then by Definition 13, in the construction
process, d(p, q)  [0, ] will be added to D(+ ), and then d(p, q)  [0, )
will be added to D(+ ). Since [0, ]  [0, ) = [0, ], g = [0, ].
 else if q is a witness of (b, a, (, )), then by Definition 13, in the construction process, d(p, q)  (, ) will be added to D(+ ), and then d(p, q) 
[0, ) will be added to D(+ ). Since (, )  [0, ) = (, ), g = (, ).
 else, by Definition 13, g = [0, +).
In C3:
 if p is a witness of (a, b, (, )), then by Definition 13, in the construction
process, d(p, q)  (, ) will be added to D(+ ), and then d(p, q)  [0, )
will be added to D(+ ). Since (, )  [0, ) = (, ), g = (, ).
 else if p is a witness of (b, a, [0, ]), then by Definition 13, in the construction
process, d(p, q)  [0, ] will be added to D(+ ), and then d(p, q)  [0, )
will be added to D(+ ). Since [0, ]  [0, ) = [0, ], g = [0, ].
 else, by Definition 13, g = [0, +).
In C4:
 if the pair p, q is a witness of (a, b, [0, 2]), then by Definition 13, in the
construction process, d(p, q)  [0, 2] will be added to D(+ ), and then
d(p, q)  [0, ) will be added to D(+ ). Since [0, 2]  [0, ) = [0, 2],
g = [0, 2].
 else if exactly one of p, q is a witness of (a, b, (, )) or (b, a, (, )),
then by Definition 13, in the construction process, d(p, q)  (, ) will be
added to D(+ ), and then d(p, q)  [0, ) will be added to D(+ ). Since
(, )  [0, ) = (, ), g = (, ).
 else if p is a witness of (a, b, (, )) and q is a witness of (b, a, (, )), then
by Definition 13, in the construction process, d(p, q)  (, ) will be added
to D(+ ), and then d(p, q)  (, ) will be added to D(+ ) again, and then
d(p, q)  [0, ) will be added to D(+ ). Since (, )  (, )  [0, ) =
(, ), g = (, ).
 else, by Definition 13, g = [0, +).
In C5:
 if the pair p, q is a witness of (a, b, [0, 4]), then by Definition 13, in the
construction process, d(p, q)  [0, 4] will be added to D(+ ), and then,
737

fiDu & Alechina

d(p, q)  (2, ) will be added to satisfy the  formulas, and then d(p, q) 
[0, ) will be added to D(+ ). Since [0, 4]  (2, )  [0, ) = (2, 4],
g = (2, 4].
 else, by Definition 13, d(p, q)  (2, ) will be added to satisfy the  formulas, then d(p, q)  [0, ) will be added to D(+ ). Since (2, )  [0, ) =
(2, ), g = (2, ).
In C6, by Definition 13, d(p, q)  (4, ) will be added, then d(p, q)  [0, )
will be added to D(+ ). Since (4, )  [0, ) = (4, ), g = (4, ).
Therefore, g  {{0}, [0, ], (, ), [0, 2], (2, ), (2, 4], (4, ), [0, )}. 
Lemma 16 (Calculation of Composition) If (m, n), (s, t), (m, ), (s, ), {l},
{r} are non-negative non-empty intervals, H1 , H2 , H are non-negative intervals, then the
following calculation rules hold:
1. {l}  {r} = [l  r, l + r], if l  r;
2. {l}  (s, t) = (s  l, t + l), if s  l;
3. {l}  (s, t) = [0, t + l), if l  (s, t);
4. {l}  (s, t) = (l  t, t + l), if t  l;
5. {l}  (s, +) = (s  l, +), if s  l;
6. {l}  (s, +) = [0, +), if s < l;
7. (m, n)  (s, t) = (s  n, t + n), if s  n;
8. (m, n)  (s, t) = [0, t + n), if (m, n)  (s, t) 6= ;
9. (m, n)  (s, +) = (s  n, +), if s  n;
10. (m, n)  (s, +) = [0, +), if s < n;
11. (m, +)  (s, +) = [0, +);
12. H1   = ;
13. H1  H2 = H2  H1 ;
14. (H1  H2 )  H = (H1  H)  (H2  H);
S
S
15. ( k Hk )  H = k (Hk  H), where k  N>0 ;
16. (H1  H2 )  H = (H1  H)  (H2  H), if (H1  H2 ) 6= ;
17. (H1  H2 )  H = H1  (H2  H).
738

fiQualitative Spatial Logics for Buffered Geometries

S
S
Proof.[for Rule 15] Suppose d  ( k Hk )  H.S By Lemma 15, there exist d1  k Hk and
d2  H such that d  {d1 }  {d2 }. Since d1  k Hk , then there exists a k  N>0 such that
d1  Hk . SinceSd1  Hk for some k, d2  H, then by Definition 7, d  Hk  H, for some k.
Therefore d  k (Hk  H).
S
Suppose S
d  k (Hk  H). Then, thereSexists a k  N>0 such that d  Hk  H.
Since Hk  k Hk , by Definition 7, d  ( k Hk )  H. 
Proof.[for Rule 16] Suppose H1  H2 6= . Then, Hi 6= , i  {1, 2}. Since H1 , H2 , H
are non-negative intervals, by the intersection rules and Definition 7, (H1  H2 )  H and
(H1 H)(H2 H) are non-negative intervals. Let L = (H1 H2 )H, R = (H1 H)(H2 H).
If H = , then by Rule 12, L = R = ; otherwise, we show L = R by cases:
 H1  H2 or H2  H1 : When H1  H2 , then Definition 7, H1  H  H2  H.
L = H2  H = R. When H2  H1 , similarly, L = H1  H = R.
 H1 6 H2 and H2 6 H1 : Without loss of generality, let us suppose glb(H1 )  glb(H2 ) 
lub(H1 )  lub(H2 ). Then, glb(H1  H2 ) = glb(H2 ), lub(H1  H2 ) = lub(H1 ).
To prove L = R, it is sufficient to show the following properties hold:
1. lub(L) = lub(R);
2. glb(L) = glb(R);
3. lub(L)  L iff lub(R)  R;
4. glb(L)  L iff glb(R)  R.
By Rules 1-14 and intersection rules, lub(L) = lub(H1  H2 ) + lub(H) = lub(H1 ) +
lub(H). lub(R) = min(lub(H1 ) + lub(H), lub(H2 ) + lub(H)) = lub(H1 ) + lub(H).
Thus, lub(L) = lub(R) (Property 1 holds).
If lub(H1 )  H1 and lub(H)  H, then by Rules 1-14 and intersection rules, lub(L)  L
and lub(R)  R; otherwise, lub(L) 6 L and lub(R) 6 R. Thus, Property 3 holds.
We prove Property 2 and Property 4 by cases:
 H  H1 =  and H  H2 = :
 lub(H)  glb(H1 ):
glb(L) = glb(H1  H2 )  lub(H) = glb(H2 )  lub(H).
glb(R) = max(glb(H1 )  lub(H), glb(H2 )  lub(H)) = glb(H2 )  lub(H).
Thus, glb(L) = glb(R) (Property 2 holds).
If glb(H2 )  H2 and lub(H)  H, then by Rules 1-14 and intersection rules,
glb(L)  L and glb(R)  R; otherwise, glb(L) 6 L and glb(R) 6 R. Thus,
Property 4 holds.
 glb(H)  lub(H2 ):
glb(L) = glb(H)  lub(H1  H2 ) = glb(H)  lub(H1 ).
glb(R) = max(glb(H)  lub(H1 ), glb(H)  lub(H2 )) = glb(H)  lub(H1 ).
Similar to the case above, it is clear that Property 2 and Property 4 hold.
 H  H1 6=  and H  H2 = : then, H  (H1  H2 ) = .
glb(L) = glb(H1  H2 )  lub(H) = glb(H2 )  lub(H).
739

fiDu & Alechina

glb(R) = max(0, glb(H2 )  lub(H)) = glb(H2 )  lub(H).
Similar to the cases above, it is clear that Property 2 and Property 4 hold.
 H  H1 =  and H  H2 6= : then, H  (H1  H2 ) = .
glb(L) = glb(H)  lub(H1  H2 ) = glb(H)  lub(H1 ).
glb(R) = max(glb(H)  lub(H1 ), 0) = glb(H)  lub(H1 ).
Similar to the cases above, it is clear that Property 2 and Property 4 hold.
 H  H1 6=  and H  H2 6= :
since H1 , H2 , H are intervals and H1  H2 6= , H  (H1  H2 ) 6= .
glb(L) = 0.
glb(R) = max(0, 0) = 0.
By Rules 1-14, glb(L)  L and glb(R)  R.
It is clear that Property 2 and Property 4 hold.
In every case, Properties 1-4 hold.
Therefore, L = R. 
Proof.[for Rule 17] Let
S L = (H1  H2 )  H, R = H1  (H2  H).
By Definition 7, LS= ( d1 H1 ,d2 H2 {d1 }  {d2 })  H.
By Rule 15, L = d1 H1 ,d2 H2 (({d1 }  {d2 })  H).
By Rule 13, ({d1 }  {d2 })  H = H
S  ({d1 }  {d2 }).
By Rule 15, H  ({d
S 1 }  {d2 }) = dH ({d}  ({d1 }  {d2 })).
By Rule 13, L =
S d1 H1 ,d2 H2 ,dH (({d1 }  {d2 })  {d}).
Similarly, R = d1 H1 ,d2 H2 ,dH ({d1 }  ({d2 }  {d})).
To prove L = R, it is sufficient to show
({d1 }  {d2 })  {d} = {d1 }  ({d2 }  {d}).
Let l = ({d1 }  {d2 })  {d}, then l = [|d1  d2 |, d1 + d2 ]  {d};
r = {d1 }  ({d2 }  {d}), then r = {d1 }  [|d2  d|, d2 + d].
We prove l = r by cases:
 d  [|d1  d2 |, d1 + d2 ]: By Definition 7, l = [0, d1 + d2 + d].
d1 + d2  d, d2 + d  d1 , d1 + d  d2 .
Thus, d1  [|d2  d|, d2 + d]. By Definition 7, r = [0, d1 + d2 + d].
 d 6 [|d1  d2 |, d1 + d2 ]:
 d > d1 + d2 : By Definition 7, l = [d  d1  d2 , d1 + d2 + d].
d1 < d  d2 = |d2  d|.
By Definition 7, r = [d  d2  d1 , d1 + d2 + d].
 d < |d1  d2 |: By Definition 7, l = [|d1  d2 |  d, d1 + d2 + d].
 d1  d2 : d < d1  d2 , this is, d1 > d2 + d.
By Definition 7, r = [d1  d2  d, d1 + d2 + d].
 d1 < d2 : d < d2  d1 , this is, d1 < d2  d.
By Definition 7, r = [d2  d  d1 , d1 + d2 + d].
In each case, l = r. Therefore, L = R. 

740

fiQualitative Spatial Logics for Buffered Geometries

Appendix B. Alternative Proof of the Path-Consistency Lemma
In this appendix, we would like to provide a sketch of an alternative proof idea of the
Path-Consistency Lemma, since it may appeal to some of the readers more than the proof
presented in Section 5.3. The alternative proof uses Lemma 44.
Lemma 44 If a distance constraint d(p, q)  h is in DS(+ ) and h =
6 , then there exist
+
d(p, q)  m1 and d(p, q)  m2 in DS( ) such that h = m1  m2 , m1 and m2 are both
identity or definable intervals.
Proof. By Lemma 31, d(p, q)  h is left-definable and right-definable. By Definition 16,
there exists a sequence of distance constraints d(pi , pi+1 )  hi (p1 = p, pn = q, 0 < i < n)
in D(+ ), such that for m1 = h1  ...  hn1 , h  m1 , h and m1 have the same greatest
lower bound (both value and openness). By Definition 15, d(p, q)  m1 is in DS(+ ). By
Lemma 21 and Definition 14, m1 is an identity or definable interval. Similarly, by Definition 17, there exists an m2 such that h  m2 , h and m2 have the same least upper bound
(both value and openness), d(p, q)  m2 is in DS(+ ), m2 is an identity or definable interval. By intersection rules, h = m1  m2 . 
Proof.[sketch of the alternative proof of the Path-Consistency Lemma] Suppose D(+ ) is
not path-consistent. Then there exist d(p, q)  g1 and d(p, q)  g2 in DS(+ ), g1 6= ,
g2 6= , and g1  g2 = . By Lemma 44, there exist d(p, q)  m1 and d(p, q)  s1 in DS(+ )
such that g1 = m1  s1 , m1 and s1 are both identity or definable intervals. Similarly, we
have g2 = m2  s2 , where m2 and s2 are both identity or definable intervals. g1  g2 = 
holds iff one of the following holds: m1  m2 = , m1  s2 = , s1  m2 = , s1  s2 = .
Without loss of generality, let us suppose m1  m2 = . By Lemma 35, m1  m2 = 
iff (m1  m2 )  {0} = . Let m = m1  m2 . Then m is an identity or definable interval.
Since d(p, q)  m1 and d(p, q)  m2 are in DS(+ ), m1 6= , m2 6= , then by Lemma 31,
d(p, q)  m1 and d(p, q)  m2 are left-definable and right-definable. Since d(p, q)  m2 , we
have d(q, p)  m2 . By Lemma 29, d(p, p)  m is left-definable and right-definable. The rest
of the proof is almost the same as the proof of the Path-Consistency Lemma (starting from
By Lemma 23, glb(h0 )  {0, , 2, 3, 4}) presented in Section 5.3. We discuss different
ways to obtain m given its greatest lower bound (the role of m is similar to h0 ) and check
whether  can be derived in every valid case. 

Appendix C. Consequences of the Path-Consistency Lemma
In this appendix, we state explicitly some implications of the Path-Consistency Lemma.
Lemma 45 Let  be a finite consistent set of formulas. If a distance constraint d(p, q)  h
is in DS(+ ), then h 6= .
Proof. Follows immediately from the proof of the Path-Consistency Lemma. 
Lemma 46 Let  be a finite consistent set of formulas. If a distance constraint d(p, p)  h
is in DS(+ ), then 0  h.
741

fiDu & Alechina

Proof. Suppose a distance constraint d(p, p)  h is in DS(+ ) and 0 6 h. By Definition 13
and Definition 15, d(p, p)  {0} is in D(+ ). By Definition 15, d(p, p)  (h  {0}) = . This
contradicts the fact that d(p, p)   is not in DS(+ ) (by Lemma 45). Therefore, 0  h. 

References
Aiello, M., Pratt-Hartmann, I., & van Benthem, J. (Eds.). (2007). Handbook of Spatial
Logics. Springer.
Allen, J. F. (1983). Maintaining Knowledge about Temporal Intervals. Communications of
the ACM, 26 (11), 832843.
Balbiani, P., Condotta, J., & del Cerro, L. F. (1999). A New Tractable Subclass of the
Rectangle Algebra. In Proceedings of the 16th International Joint Conference on
Artifical Intelligence, pp. 442447.
Bennett, B. (1996). The Application of Qualitative Spatial Reasoning to GIS. In Proceedings
of the 1st International Conference on GeoComputation, Vol. I, pp. 4447.
Bennett, B., Cohn, A. G., & Isli, A. (1997). A Logical Approach to Incorporating Qualitative Spatial Reasoning into GIS (Extended Abstract). In Proceedings of the 3rd
International Conference on Spatial Information Theory, Vol. 1329 of Lecture Notes
in Computer Science, pp. 503504. Springer.
Chen, J., Cohn, A. G., Liu, D., Wang, S., OuYang, J., & Yu, Q. (2015). A survey of
qualitative spatial representations. The Knowledge Engineering Review, 30 (1), 106
136.
Clementini, E., & Felice, P. D. (1996). An algebraic model for spatial objects with indeterminate boundaries. In Proceedings of the GISDATA specialist meeting on Geographic
Objects with Undeterminate Boundaries, pp. 155169.
Clementini, E., & Felice, P. D. (1997). Approximate Topological Relations. International
Journal of Approximate Reasoning, 16 (2), 173204.
Clementini, E., Felice, P. D., & Hernandez, D. (1997). Qualitative Representation of Positional Information. Artificial Intelligence, 95 (2), 317356.
Cohn, A. G., & Gotts, N. M. (1996a). Representing Spatial Vagueness: A Mereological Approach. In Proceedings of the 5th International Conference on Principles of Knowledge
Representation and Reasoning, pp. 230241.
Cohn, A. G., & Gotts, N. M. (1996b). The Egg-Yolk Representation of Regions with
Indeterminate Boundaries. In Proceedings of the GISDATA Specialist Meeting on
Geographical Objects with Undetermined Boundaries, pp. 171187.
Cohn, A. G., & Renz, J. (2008). Qualitative Spatial Representation and Reasoning. In
Handbook of Knowledge Representation, pp. 551596. Elsevier.
de Kleer, J. (1986). An assumption-based TMS. Artificial Intelligence, 28 (2), 127162.
Du, H. (2015). Matching Disparate Geospatial Datasets and Validating Matches using Spatial
Logic. Ph.D. thesis, School of Computer Science, University of Nottingham, UK.
742

fiQualitative Spatial Logics for Buffered Geometries

Du, H., & Alechina, N. (2014a). A Logic of Part and Whole for Buffered Geometries. In
Proceedings of the 21st European Conference on Artificial Intelligence, pp. 997998.
Du, H., & Alechina, N. (2014b). A Logic of Part and Whole for Buffered Geometries. In
Proceedings of the 7th European Starting AI Researcher Symposium, pp. 91100.
Du, H., Alechina, N., Hart, G., & Jackson, M. (2015). A Tool for Matching Crowd-sourced
and Authoritative Geospatial Data. In Proceedings of the International Conference
on Military Communications and Information Systems, pp. 18. IEEE.
Du, H., Alechina, N., Jackson, M., & Hart, G. (2016).
A Method for Matching Crowd-sourced and Authoritative Geospatial Data. Transactions in GIS.
http://dx.doi.org/10.1111/tgis.12210.
Du, H., Alechina, N., Stock, K., & Jackson, M. (2013). The Logic of NEAR and FAR. In
Proceedings of the 11th International Conference on Spatial Information Theory, Vol.
8116 of Lecture Notes in Computer Science, pp. 475494. Springer.
Du, H., Nguyen, H., Alechina, N., Logan, B., Jackson, M., & Goodwin, J. (2015). Using
Qualitative Spatial Logic for Validating Crowd-Sourced Geospatial Data. In Proceedings of the 29th AAAI Conference on Artificial Intelligence (the 27th Conference on
Innovative Applications of Artificial Intelligence), pp. 39483953.
Egenhofer, M. J. (1997). Query processing in spatial-query-by-sketch. Journal of Visual
Languages and Computing, 8 (4), 403424.
Egenhofer, M. J., & Franzosa, R. D. (1991). Point Set Topological Spatial Relations. International Journal of Geographical Information Systems, 5 (2), 161174.
Egenhofer, M. J., & Herring, J. R. (1991). Categorizing Binary Topological Relations
Between Regions, Lines, and Points in Geographic Databases. Tech. rep., University
of Maine.
Fine, K. (1975). Vagueness, truth and logic. Synthese, 30, 263300.
Frank, A. U. (1991). Qualitative Spatial Reasoning with Cardinal Directions. In Proceedings
of the 7th Austrian Conference on Artificial Intelligence, pp. 157167.
Frank, A. U. (1996). Qualitative Spatial Reasoning: Cardinal Directions as an Example.
International Journal of Geographical Information Science, 10 (3), 269290.
Goyal, R. K., & Egenhofer, M. J. (2001). Similarity of Cardinal Directions. In Jensen, C. S.,
Schneider, M., Seeger, B., & Tsotras, V. J. (Eds.), Advances in Spatial and Temporal
Databases, Vol. 2121 of Lecture Notes in Computer Science, pp. 3655. Springer.
Guesgen, H. W., & Albrecht, J. (2000). Imprecise reasoning in geographic information
systems. Fuzzy Sets and Systems, 113 (1), 121131.
Haklay, M. (2010). How good is volunteered geographical information? A comparative
study of OpenStreetMap and Ordnance Survey datasets. Environment and Planning
B: Planning and Design, 37 (4), 682703.
ISO Technical Committee 211 (2003). ISO 19107:2003 Geographic information  Spatial
schema. Tech. rep., International Organization for Standardization (TC 211).
743

fiDu & Alechina

Jackson, M., Rahemtulla, H., & Morley, J. (2010). The synergistic use of authenticated and
crowd-Sourced data for emergency response. In Proceedings of the 2nd International
Workshop on validation of GeoInformation products for crisis management, pp. 9199.
Kopczynski, M. (2006). Efficient spatial queries with sketches. In Proceedings of the ISPRS
Technical Commission II Symposium, pp. 1924.
Kutz, O. (2007). Notes on Logics of Metric Spaces. Studia Logica, 85 (1), 75104.
Kutz, O., Sturm, H., Suzuki, N., Wolter, F., & Zakharyaschev, M. (2002). Axiomatizing
Distance Logics. Journal of Applied Non-Classical Logics, 12 (3-4), 425440.
Kutz, O., Wolter, F., Sturm, H., Suzuki, N., & Zakharyaschev, M. (2003). Logics of metric
spaces. ACM Transactions on Computational Logic, 4 (2), 260294.
Lehmann, F., & Cohn, A. G. (1994). The EGG/YOLK Reliability Hierarchy: Semantic
Data Integration Using Sorts with Prototypes. In Proceedings of the 3rd International
Conference on Information and Knowledge Management, pp. 272279.
Li, S., Liu, W., & Wang, S. (2013). Qualitative constraint satisfaction problems: An extended framework with landmarks. Artificial Intelligence, 201, 3258.
Li, S., Long, Z., Liu, W., Duckham, M., & Both, A. (2015). On redundant topological
constraints. Artificial Intelligence, 225, 5176.
Ligozat, G. . (1998). Reasoning about Cardinal Directions. Journal of Visual Languages &
Computing, 9 (1), 2344.
Lutz, C., & Milicic, M. (2007). A Tableau Algorithm for Description Logics with Concrete
Domains and General TBoxes. Journal of Automated Reasoning, 38 (1-3), 227259.
Mackworth, A. K., & Freuder, E. C. (1985). The Complexity of Some Polynomial Network
Consistency Algorithms for Constraint Satisfaction Problems. Artificial Intelligence,
25 (1), 6574.
Mallenby, D. (2007). Grounding a Geographic Ontology on Geographic Data. In AAAI
Spring Symposium - Logical Formalizations of Commonsense Reasoning, pp. 101106.
Mallenby, D., & Bennett, B. (2007). Applying Spatial Reasoning to Topographical Data with
a Grounded Ontology. In Proceedings of the 2nd International Conference GeoSpatial
Semantics, No. 4853 in Lecture Notes in Computer Science, pp. 210227. Springer.
Moratz, R., Renz, J., & Wolter, D. (2000). Qualitative Spatial Reasoning about Line
Segments. In Proceedings of the 14th European Conference on Artificial Intelligence,
pp. 234238.
Moratz, R., & Wallgrun, J. O. (2012). Spatial reasoning with augmented points: Extending
cardinal directions with local distances. Journal of Spatial Information Science, 5 (1),
130.
OpenStreetMap (2012). The Free Wiki World Map. http://www.openstreetmap.org.
Ordnance Survey (2012). Ordnance Survey. http://www.ordnancesurvey.co.uk.
Pawlak, Z., Polkowski, L., & Skowron, A. (2007). Rough Set Theory. In Wiley Encyclopedia
of Computer Science and Engineering. John Wiley & Sons, Inc.
744

fiQualitative Spatial Logics for Buffered Geometries

Randell, D. A., Cui, Z., & Cohn, A. G. (1992). A Spatial Logic based on Regions and Connection. In Proceedings of the 3rd International Conference on Principles of Knowledge
Representation and Reasoning, pp. 165176.
Renz, J., & Nebel, B. (2007). Qualitative Spatial Reasoning Using Constraint Calculi. In
Aiello, M., Pratt-Hartmann, I., & van Benthem, J. (Eds.), Handbook of Spatial Logics,
pp. 161215. Springer.
Roy, A. J., & Stell, J. G. (2001). Spatial Relations between Indeterminate Regions. International Journal of Approximate Reasoning, 27 (3), 205234.
Schockaert, S., Cock, M. D., Cornelis, C., & Kerre, E. E. (2008a). Fuzzy region connection
calculus: An interpretation based on closeness. International Journal of Approximate
Reasoning, 48 (1), 332347.
Schockaert, S., Cock, M. D., Cornelis, C., & Kerre, E. E. (2008b). Fuzzy region connection calculus: Representing vague topological information. International Journal of
Approximate Reasoning, 48 (1), 314331.
Schockaert, S., Cock, M. D., & Kerre, E. E. (2009). Spatial reasoning in a fuzzy region
connection calculus. Artificial Intelligence, 173 (2), 258298.
Sirin, E., Parsia, B., Grau, B. C., Kalyanpur, A., & Katz, Y. (2007). Pellet: A practical
OWL-DL reasoner. Journal of Web Semantics, 5 (2), 5153.
Skiadopoulos, S., & Koubarakis, M. (2004). Composing cardinal direction relations. Artificial Intelligence, 152 (2), 143171.
Smith, N. J. (2008). Vagueness and Degrees of Truth. Oxford University Press.
Stocker, M., & Sirin, E. (2009). PelletSpatial: A Hybrid RCC-8 and RDF/OWL Reasoning and Query Engine. In Proceedings of the 5th International Workshop on OWL:
Experiences and Directions.
Sturm, H., Suzuki, N., Wolter, F., & Zakharyaschev, M. (2000). Semi-qualitative Reasoning
about Distances: A Preliminary Report. In Proceedings of the Logics in Artificial
Intelligence, European Workshop, JELIA, pp. 3756.
van Beek, P. (1992). Reasoning About Qualitative Temporal Information. Artificial Intelligence, 58 (1-3), 297326.
Wallgrun, J. O., Wolter, D., & Richter, K. (2010). Qualitative matching of spatial information. In Proceedings of the 18th ACM SIGSPATIAL International Symposium on
Advances in Geographic Information Systems, pp. 300309.
Wolter, F., & Zakharyaschev, M. (2003). Reasoning about Distances. In Proceedings of the
18th International Joint Conference on Artificial Intelligence, pp. 12751282.
Wolter, F., & Zakharyaschev, M. (2005). A logic for metric and topology. Journal of
Symbolic Logic, 70 (3), 795828.
Zadeh, L. A. (1975). Fuzzy logic and approximate reasoning. Synthese, 30 (3-4), 407428.
Zimmermann, K. (1995). Measuring without Measures: The Delta-Calculus. In Proceedings
of the 2nd International Conference on Spatial Information Theory, pp. 5967.

745

fiJournal of Artificial Intelligence Research 56 (2016) 573611

Submitted 11/15; published 8/16

A Study of Proxies for Shapley Allocations of Transport Costs
Haris Aziz

HARIS . AZIZ @ DATA 61. CSIRO . AU

Data61/CSIRO and University of New South Wales (UNSW),
Sydney, Australia

Casey Cahan

CCAH 002@ AUCKLANDUNI . AC . NZ

University of Auckland,
Auckland, New Zealand

Charles Gretton

CHARLES @ HIVERY. COM

Hivery,
Sydney, Australia; and
Australian National University (ANU),
Canberra, Australia; and
Griffith University,
Gold Coast, Australia

Philip Kilby

PHILIP. KILBY @ DATA 61. CSIRO . AU

Data61/CSIRO and Australian National University (ANU),
Canberra, Australia

Nicholas Mattei

NICHOLAS . MATTEI @ DATA 61. CSIRO . AU

Data61/CSIRO and University of New South Wales (UNSW),
Sydney, Australia

Toby Walsh

TOBY. WALSH @ DATA 61. CSIRO . AU

Data61/CSIRO and University of New South Wales (UNSW),
Sydney, Australia

Abstract
We survey existing rules of thumb, propose novel methods, and comprehensively evaluate a
number of solutions to the problem of calculating the cost to serve each location in a single-vehicle
transport setting. Cost to serve analysis has applications both strategically and operationally in
transportation settings. The problem is formally modeled as the traveling salesperson game (TSG),
a cooperative transferable utility game in which agents correspond to locations in a traveling salesperson problem (TSP). The total cost to serve all locations in the TSP is the length of an optimal
tour. An allocation divides the total cost among individual locations, thus providing the cost to serve
each of them. As one of the most important normative division schemes in cooperative games, the
Shapley value gives a principled and fair allocation for a broad variety of games including the TSG.
We consider a number of direct and sampling-based procedures for calculating the Shapley value,
and prove that approximating the Shapley value of the TSG within a constant factor is NP-hard.
Treating the Shapley value as an ideal baseline allocation, we survey six proxies for it that are
each relatively easy to compute. Some of these proxies are rules of thumb and some are procedures
international delivery companies use(d) as cost allocation methods. We perform an experimental
evaluation using synthetic Euclidean games as well as games derived from real-world tours calculated for scenarios involving fast-moving goods; where deliveries are made on a road network
every day. We explore several computationally tractable allocation techniques that are good proxies
for the Shapley value in problem instances of a size and complexity that is commercially relevant.

c
2016
AI Access Foundation. All rights reserved.

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

1. Introduction
We study transport scenarios where deliveries of consumer goods are made from a depot to locations
on a road network. At each location there is a customer, e.g., a vending machine or shop, that has
requested some goods, e.g. soda, milk, or crisps. The vendor who plans and implements deliveries is
faced with two vexing problems. The first difficult hurdle is solving the combinatorial optimization
problem of routing and scheduling vehicles to deliver goods in a cost-effective manner. Many varieties of this first problem exist (Golden, Raghavan, & Wasil, 2008), and for our proposes we shall
refer to it as the vehicle routing problem (VRP). We begin our investigation supposing that VRP has
been solved heuristically, and therefore the assignment of locations to routes (and delivery vehicles)
has been made.
The second vexing problem is determining how to evaluate the cost to serve each location.
Specifically, the vendor must decide how to apportion the costs of transportation to each location in
an equitable manner. The results of the cost to serve analysis have a variety of important applications. Using the allocation directly the vendor can of course charge locations their allocated portion
of the transportation costs. More realistically, vendors use the cost allocations when (re)negotiating
contracts with customers; extracting higher per-unit delivery prices from their most expensive customers. Supply chain managers also reference cost allocations when deciding whether or not to
include/continue trade with a particular location. Techniques informed by cost allocations in planning for a profitable transport business were recently reviewed by Ozener, Ergun, and Savelsbergh
(2013). Finally, provided market conditions are favourable, sales managers can be instructed to acquire new customers in territories where existing cost allocations are relatively high in order to share
the cost of delivery among more locations.
Addressing the second vexing problem, this paper stems from our work with a fast-moving consumer goods company that operates nationally both in Australia and New Zealand. The company
serves nearly 20,000 locations weekly using a fleet of 600 vehicles. Our industry partner is under
increasing economic pressure to realise productivity improvements through optimisation of their logistical operations. A key aspect of that endeavour is to understand the contribution of each location
to the overall cost of distribution. In this study, we focus at the individual route level for a single
truck, where we apportion the costs of the deliveries on that route to the constituent locations. We
formalise this setting as a traveling salesperson game (TSG) (Potters, Curiel, & Tijs, 1992), where
the cost to serve all locations is given by the solution to an underlying traveling salesperson problem
(TSP). Once formalised as a game, we can use principled solution concepts from cooperative game
theory, in particular the Shapley value (Shapley, 1953), in order to allocate costs to locations in a
fair and economically efficient manner. The unique axiomatic properties of the Shapley value are
enticing to our industry partner, as the allocation is fair in a reasonable and comprehensible sense.
Charging customers in a fair manner provides strong justification for delivery prices and encourages
trust between the operator and customer.
Calculating the Shapley value of a game is a notoriously hard problem (Chalkiadakis, Elkind,
& Wooldridge, 2011). A direct calculation of the Shapley value for a TSG requires the computation
of optimal solutions to exponentially many distinct instances of the TSP. Sampling procedures can
be used for approximating the value, however these too do not offer a practical solution for larger
games. Moreover, we prove that there is no polynomial-time -approximation of the Shapley value
for any constant   1 unless P = NP. In order to be practically applicable, we must be able to
calculate a cost allocation for each location on a route, for over 600 unique routes, that may change

574

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

daily or weekly as customers change their order volumes. Hence, we need methods that return
values within minutes, not hours. Allocations are also used to heuristically evaluate assignments of
locations to trucks in a larger VRP. In such a setting we must be able to estimate cost allocations
within seconds or fractions of a second, not minutes.
To circumvent the computational difficulties of calculating Shapley values, this work explores
six proxies1 for the Shapley value. We investigates three simple rules of thumb, including a simple
distance measure that we have seen employed in various industrial engagements. We include these to
analyze how good these proxies are relative to the Shapley value, the stated ideal cost allocation rule.
Other proxies we develop offer tractable alternatives to the Shapley value, and in some cases appeal
to other allocation concepts from cooperative game theory (Peleg & Sudholter, 2007; Curiel, 2008).
Two of our proxies appeal to the well-known Held-Karp (Held & Karp, 1962) and Christofides
(Christofides, 1976) TSP heuristics, respectively.
We report a detailed experimental comparison of proxies using a large corpus of synthetic Euclidean games, and problems derived from real-world tours calculated for fast-moving consumer
goods businesses in the cities of Auckland (New Zealand), Canberra, and Sydney (Australia). Our
experimentation uncovers a novel computationally cheap proxy that gives good approximations of
the Shapley value. Our evaluation also considers the ranking of locationsleast to most costly
induced by the Shapley and proxy values. Ranking locations is a common request from our industrial
partner and is relevant when, for example, we are interested in identifying the most costly locations
to serve. We find that two proxies, one of which is our novel proxy, provide good ranking accuracy
with respect to the rank induced by the Shapley value.

2. Preliminaries
We use the framework of cooperative game theory to gain a deeper understanding of our delivery
and cost allocation problems (Peleg & Sudholter, 2007; Chalkiadakis et al., 2011). In cooperative
game theory, a game is a pair (N, c) where N is the set of agents of size |N| = n and the second
term c : 2N  R is the characteristic function. Taking S  N, c(S) is the cost of subset S. A cost
allocation is a vector x = (x1 , . . . , xn ) denoting that cost xi is allocated to agent i  n. We restrict our
attention to economically efficient cost allocations, which are allocations satisfying in xi = c(N)
 i.e. the sum of allocated costs is equal to the cost of serving the grand coalition.
For any cooperative game (N, c), a solution concept  assigns to each agent i  N the cost
i (N, c). There may be more than one allocation satisfying the properties of a particular solution
concept, thus  is not necessarily single-valued, and might give a set of cost allocations (Peleg &
Sudholter, 2007). We sometimes omit (N, c) from our notation of  and other solution concepts
when the context is clear. A minimal requirement of a solution concept is anonymity, meaning
that the cost allocation must not depend on the identities of locations. Prominent solution concepts
include the core, least core, and the Shapley value. For   0, we say that cost allocation  is in
the (multiplicative) -core if iS i (N, c)  (1 + )c(S) for all S  N (Faigle & Kern, 1993). The
0-core is referred to simply as the core. Both the core and -core can be empty. The -core which is
1. We use the word proxy instead of approximation to ease discussion and, technically, many of these measures are
stand-ins for the Shapley value, not approximations of it; i.e., they do not give a guarantee of a quantitatively provable
approximation.

575

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

non-empty for the smallest possible  is called the least core. This particular  is referred to as the
least core value.2
Our work focuses on the single-valued solution concept called the Shapley value (Shapley,
1953). Writing SVi (N, c) for the Shapley value of agent i, formally we have:
SVi (N, c) =

|S|!(|N|  |S|  1)!
(c(S  {i})  c(S)).
|N|!
SN\{i}



(1)

In other words, the Shapley value divides costs based on the marginal cost contributions of agents.
In the traveling salesperson problem (TSP) a salesperson must visit a set of locations N =
{1, . . . , n}  {0} starting and ending at a special depot location 0. For i, j  N  {0} i 6= j, di j is the
strictly positive distance traversed when traveling from location i to j. Here, di j =  if traveling
directly from i to j is impossible. Taking distinct i, j, k  N  {0}, the problem is symmetric if and
only if di j = d ji for all i, j  N  {0}. It satisfies the triangle inequality if and only if di j + d jk 
dik (Garey & Johnson, 1979).
A TSP is Euclidean when each location is given by coordinates in a (two dimensional) Euclidean
space; therefore di j is the Euclidean distance between i and j. A Euclidean TSP is both symmetric
and satisfies the triangle inequality.
A tour is given by a finite sequence of locations that starts and ends at the depot 0. The length of
a tour is the sum of distances between consecutive locations. For example, the length of [0, 1, 2, 0]
is d01 + d12 + d20 . An optimal solution to a TSP is a minimum length tour that visits every location.
It is NP-hard to find an optimal tour, and generally there is no polynomial-time -approximation
for any  unless P = NP (Sahni & Gonzalez, 1976). An -approximation for a given optimisation
problem is an algorithm that runs on an instance x and returns a feasible solution F(x) which has
cost c(F(x)) related to the optimal solution OPT (x) as follows (Papadimitriou, 1994):
c(F(x))
 .
c(OPT (x))
Informally,  is a bound on the relative error of an approximation function. When i, j di j are finite,
the triangle inequality, and symmetry hold, then polynomial-time approximations exist for the TSP
problem (Held & Karp, 1962; Christofides, 1976).
Given a TSP, the corresponding traveling salesperson game (TSG) is a pair (N, c). N is the set of
agents which corresponds to the set of locations.3 The second term c : 2N  R is the characteristic
function. Taking S  N, c(S) is the length of the shortest tour of all the locations in S. A cost
allocation is a vector x = (x1 , . . . , xn ) denoting that cost xi is allocated to location i  N. For the
special depot location, we shall always take x0 = 0 (Potters et al., 1992). Typically, the depot is
operated by the agent who is distributing the costs and does not want to incur costs himself. Hence,
we refer to n as the number of locations, while the corresponding TSP has n + 1 points.
2. The 0-core of the transport game we focus on in this work can be empty. However, if the game is convex, the Shapley
value lies in the core (Tamir, 1989).
3. From here on we focus on a restriction of general games to delivery games (TSGs) and therefore we use location
instead of agent for ease of exposition.

576

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

3. Useful Properties of the Shapley Value
When discussing cost allocations with industrial partners, the concept of fairness is often of primary concern. A fair and principled cost allocation scheme would allow them to explain charges to
customers in an objective way; making the whole process more transparent. The Shapley value for
general games is the unique assignment of costs that satisfies three natural axioms: (1) anonymity,
the cost allocated to a particular location depends only on the impact visiting the locations has on
the total cost; (2) efficiency, the entire cost of serving all N locations is allocated; and (3) strong
monotonicity (Young, 1985), given two games (N, c) and (N, c0 ), S : ci (S)  c0 i (S) = i (N, c) 
i (N, c0 ); where the marginal contribution ci (S) from player i to the total cost of coalition S is:
(
c(S)  c(S \ {i}) if i  S
ci (S) =
c(S  {i})  c(S) if i 
/ S.
Due to these and other derivative axiomatic properties, the Shapley value has been termed the most
important normative payoff division scheme in cooperative game theory (Winter, 2002). These
axioms alone make the Shapley value attractive in a cost allocation setting.
The Shapley value has additional attractive properties in terms of existence and computability
when used as a cost allocation scheme. For example, whereas the 0-core can be empty, and therefore
not yield any allocation at all (Tamir, 1989), the Shapley value always exists in the TSG setting. In
logistics, there is often some fixed cost associated with serving a particular location, e.g., special
parking or permitting. If we treat a variant of the TSG where some locations have an associated fixed
cost in addition to their transportation costs e.g. parking and loading fees then the Shapley
value will allocate those fixed costs to only the associated locations. Formally, given a fixed cost
f (i) of serving location i, f (i) does not need to be removed before computing the Shapley value,
as follows. Suppose c is the characteristic function of the TSG defined above, and c0 satisfies the
identity c0 (S) = c(S) + iS f (i). Then, by the additivity propertity of the Shapley value (Shapley,
1953) we have SVi (N, c0 ) = SVi (N, c) + f (i).
For delivery settings, an additional observation is that charging locations according to the Shapley value may incentivise them to recruit new customers in their vicinity. Locations that recruit
nearby locations for a vendor can reasonably expect to lower the transportation costs they are allocated. In detail, consider a vendor serving locations N = {1, . . . , n}. From the vendors perspective,
adding a new location, n + 1, to an existing delivery route is clearly a good idea if the revenue
generated by delivering to that location is greater than the marginal cost c(N  {n + 1})  c(N) of
the new delivery. Because existing locations in the vicinity of n + 1 are already paying for deliveries, charging the additional customer the marginal quantity c(N  {n + 1})  c(N) will typically
be unfair. In this case, existing customers would likely be subsidizing new customers, and therefore
disincentivised from finding new business for the vendor. The Shapley value mitigates this, and can
be expected to provide recruitment incentives. Making this discussion more concrete, suppose the
game is Euclidean with N = {x} a single agent at distance 100 from the depot and the new agent y is
at distance 5 from x. The transportation cost of serving {x, y} can be as high as 210. Clearly, charging the new agent at most c({x, y})  c({x}) = 10 while x continues to pay around 200 is unfair. On
the other hand, if the vendor allocates costs according to the Shapley value, the existing customers
costs decrease when the new agent joins.
Another possible benefit in delivery settings is that, if the characteristic function is concave then
the Shapley value lies in the non-empty 0-core. Formally, concavity is satisfied if for all i  N, S 
577

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

T  N \ {i} : c(S  {i})  c(S)  c(T  {i})  c(T ). Charging customers according to Shapley/core
values actually guarantees that they are incentivised to recruit. Specifically, for all i  N : SVi (N 
{n+1}, c) < SVi (N, c). In other words, the Shapley allocation of costs to existing locations decreases
when a new customer n + 1 is added. Unfortunately general TSGs do not necessarily have concave
characteristic functions. However, as long as the cost function is roughly concave is all that is
necessary for existing locations to realise savings. In practice there are synergies, and incentives
for further recruitment on routes where we charge according to the Shapley value. In our empirical
data, even when the game is not concave we frequently observe such incentives given a Shapley
allocation. And compared to charging customers according to their marginal contribution to costs,
we do not explicitly disincentivise recruitment. Summarizing, if an agent knows that all locations are
charged according to the Shapley value, they can typically expect incentives to recruit new locations
in their vicinity.

4. Computing the Shapley Value
Our focus now shifts to computing the Shapley value. Considering games in general, it should be
noted that a direct evaluation of Equation 1 requires that we sum over exponentially many quantities.
Such a direct approach to the calculation of the Shapley value is therefore not practical for any game
of a reasonable size. Indeed, starting from Mann and Shapley (1962), authors motivate auxiliary restrictions and constraints, for example on the size and importance of coalitions, in order to describe
games where the Shapley value can be calculated. More recent literature proposes a variety of approaches to directly calculate the Shapley value for certain games (Conitzer & Sandholm, 2006;
Ieong & Shoham, 2005), however efficient calculation of the value for TSGs has remained elusive.
We require an accurate baseline in order to experimentally evaluate the proxies we later develop
for the Shapley value of the TSG. To this purpose we investigate exact and general sampling-based
approximations of the Shapley value. We treat our transport setting specifically, describing a novel
procedure for an exact evaluation of the Shapley value of a TSG by following Bellmans dynamic
programming solution to the underlying TSP. We also discuss how in general the Shapley value can
be evaluated approximately using a sampling procedure. We study this sampling approach in TSGs
using two distinct characterisations of the Shapley value which are amenable to sampling-based
evaluation. We perform a detailed empirical study of sampling-based evaluations using Synthetic
TSG instances where the underlying TSP model is Euclidean. In closing we give a hardness proof
relating to the computation of the Shapley value of TSGs, showing that approximation of the Shapley value in TSGs is intractable.
4.1 Dynamic Programming
We found that the steps performed by a dynamic programming (DP) algorithm for the underlying
TSP expose the marginsi.e. terms of the form c(S  {i})  c(S)that are summed over in a direct evaluation of Equation 1. The Shapley value of a TSG can therefore be computed as a side
effect while a DP procedure computes the optimal solution to the underlying TSP. This procedure
is formally captured in Algorithm 1: DP-TSP-Shapley. The algorithm as written assumes that distance costs are symmetric and that location 0 is a special depot location, both of these assumptions
can be relaxed for the more general case of simply computing Shapley values leveraging dynamic
programming.

578

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

Algorithm 1 DP-TSP-Shapley
Input: N = {1, . . . , n}  {0} locations with di j the cost to travel from i to j.
Output: List SV with SVi for all i  N.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

// c(S, j) is the length of the shortest path starting at 0, through all locations in S, and ending at j.
c  []
// T (S) is the length of the shortest tour of all locations in S starting and ending at location 0.
T  []
SV  []
for i  {1, . . . , |N|} do
c({0, i}, i)  d0,i
T ({i})  2  d0i
SVi  (|N|1)!
|N|!  T ({i})
end for
for s  {2, . . . , |N|} do
for each S which is a subset of N of size s do
SDEPOT  S  {0}
for j  S do
c(SDEPOT , j)  miniS,i6= j c(SDEPOT \ { j}, i) + di j
end for
T (S)  min jS c(SDEPOT , j) + d j0
for i  S do
SVi  |S1|!(|N||S1|1)!
 (T (S)  T (S \ {i}))
|N|!
end for
end for
end for

These ideas can be made concrete by following the procedure outlined by Bellman (1962). The
equations at the heart of that TSP solution procedure recursively define a cost function, c(S, j),
which is the shortest path through all locations in S starting at the depot 0 and ending at j.4
c({ j}, j) = d0 j .
c(S, j) =

min (c(S \ { j}, k) + dk j ).

kS,k6= j

Following the above recursive definition, a DP process iteratively tabulates c(S, j) for successively
larger coalitions S. At each iteration of subset size |S| < |N| the procedure tabulates all quantities
c(S, j) taking |S| = n. By computing the values c(S, 0) for |S| < |N|, we have access to the characteristic function evaluation c(S) of subtours of locations in S, as follows:

c(S) = c(S, 0) = min(c(S, j) + d j0 ).
jS

Therefore, one can incrementally evaluate the sum in Equation 1 for a TSG, while calculating optimal subtours for progressively larger coalitions (supersets) within a classical DP procedure. Intuitively, as we compute a tour using Bellmans algorithm, by additionally evaluating c(S, 0) for each
4. Our notations depart slightly from Bellmans seminal work. Whereas we take c(S, j) to be the cost of each optimal
tour-prefix path (i.e. starting at the depot 0 and ending at j), Bellman originally took c(S, j) to be the cost of optimal
tour-suffix paths starting from j, traversing the locations in S and ending at the depot 0.

579

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

encountered subset S we obtain all quantities required to calculate the marginal costs of locations.
We have therefore highlighted a concrete relationship between a classical procedure for the TSP and
the Shapley value of the corresponding TSG. The dynamic programming algorithm is very fast up
to about 18 locations, where the size of the table and the number of subsets become unmanageable.
4.2 Computational Complexity
We now consider, for the most general setting of the TSG, the complexity of calculating the Shapley
value. Below we prove that the Shapley value of a location in the TSG cannot be approximated
within a constant factor in polynomial-time unless P = NP.
Theorem 1 There is no polynomial-time -approximation of the Shapley value of the location in a
TSG for constant   1 unless P = NP.
Proof. Let G(N, E) be a graph with nodes N and edges E. If an -approximation exists we can use
it to solve the NP-complete Hamiltonian cycle problem on G. First, from G, construct a complete
weighted and undirected graph G0 (N, E 0 ), where (i, j) has weight 1 if (i, j) is in the transitive closure
of E, and otherwise has weight n!. If there is a Hamiltonian cycle in G, then the Shapley value
of any i  N in the TSG posed by G0 is at most 1. Suppose there is no Hamiltonian cycle in G. We
show there exists a permutation  of N that induces a large Shapley value for any node j as follows:
repeatedly add a node from N\ j to  so that there remains a Hamiltonian cycle amongst elements
in ; when there is no such node then add j. The marginal cost of adding j to  is at least n!.
The Shapley value of j is the average cost of adding it to a coalition S  N \ j, therefore its Shapley
value is at least . Even though edge weights in G0 are large, we can represent G0 compactly in
O(log(n) + n2 log()) space. An -approximation on G0 for j therefore decides the existence of the
Hamiltonian cycle in G.
q

4.3 Sampling-Based Evaluation
Using either the dynamic programming solution, or indeed the state-of-the-art TSP solver Concorde (Applegate, Bixby, Chvatal, & Cook, 2007) in a direct calculation of the Shapley value, we
find it impractical to compute the exact Shapley value for instances of the TSG larger than about 10
locations (recall that this does not include the depot, hence the corresponding TSPs have 11 points).
A direct method requires an exponential number of characteristic function computations, each of
which requires solving an NP-hard problem. Figure 4.3 shows the exponential increase in runtime
computing the Shapley value on our experimental setup (described in detail in Section 4.4) via a
direct enumeration method. To obtain an accurate baseline for games of a commercially interesting
size our investigation now turns to sampling procedures. Indeed, because the Shapley value is a
population average it is reasonable to estimate the value using a sampling procedure.
The first use of sampling to approximate the Shapley value of games was proposed and studied
by Mann and Shapley (1960). Perhaps the most elegant and general method proposed by Mann and
Shapley is called Type-0 sampling. This method repeatedly draws a permutation of the locations
uniformly at random. The marginal cost of each agent i is then calculated by taking the difference in
the cost of serving agents up to and including i in the permutation and the cost of serving the agents
proceeding i. By repeatedly sampling permutations and computing the marginal costs of including
each agent i in this way, we arrive at an unbiased estimate of the Shapley value.
580

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

Figure 1: Runtime of computing the Shapley value via brute force enumeration with calls to Concorde for instances with between 4 and 10 locations. The graphs show the mean and
standard deviation of the running time over 1,070 games per number of locations. Comparing with Figure 3 we observe the time increasing exponentially, with our practical
limit hit around 10 locations.

Type-0 sampling has appeared over the years in various guises, and is reported under different
names in the literature on approximating power indicesof which the Shapley value is but one
in coalitional games. A recent rediscovery of Type-0 sampling is the ApproShapley algorithm by
Castro, Gomez, and Tejada (2009); who also provide asymptotic bounds on the sampling error
of ApproShapley. ApproShapley shall be the focus of our sampling work, however prior to giving
its details, it is worth briefly reviewing other classes of game where sampling-based evaluations
have been explored. Bachrach, Markakis, Resnick, Procaccia, Rosenschein, and Saberi (2010) have
previously examined Type-0 sampling in simple gamesi.e. where the value of a coalition is either
0 or 1deriving bounds that are probably approximately correct. In other words, the actual Shapley
value lies within a given error range with high probability. Continuing in this line of work, Maleki,
Tran-Thanh, Hines, Rahwan, and Rogers (2013) show that if the range or variance of the marginal
contribution of the players is known ahead of time, then more focused (termed stratified) sampling
techniques may be able to decrease the number of samples required to achieve a given error bound.
Other methods of approximating the Shapley value, specifically for weighted voting games, have
appeared in the literature including those based on multi-linear extensions (Leech, 2003; Owen,
1972) and focused random sampling (Fatima, Wooldridge, & Jennings, 2008, 2007). Most recently,
Type-0 sampling for computing the Shapley value has been applied to a planning setting where a set
of delivery companies attempt to pool resources in order to more effectively service a probabilistic
set of orders that appear within a territory over a rolling horizon (Kimms & Kozeletskyi, 2015).

581

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

To calculate the Shapley value of a TSG via sampling we employ the Type-0 method suggested
by Mann and Shapley (1960) and Castro et al. (2009) called ApproShapley; pseudocode is given in
Algorithm 2. Writing (N) for the set of |N|! permutation orders of locations N, taking   (N)
we write i for the subset of N which precedes location i in . An alternative formulation of the
Shapley value can be written in terms of (N), by noting that value equals the marginal cost of each
location when we construct coalitions in all possible ways, as follows.
SVi (N, c) =

1
 (c(i  {i})  c(i )).
|N|! (N)

(2)

For each sampled permutation, ApproShapley evaluates the characteristic function for each
i  |N| computing the length of an optimal tour for the set of locations in the i-sized prefix. By
construction, the cost allocation produced by ApproShapley is economically efficient. As a small
but important optimisation, in our work we cache the result of each evaluation of the characteristic
function to avoid solving the same TSP twice. Note that lines 15 and 17 of Algorithm 2, which
normalize the values to sum to 1.0, are not strictly necessary since the given algorithm is efficient.
However, we include the code here so that all proxies and algorithms surveyed return a cost vector
that sums to 1.0.
Algorithm 2 ApproShapley

Algorithm 3 SubsetShapley

Input: N = {1, . . . , n} locations with cost c(S) to Input: N = {1, . . . , n} locations with cost c(S) to
serve a subset S  N and m samples.
serve a subset S  N and m samples.
Output: List SV with SVi for all i  N.
Output: List SV with SVi for all i  N.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

SV  []
for i  1 to |N| do
SVi  0
end for
for SampleNumber  1 to m do
// RAND(X) returns a random element of X.
Perm  RAND((N))
S  0/
for i  1 to |N| do
S  S  {Permi }
SVPermi  SVPermi +(c(S)c(S\{Permi }))
end for
end for
TotalValue  iN SVi
for i  1 to |N| do
SVi  SVi  (c(N)/TotalValue)
end for
return SV

582

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

SV  []
for i  1 to |N| do
SVi  0
end for
for SampleNumber  1 to m do
for i  1 to |N| do
S  0/
for j  1 to n do
// RAND(X) returns a random element of X.t
if i 6= j and RAND({0, 1}) = 1 then
S  S  { j}
end if
end for
SVi  SVi + |S|!(n  |S|  1)! 
(c(S  {i})  c(S))
end for
end for
TotalValue  iN SVi
for i  1 to |N| do
SVi  SVi  (c(N)/TotalValue)
end for
return SV

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

In our work, we also considered an alternative sampling method, which samples not over permutations, but rather over subsets of locations as implied by the formulation in Equation 1 of Section 2.
There are fewer subsets than there are permutations, seemingly an advantage in a sampling-based
evaluation of the Shapley value. Using a limited number of subsets to estimate the Shapley value
was explored, and shown to be an effective measure, by Papapetrou, Gionis, and Mannila (2011).
We name this method SubsetShapley and provide pseudocode in Algorithm 3. Like ApproShapley,
this method produces an economically efficient allocation. For ApproShapley, the estimate of SVi
is updated once per drawn permutation while for SubsetShapley if we draw only a single random
subset, we only update our estimate for one location. Thus, for SubsetShapley, at every iteration of
the sampling loop at Line 6 we draw a different set S  N \ {i} uniformly at random for each location i, making the two methods comparable based on the total number of updates per location per
iteration. However, we only use one sample in ApproShapley for all locations and one sample per
location in SubsetShapley. For each i, the update to SVi is then the weighted marginal contribution,
formally SVi  SVi + |S|!(n  |S|  1)!(c(S  i)  c(S)). The coefficient |S|!(n  |S|  1)! ensures that
for each subset S of sampled locations, we account for the number of permutations where locations
S are ordered before location i. Note that without this term, this algorithm does not converge to the
Shapley value in the limit.
4.4 Experimental Setup and Evaluation of Sampling Methods
It is important both here and in our later experimental evaluation to be confident that we have
sampled a sufficient number of times over a sufficient number of games to establish confidence in
our sampling scheme and to ensure the statistical significance of our results. We must ensure that,
for every game, we have taken enough samples to have a high probability of a low error on any
individual Shapley value. For our overall evaluation we must ensure that we have sampled enough
games from the representative population of all possible games. In this section we described our
experimental setup and derive precise statistical bounds for our results.
Not all proxies and estimators of the Shapley value that we consider yield economically efficient
allocations of the cost of the optimal tour. For this reason, we will discuss the Shapley value and all
proxies for it in terms of the induced fractional (also called normalized) allocation of the cost of the
optimal tour. Formally, iSV = SVi/ jn SV j . Fractional allocations allow us to compare efficient and
non-efficient cost allocations on equal footing, in a way that would be used in operational contexts
such as transport settings. This formulation also enables us to allocate the cost of the optimal route
only having to solve the NP-hard TSP once.
We generated a collection of Euclidian games we call the Synthetic dataset. In the Synthetic
dataset we generate locations |N|  {4, . . . , 20} on a 100  100 unit square. The coordinates of these
locations are generated in an independent and identically distributed (i.i.d.) manner represented by
32-bit floating point values.5 To these n players we add a depot location, also chosen uniformly at
random over the square. Hence, for all reported results there are a total of n + 1 locations for the
underlying routing problem and n locations that must have costs allocated to them. All timing experiments reported were performed on a computer with an Intel Xeon E5405 CPU running at 2.0 GHz
with 4 GB of RAM running Debian 6.0 (build 2.6.32-5-amd64 Squeeze10). Additional computing
power for non-timing experiments was provided by Data61/NICTAs heterogenous compute cluster.
5. Corpus available online at https://github.com/nmattei/ShapleyTSG

583

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

We use statistical measures to report all our empirical results. We provide a brief overview of
key concepts here and refer the reader to the textbook by Corder and Foreman (2009) for details.
Denote |x| as the absolute value of the quantity x. Writing x denotes the average of a set {x1 , . . . , xn }
and
p let nxi denote an estimate of value xi from the set X. The standard deviation ( ) of x is:  =
2
1/n 
i=1 (xi  x) . To measure accuracy we use root mean squared errors (RMSE), a common
metric to quantify the error over a number of predictions. For a set of k paired observations X =
{x1 , . . . , xk } and estimatesq
X = {x1 , . . . , xk }, the RMSE between X and X (the RMSE of X with
respect to X) is: RMSE = 1/k ki=1 (xi  xi )2 .
We perform a similar analysis to that of Castro et al. (2009) to determine the number of samples
required to have high confidence in the values obtained via sampling methods in our setting. To
establish that the error of our sampling procedure is below  with probability greater than 1  , we
use the central limit theorem and the assumption that our errors are normally distributed, giving:

2
ci |  )  1  ,
= P(|SVi  SV
2
where Z  N(0, 1) is a normal random variable. Given a game, we do not know the variance of
all the locations for all permutations, and it is infeasible to compute this value. We can estimate the
i ) and minimum (xi ) change in the cost function of an individual
variance given the maximum (xmax
min
i
location i  N. If i is co-located with the depot the minimum impact on cost is xmin
= 0. As
 we have
a 100  100 unit square the maximum possible distance between any two points is 100  2  142.
The greatest impact this can have on cost is if i is the only location and is added opposite the depot
i
along the diagonal, causing an increase in cost equal to xmax
= 2  142 = 284.
The maximum variance of a random variable is reached when the variable takes the two extreme
values with probability 1/2. We can then use the following inequality to estimate the variance:
2
No. Samples  Z/2

i
i
i )2
xi + xmin
xi + xmin
(xi  xmin
1 i
1 i
 2  (xmax
 max
)2 + (xmin
 max
)2  max
.
2
2
2
2
4
Applying this to our previous equation yields a formula for determining the error in our setting:
i )2
(xmax
ci  SVi |  )  1  .
= P(|SV
4  2
What we tolerate as an error bound has a significant effect on the size of the games we can
actually use for testing (as sampling is very time consuming). Selecting  = 0.75 means that each
locations Shapley value will not be off by more than 0.75 distance units (kilometers), which is very
low for a fast moving consumer goods setting. As this derivation of error only gives us an absolute
bound on the pairwise error between a locations actual Shapley value SVi and its estimated Shapley
ci , we must derive the maximum possible error for all points in order to bound the error of
value SV
SV
 . The total error for a game is given by:
2
No. Samples  Z/2


SVi + 
n
 j (SV j + )

=

SVi + 
n
 j (SV j ) + n

=

SVi + 0.75
.
n
 j (SV j ) + 0.75n

Observe that nj (SV j ) is the exact cost of the grand tour of all the points and our overestimate is
0.75n. Hence, we are overestimating the grand tour by at most 15 distance units (kilometers) for our
n = 20 instances. Thus the error in any locations  SV is negligible. We want  to be very small,
giving us high confidence that we have converged; we set  = 0.005, giving us a 99.5% probability
584

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

that our error will be less than . Substituting this into our error equation (Z0.05/2 = Z0.0025 = 2.81)
we get:
(2  142)2
 283, 052
4  0.752
In order to draw conclusions from our calculated RMSE values we must have statistical confidence in the mean (RMSE) over a set of games. The RMSE itself is an average over normalized
values, each of which take on values in [0, 1]. Assuming the errors in our problem are normally
distributed we can bound the variance using the same techniques described above, arriving at a
variance of  2 = 1/4. From this we can use standard techniques from statistics and engineering (Natrella, Croarkin, & Guthrie, 2012) to determine that the number of games we need to use in order to
have a 95% confidence interval that the absolute error in the measurement of the RMSE, which is
an aggregate measure of error for all locations in all games, is within 0.03 (or roughly 3%):
 2


1/4

2
2
No. Games = 1, 070  Z=0.05
= (1.96)
 1, 067.
2
(0.03)2
No. Samples = 300, 000  2.812 

Intuitively, this means we are 95% sure that if we re-ran our entire experiment with new values the
mean error for a particular proxy would fall within 3%. Hence we can say that the mean error value,
measured over the set of 1,070 games, is accurate.
We compare the performance of ApproShapley and SubsetShapley using our Synthetic dataset.
We use Concorde (Applegate et al., 2007) to evaluate the characteristic function of the TSG by
solving the underlying TSP.
All optimal tour lengths calculated by Concorde are cached to speed up running time. Therefore,
we never re-evaluate a TSP with the same set of points. TSPs with less than four locations are evaluated by brute force. For each game in the Synthetic dataset we calculated the exact Shapley value
of every location, so that we could compare the sampled allocation with their exact counterparts.
We find the ApproShapley method of sampling over permutations provides a faster convergence, as
seen in Figure 2. After just 1000 iterations ApproShapley achieves a RMSE below 0.01, with a significantly smaller standard deviation than SubsetShapley. Moving to 100, 000 samples in the bottom
row of Figure 2, we see that the mean RMSE for ApproShapley is still significantly lower than for
SubsetShapley.
Figures 3 and 4 depict the mean running time and the number of calls to Concorde for the two algorithms, respectively. We see that ApproShapley runs faster than SubsetShapley in all instances that
we tested. This difference in runtime grows as the number of locations increases. ApproShapleys
faster running time is likely due to the need of only randomly generating one permutation instead of
n sets. Figure 4 provides an insight into the behavior of the two algorithms. SubsetShapley fills the
cache much quicker than ApproShapley, which explains the later flattening of the runtime curve for
ApproShapley seen in Figure 3. Both methods eventually evaluate all 217 possible points, saturating the cache. However, this early filling of the cache by SubsetShapley does not translate to faster
overall runtime. In practice, ApproShapley achieves a lower error, earlier, and continues to converge
towards an error of 0 faster than SubsetShapley.

585

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

Figure 2: Comparison of the accuracy of ApproShapley (left) and SubsetShapley(right) for 10,000
iterations (top) and 100,000 iterations (bottom) for TSGs with 10 locations. The graphs
show RMSE and its standard deviation over 1,070 instances between the sampled and
actual Shapley values. ApproShapley converges in fewer samples and is more stable between samples than SubsetShapley.

586

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

Figure 3: Comparison of the runtime performance of ApproShapley (left) and SubsetShapley (right)
for TSGs with 4 to 17 locations (not including the depot). The graphs show the mean and
standard deviation over 1,070 instances of the running time of the respective algorithm.
Because ApproShapley only needs to generate one permutation, compared to SampleShapleys n sets, it generally runs more quickly.

Figure 4: Comparison of the number of calls to Concorde as a function of sample number made
by ApproShapley (left) and SubsetShapley (right) for TSGs with 4 to 17 locations (not
including the depot). The graphs show the mean and standard deviation over 1,070 instances of the number of calls to Concorde. SubsetShapley fills the cache much quicker
than ApproShapley, this explains the later flattening of the runtime curve for ApproShapley seen in Figure 3. However, the earlier cache filling does not lead to a decrease in
running time.

587

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

5. Proxies for the Shapley Value
The use of ApproShapley requires that we solve an NP-hard problem each time we evaluate the
characteristic function. This is feasible for small TSG instances with less than a dozen locations,
however it does create an unacceptable computational burden in larger, realistically sized games.
We now describe a variety of proxies for the Shapley value that require much less computation
in practice. We have seen some of these proxies in use in the real-world to allocate costs, hence
their inclusion and analysis. We define and discuss these proxies in terms of their induced fractional
allocation, iSV = SVi/ jn SV j , as discussed in Section 4.4. An overview of the worst case and practical
running times of all these algorithms is presented in Table 1.
Method
or Proxy

Worst Case
Runtime

10 Loc.

ApproShapley (Concorde)

Exponential

 30sec.

 4,500 sec.

> 90,000 sec.

Shortcut Distance ( S HORT )

Exponential

 5 sec.

 10 sec.

 15 sec.

Exponential

 20 sec.

 25 sec.

 30 sec.

Depot Distance ( D EPOT )

O(n)

 1 sec.

 1 sec.

 1 sec.

Moat Packing ( M OAT )

Exponential

 5 sec.

 5 sec.

 5 sec.

Christofides ( C HRIS )

O(n3 )

 30 sec.

 2,500 sec.

 40,000 sec.

Exponential

 5 sec.

 5 sec.

 5 sec.

Re-routed Margin

Blend

( R EROUTE )

( B LEND )

Practical Running Time
20 Loc.
30 Loc.

Table 1: Summary of the proxies for the Shapley value surveyed in this paper.

5.1 Depot Distance ( D EPOT )
The distance from the depot  i.e. di0 for location i  is our most straightforward proxy. We
allocate cost to location i proportional to di0 . The fraction allocation to location i is
iD EPOT =

di0
.
n
i=1 di0

For this proxy, a location that is twice as distant from the depot as another has to pay twice the cost.
We can evaluate this proxy in time linear in the number of locations. In practice, computing this
value is instantaneous.
5.2 Shortcut Distance ( S HORT )
Another proxy that is straightforward to calculate and which has been used in commercial routing
software is the shortcut distance. This is the change in cost realized by skipping a location when
traversing a given optimal tour. Without loss of generality, suppose the optimal tour visits the locations according to the sequence [0, 1, 2, . . . ]. Formally, S HORTi = di1,i + di,i+1  di1,i+1 , where
locations 0 and n + 1 are the depot, and di j is the cost of travel from location i to j. The fractional

588

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

allocation given by the shortcut distance is then
iS HORT =

S HORTi
.
 jN S HORT j

We can evaluate this proxy by solving one TSP instance and one operation per location. In practice
we can compute this metric in less than 30 seconds.
5.3 Re-routed Margin ( R EROUTE )
For a location i  N, R EROUTEi is defined as c(N)  c(N\i)). The allocation to a player can be
computed with at most two calls to an optimal TSP solver. The fractional allocation is
iR EROUTE =

(c(N)  c(N\i))
.
 j=N (c(N)  c(N\ j))

We can evaluate this proxy by solving n + 1 TSPs: one for the grand tour and one for leaving out
each location. In practice we can compute this metric nearly instantaneously.
5.4 Christofides Approximation ( C HRIS )
A more sophisticated proxy is obtained if we use a heuristic when performing characteristic function
evaluations in ApproShapley, rather than solving the individual induced TSPs optimally. For this
proxy we use sampling to estimate the Shapley value and we use an approximation algorithm to
estimate the underlying TSP cost. To approximate the underlying TSP characteristic function, the
Christofides (1976) heuristic, an O(|N|3 ) time procedure is used. To obtain a fractional quantity
iC HRIS , we divide the allocation to location i by the total allocated costs. Assuming a symmetric
distance matrix satisfying the triangle inequality, the Christofides heuristic is guaranteed to yield a
tour that is within 3/2 the length of the optimal tour.
We briefly describe how the heuristic works. The TSP instance is represented as complete undirected graph G = (V, E), with one vertex in V for each location, and an edge E between every
distinct pair of vertices. For i, j  V the edge (i, j)  E has weight di j . A tour is then obtained as
follows: (1) compute the minimum spanning tree (MST) for G, (2) find the minimum weight perfect
matching for the complete graph over vertices with odd degree in that MST (typically performed
using the Hungarian Algorithm), (3) calculate a Eulerian tour for the graph obtained by combining
the MST from Step 1 and the matched edges from Step 2 (this is guaranteed to yield a Eulerian
multigraph, i.e., a graph where every vertex has even degree), and (4) obtain a final tour for the TSP
by removing duplicate locations from the Eulerian tour.
In the best case, the call to the Christofides heuristic will return a solution that is exactly the
solution to the TSP. Hence, this method requires as many calls per number of locations as we derived
in Section 4.4. Figure 5 shows the runtime of ApproShapley when we replace calls to Concorde
with calls to a program that solves a TSP using the Christofides heuristic. Comparing these results
to those in Figure 3, we see that for small numbers of locations ( 10) the runtimes for Concorde
and the Christofides heuristic are almost the same. However, as the number of locations grows, the
Christofides heuristic shows a significant speed improvement. For commercially interesting sizes,
20 locations and 300,000 samples, computing  C HRIS takes on the order of 2500 seconds (about 30
minutes). It is not practically computable ( 12 hours) for problems with 30+ locations, as shown
in Table 1.
589

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

Figure 5: Runtime performance of ApproShapley with calls to the Christofides heuristic for TSGs
with 4 to 17 locations (not including the depot). The graphs show the mean and standard
deviation over 1,070 instances of the running time of the respective algorithm. Comparing with Figure 3 we see that the Christofides heuristic decreases runtime and that this
decrease grows larger as the number of locations increases.

5.5 Nested Moat-Packing ( M OAT )
Another way to allocate costs in TSGs is based on dividing the locations into regions using a concept
called a moat (Cook, Cunningham, Pulleylank, & Schrijver, 1998). Intuitively, given a Euclidian
TSP and a set of locations N with the depot location 0, a moat is a closed strip of constant width
that separates a set of locations S  N from its compliment S. We assume without loss of generality
that 0 is always in S. In order to deliver to any location in S, one would need to traverse the moat in
order to reach all points in the set S, and then cross the moat again to return to any point in S. Hence,
a reasonable cost allocation is to charge the locations in S twice the cost of traversing the moat
surrounding S. If the locations in S had their own delivery truck, there would be no reason to cross
the territory of the moat surrounding S. In the following we use techniques described by Faigle,
Fekete, Hochstattler, and Kern (1998) and additionally refined by Ozener et al. (2013) with some
extensions for our setting.
Formally, given a set of locations N  {0}, let S  N and S be the compliment of S, and let M
be the set of all bipartitions of the locations {S, S} with the assumption that 0  S. Let wS,S be the
width (distance) of the moat between the set S and S. We refer to a vector of moats ~w and the width
of an individual moat as wS . Locations themselves cannot occur in the moat, as the moat itself is a
strip of unoccupied area on the map. Additionally, one only needs to consider circular moats as
this gives the minimal straight-line distance to a location inside the moat and is less than or equal to
the moat width. In order to have a well-formed cost allocation we want the moats to be as large as

590

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

possible, i.e., have maximum width. Hence, we can formulate a linear program to find a maximum
moat packing:
MPV = max



wS,S

S,SM

wS,S  0
s.t.



{S, S}  M

(3)

wS,S  di j i, j  N  {0}.

iS, jS

Though Equation 3 has exponentially many constraints, it can be solved in time polynomial in
the number of locations using dual techniques, returning at-most a polynomial number of moats with
width  0. We shall use the notation ~w to refer to the small set of moats with non-zero width (Faigle
et al., 1998). The vector of moat widths ~w given by the solution to Equation 3 may have many
moats that overlap, leading to ambiguities over which widths to allocate to which locations. Thus,
an arbitrary solution to Equation 3 does not yield a cost allocation. For this reason we must refine
our maximum moat packing to a maximum nested moat packing. In a nested moat packing two
distinct intersecting subsets cannot be encapsulated by the same non-empty moat unless one of
those coalitions is a subset of the other. Formally, a packing is nested if and only if S0 , S00 s.t.
wS0 > 0 and wS00 > 0, if S0  S00 6 0/ then either S0  S00 or S00  S0 . For any optimal solution ~w to
Equation 3 yielding objective value MPV over the set of partitions M there is a corresponding nested
packing with the same MPV (Cornuejols, Naddef, & Pulleyblank, 1985; Faigle et al., 1998). Once
we have a nested moat packing as it is clear which set of moats must be crossed to reach any location
from any other location, at which point we can derive a cost allocation for each location. Figure 6 is
a concrete example of a nested moat packing for 6 locations. In Figure 6 each of the 6 locations has
its own moat (light colors). Additionally, the moats for locations 5 and 6 are then surrounded by an
outer darker moat for the set {5, 6}.
Given a non-nested vector of moats ~w we follow the post-processing procedure described by
Ozener et al. (2013). For the nesting criteria defined above to be violated there must be three distinct
non-empty sets of locations S, S0 and S00 , such that wSS0 > 0 and wS0 S00 > 0. Given ~w we update the
values as follows: let   min{wSS0 , wS0 S00 }, make the following assignment updates to the moat
widths: wS  wS + , wS00  wS00 + , wSS0  wSS0  , and wS0 S00  wS0 S00  . This iterative
procedure terminates yielding a nested packing, taking exponential time in the worst case. However,
in all our experiments we found that nesting takes only a fraction of a second. This leaves us with
the allocation:
1
wS
1
wS
iM OAT =
 
=
.
 
MPV wS >0,iS |S| wS >0 wS wS >0,iS |S|
There are two key observations about the allocation derived from a (nested) moat packing.
First, 2  MPV , i.e., the sum of crossing all the moats twice, is exactly the value of the Held-Karp
relaxation for the underlying TSP if the TSP is symmetric and satisfies the triangle inequality (Held
& Karp, 1962). Thus, 2  MPV is a lower bound for the optimal tour for the underlying TSP and
3  MPV is an upper bound.6 Secondly, we observe that the allocation derived from a nested moat
packing where xi is the cost of location i satisfies iN xi  c(N) and S  N : iS xi  (1 + )c(S).
These constraints are exactly those for the multiplicative core we defined in the preliminaries for
6. The tightness of the bounds of the Held-Karp relaxation, i.e., the integrality gap, is a longstanding open question in
combinatorial optimisation (Cook et al., 1998).

591

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

Figure 6: An optimal nested moat-packing (colored regions) and optimal tour (line) for a TSP with
6 locations. The locations are indicated by the labels {1,2, . . ., 6}, and occur at the center
of the light moats. Each moat around locations 1 to 4 (light colored regions) is associated
with only one location. The moat around the set of locations {5, 6} (dark colored region)
is nested. This larger moat encloses two smaller, independent moats (light colored regions) around locations 5 and 6, respectively. There are 7 moats in total, and the optimal
tour must cross each moat twice.

the -core. Although the allocation achieved using nested moat packing to distribute 3  MPV is
not economically efficient, it is a core allocation of an approximate cost. Faigle et al. (1998) present
a proof that the nested moat packing provides a 21 -core allocation with respect to the actual cost
function for each location by distributing 3  MPV ; they conjecture   13 .
5.6 Blended Proxy ( B LEND )
An interesting question is whether or not blending a set of proxies that were practically computable
could provide a good estimate of the actual Shapley value. Framing this as a prediction or machine
learning problem, we want to learn a model to predict our output  SV given an input set consisting
of the easily computable proxies, { D EPOT ,  S HORT ,  R EROUTE ,  M OAT }. All analysis in this section is
carried out using SciKitLearn (Pedregosa et al., 2011), a machine learning library for Python.
First, we need to decide what sort of model is best for this setting. As each of these proxies
is attempting to estimate the same value, they are correlated. Consequently, one place to start is
to use principal component analysis (PCA) (Bishop, 2006) to understand how much variance can
be captured by a low dimensional model given our input set. We use SciKitLearn to run a PCA
decomposition over the set of all Synthetic data. SciKitLearn uses the linear algebra package of
SciPy to perform a singular value decomposition (SVD) of the data matrix; keeping only the most
significant singular vectors to project the data into lower dimensional spaces. This decomposition
shows that 98% of the variance can be explained by one component (vector), as depicted in Figure
7. Hence, a simple linear blend of a subset of the proxies should provide good predictive power.

592

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

Figure 7: The explained variance ratio of  SV given by a PCA decomposition of the input set of
{ D EPOT ,  S HORT ,  R EROUTE ,  M OAT } (left). Using a 1 dimensional model over the input set
we can explain 98% of the observed variance in  SV . Prediction error graph for 10 fold
cross-validation over the Synthetic dataset for a linear model blending  D EPOT and  M OAT
(right). Each point represents the prediction error for a location in the Synthetic dataset
and the dotted line (y = x) would be an ideal predictor. The correlation of actual and
predicted values for the 10 fold cross-validation is R2 = 0.8825 and  = 0.0025.

As we have selected a simple linear model, we must now decide which elements of the input
set of proxies, { D EPOT ,  S HORT ,  R EROUTE ,  M OAT }, we should use. We want to use the minimal set
of features, as using too many features can cause overfitting (Bishop, 2006). For selection, we use
SciKit (Pedregosa et al., 2011) to perform a k-best feature selection which takes each of the input
set in turn and computes a cross correlation between this element and the others, this is converted
using ANOVA to a score and a significance (p) value for each feature. We can compare these scores
to find the individual elements of the input set which are the most significant. Doing this we find the
scores for all of the elements of the input set to be statistically significant, hence useable. Looking
at the normalized scores themselves, { D EPOT = 1.0,  S HORT = 0.0260,  R EROUTE = 0.4946,  M OAT =
0.6305}, we see that  D EPOT and  M OAT are the two highest scoring indicators. We choose to limit our
linear model to two highest scoring elements  D EPOT and  M OAT as these are both significantly higher
scoring than the others and adding more elements may cause overfitting.
Now that we have a model and the input variables to train on, we need to learn the model and
perform cross-validation. For this tests we take the full Synthetic dataset and perform a 10-fold
cross-validation (Bishop, 2006). To perform k-fold cross-validation, we take the dataset and break
it into k equal sized folds F = { f1 , . . . , fk }. We hold out each one of these pieces from the training
set in turn (i.e., train on F \ { fi }) and use it as the test set (i.e., predict fi ). To select the 10 folds to
be used for cross-validation we use a stratified k-fold sampling, which ensures that every k fold has
the same statistical distribution as the whole training set (Pedregosa et al., 2011). Since we are using
a linear model, we use the coefficient of determination, R2 , as our fitness measure. For the 10 fold
cross-validation, we get a mean R2 = 0.8825 with a standard deviation of  = 0.0025. The graph

593

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

of predicted  SV as a function of the actual value is shown in Figure 7. The low  shows that our
model is robust and the high value of R2 indicates that our model is a good predictor. By training
over the entire Synthetic dataset we get our final model:
 B LEND = 0.579   D EPOT + 0.318   M OAT + 0.009.

6. Analysis of Nave Proxies
We refer to the three proxies  D EPOT ,  S HORT and  R EROUTE as being nave. Contrastingly, we call
 C HRIS ,  M OAT and  B LEND the sophisticated proxies. The formulation of the nave proxies  D EPOT
and  S HORT make them amenable to direct analysis of their worst case performance. We consider
settings where the nave proxies  D EPOT and  S HORT can perform quite badly.
In order to illustrate this, consider a TSG where the depot is at one corner of a square of dimension a with one location at each of the other 3 corners. Locations nearest the depot are indexed 1
and 3, and the third location indexed 2.
Location 1

a

Location 2

a
Depot

a
Location 3

a

Our nave proxies yield the following allocations:
i
 SV
 D EPOT  S HORT
1, 3 0.299a 0.293a 0.333a
2 0.402a 0.415a 0.333a
Observe  D EPOT performs well in this case (maximum of  11% error) while  S HORT does not (minimum of  16% error).
We now identify some pathological cases on which the  S HORT and  D EPOT proxies perform
poorly. Our first result demonstrates that  D EPOT and  S HORT may under-estimate the true Shapley
value badly.
Theorem 2 There exists an n location TSP problem on which, for some location i, the ratio iDEPOT/iSV
goes to 0 as n goes to . In the same instance, the ratio iSHORT/iSV goes to 0 as n goes to  for (n)
of the locations.
Proof. Suppose the first n  1 locations are at distance a from the depot, whilst the nth location is
located at a distance a in the opposite direction from the depot.
a
Locations 1, . . . , n  1

a
Depot

594

Location n

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

Note that the normalization constant for  SV ,  jn SV j = 4a. Now nSV = 2a/4a = 1/2 since the cost
of adding the nth location to any coalition is 2a. Leaving, for i < n,
iSV =

2a/(n1)

4a

=

1
.
2(n  1)

On the other hand, the normalization constant for  D EPOT , ni=1 di0 = na since all locations are
equidistant from the depot. Giving, for all i  n, iD EPOT = 1n .
Thus for i < n,
1/n
iD EPOT
2n  1
=
=
SV
1/2(n1)
n
i
which goes to 2 as n  . On the other hand,
1/n
nD EPOT
1
=
=
1/2
nSV
2n

which goes to 0 as n  .
Note that the shortcut proxy,  S HORT performs poorly on this example. For i < n, iS HORT = 0
since all the locations are co-located, leaving nS HORT = 1. For i < n we have iSV = 1/2(n  1). Thus,
for i < n,
iS HORT
0
=0
=
SV
1
/2(n1)
i
and

nS HORT
1
=
=2
SV
1
n
/2
q

Our second result demonstrates that  S HORT can also grossly over-estimate the true Shapley
value.
Theorem 3 There exists an n location TSG where the ratio iSV/iDEPOT goes to 0 as n goes to  for
(n) of the locations.
Proof. Suppose the first n  1 locations are at distance a from the depot, whilst the nth location is
located at a distance (n + 1)a from the depot in the opposite direction.
a
Locations 1, . . . , n  1

a(n + 1)
Depot

Location n

Note that the normalization constant for  SV ,  jn SV j = 2a + 2a(n + 1) = 2a(n + 2). The Shapley
2a
value SVi for any i < n is n1
, thus
iSV =

2a/n1

2a(n + 2)

=

595

1
.
(n  1)(n + 2)

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

While the fractional Shapley allocation for location n is
nSV =

2a(n + 1) 1
= .
2a(n + 2) 2

The normalization constant for  D EPOT is ni=1 di0 = a(n  1) + a(n + 1) = 2an. For location n
the assignment from the distance based proxy is
nD EPOT =

a(n + 1) n + 1
=
.
2an
2n

For i < n,
iD EPOT =

a
1
= .
2an 2n

Thus, for location n we have
nSV
nD EPOT
which goes to 1 as n goes to .
For i < n we have

iSV

iD EPOT

=

1/2

=

n+1/2n

1/(n1)(n+2)
1
2n

=

=

2n
2n + 1

2n
(n  1)(n + 2)

which goes to 0 as n goes to .
For the  S HORT we again have i < n, iS HORT = 0 leaving nS HORT = 1. Thus, nSV/nSHORT = 1/2 while
for i < n, iSV/iSHORT is undefined.
q
Our third result demonstrates that  S HORT may under-estimate the Shapley value badly even on
very simple examples which may be embedded in larger problems.
Theorem 4 There exists a 2 location TSG instance for which
locations.

 S HORT/ SV

= 0 for one of the two

Proof. Suppose the first location is located a distance a from the depot with the second location
located a distance of a farther down the road.
a
Depot

a
Location 1

Location 2

For the first location 1S HORT = 0, since removing it has no effect on the distance to the second
location. This leaves 2S HORT = 1. The Shapley value for the first location is
SV =

2a 0
+ = a.
2
2

Which gives  SV = a/4 and thus
 S HORT
0
=
= 0.
a/4
 SV
q
Our fourth and final result demonstrates that  S HORT may over-estimate the Shapley value badly.
596

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

Theorem 5 There exists a four location TSG for which  SV/ SHORT = 0 for two of the four cities.
Proof. Consider a four location TSG where locations 1 and 2 are  from each other and the depot
while cities 3 and 4 are at a distance a from the depot and  from each other.
Location 1

ka


Depot



Location 4




ka

Location 2

Location 3

We note that here   k  a, as such we will hide  terms in O(). The marginal cost saved
by skipping any location is , this means that all locations have the same allocation according to
 S HORT , namely for all i  {1, . . . , 4}, iS HORT = 1/4.
Note that the normalization constant for  SV ,  jn SV j = 2ka + O(). To compute the Shapley
values for locations 1 and 2 we observe that, in any given permutation, each location adds a multiple
of , thus by symmetry, for i  {3, 4},
iSV =

O()
2ka + O()

To compute the Shapley value for locations 3 and 4 we observe that, no matter where in the permutation they appear, the first contributes 2ka while the other contributes only . Consequently, by
symmetry, for locations i  {3, 4},
iSV

=

2ka+O()
2

1
= .
2ka + O() 2

Thus, locations i  {1, 2}, we have
 SV
 S HORT

=

O()
2ka+O()
1/4

=

4O()
.
2ka + O()

The term goes to 0 as k goes to .
q
All of the games illustrated above to illustrate poor performance by these proxies are relatively
simple and extremely degenerate. In real-world settings we would not expect the locations for delivery to be setup along a straight line or in a symmetrical box. Hence we are motivated to compare the
proxies using data that more accurately reflects the domain in which we hope to deploy our proxies.

7. Empirical Study
We implemented each of the six proxies discussed, along with a version of ApproShapley that uses
Concorde (Applegate et al., 2007) to evaluate the characteristic function of the TSG. All code and
data used in this project is available in a public Git repository at: https://github.com/nmattei/
ShapleyTSG. Rather than calculating  SV by direct enumeration as a baseline to compare proxies,
597

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

we estimate that value using ApproShapley with Concorde. As described in Section 4.4, this method
achieves an extremely good approximation of the true Shapley value and is computable in a reasonable time for testing on games with up to 20 locations.
We use our corpus of 1070 Synthetic games, constructed as described in Section 4.4, for games
with n  {4, . . . , 20} locations. We also test against a corpus of 119 Real-World games generated
from large VRPs in the cities of Auckland (New Zealand), Canberra, and Sydney (Australia).
Heuristic solutions to those VRPs are calculated using the Indigo solver (Kilby & Verden, 2011).
Indigo is a flexible heuristic solver implementing an Adaptive Large Neighbourhood Search, the
basic structure of which is described in detail by Ropke and Pisinger (2006).7 To give an indication of the scale and difficulty of these VRPs, the Auckland model comprises 1, 166 locations to be
served using a fleet of at most 25 vehicles over a 7 day period. From these heuristic solutions we
collect we collect tours of length 10 and 20 to create TSGs for testing. Because real-world distance
matrices are asymmetric (in all cases asymmetry is negligible), and we induce symmetric problems
by resolving for the greater of di j and d ji , i.e., setting di j = d ji = max{di j , d ji }. In total, we obtain
71 Real-World games of size 10 (14 in Auckland, 5 in Canberra, and 52 in Sydney) and 48 games
of size 20 (10 in Auckland, 7 in Canberra, 31 in Sydney).8
To evaluate how well proxies perform in approximating  SV we use several different test statistics, which we briefly review here (Corder & Foreman, 2009). Already discussed in Section 4.4 is
the root-mean-squared-error (RMSE) for each game. Additionally, we may want to know the maximum absolute point-wise error (MAPE), i.e., the maximum of the absolute error values for each
point-wise estimate:
MAPE = arg maxx,x[X,X] |x  x|.
We can use this measure for a particular game or compare the average maximum absolute pointwise error over a set of games (MAPE). This value lets us know, on average, the most we are
overcharging a particular customer (unlike RMSE which only tells us the aggregate error). Note that
using the same arguments from Section 4.4 we have the same guarantees on the acuracey of MAPE
as we do for RMSE, i.e., 3%.
One question often repeated in our consultation with logistics companies is who is my most
expensive customer? In order to know where to focus efforts on contract negotiations or sales functions, companies desire an understanding of the rank ordering of the cost of servicing locations. We
use Kendalls , written as KT and first introduced by Kendall (1938), to compare the ranking, i.e.,
least expensive to most expensive, of locations induced by the Shapley allocation and our proxies.
The value  measures the amount of disagreement between two rankings. It is customary to report
 as a normalized value (correlation coefficient) between 1 and -1, where  = 1 means that two lists
are perfectly correlated (equal) and  = 1 means that two lists are perfectly anti-correlated (they
are equal if one list is reversed). An intuitive interpretation of  between two lists is that % of the
orderings in the two lists are the same.
In detail, let X and Y be two partial orders over a set of items. If a  b  X Y then we say X
and Y are concordant on (a, b). If a = b  X  Y then we say there is a tie, and otherwise (a, b) is
7. Indigo is a strong vehicle routing solution platform, recently computing 5 new best solutions for 1, 000 customer problems from the VRPTW benchmark library. The solutions computed using Indigo were certified
by Dr. Geir Hasle, Chief Research Scientist at SINTEF and maintainer of the VRPTW benchmark library,
as the best currently known on September 24th of 2013. http://www.sintef.no/Projectweb/TOP/VRPTW/
Homberger-benchmark/1000-customers.
8. Due to commercial agreements with our industrial partners we cannot release these Real-World games.

598

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

discordant. Where M is the number of concordant pairs, N discordant pairs, T ties exclusively in X,
U ties exclusively in Y , the normalised KT distance  between X and Y is:
=p

MN
.
(M + N + T )  (M + N +U)

Our analysis makes use of the significance, or p-value, of a computed . The p-value is computed using a two-tailed t-test where the null hypothesis is that there is no correlation between
orderings ( = 0). Taking our significance threshold to be the customary 0.05, we can reject the null
hypothesis when p  0.05. When p  0.05 we fail to reject the null hypothesis. Hence, a p-value
 0.05 is a statistically significant result. Intuitively this means that it is so unlikely that two random lists would show such a high degree of correlation we can say the two lists are significantly
correlated.
7.1 Synthetic Data
Figure 8 gives an overview of our data, showing the RMSE and  for each proxy from  SV for all
game sizes of the Synthetic data. Tables 2 to 5 give a more in-depth look at the performance of the
proxies on a variety of interesting measures including their RMSE, MAPE, , number of statistically
significant s, and number of games with correctly identified top elements. In general,  S HORT and
 R EROUTE proxies are by far the worst, particularly in terms of approximating Shapley value, but
also in terms of the ranking induced by the corresponding allocations. The computationally more
expensive proxy  R EROUTE always dominates  S HORT ; though both of these proxies are dominated
by  D EPOT ,  M OAT ,  B LEND , and  C HRIS in all tests save one.
10 Locations
RMSE


15 Locations
RMSE


20 Locations
RMSE


All Data
RMSE


Shortcut Distance
Re-routed Margin
Depot Distance

0.3850
0.2565
0.0994

0.0968
0.0699
0.0275

0.3342
0.2168
0.0950

0.0764
0.0533
0.0235

0.2992
0.1915
0.0893

0.0606
0.0424
0.0195

0.3727
0.2493
0.0978

0.0564
0.0488
0.0059

Moat-Packing
Christofides
Blend

0.1617
0.0495
0.0710

0.0502
0.0216
0.0191

0.1437
0.0526
0.0742

0.037
0.0177
0.0168

0.1302
0.0523
0.0733

0.0293
0.0142
0.0154

0.1564
0.0520
0.0745

0.0197
0.0046
0.0075

Table 2: Average root mean squared error (RMSE) and standard deviation ( ) for the Synthetic data
for games with 10, 15, and 20 locations. Lower is better.

599

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

Figure 8: Performance of the proxies according to: (left) RMSE over the 1070 games generated for
each number of locations, and (right)  over the 1070 games generated for each number
of locations. The error bands correspond to plus or minus one standard deviation. The
vertical axis of our  plot has been inverted for ease of comparison, i.e., more correlated
lists are towards the bottom of the graph (1.0).

10 Locations
MAPE


15 Locations
MAPE


20 Locations
MAPE


All Data
MAPE


Shortcut Distance
Re-routed Margin
Depot Distance

0.2802
0.1866
0.0637

0.1088
0.0805
0.0238

0.2278
0.1460
0.0589

0.0843
0.0596
0.0226

0.1944
0.1203
0.0523

0.0700
0.0461
0.0193

0.2605
0.1741
0.0620

0.1155
0.089
0.0261

Moat-Packing
Christofides
Blend

0.1078
0.0311
0.0441

0.0452
0.0147
0.0154

0.0888
0.0318
0.0443

0.035
0.0137
0.0155

0.0722
0.0299
0.0417

0.0252
0.0108
0.0145

0.1003
0.0329
0.0472

0.0508
0.0158
0.0224

Table 3: Average maximum absolute error (MAPE) and standard deviation ( ) for the Synthetic
data for games with 10, 15, and 20 locations. Lower is better.

Looking first at the error in the estimation of  SV , the top of Figure 8 depicts the RMSE and
 over 1070 games for each proxy as we increase the number of locations per game. The overall
trend is positive with each proxy becoming more accurate (lower RMSE) as we increase the number
of locations. In the figure  C HRIS strictly dominates all of the other proxies in RMSE performance.
However, we also see that  B LEND and  D EPOT are competitive with  C HRIS both in terms of RMSE
and  .  B LEND is the winner in this category for practical purposes as it offers performance extremely
close to  C HRIS , with a tighter distribution on error than  D EPOT , for a fraction of the computation

600

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

time. Table 2 shows a more detailed breakdown of Figure 8 for particular numbers of locations. This
table allows us to see that the RMSE for  B LEND never goes above 0.1 with   0.01 for the larger
instances that are commercially interesting.
Table 3 sheds more light on the types of error that the proxies are likely to make. Again we
see that  B LEND achieves a better MAPE than any of the other proxies save  C HRIS , on average only
overcharging in the worst case by  4.7% with  = 2.2% of the true Shapley value, a mere 1%
more than  C HRIS . Again we see that  D EPOT is a fairly accurate proxy for  SV , only overcharging
by  6% with  = 2.6% for the largest instanes tested. However,  D EPOT is strictly dominated by
 B LEND in all error measures we considered and can be computed in similar time. Given that  B LEND
is computable in a fraction of the time for  C HRIS , has competitive overall error, and scales up to and
beyond commercially interesting problem sizes. It is the clear winner for this measure.
10 Locations



15 Locations



20 Locations



All Data




Shortcut Distance
Re-routed Margin
Depot Distance

0.0098
0.4732
0.5815

0.2403
0.1947
0.1791

0.0031
0.4160
0.5400

0.1931
0.1505
0.1524

-0.0076
0.3908
0.5018

0.1604
0.1397
0.1454

-0.0027
0.4828
0.5659

0.0106
0.0892
0.0385

Moat-Packing
Christofides
Blend

0.4098
0.7186
0.6834

0.2235
0.1663
0.1567

0.3526
0.6791
0.6206

0.1787
0.1430
0.1385

0.3392
0.6463
0.5706

0.1610
0.1286
0.1369

0.4190
0.7048
0.6616

0.0829
0.0374
0.0593

Table 4: Average Kendalls tau rank correlation coefficient () and Standard Deviation ( ) for the
Synthetic data for games with 10, 15, and 20 locations. Higher is better; +1 means the two
lists are perfectly correlated and 1 means the two lists are perfectly anti-correlated.

10 Locations
% Sig.
% Top

15 Locations
% Sig.
% Top

20 Locations
% Sig.
% Top

All Data
% Sig.
% Top

Shortcut Distance
Re-routed Margin
Depot Distance

1.49%
18.13%
16.44%

19.81%
77.75%
68.59%

3.73%
49.90%
70.18%

9.62%
73.08%
51.40%

5.32%
60.74%
84.85%

6.91%
67.00%
46.26%

4.72%
53.15%
69.67%

10.45%
69.85%
52.04%

Moat-Packing
Christofides
Blend

12.42%
29.90%
29.90%

71.68%
82.52%
80.84%

39.53%
90.18%
89.15%

62.42%
78.31%
68.41%

45.60%
96.54%
94.11%

56.82%
74.39%
59.71%

41.76%
85.28%
83.12%

61.13%
76.08%
65.38%

Table 5: (Left columns) The percentage of games out of 1070 where  is statistically significant
(p < 0.05) between the ranking induced by  SV and the ranking induced by  P ROXY . (Right
columns) The percentage out of 1070 games where the most expensive element according
to the raking induced by  SV matched the most expensive element in the ranking induced
by  P ROXY . Higher is better for both statistics.

601

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

Turning to the proxies performance on ranking, the bottom of Figure 8 depicts the average
Kendalls tau rank correlation coefficient () and the standard deviation ( ) for 1070 games for each
proxy as we increase the number of locations per game. The overall trend in this graph, as opposed
to the top one, is slightly negative. As we increase the number of locations, the ranking computed
by each proxy is increasingly uncorrelated with the ranking induced by  SV . On the positive side,
 C HRIS ,  B LEND , and  D EPOT , all return lists which have over a 0.6 correlation, i.e., 60% of the pairs
of elements are ordered correctly. Table 4 gives a closer look at the results for particular numbers
of locations. We see that  C HRIS and  B LEND both maintain  near 0.6 across the range of problems,
hence they correctly order most of the pairs of elements. These two proxies again strictly dominate
all other proxies; even  D EPOT performs poorly when measured against .
Table 5 gives us a more nuanced look at the ranking results. We see again that for larger games
the percentages of s that are statistically significant increase for all proxies for all locations, even
as the s themselves decrease. This is because the lists are recovering a significant portion of the
pairwise relations compared to the total number of pairwise relations. We see again that in terms
of statistically significant s,  C HRIS and  B LEND strictly dominate all other proxies by almost 15%
for all of the data considered. As an answer to the common customer question of whose costing me
the most, the results are a bit more mixed. Comparing just the highest ranked elements we see that
the performance of  B LEND drops below that of the performance of  R EROUTE , a surpassingly strong
proxy for this measure. However, if we want the top element according to  SV to be in the top 3
elements according to  P ROXY ,  B LEND achieves this feat over 90% of the time. Though all other
proxies see an increase in performance in this relaxed measure as well, only  C HRIS and  B LEND are
above 90% for all numbers of locations studied. Hence, we again see that  B LEND provides strong
performance at a practically computable running time across a range of game sizes.
7.2 Real-World Data
Measuring the performance of proxies on the Real-World corpus from Auckland, Canberra, and
Sydney, we find that the overall quality of allocation is slightly lower compared to the measurements
on the Synthetic corpus. We identified no significant performance differences between cities, and
therefore report all data here as aggregate statistics over the Real-World corpus of 71 games with
10 locations and 48 games with 20 locations. Tables 6 to 9 provide an in-depth perspective on the
performance of the proxies on the Real-World dataset with the same measures as the Synthetic
dataset. Again we see that the performance of  S HORT and  R EROUTE is strictly dominated according
to all statistical measures by all the other proxies; except  R EROUTE s ability to select the most costly
location with surprisingly high accuracy.

602

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

10 Locations
RMSE


20 Locations
RMSE


All Games
RMSE


Shortcut Distance
Re-routed Margin
Depot Distance

0.4511
0.4380
0.1380

0.1477
0.1472
0.065

0.3245
0.3030
0.0838

0.0929
0.0934
0.0313

0.3878
0.3705
0.1109

0.0633
0.0675
0.0271

Moat-Packing
Christofides
Blend

0.2692
0.1519
0.1442

0.1486
0.0823
0.0687

0.2088
0.1104
0.0826

0.0937
0.0529
0.0292

0.2390
0.1311
0.1134

0.0302
0.0207
0.0308

Table 6: Average root mean squared error (RMSE) and standard deviation ( ) for the Real-world
data for the 71 games with 10 locations and 48 games with 20 locations. Lower is better.

10 Locations
MAPE


20 Locations
MAPE


All Games
MAPE


Shortcut Distance
Re-routed Margin
Depot Distance

0.3678
0.3568
0.0835

0.1716
0.1695
0.0388

0.2462
0.2286
0.0390

0.1050
0.1012
0.0118

0.3187
0.3051
0.0655

0.1599
0.1588
0.0378

Moat-Packing
Christofides
Blend

0.2196
0.1178
0.0961

0.1463
0.0780
0.0523

0.1498
0.0739
0.0412

0.0959
0.0536
0.0134

0.1914
0.1001
0.0740

0.1328
0.0725
0.0493

Table 7: Average maximum absolute error (MAPE) and standard deviation ( ) for the Real-world
data for the 71 games with 10 locations and 48 games with 20 locations. Lower is better.

Turning first to the error in the estimation of  SV , we see that the results reported in Table 6 are
strictly higher on every measure for every proxy compared to the results in Table 2, the corresponding test for the Synthetic dataset. We see again that the error decreases as we increase the number
of locations for all proxies. The difference between Real-World and Synthetic does not render the
proxies unuseable. Observe that the RMSE for  B LEND only increases by 0.01 between Synthetic
and Real-world while the RMSE for  D EPOT only increases by about 0.003. This is a solid indicator
for the usefulness of  B LEND , as none of the Real-World instances were included in the training set
for the model. In an interesting twist, the more computationally expensive  C HRIS fares worse on the
Real-World data, doubling its error (an increase of 0.05) with respect to the Synthetic dataset.
The increase in RMSE is not followed when looking at MAPE. Comparing Table 7 to its Synthetic dataset partner Table 3, we see that both  B LEND and  D EPOT actually have a lower MAPE and
a lower  for the Real-World datasets with 20 locations. Observe that when comparing performance
according to MAPE we see that  B LEND and  D EPOT are only separated by 1% of performance, and
both strictly outperform all other metrics, even  C HRIS . We can see that  B LEND and  D EPOT are both
reasonable proxies for  SV in the Real-world corpus, achieving an overall RMSE less than 0.09 and
an absolute worst error per location of less than 0.05 (5% of true cost).
603

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

A possible explanation for the extremely good performance of  D EPOT requires a closer look at
the distribution of costs in the Real-World dataset. As the locations along a route are heuristically
allocated from a larger VRP, the allocations tend to cluster around a uniform allocation of around
0.050.08 per location, and many locations are equidistant from the depot. Consequently, the RealWorld data seems to be drawn from a different distribution than the Synthetic data (i.e., the locations
are not selected uniformly at random). Thus, the performance of  B LEND in both the ideal, uniformly
random case and a strongly degenerate real-world case is a strong argument for the portability of
 B LEND across domains.
10 Locations



20 Locations



All Games



Shortcut Distance
Re-routed Margin
Depot Distance

0.0756
0.3651
0.1055

0.3015
0.2793
0.3416

0.0061
0.4734
0.3322

0.2148
0.1602
0.1932

0.0408
0.4193
0.2188

0.0348
0.0542
0.1134

Moat-Packing
Christofides
Blend

0.3480
0.2457
0.1498

0.2504
0.3408
0.3287

0.3814
0.5403
0.4093

0.1721
0.1589
0.1809

0.3647
0.3930
0.2796

0.0167
0.1473
0.1297

Table 8: Average Kendalls  rank correlation coefficient () and Standard Deviation ( ) for the
Real-World data for the 71 games with 10 locations and 48 games with 20 locations.
Higher is better; +1 means the two lists are perfectly correlated and 1 means the two
lists are perfectly anti-correlated.

10 Locations
% Sig.
% Top

20 Locations
% Sig.
% Top

All Data
% Sig.
% Top

Shortcut Distance
Re-routed Margin
Depot Distance

4.22%
28.16%
12.67%

12.67%
57.74%
53.52%

8.33%
72.91%
43.75%

18.75%
70.83%
60.41%

5.88%
46.21%
25.21%

15.12%
63.02%
56.30%

Moat-Packing
Christofides
Blend

25.35%
22.53%
14.08%

60.56%
57.74%
56.33%

62.50%
89.58%
68.75%

56.25%
62.50%
64.58%

40.33%
49.57%
36.13%

58.82%
59.66%
59.66%

Table 9: (Left columns) The percentage of Real-World data for the 71 games with 10 locations
and 48 games with 20 locations where  is statistically significant (p < 0.05) between
the ranking induced by  SV and the ranking induced by  P ROXY . (Right columns) The
percentage of Real-World data for the 71 games with 10 locations and 48 games with
20 locations where the most expensive element according to the raking induced by  SV
matched the most expensive element in the ranking induced by  P ROXY . Higher is better
for both statistics.

604

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

Tables 8 and 9 give an indication of how the proxies perform in terms of ranking. A closer look
at Table 8 reveals the difference between  B LEND and  D EPOT . Again, comparing against the results
the Synthetic dataset shows that all proxies perform strictly worse on the Real-world data, except
 S HORT which manages to go from a negative list correlation on the Synthetic dataset to a (barely)
positive correlation on the Real-world dataset. Judging the performance of  we see that most of the
proxies are still recovering about 50% of the pairwise comparisons on the Real-World data. Again,
we also see the good performance of  R EROUTE on the ranking metric. Additionally, for games with
20 locations,  R EROUTE ,  C HRIS ,  M OAT , and  B LEND have about the same , with  C HRIS the best.
A review of Table 9 reveals that while the measure of  is lower overall, the majority of the
ranking correlations are still statistically significant for 20 location games. At first glance the proxies appears to not hold up when looking at only the top element. Every proxy sees decreased performance to  60% accuracy when selecting the top element, and  R EROUTE has the best performance,
followed by  B LEND and  C HRIS . Relaxing the notion of top (most costly) element as we did in the
Synthetic data, i.e., that the top element according to  SV is in the top 3 elements according to
 P ROXY ,  B LEND outperforms all other proxies (including  R EROUTE ) on the 20 location games with
93% accuracy, and comes within 3% of outperforming  R EROUTE on the entire corpus or Real-world
data with 79% accuracy.
In summary we see that the proxies perform worse in terms of both RMSE and  in the RealWorld dataset than they do on the Synthetic dataset. In all of our testing we see that  B LEND ,  D EPOT ,
and  C HRIS out perform the other proxies on the majority of measures. When comparing the proxies
against a variety of decision criteria including practical running time, overall numerical error, and
ranking performance,  B LEND emerges as the clear winner and overall most consistent performer on
both the Synthetic and Real-world data.

8. Related Work
The theory of cooperative games has a rich history in which various solution concepts for allocating
costs and other quantities have been proposed (Peleg & Sudholter, 2007; Young, 1994). In addition
to the Shapley value, other solution concepts include the core, the nucleolus and the bargaining set.
Of these, the Shapley value is considered the most important allocation scheme in cooperative
game theory (Winter, 2002).
Application of the Shapley value spans well beyond transportation setting. For example, the
Shapley value has been applied in allocating the cost of network infrastructure (Koster, 2009; Marinakis, Migdalas, & Pardalos, 2008), promoting collaboration between agents (Zlotkin & Rosenschein, 1994) by prescribing an allocation that incentivises agents to collaborate in the completion
of tasks, and as an incentive compatible way to share departmental costs in corporations (Young,
1985). Considering applications in networks more broadly, use of the Shapley value follows a general framework, where agents correspond to the nodes (or edges) of a graph (Curiel, 2008; Koster,
2009; Marinakis et al., 2008; Tijs & Driessen, 1986; Aziz & de Keijzer, 2014). Here the definition
of the characteristic function depends on the application domain, with proposed evaluations based
on: (i) the size of maximum matching, (ii) network flow, (iii) the weight of a minimum spanning
tree, and (iv) the weight of a Hamiltonian cycle (Curiel, 2008; Deng & Fang, 2008). Allocation
concepts are not solely devised and employed for allocating costs. For example, the Shapley value
has been used to measure the importance of agents in social networks (Moretti & Patrone, 2008),

605

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

and to measure the centrality of nodes in networks (Michalak, Aadithya, Szczepanski, Ravindran,
& Jennings, 2013).
Another solution concept that has been used to gauge the importance of agents is the Banzhaf
value (Banzhaf III, 1964). The Banzhaf value is defined for simple voting games  i.e. cooperative
games in which the value of the coalition is either zero or one  but the Banzhaf value of an agent
can suitably be extended to general cooperative games. However, even within the context simple
voting games, the Banzhaf value is more suitable for measuring the influence of an agent and less
suitable for allocate power between agents (Felsenthal & Machover, 1998). Since our focus is to
allocate costs, we focus on the Shapley value.
While solution concepts from the theory of transferable utility (TU) cooperative games (Peleg
& Sudholter, 2007; Chalkiadakis et al., 2011) have been used for allocations of costs, the Shapley
value has rarely received serious attention in the transportation science literature. The associated
computational cost is prohibitively high for the general case, and consequently strong notions of
fairness are often taken to be a secondary consideration. Though ApproShapley is an FPRAS (fully
polynomial-time randomized approximation scheme) for computing the Shapley value if the game
is convex (Liben-Nowell, Sharp, Wexler, & Woods, 2012), this does not apply for the domain considered in this work. The website Spliddit uses the Shapley value to split cab fares between up to 6
people (Goldman & Procaccia, 2014).
Other prominent TU game solution concepts are nucleolus and core. TSGs are introduced in
Potters (1992), where in addition to describing that game, the authors describe a variety of game
known as the routing game.9 For the latter an auxiliary constraint forces locations to be visited, in
any coalition, in the order they are traversed by a specific tour. Assuming that the tour corresponds to
the optimal for the underlying TSP, then the game has a non-empty core. Derks and Kuipers (1997)
presented a quadratic-time procedure for computing a core allocation of the routing game. They
also characterize suboptimal tours that specify routing games with non-empty cores. It should be
noted that there are no known tractable procedures to compute a tour which guarantees the core is
non-empty for the routing game. Conditions for the non-emptiness of the core in TSGs were further
developed by Tamir (1989). We have already noted that Faigle et al. (1998) developed a procedure
to calculate a multiplicative -core allocation for Euclidean TSGs. Yengin (2012) develop a notion
of a fixed route game with appointments which admits a tractable procedure for computing Shapley
values. That model is not suitable for typical scenarios that involve the delivery of goods to locations
from a depot. TU concepts in TSGs and routing games are developed for a practical gas delivery
application by Engevall et al. (1998).
Turning our attention to vehicle routing problems and transportation settings more generally,
Gothe-Lundgren, Jornsten, and Varbrand (1996) develop a column generation procedure to calculate the nucleolus of a homogeneous vehicle routing problem, i.e., all vehicles are equivalent.
In doing so they develop a procedure to determine if the core of that vehicle routing game is
empty. Engevall et al. (2004) extend that work for a very practical setting of distributing gas using a heterogeneous fleet of vehicles. More recently Ozener et al. (2013) examine a number of
solution conceptsincluding allocations derived according to the nested moat-packing of Faigle
et al. (1998), and a highly specialized approximation of the Shapley allocationin deriving cost
allocations for real-world inventory routing problems. They show that TU game allocations, espe9. Note that the journal publication of Potters et al. (1992) extends a technical report introducing the game as early as
1987.

606

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

cially core/duality-based allocations, have significant advantages over the existing cost allocations
which their industrial client was using.

9. Conclusions and Future Work
We studied the problem of fairly apportioning costs in transportation scenarios, specifically TSGs.
The Shapley value is an appealing division concept for this task as its axiomatic fairness properties
are ones most appreciated by our commercial partners. Since the Shapley value cannot be evaluated
in reasonable time, we considered a number of proxies for the Shapley value. We examined proxy
performance both in terms of their approximation quality with respect to the Shapley value and the
induced ranking of locations by Shapley value, a key question for operational and business concerns.
The stand-out proxies with respect to both measures as tested on Synthetic and Real-world data are
 C HRIS and  B LEND , a mixture of  D EPOT and  M OAT . However, when taking computation time into
account and the ability to scale to problems of commercial interest: around 30 locations per route
and over 600 total routes for a delivery day, only  B LEND remains feasible.
A key extensions of our work is the more general setting of vehicle routing games (VRPs). The
Shapley value would be useful to quantify the importance of location synergies that are unique to
the multi-vehicle model. The transport companies we interact with desire to understand the impact
of time windows (both the duration and position of allowable service times), and the effect of delivery frequency on allocated costs. Thus, a highly motivated and rich variety of problems is available
for future work. Additionally, future research should consider weighted Shapley values for situations where some coalitions (and therefore margins) are more likely to occur than others. Formal
approximation ratios, to complement the strong empirical evidence we obtained, is an important
subject for future research. There also remains the need for formal studies which employ proxy
allocations to inform solutions to hard optimisation problems in transportation domains. Finally,
scaling to larger transportation scenarios may require abstracting locations in a meaningful way.
An approximation approach that may be fruitful here was proposed by Soufiani, Charles, Chickering, and Parkes (2014), where agents are partitioned into groups and assigned weights within those
groups in a novel and effective way.

Acknowledgments
Data61/CSIRO (formerly known as NICTA) is funded by the Australian Government through the
Department of Communications and the Australian Research Council through the ICT Centre of
Excellence Program. Casey Cahan was supported by an Summer Research Scholarship at The Australian National University. Toby Walsh also receives support from the Asian Office of Aerospace
Research and Development (AOARD 124056) and the German Federal Ministry for Education and
Research through the Alexander von Humboldt Foundation.
We would like to thank Stefano Moretti and Patrice Perny from LIP6; Hossein Azari Soufiani
from Harvard University; David Rey and Vinayak Dixit from the rCiti Project at the University of
New South Wales School of Civil and Environmental Engineering, Tommaso Urli from Data61 and
ANU; and the reviewers and attendees of the 5th Workshop on Cooperative Games in MultiAgent
Systems (CoopMAS-2014) for their helpful feedback and comments on early version of this work.

607

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

References
Applegate, D. L., Bixby, R. E., Chvatal, V., & Cook, W. J. (2007). The traveling salesman problem:
a computational study. Princeton University Press.
Aziz, H., & de Keijzer, B. (2014). Shapley meets Shapley. In Proceeding of the 31st International
Symposium on Theoretical Aspects of Computer Science (STACS 2014), pp. 99111.
Bachrach, Y., Markakis, E., Resnick, E., Procaccia, A. D., Rosenschein, J. S., & Saberi, A. (2010).
Approximating power indices: theoretical and empirical analysis. Autonomous Agents and
Multi-Agent Systems, 20(2), 105122.
Banzhaf III, J. F. (1964). Weighted voting doesnt work: A mathematical analysis. Rutgers Law
Review, 19, 317343.
Bellman, R. (1962). Dynamic programming treatment of the travelling salesman problem. Journal
of the ACM (JACM), 9(1), 6163.
Bishop, C. M. (2006). Pattern recognition and machine learning. Springer.
Castro, J., Gomez, D., & Tejada, J. (2009). Polynomial calculation of the shapley value based on
sampling. Comput. Oper. Res., 36(5), 17261730.
Chalkiadakis, G., Elkind, E., & Wooldridge, M. (2011). Computational aspects of cooperative game
theory. Synthesis Lectures on Artificial Intelligence and Machine Learning, 5(6), 1168.
Christofides, N. (1976). Worst-case analysis of a new heuristic for the travelling salesman problem..
Tech. rep., DTIC Document.
Conitzer, V., & Sandholm, T. (2006). Complexity of constructing solutions in the core based on
synergies among coalitions. Artificial Intelligence, 170(6), 607619.
Cook, W. J., Cunningham, W. H., Pulleylank, W. R., & Schrijver, A. (1998). Combinatorial Optimization. John Wiley & Sons, Inc.
Corder, G. W., & Foreman, D. I. (2009). Nonparametric statistics for non-statisticians: a step-bystep approach. Wiley.
Cornuejols, G., Naddef, D., & Pulleyblank, W. (1985). The traveling salesman problem in graphs
with 3-edge cutsets. Journal of the ACM, 32(2), 383410.
Curiel, I. (2008). Cooperative combinatorial games. In Chinchuluun, A., Pardalos, P., Migdalas, A.,
& Pitsoulis, L. (Eds.), Pareto Optimality, Game Theory And Equilibria, Vol. 17 of Springer
Optimization and Its Applications, pp. 131157. Springer New York.
Deng, X., & Fang, Z. (2008). Algorithmic cooperative game theory. In Chinchuluun, A., Pardalos,
P. M., Migdalas, A., & Pitsoulis, L. (Eds.), Pareto Optimality, Game Theory And Equilibria,
Vol. 17 of Springer Optimization and Its Applications. Springer-Verlag.
Derks, J., & Kuipers, J. (1997). On the core of routing games. International Journal of Game
Theory, 26(2), 193205.
Engevall, S., Gothe-Lundgren, M., & Varbrand, P. (1998). The traveling salesman game: An application of cost allocation in a gas and oil company. Annals of Operations Research, 82,
203218.

608

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

Engevall, S., Gothe-Lundgren, M., & Varbrand, P. (2004). The heterogeneous vehicle-routing game.
Transportation Science, 38(1), 7185.
Faigle, U., & Kern, W. (1993). On some approximately balanced combinatorial cooperative games.
ZOR Methods and Models of Operations Research, 38(2), 141152.
Faigle, U., Fekete, S., Hochstattler, W., & Kern, W. (1998). On approximately fair cost allocation
in euclidean tsp games. Operations-Research-Spektrum, 20(1), 2937.
Fatima, S. S., Wooldridge, M., & Jennings, N. R. (2007). A randomized method for the Shapley
value for the voting game. In Proceedings of the 6th International Conference on Autonomous
Agents and Multiagent Systems (AAMAS 07), pp. 157165, New York, New York, USA.
Fatima, S. S., Wooldridge, M., & Jennings, N. R. (2008). A linear approximation method for the
Shapley value. Artificial Intelligence, 172(14), 16731699.
Felsenthal, D. S., & Machover, M. (1998). The Measurement of Voting Power: Theory and Practice,
Problems and Paradoxes. Edward Elgar Cheltenham.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory of
NP-Completeness. New York: W.H. Freeman.
Golden, B. L., Raghavan, S., & Wasil, E. A. (2008). The Vehicle Routing Problem: Latest Advances
and New Challenges: latest advances and new challenges, Vol. 43. Springer.
Goldman, J., & Procaccia, A. D. (2014). Spliddit: Unleashing fair division algorithms. Journal of
the ACM, 13(2), 4146.
Gothe-Lundgren, M., Jornsten, K., & Varbrand, P. (1996). On the nucleolus of the basic vehicle
routing game. Mathematical Programming, 72(1), 83100.
Held, M., & Karp, R. M. (1962). A dynamic programming approach to sequencing problems.
Journal of the Society for Industrial & Applied Mathematics, 10(1), 196210.
Ieong, S., & Shoham, Y. (2005). Marginal contribution nets: a compact representation scheme for
coalitional games. In Proceedings of the 6th ACM conference on Electronic Commerce (EC
06), pp. 193202.
Kendall, M. G. (1938). A new measure of rank correlation. Biometrika, 30(1/2), 8193.
Kilby, P., & Verden, A. (2011). Flexible routing combing constraint programming, large neighbourhood search, and feature-based insertion. In 2nd Workshop on Artificial Intelligence and
Logistics. Barcelona, Spain.
Kimms, A., & Kozeletskyi, I. (2015). Shapley value-based cost allocation in the cooperative traveling salesman problem under rolling horizon planning. EURO Journal on Transportation and
Logistics, 122.
Koster, M. (2009). Cost Sharing. Springer-Verlag New York.
Leech, D. (2003). Computing power indices for large voting games. Management Science, 49(6),
831837.
Liben-Nowell, D., Sharp, A., Wexler, T., & Woods, K. (2012). Computing shapley value in supermodular coalitional games. In 18th International Conference on Computing and Combinatorics (COCOON 2012), pp. 568579.

609

fiA ZIZ , C AHAN , G RETTON , K ILBY, M ATTEI , & WALSH

Maleki, S., Tran-Thanh, L., Hines, G., Rahwan, T., & Rogers, A. (2013). Bounding the estimation error of sampling-based shapley value approximation with/without stratifying. CoRR,
abs/1306.4265.
Mann, I., & Shapley, L. S. (1960). Values for large games IV: Evaluating the electoral college by
monte carlo. Technical report, The RAND Corporation, Santa Monica, CA, USA.
Mann, I., & Shapley, L. S. (1962). Values for large games IV: Evaluating the electoral college
exactly. Technical report, The RAND Corporation, Santa Monica, CA, USA.
Marinakis, Y., Migdalas, A., & Pardalos, P. M. (2008). Cost allocation in combinatorial optimization
games. In Chinchuluun, A., Pardalos, P., Migdalas, A., & Pitsoulis, L. (Eds.), Pareto Optimality, Game Theory And Equilibria, Vol. 17 of Springer Optimization and Its Applications,
pp. 217244. Springer New York.
Michalak, T. P., Aadithya, K. V., Szczepanski, P. L., Ravindran, B., & Jennings, N. R. (2013).
Efficient computation of the Shapley value for game-theoretic network centrality. Journal of
Artificial Intelligence Research, 46, 607650.
Moretti, S., & Patrone, F. (2008). Transversality of the Shapley value. TOP, 16(1), 141.
Natrella, M., Croarkin, C., & Guthrie, W. (2012). NIST/SEMATECH e-Handbook of Statistical
Methods. U.S. Department of Commerce. URL: http://www.itl.nist.gov/div898/handbook/.
Owen, G. (1972). Multilinear extensions of games. Management Science, 18(5-part-2), 6479.
Ozener, O. O., Ergun, O., & Savelsbergh, M. (2013). Allocating cost of service to customers in
inventory routing. Oper. Res., 61(1), 112125.
Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley Publishing Company, Inc.
Papapetrou, P., Gionis, A., & Mannila, H. (2011). A Shapley value approach for influence attribution. In Proceedings of the 2011 European Conference on Machine Learning and Principles
of Knowledge Discovery in Databases (ECML PKDD 2011), pp. 549564. Springer.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M.,
Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research, 12, 28252830.
Peleg, B., & Sudholter, P. (2007). Introduction to the Theory of Cooperative Games. Springer.
Potters, J. A., Curiel, I. J., & Tijs, S. H. (1992). Traveling salesman games. Mathematical Programming, 53(1-3), 199211.
Ropke, S., & Pisinger, D. (2006). An adaptive large neighborhood search heuristic for the pickup
and delivery problem with time windows. Transportation Science, 40(4), 455472.
Sahni, S., & Gonzalez, T. (1976). P-complete approximation problems. Journal of the ACM, 23(3),
555565.
Shapley, L. S. (1953). A value for n-person games. In Kuhn, H., & Tucker, W. W. (Eds.), Contributions to the Theory of Games, Vol. 2 of Annals of Mathematical Studies. Princeton University
Press.

610

fiA S TUDY OF P ROXIES FOR S HAPLEY A LLOCATIONS OF T RANSPORT C OSTS

Soufiani, H. A., Charles, D. J., Chickering, D. M., & Parkes, D. C. (2014). Approximating the shapley value via multi-issue decomposition. In Proceedings of the 13th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 14), pp. 12091216.
Tamir, A. (1989). On the core of a traveling salesman cost allocation game. Operations Research
Letters, 8(1), 3134.
Tijs, S. H., & Driessen, T. S. H. (1986). Game theory and cost allocation problems. Management
Science, 32(8), 10151028.
Winter, E. (2002). The Shapley value. In Handbook of Game Theory with Economic Applications,
chap. 53, pp. 20252054. Elsevier.
Yengin, D. (2012). Appointment games in fixed-route traveling salesman problems and the Shapley
value. International Journal of Game Theory, 41(2), 271299.
Young, H. P. (1985). Producer incentives in cost allocation. Econometrica, 53(4), 757765.
Young, H. P. (1994). Cost allocation. In Handbook of Game Theory with Economic Applications,
Vol. 2, pp. 11931235. Elsevier B.V.
Young, H. P. (1985). Monotonic solutions of cooperative games. International Journal of Game
Theory, 14(2), 6572.
Zlotkin, G., & Rosenschein, J. S. (1994). Coalition, cryptography, and stability: Mechanisms for
coalition formation in task oriented domains. In Proceedings of the 12th National Conference
on Artificial Intelligence (AAAI 1994), pp. 432437.

611

fiJournal of Artificial Intelligence Research 56 (2016) 657-691

Submitted 01/16; published 08/16

Engineering Note
The IBaCoP Planning System: Instance-Based Configured Portfolios
Isabel Cenamor
Tomas de la Rosa
Fernando Fernandez

ICENAMOR @ INF. UC 3 M . ES
TROSA @ INF. UC 3 M . ES
FFERNAND @ INF. UC 3 M . ES

Departamento de Informatica, Universidad Carlos III de Madrid
Avda. de la Universidad, 30. Leganes (Madrid). Spain

Abstract
Sequential planning portfolios are very powerful in exploiting the complementary strength
of different automated planners. The main challenge of a portfolio planner is to define which
base planners to run, to assign the running time for each planner and to decide in what order they
should be carried out to optimize a planning metric. Portfolio configurations are usually derived
empirically from training benchmarks and remain fixed for an evaluation phase. In this work, we
create a per-instance configurable portfolio, which is able to adapt itself to every planning task.
The proposed system pre-selects a group of candidate planners using a Pareto-dominance filtering
approach and then it decides which planners to include and the time assigned according to predictive
models. These models estimate whether a base planner will be able to solve the given problem and,
if so, how long it will take. We define different portfolio strategies to combine the knowledge
generated by the models. The experimental evaluation shows that the resulting portfolios provide
an improvement when compared with non-informed strategies. One of the proposed portfolios was
the winner of the Sequential Satisficing Track of the International Planning Competition held in
2014.

1. Introduction
Planning is a process that chooses and organizes actions by anticipating their outcomes with the
aim of achieving some pre-stated objectives. In Artificial Intelligence, Automated Planning (AP) is
the computational study of this deliberation process (Ghallab, Nau, & Traverso, 2004). Automated
planners are systems that, regardless of the application domain, are able to receive a declarative
representation of an environment, an initial state and a set of goals as input. The output is a synthesized plan that will achieve these goals from the initial situation. In this context, the International
Planning Competition (IPC) is an excellent initiative to foster the studying and development of automated planning systems. IPC was created in 1998 to set a common framework for comparing
automated planners.
Different planning systems won awards in previous IPCs. However, one of the main invariants of
the competition is that there is no single planner which is always the best planner (or at least equal)
for every domain or every problem. This means that, although there is a planner which, following the
quality metrics of the competition, can be considered the best, we can always find some problems in
different domains in which other planners outperform the overall winner. Therefore, we can assume
that the AP community has generated a set of single planners that are better than all others in specific
situations. For this reason, discarding a priori any of those solvers seems meaningless.
c
2016
AI Access Foundation. All rights reserved.

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

In fact, the idea of reusing a set of individual or base systems to generate more accurate solutions than those obtained separately is not new in Artificial Intelligence. For instance, in Machine
Learning, meta-classifiers use different base classifier to increase the coverage of the representation
bias of the resulting classifier (Dietterich, 2000). In problem solving, portfolios of search algorithms
have also demonstrated that they can outperform the results of a single search strategy (Xu, Hutter,
Hoos, & Leyton-Brown, 2008; Xu, Hoos, & Leyton-Brown, 2010; Malitsky, Sabharwal, Samulowitz, & Sellmann, 2013). For example, the SAT competition in 2013 included a special track on
portfolios. In the automated planning community, planner portfolios have also been subject to great
deal of interest. In IPCs from 2006 to 2014, portfolio approaches won or were very close to winning
the tracks in which they took part.
However, although the use of portfolios has become usual in the community, there is still no
agreement as to what a planning portfolio is (Vallati, Chrpa, & Kitchin, 2015). In this work, we
assume that a portfolio of planners is a set of base planners with a selection strategy. This selection
strategy is what generates a specific portfolio configuration, whose goal is to maximize the performance metrics. Therefore, a configuration has to define three main elements: (1) which sub-set of
planners to run, (2) how long to run each planner? and (3) in which order. There are many
techniques to configure a planning portfolio (Vallati, 2012), and depending on how accurate they
are, the chances of selecting the best planner in a given situation will increase. Note that, in this
definition, if a planner has different configuration parameters which modify its behavior, each parameterization is considered a different base planner, so base planners can be considered as black
boxes.
The number of planners in the state of the art is huge, so a first filtering is to select the minimum
number that ensures the best performance is achieved, for each evaluated planning domain (or even
for each problem in each domain). Obviously, good results in current domains do not ensure good
results in new domains but, as will be shown, it is a good estimator. In this sense, a Pareto efficiencybased approach (Censor, 1977) to reduce the number of planners that we consider eligible for a
planning portfolio is presented. However, we will show that with this mechanism, the first of the
aforementioned questions can only be answered partially since the number of candidate planners
might still be large.
So the best solution to the portfolio configuration problem is to have an oracle that predicts,
given a domain and a problem, which planner will obtain the best performance and how long it will
take. Given that we do not have this oracle, in this work we propose the use of predictive models, automatically generated with Machine Learning and Data Mining techniques. These models summarize the results of all the candidate planners from the past: whether they were able to solve planning
problems, as well as the time that they required to generate a good solution (Cenamor, de la Rosa,
& Fernandez, 2012, 2013). Given this knowledge on the past, the inductive hypothesis gives also us
an estimation on how they will behave in future planning domains and with different problems, so
the order in which the planners are implemented can be given by the accuracy of these predictions.
Therefore, with these predictive models, we are able to configure a portfolio for each planning problem, like in previous works on the use of portfolios in search (Gomes & Selman, 2001). This is a
renewed idea in automated planning since recent works have focused in static (Helmert, 2006) or
domain-specific portfolios (Gerevini, Saetti, & Vallati, 2009, 2014), in which the configuration of
the portfolio is fixed for all the domains or chosen for each one respectively.
IBAC O P (Instance-based Configured Portfolio) is a family of planning portfolios that were built
for competing in IPC-2014. In this article we first present IBAC O P as a general framework with
658

fiT HE IBAC O P P LANNING S YSTEM

the ultimate goal of building per-instance configurable portfolios. The technique can be reproduced
again whenever new automated planners or new planning benchmarks arise. Then, we describe how
to build different version of IBAC O P following the defined processes. One of these versions was the
winner of the Sequential Satisficing Track of IPC-2014. We also include the results of an empirical
study that confirms the good performance of IBAC O P planners when compared to different base
planners and different portfolio configuration strategies. Then, we summarize the related work, and
finally, the last section sets out the conclusions and future lines of research.

2. System Architecture
In this section, we present the general idea of building a planning portfolio that can be configured
for a particular planning task using predictive models. This process should be seen as a general
technique given that the inputs (planners and benchmarks) might change in the future due to progress
in the planning community, so new portfolio configurations can be generated through the use of
these new inputs.
2.1 Portfolio Construction
We consider that the construction of an instance-based planning portfolio comprises three main
parts. (1) Planner filtering, for making a pre-selection of good candidate planners from the set of
known or available planners. The proposed pre-selection technique is based on a multi-criteria approximation. This is a previously unexplored technique for selecting a set of planners that provides
enough diversity in the planner portfolio. (2) Performance modeling, for providing predictors of
the planners behavior as a function of planning task features. In our research, we include a set of
well-known features (Cenamor et al., 2012), some of which are built into the preprocessing step
of FAST D OWNWARD (Helmert, 2006). We also take advantage of both the output information in
the translation process (Fawcett, Vallati, Hutter, Hoffmann, Hoos, & Leyton-Brown, 2014) and the
heuristic values computed in the first step of the search process of FAST D OWNWARD. In addition,
the use of several totally new features on the characteristics of the relaxed plan in the initial state is
proposed. Finally, (3) strategy selection: to establish a procedure that combines the performance
predictions and then to output a portfolio configuration. We propose a novel strategy selection to
exploit the effectiveness of the predictive models. Next, we explain the details of each of these
construction steps.
2.1.1 P LANNER F ILTERING
The planner filtering process consists of the pre-selection of good candidate base planners from a
larger amount of available planners. Even though there is a sufficient evidence that there is not
an overall best planner across a variety of benchmarks, it can be verified empirically that there
is a dominance of some planners over others. Therefore it does not make sense to include, as base
planners, those that are always worse in terms of performance metrics. We want this filtering process
to select a diverse, but small, subset of planners to have few elements among which to divide the
available execution time.
In this work, we propose a multi-criteria pre-selection mechanism that focuses in two IPC metrics (quality and time) as alternative to the most extended ones for planner filtering. For example,
FDSS (Helmert, Roger, Seipp, Karpas, Hoffmann, Keyder, Nissim, Richter, & Westphal, 2011)
659

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

uses the selection of planners that maximizes the coverage; MIP LAN (Nunez, Borrajo, & Linares
Lopez, 2015) uses the portfolio configuration that obtains the best achievable performance in terms
of score.
For filtering we propose to run the candidate planners on a representative set of benchmarks
and then to evaluate them in terms of time and quality. To consider both metrics we propose an
approach based on Pareto-efficiency (Censor, 1977) that allows us to determine the dominance
between planners in a multi-criteria fashion. In particular, we select a planner as a candidate for the
portfolio if it is the best planner for at least one domain in terms of the IPC-2011 multi-criteria QT
score (Linares Lopez, Celorrio, & Olaya, 2015). Briefly, for a single problem, this metric computes
the tuple hQ, T i for each planner, where Q is the quality of the planners best solution and T the
time used to find this solution. Then, for a given planner, p, the dominance relations between p and
the rest of planners are computed.
A tuple hQ, T i Pareto-dominates the tuple hQ , T  i if and only if Q  Q and T < T  . Planner
p gets NN points, where N is the number of tuples where p Pareto-dominates another planner, and
N  is the number of different tuples in which planner p appears. Finally, the QT-Pareto score for
a domain is a sum of points achieved in all the problems in the domain. The idea of this selection
mechanism is as follows: if a planner shows good dominance property in a given domain, it should
be included in the portfolio because it will be a good candidate for solving the problems of the same
domain or even other planning tasks that have similar characteristics. Therefore, a simple strategy
to filter a first pool of planners is given by the procedure that selects only the planners with the
maximum QT-Pareto score for at least one domain. We refer to this procedure as QT-Pareto Score
Filtering.
2.1.2 P ERFORMANCE M ODELING
Given a planning task, we want to predict how the selected base planners will perform in order
to decide whether to include them or not and to make a good assignment of time and ordering
when configuring the portfolio. Thus, modeling the planner behavior as a function of planning
task features becomes a key process in building instance-based portfolios. To learn these predictive
models we follow a Data Mining approach, as shown in Figure 1. In our case, we start from a set of
candidate planners and a set of planning benchmarks. The output of the process is the set of models
that will predict the performance of the candidate planners. We have defined the data mining goal
as the creation of two predictive models. First, whether a planner will be able to solve a problem
(i.e. a classification task) and, if so, what will be the time required to compute the best plan (i.e., a
regression task).
The first step of the mining process comprises the generation of training and test datasets. On
the one hand, the planners are run on the set of benchmarks to obtain their performance data. This
data includes the outcome of the execution (success or failure) and, for the positive cases, the time
elapsed in finding the best solution. On the other hand, planning tasks are processed to extract a set
of features that characterize them. These features are an extended set of the previously proposed
set (Cenamor et al., 2013). According to the mechanism for generating these features, we classify
them into the following categories:
 PDDL features: Basic features extracted from the PDDL representation of the domain and
problem files, for instance, the number of actions, objects or goals.
660

fiT HE IBAC O P P LANNING S YSTEM

Figure 1: General Diagram for Learning the Planning Performance Predictive Models
 FD Instantiation features: the Fast-Downward pre-processor instantiates and translates the
planning tasks into a finite domain representation (Helmert, 2009). From this output we take
some general information such as the number of instantiated actions or the number of relevant
facts, and data specific to the FD-translator, such as the number of auxiliary atoms.
 SAS+ features: The finite domain representation of SAS+ has an associated Causal Graph
(CG) and a set of Domain Transition Graphs (DTGs). From CG we extract basic properties
(e.g., number of variables and edges), and the ratios between these properties. As regards
DTGs, the number of graphs in a problem corresponds to the number of edges in the CG,
which makes it difficult to encode the general attributes for each DTG. Therefore, we summarize the DTGs characteristics by aggregating the relevant properties of all graphs. Thus,
features from DTGs are statistics on them such as the maximum, the average or the standard
deviation of their graph properties.
 Heuristic features: For the initial state, we compute heuristic values using a set of widely-used
unit cost heuristic functions (e.g., hmax , hFF ,. . . ). We compute these heuristics only for the
initial state, which can be obtained at a reasonable cost. We use only unit cost heuristics to
obtain a domain-independent estimation that helps in the characterization of the problem size
and/or difficulty.
 Fact Balance Features: Using the relaxed plan (RP ) of the initial state, extracted when computing the hFF heuristic, we also compute a set of features to represent the fact balance of the
RP . We define the fact balance for fact p, as the number of times that p appears as an added
effect of an action belonging to RP , minus the number of times that p is a deleted effect of
an action in RP , considering original actions where deletes are not ignored. The intuition
behind fact balances is that high positive values would characterize easier (relaxed) problems
for a given domain, since achieved facts do not need to be deleted many times. Given that the
number of relevant facts of a planning task is variable, we compute statistics (i.e., min, max,
average and variance) for the fact balance of the relevant facts. Additionally, we compute
statistics only by considering facts that are goals, following the same procedure.
661

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

The complete set of 89 features is listed and organized by their category in Appendix A. The
Data Integration process in Figure 1 receives the features and the performance datasets as inputs
to produce a final dataset according to the modeling goal. In the dataset for the classification task,
a training/test instance includes the planning task features plus the planner name and the Boolean
feature indicating whether this planner solved the planning task. The dataset for the regression task
only includes the cases in which the planning tasks are solved. We make this exclusion because it
does not make sense to model or estimate the planning time beyond the given time limit and because
in most cases this time is unknown. A training/test instance in the regression dataset includes the
planning task features, the planner name and the time this planner used to find its best solution.
The Feature Selection is an optional process for reducing the number of features used for the
modeling. This procedure is applied because there might be irrelevant or redundant features that
could degrade the modeling capabilities of some learning techniques (Blum & Langley, 1997). The
outcome of the process is dependent on the original data. Thus, the decision of whether to apply it
or not is taken based on the results of the model evaluation.
For the Modeling process, we use an off-the-shelf data-mining tool that provides a set of learning
algorithms for both classification and regression. The generated models are then evaluated in the
Evaluation process to determine the best model for the classification and regression tasks. There
are many different ways of carrying out the model evaluation and comparison (Han, Kamber, & Pei,
2011; Witten & Frank, 2005), which will reflect the generalization ability of the different models
when making predictions of unseen data.
2.1.3 S TRATEGY S ELECTION
The strategy selection is the final step in the construction of an IBAC O P planner. Selecting a strategy implies that we have to decide how to transform the predictions of the best models into an actual
portfolio configuration. There are several alternatives that range from ignoring both model predictions to trusting them completely. For the classification model, each candidate planner will get a
yes/no prediction given a new planning task. The direct use of the Boolean variable makes difficult
to decide which planners to include in the portfolio. Consider, for instance. the two extreme cases:
(1) If all planners get a positive prediction, should we include all of them? (2) If all planners get a
negative prediction, which planner should we include in the portfolio? Instead of using the Boolean
prediction we propose to rank the predictions by their confidence in the positive class, and then
make the selection of planners according to this ranking. Then, each planner should be assigned a
slide of the total time, in which this assignment can be carried out uniformly or dependently, again,
from the predictive models learned. Therefore, depending on the use that we make of the predictive
models, we propose three basic strategies:
1. Equal Time for all (ET): This strategy does not use the predictive models at all. It will assign
equal time for each planner (uniform strategy). The idea behind this strategy is to have more
planners but with less time for each one. This strategy has obtained good results in other
portfolios (Seipp, Braun, Garimort, & Helmert, 2012).
2. Best N confidence (BN): This strategy will include the subset of N planners with the best
prediction confidence in the positive class in the portfolio. Then, they get equal time for
solving the planning task. In this case, the idea is that we select a subset of promising planners
so they can spend more time in solving the planning task.
662

fiT HE IBAC O P P LANNING S YSTEM

3. Best N Estimated Time (BNE): The subset of planners is selected as mentioned before, but now
the time is assigned proportionally to the estimated time provided by the regression model.
2.2 Portfolio Configuration
An instance-based configuration of a portfolio implies that the subset of base planners and the time
assigned to each one varies as a function of the planning task features. The set of candidate planners,
the predictive models and the configuration strategy are previously fixed in the construction phase.
Algorithm 1 shows how to use these components to configure the portfolio for a given planning
task.
Algorithm 1: Algorithm for configuring the portfolio for a particular planning task.
Data: Problem (), Domain (d), Set of base planners (Pini ), Classification model (C),
Regression model (R), Available time (T ), Strategy (SN )
Result: Portfolio Configuration: A sequence of planners with their assigned runtime,
Portfolio = [hp1 , t1 i, . . . , hpc , tc i]
Portfolio=[];
if SN == ET then
/*(No classification nor regression models available)*/
n = size(Pini );
for p in Pini do
append(hp, Tn i, Portfolio);
else
hF, tF i = extractFeatures(d, );
for pk in Pini do
predictionhpk , confk i  predict (C, hF, pk i);

sorted candidates  sort(prediction, key = conf  );
p  sorted candidates[i . . . N ];
if SN == BN then
/*Classification model available, applying Best N confidence strategy*/
for i = 1 to N do
F
append(hpi , T t
N i, Portfolio);
else
/*Regression model available, applying Best N Estimated Time*/
for i = 1 to N do
ti = predict time(R, hF, pi i);
t = scaleTime(t, T  tF );
for i = 1 to N do
append(hpi , ti i, Portfolio) ;

The method receives a problem (), a domain (d), the set of base planners (Pini ), the classification model (C), the regression model (R), the time available (T ) and the portfolio configuration
strategy (SN  {ET, BN, BN E}). The procedure calls several functions described below:
663

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

 extractFeatures: This is the same feature extraction procedure used in the portfolio construction phase. From the pair (domain, problem) the function outputs the set of features F . This
function also computes the time (tF ) as the time spent in extracting all features.
 predict: This function is a query to the classification model C. It receives a new instance
represented by the tuple hF, pi, where F is the previously computed features, and p is the
planner name. From the result of the function we ignore the class, and only keep the prediction
confidence of the positive class, forming the tuple hp, conf  i. This output represents the
confidence that the planner p will solve the problem.
 predict time: This function uses the model R to estimate the execution time for the subset
of planners PN  Pini that has been established as the best N candidates in terms of classification confidence. As in the classification model, this function receives the input tuple
hF, pi.
 scaleTime: This function transforms the vector of estimated times into another proportional
vector for which its sum fits in the available time, which is the original time bound T minus the
time used to compute the features tF . Thus, the time t assigned to each planner is computed
F )t
with the formula t = (TPt
N
i=1 ti

The output of the algorithm is a sequence of planners and their assigned time. The execution of
a particular configuration of the portfolio comprises the sequential execution of these base planners
ensuring that each CPU process does not exceed the assigned time.

3. IBaCoP Planning System
In this section we describe how we follow the approach presented in Section 2 to build different
portfolios.
3.1 Candidate Planners
The initial set of planners includes the 27 planners of the Sequential Satisficing Track of IPC-2011
plus LPG- TD (Gerevini, Saetti, & Serina, 2006). Although LGP- Td did not compete in IPC-2011
we considered worthwhile to include it because it is still considered a state-of-the-art planner due to
its great performance in previous competitions.
The first step is to apply the QT-Pareto Score Filtering described in subsection 2.1.1 to reduce
the initial set of candidate planners. The benchmarks for computing the QT-Pareto Score is the set
of domains and problems of the Sequential Satisficing Track of IPC-2011.
Table 1 shows the best planner in terms of QT-Pareto score for each domain. Additionally,
we include the number of problems solved by the best planner to highlight the correlation among
both values. The QT-Pareto score values closer to 20 reflect that the planner is able to beat the
other planners in most problems. P ROBE was the best planner in 4 domains. However the other
planners only stood out in one domain. This reinforces the motivation to find a diverse subset of
planners. Finally, out of 28 initial planners, the QT-Pareto score filtering pre-selected as candidate
planners the subset of 11 planners, which was made up of: LAMA -2011, PROBE , ARVAND , FDSS 2, FD - AUTOTUNE -1, FD - AUTOTUNE -2, LAMAR , LAMA -2008, MADAGASCAR , YAHSP 2- MT and
LPG- TD. A brief description of these planners can be found in Appendix D.
664

fiT HE IBAC O P P LANNING S YSTEM

Planner
PROBE
PROBE
PROBE
PROBE
ARVAND
MADAGASCAR
LAMA -2008
LAMA -2011
FD - AUTOTUNE -1
FD - AUTOTUNE -2
FDSS -2
LAMAR
YAHSP 2- MT
LPG- TD

Domain
scanalyzer
woodworking
tidybot
barman
pegsol
parcprinter
transport
openstacks
sokoban
nomystery
elevators
parking
visitall
floortile

total

QT
16.59
18.55
16.77
19.42
18.88
17.63
17.84
17.30
17.56
16.73
17.84
18.12
18.74
11.96
243.77

Coverage
20
20
18
20
20
20
19
20
19
19
20
20
20
12
267

Table 1: List of the best planners ordered by their QT-Pareto score for each domain of IPC-2011.
Table 2 shows the ranking of planners of the IPC results (i.e., planner ordering established by
the quality score) (Linares Lopez et al., 2015) and which of them were selected by QT-Pareto Score
Filtering. It is worth noting of attention that 10 of the 11 best planners in the IPC are built on top
of FD, which reduces the diversity of the planners. However, the QT-Pareto Score Filtering only
includes 8 of them. In addition, it should be pointed out that the last three selections of the QT-Pareto
Score Filtering are planners from the lower positions of the table which, as will be demonstrated
later, increases the diversity of the portfolio and its performance.
Ranking
1
2
3
4
5
6
7
8
9
10
11
17
22
24

planner
LAMA -2011
FDSS -1
FDSS -2
FD - AUTOTUNE -1
ROAMER
FORKUNIFORM
FD - AUTOTUNE -2
PROBE
ARVAND
LAMA -2008
LAMAR
YAHSP 2- MT
MADAGASCAR
LPG- TD

Eligible












FD











Table 2: List of 11 best planners ordered by its score at IPC-2011. The third column shows whether
they are selected by the QT-Pareto Score Filtering. The forth column shows if the planners
are built on the top of FD.

665

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

3.2 Performance Models
The inputs to the performance modeling phase are the candidate planners (i.e., the 11 candidates
selected in the previous section) and the benchmark planning tasks selected for this purpose. Next,
we describe the generated training data, and then how these inputs produce specific instances of
IBAC O P planners.
3.2.1 T RAINING DATA
The training data for the learning process requires a set of domains and problems used to gather
the input features. We need a wide range of domains and problems to generalize future unknown
planning tasks properly. We have included the planning problems available from IPC-2006 onwards. If we do not mention the test set explicitly, it will always refer to the satisficing tracks of the
competitions. The included domain and problems are:
 IPC-2006: openstacks, pathways, rovers, storage, tpp and trucks.
 IPC-2008: cybersec, elevators, openstacks, pegsol, pipesworld, scanalyzer, sokoban, transport and woodworking.
 IPC-2011: barman, elevators, floortile, nomystery, visitall, tidybot, openstacks, parcprinter,
parking, pegsol, sokoban, scanalyzer, transport and woodworking.
 Learning track IPC-2008: gold-miner, matching-bw, n-puzzle, parking, thoughful and sokoban.
 Learning track IPC-2011: barman, blockworld, depots, gripper, parking, rovers satellite,
spanner and tpp.
From this list we obtained 45 different domain descriptions. Although some of them represent
alternative encodings of the same domain, all have been included. Candidate planners were run on
these benchmarks to obtain the features related to the performance of the planners. Thus, we used a
total of 1, 251 planning tasks. The performance data comprises 13, 761 instances (i.e., 1, 251 problems  11 planners) where 8, 697 were successful and 5, 394 failed. The proportion of instances
solved by each candidate planner is different. Table 16 in Appendix C shows a per-planner summary
of the performance data.
The 89 features representing each planning task are automatically generated from the domain
and problem definitions. The PDDL features, FD instantiation and SAS+ features are computed
using the FAST-D OWNWARD pre-processor. The computation time needed to extract these features
is negligible compared to the SAS+ translation, given that we only compute sums and statistics on
the data provided by the SAS+ representation. The heuristic features are computed using the FASTD OWNWARD search engine, and fact balance features are generated using the relaxed planning
graph structures (of the initial state) provided by the FF planner (Hoffmann, 2003). The FASTD OWNWARD pre-processor could fail when instantiating a planning task. In which case, regarding
features are not computed and missing values are assumed.
Table 3 shows the success rate for extracting the features of each type from the training problems, and the average and maximum time in seconds to extract them. The PDDL, FD and SAS+
features are extracted from the FD pre-processor which is why they have the same success rate. The
time required to compute the heuristic features is only the time for calculating the heuristic value of
the initial state, which is calculated only if the FD pre-process has finished successfully.
666

fiT HE IBAC O P P LANNING S YSTEM

Class
PDDL
FD
SAS+
Heuristic
Fact Balance
Total

Success
97%
97%
97%
87.54%
93%
-

Average (s.)
6.97
52.73
22.60
20.20
5.20
107.7

Max (s.)
46.00
141.40
60.60
30.50
21.20
299.7

# features
8
16
50
8
7
89

Table 3: Summary of the extracted features with the average and maximum time in seconds (s.) to
extract them. These processes are on the top of the two first step of the all planners based
on FD.

3.2.2 F EATURE S ELECTION
We have carried out a feature selection process for two main reasons. On the one hand, some features
might be irrelevant whilst others might be redundant for the modeling purpose. Therefore we want
to analyze whether it is possible to obtain better models using only a subset of the available features.
On the other hand, this study will allow us to recognize most relevant features for characterizing a
planning task.
The feature selection was carried out using J48 algorithm, a top-down induction algorithm to
build decision trees (Quinlan, 1993), by selecting the features that appear in the top nodes of the
tree (Grabczewski & Jankowski, 2005). Decision trees make an implicit feature selection as the
model includes queries to those features considered relevant. After applying this feature selection
process on the feature dataset, the total number of features decreased from 89 to 34. This leads
to a dataset size reduction of around 62%. Table 4 contains the list of features resulting from the
feature selection process. The selection chooses features from all categories. For the modeling and
evaluation process we kept both datasets separate, one with all available features (f-all) and the other
one with the selected features (f-34).
3.2.3 C LASSIFICATION M ODELS
We have trained the classifiers using 31 classification algorithms provided by Weka (Witten & Frank,
2005), which includes different model types such as decision trees, rules, support vector machines
and instance based learning. We recall that training instances include the planning task features
described in Section 2.1.2 plus the planner name and the Boolean feature indicating whether this
planner solved the planning task or not. The performance of the predictive models was evaluated
with a 10-fold cross-validation on a uniform random permutation of all training data. The best
model for both datasets f-all and f-34 was that generated by Rotation Forest (Rodriguez, Kuncheva,
& Alonso, 2006), achieving 93.39 and 92.35% of accuracy respectively. These results are quite
better than the result of the default model (ZeroR), which obtained 61.72% of accuracy. See all the
results of the classification models in Table 14 of Appendix B.
Even though a good accuracy in the classification model does not guarantee a good performance
of the portfolio, this result is a great starting point for selecting promising planners. The accuracy
results of the feature selection only showed small differences compared to results obtained with all
667

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

Type

PDDL
(4)

CG & DTG
(11)

Features
types
goal
objects
functions

numberVariablesCG
inputEdgeCGStd
outputEdgeCGAvg
outputWeightCGMax
outputWeightCGAvg
outputEdgeHVStd
outputWeightHVMax
numberVariablesDTG
totalEdgesDTG
inputWeightDTGMax
hvRatio

Type

FD
(6)

Heuristics
(7)

Balance
(6)

Features
auxiliary atoms
implied effects removed
translator facts
translator total mutex groups size
num relevant facts
num instance actions
Additive
Context-enhanced additive
FF
Goal count
Landmark count
Landmark-cut
Max
rp fact balance avg
rp fact balance var
rp goal balance min
rp goal balance avg
rp goal balance var
h ff ratio

Table 4: List of features from the feature selection. The complete set of features is listed in Appendix A.

the features. Only 3 algorithms have statistically better accuracy with f-34 dataset and nine of them
have the similar accuracy, but in all cases they were below the best achieved accuracy

3.2.4 R EGRESSION M ODELS
We have trained regression models only with the positive instances of the classification training
phase. In the classification phase, all the planners have the same proportion of instances, but in
this case, not all the planners have the same number of instances given that they solved a different
number of problems. Nevertheless we do not consider this a relevant bias because the models
include the planner name, which somehow encodes single models for each planner, but in a grouped
model. We have trained the models with 20 regression algorithms, also provided by Weka.
The best algorithm for f-all was Decision Table (Kohavi, 1995) with a Relative Absolute Error
(RAE) of 49.87 and the best one for f-34 was Bagging (Breiman, 1996) with a RAE of 50.62.
Nevertheless, for simplicity we have selected the Decision Table model for the regression task in
both datasets (f-all and f-34). This decision is justified because the results do not show a significant
difference with the t-test result. In following sections, the regression model will always refer to
that trained with the Decision Table algorithm. See all the results of the regression models in the
Table 15 of Appendix B.
668

fiT HE IBAC O P P LANNING S YSTEM

3.3 IBaCoP Strategies
We have considered various strategies for the configuration of the IBAC O P portfolios. The list of the
strategies is ordered depending on the use they make of the knowledge provided by the predictive
models. In the experiments, each configuration will run for 1800 seconds. We have named the
portfolios according to the names given in IPC-2014.
IBAC O P: This portfolio uses an equal time strategy (ET) on the set of 11 candidate planners previously filtered by the QT-Pareto Score Filtering procedure. Therefore, the single planners will
run for 163 seconds. This strategy does not use the predictive models. The planner using this
strategy was awarded runner-up in the sequential satisficing track of IPC-2014.
IBAC O P2: This portfolio uses the Best N confidence strategy (BN), where N = 5. This means
that the 5 planners with the best prediction confidence in solving the problem are included in
the configuration. The run time is assigned uniformly to each planner (360 seconds). This
strategy, using the f-34 model was the winner of the sequential satisficing track of IPC-2014.1
IBAC O P2-B5E: This portfolio uses the Best estimated time strategy (BNE) with N = 5. It follows the same procedure as IBAC O P2 to select 5 planners, and then the time is assigned by
scaling the time prediction provided by the regression model (Decision Table). This strategy
participated in the learning track of IPC-2014 under the name of LIBAC O P2. In this case the
training data and models were generated for each domain separately, since the learning track
provides a training problem set for each domain a priori.
In addition, we have built other portfolio configurations that will serve as the baseline for comparison.
Overall Equal Time (OET): This strategy is a non-informed strategy which does not carry out any
planner filtering or use predictive models. It assigns equal time for each available planner.
Given that we have 28 planners (all the participants of IPC-2011 plus LPG-td), each planner
will run for 64 seconds. With this planner we see the need for some planner filtering since,
although it already obtains results close to current state of the art base planners, these results
can be improved by selecting a reduced set of planners.
Best 11 Planners (B11): This strategy selects the top 11 planners of IPC-2011 ordered by the score
in the competition, as shown in Table 2. Although selecting the best 11 planners is a good
choice intuitively, we show in the table that this selection reduces the planner diversity in the
portfolio, since most top planners in the competition are based on FD, with the only exception of Probe. This strategy is comparable with that implemented in BUS portfolio (Howe,
Dahlman, Hansen, Scheetz, & von Mayrhauser, 1999), in which the control strategy for ordering the planners and allocating time is derived from the performance study data.
Random 5 Planners (Rand): This strategy is one of the baselines to compare to the best 5 confidence strategy (IBAC O P2). Given a planning task, this strategy takes a random sample of
5 planners from the population of 11 candidate planners selected by the QT-Pareto filtering,
1. Predictive models submitted with IBAC O P2 to IPC-2014 were trained in a different benchmark set. In that case the
best accuracy was achieved by a Random Forest (Breiman, 2001).

669

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

and assigns equal time to them. We expect that a wise selection of 5 planners (IBAC O P2)
will be on average better than a random selection.
Default 5 Planners (Def): In this case, the strategy always includes the 5 best planners in terms
of quality score over the training data. These 5 planners are a subset from the 11 candidate
planners selected by the QT-Pareto filtering (i.e., LAMA -2011, PROBE , FD -AUTOTUNE -1,
LAMA -2008 and FD - AUTOTUNE -2). Then, the time is assigned equitably. We want to see
whether using the best 5 planners is better than making a per-instance selection of 5 planners.
3.4 Other Implementation Details
In this section we describe some of the engineering details we have incorporated into IBAC O P
planners. For instance, the competition rules proposed to include domains with conditional effects.
Because of this, we have included a parser that translates tasks with conditional effects into an
equivalent planning task without this property. This translator was based on a previous translator ADL2STRIPS (Hoffmann, Edelkamp, Thiebaux, Englert, dos Santos Liporace, & Trug, 2006).
Specifically, we have implemented the compilation that creates artificial actions for effect evaluations (Nebel, 2000).
Furthermore, many of the 11 candidate planners were built on the FAST-D OWNWARD framework, which among other things, separate the planning process into the sub-process of translation,
pre-processing and search. Indeed, the translation and the pre-process steps are already executed
when the feature generation for a given task is performed. We take advantage of this fact to avoid
doing the first two steps repeatedly if some of these planners are included in the configuration of
the portfolio for the regarding task. For version compatibility reasons this procedure is divided
into two groups. The output of the FD pre-process, used for feature extraction, is also used as the
search input for LAMA -2011, FDSS -2 and FD - AUTOTUNE (1 & 2). The previous FD pre-processor
2 was used in common for LAMA -2008, ARVAND and LAMAR . This optimization is used by all the
strategies evaluated. The remaining planners are totally independent of the FD pre-processing.
Moreover, some bugs arose during the execution of IPC-2014, as some issues in the domain
models required updates (Vallati, Chrpa, & McMcluskey, 2014a), and some planners were updated
such as Mercury (Vallati, Chrpa, & McMcluskey, 2014b). These issues were also fixed prior to
running the experimental evaluation presented in this article.

4. Experimental Evaluation
In this section, we describe the settings of the experimental evaluation and present the results of
the planners on the benchmarks used in the IPC-2014, specifically, in the Sequential Satisficing
track. In addition, we provide an analysis of the diversity of the planner selection achieved by some
configurations.
4.1 Experimental Settings
We have evaluated the different portfolio strategies described in Section 3.3, which permits different
portfolio configurations to be created. IBAC O P2 and IBAC O P2-B5E were run with two predictive
model versions, one trained with all features (f-all) and the other one trained with the selected fea2. This version corresponds to the version used to submit planners to IPC-2011

670

fiT HE IBAC O P P LANNING S YSTEM

tures (f-34). The Random strategy was run for 5 times and the average is reported. In addition, we
have included the JASPER and M ERCURY planners in the comparison. These planners also competed in IPC-2014. M ERCURY (Domshlak, Hoffmann, & Katz, 2015) was the second best planner
in terms of IPC score and JASPER (Xie, Muller, & Holte, 2014) was the second best planner in terms
of problems solved (coverage). As the test set we have used all the benchmarks of IPC-2014, with
the updates described in Section 3.4. This test set comprises 14 domains with 20 problems for each
domain.
Experiments were run on a cluster with Intel XEON 2.93 Ghz nodes, each with 8 GB of RAM,
using Linux Ubuntu 12.04 LTS. All planners had a cutoff of 1, 800 seconds and 4 GB of RAM.
For IBAC O P configurations requiring feature extraction, this process was limited to 4 GB of RAM
(following IPC competition rules) and 300 seconds (which is the maximum time used in the training
set to obtain the features, as described in Table 3). The time to extract the features is included in the
execution of the portfolio where, in the worse case, the feature extraction process took 300 seconds
and, therefore, the candidate planners only have 1, 500 to run. If the system does not extract the
features in this time, the input features are treated as missing values.
4.2 Results
Table 5 shows the results of all evaluated planners using the IPC quality score. We recall that this

score gives the ratio Q
Qi to planner i for each problem, where Qi is the quality of the best solution
found by planner i, and Q is the best solution found by any planner. If planner i does not solve the
problem the score is 0.

Hiking
Openstacks
Thoughtful
GED
Barman
Parking
Visitall
Maintenance
Tetris
Childsnack
Transport
Floortile
CityCar
CaveDiving
Total

IBaCoP2
f-all
f-34

IBaCoP2-B5S
f-all
f-34

Mercury

Jasper

OET

B11

Def

Rand

IBaCoP

18,9
19.6
12.7
19.4
14.6
18.0
20.0
5.1
16.3
0.0
19.9
2.0
4.1
7.0

18.1
18.8
16.4
17.9
19.0
17.0
15.4
10.0
16.2
0.0
12.0
2.0
11.5
8.0

18.2
15.4
14.5
18.3
16.7
17.6
13.3
15.0
5.0
12.0
8.9
4.8
6.0
0.0

19.2
17.2
19.4
17.1
16.7
13.8
8.1
15.9
11.5
3.4
3.8
3.4
8.8
0.0

18.7
19.2
19.2
16.3
17.2
18.0
13.7
11.6
9.3
2.6
6.9
4.1
5.0
7.0

18.4
16.3
17.4
13.0
13.8
11.6
15.0
14.5
11.9
8.9
8.2
12.3
9.4
7.0

19.0
17.8
19.2
17.5
16.9
16.3
15.2
15.6
13.3
19.2
10.3
16.2
12.5
6.3

18.9
18.6
19.2
17.6
17.1
18.1
18.0
15.5
12.5
18.4
11.5
15.3
9.0
7.0

18.6
18.5
17.4
17.5
17.1
18.1
18.0
15.4
11.9
18.9
11.6
17.2
6.2
7.0

18.8
18.2
17.6
17.6
17.2
18.5
18.0
15.5
15.7
15.0
11.1
17.5
9.9
7.0

18.6
18.3
19.2
17.5
17.2
18.1
18.0
15.4
13.6
18.9
12.1
12.0
7.78
7.0

177.6

182.1

165.7

158.3

168.8

177.6

215.3

216.5

213.4

217.4

213.7

Table 5: Results of IBAC O P configurations. The table also includes the results of Jasper, Mercury
and the four baseline configurations, OET, Best 11, Default and Random.

The overall best planner was IBAC O P2-B5E (f-all), closely followed by IBAC O P2 (f-all). The
difference between these two configurations is negligible. All the configurations using predictive
models are much better than OET, Default, Best 11 or Random. IBAC O P has a very good performance, comparable to the best performance. Moreover, there is a big difference between our
671

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

configurations and the other planners (Jasper and Mercury). IBAC O P based configurations are 32
or more points higher in all cases.
Figure 2 details the evolution of the number of problems solved as a function of the run-time
elapsed. The far right-hand point of the figure represents the final coverage. The best planner in
terms of coverage is IBAC O P, with 249 problems, and the second is IBAC O P2 (f-all) with 246. In
Figure 2, the planners show two different behaviors. On the one hand, an asymptotic growing in the
number of problems solved demonstrates that giving more time to the planners does not permit the
number of problems solved to be increased. JASPER is an extreme case, which after 300 seconds is
almost unable to improve. M ERCURY has the same problem, as well as the portfolio configurations
that do not take care of diversity. However, the IBAC O P, IBAC O P2 and IBAC O P2-B5E, which
selected a diverse set of planners, show a growing behavior throughout the time.
250

200

Problems

150

100

IBaCoP
IBaCoP2
IBaCoP2-B5E
Random
Jasper
Default
Mercury
Best11
0ET

50

0
0

200

400

600

800

1000

1200

1400

1600

1800

Time

Figure 2: Comparison of IBAC O P configurations, the baseline configurations, and the Mercury and
Jasper planners.
From the results we can derive some insights regarding different configurations. The score
difference between OET and IBAC O P reveals the importance of making a pre-selection of candidate
planner with an accurate filtering procedure. The Pareto-dominance approach allows us to have a
smaller set of planners, which means having more time per planner. There is a trade-off between
having more time per planner and loosing the diversity of solvers, and the results suggest that it is
more important to maintain diversity than increasing running time per planner. For instance, the
11 best IPC-2011 planners (B11) obtain worse results than those using the original 28 (OET), even
though B11 base planners have a longer running time. However, the QT-Pareto filtering approach is
able to reduce the number of planners while not sacrificing the diversity, which produces very good
results.
Reducing the number of planners for the portfolio configuration from 11 to 5 puts in risk the
diversity of solvers, as shown in the results of the Def approach (the best 5 planners in terms of
672

fiT HE IBAC O P P LANNING S YSTEM

performance) or in Rand (the random selection of 5 planners). Nevertheless, IBAC O P2 (f-all) and
(f-34) perform quite better than Def and Rand, which demonstrates that the classification models
select on average a good subset of planners for solving each particular task. These results are quite
promising for exploiting empirical performance models in planning portfolios. However, in the
current setting, results of IBAC O P2 are quite similar to IBAC O P. Thus, the classification models
manage to reduce the set of planners without deteriorating the performance of the fixed portfolio,
but they hardly contribute to a better overall performance.
Table 6 presents the number of problems solved by each of the 11 candidate planners. The final
column has the maximum number of problems that can be solved by the complete set of candidate
planners (i.e., a problem can be solved only if at least one of the candidate planners solved the
problem). The optimal selection of 5 planners for each planning task would lead to 253 problems
solved. IBAC O P2 is close to this optimum, confirming its ability for selecting good candidates for
the portfolio. The default configuration solved 193 problems, and the average number of problems
solved by the random configuration is 207 problems. Both of them are far from the best possible
value.
Hiking
Thoughtful
Openstacks
Tetris
GED
Transport
Parking
Barman
Maintenance
CityCar
Visitall
Childsnack
Floortile
CaveDiving
total

lama11
18
15
20
9
20
15
20
20
7
1
20
0
2
0

probe
20
12
4
14
20
12
9
19
8
0
10
0
2
0

FDA1
18
16
19
15
20
7
14
15
10
5
0
2
2
0

lama08
20
17
20
8
0
12
13
13
1
4
2
2
2
0

FDA2
20
12
20
1
0
6
2
2
8
5
0
0
5
0

lamar
20
14
20
13
0
7
18
15
1
8
0
2
2
0

arvand
20
20
20
18
0
5
0
0
17
19
2
8
1
0

fdss2
20
17
12
17
20
10
16
8
16
5
0
3
2
0

ya2-mt
4
13
0
0
0
20
0
0
3
2
20
0
0
0

LPG
20
8
1
0
14
0
0
0
8
0
1
7
19
0

M
3
5
0
0
0
0
0
0
6
14
0
20
0
7

Max
20
20
20
18
20
20
20
20
17
19
20
20
19
7

166

130

143

114

81

120

128

146

62

74

55

260

Table 6: Results of the candidate planners defined in Table 1 and the maximum number of problems
that can be solved by the complete set of these planners.

Once the set of 5 planners has been selected for the per-instance configuration, the regression
models do not contribute to a better performance. The task of estimating the run time needed to
solve a problem is more difficult than the classification task (Schwefel, Wegener, & Weinert, 2013).
Additionally, given that the aggregated time predictions could exceed the time limit, our proposal rescales these estimations and alters the real predictions. One alternative to this proposal is to keep the
real prediction and run the planners in the order established by the confidence in the classification
prediction, until one of them reaches the time limit. However, preliminary experiments during the
development of the planner showed us that this approach does not compensate the risk of losing
diversity due to fewer planner executions.
Another aspect to be analyzed is the performance of the planners in the new domains. The IPC2014 incorporated seven new domains, which means that the QT-Pareto Filtering and the predictive
models have not been trained with them. These domains are Cave Diving, Child-Snack, CityCar,
673

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

GED, Hiking, Maintenance and Tetris. From the results we can conclude that the behavior of all
IBAC O P configurations in new domains is on average similar to the performance in previously seen
domains.
4.3 Per-Instance Selection of Planners
In the previous section we showed that the benefit of configuring a portfolio per problem is that the
set of selected planners can be better adjusted to the problem, using fewer planners, and providing
more execution time to each planner. In this section we want to analyze the diversity of the planner
selections made by IBAC O P2 to see if the predictive models are classifying planners by how good
they are at solving specific domains or if they are identifying properties of specific problems in
different domains. Note that the test problems of a given domain usually range from easy to hard.
The increase in difficulty is mainly due to a larger size of the problems. Nevertheless, this increase
affects the learning features at a different scale and intensity.

lama-2011
probe
fd-autotune-1
lama-2008
fd-autotune-2
lamar
arvand
fdss-2
yahsp2-mt
LPG-td
madagascar
ll
ita
Vis
rt
po
ns
l
Tra
tfu
gh
ou
Th
tris
Te
ng
rki
Pa tacks
s
en
e
Op nanc
e
int
Ma
ing

Hik

D
GE e
il
ort
Flo r
a
yC
k
Cit
ac
sn
ild
Ch
ing
Div
ve
Ca n
a
rm
Ba

Figure 3: Proportion of the number of times each planner has been selected in a domain. In red
dots, the proportion for IBAC O P2 (f-all), and in blue dots, the proportion for IBAC O P2
(f-34).

Figure 3 shows the diversity of planners according to the selection made by IBAC O P2 (blue
dots for f-34 and red dots for f-all). The x axis shows the IPC-2014 domains and the y axis lists
the 11 candidate planners that the portfolio can use. The size of the dots is proportional to the
number of times a planner has been selected for a particular domain, i.e. the number of problems
for which the planner was selected. If a domain has five dots in one column (one domain), it means
that it was selected by the portfolio configuration for all problems in the domain. However, every
674

fiT HE IBAC O P P LANNING S YSTEM

column with more than five dots reveals the use of different 5-planner sets for different problems
in the same domain. The highlight of this analysis is that the 11 planners have been selected in
at least one domain, and in 13 out of 14 domains the selections involve more than 5 planners.
Note, for instance, that LAMA -2011 has the best a priori confidence on solving problems, but it
is sometimes not used (i.e., it was selected only 6 times in Floortile and 11 times in Openstacks).
Furthermore, some planners have a low a priori probability of being selected, but are frequently
used in some domains (like LPG- TD in Floortile).
Table 7 shows the sum of the number of times that each planner has been selected. The maximum number of times that a planner could be selected is 14  20 = 280. The last column reports
the average and the standard deviation of the number of times that each planner has been selected
per domain in both approximations (all and the reduced set of features).

LAMA -2011
PROBE
FD - AUTOTUNE -1
LAMA -2008
FD - AUTOTUNE -2
LAMAR
ARVAND
FDSS -2
YAHSP 2- MT
LPG- TD
MADAGASCAR

f-34
248
200
173
173
93
152
65
122
95
29
35

f-all
256
206
151
157
88
133
111
149
71
31
45

Average
18,00
14,50
11,57
14,50
6,46
10,18
6,29
9,68
5,93
2,14
2,86














STD
4,02
6,71
6,29
6,71
7,13
6,98
5,90
7,94
7,26
4,99
5,73

Table 7: Number of times a candidate planner has been selected by the two different classification
models (f-34 and f-all).

In addition to the previous analysis, we wanted to delve into the underlying mechanism to
achieve the per-instance selection of planners. We recall that planners are selected based on the
confidence of the success prediction. Therefore, in order to achieve different 5-planner sets in
the same domain, the ranking of the prediction confidence should vary throughout the problem.
To visualize and confirm this fact, we have selected the Tetris domain, which is one of the new
domains in IPC-2014 and it shows a good diversity selection as shown in Figure 3. This domain is
a simplified version of the well-known Tetris game.
A heatmap with the success prediction confidences appears in Figure 4. At a glance we realized that in general, a planner with higher success rate in training time obtains higher confidence,
but confidence ranking varies throughout different problems of the same domain. Another way to
read the picture is that the 5 darkest squares per column form the set of selected planners. For instance, lama-2011 was selected in all problems and probe was selected 18 times. On the other hand
Madagascar was not selected, and LPG-td was selected 3 times.
675

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

lama-2011
probe
fd-autotune-1
lama-2008

Score

fd-autotune-2
lamar
arvand
fdss-2
yahsp2-mt
LPG-td
madagascar
0

1

2

3

4

5

6

7

8

9

10 11 12 13 14 15 16 17 18 19

Figure 4: Success prediction confidence provided by the classification model (f-all) for each planner
and problem in the tetris Domain. Scale goes from 0.0 (white) or no confidence at all to
1.0 (dark blue) or complete confidence.

5. Related Work
In this section, we summarize the relevant research into portfolio configuration and how it relates to
our work. In addition, we summarize different approaches for the characterization of the planning
tasks, which is a cornerstone of this work to predict the behavior of the planners.
The idea of exploiting the synergy of different solvers to improve the performance of the individual ones is applied in propositional satisfiability problems (SAT), constraint satisfaction problems
(CSP), answer set programming (ASP) and in the scope of this paper, Automated Planning. The
SAT area has carried out extensive research into the importance of selecting the components of
the portfolio (Xu, Hutter, Hoos, & Leyton-Brown, 2012) and how to select each component (Lindauer, Hoos, Hutter, & Schaub, 2015b) automatically. The study of strategy selection in this area
includes per-instance selections (Lindauer, Hoos, & Hutter, 2015a). In addition, there is an intensive
study into solver runtime prediction (Hutter, Xu, Hoos, & Leyton-Brown, 2015), including a good
characterization of the satisfiability task. In other fields of Artificial Intelligence, CSP has portfolio configurations based on machine learning techniques such as SUNNY (Amadini, Gabbrielli, &
Mauro, 2014b) and other empirical research (Amadini, Gabbrielli, & Mauro, 2014a). For example
in ASP, the ASP-based Solver Scheduling (Hoos, Kaminski, Schaub, & Schneider, 2012) is a multicriteria optimization problem and provides the corresponding ASP encodings. In this paper we only
report the main systems related to Automated Planning in detail.
5.1 Portfolios in Automated Planning
Howe et al. (1999) describes one of the first planner portfolios. They implement a system called
BUS that runs 6 planners and whose goal is to find a solution in the shortest period of time. To
achieve it, they run the planners in portions of time and in circular order until one of them finds
a solution. In this portfolio, the planners are sorted following the estimation provided by a linear
676

fiT HE IBAC O P P LANNING S YSTEM

regression model of their success and run-time so, as in our case, they use predictive models of
the behavior of the planners to decide their order of execution. However, they use only 5 features
extracted from the PDDL description. For the domain, they count the number of actions and the
number of predicates. For the problem, they count the number of objects, the number of predicates
in the initial conditions and the number of goals. BUS minimizes the expected cost of implementing
a sequence of algorithms until one works, in contrast to IBAC O P and IBAC O P2, that does not stop
until the assigned time is over.
Fast Downward Stone Soup (FDSS, Helmert et al., 2011) is based on the Fast Downward (FD)
planning system (Helmert, 2006), with several versions for the different tracks. FDSS is an approach
to select and combine heuristics and search algorithms. A configuration is a combination of a search
algorithm and a group of heuristics. In training, they evaluate the possible configurations with a time
limit, and select the set of configurations that maximizes the coverage. For the portfolio presented
in the IPC-2011 Sequential Satisficing Track, they sort the configurations by decreasing the order
of coverage, hence beginning with algorithms likely to succeed quickly. The time limit for each
component is the lowest value that would still lead to the same portfolio score in the training phase.
However, the order is important, since each setting communicates the quality of the best solution
found so far to the following one, and this value is used to improve the performance of the next
setting. Therefore, FDSS can only include configurations within the FD framework. Conversely,
IBAC O P and IBAC O P2 build a portfolio using a mixture of generic planners of different styles and
techniques. Indeed FDSS is one of IBAC O P candidate planners.
PbP (Gerevini et al., 2009) configures a domain-specific portfolio. This portfolio incorporates
macro-actions in the specific knowledge of the domains. The incorporation of this knowledge establishes the order of a subset of planners which contain macro-actions. The running time is assigned
through a round-robin strategy. This portfolio incorporates seven planners (the latest version, PbP2,
adds lama-2008, see Gerevini et al., 2014). The automatic portfolio configuration in PbP and IBA C O P aims to build different types of planning systems: a domain-optimized portfolio planner for
each given domain in PbP and IBAC O P is an efficient domain-independent planner portfolio. The
IBAC O P and PbP configuration processes are significantly different. PbP uses several planners
that focus on macro-actions whilst IBAC O P only uses generic planners. The execution scheduling
strategy of PbP runs the selected planners in round-robin rather than sequentially in the case of
IBAC O P.
Fast Downward Cedalion (Seipp, Sievers, Helmert, & Hutter, 2015) is an algorithm for automatically configuring sequential planning portfolios of a parametric planner. Given a parametric
planner and a set of training instances, it selects the pair of planner and time iteratively. At the end
of each iteration all instances for which the current portfolio finds the best solution are removed
from the training set. The algorithm stops when the total run time of the added configurations
reaches the portfolio time limit or if the training set becomes empty. Configurations are generated
using the SMAC (Hutter, Hoos, & Leyton-Brown, 2011) model-based algorithm configurator on
the remaining training instances. Cedalion has the same configuration for all the problems but a
different configuration per version and IBAC O P has a different configuration per problem. The diversity of the candidate planner is limited while IBAC O P may completely include independent base
planners. The configuration processes and the resulting configured portfolios of Cedalion are the
same as FDSS.
The Fast Downward Uniform (Seipp et al., 2012) portfolio runs 21 automatically configured Fast
Downward instantiations sequentially for the same amount of time. Uniform portfolio approaches
677

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

are configured using the automatic parameter tuning framework ParamILS (Hutter, Hoos, LeytonBrown, & Stutzle, 2009) to find fast configurations of the Fast Downward planning system for 21
planning domains separately. At runtime, all configurations found are run sequentially for the same
amount of time for at most 85 seconds.
MiPlan (Nunez et al., 2015) is a sequential portfolio using Mixed-Integer Programming, which
computes the portfolio that obtains the best achievable performance with respect to a selection
of training planning tasks. In their case they have created a sequential portfolio with a subset of
sequential planners with fixed times whilst IBAC O P2 has different configurations per problem. For
this approximation, the planner does not consider the other portfolios, only their components. In
contrast, IBAC O P and IBAC O P2 includes the planners as they appear in other competitions, i.e. as
black boxes.
5.2 Features in Planning Problems
The construction of models to predict the performance of planners is not a novel idea. Roberts et
al. (2008, 2009) showed that models learned from the planners performance on known benchmarks
up to 2008 obtain a high accuracy when predicting whether a planner will succeed or not. They
use 19-32 features extracted from the domain and problem definition. The main difference with our
approach is that we also include features based on SAS+ , the heuristics of the initial state and the
fact balance of the relaxed plan. Most of our features come from the ground instantiation of the
problem, which are the key to differentiate tasks that share the same feature values at the PDDL
level.
Torchlight (Hoffmann, 2011) is a toolkit which allows the search space topology to be analyzed
without actually running any search. The analysis is based on the relation between the topology
under delete relaxation heuristics and the causal graph as well as DTGs. The feature extraction
process is built on top of the FF planner (Hoffmann & Nebel, 2001).
Recently, Fawcett et al. (2014) has generated models for accurately predicting the planner run
time. These models exploit a large set of instance features, including many of the features depicted
in Section 2.1.2. These features are derived from the PDDL and SAS+ representations of the problem, a SAT encoding of the planning problem and short runs of planners. Some other features are
extracted with Torchlight (Hoffmann, 2011). The experimental results in the work indicate that the
performance models generated are able to produce very accurate run time predictions. This study
of empirical performance models has not been applied to portfolio configurations.

6. Conclusion and Future Work
In this work we have introduced a framework for the creation of configurable planning portfolios,
IBAC O P. In the first step of the portfolio creation we find a small number of planners that maintains
the diversity of the initial planner set based on the QT-Pareto score filtering. Then we train predictive
models that select a promising sub-set of planners for solving a particular planning task.
The experimental evaluation confirmed the great performance of IBAC O P and IBAC O P2 in
IPC-2014. We can summarize the lessons learned from the development of the current IBAC O P
portfolios as the following:
 What really matters in the generation of a good portfolio is the selection of a diverse set of
planners. We have shown that the QT-Pareto score filtering reduces the set of candidate plan678

fiT HE IBAC O P P LANNING S YSTEM

ners while preserving the diversity. This filtering produces better results than other rankings
based on coverage or quality score.
 The selection of smaller sets of planners for the portfolio configuration (e.g., a sub-set of 5
planners in our experiments) is dangerous given that the portfolio might lose planner diversity.
We observed this situation in the Def and Random configurations, which select 5 out of 11
planners.
 The portfolio configurations using the classification models are able to select a good subset
of 5 planners, which with uniformly distributed time outperformed the selection provided by
a random and default selection with the same number of planners.
 Estimating the runtime for solving a problem is still very difficult and for this reason regression models are not providing additional useful information for the portfolio construction.
 In their current form, predictive models hardly contribute to the overall performance of the
portfolio. Per-instance configurations using classification models achieve similar performance to the fixed portfolio, but running fewer planners.
Even though in the current architecture the benefits of using predictive models are limited,
the results are promising because of the good performance of IBAC O P2 compared to the baseline
configurations. We think there is some room for research in this direction. Our argument is that
static portfolio configurations (including IBAC O P) are limited by the components and the fixed
time bound for each base planner. Their performance has an upper-limit, as computed by MiPlan,
that is smaller than the achievable performance of a dynamic configuration. This is because in a
per-instance configuration the portfolio strategy could assign different times to the base planners.
As future work we want to study additional features for a better characterization of the planning
tasks. Any computation that could be carried out as a pre-process step, or even with information
on first evaluated search nodes, could help with making predictive models more accurate. Our
models could incorporate information, for instance, about the landmark graph or the time elapsed
in computing the initial state heuristics. Other future work is a study of importance of the created
features, including a comparison between different groups of them in accordance with the semantics
of the features.

7. Acknowledgments
We thank the authors of the base planners because our work is based largely on their previous effort.
This work has been partially supported by the Spanish projects TIN2011-27652-C03-02, TIN201238079-C03-02 and TIN2014-55637-C2-1-R.

679

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

A. Appendix: Complete Feature Description
In this Appendix we present the list of the features used to characterize a planning task. For each
feature we include a brief description of what it is or how it is computed. Features are grouped by
their category in separate tables.
A.1 PDDL Features
N.
1
2
3
4
5
6
7
8

Name
Objects
Goals
Init
Types
Actions
Predicates
Axioms
Functions

Description
The number of objects in the problem.
The number of goals in the problem.
The number of in facts in the initial state.
The number of types in the domain.
The number of actions in the domain.
The number of predicates in the domain.
The number of axioms in the domain.
The number of functions in the domain.

Table 8: PDDL Features.

A.2 FD Instantiation Features
N.
9
10
11
12
13

Name
Relevant facts
Cost metric
Generated rules
Relevant atoms
Auxiliary atoms

14

Final queue length

15

18

Total queue pushes
Implied
effects
removed
Effect preconditions
added
Translator variables

19

Derived variables

20
21
22
23
24

Translator facts
Mutex groups
Total mutex size
Translator operators
Total task size

16
17

Description
The number of facts marked as relevant by FF instantiation.
Whether action costs are used or not.
The number of created rules in the translation process to create SAS+ task.
The number of relevant atoms found in the translator process.
The number of auxiliary atoms found in the translator process.
The length of the queue at end of the translation. This queue is an auxiliary
list that is used in the translation process to compute the model.
The number of times an element has been pushed into the queue.
The number of implied effects removed. Where the implied effects that the
translator knows are already included.
The number of implied effects added.
The number of created variables in SAS+ formulation.
The number of state variables that correspond to derived predicates or to
other artificial variables not directly affected by operator applications.
The number of facts that the pre-process takes into account.
The number of mutex groups.
The sum of all mutex group sizes.
The number of instantiated operators in SAS+ formulation.
The allowed memory for the translation process.

Table 9: Features extracted from the console output of the FD system.

680

fiT HE IBAC O P P LANNING S YSTEM

A.3 SAS+ Feature Description
We recall that in CG, the high-level variables are the variables for which there is a defined value in
the goal. Although the common definition of the CG does not consider the edges as weighted, the
FD system computes the edge weights of the CG as the number of instantiated actions that induced
each edge. We also consider these weights for computing our features.
N.

Name

25
26
27
28

Number Variables
High-Level Variables
TotalEdges
TotalWeight

29

VERatio

30

WERatio

31

WVRatio

32

HVRatio

33-35

InputEdge

36-38

InputWeight

39-41

OutputEdge

42-44

OutputWeight

45-47

InputEdgeHV

48-50

InputWeightHV

51-53

OutputEdgeHV

54-57

OutputWeightHV

Description
General Features
The number of variables of the CG.
The number of high-level variables.
The number of edges.
The sum of the edge weights.
CG Ratios
The ratio between the total number of variables and the total number of
edges. This ratio shows the level of connection in the CG.
The ratio between the sum of the weights and the number of edges. This
ratio shows the average weight for the edges.
The ratio between the sum of the weights and the number of variables.
The ratio between the number of high-level variables and the total number of variables. This ratio shows the percentage of variables involved
in the problem goals.
Statistics of the CG
Maximum, average and standard deviation of the number of incoming
edges for each variable.
Maximum, average and standard deviation of the sum of the weights of
the incoming edges for each variable.
Maximum, average and standard deviation of the number of outgoing
edges for each variable.
Maximum, average and standard deviation of the sum of the weights of
the incoming edges for each variable.
Statistics of high-level Variables
The number of incoming edges for each of the high level variables. This
value produces three new features following the same computation as
InputEdgeCG (features 33-35).
The edge weight sum of the incoming edges for each of the high level
variables. This value produces three new features following the same
computation as InputWeightCG.
The number of outgoing edges for each of the high level variables.
The sum of the weights of the incoming edges for each high level variables.

Table 10: Features from the Causal Graph.

681

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

N.

Name

58
59

Number Vertices
Total Edges

60

Total Weight

61

edVa Ratio

62

weEdRatio

63

weVaRatio

64-66

Input Edge

67-69

Input Weight

70-72

Output Edge

73-75

Output Weight

Description
General Aggregated Features DTG
The sum of the number of nodes of all DTGs.
The sum of the number of edges of all DTGs.
The sum of the edge weights of all DTGs. The edge weight in a DTG
corresponds to the cost of applying the action that induced the edge.
DTG Ratios
The ratio between the total number of edges and the total numbers of
variables. This ratio shows the level of connection in the DTG.
The ratio between the sum of the weights and the number of edges. This
ratio shows the number of restrictions that need to make the transition.
The ratio between the sum of the weights and the number of variables.
Statistics of DTGs
Maximum, average and standard deviation of the number of incoming
edges for a vertex in a DTG.
Maximum, average and standard deviation of the sum of the weights of
the incoming edges of all nodes.
Maximum, average and standard deviation of the number of outgoing
edges for a vertex in a DTG.
Maximum, average and standard deviation of the sum of the weights of
the outgoing edges of all nodes.

Table 11: Features that aggregate the information from the DTGs.

682

fiT HE IBAC O P P LANNING S YSTEM

A.4 Heuristic Features
N.

Name

76

Max

77

Landmark cut

78

Landmark
count
Goal count

79

FF

80

Additive

81

Causal Graph

82

Contextenhanced
additive

Description
(Bonet, Loerincs, & Geffner, 1997; Bonet & Geffner, 2000) The maximum
of the accumulated costs of the paths to the goal propositions in the relaxed
problem.
(Helmert & Domshlak, 2009) The sum of the costs of each disjunctive action
landmark that represents a cut in a justification graph towards the goal propositions.
(Richter, Helmert, & Westphal, 2008) The sum of the costs of the minimum
cost achiever of each unsatisfied or required again landmark.
The number of unsatisfied goals.
(Hoffmann & Nebel, 2001) The cost of a plan that reaches the goals in the
relaxed problem that ignores negative interactions.
(Bonet et al., 1997; Bonet & Geffner, 2000) The sum of the accumulated costs
of the paths to the goal propositions in the relaxed problem.
(Helmert, 2004) The cost of reaching the goal from a given search state by
solving a number of sub problems of the planning task which are derived from
the causal graph.
(Helmert & Geffner, 2008) The causal graph heuristic modified to use pivots
that define contexts relevant to the heuristic computation.

Table 12: Unit cost heuristics included as features.

A.5 Fact Balance
N.

Name

83-85

RP init

86-88

RP goal

89

Ratio ff

Description
Minimum, average and variance of the number of times that a fact in the
initial state is deleted in the computation of the relaxed plan.
Minimum, average and variance of the number of times that a goal is
deleted in the computation of the relaxed plan.
The ratio between the value of the max and FF heuristic. This proportion
shows the idea of parallelization of the plan.

Table 13: Fact balance features.

683

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

B. Appendix: Learning Results
This appendix shows the detailed results for the machine learning algorithms used to train the predictive models.
B.1 Classification
Algorithm
rules.ZeroR
rules.Ridor
rules.PART
rules.JRip
rules.DecisionTable
rules.ConjunctiveRule
trees.REPTree
trees.RandomTree
trees.RandomForest
trees.LMT
trees.J48
trees.ADTree
trees.NBTree
trees.DecisionStump
lazy.LWL
lazy.IBk -K 1
lazy.IBk -K 3
lazy.IBk -K 5
meta.RotationForest
meta.AttributeSelectedClassifier
meta.ClassificationViaClustering
meta.ClassificationViaRegression
meta.Bagging
meta.MultiClassClassifier
functions.SimpleLogistic
functions.MultilayerPerceptron
functions.RBFNetwork
functions.SMO
bayes.NaiveBayes
bayes.NaiveBayesUpdateable
bayes.BayesNet

f-all dataset
61.72  0.03
82.52  2.48
90.81  0.89
87.21  1.38
85.78  0.98
69.33  1.20
89.08  0.85
86.39  1.81
90.96  0.78
91.11  0.72
90.84  1.01
75.46  1.24
90.38  0.88
67.96  0.96
67.96  0.96
85.93  0.84
86.04  0.90
85.36  0.91
93.39  0.70
89.69  0.89
52.32  1.98
90.82  0.84
90.99  0.74
77.15  1.09
76.37  1.12
87.27  1.65
67.71  1.03
75.39  1.16
69.00  0.98
69.00  0.98
75.43  1.29

f-34 dataset
61.72  0.03
81.76  2.11
89.62  0.89
86.26  1.20
84.94  1.37
69.64  1.61
88.06  0.89
87.91  0.95
90.27  0.85
90.03  0.94
89.24  0.87
74.39  1.30
89.47  0.92
64.10  1.30
63.48  1.86
82.97  1.03
84.13  1.03
84.17  1.01
92.35  0.73
88.64  1.00
57.99  2.66
89.80  0.75
89.83  0.85
75.02  1.14
74.48  1.23
88.65  1.01
68.10  1.17
73.94  1.10
68.87  0.97
68.87  0.97
75.05  1.21

























Table 14: Accuracy and standard deviation for each training algorithm using 10-fold crossvalidation. Also, results of a t-test (OMahony, 1986) for the two training sets is shown.
Symbols ,  means statistically significant improvement or degradation respectively.
The significance level in the t-test is 0.05 and the baseline is the left column.

684

fiT HE IBAC O P P LANNING S YSTEM

B.2 Regression

trees.DecisionStump
trees.REPTree
trees.RandomTree
trees.RandomForest
functions.M5P
rules.ConjunctiveRule
rules.DecisionTable
rules.M5Rules
meta.Bagging
meta.AdditiveRegression
lazy.IBk 1
lazy.IBk 3
lazy.IBk 5
lazy.KStar
lazy.LWL
functions.LinearRegression
functions.MultilayerPerceptron
functions.LeastMedSq
functions.RBFNetwork
functions.SMOreg

f-all dataset
RAE

82.09  2.36 0.42  0.05
57.70  3.40 0.66  0.05
59.28 6.06 0.55 0.07
52.54 2.66 0.71 0.04
60.44  13.26 0.59  0.18
87.31  2.79 0.38  0.06
49.87  3.03 0.69  0.04
90.60  138.25 0.58  0.18
50.95  2.71 0.74  0.04
80.91  3.21 0.51  0.04
92.96  11.09 0.36  0.06
74.31  6.31 0.47  0.06
73.03  5.91 0.47  0.06
69.26 3.35 0.44 0.05
81.82 2.30 0.43 0.05
77.71  2.55 0.55 0.04
86.01 72.86 0.66 0.05
66.36  2.94 0.33  0.08
94.201.60 0.23 0.05
57.01  2.88 0.48  0.05

f-34 dataset
RAE

82.09  2.36 0.42  0.05
56.69  3.36 0.67  0.05
53.71 4.54 0.61 0.06
45.62 2.68 0.76  0.03
56.38  4.22 0.65  0.09
87.25  2.80 0.39  0.06
51.19  2.78 0.68  0.05
65.84  12.74 0.61  0.14
50.62  2.58 0.74  0.04
79.93  3.29 0.51  0.04
66.73  5.17 0.54  0.06
63.57  4.25 0.60  0.05
64.38  3.81 0.60  0.05
67.75 3.36
0.470.05
81.82 2.33 0.43 0.05
78.58 2.52 0.51 0.04
81.59 45.93 0.66 0.05
66.29  3.01 0.31  0.07
94.251.54
0.210.04
58.75  2.62 0.45  0.05








Table 15: Results for the 10-fold cross-validation in the regression models. RAE is the Relative
Absolute Error and  is the correlation coefficient. The small RAE values are better.
Symbols ,  means statistically significant improvement or degradation respectively.
The significance level in the t-test is 0.05 and the baseline is the left column.

685

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

C. Appendix: Training Results
t
openstacks
pathways
rovers
storage
tpp
trucks
pipesworld
cybersec
Openstacks-adl
openstacks
pegsol
scanalyzer
sokoban
transport
woodworking
elevators
barman
elevators
floortile
nomystery
openstacks
parcprinter
parking
pegsol
scanalyzer
sokoban
tidybot
transport
visitall
woodworking
Gold-miner
Matching-bw
N-puzzle
parking
sokoban
thoughtful
barman
blocksworld
depots
gripper
parking
rovers
satellite
spanner
tpp
Total

L-11

Probe

FDA1

L-08

FDA2

Lamar

Arvand

FDSS2

ya2-mt

LPG

30
30
40
40
30
19
42
28
31
30
30
30
29
18
23
30
20
20
6
10
20
20
20
20
20
19
16
19
20
20
30
25
29
28
23
0
9
29
1
0
18
30
16
0
30

30
30
40
40
30
8
44
24
31
30
30
30
27
10
30
29
20
20
5
6
14
14
19
20
20
17
18
20
20
20
30
15
20
24
23
18
5
30
30
0
9
30
10
0
20

30
26
40
40
30
18
40
28
31
30
30
30
29
17
25
30
20
20
7
10
20
20
19
20
20
19
15
11
2
20
30
24
30
25
30
0
0
22
0
0
6
30
3
0
30

30
29
40
40
30
16
38
28
31
30
30
30
25
17
26
25
4
6
3
12
20
1
20
20
20
15
14
19
20
14
30
23
29
28
18
0
0
21
0
0
13
29
3
0
30

30
29
40
40
30
22
33
26
31
30
30
27
27
18
24
30
6
17
9
19
20
14
9
20
17
16
17
10
5
14
26
23
9
16
30
0
0
15
0
30
1
24
29
0
6

30
30
40
40
30
15
43
27
31
30
30
30
25
17
25
27
6
11
3
12
20
0
20
20
20
14
19
3
11
9
29
17
27
30
17
0
0
0
6
0
19
30
1
0
21

27
30
40
40
30
15
46
28
31
30
30
30
8
19
30
30
0
20
3
19
20
20
4
20
20
2
17
15
10
20
30
16
6
17
30
0
0
0
0
4
4
30
2
0
30

30
0
40
40
30
20
42
28
31
30
30
30
29
15
30
30
17
20
7
12
19
20
20
20
20
19
18
15
6
20
0
0
0
0
30
0
13
20
0
0
9
30
22
0
25

0
0
40
40
30
0
41
0
0
1
22
27
0
11
23
2
12
0
8
10
0
13
3
15
17
0
0
20
20
19
30
25
20
13
28
22
0
16
29
0
0
30
13
0
30

27
30
40
40
24
11
33
7
1
0
1
0
0
0
0
0
0
0
12
0
2
0
0
0
0
0
15
0
8
0
30
22
30
13
15
7
0
29
6
30
0
11
30
30
1

M
11
30
40
40
30
21
14
0
16
15
27
21
2
9
2
0
0
0
0
17
0
20
0
17
11
0
1
0
0
1
30
1
0
0
22
0
0
0
0
0
0
14
0
15
9

#
30
30
40
40
30
30
50
30
31
30
30
30
30
30
30
30
20
20
20
20
20
20
20
20
20
20
20
20
20
20
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30

998

960

927

877

869

835

823

837

630

505

436

1251

Table 16: Solved problems in the training phase. The first part in this table is the results of IPC2005, the second part IPC-2008 and IPC-2011 in the satisficing tracks. The two last rows
(from Gold-miner to tpp) are IPC-2008-2011 in the learning track. The last column is the
number of problems included for training.

686

fiT HE IBAC O P P LANNING S YSTEM

D. Appendix: Planners
The following list the set of planners pre-selected as candidates from the Pareto-dominance filtering
described in Section 3.1
 Arvand (Nakhost, Muller, Valenzano, & Xie, 2011): is a stochastic planner that uses Monte
Carlo random walks to balance exploration and exploitation in heuristic search. This version
uses an online learning algorithm to find the best configuration of the parameters for the given
problem.
 Fast Downward Autotune-1 and Fast Downward Autotune-2 (Fawcett, Helmert, Hoos, Karpas,
Roger, & Seipp, 2011): are two instantiations of the FD planning system automatically configured for performance on a wide range of planning domains, using the well-known ParamILS
configurator. The planners use three main types of search in combination with several heuristics.
 Fast Downward Stone Soup-2 (Helmert et al., 2011) (FDSS-2): is a sequential portfolio with
several search algorithms and heuristics. Given the results of the training benchmarks, the
best combination of algorithms and heuristics is found through a hill-climbing search. Here,
the only information communicated between the component solvers is the quality of the best
solution found so far.
 LAMA-2008 and LAMA-2011 (Richter & Westphal, 2010; Richter, Westphal, & Helmert,
2011) is a propositional planner based on the combination of landmark count heuristic and
FF heuristic. The search performs a set of weighted A with iteratively decreasing weights.
The planner was developed within the FD Planning System (Helmert, 2006).
 Lamar (Olsen & Bryce, 2011) is a modification of the LAMA planner that includes a randomized construction of the landmark count heuristic.
 Madagascar (Rintanen, 2011): implements several innovations of SAT planning, including
compact parallelized/interleaved search strategies and SAT-based heuristics.
 Probe (Lipovetzky & Geffner, 2011): exploits the idea of wisely constructed lookaheads or
probes, which are action sequences computed without searching from a given state that can
quickly go deep into the state space, terminating either in the goal or in failure. This technique
is integrated within a standard greedy best first search.
 YAHSP2-MT (Vidal, 2011) extracts information from the relaxed plan in order to generate
lookahead states. This strategy is implemented in a complete best-first search algorithm,
modified to take helpful actions into account.
 LPG-td (Gerevini et al., 2006) is based on stochastic local search in the space of particular
action graphs derived from the planning problem specification.

References
Amadini, R., Gabbrielli, M., & Mauro, J. (2014a). Portfolio approaches for constraint optimization
problems. TPLP, 8426, 2135.
687

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

Amadini, R., Gabbrielli, M., & Mauro, J. (2014b). SUNNY: a lazy portfolio approach for constraint
solving. TPLP, 14(4-5), 509524.
Blum, A. L., & Langley, P. (1997). Selection of relevant features and examples in machine learning.
Artificial intelligence, 97(1), 245271.
Bonet, B., & Geffner, H. (2000). Planning as heuristic search: New results. In Recent Advances in
AI Planning, pp. 360372. Springer.
Bonet, B., Loerincs, G., & Geffner, H. (1997). A robust and fast action selection mechanism for
planning. In AAAI/IAAI, pp. 714719.
Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123140.
Breiman, L. (2001). Random forests. Machine learning, 45(1), 532.
Cenamor, I., de la Rosa, T., & Fernandez, F. (2012). Mining IPC-2011 results. In Proceedings of
the Third Workshop on the International Planning Competition - ICAPS.
Cenamor, I., de la Rosa, T., & Fernandez, F. (2013). Learning predictive models to configure planning portfolios. In Proceedings of the Workshop on the Planning and Learning - ICAPS.
Censor, Y. (1977). Pareto optimality in multiobjective problems. Applied Mathematics and Optimization, 4(1), 4159.
Dietterich, T. G. (2000). Ensemble methods in machine learning. In Kittler, J., & Roli, F.
(Eds.), Multiple Classifier Systems, First International Workshop, MCS 2000, Cagliari, Italy,
June 21-23, 2000, Proceedings, Vol. 1857 of Lecture Notes in Computer Science, pp. 115.
Springer.
Domshlak, C., Hoffmann, J., & Katz, M. (2015). Red-black planning: A new systematic approach
to partial delete relaxation. Artificial Intelligence, 221, 73114.
Fawcett, C., Helmert, M., Hoos, H., Karpas, E., Roger, G., & Seipp, J. (2011). FD-Autotune:
Domain-specific configuration using fast-downward. Proceedings of the Workshop on the
Planning and Learning - ICAPS, 2011(8).
Fawcett, C., Vallati, M., Hutter, F., Hoffmann, J., Hoos, H. H., & Leyton-Brown, K. (2014). Improved features for runtime prediction of domain-independent planners. In In Proceedings of
the 24th International Conference on Automated Planning and Scheduling (ICAPS-14).
Gerevini, A., Saetti, A., & Vallati, M. (2009). An automatically configurable portfolio-based planner
with macro-actions: PbP. In Proceedings of the 19th International Conference on Automated
Planning and Scheduling (ICAPS-09).
Gerevini, A., Saetti, A., & Serina, I. (2006). An approach to temporal planning and scheduling in
domains with predictable exogenous events. Journal of Artificial Intelligence Research, 25,
187231.
Gerevini, A., Saetti, A., & Vallati, M. (2014). Planning through automatic portfolio configuration:
The PbP approach. Journal of Artificial Intelligence Research, 50, 639696.
Ghallab, M., Nau, D., & Traverso, P. (2004). Automated planning: theory & practice. Access Online
via Elsevier.
Gomes, C. P., & Selman, B. (2001). Algorithm portfolios. Artificial Intelligence, 126(1), 4362.
688

fiT HE IBAC O P P LANNING S YSTEM

Grabczewski, K., & Jankowski, N. (2005). Feature selection with decision tree criterion. In Proceedings of the Fifth International Conference on Hybrid Intelligent Systems (HIS05), pp.
212217. IEEE.
Han, J., Kamber, M., & Pei, J. (2011). Data mining: concepts and techniques. Elsevier.
Helmert, M. (2004). A planning heuristic based on causal graph analysis. In In Proceedings of the
14th International Conference on Automated Planning and Scheduling (ICAPS-04), Vol. 16,
pp. 161170.
Helmert, M. (2006). The Fast Downward Planning System. Journal of Artificial Intelligence Research, 26, 191246.
Helmert, M. (2009). Concise finite-domain representations for PDDL planning tasks. Artificial
Intelligence, 173, 503535.
Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths and abstractions: Whats the difference anyway?. In In Proceedings of the 19th International Conference on Automated
Planning and Scheduling (ICAPS-09).
Helmert, M., & Geffner, H. (2008). Unifying the causal graph and additive heuristics. In In Proceedings of the 18th International Conference on Automated Planning and Scheduling (ICAPS08), pp. 140147.
Helmert, M., Roger, G., Seipp, J., Karpas, E., Hoffmann, J., Keyder, E., Nissim, R., Richter, S.,
& Westphal, M. (2011). Fast downward stone soup. The Seventh International Planning
Competition, IPC-7 planner abstracts, 38.
Hoffmann, J. (2003). The metric-FF planning system: Translating ignoring delete lists to numeric
state variables. Journal of Artificial Intelligence Research, 20, 291341.
Hoffmann, J. (2011). Analyzing search topology without running any search: On the connection
between causal graphs and h+. Journal of Artificial Intelligence Research, 41, 155229.
Hoffmann, J., Edelkamp, S., Thiebaux, S., Englert, R., dos Santos Liporace, F., & Trug, S. (2006).
Engineering benchmarks for planning: the domains used in the deterministic part of IPC-4.
Journal of Artificial Intelligence Research, 26, 453541.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through heuristic
search. Journal of Artificial Intelligence Research, 14, 253302.
Hoos, H., Kaminski, R., Schaub, T., & Schneider, M. T. (2012). aspeed: ASP-based solver scheduling. ICLP (Technical Communications), 17, 176187.
Howe, A. E., Dahlman, E., Hansen, C., Scheetz, M., & von Mayrhauser, A. (1999). Exploiting
competitive planner performance. In Biundo, S., & Fox, M. (Eds.), Recent Advances in AI
Planning, 5th European Conference on Planning, ECP99, Durham, UK, September 8-10,
1999, Proceedings, Vol. 1809 of Lecture Notes in Computer Science, pp. 6272. Springer.
Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2011). Sequential model-based optimization for
general algorithm configuration. In Learning and Intelligent Optimization, pp. 507523.
Springer.
Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: An automatic algorithm
configuration framework. Journal of Artificial Intelligence Research, 36, 267306.
689

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

Hutter, F., Xu, L., Hoos, H., & Leyton-Brown, K. (2015). Algorithm runtime prediction: Methods
and evaluation (extended abstract). In Yang, Q., & Wooldridge, M. (Eds.), Proceedings of the
Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos
Aires, Argentina, July 25-31, 2015, pp. 41974201. AAAI Press.
Kohavi, R. (1995). The power of decision tables. In Machine Learning: ECML-95, pp. 174189.
Springer.
Linares Lopez, C., Celorrio, S. J., & Olaya, A. G. (2015). The deterministic part of the seventh
international planning competition. Artificial Intelligence, 223, 82119.
Lindauer, M. T., Hoos, H. H., & Hutter, F. (2015a). From sequential algorithm selection to parallel
portfolio selection. In Dhaenens, C., Jourdan, L., & Marmion, M. (Eds.), Learning and Intelligent Optimization - 9th International Conference, LION 9, Lille, France, January 12-15,
2015. Revised Selected Papers, Vol. 8994 of Lecture Notes in Computer Science, pp. 116.
Springer.
Lindauer, M. T., Hoos, H. H., Hutter, F., & Schaub, T. (2015b). Autofolio: An automatically configured algorithm selector. Journal of Artificial Intelligence Research, 53, 745778.
Lipovetzky, N., & Geffner, H. (2011). Searching for plans with carefully designed probes. In In
Proceedings of the 21st International Conference on Automated Planning and Scheduling
(ICAPS-11), pp. 154161.
Malitsky, Y., Sabharwal, A., Samulowitz, H., & Sellmann, M. (2013). Algorithm portfolios based
on cost-sensitive hierarchical clustering. In Proceedings of the Twenty-Third international
joint conference on Artificial Intelligence, pp. 608614. AAAI Press.
Nakhost, H., Muller, M., Valenzano, R., & Xie, F. (2011). Arvand: the art of random walks. The
Seventh International Planning Competition, IPC-7 planner abstracts, 1516.
Nebel, B. (2000). On the compilability and expressive power of propositional planning formalisms.
Journal of Artificial Intelligence Research, 12, 271315.
Nunez, S., Borrajo, D., & Linares Lopez, C. (2015). Automatic construction of optimal static
sequential portfolios for AI planning and beyond. Artificial Intelligence, 226, 75101.
Olsen, A., & Bryce, D. (2011). Randward and Lamar: Randomizing the FF heuristic. The Seventh
International Planning Competition, IPC-7 planner abstracts, 55.
OMahony, M. (1986). Sensory evaluation of food: statistical methods and procedures, Vol. 16.
CRC Press.
Quinlan, J. R. (1993). C4. 5: programs for machine learning, Vol. 1. Morgan kaufmann.
Richter, S., Helmert, M., & Westphal, M. (2008). Landmarks revisited. In AAAI, Vol. 8, pp. 975
982.
Richter, S., & Westphal, M. (2010). The LAMA planner: Guiding cost-based anytime planning with
landmarks. Journal of Artificial Intelligence Research, 39(1), 127177.
Richter, S., Westphal, M., & Helmert, M. (2011). Lama 2008 and 2011. The Seventh International
Planning Competition, IPC-7 planner abstracts, 50.
Rintanen, J. (2011). Madagascar: Efficient planning with SAT. The Seventh International Planning
Competition, IPC-7 planner abstracts, 61.
690

fiT HE IBAC O P P LANNING S YSTEM

Roberts, M., & Howe, A. (2009). Learning from planner performance. Artificial Intelligence, 173,
536561.
Roberts, M., Howe, A. E., Wilson, B., & desJardins, M. (2008). What makes planners predictable?.
In In Proceedings of the 18th International Conference on Automated Planning and Scheduling (ICAPS-08), pp. 288295.
Rodriguez, J. J., Kuncheva, L. I., & Alonso, C. J. (2006). Rotation forest: A new classifier ensemble
method. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(10), 1619
1630.
Schwefel, H.-P., Wegener, I., & Weinert, K. (2013). Advances in computational intelligence: Theory
and practice. Springer Science & Business Media.
Seipp, J., Braun, M., Garimort, J., & Helmert, M. (2012). Learning portfolios of automatically tuned
planners. In McCluskey, L., Williams, B., Silva, J. R., & Bonet, B. (Eds.), Proceedings of
the Twenty-Second International Conference on Automated Planning and Scheduling, ICAPS
2012, Atibaia, Sao Paulo, Brazil, June 25-19, 2012. AAAI.
Seipp, J., Sievers, S., Helmert, M., & Hutter, F. (2015). Automatic configuration of sequential
planning portfolios. In Bonet, B., & Koenig, S. (Eds.), Proceedings of the Twenty-Ninth
AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., pp.
33643370. AAAI Press.
Vallati, M. (2012). A guide to portfolio-based planning. In Multi-disciplinary Trends in Artificial
Intelligence, pp. 5768. Springer.
Vallati, M., Chrpa, L., & Kitchin, D. E. (2015). Portfolio-based planning: State of the art, common
practice and open challenges. AI Communications, 29, 117.
Vallati, M., Chrpa, L., & McMcluskey, L. (2014a).
https://helios.hud.ac.uk/scommv/IPC-14/benchmark.html.

Competition

Domains.

Vallati, M., Chrpa, L., & McMcluskey, L. (2014b). Source code and Erratum Deterministic part.
https://helios.hud.ac.uk/scommv/IPC-14/errPlan.html.
Vidal, V. (2011). YAHSP2: Keep it simple, stupid. The Seventh International Planning Competition,
IPC-7 planner abstracts, 83.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical Machine Learning Tools and Techniques.
2nd Edition, Morgan Kaufmann.
Xie, F., Muller, M., & Holte, R. (2014). Jasper: the art of exploration in greedy best first search. In
Planner abstracts. IPC-2014.
Xu, L., Hoos, H., & Leyton-Brown, K. (2010). Hydra: Automatically configuring algorithms for
portfolio-based selection. In Proceedings of the Twenty-Fourth AAAI Conference on Artificial
Intelligence (AAAI 2010), pp. 210216.
Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2012). Evaluating component solver contributions to portfolio-based algorithm selectors. In Theory and Applications of Satisfiability
TestingSAT 2012, pp. 228241. Springer.
Xu, L., Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2008). SATzilla: Portfolio-based algorithm
selection for SAT. Journal of Artificial Intelligence Research, 32, 565606.

691

fiJournal of Artificial Intelligence Research 56 (2016) 329-378

Submitted 12/15; published 06/16

DL-Lite Contraction and Revision
Zhiqiang Zhuang

z.zhuang@griffith.edu.au

Institute for Integrated and Intelligent Systems
Griffith University, Australia

Zhe Wang

zhe.wang@griffith.edu.au

School of Information and Communication Technology
Griffith University, Australia

Kewen Wang

k.wang@griffith.edu.au

School of Information and Communication Technology
Griffith University, Australia

Guilin Qi

gqi@seu.edu.cn

School of Computer Science and Engineering
Southeast University, China
State Key Lab for Novel Software Technology
Nanjing University, China

Abstract
Two essential tasks in managing description logic knowledge bases are eliminating problematic axioms and incorporating newly formed ones. Such elimination and incorporation
are formalised as the operations of contraction and revision in belief change. In this paper,
we deal with contraction and revision for the DL-Lite family through a model-theoretic
approach. Standard description logic semantics yields an infinite number of models for
DL-Lite knowledge bases, thus it is difficult to develop algorithms for contraction and revision that involve DL models. The key to our approach is the introduction of an alternative
semantics called type semantics which can replace the standard semantics in characterising
the standard inference tasks of DL-Lite. Type semantics has several advantages over the
standard one. It is more succinct and importantly, with a finite signature, the semantics
always yields a finite number of models. We then define model-based contraction and
revision functions for DL-Lite knowledge bases under type semantics and provide representation theorems for them. Finally, the finiteness and succinctness of type semantics allow
us to develop tractable algorithms for instantiating the functions.

1. Introduction
Description logic (DL) (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003)
knowledge bases (KBs) are subject to frequent change. For instance, outdated or incorrect
axioms have to be eliminated from the KBs and newly formed ones have to be incorporated
into them. Therefore a mandatory task for managing DL KBs is to deal with such changes.
In the field of belief change, extensive work has been done on formalising various kinds of
changes over KBs. In particular, elimination of old knowledge is called contraction and
incorporation of new knowledge is called revision. To deal with changes to DL KBs, it
makes sense to take advantage of the existing techniques in belief change. In fact, many
have investigated contraction and revision over DL KBs (DL contraction and DL revision
c
2016
AI Access Foundation. All rights reserved.

fiZhuang, Wang, Wang, & Qi

for short) (Qi et al., 2006; Qi & Du, 2009; Qi et al., 2008; Ribeiro & Wassermann, 2009;
Wang et al., 2015).
The dominant approach in belief change is the so called AGM framework (Alchourron,
Gardenfors, & Makinson, 1985; Gardenfors, 1988). In this framework, the KB to which
changes are made is called a belief set which is a logically closed set of formulas. An AGM
.
contraction function  takes as input a belief set K and a formula  and returns as output
.
a belief set K  that does not entail . Taking the same inputs, an AGM revision function
 returns a belief set K  that entails . The framework also provides rationality postulates
which capture the intuitions behind rational contraction and revision. The hallmark of this
framework is that the so called representation theorems are proved which ensure that AGM
contraction and revision functions are not only sound but also complete with respect to the
rationality postulates.
Regardless of its wide acceptance, a limitation of the AGM framework is that it has a
minimal requirement on the underlying logic, that the logic subsumes classical propositional
logic. This means the underlying logic must fully support all the truth functional logical
connectives such as negation and disjunction. Since most DLs are not so, the AGM framework is incompatible with DLs and cannot be applied directly to deal with changes over
DLs KBs. The incompatibility is the major difficulty in defining DL contraction and revision. Additionally, DL revision is more involved than AGM revision. AGM revision aims
to resolve any inconsistency caused while incorporating a new formula. Since a meaningful DL KB has to be both consistent and coherent (i.e, absence of unsatisfiable concepts),
DL revision has to resolve not only inconsistency but also incoherence. Finally, despite
its mathematical elegance, the AGM framework has been grappled with the issue of computational efficiency which is crucially important for DL KBs. Therefore, DL contraction
and revision should lead to tractable instantiations and at the same time respecting the
rationality postulates of AGM contraction and revision.
Due to the many difficulties, existing works on DL contraction and revision are not fully
satisfactory. None of them provides representation theorem for their contraction or revision
function except the work by Ribeiro and Wassermann (2009) which inherits the representation results from a more general work (Hansson & Wassermann, 2002).1 In defining DL
revision, many did not appreciate its incoherence resolving nature, as their revisions cannot
guarantee coherence of the outputs (Qi et al., 2006; Ribeiro & Wassermann, 2009; Wang
et al., 2015). Qi and Du (2009) appreciate the incoherence resolving nature, but the postulates they provided for capturing properties of their revision function are not formulated
appropriately to capture the rationales behind incoherence resolving.
In this paper, we provide DL contraction and revision that overcome these limitations.
Specifically, we define contraction and revision functions for logically closed DL-Litecore and
DL-LiteR KBs. DL-Litecore and DL-LiteR are the main languages of the DL-Lite family
(Calvanese et al., 2007). In defining the functions we take a model-theoretic approach
1. Hansson and Wassermann (2002) proved a series of representation theorems for contraction and revision
functions under monotonic and compact logics. Since Ribeiro and Wassermann (2009) considered the
same contraction and revision functions for DLs which are monotonic and compact, the representation
results of Hansson and Wassermann (2002) also hold for these contraction and revision functions. Since
our approach in defining contraction and revision are different from Hansson and Wassermann (2002),
we can not inherit their representation results and have to prove them from scratch.

330

fiDL-Lite Contraction and Revision

similar to that of Katsuno and Mendelzon (1992). Instead of DL models the functions are
based on models of a newly defined semantics for DL-Lite called type semantics. Given
that type semantics is equivalent to DL semantics with respect to major DL-Lite reasoning
tasks, models of type semantics (i.e., type models) are more succinct than DL models. More
importantly, given a finite signature, any DL-Lite KB has a finite number of type models,
whereas it usually has an infinite number of DL models.
We fully appreciate the incoherence resolving nature of DL revision and reflect it in both
the definition of the revision functions and the postulates capturing their properties. We
are able to prove AGM-style representation theorems for all our contraction and revision
functions. Such theorems are crucial because they guarantee the functions defined in our
method behave properly (in the sense of satisfying a set of commonly accepted postulates)
and all properly behaved functions can be defined through our method. In addition to
the rigorous mathematical properties, we provide tractable algorithms for instantiating the
contraction and revision functions.
Some of the material in this paper was presented previously by Zhuang, Wang, Wang,
and Qi (2014).

2. DL-Lite
DL-Litecore is the core language of the DL-Lite family. It has the following syntax
B  A | R

R  P | P

C  B | B

E  R | R

where A denotes an atomic concept; P an atomic role, P  the inverse of the atomic role P ; B
a basic concept which is either an atomic concept or an unqualified existential quantification;
C a general concept which is either a basic concept or its negation; R a basic role which
is either an atomic role or its inverse; and E a general role which is either a basic role or
its negation. We also include in the language the nullary predicates  and > that denote
universal false and universal truth respectively. We assume the set of all basic concepts,
denoted as B, and the set of all basic roles, denoted as R, are finite. For an inverse role
R = P  , we write R to represent P .
A DL-Litecore KB consists of a TBox and an ABox. We sometime denote a KB as
(T , A) where T is the TBox of the KB and A is the ABox of the KB. A TBox is a finite set
of concept inclusions of the form B v C, B v , and > v C. That is only basic concept
or > can appear on the left-hand side of a concept inclusion. An ABox is a finite set of
concept assertions of the form A(a) and role assertions of the form P (a, b), where A is an
atomic concept, P an atomic role, and a, b individuals. We assume the set of all individuals,
denoted as D, is finite. Throughout the paper, individual names are denoted by lower case
Roma letters (a, b, . . .).
A major extension of DL-Litecore is DL-LiteR . It extends DL-Litecore with role inclusions of the form R v E. That is only basic roles can appear on the left-hand side of a role
inclusion. A concept or role inclusion is also called a TBox axiom and a concept or role
assertion is also called an ABox axiom. Throughout the paper, TBox and ABox axioms are
denoted by lower case Greek letters (, , . . .).
The semantics of a DL-Lite language is given in terms of interpretations. An interpretation I = (I , I ) consists of a nonempty domain I and an interpretation function I
331

fiZhuang, Wang, Wang, & Qi

that assigns to each atomic concept A a subset AI of I , to each atomic role P a binary
relation P I over I , and to each individual a an element aI of I . The interpretation
function is extended to general concept, general roles, and special symbols as follows:
I = 
>I = I
(P  )I = {(b, a) | (a, b)  P I }
(R)I = {a | b.(a, b)  RI }
(B)I = I \ B I
(R)I = I  I \ RI
The set of all interpretations is denoted as . An interpretation I satisfies a concept
inclusion B v C if B I  C I , a role inclusion R v E if RI  E I , a concept assertion A(a)
if aI  AI , and a role assertion P (a, b) if (aI , bI )  P I .
I satisfies a KB K (a TBox T or an ABox A) if I satisfies all axioms in K (resp. T ,
A). I is a model of a KB K (a TBox T , an ABox A, or an axiom ) written I |= K, (resp.
I |= T , I |= A, I |= ) if it satisfies K (resp. T , A, or ). We denote the models of a
KB K (a TBox T , an ABox A, or an axiom ) as |K| (resp. |T |, |A|, or ||). Two axioms
 and  are logically equivalent, written   , if they have identical set of models. A
KB K (a TBox T , an ABox A, or an axiom ) entails an axiom , written K |=  (resp.
T |= , A |= , or  |= ), if all models of K (resp. T , A, or ) are also models of . A KB
(a TBox, an ABox or an axiom) is consistent if it has at least one model and inconsistent
otherwise. We use K , T , A to denote respectively the (unique) inconsistent KB, TBox,
and ABox. We use |=  to denote that  is a tautology (e.g., A v A) and 6|=  that  is not
one.
The closure of a TBox T , denoted as cl(T ), is the set of all TBox axioms  such that
T |= . We say that a TBox T is closed if T = cl(T ). The closure of a DL-Lite TBox is
finite. Actually, the size of cl(T ) is quadratic with respect to the size of T (Pan & Thomas,
2007). The closure of an ABox A with respect to a TBox T , denoted as clT (A), is the set
of all ABox axioms  such that (T , A) |= . We say that an ABox A is closed with respect
to T if A = clT (A). The closure of an ABox with respect to a TBox in DL-Lite is finite and
can be computed efficiently through a chase procedure (Calvanese et al., 2007). In Section
4 and Section 5, all TBoxes and ABoxes are assumed to be closed.
A basic concept B is satisfiable with respect to a TBox T if there is a model I of T such
that B I is non-empty and unsatisfiable otherwise. It is easy to see that B is unsatisfiable
with respect to T if and only if B v   cl(T ). A TBox is coherent if all basic concepts
are satisfiable and incoherent otherwise.2
In defining contraction and revision functions for DL-Lite KBs, we need to refer to the
notion of conjunction of axioms. Given a set of axioms {1 , . . . , n }, their conjunction is
denoted as 1      n . An interpretation is a model of 1      n if it satisfies all the
conjuncts that is |1      n | = |1 |      |n |.
2. In DL literatures, often coherence comes with the absence of unsatisfiable atomic concepts. Since in
DL-Lite unsatisfiable non-atomic concepts like R are also unexpected we use the stricter condition for
coherence.

332

fiDL-Lite Contraction and Revision

3. Type Semantics
In this section, we provide an alternative semantics for DL-Lite, namely type semantics.
In short, type semantics takes the semantics underlying propositional logic (i.e., propositional semantics) as the basis and equips it with extra facilities to take care of the nonpropositional behaviours of DL-Lite.
We first introduce a simplified version of type semantics called ct-type semantics (ct
stands for Core TBox), which is sufficient for characterising the standard inference tasks of
DL-Litecore TBoxes. An important consideration in defining the semantics is succinctness,
that is the defined semantics should avoid any redundancy. With this consideration, cttype semantics has no facility other than those required for characterising the inference
tasks of DL-Litecore TBoxes. Accordingly, as DL-Litecore TBoxes do not allow inferences
that involve role inclusions or ABox axioms, ct-type semantics is incapable of characterising
such inferences. Next, we extend ct-type semantics with the facilities for role inclusions
which results in another simplified version of type semantics called t-type semantics (t
stands for TBox). The semantics is sufficient for characterising the standard inference
tasks of DL-LiteR TBoxes. Again, for the sake of succinctness, t-type semantics is intended
to capture the inference tasks for DL-LiteR TBoxes only, thus is incapable of characterising
those involving ABox axioms. For these inferences, we introduce a-type semantics (a
stands for ABox). The semantics is sufficient for DL-LiteR ABoxes (with a background
TBox). It is also a simplified version of type semantics, but is not built upon ct-type or
t-type semantics. Finally, we introduce the full version of type semantics, which is sufficient
for DL-LiteR KBs. The semantics includes all facilities of t-type and a-type semantics.
Figure 1 shows the hierarchy of type semantics.

type
t-type
ct-type

a-type

Figure 1: Each rectangle represents a semantics, with the largest representing type semantics. A rectangle containing one or more smaller ones indicates the represented
semantics of the larger rectangle subsumes those of the smaller ones.

333

fiZhuang, Wang, Wang, & Qi

The propositional origin and the assumption of finite signature guarantee the finiteness
of type semantics. As mentioned, an important consideration in defining type semantics is
succinctness. The more succinct a semantics is the more efficient the computations involving its models. Type semantics can replace ct-type semantics, t-type semantics, and a-type
semantics in characterising DL-Litecore TBoxes, DL-LiteR TBoxes, and DL-LiteR ABoxes;
t-type semantics can replace ct-type semantics in characterising DL-Litecore TBoxes. However, it will be a waste of computational power to use type semantics to characterise for
instance DL-LiteR TBoxes as its facilities for ABox axioms are redundant for DL-LiteR
TBoxes. The same holds in using t-type semantics for characterising DL-Litecore TBoxes as
the facilities of t-type semantics for role inclusions are redundant for DL-Litecore TBoxes.
The finiteness and succinctness are significant advantages of type semantics over DL semantics when DL-Lite KBs need to be represented model-theoretically and the related
computational tasks involve their models.
Depending on the application scenarios, changes to DL-Lite KBs may be applied to
(1) the whole KB or restricted to either (2) the TBox or (3) the ABox with a background
(unchanged) TBox. If we take a model-theoretic approach in addressing the changes, then
the most suitable semantics for scenario (1) is type semantics; that for scenario (2) is ct-type
or t-type semantics; and that for scenario (3) is a-type semantics.
3.1 Characterising DL-Litecore TBoxes
We start with ct-type semantics. Standard inference tasks for DL-Lite TBoxes such as
checking satisfiability, subsumption, equivalence, disjointness, and consistency can be reduced to that of checking whether an entailment relationship holds between some TBox
axioms. Given a TBox T , a basic concept B is satisfiable in T if and only if T does not
entail B v ; A is subsumed by B in T if and only if T entails A v B; A and B are
equivalent in T if and only if T entails A v B and B v A; A is disjoint with B in T if and
only if T entails A v B; and T is inconsistent if and only if T |= > v . For this reason,
in defining ct-type (t-type) semantics, it suffices to focus on the entailment relationships
between DL-Litecore (resp. DL-LiteR ) TBox axioms.
In propositional semantics or standard DL semantics, we have the notion of interpretations. Analogously, in type semantics, the central notion is that of types.3 The definition of
type will be given in Section 3.4. For ct-type semantics, we only need a simplified version,
called ct-type. A ct-type  is a possibly empty set of basic concepts (i.e.,   B). For
example, if B = {R, R , A}, then {R, A} is a ct-type and for simplicity we sometimes
write it as RA.4 We denote the set of all ct-types as tc . If we consider basic concepts as
propositional atoms, and concept inclusion B v C as propositional formula B  C, then a
ct-type is nothing but an interpretation (represented by atoms interpreted as true) in propositional logic. Given a DL-Litecore TBox T , we use kT ktc to denote the set of propositional
models of the corresponding propositional formulas of T . Note that kT ktc  tc .
Many entailment relationships between DL-Litecore TBox axioms are propositional in
the sense that the entailments also hold when we treat the axioms as propositional formulas
3. The notion of types is mentioned in the work of Kontchakov et al. (2010), which have similar structures
as ct-types in this paper but cannot capture role inclusions or ABox axioms.
4. We work with DL-Lite throughout the paper. Since DL-Lite does not allow quantified existential quantifications such as R.C, the ct-type RA cannot be confused with the concept R.A

334

fiDL-Lite Contraction and Revision

and consider propositional semantics. For example, the entailment from A v B and B v C
to A v C also holds for the corresponding propositional formulas A  B, B  C, and
A  C, under propositional semantics. As expected there are entailments that are not
propositional. The following example shows a common pattern of the non-propositional
entailments. Note that, R v  and R v  entail one another but, under propositional
semantics, the corresponding propositional formulas R and R do not. The reason
is simple. For DLs, a role R represents a binary relation and the axiom R v  and
R v  both indicate the relation is empty. Propositional logic does not have the facility
for binary relations and entailments involving such relations, thus can not characterise the
entailments.
Then it is clear, for ct-type semantics, we need all facilities of propositional semantics
to characterise the propositional entailments and an extra one to characterise the nonpropositional ones. We capture such facilities by the conditions under which a ct-type
satisfies a DL-Litecore TBox. In DL semantics, interpretations satisfying a TBox is called
models of the TBox. Analogously, a ct-type satisfying a DL-Litecore TBox is called a ctmodel of the TBox.
Definition 1 A ct-model  of a DL-Litecore TBox T is a ct-type such that
1.   kT ktc and
2. if T |= R v  then R 6  .
For a ct-type to satisfy a TBox T , firstly it has to be a propositional model of T and secondly
if T entails R v , then it can not contain the basic concept R. The first condition
guarantees the proper handling of propositional entailments and the second guarantees
that of non-propositional entailments.
Example 1 Consider a fragment of (slightly modified) NCI KB concerning heart diseases
and their associated anatomic locations, which consists of concepts Heart Disease (HD),
Cardiovascular System (CS), Respiratory System (RS), and Organ System (OS), as well as
a role that relates diseases to their primary locations Disease Has Primary Anatomic Site
(Loc). Let DL-Litecore TBox T consist of the following concept inclusions: HD v Loc,
Loc v CS, HD v OS, RS v OS, CS v OS, and RS v CS.
Some ct-models of T are 1 = {HD, Loc}, 2 = {Loc , CS, OS}, and 3 = {RS, OS}.
If concept inclusion Loc v RS is in T , then T |= Loc v  and T |= Loc v , and
neither 1 nor 2 is a ct-model of T .
We denote the set of ct-models of a TBox T as |T |tc . The ct-models of a conjunction of
axioms 1  2     n , denoted as |1  2     n |tc , is defined as
|1  2     n |tc = |{1 , 2 , . . . , n }|tc .
The ct-models of a negated (conjunctions of) axiom(s) , denoted ||tc , is defined as
tc \ ||tc .
335

fiZhuang, Wang, Wang, & Qi

The notions of entailment, logical equivalence, and consistency under ct-type semantics are
defined in the same manner as DL semantics. Under ct-type semantics, a TBox T entailing
a conjunction  of axioms is written as T |=tc .
To make the non-propositional behaviour of ct-type semantics explicit, we propose the
following notion of role-complete set of ct-types. A set of ct-types M is role-complete if, for
all R  R, whenever there is a ct-type  in M such that R   , then there is a ct-type  0
in M such that R   0 ( and  0 may be identical). Roughly speaking, role-completeness
indicates that the set of ct-types M have complete information about each role R. We can
show that the set of ct-models for any DL-Litecore TBox is role-complete.
Proposition 1 Let T be a DL-Litecore TBox. Then |T |tc is role-complete.
Now we show some connections between the DL models and the ct-models of a DLLitecore TBox. Let I be a DL interpretation. For each element d in the domain of I, d
induces a unique ct-type as follows
 (I, d) = {B  B | d  B I }.
We call  (I, d) the ct-type induced by d in I. I is a model of some TBox if and only if each
ct-type induced by I is a ct-model of the TBox.
Proposition 2 Let T be a DL-Litecore TBox and I a DL interpretation. Then I  |T | iff
 (I, d)  |T |tc for each d  I .
Moreover, for each ct-model  of a TBox, a DL model of the TBox can be constructed from
 by reversing the inducing process.
Proposition 3 Let T be a DL-Litecore TBox and  a ct-model of T . Then there are I  |T |
and d  I such that  (I, d) =  .
Through these connections, we can prove that the entailments over DL-Litecore TBoxes
axioms induced by ct-type semantics are identical to those induced by DL semantics.
Theorem 1 Let T be a DL-Litecore TBox and  a conjunction of DL-Litecore TBox axioms.
Then T |=  iff T |=tc .
Thus ct-type semantics is as effective as DL semantics in characterising the standard inferences tasks of DL-Litecore TBoxes. In comparison with DL semantics, ct-type semantics has
the clear advantage of being finite and more succinct. While a DL-Litecore TBox usually
has an infinite number of DL models it always has a finite number of ct-models.
Proposition 4 Let T be a DL-Litecore TBox. Then T has at most 2n ct-models, for n the
number of basic concepts.
If we are working with a coherent TBox T , then ct-type semantics shares one more
property with DL semantics, that is the set of ct-models of T is identical to the intersection
of the set of ct-models of each axiom in T . The property turns out to be essential for
developing algorithms for eliminating and incorporating axioms over DL-Lite KBs. It allows
us to deal with each axiom one by one in a model-theoretic setting.
336

fiDL-Lite Contraction and Revision

Theorem 2 Let T be a DL-Litecore TBox such that T = {1 , . . . , n }. If T is coherent,
then |T |tc = |1 |tc      |n |tc .
As we have shown, ct-type semantics shares many crucial properties with DL semantics,
however it differs from the DL one in dealing with unions of axioms.
Theorem 3 Let T be a DL-Litecore TBox and ,  DL-Litecore TBox axioms. If |T | 
||  || then |T |tc  ||tc  ||tc but the converse does not necessarily hold.
For a counter example, suppose B is {A, B, C, D}, T is {A v D},  is A v B, and 
is C v D. Lets work out their ct-models. A ct-type does not satisfy A v D only if it
contains A but not D, so we can get the ct-models of T by eliminating all such unsatisfying
ct-types from the set of ct-types, that is |T |tc = c \ {AB, AC, ABC}. Similarly, a ct-type
does not satisfy both A v B and C v D only if it contains A and C but not B or D, so
||tc  ||tc = c \ {AC}. Note that we have |T |tc  ||tc  ||tc . Now let a DL interpretation
I be such that I = {a, b}, AI = {a}, B I = {b}, C I = {b}, and DI = {a}. Since I |= T ,
I 6|= A v B, and I 6|= C v D, we have |T | 6 ||||. Roughly speaking, the reason for such
behaviour is that type semantics (and all its simplified versions) is a variant of propositional
semantics and it lacks the first-order structure in DL semantics. Identification of such
behaviour turns out to be crucial in proving the representation theorem for our contraction
functions.
Most DLs have the inexpressibility problem that some sets of DL interpretations have no
syntactic representation. It is no exception for DL-Litecore under ct-type semantics. Given
a set of ct-types M , there may not be a DL-Litecore TBox T whose set of ct-models is M .
In such cases, we define a corresponding DL-Litecore TBox for M to be one that has the
minimal set of ct-models including M .
Definition 2 Let M be a set of ct-types. A DL-Litecore TBox T is a corresponding DL0
Litecore TBox for M iff M  |T |tc and there is no DL-Litecore TBox T such that M 
0
|T |tc  |T |tc .
Given a set of ct-types, we may have several corresponding TBoxes. Let B consists
of {R, R , A} and M a set of ct-types consists of A, , and R. Note that there is a
ct-type in M that contains R but none of them contains R . By Proposition 1, any
TBox whose set of ct-models contains M must have a ct-model that contains R . Since,
for the current set of basic concepts B, there are four ct-types containing R which are
R , R A, R R, and R AR, we have four corresponding TBoxes for M which are
{A v R, A v R , R v R }, {A v R, R v A, R v R }, {A v R, A v
R , R v R}, and {R v A, R v R}, each having M and one of the above
ct-types as its ct-models.
As shown in the example, if R is in some ct-types of M but R is not, then we have
several ways of generating a corresponding TBox. Intuitively if M has both of the concepts,
then we dont have that many choices but one for generating a corresponding TBox. Clearly,
such M is role-complete and we can show that role-completeness is sufficient to guarantee
the uniqueness of corresponding TBox.
Theorem 4 Let M be a set of ct-types. If M is role-complete, then there is a unique
corresponding DL-Litecore TBox for M .
337

fiZhuang, Wang, Wang, & Qi

3.2 Characterising DL-LiteR TBoxes
Ct-type semantics is able to characterise the entailments over DL-Litecore TBox axioms,
but not those over DL-LiteR ones, as they involve role inclusions. In this subsection, we
present t-type semantics which is able to do so.
To characterise entailments involving role inclusions, we need to introduce the copy
B 0 of the set of basic concepts B and the copy R0 of the set of basic roles R. So, if
B = {A, R, R } and R = {R, R }, then B 0 = {A0 , (R)0 , (R )0 } and R0 = {R0 , (R )0 }.
We also need the notion of extension of a DL-LiteR TBox. Let T be a DL-LiteR TBox.
Then its extension, denoted as T  , is the TBox obtained by adding to T a new concept
inclusion for each concept inclusion B v C in T and a new role inclusion R0 v E 0 for each
role inclusion R v E in T . Note that if C = B, then C 0 denotes B 0 ; and if E = R,
then E 0 denotes R0 .
A t-type  is a possibly empty set of basic concepts, basic roles, and their copies (i.e.,
  B  R  B 0  R0 ). Intuitively, a t-type combines two ct-types (for pairs of individuals)
and a set of roles (between these pairs of individuals). For any pair of individuals a, b such
that (a, b) is a relation captured by a role R, the B part of a t-type aims to characterise
the constraints to a (in the same way as a ct-type characterises the constraints to each
individual); the B 0 part aims to characterises the constraints to b (in the same way as
a ct-type characterises the constraints to each individual); and the R  R0 part aims to
characterise the constraints to R (which a ct-type does not have to consider). We denote
the set of all t-types as tr .
If we consider basic concepts, basic roles, and their copies as propositional atoms, and
concept inclusion B v C and role inclusion R v E as propositional formulas B  C and
R  E, then a t-type is nothing but an interpretation (represented by atoms interpreted
as true) in propositional logic. For a DL-LiteR TBox T , we use kT ktr to denote the set of
propositional models of the corresponding propositional formulas of T  . Note that kT ktr 
tr .
DL-Litecore permits non-propositional entailments, so does DL-LiteR . While there is
only one group of non-propositional entailments for DL-Litecore , four more can be identified
for DL-LiteR . (1) Apart from implying R v , R v  also implies R v R and R v
R . (2) The role inclusion R v S implies the concept inclusion R v S, R v S  ,
and the role inclusion R v S  . (3) The role inclusion R v S implies R v S  . (4)
The concept inclusion R v S implies the role inclusion R v S.
In the following, we give the conditions under which a t-type satisfies a DL-LiteR TBox
T . We call such t-types t-models of the TBox.
Definition 3 A t-model  of a DL-LiteR TBox T is a t-type such that
1.   kT ktr ;
2. if T |= R v  then R 6  and (R)0 6  ;
3. if T |= R v S then R   implies S   , and (R)0   implies (S)0   ;
4. if R   then R   and (R )0   ;
5. R   iff (R )0   for each R  R.
338

fiDL-Lite Contraction and Revision

For a t-type to satisfy T , firstly it has to be a propositional model of T , this takes care
of the propositional entailments. Then conditions 25 take care of the four groups of nonpropositional entailments summarised earlier. Conditions 4 and 5 are required for any t-type
to be a t-model (independent from the TBox), and are referred to as model conditions for
t-type semantics. Note that the use of copies of basic concepts and basic roles are necessary.
Consider a TBox T with two axioms R v A and A v S  . T entails R v S  and
R v S (by (3) and (4)). We would expect the t-models of T also satisfy R v S. If copies
are discarded, a t-type  = {R, S, R, S} would be a t-model of T (omitting R0 and (R)0
in Definition 3), yet  clearly does not satisfy R v S. This cannot be resolved by adding
a condition in Definition 3 (for details refer to the proof of Lemma 7 in Appendix B).
Example 2 (contd Example 1) Consider another role that associates diseases to some
anatomic sites, Disease Has Associated Anatomic Site (Das), and the DL-LiteR TBox T obtained by adding the role inclusion Loc v Das to T .
Some t-models of T are 10 = {HD, Loc, Das, Loc, Das, (Loc )0 , (Das )0 , (Loc )0 , (Das )0 ,
(CS)0 , (OS)0 }, 20 = {Loc , Das , CS, OS, Das , (Das)0 , (Das)0 }, and 30 = {RS, OS}.
Given a DL-LiteR TBox T , we denote the set of t-models of T as |T |tr . The t-models
of a conjunction of DL-LiteR TBox axioms are denoted and defined in the same manner as
ct-type semantics. The t-models of a negated (conjunction of) axiom(s) ,5 denoted as
||tr , is defined as
{  tr \ ||tr |  satisfies model conditions}.
The notions of entailment, logical equivalence, and consistency under t-type semantics are
defined in the same manner as DL semantics. Under t-type semantics, a TBox T entailing
a conjunction  of axioms is written as T |=tr .
As for DL-Litecore , we can establish a connection between the DL models and t-models
of a DL-LiteR TBox. Let I be a DL interpretation and d, e a pair of (not necessarily
distinct) elements in the domain of I. Then I, d and e induce a t-type as follows
 (I, d, e) ={B  B | d  B I }  {R  R | (d, e)  RI }
{B 0  B 0 | e  B I }  {R0  R0 | (e, d)  RI }.
We call  (I, d, e) the t-type induced by d and e in I. We can show that for each DL
interpretation I, I is a DL model of some TBox if and only if each t-type induced by I is
a t-model of the TBox.
Proposition 5 Let T be a DL-LiteR TBox T and I a DL interpretation. Then I  |T |
iff  (I, d, e)  |T |tr for each pair of d, e  I .
Moreover, given a t-model  of a TBox T , a DL model of T can be constructed from  by
reversing the inducing process.
5. For simplicity, we assume that Definition 3 does not apply to tautologies and define, for any tautological
axiom , ||tr = tr and ||tr = .

339

fiZhuang, Wang, Wang, & Qi

Proposition 6 Let T be a DL-LiteR TBox and  a t-model of T . Then there is I  |T |
and d, e  I such that  (I, d, e) =  .
Through these connections, we can prove that t-type semantics induces the same set of
entailments over DL-LiteR TBox axioms as that induced by DL semantics.
Theorem 5 Let T be a DL-LiteR TBox and  a conjunction of DL-LiteR TBox axioms.
Then T |=  iff T |=tr .
Extended with roles and copies of basic concepts, the number of t-types is more than
that of ct-types. However, compared with DL semantics, t-type semantics still has the
advantage of being finite and more succinct.
Proposition 7 Let T be a DL-LiteR TBox. Then T has at most 22n+2m t-models, for n
the number of basic concepts and m that of basic roles.
For sets of ct-types, we proposed a condition called role-complete. The condition characterises the property of ct-type semantics such that the set of ct-models for any DL-Litecore
TBox is role-complete, and any role-complete set of ct-types corresponds to a unique DLLitecore TBox. Now we give the corresponding role-complete condition for sets of t-types.
A set of t-types M is role-complete if all t-types in M satisfy the model conditions for
t-type semantics, and for all R  R, whenever there is a t-type  in M such that R  
or (R)0   , then there is a t-type  0 in M such that {R, R0 }   0 6=  ( and  0 may be
identical).
For a set of t-types M , the corresponding DL-LiteR TBoxes of M are defined in the same
way as for a set of ct-types. Also it can be shown analogously that |T |tr is role-complete
for any DL-LiteR TBox T and M being role-complete guarantees the existence of a unique
corresponding DL-LiteR TBox.
Theorem 6 Let M be a set of t-types. If M is role-complete, then there is a unique
corresponding DL-LiteR TBox for M .
Moreover, properties of ct-type semantics on conjunctions and unions of axioms (i.e., Theorem 2 and Theorem 3) also hold for t-type semantics.
So far we have shown that t-type semantics possesses every property of ct-type semantics,
except the number of possible models. What about their connections? In fact, t-type
semantics generalises ct-type semantics in the sense that for a DL-Litecore TBox T , the
ct-models of T are exactly the B-projections of the t-models of T .
Proposition 8 Let T be a DL-Litecore TBox. Then |T |tc = {  B |   |T |tr }.
Hence, t-types contain more information than ct-types, and they are more than enough to
capture the semantics of DL-Litecore TBoxes.
Finally, we extend the notion of coherence to sets of t-types. A set of t-types M is
coherent if and only if all t-types in M satisfy the model conditions for t-type semantics,
and M does not satisfy B v  or R v R for each B in B and each R in R (i.e.,
M 6 |B v |tr and M 6 |R v R|tr ). If M is a coherent set of t-types, then every basic
340

fiDL-Lite Contraction and Revision

concept and role are contained in some t-types of M . Therefore a coherent set of t-types
is always role-complete. It will be clear that, in defining contraction and revision functions
for DL-LiteR TBoxes, the sets of t-types we will encounter are always coherent thus are
role-complete which means we always have unique corresponding TBoxes. We let Tr be a
function that takes as input a set of t-types M and is such that if M is coherent, then
Tr (M ) is the closure of the corresponding DL-LiteR TBox, otherwise Tr (M ) = T .
3.3 Characterising DL-LiteR ABoxes (with a Background TBox)
T-type semantics extends ct-type semantics with the ability of characterising entailments
involving role inclusions. Both of them, however, are incapable of characterising entailments
over ABox axioms. In this subsection, we introduce a-type semantics which is able to do
so. As for TBoxes, we can also reduce the standard inferences tasks of ABoxes to that of
checking an entailment relationships between some ABox axioms. Thus in defining a-type
semantics, it suffices to focus on such entailment relationships.
Although our focus is on entailments over ABox axioms, it is important to note that
such entailments are induced by a background TBox. For example, if A(a) entails B(a),
then it must be that the background TBox entails A v B. For the sake of simplicity, we
sometimes denote an ABox A as AT to indicate that the background TBox is T .
A TBox captures subsumption relationships between concepts (i.e., concept inclusions)
and those between roles (i.e., role inclusions) whereas an ABox captures assertions over
individuals (i.e., concept assertions) and pairs of individuals (i.e., role assertions). In an
ABox, an individual can be asserted to be an element of a basic concept and a pair of
individuals can be asserted to an element of a basic role. To this end, we introduce a
copy B a of B for each a in D and a copy Rab of R for each pair of a, b in D. So, if
B = {A, R, R }, R = {R, R }, and D = {a, b}, then B a = {Aa , Ra , (R )a }, B b =
{Ab , Rb , (R )b }, Rab = {Rab , (R )ab }, Rba = {Rba , (R )ba }, Raa = {Raa , (R )aa }, and
Rbb = {Rbb , (R )bb }.
empty set of such copies of basic concepts
S An a-type
S  is a possibly
a
ab
and roles (i.e.,   aD B  a,bD R ). We denote the set of all a-types as ar .
For any DL-LiteR TBox T , we let T a be the TBox that consists of for each concept
inclusion B v C in T and each individual a in D, a concept inclusion B a v C a , and for each
role inclusion R v E in T and each pair of individuals a, b in D, a role inclusion Rab v E ab .
If we consider copies of basic concepts and roles as propositional atoms, concept inclusion
B v C as propositional formula B  C, role inclusion R v E as propositional formula
R  E, concept assertion A(a) as propositional formula Aa , (i.e., an atomic formula) and
role inclusion P (a, b) as propositional formula P ab , (i.e., an atomic formula) then an a-type
is nothing but an interpretation (represented by atoms interpreted as true) in propositional
logic. For a DL-LiteR ABox AT we use kAT kar to denote the set of propositional models of
the corresponding propositional formulas of T a and A. Note that kAT kar  ar .
Since entailments over axioms of an ABox AT have a lot to do with axioms of the
background TBox T , we have to embed some facilities of t-type semantics6 into a-type
semantics. With these considerations, the conditions under which an a-type satisfies a
DL-LiteR ABox AT is defined as follows and we call such a-types a-models of the ABox.
6. Note that conditions 15 in Definition 4 are adapted from those in Definition 3.

341

fiZhuang, Wang, Wang, & Qi

Definition 4 An a-model  of a DL-LiteR ABox AT is an a-type such that
1.   kAT kar ;
2. if T |= R v  then (R)a 6  for each a  D;
3. if T |= R v S then (R)a   implies (S)a   for each a  D;
4. if Rab   then (R)a   and (R )b   ;
5. Rab   iff (R )ba   for each R  R and each pair of a, b  D.
Similarly, conditions 4 and 5 are referred to as model conditions for a-type semantics.
Example 3 (contd Example 2) Consider DL-LiteR KB K = (T , A) where A consists
of a concept assertion HD(d) and a role assertion Loc(d, s). An a-model of AT is  00 =
{HDd , (Loc)d , (Das)d , Locds , Dasds , (Loc )sd , (Das )sd , (Loc )s , (Das )s , CSs , OSs }.
We denote the set of a-models of a DL-LiteR ABox AT as |AT |ar . The a-models of a
conjunction 1  2     n of DL-LiteR ABox axioms with respect to a background TBox
T , denoted as |(1  2     n )T |ar , is defined as
|(1  2     n )T |ar = |{1 , 2 , . . . , n }T |ar .
The a-models of a negated (conjunction of) axiom(s)  with respect to a background TBox
T , denoted as |T |ar , is defined as
|T |ar \ |T |ar
where |T |ar is the set of a-models of an empty ABox when the background TBox is T .
The notions of entailment, logical equivalence, and consistency under a-type semantics are
defined in the same manner as DL semantics. Under a-type semantics, an ABox AT entailing
a conjunction  of ABox axioms is written as AT |=ar .
We can establish a connection between the DL models and the a-models of a DL-LiteR
ABox, and this connection is a tighter one. Compared with ct-type and t-type semantics, atype semantics contains information about individuals, thus is closer to a DL interpretation.
For this reason, each DL interpretation I induces exactly one a-type. The inducing process
is as follows.
 a (I) ={B c  B c | c  D, cI  B I }  {Rcb  Rcb | c, b  D, (cI , bI )  RI }.
We call  a (I) the a-type induced by I. We can show that for each DL interpretation I, I
is a model of a KB K = (T , A) if and only if I is a model of T and the a-type induced by
I is an a-model of the ABox AT .
Proposition 9 Let K = (T , A) be a DL-LiteR KB and I a DL interpretation. Then
I  |K| iff I  |T | and  a (I)  |AT |ar .
Moreover, given an a-model  of a ABox AT , a DL model of the KB K = (T , A) can be
constructed from  by reversing the inducing process.
342

fiDL-Lite Contraction and Revision

Proposition 10 Let K = (T , A) be a DL-LiteR KB and  an a-type. If   |AT |ar , then
there is I  |K| such that  a (I) =  .
Through these connections, we can show that a-type semantics induces the same set of
entailments over DL-LiteR ABox axioms (with a background TBox) as that induced by DL
semantics.
Theorem 7 Let K = (T , A) be a DL-LiteR KB,  a conjunction of DL-LiteR ABox axioms.
Then K |=  iff AT |=ar .
Although we include multiple copies of basic concepts and roles to capture DL-LiteR
ABoxes, the semantics is still more superior than DL semantics in terms of finiteness and
succinctness.
2

Proposition 11 Let A be a DL-LiteR ABox. Then AT has at most 2nm+n l a-models, for
n the number of individuals, m that of basic concepts, and l that of basic roles.
Moreover properties of ct-type and t-type semantics on conjunctions and unions of TBox
axioms also hold for a-type semantics on ABox axioms.
It is important to note that, if the TBoxes T and T 0 are not equivalent, then the amodels of AT may be different from those of AT 0 (i.e., |AT |ar 6= |AT 0 |ar ). Thus to identify
the corresponding ABoxes for a set of a-types we have to fix the background TBox.
Definition 5 Let T be a DL-LiteR TBox and M a set of a-types. Then a corresponding
DL-LiteR ABox for M with respect to T is a DL-LiteR ABox A such that M  |AT |ar and
there is no DL-LiteR ABox A0 such that M  |A0T |ar  |AT |ar .
Note that if an ABox A is an empty set, then the set of a-models of AT is not the set
of all a-types, as we still have the background TBox to restrict the set of satisfying a-types.
For a set of a-types M and a DL-LiteR TBox T , we say M is consistent with T if all a-types
in M are a-models of the empty ABox with T as the background TBox (i.e., M  |T |ar ).
We can show that for a set of a-types M , consistency with the background TBox ensures
the existence and uniqueness of corresponding DL-LiteR ABox for M with respect to T .
Theorem 8 Let T be a DL-LiteR TBox T and M a set of a-types. If M is consistent with
T , then there is a unique corresponding DL-LiteR ABox for M with respect to T .
We let ATr be a function that takes as input a set of a-types M and is such that if M
is consistent with T , then ATr (M ) is the closure of the corresponding DL-LiteR ABox with
respect to T , otherwise ATr (M ) is the inconsistent ABox A .
3.4 Characterising DL-LiteR KBs
We have shown that t-type and a-type semantics are capable of characterising respectively
entailments over the TBox of a KB and those over the ABox. Intuitively, by combining the
two we can characterise entailments over the KB. In fact this is how the full version of type
semantics is defined.
343

fiZhuang, Wang, Wang, & Qi

0
Recall that a t-type is any subset of
R  R0 where B 0 and R0 are copies of B
S B  Ba  S
and R and an a-type is any subset of aD B  a,bD Rab where B a and Rab are copies
of B and R for each individual a and each pair of individuals
S a, b. AStype  is the union of
a pair of t-type and a-type, that is   B  B 0  R  R0  aD B a  a,bD Rab . We denote
the set of all types as r .
0
Note that for any type  , its t-type part t can be obtained by intersecting
 with
S
S B Bab
0
a
R  R and its a-type part a can be obtained by intersecting  with aD B  a,bD R .
A type  satisfies a KB K = (T , A) if and only if t is a t-model of T and a is an a-model
of AT . We call such types type models of K.

Definition 6 A type model  of a DL-LiteR KB K = (T , A) is a type such that t  |T |tr
and a  |AT |ar .
We denote the set of type models of a DL-LiteR KB K as |K|r . The type models of
a conjunction and a negation of DL-LiteR axioms7 are denoted and defined in the same
manner as ct-type semantics. The notions of entailment, logical equivalence, and consistency under type semantics are defined in the same manner as DL semantics. Under type
semantics, a KB K entailing an DL-LiteR axiom  is written as K |=r .
We can establish a connection between the DL models and the type models of a KB.
Let I be a DL interpretation. For each pair of (not necessarily distinct) elements d, e in the
domain of I, d and e induce a type as follows.
  (I, d, e) =  (I, d, e)   a (I).
We call   (I, d, e) the type induced by d and e in I. Note that  (I, d, e) induces a t-type
and  a (I) induces an a-type which forms receptively the t-type and the a-type part of the
induced type. We can show that for each DL interpretation I, I is a model of some KB if
and only if each type induced by I is a type model of the KB.
Proposition 12 Let K be a DL-LiteR KB and I a DL interpretation. Then I  |K| iff
  (I, d, e)  |K|r for each pair d, e  I .
Also we can construct a DL model of some KB from each type model of the KB.
Proposition 13 Let K be a DL-LiteR KB. If   |K|r , then there is I  |K| and d, e  I
such that   (I, d, e) =  .
Through these connections, we can show that type semantics induces the same set of entailments over DL-LiteR axioms as that induced by DL semantics.
Theorem 9 Let K be a DL-LiteR KB and  a conjunction of DL-LiteR axioms. Then
K |=  iff K |=r .
Since type semantics is obtained by combining t-type and a-type semantics, it inherits
the finiteness and succinctness properties from the two.
7. A DL-LiteR axioms is either a DL-LiteR TBox axiom or DL-LiteR ABox axiom.

344

fiDL-Lite Contraction and Revision

2

Proposition 14 Let K be a DL-LiteR KB. Then K has at most 2(n+2)m+(n +2)l type models,
for n the number of individuals, m that of basic concepts, and l that of basic roles.
Now we give the corresponding role-complete condition for sets of types. A set of types
M is role-complete if all types in M satisfy the model conditions for t-type semantics
and a-type semantics, and for any R  R, whenever there is a type  in M such that
R   , (R)0   , or (R)a   for some a  D, then there is a type  0 in M such that
{R, R0 , Rbc }   0 6=  for some b, c  D ( and  0 may be identical, and any pair among a, b, c
may be identical).
For a set of types M , the corresponding KB is defined in the same way as ct-type
semantics. Also it can be shown analogously that |K|r is role-complete for any DL-LiteR
KB K and M being role-complete guarantees the existence of a unique corresponding DLLiteR KB.
Theorem 10 Let M be a set of types. If M is role-complete, then there is a unique corresponding DL-LiteR KB for M .
By now we have introduced all versions of type semantics, ranging from the simplest
ct-type to the most comprehensive one presented in this subsection. Assuming the same
signature, we have tc  tr , ar  r , and ar  tr  r . Their characterising abilities
which are depicted in Figure 1 match these subset relationships.

4. Axiom Elimination
In this section, we deal with the elimination of axioms from DL-Lite KBs. There are several
application scenarios for such elimination, which are (1) to eliminate axioms from a TBox
while no ABox is considered; (2) to eliminate axioms from an ABox while a background
TBox is assumed and remains unchanged; and (3) to eliminate axioms from a KB while
both of its TBox and ABox are considered and subject to change. As discussed in the
previous section, although type semantics can be used in all scenarios, it will be a waste of
computational power to use it for scenarios (1) and (2) for which the simpler ct-type, t-type
and a-type semantics can be used. We will only pursue scenario (1) as the other two can
be handled in the same manner. The only difference is that in those scenarios we have to
switch the underlying semantics to a-type and to type semantics.
.
Our strategy for axiom elimination is to define a contraction function  that takes as
input a logically closed TBox T and a conjunction of TBox axioms  and returns as output
.
a TBox T  such that  is not entailed. For convenience, T is called the original TBox, 
.
the contracting axiom, and T  the contracted TBox.
In defining the contraction functions, our approach is inspired by that of Katsuno and
Mendelzon (1992). However, we take a more general approach in which no explicit ordering
over models is assumed and instead of propositional models we work with t-type models.
Also we assume the original TBox is coherent.
We only present contraction functions for DL-LiteR TBoxes as those for DL-Litecore
ones can be defined and instantiated analogously. Thus KBs, TBoxes, ABoxes, and axioms
are assumed to be DL-LiteR ones throughout this section.
345

fiZhuang, Wang, Wang, & Qi

4.1 Eliminating Axioms from a TBox
Intuitively, if the model set of a TBox contains some counter-models of an axiom  (i.e,
models of ) then the TBox does not imply . Thus, to eliminate an axiom  from a TBox
T we can first add some counter-models of  to those of T to form an intermediate model
set then obtain the corresponding TBox of the model set. Since the intermediate model set
contains counter-models of , we can be sure that the obtained TBox does not entail .
Note that to apply this approach, a decision has to be made on which counter-models
to add. The extralogical information required for making the decision could be provided
by a domain expert of the KB. To study the theoretical properties we assume there is a
selection function that plays the role of decision making. A limiting case is when the set of
counter-models is empty that is the contracting axiom is a tautology. As it is not possible
to stop a TBox from implying a tautology, a convenient and reasonable way is to do nothing
and return the original TBox. In line with this intuition, a selection function should return
the empty set in such cases. Formally,  is a selection function if and only if for any set of
t-types M , (M ) is a non-empty subset of M unless M is empty.
Selection function has to be further restricted to handle the special case when T does not
entail . In this case, the model set of T contains counter-models of . Intuitively, if asked
to eliminate an axiom that is not entailed by the TBox then nothing has to be done and
the original TBox should be returned as the outcome. In line with this intuition, a selection
function is required to be faithful such that if the intersection of models of T and those of
 is not empty, then the selection function picks the intersecting models and no others.
Formally, a selection function  is faithful with respect to a TBox T if (M ) = |T |tr  M
whenever |T |tr  M 6= , for any set of t-types M .
With the above considerations, our contraction function called T-contraction function
is defined as follows. Recall that Tr is a function that takes as input a set of t-types M
and is such that if M is coherent, then Tr (M ) is the closure of the corresponding DL-LiteR
TBox, otherwise Tr (M ) = T .
.
Definition 7 A function  is a T-contraction function for a TBox T iff for all conjunctions
of TBox axioms 
.
T  = Tr (|T |tr  (||tr ))
where  is a faithful selection function for T .
Note that each r-model in the intermediate model set |T |tr  (||tr ) satisfies the model
conditions for t-type semantics, and since the original TBox T is assumed to be coherent,
|T |tr  (||tr ) which includes models of T must be coherent.
Now we present properties of T-contraction functions. It is commonly accepted that
the AGM postulates for contraction best capture the desirable properties of contraction
functions. In the following, we adapt the AGM postulates and some of their alternatives
to the current contraction problem where T is a closed TBox and ,  are conjunctions of
TBox axioms.
.
.
.
(T 1) T  = cl(T )
.
.
(T 2) T   T
346

fiDL-Lite Contraction and Revision

.
.
(T 3) If T 6|= , then T  = T
.
.
(T 4) If 6|= , then T  6|= 
.
.
(T 5) T  cl((T )  {})
.
.
.
(T de) If T |=  and |T |tr  ||tr  ||tr then T  |= 
.
.
.
(T 6) If   , then T  = T 
.
According to the postulates, a contraction function is syntax-insensitive (T 6) and
.
produces a closed TBox (T 1) which does not entail the contracting axiom unless it is a
.
.
tautology (T 4). The produced TBox is not larger than the original one (T 2). If the
.
contracting axiom is not entailed, then nothing has to be done (T 3).
.
The AGM origin of (T 5) is called Recovery and is the main postulate for formalising
the minimal change principle for contraction. It requires the information loss during contraction to be minimal such that the original TBox can be recovered through expanding
the contracted TBox by the contracting axiom. Recovery has been criticised by many researchers among which Hansson (1991) argued that it is an emerging property rather than
a fundamental postulate for contraction. One evidence is that other than the contraction
itself, its satisfaction relies also on properties (viz, AGM-compliance) of the underlying logic
(Ribeiro et al., 2013). In particular most of the DLs including DL-Lite are incompatible
with Recovery.
Due to the controversy of Recovery, many have proposed alternative postulates for
capturing the minimal change principle. In the quest of a proper postulate for DL-Lite, we
notice that Recovery can be replaced by the following postulate of Disjunctive Elimination
(Ferme et al., 2008):
.
.
If   K and     K  then   K .
Disjunctive Elimination captures the principle of minimal change by stating the condition
for retaining a formula during a contraction. That is if a formula is in the original belief set
and its disjunction with the contacting formula is retained during the contraction then the
formula is retained. Since disjunction of axioms is not permitted in DL-Lite, in adapting the
.
postulate we describe the disjunction in terms of their models, thus the postulate (T de).
.
Notice that we use t-models instead of DL models in (T de). Due to the property of t-type
.
.
semantics on unions of axioms, we have |T |  ||  || implies |T |tr  ||tr  ||tr but
not vice versa. Thus using DL models instead of t-models enforces a stricter condition for
retaining  which means less number of axioms will be retained after the contraction. It is
obvious that the principle of minimal change favours the use of t-models.
.
.
.
.
We can show that a T-contraction function satisfies (T 1)(T 4), (T de), and (T 6)
and all functions satisfying these postulates are T-contraction functions. In other words,
the set of postulates fully characterises the properties of a T-contraction function.
.
.
.
Theorem 11 A function  is a T-contraction function for a TBox T iff  satisfies (T 1)
.
.
.
(T 4), (T de), and (T 6).
347

fiZhuang, Wang, Wang, & Qi

By now we have presented the definition of T-contraction functions and their properties.
It is should be clear that a T-contraction function cannot be seen as an update operator in
KB update literatures. Such operators (e.g., Winsletts operator, see Winslett, 1990) usually
apply a fixed rule or update semantics (e.g., WIDTIO) in determining the update outcome.
For T-contraction functions, the rule for deciding the contraction outcome is simulated by
the associated selection function. It is important to note that, we intentionally leave open
the details of the selection function except that we require it to be faithful. Thus it is
flexible enough to simulate any rules that respect the faithfulness condition. In fact, our
T-contraction function represents a general framework for dealing with changes to DL KBs
and it subsumes many update operators in the sense that the rules such operators applied
can be simulated by some faithful selection functions. In the remaining of this section, we
provide an algorithm called TCONT which implements one such operator.
Algorithm 1: TCONT
Input: TBox T and conjunction of TBox axiom 
Output: TBox T
1 if  is a tautology or T 6|=  then
2
return T := T ;

6

Let  = PickCounterModel ();
foreach   T do
if  6|=tr  then
T := T \ {};

7

return T := T ;

3
4
5

TCONT takes as input a TBox T and a conjunction of TBox axioms , and return as
output a TBox. TCONT first checks if  is a tautology or not implied by T (line 1) in which
case T is returned (line 2). Otherwise the procedure PickCounterModel is applied which
picks a counter-model  of  (line 3). Then TCONT checks the counter-model against each
axiom in T (line 4). If an axiom is not satisfied by  (line 5) then it is removed from T
(line 6). Finally, whatever is left of T is returned (line 7).
The procedure PickCounterModel takes a conjunction of TBox axioms  and return
a counter-model of . Essential, its goal is to obtain a t-model  of  and this can be
achieved for example through the following two steps: (1) Consider one conjunct 1 in ,
and if 1 = B v D with B, D  B, let  contain B but not D; otherwise if 1 = B v D
(or 1 = R v S with R, S  R), let  contain both B and D (resp., both R and S);
otherwise if 1 = R v S, let  contain R but not S (or contain R but not S). (2) Add
other elements to  so that it satisfies model conditions for t-type semantics.
TCONT runs in polynomial time with respect to the size of T and . In particular,
checking whether  is a tautology or T entails  takes polynomial time (line 1), the procedure
PickCounterModel as shown above runs in linear time, and each satisfiability check (line 5)
runs in linear time.
Proposition 15 Let T be a TBox and let  be a conjunction of TBox axioms. Then
TCONT (T , ) terminates and returns a TBox in polynomial time with respect to the size of
348

fiDL-Lite Contraction and Revision

.
.
.
T and  and if  is a function such that T  = TCONT (T , ), then  is a T-contraction
function.

Example 4 (contd Example 2) The logical closure of T contains axiom, among others,
Loc v RS, which is derived from Loc v CS and RS v CS.
To contract  := Loc v RS from T , TCONT takes both T and  as input. In
line 3, suppose a counter-model  = {Loc , Das , CS, RS, OS} is selected.  does not
satisfy Loc v RS or RS v CS, and hence these two axioms (and only these two) are
eliminated from the closure.

5. Axiom Incoporation
In this section, we deal with the incorporation of axioms into DL-Lite KBs. Similar to
axioms elimination, there are three application scenarios that are often encountered, which
are (1) to incorporate axioms into a TBox while no ABox is considered; (2) to incorporate
axioms into an ABox while a background TBox is assumed and remains unchanged; and
(3) to incorporate axioms into a KB while both of its TBox and ABox are considered and
subject to change. From previous discussions, it is best to use ct-type or t-type semantics
for scenario (1), a-type semantics for scenario (2), and for scenario (3) we have to use the
full version of type semantics. We will focus on the scenarios (1) and (2). In managing DL
KBs, scenario (3) is less common and it has been handled in (Wang et al., 2015) through a
similar approach.8
Similar to axiom elimination, our strategy is to define a revision function  that takes
as input a logically closed TBox T (or ABox AT ) and a conjunction of TBox axioms (resp.
ABox axioms)  and returns as output a TBox T   (resp. ABox AT  ) such that 
is entailed. For convenience, T (or AT ) is called the original TBox (resp. ABox ),  the
revising axiom, and T   (resp. AT  ) the revised TBox (resp. ABox ). In defining such
functions, we assume the original TBox is coherent; the original ABox is consistent with
the background TBox9 and the background TBox is itself coherent.
In the AGM framework, revision can be constructed indirectly through contraction via
.
the Levi identity (Levi, 1991). Formally, let  be a contraction function for a belief set K,
.
a revision function  for K can be defined as K   = Cn((K )  {}) for all formulas .
Since the syntax of DL-Lite does not permit axiom negation the approach is not applicable
for DL-Lite. We will define revision functions directly in a model-theoretic approach. As
for contraction the approach is inspired that of Katsuno and Mendelzon (1992) and is based
on type semantics.
We only present revision functions for DL-LiteR as those for DL-Litecore can be defined
and instantiated analogously. Thus KBs, TBoxes, ABoxes, and axioms are assumed to be
DL-LiteR ones throughout the section.
8. They also proposed an alternative semantic characterisation for DL-Lite but used structures that could
be exponentially larger than a type. Hence, a polynomial time algorithm is not available.
9. To be consistent with the background TBox, the ABox itself has to be consistent.

349

fiZhuang, Wang, Wang, & Qi

5.1 Incorporating Axioms into a TBox
We start with the revision function for incorporating axioms into a TBox. Before presenting
the function, we need to clarify a fundamental difference between AGM revision and revision
over DL TBoxes (TBox revision for short). AGM revision aims to incorporate new beliefs
while resolving any inconsistency. TBox revision goes beyond inconsistency resolving. In
addition to consistency, meaningful DL TBoxes have to be coherent, thus TBox revision has
to resolve both the inconsistency and incoherence caused in incorporating new axioms.10
Now we give the intuitions behind our revision function. If the model set of a TBox T
is the subset of that of an axiom  then T entails . Thus to incorporate an axiom  into
a TBox T , we can pick some models of  to form an intermediate model set then obtain its
corresponding TBox. Since the intermediate model set is the subset of that of , we can be
sure that the obtained TBox entails .
Note that to apply this approach, a decision has to be made on which models of  to
pick. As for contraction, a selection function is assumed. Previously, for contraction, a
selection function returns the empty set in the limiting case when the contracting axiom
is a tautology. Now the limiting case is when the revising axiom is incoherent. As it is
not possible to return a coherent TBox that entails an incoherent axiom, a convenient and
reasonable way is to do nothing and return the inconsistent TBox. Formally, we define
that, a function  is a selection function if and only if for any set of t-types M , (M ) is a
non-empty subset of M unless M is incoherent.
To illustrate the new definition of selection function, suppose in revising T by ,  is
an incoherent axiom. As discussed, in this case, the revision fails. Since  is incoherent, its
set of t-models must be incoherent. The definition of selection function guarantees that an
empty set of t-types is picked which means the revision outcome is the expected inconsistent
TBox for indicating failure of the revision.
The faithfulness condition also has to be modified from the contraction case. A selection
function  is faithful with respect to a TBox T if it satisfies
1. if M is coherent, then |T |tr  M  (M ), and
2. if |T |tr  M is coherent, then (M ) = |T |tr  M .
In revising T by , condition 1 deals with the case when models of T overlaps with those
of  which means T  {} is consistent. In line with the principle of minimal change, in
this case, the selection function has to pick all the overlapping models to preserve as many
as possible the original TBox axioms. Condition 2 deals with the case that not only the
overlapping exists but also it is coherent. Since in this case T  {} is not only consistent
but also coherent, the revision boils down to a set union operation (i.e., cl(T  {})). The
selection function therefore picks all the overlapping models and no others.
To illustrate the new notion of faithfulness, suppose in revising T by , the t-models of
 overlap with those of T (i.e., ||tr  |T |tr ) and the set of overlapping t-models is coherent.
Since there is no incoherence to resolve, the most intuitive way to deal with this revision is
to add  to T without making any further change, that is to have the union of  and T as
10. In fact we can concentrate on incoherence resolving when ABox is not considered. By its definition, a
coherent TBox must be consistent. Inconsistency resolving is thus a part of incoherence resolving.

350

fiDL-Lite Contraction and Revision

the revision outcome. Since the set of t-models of this union is ||tr  |T |tr and the revision
outcome is obtained by taking the corresponding TBox of the t-types picked by a selection
function, it is clear that in this revision the more intuitive selection function should picked
all t-types in ||tr |T |tr and no other. In our terms, we call such a selection function faithful.
In addition to faithfulness, the selection function has to guarantee the t-types picked
are coherent, thus we introduce the following condition. We say a selection function  is
coherent preserving if for all coherent sets of t-types M , (M ) is coherent.
With the above considerations, our revision function called T-revision function is defined
as follows.
Definition 8 A function  is a T-revision function for a TBox T iff for all conjunctions
of TBox axioms 
T   = Tr ((||tr ))
where  is a faithful and coherent preserving selection function.
Now we present properties of T-revision functions. Since AGM revision deals with
inconsistency, AGM revision postulates are formulated to capture the rationale behind
the inconsistency resolving process. TBox revision also deals with incoherence, thus the
postulates for TBox revision have to capture the rationale behind not only inconsistency
but also incoherence resolving. By replacing conditions on consistency with coherence,
AGM revision postulates are reformulated as follows for TBox revision, where T is a closed
TBox and ,  conjunctions of TBox axioms.
(T  1) T   = cl(T  )
(T  2) T   |= 
(T  3) If  is coherent, then T    cl(T  {})
(T  4) If T  {} is coherent, then cl(T  {})  T  
(T  5) If  is coherent, then T   is coherent
(T  6) If   , then T   = T  
(T  f ) If  is incoherent, then T   = T
According to the postulates, the revised TBox is closed (T  1), it entails the revising
axiom (T  2); if the revising axiom is coherent, then the revised TBox entails no axiom
other than those entailed by the original TBox and the revising axiom (T  3); if the revising
axiom causes no incoherence then the revised TBox is the closure of the original TBox and
the revising axiom (T  4). The revised TBox is coherent whenever the revising axiom is so
(T  5); Also the revision function is syntax-insensitive (T  6). Since the revising axiom has
to be in the revised TBox, if the revising axiom is itself incoherent then the revised TBox
must be so. The failure postulate (T  f ) requires that in such case we simply return the
inconsistent TBox. The purpose of TBox revision is to incorporate an axiom and resolve
any incoherence caused. If the input axiom is itself incoherent, then the revision is doomed
351

fiZhuang, Wang, Wang, & Qi

to be a failure. When it fails there is no ground to argue what a proper revision outcome
is, so it comes down to what convention to take. Following AGM revision, we take the
convention of returning the inconsistent TBox. The AGM origin of (T  f ) which states if
the revising formula is inconsistent then return the inconsistent belief set, is deducible from
the other AGM postulates thus is not postulated explicitly in the AGM framework.
We can show that a T-revision function satisfies (T  1)(T  6), and (T  f ) and all
functions satisfying these postulates are T-revision functions. In other words, the set of
postulates fully characterises the properties of a T-revision function.
Theorem 12 A function  is a T-revision function for a TBox T iff  satisfies (T  1)
(T  6) and (T  f ).
As for T-contraction function, a T-revision function is not an update operator, rather
it represents a general framework for incorporating axioms into DL-Lite TBox. As with
T-contraction function, a T-revision function subsumes many update operators. In the
following, we provide an algorithm called TREVI which implements one such operator.
Algorithm 2: TREVI
Input: TBox T and conjunction of TBox axioms 
Output: TBox T
1 if  is incoherent then
2
return T := T ;

8

// N is the set of atomic concepts and atomic roles in B  R
foreach F  N do
if T  {} |= F v F then
Let  = PickSatModel (, F );
foreach   T do
if  6|=tr  then
T := T \ {};

9

return T := cl(T  {});

3
4
5
6
7

TREVI takes as input a TBox T and a conjunction of TBox axioms , and return as
output a TBox. TREVI starts by checking whether  is incoherent (line 1), and if so it
returns the inconsistent TBox (line 2). Otherwise, it checks for each atomic concept and
each atomic role if it is unsatisfiable under the union of T and  (line 34). The union is
incoherent if and only if one such concept or role is unsatisfiable. For each unsatisfiable
concept or role F , the procedure PickSatModel is applied which picks a t-model  of  that
includes F (line 5). Then TREVI checks  against each axiom in T (line 6). If an axiom
is not satisfiable under  (line 7), then it is removed from T (line 8). Finally, the closure of
the union of whatever are left of T and  is returned (line 9).
The procedure PickSatModel takes a conjunction of TBox axiom  and an atomic concept or role F and return a t-model of  that includes F . This can be achieved for example
through the following four steps: (1) Let  contain F , and extend  so that it satisfies {}11
11. Recall that given a TBox T , T  represents the extension of T .

352

fiDL-Lite Contraction and Revision

propositionally. (2) If  |= R v S for some R, S  R and  contains R (or (R)0 ), then let
 contain S (resp., (S)0 ). (3) Further extend  so that it satisfies model conditions for
t-type semantics. (4) Repeat steps (1)(3) till  no longer changes.
TREVI runs in polynomial time with respect to the size of T and . In particular,
checking coherence of  (line 1) takes polynomial time, each concept or role satisfiability check (line 4) takes polynomial time, procedure PickSatModel as shown above takes
polynomial time, and each satisfiability check (line 7) takes linear time.
Proposition 16 Let T be a TBox and let  be a conjunction of TBox axioms. Then
TREVI (T , ) terminates and computes a TBox in polynomial time with respect to the size
of T and  and if  is a function such that T   = TREVI (T , ), then  is a T-revision
function.
Example 5 (contd Example 2) Adding  = Das v RS to T introduces incoherence,
i.e., T  {} |= Loc v , due to Loc v CS, RS v CS, and Loc v Das in T .
To revise T with , TREVI takes both T and  as input. In line 5, suppose t-type
 = {Loc , Das , CS, RS, OS} is picked.  does not satisfy Loc v RS or RS v CS,
and hence these two axioms (and only these two) are eliminated from the closure of T to
achieve a coherent union with .
5.2 Incorporating Axioms into an ABox
Now we turn to the revision function for incorporating axioms into an ABox. When dealing
with TBox revision, we argued that since meaningful TBox is coherent, an essential task of
the revision is incoherence resolving. Coherence is no longer an issue here, as we assume the
background TBox is coherent and it remains unchanged throughout the revision process.
Therefore we only need to concern with inconsistency resolving. Also note that we are
working with a-type semantics now.
The idea for defining T-revision function can also be used here. First, we pick some
models of the revising ABox axiom to form an intermediate model set, then return its
corresponding ABox as the revised ABox. The decision making on which models to pick is
again modelled by a selection function. Formally, a function  is a selection function if and
only if for any set of a-types M , (M ) is a non-empty subset of M unless M is empty.
Recall that by AT we mean an ABox A with a background TBox T . In revising AT
by an axiom , a special case is when the models of AT overlap with those of  indicating
the KB (T , A  {}) is consistent. Since there is no inconsistency to resolve, we can simply
return the union AT  {} as the revised ABox. In line with this intuition, a selection
function has to pick all the overlapping models and no other and we say the selection
function is faithful. Formally, a selection function  is faithful with respect to an ABox AT
if (M ) = |AT |ar  M whenever |AT |ar  M 6= .
With the above considerations, our revision function called A-revision function is defined
as follows. Recall that ATr is a function that that takes as input a set of a-types M and
is such that if M is consistent with T , then ATr (M ) is the closure of the corresponding
DL-LiteR ABox with respect to T , otherwise ATr (M ) is the inconsistent ABox A .

353

fiZhuang, Wang, Wang, & Qi

Definition 9 A function  is an A-revision function for an ABox AT iff for all conjunctions
of ABox axioms 
AT   = ATr ((|{}T |ar ))
where  is a faithful selection function.
Now we present properties of A-contraction functions. The AGM postulates for revision
are commonly accepted to capture the desirable properties of revision. In the following,
we adapt them to the current revision problem where AT is a closed ABox, and ,  are
conjunctions of ABox axioms.
(A  1) AT   = clT (AT  )
(A  2) AT   |=ar 
(A  3) AT    clT (AT  {})
(A  4) If |(A  {})T |ar 6= , then clT (AT  {})  AT  
(A  5) If |{}T |ar 6= , then |(AT  )T |ar 6= 
(A  6) If   , then AT   = AT  
(A  f ) If |{}T |ar = , then AT   = A
As incoherence resolving is out of the picture, the adapted postulates are like their AGM
origins concern with inconsistency resolving. According to the postulates, the revised ABox
is closed (A  1); it entails the revising axiom (A  2); it entails no axiom other than those
entailed by the original ABox and the revising axiom (A  3); it is the closure of the union
of the original ABox and the revising axiom if the revising axiom causes no inconsistency
(A  4), and it is consistent whenever the revising axiom is so (A  5); Also the revision
function is syntax-insensitive (A  6). In the limiting case that the revising axiom  is itself
inconsistent, since it is not possible to have a revised ABox that entails  and at the same
time be consistent, we take the convention to return the inconsistent ABox as the revised
ABox (A  f ).
We can show that an A-revision function satisfies (A  1)(A  6) and (A  f ) and all
functions satisfying these postulates are A-revision functions. In other words, the set of
postulates fully characterises the properties of an A-revision function.
Theorem 13 A function  is an A-revision function for an ABox AT iff  satisfies (A1)
(A  6), and (A  f ).
As for T-contraction and T-revision functions, an A-revision function is not an update
operator, rather it represents a general framework for incorporating axioms into DL-Lite
ABoxes. As with T-contraction and T-revision functions, an A-revision function subsumes
many update operators. In the following, we provide an algorithm called AREVI which
implements one such operator.
AREVI takes as input a TBox T , an ABox A, and a conjunction of ABox axioms ,
and return as output an ABox. AREVI first checks if  is inconsistent with T (line 1) in
354

fiDL-Lite Contraction and Revision

Algorithm 3: AREVI
Input: TBox T , ABox A, and conjunction of ABox axioms 
Output: ABox A
1 if  is inconsistent with T then
2
return A := A ;
3
4

if  is consistent with (T , A) then
return A := clT (A  {});

8

Let  = PickModel ();
foreach   A do
if  6|=ar  then
A := A \ {}

9

return A := clT (A  {});

5
6
7

which case the inconsistent ABox is returned (line 2). Otherwise, if the revising axiom is
consistent with the original ABox and the background TBox then the union of the axiom
and the original ABox is returned (lines 34). Otherwise the procedure PickModel is applied
which picks an a-model  of  (line 5). Then AREVI checks the a-model against each axiom
in A (line 6). If an axiom is not satisfied by  (line 7) then it is removed from A (line 8).
Finally, whatever is left of A is combined with  and their logical closure is returned (line
9).
The procedure PickModel takes an ABox axiom  and returns an a-model of . This can
be achieved for example through the following five steps: (1) Let  contain the propositional
forms of all conjuncts of  (recall that the propositional form of an ABox axiom A(a) is Aa ).
(2) Extend  so that it satisfies T a propositionally. (3) If T |= R v S for some R, S  R
and  contains (R)a for some a  D, then let  contain (S)a . (4) Further extend  so
that it satisfies model conditions for a-type semantics. (5) Repeat steps (2)(4) till  no
longer changes.
Proposition 17 Let AT be an ABox and  be a conjunction of ABox axioms. Then
AREVI (T , A, ) terminates and computes an ABox in polynomial time with respect to the
size of A  T and  and if  is a function such that AT   = AREVI (T , A, ), then  is
an A-revision function.
AREVI runs in polynomial time with respect to the size of T , A and . In particular,
checking consistency between  and T (line 1) and between  and (T , A) (line 3) both take
polynomial time. The procedure PickModel as shown above runs in polynomial time, and
each satisfiability check (line 7) takes linear time.
Example 6 (contd Example 3) Adding  = RS(s) to A introduces inconsistency, due
to axioms Loc(d, s) in A, Loc v CS and RS v CS in T .
To revise AT with , AREVI takes T , A, and  as inputs. In line suppose a-type  =
{HDd , (Loc)d , (Das)d , Dasds , (Das )sd , (Das )s , RSs , OSs } is picked.  does not satisfy
Loc(d, s), and hence this assertion (and only this one) are eliminated from the closure of
AT to achieve a consistent union with .
355

fiZhuang, Wang, Wang, & Qi

6. Related Work
In dealing with changes to DL KBs, many are like us considering it as a belief change problem (Qi et al., 2006; Qi & Du, 2009; Qi et al., 2008; Ribeiro & Wassermann, 2009; Wang
et al., 2015). Qi et al. (2006) gave a weakening based approach for revising ALC KBs. The
idea is to weaken the TBox axioms until all inconsistencies are resolved. Qi and Du (2009)
and Wang et al. (2015) adapted Dalals (1988) and Satohs operators (1988) respectively
for revising DL KBs. The main issue with these works is that their revision postulates
are not formulated appropriately to capture the rationales of incoherence resolving. Moreover, the adapted revision operators cannot guarantee coherence of the revision outcome.
In contrast to our approach, Qi et al. (2008) and Ribeiro and Wassermann (2009) studied
contraction and revision over TBoxes and KBs that are not necessarily closed. In particular, Qi et al. (2008) adapted kernel revision (Hansson, 1994). Ribeiro and Wassermann
(2009) adapted partial meet contraction and revision (Hansson, 1999) kernel contraction
and revision (Hansson, 1994), and semi-revision (Hansson, 1997).
Due to the popularity of DL-Lite, many have worked on the problem of updating DLLite KBs (De Giacomo et al., 2009; Calvanese et al., 2010; Kharlamov & Zheleznyakov,
2011; Kharlamov et al., 2013; Lenzerini & Savo, 2011, 2012). The update however has
very different meaning from the update operation in belief revision literatures (Katsuno &
Mendelzon, 1991). In these works, it can be interpreted as both contraction and revision
and they mainly focused on issues with the expressibility of the update outcome. We also
tackled the expressibility issues while assuming type semantics. Due to the succinctness
and finiteness of type semantics, the issue can be settled relatively easy. Of these works
that are more comparable to ours, Lenzerini and Savo (2011) dealt with instance level
update, that is the TBox remains unchanged and the ABox undergoes changes. Later
Lenzerini and Savo (2012) extended the approach to updating inconsistent KBs. The main
idea is to first obtain ABoxes (called repairs) that are consistent with the background
TBox; differ minimally from the original ABox; and accomplish the insertion or deletion
of certain axioms. Then the intersection of these repairs are taken as the update outcome.
The problem setting is similar to that for A-revision functions. Although they targeted a
more expressive DL-Lite (i.e., DL-LiteA,id ), when considering DL-LiteR their idea can be
simulated by our A-revision function. By restricting the associated selection function, an
A-revision function can always return the same outcome as their approach.
Grau et al. (2012) studied operations that contract and revise at the same time. A
constraint which states the set of axioms to be incorporated and those to be eliminated is
first specified. Then the operation maps a KB to another that satisfies the constraint. The
operation reduces to a revision and contraction function after making empty the so called
eliminating set and the incorporating set respectively. However, they did not identify the
postulates that characterise the contraction and revision functions. When working with
DL-Lite, such functions can be simulated by our T-contraction and T-revision function.
In a more general setting, Ribeiro et al. (2013) identified properties of a monotonic logic
under which a contraction function can be defined that satisfies the Recovery postulate. By
their result, DL-Lite is not one such logic, which is consistent with ours (i.e., Theorem 11).
Axiom negation is not supported by most DLs but is required in defining revision functions
through contraction functions via the Levi identity. Flouris, Huang, Pan, Plexousakis, and
356

fiDL-Lite Contraction and Revision

Wache (2006) proposed several notions of negated axioms for DLs. They also explored the
notions of inconsistent and incoherent TBoxes and emphasised the importance of resolving
incoherence in addition to inconsistency.
Similar to T-revision function, a group of works usually referred to as ontology debugging
also deals with unsatisfiable concepts (e.g., Kalyanpur et al., 2006). The method they used
are based on the notion of Minimal Unsatisfiability Preserving Sub-TBoxes (MUPS ). For
each unsatisfiable concept B, the MUPS based method first computes all the MUPSs for
B, then it computes a minimal hitting set for the MUPSs. The incoherence is then resolved
by removing axioms in the minimal hitting set. TREVI deals with the same problem in a
more efficient way. Roughly speaking each minimal hitting set for the MUPSs corresponds
to a t-model formed in line 5 of TREVI , thus we can avoid the computations of the MUPSs
and their minimal hitting sets which is a significant saving in computational power.

7. Conclusion
Due to the diversity of DLs, it is difficult if not impossible to come up with generalised
contraction and revision functions that work best for all DLs. Each DL is unique that
they deserve to be treated individually to make the most out of their uniqueness. A distinguishing feature of DL-Lite is that it only allows a restricted version of existential and
universal quantifiers. By taking advantage of this feature, we developed type semantics for
DL-Lite that resembles the underlying semantics for propositional logic. We then defined
and instantiated contraction and revision functions for DL-Lite KBs whose outcomes are
obtained by manipulating type models of the KBs and the contracting and revising axioms.
Our first contribution is the development of type semantics for DL-Lite. Given that type
semantics is equivalent to DL semantics in characterising the standard inference tasks of
DL-Lite, it outperforms DL semantics in terms of finiteness and succinctness. Our second
contribution is the axiomatic characterisation for the contraction and revision functions.
The key in obtaining the result for T-revision functions is the reformulation of AGM revision
postulates from inconsistency centred to incoherence centred. As TBox revision deals not
only with inconsistency but also incoherence, postulates for TBox revision must capture
the rationales behind incoherence resolving. Our third contribution is providing tractable
algorithms that instantiate the contraction and revision functions.
For future work, we plan to study contraction and revision for DLs that are more
expressive than DL-Lite. Since these DLs may allow unrestricted existential and universal
quantifiers, concepts can be formed through unbound nesting of quantifies. The semantic
characterisation of this kinds of concepts through type semantics may not be possible. We
need some other techniques that are tailored to these DLs.

Acknowledgement
We thank the anonymous reviews for their comments which helped improve the paper
substantially.
357

fiZhuang, Wang, Wang, & Qi

Appendix A
Before presenting the proofs for the technical results, we introduce some notions that will
simplify the presentation of the proofs.
First, given an ABox A, we write P  (a, b)  A to mean P (b, a)  A.
Now we present the notion of chase. Given a DL-LiteR (DL-Litecore ) ABox A and TBox
T , the chase of A w.r.t. T , denoted chaseT (A) is defined procedurally as follows: initially
take chaseT (A) := A, then exhaustively apply the following rules:
 if A(s)  chaseT (A), A v A0  T , and A0 (s) 6 chaseT (A), then chaseT (A) :=
chaseT (A)  {A0 (s)};
 if A(s)  chaseT (A), A v R  T , and there is no t such that R(s, t)  chaseT (A),
then chaseT (A) := chaseT (A)  {R(s, v)} where v is a fresh constant that has not
appeared in chaseT (A) before;
 if R(s, t)  chaseT (A), R v A  T , and A(s) 6 chaseT (A), then chaseT (A) :=
chaseT (A)  {A(s)};
 if R(s, t)  chaseT (A), R v S  T , and there is no t such that S(s, t)  chaseT (A),
then chaseT (A) := chaseT (A)  {S(s, v)} where v is a fresh constant that has not
appeared in chaseT (A) before;
 if R(s, t)  chaseT (A), R v S  T , and S(s, t) 6 chaseT (A), then chaseT (A) :=
chaseT (A)  {S(s, t)}.
Note that in the above rules, R and S can be a role or the inverse of a role. It is well
known that an ABox A induces a unique interpretation IA such that the domain of IA
consists of all the constants in A; for each concept name A, AIA = {d | A(d)  A}; and
for each role name P , P IA = {(e, f ) | P (e, f )  A}. In the proofs, we slightly misuse the
notation to let A denote also the interpretation induced by A. In this way, chaseT (A) can
also be used to denote an interpretation.
Finally, we present the notions of positive inclusions, negative inclusions, and closures of
negative inclusions. We call TBox axioms of the forms B v D and R v S positive inclusions
(PIs), and call axioms of the forms B v D and R v S negative inclusions (NIs), where
B, D  B and R, S  R. Given a DL-LiteR (DL-Litecore ) TBox T , the closure of NIs for
T , denoted cln(T ), is defined inductively as follows:
 All NIs in T are in cln(T );
 If B1 v B2  T and B2 v B3 or B3 v B2 is in cln(T ), then B1 v B3  cln(T );
 If R1 v R2  T and R2 v B or B v R2 is in cln(T ), then R1 v B  cln(T );
 If R1 v R2  T and R2 v B or B v R2 is in cln(T ), then R1 v B  cln(T );
 If R1 v R2  T and R2 v R3 or R3 v R2 is in cln(T ), then R1 v R3  cln(T );
 If R v R or R v R or R v R is in cln(T ), then all three are in cln(T ).
358

fiDL-Lite Contraction and Revision

It is clear from the above definition that T |= cln(T ). The following result is shown in
(Calvanese et al., 2007) which provides a method to build DL models using chase. The
result will be used to prove Propositions 3 and 6.
Lemma 1 Let (T , A) be a DL-LiteR (DL-Litecore ) KB. If A is a model of cln(T ), then
chaseT (A) is a model of (T , A).
Proof for Proposition 1
If there is   |T |tc such that R   for some R  R, then by condition 2 of Definition 1,
T 6|= R v . Thus, there is a model I of T such that RI 6= . Suppose (d, e)  RI , let
 0 =  (I, e). Then, R   0 . Also, by Proposition 2,  0  |T |tc .

Proof for Proposition 2
For the if direction, it suffices to show for each d  I and each axiom B v C in T ,
d  B I implies d  C I . Let  =  (I, d). Suppose d  B I , then by the definition of  we
have B   . Since  is a ct-model of T ,  satisfies B v C propositionally. If C is a basic
concept, then B   implies C   ; otherwise C = D is a negated basic concept, and
B   implies D 6  . In both cases, by the definition of  , d  C I .
For the only if direction, let  =  (I, d) for an arbitrary d  I . We first show that
  kT ktc . For each concept inclusion B v C in T , assume B   , then d  B I . If C is
a basic concept, I being a model of T implies d  C I , which in turn implies C   . If
C = D is a negated basic concept, then with a similar argument, d 6 DI and D 6  . That
is,  satisfies B v C propositionally. That is,   kT ktc . For the second half of Definition 1,
if T |= R v  then RI = . Clearly, R 6  . We have shown that   |T |tc .

Before proving Proposition 3, we first show the following lemma as a preparation.
Lemma 2 For a DL-Litecore TBox T , each ct-model  of T satisfies   kcln(T )ktc .
Proof : Towards a contradiction, suppose there exists a ct-model  of T and an NI  in
cln(T ) such that  does not satisfy  propositionally. We show that  must violate some
NI in T (which contradicts to the fact that  is a ct-model of T ). We prove this through
induction. For convenience, we assume the inclusions in cln(T ) are added inductively
following the definition.
For initialization, if  is in T then  violates some NI in T . For induction steps, we show
that if  is added to cln(T ) due to another axiom  already in cln(T ),  violates . There are
two cases: (1) Suppose  is B1 v B3 , added due to PI B1 v B2 in T and NI B2 v B3
(or B3 v B2 ) in cln(T ). Then, by the fact that  does not satisfy  propositionally,
{B1 , B3 }   . Also, as  satisfies B1 v B2 in T , B2   . Hence,  violates NI B2 v B3
(B3 v B2 ). (2) Suppose  is R v R, added due to R v R in cln(T ). Then, by
the fact that  does not satisfy  propositionally, R   . As R v R |= R v , by
condition 2 in Definition 1,  violates NI R v R .

Proof for Proposition 3
359

fiZhuang, Wang, Wang, & Qi

We construct an interpretation I from  using chase: Let d be a constant, NC be the
set of all concept names, and
A ={A(d) | A  NC   } 
{R(d, ed,R ) | R  , ed,R is a fresh constant}.
Take I = chaseT (A ). We want to show that I is a model of T and  (I , d) =  .
To show the former, by Lemma 1, we only need to show that A is a model of cln(T ).
Towards a contradiction, suppose it is not the case, then there is an axiom B v D in
cln(T ) that is violated by A . This is the case when A |= B(s) and A |= D(s) for some
constant s in A . There are essentially four cases (note that B and D are symmetric):
(i) Suppose B and D are both concept names, then A |= B(s) and A |= D(s) only if
s = d, B(d)  A , and D(d)  A . From the construction of A , {B, D}   and thus 
does not propositionally satisfy B v D. That is,  6 kcln(T )ktc , which violates Lemma 2.
(ii) Suppose B = R for some R and D is a concept name, then A |= B(s) only if
s = d and R(d, t)  A for some t, and A |= D(d) only if B(d)  A . Thus, {B, R}  
and thus  does not propositionally satisfy B v R. Again,  6 kcln(T )ktc , which violates
Lemma 2.
(iii) Suppose B = R for some R and D = S for some S with R 6= S, then A |= B(s)
and A |= D(s) only if R(s, t)  A and S(s, u)  A for some t, u. This is only when s = d
and t, u are fresh constants. In this case, {R, S}   and  does not propositionally
satisfy R v S, which again violates Lemma 2.
(iv) Suppose B = R for some R and D = R, then A |= B(s) only if R(s, t)  A for
some t. This is only when s = d and t is a fresh constant; or t = d and s is a fresh constant.
In the former case, R   , and in the latter case, R   . Since R v   cln(T ),
T |= R v  and T |= R v . Both cases violate the fact that   |T |tc and condition 2
in Definition 1.
We have shown that A is a model of cln(T ), and thus I is a model of T .
Now it remains to show that  (I , d) =  . Since it is clear that  (A , d) =  , from the
definition of chase,  (A , d)   (I , d). We only need to show that  (I , d)   (A , d).
This is equivalent to show that I \ A does not contain any assertion of the form A(d) or
R(d, s) such that no assertion R(d, t)  A (as otherwise, A or R, respectively, will be in
 (I , d) \  (A , d) according to the definition of  (I , d)). Towards a contradiction, suppose
there is such an assertion in I \ A . From the chase rules, it can happen only if some chase
rule is applicable to an assertion g of the form B(d) or S(d, t). Let g be the first of such
assertions that triggers a chase rule. By the chase rules, we observe that g must be in A .
 Suppose g = B(d), then from the construction of A , B   . If g triggers a chase
rule with B v A  T , then by condition 1 in Definition 1,  propositionally satisfies
B v A, and hence A   and A(d)  A , which is a contradiction to (the applicability
of) the chase rule; otherwise, g triggers a chase rule with B v R  T , then by
condition 1 in Definition 1,   kB v Rktc , and thus R   and R(d, u)  A for
some u, which again contradicts the chase rule.
 Suppose g = S(d, t), then from the construction of A , S   . If g triggering a
chase rule with S v A, then by condition 1 in Definition 1,  propositionally satisfies
360

fiDL-Lite Contraction and Revision

S v A. As S   , A   and A(d)  A , which contradicts to the chase rule. If g
triggering a chase rule with S v R, then  propositionally satisfies S v R, and
thus R   and R(d, u)  A for some u, which is again a contradiction.
We have shown that I \ A does not contain any assertion of the form A(d) or R(d, s).
Thus,  (I , d) =  (A , d) =  .

Proof for Theorem 1
If |T | 6= , then there is a model I  |T |. Let d  I , and  =  (I, d). From
Proposition 2,   |T |tc . That is, |T |tc 6= . Conversely, suppose |T |tc 6= , let   |T |tc .
From Proposition 3, there is a model I of T . That is, |T | =
6 . Thus, |T | is empty if and
only if |T |tc is empty. If |T | and |T |tc are both empty, the statement trivially holds. In what
follows, we assume that |T | and |T |tc are both non-empty.
For the if direction, we want to show that if T 6|=  then |T |tc 6 ||tc . Then, there
is a model I of T such that I does not satisfy . That is, there is a TBox axiom in the
conjunction  that is not satisfied by I. Without loss of generality, assume  contains only
this (single) TBox axiom. Suppose  is B v C. Then, there is an domain element d  I
such that d  B I and d 6 C I . Let  =  (I, d). Since I is a model of T , from Proposition 2,
 is a ct-model of T . If C is a basic concept, then B   implies d  B I and d 6 C I , which
in turn implies C 6  . If C = D is a negated basic concept, then with a similar argument,
B   implies D   . That is,  does not propositionally satisfy B v C and  6 kktc . We
have shown that |T |tc 6 ||tc .
For the only if direction, we want to show that if |T |tc 6 ||tc then T 6|= . Since
t
|T |c 6 ||tc , there is a ct-type   |T |tc and  6 ||tc . From Proposition 3, there exists a
model I of T and a domain elements d  I such that  (I, d) =  . We only need to show
that I is not a model of . Suppose otherwise, I is a model of , then by Proposition 2, 
must be a ct-model of , which contradicts the fact  6 ||tc . Thus, I is not a model of ,
and we have shown that T 6|= .

Proof for Theorem 2
For each   |T |tc , by condition 1 in Definition 1,  propositionally satisfies i for i =
1, . . . , n. Moreover, as T is coherent, by the monotonicity of DL-Lite, there exists no R  R
such that i |= R v . Hence,   |i |tc for i = 1, . . . , n. That is, |T |tc  |1 |tc      |n |tc .
Conversely, for each   |1 |tc      |n |tc , by condition 1 in Definition 1,   kT ktc . Further,
as T is coherent, there exists no R  R such that T |= R v . Hence,   |T |tc . That is,
|1 |tc      |n |tc  |T |tc .

Proof for Theorem 3
For each   |T |tc , by Proposition 3, there is a model I  |T | and some d  I such that
 (I, d) =  . Since |T |  ||  ||, I  || or I  ||. Suppose without loss of generality
that I  ||, then by Proposition 2,  (I, d)  ||tc . That is,   ||tc . We have shown that
|T |tc  ||tc  ||tc .

Before presenting the proof for Theorem 4, we first show Lemmas 3 and 4 regarding
the union of TBoxes (or equivalently, conjunction of TBox axioms). In a similar way as
361

fiZhuang, Wang, Wang, & Qi

for Lemma 2, we can show the following lemma. The difference is that we cannot assume
each ct-type   |T1 |tc  |T2 |tc satisfies   |T1  T2 |tc , and thus cannot apply condition 2 in
Definition 1 in the proof.
Lemma 3 For two DL-Litecore TBox T1 and T2 , and a role-complete set M of ct-types with
M  |T1 |tc  |T2 |tc , it holds that M  kcln(T1  T2 )ktc .
Proof : Towards a contradiction, suppose there exists a ct-type   M and an NI  in
cln(T1 T2 ) such that  does not satisfy  propositionally. We show that some ct-type  0  M
exists that violates some NI in T1  T2 , which contradicts to the fact that M  |T1 |tc  |T2 |tc
(as  0  |T1 |tc  |T2 |tc implies that  0 satisfies all the NIs in T1  T2 propositionally). Similar
as the proof of Lemma 2, we prove this through induction.
For initialization, if  is in T1  T2 then let  0 =  and  0 violates some NI in T1  T2 .
For induction steps, we show that if  is added to cln(T1  T2 ) due to another axiom 
already in cln(T1  T2 ), we show that some  00  M exists that violates  (and eventually
take  0 =  00 when  is in T1  T2 ). There are two cases: (1) Suppose  is B1 v B3 , added
due to PI B1 v B2 in T1  T2 and NI B2 v B3 (or B3 v B2 ) in cln(T1  T2 ). Then, by the
fact that  does not satisfy  propositionally, {B1 , B3 }   . Also, as  satisfies B1 v B2 in
T1  T2 , B2   . Hence, let  00 =  and  00 violates NI B2 v B3 (B3 v B2 ). (2) Suppose
 is R v R, added due to R v R in cln(T1  T2 ). Then, by the fact that  does
not satisfy  propositionally, R   . As M is role-complete, there exists some  00  M
with R   00 . Hence,  00 violates NI R v R .

Lemma 4 Let M be a set of ct-types, and 1 , 2 be two conjunctions of DL-Litecore TBox
axioms. Suppose M is role-complete, then M  |1 |tc  |2 |tc implies M  |1  2 |tc .
Proof : For each   M , we want to show that   |1  2 |tc . To this end, we construct a
model of 1 2 . Let Ti be the set of axioms (as conjuncts) in i for i = 1, 2, A be as in the
proof of Proposition 6, and I = chaseT1 T2 (A ). We can show that I is a model of T1  T2
and  (I , d) =  in a similar way as in the proof of Proposition 3 (by using Lemma 3 instead
of Lemma 2). Except for case (iv): Suppose A violates R v R  cln(T1  T2 ). Note
that, different from the proof of Proposition 3, we cannot assume either R v R  cln(T1 )
nor R v R  cln(T2 ) (That is, we cannot apply condition 2 in Definition 1). Yet we can
still derive contradiction. A violates R v R only if R(d, t)  A with t being a fresh
constant or R(s, d)  A with s being a fresh constant. In the former case, R   , and
 does not propositionally satisfy R v R. That is,  6 kcln(T1  T2 )ktc , which violates
Lemma 3. In the latter case, R   . Since M is role-complete, R   0 for some  0  M .
Hence,  0 does not propositionally satisfy R v R. That is,  0 6 kcln(T1  T2 )ktc , which
again violates Lemma 3.
Now, we have shown that I is a model of 1  2 and  (I , d) =  . By Proposition 2,
  |1  2 |tc .

Proof for Theorem 4
Suppose there are two TBoxes T1 and T2 corresponding to M . That is, M  |T1 |tc
and M  |T2 |tc . From Lemma 4, M  |T1  T2 |tc . By the minimality requirement of a
362

fiDL-Lite Contraction and Revision

corresponding TBox, |T1  T2 |tc 
6 |Ti |tc for i = 1, 2. That is, |T1  T2 |tc = |Ti |tc for i = 1, 2.
By Theorem 1, T1 is equivalent to T2 .

Proof for Proposition 5
For the if direction, it suffices to show for each d  I and each concept inclusion B v C
in T , d  B I implies d  C I ; and additionally, for each e  I (not necessarily d 6= e)
and each role inclusion R v E in T , (d, e)  RI implies (d, e)  E I . Let  =  (I, d, e).
Suppose d  B I , then by the definition of  we have B   . Since  is a t-model of T ,
 propositionally satisfies B v C. If C is a basic concept, then B   implies C   ;
otherwise C = D is a negated basic concept, and B   implies D 6  . In both cases,
by the definition of  , d  C I . For role inclusion R v E, suppose (d, e)  RI , then by the
definition of  we have R   . As  is a t-model of T ,  propositionally satisfies R v E. If
E is a role then E   ; otherwise E = S is a negated role and S 6  . In both cases, by
the definition of  , (d, e)  E I .
For the only if direction, let  =  (I, d, e) for some arbitrary d, e  I . We first show
that  satisfies condition 1 of Definition 3. For each concept inclusion B v C in T , assume
B   , then by the definition of  , d  B I . If C is a basic concept, I being a model of T
implies d  C I , which in turn implies C   . If C = D is a negated basic concept, then
with a similar argument, d 6 DI and D 6  . That is,  propositionally satisfies B v C.
In a similar way, for each concept inclusion B 0 v C 0 in T  with B 0 , C 0  B 0 , we can show
that  satisfies B 0 v C 0 . For each role inclusion R v E in T , assume R   . Then, by the
definition of  , (d, e)  RI . Since I is a model of T , (d, e)  E I . If E is a role then E   ;
otherwise E = S is a negated role and S 6  . Thus,  propositionally satisfies kR v E.
Similarly,  satisfies R0 v E 0 for each role inclusion R0 v E 0 in T  . We haven shown that
  kT ktr .
We next show that  satisfies conditions 25 of Definition 3. For condition 2, if T |=
R v  then RI = . Clearly, d 6 (R)I and e 6 (R )I . By the definition of  , R 6 
and (R )0 6  . For condition 3, if T |= R v S then (R)I  (S)I . Suppose R   ,
which by the definition of  implies d  (R)I . Then, d  (S)I , and thus S   . Similarly,
suppose (R)0   , which implies e  (R)I . Hence, e  (S)I , and thus (S)0   . For
condition 4, if R   then by the definition of  , (d, e)  RI , which implies d  (R)I and
e  (R )I . By the definition of  , R   and (R )0   . Condition 5 is clearly satisfied
by the definition of  (I, d, e).
We have shown that  satisfies all the conditions in Definition 3, that is,   |T |tr .

Before proving Proposition 6, we first show Lemma 5. Note that cln(T ) is obtained
from cln(T ) by adding a copy for each axiom in cln(T ), and kcln(T )ktr is the set of t-types
that are the propositional models of cln(T ) .
Lemma 5 For a DL-LiteR TBox T , each t-model  of T satisfies   kcln(T )ktr .
Proof : Towards a contradiction, suppose there exists a t-model  of T and an NI  in
cln(T ) such that  does not propositionally satisfy . We show that  must violate some
NI in T , through induction. Here we only consider  being a NI in cln(T ), and the case of
 being a copy of some NI in cln(T ), i.e.,   cln(T ) \ cln(T ), can be shown similarly.
363

fiZhuang, Wang, Wang, & Qi

For initialization, if  is in T then  violates some NI in T . For induction steps, we
show that if  is added to cln(T ) due to another axiom  already in cln(T ),  violates .
(1) Suppose  is B1 v B3 , added due to PI B1 v B2 in T and NI B2 v B3 (or
B3 v B2 ) in cln(T ). Then, by the fact that  does not propositionally satisfy ,
{B1 , B3 }   . Also, as  satisfies B1 v B2 in T , B2   . Hence,  violates NI
B2 v B3 (B3 v B2 ).
(2) Suppose  is R1 v B, added due to PI R1 v R2 in T and NI R2 v B (or
B v R2 ) in cln(T ). Then, by the fact that  does not propositionally satisfy ,
{R1 , B}   . Also, by condition 3 in Definition 3, R2   . Hence,  violates NI
R2 v B (B v R2 ).
(3) Suppose  is R1 v B, added due to PI R1 v R2 in T and NI R2 v B (or
B v R2 ) in cln(T ). Then, by the fact that  does not propositionally satisfy ,
{R1 , B}   . Also, by condition 3 in Definition 3, R2   . Hence,  violates NI
R2 v B (B v R2 ).
(4) Suppose  is R1 v R3 , added due to PI R1 v R2 in T and NI R2 v R3 (or
R3 v R2 ) in cln(T ). Then, by the fact that  does not propositionally satisfy ,
{R1 , R3 }   . Also, as  satisfies R1 v R2 in T , R2   . Hence,  violates NI
R2 v R3 (R3 v R2 ).
(5) Suppose  is R v R, added due to NI R v R (or R v R) in cln(T ).
Then, by the fact that  does not propositionally satisfy , R   . As R v
R |= R v  (R v R |= R v ), by condition 2 in Definition 3,  violates NI
R v R (R v R).
(6) Suppose  is R v R, added due to NI R v R (or R v R ) in cln(T ). Then,
by the fact that  does not propositionally satisfy , R   . Also, by condition 4
in Definition 3, R   . As R v R |= R v  (R v R |= R v ), by
condition 2 in Definition 3,  violates NI R v R (R v R ).

Proof for Proposition 6
We construct an interpretation I from  using chase: Let d, e be two distinct constants,
NC and NC0 be the set of concept names in B and B 0 , respectively, and
A ={A(d) | A  NC   }  {A(e) | A0  NC0   }  {R(d, e) | R  R   } 
{R(d, fd,R ) | R  B  , R 6 , fd,R is a fresh constant} 
{R(e, fe,R ) | (R)0  B 0  , R 6 , fe,R is a fresh constant}.
Take I = chaseT (A ). We want to show that I is a model of T and  (I , d, e) =  .
To show the former, by Lemma 1, we only need to show that A is a model of cln(T ).
Towards a contradiction, suppose it is not the case, then there is an axiom B v D or
R v S in cln(T ) that is violated by A .
(1) Suppose A violates B v D, this is the case when A |= B(s) and A |= D(s) for
some constant s in A . There are essentially four cases:
364

fiDL-Lite Contraction and Revision

(i) Suppose B and D are both concept names, then A |= B(s) and A |= D(s) only
if s = d or s = e, B(s)  A , and D(s)  A . If s = d then from the construction of A ,
{B, D}   and thus  does not propositionally satisfy B v D; otherwise if s = e then
{B 0 , D0 }   and  does not propositionally satisfy B 0 v D0 . Both cases violate Lemma 5.
(ii) Suppose B = R for some R and D is a concept name, then A |= B(s) only if s = d
or s = e, and R(s, t)  A for some t. A |= D(s) only if D(s)  A . Suppose without loss
of generality that s = d (similar as in (i), the case where s = e can be shown in the same
way). If t = e then from the construction of A , R   , and by condition 4 in Definition 3,
R   ; otherwise, t is a fresh constant, and R   . In both cases, {B, R}   and thus
 does not propositionally satisfy B v R, which again violates Lemma 5.
(iii) Suppose B = R for some R and D = S for some S such that R 6= S, then
A |= B(s) and A |= D(s) only if R(s, t)  A and S(s, u)  A for some t, u. This is only
when (a) s = d or s = e, and t, u are fresh constants; or (b) s = d and t = u = e; or (c)
s = e and t = u = d. In case (a), suppose w.o.l.g s = d, then {R, S}   and  does not
propositionally satisfy R v S, which violates Lemma 5. In case (b), {R, S}   , and
by condition 4 in Definition 3, {R, S}   , which again violates Lemma 5. In case (c),
{R , S  }   . By condition 4 in Definition 3, {(R)0 , (S)0 }   , and hence  does not
propositionally satisfy (R)0 v (S)0 . It again violates Lemma 5.
(iv) Suppose B = R for some R and D = R, then A |= B(s) only if R(s, t)  A
for some t. This is only when (a) s = d or s = e, and t is a fresh constant; or (b) t = d
or t = e, and s is a fresh constant; or (c) s = d and t = e; or (d) s = e and t = d. In
case (a), suppose w.l.o.g. s = d, then R   . In case (c), R   , and by condition 4 in
Definition 3, R   . In both cases,  does not propositionally satisfy R v R, which
violates Lemma 5. In case (b), suppose w.l.o.g. t = d, then R   . In case (d), R   ,
and by condition 4 in Definition 3, R   . Since T |= R v , it violates condition 2
in Definition 3.
(2) Suppose A violates R v S, this is the case when A |= R(s, t) and A |= S(s, t)
for some constants s, t in A . This is the case only if (a) s = d and t = e, (b) s = e and
t = d, (c) R = S and s = d or s = e with t a fresh constant, or (d) R = S and t = d or t = e
with s a fresh constant. In case (a), {R, S}   , and  does not propositionally satisfy
R v S. In case (b), {R , S  }   , and by condition 5 in Definition 3, {R0 , S 0 }   .
Hence,  does not propositionally satisfy R0 v S 0 . In neither case,   kcln(T )ktr and it
violates Lemma 5. In case (c), T |= R v R, that is, T |= R v . If s = d then R   ,
and otherwise if s = e then (R)0   , it violates condition 2 in Definition 3. Similarly, in
case (d), T |= R v . If t = d then R   , and otherwise if t = e then (R )0   , it
again violates condition 2 in Definition 3.
We have shown that A is a model of cln(T ), thus I is a model of T .
Now it remains to show that  (I , d, e) =  . Since it is clear that  (A , d, e) =  , from
the definition of chase,  (A , d, e)   (I , d, e). We only need to show that  (I , d, e) 
 (A , d, e). This is equivalent to show that I \ A does not contain any assertion of the
form A(d), A(e), R(d, e), R(d, s) or R(e, s) with s being a fresh constant such that no
assertion R(d, t)  A or respectively R(e, t)  A (as otherwise, A, A0 , R, R, (R)0 ,
respectively, will be in  (I , d, e) \  (A , d, e) according to the definition of  (I , d, e)).
Towards a contradiction, suppose there is such an assertion in I \ A . From the chase
rules, it can happen only if some chase rule is applicable to an assertion g of the form B(d),
365

fiZhuang, Wang, Wang, & Qi

B(e), S(d, e), S(d, t) or S(e, t) with t being a fresh constant in the chase. Let g be the first
of such assertions that triggers a chase rule, then by the chase rules, g must be in A .
 Suppose g = B(d), then from the construction of A , B   . If g triggers a chase
rule with B v A  T , then by condition 1 in Definition 3,  propositionally satisfies
B v A, and hence A   and A(d)  A , which is a contradiction to (the applicability
of) the chase rule; otherwise, g triggers a chase rule with B v R  T , then by
condition 1 in Definition 3,  propositionally satisfies B v R, and thus R  
and R(d, u)  A for some u, which is again contradicts the chase rule. The case of
g = B(e) is shown similarly, by replacing  propositionally satisfying B v A with 
propositionally satisfying B 0 v A0 , and  propositionally satisfying B v R with 
propositionally satisfying B 0 v (R)0 .
 Suppose g = S(d, e), then from the construction of A , S   . From conditions 4
and 5 in Definition 3, S   , (S  )0   , and (S  )0   .
 If g triggers a chase rule with S v R  T , then by condition 1 in Definition 3, 
propositionally satisfies S v R, R   and R(d, e)  A , which contradicts the
chase rule.
 If g triggers a chase rule with S  v R  T , then by condition 1 in Definition 3, 
propositionally satisfies (S  )0 v R0 , and R0   . By condition 5 in Definition 3,
R   and R (d, e)  A , which contradicts the chase rule.
 If g triggers a chase rule with S v A, then  propositionally satisfies S v A.
As S   , A   and A(d)  A , which contradicts the chase rule.
If g triggers a chase rule with S  v A, then  propositionally satisfies (S  )0 v
A0 . As (S  )0   , A0   and A(e)  A , which contradicts the chase rule.
 If g triggers a chase rule with S v R, then  propositionally satisfies S v
R. As S   , R   . Hence, R(d, u)  A for some u, which again is a
contradiction.
Similarly we can show the case of g triggering a chase rule with S  v R.
 Suppose g = S(d, t) with t being a fresh constant, then from the construction of A ,
S   .
 If g triggers a chase rule with S v R  T , then R(d, t) is added. By condition 3
in Definition 3, R   , and again by the construction of A , R(d, u)  A for
some u. From the chase rules, R(d, t) behaves not differently from R(d, u) in the
chase. Thus, we could equally consider g = R(d, u) for our discussion. That is,
the application of the chase rule with S v R  T has no effect to the proof.
 If g triggering a chase rule with S v A, then by condition 1 in Definition 3,
 propositionally satisfies S v A. As S   , A   and A(d)  A , which
contradicts to the chase rule.
 If g triggering a chase rule with S v R, then  propositionally satisfies S v
R, and thus R   and R(d, u)  A for some u, which is again a contradiction.
The case of g = S(e, t) can be shown in a similarly way.
366

fiDL-Lite Contraction and Revision

We have shown that I \ A does not contain any assertion of the form A(d), A(e), R(d, e),
R(e, d), R(d, s) or R(e, s). Thus,  (I , d, e) =  (A , d, e) =  .

Proof for Theorem 5
If |T | =
6  then there is a model I  |T |. Let d, e  I and  =  (I, d, e). By
Proposition 5,   |T |tr . That is, |T |tr 6= . Conversely, suppose |T |tr 6= , let   |T |tr . By
Proposition 6, there is a model I of T . That is, |T | 6= . If both |T | and |T |tr are empty,
the statement trivially holds. In what follows, we assume both |T | and |T |tr are non-empty.
For the if direction, we want to show that if T 6|=  then |T |tr 6 ||tr . Then, there
is a model I of T such that I does not satisfy . Similar as in the proof of Theorem 1,
we can assume w.l.o.g. that  contains a single TBox axiom. If the axiom is of the form
B v C. Then, there is an domain element d  I such that d  B I and d 6 C I . Let
 =  (I, d, d). Since I is a model of T , from Proposition 5,   |T |tr . If C is a basic
concept, then B   implies d  B I and d 6 C I , which in turn implies C 6  . If C = D
is a negated basic concept, then with a similar argument, B   implies D   . That is, 
does not propositionally satisfy B v C. By condition 1 of Definition 3,  6 ||tr . Suppose
 is of the form R v E. Then, there are domain elements d, e  I such that (d, e)  RI
and (d, e) 6 E I . Let  =  (I, d, e). Again, from Proposition 5,   |T |tr . If E is a role,
then R   implies (d, e)  RI and (d, e) 6 E I , which in turn implies E 6  . If E = S
is a negated role, then with a similar argument, R   implies S   . That is,  does not
propositionally satisfy R v E. By condition 1 of Definition 3,  6 ||tr . We have shown
that in both cases |T |tr 6 ||tr .
For the only if direction, we want to show that if |T |tr 6 ||tr then T 6|= . Since
t
|T |r 6 ||tr , there is a t-type   |T |tr such that  6 ||tr . From Proposition 6, there exist a
model I of T and domain elements d, e  I such that  (I, d, e) =  . We only need to show
that I is not a model of . Suppose otherwise, I is a model of , then by Proposition 5, 
must be a t-model of , which contradicts to the fact  6 ||tr . Hence, I is not a model of
, and we have shown that T 6|= .

Before presenting the proof for Theorem 6, we first show Lemma 6 and Lemma 7. The
two lemmas extend Lemma 3 and Lemma 4 respectively to DL-LiteR .
Lemma 6 For two DL-LiteR TBox T1 and T2 , and a role-complete set M of t-types with
M  |T1 |tr  |T2 |tr , it holds that M  kcln(T1  T2 )ktr .
Proof : Towards a contradiction, suppose there exists a t-type   M and an NI  in
cln(T1  T2 ) such that  does not propositionally satisfy . We show that some t-type in M
exists that violates some NI in T1  T2 , which contradicts to the fact that M  |T1 |tr  |T2 |tr .
Similar as the proof of Lemma 5, we prove this through induction. Here we only present the
case where  is a NI in cln(T1 T2 ), and the case of  being a copy of some NI in cln(T1 T2 ),
i.e.,   cln(T1  T2 ) \ cln(T1  T2 ), can be shown similarly. Without loss of generality,
we assume axioms are added to cln(T1  T2 ) incrementally according to the definition and
copies (e.g., B 0 v C 0 ) are added immediately after the original axioms (B v B) are added.
For initialization, if  is in T1  T2 then  violates some NI in T1  T2 . For induction
steps, we show that if  is added to cln(T1  T2 ) due to another axiom  already in
367

fiZhuang, Wang, Wang, & Qi

cln(T1  T2 ) , we show that some  0  M exists that violates . The proof for cases (1)(4)
are the same as the proof of Lemma 5, where we simply let  0 =  .
For cases (5), suppose  is R v R, added due to NI R v R (or R v R) in
cln(T1 T2 ) . Note that (R )0 v (R )0 (resp., R0 v R0 ) is also in cln(T1 T2 ) . Then, by
the fact that  does not propositionally satisfy , R   . As M is role balance, there exists
 0  M with R   0 or R0   0 . If R   0 , by conditions 4 and 5 in Definition 3, (R )0   0 ,
and  0 violates NI (R )0 v (R )0 (resp., R v R). If R0   0 , by conditions 4 and 5
in Definition 3, R   0 and R   0 , and again  0 violates NI R v R (resp.,
R0 v R0 ).
For case (6), suppose  is R v R, added due to NI R v R (or R v R ) in
cln(T1  T2 ) . Note that (R)0 v (R)0 (resp., (R )0 v (R )0 ) is also in cln(T1  T2 ) .
Then, by the fact that  does not propositionally satisfy , R   . By conditions 4
and 5 in Definition 3, R   and (R )0   . Hence,  violates NI R v R (resp.,
(R )0 v (R )0 ).

Lemma 7 Let M be a set of t-types and 1 , 2 be two conjunctions of DL-LiteR TBox
axioms. Suppose M is role-complete, then M  |1 |tr  |2 |tr implies M  |1  2 |tr .
Proof : For each   M , we want to show that   |1  2 |tr . To this end, we construct
a model of 1  2 from  in the same way as in the proof of Proposition 6. Let Ti be
the set of axioms in i for i = 1, 2, A be constructed in the same way as in the proof of
Proposition 6, and take I = chaseT1 T2 (A ). We can show that I is a model of T1  T2
and  (I , d, e) =  in a similar way as in the proof of Proposition 6 (using Lemma 6 instead
of Lemma 5), except for cases (1) (iv) and (2).
In case (1) (iv), suppose A violates R v R in cln(T1  T2 ). Different from the
proof of Proposition 6, we cannot assume R v R in either cln(T1 ) or cln(T2 ), and thus
cannot apply condition 2 in Definition 3. Yet we can still derive contradiction. A violates
R v R only if R   or R   . In the former case,  does not propositionally satisfy
R v R. That is,  6 kcln(T1  T2 )ktr , which violates Lemma 6. In the latter case, since
M is role-complete, R   0 for some t-type  0  M . Hence,  0 does not propositionally
satisfy R v R, which again violates Lemma 6.
In case (2), suppose A violates R v S in cln(T1 T2 ). This is the case when {R, S}   ,

{R , S  }   , or R = S and {R, R , (R)0 , (R )0 }   6= . The first two cases can be
shown in the same way as in the proof of Proposition 6. For the third case where R = S,
different from the proof of Proposition 6, we cannot assume R v R in either cln(T1 ) or
cln(T2 ), and thus cannot apply condition 2 in Definition 3. Note that from the facts that
{R, R , (R)0 , (R )0 }   6=  and that M is role-complete, there exists some t-type
 0  M with R   0 or R0   0 . If R   0 then  0 does not propositionally satisfy R v R;
otherwise if R   0 ,  0 does not propositionally satisfy R0 v R0 . In both cases, Lemma 6
is violated.
Now, we have shown that I is a model of T1  T2 and  (I , d, e) =  . By Proposition 5,
  |1  2 |tr .

Proof for Theorem 6
368

fiDL-Lite Contraction and Revision

The theorem be proved similarly to Theorem 4, and the proof is based on Lemmas 6
and 7.

Proof for Proposition 8
For each ct-type   |T |tc , by Proposition 3, there is a model I of T and some d  I
such that  (I, d) =  . Let  0 be the t-type such that  0 =  (I, d, d). Then, from the
definitions of  (I, d) and  (I, d, d),  =  0  B. Also, by Proposition 5,  0  |T |tr . That is,
  {  B |   |T |tr }, and hence |T |tc  {  B |   |T |tr }.
Conversely, for each ct-type   {  B |   |T |tr }, there is a t-type  0  |T |tr such
that  =  0  B. By Proposition 6, there is a model I of T and some d, e  I such that
 (I, d, e) =  0 . It is easy to see that  =  (I, d) from the definitions of  (I, d) and  (I, d, e).
By Proposition 2,   |T |tc . That is, {  B |   |T |tr }  |T |tc .

The following theorem generalises Theorem 2 to DL-LiteR .
Theorem 14 Let T be a DL-LiteR TBox such that T = {1 , . . . , n }. If T is coherent
then |T |tr = |1 |tr      |n |tr .
Proof : For each   |T |tr ,  satisfies conditions 4 and 5 in Definition 3. Also, for i = 1, . . . , n,
by condition 1 in Definition 3 w.r.t. T ,  propositionally satisfies i . That is,  satisfies
condition 1 w.r.t. i . Further, as T is coherent, by the monotonicity of DL-Lite, there exists
no R  R such that i |= R v , and  trivially satisfies condition 2 w.r.t. i . Moreover,
if i |= R v S, then T |= R v S, and by condition 3 in Definition 3 w.r.t. T ,  satisfies
condition 3 w.r.t. i . Hence,   |i |tr for i = 1, . . . , n. That is, |T |tr  |1 |tr      |n |tr .
Conversely, for each   |1 |tr      |n |tr , we can construct a model I of T from  in
the same way as in the proofs of Proposition 6 and Lemma 7 such that I induces  . By
Proposition 5,   |T |tr . That is, |1 |tr      |n |tr  |T |tr .

Proof for Proposition 9
For the if direction, since I is a model of T , it suffices to show that I is a model of A,
i.e., for each concept assertion A(a)  A, aI  AI , and for each role assertion P (a, b)  A,
(aI , bI )  P I . Let  =  a (I), then   kAT kar and  propositionally satisfies Aa . That is,
Aa   . From the definition of  , aI  AI . Similarly,  propositionally satisfies P ab and
P ab   , and hence (aI , bI )  P I . We have shown I  |K|.
For the only if direction, we only need to show the second half of the statement since
I  |K| implies I  |T |. Let  =  a (I), and we want to show that   |AT |ar . For condition 1
of Definition 4, we can show  satisfies T a in the same way as in the proof of Proposition 5.
For each A(a)  A and each P (a, b)  A, since aI  AI and (aI , bI )  P I , Aa   and
P ab   . That is,  propositionally satisfies A. We haven shown that   kAT kar . Further,
it can be shown that  satisfies conditions 25 of Definition 4 in a similar manner as in the
proof of Proposition 5 (roughly, by replacing Definition 3 with Definition 4, d with each
a  D, e with each b  D, B with B a , B 0 with B b , R with Rab , R with (R)a , (R)0 with
(R)b , and so on).

Before presenting the proof for Proposition 10, we first show Lemma 8. The lemma can
be proved in the same manner as Lemma 5. Note that cln(T )a is the TBox that consists
369

fiZhuang, Wang, Wang, & Qi

of a copy of each concept inclusion in cln(T ) for each individual in D, and a copy of each
role inclusion in cln(T ) for each pair of individuals in D.
Lemma 8 For a DL-LiteR KB K = (T , A), each a-model  of AT is a propositional model
of cln(T )a .
Proof for Proposition 10
Similar as before, we construct an interpretation I from  using chase: Let NCa be the
set of concept names in B a (with a  D), and
Aa ={A(a) | a  D, Aa  NCa   }  {R(a, b) | a, b  D, Rab  Rab   } 
{R(a, fa,R ) | a  D, (R)a  B a  , Rab 6  for any b  D, fa,R is a fresh constant}.
Take I = chaseT (Aa ). We want to show that I is a model of K and  a (I ) =  .
To show the former, we first show that A  Aa . For each concept assertion A(a)  A, as
 is an a-model of K,  propositionally satisfies Aa . That is, Aa   and hence A(a)  Aa .
Similarly, for each role assertion P (a, b)  A, P ab   and P (a, b)  Aa . We have shown
that A  Aa . To show that I is a model of K, we want to show that I is a model of
(T , Aa ). By Lemma 1, we only need to show that Aa is a model of cln(T ). This can be
shown in a similar way as in the proof of Proposition 6 (roughly, by replacing Definition 3
with Definition 4, Lemma 5 with Lemma 8, d with each a  D, e with each b  D, B with
B a , B 0 with B b , R with Rab , R with (R)a , (R)0 with (R)b , and so on).
Now it remains to show that  a (I ) =  . Again, we only need to show that  a (I ) 
a
 (Aa ). This again can be shown in a similar way as the proof of Proposition 6.

Proof for Theorem 7
If |K| =
6  then there is a model I  |K|. Let  =  a (I). By Proposition 9,   |AT |ar .
That is, |AT |ar 6= . Conversely, suppose |AT |ar 6= , let   |AT |ar . By Proposition 10,
there is a model I of K. That is, |K| 6= . If both |K| and |AT |ar are empty, the statement
trivially holds. In what follows, we assume both |K| and |AT |ar are non-empty.
For the if direction, we want to show that if K 6|=  then |AT |ar 6 ||ar . Then, there
is a model I of K such that I does not satisfy . Let  =  a (I). From Proposition 9,
  |AT |ar . Similar as in the proof of Theorem 1, we can assume w.l.o.g. that  contains a
single ABox assertion. Suppose  is of the form A(a). Then, aI 6 AI . From the definition
of  a (I), Aa 6  , and hence  6 kkar . By condition 1 of Definition 4,  6 ||ar . Suppose 
is of the form P (a, b). Then, (aI , bI ) 6 P I . Again, from the definition of  a (I), P ab 6  ,
and hence  6 ||ar . In both cases, |AT |ar 6 ||ar .
For the only if direction, we want to show that if |AT |ar 6 ||ar then K 6|= . Since
|AT |ar 6 ||ar , there is an a-type   |AT |ar such that  6 ||ar . From Proposition 10, there
exist a model I of K such that  a (I) =  . We only need to show that I is not a model of
. Suppose otherwise, I is a model of , then by Proposition 9,  must be an a-model of
, which contradicts to the fact  6 ||ar . Hence, I is not a model of , and we have shown
that K 6|= .

Proof for Theorem 8
Let A = {A(a) | Aa   for each   M }  {P (a, b) | P ab   for each   M }. We
want to show that A is the unique corresponding ABox for M w.r.t. T .
370

fiDL-Lite Contraction and Revision

We first show that A is a corresponding ABox. To show that M  |AT |ar , we need to
show for each a-type   M ,  satisfies conditions 15 in Definition 4. As M is consistent
with T ,   |T |ar . Hence,  is a propositional model of T a , and  satisfies conditions 25.
Also, from the construction of A,  satisfies Aa for each A(a)  A and satisfies P ab for each
P (a, b)  A. That is,   kAT kar . We have shown that M  |AT |ar .
Further, for any ABox A0 such that M  |A0T |ar and for each a-type   M , since
  kA0T kar ,  satisfies Aa for each concept assertion A(a)  A0 and satisfies P ab for each
role assertion P (a, b)  A0 . Note that this holds for each a-type in M . From the construction
of A, A0  A. That is, |AT |ar  |A0T |ar . Thus, A is a corresponding ABox. Also, based on
the above observation, any corresponding ABox must be equivalent to A.

The following theorem can be proved similar to Theorem 14. Since only one TBox is
concerned, the proof does not require Lemmas 6 and 7.
Theorem 15 Let K = (T , A) be a DL-LiteR KB such that A = {1 , . . . , n }. Then
|AT |ar = |{1 }T |ar      |{n }T |ar .
Proof for Proposition 12
Let K = (T , A). For the if direction, it suffices to show that (i) for each d  I and
each concept inclusion B v C in T , d  B I implies d  C I ; (ii) for each e  I (not
necessarily d 6= e) and each role inclusion R v E in T , (d, e)  RI implies (d, e)  E I ;
(iii) for each concept assertion A(a)  A, aI  AI , and for each role assertion P (a, b)  A,
(aI , bI )  P I . Let  =   (I, d, e) =  (I, d, e)   a (I). Conditions (i) and (ii) are shown in
the proof of Proposition 5. That is, I  |T |. Then, condition (iii) is shown in the proof of
Proposition 9.
For the only if direction, let  =   (I, d, e) for some arbitrary d, e  I . That is,
 =  (I, d, e)   a (I). We want to show that  (I, d, e)  |T |tr and  a (I)  |AT |ar , which
have been shown in the proofs of Propositions 5 and 9, respectively.

Proof for Proposition 13
Let K = (T , A). We construct an interpretation I from  using chase: Let A and
a
A be defined as in the proofs of Propositions 6 and 10, and I = chaseT (A  Aa ). We
want to show that I is a model of K and   (I , d, e) =  . To show the former, we have
A  Aa from the proof of Proposition 10, and we only need to show that I is a model of
(T , A  Aa ). By Lemma 1, it suffices to show that A  Aa is a model of cln(T ), which has
been shown in the proofs of Propositions 6 and 10. To show later, that is   (I , d, e) =  ,
we only need to show that   (I , d, e)    (A , d, e). By the definition of   (I , d, e), it
suffices to show that  (I , d, e)   (A , d, e) and  a (I )   a (Aa ), which again has been
shown in the proofs of Propositions 6 and 10.

Proof for Theorem 9
It can be shown in the same way as in the proof for Theorem 7 that |K| is empty if and
only if |K|r is empty. In what follows, we assume both |K| and |K|r are both non-empty.
For the if direction, we want to show that if K 6|=  then |K|r 6 ||r . Then, there is a
model I of K such that I does not satisfy . Similar as in the proof of Theorem 1, we can
assume w.l.o.g. that  contains a single TBox axiom or a single ABox assertion. Suppose
371

fiZhuang, Wang, Wang, & Qi

 is of the form B v C or R v E, then in the same way as the proof of Theorem 5, we can
construct  =   (I, d, e) for some d, e  I such that   |K|r and  6 ||r . Suppose  is
of the form A(a) or P (a, b), then in the same way as the proof of Theorem 7, we can show
that   |K|r and  6 ||r .
The only if direction can be shown in the same way as in the proof for Theorem 7.

Before presenting the proof for Theorem 10, we first show Lemma 9 and Lemma 10.
The two extend Lemmas 6 and 7 respectively to DL-LiteR KBs.
Lemma 9 For two DL-LiteR TBox T1 and T2 , and a role-complete set M of types with
M  |T1 |r  |T2 |r , all the types in M must satisfy cln(T1  T2 )  cln(T1  T2 )a .
Proof : Towards a contradiction, suppose there exists a type   M and an NI  in cln(T1 
T2 )  cln(T1  T2 )a such that  does not propositionally satisfy . We show that some type
in M exists that violates some NI in T1  T2  T1a  T2a , which contradicts to the fact that
M  |T1 |r  |T2 |r . It can be shown in a similar way as the proof of Lemma 6. When an
axiom in cln(T1  T2 ) is concerned, the proof is as that of Lemma 6. When an axiom in
cln(T1  T2 )a is concerned, the proof is adapted by replacing B with B a , B 0 with B b , R
with Rab , R with (R)a , (R)0 with (R)b , and so on.

Lemma 10 Let M be a set of types and 1 , 2 be two conjunctions of DL-LiteR axioms.
Suppose M is role-complete, then M  |1 |r  |2 |r implies M  |1  2 |r .
Proof : For each type   M , we want to show that   |1  2 |r . To this end, we
construct a model of 1  2 from  in the same way as in the proof of Proposition 13. Let
T and A be the sets of respectively TBox axioms and ABox axioms in both 1 and 2 ,
be the set of ABox axioms in A and Aa be as in the proof of Proposition 13, and take
I = chaseT (A  A  Aa ). We can show that I is a model of (T , A) and   (I , d, e) = 
in a similar way as in the proofs of Lemma 7 and Proposition 13. By Proposition 12,
  |1  2 |r .

Proof for Theorem 10
The theorem can be proved similarly to Theorem 4 and the proof is based on Lemma 9
and Lemma 10.

The following theorem can be proved similar to Theorem 14.
Theorem 16 Let K be a DL-LiteR KB such that K = {1 , . . . , n }. If K is coherent then
|K|r = |1 |r      |n |r .
Proof for Theorem 11
.
For one direction, suppose  is a T-contraction function for a TBox T and the as.
.
.
.
sociated selection function is . We need to show  satisfies (T 1)(T 4), (T de), and
.
.
.
.
.
(T 6). (T 1), (T 2), (T 4) and (T 6) follow directly from the definition of T-contraction
.
.
function. We only show the proof for (T 3) and (T de).
372

fiDL-Lite Contraction and Revision

.
(T 3): Suppose T 6|= . Then |T |tr 6 ||tr which implies |T |tr  ||tr 6= . Thus by the
.
faithfulness of , we have (||tr )  |T |tr . Thus T  = Tr (|T |tr (||tr )) = Tr (|T |tr ) = T .
.
.
(T de): We prove its contrapositive. Suppose T |=  and T  6|= . Then we have
.
.
.
|T |tr  ||tr and |T |tr 6 ||tr . It remains to show |T |tr 6 ||tr  ||tr . Assume |T |tr 
.
||tr  ||tr . Since by the definition of T-contraction function, |T |tr  (||tr )  |T |tr ,
we have |T |tr  (||tr )  ||tr  ||tr which implies for each   (||tr ),   ||. Thus
|T |tr  (||tr ))  ||tr . It then follows from the definition of corresponding TBoxes that
.
|T |tr  (||tr ))  |T |tr  ||tr , a contradiction!
.
.
For the other direction, suppose  is a function for a TBox T that satisfies (T 1)
.
.
.
(T 4), (T de), and (T 6). Let  be defined as
.
(||tr ) = ||tr  |T |tr
for all conjunctions of TBox axioms . For a set of t-types M , if there is no conjunction of
TBox axiom  such that ||tr = M , then define (M ) = M  |T |tr whenever M  |T |tr 6= 
and M otherwise. We need to show that (1)  is a faithful selection function for T and (2)
.
T  = Tr (|T |tr  (||tr )).
Part (1): For  to be a faithful selection function, it has to be a function first. And this
.
follows directly from the definition of  and (T 6).
To prove  is a selection function, suppose M = . We need to show (M ) = . If
a TBox axiom  is a tautology, then we have ||tr =  = M . Thus (M ) = (||tr ) =
.
||tr  |T |tr = . Now suppose M 6= . We need to show (M ) 6= . By the definition of
, the result trivially holds if there is no conjunction of TBox axioms  such that ||tr = M .
.
If there is  such that ||tr = M , then since ||tr 6=  implies 6|= , it follows from (T 4)
.
.
that |T |tr  ||tr 6= . Thus (M ) = (||tr ) = ||tr  |T |tr 6= .
To prove  is faithful with respect to T , suppose M  |T |tr 6= . We need to show
(M ) = M |T |tr . Again, the result trivially holds if there is no conjunction of TBox axioms
 such that ||tr = M . If there is  such that ||tr = M , then since ||tr  |T |tr 6= 
.
.
implies T 6|= , it follows from (T 3) that |T |tr = |T |tr . Thus (M ) = (||tr ) =
.
||tr  |T |tr = ||tr  |T |tr .
.
.
Part (2): Since (T 1) implies T  is closed and Tr is a function that returns closed
.
TBoxes, it suffices to show |Tr (|T |tr  (||tr ))|tr = |T |tr .
.
.
.
It follows from (T 2) that T   T which implies |T |tr  |T |tr . It follows from the
.
.
definition of  that (||tr )  |T |tr . So we have |T |tr  (||tr )  |T |tr which implies
.
by the definition of corresponding TBoxes that |Tr (|T |tr  (||tr ))|tr  |T |tr .
.
It remains to show |T |tr  |Tr (|T |tr  (||tr ))|tr . Assume to the contrary that
. t
|T |r 6 |Tr (|T |tr  (||tr ))|tr . Let  be a conjunction of TBox axioms such that
.
.
||tr = |Tr (|T |tr  (||tr ))|tr . Then T |=  and T  6|= . It follows from (T de)
. t
.
that |T |r 6 ||tr  ||tr = ||tr  |Tr (|T |tr  (||tr ))|tr . Let u  |T |tr , if u  ||tr
then u  ||tr  |Tr (|T |tr  (||tr ))|tr and if u  ||tr , then by the definition of ,
.
u  (||tr ). Thus in either case, u  ||tr  |Tr (|T |tr  (||tr ))|tr which implies |T |tr 
||tr  |Tr (|T |tr  (||tr ))|tr = ||tr  ||tr , a contradiction!

Proof for Proposition 15
.
The complexity results have been explained earlier. Let  be a function such that
.
.
T  = TCONT (T , ) for any TBox T and conjunction of axioms . We need to show 
373

fiZhuang, Wang, Wang, & Qi

.
.
.
is a T-contraction function. By Theorem 11 it suffices to show  satisfies (T 1)(T 4),
.
.
.
.
.
(T de), and (T 6). (T 2), (T 3) and (T 6) are trivially satisfied.
Let  be the counter-model picked in line 3 of TCONT . Since all axioms of T that violet
.
.
 are removed in line 6, we have   ||tr for all   T . Suppose T  = {1 , . . . , n }.
.
.
Then by Theorem 14, we have |1 |tr   |n |tr = |T |tr which implies   |T |tr . Then it
.
.
.
.
is obvious that T  6|= , so (T 4) is satisfied. For (T 1), suppose   T and T  |= .
. t
.
.
. t
t
We need to show   T . Since T  |=  implies |T |r  ||r and   |T |r , we
.
have   ||tr that is  |=tr . Thus  is not removed in line 6 which means   T .
.
.
For (T de), suppose   T and |T |tr  ||tr  ||tr . Then it follows from  6 ||tr
. t
and   |T |r that   ||tr . Thus  is not removed in line 6 of TCONT which means
.
  T .

Proof for Theorem 12
For one direction, suppose  is a T-revision function for a TBox T and the associated
selection function is . We need to show  satisfies (T  1)(T  6) and (T  f ).
(T  1), (T  2), (T  6), and (T  f ) follow immediately from the definition of T-revision.
We only show the proof for (T  3)(T  5).
(T  3): Suppose  is coherent. It follows from the definition of T-revision function that
(||tr )  |T  |tr . Since  is faithful with respect to T , we have |T |tr  ||tr  (||tr ). Thus
|T |tr  ||tr  |T  |tr which implies T    cl(T  {}).
(T  4): Suppose T  {} is coherent. Then |T |tr  ||tr is coherent. It follows from the
faithfulness of  that (||tr ) = |T |tr  ||tr . By the definition of T-revision function, we have
(||tr )  |T  |tr . So we have |T |tr  ||tr  |T  |tr which implies T   = cl(T  {}).
(T 5): Suppose  is coherent. Since  is coherent preserving, (||tr ) is coherent. By the
definition of T-revision function, we have (||tr )  |T  |tr . Thus |T  |tr is also coherent
which implies T   is coherent.
For the other direction, suppose  is a function for a TBox T that satisfies (T  1)(T  6)
and (T  f ). Let  be defined as
(||tr ) = |T  |tr
for all conjunctions of TBox axioms . For a set of t-types M , if there is no conjunction
of TBox axioms  such that ||tr = M , then define (M ) =  if M is incoherent; (M ) =
M |T |tr if M |T |tr is coherent; and (M ) = M if M is coherent and M |T |tr is incoherent.
We need to show that (1)  is a faithful and coherent preserving selection function for T
and (2) T   = Tr ((||tr )).
Part (1): For  to be a faithful and coherent preserving selection function, it has to be
a function first. And this follows directly from the definition of  and (T  6). Let M be a
set of t-types. Suppose M is coherent. We need to show (M ) 6= . By the definition of ,
the result trivially holds if there is no conjunction of TBox axioms  such that ||tr = M .
If there is  such that ||tr = M , then we have by (T  5) that |T  |tr is coherent. So we
have (||tr ) = |T  |tr 6= .
Suppose M is incoherent. We need to show (M ) = . By the definition of , the result
trivially holds if there is no conjunction of TBox axioms  such that ||tr = M . If there is 
such that ||tr = M , then it follows from (T f ) that |T |tr = . Thus (||tr ) = |T |tr = .
374

fiDL-Lite Contraction and Revision

For faithfulness, suppose M is coherent. We need to show |T  |tr  M  (M ). By
the definition of , the result trivially holds if there is no conjunction of TBox axioms 
such that ||tr = M . If there is  such that ||tr = M , then it follows from (T  3) that
T    cl(T  {}) which implies |T  |tr  ||tr  |T  |tr . Since (||tr ) = |T  |tr ,
we have |T  |tr  ||tr  (||tr ). Now suppose |T  |tr  M is coherent. We need to
show (M ) = |T  |tr  M . Again the result trivially holds if there is no conjunction of
TBox axioms  such that ||tr = M . If there is  such that ||tr = M , then it follows from
(T  3) and (T  4) that T   = cl(T  {}) which implies |T  |tr  ||tr = |T  |tr . Since
(||tr ) = |T  |tr , we have |T  |tr  ||tr = (||tr ).
For coherent preserving, suppose M is coherent. We need to show (M ) is coherent.
Again the result trivially holds if there is no conjunction of TBox axioms  such that
||tr = M . If there is  such that ||tr = M , then it follows from (T  5) that T   is
coherent which implies |T  |tr is coherent. Since (||tr ) = |T  |tr , (||tr ) is coherent.
Part (2): By the definition of , we have (||tr ) = |T  |tr . Since it follows from (T  1)
that T   is closed, we have by the definition of Tr that T   = Tr (|T  |tr ) = Tr ((||tr )).

Proof for Proposition 16
The complexity results have been explained earlier. Let  be a function such that
T   = TREVI (T , ) for all TBox T and conjunction of TBox axioms . We need to show
 is a T-revision function. By Theorem 12, it suffices to show  satisfies (T  1)(T  6) and
(T  f ). (T  1), (T  2), (T  6), and (T  f ) are trivially satisfied.
For (T  3), if T  {} is inconsistent then cl(T  {}) includes all axioms thus the
postulates holds trivially. So suppose T  {} is consistent. Then |T  {}|tr 6=  and
|T  {}|tr  |T |tr . As no new axiom is added to T throughout TREVI , we have |T 
{}|tr  |REV I(T , )|tr which implies TREVI (T , )  cl(T  {}). For (T  4), suppose
T  {} is coherent. Then the condition in line 4 is never fulfilled thus no axioms get
removed which means TREVI (T , ) = cl(T  {}). Now we focus on (T  5). Given
a t-type  and an atomic concept or role F , by the definition of t-model if F   then
 6|=tr B v . It is not hard to see that {1 , . . . , n }  (|T |tr  ||tr )  |TREVI (T , )|tr for i
the t-models of  picked at line 5 of TREVI . Due to line 48 of TREVI , if F is such that
|T |tr  ||tr |=tr F v  then there is   {1 , . . . , n } such that F   which means for any
F we have {1 , . . . , n }  (|T |tr  ||tr ) 6|=tr F v  which implies |TREVI (T , )|tr 6|=tr F v .
Thus TREVI (T , ) is coherent.

Proof for Theorem 13
For one direction, suppose  is an A-revision function for AT and the associated selection
function is . We need to show  satisfies (A  1)(A  6), and (A  f ).
(A  1), (A  2), (A  6), and (A  f ) follow immediately from the definition of A-revision
function. We only show the proof for (A  3)(A  5).
(A  3), (A  4): If |(A  {})T |ar = , then the two postulates hold trivially. So suppose
|(A  {})T |ar 6=  which implies |AT |ar  |{}T |ar 6= . Since  is faithful, we have |AT |ar 
|{}T |ar = (|{}T |ar ). Then it follows from the definition of A-revision function that
AT   = ATr ((|{}T |ar )) = ATr (|AT |ar  |{}T |ar ) = cl(AT  {}).
(A  5): Suppose |{}T |ar 6= . Then by the definition of , (|{}T |ar ) 6= . Since it
follows from the definition of A-revision function that (|{}T |ar )  |AT  |ar , |AT  |ar 6= .
375

fiZhuang, Wang, Wang, & Qi

For the other direction, suppose  is a function for an ABox AT that satisfies (A  1)
(A  6), and (A  f ). Let  be defined as
(|{}T |ar ) = |(AT  )T |ar
for all conjunctions of ABox axioms . For a set of a-types M , if there is no conjunction
of ABox axioms  such that |{}T |ar = M , then define (M ) = M  |AT |ar whenever
M  |AT |ar 6=  and M otherwise. We need to show that (1)  is a faithful selection function
for AT and (2) AT   = ATr ((|{}T |ar )).
Part (1): For  to be a faithful selection function, it has to be a function first. And this
follows directly from the definition of  and (A6). Let M be a set of a-types. Suppose M 6=
. We need to show (M ) 6= . By the definition of , the result trivially holds if there is no
conjunction of ABox axioms  such that |{}T |ar = M . If there is  such that |{}T |ar = M ,
then we have by (A  5) that |(AT  )T |ar 6= . So we have (|{}T |ar ) = |(AT  )T |ar 6= .
Suppose M = . We need to show (M ) = . For any axiom  that is inconsistent with
T , we have |{}T |ar = . It follows from (A  f ) that in this case |(AT  )T |ar = , thus
(|{}T |ar ) = |(AT  )T |ar = . For faithfulness, suppose |AT |ar  M 6= , we need to show
(M ) = M  |AT |ar . The result holds trivially if there is no  such that |{}T |ar = M .
If there is  such that |{}T |ar = M , then we have |AT |ar  |{}T |ar 6=  which implies
|A  {}T |ar 6= . So it follows from (A  3) and (A  4) that |clT (A  {}T )|ar = |AT  |ar .
Thus (|{}T |ar ) = |(AT  )T |ar = |cl(A  {}T )|ar = |AT |ar  |{}T |ar .
Part (2): By the definition of , we have (|{}T |ar ) = |(AT )T |ar . Since it follows from
(A1) that A is closed, we have by the definition of ATr that AT  = ATr (|(AT )T |ar ) =
ATr ((|{}T |ar )).

Proof for Proposition 17
The complexity results have been explained earlier. Let  be a function such that
AT   = AREVI (AT , ) for all ABox AT and conjunction of ABox axioms . We need to
show  is an A-revision function. By Theorem 13, it suffices to show  satisfies (A1)(A6)
and (A  f ). (A  1), (A  2), (A  6), and (A  f ) are trivially satisfied.
For (A  3), suppose  is inconsistent with T or is consistent with (T , A). Then line 2
and line 3 of AREVI (T , A, ) guarantee that the postulate holds. So suppose  is consistent
with T but is inconsistent with (T , A). Since no new axiom is added to A in lines 68, in line
9 the returned ABox must be a subset of clT (AT  {}). For (A  4), suppose  is consistent
with (T , A). Then line 4 of AREVI (T , A, ) guarantees that clT (AT  {}) = AT  .
For (A  5), suppose  is consistent with T . Then if  is consistent with (T , A), the ABox
returned in line 4 must be consistent, and if  is inconsistent with (T , A), lines 68 guarantee
that all axioms in A that are inconsistent with  is removed, thus the ABox returned in
line 9 is also consistent.


References
Alchourron, C. E., Gardenfors, P., & Makinson, D. (1985). On the logic of theory change:
Partial meet contraction and revision functions. The Journal of Symbolic Logic, 50 (2),
510530.
376

fiDL-Lite Contraction and Revision

Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2003).
The Description Logic Handbook. CUP, Cambridge, UK.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
reasoning and efficient query answering in description logics: The DL-Lite family.
Journal of Automatic Reasoning, 39 (3), 385429.
Calvanese, D., Kharlamov, E., Nutt, W., & Zheleznyakov, D. (2010). Evolution of DL-Lite
knowledge bases. In Proceedings of the 9th International Semantic Web Conference
(ISWC-2010), pp. 112128.
Dalal, M. (1988). Investigations into a theory of knowledge base revision. In Proceedings of
the 7th National Conference on Artificial Intelligence (AAAI-1988), pp. 475479.
De Giacomo, G., Lenzerini, M., Poggi, A., & Rosati, R. (2009). On instance-level update
and erasure in description logic ontologies. Journal of Logic Computation, 19 (5),
745770.
Ferme, E., Krevneris, M., & Reis, M. (2008).
An axiomatic characterization of
ensconcement-based contraction. Journal of Logic and Computation, 18 (5), 739753.
Flouris, G., Huang, Z., Pan, J. Z., Plexousakis, D., & Wache, H. (2006). Inconsistencies,
negations and changes in ontologies. In Proceedings of the 21st National Concference
on Artificial Intelligence (AAAI-2006).
Gardenfors, P. (1988). Knowledge in Flux: Modelling the Dynamics of Epistemic States.
MIT Press.
Grau, B. C., Ruiz, E. J., Kharlamov, E., & Zhelenyakov, D. (2012). Ontology evolution
under semantic constraints. In Proceedings of the 13th International Conference on
Principles of Knowledge Representation and Reasoning (KR-2012), pp. 137147.
Hansson, S. O. (1991). Belief Contraction Without Recovery. Studia Logica, 50 (2), 251260.
Hansson, S. O. (1994). Kernel contraction. The Journal of Symbolic Logic, 59 (3), 845859.
Hansson, S. O. (1997). Semi-revision. Journal of Applied Non-Classical Logics, 7 (1-2),
151175.
Hansson, S. O. (1999). A Textbook of Belief Dynamics Theory Change and Database Updating. Kluwer.
Hansson, S. O., & Wassermann, R. (2002). Local change. Studia Logica, 70 (1), 4976.
Kalyanpur, A., Parsia, B., Sirin, E., & Cuenca-Grau, B. (2006). Repairing unsatisfiable
concepts in OWL ontologies. In Proceedings of the 3rd European Semantic Web Conference (ESWC-2006), pp. 170184.
Katsuno, H., & Mendelzon, A. O. (1991). On the difference between updating a knowledge
base and revising it. In Proceedings of the 2nd International Conference on Principles
of Knowledge Representation and Reasoning (KR-1991), pp. 387394.
Katsuno, H., & Mendelzon, A. O. (1992). Propositional knowledge base revision and minimal change. Artificial Intelligence, 52 (3), 263294.
377

fiZhuang, Wang, Wang, & Qi

Kharlamov, E., & Zheleznyakov, D. (2011). Capturing instance level ontology evolution for
DL-Lite. In Proceedings of the 10th International Semantic Web Conference (ISWC2011), pp. 321337.
Kharlamov, E., Zheleznyakov, D., & Calvanese, D. (2013). Capturing model-based ontology
evolution at the instance level: The case of DL-Lite. Journal of Computer and System
Sciences, 79 (6), 835872.
Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2010). Logic-based ontology comparison
and module extraction, with an application to DL-Lite. Artificial Intelligence, 174 (15),
10931141.
Lenzerini, M., & Savo, D. F. (2011). On the evolution of the instance level of DL-Lite
knowledge bases. In Proceedings of the 24th International Workshop on Description
Logics (DL-2011).
Lenzerini, M., & Savo, D. F. (2012). Updating inconsistent description logic knowledge
bases. In Proceedings of the 20th European Conference on Artificial Intelligence
(ECAI-2012), pp. 516521.
Levi, I. (1991). The Fixation of Beliefs and its Udoing. Cambridge University Press.
Pan, J. Z., & Thomas, E. (2007). Approximating OWL-DL ontologies. In Proceedings of
the 22nd National Conference on Artificial Intelligence (AAAI-2007), pp. 14341439.
Qi, G., & Du, J. (2009). Model-based revision operators for terminologies in description
logics. In Proceedings of the 21st International Joint Conferences on Artificial Intelligence (IJCAI-2009), pp. 891897.
Qi, G., Haase, P., Huang, Z., Ji, Q., Pan, J. Z., & Volker, J. (2008). A kernel revision operator
for terminologies algorithms and evaluation. In Proceedings of the 7th International
Semantic Web Conference (ISWC-2008), pp. 419434.
Qi, G., Liu, W., & Bell, D. A. (2006). Knowledge base revision in description logics.
In Proceedings of the 10th European Conference on Logics in Artificial Intelligence
(JELIA-2006), pp. 386398.
Ribeiro, M. M., & Wassermann, R. (2009). Base revision for ontology debugging. Journal
of Logic and Computation, 19 (5), 721743.
Ribeiro, M. M., Wassermann, R., Flouris, G., & Antoniou, G. (2013). Minimal change:
Relevance and recovery revisited. Artificial Intelligence, 201, 5980.
Satoh, K. (1988). Nonmonotonic reasoning by minimal belief revision. In Proceedings of
the International Conference on Fifth Generation Computer Systems, pp. 455462.
Wang, Z., Wang, K., & Topor, R. W. (2015). DL-Lite ontology revision based on an
alternative semantic characterization. ACM Transaction on Computational Logic,
16 (4), 31:131:37.
Winslett, M. (1990). Updating Logical Databases. Cambridge University Press.
Zhuang, Z., Wang, Z., Wang, K., & Qi, G. (2014). Contraction and revision over DLLite TBoxes. In Proceedings of the 28th AAAI Conference on Atificial Intelligence
(AAAI-2014), pp. 11491156.

378

fiJournal of Artificial Intelligence Research 56 (2016) 89

Submitted 10/15; published 05/16

Optimal Any-Angle Pathfinding In Practice
Daniel Harabor

daniel.harabor@nicta.com.au

The University of Melbourne and
National ICT Australia, Victoria Laboratory
115 Batman St, Melbourne, 3003, Australia

Alban Grastien

alban.grastien@nicta.com.au

National ICT Australia, Canberra Laboratory
7 London Circuit, Canberra, 2601, Australia

Dindar Oz

dindar.oz@yasar.edu.tr

Yasar University
Bornova, Izmir, 35100, Turkey

Vural Aksakalli

aksakalli@sehir.edu.tr

Istanbul Sehir University
Altunizade, Istanbul, 34662, Turkey

Abstract
Any-angle pathfinding is a fundamental problem in robotics and computer games. The
goal is to find a shortest path between a pair of points on a grid map such that the
path is not artificially constrained to the points of the grid. Prior research has focused
on approximate online solutions. A number of exact methods exist but they all require
super-linear space and pre-processing time. In this study, we describe Anya: a new and
optimal any-angle pathfinding algorithm. Where other works find approximate any-angle
paths by searching over individual points from the grid, Anya finds optimal paths by
searching over sets of states represented as intervals. Each interval is identified on-thefly. From each interval Anya selects a single representative point that it uses to compute
an admissible cost estimate for the entire set. Anya always returns an optimal path if
one exists. Moreover it does so without any offline pre-processing or the introduction of
additional memory overheads. In a range of empirical comparisons we show that Anya is
competitive with several recent (sub-optimal) online and pre-processing based techniques
and is up to an order of magnitude faster than the most common benchmark algorithm, a
grid-based implementation of A*.

1. Introduction
Any-angle pathfinding is a common navigation problem in robotics and computer games. It
takes as input a pair of points from a uniform two-dimensional grid and asks for a shortest
path between them that is not artificially constrained to the points of the grid. Such anyangle paths are desirable to compute as they are typically shorter than their grid-constrained
counterparts and because following such a trajectory can give the appearance of realism and
intelligence; e.g. to the player of a computer game. Despite its apparent simplicity anyc
2016
AI Access Foundation. All rights reserved.

fiHarabor, Grastien, Oz & Aksakalli

angle pathfinding is surprisingly challenging. So far many successful and popular methods
have been proposed, yet they all involve trade-offs of some kind. We begin with a few
examples that highlight, in broad strokes, the main research trends and their limitations,
to date.
In the communities of Artificial Intelligence and Game Development the any-angle
pathfinding problem is often solved efficiently using a technique known as string pulling.
The idea is to compute a grid-optimal path and then smooth the result; either as part of a
post-processing step (e.g. Pinter, 2001; Botea, Muller, & Schaeffer, 2004) or by interleaving
string pulling with online search (e.g. Ferguson & Stentz, 2005; Nash, Daniel, Koenig, &
Felner, 2007). Regardless of the particular approach, all string pulling techniques suffer
from the same disadvantages: (i) they require more computation than just finding a path
and; (ii) they only yield approximately shortest paths.
In the communities of Robotics and Computational Geometry a related and more general problem has been well-studied: finding Euclidean shortest paths between polygonal
obstacles in the plane. Visibility Graphs (Lozano-Perez & Wesley, 1979) and the Continuous Dijkstra paradigm (Mitchell, Mount, & Papadimitriou, 1987) are among the best known
and most influential techniques that originate from this line of research. Even though both
of these methods are optimal and efficient in practice they nevertheless suffer from having
often undesirable properties: (i) the search graph1 must be pre-computed during an offline
pre-processing step; (ii) if the map changes at any point the search graph is invalidated and
must be recomputed, usually from scratch.
To date, it is not clear if there exists an any-angle pathfinding algorithm that is simultaneously online, optimal and also practically efficient (i.e. at least as fast in practice as
grid-based pathfinding using A* search). In this manuscript, we present new work that
answers this open question in the affirmative by introducing a new any-angle pathfinding
algorithm called Anya. Our approach bears some similarity to existing works from the
literature, most notably those algorithms based on the Continuous Dijkstra paradigm. In
rough overview:
 Where other methods search over the individual nodes of the grid, Anya searches
over contiguous sets of states that form intervals.
 Each Anya interval has a single representative point that is used to derive an admissible cost estimate (i.e f -value) for all points in the set.
 To progress the search process Anya projects each interval, from one row of the grid
onto another, until the target is reached.
Anya always finds an optimal any-angle path, if one exists. In addition Anya does not
rely on any pre-computation nor does it introduce any memory overheads (in the form of
auxiliary data structures) beyond what is required to maintain an open and closed list. A
theoretical description of this algorithm has previously appeared in the literature (Harabor
& Grastien, 2013). In this study we extend that work in several ways: (i) we give a
1. We distinguish between the search graph and the input grid map. Though in some contexts these terms
coincide exactly this is not true in general. In particular the search graph may be a subset of the input
grid or may be a related but entirely separate data structure.

90

fiOptimal Any-Angle Pathfinding In Practice

Visible

Visible

Non-visible

2

2

2

1

1

1

0

0
0

1

2

Non-visible

2
1

0
0

1

2

0

1

2

0
0

1

2

Figure 1: Examples of visible and non-visible pairs of points.

detailed conceptual description of the Anya algorithm and provide an extended theoretical
argument for optimality and completeness; (ii) we discuss the practical considerations that
arise when implementing the algorithm and we give a technical description for one possible
and efficient implementation; (iii) we make detailed empirical comparisons showing that
Anya is competitive with a range of recent sub-optimal techniques from the literature,
including those based on offline pre-processing, and is up to one order of magnitude better
than our benchmark grid-based implementation of A*; (iv) we discuss a range of possible
extensions for further improving the current results.

2. The Optimal Any-Angle Pathfinding Problem
A grid is a planar subdivision consisting of W  H square cells. Each cell is an open set
of interior points which are all traversable or all non-traversable. The vertices associated
with each cell are called the discrete points of the grid. Edges in the grid can be interpreted
as open intervals of intermediate points; each one representing a transition between two
discrete points. Each type of point p = (x, y) has a unique coordinate where x  [0, W ] and
y = [0, H], with discrete points limited to the subset of integer x and y values.
A discrete or intermediate point is traversable if it is adjacent to at least one traversable
cell. Otherwise it is non-traversable. A discrete point which is common to exactly four
adjacent cells is called an intersection. Any intersection where three of the adjacent cells
are traversable and one is not is called a corner. Two points are visible from one another
if they can be connected by a straight-line path (i.e. a sequence of adjacent points, either
intermediate or discrete) that does not: (i) pass through any non-traversable point or
(ii) pass through an intersection formed by two diagonally-adjacent non-traversable cells.
Figure 1 shows some examples that help to better illustrate this idea.
An any-angle path  is a sequence of points hp1 , . . . , pk i where each pi is visible from pi1
and pi+1 . The length of  is the cumulative distance between every successive
pair of points
p
0
0
0
0
d(p1 , p2 )+. . .+d(pk1 , pk ). The function d(p = (x, y), p = (x , y )) = (x  x )2 + (y  y 0 )2
is a uniform Euclidean distance metric. We will say pi   is a turning point if the segments
(pi1 , pi ) and (pi , pi+1 ) form an angle not equal to 180 2 . Finally, the any-angle pathfinding
problem is one that requires as input a pair of discrete points, s and t, and asks for an anyangle path connecting them. The point s designates the source (equivalently, start) location
2. It is well-known that the turning points in optimal any-angle paths are corner points; e.g. as shown
by Mitchell et al. (1987).

91

fiHarabor, Grastien, Oz & Aksakalli

while the point t designates the target (equivalently, goal) location. Such a path is optimal
if there exists no alternative any-angle path between s and t that is strictly shorter.
Figure 2 provides an example of an optimal any-angle pathfinding problem. As can be
seen the source, target and all obstacles have discrete positions however the path itself does
not need to follow the grid. Notice also that the trajectory of this path appears much more
realistic than any alternative restricted to turning at modulo 45 deg or 90 deg.
4
3
2
1

s
t

0
0
1
2
3
4
5
6
7
8
Figure 2: Example of an any-angle pathfinding problem together with its solution.

3. An Overview of Anya
Consider the any-angle instance shown in Figure 3. In this example the optimal path
between s and t needs to first head towards the corner point n and then change direction
toward the target t. One possible approach to solving this problem involves computing
a visibility graph: i.e., identifying all pairs of corners that are visible from one another,
and also visible from the start and target locations, and then searching for a path on this
graph. The main drawback in this case is that the visibility graph can be quite large (up
to quadratic in the size of the grid) and very expensive to compute.
An alternative approach, which avoids these overheads, is to solve the problem online.
Unfortunately online search methods generally consider only the discrete points of the grid
and their immediate neighbours. For example, when expanding the point s it is common to
only generate the neighbours: (1, 0), (2, 1), and (3, 0) in the example
of Figure3. The A*

f -value for
 each of the three neighbours is, respectively, 1 + 34 ' 6.83, 1 + 20 ' 5.47,
and 1 + 26 ' 6.1 (using Euclidean-distance
as a heuristic). By comparison the optimal


any-angle path has cost of 10 + 5 ' 5.4. Immediately we can see that the heuristic at
hand does not satisfy one of the essential properties of A* search: that the f -value of each
node should always be an underestimate of the actual distance to the goal. Without this
property A* is not guaranteed to be optimal.
The issue described above comes from the fact that the optimal path does not go through
any of the points (1, 0), (2, 1), or (3, 0). Instead the optimal path crosses row 1 at point y1 ,
which is not part of the search space. To ensure optimality we should consider all points
such as y1 rather than just the discrete points of the grid. There are however many such
points including e.g., points such as y10 (leading to (3, 6)), which apriori seems a reasonable
candidate for expansion, but which does not appear on any optimal path.
92

fiOptimal Any-Angle Pathfinding In Practice

6
5

t

4
n

3
2

y2

y10

1

y1
s

0
0

1

2

3

4

5

6

Figure 3: When pathfinding from s to n, online algorithms such as A* and Theta* only
expand discrete points from the grid and never intermediate points such as yi .
In general we need to consider all potential yi points defined as a fraction wh where
h  {0, . . . , H} and w  {1, . . . , W }. This set is quadratic in n = min(W, H). To understand why, we consider the Farey Sequence of order n, the sequence (ordered by increasing
number) of all rational numbers between 0 and 1 that can be written as a fraction whose
denominator is an integer lower than n. For instance, the Farey Sequence of order n = 6
is: 0, 16 , 15 , 14 , 31 , 25 , 21 , 53 , 23 , 43 , 54 , 65 , 1. Notice that 13 = 26 , which explains why the length of this
sequence is not n(n + 1)  2; still the asymptotic cardinality of this sequence is known to
2
be 3n
(Graham, Knuth, & Patashnik, 1989, ch. 9).
2
Since the quadratic behaviour of the Farey Sequence makes it impractical to enumerate
all potential yi points we propose to consider, instead of individual points, a set of points
that appear together as part of a contiguous interval on the grid. In the example of Figure 3
we would consider all the points lying between (0, 1) and (3, 1), at the same time and as
part of a single A* search node. In this framework we need to:
 define formally an Anya search node,
 define the set of successors of a search node,
 define how to compute the f -value of a search node,
 prove optimality of the returned path,
 terminate search when no path is available,
 ensure the Anya algorithm is efficient in practice.
93

fiHarabor, Grastien, Oz & Aksakalli

4. Algorithm Description
This section presents in detail the Anya algorithm and its properties. Since Anya is a
variant of A* we first present its search space: the search nodes, the successors of a node
and the evaluation function used to rank nodes during search. We then a give pseudo-code
description of the algorithm and discuss its properties. Improvements that make Anya
efficient in practice are presented in the next section.
4.1 Anya Search Nodes
We now define the notion of interval, which is at the core of Anya.
Definition 1 A grid interval I is a set of contiguous and pairwise visible points drawn
from any discrete row of the grid. Each interval is defined in terms of its endpoints a and b.
With the possible exception of a and b, each interval contains only intermediate and discrete
non-corner points.
By definition, all points in an interval share the same y position, which is a positive
integer. Moreover, the x position of all points in an interval (including that of endpoints
a and b) is a rational number3 . We will use normal parentheses ( or ) to indicate an
interval endpoint that is open and square brackets [ or ] to indicate an interval endpoint
that is closed. For example, the interval I = (a, b] is open at (i.e. does not include) a and
closed at (i.e. does include) b.
Identifying intervals is simple: any row of the grid can be naturally divided into maximally contiguous sets of traversable and non-traversable points. Each traversable set forms
a tentative interval which we can split, repeatedly if necessary, until all corner points are
end points of intervals. Intervals can also be identified through an operation called projection. We discuss this procedure in the next sub-section. For now we note only that intervals
produced by way of projection can also have non-discrete and non-corner endpoints.
A significant advantage of Anya is that we construct intervals on-the-fly. This allows
us to start answering queries immediately and for any discrete start-target pair. Similar
algorithms, e.g. Continuous Dijkstra (Mitchell et al., 1987), require a pre-processing step
before any queries can be answered and then only from a single fixed start point.
Definition 2 A search node (I, r) is a tuple where r 6 I is a point called the root and I is
an interval such that each point p  I is visible from r. To represent the start node itself,
set I = [s] and assume r is located off the plane and visible only from s; the cost from r to
s in this case is zero.
An A* search node, together with its parents, traditionally represents the single path
defined by travelling in straight line between the points in the search nodes from the root
to the current node. An Anya search node similarly defines paths obtained by visiting
the roots of each nodes and ending in the interval of the current node. A node therefore
represents many paths and the root of the search node is always the last (common) turning
3. As per the problem definition, every point (x, y) appearing on an optimal any-angle path belongs to a
Farey Sequence and all such points are rational.

94

fiOptimal Any-Angle Pathfinding In Practice

point of these paths: it will always be either the root of the parent node or one of the end
points of the parent interval.
Besides the start node, which we treat as a special case, there are two other types of
search nodes: cone nodes and flat nodes. An example of a cone node is shown in Figure 4.
Such nodes are characterised by the fact that the root r is not on the same row as its
associated interval I. Notice in the example that although the interval I = [a, b] is maximal,
it does not have any endpoints which are obstacles, corners or indeed even discrete points of
the grid (here the left endpoint a is (2.5, 4) while the right endpoint b is (5.5, 4)). Examples
of flat nodes are shown in Figure 5. The two nodes are: ((a1 , b1 ], r) and ((a2 , b2 ], r). Flat
nodes are characterised by the fact that the root r is on the same row as the interval I.
Notice in the examples given that a1 = r (resp. a2 = b1 ) is excluded from the first (resp.
second) interval. The semantics of every search node is that the current position is located
somewhere in the interval I and we reach that point by an any-angle path whose most
recent turning point is r.
5

5
a

4

b

4

3

3

2

2

1

r = a1

b1 = a2

b2

1

r

0

0
0

1

2

3

4

5

6

Figure 4: Example of a cone search node.

0

1

2

3

4

5

6

Figure 5: Example of two flat search nodes.

4.2 Searching with Anya: Successors
The successors of a search node n are identified by computing intervals over sets of traversable
points; from the same row of the grid as the current node n and from the rows immediately
adjacent. We want to guarantee that each point in such a set can be reached from the root
of n via a local path which is taut. Taut simply means that if we pull on the endpoints of
the path we cannot make it any shorter. We now provide a formal definition of a successor
and then discuss how this definition can be applied in practice.
Definition 3 A successor of a search node (I, r) is a search node (I 0 , r0 ) such that
1. for all points p0  I 0 , there exists a point p  I such that the local path hr, p, p0 i is taut;
2. r0 is the last common point shared by all paths hr, p, p0 i; and
3. I 0 is maximal according to the points above and the definition of a search node.
95

fiHarabor, Grastien, Oz & Aksakalli

The first requirement (tautness) implies that each successor p0  I 0 can be reached from
the root of the current node r by a path that is locally optimal. We will use this property
in the next subsection to show that Anya always finds a globally optimal path if one exists
at all. The third property, requiring that each successor have an interval that is maximal,
exists for the purpose of practical efficiency: simply put, we do not want to have arbitrarily
small and arbitrarily many successors. Instead, we will make each successor interval as
large as possible. The second property has two interpretations. When r0 = r we will say
that the successor node is observable. Similarly when r0 = p we will say that the successor
is non-observable. We explore each of these ideas in turn.
5
v1

4

v2

u2

u3

v3
r0

a

3

=b
u1

2
1

r

0
0

1

2

3

4

5

6

7

Figure 6: Successors of a cone search node, n = ([a, b], r). There are five successors: ([v1 , v2 ], r) and ((v2 , v3 ], r) which are observable and ((r0 , u1 ], r0 ), ((v3 , u2 ), r0 ), and
([u2 , u3 ], r0 ) which are not.

5
4
a

3
2

b

c

d

e

1
0
0

1

2

3

4

5

6

Figure 7: Successors of a flat search node, n = ((a, b], a). There are two successors: ((b, c], a)
which is observable and ([d, e], b) which is not.
96

fiOptimal Any-Angle Pathfinding In Practice

Algorithm 1 Computing the successor set
1: function successors(n = (I, r))
. Takes as input the current node
2:
if n is the start node s then
3:
return generate-start-successors(I = [s])
4:
end if
5:
successors  
6:
if n is a flat node then
7:
p  endpoint of I farthest from r
. Successor interval starts from p
8:
successors  generate-flat-successors(p, r)
. Observable successors
9:
if p is a turning point on a taut local path beginning at r then
10:
successors  successors  generate-cone-successors(p, p, r) . Non-observable successors
11:
end if
12:
else
. If the node is not flat, it must be a cone
13:
a  left endpoint of I
14:
b  right endpoint of I
15:
successors  generate-cone-successors(a, b, r)
. Observable successors
16:
if a is a turning point on a taut local path beginning at r then
17:
successors  successors  generate-flat-successors(a, r)
. Non-observable
18:
successors  successors  generate-cone-successors(a, a, r)
. Non-observable
19:
end if
20:
if b is a turning point on a taut local path beginning at r then
21:
successors  successors  generate-flat-successors(b, r)
. Non-observable
22:
successors  successors  generate-cone-successors(b, b, r)
. Non-observable
23:
end if
24:
end if
25: end function

An observable successor is characterised by the fact that all points p0  I 0 are visible from
the current root point r. In this case the last common point shared by all local paths of the
form hr, p, p0 i is r. Observable successors are computed by projecting the current interval
on the next row. The projection identifies a maximal interval Imax that we will split at
each internal corner point point. Each interval produced by the split operation leads to a
new observable successor, and all such successors share the same root point as the original
(parent) node. This process is illustrated in Figure 6 where the interval I = [a, b] is projected
onto the next row. The projection identifies a maximal observable interval Imax = [v1 , v3 ]
which is subsequently split to create two observable successors: ([v1 , v2 ], r) and ((v2 , v3 ], r).
By comparison, a non-observable successor is characterised by the fact that all points
0
p  I 0 are not visible from the current root r. In this case all local paths of the form
hr, p, p0 i must pass through a (visibility obstructing) corner point whose identity is r0 := p.
Figure 6 illustrates the process of computing non-observable successors. First, from the
non-observable points to the right of the current interval I = [a, b], we construct a single
flat successor with I 0 = (b, u1 ] and root r0 := b. Non-observable points also exist to the
left of the current interval I but the local path to each such point (from r through a) is
not taut. Other non-observable successors can be found on rows of the grid adjacent to the
current interval I. By projecting the corner endpoint b onto the next row of the grid we
can construct two further non-observable successors: ((v3 , u2 ), b) and ([u2 , u3 ], b).
In Algorithm 1 we give an overview of the procedure that generates the successor set for
each search node. An overview of the sub-functions appearing in Algorithm 1 is given in
97

fiHarabor, Grastien, Oz & Aksakalli

the appendix. The implementation is straightforward, requiring nothing more complicated
than grid scanning operations and linear projections.
It is important to note at this stage that Anya does not perform any visibility checks
during the generation of successor nodes. Visibility checks are at the heart of many contemporary online algorithms, including Theta* (Nash & Koenig, 2013), which must determine
whether each successor is visible from some other node (e.g. the grand-parent node). On
the one hand visibility checks help Theta* et al. find shorter paths and expand fewer nodes
than traditional A* search. On the other hand, the computational overhead introduced by
these checks means that run-times can often be larger than A*. By comparison when Anya
projects an interval I, from one row of the grid to the next, the process involves only local
reasoning. In particular we can determine if the projection Imax is valid, invalid or if it
needs to be clipped by simply testing the traversability of cells located above, below and
to the left and right of the current interval I and the proposed Imax . The elimination of
visibility checks is an important practical advantage for Anya. As we will see in Section 9,
Anya not only finds shorter paths than online methods such as Theta* et al. it is also
usually much more efficient in terms of running time.
We now illustrate Algorithm 1 using previous examples. Consider the flat node ((a, b], a)
of Figure 7. The point p of Line 7 is set to b and the observable flat successor ((b, c], a) is
generated on Line 8. Furthermore since b is a turning point from a (Line 9), the interval
Imax = [d, e] is considered. Since Imax contains no interior corner points it is not split and
a single non-observable cone successor (I = Imax , b) is generated (Line 10).
Next, consider the cone node ([a, b], r) of Figure 6. First we generate the observable
successors (Line 15): the interval [a, b] is projected and the maximal interval Imax = [v1 , v3 ]
is identified. Imax is then split at the internal corner point v2 leading to two observable cone
successor nodes, (I1 = (v1 , v2 ], r) and (I2 = (v2 , v3 ], r). Notice that no line-of-sight visibility
check is required here. Next since b is a turning point we look for non-observable successors
as well (Lines 20-22). The flat successor ((b, u1 ], b) is generated as per the previous example.
Meanwhile the maximal (non-observable) cone interval Imax = (v3 , u3 ] is also identified.
This interval is split at the internal corner point u2 resulting in two non-observable cone
successor nodes, (I3 = (v3 , u2 ], b) and (I4 = (u2 , u3 ], b).
Algorithm 1 treats the start node (Lines 2-4) as a special case because its root point
is located off the grid. The successors of the start node (i) are all non-observable intervals
from the root and (ii) can be found to the left and right of the start location, on the row
immediately above the start location and on the row immediately below.
4.3 Evaluating an Anya Search Node
The search procedure of Anya, similarly to that of A*, always expands the most promising node found so far. It is therefore necessary to evaluate each root and interval pair.
This evaluation corresponds to an estimate f of the minimal length of a path from the
source to the target through the current interval. An optimality condition of A* is that
this estimate is optimistic (i.e. it is never larger than the actual optimal path length). In
classical A* where a search node n corresponds to a single point p on the grid the value
f (n) is computed as the sum of g(p), the length of the path from the source to p, and h(p),
an (under)estimation of the length of the shortest path from p to the target.
98

fiOptimal Any-Angle Pathfinding In Practice

As a search node n = (I, r) represents a set of points its f value is the minimum f value
of the points in the node:
f (n) = inf f (s, r, p, t)
pI

where f (s, r, p, t) is an (under)estimate of the shortest path from s to t through r and p.
It should be noted that, because the set of points p is continuous and potentially open, the
minimum is replaced by the infimum. Since all points in the interval are visible from r, this
value can be broken down as follows:
f (s, r, p, t) = g(r) + d(r, p) + h(p)
where d(r, p) is the distance between the points r and p.
Finding the point of the interval that minimises the f value may seem like a hard problem
since the interval contains a large number of points and we want to avoid generating all of
them. However the straight-line distance heuristic h (h(p) = d(p, t)) makes it easy to isolate
the point p that minimises the f value, thanks to two simple geometric observations. More
precise heuristics are available but these could make it harder to find the point p.
Lemma 1 Let t and r be two points s.t. the interval I is on the same row as t or on a row
between the rows of r and t. Then the point p of I with infimal f -value is the point in I
closest to the intersection of the straight-line path ht, ri with the row of I.
If the line between r and t intersects the interval then the point p is the intersection.
Otherwise this point p is one of the endpoints of the interval. In the event that the precondition of Lemma 1 is not satisfied, it is possible to replace t by its mirrored version t0
through I and thus satisfy the precondition. This case is described in Lemma 2.
Lemma 2 The mirrored point t0 of target t through interval I is such that d(p, t) = d(p, t0 )
for all p  I.
Lemma 2 is a trivial geometrical result. Both lemmas are illustrated on Figure 8.
4.4 Search Procedure
The search procedure employed by Anya is presented in Algorithm 2. It follows the pattern
of A* and uses priority queue, open, that stores all the yet-to-be-expanded search nodes
ordered by f value. Each node stores a pointer to its parent. At each step of the search
Anya extracts the best node from open and checks if the corresponding interval contains
the target. In the event that the target is found (Line 6) the returned path is a sequence
of root points constructed by following back-pointers, from the current node to the start
location. If the target is not found the current node is expanded and its successors are
added to the priority queue (Line 8). Some successors may be considered redundant and
these can be safely discarded without insertion into the priority queue (Line 9). We discuss
this aspect of the algorithm in Section 6; for now it suffices to know that such successors
are not on any optimal path. The expansion process continues until the target is found or
the open list is exhausted, in which case the algorithm returns failure (Line 14).
In the next sections we will prove some fundamental properties about this algorithm:
correctness, optimality and completeness.
99

fiHarabor, Grastien, Oz & Aksakalli

4

t1
t04

t2

3

t3

2

a

b

1

t4

0
0

1

2

r

3

4

5

6

Figure 8: An illustration of Lemmas 1 and 2. We evaluate the node n = ([a, b], r). The
points t1 and t04 correspond to the case where the row of the target t intersects the interval
I; t2 and t3 where it does not; t4 where the mirrored target t04 must be used.

5. Correctness and Optimality
In this section we prove that Anya is correct and always finds an optimal path. In particular
we will show (i) that the optimal path appears in the search space, (ii) when the target
is expanded we have found an optimal path, and (iii) that each node in the search space
will be reached in a finite number of steps. The topics of termination and completeness are
discussed in Section 6.
We begin the analysis by recalling that a search node n = (I, r) represents a set of potential
paths (from s to r and from r to each point p  I). Following these semantics we will say
that n is a search node of a path  if r   and I intersects .
Lemma 3 If n = (I, r) is a search node of an optimal path   then: either n contains the
target t or n has at least one successor n0 that is also a search node of   .
Proof: Start node: n is the start node with I = [s] and r located off the grid. Additionally, n is a search node of   (hypothesis). Algorithm 1 (Line 3) scans the traversable
Algorithm 2 Anya
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

input: Grid, source location s, target location t
open  {(I = [s], r0 )}
while open is not empty do
(I, r)  pop(open)
if t  I then
return path to(I)
end if
for all (I 0 , r0 )  successors(I, r) do
if should prune(I 0 , r0 ) then
open  open  {(I 0 , r0 )}
end if
end for
end while
return null

100

. Start node with root r0 located off the grid

. Successor pruning

fiOptimal Any-Angle Pathfinding In Practice

points of the grid that are visible from and adjacent to s. These points are located to the
left and right of s and they are located on the rows immediately above and immediately
below the row of s. Algorithm 1 assigns each of these points to an interval I 0 and each I 0 is
associated with a successor node that has as its root r0 = s. Every optimal path must pass
through I = [s] and there are no traversable points that can be reached from s without
passing through an interval associated with a successor of s. This is sufficient to satisfy the
lemma.
Other nodes: n is an arbitrary node of   and t 6 I (if t  I there are no further successors
and we are done). By definition r    and p  I is the (apriori unknown) intersection of
  with the interval I. There are now two possibilities to consider, depending on whether
p is a turning point or not. We will show that in both cases there is a successor of n whose
interval I 0 intersects   , which is sufficient to satisfy the lemma.
Case 1 p  I is not a turning point. Algorithm 1 (Lines 8 and 15) scans all points that are
adjacent to I and (straight-line) visible from r through I. Each point is assigned to
a successor with observable interval I 0 and root point r. Thus at least one of the
successors of n intersects every straight line path from r through p which means at
least one successor of n intersects   .
Case 2 p  I is a turning point. In this case p must be a corner endpoint of I, otherwise  
is not taut and thus cannot be optimal. Algorithm 1 (Lines 10, 17, 18, 21) scans all
points that are adjacent to I and reachable from r through p by a taut local path.
These points are located on the row of p or on a row that is immediately adjacent.
Each such point is assigned to a successor with a non-observable interval I 0 and root
r = p. As the process is exhaustive all points reachable by a taut local path, from
r though p, must be assigned to an interval. Thus   must intersect at least one of
the successors of n.

Corollary 4 If there is a path between the source and the target, then the open list always
contains a search node of an optimal path (or this node is currently being processed).
Proof: By induction.
base case: The initial search node is a node of any path from s.
inductive case: assume that the open list contains a search node of an optimal path.
Then this node can only be removed if it is expanded. If this node does not contain the
target then we know from Lemma that one successor will be generated that is a search node
of this optimal path. Therefore a new search node of an optimal path will be inserted in
the open list.


Lemma 5 The first expanded node that contains the target t corresponds to one optimal
path to t.
101

fiHarabor, Grastien, Oz & Aksakalli

Proof: Sketch. First we notice that the f -value of a node is indeed the minimal value of
all the nodes in the interval, which means that f is an under estimate () of the actual cost
to the target. Second we notice that, given a search node (I, r) and its successor (I 0 , r0 ), for
each point p0  I 0 , the f -value of p0 is greater than or equal to the f -value of some point
p  I ( p = r0 if r0 6= r; p is the intersection of I and (r, p0 ) otherwise); the f function
is therefore monotonically increasing. Finally, the f function of a search node (I, r) is the
length of the path if t  I. Hence the f function of the nodes representing a sub-optimal
path to t will eventually exceed the optimal path distance, while the f function of the nodes
representing the optimal path will always remain under this value.


Lemma 6 If the target is reachable Anya will eventually expand a node whose interval
includes the target.
Proof: By contradiction, assume that Anya does not expand a node whose interval includes
the target. From Lemma 5 we know that failure to expand this node means that Anya
expand infinitely many nodes. We shall prove that doing so implies that the f value of
these nodes is unbounded and, therefore, the target is not reachable.
For most search nodes (I 0 , r0 ), the interval I 0 is on a different row than that of its parent
(I, r). Therefore, for those nodes, the value g(p0 ) is larger than the value g(p) by 1 or more.
This does not happen when the node is flat, but there can be only a bounded number of
successive flat nodes.4 Hence an infinite sequence of successive Anya nodes has an infinite length. Finally each Anya node has a bounded number of successors, meaning that an
infinite number of expansions will have to generate an infinite number of successive nodes. 

6. Completeness and Termination
Until now we have not specified any policy by which Anya can detect nodes that have
been previously expanded. In the context of optimal A* search such a policy is essential
to prevent cyclical re-expansion and to ensure that the algorithm eventually terminates,
even if there is no path between the start and target locations. In this section we describe
such a policy for Anya. Conceptually similar to the A* closed list our approach works
by tracking the best g-value associated with every root location (cf. every search node)
encountered during search.
As a motivating example consider Figure 9 where the root r is reached via two paths of
different length. In the example the green path is strictly longer than the red path and any
points reached via the green path will have a g-value that is strictly larger than the same
point when reached via the red path. Figure 10 shows a similar example where both green
and red paths reach the root point r with the same cost, resulting in two identical copies
of the successor node (I, r). Without any strategy to handle such root-level redundancies
the search process can generate many unnecessary nodes that slow progress to the goal.
Moreover, if there exists no path between the start and target location, the search may not
4. And, furthermore, the value g(p) does not increase significantly only for an unobservable flat cone.

102

fiOptimal Any-Angle Pathfinding In Practice

6

6
I

5
r

4

3

2

2

1

1
s
0

r

4

3

0

I

5

s

0
1

2

3

4

5

6

0

Figure 9: Root r is reached via two paths
of different lengths.

1

2

3

4

5

6

Figure 10: Root r is reached via two
paths of equal length.

terminate (e.g. when the input graph contains cycles it is possible to endlessly generate
copies of states with ever increasing g-values).
We propose the following strategy to avoid root-level redundancies:
1. We store a hash table of all visited roots with their best g-values. We call this table
the root history and apply it in a similar way to (and indeed in lieu of) a traditional
A* closed list.
2. When generating a search node n we check if its root is already in the root history
with a g-value less than or equal to its current g-value.
3. If the current g-value of the root improves on the value stored in the root history we
add the node to the open list. We also update the g-cost of the root5 in the root
history list.
4. Alternatively, if the current g-value of the root does not improve on the value stored
in the root history we simply discard the node (i.e. it is not added to open).
The root history is implemented as a hash table. Its size is O(n) where n is the number of
discrete points on a given input map. We now show that keeping a root history list does
not affect the correctness or optimality of search and that Anya is indeed complete and
does terminate.
Lemma 7 Anya search prunes only sub-optimal paths.
5. Similar updates to nodes on the closed list are sometimes performed in the context of incremental,
bounded cost or bounded sub-optimal search. Such updates are performed as part of an operation called
node re-opening. Our updates are not the same as node re-opening. In particular root points are never
directly expanded and thus never appear on the open list (Anya search comprise root-interval pairs).

103

fiHarabor, Grastien, Oz & Aksakalli

Proof: Trivial. If a search node has a root with a sub-optimal g value, then it represents
a sub-optimal path.


Lemma 8 Anya always terminates.
Proof: For Anya to not terminate, it must explore paths of arbitrary length. Such paths
must eventually involve the same root twice with the root being different in-between. Let
n and n0 be the two such search nodes. The g value associated with n0 must be higher than
the g value associated with n and, therefore, node n0 must be pruned. Indeed all sufficiently
long paths will be pruned and the open list will eventually be empty.


Lemma 9 Anya with redundant node pruning keeps at least one optimal path.
Proof: If a search node n = (I, r) is removed there exists another search node n0 (but
with different search parents) with a smaller (or equal) g-value that is kept. Assume that
n is a search node of an optimal path p1 , . . . , pk , and let pi  I be the point of this path
that intersects I. Since the g-value of n is similar to that of n0 , there exists another path

p01 , . . . , p0i , pi+1 , . . . , pk of similar length, and this path is not pruned.

7. Practical Pruning Strategies
A* orders nodes for expansion by evaluating and ranking how promising they each appear
(i.e. by their f -values). It is, however, possible to alter the order of expansion without
compromising the guarantees provided by A*: correctness, optimality and completeness.
Indeed such a strategy can even have a positive effect on the efficiency of the overall search.
In this section we discuss two practical strategies that modify expansion order and speed up
search. Both enhancements are applied on-the-fly and both focus on reducing the size of the
priority queue. The first strategy, Cul-de-sac Pruning, identifies nodes that can be safely
discarded because they cannot possibly lead to the goal. The second strategy, Intermediate
Pruning, is similar but works by avoiding the explicit generation of nodes that have only
a single successor (these successors are expanded immediately, without being added to the
open list).
7.1 Cul-de-sac Pruning
One way of reducing the size of the priority queue involves the early identification of culde-sacs (cds). A cds is a search node that has no successor and does not contain the target.
By definition a cds does not need to be added to the open list since its expansion cannot
lead to the target. A simple test to identify cds nodes is given in Algorithm 3 by way of
the procedure Is-cul-de-sac.
Early pruning of cds nodes speeds up search (and reduces required memory) by preventing some unnecessary operations on open and also by reducing the size of the list, which
104

fiOptimal Any-Angle Pathfinding In Practice

Algorithm 3 Cul-de-sac and intermediate node pruning.
1: function Is-cul-de-sac(n = (I, r))
. Assumes I does not contain the target point
2:
Imax  projection of n
. Flat projection or cone projection depending on n
3:
if Imax is valid then
. Valid means every p  Imax is visible from r
4:
return f alse
. n cannot be a cul-de-sac; it has at least one successor with interval I 0  Imax
5:
end if
6:
return true
. n is a cul-de-sac; it cannot be projected further and it has no successors
7: end function
8: function Is-Intermediate(n = (I, r))
. Assumes I does not contain the target point
9:
if n is a flat node then
10:
p  endpoint of I furthest from r
11:
if p is a turning point for a taut local path with prefix hr, pi then
12:
return f alse
. n has at least one non-observable successor; it cannot be intermediate
13:
end if
14:
else
. n is not a flat node so it must be a cone node
15:
if I has a closed endpoint that is also a corner point then
16:
return f alse
. n has at least one non-observable successor; it cannot be intermediate
17:
end if
18:
I 0  interval after projecting r through I
19:
if I 0 contains any corner points then
20:
return f alse
. n has more than one observable successors; it cannot be intermediate
21:
end if
22:
end if
23:
return true
24: end function

makes every other operation faster. For reference, when the open list is implemented as a
binary heap each add or remove operation has a time complexity of log n, where n is the
size of the list. Examples of cds pruning, for cone nodes and for flat nodes, are illustrated in
Figure 11 and Figure 12. In both cases the current node and root are shown in blue while
the intervals in red can be pruned.
7.2 Intermediate Pruning
Our second pruning strategy can be described as pushing the expansion in one direction
as far as possible as long as it does not increase the branching factor. Practically, if a search
node is generated that is guaranteed to have only one successor, then we immediately
generate this successor instead of the originally intended node. If said successor also has
only one successor the process can be recursively applied. Examples showing the application
of intermediate pruning are given in Figure 13 for cone nodes and on Figure 14 for flat nodes.
A simple test to identify intermediate nodes is given in Algorithm 3 by way of the procedure
Is-Intermediate.
The first obvious benefit of intermediate pruning is a reduction in the number of operations on the open list. However a second benefit is that pushing the expansion of a node
can lead to a cul-de-sac. When this happens no node at all is added to the open list, which
helps keep the size of this list small and the operations on this list fast.
A potential issue with Intermediate Pruning is that that recursive application to nonpromising successor nodes could be more costly (in terms of time) than simply adding those
105

fiHarabor, Grastien, Oz & Aksakalli

4

4

3

3

2

2

c
a

1

d

e f
b

r

a

b

c

1
r

0

0
0

1

2

3

4

5

6

0

Figure 11: Cul-de-sacs in cone nodes:
nodes ([c, d), r) and ((e, f ], r) are not generated.

2
1

2

3

4

5

6

Figure 12: Cul-de-sac on flat nodes: node
((b, c], r) is not generated.

4
3

1

4
r

3

a

b

c

d

2

r

a

b

c

1

0

0

0
1
2
3
4
5
6
Figure 13: Intermediate node ([a, b], r) has
only one successor, ([c, d], r), which is immediately generated.

0
1
2
3
4
5
6
Figure 14: Intermediate node ([a, b], r) has
only one successor, ((b, c], r) which is immediately generated.

nodes to open. We discuss this issue in more detail in Section 7.3. We also note that in our
run-time experiments the application of Intermediate Pruning has a net positive effect on
the performance of search.
7.3 Discussion
We have introduced two different ways in which nodes from the frontier of search can be
pruned: Cul-de-sac Pruning and Intermediate Pruning. Both modify the expansion order
of search and both improve performance along a single fixed path. They do this by pruning
away sterile branches and by skipping over intermediate locations where no actual branching
occurs. Similar strategies have been previously discussed in the literature. For example Culde-sac Pruning is based on the same set of principles as the Dead-end Heuristic (Bjornsson &
Halldorsson, 2006); although our method reasons more locally and is applied purely online.
Intermediate Pruning shares some similarities with Fast Expansion (Sun, Yeoh, Chen, &
106

fiOptimal Any-Angle Pathfinding In Practice

Koenig, 2009); the main difference is that we prune nodes without reference to their f -value.
Intermediate Pruning is also similar to Jump Point Search (Harabor & Grastien, 2014) but
applied outside the context of symmetry breaking and extended to sets of points taken as
intervals rather than applied over the individual cells of the grid.
Anyas root history list, discussed in Section 6, can also be regarded as a type of pruning
enhancement. In this case we reason more generally about the set of all possible paths that
could be used to reach a given point and prune away only those successors that cannot
possibly be on any optimal path. The approach we have taken here is similar in principle
(but not in practice) to the pruning of redundant states in real-time search (Sturtevant &
Bulitko, 2011).
Pruning search nodes in Anya is more difficult than in classical A* search and its
many modern progenitors. This is because each Anya node represents a set of positions
rather than just one. Consider the example of Figure 15; we are particularly interested
in the interval [a, b] which can be generated with root r1 or r2 . The shortest path from
s to t is through r1 ( 14.24 against  14.99 for r2 ). However if an obstacle is put on
the cell labeled with O, then the optimal path switches to r2 ( 15.62 against  15.94).
The diagram suggests that, when given the target and two search nodes sharing the same
interval, it may not be possible to prune either of them.
The situation described in Figure 15 is not uncommon in practice and such examples
may motivate us to derive new and more sophisticated pruning rules to further enhance the
performance of the Anya algorithm. We must be careful however to weigh the improved
pruning power of such new techniques against the overhead of applying them in the first
instance. For example, an alternative (arguably, better) approach to avoiding redundant
node expansions is to keep an interval history list in addition to (or instead of) a root history.
Such a method would certainly avoid the problem outlined in Figure 15 but there are many
more possible intervals than roots, which means the size of the hash table is potentially
much larger and memory accesses are potentially slower. Additionally, comparing intervals
for equality and membership requires extra time and may not be worth the investment6 .

6. We attempted a similar experiment but the results were not clearly positive.

107

fiHarabor, Grastien, Oz & Aksakalli

t

11
10

O
9
8
7
6
a

5

b

r1

4

r2

3
2

s

1
0
0

1

2

3

4

5

6

7

8

9

10

11

12

13

Figure 15: Illustrating that search nodes cannot be trivially pruned with search nodes
n1 = ([a, b], r1 ) and n2 = ([a, b], r2 ): if O is not an obstacle the optimal path between s and
t goes through n1 (red); otherwise it goes through n2 (blue).

8. Experimental Setup
We conduct experiments on seven benchmark problem sets taken from Nathan Sturtevants
well known repository (Sturtevant, 2012). Three of the benchmarks originate from popular
computer games and often appear in the literature. They are: Baldurs Gate II , Dragon
Age Origins and StarCraft. The maps in these benchmarks vary in size; from several
thousand nodes up to several million. The remaining four benchmarks comprise grids of
size 512  512 with randomly placed obstacles of varying densities, from 10% to 40%.
Table 1 gives an overview of the benchmark problems. We give the number of maps and
instances per problem set and a distribution for the number of node expansions required
by a reference algorithm, A* using an octile distance heuristic7 , to solve all problems in
each benchmark set. The latter metric gives us a baseline for comparing the difficulty of
problems appearing in each benchmark set.
7. Octile distance is analogous to Manhattan distance but generalised to 8-connected grids.

108

fiOptimal Any-Angle Pathfinding In Practice

Benchmark

#Maps

#Instances

Baldurs Gate II
Dragon Age
StarCraft
Random 10%
Random 20%
Random 30%
Random 40%

75
156
75
10
10
10
10

93160
159465
198230
16770
17740
19200
35360

Nodes Expanded by A*
Min
Q1 Median Mean
Q3
Max StDev
2
166
2019 6302 9170
86720
9136
1
622
5880 14080 19150 126800 19744
3 4808
26840 50000 70110 578900 63507
2
239
548
1886 1485
59280
3921
3
749
3869 8606 14680
53760
9905
4 3520
14190 20290 33710
96090 19162
3 12520
42850 51920 83770 169900 43558

Table 1: An overview of the seven benchmark problems used in our experiments. We give the
number of maps and problem instances in each benchmark and the distribution of nodes expanded
by a reference algorithm (A*) when solving all problems in each benchmark set.

We compare our purely online and optimal Anya algorithm with a number of state-ofthe-art any-angle techniques. These are: Theta* (Nash et al., 2007), Lazy Theta* (Nash,
Koenig, & Tovey, 2010), Field A* (Uras & Koenig, 2015a) and an any-angle variant of
two-level Subgoal Graphs (SUB-TL) (Uras & Koenig, 2015b). All of these approaches are
near-optimal and are not guaranteed to return the shortest path. The methods Theta*, Lazy
Theta* and Field A* are all purely online. Only SUB-TL relies on an offline pre-processing
step to further improve the performance of search. We use C++ implementations for each of
these algorithms; the source codes are made publicly available by Uras and Koenig (2015a).
Anya is implemented in Java and executed on JVM 1.8. To allow for comparisons across
different implementation languages we use the A* algorithm (Hart, Nilsson, & Raphael,
1968), implemented in both C++ and Java, as a reference point8 . We compare the performance of Anya against the Java implementation of A* and all other algorithms with
the C++ implementation of A*. All experiments are performed on a 3GHz Intel Core i7
machine with 8GB of RAM and running OSX 10.8.4. Source code for our implementation
of Anya is available from https://bitbucket.org/dharabor/pathfinding.

9. Results
We evaluate performance using three different metrics: search time, nodes expanded and
path length. All results are presented relative to a benchmark algorithm, A*, which we
combine with a standard octile distance heuristic. For example, when comparing search
time or nodes expanded, we will give figures for the relative speedup of each algorithm vs
A*. Under this paradigm a search time speedup of 2 means twice as fast while a node
expansion speedup of 2 means half as many nodes were expanded. When comparing path
length we give the percent improvement in path length vs A*. In all cases higher is better.

8. The C++ implementation is due to Uras and Koenig (2015a); the Java implementation is our own.

109

fiHarabor, Grastien, Oz & Aksakalli

Benchmark
Baldurs Gate II
Dragon Age
StarCraft 40%
Random 10%
Random 20%
Random 30%
Random 40%

Avg. Node Expansion Speedup
Anya Theta* L.Theta* F.A* SUB-TL
91.13
1.95
1.96 1.01 907.10
19.60
1.05
1.05 0.90
57.45
40.73
1.27
1.27 0.95 166.00
0.80
2.34
2.38 1.14
6.60
0.77
1.23
1.17 0.80
2.56
1.06
0.82
0.75 0.64
1.68
2.20
0.90
0.86 0.82
2.40

Avg. Path Length Improvement (%)
Anya Theta* L.Theta* F.A* SUB-TL
4.65% 4.62%
4.61% 4.38%
4.58%
4.34% 4.27%
4.22% 4.05%
4.28%
5.02% 4.95%
4.92% 4.70%
4.88%
4.77% 4.63%
4.58% 3.83%
4.59%
4.57% 4.34%
4.15% 3.26%
4.30%
4.44% 4.12%
3.77% 3.12%
4.03%
4.14% 3.95%
3.48% 3.22%
3.74%

Table 2: We compare the performance of each algorithm in terms of average node expansion speedup
and average path length improvement. Both metrics are taken with respect to a reference algorithm
(A*). In both cases higher is better.
We begin with Table 2 which shows average performance figures for nodes expanded and
path length on each of our seven benchmark problem sets. We make the following observations:
 Anya is the best of the four purely-online algorithms, expanding fewer nodes in
five of the seven benchmarks. On the three benchmarks drawn from real computer
games Anya expands one order fewer nodes, on average, than its nearest purelyonline contemporary. Only the pre-processing-based SUB-TL algorithm expands fewer
nodes, on average.
 Anya, as with all methods in our comparison, struggles to achieve a speedup on the
four random benchmarks. In two of the four cases its performance is below that of
the reference A* algorithm. Again, only the pre-processing-based SUB-TL algorithm
is able to achieve a consistent, though much reduced, node expansion speedup.
 Anya, being optimal, shows the best improvement in path length; however all algorithms in our comparison are very close to optimal, on average.
Next, we evaluate performance in terms of search time. Rather than taking a simple
average on a per benchmark basis (or across all benchmarks) we instead sort instances
according to difficulty, as measured by the number of node expansions required for the
reference A* algorithm to solve each problem. This approach gives a more holistic overview
of performance and reduces the effect of any bias associated with the selection of instances
that comprise each benchmark set9 . Results from this analysis are given in Figure 16. We
make the following observations:
 Anya is often more than one order of magnitude faster than the reference A* algorithm on the benchmarks drawn from real computer games. Performance is mixed
on the four random benchmarks, with all evaluated methods struggling to achieve a
speedup.
9. As per Table 1, problem instances that can be regarded as easy often outnumber instances that can
be regarded as hard. These difference have the effect of skewing performance indicators that are
computed as simple averages over all instances in each benchmark set.

110

fiOptimal Any-Angle Pathfinding In Practice

All Benchmarks

Baldur's Gate II
Anya
Theta*
Lazy Theta*
Field A*
SUBTL

Speedup vs A*

100

10

1000

Anya
Theta*
Lazy Theta*
Field A*
SUBTL

100

Speedup vs A*

1000

10

1

1

0.1

0.1
102

103

104
Nodes Expanded by A*

105

106

102

103

Dragon Age Origins

104
Nodes Expanded by A*

105

106

StarCraft
Anya
Theta*
Lazy Theta*
Field A*
SUBTL

Speedup vs A*

100

10

1000

Anya
Theta*
Lazy Theta*
Field A*
SUBTL

100

Speedup vs A*

1000

10

1

1

0.1

0.1
102

103

104
Nodes Expanded by A*

105

106

102

Random; 512x512 10% obstacles

Speedup vs A*

105

106

Random; 512x512 20% obstacles
Anya
Theta*
Lazy Theta*
Field A*
SUBTL

100

104
Nodes Expanded by A*

10

1000

Anya
Theta*
Lazy Theta*
Field A*
SUBTL

100

Speedup vs A*

1000

103

10

1

1

0.1

0.1
102

103

104
Nodes Expanded by A*

105

106

102

Random; 512x512 30% obstacles

Speedup vs A*

105

106

Random; 512x512 40% obstacles
Anya
Theta*
Lazy Theta*
Field A*
SUBTL

100

104
Nodes Expanded by A*

10

1000

Anya
Theta*
Lazy Theta*
Field A*
SUBTL

100

Speedup vs A*

1000

103

10

1

1

0.1

0.1
102

103

104
Nodes Expanded by A*

105

106

102

103

104
Nodes Expanded by A*

105

106

Figure 16: Search time speedup. We compare performance on each of our seven benchmarks
in terms of search time. Figures are given as relative speedup vs. a reference A* algorithm.
Problem instances are sorted by difficulty using A* node expansion rank. Note that each
plot is log-log.

111

fiHarabor, Grastien, Oz & Aksakalli

 Anya is the fastest of the four purely online methods under evaluation. Its performance is often comparable with the pre-processing based SUB-TL technique and, on
particularly challenging instances from the StarCraft domain, Anya is non-dominated10 .
 Anyas performance in terms of search time is less than the value suggested by the
(previously evaluated) node expansion metric. This reflects the fact that each node
expansion made by Anya involves analysing the grid; looking for roots and searching
for intervals.
9.1 Discussion
We have seen that Anya compares well with current state-of-the-art any-angle pathfinding
algorithms. In an (almost) apples-to-apples comparison with three contemporary and purely
online search technique (Theta*, Lazy Theta* and Field A*) we have seen that Anya usually
expands fewer nodes per search and terminates up to one order of magnitude faster. These
results are further underscored by the fact that Anya is the only online algorithm that is
guaranteed to return a Euclidean-optimal path. We may surmise that, in many cases and
applications, Anya appears preferable to each of these alternative algorithms.
Next, we make an apples-to-oranges comparison between the purely online Anya algorithm and the near-optimal and offline enhanced SUB-TL algorithm. We have seen that
while Anya is usually not as fast as SUB-TL its performance is sometimes comparable.
Moreover, Anya retains an advantage when solving especially challenging instances drawn
from real computer games. SUB-TL appears to be preferable to Anya in cases where additional space and time is available to create and store its associated subgoal graph or in cases
where such overheads can be amortised over many online instances. When extra space and
time is not available, or in cases where the map is subject to change (e.g. new obstacles are
added or existing obstacles are removed), Anya appears to be preferable to SUB-TL.
The main strength of Anya is that it searches over sets of nodes from the grid rather
than considering individual locations one at a time. Expansion can thus be considered as
a macro operator, meaning that Anya bears some similarity to speedup techniques using
hierarchical abstraction; e.g. HPA* (Botea et al., 2004). An important difference is that
Anya constructs its abstract graph on-the-fly rather than as part of a pre-processing step.
One current drawback associated with Anya is that nodes can contain overlapping
intervals. This occurs when an interval is reachable from two different root points, neither
of which can be pruned (e.g. when both root locations are reached for the first time; as
illustrated in Figure 15). Such nodes are, either in part or in whole, redundant and 
provided their f -value is smaller than the optimal distance to the goal  will themselves
beget yet more redundant successors. We can see this behaviour especially in the results for
benchmarks Random 10% and Random 20% where SUB-TL achieves a speedup of several
factors while Anya struggles to maintain parity with the reference A* algorithm. It seems
reasonable to improve the current algorithm by attempting to identify such overlaps in order
to prune them from consideration. An efficient and effective algorithm for achieving this
goal is the subject of further work.
10. In the Pareto sense; i.e. there are problem instances where Anya is better than SUB-TL according to
some metric of interest such as node expansions or search time

112

fiOptimal Any-Angle Pathfinding In Practice

10. Related Work
Among the simplest and most popular approaches for solving the any-angle pathfinding
problem is string-pulling. The main idea is to find a path on the input grid map, often
using some variant of A* (Hart et al., 1968), and then post-process that path in order to
remove unnecessary turning points. Several such methods have appeared in the literature
of Game Development; e.g. see the work of Pinter (2001) and Botea et al. (2004).
A number of algorithms improve on string-pulling by interleaving node expansion and
path post-processing during online search. Particular examples include Field D* (Ferguson
& Stentz, 2005) and Field A* (Uras & Koenig, 2015a), both of which use linear interpolation
to smooth grid paths one cell at a time, and Theta* (Nash et al., 2007), which introduces a
shortcut each time a successful line-of-sight check is made; from the parent of the current
node to any of its successors. Though still sub-optimal in many cases such approaches
are nevertheless attractive for being able to search purely online and for being efficient in
practice. In addition to the two examples given there are numerous other works, often
appearing in the literature of Artificial Intelligence, that apply and improve on the basic
interleaving idea. We refer the interested reader to Nash & Koenig, 2013 for a recent survey
and overview.
Accelerated A* (Sislak, Volf, & Pechoucek, 2009) is an online any-angle algorithm that is
conjectured to be optimal but for which no strong theoretical argument is made. Similar to
Theta*, it differs primarily in that line-of-sight checks are performed from a set of expanded
nodes rather than a single ancestor. The size of the set is only loosely bounded and, for
challenging problems, can include a large proportion of nodes on the closed list.
One recent and successful line of research involves the combination of string-pulling with
an offline pre-processing step. Such works are compelling because they can significantly
improve on the performance of purely online search; not just in terms of solution quality
but also running time. Block A* (Yap, Burch, Holte, & Schaeffer, 2011) is one such example.
This sub-optimal algorithm pre-computes a database of Euclidean-optimal distances in all
possible tile configurations of a certain size (e.g. all possible 3x3 blocks). The database
obviates the need for explicit visibility checks or indeed any type of online string-pulling.
The pre-processing step needs to be performed exactly once; the database remains valid if
the tiles on the map change or indeed if the map itself changes entirely. Another recent
work improves on Theta* by combining that algorithm with a pre-processing based graph
abstraction technique (Uras & Koenig, 2015b). This approach, referred to in Section 9 as
SUB-TL, is shown to improve on both the running time and solution quality of Block A*.
The main disadvantage (vs. Block A*) is that the abstract graph needs to be re-computed
or repaired each time the map changes.
The Euclidean Shortest Path Problem is a well known and well researched topic in the
areas of Computational Geometry and Computer Graphics. It can be seen as a generalisation of the Any-angle Pathfinding Problem. It asks for a shortest path in a plane but
does not impose any restrictions on obstacle shape or obstacle placement (cf. grid aligned
polygons made up of unit squares).
Visibility graphs (Lozano-Perez & Wesley, 1979) are a family of well-known and popular
techniques for optimally solving the Euclidean Shortest Path Problem. Searching in such
graphs requires O(n2 log2 n) time but the approach can be much faster in practice. There
113

fiHarabor, Grastien, Oz & Aksakalli

are two main disadvantages: (i) computing the graph requires an offline pre-processing
step and O(n2 ) space to store; (ii) the graph is static and must be recomputed or repaired
if the environment changes. More sophisticated variants such as Tangent Graphs (Liu &
Arimoto, 1992) and Silhouette Points (Young, 2001) are particularly efficient variants of
visibility graphs but the same disadvantages apply.
Another family of exact approaches for solving the Euclidean Shortest Path Problem
is based on the Continuous Dijkstra paradigm (Mitchell et al., 1987). The most efficient of these algorithms (Hershberger & Suri, 1999) involves a pre-computation requiring O(n log2 n) space and O(n log2 n) time. The result is a Shortest Path Map; a planar
subdivision of the environment that can be used to find a Euclidean shortest path in just
O(log2 n) time; but only for queries originating at a fixed source. Like visibility graphs,
this approach also introduces additional memory overheads (storing the subdivision) and
the pre-processing step must be re-executed each time the environment or the start location
changes.

11. Conclusion
We study any-angle pathfinding: a problem commonly found in the areas of robotics and
computer games. The problem involves finding a shortest path between two points in a grid
but asks that the path is not artificially constrained to the fixed points of the grid. The
best known online algorithms for the any-angle problem, to date, all compute approximate
solutions rather than optimal shortest paths. Additionally no online methods have been
able to achieve a consistent speedup vs. the A* algorithm  a common reference point
for measuring performance in the literature. In this work we present a new online, optimal and practically efficient any-angle technique: Anya. Where other works obtain good
performance by reasoning at the grid level our method considers sets of points from the
grid which are taken together as contiguous intervals. This approach requires revisiting the
classical definition of search nodes and successors and requires the introduction of a new
technique for computing the f -value of each node. We give a thorough algorithmic description of this new search paradigm and we give theoretical arguments for its completeness
and optimality preserving characteristics.
In an (almost) apples-to-apples comparison we evaluate Anya against three contemporary near-optimal and online techniques: Theta*, Lazy Theta* and Field A*. We show
that, on a range of popular benchmarks, Anya is faster than each of these alternatives, all
while guaranteeing to find an optimal shortest path. In an apples-to-oranges comparison we
evaluate Anya against SUB-TL: a very fast pre-processing-based near-optimal any-angle
technique. We show that Anya is non-dominated when compared to SUB-TL and even
maintains an advantage on some particularly challenging instances drawn from real computer games. Another advantage is that, unlike SUB-TL, Anya does not assume the map is
static; i.e. it can be readily applied to pathfinding problems involving dynamically changing
terrain.
Any-angle pathfinding has received significant attention from the AI and Game Development communities but until now it has been an open question whether any optimal and
online algorithm exists. Anya answers this question in the affirmative.
114

fiOptimal Any-Angle Pathfinding In Practice

11.1 Future Work
There are several possible directions for future work. Perhaps most obvious is the development of improvements and extensions to the current Anya algorithm. For example, we
believe the empirical performance of Anya could be enhanced by generating successors
nodes that do not contain any redundant (or partially redundant) intervals. One possibility
is to keep a closed list of previously encountered intervals. A stronger variant of this idea
involves bounding the g-value of grid intervals and only generating successor nodes when at
least one point inside a candidate interval can be relaxed. A related and orthogonal improvement involves pre-processing the grid and identifying intervals apriori. This enhancement
can speed up search by avoiding entirely all grid scanning and interval projection operations
that are currently necessary in order to generate each node.
We have seen that reasoning over sets of points from the grid, rather than individual
locations, is computationally beneficial. We believe the same type of search paradigm
employed by Anya can be generalised to improve the performance of grid-optimal search
in addition to any-angle pathfinding.
As a final suggestion for further work, we believe Anya might also be generalised to
two-dimensional maps with arbitrarily shaped polygonal obstacles, rather than just grids.
A benefit of this generalisation would be to avoid the discretisation of the world in which
a path is searched for. This would even improve the quality of the path returned as the
optimal any-angle path is often non optimal in the non-discretised version of the map.

Acknowledgements
We thank Tansel Uras for assistance with the source codes used in the experimental section
of this paper. We also thank Adi Botea and Patrik Haslum for helpful suggestions during
the early development of this work.
The work of Daniel Harabor and Alban Grastien is supported by NICTA. NICTA is
funded by the Australian Government as represented by the Department of Broadband,
Communications and the Digital Economy and the Australian Research Council through
the ICT Centre of Excellence program.
The work of Dindar Oz and Vural Aksakalli is supported by The Scientific and Technological Research Council of Turkey (TUBITAK), Grant No. 113M489.

115

fiHarabor, Grastien, Oz & Aksakalli

Appendix A.
We provide additional details for the implementation of Anyas successor set generation
algorithm. Our method depends on basic operations which are technically simple: grid
scanning, traversability tests and linear projection operations. We do not attempt to reproduce the mechanical details of such operations. Instead we focus our presentation toward
intuitive understanding of the overall process.
Algorithm 4 Computing the successor set, supplemental.
1: function generate-start-successors(a traversable and discrete start location s)
1
2:
Construct a maximal half-closed interval Imax
containing all points observable and to the left of s
2
3:
Construct a maximal half-closed interval Imax
containing all points observable and to the right of s
3
4:
Construct a maximal closed interval Imax
containing all points observable and from the row above s
4
5:
Construct a maximal closed interval Imax
containing all points observable and from the row below s
k
6:
intervals  Split each Imax
at each corner point and take their union
7:
Construct for each I  intervals a new (cone or flat) successor node with r = s
8:
return all start successors
9: end function
10: function generate-flat-successors(an interval endpoint p, a root point r)
11:
p0  first corner point (else farthest obstacle vertex) on the row of p such that hr, p, p0 i is taut
12:
Imax  new maximal interval with endpoints p (open) and p0 (closed)
13:
if points r and p are on the same row then
. Observable successors
14:
successors  new flat node n = (Imax , r)
15:
else
16:
successors  new flat node n = (Imax , p)
. Non-observable flat successors
17:
end if
18:
return successors
19: end function
20: function generate-cone-successors(an interval endpoint a, an interval endpoint b, a root point r)
21:
if a and b and r are from the same row then
. Non-observable successors of a flat node
22:
r0  a or b, whichever is farthest from r
. Previously established this is a turning point
23:
p  a point from an adjacent row, reached via a right-angle turn at a
. Obstacle following
24:
Imax  a maximum closed interval, beginning at p and entirely observable from r0
25:
else if a == b then
. Non-observable successors of a cone node
26:
r0  a
27:
p  a point from an adjacent row, computed via linear projection from r through a
28:
Imax  a maximum closed interval, beginning at p and entirely observable from r0
29:
else
. Observable successors of a cone node
30:
r0  r
31:
p  a point from an adjacent row, computed via linear projection from r through a
32:
p0  a point from an adjacent row, computed via linear projection from r through b
33:
Imax  a maximum closed interval, with endpoints a and b, which is entirely observable from r0
34:
end if
35:
for all I  { split Imax at each corner point } do
36:
n0  new search node with interval I and root point r0
37:
successors  successors  I
38:
end for
39:
return successors
40: end function

116

fiOptimal Any-Angle Pathfinding In Practice

References
Bjornsson, Y., & Halldorsson, K. (2006). Improved Heuristics for Optimal Path-finding
on Game Maps. In Proceedings of the Second Artificial Intelligence and Interactive
Digital Entertainment Conference, June 20-23, 2006, Marina del Rey, California, pp.
914.
Botea, A., Muller, M., & Schaeffer, J. (2004). Near Optimal Hierarchical Path-Finding.
Journal of Game Development, 1 (1), 728.
Ferguson, D., & Stentz, A. (2005). Field D*: An Interpolation-based Path Planner and
Replanner. In Robotics Research: Results of the 12th International Symposium, ISRR
2005, October 12-15, 2005, San Francisco, CA, USA, pp. 239253.
Graham, R. L., Knuth, D. E., & Patashnik, O. (1989). Concrete Mathematics - A Foundation for Computer Science. Addison-Wesley.
Harabor, D. D., & Grastien, A. (2013). An Optimal Any-Angle Pathfinding Algorithm.
In Proceedings of the Twenty-Third International Conference on Automated Planning
and Scheduling, ICAPS 2013, Rome, Italy, June 10-14, 2013.
Harabor, D. D., & Grastien, A. (2014). Improving Jump Point Search. In Proceedings of
the Twenty-Fourth International Conference on Automated Planning and Scheduling,
ICAPS 2014, Portsmouth, New Hampshire, USA, June 21-26, 2014.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). A Formal Basis for the Heuristic Determination of Minimum Cost Paths. IEEE Transactions on Systems Science and
Cybernetics, 4 (2), 100107.
Hershberger, J., & Suri, S. (1999). An Optimal Algorithm for Euclidean Shortest Paths in
the Plane. SIAM Journal on Computing, 28 (6), 22152256.
Liu, Y.-H., & Arimoto, S. (1992). Path Planning Using A Tangent Graph For Mobile
Robots Among Polygonal And Curved Obstacles. International Journal of Robotics
Research, 11, 376382.
Lozano-Perez, T., & Wesley, M. A. (1979). An Algorithm for Planning Collision-Free Paths
Among Polyhedral Obstacles. Communications of the ACM, 22 (10), 560570.
Mitchell, J. S. B., Mount, D. M., & Papadimitriou, C. H. (1987). The Discrete Geodesic
Problem. SIAM Journal on Computing, 16 (4), 647668.
Nash, A., Daniel, K., Koenig, S., & Felner, A. (2007). Theta*: Any-Angle Path Planning on Grids. In Proceedings of the Twenty-Second AAAI Conference on Artificial
Intelligence, July 22-26, 2007, Vancouver, British Columbia, Canada, pp. 11771183.
Nash, A., & Koenig, S. (2013). Any-Angle Path Planning. AI Magazine, 34 (4), 9.
Nash, A., Koenig, S., & Tovey, C. A. (2010). Lazy Theta*: Any-angle Path Planning and
Path Length Analysis in 3D. In Proceedings of the Twenty-Fourth AAAI Conference
on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010.
Pinter, M. (2001). Toward More Realistic Pathfinding. Game Developer Magazine, 8 (4).
117

fiHarabor, Grastien, Oz & Aksakalli

Sislak, D., Volf, P., & Pechoucek, M. (2009). Accelerated A* Trajectory Planning: Gridbased Path Planning Comparison. In 4th ICAPS Workshop on Planning and Plan
Execution for Real-World Systems.
Sturtevant, N. (2012). Benchmarks for Grid-Based Pathfinding. Transactions on Computational Intelligence and AI in Games, 4 (2), 144  148.
Sturtevant, N. R., & Bulitko, V. (2011). Learning Where You Are Going and from Whence
You Came: h- and g-Cost Learning in Real-Time Heuristic Search. In 22nd International Joint Conference on Artificial Intelligence, IJCAI 2011, pp. 365370.
Sun, X., Yeoh, W., Chen, P.-A., & Koenig, S. (2009). Simple Optimization Techniques For
A*-based Search. In 8th International Joint Conference on Autonomous Agents and
Multiagent Systems, AAMAS 2009, Budapest, Hungary, May 10-15, 2009, Volume 2,
pp. 931936.
Uras, T., & Koenig, S. (2015a). An Empirical Comparison of Any-Angle Path-Planning
Algorithms. In Proceedings of the Eighth Annual Symposium on Combinatorial Search,
SOCS 2015, 11-13 June 2015, Ein Gedi, the Dead Sea, Israel, pp. 206211.
Uras, T., & Koenig, S. (2015b). Speeding-Up Any-Angle Path-Planning on Grids. In
Proceedings of the Twenty-Fifth International Conference on Automated Planning and
Scheduling, ICAPS 2015, Jerusalem, Israel, June 7-11, 2015, pp. 234238.
Yap, P., Burch, N., Holte, R. C., & Schaeffer, J. (2011). Block A*: Database-Driven Search
with Applications in Any-Angle Path-Planning. In Proceedings of the Twenty-Fifth
AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco, California,
USA, August 7-11, 2011.
Young, T. (2001). Optimizing Points-of-Visibility Pathfinding. In Game Programming
Gems 2, pp. 324329. Charles River Media.

118

fiJournal of Artificial Intelligence Research 56 (2016) 197-245

Submitted 07/15; published 06/16

Two Aspects of Relevance in Structured Argumentation:
Minimality and Paraconsistency
Diana Grooters

dianagrooters@gmail.com

ORTEC Finance Rotterdam, The Netherlands

Henry Prakken

H.Prakken@uu.nl

Department of Information and Computing Sciences, Utrecht University
Faculty of Law, University of Groningen
The Netherlands

Abstract
This paper studies two issues concerning relevance in structured argumentation in the
context of the ASPIC + framework, arising from the combined use of strict and defeasible
inference rules. One issue arises if the strict inference rules correspond to classical logic. A
longstanding problem is how the trivialising effect of the classical Ex Falso principle can be
avoided while satisfying consistency and closure postulates. In this paper, this problem is
solved by disallowing chaining of strict rules, resulting in a variant of the ASPIC + framework called ASPIC ? , and then disallowing the application of strict rules to inconsistent sets
of formulas. Thus in effect Rescher & Manors paraconsistent notion of weak consequence
is embedded in ASPIC ? .
Another issue is minimality of arguments. If arguments can apply defeasible inference
rules, then they cannot be required to have subset-minimal premises, since defeasible rules
based on more information may well make an argument stronger. In this paper instead
minimality is required of applications of strict rules throughout an argument. It is shown
that under some plausible assumptions this does not affect the set of conclusions. In addition, circular arguments are in the new ASPIC ? framework excluded in a way that satisfies
closure and consistency postulates and that generates finitary argumentation frameworks if
the knowledge base and set of defeasible rules are finite. For the latter result the exclusion
of chaining of strict rules is essential.
Finally, the combined results of this paper are shown to be a proper extension of
classical-logic argumentation with preferences and defeasible rules.

1. Introduction
One tradition in the logical study of argumentation is to allow for arguments that combine
strict and defeasible inference rules. This approach was introduced in AI by Pollock (1987,
1990, 1992, 1994, 1995), was studied in the past also by e.g. Lin and Shoham (1989), Simari
and Loui (1992), Vreeswijk (1997), Prakken and Sartor (1997) and Garcia and Simari (2004)
and is currently studied by e.g. Dung and Thang (2014), Dung (2014, 2016) and in work on
the ASPIC + framework (Prakken, 2010; Modgil & Prakken, 2013, 2014). Strict inference
rules are intended to capture deductively valid inferences, where the truth of the premises
guarantee the truth of the conclusion. Defeasible inference rules are meant to capture presumptive inferences, where the premises create a presumption in favour of the conclusion,
which can be refuted by evidence to the contrary. Much research in this tradition has shown
that the idea of defeasible inference rules makes sense. For example, Pollock applied it to
c
2016
AI Access Foundation. All rights reserved.

fiGrooters & Prakken

formalize his theory of defeasible epistemic reasons, which includes reasons concerning perception, memory, enumerative induction, the statistical syllogism and temporal persistence.
Moreover, several publications on ASPIC + use defeasible inference rules to formalise Walton
(1996)-style presumptive argumentation schemes (Prakken, 2010; Modgil & Prakken, 2014)
and to apply these to legal reasoning (Prakken, Wyner, Bench-Capon, & Atkinson, 2015) or
policy debate (Bench-Capon, Prakken, & Visser, 2011). Finally, Garcia and Simaris (2004)
Defeasible Logic Programming approach has been applied in many domains.
In this tradition, two issues arise concerning relevance, namely, minimality of arguments and paraconsistency in strict-rule application. We will study both issues in the
context of the ASPIC + framework. The choice of ASPIC + for these purposes is justified by its framework nature, which allows the study of various classes of instantiations.
Moreover, it has been shown that various other approaches can be reconstructed as instantiations of the ASPIC + framework. Prakken (2010) showed this for assumption-based
argumentation as reconstructed by Dung, Mancarella, and Toni (2007) as an instance of
abstract argumentation (Dung, 1995), and this result carries over to the original formulation of assumption-based argumentation (Bondarenko, Dung, Kowalski, & Toni, 1997) for
all known semantics except semi-stable and eager semantics (cf. Caminada, Sa, Alcantara,
& Dvorak, 2015). Furthermore, Modgil and Prakken (2013) reconstructed two forms of
classical argumentation with premise attack as studied by Gorogiannis and Hunter (2011)
and several uses of Tarskian abstract logics as studied by Amgoud and Besnard (2013) as
instances of ASPIC + . For these reasons, results in terms of ASPIC + are representative for
large classes of argumentation systems.
ASPIC + is sometimes criticised on the fact that it allows for instantiations with bad
properties but such criticism is besides the point, since it ignores the framework nature
of ASPIC + (Prakken & Modgil, 2012). Being a framework instead of a concrete system,
ASPIC + is intended to allow the study of properties of various instantiations, such as
whether they satisfy the rationality postulates of Caminada and Amgoud (2007). It is the
very idea of the framework to allow for bad instantiations so that they can be identified.
Therefore, the framework cannot be criticised on the existence of such bad instantiations.
Moreover, there is a growing body of results on good instantiations of ASPIC + (Caminada
& Amgoud, 2007; Prakken, 2010; Modgil & Prakken, 2013; Dung, 2014, 2016; Caminada,
Modgil, & Oren, 2014; Grooters & Prakken, 2014; Wu & Podlaszewski, 2015) and this paper
aims at identifying another class of good instantiations.
One relevance issue discussed in this paper is minimality of arguments. In deductive
approaches to argumentation (e.g., Besnard & Hunter, 2008; Gorogiannis & Hunter, 2011;
Amgoud & Besnard, 2013) arguments are required to have a subset-minimal set of premises.
However, if arguments can apply defeasible inference rules, then this requirement is undesirable, since defeasible rules that are based on more information may well make an argument
stronger. For example, Observations done in ideal circumstances are usually correct is
stronger than Observations are usually correct. Note that this remark does not apply to
strict inference rules, so it still makes sense to improve efficiency by requiring that strict
inference rules are only applied to a subset-minimal set of formulas. So far, no system in
the defeasible-rule tradition enforces this requirement. One contribution of this paper is to
do so for the ASPIC + approach and to show that under some plausible conditions on its
argument ordering, this does not affect the set of conclusions.
198

fiTwo Aspects of Relevance in Structured Argumentation

Another aspect of minimality is circularity. So far presentations of ASPIC + have not
prevented arguments from repeating conclusions of their subarguments. Yet in argumentation theory circular arguments are generally regarded as fallacious, so it makes sense to
exclude them. In this paper we will do so and prove that the results on the rationality postulates are not affected by it. Moreover, we will prove that excluding non-circular arguments
has some computational benefits.
Another relevance issue arises if the strict inference rules are chosen to correspond to
classical logic. Then a longstanding unsolved problem originally identified by Pollock (1994,
1995) is how the trivialising effect of the classical Ex Falso principle can be avoided when
two arguments that use defeasible rules have contradictory conclusions. The problem is
especially hard since any solution should arguably preserve satisfaction of the rationality
postulates of consistency and strict closure (Caminada & Amgoud, 2007).
In a nutshell, the problem is as follows. Suppose two arguments have contradictory
conclusions  and . If the strict inference rules include the Ex Falso principle that an
inconsistent set implies any formula, then these two arguments can be combined into an
argument for  for any formula . This combined argument can potentially defeat any
argument for  by applying the Ex Falso inference rule to their joint conclusions. So when
there are arguments for contradictory conclusions, any other argument is potentially under
threat, which is clearly undesirable, since the conflict about  is in general unrelated to .
Pollock (1994, 1995) thought that he had avoided such trivialising arguments by allowing for multiple labellings, but Caminada (2005) showed that Pollocks solution does
not fully avoid them. The problem is a genuine one, since there arguably is a real need for
argumentation systems that allow for combinations of strict and defeasible inferences and
that, moreover, allow for the full reasoning power of a deductive logic. Although for many
cases less expressiveness may suffice, a full theory of the logic of argumentation cannot
exclude the general case.
To solve the problem, two approaches are possible. One is to change the definitions of
the argumentation framework, while the other is to derive the strict inference rules from
a weaker logic than classical logic. The first approach is taken by Wu (2012) and Wu
and Podlaszewski (2015), who for the ASPIC + framework require that for each argument
the set of conclusions of all its subarguments are classically consistent. They show that
this solution works for a restricted version of ASPIC + without preferences, but they give
counterexamples to the consistency postulates for the case with preferences.
A second approach to solve the problem is to replace classical logic as the source for
strict rules with a weaker, monotonic paraconsistent logic, in order to invalidate the Ex
Falso principle as a valid strict inference rule. This paper explores this possibility. We first
show that two well-known paraconsistent logics, the system C of Da Costa (1974) and
the Logic of Paradox of Priest (1979, 1989), cannot be used for these purposes, since they
induce violation of the postulate of indirect consistency. We then show that using Rescher
and Manors (1970) paraconsistent consequence notion satisfies all closure and consistency
postulates and also avoids trivialisation. While thus initially taking the second approach,
we will have to combine it with the first approach (changing the definitions) since it will
turn out that chaining strict rules in arguments has to be disallowed. This change in turn
motivates a new interpretation of Caminada and Amgouds (2007) strict-closure postulate
199

fiGrooters & Prakken

and the introduction of a new rationality postulate of logical closure. This contribution of
our paper is based on and extends results of Grooters and Prakken (2014).
After making these contributions, we will argue that in combination they shed light on
the relation between the adapted version of ASPIC + and classical argumentation as studied
by Besnard and Hunter (2008) and Gorogiannis and Hunter (2011), in which arguments
are essentially classical proofs from consistent and subset-minimal subsets of a classical
knowledge base. For two of their versions of classical argumentation with premise attack
the adapted version of ASPIC + is shown to a be proper extension with defeasible rules and
preferences. This observation justifies a combined treatment of both issues (minimality of
arguments and paraconsistency) in the same paper.
Caminada, Carnielli, and Dunne (2012) formulated a new set of rationality postulates
in addition to those of Caminada and Amgoud (2007), to characterise cases under which
the trivialisation problem is avoided (called the postulates of non-interference and crashresistance). Wu (2012) and Wu and Podlaszewski (2015) prove for their adaptation of
ASPIC + with consistent arguments that these new postulates are satisfied for complete
semantics. However, we will not attempt to prove Caminada et al.s (2012) postulates, for
two reasons. First, we want to obtain results for other semantics as well and, second, we will
argue in Section 10 that Caminada et al.s postulates in fact capture a stronger intuitive
notion than the one we study in this paper.
The remainder of this article is organised as follows. First in Section 2 the ASPIC +
framework is summarised and in Section 3 the rationality postulates of Caminada and Amgoud (2007) are presented. Then in Section 4 the trivialisation problem is illustrated in
more detail, after which in Section 5 instantiations of ASPIC + with the paraconsistent logics LP and C are studied as an attempt to avoid trivialisation in the face of inconsistency.
It will be shown that these instantiations violate the rationality postulate on indirect consistency. Then in Section 6 Rescher and Manors (1970) paraconsistent consequence notion
is introduced as another attempt to avoid trivialisation. It turns out that its embedding
in ASPIC + requires an adaptation of the ASPIC + framework into a framework called ASPIC ? , which disallows chaining of strict rules, which in turn motivates new notions of strict
closure and indirect consistency. Then in Section 7 the first main contribution of this paper is proved: satisfaction of the closure and consistency postulates by the instantiation of
the ASPIC ? framework with Rescher and Manors consequence notion. In Section 8 the
second and third main contribution is presented: an equivalence result between versions of
ASPIC ? with and without minimality constraints on strict inferences, and proofs that show
that a version of ASPIC ? that excludes circular arguments is well-behaved. In Section 9
we present our fourth main result, namely, that ASPIC ? with minimal arguments properly
generalises two versions of classical argumentation. Finally, in Section 10 we discuss our
results and put them in the context of related work.

2. The ASPIC + Framework
In this section, the ASPIC + framework is reviewed. Since it makes use of Dungs (1995)
theory of abstract argumentation, that theory is first briefly summarised. An abstract
argumentation framework (AF ) is a pair (A, D), where A is a set of arguments and D 
A  A is a binary relation of defeat. An argument A defeats argument B if (A, B)  D. A
200

fiTwo Aspects of Relevance in Structured Argumentation

set S of arguments defeats an argument B if there is an argument A  S such that A defeats
B. A set S defeats a set S 0 if there is an argument A  S 0 such that S defeats A. A set of
arguments is said to be conflict-free if it does not attack itself; otherwise it is conflicting.
A set S  A defends an argument A  A iff for each BinA that defeats A there exists a
C  S that defeats B. A set is admissible if it is conflict-free and defends itself by attacking
each argument attacking S. Each argumentation framework has zero or more extensions,
which intuitively are maximal sets of arguments that can be accepted together since they
are conflict-free and defend all their members against attacks. Formally, extensions are
admissible sets with some additional properties. They can be defined according to Dungs
characteristic function.
Definition 2.1. [Dungs characteristic function F ] FAF : 2A  2A such that FAF (S) =
{A  A|A is defended by S}.
Henceforth the subscript AF will be omitted if there is no danger of confusion.
Definition 2.2. [Extensions of abstract argumentation frameworks] For any AF =
(A, D) and any E  A:
 E is conflict-free iff there are no A, B in E such that (A, B)  D.
 E is admissible iff E is conflict-free and E defends each A  E.
 E is a complete extension of AF iff E is conflict-free and FAF (E) = E.
 E is a preferred extension of AF iff E is a set-inclusion-maximal complete extension
of AF .
 E is a stable extension of AF iff E is conflict-free and for all A 6 E there exists a
B  E such that B defeats A.
 E is the grounded extension of AF iff E is the set-inclusion-minimal complete extension of AF .
Finally, for T  {complete, preferred, grounded, stable}, X is sceptically or credulously
T -justified if X belongs to all, respectively at least one, T extension. Other notions of
extensions have been proposed in the literature but in this paper we confine ourselves to
these four notions.
The ASPIC + framework (Prakken, 2010; Modgil & Prakken, 2013) gives structure to
Dungs arguments and defeat relation. As in the work of Vreeswijk (1997) it defines arguments as directed acyclic inference graphs formed by applying strict or defeasible inference
rules to premises formulated in some logical language. Intuitively, strict rules guarantee the
truth of their consequent if their antecedents are true, while defeasible rules only create a
presumption in favour of the truth of their consequent if their antecedents are true. Arguments can be attacked on their (ordinary) premises and on their applications of defeasible
inference rules. Some attacks succeed as defeats, which is partly determined by preferences.
The acceptability status of arguments is then defined by applying any of Dungs (1995)
semantics for abstract argumentation frameworks to the resulting set of arguments with its
defeat relation.
201

fiGrooters & Prakken

Below the special case with symmetric negation of the version of ASPIC + defined by
Modgil and Prakken (2013) is presented, but with some minor improvements. Nontrivial
improvements will be indicated below when they are made.
ASPIC + is not a system but a framework for specifying systems. As said above, the
framework is intended to allow the study of properties of instantiations, such as whether
they satisfy the rationality postulates of Caminada and Amgoud (2007). To this end it
defines the notion of an abstract argumentation system as a structure consisting of a logical
language L with a unary negation symbol , a set R consisting of two disjoint subsets Rs
and Rd of strict and defeasible inference rules, and a naming convention n in L for defeasible
rules in order to talk in L about the applicability of defeasible rules. All these elements are
left undefined in general and have to be specified for each specific instantiation.
Definition 2.3. [Argumentation systems] An argumentation system is a triple AS =
(L, R, n) where:
 L is a nonempty logical language with a unary negation symbol .
 R = Rs  Rd is a set of strict (Rs ) and defeasible (Rd ) inference rules of the form
1 , . . . , n   and 1 , . . . , n   respectively (where i ,  are meta-variables
ranging over wff in L), and Rs  Rd = .
 n : Rd  L is a naming convention for defeasible rules.
Informally, n(r) is a wff in L, which says that rule r  R is applicable. We write  = 
just in case  =  or  = . Note that  is not part of the logical language L but a
metalinguistic function symbol to obtain more concise definitions. Furthermore, if there is
no danger for confusion, we will sometimes write the sequence of antecedents of a strict or
defeasible rule as a set.
Example 2.1. An example argumentation system is with
L = {p, p, q, q, r, r, s, s, t, t, r1 , r2 , r1 , r2 },
Rs = {p, r  s;  r1 }, Rd = {q  r; t  s},
n(q  r) = r1 and n(t  s) = r2 .
ASPIC + as a framework abstracts from the origins of the strict and defeasible rules.
Several ways to identify rules are possible. One way, quite usual in AI, is to let the rules
express domain-specific knowledge. For example, the strict rules could contain terminological knowledge such as bachelors are not married, and the defeasible rules could contain
defeasible generalisations such as Birds fly or defeasible norms such as Thou shalt not lie.
Another way is to base the rules on general accounts of deductive and defeasible reasoning. For example, the strict rules might be chosen to correspond to a monotonic logic and
the defeasible rules might be instantiated with argument schemes (Walton, 1996). These
two ways to identify inference rules are only pragmatically different; formally, the ASPIC +
framework treats rules as inference rule regardless of their origin. In this paper we abstract
from the origin of the defeasible rules and we focus on the choice of strict rules. We are
in particular concerned with instantiations of ASPIC + in which the strict rules are chosen
to correspond to a monotonic logic (although several results will apply more generally). In
202

fiTwo Aspects of Relevance in Structured Argumentation

such instantiations Rs is defined as follows, given a monotonic consequence notion `L for a
logic L:
 Rs = {S   | S `L  and S is finite}
If Rs is defined in this way over the logical language of L, then we say that Rs corresponds
to logic L.
Definition 2.4. [Knowledge bases] A knowledge base in an AS = (L, R, n) is a set K  L
consisting of two disjoint subsets Kn and Kp (the necessary and ordinary premises).
Intuitively, the necessary premises are certain knowledge and thus cannot be attacked,
whereas the ordinary premises are uncertain and thus can be attacked.
Definition 2.5. [Consistency and strict closure] For any X  L, let the closure of X
under strict rules, denoted ClRs (X), be the smallest set containing X and the consequent
of any strict rule in Rs whose antecedents are in ClRs (X). Then a set X  L is
 directly consistent iff @ ,   X such that  = ;
 indirectly consistent iff ClRs (X) is directly consistent.
Example 2.2. In our example argumentation system, an example of a directly inconsistent
set is {p, p} and an example of a directly consistent but indirectly inconsistent set is
{p, r, s}. Finally, an example of a closure under strict rules is ClRs ({p, r}) = {p, r, s, r1 }.
Arguments can be constructed step-by-step from knowledge bases by chaining inference
rules into directed acyclic graphs (or trees if no formula is used more than once). In what
follows, for a given argument the function Prem returns all its premises, Conc returns its
conclusion and Sub returns all its sub-arguments, while TopRule returns the last rule used
in the argument.
Definition 2.6. [Argument] An argument A on the basis of a knowledge base K in an
argumentation system (L, R, n) is:
1.  if   K with:
Prem(A) = {};
Conc(A) = ;
Sub(A) = {};
TopRule(A) = undefined.
2. A1 , . . . An /  if A1 , . . . , An are arguments such that Rs /Rd contains the strict/defeasible
rule Conc(A1 ), . . . , Conc(An ) / , with:
Prem(A) = Prem(A1 )  . . .  Prem(An ),
Conc(A) = ,
Sub(A) = Sub(A1 )  . . .  Sub(An )  {A};
TopRule(A) = Conc(A1 ), . . . , Conc(An ) / .
203

fiGrooters & Prakken

Each of these functions Func are also defined on sets of arguments S = {A1 , . . . , An } as
follows: Func(S) = Func(A1 )  . . .  Func(An ). If an argument only uses strict rules, the
argument is said to be strict, otherwise it is defeasible. If an argument only has necessary
premises, then the argument is firm, otherwise it is plausible. For any argument A we define
Premn (A) = Prem(A)  Kn and Premp (A) = Prem(A)  Kp . The set of all arguments that
just consist of a necessary premise are denoted with N P (S).
Example 2.3. If our example argumentation system is combined with a knowledge base
with Kn = {p} and Kp = {q, t}, then the following arguments can be constructed:
A1
A2
A3
A4

=
=
=
=

p
q
t
A2  r

A5 =
A6 =
A7 =

A3  s
A1 , A4  s
A5  r1

Argument A1 is strict and firm, A2 and A3 are strict and plausible, and the remaining
arguments are all defeasible and plausible.
In Figure 1 these arguments are visualised. The type of a premise is indicated with a
superscript and defeasible inferences are displayed with dotted lines. The dotted boxes and
thick arrows will be explained below in Example 2.4.

Figure 1: Arguments and attacks of Example 2.1. The premises are at the bottom and
the conclusion at the top of the tree. Thin vertical links between boxes are
inferences while the thick diagonal links are attacks. The type of a premise is
indicated with a superscript and defeasible inferences, underminable premises and
rebuttable conclusions are displayed with dotted lines.

Arguments can be attacked in three ways: on their ordinary premises (undermining
attack), on a defeasible inference (undercutting attack) or on the conclusion of a defeasible
inference.
Definition 2.7. [Attack] An argument A attacks an argument B iff A undercuts, rebuts
or undermines B, where:
204

fiTwo Aspects of Relevance in Structured Argumentation

 A undercuts argument B (on B 0 ) iff Conc(A) = n(r) and B 0  Sub(B) such that
B 0 s top rule r is defeasible.
 A rebuts argument B (on B 0 ) iff Conc(A) =  for some B 0  Sub(B) of the form
B100 , . . . , Bn00  .
 A undermines argument B (on ) iff Conc(A) =  for some   Premp (B).
Example 2.4. In our running example A6 rebuts A5 and therefore also A6 rebuts A7 on
A5 (since A5 is a subargument of A7 ). Note that A5 does not rebut A6 since A6 has a strict
top rule. Furthermore, A7 undercuts A4 and A6 on A4 . In Figure 1 rebuttable conclusions
are visualised with dotted boxes and direct defeat relations are displayed with thick arrows.
Note that the indirect attacks of A6 on A7 and A7 on A6 are not explicitly visualised.
An argument A is a basic fallible argument 1 iff A  Kp or TopRule(A)  Rd . A basic
fallible argument is thus an argument that has a defeasible top rule or equates an ordinary
premise and so can be attacked on its final conclusion or inference. The set of all basic
fallible arguments for a set of arguments S is denoted by F A(S).
Argumentation systems plus knowledge bases form argumentation theories. These are
in turn combined with a preference ordering on the set of all arguments constructible in the
theory, to induce structured argumentation frameworks. Like the elements of argumentation
systems, the nature of the ASPIC + argument ordering is undefined in general and has to
be specified for each specific instantiation.
Definition 2.8. [Structured Argumentation Frameworks] Let AT be an argumentation theory (AS, K). A structured argumentation framework ( SAF) defined by AT , is a
triple hA, C,  i where A is the set of all finite arguments constructed from K in AS,  is
a binary relation on A, and (X, Y )  C iff X attacks Y .
Unlike Modgil and Prakken (2013) we will not consider versions of ASPIC + that require
arguments to have consistent premises (except briefly in Section 9 for purposes of comparison). In our approach strict arguments with inconsistent premises will be handled by our
choice to let Rs correspond to a paraconsistent logic, which will prevent trivialisation, i.e.,
it will prevent systems that generate arguments for any random conclusion from contradictions. Furthermore, we will leave it to the users whether they want to ensure that there are
no defeasible rules with inconsistent antecedents. This can be left to the user since allowing
defeasible rules with inconsistent antecedents does not cause trivialisation. Furthermore, all
results proved in this paper still hold whether defeasible rules with inconsistent antecedents
are excluded or not.
The notion of defeat can then be defined as follows. Undercutting attacks succeed
as defeats independently of preferences over arguments, since they are meant to express
exceptions to defeasible inference rules. Rebutting and undermining attacks succeed only
if the attacked argument is not stronger than the attacking argument (A  B is defined as
usual as A  B and B 6 A).
Definition 2.9. [Defeat] A defeats B iff:
1. This is a renaming of Dung and Thangs (2014) notion of a basic defeasible argument.

205

fiGrooters & Prakken

1. A undercuts B; or
2. A rebuts/undermines B on B 0 and A  B 0 .
Example 2.5. In our running example A6 defeats A5 and A7 unless A6  A5 . Furthermore,
with any preference relation between A4 and A7 , even with A7  A4 , we have that A7 defeats
A4 (and thus A7 also defeats A6 ).
SAFs generate abstract argumentation frameworks in the sense of Dung (1995), which
can then be used to evaluate arguments and their conclusions:
Definition 2.10. [Argumentation frameworks]
An abstract argumentation framework (AF ) corresponding to a SAF = hA, C,  i is a
pair (A, D) such that D is the defeat relation on A determined by SAF .
Let T  {complete, preferred, grounded, stable} and let L be from the AT defining
SAF . A wff   L is sceptically T -justified in SAF if  is the conclusion of a sceptically
T -justified argument, and credulously T -justified in SAF if  is not sceptically T -justified
and is the conclusion of a credulously T -justified argument.
Example 2.6. Suppose in our running example that D is such that A6 defeats A5 and A7
while A7 defeats A4 and A6 . The resulting AF is visualised twice in Figure 2. The grounded
extension is {A1 , A2 , A3 } while there are two preferred extensions E1 = {A1 , A2 , A3 , A4 , A6 }
and E2 = {A1 , A2 , A3 , A5 , A7 }. Both preferred extensions are also stable. The two preferred
extensions are visualised in Figure 2: members of an extension are coloured white.

Figure 2: Two preferred extensions of the Dung AF of Example 2.1.
We finally need the notion of a strict continuation of a set arguments, which we define
in a slightly different way than Modgil and Prakken (2013). The new definition is arguably
simpler but does not affect the proofs of Modgil and Prakken. It identifies arguments that
are formed by extending a set of arguments with only strict inferences into a new argument,
so that the new argument can only be attacked on the arguments that it extends.
Definition 2.11. [Strict continuations] The set of strict continuations of a set of arguments from A is the smallest set satisfying the following conditions:
206

fiTwo Aspects of Relevance in Structured Argumentation

1. Any argument A is a strict continuation of {A}.
2. If A1 , . . . , An and S1 , . . . , Sn are such that for each i  {1, . . . , n}, Ai is a strict
continuation of Si and B1 , . . . , Bn are all strict-and-firm arguments, and
Conc(A1 ), . . . , Conc(An ), Conc(B1 ), . . . , Conc(Bm )   is a strict rule in Rs , then
A1 , . . . , An , B1 , . . . , Bn   is a strict continuation of S1  . . .  Sn .
If argument A is a strict continuation of arguments A1 , . . . , An , then A is a strict argument
over {Conc(A1 ), . . . , Conc(An )}.
Example 2.7. In our running example all arguments are strict continuations of themselves
while A6 is a strict continuation of {A1 , A4 } and A7 is a strict continuation of A5 . Suppose
we temporarily add a strict rule p, s  r2 to Rs . Then A8 = A1 , A6  r2 is a strict
continuation of {A6 }.

3. The Rationality Postulates
Extensions of abstract argumentation frameworks are intuitively maximal sets of arguments
that can be rationally accepted together given such frameworks. Dungs (1995) various
semantics, yielding different types of extensions, can be seen as various alternative ways
to formalize rationality constraints on acceptable sets of arguments. When arguments
have structure, additional rationality constraints can defined for extensions above those of
abstract argumentation semantics. Caminada and Amgoud (2007) proposed the following
rationality postulates for structured argumentation.
Subargument closure: For every extension E, if an argument A is in E then
all subarguments of A are in E.
Closure under strict rules: For every extension E, the set Conc(E) is closed
under application of strict rules.
Direct consistency: For every extension E, the set Conc(E) is directly consistent.
Indirect consistency: For every extension E, the set Conc(E) is indirectly
consistent.
Note that closure under strict rules and direct consistency together imply indirect consistency.
Modgil and Prakken (2013) identify a set of conditions under which ASPIC + satisfies
all four postulates. The first condition is that the set of strict rules is either closed under
transposition or closed under contraposition.
Definition 3.1. [Closure under transposition, (Modgil & Prakken, 2013)] A set of
strict rules Rs is said to be closed under transposition if for each rule 1 , . . . , n   in Rs
all the rules of the form 1 , . . . , i1 , , i+1 , . . . , n  i for any i and  2 also
belong to Rs . An argumentation theory (AS, K) is closed under transposition if the strict
rules Rs of AS are closed under transposition.
2. Note that a wff  can have more than one  such that  = . For example, p = p and p = p.

207

fiGrooters & Prakken

Definition 3.2. [Closure under contraposition, (Modgil & Prakken, 2013)] An
argumentation system is said to be closed under contraposition if for all X  L, all s  X
and all  it holds that if   ClRs (X) then   ClRs (X\{}  {}) for any  and
. An argumentation theory (AS, K) is closed under contraposition if the argumentation
system AS is closed under contraposition.
The second condition states that the argument ordering should have the following properties:
Definition 3.3. [Reasonable argument ordering, (Modgil & Prakken, 2013)]  is
a reasonable argument ordering if and only if:
 A, B, if A is strict and firm and B is plausible or defeasible, then B  A;
 A, B, if B is strict and firm, then B  A;
 A, A0 , B, C, such that C  A, A  B and A0 is a strict continuation of {A}, then
C  A0 , A0  B;
 Let {C1 , . . . , Cn } be a finite subset of A and for i = 1, . . . , n let C +/i be some strict
continuation of {C1 , . . . , Ci1 , Ci+1 , . . . , Cn }. Then it is not the case that i, C +/i 
Ci .
Modgil and Prakken (2013) identify several types of argument orderings that are reasonable.
The third condition is axiom consistency.
Definition 3.4. [Axiom consistent, (Modgil & Prakken, 2013)] An argumentation
theory is axiom consistent if and only if ClRs (Kn ) is consistent.
Modgil and Prakken (2013) prove that any argumentation theory that satisfies all three
conditions only induces extensions that satisfy all four rationality postulates.

4. The Trivialisation Problem
In this section we illustrate the trivialisation problem in more detail. The following abstract
example illustrates the problems that can arise if the strict rules of an argumentation system
correspond to classical logic, i.e. X    Rs if and only if X `  and X is finite (where
` denotes classical consequence).
Example 4.1. Let Rd = {p  q; r  q; t  s}, Kp =  and Kn = {p, r, t}, while Rs
corresponds to classical logic. Then the corresponding AF includes the following arguments:
A1 : p
B1 : r
D1 : t

A2 : A1  q
B2 : B1  q
D 2 : D1  s

C: A2 , B2  s

Figure 3 displays these arguments and their attack relations. Dotted lines indicate defeasible inferences and dotted boxes indicate rebuttable conclusions. Argument C attacks D2 .
208

fiTwo Aspects of Relevance in Structured Argumentation

Figure 3: Illustrating trivialisation
Whether C defeats D2 depends on the argument ordering but plausible argument orderings
are possible in which C 6 D2 and so C defeats D2 . This is problematic, since s can be
any formula, so any defeasible argument unrelated to A2 or B2 , such as D2 , can, depending
on the argument ordering, be defeated by C. Clearly, this is extremely harmful, since the
existence of just a single case of mutual rebutting attack, which is very common, could
trivialise the system. It should be noted that simply disallowing application of strict rules
to inconsistent sets of formulas does not help, since then an argument for s can still be
constructed as follows:
A3 :
C 0:

A2  q  s
A3 , B2  s

Note that argument C 0 does not apply any strict inference rule to an inconsistent set of
formulas.
This example suggests the following formalisation of the property of trivialisation.
Definition 4.1 (Trivialising argumentation systems). An argumentation system AS is
trivialising iff for all ,   L and all knowledge bases K such that {, }  K a strict
argument on the basis of K can be constructed in AS with conclusion .
We are then interested in defining classes of non-trivialising argumentation systems.
The argumentation system in our example is clearly trivialising since Rs contains strict
rules ,    for all ,   L.
Example 4.1 does not cause any problems for preferred or stable semantics, since A2
and B2 attack each other and at least one of these attacks will (with non-circular argument
orderings) succeed as defeat. Therefore, all preferred or stable extensions contain either
A2 or B2 but not both. Since both A2 and B2 attack C (by directly attacking one of its
subarguments), C is for each preferred or stable extension defeated by at least one argument
in the extension, so C is not in any of these extensions, so D2 is in all these extensions.
This is intuitively correct since there is no connection between D2 and the arguments A2
and B2 .
In fact, the only semantics defined by Dung (1995) that has problems with Example 4.1
is grounded semantics. Since both A2 and B2 defeat each other, neither of them is in the
grounded extension. So that extension does not defend D2 against C and therefore does
not contain D2 .
209

fiGrooters & Prakken

Pollock (1994, 1995) thought that the just-given line of reasoning for preferred semantics
suffices to show that his recursive-labelling approach (which was later in Jakobovits &
Vermeir, 1999 proved to be equivalent to preferred semantics) adequately deals with this
problem. However, Caminada (2005) showed that the example can be extended in ways
that also cause problems for preferred and stable semantics. Essentially, he replaced the
facts p and r with defeasible arguments for p and r and let both these arguments be defeated
by a self-defeating argument. On the one hand, such self-defeating arguments cannot be
in any extension, since extensions are conflict free. However, if a self-defeating argument
is not defeated by other arguments, it prevents any argument that it defeats from being
acceptable with respect to an extension. In our example, if both A2 and B2 are defeated
by a self-defeating argument that is otherwise undefeated, then neither A2 not B2 is in any
extension, so no argument in an extension defends D2 against C.
A critic of ASP IC + or Pollocks approach might argue that the problem is caused by
the combination of strict (i.e., deductive) and defeasible inference rules. Indeed, in classical argumentation (Besnard & Hunter, 2008; Gorogiannis & Hunter, 2011) the problem
can be easily avoided by requiring that the premises of an argument are consistent. However, there are reasons to believe that classical logic is too strong to be able to model all
forms of defeasible reasoning; see, for instance, the discussions by Brewka (1991), Ginsberg
(1994) and Prakken (2012). Furthermore, in assumption-based argumentation (ABA) as
reconstructed by Dung et al. (2007), which only has strict inference rules but does not
require them to be classical, and which does not require that the premises of arguments are
consistent, the problem may or may not arise depending on how it is instantiated. When
reconstructed in ASPIC + as by Prakken (2010), ABA arguments are built from ordinary
premises Kp and strict inference rules Rs . The following example (in the notation of the
ASPIC + reconstruction of ABA) shows that the trivialisation problem can also arise in
ABA.
Example 4.2. Take Kp = {p, p, s} and let Rs correspond to classical logic, i.e, S   
Rs iff S is a finite set of wff that classically implies . Then the following arguments can
be constructed.
A:p
B : p
C : A, B  s
D:s
The trivialising argument C prevents argument D from being in any extension.
The problem now is to instantiate and/or redefine ASPIC + in a way that avoids the
trivialising effects of strict inferences from an inconsistent set, while still satisfying the
rationality postulates of Caminada and Amgoud (2007).

5. Instantiating ASPIC + with Two Well-Known Paraconsistent Togics
As said in the introduction, one way to avoid trivialisation is to derive the strict rules
of ASPIC + from a paraconsistent logic. A logical consequence relation `L is said to be
paraconsistent if it is not explosive, i.e. when it does not hold for all A and B that
210

fiTwo Aspects of Relevance in Structured Argumentation

{A, A} `L B. In this section we investigate this strategy for two well-known paraconsistent
logics, the Logic of Paradox of Priest (1979, 1989) and the system C of Da Costa (1974).
Another well-known paraconsistent logic is the family of relevant logics. However, this logic
is nonmonotonic (Read, 1988, p. 100). This is a problem since the idea of ASPIC + is that
if the strict rules are based on a logic, this logic is monotonic. For this reason, relevance
logics are not considered in this paper.
5.1 Logic of Paradox
The Logic of Paradox (Priest, 1979, 1989) is obtained by relaxing the assumption of classical
propositional logic that a sentence cannot be both true and false. Sentences in the Logic of
Paradox (LP ) can have two truth values instead of one. The set of possible truth values is
{{1}, {0}, {0, 1}}, where {0, 1} is the paradoxical true and false.
The semantics for the propositional version of LP is as follows.
1. (a) 1  v(A)  0  v(A)
(b) 0  v(A)  1  v(A)
2. (a) 1  v(A  B)  1  v(A) and 1  v(B)
(b) 0  v(A  B)  0  v(A) or 0  v(B)
3. (a) 1  v(A  B)  1  v(A) or 1  v(B)
(b) 0  v(A  B)  0  v(A) and 0  v(B)
4. (a) 1  v(A  B)  0  v(A) or 1  v(B)
(b) 0  v(A  B)  1  v(A) and 0  v(B)
An interpretation is a model of a formula f if and only if 1  v(f ) holds in that interpretation. It is a model of a set of formulas if and only if it is a model of every formula in the
set. The semantical notion of logical consequence is defined as follows:
  LP A  for all evaluations v either 1  v(A) or for some B  , v(B) = {0}
It has been shown that LP coincides with propositional logic on its tautologies but not on its
valid inferences. In particular, although AA  B is a tautology in LP , the corresponding
inferences {A  A} |=LP B and also {A, A} |=LP B are invalid. For a counterexample,
consider an evaluation such that B is strictly false and A, A are undetermined (both true
and false), so A  A is undetermined as well. Then 0  v(A  A), so by the valuation
postulates 1  v(A  A  B). Therefore, {A  A} 2LP B and also {A, A} 2LP B.
Therefore, the Logic of Paradox is a paraconsistent logic.
It turns out that the postulate of indirect consistency is not satisfied in case the strict
rules of ASPIC + are instantiated with all valid inferences in the Logic of Paradox, that is,
if S    Rs iff S is finite and S |=LP . The following counterexample was brought to
our attention by Graham Priest (personal communication).
Example 5.1. Take a SAF defined by an argumentation theory with the knowledge base
Kp  Kn , with Kn =  and Kp = {a, a  b, a  c, b  c}. Further suppose that Rs
211

fiGrooters & Prakken

corresponds to the Logic of Paradox and that there are no defeasible rules (Rd = ).
Finally, assume that all arguments with at least one ordinary premise are equally preferred
according to the argument ordering  of SAF .
It is easily checked that Kp implies that at least one of a, b or c must be paradoxical.
Therefore, there exists an argument A1 : a, ab, ac, bc  (aa)(bb)(cc).
Since tautologies are preserved in the Logic of Paradox, (bb), (aa) and (cc) are
also entailed by K. This implies that there exists an argument A2 : a, ab, ac, bc 
((a  a)  (b  b)  (c  c)). These arguments only use strict rules so they can only be
attacked on their premises. However, there does not exist an argument built from K which
has a conclusion d for a d  Kp . To show this, for each d  Kp a model has to be found
for which v(d) 6= {0} holds while not 1  v(d).
Model 1: to show that it is not the case that a follows from Kp .
Take the model v(a) = {1}, v(b) = {1} and v(c) = {0, 1}. Then it is clear that v(a) =
{1}, v(a  b) = {1}, v(a  c) = {0, 1} and v(b  c) = {0, 1}, but not 1  v(a).
Model 2: to show that it is not the case that (a  b) follows from Kp .
Take again the model v(a) = {1}, v(b) = {1} and v(c) = {0, 1}. Then it is clear that v(a) =
{1}, v(a  b) = {1}, v(a  c) = {0, 1} and v(b  c) = {0, 1}, but not 1  v((a  b)).
Model 3: to show that it is not the case that (a  c) follows from Kp .
Take the model v(a) = {1}, v(b) = {0, 1} and v(c) = {1}. Then it is clear that v(a) =
{1}, v(a  b) = {0, 1}, v(a  c) = {1} and v(b  c) = {0, 1}, but not 1  v((a  c)).
Model 4: to show that it is not the case that (b  c) follows from Kp .
Take the model v(a) = {0, 1}, v(b) = {0} and v(c) = {0}. Then it is clear that v(a) =
{0, 1}, v(ab) = {0, 1}, v(ac) = {0, 1} and v(bc) = {1}, but not 1  v((bc)).
This means that there are no arguments which defeat one of the arguments A1 and A2 , so
A1 and A2 are elements of a complete extension E. This means that (aa)(bb)(cc)
and ((a  a)  (b  b)  (c  c)) are elements of Conc(E). Therefore, this argumentation
theory does not satisfy the postulate of direct consistency.
5.2 Da Costas Basic C-system: C
The system C of Da Costa (1974) adds the axioms A  A and A  A to positive
logic, a negation free first-order logic (these added axioms are called the Dialectic Double
Negation (DDN) and Exclusive Middle (EM) respectively). C is in certain aspects the
dual of intuitionistic logic, since in intuitionistic logic the axiom EM is invalid and the
axiom Non-Contradiction (NC, (A  A)) is valid. In C , the axiom EM is valid and NC
is invalid. Intuitionistic logic tolerates incomplete situations to avoid inconsistency, while
the C-systems admit inconsistent situations, but incomplete situations are removed. For
example, in C it is possible that all three sentences A, A, A are true. However unlike
in the Logic of Paradox, sentences can only have one truth value. Next the semantics and
the proof theory are given which are sound and complete with respect to each other.
The propositional version of C has the following bivalent valuation for formulas built
from a logical language L.
1. v(A  B) = 1  v(A) = 1 and v(B) = 1
2. v(A  B) = 1  v(A) = 1 or v(B) = 1
212

fiTwo Aspects of Relevance in Structured Argumentation

3. v(A  B) = 1  v(A) = 0 or v(B) = 1
4. v(A) = 1  v(A) = 0
5. v(A) = 1  v(A) = 1
An interpretation of a formula f by its valuation form is a model if and only if v(f ) = 1 in
that interpretation. An interpretation is a model of a set of formulas if and only if it is a
model of every formula in the set. The semantical logical consequence:
  C A  for all evaluations v either v(A) = 1 or for some B  , v(B) = 0
It is easy to show that {A, A} 0C B, since the following valuation function can be chosen:
v(A), v(A) = 1 while v(B) = 0.
Replacing LP with C as the source of strict rules in ASPIC + still yields counterexamples to direct consistency. (In this example,  denotes the material implication).
Example 5.2. Suppose that the knowledge base is K = Kp  Kn , with Kn =  and
Kp = {a, a  b, a  b} with the following valuation: v(a) = 1, v(a  b) = 1 and
v(a  b) = 1. Further suppose that Rs corresponds to C and that there are no defeasible
rules (Rd = ). Finally, assume that all basic fallible arguments are equally strong. Then
the following two arguments exist: A1 : a, a  b  b and A2 : a, a  b  b. These
are shown in Figure 4. These two arguments both use a strict rule. This means that
these arguments can only be defeated on their premises. However, none of a, (a  b),
(a  b) are in ClRs (K), so there are no arguments which defeat A1 or A2 on their
premises. Therefore, A1 and A2 will be elements of a complete extension E, which means
that b, b  Conc(E).

Figure 4: Arguments of Example 5.2

6. Another Attempt: Instantiating ASPIC + with Weak Consequence
In this section we investigate the use of another paraconsistent consequence notion, the socalled weak consequence relation originally proposed by Rescher and Manor (1970). Its basic
idea is that a sentence weakly follows from a set S of sentences if it classically follows from at
least one consistent subset of S. This notion is clearly related to classical argumentation, as
we will formally show in Section 9. It also inspired by early consistency-based approaches
to argumentation, such as Krause, Ambler, Elvang-Gransson, and Fox (1995). To our
knowledge, we are the first to use it in a system with defeasible rules. We first discuss the
weak-consequence notion and then define how it can be used to overcome the trivialisation
problem in ASPIC + .
213

fiGrooters & Prakken

6.1 Weak Consequence
Weak consequence over a standard propositional language is formally defined as follows.
Definition 6.1 (Weak consequence relation, `W ).  `W  if and only if there is a
maximal consistent subset  of  such that  `  in classical logic.
Note that the word maximal is in fact not required, since according to Lindenbaums
Lemma every consistent set of formulas can be extended into a maximally consistent one.
It is easy to see that {a, a} `W b does not hold, because {a, a} is not a maximal
consistent subset of {a, a}. Therefore, this consequence relation is paraconsistent.
We next discuss three other common properties.
[Reflexivity] If   , then  `W .
This property holds for `W . Each    belongs to some maximally consistent subset
 of . In classical logic, it holds that if   , then  ` . Therefore, it obviously holds
that  `W .
[Monotonicity]  `W , then ,  `W .
The monotonicity property can be proven for `W as follows. There must be a maximal
consistent subset  of  such that  ` . Since     , there must exist a maximal
consistent extension of  in   , 0 , such that 0 ` . Therefore, ,  `W .
[Cut] ,  `W  and  `W , then  `W .
This rule does not hold. For a counterexample, consider the set  = {a, a  b}. Then
 `W b and , b `W a  b, while it is not the case that  `W a  b.
Since the Cut rule does not hold, a naive instantiation of ASPIC + s strict rules with
this logic W would not avoid explosion, as shown in the following example:
Example 6.1. Consider the following knowledge base Kp = {p, p, r}, Kn = , instantiate
the strict rules with all valid inferences from finite sets in the logic W and let Rd = . Then
the following arguments can be constructed:
A1 : p
B : p
D:r

A2 : A1  p  r
C : A2 , B  r

Argument C concludes with r.
The underlying reason for this problem is that the Cut rule does not hold for `W , so that
in our example Kp 0W r. So if we want ASPIC + s strict part to behave according to `W ,
chaining of strict rules should be excluded.3 In Example 6.1, the argument C is not allowed
3. A similar idea was suggested by Martin Caminada in personal communication. We will discuss his idea
in Section 10.

214

fiTwo Aspects of Relevance in Structured Argumentation

since A2 already has a strict top rule. The prohibition of the chaining of strict rules will
prevent trivialisation. To this end, the ASPIC + notion of an argument must be redefined,
which results in the ASPIC ? framework.
6.2 The ASPIC ? Framework
We now change the ASPIC + framework into the ASPIC ? framework by disallowing the
chaining of strict rules in arguments. We first need to change the definition of an argument:
Definition 6.2. [Argument? in ASPIC ? ] An argument? A on the basis of a knowledge
base K = (K, ) in an argumentation system (L, R, n, 0 ) is:
1.  if   K with
Prem() = {},
Conc() = ,
Sub(A) = {},
TopRule(A) = undefined.
2. A1 , . . . , An   if A1 , . . . , An are arguments? with a defeasible top rule or are from
K and such that there exists a strict rule Conc(A1 ), . . . , Conc(An )   in Rs .
Prem(A) = Prem(A1 )  . . .  Prem(An ),
Conc(A) = ,
Sub(A) = Sub(A1 )  . . .  Sub(An )  {A},
TopRule(A) = Conc(A1 ), . . . , Conc(An )  .
3. A1 , . . . , An   if A1 , . . . , An are arguments? such that there exists a defeasible rule
Conc(A1 ), . . . , Conc(An )   in Rd .
Prem(A) = Prem(A1 )  . . .  Prem(An ),
Conc(A) = ,
Sub(A) = Sub(A1 )  . . .  Sub(An )  {A},
TopRule(A) = Conc(A1 ), . . . , Conc(An )  .
Arguments? are just a special case of normal arguments. Therefore, all definitions for
(sets of ) arguments are the same in case the term argument can be replaced by argument?
without problems. Attack and defeat are just the same for arguments? . Structured and
abstract argumentation frameworks for the ASPIC ? framework are just the same except
that they only contain arguments? . Accordingly, the notions of justified and defensible
arguments? and conclusions are still defined as in Section 2.
The new ASPIC ? framework motivates a new interpretation of the strict-closure postulate in case Rs corresponds to a logic L. The fact that the weak-consequence notion `W
does not satisfy the Cut rule shows that closure under Rs does not in general coincide
with closure under the consequence notion of the logic to which Rs corresponds. In fact,
Example 6.1 can be easily extended to a counterexample for `W , since r is in the strict
closure of {p, p} while {p, p} 6`W r. This in turn gives a reason to doubt whether full
closure under strict rules is always desirable. Arguably this desirability depends on the
properties of the logic to which Rs corresponds: if this logic does not satisfy the Cut rule,
then full strict closure is not desirable. Instead, what is desirable in such cases is that
extensions are closed under consequence in the adopted logic for Rs . This is what we will
215

fiGrooters & Prakken

prove for ASPIC ? when its strict rules correspond to `W . To this end we will below restrict
the strict-closure postulate to allowed applications of strict rules and we will, moreover,
introduce a new rationality postulate of logical closure.
We think that this analysis is compatible with Caminada and Amgouds (2007) reason
for proposing the strict-closure postulate, since arguably their implicit assumption behind
this postulate was that formulas in the strict closure are always reachable in that arguments
for them can be constructed from the arguments in the extension:
The idea of closure is that the answer of an argumentation-engine should be
closed under strict rules. That is, if we provide the engine with a strict rule
a  b (. . . ), together with various other rules, and our inference engine outputs
a as justified conclusion, then it should also output b as justified conclusion.
Consequently, b should also be supported by an acceptable argument (emphasis
added by current authors). (Caminada & Amgoud, 2007, p. 294).
This quote is compatible with our new account of strict closure and our new logical-closure
postulate, since if Rs is based on `W , the implicit assumption that being in the strict
closure of an extension equates supportable by an acceptable argument is not satisfied.
To formalise our new interpretation of the strict-closure postulate, new notions of strict
closure and indirect consistency are needed. We first explain some notation. Recall from
Section 2 that for any set S of arguments, F A(S) denotes the set of basic fallible arguments
in S and N P (S) denotes the set of all necessary premises of any argument in S. The
corresponding notions for ASPIC ? will be denoted by F A? (S) and N P ? (S). Next, for
any set S of arguments? let S # be defined as F A? (S)  N P ? (S). The set S # contains
those arguments? in S that have no strict top rule, so they are the arguments? in S to
the conclusions of which a strict rule can be applied to form new arguments? . Then strict
closure and indirect consistency can be redefined as follows.
Definition 6.3. [Closure? ] For any X  L, let the closure ? of X under strict rules,
? (X), be the smallest set containing X and the consequent of any strict rule in
denoted ClR
s
Rs whose antecedents are in X. The set of arguments? S is said to be closed? under strict
? (Conc(S # )).
rules if and only if Conc(S) = ClR
s
The new strict closure notion amounts to one-steps application of strict rules.
Definition 6.3 is clarified in the following example.
Example 6.2. Suppose we have the following sets: Kn = {p}, Kp = {q, t} and Rd = {q 
r, t  s} and RS corresponds to classical propositional consequence. Then the following
set S of arguments? can be constructed.
A1 = p
A5 = A3  s
A2 = q
A6 = A1 , A4  p  r
A3 = t
A7 = A5  s  v
A4 = A2  r
In this example S # = {A1 , . . . , A5 } because it is allowed to apply strict rules to these
arguments? . S is not closed? under strict rules, since for example p  q 
/ Conc(S) while
? (Conc(S # )).
argument? A8 = A1 , A2  p  q can be constructed, so p  q  ClR
s
216

fiTwo Aspects of Relevance in Structured Argumentation

Definition 6.4. [Indirect consistency? ] A set X  L is indirectly consistent ? if there is
? (X). Otherwise it is indirectly inconsistent? . A set of
not a   L such that ,   ClR
s
arguments? S is said to be indirectly consistent ? if Conc(S # ) is indirectly consistent? .
Henceforth by consistent ? we mean indirectly consistent ? . The rest of the ASPIC ?
framework is the same as in the ASPIC + framework.

7. Verifying the Postulates for ASPIC ? with Weak Consequence
In this section we investigate a class of instantiations of the just-defined ASPIC ? framework
in which the language L is a (nonempty) propositional language and the set Rs of strict rules
corresponds to Rescher and Manors (1970) notion of weak consequence over this language.
More precisely:
 Rs = {S   | S `W  and S is finite}
Below we will speak of such instantiations of ASPIC ? as ASPIC ? SAFs with `W . The
theorem below states that SAFs avoid trivialisation in the sense of Definition 4.1.
Theorem 7.1. No ASPIC ? SAFs with `W is defined by an argumentation theory with a
trivialising argumentation system.
It remains to be investigated whether this class of instantiations of ASPIC ? satisfies
Caminada and Amgouds (2007) rationality postulates and the newly proposed postulate
of logical closure. To this end, we first formally specify these postulates for ASPIC ? .4
Definition 7.1. [Rationality postulates for ASPIC ? ] Let  = (A, C, ) be an ASPIC ?
structured argumentation framework defined by an ASPIC ? AT with AS = (L, R, n) and
K = Kn  Kp . Let AF be the abstract argumentation framework corresponding to  and
let T  {complete, preferred, grounded, stable}. Then:
  satisfies the closure under subarguments postulate iff for all T -extensions E of AF
it holds that if an argument? A is in E then all subarguments? of A are in E;
  satisfies the consistency postulate iff for all T -extensions E of AF it holds that
Conc(E) is consistent ? ;
  satisfies the strict closure postulate iff for all T -extensions E of AF it holds that
? (Conc(E # )).
Conc(E) = ClR
s
 If Rs corresponds to logic L, then  satisfies the L-closure postulate iff for all T extensions E of AF and all   L it holds that if Conc(E) `L  then   Conc(E).
Since all grounded (preferred, stable) extensions are also complete extensions, it suffices
the prove the postulates for complete extensions. Now one way to prove the first three
postulates is to try to adapt the proofs of Modgil and Prakken (2013) for ASPIC + to
4. Caminada and Amgoud (2007) also propose postulates for the intersection of extensions and their conclusion sets, but since their satisfaction directly follows from satisfaction of the postulates for individual
extensions, these postulates will below be ignored.

217

fiGrooters & Prakken

ASPIC ? . However, a problem here is that in general, if Rs corresponds to `W , then
closure under transposition and contraposition do not hold in ASPIC ? . We first give a
counterexample to closure under transposition.
Example 7.1. Since {a, b, c} `W a  b and because of the monotonicity of the logic W , it
holds that {a, b, c, a} `W a  b. This means that a, b, c, a  a  b is in Rs . However,
there is no maximal consistent subset of {a, b, (a  b), a} that proves c in classical
logic. Therefore {a, b, (a  b), a} 0W c and so a, b, (a  b), a  c 
/ Rs . This
means that if the strict rules Rs in an argumentation system AS of the argumentation
theory AT = (AS, K) are instantiated with the valid inferences in the logic W , then the
argumentation theory AT is not closed under transposition.
A similar counterexample can be given to closure under contraposition.
Example 7.2. Consider a knowledge base with Kn =  and Kp = {a, b, c, a}. Since
{a, b, c, a} `W ab and {a, b, (ab), a} 0W c, it follows that ab  ClRs ({a, b, c, a}),
but not c  ClRs ({a, b, (a  b), a}) (because chaining of strict rules is not allowed).
Therefore, if the strict rules Rs in an argumentation system AS of the argumentation
theory AT are instantiated with the valid inferences in the logic W , then AT is not closed
under contraposition.
Therefore, the results of Modgil and Prakken (2013) cannot be directly used for our
purposes. However, in a somewhat different formal setting, Dung and Thang (2014) provide
weaker conditions for satisfying the rationality postulates, which are implied by but do not
imply closure under transposition or contraposition. We will therefore use their results as a
guidance in verifying the postulates for the just-defined class of instantiations of ASPIC ? .
Dung and Thang (2014) formulate their results in terms of an adaptation of Amgoud and
Besnards (2013) abstract-logic approach to abstract argumentation with abstract attack
and support relations between arguments. After defining their adaptation they apply it to
what they call rule-based systems. It turns out that for our purposes we do not need Dung
and Thangs general abstract framework but that we can instead adapt their definitions for
their rule-based instantiation to ASPIC ? . In doing so, we will for each of our definitions and
results indicate its counterpart for Dung and Thangs rule-based systems. Our definitions
below implicitly assume a given ASPIC ? structured argumentation framework.
Definition 7.1. [Base of an argument? , (cf. Dung & Thang, 2014, Def. 6)] Let A
be an argument? and BA be a finite set of subarguments? of A. BA is a base of A if
? (Conc(BA));
 Conc(A)  ClR
s

 For each argument? C, C defeats A if and only if C defeats BA.
The following example shows the intuitive idea of a base.
Example 7.3. Take Kn = , Kp = {a, b}, Rs = {c  d} and Rd = {a, b  c}. Then the
following arguments? can be constructed: A1 : a, A2 : b, A3 : A1 , A2  c and A4 : A3  d.
See Figure 5.
A4 can only be attacked on its subarguments? A1 , A2 , or A3 because of the strict top
rule. Every argument? that attacks A1 or A2 also attacks A3 , so every argument? that
218

fiTwo Aspects of Relevance in Structured Argumentation

Figure 5: Arguments of Example 7.3
attacks A4 also attacks A3 . It is easy to see that every argument? that attacks A3 also
attacks A4 . Conc(A4 )  ClRs (Conc(A3 )), so {A3 } is a base of A4 . The same kind of
reasoning applies to the fact that the set {A1 , A2 , A3 } is also a base of A4 .
However note that the set {A1 , A2 } is not a base of A4 , because A3 can be rebutted without
A1 or A2 being attacked.
Definition 7.2. [Generation of arguments? , (cf. Dung & Thang, 2014, Def. 7)]
An argument? A is said to be generated by a set of arguments? S, if there is a base B of A
such that B  Sub(S). The set of all arguments? generated by S is denoted by GN (S).
The following lemma follows by definition of GN (S).
Lemma 7.2. [(cf. Dung & Thang, 2014, Lemma 1(2))] For every set of arguments?
S, Sub(S)  GN (S).
Theorem 7.3. [(cf. Dung & Thang, 2014, Thm. 1)] Let E be a complete extension,
then GN (E) = E.
Note that Lemma 7.2 and Theorem 7.3 immediately imply the closure? under subarguments?
postulate since for every complete extension E: Sub(E)  GN (E) = E.
Theorem 7.4. Each ASPIC ? SAF satisfies the closure? under subarguments? postulate.
Definition 7.3. [Compact, (cf. Dung & Thang, 2014, Def. 8)] An ASPIC ? SAF is
compact if for each set of arguments? S, GN (S) is closed? under strict rules. This is equal
? (Conc(GN (S)# )).
to Conc(GN (S)) = ClR
s
The following two theorems can later be combined for proving closure? under subarguments?
postulate.
Theorem 7.5. [(cf. Dung & Thang, 2014, Thm. 2)] Each compact ASPIC ? SAF
satisfies the closure? under strict rules postulate.
Theorem 7.6. Each ASPIC ? SAF with `W is compact.
Definition 7.4. [Cohesive, (cf. Dung & Thang, 2014, Def. 9)] An ASPIC ? SAF is
cohesive, if for each inconsistent? set of arguments? S, GN (S) is conflicting (attacks itself).
219

fiGrooters & Prakken

Theorem 7.7. [(cf. Dung & Thang, 2014, Thm. 3)] Each cohesive ASPIC ? SAF
satisfies the consistency? postulate.
The next two definitions are needed for proving cohesiveness.
Definition 7.5. [Self-contradiction axiom, (cf. Dung & Thang, 2014, Def. 14)] An
ASPIC ? SAF is said to satisfy the self-contradiction axiom, if for each minimal inconsistent
? (X).
set X  L: X  ClR
s
Definition 7.6. [Axiom consistent? ] An ASPIC ? SAF is axiom consistent? if and only
? (K ) is consistent? .
if ClR
n
s
Theorem 7.8. [(cf. Dung & Thang, 2014, Thm. 5)] If a compact, axiom consistent?
ASPIC ? SAF has a reasonable argument ordering and satisfies the self-contradiction axiom,
then SAF is cohesive.
Theorem 7.9. Each ASPIC ? SAF with `W satisfies the self-contradiction axiom.
Combining Theorem 7.5, 7.6, 7.7, 7.8 and 7.9 results in the following important conclusion.
Theorem 7.10. Each ASPIC ? SAF with `W which is axiom consistent? and has a reasonable argument ordering satisfies the strict-closure? and consistency? postulates.
Finally, satisfaction can be proved of the newly proposed postulate of logical closure.
Below, `CL denotes classical consequence.
Lemma 7.11. Each ASPIC ? SAF with `W satisfies the following property: for any set S
of arguments? it holds that if Conc(S) `CL , then Conc(S # ) `CL .
Lemma 7.12. Each ASPIC ? SAF with `W satisfies the following property: for any set E
of arguments? it holds that if Conc(E) is strictly closed and consistent , then Conc(E) is
classically consistent.
Theorem 7.13. Each ASPIC ? SAF with `W which is axiom consistent? and has a reasonable argument ordering satisfies the logical closure postulate.
It can be concluded that we have identified a class of instantiations of the new ASPIC ? framework with Rescher and Manors (1970) weak consequence notion that satisfies
the consistency and closure postulates while preventing trivialisation in case of rebutting
arguments. In order to obtain these results, the ASPIC + framework had to be adapted
by prohibiting chaining of strict rules, resulting in the new ASPIC ? framework and new
notions of strict closure and indirect consistency, plus a new postulate of closure under
logical consequence.

8. Minimality of Arguments
In this section we address two aspects of minimality of arguments. We first explain the issue
of inference rules with non-minimal sets of antecedents in more detail, we then investigate
the effects of requiring strict rules to have minimal antecedent sets, and we finally study
the issue of non-circular arguments.
220

fiTwo Aspects of Relevance in Structured Argumentation

8.1 The Issue of Minimality of Antecedent Sets
As said in the introduction, deductive approaches to argumentation (e.g., Besnard & Hunter,
2008; Gorogiannis & Hunter, 2011; Amgoud & Besnard, 2013) require arguments to have a
subset-minimal set of premises. For example, Gorogiannis and Hunter define an argument
as any (S, p) where S is a set of well-formed propositional formulas and p a well-formed
propositional formula, such that:
1. S is consistent in classical propositional logic
2. S implies p in classical propositional logic
3. no proper subset of S implies p in classical propositional logic
(Hunter 2007 explores relaxations of these properties with his notion of approximate arguments.) However, if arguments can apply defeasible inference rules, then the third requirement is undesirable, since defeasible rules that are based on more specific information may
well be stronger. Consider the following example:
Example 8.1. Consider an argumentation system with Rs = , Rd = {p  q; p, r 
q; r  q} and consider a knowledge base with Kn = {p, r} and Kp = . The following
arguments can be constructed.
A1 :
A2 :
B1 :
B2 :
C:

p
A1  q
r
A1 , B1  q
B1  q

For a real-world example of the two defeasible rules for q consider again the example from
the introduction: Observations done in ideal circumstances are usually correct is on any
reasonable account of rule strength stronger than Observations are usually correct. Now
consider an argument ordering where arguments are compared on specificity. Then we have
that C  B2 while A2 and C are incomparable. Then in all semantics a unique extension
{A1 , A2 , B1 , B2 } results, which is the intuitively correct outcome. However, if all arguments
are required to have subset-minimal premises, then B2 cannot be constructed and the
outcome is different: the grounded extension is {A1 , B1 } while there are two preferred and
stable extensions, namely, {A1 , B1 , A2 } and {A1 , B1 , C}.
The same analysis holds for defeasible versus strict rules. Consider a defeasible rule
p  q and a strict rule p, r  q: then we clearly do not want to rule out an argument for
q with premises p and r, since it could well be stronger than the defeasible argument for q
with premise p. However, as noted above, this analysis does not apply to strict inference
rules, since any strict inference guarantees its conclusion given its premises. So it still makes
sense that strict inference rules should only be applied to a subset-minimal set of formulas.
With this requirement, the number of arguments that can be generated can be significantly
reduced in case this restriction is introduced, which can result in a more efficient system.
The following example illustrates the problems that arise without the requirement that
strict rules have subset-minimal sets of antecedents.
221

fiGrooters & Prakken

Example 8.2. Suppose the strict rules of an argumentation system are instantiated with
classical logic and consider a knowledge base with Kn =  and Kp = {p, q, r, s, t, u}. Then
among other things, argument A1 : p, q  p  q can be constructed. Since classical logic is
monotonic, all following arguments (and more) for p  q can also be constructed.
A2 : p, q, r  p  q
A4 : p, q, t  p  q
A6 : p, q, r, s  p  q

A3 : p, q, s  p  q
A5 : p, q, u  p  q
A7 : p, q, t, u  p  q

All arguments in the table of Example 8.2 can be considered as redundant given A1 .
Recall that for defeasible rules this is different, since more specific defeasible rules for the
same conclusion may well be stronger, as just explained. The problem then is to adapt the
minimality requirement to the setting with defeasible rules and to investigate the extent to
which this affects the conclusions that can be drawn.
8.2 Minimal Arguments? for the ASPIC ? Framework
We next investigate whether excluding strict inferences from non-subset-minimal sets of
formulas makes a difference. In line with the focus in this paper we will only prove results
for ASPIC ? , but the proofs for their ASPIC + counterparts would be entirely similar. In
particular, we want to know whether the conclusions that can be drawn from an argumentation framework are affected in case arguments? are required to be minimal. We will prove
that under a rather weak condition on the argument? ordering the conclusions are the same
in both cases.
First, the above described idea of minimal arguments is formally defined. The main
difference with Definition 6.2 is that clause (2) now disallows application of strict rules with
a non-minimal antecedent set.
Definition 8.1. [Minimal argument? ] A minimal argument? A on the basis of a knowledge base K = (K, ) in an argumentation system (L, R, n, 0 ):
1.  if   K with
Prem() = {},
Conc() = ,
Sub(A) = {},
TopRule(A) = undefined.
2. A1 , . . . , An   if A1 , . . . , An are minimal arguments? with a defeasible top rule or
are from K and such that there exists a strict rule Conc(A1 ), . . . , Conc(An )   in Rs
and does not exist a strict rule a1 , . . . , ai   for {a1 , . . . , ai }  Conc({A1 , . . . , An })
in Rs .
Prem(A) = Prem(A1 )  . . .  Prem(An ),
Conc(A) = ,
Sub(A) = Sub(A1 )  . . .  Sub(An )  {A},
TopRule(A) = Conc(A1 ), . . . , Conc(An )  .
222

fiTwo Aspects of Relevance in Structured Argumentation

3. A1 , . . . , An   if A1 , . . . , An are arguments? such that there exists a defeasible rule
Conc(A1 ), . . . , Conc(An )   in Rd .
Prem(A) = Prem(A1 )  . . .  Prem(An ),
Conc(A) = ,
Sub(A) = Sub(A1 )  . . .  Sub(An )  {A},
TopRule(A) = Conc(A1 ), . . . , Conc(An )  .
Recall that each of the functions defined in this definition can also be defined for sets of
arguments? .
It is easy to see that our minimality constraint does not exclude the construction of
argument B2 in Example 8.1, as desired.
In order to show that non-minimal arguments? are not required to obtain the right
extensions, we first define the notions of a minimal and an extended version of an argument? .
Informally, a minimal version of an argument? A is an argument? A that is the same as
A except that in A any non-minimal strict rule in A is replaced with a minimal version of
the strict rule, i.e., a strict rule with the same consequent but a subset-minimal antecedent
set. This may lead to the deletion of subarguments? from A, namely, subarguments? for
the deleted antecedents. Conversely, an extended version of a minimal argument? A is
any argument that is the same as A except that in A zero or more strict rules in A are
replaced with non-minimal version of these strict rules. This may lead to the addition of
subarguments? , namely, subarguments? for the added antecedents.
Definition 8.2. [A and S  ] For any argument? A, an argument? A is a minimal
argument? of A iff:
 A  K and A = A; or

 A is of the form A1 , . . . , An   and A = A
i , . . . , Aj   such that:

 Conc({Ai , . . . , Aj }) is a minimal subset of Conc({A1 , . . . , An }) such that
Conc(Ai ), . . . , Conc(Aj )    Rs ; and
?
 For every k  {i, . . . , j} it holds that A
k is a minimal argument of Ak ;

or

 A is of the form A1 , . . . , An   and A = A
1 , . . . , An   such that for every

1  i  n it holds that Ai is a minimal argument? of Ai .

For a set of arguments? S, define S  as all minimal arguments? of S.
Below, when we write A , we mean that argument? A is a minimal argument? of A.
Note that A is not guaranteed to be unique. For example, argument? A : pq, qp  p
has two minimal variants, namely A1 : p  q  p and A2 : q  p  p.
Obviously, the following structured argumentation frameworks are ASPIC ? frameworks
that only contain arguments? .
Definition 8.3. [Minimal SAF , SAF  ] For a SAF = (A, C, ), let SAF  be the minimal
SAF with SAF  = (A , C  ,  ). Where C  is defined as C  (A  A ) and  =
(A  A ).
223

fiGrooters & Prakken

Definition 8.4. [Extended argument? , A+ ] For any argument? A, an argument? A+ is
an extended argument? of A iff:
 A  K and A+ = A; or
 A = A1 , . . . , An  , and A+ = A01 , . . . , A0m   such that m  n and for every A0i
(1  i  n) it holds that A0i is an extended argument? A+
i of Ai ; or
 A = A1 , . . . , An  , and A+ = A01 , . . . , A0n   such that for every A0i (1  i  n)
it holds that A0i is an extended argument? A+
i of Ai .
Below, whenever we write A+ , we mean that argument? A+ is an extended argument? of
A.
Note that A is also an A+ and that A is an A+ . In general, A+ is not unique.
The following example clarifies the definitions given above.
Example 8.3. Consider a SAF with the following arguments:
A1 : p
A3 : r

A2 : p  q
A4 : p, r  q

Then A1 , A2 and A3 are minimal arguments? , so SAF  contains these three arguments? .
A2 is the minimal argument? corresponding to A4 , so A2 = A
4 . Now, it is also easy to see
.
Furthermore
A
is
one
example
of
an
A+
that indeed A4 is an A+
4
2.
2
For some results that follow, it is needed that for any argument? A, A+ cannot be
strictly preferred in the argument ordering over A and that A cannot be strictly preferred
over A . This is not implied by the current definition of a reasonable argument? ordering.
This is illustrated with an example.
Example 8.4. Consider again Example 8.3, assume that p and r are in Kn and assume
the following argument? ordering (where x  y means as usual that x  y and y  x):
A1  A2 ; A3  A4 , A2  A4 . This argument? ordering satisfies all properties of a reasonable
argument? ordering but is nevertheless counterintuitive, since the only difference between
A2 and A4 is that A4 contains a non-minimal version of a strict rule applied in A2 : since
strict rules guarantee their consequent given their antecedent, A4 should intuitively not be
strictly preferred over A2 .
Therefore, we introduce the definition of a tolerable argument? ordering.
Definition 8.5. [Tolerable argument? ordering]  is a tolerable argument? ordering if
and only if:
 For every A+ of A, A+  A;
 For any A of A, A  A .
224

fiTwo Aspects of Relevance in Structured Argumentation

Intuitively, an argument? ordering is tolerable if replacing a minimal strict rule with
one of its non-minimal versions cannot make an argument? stronger and if replacing a
non-minimal strict rule with its minimal version cannot make an argument? weaker. For
strict inference rules, which are meant to capture deductively valid inferences, this is a very
natural property, since these operations always amount to adding, respectively, deleting
attackable subarguments? .
The next lemmas are all needed for proving the equivalence of the conclusions that are
drawn from minimal and non-minimal structured argumentation frameworks.
Lemma 8.1. For any argument? A and any extended argument? A+ the following holds:
for any A0  Sub(A) there is an argument? A00  Sub(A+ ) such that A00 = A0+ .
This lemma is clarified with the example below.
Example 8.5. Take the following arguments? :
A1 : p  p  q
B1 : p, r  p  q

A2 : A1  s
B2 : B1  s

?
0
Then it is obvious that B2 is an A+
2 . Lemma 8.1 states that for every subargument A of
+
0
0+
0
?
0
A2 there is a subargument B of A2 such that B = A . For example, take A to be A1 .
Then B1 is the subargument? of B2 such that B1 = A+
1.

The preceding result is needed for proving that if argument? A attacks/defeats B, then
the minimal argument? corresponding to A attacks/defeats every extended version of B.
Lemma 8.2. If  is a tolerable argument? ordering and argument? A defeats/attacks B,
then every A defeats/attacks every B + .
The following lemma follows from Lemma 8.2.
Lemma 8.3. If  is a tolerable argument? ordering, then for all complete extensions E:
1. If A  E, then A  E for every A ;
2. If B 
/ E, then B + 
/ E for all B + .
The following lemma states that Dungs characteristic function (see Definition 2.1) is a
monotonic bijection from all complete extensions of a SAF  onto all complete extensions
of SAF . This lemma is based on the results of Dung, Toni, and Mancarella (2010) for the
ABA framework, which will be discussed below. First, the following definition is needed.
Definition 8.6. [S  ] For a set of arguments S, let S  be the set of arguments in S such
that each argument is minimal.
Lemma 8.4. Let SAF  = (A , C  ,  ) be the minimal structured argumentation framework corresponding to SAF = (A, C, ) (for the ASPIC ? framework), and let AF be the
abstract argumentation framework corresponding to SAF . Let  be a tolerable argument?
ordering. Also, let C and C  be the sets of complete extensions of SAF and SAF  respectively and let FAF be Dungs characteristic function of AF . Then:
225

fiGrooters & Prakken

1. For each E  C  : FAF (E) = E.
2. For each E  C : FAF (E  ) = E and E   C  .
Clause (1) says that the set of acceptable minimal arguments? w.r.t. an extension of a
minimal SAF does not change if also non-minimal arguments? are considered. Clause (2)
says that the set of acceptable arguments? w.r.t. an extension of a non-minimal SAF does
not change if only minimal arguments? are considered.
Below, the main result is stated for the conclusions that can be drawn from SAF s and
SAF  s. This theorem is also based on the results of Dung et al. (2010) for the ABA
framework.
Theorem 8.5. Let SAF  = (A , C  ,  ) be the minimal structured argumentation framework corresponding to SAF = (A, C, ) (for the ASPIC ? framework). Let  be a tolerable
argument? ordering. Take T  {complete, grounded, preferred, stable} and F as defined in
Definition 2.1, then:
1. Let E be a T extension in SAF , then E  is a T extension in SAF  .
2. Let E be a T extension in SAF  , then F (E) is a T extension in SAF .
By combining all these results it can be concluded that the conclusions that can be drawn
from an ASPIC ? structured argumentation framework are not affected in case arguments?
are required to be minimal.
Our results generalise those of Modgil and Prakken (2013), who prove the same result for
the special case of arguments with minimal sets of premises (their Proposition 28). Other
related work is Dung et al. (2010), which study minimal arguments for assumption-based
argumentation (ABA) as reconstructed by Dung et al. (2007) in terms of Dungs (1995)
abstract argumentation frameworks. As mentioned above, this version of ABA can be
reconstructed as a special case of the ASPIC + framework without preferences and defeasible
rules. In fact, Dung et al. (2010) define the notion of a non-redundant argument, which is
more general than the notion of a minimal argument defined in this paper. A non-redundant
argument is in turn defined in terms of a less redundant relation. Then Dung et al. (2007)
prove similar results for ABA as our Theorem 8.5 for ASPIC ? . Our above constructions and
proofs are clearly inspired by the work of Dung et al. (2010). However, a purely formal link
with ASPIC ? cannot be established, since unlike ABA, ASPIC ? does not allow chaining of
strict rules. For this reason we will not make a detailed formal comparison here.
8.3 Disallowing Repetition of Conclusions
We now address a second aspect of minimality, by studying a modification of ASPIC ? in
which circular arguments? are avoided, that is, in which an argument? cannot have the
same conclusion as one of its subarguments? . This requirement is not part of ASPIC + as
defined by Prakken (2010) and Modgil and Prakken (2013) but it is part of the system
of Vreeswijk (1997), from which the ASPIC -style definition of an argument originates. In
argumentation theory, circular arguments are generally regarded as fallacious, so it seems a
good idea to exclude them. In addition, this has computational benefits, as will be shown
in this section. First Definition 6.2 of an argument? is modified as follows.
226

fiTwo Aspects of Relevance in Structured Argumentation

Definition 8.7. [Strongly minimal arguments? in ASPIC ? ] Strongly minimal arguments?
are defined as arguments? in Definition 8.1 except that the following condition is added to
clauses (2) and (3):
 6 Conc{(A1 , . . . , An )}.
Below we call a structured argumentation framework strongly minimal if its set of
arguments? is defined with Definition 8.7.
Disallowing repetition of conclusions can in general change the extensions, as the following example shows.
Example 8.6. Consider an argumentation theory with Kn = Kp =  and Rd = { p, 
p, p  p}. Then there are at least two arguments? for p and at least one for p:
A1 :
A2 :
B:

p
A1  p
 p

If we have an ordering on rules Rd such that  p < p < p  p (where x < y means
that y is strictly preferred over x), then with the last-link ordering as defined by Modgil and
Prakken (2013), B defeats A1 while A2 defeats B, which yields a grounded extension that
contains neither of these three arguments? . However, if A2 cannot be constructed, then the
grounded extension contains B.
As just noted, excluding circular arguments? not only avoids fallacies but also has computational benefits. In particular, it can be shown that if K and Rd are finite, then each
argument? has at most a finite number of attackers. In the words of Dung (1995) this
means that the induced abstract argumentation framework is finitary. As shown by Dung,
finitary AFs have a number of computational benefits. Among other things, the grounded
extension can be constructed by iterative application of the F operator (see Definition 2.1
above) on the empty set.
To prove this result, we first introduce some notation relative to a given AT .
 For any r which is defined as S    Rs or as S    Rd let Cons(r) = .
 For any set T  R let Cons(T ) = { |  = Cons(r) for some r  T }.
 Let X = {S    Rs | S  K  Cons(Rd )}. Informally, X is the set of all strict rules
that are potentially applicable in the AT, that is, of which the antecedents all belong
to K or are a consequent of a defeasible rule. Note that this set does not have to be
equal to Rs , since it might be that all rules in Rs apply to some set of well-formed
formulas from L while yet  
/ Kand there is no defeasible rule with consequent .
 For any Y  L define X Y = {S    X |   Y }. Informally, X Y is the set of all
potentially applicable rules for a formula in Y .
We next prove the following lemma.
Lemma 8.6. For any finite Y  L it holds that X Y is finite.
227

fiGrooters & Prakken

Then the following result can be proven.
Theorem 8.7. Let SAF = (A, C, ) be a strongly minimal structured argumentation
framework corresponding to an argumentation theory with a finite K and Rd . Then for any
  L the set {A  A | Conc(A) = } is finite.
Note that this result cannot be proved without the exclusion of arguments that chain
strict rules, since otherwise infinite sets of arguments for the same conclusion p can be
generated by constructing arguments as follows for any n, which provides a counterexample
to Theorem 8.7 in case such arguments are not excluded:
A1 = p
A2 = A1  p
A3 = A2  p
...
An = An1  p
It remains to verify the rationality postulates for strongly minimal SAFs. It turns out that
the only result of Section 7 that needs to be reproved is the only-if half of Theorem 7.6.
Theorem 8.8. Each strongly minimal ASPIC ? SAF with `W is compact.
Combined with the other results of Section 7 this implies that
Theorem 8.9. Each strongly minimal ASPIC ? SAF with `W which is axiom consistent?
and has a reasonable argument? ordering satisfies the closure? and consistency? postulates.

9. ASPIC ? as a Generalisation of Classical Argumentation
In this section we explain that by combining the two main contributions so far, a class of
instantiations of ASPIC ? is obtained that is a proper generalisation in three respects of
two versions of classical argumentation defined by Besnard and Hunter (2008) and Gorogiannis and Hunter (2011)5 . The two versions for which this holds are of particular interest
since they are by Gorogiannis and Hunter proven to be the only two of seven versions of
classical argumentation that satisfy the consistency postulates. The observation can be
explained as follows. Modgil and Prakken (2013) prove that classical argumentation with
two forms of premise attack called direct undercut and direct defeat can be reconstructed as
the following class of instantiations of ASPIC + : no defeasible rules, no preference relations
between arguments, only ordinary premises, L has a negation symbol as defined above in
Definition 2.3, all arguments have indirectly consistent premise sets, and the strict rules
are instantiated with all classically valid inferences from finite sets of premises. Modgil
and Prakken (2013) also prove that requiring arguments to have subset-minimal premises
implying their conclusion does not change this result. Then it should be shown that for
such classical-logic instantiations the prohibition in ASPIC ? to chain strict rules does not
make a difference with ASPIC + either. If we can show this, then we have shown that
instantiations of ASPIC + in which Rs corresponds to Rescher and Manors (1970) notion
5. We thank Sanjay Modgil for suggesting this to us in personal communication

228

fiTwo Aspects of Relevance in Structured Argumentation

of weak consequence and in which all arguments? are minimal are proper generalisations of
classical argumentation. We will actually prove this for the case with nontrivial preferences
but where the preferences are fully determined by the premises; then the result for the case
with an empty preference relation follows as a special case.
Below with a minimal ASPIC + or ASPIC ? argumentation theory we mean an argumentation theory where all arguments or all arguments? have a subset-minimal set of premises.
Clearly, for arguments? this means that they are also minimal in the sense of Definition 8.1.
Moreover, a c-structured argumentation framework is a notion from Modgil and Prakken
(2013): restricted to the present context it amounts to the requirement on SAFs that all
arguments have classically consistent premises. The notion of a minimal c-structured argumentation framework is defined accordingly.
Theorem 9.1. Let AS be an argumentation system with L a classical-logic language and
Rs corresponding to classical logic, and let K = Kp be a knowledge base in L. Let AS 
be obtained from AS by removing from Rs all inference rules that are invalid according to
`W . Let AT = (AS, K) be a minimal ASPIC + argumentation theory and AT  = (AS  , K)
the corresponding minimal ASPIC ? argumentation theory. Then let SAF = (A, C, ) and
SAF  = (A , C  ,  ) be, respectively, the minimal c-structured argumentation framework
defined by AT in ASPIC + and the minimal structured argumentation framework defined
by AT  in ASPIC ? such that:
 For all A, B, C  A: if Prem(A) = Prem(B) then (C  A iff C  B and A  C iff
B  C). Likewise for  .
 For all A , B   A : A  B  iff A  B  .6
Then for T  {complete, grounded, preferred, stable} it holds that:
1. Let E be a T extension in SAF , then E  A is a T extension in SAF  such that
Conc(E) = Conc(E  A ).
2. Let E be a T extension in SAF  , then F (E) is a T extension in SAF such that
Conc(E) = Conc(F (E)).
A special case of this result is that for the case without defeasible rules, necessary
premises and preferences ASPIC ? with `W gives the same conclusion sets as classical argumentation. Then the first proper extension of classical argumentation is with preferences.
The second proper extension is with the necessary premises Kn while the third proper extension is to the case with defeasible rules, by observing that in ASPIC ? any strict rule that
is applied to the conclusion of at least one defeasible subargument? is applied to a subsetminimal and classically consistent set of formulas that classically implies the consequent of
the strict rule.

10. Summary, Discussion and Conclusion
In this section we summarise and discuss our results and put them in the context of related
work.
6. This is well-defined since by construction of SAF and SAF  and the fact that `W draws no inferences
from inconsistent sets it holds that A  A.

229

fiGrooters & Prakken

10.1 Summary of the Results
In this paper we tackled several related issues concerning relevance in structured argumentation. We carried out our investigations in the context of the ASPIC + framework, which
consolidates and extends one of the main AI approaches to argumentation: modelling combined reasoning with strict and defeasible inference rules. One main contribution of this
paper was to solve the long-standing trivialisation problem first identified by Pollock (1994,
1995). The problem is to tame the trivialising effect of the Ex Falso principle in classical
logic in a way that preserves consistency and closure properties. To solve this problem, we
instantiated the strict rules of ASPIC + with Rescher and Manors (1970) paraconsistent
logic W. To make this work, we had to disallow chaining of strict rules in arguments since
the logic W does not satisfy the Cut rule; this resulted in the adapted framework ASPIC ?
and in a new view on the postulate of strict closure. We argued that what is important is
not whether conclusion sets are closed under strict rules but whether they are closed under
the consequence notion of the logic to which the strict rules correspond. Accordingly, we
modified the notion of strict closure and we introduced a new rationality postulate of logical
closure. We then proved new versions of the consistency and closure postulates for ASPIC ? .
We also investigated whether two other well-known paraconsistent logics, the system C of
Da Costa (1974) and the Logic of Paradox of Priest (1979, 1989), are suitable sources of
strict rules. We showed that in both cases this would lead to violation of indirect consistency. In future research we want to consider other paraconsistent logics and we want to
more generally study the properties that a paraconsistent logic should satisfy to be useful
as a source for strict rules in ASPIC ? .
The second issue studied in this paper was minimality of arguments. We first showed
that under a natural assumption on the argument ordering, restricting strict-rule application to subset-minimal sets of formulas does not affect the conclusions drawn in ASPIC ? .
This will in many cases make the reasoning more efficient by ignoring irrelevant information. We also noted that this result can be easily adapted to ASPIC + . We then disallowed
circular arguments in ASPIC ? and showed that this may change the status of arguments
but does not affect the satisfaction of the rationality postulates. In addition, we proved that
with a finite set of defeasible rules and a finite knowledge base ASPIC ? without circular
arguments has the computationally attractive property that the induced abstract argumentation frameworks are finitary in the sense of Dung (1995). This latter result cannot be
adapted to ASPIC + since it crucially relies on the prohibition to chain strict rules. All
these results on minimality hold independently of the choice of strict rules.
Finally, we proved that by combining the contributions of this paper a version of ASPIC ?
is obtained that is a proper generalisation of two versions of classical argumentation with
premise attack defined by Besnard and Hunter (2008) and Gorogiannis and Hunter (2011).
These two versions are of particular interest since they are by Gorogiannis and Hunter
proven to be the only two of seven versions of classical argumentation that satisfy the
consistency postulates.
It should be noted that in the course of our investigations we have changed the ASPIC +
framework as originally defined by Prakken (2010). In fact, we are not the first to do so.
Modgil and Prakken (2013) consider four variants of the ASPIC + framework. First, they
consider versions with and without the constraint that arguments have consistent premises
230

fiTwo Aspects of Relevance in Structured Argumentation

and then for both of these variants they define a variant in which conflict-freeness of sets of
arguments is not defined relative to the defeat relation but to the attack relation. Furthermore, Caminada et al. (2014) study a variant of ASPIC + in which arguments can also be
rebutted on conclusions derived by strict rules, provided that at least one subargument has
an ordinary premise or applies a defeasible rule. Finally, Wu and Podlaszewski (2015) study
a variant of ASPIC + in which the set of all conclusions of all subarguments of an argument
has to be consistent. Thus the work on ASPIC + has resulted in a family of frameworks
based on the same core ideas but making different design choices at specific points, and new
members of this family may result in the future. We think that this is a healthy situation,
since it amounts to a systematic investigation of the effects of different design choices within
a common approach, which each may be applicable to certain kinds of problems.
10.2 Discussion of Related Work
We next discuss related work.
10.2.1 The Additional Rationality Postulates of Caminada et al. (2012)
As mentioned in the introduction, Caminada et al. (2012) formulate a new set of rationality
postulates in addition to those of Caminada and Amgoud (2007), to characterise cases under
which the trivialisation problem is avoided (called the postulates of non-interference and
crash-resistance). Wu (2012) and Wu and Podlaszewski (2015) prove for their adaptation
of ASPIC + with consistent arguments and no preferences that these new postulates are
satisfied for complete semantics. They did not investigate other semantics. However, we
will not attempt to prove Caminada et al.s postulates, for two reasons. First, we want to
obtain results for the case with preferences and for other semantics as well and, second, it
seems to us that Caminada et al.s postulates in fact capture a stronger intuitive notion
than the one we study in this paper, so that proving satisfaction of the new postulates
would be more than required for the purposes of this paper.
The intuition of Caminada et al.s notion of trivialisation is as follows. They consider
knowledge bases, which are pairs of sets of formulas from L and sets of defeasible rules.
Two knowledge bases are defined to be disjoint if they are composed from disjoint sets of
atomic formulas from L. Then a knowledge base KB1 = (K1 , D1 ) is contaminating if for
every knowledge base KB2 = (K2 , D2 ) the set of extensions (under a given semantics) is
the same for KB1 as for KB1  KB2 (where (K1 , D1 )  (K2 , D2 ) = (K1  K2 , D1  D2 ).
Then a system is said to satisfy crash resistance if there does not exist a contaminating
knowledge base in the system.
Now consider an ASPIC + or ASPIC ? instantiation with Kp = , Kn = {p} and a single
defeasible rule  d, where n( d) = d. This instantiation has no stable extensions, since
the argument A = d defeats itself and is not defeated by any other argument. Then
the knowledge base (, { d}) is contaminating, since no syntactically disjoint knowledge
base to which it is added will have extensions. More generally, this situation can arise with
any knowledge base with which there are no stable extensions.
However, in this paper we were not interested in excluding such situations but only
in taming the contaminating effect of the Ex Falso principle. To this end we introduced
a simpler definition of trivialisation in Definition 4.1 and managed to avoid trivialisation
231

fiGrooters & Prakken

as thus defined even for stable semantics, since the just-given example is not a case of
trivialisation in the sense of Definition 4.1. We conclude from this that Caminada et al.s
postulates capture a stronger notion than the notion of paraconsistency (the focus of our
paper). In the future it would be interesting to study whether our class of instantiations
of ASPIC ? satisfies Caminada et al.s postulates, but this would ideally be preceded by a
study of what exactly is captured by these postulates.
10.2.2 Wu (2012), Wu and Podlaszewski (2015)
In an alternative attempt to solve the trivialisation problem, Wu and Podlaszewski (2015)
introduced the inconsistency-cleaned ASP IC Lite system. This system is similar to the
argumentation formalism treated by Caminada and Amgoud (2007) and can be seen as a
system specified in ASPIC + in which all arguments are equally preferred. Wu and Podlaszewski define an argument to be consistent if the set of conclusions of all its subarguments
is directly consistent. An argumentation framework is inconsistency-cleaned if all inconsistent arguments are removed. Wu and Podlaszewski prove that an inconsistency-cleaned
version of the ASP IC Lite satisfies both the original rationality postulates of Caminada
and Amgoud (2007) and the new postulates of Caminada et al. (2012). So it solves the
trivialisation problem while retaining known results on closure and consistency. However,
Wu (2012) and Wu and Podlaszewski (2015) provide a counterexample (originally due to
Leon van der Torre) to satisfaction of the consistency and strict-closure postulates in case
preferences are added and the last-link argument ordering of Prakken (2010) is applied.
This example was originally presented in the ASP IC Lite system, but here it is translated
into the ASPIC + framework.
Example 10.1. [(Wu, 2012; Wu & Podlaszewski, 2015)] Given the knowledge base K = ,
Rd = { p; p  q;  p  q} and Rs is instantiated with all valid inferences in classical
logic. Assume that  p has priority 1 (lowest),  p  q has priority 2 (middle) and
p  q has priority 3 (highest). In that case, we can construct the following arguments with
associated (last-link principle) preferences. Table 1 depicts some arguments that can be
generated7 and Figure 6 shows the defeat relation between these arguments.
Argument
A1 :  p
A2 :  p  q
A3 : A1  q
A4 : A1 , A2  q
A5 : A1 , A3  (p  q)
A6 : A2 , A3  p

Preference
(1)
(2)
(3)
(1)
(1)
(2)

Table 1: Six arguments

Figure 6: Partial abstract AF

7. Note that classical reasoning allows the generation of infinitely more arguments but they are all irrelevant
to any of these six arguments.

232

fiTwo Aspects of Relevance in Structured Argumentation

Argument A6 is an inconsistent argument. So according to the solution proposed by
Wu and Podlaszewski for the case without preferences, A6 needs to be deleted from the
argumentation framework. Figure 7 shows the resulting argumentation framework.

Figure 7: Inconsistency-cleaned version
There is a complete extension E = {A1 , A2 , A3 , A4 , A5 }. It does not satisfy closure under
strict rules because A2 and A3 are in E and A6 is not in E. Moreover, direct consistency is
also not satisfied since A3 and A4 are both in the complete extension E, but the conclusions
q and p  q are not consistent.
Arguments A3 and A4 have opposite conclusions so without preferences A4 would defeat
A3 . However, with the preference ordering chosen in the counterexample, A4 is weaker than
A3 so A4 cannot defeat A3 . A3 and A4 are also not being attacked on their subarguments.
The fact that these arguments are both in the same complete extension causes the problem.
Every argument that concludes with p uses A1 as a subargument, so this implies that
it is an inconsistent argument and it has to be removed from the framework. Therefore,
A1 is not defeated by any argument, which means that A3 and A4 are both in a complete
extension. It can be concluded that an inconsistent argument like A6 is really needed to
defeat A3 and A4 .
Furthermore, it can be observed that if A6 is not deleted, there are no problems at all.
Figure 6 shows that in that case there is only one complete extension E = {A2 }. and it
satisfies both consistency and strict closure. Therefore, in this example, it is undesirable
that A6 is removed.
Consider next the ASPIC ? framework as defined in this paper. The same arguments can
be constructed as in the original framework. This way, there is only one complete extension
{A2 } and, as explained above, the rationality postulates are all satisfied. The approach
with ASPIC ? is therefore more general than the solution of Wu and Podlaszewski (2015),
since it applies to frameworks that include preferences and defeasible rules.
It should be noted that our idea to forbid chaining of strict rules was earlier suggested to
us by Martin Caminada (personal communication). However, he combined his suggestion
with the idea to disallow inconsistent arguments; in that case, the above counterexample to
consistency and strict closure can still be constructed. Apart from this, one could say that
the logic W , which does not satisfy the Cut rule, provides a theoretical foundation for the
idea to disallow chaining of strict rules.
At first sight, it would seem that a system that allows inconsistent arguments is flawed
even if it satisfies the consistency and closure postulates. However, note that such arguments
will never be in any extension. Although as explained above and by Caminada (2005),
inconsistent arguments can sometimes prevent other arguments from being in an extension,
this is only a problem if such arguments are based on the Ex Falso principle, since that
233

fiGrooters & Prakken

principle holds as a matter of logic. Among other things, this means that allowing arguments
based on Ex Falso would dramatically increase the number of (counter)arguments, which
would lead to computational problems. By contrast, inconsistent arguments as argument
A6 in Example 10.1 arise because of specific modeling choices in Rd not dictated by logic
and they do not proliferate, so from a logical or computational point of view there is no
need to exclude them.
10.3 Dungs (2014) Rule-Based Systems
Dung (2014) introduces a formalism of rule-based systems (further studied in Dung, 2016),
which essentially is a notational variant of ASPIC + restricted to literal languages and empty
knowledge bases (necessary or ordinary facts are represented as strict or defeasible rules with
empty antecedents). Dung introduces three new rationality postulates. His postulate for
attack monotonicity informally says that strengthening an argument cannot eliminate an
attack of that argument on another. His postulate of credulous cumulativity informally
means that changing a conclusion of an argument in some extension to a necessary fact
cannot eliminate that extension. Finally, his property of irrelevance of redundant defaults
says that adding redundant defaults should not change the set of extensions.
Dung then investigates the argument orderings studied by Modgil and Prakken (2013) on
whether they satisfy these and the consistency postulates, with both positive and negative
results. While these results are valuable, Dung (2014) unfortunately, somewhat confuses
matters by referring to the orderings studied by Modgil and Prakken as the ASPIC +
semantics. Thus he overlooks the distinction between ASPIC + as a general framework and
instantiations of the framework. The argument orderings studied by Modgil and Prakken are
not inherent to the ASPIC + framework but are just some example orderings. The ASPIC +
framework and its variants leave every room for other ways to define the argument ordering.
It should be noted that Dung (2016) does not refer to Modgil and Prakkens orderings as
the ASPIC + semantics any more and thus respects that these orderings are not inherent
to ASPIC + .
For present purposes Dungs findings are strictly speaking irrelevant since we have not
studied particular argument orderings. In future research it would be interesting to investigate whether the use of particular argument orderings in ASPIC ? satisfies Dungs postulates
of attack monotonicity and irrelevance of redundant defaults. However, we disagree with
Dung (2014) that credulous cumulativity would be a desirable property. On the contrary,
with Prakken and Vreeswijk (2002, section 4.4) we believe that this property is instead
undesirable, since strengthening a defeasible conclusion to an indisputable fact may make
arguments stronger than before. This can give them the power to defeat other arguments
that they did not have before. This may well result in the loss of the extension from which
the conclusion was promoted to an indisputable fact.
10.4 Conclusion
In this paper we have successfully addressed some open issues concerning relevance in structured argumentation. We have solved the trivialisation problems that arise when argumentation includes full classical logic and we have created the prospects for reducing computational complexity by enforcing minimality and non-circularity of arguments while ensuring
234

fiTwo Aspects of Relevance in Structured Argumentation

closure and consistency results. All this was done in the context of the ASPIC approach,
resulting in a new variant of the ASPIC + framework called ASPIC ? and a well-behaved
class of instantiations of the new framework. This class of instantiations was shown to be a
proper generalisation of classical argumentation with preferences and defeasible rules. It is
this class of instantiations and its properties that are the main contribution of this paper.
In our paper we have employed a flexible attitude towards design choices within the ASPIC
approach. We have thus shown that this approach is a fruitful one, provided the distinction
between frameworks and their instantiations is kept in mind.

Acknowledgement
We thank all three JAIR reviewers for their many useful comments on earlier versions of
our paper.

Appendix A. Proofs
This appendix contains the proofs of all results reported in the paper.
A.1 Proofs for Section 7
Theorem 7.1 No ASPIC ? SAFs with `W is defined by an argumentation theory with a
trivialising argumentation system.
Proof. To this end, we must show that for any argumentation system AS with a propositional language and with Rs as just defined, a knowledge base K in AS can be defined
such that for some {, }  K and for some   L it does not hold that an argument?
with conclusion  can be constructed on the basis of K in AS. Consider any such AS.
We choose K = Kn  Kp where Kp =  and Kn = {, } for any formula  from L ( is
guaranteed to exist since L is assumed nonempty). Then by definition of `W it clearly holds
that K 6`W (  ) since no consistent subset of K can classically imply a contradiction.
So there exists no strict argument? for (  ) on the basis of K in AS.
Theorem 7.3 Let E be a complete extension, then GN (E) = E.
Proof. First note that according to Definition 7.2 for each set S of arguments? Sub(S) 
GN (S), therefore E  GN (E).
Suppose now that an argument? C defeats an argument? A  GN (E). Let BA be a base of
A such that BA  Sub(E), then C defeats BA. Hence C defeats Sub(E) and so it defeats
E. Since E is a complete extension, every defeat against E is counter defeated by E. A is
defended by E, so A  E. Therefore GN (E)  E.
Theorem 7.5 Each compact ASPIC ? SAF satisfies the closure? under strict rules postulate.
Proof. Let E be a complete extension. The compactness? implies that GN (E) is closed?
under strict rules. From Theorem 7.3 E is closed? under strict rules.
Theorem 7.6 Each ASPIC ? SAF with `W is compact.
235

fiGrooters & Prakken

Proof.
? (GN (S)# )]
[Conc(GN (S))  ClR
s
? (GN (S)# ). It needs to be shown that  
Let S be a set of arguments? and   ClR
s
? (X).
Conc(GN (S)). Let X be a minimal subset of Conc(GN (S)# ) such that   ClR
s
?
Hence there is a strict argument A0 over X with conclusion . Further let SX be a minimal set of arguments? from GN (S)# s.t. Conc(SX ) = X. Let A be the argument? obtained
by replacing each leaf in A0 (viewed as a directed acyclic graph) labelled by a literal 
from X by an argument? with conclusion  from SX . Note that this is possible since all
arguments? in SX are basic fallible arguments? or are just necessary premises. It is obvious that the conclusion of A is . It is shown that SX is a base of A. Suppose B is an
argument? defeating A. Since A0 is a strict argument? over X, B must defeat a basic fallible
subargument? in SX . Hence B defeats SX . Thus A  GN (S). Hence   Conc(GN (S)).
? (GN (S)# )]
[Conc(GN (S))  ClR
s
? (GN (S)# ).  
Suppose   Conc(GN (S)), then it has to be shown that   ClR
s
?
Conc(GN (S)) means that there is an argument A  GN (S) with Conc(A) = .
Suppose A is of the form   or   Kp , then A  F A? (GN (S)) and thus A  GN (S)# .
? () or   N P ? (GN (S)) respecSuppose A is of the form   or   Kn , then   ClR
s
? (GN (S)# ).
tively, so   ClR
s
Suppose A is of the form A1 , . . . , An  , then A  F A? (GN (S)) and thus A  GN (S)# .
Finally, suppose A is of the form A1 , . . . , An  , then since A1 , . . . , An are basic fallible
? (GN (S)# ).
arguments? A1 , . . . , An  GN (S)# . Therefore   ClR
s
? (GN (S)# ).
It can be concluded that   ClR
s
It is proven that the AF is compact.
Theorem 7.7 Each cohesive ASPIC ? SAF satisfies the consistency? postulate.
Proof. Let E be a complete extension. Suppose E is inconsistent? . From cohesion, it
follows that GN (E) is conflicting. Theorem 7.3 states that then E is conflicting. This is a
contradiction since E is a complete extension, so E has to be consistent? .
Theorem 7.8 If a compact, axiom consistent? ASPIC ? SAF has a reasonable argument?
ordering and satisfies the self-contradiction axiom, then SAF is cohesive.
Proof. Let S be an inconsistent? set of arguments? and take a minimal inconsistent? subset
S 0 of Sub(S). Definition 6.4 combined with axiom consistency? and the minimality of S 0
causes that S 0 6=  and only contains basic fallible arguments? or necessary premises. Remark that S 0 cannot consist of only necessary premises, because of axiom consistency? .
Further note that Conc(S 0 ) is a minimal inconsistent set. Since AF satisfies the self? (Conc(S 0 )). Let B
contradiction axiom, for all   Conc(S 0 ) it holds that   ClR
s
?
0
be the weakest argument of S with Conc(B) = . Note that B cannot be a necessary premise because of the reasonable argument? ordering and the fact that S 0 must
contain basic fallible arguments? . By construction of S 0 it holds that S 0  GN (S 0 )# .
? (Conc(GN (S 0 )# )). Because of the compactness of AF it follows
Therefore   ClR
s
that   Conc(GN (S 0 )). Therefore, there is an argument? A  GN (S 0 ) such that
Conc(A) = . Hence A attacks B. The base of A is S 0 , so it can be concluded that all basic
fallible subarguments? of A are in S 0 . B is the weakest argument? of S 0 , so because of the
236

fiTwo Aspects of Relevance in Structured Argumentation

reasonable argument? ordering and the fact that all basic fallible subarguments? of A are in
S 0 implies that A  B. This means that A defeats B. Since B  S 0  GN (S 0 )  GN (S),
GN (S) is conflicting. Therefore, AF is cohesive.
Theorem 7.9 Each ASPIC ? SAF with `W satisfies the self-contradiction axiom.
Proof. It has to be proved that for every minimal inconsistent? set X  L it holds that for
? (X). Let X be a minimally inconsistent? set and take S = X\.
each   X,   ClR
s
Note that S is a maximal consistent? subset of X and that S,  `  (where ` denotes
classical entailment). By the deduction theorem for classical logic S `   , which implies
S ` . Since S is a maximal consistent? subset of X, X `W . This holds for every
? (X). It can be concluded that AF satisfies the self-contradiction
  X, so   ClR
s
axiom.
Lemma 7.11 Each ASPIC ? SAF with `W satisfies the following property: for any set S
of arguments? it holds that if Conc(S) `CL , then Conc(S # ) `CL .
Proof. Suppose that Conc(S) `CL . Consider any argument? Ti  i in S \ S # . By
definition of `W and choice of Rs it holds that Conc(Ti ) `CL i for any i. Then since `CL
satisfies the Cut rule, we have that Conc(S # ) `CL .
Lemma 7.12 Each ASPIC ? SAF with `W satisfies the following property: for any set E
of arguments? it holds that if Conc(E) is strictly closed and consistent , then Conc(E) is
classically consistent.
Proof. Assume that Conc(E) is strictly closed and consistent and suppose for contradiction that Conc(E) `CL , . Consider any subset-minimal S  E such that Conc(S) `CL
, . Then by Lemma 7.11 we have that Conc(S # ) `CL , . Consider any minimal T  S # such that Conc(T ) `CL , . Note that T cannot be empty. Then for any
  Conc(T ) it holds that Conc(T )\{} `CL . By choice of T it holds that Conc(T )\{}
is classically consistent. But then since Conc(E) is strictly closed and no argument? in S #
has a strict top rule, there exists an argument? T 0   in E for some T 0  T such that
Conc(T 0 ) = Conc(T ) \ {}. But then Conc(E) is not consistent .
Theorem 7.13 Each ASPIC ? SAF with `W which is axiom consistent? and has a reasonable argument ordering satisfies the logical closure postulate.
Proof. Suppose Conc(E) `W  for some complete extension E and consider any subsetminimal S  E such that Conc(S) `W . Then by definition of `W and choice of Rs
it holds that Conc(S) is classically consistent and Conc(S) `CL . Then by Lemma 7.11
we have that Conc(S # ) `CL . Moreover, Conc(S # ) is classically consistent because of
Lemma 7.12 and the fact that any subset of a classically consistent set is also classically
consistent (note that by Theorem 7.10 Conc(E) is strictly closed and consistent , so the
conditions of Lemma 7.12 are fulfilled). But then Conc(S # ) `W  and by choice of Rs
there exists a strict rule Conc(S # )  . Since no argument? in S # has a strict top rule,
  Conc(E) by strict closure? of E.
237

fiGrooters & Prakken

A.2 Proofs for Section 8.2
Lemma 8.1 For any argument? A and any extended argument? A+ the following holds:
for any A0  Sub(A) there is an argument? A00  Sub(A+ ) such that A00 = A0+ .
Proof. This proof is a proof by induction on the height of argument? A (viewed as a directed
acyclic graph). Suppose A is an element of K (so the height is 1), then A0 and A+ have to
be equal to A. This means that A00 is also equal to A and it is easy to see that A00 = A0+ .
Suppose that the lemma holds for all arguments? of height i for an i  {1, 2, . . .}. Now it has
to be proven for arguments? of height i + 1. Take an arbitrary argument? A of height i + 1
and take a subargument? A0 of A. Note that A cannot be an element of K since the height
is greater than 1. Therefore, A has to be of the form A1 , . . . , An  /  . Then there are
two possibilities: either (i) A0 is a subargument? of one of the arguments? A1 , . . . , An , or
(ii) A0 is equal to A.
(i). There is a j  {1, . . . , n} such that A0 is a subargument? of Aj . Aj has a height of i, so
00
0+
00
there must be an A00  Sub(A+
j ) such that A = A . According to Definition 8.4, A is a
?
+
subargument of A .
(ii). A0 is equal to A. Take A00 to be A+ , then it follows that A00 = A0+ and A00  Sub(A+ ).
Now it is proved that for every argument? the lemma holds.
Lemma 8.2 If  is a tolerable argument? ordering and argument? A defeats/attacks B,
then every A defeats/attacks every B + .
Proof.
Attack
(i). Suppose that A undercuts B, so Conc(A) = n(r) for a defeasible top rule r of a
B 0  Sub(B). By definition of A , the conclusion for every A is the same as for A. By
Lemma 8.1 it holds that for every B + , there is a subargument? B 00  Sub(B + ) such that
B 00 = B 0+ . Note that B 0 and B 00 both have the same defeasible top rule, since by definition
of B + , only the strict rules can mutate. Then it follows that every A undercuts B + on
B 00 .
(ii). Suppose now that A undermines B, so Conc(A) is the ordinary premise B 0 . Every
B + also has this ordinary premise. By definition of A (Definition 8.2), the conclusion of
every A is the same as for A. Therefore any A undermines every B + on B 0 .
(iii). Suppose that A rebuts B, so Conc(A) is the conclusion of some basic fallible
argument? B 0  Sub(B). The conclusion for every A is the same as for A. By Lemma 8.1,
it holds that for every B + , there is a subargument? B 00  Sub(B + ) such that B 00 = B 0+ .
Note that the conclusions of B 0 and B 00 are the same and that B 00 also has a defeasible top
rule. Therefore any A rebuts B + on B 00 .
Defeat
Suppose argument? A defeats B. This means that A attacks B on a subargument? B 0 .
Then (i) A undercuts B, or (ii) A  B 0 .
(i). Suppose that A undercuts B, then it follows that every A undercuts B + on B 00 (see
reasoning above for attack (i)). This implies that any A defeats every B + .
(ii). Otherwise it has to be the case that A  B 0 . Note that, because of the tolerable
argument? ordering, A  A. In the proof for attack it is shown that every A attacks
238

fiTwo Aspects of Relevance in Structured Argumentation

every B + on B 00 , where B 00 = B 0+ . The tolerable argument? ordering causes B 00  B 0 .
Therefore for every A and B 00 it holds that A  B 00 , so every A defeats every B + .
Lemma 8.3 If  is a tolerable argument? ordering, then for all complete extensions E:
1. If A  E, then A  E for every A ;
2. If B 
/ E, then B + 
/ E for all B + .
Proof. (1). Suppose A  E and let B be an argument? defeating an A . Then by Lemma
8.2, B defeats A. Therefore, there must be an argument? C  E such that C defeats B.
Hence, A is defended by E and thus A  E.
(2). Suppose B 
/ E. Then there exists an argument? A such that A defeats B and there
does not exist a C  E that defeats A. According to Lemma 8.2, A defeats every B + , so
B + is not defended by E and hence B + 
/ E.
Lemma 8.4 Let SAF  = (A , C  ,  ) be the minimal structured argumentation framework corresponding to SAF = (A, C, ) (for the ASPIC ? framework), and let AF be the
abstract argumentation framework corresponding to SAF . Let  be a tolerable argument?
ordering. Also, let C and C  be the sets of complete extensions of SAF and SAF  respectively and let FAF be Dungs characteristic function of AF . Then:
1. For each E  C  : FAF (E) = E.
2. For each E  C : FAF (E  ) = E and E   C  .
Proof. Take two sets of arguments? X and Y such that X  Y . Suppose FAF (Y ) 
FAF (X), then there must be an argument? A that is defended by X but not by Y . Since
X  Y , A has to be defended by Y . Therefore, the monotonicity of FAF (E) with respect
to set inclusion is obvious.
(1). It will be shown that FAF (E) is indeed a function from C  into C such that FAF (E) =
E by showing that FAF (E) is a complete extension in SAF , if E is a complete extension
in SAF  . Let E be a complete extension in SAF  . First it has to be shown that E is an
admissible set in SAF .
E is conflict-free in A , so it has to be conflict-free in A. It also defeats each minimal
argument? defeating E and it contains every minimal argument? defended by E.
For any A  E and any B  A that defeats A, take a B  . Then B   A defeats A
by Lemma 8.2, so some C  E defeats B  . But then C also defeats B by Lemma 8.2
combined with the fact that B is a B + . This means that A  A is defended by E. It can
be concluded that E defeats every argument? that defeats E. It was already shown that E
is conflict-free, so E  A is an admissible set.
For each argument? A  FAF (E), any A has to be defended by E in A , so every A is
in E. Therefore, (FAF (E)  E) = , thus FAF (E) = E.
Now, it has to be shown that FAF (E) is a complete extension. Let A be defended by
FAF (E) and let B defeat A. Hence, there is a C  FAF (E) defeating B. C  FAF (E)
means that C is defended by E, thus E defeats every argument? defeating C. Suppose an
argument? D defeats C  , then D also defeats C by Lemma 8.2. Therefore, E must defeat
D, so C  is defended by E. Since E is a complete extension of SAF  , any C  is in E. C 
239

fiGrooters & Prakken

defeats B (Lemma 8.2). Therefore, E defeats B. Thus, A is defended by E, and therefore
A  FAF (E). As a consequence, FAF (E) is complete.
(2). Let E be a complete extension in SAF . It will be shown that E  is complete in SAF  .
First it has to be shown that E  is an admissible set of A . Since E is conflict-free, E 
has to be conflict-free as well.
From Lemma 8.3 and the fact that E is complete in SAF , each minimal version of the
arguments? in E belongs to E. Let A  A defeat E  . Hence, there is B  E defeating A.
According to Lemma 8.3 B   E, so B   E  . Hence, B  defeats A (Lemma 8.2). Thus,
E  is admissible.
Each minimal argument? defended by E  is defended by E and hence belongs to E and so
to E  . E  is therefore complete.
Since E   E and E is complete, it is clear that FAF (E  )  FAF (E) = E. It is now
shown that each argument? defended by E is also defended by E  . Let A be an argument?
defended by E in SAF and let B be an argument? defeating A. Hence, there is an argument?
C  E defeating B and so each C   E defeats B. Hence E  defeats B. Thus A defended
by to E  in SAF . It can be concluded that FAF (E  )  FAF (E) = E, i.e. FAF (E  ) = E.
[Injective] Take X, Y  C  such that FAF (X) = FAF (Y ). It is obvious that FAF (X) =
FAF (Y ) . Then according to the proof for point (1) FAF (X) = X and FAF (Y ) = Y .
Therefore, it follows that X = Y .
[Surjective] It has to be shown that for all Y  C there is an X  C  such that FAF (X) = Y .
Now take X to be Y  , then the proof for point (2) provides that X  C  and that
FAF (X) = Y .
[Bijective] Injectivity and surjectivity provides that FAF is a bijection from C  onto C.
Theorem 8.5 Let SAF  = (A , C  ,  ) be the minimal structured argumentation framework corresponding to SAF = (A, C, ) (for the ASPIC ? framework). Let  be a tolerable
argument? ordering. Take T  {complete, grounded, preferred, stable}, then:
1. Let E be a T extension in SAF , then E  is a T extension in SAF  .
2. Let E be a T extension in SAF  , then F (E) is a T extension in SAF .
Proof.
[T = complete]
Let C and C  be the sets of complete extensions of SAF and SAF  , respectively. Lemma
8.4 states that FAF is a bijection from C  to C. This immediately provides that F (E)  C.
The second point of Lemma 8.4 states that E   C  .
[T  {grounded, preferred}]
From Lemma 8.4 it follows immediately that for each E  C  , FAF (E) is minimal or
maximal with respect to set inclusion in E if and only if E is minimal or maximal respectively
in C  . Hence E is grounded or preferred in SAF  if and only if FAF (E) is grounded or
preferred in SAF , respectively.
[T = stable]
(1). Take E to be a stable extension in SAF . Suppose for contradiction that E  is not a
stable extension in SAF  . Then there must be an argument? A  A , A 
/ E  such that
A is not defeated by any argument? in E  . However, A 
/ E  implies A 
/ E since A is
?
minimal. E is stable, thus there must be an argument B  E such that B defeats A. But
240

fiTwo Aspects of Relevance in Structured Argumentation

then any B   E (Lemma 8.3) defeats A. It is clear that B  E  , so this is a contradiction
with the fact that E  does not defeat A. Therefore, E  is a stable extension in SAF  .
(2). Take E to be a stable extension of SAF  . Suppose for contradiction that FAF (E)
is not a stable extension of SAF . Then there must be an argument? A  A such that
A
/ FAF (E) and FAF (E) does not defeat A. A 
/ FAF (E) means that A is defended by
E. Therefore, there must be an argument? B  A that defeats A such that there is not a
C  E that defeats B. Since E is stable in SAF  , B  E so E defeats A. This implies
that FAF (E) defeats A, which is a contradiction with the fact that FAF (E) does not defeat
A. Therefore, FAF (E) has to be a stable extension of SAF .
A.3 Proofs for Section 8.3
Lemma 8.6 For any finite Y  L it holds that X Y is finite.
Proof. Note that for any  there is at most a single strict rule  , while since both K and
Rd are finite, the set of all sets that equal the set of antecedents of a strict rule for  in X
is also finite.
Theorem 8.7 Let SAF = (A, C, ) be a strongly minimal structured argumentation framework corresponding to an argumentation theory with a finite K and Rd . Then for any   L
the set {A  A | Conc(A) = } is finite.
Proof. We prove the result by iteratively constructing of the set of all finite arguments?
constructible from K in AS.
 A0 = K and Ad0 = As0 = ;
 Ai+1 = Ai  Adi+1 = Asi+1 where
 Adi+1 = {A = Q   | Q  Ai and Conc(Q)    Rd and  6 Conc(Q)}.
 Asi+1 = {A = Q   | Q  Ai and Conc(Q)    Rs and  6 Conc(Q) and for
no B  Q it holds that TopRule(B)  Rs }.
S
It is easy to see that ni=0 Ai is the set of all constructible finite arguments? .
To prove the theorem, note first that A0 is finite since K is finite and Adi is finite for
any i since Rd is finite. By contrast, Asi is infinite for any i since given our choice of Rs , to
any set of wff an infinite number of strict rules applies. However, it follows from Lemma 8.6
that for any i and any   L there exists at most a finite number of arguments? in Asi with
conclusion . Together these observations imply that for any i and any   L there exists
at most a finite number of arguments? with conclusion  in Ai .
It is left to prove that the construction is finite. This follows from the fact that both
K and Rd are finite and the fact that by Definition 8.7 rules cannot be repeated in an
argument? . So at some point, Adi = . But then also Asi+1 =  since strict rules cannot be
chained in an argument? . So the construction of the set of all finite arguments? constructible
from K in AS is finite, while for each   L at each step in the construction only a finite
set of arguments? with conclusion  is created. This proves the theorem.

241

fiGrooters & Prakken

Theorem 8.8 Each strongly minimal ASPIC ? SAF with `W is compact.
Proof.
? (GN (S)# )]
[Conc(GN (S))  ClR
s
? (GN (S)# ). It needs to be shown that  
Let S be a set of arguments? and   ClR
s
? (X).
Conc(GN (S)). Let X be a minimal subset of Conc(GN (S)# ) such that   ClR
s
Hence there is a strict argument? A0 over X with conclusion . Further let SX be a
minimal set of arguments? from GN (S)# s.t. Conc(SX ) = X. There is no problem with
repetition of conclusions here, since if   X then A0 = . Let A be the argument? obtained
by replacing each leaf in A0 (viewed as a directed acyclic graph) labelled by a literal  from
X by an argument? with conclusion  from SX . Note that if A repeats conclusion  then
we do not need A since then clearly   Conc(GN (S)) and we are done. Otherwise, the
construction of A is possible since, firstly, all arguments? in SX are basic fallible arguments?
or are just necessary premises and, second, by definition of GN (S)# it holds that A will
not repeat any conclusion in X. It is obvious that the conclusion of A is . It is shown that
SX is a base of A. Suppose B is an argument? defeating A. Since A0 is a strict argument?
over X, B must defeat a basic fallible subargument? in SX . Hence B defeats SX . Thus
A  GN (S). Hence   Conc(GN (S)).
? (GN (S)# )]
[Conc(GN (S))  ClR
s
As for Theorem 7.6.
It is proven that the strongly minimal SAF is compact.
A.4 Proofs for Section 9
Theorem 9.1 Let AS be an argumentation system with L a classical-logic language and
Rs corresponding to classical logic, and let K = Kp be a knowledge base in L. Let AS 
be obtained from AS by removing from Rs all inference rules that are invalid according to
`W . Let AT = (AS, K) be a minimal ASPIC + argumentation theory and AT  = (AS  , K)
the corresponding minimal ASPIC ? argumentation theory. Then let SAF = (A, C, ) and
SAF  = (A , C  ,  ) be, respectively, the minimal c-structured argumentation framework
defined by AT in ASPIC + and the minimal structured argumentation framework defined
by AT  in ASPIC ? such that:
 For all A, B, C  A: if Prem(A) = Prem(B) then C  A iff C  B and A  C iff
B  C. Likewise for  .
 For all A , B   A : A  B  iff A  B  .
Then for T  {complete, grounded, preferred, stable} it holds that:
1. Let E be a T extension in SAF , then E  A is a T extension in SAF  such that
Conc(E) = Conc(E  A ).
2. Let E be a T extension in SAF  , then F (E) is a T extension in SAF such that
Conc(E) = Conc(F (E)).
Proof. First, it is easy to see that A  A. Next, we show by induction on the structure
of arguments? that for any argument? A  A \ A there exists an argument? B  A such
that Prem(A) = Prem(B) and Conc(A) = Conc(B).
242

fiTwo Aspects of Relevance in Structured Argumentation

The base case is that A  K. Then B = A. For the inductive case consider A =
B1 , . . . , Bn  . By the induction hypothesis the arguments? B1 , . . . , Bn satisfy the coni   the induction
ditions. Then for any Bi (1  i  n) of the form Bi = Bki , . . . , Bm
i
i
i
i
hypothesis yields that Bk , . . . , Bm are from K. Then replacing all such Bi with Bki , . . . , Bm
in B1 , . . . , Bn   yields an argument? B satisfying the conditions. Note that the corresponding strict inference rule is in Rs since classical logic satisfies the cut rule and all
arguments? in A have consistent premises.
Then it is easy to prove along the lines of the proof of Lemma 8.2 for any argument?
A  A that any argument? attacked/defeated by A is also attacked/defeated by any B and
vice versa, and any argument? attacking/defeating A also attacks/defeats any B and vice
versa. Then the proof of Theorem 9.1 can be completed along the lines of the proof of
Theorem 8.5.

References
Amgoud, L., & Besnard, P. (2013). Logical limits of abstract argumentation frameworks.
Journal of Applied Non-classical Logics, 23, 229267.
Bench-Capon, T., Prakken, H., & Visser, W. (2011). Argument schemes for two-phase
democratic deliberation. In Proceedings of the Thirteenth International Conference
on Artificial Intelligence and Law, pp. 2130, New York. ACM Press.
Besnard, P., & Hunter, A. (2008). Elements of Argumentation. MIT Press, Cambridge,
MA.
Bondarenko, A., Dung, P., Kowalski, R., & Toni, F. (1997). An abstract, argumentationtheoretic approach to default reasoning. Artificial Intelligence, 93, 63101.
Brewka, G. (1991). Nonmonotonic Reasoning: Logical Foundations of Commonsense. Cambridge University Press, Cambridge.
Caminada, M. (2005). Contamination in formal argumentation systems. In Proceedings
of the Seventeenth Belgian-Dutch Conference on Artificial Intelligence (BNAIC-05),
Brussels, Belgium.
Caminada, M., & Amgoud, L. (2007). On the evaluation of argumentation formalisms.
Artificial Intelligence, 171, 286310.
Caminada, M., Carnielli, W., & Dunne, P. (2012). Semi-stable semantics. Journal of Logic
and Computation, 22, 12071254.
Caminada, M., Modgil, S., & Oren, N. (2014). Preferences and unrestricted rebut. In Parsons, S., Oren, N., Reed, C., & Cerutti, F. (Eds.), Computational Models of Argument.
Proceedings of COMMA 2014, pp. 209220. IOS Press, Amsterdam etc.
Caminada, M., Sa, S., Alcantara, J., & Dvorak, W. (2015). On the difference between
assumption-based argumentation and abstract argumentation. IFCoLog Journal of
Logic and its Applications, 2, 1534.
Da Costa, N. (1974). On the theory of inconsistent formal systems. Notre Dame Journal
of Formal Logic, 15 (4), 497510.
243

fiGrooters & Prakken

Dung, P. (1995). On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming, and nperson games. Artificial Intelligence, 77,
321357.
Dung, P. (2014). An axiomatic analysis of structured argumentation for prioritized default
reasoning. In Proceedings of the 21st European Conference on Artificial Intelligence
(ECAI 2014), pp. 267272.
Dung, P. (2016). An axiomatic analysis of structured argumentation with priorities. Artificial Intelligence, 231, 107150.
Dung, P., Mancarella, P., & Toni, F. (2007). Computing ideal sceptical argumentation.
Artificial Intelligence, 171, 642674.
Dung, P., & Thang, P. (2014). Closure and consistency in logic-associated argumentation.
Journal of Artificial Intelligence Research, 49, 79109.
Dung, P., Toni, F., & Mancarella, P. (2010). Some design guidelines for practical argumentation systems. In Baroni, P., Cerutti, F., Giacomin, M., & Simari, G. (Eds.),
Computational Models of Argument. Proceedings of COMMA 2010, pp. 183194. IOS
Press, Amsterdam etc.
Garcia, A., & Simari, G. (2004). Defeasible logic programming: An argumentative approach.
Theory and Practice of Logic Programming, 4, 95138.
Ginsberg, M. (1994). AI and nonmonotonic reasoning. In Gabbay, D., Hogger, C., & Robinson, J. (Eds.), Handbook of Logic in Artificial Intelligence and Logic Programming,
pp. 133. Clarendon Press, Oxford.
Gorogiannis, N., & Hunter, A. (2011). Instantiating abstract argumentation with classicallogic arguments: postulates and properties. Artificial Intelligence, 175, 14791497.
Grooters, D., & Prakken, H. (2014). Combining paraconsistent logic with argumentation.
In Parsons, S., Oren, N., Reed, C., & Cerutti, F. (Eds.), Computational Models of
Argument. Proceedings of COMMA 2014, pp. 301312. IOS Press, Amsterdam etc.
Hunter, A. (2007). Real arguments are approximate arguments. In Proceedings of the 22nd
National Conference on Artificial Intelligence (AAAI-07), pp. 6671.
Jakobovits, H., & Vermeir, D. (1999). Robust semantics for argumentation frameworks.
Journal of Logic and Computation, 9, 215261.
Krause, P., Ambler, S., Elvang-Gransson, M., & Fox, J. (1995). A logic of argumentation
for reasoning under uncertainty. Computational Intelligence, 11 (1), 113131.
Lin, F., & Shoham, Y. (1989). Argument systems. A uniform basis for nonmonotonic
reasoning. In Principles of Knowledge Representation and Reasoning: Proceedings of
the First International Conference, pp. 245255, San Mateo, CA. Morgan Kaufmann
Publishers.
Modgil, S., & Prakken, H. (2013). A general account of argumentation with preferences.
Artificial Intelligence, 195, 361397.
Modgil, S., & Prakken, H. (2014). The ASPIC+ framework for structured argumentation:
a tutorial. Argument and Computation, 5, 3162.
244

fiTwo Aspects of Relevance in Structured Argumentation

Pollock, J. (1987). Defeasible reasoning. Cognitive Science, 11, 481518.
Pollock, J. (1990). A theory of defeasible reasoning. International Journal of Intelligent
Systems, 6, 3354.
Pollock, J. (1992). How to reason defeasibly. Artificial Intelligence, 57, 142.
Pollock, J. (1994). Justification and defeat. Artificial Intelligence, 67, 377408.
Pollock, J. (1995). Cognitive Carpentry. A Blueprint for How to Build a Person. MIT
Press, Cambridge, MA.
Prakken, H. (2010). An abstract framework for argumentation with structured arguments.
Argument and Computation, 1, 93124.
Prakken, H. (2012). Some reflections on two current trends in formal argumentation. In
Logic Programs, Norms and Action. Essays in Honour of Marek J. Sergot on the
Occasion of his 60th Birthday, pp. 249272. Springer, Berlin/Heidelberg.
Prakken, H., & Modgil, S. (2012). Clarifying some misconceptions on the ASPIC+ framework. In Verheij, B., Woltran, S., & Szeider, S. (Eds.), Computational Models of
Argument. Proceedings of COMMA 2012, pp. 442453. IOS Press, Amsterdam etc.
Prakken, H., & Sartor, G. (1997). Argument-based extended logic programming with defeasible priorities. Journal of Applied Non-classical Logics, 7, 2575.
Prakken, H., & Vreeswijk, G. (2002). Logics for defeasible argumentation. In Gabbay, D.,
& Gunthner, F. (Eds.), Handbook of Philosophical Logic (Second edition)., Vol. 4, pp.
219318. Kluwer Academic Publishers, Dordrecht/Boston/London.
Prakken, H., Wyner, A., Bench-Capon, T., & Atkinson, K. (2015). A formalisation of
argumentation schemes for legal case-based reasoning in ASPIC+. Journal of Logic
and Computation, 25, 11411166.
Priest, G. (1979). The logic of paradox. Journal of Philosophical Logic, 8 (1), 219241.
Priest, G. (1989). Reasoning about truth. Artificial Intelligence, 39 (2), 231244.
Read, S. (1988). Relevant logic. Blackwell, Oxford.
Rescher, N., & Manor, R. (1970). On inference from inconsistent premises. Journal of
Theory and Decision, 1, 179219.
Simari, G., & Loui, R. (1992). A mathematical treatment of defeasible argumentation and
its implementation. Artificial Intelligence, 53, 125157.
Vreeswijk, G. (1997). Abstract argumentation systems. Artificial Intelligence, 90, 225279.
Walton, D. (1996). Argumentation Schemes for Presumptive Reasoning. Lawrence Erlbaum
Associates, Mahwah, NJ.
Wu, Y. (2012). Between Argument and Conclusion. Argument-based Approaches to Discussion, Inference and Uncertainty. Doctoral Dissertation Faculty of Sciences, Technology
and Communication, University of Luxemburg.
Wu, Y., & Podlaszewski, M. (2015). Implementing crash-resistence and non-interference in
logic-based argumentation. Journal of Logic and Computation, 25, 303333.

245

fiJournal of Artificial Intelligence Research 56 (2016) 429-461

Submitted 02/16; published 07/16

Efficient Mechanism Design for Online Scheduling
Xujin Chen
Xiaodong Hu

xchen@amss.ac.cn
xdhu@amss.ac.cn

AMSS, Chinese Academy of Science, Beijing, China

Tie-Yan Liu
Weidong Ma
Tao Qin

tyliu@microsoft.com
weima@microsoft.com
taoqin@microsoft.com

Microsoft Research, Beijing, China

Pingzhong Tang

kenshin@mail.tsinghua.edu.cn

Tsinghua University, Beijing, China

Changjun Wang

wcj@amss.ac.cn

Beijing University of Technology, Beijing, China

Bo Zheng

zhengb10@mails.tsinghua.edu.cn

Tsinghua University, Beijing, China

Abstract
This paper concerns the mechanism design for online scheduling in a strategic setting.
In this setting, each job is owned by a self-interested agent who may misreport the release
time, deadline, length, and value of her job, while we need to determine not only the
schedule of the jobs, but also the payment of each agent. We focus on the design of
incentive compatible (IC) mechanisms, and study the maximization of social welfare (i.e.,
the aggregated value of completed jobs) by competitive analysis. We first derive two lower
bounds on the competitive ratio of any deterministic IC mechanism to characterize the
landscape of our research: one bound is 5, which holds for equal-length jobs; the other
bound is ln + 1  o(1), which holds for unequal-length jobs, where  is the maximum ratio
between lengths of any two jobs. We then propose a deterministic IC mechanism and show
that such a simple mechanism works very well for two models: (1) In the preemption-restart
model, the mechanism can achieve the optimal competitive ratio of 5 for equal-length jobs
1

and a near optimal ratio of ( (1)
2 + o(1)) ln  for unequal-length jobs, where 0 <  < 1
is a small constant; (2) In the preemption-resume model, the mechanism can achieve the
optimal competitive ratio of 5 for equal-length jobs and a near optimal competitive ratio
(within factor 2) for unequal-length jobs.

1. Introduction
Online scheduling has been widely studied in the literature (Baruah, Koren, Mao, Mishra,
Raghunathan, Rosier, Shasha, & Wang, 1992; Baruah, Haritsa, & Sharma, 1994; Porter,
2004; Zheng, Fung, Chan, Chin, Poon, & Wong, 2006; Ting, 2008), where each job is characterized by a release time, a deadline, a length, and a value for its successful completion
by the deadline. Inspired by emerging areas like computational economics and cloud computing, we consider a strategic setting of the online scheduling problem, where each job is
owned by a self-interested agent and she may have the incentive to manipulate the schedulc
2016
AI Access Foundation. All rights reserved.

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

ing algorithm in order to be better off. To be specific, the agent may deliberately delay the
release time of her job, inflate its length, and misreport its value and deadline.
Given this situation, a carefully designed online scheduling mechanism is needed to
regulate the strategic behaviors of the agents and to (approximately) optimize some system
objectives. In this work, we focus on the maximization of social welfare, i.e., the total
value of completed jobs.1 We use competitive analysis (Lavi & Nisan, 2004) to evaluate
the performance of such a mechanism, which compares the social welfare implemented by
the mechanism (without any knowledge of all future jobs) with that of the optimal offline
allocation (with the knowledge of future jobs).
In this work, we consider two scheduling models: the preemption-restart model (Ting,
2008) and the preemption-resume model (Porter, 2004). Once preempted, jobs in the first
model have to restart from the beginning; while jobs in the second model can resume from
the break point. Since preemption is always assumed in this work, the two models are also
referred to as restart model and resume model, respectively, and their involved jobs are
called non-resumable and resumable, respectively.
1.1 Problem Formulation
We consider online scheduling models with infinite time period T = R0 . Suppose there
is a single machine that processes at most one job at any given time. Jobs come over
time, and we use J to denote the set of jobs. Each job j  J is owned by a self-interested
agent (which is also denoted as j for simplicity); and it is characterized by a private type
j = (rj , dj , lj , vj )  T  T  R>0  R>0 , where rj is the release time2 , dj is the deadline,
lj is the length (i.e., the processing time), and vj is the value if the job is completed by its
deadline.
A resumable job j is completed if and only if it is processed for lj time units in total
between its release time rj and deadline dj , while a non-resumable job j is completed if and
only if it is processed for lj consecutive time units between its release time rj and deadline
dj .
Let  = maxi,jJ llji be the maximum ratio between the lengths of any two jobs. For
simplicity, we assume all job lengths are normalized, i.e., lj  [1, ] for all j  J, and assume
 is known in advance following the practice in the work of Chan et al. (2004) and Ting
(2008).
We study direct revelation mechanisms, in which each agent participates by simply
declaring the type of her job j = (rj , dj , lj , vj ) at time rj . We use  to denote the profile
of reported types of all the agents. Given the declared types of the agents, a mechanism
M is used to schedule/allocate the jobs and determine the payment of each agent. Here we
only consider reasonable mechanisms which (1) do not schedule a job after its reported
deadline and (2) do not schedule a job once it has been processed for a reported length.
Given a certain mechanism M and a job sequence , we use qj (, t) to denote whether
job j is completed by time t (if it is completed, qj (, t) = 1; otherwise qj (, t) = 0). Then
1. It is also referred as weighted throughput in the scheduling literature.
2. Note that release time is also referred as arrival time in the online auction literature (Parkes, 2007). It
is the earliest time at which the agent has full knowledge of her job. Thus it is the earliest time the job
is available to the scheduling process.

430

fiEfficient Mechanism Design for Online Scheduling

the value that agent j extracts from the mechanism can be represented by qj (, dj )vj , and
P
the social welfare of the mechanism can be represented by W (M, ) = j qj (, dj )vj .
Let pj () denote the amount of money that the mechanism charges agent j. We assume that agents have quasi-linear preferences (Nisan, 2007), i.e., the utility of agent j is
uj (, j ) = qj (, dj )vj  pj ().
Since agents are self-interested, they may misreport their types in a strategic way. It is
easy to see that the misreport of a shorter length is a dominated strategy; otherwise, her
job cannot be completed even if it is scheduled by the mechanism (since lj < lj ). Therefore,
the agents will not underreport the lengths of their jobs. Similar to the work of Porter
(2004), we assume that the system will not return a completed job to agent j until dj .3 In
this way, we restrict the agents report to be dj  dj . In addition, we assume that no agent
has knowledge about her job before its release time, so we also have rj  rj .
Considering the potential misreport of the agents, we are concerned with incentive
compatible and individually rational mechanisms. A mechanism is incentive compatible
(IC) if, for any agent j, regardless of the behaviors of other agents, truthful reporting
her own type maximizes her utility. A mechanism is individually rational (IR) if for each
job j, truthful reporting leads to a non-negative utility. In addition, we would also like the
mechanism to (approximately) maximize social welfare. We say a mechanism M is (strictly)
c-competitive if there does not exist any job sequence  such that c  W (M, ) < W (opt, ),
where opt denotes the optimal offline mechanism4 . Sometimes we also say that M has a
competitive ratio of c.
1.2 Related Work
The online scheduling problem has been studied in both the non-strategic setting (Lipton &
Tomkins, 1994; Borodin & El-Yaniv, 1998; Bar-Noy, Guha, Naor, & Schieber, 2001; Zheng
et al., 2006; Kolen, Lenstra, Papadimitriou, & Spieksma, 2007; Ting, 2008; Nguyen, 2011)
(whose focus is algorithm design) and the strategic setting (Nisan & Ronen, 2001; Lavi &
Nisan, 2004; Friedman & Parkes, 2003; Porter, 2004; Hajiaghayi, Kleinberg, Mahdian, &
Parkes, 2005; Parkes, 2007) (whose focus is mechanism design).
Non-strategic setting. For the case of  = 1, a lower bound of 4 on the competitive
ratio of any deterministic algorithm is given by Woeginger (1994). A 4.56-competitive
deterministic algorithm is constructed by Zheng et al. (2006) for the restart model, and
a 4.24-competitive deterministic algorithm is designed by Kim (2011) for both restart and
resume models. A 2-competitive randomized algorithm is introduced for restart model in
the work of Fung et al. (2014), and a lower bound of 1.693 is provided in the work of Epstein
and Levin (2010). By restricting release time and deadlines to be integers, a randomized
e
algorithm with competitive ratio e1
 1.582 is proposed by Chin et al. (2006), and a

deterministic algorithm with competitive ratio 2 2  1  1.828 is proposed by Englert et
3. Actually, it should be viewed as a decision by the mechanism designer rather than an assumption.
This decision is crucial to ensure the incentive compatibility which we will see later.
4. Since we only care about the social welfare performance of opt in the competitive analysis, which only
depends on the schedule, regardless of the payments, we also call opt as optimal offline allocation, or
simply optimal allocation.

431

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

al. (2012). The best lower bounds currently are 1.25 for randomized algorithms (Chin &
Fung, 2003) and 1.618 for deterministic algorithms (Hajek, 2001).

For general values of , a lower bound of  on the competitive ratio of any deterministic
algorithm is derived in the work of Chan et al. (2004). The lower bound is improved to

6
5/6 )
2 ln   1 by Ting and Fung (2008), and an algorithm with competitive ratio log  + O(
is given for the restart model. The scheduling problem with discrete time is considered in
the work of Durr, Jez and Nguyen (2012). In particular, the lower bound is improved to


model. A
ln   o(1), and a (3 + o(1)) ln  -competitive algorithm is designed for the resume q
randomized algorithm with competitive ratio O(log()) and a lower bound of ( logloglog  )
is provided by Canetti and Irani (1998).
Assuming the maximum ratio between the value densities (value divided by length) of

any two jobs is bounded above by a known number , a (1 + )2 -competitive algorithm is

given by Koren and Shasha (1995). The bound (1 + )2 is optimal as a matching lower
bound is given by Baruah et al. (1992).
There is also a rich literature concerned with non-preemptive scheduling (Lipton &
Tomkins, 1994; Goldman, Parwatikar, & Suri, 2000; Goldwasser, 2003; Ding & Zhang,
2006; Ding, Ebenlendr, Sgall, & Zhang, 2007; Ebenlendr & Sgall, 2009). However, it can
be easily verified that an algorithm with bounded competitive ratio cannot be designed in
the setting of unrestricted values and arbitrary release time. Therefore, the most common
assumption added in the non-preemptive scheduling problem is proportional values, i.e., the
value of each job is proportional to the length. In the work of Goldman et al. (2000), a
tight upper and lower bound of 2 are given for the deterministic competitiveness when all
jobs have equal length (thus, equal value), and a 6(blog2 c + 1)-competitive randomized
algorithm is provided for general value of , matching the (log ) lower bound (Lipton &
Tomkins, 1994) within a constant factor.
Strategic setting. In the work of Lavi and Nisan (2015), by assuming integer time
points, a scheduling problem for the  = 1 case is studied. The authors show that there
is no incentive compatible mechanism which can obtain a constant competitive ratio, if
the payment must be made when the job is completed. Hence, they propose a family of
semi-myopic algorithms with competitive ratio 3, under the assumption of semi-myopic
strategies. In the work of Hajiaghayi et al. (2005), a specific scheduling problem in which
 = 1 is considered under the restart model. A deterministic IC mechanism with competitive
ratio 5 is designed, and a lower bound of 2 is given to any deterministic IC mechanism.
However, to our knowledge, the case  > 1 in either the restart model or the resume model
has not been studied from the perspective of mechanism design (considering the incentive
issues). Our work fills this gap.
Assuming the maximum ratio between the value densities (value divided by length) of
any two jobs is bounded above by a known number , an IC mechanism with a competitive


ratio of (1 + )2 + 1 is designed by Porter (2004), and it is proved that (1 + )2 + 1 is a
lower bound of the competitive ratio for any deterministic mechanism.
Recently, online scheduling mechanisms have been investigated in cloud computing (Zaman & Grosu, 2012; Azar, Ben-Aroya, Devanur, & Jain, 2013; Zhang, Li, Jiang, Liu,
Vasilakos, & Liu, 2013; Lucier, Menache, Naor, & Yaniv, 2013; Mashayekhy, Nejad, Grosu,
& Vasilakos, 2014; Wu, Gu, Li, Tao, Chen, & Ma, 2014). In these works, mechanisms are
432

fiEfficient Mechanism Design for Online Scheduling

designed to allocate computational resources to users, and users can use those virtual machines during the entire period requested. In these model, jobs are non-preemptive, which
differs from our setting.
1.3 Our Results
Our main results can be summarized as follows.
First, in order to characterize the boundary of our research, we derive two lower bounds
on the competitive ratio for any online deterministic IC mechanism. One bound is 5, which
holds for the situation where all the jobs have equal length (i.e.,  = 1). This bound
improves the previous lower bound of 2 (Hajiaghayi et al., 2005). The other bound is

ln  + 1  o(1), which characterizes the asymptotical property of the competitive ratio when
the variance of job lengths, i.e., , is sufficiently large.
Second, we design a simple mechanism 1 and prove that in both the restart and resume
models 1 is not only IC, but also achieves good social welfare.
 In the restart model, 1 has a competitive ratio of  + 2 + (1 + 1 ) when  is small (in
1

particular, the ratio is 5 for  = 1), and ( (1)
2 + o(1))  ln  when  is large (  16
is enough), where 0 <  < 1 is a small constant.
 In the resume model, 1 has a competitive ratio of ( + 1)(1 + 1 ) + 1 when  is
2

small (in particular, the ratio is 5 for  = 1), and ( (1)
2 + o(1))  ln  when  is large
(  16 is enough), which is just slightly worse than that for the restart model (within
a factor of 2).
It is also worth mentioning that:
 Comparing with the lower bounds, we can see that, in both the restart and resume
models, 1 is optimal for equal-length jobs ( = 1), and near optimal (within a
constant factor) for unequal-length jobs.
 In comparison with the best-known algorithms without considering incentive compat5
6
6
ibility, asymptotically speaking, 1 improves the best-known ratio log
 +O( ) (Ting,
1

2008) in the restart model to ( (1)
2 + o(1))  ln  ; and improves the best-known ratio
2

(3+o(1)) ln (Durr, Jez, & Nguyen, 2012) in the resume model to ( (1)
2 +o(1)) ln  .
Thus even if one does not care about the strategic aspect, 1 would still be a very
nice algorithm to use.

Note that designing mechanisms for online scheduling problems is generally difficult
since it combines the challenges of mechanism design (i.e., ensuring incentive compatibility)
with the challenges of online algorithms (i.e., dealing with uncertainty about future inputs).
We would like to highlight the main techniques used in this work to tackle these challenges.
(1) The allocation rule of mechanism 1 uses a carefully selected function to trade-off
three key elements: value, length, and degree of completion. The trade-off function is
delicate in the sense that it ensures both the efficiency and the monotonicity which is
crucial to the incentive compatibility.
433

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

(2) In order to obtain good competitive ratios for the resume model, we design two nontrivial virtual charging schemes to bound the performance of the proposed mechanism:
the integral charging scheme and the segmental charging scheme.
While we focus on single machine model in this paper, our work extends to multiple
identical machines. One way of the extension is similar to the work of Lucier et al. (2013),
in which it is assumed that at most h machines can be allocated to each job at any given
time, and the parameter h stands for a common parallelism bound of the system. The
details of this extension can be found in Appendix E. Another way to extend our results to
multiple identical machines is to assume that each job j needs a fixed number of machines
when it is processed. Please refer to our working paper (Ma, Zheng, Qin, Tang, & Liu,
2014) for more details.5

2. Lower Bounds
In this section, we present two lower bounds on the competitive ratio of any deterministic
IC mechanism, which hold for both the restart and resume models.
The competitive analysis can be interpreted as a game between the designer of the online
mechanism and an adversary. Given mechanism 1 , the adversary selects the sequence of
jobs that maximizes the competitive ratio, the ratio of the social welfare obtained by an
offline optimal algorithm over the social welfare obtained by 1 . Therefore, the key of
proving lower bounds is to construct subtle adversary behaviors.
We first introduce two notions, the dominant job and the shadow job.
Definition 2.1 (Dominant Job). For a deterministic IC mechanism with competitive
ratio c, job i is called a dominant job at its release time ri , if and only if vi is larger than
c times the total value of all other jobs whose release time is no later than ri .
It is easy to see that, in order to obtain a reasonable competitive ratio, if a dominant job
i has a tight deadline, then the mechanism must schedule i at its release time ri . Otherwise,
consider the case in which no more jobs are released after ri . In this case, the mechanism
cannot obtain a competitive ratio of c if it gives up the dominant job i.
Definition 2.2 (Shadow Job). Suppose a job i has a tight deadline, i.e., di = ri + li , then
job i0 is called a shadow job of i, if i0 has the same parameters (ri , li , vi ) as i, except for a
later deadline (d0i > di ).
Clearly, the shadow job i0 is more flexible and can be completed later. As for shadow
jobs, we show that the following lemma holds for any IC mechanism with a non-trivial
competitive ratio.
Lemma 2.3 (Shadow Job Argument). For a deterministic IC mechanism  with a
non-trivial competitive ratio c, if  completes a job i (with tight deadline di ) under some
scenario I, then under scenario I 0 , which substitutes some shadow job i0 for job i,  must
also complete job i0 at time di .
5. In the working paper, we only consider the restart model, and ignore the misreport of release time or
deadline.

434

fiEfficient Mechanism Design for Online Scheduling

Proof. Suppose  has not completed job i0 at di under scenario I 0 , we could consider a subsidiary scenario I 00 , which includes all jobs in scenario I 0 and adds on several dominant jobs.
Remember that we call some job dominant if its value is sufficiently large (see Definition 2.1).
These dominant jobs are released one by one at di , di + 1, . . . , di + bd0i  di c respectively,
and denoted as 0, 1, . . . , bd0i  di c accordingly, where di is the deadline of job i and d0i is the
deadline of shadow job i0 . Whats more, each of these dominant jobs is of unit length and
has a tight deadline. We claim that, to achieve the desired (non-trivial) competitive ratio,
 must complete all these dominant jobs, thus the time interval [di , d0i ) is occupied. (The
reason is as follows: if  does not schedule any dominant job j  {0, 1, . . . , bd0i  di c}, then
we consider a scenario I 000 , which only includes jobs with release time no later than di + j
in I 00 . Since scenario I 000 is indistinguishable from I 00 up to time di + j, we know  does not
schedule the dominant job j in scenario I 000 , hence cannot obtain a competitive ratio of c.)
Because the subsidiary scenario I 00 is indistinguishable from scenario I 0 up to time di ,
job i0 will not be completed at di . Furthermore, because of the existence of dominant jobs,
job i0 will not be completed finally. However, if job i0 falsely declares its type to be the
same as that of job i, i.e., misreports its deadline to be di , it would be completed at time
di and be better off, contradicting the incentive compatibility6 .
In the following, we will derive lower bounds leveraging Lemma 2.3. First, the following
theorem specifies a lower bound when jobs have equal length (i.e.,  = 1). Note that our
result concerns the strategic setting, while Woeginger (1994) shows that the competitive
ratio of any deterministic algorithm in the non-strategic setting is at least 4.
Theorem 2.4. When  = 1, no deterministic IC mechanism can obtain a competitive ratio
less than 5.
To prove the theorem, in addition to using an adversary argument similar to that in the
work of Woeginger (1994), we need to further perturb the job sequence and leverage the
shadow job argument.
Intuitively, we construct a special job set, in which tight-deadline jobs are released one
by one, and any two jobs collide with each other (that is, the deadline of one job is later than
the release time of the other, and under any mechanism, it is impossible for these two jobs to
be both completed). The values of these jobs are carefully selected such that a later released
job is more valuable than the earlier one (predecessor), and the value difference between
such two neighboring jobs is constrained by a small-enough additive constant. Furthermore,
in such a job set, the values of the first and last jobs are set to obey a specific amplification.
Along with the execution of any mechanism, the adversary would release a series of such job
sets. Once the mechanism completes one job, the adversary stops releasing any job. The
subtleness lies in choosing the time to release such job sets: once the mechanism almost
completes some job a in a job set, the adversary may release a new job set whose jobs
all collide with job a but do not collide with the predecessor of job a. In this way, if the
mechanism would not abandon the current job a but complete it, then there should be an
optimal allocation which completes: (1) several jobs in the previous job sets, (2) the most
6. The above scenario contradicts the monotonicity condition (see a strict definition at start of Section 3.2);
And Theorem 1.15 of the work of Parkes (2007) shows that monotonicity is necessary for incentive
compatibility.

435

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

valuable job (i.e., the last job) in the newly released job set, and (3) the job a.7 However,
the mechanism can only complete job a. This discrepancy leads to the lower bound of
competitive ratio. The detailed proof can be found below.


   

SET
r1


   

vq = w



   
v2
v1 = v

t

Figure 1: Structure of SET (v, w, t, )
Proof. Suppose by contradiction that there exists a deterministic IC mechanism  which
achieves a competitive ratio of 5   for some 0 <  < 1. We adopt the notation of SET
introduced by Woeginger (1994). Define SET (v, w, t, ) (for w  v > 0, t > 0 and  > 0)
as a set of jobs {1, 2, . . . , q} satisfying the following properties:
(1) v1 = v, vq = w, and vj < vj+1  vj +  for 1  j  q  1. Hence, q can be any integer
no less than d wv
 e. We call  as the magnifying parameter of a SET .
(2) lj = dj  rj = 1, j, i.e., all jobs are unit-length and have tight deadlines.
(3) 0  r1 <    < rq < t < d1 <    < dq , thus, any two jobs collide with each other. We
call t as the split point of a SET .
We define the release time of a SET as the release time of its first job. Figure 1 shows the
visual structure of SET (v, w, t, ). The adversary behavior is as follows.
Adversary Behavior: The adversary will release some SET s one after another depending
on . First, SET0 = SET (1, , 1/2, ) is released at time 0, where  = 4  /2 and  < /4.
From the definition of SET , we know that the first job in SET0 has value 1, the last job
in SET0 has value , and the value difference between any two neighboring jobs is upper
bounded by .
Next, we specify: (1) when will the adversary release a new SETi (i  1), and (2) how
the adversary sets the parameters of SETi (i  1). For (1), we specify by Algorithm 1. The
notations used in Algorithm 1 are detailed in Table 1.
7. In the proof, we construct a new scenario, in which job a is perturbed to have later deadline, thus can
be completed later. We make use of the shadow job argument in the analysis, which makes the lower
bound increased by 1, compared with the previous lower bound in the non-strategic setting.

436

fiEfficient Mechanism Design for Online Scheduling

SETi
job ij
rij , dij and vij
wi
ti
i
job i
job i

Table 1: Summary of notation in the proof of Theorem 2.4
i-th released SET , in full, SET (vi1 , wi , ti , i )
j-th job in SETi .
release time, deadline and value of job ij.
value of the last job in SETi
split point of SETi
magnifying parameter of SETi
trigger job in SETi1
the preceding job of i in SETi1

Algorithm 1: The Adversary Behavior
1: Initial: Release SET0 at time 0.
2: while  has not completed any job, do
3:
if  almost completes the j-th job (j  2) in SETi (Precisely,  has been executing
job ij for di(j1)  rij period of time since rij ). then
4:
Release SETi+1 at time di(j1) .
5:
else
6:
Do not release any other job.
7:
end if
8: end while
It is worth mentioning that: (i) SETi+1 is only triggered when a non-first job in SETi
is almost completed, and we call such a job a trigger job. (ii) No more SET will be released
once some job is completed by .
Suppose the trigger jobs in SET0 , . . . , SETi1 are named 1 , . . . , i successively. Accordingly, we denote the job with release time just earlier than each trigger job as 1, . . . , i,
and we call them preceding jobs. From Line 4 of Algorithm 1, we know that each new SETi
is released at the deadline of i. Note that trigger job i and its preceding job i are both
located in SETi1 .
We now specify the parameters of SETi = SET (vi1 , wi , ti , i ), i  1. Remember that
SET0 is defined as SET (1, , 1/2, ), in which  = 4  /2 and  < /4.
 The adversary sets vi1 equal to the value of the trigger job i in SETi1 , that is
vi = vi1 . Note that vi1 is the value of the first job in SETi .
Pi1
 The adversary sets i = /2i , wi = max{(  1)vi1  j=1
vj1 , vi1 } for i  2, and
w1 = (  1)v11 .
 The adversary sets ti = (di + di )/2, where di and di are deadlines of trigger job i
and its preceding job i. Note that by setting ti = (di + di )/2, all jobs in SETi are
released after di but before di . Hence, all the new jobs collide with trigger job i and
none of them collides with job i.
Figure 2 illustrates how the adversary releases a new SET by an example. In this
example,  almost completes the j-th job (j  2) in SETi . SETi+1 is released at the
deadline of job i(j  1), and the value of the first job of SETi+1 is equal to vij .
437

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng


   

   
ri1


   

SETi

   

   
vi1

SETi+1

   
vij
vi(j1)

     

v(i+1)1 = vij

ti+1
Figure 2: An example of SETi+1 and SETi

According to Algorithm 1, if  always gives up trigger jobs and switches to schedule
some job in the newly released SET , the adversary will release new SET s one after another.
One may wonder whether the adversary will release new SET s infinitely. In other words,
will subscript of SETi tend to infinity?
The answer is no, which can be seen from the definition of wi . Since 2 <  < 4, by
Lemma 4.3 of the work of Woeginger (1994), after a finite numberP(denote k) of steps, vk1
must be no less than the corresponding sum term (  1)vk1  k1
j=1 vj1 , and wk = vk1
must hold. Remember that vk1 and wk denote the value of first job and last job in SETk
respectively, thus there exists only one job in SETk . According to Algorithm 1, no matter
whether  completes this job or not, the adversary will not release any other job. Therefore,
SETk is the ultimate SET and job k1 is the ultimate job.
So far, we have clarified the adversarys behaviors. Next, we show how to derive the
lower bound based on such an adversary.
According to Algorithm 1 and the structure of SET , we know the adversary allows 
to complete at most one job. Actually, the completed job can be: (1) the first job in SET0
(i.e., job 01); (2) a trigger job i , 1  i  k; or the first job in SETi , 1  i < k (i.e., job
i1); or (3) the ultimate job k1. Let us analyze them one by one.
(1) If  completes job 01, then we consider a scenario in which job 01 is substituted by its
shadow job 010 , whose deadline is late enough (i.e., even if it started being executed
from the deadline of last job in SET0 , it still can be completed in time). According
to Lemma 2.3, mechanism  must complete job 010 at time 1, and thus abandon the
last job (with value w0 = ) in SET0 . Therefore, it only obtains a social welfare of
v01 = 1. However, the optimal allocation (which first completes the last job in SET0
and then job 010 ) obtains a social welfare of  + 1. This contradicts the fact that 
has a competitive ratio of 5  , since  + 1 = (4  /2) + 1 > 5  .
(2) If  completes a trigger job i or a job i1, 1  i  k, without loss of generality,
we denote such job as job ij, and we know vij = vi = vi1 . If  completes job ij,
1  i  k, then similarly, we consider a scenario in which job ij is substituted by
its shadow job (ij)0 , whose deadline is late enough. By Lemma 2.3,  must complete
job (ij)0 at time dij , obtaining a social welfare of vij = vi = vi1 . However, social
welfare of the optimal allocation
P(which completes jobs
P1, . . . , i, the last job in SETi ,
and then job (ij)0 ) is at least ij=1 vj + wi + vij > ij=1 (vj1  j1 ) + wi + vij >
438

fiEfficient Mechanism Design for Online Scheduling

P
 2 + (  1)vi1  i1
j=1 vj1 + vij > ( + 1)vi1  /2 > (5  )vi1 . This
contradicts the fact that  has a competitive ratio of 5  .

Pi

j=1 vj1

(3) If  completes the ultimate job k1, we consider a scenario in which the adversary
releases two copies of job k1 in SETk . Clearly, in this scenario,  will choose one
copy to complete. We denote the completed copy as job (k1)1 and the other as job
(k1)2 . We then consider a scenario in which job (k1)1 is substituted by its shadow job
(k1)0 , whose deadline is unit-time later than that of job k1. According to Lemma 2.3,
 must complete job (k1)0 at dk1 and obtains a social welfare of vk1 . However, the
0
optimal allocation (which completes
Pk jobs 1, . . . , k, job (k1)
Pk 2 , and then job (k1) ) can
obtain a social welfare of at least j=1 vj + vk1 + vk1 > j=1 (vj1  j1 ) + wk + vk1 >
Pk
Pk1
j=1 vj1 + vk1 > ( + 1)vk1  /2 > (5  )vk1 . Remember
j=1 vj  2 + (  1)vk1 
P
that in SETk , we have vk1 = wk = (  1)vk1  k1
j=1 vj1 . This contradicts the fact
that  has a competitive ratio of 5  .

Second, to understand the asymptotic property of the lower bound when  is large, we
construct scenarios inspired by the example of Durr et al. (2012) and obtain the following
theorem.
Theorem 2.5. When  is sufficiently large, no deterministic IC mechanism can obtain a
competitive ratio less than ln + 1  o(1). In particular, no deterministic IC mechanism
can obtain a competitive ratio less than ln + 0.94 for   16.
Proof. For convenience of analysis, we denote  =
Let us consider the following adversary behaviors.


ln  ,

r = de  1, and assume   16.

Adversary Behavior: At time 0, a long job B with type B = (0, , , ) is released, as
well as two short jobs a1 and a1 with the same type (0, 1, 1, 1). Moreover, at each integer
moment 0  t    1, if the mechanism schedules only job B in [0, t), then two short jobs
at+1 and at+1 of unit length are released at t, with tight deadline t + 1, and no new job is
released otherwise. The values of jobs at and at satisfy:
(
1
if t < ,
v(at ) = v(at ) =
(1)
t
1
e
if t  .
Note that job at and job at are of the same type, and the cases analyzed below for at0 can
be naturally applied to at0 .
According to the adversary behavior, we know the adversary allows  to complete at
most one job. Actually, the completed job can be: (1) a job at0 with t0 < ; (2) a job at0
with t0  ; or (3) job B. We analyze these three cases as follows.
(1) If the mechanism schedules a job at0 with t0 < , then we consider a scenario that
includes jobs B, a1 , a1 , . . . , at0 1 , at0 1 , at0 and job a0t0 . Here, job a0t0 with type (t0 
1,  + 1, 1, 1), is a shadow job of at0 . According to Lemma 2.3, the mechanism must
complete job a0t0 at t0 and only obtains a social welfare of 1. However, in this scenario,
the optimal mechanism will complete job B first, and then schedule a0t0 at time  and
complete it, with the optimal social welfare  + 1. So the ratio is  + 1.
439

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

(2) If the mechanism schedules a job at0 with t0  , then we consider a scenario that
includes jobs B, a1 , a1 , . . . , at0 1 , at0 1 , at0 , and job a0t0 . Here, job a0t0 with type (t0 
1, t0 + 1, 1, et0 /1 ), is a shadow job of at0 . According to Lemma 2.3, the mechanism
should schedule job a0t0 at time t0 and complete only a0t0 . Thus, the mechanism
only obtains a social welfare of v(a0t0 ). However, one of the optimal mechanisms will
schedule and complete all jobs at for t = 1, . . . , t0 , and then schedule a0t0 at time t0
and complete it, resulting in the following optimal social welfare
t0
X

de  1 +

e

t
1


+e

t0
1


=r+

t0
X

e

t
1


+e

t0
1


Z

t0

r

t

t0

e  1 + e  1

r

t=r+1

t=de

t0

r+

t0

=r  e  1 + ( + 1)e  1 = f (, r) + ( + 1)e  1 = f (, r) + ( + 1)v(a0t0 ).
r

Here, we have introduced a function f defined as f (, r)  r  e  1 . Considering
 = ln and r =   1, we have   r  (0, 1]. As ex  1 + x and both sides converge
to 1 as x approaches 0, we have
r

f (, r) = r  e  1  r   

r
= 0,


(2)

and f (, r) approaches 0 as  grows. So the ratio is  + 1  o(1).
(3) If the mechanism schedules and completes job B, obtaining a social welfare of ,
then we consider a scenario that includes jobs a1 , a1 , . . . , a , a and job B 0 . Here, job
B 0 with type B 0 = (0, 2, , ), is a shadow job of B. Similarly, we claim that an
IC mechanism should schedule job B 0 at time 0 and complete it at time . Thus,
the mechanism only obtains a social welfare of v(B 0 ). However, one of the optimal
mechanisms will schedule and complete all  small jobs at from t = 0 to   1, and
then schedule and complete job B 0 . This leads to a social welfare at least
Z 


X
X
t
t
t
1
1
de  1 +
e +  = r +
e  1 + 
e +   r +
r

t=r+1

t=de
r
1



1



1


+ e
+  = f (, r) + e
+  = f (, r) + eln 1 + 

ln 
ln 
=f (, r) +   +  = f (, r) + 2 
+  = f (, r) + ( 
+ 1)v(B 0 ).
e
e
e
=r  e

(3)

When   16, we have e  ln . Then the above equation is larger than f (, r) +
( + 1)v(B 0 ). Therefore the ratio is  + 1  o(1).
Combining the three cases together, we prove the nonexistence of ( ln + 1  o(1))competitive mechanisms. Since f (, r)  0.06 when   16, the competitive ratio is at
least ln + 0.94 for   16.

3. Mechanism Design
In this section, we describe a simple mechanism 1 (whose allocation and payment rules
are given in Algorithm 2), which works surprisingly well for both the restart and resume
440

fiEfficient Mechanism Design for Online Scheduling

models, and handles the settings with different values of  in a unified framework. In
contrast, previous works (Durr et al., 2012) need to design separate and very different
algorithms to deal with different values of .
3.1 The Mechanism 1
Before introducing our mechanism, we first introduce the concept of the valid active time
of an uncompleted job j, until time t, denoted as

(
t  min{s|x(t0 ) = j, t0  [s, t)}, for the restart model
ej (t) := R t
for the resume model
0 (x(s) = j)ds,

(4)

where x(t) is the mechanisms allocation function, which maps each time point to an available job, or to 0 if the machine is idle.8 And () is an indicator function that returns 1
if the argument is true, and zero otherwise. Note that ej () can also take a vector  as an
argument. For example, ej (, t) is shorthand for the ej (t) for the job sequence .
It can be seen that in the restart model, at time t, if a job j has received an allocation
at time t0 < t and has not been preempted after that, then ej (t) = t  t0 . In the resume
model, ej (t) is the accumulated processing time of job j until time t.
We say that a job j is feasible at time t if (1) its reported release time is before t; (2) it
has not been completed yet; and (3) it has enough time to be completed before its reported
deadline, i.e., dj  t  lj  ej (t). We use JF (t) to denote the set of all feasible jobs at time
t.
According to Algorithm 2, at any time t, 1 assigns a priority score, vj   lj ej (,t) ,
to each feasible job j  JF (t), and always processes the feasible job with the highest
priority (ties are broken in favor of the job with the smaller rj ). Here  is located in (0, 1)
and will be determined later during the competitive analysis. The payment rule of 1 is
essentially the critical-value payment (Parkes, 2007), which is similar to that of the secondprice auction. Hence, the payment is equal to the minimum bid the agents have to make to
remain allocated.9 In the following pseudocode, j denotes the reported types of all jobs
other than j.

8. In Equation 4, since s=t is a valid candidate for the minimization, if there does not exist an s, s.t.,
x(t0 ) = j, t0  [s, t) in the restart model, then ej (t) = 0.
9. Note that we use the critical-value payment, so the payment of a completed job j depends on other
jobs types between rj and dj . If our mechanism allows returning completed job before its reported
deadline, the calculation of critical-value payment will face trouble: it is possible that agent j misreports
a much later deadline to obtain a cheaper payment, but his job is completed and returned before its
true deadline. That is the reason why we restrict our mechanism to return completed job at its reported
deadline. It is worth mentioning that, if the payment must be made when the job is completed, (Lavi
& Nisan, 2015) has shown that there is no incentive compatible mechanism which can obtain a constant
competitive ratio.

441

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

Algorithm 2:
Allocation Rule
for all time t do
if JF (t) 6=  then
x(t)  arg maxjJF (t) (vj   lj ej (,t) )
else x(t)  0
end
Payment Rule
for all job j do
if qj (, dj ) = 1 then
pj () = min(v 0 |qj (((rj , dj , lj , v 0 ), j ), dj ) = 1)
j

j

else pj () = 0
end
The intuition of our mechanism is two-fold. First, to ensure efficiency, one must trade
value against length: a job with a larger value has a higher priority, and a job with a larger
remaining length has a lower priority. 1 uses a simple priority function to achieve the
tradeoff: as can be seen, the priority score vj   lj ej (,t) of a job is positively correlated
with its value and negatively correlated with its remaining length. Second, to ensure IC,
1 uses the critical-value payment rule and a monotone10 allocation rule.
Note that both the allocation rule and the payment rule can be implemented efficiently.
For the allocation rule, it is enough to consider the time point when some new jobs arrive
or some existing jobs are completed. And, we give algorithms in Appendix A to show that
the payment for each agent can be computed in polynomial time.
Clearly, because of the critical-value payment rule, 1 is individually rational. In the
following subsection, we prove its incentive compatibility.
3.2 Incentive Compatibility
We call an allocation rule of a mechanism monotone, if a job with truthfully reported type
j = (rj , dj , lj , vj ) cannot be completed in the mechanism, then a dominated11 declaration
of its type j = (rj , dj , lj , vj ) cannot make it completed either.
According to Theorem 1.13 of the work of Parkes (2007), in order to establish the
truthfulness of a mechanism, it is enough to prove the monotonicity of its allocation rule.
Theorem 3.1. Mechanism 1 is incentive compatible, in both the restart model and resume
model.
Proof. We prove the monotonicity of the allocation rule of 1 . Assume a job j is not
completed under 1 when j is truthfully declared (we denote this case as T rue). We now
show that j cannot be completed either by declaring j = (rj , dj , lj , vj ), where rj  rj ,
lj  lj , dj  dj and vj  vj . And we denote any such case as F alse.
Suppose job j has ever been executed for k > 0 times in the T rue case, we define the
following points in the execution of job j: let tsi and tpi be the ith time that job j starts
10. We have a strict definition of monotonicity at start of Section 3.2.
11. We say a type j is dominated by type j (denoted as j  j ) if rj  rj , dj  dj , 
lj  lj and vj  vj .

442

fiEfficient Mechanism Design for Online Scheduling

execution and is preempted respectively, where i = 1, 2, . . . , k, and let ta = arg inf t (ej (t) +
dj  t < lj ) be the time that job j is abandoned. If job j is never started, then we set
ts1 = tp1 = ta .
We also refer to P = [rj , ts1 )  [tp1 , ts2 ) . . .  [tpk , ta ] = P0  P1 . . .  Pk as the pending period
of job j, and A = [ts1 , tp1 )  [ts2 , tp2 ) . . .  [tsk , tpk ) = A1  A2 . . .  Ak as the executing period of
job j.
We first consider monotonicity with regard to rj , regardless of other variables. Clearly,
from the definition of ta , declaring rj > ta could not cause the job to be completed. Thus,
we can restrict our attention to rj  [rj , ta ] = P  A.
A necessary condition for job j to be completed (in F alse) is that job j should be
executed sometime in the period P . However, according to Lemma 3.2 (see below), job j
cannot be executed in P . Therefore, declaring rj  rj cannot cause the job to be completed.
Intuitively, Lemma 3.2 says that, under case T rue and F alse, the set of jobs that are
scheduled in the period P must be the same. Thus, job j cannot be executed in period P .
We then consider dj , lj and vj . The proof is essentially the same as the proof of rj :
declaring dj  dj , lj  lj and vj  vj will not improve job js priority, and as a result, there
cannot be a change in the execution of jobs in the pending period P . So declaring dj  dj ,
lj  lj and vj  vj cannot cause the job to be completed. This proves that the allocation
rule of 1 is monotone.
In the following, we formally introduce Lemma 3.2, which is used in the above theorem.
For this lemma we introduce some additional notation: under case T rue and F alse, denote
by J and J respectively the set of jobs which have ever been executed in P , and denote
by I and I respectively the set of jobs which have ever been pending in A.
Lemma 3.2. (1) I  J = , (2) I  J = , (3) J = J.
Proof. Consider a job i  I, according to the defintion of I, under case T rue, job i has
lower priority than job j in period A  P .
Relation (1) means that, under case T rue, job i cannot be executed in period P . It is
obvious, since job j (with higher priority than i) is pending in period P .
Relation (2) means that, under case F alse, job i cannot be executed in period P either.
We prove this by contradiction. Suppose job i is executed at some time point in P . We
denote ti = min{t  P |x(t) = i}, and assume ti  Pn for 0  n  k. We have an observation
for the pending period Pn , 0  n  k.
h(n)

Observation 3.3. In pending period Pn , if 1 schedules jobs by a sequence12 of jn1 . . . jn
(h(n)  1, is the number of such active jobs in Pn ) under case T rue, then we know (1) the
h(n)
release time of each job jn2 . . . jn is in the period Pn ; in particular, the release time of job
h(n)
jn1 in Pn (n  1) is exactly time tpn (2) each job jn1 . . . jn is either completed or abandoned
in Pn ; and there is no idle time in Pn .
Here, we use fj (t) to denote the priority of job j at time t. Suppose that, under case
h(n)
T rue, it is job jni (one of jn1 . . . jn ) that is executed at ti , and its priority is fjni (ti ). Then
12. A job may appear more than once in the sequence if it is preempted and resumed/restarted later.

443

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

under case F alse, since i is executed at ti , according to Observation 3.3, we can deduce
that the priority of job i at time ti , i.e., fi (ti ) must be larger than fjni (ti ).
Therefore, we can deduce that i must have been executed sometime in the period Ui =
(A1 . . .An ). Otherwise, i should also be executed at time ti under case T rue, contradicting
the fact that i  I. Similarly, we denote si = min{t  Ui |x(t) = i}, and assume si  Am
for 1  m  n.
We claim, under case F alse, the priority of job i at time si , i.e., fi (si ) satisfies the
inequality as below.
(
p
fjni (ti )   |An |+|An1 |++|Am+1 |+|tm si | ,
fi (si ) >
p
fjni (ti )   |tm si | ,

if m  n  1;
if m = n.

Otherwise, the priority of job i at time ti is at most fjni (ti ) (consider the case that all the
periods [si , tpm ), Am+1 , . . . , An1 , An are allocated to i).
According to the definition of si , we know si is the first time that i is executed in period
A. Therefore, the priority of job i at si remains the same when shifting from case T rue
to case F alse. However, under case T rue, job j is executed at time si (hence, with a
priority larger than fi (si )), and all the periods [si , tpm ), Am+1 , . . . , An1 , An are allocated to
j. Therefore, at time ti , job j will have a priority larger than fjni (ti ), contradicting the fact
that jni is executed at time ti .
Relation (3) means that, no matter case T rue or case F alse, the jobs that are executed
in the period P are the same. Relation (3) can be derived naturally from Relation (2).

4. Competitive Analysis
In this section, we show that mechanism 1 performs quite well in terms of social welfare
by comparison with the optimal offline allocation, which has full knowledge of the future
jobs at the beginning of the execution.
To perform the competitive analysis, we need to design virtual charging schemes. Under
a certain virtual charging scheme, for every job j completed by the optimal allocation opt,
we charge its value (or partial value) to some job f completed by 1 . If this virtual charging
scheme satisfies the property that every job f completed by 1 receives a total charge of
at most cvf , then we succeed in showing that 1 has a competitive ratio of at most c.
Designing an ingenious virtual charging scheme is crucial to the competitive analysis. In
the following, we will design different virtual charging schemes to obtain the competitive
ratio of 1 for the restart model and the resume model respectively.
As we use a parameter  in the priority function of mechanism 1 , we first derive
competitive ratios as functions of . We will specify later (in Section 4.3) how to choose
a suitable  (with respect to ) to optimize the performance of 1 , and derive competitive
ratios in terms of .
Here, we introduce some notation which will be used in both Section 4.1 and Section 4.2.
Denote by (1, 2, . . . , F ) the sequence of jobs completed by 1 over time. For each job f in
this sequence, let tf be the time when job f is completed, and for convenience denote t0 = 0.
Divide the time into F + 1 intervals If = [tf 1 , tf ), f = 1, 2, . . . , F , and [tF , +).
444

fiEfficient Mechanism Design for Online Scheduling

4.1 Analysis of the Restart Model
We study the restart model first. We assume, without loss of generality, that the optimal
allocation opt does not interrupt any allocation, since all interrupted jobs are non-resumable.
We have the following theorem.
Theorem 4.1. For the restart model, 1 has a competitive ratio of

1
1

+

1


+ 1.

Proof. We introduce the virtual charging scheme as follows. For any completed job j in
opt, if it is also completed in mechanism 1 , then its value is charged to itself.
Otherwise (i.e., job j is not completed by 1 ), we consider the time sj at which j begins
execution in opt. Note that opt does not interrupt any allocation, so j is exactly allocated
the time period [sj , sj + lj ). Then sj must be in some time interval If (recall If = [tf 1 , tf )),
and we charge the value of j to f . Define j := tf  sj to be the time amount between sj
and tf . As job j is feasible at time sj , according to Lemma 4.2, we know that the priority
jobs j at time sj is at most vf  tf sj = vf  j ; in the meanwhile, the priority of j at time
sj is vj  lj . We have vj  lj  vf  j , i,e., vj  vf  j lj . We defer the formal statement and
the proof of Lemma 4.2 to the end of this subsection.
We now calculate the maximum total value charged to a completed job f in 1 . In the
time interval If , denote by (1, 2, . . . , m), the sequence of jobs in opt whose starting time sj
belongs to If and ordered as s1 > s2 >    > sm . Remember that we define j := tf  sj to
be the time amount between sj and tf . Then it is clear that we have 0 < 1 < 2 <    < m
and j  lj  j1 for 2  j  m, since j is allocated and completed during time interval
[sj , sj1 ]. Furthermore, as the job lengths are normalized, i.e., 1  lj  , we can deduce
that:
(
0
for j = 1
j 
(5)
j  1 for j  2.
Recall that  < 1 P
and f may also be completed in opt. Therefore the total charge to
job f is at most vf + m
j=1 vj , which is upper bounded by
vf + vf

m
X
j=1



j lj

 vf (1 + 

l1

+

m
X



j1

)  vf (1 + 

l1

+

j=1

j=2

1
+
This shows that mechanism 1 is ( 1

m1
X

1


j

 )  vf (1 + 



+


X
j=0

+ 1)-competitive.

1
Actually, the competitive ratio obtained in this way is tight, i.e., the ratio 1
+ 1 + 1
is best possible for 1 . We give an example in Appendix B to show tightness.

Lemma 4.2. For any time point sj  If , if job j (6= f ) is feasible at time sj , then the
priority of j at sj is at most vf  tf sj . Moreover, the value of j, vj , is at most vf  tf sj lj .
Proof. Note that, sj is in time interval If , and according to the definition of If , we know
that f is the unique job that is completed in If by 1 . Now we prove the lemma by
enumerating all possible cases.
(1) If the executing job at sj is job f , then we know that the priority of job f at time
sj is exactly vf  tf sj (because the priority of job f at time tf is vf ). Clearly, the priority
of j at sj is not larger than that of job f , and thus not larger than vf  tf sj .
445

 j ).

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

(2) If the executing job at sj is not job f , then we assume that 1 executes job j1 , . . . , jk
and f successively13 in the time period [sj , tf ), where k  1. Since f is the unique job
completed in If , we can deduce that: j1 is preempted by j2 , j2 is preempted by j3 ,...,jk is
preempted by f , and finally f is completed at time tf . Denote 1 , . . . , k as the time points
at which j1 , . . . , jk are preempted respectively. We also denote fj (t) as the priority of job
j at time t. We now use backward induction: First, we know that the priority of job jk at
k is not larger than that of job f , i.e., fjk (k )  vf  tf k . Then, since jk1 is preempted
by jk at k1 , we know that the priority of jk1 at k1 is not larger than that of jk .
Hence, we have fjk1 (k1 )  fjk (k1 ) = fjk (k ) k k1  vf  tf k1 . And eventually,
we can get that fj1 (1 )  vf  tf t1 . Since j1 is executed at time sj , we can deduce that
fj1 (sj )  vf  tf sj . Clearly, the priority of j at time sj (i.e., vj  lj ) is not larger than that
of j1 , thus not larger than vf  tf sj .
By arranging vj  lj ej (sj )  vf  tf sj , we can get vj  vf  tf sj lj +ej (sj )  vf  tf sj lj ,
where ej (sj )  0 is the valid active time of job j at time sj .
Some remarks on Lemma 4.2: (1) Because f is the unique job completed by 1 in the
time interval If , the priorities of the executing jobs monotonically increase during If . (2)
Lemma 4.2 applies in both the restart model and resume model. (3) Lemma 4.2 provides
a useful tool to relate the priority of a feasible job (j) at some time point (sj  If ) to the
completed job f .
4.2 Analysis of the Resume Model
Compared with the restart model, the competitive analysis for the resume model is much
more complicated, because in the resume model, a job can be executed in several disjointed
time intervals. The charging scheme used in the previous subsection no longer works, and
we need to design a new virtual charging scheme.
Before introducing the new virtual charging scheme, we introduce some notation that
will be used in this subsection. Let (j) denote the number of disjoint time segments
(j)
allocated to a completed job j in opt, and s1j , s2j , . . . , sj denote the corresponding starting
time of each segment.
We say an allocation contains a violation if there exist two completed jobs i and j, each
of which has two segments with starting time sai , sci and sbj , sdj such that sai < sbj < sci < sdj .
An allocation is called standard if it does not contain a violation. This means if an allocation
is standard, for any completed job, if its starting time of execution is between two segments
of another jobs allocation, then its completion time is also in the same time interval (i.e.,
between the same two segments). We provide an obvious yet useful fact for the offline
optimal allocation below.
Claim 4.3. There exists an optimal allocation that is standard.
For the detailed proof, please refer to Appendix C. Without loss of generality, we assume
that the optimal allocation opt is standard.
Claim 4.4 presents an important property of the standard allocation, which will be used
in the following proofs.
13. Here, j1 can be job j, which does not affect the analysis.

446

fiEfficient Mechanism Design for Online Scheduling

Claim 4.4. Under the execution of opt, if a job js execution-starting time is between two
segments of another jobs allocation, then job js completion time is also in the same time
interval (i.e., between the same two segments).
To analyze the competitive ratio of 1 for the resume model, we propose two new virtual
charging schemes (referred to as integral charging scheme and segmental charging scheme,
respectively). In the integral charging scheme, we charge the whole value of job j in the
optimal allocation opt to some job completed by mechanism 1 ; while in the segmental
charging scheme, we charge the value of j by segment, and different segments of the same
job may be charged to different jobs completed by mechanism 1 . By using these two
 
schemes, in Theorem 4.5 we upper bound the competitive ratio of mechanism 1 by 1
+1
1
2
and   +  ln  +1 respectively. As discussed in Section 4.3, the two ratios work for situations
with different  values, i.e., the first one works well for small  and the second one works
well for large .
Theorem 4.5. For the resume model, the competitive ratio of 1 is at most

 
1

 

+ 1. In

particular, if  satisfies    , the competitive ratio of 1 is at most min{ 1 + 1,
2
 ln  + 1}.

1


+

The proof of the theorem will be given in Section 4.2.1 and Section 4.2.2.
4.2.1 Integral Charging Scheme
Remember that we denote (1, 2, . . . , F ) as the sequence of jobs completed by 1 over time.
For each job f in this sequence, we denote the tf as the time that job f is completed.
In the integral charging scheme, we restrict the total number of jobs (excluding f itself)
that charged to job f : we does not allow this number to exceed btf tf 1 c+1. In particular,
we introduce the notation of saturation  in Definition 4.6.
Definition 4.6 (Saturated). For any job f , if the number of jobs (excluding f itself )
charged to f is less than btf  tf 1 c + 1, we say that f is unsaturated; otherwise f is
saturated.
Let W denote the set of jobs completed by opt, and Wf  W denote the set of jobs
j  W with s1j  If . Let A denote the set of jobs in W whose values have already been
charged to some jobs completed by 1 .
The integral charging scheme is described in Scheme 1. For simplicity, we refer to Line
1  2 as Step 1, Line 4  11 as Step 2, and Line 12  21 as Step 3.
Here we give some intuitive explanations about Step 2 and Step 3.
In Step 2, for each job f (f = 1, . . . , F ), we pick at most btf  tf 1 c + 1 jobs from Wf
and charge their values to f . The rule of picking jobs follows largest s1j first, and the k-th
picked job14 with s1j no later than tf  k + 1.
14. By slight abuse of notations, we still denote it as job j, and thus the start time of its first segment is s1j .

447

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

SCHEME 1: Integral Charging Scheme
1: Initial: A  .
2: For any job in W , if it is also completed by mechanism 1 , charge its value to itself,
and add it to A.
3: while W \ A 6= , do
4:
for f = 1 to F , do
5:
for k = 0 to btf  tf 1 c, do
6:
J k := {j 0 | (s1j 0  tf  k)  (j 0  Wf \ A)};
7:
if J k 6= , then
8:
Set j = arg maxj 0 J k (s1j 0 ), add j to A, and charge its value to f .
9:
end if
10:
end for
11:
end for
12:
for f = F to 1, do
13:
while Wf \ A 6= , do
14:
Set j = arg maxj 0 Wf \A (s1j 0 ), and add j to A;
15:
16:
17:
18:
19:
20:
21:
22:

(j)

if sj  If +hj for some 0  hj  F  f , then
Charge js value to the unsaturated job with smallest completion time in the
set {f + 1, . . . , f + hj };
(j)
else if sj  [tF , +), then
Charge js value to the unsaturated job with smallest completion time in the
set {f + 1, . . . , F };
end if
end while
end for
end while

In Step 3, we consider jobs (in W ) whose values are not charged to any job in the first
(j)
two steps. Consider a job j with s1j located in interval If and sj
located in If +hj (or
[tF , +)). We charge its value to an unsaturated job in the job set {f + 1, . . . , f + hj } (or
{f + 1, . . . , F }). The rule of selecting the unsaturated job follows smallest completion time
first.
We will show that after three steps all jobs in W are charged to some completed jobs in
1 (see Claim 4.9). First, we give two observations below.
Observation 4.7. In the integral charging scheme, for any job f  {1, 2, . . . , F } and any
time t  If , the number of jobs charged to f with their start time in opt being in [t, tf )
(charged at step 2) is no more than btf  tc + 1.
Observation 4.8. In the integral charging scheme, for any job f  {1, 2, . . . , F } completed
by mechanism 1 , the total number of jobs charged to f (excluding f itself ) is at most
btf  tf 1 c + 1.
Observation 4.7 is derived from Lines 5-6 in Scheme 1, and Observation 4.8 is derived
from the restriction that a saturated job can not be charged any more.

448

fiEfficient Mechanism Design for Online Scheduling

Claim 4.9. In the integral charging scheme, all jobs in W have been charged to some jobs
completed by mechanism 1 .
Proof. Suppose on the contrary that there exists i  Wf such that i is not charged to any job
Rt
in {f, f + 1, . . . , f + hi }. Here, we introduce a notation ei (t) = 0 (opt(s) = i)ds to denote
the valid active time of resumable job i at time t in opt. Since the length of every job is at
least 1,15 there exists an allocation segment [s0 , s00 ] of job i such that ei (s0 ) < 1, ei (s00 )  1,
and opt(t) = i for any t  [s0 , s00 ]. Suppose s00 belongs to If +h . By the definition of hi , we
have h  hi .
According to the assumption, we know: (a) i is not charged to f . (b) All jobs in
{f + 1, f + 2, . . . , f + h} have been saturated in the above charging process when we charge
job i.
From point (a), we can deduce that in Step 2, there are at least btf  s1i c + 1 jobs (whose
values are charged to f ) with s1j  (s1i , tf ] (by Observation 4.7). Otherwise i would be
charged to f in Step 2. We denote Ja as the set of these btf  s1i c + 1 jobs.
As for point (b), recall that a job f 0 (f 0  {f + 1, . . . , f + h}) is saturated if there are
bti ti1 c+ 1 jobs whose values are charged to f 0 (see Definition 4.6). Hence, we can deduce
that there are at least (btf +1  tf c + 1) +    + (btf +h  tf +h1 c + 1) jobs (whose values
are charged to {f + 1, . . . , f + h}) with their starting time satisfying s1j  (s1i , tf +h ]. In
particular, among these jobs, there are at most (btf +h  s00 c + 1) jobs with s1j  (s00 , tf +h ]
(whose value must be charged to f + h).16 Therefore, we can deduce that there are at least
(btf +1  tf c + 1) +    + (btf +h  tf +h1 c + 1)  (btf +h  s00 c + 1)
jobs (whose values are charged to {f + 1, . . . , f + h}) with s1j  (s1i , s00 ] (denote Jb as the
set of these jobs).
Note that Ja Jb = , as all jobs in Ja are charged to f , while all jobs in Jb are charged to
{f + 1, . . . , f + h}. Therefore, we deduce that the number of jobs with start time contained
in (s1i , s00 ] is at least |Ja | + |Jb |, i.e.,
(btf  s1i c + 1) + (btf +1  tf c + 1) +    + (btf +h  tf +h1 c + 1)  (btf +h  s00 c + 1)
>(tf +h  s1i )  (btf +h  s00 c + 1)  bs00  s1i c  1.
(6)
So, there are more than bs00  s1i c  1 jobs different from i in [s1i , s00 ]. Recall that we assume
opt is standard, hence, these jobs are entirely scheduled in (s1i , s00 ), i.e., all time segments
of such a job are allocated in (s1i , s00 ) (Claim 4.4). Since the length of every job is at least
1, we reach a contradiction.
According to the integral charging scheme, the charges to a completed job f have three
origins, corresponding to the three steps in Scheme 1. From Step 1, obviously, the charge
to job f is at most vf . We now calculate the maximum total charge from Step 2.
15. As stated in the Problem Formulation section, we assume job lengths are located in [1, ] for simplicity.
However, by scaling, all our results and proofs can be easily generalized to the case of [lmin ,   lmin ],
where lmin is the shortest length of jobs.
16. Because: (i) in Step 2, there might be at most (btf +h  s00 c + 1) jobs with s1j  (s00 , tf +h ] which could
be charged to f + h; (ii) in Step 3, the jobs with s1j  If +h could not be charged to f + h.

449

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

Suppose the total number of jobs charged to f from Step 2 is m. We rename them as
1, 2, . . . , m according to 1  2      m , and claim vj  vf  j lj (Lemma 4.2 is used
here), where j := tf  s1j , for 1  j  m. According to the rule of picking jobs in Step 2,
we have j  j  1. So it is clear that the sum of values of all these m jobs is at most
vf

m
X
j=1

 j lj  vf

m
X

 j1 = vf

j=1

m
X

 j .

(7)

j=0

It remains to calculate the maximum total charge from Step 3. According to Observation
4.8, we know that the number of jobs charged to f from Step 3 is at most btf tf 1 c+1m.
Now we need to bound the value of each such job j. The key is to build a relationship
between its value and the value of job f . However, according to the charging rule in Step
3, the start time s1j of job j is not located in the time interval If . In this case, we cannot
use Lemma 4.2 directly to derive an inequality like vj  vf  j lj . Because it remains to
check whether j is feasible at tf 1 (note that tf 1 is the left endpoint of time interval If ).
We define the critical time of a job as tj := dj  lj . If we can prove that tj  tf 1 , then
job j must be feasible at time tf 1 for 1 . Thus, by applying Lemma 4.2, we can easily get
vj  vf  tf tf 1 lj  vf  tf tf 1  .

(8)

Fortunately, the following lemma shows that tj  tf 1 holds.
Lemma 4.10. According to the charging scheme, if j  Wf is charged to a completed job
f + k (where 1  k  hj ), then the critical time of job j satisfies tj  tf +k1 .
Proof. We prove the lemma by contradiction and suppose tj < tf +k1 . Then the total length
(j)

of all the other jobs whose opt allocation is between s1j and sj
which is at most

(j)

is (sj

dj  s1j  lj = (dj  lj )  s1j = tj  s1j < tf +k1  s1j .

(j)

+ lj

)  s1j  lj ,

(9)

Since j is charged to f +k, from Step 3 we know that all jobs in {f +1, f +2, . . . , f +k1}
are saturated. Thus, there are at least
(btf  s1j c + 1) + (btf +1  tf c + 1) +    + (btf +k1  tf +k2 c + 1)  btf +k1  s1j c + 1 (10)
jobs whose start time belongs to the interval (s1j , tf +k1 ).
Recall that opt is standard. Hence, all these jobs allocated time segments are between
the first segment and the last segment of job j (according to Claim 4.4), Equation (9) and
Equation (10) constitute a contradiction since every jobs length is at least 1.
Combining the analysis above, we know that: (1) the total charge to f from Step 1 is
at most vf ; (2) assuming
jobs are charged to f from Step 2, the total charge from these
Pm m j
m jobs is at most vf j=0 
according to Equation (7); (3) the number of jobs charged
to f from Step 3 is at most btf  tf 1 c + 1  m according to Definition 4.6, and the value
450

fiEfficient Mechanism Design for Online Scheduling

of each job is at most vf  tf tf 1  according to Equation (8). Therefore, the total charge
to f is at most
vf + vf

m1
X

btf tf 1 c+1

 j + (btf  tf 1 c + 1  m)vf  tf tf 1   vf (1 +  

j=0

X

 j ),

j=0



which is upper bounded by vf (1 + 1
), indicating that the competitive ratio of mechanism

1 is upper bounded by

 
1

+ 1.

4.2.2 Segmental Charging Scheme
(j)

Recall that we use s1j , s2j , . . . , sj

to denote the starting time of all time segments allocated

(j)
1j , 2j , . . . , j

(j)

to job j in opt. Let
denote those time segments, and lj1 , lj2 , . . . , lj denote
the length of them.
In the segmental charging scheme, each segment kj is given a value j ljk , in which
v
j := ljj is the value density of job j. We describe the segmental charging scheme in
Scheme 2. For simplicity, we refer to Line 2  3 as Type-1 charge, Line 4  5 as Type-2
charge, and Line 6  7 as Type-3 charge.
SCHEME 2: Segmental Charging Scheme
1:
2:
3:
4:
5:
6:
7:
8:
9:

for each segment kj in opt do
if mechanism 1 also completes j by its deadline, then
Charge the value j ljk to j.
else if skj  If for some f  {1, 2, , . . . , F }, and j  vf  j 1 , where j := tf  skj ,
then
Charge the value j ljk to f .
else
Charge j ljk to f  , where f  is the first job completed by 1 from time tj on,
where tj is the critical time of job j.
end if
end for

It is clear that the Type-1 charge received by a job f is at most vf . Next, we bound the
Type-2 and Type-3 charges.
v

Lemma 4.11. The total Type-2 charge that a job f receives is at most   lnf  .
Proof. Let R2 denote the set of job segments whose charges to f are Type-2. For each
kj  R2 , the charge from it is j ljk . And from Line 4 in Scheme 2, we know j  vf  j 1 ,
where j = tf  skj . Thus the total Type-2 charge is at most
X
kj R2

j ljk

 vf

X
kj R2

 j 1 ljk

 vf

X Z
kj R2

j

j ljk



x1

Z
dx  vf
0



 x1 dx  

vf
,
 ln 

where the second inequity holds by  < 1. Therefore, f receives a total Type-2 charge of
v
at most   lnf  .
451

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

In the following, we study the Type-3 charge and denote R3 as the set of job segments
which constitute Type-3 charges to f .
First, we claim that, if  satisfies some condition, then we can get [skj , skj + ljk ]  [tf , tf +
lj ]  [tf , tf + ] for each kj  R3 (Claim 4.12).
Claim 4.12. If  satisfies the function: g(x) = x x   for 1  x  ; then we have
[skj , skj + ljk ]  [tf , tf + lj ], for each kj  R3 .
Proof. To prove [skj , skj + ljk ]  [tf , tf + lj ], we only need to prove the inequality below:
tf  skj  tf + lj  ljk .

(11)

The inequality skj  tf + lj  ljk holds because (skj + ljk )  lj  dj  lj = tj  tf .
Next we prove tf  skj . Suppose skj  If 0 for some f 0 (If 0 is not later than If , and might
equal If ). Then according to the Type-3 charging rule, we have j =

vj
lj

0

> vf 0  j 1 , where

j0 = tf  skj .
We now use the condition for : g(x) = x x   for 1  x  . Then we have
0
v
v
l
lj  j  , hence ljj  vj  lj 1 . Combining the above two inequalities ( ljj > vf 0  j 1 and
0

vj
lj

0

 vj  lj 1 ), we have vj  lj > vf 0  j , thus vj > vf 0  j lj , which contradicts the fact
that f 0 is completed at tf 0 with priority vf 0 (Lemma 4.2 is used here). Therefore, we have
tf  skj .
By Claim 4.12, we know the allocation of all the segments P
with Type-3 charges to f are
in a restricted interval [tf , tf + ]. Hence, we can derive that k R3 ljk  .
j

Lemma 4.13. If  satisfies the function: g(x) = x x   for 1  x  ; then the total
 ).
Type-3 charge that a job f receives is at most vf ( 1
ln  + 
Proof. According to the Type-3 charging rule, j is not completed by the mechanism; if we
consider the critical point of j, i.e., tj (in time interval If ), then by applying Lemma 4.2, we

v
v
can deduce that vj  lj  vf  tf tj  vf . Therefore we have ljj  flj . Now we can bound
lj 

the total Type-3 charge that f receives
X
kj R3

j ljk =

X vj
X vf
X ljk
k
ljk 
l
=
v
,
f
lj
g(lj )
lj  lj j
k
k
k

j R3

j R3

Note that the function g(lj ) = lj  lj is increasing for 1  lj 
1
lj  ln
 . So we have
g(lj ) 

(

 

for 1  lj 
for

1
ln 

(12)

j R3

1
ln 

 lj  , if  >

1
ln  .

1
ln 

and decreasing for

(13)

By Claim 4.12, we know [skj , skj + ljk ]  [tf , tf + lj ]  [tf , tf + ] for each kj  R3 .
1
a
Therefore, on the one hand, for each kj  R3 with ln
  lj   (denote this set as R3 ),
452

fiEfficient Mechanism Design for Online Scheduling

k
kj R3a lj

 ; on the other hand, for each kj  R3 with lj 
P
1
set as R3b ), we have k Rb ljk  ln
.
3
j
Then, (12) becomes

we have

P

X
kj R3

X

j ljk  vf

kj R3

X ljk
X ljk
ljk
= vf (
+
)
g(lj )
g(lj )
g(lj )
k
a
k
b
j R3

P
 vf (

1
ln 

kj R3a

 

ljk

P
+

kj R3b



j R3

ljk

)  vf (

which means the Type-3 charge is bounded by vf ( 1
ln  +

(denote this

(14)

1
ln 


+
),
 


1
  ).

Based on Lemmas 4.11 and 4.13, we can obtain that when    ,17 the total charge
to a job f completed by mechanism 1 is at most vf ( 1 + 2
ln  + 1). This implies that the
1
competitive ratio of mechanism 1 is upper bounded by   + 2
ln  + 1.
4.3 Discussions
An advantage of our mechanism is that it can handle the settings with different values of
 in a unified framework. We only need to set parameter  to different values in Theorem
4.1 and Theorem 4.5 so as to adapt to different settings of job lengths (as shown in the
following corollaries).
Corollary 4.14. By setting  = 1(1)2  ln , where  > 0 is an arbitrary small constant,
1

mechanism 1 achieves a competitive ratio ( (1)
2 + o(1))  ln  for the restart model and a
2
competitive ratio ( (1)
2 + o(1)) 


ln 

for the resume model.

The proof can be found in Appendix D. As for Corollary 4.14, we have the following
discussions:
1

(1) For the restart model, mechanism 1 achieves a competitive ratio of ( (1)
2 +o(1)) ln  ,
5

6
6
which improves upon the best-known algorithmic result log
 + O( ) (Ting, 2008) for
the standard online scheduling without strategic behavior.

(2) For the resume model, when  is large, mechanism 1 achieves a competitive ratio of
2

( (1)
2 + o(1))  ln  , which is slightly worse than the result obtained for the restart
model (within a factor of 2). Asymptotically speaking, 1 is near optimal, since
its competitive ratio has the same order (w.r.t. ) as the lower bound shown in
Theorem 2.5. Furthermore, our analysis generalizes the results obtained by Durr et
al. (2012) to the continuous value of time and the strategic setting.
(3) When  is relatively small, the ratios given in Corollary 4.14 will become loose. In
particular, when  approaches 1, the above ratios will approach infinity since ln 
approaches 0. In this case, we need a different setting of  (see Corollary 4.15).
1
1
17. Note that, the function g(x) = x x is increasing for 1  x  ln
and decreasing for x  ln
. Therefore,


we only need to require    , and we can naturally derive g(x) = x x   for 1  x  .

453

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng


Corollary 4.15. By choosing  = +1
, the competitive ratio of mechanism 1 is  + 2 +
1 
(1 +  ) <  + 2 + e for the restart model and ( + 1)(1 + 1 ) + 1 for the resume model.

Similarly, we have the following discussions:
(1) The competitive ratio of 1 is linear in , since (1 + 1 ) is bounded by e.
(2) In particular, when  = 1, the ratios in the above corollary become 5 for both the
restart and resume model, which matches the lower bound given in Theorem 2.4. In
this regard, we say that 1 is optimal. On the other hand, this also shows that the
lower bound of 5 in Theorem 2.4 is tight.

5. Conclusion and Future Work
In this paper, we studied the online scheduling problem in a strategic setting. As summarized in Table 2, we proved that in both the restart model and the resume model, the
competitive ratio of any IC mechanism cannot be less than 5 when  = 1 and cannot be less
than ln + 1  o(1) for large . We designed a simple IC mechanism 1 to schedule jobs on
a single machine and proved that it has near optimal approximation guarantees (in terms
of social efficiency) in both the restart model and the resume model through competitive
analysis: as shown in Table 2, the mechanism is optimal in terms of competitive ratio in
both the restart model and the resume model when  = 1, and it is near optimal for the
restart model when  is large enough.
Table 2: Summary of bounds on competitive ratio
Restart Model
Resume Model
Model
=1
asymptotic 
=1
asymptotic 


LB for any IC mech.
5
+
1

o(1)
5
ln 
ln  + 1  o(1)
1
2


UB of the proposed mech.
5
( (1)
5
( (1)
2 + o(1))  ln 
2 + o(1))  ln 
In proving the lower bounds, we introduce the shadow job argument which reflects the
IC constraint. This argument is very helpful in extending bounds in non-strategic setting
to strategic setting. The second contribution of this work is that we design several virtual
charging schemes to analyze the competitive ratio of our mechanism. The ideas of these
virtual charging schemes are of methodological significance and may be used to address
other problems.
There are multiple directions to explore in the future.
It is an interesting problem whether an IC competitive mechanism can be designed for
the hybrid model, in which there exist both resumable and non-resumable jobs. Many new
strategic issues may arise in the hybrid model. For example, can a resumable job disguise
as a non-resumable job to get better off?
Another open problem is whether a tighter competitive analysis of 1 can be made for
the resume model. Our conjecture is that the competitive ratio obtained by 1 has an
1
+ 1 + 1, for both the restart model and the resume model.
uniform form: 1
Furthermore, given the popularity of cloud computing in todays IT industry, it is of
practical importance to extend our work to the setting of job scheduling on multiple heterogeneous machines.
454

fiEfficient Mechanism Design for Online Scheduling

Appendix A. Algorithms for the Critical-Value Payment
Please note that the critical time point in Algorithm 3 and Algorithm 4 means the time
point when some new jobs arrive or some existing jobs are completed.
Algorithm 3: Compute the critical-value payment in the restart model
for each job j which is completed do
Run Algorithm 1 without job j. Let T be the set of all critical time points
t  [rj , dj ).
for every t  T do
if there exists job k such that x(t) = k, then define ft = vk  lk ek (t) ;
else ft = 0;
end-for
for every time point t0  T  [rj , dj  lj ) do
0
Define ft0 = max{ft / t t : t  (T  [t0 , t0 + lj ))};
end-for
Let f  = mint0 ft0 .
pj = f  / lj .
end
Algorithm 4: Compute the critical-value payment in the resume model
for each job j which is completed do
Run Algorithm 1 without job j.
Let {t0 , t1 , . . . , tm } be the set (denoted as T ) of all critical time points in [rj , dj ),
and t0 = rj .
Denote the period between two critical time point as zi = ti  ti1 , where
i = 1, 2, . . . , m.
for every ti  T do
if there exists job k such that x(ti ) = k, then define fti = vk  lk ek (ti ) ;
else fti = 0;
end-for
Initially, T   , h = 0.
while h < lj do
t0 = arg minti T \T  fti , ties are broken in favor of smaller ti ;
Initially, e0 = 0;
0
for every time point ti  t0 that satisfies fti  ft0  e do
e0 = e0 + zi ; and
if ti 
/ T ,
then add ti to T  , and h = h + zi ;
end-for
end-while
Let t01 be the earliest critical time point in T  . Let t = arg maxti T  fti .
Denote the critical time points in T  and between t01 and t as t02 , t03 , . . . , t0k .
Denote the relevant periods of those critical time points as z10 , z20 , . . . , zk0 , and z  .
0
0
pj = ft / lj (z1 ++zk ) .
end
455

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

Appendix B. An Example for Analysis Tightness
Example B.1. There are two types of jobs: long and short. The length of long jobs is ,
while the length of short jobs is 1. Let p be a large integer, and the number of long and short
jobs are p + 1 and p  1 respectively. The first long job J0l is released at time 0, and its type
is 0l = (0, , , 1). For p  2  i  1, job Jil has type il = (i(  ), (i + 1)  i, ,  i ).
l
l
Long job Jp1
has type p1
= ((p  1)(  ), (p + 2), ,  (p1) + ). Job Jpl has type
l
p = (p(), (p+1)p, ,  p+ ). Here,  and  are small constants satisfying p  1
and   . In the meanwhile, we have short jobs as follows. For j = 1, . . . , p1, we denote
Jjs as the jth short job, whose type is js = (j (p1), j +1(p1), 1,  (j+1)+(p1)   ).
for j = 1, . . . , p  1.
l
It can be verified that only one job Jp1
can be completed in mechanism 1 , with
(p1)
a social welfare  
. While in the optimal solution, all the short jobs will be
l
l
will be completed successively, with a social welfare
completed, and after that, Jp and Jp1
2
3
(p1)
 (
+
+ ... + 
) +  (p1) +  p . Therefore, the competitive ratio of
p1
mechanism 1 is at least ( p2 + . . . +  + 1) + 1 +   = 1
+ 1 +   , which tends
1
1
to 1
+ 1 + 1, when p  .

Appendix C. Proof of Claim 4.3
Proof. Suppose an optimal allocation opt is not standard, i.e., there exist a completed job
i with two segments beginning at time sai and sci and a completed job j with two segments
beginning at time sbj and sdj such that sai < sbj < sci < sdj . We now do the following process
to obtain a standard optimal allocation: if the length of job js b-th segment (denote as ljb )
is larger than that of is c-th segment (denote as lic ), we exchange is c-th segment with js
b-th segment located in [sbj , sbj + lic ]; otherwise, we exchange js b-th segment with is c-th
segment located in [sci + lic  ljb , scj + lic ]. For all the other segments, their order remains
unchanged. It is easy to see that the new allocation is still feasible and obtains the same
social welfare. We do such kind of exchanges until there is no violation, and then obtain a
standard optimal allocation.

Appendix D. Proof of Corollary 4.14
Proof. For every constant c < 1 and large enough x, we have (1  xc )x  e. When  is
large enough, by choosing  = 1  (1  )2  ln , we have   1, and
(1  )2 ln   (1) ln  (1) ln 

)
 e(1) ln   (1) = o(
).

ln 
By using Taylors theorem, we know
  = (1 

 ln  = (1 + o(1))(1  ) = (1 + o(1)) 
Thus the competitive ratio is

1
1

c2 ln 
.


1

+ 1 + 1 = ( (1)
2 + o(1))  ln  for the restart model, and

2
 ln 

2

+ 1 + 1 = ( c2 ln  (2 + o(1)) + o( ln )) + 1 = ( (1)
2 + o(1))  ln  for the resume model,
respectively.

456

fiEfficient Mechanism Design for Online Scheduling

Appendix E. The Multiple Machines Extension
Suppose there are C identical machines, and each of them can process at most one job at
any given time. Similar to the work of Lucier et al. (2013), we assume that at most h
machines can be allocated to a single job at any given time. This parameter stands for a
common parallelism bound of the system.
The notion of preemption is specified as follow: A job may be processed on any number of
machines between 1 and h, and the number of machines allocated to this job may fluctuate,
and only if the number decreases to 0, we treat this job as preempted. Thus, the notation
of preemption-restart and preemption-resume can be defined accordingly.
Each job j  J is characterized by a private type j = (rj , dj , sj , vj ). Instead of lj , where
we use sj to denote jobs size (e.g., the number of machine hours required to complete the
job). Without causing any confusion, we let  be the maximum ratio between the sizes of
any two jobs:  = maxi,jJ ssji . For simplicity, we assume all job sizes fall in [1, ]. If  = 1,
all the jobs have the identical size; otherwise they have different sizes.
E.1 A Simple Case: h = 1
In this case, we design a new mechanism 2 based on the single-machine mechanism 1 .
The payment rule of 2 is exactly the same as 1 , and its allocation rule is shown in
Algorithm 2, which is also similar to that of 1 . Since each job can be processed on at
most one machine, the mechanism will choose the C jobs (if any) in JF (t) with the highest
priorities vi   si ei (,t) to execute. Note that here the valid active time of job j until time
t is computed as
C Z t
X
ej (t) =
(xi (s) = j)ds.
(15)
i=1

t0

P
where () is an indicator function, and t0 = arg maxst [ C
i=1 (xi (s) = j)] = 0. That is
to say, we treating resumable jobs as non-resumable jobs for simple. We summarize the
theoretical properties of 2 in Theorem E.1.
Algorithm 5: The allocation rule of Mechanism 2
for all t do
if |JF (t)|  C then
process the C jobs with highest priorities in JF (t);
else process all the jobs in JF (t).
end
Theorem E.1. Mechanism 2 is IC and has the following properties:

(+1) ,
1  (1  )2

 In the restart model, by setting  =
1 
)

we can get a competitive ratio of  + 2 +

(1 +
for 2 ; by setting  =
 ln for arbitrary small  > 0, we can get
1
another competitive ratio of ( (1)2 + o(1))  ln .
 In the resume model, by setting  = 1  (1  )2  ln for arbitrary small  > 0, we
2

can get a competitive ration of ( (1)
2 + o(1))  ln  .
As for the above theorem, we have the following discussions.
457

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

(1) Similar to what we have done in the single-machine setting, for the restart model,
we give two competitive ratios for 2 . When  is small, the first ratio is better
(in particular, when  = 1, this competitive ratio becomes 5 and is thus optimal
according to Theorem 2.4). When  is large, the second ratio is better instead and is
near optimal according to Theorem 2.5.
(2) Different from what we have obtained in the single-machine setting, for the resume
model, we cannot match the lower bound 5 when  = 1 in the multi-machine setting.
Proof. The proof of Theorem E.1 is essentially the same as the proof of single machine
setting, in our virtual charging scheme, we charge a completed job in the optimal allocation
to some job completed by 2 on the exactly same machine. The difference is that the
integral charging scheme for the resume model will not apply to the multiple machines
setting any more. We only use the segment charging scheme for the resume model.
E.2 General Case: h  1
To handle this general case, we design a new mechanism 3 , which divides the C machines
into bC/hc equally-sized virtual machines (each consisting of h machines), and treats every
virtual machine as a single machine when performing the scheduling. That is, each virtual
machine will be used to process one job, and the remaining C  bC/hc  h machines will be
idle.
Algorithm 6: The allocation rule of Mechanism 3
(1) Divide the C machines into bC/hc equal-sized virtual machines.
(2) Run mechanism 2 under the following modification:
 Capacity: bC/hc.
 Demand size: sj /h for each job j.
As compared to the case of h = 1, the setting h  1 imposes more flexibilities to the
optimal offline allocation. For example, a job may be processed on any number of machines
between 1 and h in the optimal allocation and it might not always be executed on exactly
h machines. Fortunately, we can use the similar segmental charging idea as h = 1 case to
resolve the challenge and get the competitive ratio as shown in the following theorem.
4

Theorem E.2. Mechanism 3 is IC and has a competitive ratio of ( (1)
2 + o(1))  ln  for

3 by setting  = 1  (1  )2 
resume model.

ln 


for arbitrary small  > 0, no matter restart model or

We have the following discussions for the above theorem. The setting of h  1 is more
complicated and we could not always obtain the same results as in the setting of h = 1.
In particular, if h divides C, there will be no idle machine and we may obtain the same
competitive ratio as in the setting of h = 1. However, when h does not divide C, the idle
machines will introduce an additional factor of at most 2 to the competitive ratio. Besides,
the competitive ratio for the restart model is no better than that for the resume model, and
the competitive ratio cannot reach 5 when  = 1.
458

fiEfficient Mechanism Design for Online Scheduling

Proof. Here we only need to show that there exists an optimal allocation (we can view
all jobs as resumable jobs in the optimal allocation) such that at any time every job is
processed on either exactly h machines or no machine (assuming h divides C). Then we
can directly use the results obtained for the special case h = 1. Suppose opt is an optimal
offline allocation and J  is the set of jobs completed under opt. For each j  J  , we use
mj (t) to denote the number of machines processing j at time t under opt. Then we can
divide the time into intervals [tk , tk+1 ), where k = 0, 1, 2,    , such that at any time interval
[tk , tk+1 ), mj (t) does not change for any j  J  . Now we show how to allocate the jobs at
any time interval [tk , tk+1 ). For the bC/hc virtual machines, we allocate the jobs on them
one by one, i.e., only if the previous virtual machine is full, we start to allocate jobs on
another empty virtual machine from tk (empty is only with respect to [tk ,Rtk+1 )). Besides,
t
every job is allocated continuously one by one and the size of allocation is tkk+1 mj (t)dt. It
can be easily verified that under this allocation every job j  J  is allocated legitimately
(j is allocated during [rj , dj ] and processed by at most h machines at any time) and can
complete before its deadline since j is legitimately completed under opt.

References
Azar, Y., Ben-Aroya, N., Devanur, N. R., & Jain, N. (2013). Cloud scheduling with setup
cost. In Proceedings of the twenty-fifth annual ACM symposium on Parallelism in
algorithms and architectures, pp. 298304. ACM.
Bar-Noy, A., Guha, S., Naor, J., & Schieber, B. (2001). Approximating the throughput
of multiple machines in real-time scheduling. SIAM Journal on Computing, 31 (2),
331352.
Baruah, S., Koren, G., Mao, D., Mishra, B., Raghunathan, A., Rosier, L., Shasha, D., &
Wang, F. (1992). On the competitiveness of on-line real-time task scheduling. RealTime Systems, 4 (2), 125144.
Baruah, S. K., Haritsa, J., & Sharma, N. (1994). On-line scheduling to maximize task
completions. In Proceedings of Real-Time Systems Symposium, pp. 228236. IEEE.
Borodin, A., & El-Yaniv, R. (1998). Online computation and competitive analysis, Vol. 2.
Cambridge University Press Cambridge.
Chin, F. Y., & Fung, S. P. (2003). Online scheduling with partial job values: Does timesharing or randomization help?. Algorithmica, 37 (3), 149164.
Ding, J., Ebenlendr, T., Sgall, J., & Zhang, G. (2007). Online scheduling of equal-length
jobs on parallel machines. In Proceedings of the 15th annual European conference on
Algorithms, pp. 427438. Springer-Verlag.
Ding, J., & Zhang, G. (2006). Online scheduling with hard deadlines on parallel machines.
In Algorithmic Aspects in Information and Management, pp. 3242. Springer.
Durr, C., Jez, L., & Nguyen, K. T. (2012). Online scheduling of bounded length jobs to
maximize throughput. Journal of Scheduling, 15 (5), 653664.
459

fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng

Ebenlendr, T., & Sgall, J. (2009). A lower bound for scheduling of unit jobs with immediate
decision on parallel machines. In Approximation and Online Algorithms, pp. 4352.
Springer-Verlag.
Friedman, E. J., & Parkes, D. C. (2003). Pricing wifi at starbucks: issues in online mechanism
design. In Proceedings of the 4th ACM conference on Electronic commerce, pp. 240
241. ACM.
Goldman, S. A., Parwatikar, J., & Suri, S. (2000). Online scheduling with hard deadlines.
Journal of Algorithms, 34 (2), 370389.
Goldwasser, M. H. (2003). Patience is a virtue: The effect of slack on competitiveness for
admission control. Journal of Scheduling, 6 (2), 183211.
Hajek, B. (2001). On the competitiveness of on-line scheduling of unit-length packets with
hard deadlines in slotted time. In Proceedings of the 35th annual Conference on
Information Sciences and Systems.
Hajiaghayi, M., Kleinberg, R., Mahdian, M., & Parkes, D. C. (2005). Online auctions with
re-usable goods. In Proceedings of the 6th ACM conference on Electronic commerce,
pp. 165174. ACM.
Kolen, A. W., Lenstra, J. K., Papadimitriou, C. H., & Spieksma, F. C. (2007). Interval
scheduling: A survey. Naval Research Logistics (NRL), 54 (5), 530543.
Lavi, R., & Nisan, N. (2004). Competitive analysis of incentive compatible on-line auctions.
Theoretical Computer Science, 310, 159180.
Lavi, R., & Nisan, N. (2015). Online ascending auctions for gradually expiring items.
Journal of Economic Theory, 156, 4576.
Lipton, R. J., & Tomkins, A. (1994). Online interval scheduling. In In Proceedings of the
Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, Vol. 94, pp. 302311.
Lucier, B., Menache, I., Naor, J. S., & Yaniv, J. (2013). Efficient online scheduling for
deadline-sensitive jobs. In Proceedings of the 25th ACM symposium on Parallelism in
algorithms and architectures, pp. 305314. ACM.
Ma, W., Zheng, B., Qin, T., Tang, P., & Liu, T. (2014). Online mechanism design for cloud
computing. CoRR, abs/1403.1896.
Mashayekhy, L., Nejad, M. M., Grosu, D., & Vasilakos, A. V. (2014). Incentive-compatible
online mechanisms for resource provisioning and allocation in clouds. In Cloud Computing (CLOUD), 2014 IEEE 7th International Conference on, pp. 312319. IEEE.
Nguyen, K. T. (2011). Improved online scheduling in maximizing throughput of equal length
jobs. In Computer ScienceTheory and Applications, pp. 429442. Springer.
Nisan, N. (2007). Introduction to mechanism design (for computer scientists). Algorithmic
game theory, 209, 242.
Nisan, N., & Ronen, A. (2001). Algorithmic mechanism design. Games and Economic
Behavior, 35, 166196.
Parkes, D. C. (2007). Online mechanisms. Algorithmic Game Theory, ed. N. Nisan, T.
Roughgarden, E. Tardos, and V. Vazirani, Cambridge University Press, 411439.
460

fiEfficient Mechanism Design for Online Scheduling

Porter, R. (2004). Mechanism design for online real-time scheduling. In Proceedings of the
5th ACM conference on Electronic commerce, pp. 6170. ACM.
Ting, H.-F. (2008). A near optimal scheduler for on-demand data broadcasts. Theoretical
Computer Science, 401 (1), 7784.
Wu, X., Gu, Y., Li, G., Tao, J., Chen, J., & Ma, X. (2014). Online mechanism design for
VMS allocation in private cloud. In Network and Parallel Computing, pp. 234246.
Springer.
Zaman, S., & Grosu, D. (2012). An online mechanism for dynamic vm provisioning and
allocation in clouds. In 5th International Conference on Cloud Computing (CLOUD),
pp. 253260. IEEE.
Zhang, H., Li, B., Jiang, H., Liu, F., Vasilakos, A. V., & Liu, J. (2013). A framework for
truthful online auctions in cloud computing with heterogeneous user demands. In
Proceedings of INFOCOM, pp. 15101518. IEEE.
Zheng, F., Fung, S. P., Chan, W.-T., Chin, F. Y., Poon, C. K., & Wong, P. W. (2006). Improved on-line broadcast scheduling with deadlines. In Computing and Combinatorics,
pp. 320329. Springer.

461

fiJournal of Artificial Intelligence Research 56 (2016) 269327

Submitted 12/15; published 05/16

Combining the Delete Relaxation with Critical-Path Heuristics:
A Direct Characterization
Maximilian Fickert
Jorg Hoffmann
Marcel Steinmetz

S 9 MAFICK @ STUD . UNI - SAARLAND . DE
HOFFMANN @ CS . UNI - SAARLAND . DE
STEINMETZ @ CS . UNI - SAARLAND . DE

Saarland University, Saarbrucken, Germany

Abstract
Recent work has shown how to improve delete relaxation heuristics by computing
relaxed plans, i. e., the hFF heuristic, in a compiled planning task C which represents
a given set C of fact conjunctions explicitly. While this compilation view of such partial
delete relaxation is simple and elegant, its meaning with respect to the original planning
task is opaque, and the size of C grows exponentially in |C |. We herein provide a direct characterization, without compilation, making explicit how the approach arises from
a combination of the delete-relaxation with critical-path heuristics. Designing equations
characterizing a novel view on h+ on the one hand, and a generalized version hC of hm
on the other hand, we show that h+ (C ) can be characterized in terms of a combined
hC+ equation. This naturally generalizes the standard delete-relaxation framework: understanding that framework as a relaxation over singleton facts as atomic subgoals, one
can refine the relaxation by using the conjunctions C as atomic subgoals instead. Thanks
to this explicit view, we identify the precise source of complexity in hFF (C ), namely maximization of sets of supported atomic subgoals during relaxed plan extraction, which is
easy for singleton-fact subgoals but is NP-complete in the general case. Approximating
that problem greedily, we obtain a polynomial-time hCFF version of hFF (C ), superseding
C compilation which achieves the
the C compilation, and superseding the modified ce
same complexity reduction but at an information loss. Experiments on IPC benchmarks
show that these theoretical advantages can translate into empirical ones.

1. Introduction
The delete relaxation in classical planning (McDermott, 1999; Bonet & Geffner, 2001) originates from work using a STRIPS representation, where state variables are Boolean, action
effects are conjunctions of literals, and action preconditions as well as the goal are restricted
to conjunctions of positive literals (facts). The relaxation assumes that there are no negative
(delete) effect literals, hence the name. More generally, i. e., with non-Boolean state variables, this amounts to assuming that state variables accumulate their values, rather than
switching between them. While optimal delete-relaxed planning is still NP-hard, satisficing delete-relaxed planning is polynomial-time (Bylander, 1994). Relaxed plan heuristics,
employing satisficing delete-relaxed planning for the generation of inadmissible heuristic
functions, have proved highly successful (e. g. Hoffmann & Nebel, 2001; Gerevini, Saetti,
& Serina, 2003; Richter & Westphal, 2010). They form a key ingredient for successful satisficing planning (not giving an optimality guarantee), in particular in almost all winners of
the satisficing-planning tracks of the International Planning Competitions (IPC).
c
2016
AI Access Foundation. All rights reserved.

fiF ICKERT & H OFFMANN & S TEINMETZ

Despite this success, the pitfalls of delete-relaxation heuristics (for example, ignoring
resource consumption) have been known since a long time, and there have been intense
efforts from the outset to take some deletes into account (e. g., Fox & Long, 2001; Do &
Kambhampati, 2001; Gerevini et al., 2003; Helmert, 2004; van den Briel, Benton, Kambhampati, & Vossen, 2007; Helmert & Geffner, 2008; Cai, Hoffmann, & Helmert, 2009; Baier
& Botea, 2009; Coles, Coles, Fox, & Long, 2013; Alcazar, Borrajo, Fernandez, & Fuentetaja,
2013). Two recent approaches, red-black planning (Domshlak, Hoffmann, & Katz, 2015) and
what we refer to as explicit conjunctions, were devised that allow to do so systematically:
partial delete relaxation, that can in principle render the heuristic estimate perfect. We herein
focus on explicit conjunctions.
To summarize our results in what follows, we need some basic notation and concepts
well known in the planning community. We denote the perfect heuristic, returning the precise remaining cost, by h ; the heuristic returning the cost of an optimal relaxed plan by h+ ;
and relaxed plan heuristics by hFF (from the system FF where these were first introduced,
see Hoffmann & Nebel, 2001). We assume the common method of computing relaxed plan
heuristics by relaxed plan extraction on a best-supporter function (Keyder & Geffner, 2008). We
will assume by default that the best-supporter function is derived from the max heuristic
hmax (Bonet & Geffner, 2001). Relaxed plan extraction on a hmax best-supporter function is,
on unit-cost problems, equivalent to the original formulation in terms of relaxed planning
graphs by Hoffmann and Nebel (2001).
Explicit conjunctions were first introduced by Haslum (2009) as a compilation-based
characterization of the critical-path relaxation introduced earlier on by Haslum and Geffner
(2000). This relaxation assumes that, from any goal set of facts (a fact conjunction that
needs to be achieved at some point during a plan), it suffices to achieve the most costly
subgoal (subconjunction) of size at most m. Here, m is a parameter and the corresponding
heuristic is denoted hm . The special case m = 1 is equivalent to the max heuristic, i. e., h1 =
hmax . Haslums (2009) compiled planning task m represents each size- m conjunction
c via a newly introduced -fluent c , and arranges the preconditions and effects on these
-fluents such that h1 (m ) = hm .
Subsequently, Haslum (2012) introduced the modified compilation C , which admits
arbitrary sets C of conjunctions and guarantees admissibility of h+ on the compilation,
i. e., that h+ (C )  h , which is not true of h+ (m ). He furthermore showed that this
method converges to h , i. e., that h+ (C ) = h for appropriately chosen C. The downside
of C is that its size is worst-case exponential in |C |: Say that an action a can support a
conjunction c if c can be regressed over a, i. e., a makes part of c true and makes none of c
false. In order to guarantee admissibility of h+ (C ), C explicitly enumerates all subsets
C 0  C of conjunctions c that any occurrence of an action a in the plan may support. This
C compilation (Keyder, Hoffmann, & Haslum, 2012,
size explosion was tackled by the ce
C still
2014), which handles each possibly-supported c by a separate conditional effect. ce
guarantees convergence, yet loses information as it ignores cross-context conditions, i. e.,
precondition -fluents which arise only from the combination of several supported c  C 0 .
One thing evident from this history of explicit conjunctions is that the resulting heuristic functions combine information inherent in the critical-path relaxation, with information
inherent in the delete relaxation. But in what way, exactly? While the compilation view is
simple and elegant, its meaning with respect to the original planning task is opaque.
270

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

A first simple observation is that the step from size- m conjunctions to arbitrary conjunctions C is not specific to the, historically, simultaneous step from critical-path to partial
delete relaxation heuristics. The hm heuristic is straightforwardly generalizable to consider,
not all size- m subgoals, but an arbitrary set C of subgoals. Intuitively, we can choose
any set of atomic subgoals, to be kept intact by the critical-path relaxation. We denote the
generalized heuristic by hC .
A second simple observation is that the delete relaxation can be viewed as allowing to
achieve each fact in separation: achieving the facts p in a goal set/conjunction one-by-one,
the negative effects within each step do not matter because they concern facts other than p.
Given this, h+ can be characterized in terms of an equation related to that characterizing
h1 , but requiring to achieve all size-1 subgoals instead of achieving only the single most
costly one.
Putting the two observations together, we obtain a natural generalization of the standard delete-relaxation framework: where the standard delete relaxation, like h1 , works with
singleton facts as its atomic subgoals, one can use the conjunctions C as atomic subgoals instead.
We spell this out in the form of two heuristic functions we denote by hC+ and hCFF :
(1) The h1 -like equation characterizing h+ translates into a hC -like equation characterizing
hC+ , equivalent to h+ (C ).
(2) Relaxed plan extraction to obtain hFF from a h1 best-supporter function translates into
relaxed plan extraction to obtain hCFF (a relaxed plan for C ) from a hC best-supporter
function.
Result (1) is of theoretical interest. It formulates hC+ = h+ (C ) without a compilation,
shedding a different light on Haslums (2012) equivalent proposal. Result (2) has more
immediate practical ramifications. It provides an alternative technique to obtain relaxed
plans for C , exponentially more efficient in the worst case because it does not require to
exhaustively enumerate subsets C 0  C. The hC best-supporter function can be computed
in time polynomial in |C |, similar to hm . Intuitively, as the critical path pertains to single
atomic subgoals, there is no need to enumerate combinations of atomic subgoals here. For
relaxed plan extraction, we avoid such enumeration by identifying and tackling the precise
source of complexity.
Relaxed plan extraction on hC is more complex than relaxed plan extraction on h1 for
two reasons, of which the first corresponds to Haslums (2012) observations, yet the second
one only becomes apparent in our new direct formulation:
(a) To ensure convergence, allowing hFF (C ) to find real plans in the limit, we need to
collect a set of action occurrences, i. e., pairs ( a, C 0 ) of action a and set of supported
conjunctions C 0 , instead of just a set of actions as in the standard setting (where actions
merely support the facts in their direct effect).
(b) In every occurrence ( a, C 0 ) selected during relaxed plan extraction, C 0 should be as
large as possible, as atomic subgoals (conjunctions) may now overlap, incurring the
risk of dramatic overestimation (e. g., achieving every fact pair in the global goal separately). But it is NP-complete to find a cardinality-maximal C 0 that does not incur
infeasible cross-context conditions.
To understand this, consider an action a which can support at least one subgoal c  C
during relaxed plan extraction. We need to decide which other current subgoals c0  C
271

fiF ICKERT & H OFFMANN & S TEINMETZ

to support with that same action occurrence. In the standard setting, where c and c0 are
singletons (e. g. c = { p} and c0 = {q}), one can simply support all c0 in as positive effects
(e. g. add( a) = { p, q}). In the general case, for arbitrary C, this is no longer so because
the part of c0 not contained in as positive effects gets propagated into the new subgoal
regressing over a (e. g. r1 for c10 = {q, r1 } and r2 for c20 = {q, r2 }). Combinations of several
c0 may incur cross-context conditions (e. g. {r1 , r2 }) harder to achieve than the conjunctions
c0 themselves in isolation.
We will refer to (b), maximization of |C 0 | during relaxed plan extraction, as the subgoalsupport selection problem. It is striking here that the underlying phenomena  supported
conjunction sets C 0 and cross-context conditions  were previously identified and addressed,
yet not put into the specific context relevant for relaxed plan extraction. From this perspective, Haslum (2012) solves an NP-complete problem enumeratively, putting all solution
candidates (choices of C 0 ) into memory in the form of the compiled task C ; and Keyder et
C compilation over-simplifies the problem, ignoring cross-context conal.s (2012, 2014) ce
ditions completely. Yet, if cardinality-maximal C 0 is the real issue, why dont we simply
select a subset-maximal C 0 instead? Using a simple greedy approximation to this effect, we
obtain hCFF , extracting relaxed plans for C in polynomial time without having to ignore
cross-context conditions. That heuristic supersedes, from a theoretical perspective and as
C compilations.
far as hFF is concerned, both the C and ce
It is at this point necessary to mention that our observation (2) is not entirely new.
Alcazar et al. (2013) already devised a heuristic they call FFm , extracting a relaxed plan
from a hm best-supporter function (they implement this for m = 2). This is essentially
(2), without the generality of an arbitrary conjunction set C (which can easily be fixed).
However, Alcazar et al.s work was conducted as part of a much broader scope addressing
heuristic search regression planning, and does not investigate (2) in detail. The design of
FFm does not recognize, and therefore not appropriately address, (a) and (b). Regarding
(b), FFm always selects a single conjunction C 0 = {c} to support, a trivial approximation of
the NP-complete |C 0 |-maximization problem, which may lead to dramatic overestimation.
The overestimation is counter-acted given that FFm also disregards (a), collecting a set of
actions as in standard relaxed plan extraction methods. But that loses convergence  the
value of FFm is bounded by the number of actions  defeating the purpose of the method.
From an empirical perspective, matters are not as clear-cut. Obviously, one can construct cases in which our computational advantage over C , and our information advanC and FF2 , leads to exponential savings. IPC benchmarks are another matter.
tage over ce
Evaluating all heuristic functions, we find that larger conjunction sets C do typically lead
to smaller search spaces, and that hCFF indeed is much faster than hFF (C ) for large C.
Unfortunately, even the slowdown in hCFF typically outweighs the search space reduction,
and best overall performance is most often obtained with small C. On the positive side,
our techniques can yield advantages even with small C, and in some IPC benchmarks large
C is beneficial.
C compilations in SecWe next introduce our basic notation as well as the C and ce
+
C
tion 2. We spell out our direct characterization of h ( ) in Section 3, and we spell out
our generalized relaxed plan extraction methods in Section 4. We summarize our implementation and experiments in Section 5, before concluding in Section 6. Most proofs are
replaced in the main text by brief proof sketches. Full proofs are available in Appendix A.
272

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

2. Notations and Technical Background
We use the STRIPS framework. A planning task is a tuple  = (F , A, I , G) where F is a
set of facts, A a set of actions, I  F is the initial state, and G  F is the goal. Each action
a  A is a triple (pre( a), add( a), del( a)) of precondition, add list, and delete list, each a subset
of F . We henceforth tacitly assume a given input task  = (F , A, I , G).
A state s is a subset of facts s  F . Action a is applicable to s if pre( a)  s; in that case,
applying a in s leads to the state (s  add( a)) \ del( a). A plan for s is a sequence of iteratively
applicable actions leading from s to a state that contains the goal G . A plan for the task 
is a plan for the initial state I . A plan is optimal if its length is minimal among all plans.
We assume throughout that add( a)  del( a) = . This is a natural and common assumption  an action adding p does not also delete it  and is without loss of generality
as any facts in the intersection can be equivalently removed from add( a). The assumption
is necessary for the achieving each fact in separation view of the delete relaxation, as
outlined in the introduction.
Note that, for simplicity, we consider unit costs: all action costs are 1, and plan quality is
just plan length. All our results straightforwardly extend to arbitrary non-negative action
costs, and plan quality measured in terms of summed-up cost.
Example 1 For illustration, we will frequently consider the following car-driving example. A
car moves on a one-way line X  Y  Z of locations, from X to Z. Each car move consumes a
fuel unit, and the cars tank only holds one unit so we must refuel in Y.
To encode this in STRIPS, we design the task  = (F , A, I , G) as follows. F = {carX, carY,
carZ, f uel }, I = {carX, f uel }, and G = {carZ }. A consists of: a XY with precondition
{carX, f uel }, add list {carY }, and delete list {carX, f uel }; aYZ with precondition {carY, f uel },
add list {carZ }, and delete list {carY, f uel }; and are f uel with precondition {carY }, add list
{ f uel }, and empty delete list. The only plan for this task is h a XY , are f uel , aYZ i.
Given a planning task , we denote the set of all states by S. A heuristic (also heuristic
function) is a function h : S 7 N0+  {} mapping states to natural numbers including
0, or to  to indicate that the state is a dead-end. The perfect heuristic h maps any state
s to the length of an optimal plan for s (or to  if there is no plan for s). A heuristic h is
admissible if h(s)  h (s) for all s  S. Abusing notation, we will often identify a heuristic
h with its value h(I) in the initial state. All statements made generalize to arbitrary states
s by setting I := s. By h(0 ), we denote a heuristic for  whose value is given by applying
h in a modified task 0 . To make explicit that h is computed on  itself, we write h().
We will characterize heuristic functions in terms of equations over regressed subgoals.
The regression of fact set G over action a, R( G, a), is defined if add( a)  G 6=  and del( a) 
G = . In that case, R( G, a) = ( G \ add( a))  pre( a); otherwise, we write R( G, a) = .
The critical-path relaxation (Haslum & Geffner, 2000) assumes that, from any goal set of
facts, it suffices to achieve the most costly subgoal of size at most m. Here, m is a parameter
and the corresponding heuristic is denoted hm . Precisely, hm is defined as hm := h(G)
where h is a function on fact sets G that satisfies

GI
 0
1 + minaA,R(G,a)6= h( R( G, a)) | G |  m
(1)
h( G ) =

0
maxG0 G,|G0 |m h( G )
else
273

fiF ICKERT & H OFFMANN & S TEINMETZ

It is easy to see that there is exactly one such h, as assuming two such functions h, h0 where
h( G ) 6= h0 ( G ) recursively leads to a contradiction on the initial state.1 The same argument applies to all h-defining equations considered herein, so henceforth we will assume
uniqueness as given.
For m = 1, the definition of hm becomes identical to that of the max heuristic hmax (Bonet
& Geffner, 2001), which assumes that, to achieve a goal fact set, it is enough to achieve the
maximum costly single fact. For m = |F |, hm = h simply because subgoals of size > |F |
do not exist. Computing hm takes time exponential in m but polynomial in the size of .
The delete relaxation assumes that all delete lists are empty; a plan under this relaxation
is a relaxed plan. The ideal delete-relaxation heuristic h+ maps s to the length of an optimal
relaxed plan for s. But optimal relaxed planning is NP-complete (Bylander, 1994). A relaxed plan heuristic maps s to the length of some, not necessarily optimal, relaxed plan for
s, which can be computed easily (Hoffmann & Nebel, 2001). The resulting heuristic functions are not admissible, but are often very informative in practice for satisficing planning.
We will follow the common approach of considering the idealized heuristic h+ in theoretical examinations of the delete relaxation (compare, e. g., Hoffmann, 2005, 2011; Bonet &
Helmert, 2010), and considering its effective approximation through relaxed plan heuristics in practice.
Relaxed plan heuristics differ in how they find the relaxed plan. A flexible way of
specifying this are the best-supporter functions introduced by Keyder and Geffner (2008).
A best-supporter function maps each fact p to the action the relaxed plan should use to
support p. Given such a function, relaxed plan extraction starts at the goals, and keeps selecting best supporters, opening their preconditions as new subgoal facts, until initial state
facts are reached. We will denote any heuristic arising from such a process by hFF (disambiguating in context where needed). A detailed and formal characterization of relaxed
plan extraction will be given in Section 4, where these details are technically relevant.
Practical best-supporter functions are based on hmax = h1 , selecting for each p an action
a  A, R( G, a) 6=  minimizing the expression in the middle case of Equation 1 with m = 1
(where the subgoal G is a singleton set { p} which can be identified with its element p).
Alternatively, one can assign best supporters based on the additive heuristic hadd (Bonet &
Geffner, 2001) instead, which differs from h1 by using the sum, rather than the maximum,
over the estimated cost of the facts in a goal set (bottom case in Equation 1). Note that
(in both hmax and hadd ) there may be several actions eligible as best supporter. Hence
the construction of a best-supporter functions encompasses tie-breaking, in the sense of
choosing an action from a set of actions supporting a given fact p. Such tie-breaking can
have a large effect on the empirical performance of a relaxed plan heuristic. We will get
back to this in detail in our experiments.
Throughout the paper, we will be concerned with conjunctions c. Following the STRIPS
convention of formulating conjunctive conditions (action preconditions and the goal) as
fact sets, a conjunction c here is a fact set c  F , e. g. c = { p, q}. However, to improve
readability, we will often notate c as a conjunctive formula instead, e. g. c = p  q.
1. Matters are more complicated in case of 0-cost actions, where the recursion may lead into cycles and only
the point-wise maximal h is unique.

274

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

We will henceforth assume a given set C of conjunctions. In practice, C will be computed once on the input task , prior to search.2 We assume throughout that C contains
all singleton conjunctions, {{ p} | p  F }  C. This is just for convenience, notating facts
as a special case of conjunctions. We will sometimes identify singleton conjunctions { p}
with the respective facts p, i. e., notate them without set brackets to avoid clutter; note that
p also is the notation we get when writing { p} as a conjunctive formula.
It will be convenient to introduce a shorthand for the operation of collecting the atomic
conjunctions contained in a fact set. Given a set of facts X  F , and assuming the given
conjunction set C as described, we define X C := {c | c  C, c  X }. We will sometimes
extend this notation to sets X = { X1 , . . . , Xn } of fact sets, where X C is defined pointwise,
S
i. e., X C := i XiC .
The C compilation and its relatives are based on representing conjunctions explicitly,
in terms of introducing new facts which are called -fluents. They introduce one such
fluent, c , for each c  C. In correspondence to the shorthand just introduced, for a fact
set X  F , by X C := {c | c  C, c  X } we denote the set collecting the -fluents
for all atomic conjunctions entailed by X. We extend this notation to sets of fact sets in a
pointwise manner as above.
Using these notations, C can be defined as follows:
Definition 1 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing all singleton conjunctions. The explicit-C compilation C is the planning task (F C , AC ,
I C , G C ). Here, F C , I C , and G C are defined as per the shorthand. The set of actions AC
contains an action a[C 0 ], for every pair a  A and  6= C 0  {c  C | R(c, a) 6= }, with a[C 0 ]
given by
 pre( a[C 0 ]) = [

S

cC 0 (pre( a )  ( c

\ add( a)))]C , and

 add( a[C 0 ]) = {c | c  C 0 }.
C is identical to C except that pre( a [C 0 ]) =
The no cross-context explicit-C compilation nc
0
C
{pre( a)  (c \ add( a)) | c  C } .

We refer to pairs ( a, C 0 ) of action a and set C 0 of supported conjunctions, corresponding
C , as action occurrences. We refer to c \ add( a ), for
to the compiled actions a[C 0 ] in C and nc
0
c  C , as the context of c in a: for a to support c, the context must be true in the preceding
C , which extend the original prestate. This is captured by the preconditions in C and nc
condition with the contexts of the supported conjunctions. For C , the context is collected
C this is done for each supported conjunction inacross all supported conjunctions, for nc
dividually.
Our definition of C diverges from the original definition by Haslum (2012) in several
minor ways. First, we do not distinguish explicitly between the actions original effects
vs. its supported conjunctions, instead expressing the add list of a as part of the set C 0
of conjunctions that may be supported. This is possible as C is assumed to contain all
singleton conjunctions. As a consequence, we can demand that C 0 6=  (otherwise the
action would have no effect and thus be useless). Second, we do not automatically include
2. The details of exactly how this is done are not relevant to our contribution. We will briefly describe the
methods we use (adopted from Keyder et al., 2012, 2014) in the discussion of experiments, Section 5.

275

fiF ICKERT & H OFFMANN & S TEINMETZ

c facts relying on a context consisting only of non-deleted preconditions. Third, we do
not demand C 0 to be downward closed, i. e., to contain all subsumed conjunctions c0 ,
where there exists c  C 0 such that c0  c. Fourth, we do not include any delete effects.
None of these changes have any consequences for the results we present. Changes two to
four introduce some superfluous actions, simplifying our presentation while not affecting
our results. The fourth change is suitable as we will use C only for generating deleterelaxation heuristics. For consistent use of language, we will speak of relaxed plans for
C nevertheless.
We also modify the notation a bit, relative to Haslum (2012). We notate the actions
0
as a[C 0 ] instead of AC . This will be more convenient. We furthermore somewhat modified the definition of a[C 0 ] preconditions, exploiting that C 0 6= . Namely, pre( a[C 0 ]) =
S
[ cC0 (pre( a)  (c \ add( a)))]C in C is equivalent to the perhaps more intuitively straightS
forward definition, namely pre( a[C 0 ]) = [pre( a)  cC0 (c \ add( a))]C as used by Haslum.
C , the precondition pre( a [C 0 ]) = {pre( a )  ( c \ add( a )) | c  C 0 }C , given the pointFor nc
S
wise interpretation of the C superscript, equals cC0 [pre( a)  (c \ add( a))]C , which
itself is the same as the perhaps more intuitively straightforward definition pre( a[C 0 ]) =
S
pre( a)C  cC0 [pre( a)  (c \ add( a))]C . Observe that our modifications allow to write the
action preconditions in terms of regression, thanks to R(c, a) = pre( a)  (c \ add( a)). In
S
C , it reads pre( a [C 0 ]) =
C , the precondition then reads pre( a[C 0 ]) = [ cC0 R(c, a)]C . In nc
0
C
{ R(c, a) | c  C } . These simplified notations will conveniently link-in with the concepts
we introduce later on.
Note finally that the C compilation introduces all atomic conjunctions into action preconditions and the goal, even ones subsumed by other, larger, atomic conjunctions contained in the same precondition/goal. We stick to this convention throughout, for simplicity. In practice, we ignore the subsumed conjunctions. In the remainder of the paper,
this corresponds to a modified C superscript, only including conjunctions c  C, c  X,
where there does not exist c0  C, c0  X, so that c ( c0 ; correspondingly for the C
superscript. This leaves all results intact exactly as stated.
Example 2 Reconsider our car-driving example task  from Example 1. As the delete relaxation
ignores the negative effect of a XY , a shortest relaxed plan is h a XY , aYZ i and h+ = 2.
However, say we set C to contain (all singleton conjunctions as well as) c = carY  fuel.
Then, in C , c is a precondition of all actions aYZ [C 0 ], i. e., of all actions adding the goal carZ.
The only actions adding c have the form arefuel [C 0 ] where c  C 0 . Hence h a XY , aYZ i is not a
relaxed plan for C . Instead, we need to perform a refueling action, for example in the relaxed plan
h a XY [{carY }], arefuel [{carY  fuel}], aYZ [{carZ }]i. We get h+ (C ) = 3 = h ().
C is exponential in |C | because action occurrences enumerThe growth of C and nc
ate subsets C 0  C of supported conjunctions. This complexity is necessary because,
C ) would not be admissible. As a simple example, say
otherwise, h+ (C ) and h+ (nc
the goal in n is { g1 , . . . , gn }, C contains the singleton conjunctions as well as all fact
pairs, and there is a single action a achieving all of { g1 , . . . , gn }. Then h (n ) = 1, and
h+ (Cn ) = 1 thanks to the optimal plan h a[C 0 ]i where C 0 is the set of all conjunctions,
C 0 = C = {{ gi } | 1  i  n}  {{ gi , g j } | 1  i 6= j  n}. However, if we had to achieve
every conjunction separately in Cn , that is, if we included into AC only actions of the
n(n1)
form a[{c}] for c  C, then we would get h+ (Cn ) = n +
because we would have
2

276

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

to achieve every conjunction c  C with a separate compiled action. (This observation will
become relevant again later on, compare Example 7 in Section 4.3.1.)
C is that the latter, but not the former, ignores what
The difference between C and nc
has been termed cross-context conditions: conjunction preconditions of a[C 0 ] in C which
arise only from the combination of several c  C 0 . Precisely, a cross-context condition for
S
a and C 0 is a conjunction c  C where c  cC0 [pre( a)  (c \ add( a))], but there does not
C compilation, the preexist any single c  C 0 such that c  pre( a)  (c \ add( a)). In the nc
0
condition of a[C ] does not contain any cross-context conditions, because the superscript
C in pre( a[C 0 ]) = {pre( a)  (c \ add( a)) | c  C 0 }C , i. e., the collection of conjunctions, is done for the context of each c  C 0 separately. This is in contrast to C where, in
S
pre( a[C 0 ]) = [ cC0 (pre( a)  (c \ add( a)))]C , the conjunctions are collected from the union
of contexts across c  C 0 .
Example 3 To illustrate cross-context conditions, we will consider the following abstract example (given by Keyder et al., 2014, as part of the proof of their Theorem 3).  = (F , A, I , G) where
F = { g1 , g2 , p, q1 , q2 }, I = {q1 }, G = { g1 , g2 } and A consists of: a g1 with precondition { p, q1 },
add list g1 , and empty delete list; a g2 with precondition { p, q2 }, add list g2 , and empty delete list;
a p with empty precondition, add list { p}, and empty delete list; aq2 with precondition {q1 }, add
list {q2 }, and delete list {q1 , p}. In this construction, q1 and q2 are mutex; achieving g1 requires
q1 and thus has to be done first, via a p and a g1 . To achieve g2 , we require q2 . Getting q2 through aq2
deletes p, so that we must apply a p a second time before applying a g2 . As in the delete relaxation
there never is a need to apply the same action twice, h+ = 4 < 5 = h .
Say we set C to contain cq1p = q1  p, cq2p = q2  p, and cq1q2 = q1  q2 . Then any
relaxed plan for C must contain two occurrences of a p : cq1p and cq2p are required to achieve the
goal; a p is the only action that can support these conjunctions (note here that aq2 deletes p); and
a p [{cq1p , cq2p }] supporting both conjunctions with a single action occurrence has the unreachable
cross-context condition cq1q2 . Consequently, h+ (C ) = 5 = h ().
C , a [{ c
In contrast, in nc
p
q1p , cq2p }] does not have the cross-context condition, so h aq2 [{ q2 }],
C ) = 4 = h + (  ) < h  (  ).
a p [{ p, cq1p , cq2p }], a g1 [{ g1 }], a g2 [{ g2 }]i is a relaxed plan and h+ (nc
C compilation, which achieves the same
Keyder et al. (2012, 2014) introduced the ce
C
effect as nc but has size polynomial in |C |. This is done by augmenting every original
action a  A with one conditional effect for each c  C that can be regressed over a,
adding c and requiring the context c \ add( a) as the effect condition.
C compilation is equivalent to C in the sense that h+ ( C ) = h+ ( C ). InThe ce
nc
ce
nc
C , where the conditional effects for the set of
tuitively, any occurrence of an action in ce
C action a [C 0 ] because in C the conjunctions
conjunctions C 0 fire, is equivalent to the nc
ce
c  C 0 are handled separately, ignoring cross-context conditions.3 Given this equivalence,
in our theoretical discussion of heuristic functions and their properties  where the size
C and C does not matter  we will refer throughout to C rather
difference between nc
ce
nc
C . This simplifies matters because we do not have to switch between formalisms
than ce
(STRIPS with vs. without conditional effects).
C )  h+ ( C ) was proved by Keyder et al. (2014) in the proof to their Lemma 2, and the
3. Technically, h+ (ce
nc
C )  h+ ( C ) is symmetric.
opposite direction h+ (ce
nc

277

fiF ICKERT & H OFFMANN & S TEINMETZ

C has its correWe will see in Section 4.3 that the complexity reduction from C to ce
spondence in a complexity reduction of the subgoal-support selection problem (maximization of |C 0 | during relaxed plan extraction): while that problem is NP-complete for C , it
C.
is polynomial-time for nc

3. Combining the Delete Relaxation with Critical Paths: hC+
We now spell out observation (1) from the introduction, characterizing the combination
of the delete relaxation with critical paths directly, without a compilation, in terms of a
heuristic function we call hC+ . Section 3.1 starts with simple novel views on each of the
two components, and Section 3.2 combines these into an equation characterizing hC+ . Section 3.3 sketches our proof of correctness i. e., that hC+ = h+ (C ). Section 3.4 summarizes
the properties of hC+ , pointing out that the combination of the delete relaxation with critical paths naturally generalizes its components.
3.1 Novel Views on hm and h+
First, consider the following straightforward characterization of h , which will be relaxed
in different manners below: h := h(G) where h is the function on fact sets G that satisfies

0
GI
h( G ) =
(2)
1 + minaA,R(G,a)6= h( R( G, a)) else
This equation obviously characterizes optimal planning, and therewith h : we minimize
plan length over all actions that can support our subgoal G.
Slightly rephrasing the critical-path relaxation, it assumes that, to achieve a subgoal G,
it suffices to achieve the most costly atomic subgoal, where the notion of atomic subgoal
is a parameter. In the traditional formulation, that parameter is instantiated with all fact
sets of size at most m. But there is no need to be so restrictive. The atomic subgoals can be
an arbitrary set of fact-sets, in other words: an arbitrary set C of conjunctions. We merely
need to replace the subgoal-selection mechanisms in hm (Equation 1) with accordingly generalized ones. We denote the resulting heuristic by hC , defined as hC := h(G) where h is
the function on fact sets G that satisfies

GI
 0
1 + minaA,R(G,a)6= h( R( G, a)) G  C
(3)
h( G ) =

0
maxG0 G,G0 C h( G )
else
Trivially, hC = hm if C consists of all conjunctions of size  m. As we shall see below,
hC = h1 (C ) as one would expect. The latter property is useful only from a theoretical
perspective though, connecting hC to known results about h1 . In practice, hC can be computed like hm , by a fixed point process on value assignments to the atomic subgoals C,
taking time polynomial in |C |, in contrast to the size of C . Our particular implementation
will be described in Section 5.1.
It is worth pointing out that the simple generalization from hm to hC already can be
quite useful:

278

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

Example 4 Consider our car-driving example from Example 1, but without the refuel action. This
modified task is unsolvable, and h2 =  recognizes that. Yet, there is no need to reason about all fact
pairs to arrive at this conclusion: Considering the single fact pair C = {c} where c = carY  fuel,
like in Example 2, suffices to get hC = , as c becomes a precondition for achieving the goal,
and there is no action over which c can be regressed. While this particular example only has 4
facts and thus 6 fact pairs, we could scale it arbitrarily by adding solvable parts, blowing up the
computational overhead of h2 while still recognizing unsolvability using the single fact pair c.
Getting back to our discussion of alternate ways to relax h , i. e., Equation 2, observe
that Equation 3 uses the correct regression semantics, but relaxes the subgoals considered.
The delete relaxation can be viewed as approaching this vice-versa, keeping the correct
subgoaling but relaxing the regression semantics. This is immediately visible in the following straightforward characterization of h+ , as h+ := h(G) where h is the function on
fact sets G that satisfies

0
GI
h( G ) =
(4)
1 + minaA,6=Gadd(a) h(( G \ add( a))  pre( a)) else
This is identical to Equation 2 except for pretending that R( G, a) 6=  even if del( a)  G 6=
, i. e., replacing R( G, a) with the relaxed concept that only asks for non-empty add-list
intersection.
The basic observation towards combining hC with h+ is that the underlying relaxation
principles, though they seem unrelated given Equations 3 and 4, can both be viewed as
relaxations pertaining to the subgoaling structure. This becomes visible in the following
alternative characterization of h+ :
Lemma 1 Let  = (F , A, I , G) be a planning task, and let h be the function on fact sets G that
satisfies

0
GI
S
h( G ) =
(5)
0
1 + minaA,6=G0 ={ p| pG,R({ p},a)6=} h(( G \ G )  pG0 R({ p}, a)) else
Then h(G) = h+ .
Proof: Observe that, for singleton fact sets G = { p}, (a) regressability of G over a trivializes
to add-list intersection, i. e., R({ p}, a) 6=  iff p  add( a), because with add( a)  del( a) = 
we get p 6 del( a); and (b) if G can be regressed over a then the regression simply generates the action precondition as the new subgoal, i. e., R({ p}, a) = pre( a). So Equation 5
simplifies to

0
GI
h( G ) =
1 + minaA,6=G0 =Gadd(a) h(( G \ G 0 )  pre( a)) else
With G 0 = G  add( a) we have G \ G 0 = G \ add( a), so this is equivalent to Equation 4.
As per Equation 5, the delete relaxation can be understood as splitting subgoals up into
singleton facts, and considering regression separately with respect to each of these. As singleton
regression trivializes, in effect we need to worry only about the part of the subgoal we can
279

fiF ICKERT & H OFFMANN & S TEINMETZ

support, not about other parts that the same action may contradict.4 While this reformulation is awkward and not useful in the standard setting, it exhibits a possible refinement to
that setting: instead of singleton facts, consider atomic subgoals in the form of an arbitrary
set C of conjunctions.
3.2 The Combined Heuristic
Consider again Equation 5, and compare it with the following equation characterizing h1 :

GI
 0
1 + minaA,R({ p},a)6= h( R({ p}, a)) G = { p}
(6)
h( G ) =

max pG h({ p})
else
Equation 5 can be understood as a less relaxed version of Equation 6. Both decompose
a subgoal G into its atomic subgoals, instantiated as singleton facts, and both minimize
over actions regressing atomic subgoals. The difference is that, while Equation 6 picks the
single most costly atomic subgoal, Equation 5 requires to achieve every atomic subgoal (in
particular, including the ones not supported by a, i. e., G \ G 0 , in the recursive invocation
of h). As the set G consists exactly of its atomic subgoals, Equation 5 does not need a third
case identifying the atomic subgoals.
Now, hC generalizes h1 in considering the more general atomic subgoals C. Applying
a similar generalization to Equation 5, we obtain our desired combination of the delete
relaxation with critical paths:
Definition 2 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing all singleton conjunctions. The critical-path delete relaxation heuristic, short C-relaxation
heuristic, is defined as hC+ := h(G C ), where h is the function on conjunction sets G that satisfies
(
0
c  G : c  I
h( G ) =
(7)
C
0
0
1 + minaA,6=G0 {c|cG,R(c,a)6=} h(( G \ G )  Gr ) else
with Gr0 defined as Gr0 := cG0 R(c, a).
The no cross-context critical-path delete relaxation heuristic, short nc-C-relaxation heurisC + , is defined identically to hC + except that we define G 0 : = { R ( c, a ) | c  G 0 }.
tic, denoted hnc
r
S

Recall here that, for a fact set X, X C := {c | c  C, c  X } denotes the set of atomic
C+ )
conjunctions contained in X, and that for a set of fact sets (as in the case of Gr0 for hnc
we apply this notation pointwise. As a convention, we will refer to the expression ( G \
C
G 0 )  Gr0  in Equation 7, and in related equations, as the recursive subgoal, and to Gr0 as the
regressed subgoal.
Intuitively, hC+ supports atomic subgoals from C individually by regression as in hC ,
but instead of achieving only the most costly one, it achieves all of them. This parallels
our previous comparison between h+ and h1 . The subgoals G recursed over now are sets
of conjunctions, because in difference to h+ (Equation 5) atomic subgoals are conjunctions
4. The independence assumptions identified by Keyder and Geffner (2009) are somewhat related to this. But
our formulation pertains to the delete relaxation heuristic h+ itself, ignoring negative side effects; whereas
Keyder and Geffners observations pertain to simplifying assumptions in approximations of h+ , ignoring
positive side effects.

280

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

instead of single facts, and in difference to hC (Equation 3) we estimate the cost of sets
of atomic subgoals instead of single atomic subgoals. The initializing call on G C inserts
all atomic conjunctions from the global goal, and the recursive subgoals insert all atomic
conjunctions from the regressed subgoal Gr0 . Hence, like in Equation 5 the set G consists
exactly of its atomic subgoals, and we do not need a third case identifying the atomic
subgoals.
The top case in Equation 7 is self-explanatory. The bottom case generalizes that in
Equation 5. Because atomic subgoals now are non-unit conjunctions, in difference to our
arguments in Lemma 1, regression no longer trivializes: a is not allowed to contradict any
conjunction c  G 0 , and R(c, a) may be a proper superset of pre( a), no longer trivializing
to the action precondition. Hence, in difference to Equation 5, the more complex notation
is now necessary. There also is a major new source of complexity, relative to Equation 5,
namely the need to allow G 0 to be a subset of the supportable atomic subgoals, rather than
just setting G 0 to that entire set. This corresponds to the aforementioned subgoal-support
selection problem. We illustrate that problem in Example 5 below; Section 4.3 conducts an
in-depth analysis in the context of relaxed plan extraction.
C + is, as the notation suggests, designed to match the
The difference between hC+ and hnc
C . The expressions S
0
difference between C and nc
c G 0 R ( c, a ) vs. { R ( c, a ) | c  G }
are in obvious correspondence with the action preconditions in Definition 1 (thanks to our
modifications with respect to the original definition). A pair ( a, G 0 ) of action and subset of supported atomic subgoals in the hC+ equation corresponds to the C action a[C 0 ]
C + and C . An instructive alternative way to read the rewhere C 0 = G 0 , similarly for hnc
nc
S
V
gressed subgoals Gr0 is in terms of conjunctions. This gives cG0 R(c, a) = cG0 R(c, a) =
V
V
0
0
c G 0 ,p R(c,a) p, vs. { R ( c, a ) | c  G } = { p R(c,a) p | c  G }: one large conjunction vs.
several small ones. This makes a difference because larger conjunctions may contain larger
C
atomic subgoals, as captured in Definition 2 through the respective use of Gr0 .
Example 5 Consider, as in Example 2 (page 276), our car-driving example with C containing the
singleton conjunctions as well as c = carY  fuel. We get hC+ = h({carZ }), i. e., h defined
as per Equation 7, applied to the conjunction set containing the single goal atomic conjunction
carZ. The only ( a, G 0 ) pair supporting carZ is ( aYZ , {carZ }). Selecting ( a, G 0 ), we get the
recursive subgoal G = {carY, fuel, carY  fuel}. As carY  fuel cannot be supported by a XY
which deletes fuel, the only supporting action for that subgoal is arefuel . Say we select that action,
and G 0 := {carY  fuel}. The recursive subgoal then is {carY, fuel} because the conjunctions
carY and fuel in G are not included in G 0 . In detail, the recursive subgoal results from the exS
pression ( G \ G 0 )  [ cG0 R(c, a)]C = ({carY, fuel, carY  fuel} \ {carY  fuel})  R(carY 
fuel, arefuel )C = {carY, fuel}  {carY }C = {carY, fuel}  {carY }. That subgoal can be resolved
using ( a XY , {carY }), yielding hC+ = h+ (C ) = 3 due to the same relaxed plan as in Example 2.
Now consider, as in Example 3 (page 277), our abstract example with C containing the singleton conjunctions as well as cq1p = q1  p, cq2p = q2  p, and cq1q2 = q1  q2 . We have
hC+ = h({ g1 , g2 }), requiring to support each of the two goal facts (written as singleton conjunctive formulas here). This can be done only by ( a g1 , { g1 }) and ( a g2 , { g2 }) respectively; after using
these, we get the recursive subgoal G = {q1 , q2 , p, q1  p, q2  p}. Now we have a non-trivial
subgoal-support selection problem. Ignoring the subsumed subgoals q1 , q2 , p which can be tackled
as a side effect of tackling the non-subsumed ones q1  p and q2  p, we can choose any of (a)
281

fiF ICKERT & H OFFMANN & S TEINMETZ

( a p , {q1  p, q2  p}), (b) ( a p , {q1  p}), or (c) ( a p , {q2  p}). If we choose (a), then our subgoal
S
is fully supported i. e., G \ G 0 = , yet [ cG0 R(c, a p )]C = {q1 , q2 }C = {q1 , q2 , q1  q2 }. The
cross-context conjunction q1  q2 is not supported by any action, so we cannot get to the initial state
this way. Instead, we need to take either (b) or (c), yielding the recursive subgoals (b) {q2  p, q1 }
respectively (c) {q1  p, q2 }, each of which necessitates support by aq2 as well as another occurrence
of a p , leading to hC+ = h+ (C ) = h = 5.
C + instead, option (a) produces the different subgoal { R ( c, a ) | c  G 0 }C =
Using hnc
{{q1 }, {q2 }}C = {q1 , q2 }, not containing the cross-context conjunction q1  q2 . This subgoal
C + = h+ ( C ) = h+ = 4.
is feasible, and only requires support by aq2 , leading to hnc
nc
3.3 Proof of Correctness
We prove that Equation 7 does indeed capture h+ (C ), i. e., that h+ (C ) = hC+ (). For
illustration, we first consider the simple case where C contains only the singleton conjunctions:
Proposition 1 Let  = (F , A, I , G) be a planning task, and C = {{ p} | p  F }. Then
h+ = hC + .
Proof: With C = {{ p} | p  F }, the recursive subgoals G in Equation 7 are sets of singleton
fact-sets, so we can instead perceive G as a set of facts. We can then re-write Equation 7 to:

0
GI
S
h( G ) =
0
1 + minaA,6=G0 { pG| R({ p},a)6=} h(( G \ G )  pG0 R({ p}, a)) else
This is identical to Equation 5 except that G 0 is allowed to be a subset of { p  G |
R({ p}, a) 6= }. However, because R({ p}, a) = pre( a), the minimum in the bottom case
can always be achieved using G 0 = { p  G | R({ p}, a) 6= }, which can only yield smaller
recursive subgoals than G 0  { p  G | R({ p}, a) 6= }. This concludes the proof with
Lemma 1.
Observe that Proposition 1 proves that h+ (C ) = hC+ () for C = {{ p} | p  F }: with
singleton conjunctions only, h+ = h+ (C ), so by Proposition 1 we have h+ (C ) = h+ =
hC+ () as desired. We now extend this to the general case, for arbitrary conjunction sets:
Theorem 1 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing
all singleton conjunctions. Then h+ (C ) = hC+ ().
Proof Sketch: We apply Equation 5 to C , characterizing h+ (C ). Making explicit that the
individual facts in C all are -fluents, we obtain: h+ (C ) = h({c | c  G C }), where
h is the function on fact sets G that satisfies h( G ) =

0
c  G : c  I C
0
0
1 + mina[C0 ]AC ,6=G0 ={c |c G,R({c },a[C0 ])6=} h(( G \ G )  Gr ) else
with Gr0 defined as Gr0 := c G0 R({c }, a[C 0 ]).
The condition R({c }, a[C 0 ]) 6=  here simplifies to c  C 0 , because these are exactly
the -fluents added by a[C 0 ]. So we have G 0 = {c | c  G, c  C 0 } and the minimization
is over those a[C 0 ] supporting a non-empty subset of subgoals c . The c we can in principle
S

282

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

include into C 0 are, by the definition of C , exactly those where R(c, a) 6= . There is no
point in including c where c 6 G, as this will support the same subgoals yet can only
result in a larger precondition. Hence, renaming C 0 into G 0 in order to unify notation, we
obtain h( G ) =

0
c  G : c  I C
1 + mina[G0 ]AC ,6=G0 {c|c G,R(c,a)6=} h(( G \ G 0 )  Gr0 ) else
with Gr0 defined as Gr0 := cG0 R({c }, a[ G 0 ]).
Comparing this equation with Equation 7, it is easy to see that the equations are in
exact correspondence via () G = {c | c  G [7]}, where G [7] denotes the subgoal sets
in Equation 7. (*) is true by definition for the initializing calls, hC+ () = h(G C ) respectively h+ (C ) = h({c | c  G C }). (*) is invariant over the bottom cases in both
S
S
equations, as Gr0 = cG0 R({c }, a[ G 0 ]) = pre( a[ G 0 ]) = [ cG0 (pre( a)  (c \ add( a)))]C =
S
S
[ cG0 R(c, a)]C , which matches the regressed subgoal Gr0 [7] = [ cG0 R(c, a)]C of Equation 7 as desired.
S

C and hC + :
A similar proof shows the same correspondence for nc
nc

Theorem 2 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing
C ) = h C + (  ).
all singleton conjunctions. Then h+ (nc
nc
3.4 Properties of the Combination
The combination of the delete relaxation with critical paths, as per Definition 2, naturally
generalizes the properties of its components. This follows from known results along with
the following simple observation:5
Theorem 3 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing
C ) = h C (  ).
all singleton conjunctions. Then h1 (C ) = h1 (nc
Proof Sketch: Consider first C . Applying Equation 6 (page 6) characterizing h1 to C ,
we get h1 (C ) = h(G C ) where h is the function on fact sets G that satisfies

G  I C
 0
0
1 + mina[C0 ]AC ,R(G,a[C0 ])6= h( R( G, a[C ])) G = {c }, c  C
h( G ) =

maxc G h({c })
else
Observe that, in the middle case, we must have c  C 0 because otherwise c 6 add( a[C 0 ]);
and that there is no point in including any other conjunctions into C 0 , i. e., C 0 ) {c}, because this can only yield a larger recursive subgoal R( G, a[C 0 ]). Hence we can re-write the
previous equation to:

G  I C
 0
1 + mina[{c}]AC ,R(G,a[{c}])6= h( R( G, a[{c}])) G = {c }, c  C
h( G ) =

maxc G h({c })
else
C ) part of this observation, using a different proof
5. Keyder et al. (2014) already proved the h1 (C )  h1 (nc
argument.

283

fiF ICKERT & H OFFMANN & S TEINMETZ

Comparing this equation with Equation 3 (page 278) characterizing hC , it is easy to see that
the equations are in exact correspondence via () G = {c | c  C, c  G [3]}, where G [3]
denotes the subgoal (fact) sets in Equation 3.
C is identical because, for single-conjunction sets C 0 = { c }, the two
The argument for nc
compilations coincide.
Note that, in the step from the first to the second equation stated in this proof, the exponential size of C is reduced to the polynomial-size compilation which is like C but
includes only the actions a[{c}] for pairs a  A and c  C where c can be regressed through
a. Intuitively, as h1 only considers singleton subgoals, there is no need to enumerate supported conjunction sets of size greater than 1. This reduced compilation is essentially a
version of Haslums (2009) m compilation for arbitrary conjunction sets C. (This simple
generalization was not mentioned by Haslum in his works on m and C .)
Together with results by Haslum (2012) and Keyder et al. (2014), as well as basic known
results about h1 and h+ , Theorems 1  3 immediately imply all the properties one would
C + to have:
naturally expect hC+ and hnc
Corollary 1 Let  be a planning task. Then, for any set C of conjunctions in  containing all
singleton conjunctions, we have:
C +  hC +  h ; and
(i) hC , h+  hnc
C + =  iff hC = .
(ii) hC+ =  iff hnc
C + converge to h , i. e., there exist sets C of conjunctions such that
Furthermore, both hC+ and hnc
C
+

C+ = h .
(iii) h = h respectively (iv) hnc
C + = h+ ( C ) and by Theorem 3 hC = h1 ( C ),
Proof: Regarding (i): By Theorem 2, hnc
nc
nc
C
C
+
1
+
C ) = h+ ( C ) and hence hC + = h+ ( C ),
so h  hnc follows from h  h . As h+ (nc
ce
nc
ce
C + holds by the corresponding result of Keyder et al. (2014) (h+  h+ ( C ), their
h+  hnc
ce
C drops preconditions from C , we get hC + = h+ ( C )  h+ ( C ),
Corollary 1). As nc
nc
nc
+
C
C
where h ( ) = h + by Theorem 1. Finally, hC+  h holds by the corresponding result
of Haslum (2012) (h+ (C )  h , his Theorem 4).
C + = h+ ( C ), and hC = h1 ( C ) = h1 ( C ), this
Regarding (ii): As hC+ = h+ (C ), hnc
nc
nc
follows from h+ =  iff h1 = .
Finally, (iii) holds by convergence of h+ (C ) (Haslum, 2012, Theorem 5) because hC+ =
+
C ) (Keyder et al., 2014,
h (C ) as per Theorem 1, and (iv) holds by convergence of h+ (ce
C ) = h+ ( C ) and hC + = h+ ( C ) as per Theorem 2.
Theorem 5) because h+ (nc
ce
nc
nc

4. Extracting Relaxed Plans: hCFF
We just observed that, like in the standard setting, hC+ =  iff hC = , i. e., a relaxed plan
exists iff the critical-path component of our heuristic is solvable. So hC behaves like h1 in
the role of deciding relaxed plan existence. But then, can hC also fulfill the role of h1 in
relaxed plan extraction, i. e., finding some not necessarily optimal relaxed plan?
Implicitly, this is already being done in the C compilation, via relaxed plan extraction on h1 (C ) = hC , but this construction is wasteful as computing hC does not actually
require the exponential blow-up inherent in C . Can we make do without this blow-up?
284

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

As we indicated in the introduction (in observation (2)), the answer is yes, in the form
of a heuristic function we denote by hCFF : Relaxed plan extraction to obtain hFF from a h1 bestsupporter function translates into relaxed plan extraction to obtain hCFF from a hC best-supporter
function. Thanks to this direct formulation, not using a compilation, hCFF computes relaxed
plans for C in time polynomial in |C |.
To spell this out in detail, we start in Section 4.1 by, similarly as before, reformulating
standard relaxed plan extraction in a way preparing the generalization to arbitrary conjunction sets C. Section 4.2 specifies that generalization and proves it correct. Section 4.3
analyzes the subgoal-support selection problem, which is benign in the standard setting
but is NP-complete in the general case; we define our heuristic function hCFF using a greedy
solution to that problem.
Throughout, we use equation-based formulations as these generalize directly to arbitrary action costs. To improve readability, we also include pseudo-code formulations
which apply only to the simpler unit-cost case. Like before, we distinguish between variC ).
ants taking cross-context conditions into account (C ) vs. not doing so (nc
4.1 Relaxed Plan Extraction from h1
Relaxed plan extraction was first formulated in terms of best-supporter functions by Keyder and Geffner (2008). The advantage over more traditional relaxed planning graph formulations (Hoffmann & Nebel, 2001) is that best-supporter functions are more flexible,
allowing to use hadd instead of h1 , and generalizing to arbitrary action costs. As we shall
see, the best-supporter formulation also generalizes easily to the use of hC instead of h1 .
Best-supporter functions map facts to actions. Based on h1 , any fact p is mapped to
an action achieving h1 ({ p}), i. e., achieving the minimum in the h1 equation (Equation 6,
page 279). For unit-cost actions, the latter is equivalent to h1 (pre( a)) = h1 ({ p})  1. We
can hence write Keyder and Geffners formulation of relaxed plans  FF for the input task
S
 = (F , A, I , G) as  FF := pG  ( g), where  is a function on facts p that satisfies:
 ( p) =


 
S


pI
 (q)  { a} where a  A,
p  add( a), and h1 (pre( a)) = h1 ({ p})  1 else

qpre( a)

(8)

It is easy to see that the action set  FF can be sequentialized to form a relaxed plan for .
The generalization to arbitrary cost can be done by working with h1 (pre( a)) = h1 ({ p}) 
c( a) instead, and using a modified action-costs function c0 := c + e, for some e > 0, in case
there are 0-cost actions (e can in principle be chosen so as to preserve optimality; this is a
minor concern here as relaxed-plan heuristic functions are inadmissible anyway).
Note the special case where h1 (pre( a)) =  for all a with p  add( a), and hence
h1 ({ p}) = . In this situation, p does not have a best supporter in Keyder and Geffners
formulation. In our formulation,  ( p) is undefined (i. e., in our notation  is not equal to
  1). We abstract from this issue throughout the present subsection, just assuming that
h1 ({ p}) <  for all facts. We will deal with the issue below in our extension to conjunction
sets C, where  will be a partial function.
Note furthermore that we intentionally specify  to be a function that satisfies Equation 8. The relaxed plan  FF is unique only up to tie-breaking. Keyder and Geffners
285

fiF ICKERT & H OFFMANN & S TEINMETZ

formulation moves the tie-breaking into the definition of best supporters. We find it more
convenient, for our purposes here, to make the tie-breaking an explicit part of our equations (i. e., of Equation 8 and all relaxed-plan equations below).
Towards our generalization to arbitrary C, we first change Keyder and Geffners equation to account for positive side effects, to the extent of supporting, with the same action,
all open subgoals for which that action is a best supporter. Reformulating Equation 8 to
this end, we obtain  FF :=  (G), where  is a function on fact sets G that satisfies:

GI
 
0
 (( G \ G )  pre( a))  { a} where a  A,
(9)
 (G) =

 6= G 0 = { p  G | p  add( a), h1 (pre( a)) = h1 ({ p})  1} else
Compared to Equation 8, we need to recurse not over single facts but over sets of facts,
so that each recursive call knows the open facts and can select the entire best-supported
subset thereof.6
Equation 9 is in correspondence with typical relaxed planning graph based implementations, as depicted in Algorithm 1. The definition of G 0 in the equation corresponds to
the maintenance of TRUE flags for facts at relaxed planning graph layers, where upon
selecting an action a at layer i all of as add effects are marked as TRUE at i (to see this, observe that, with h1 (pre( a)) = i  1, we have h1 (pre( a)) = h1 ({ p})  1 iff h1 ({ p}) = i). We
will extend Algorithm 1 to relaxed plan extraction from hC below. Note that Algorithm 1
deviates a bit from more common descriptions, explicitly including the computation of h1
instead of assuming an input relaxed planning graph caching the outcome of this computation. This is just to simplify notation and to tie in easily with our extension below.
The step from Equation 8 to Equation 9 is benign in the standard setting, in the sense
that its practical impact can be expected to be small: a single action typically does not add
many open facts, i. e., does not support many open atomic subgoals. Yet this step is of
paramount importance for our generalization of atomic subgoals to arbitrary conjunctions
C. In that general setting, atomic subgoals typically overlap, and supporting a subgoal just
means to add part of it, which may very well be the case for many subgoals.
We finally need to formulate the delete relaxation, not in terms of relaxing the regression semantics, but in terms of splitting subgoals up into singleton facts, and considering
the correct regression semantics but separately with respect to each of these singleton-fact
subgoals. In other words, we need to use the formulation underlying h+ in Equation 5. As
a reminder for convenience, that equation is: h( G ) =

0
GI
S
1 + minaA,6=G0 ={ p| pG,R({ p},a)6=} h(( G \ G 0 )  pG0 R({ p}, a)) else
Doing a similar transformation step to Equation 9, we obtain  FF :=  (G), where  is a
function on fact sets G that satisfies  ( G ) =

GI
 
0
0
 (( G \ G )  Gr )  { a} where a  A,
(10)

 6= G 0 = { p | p  G, R({ p}, a) 6= , h1 ( R({ p}, a)) = h1 ({ p})  1} else
6. Note that this selection is dynamic as a function of the open facts, as opposed to the up-front design of
a best-supporter function sharing supporting actions as much as possible. This is not important in the
standard setting here. Yet, as we discuss in detail below, it does become important when using arbitrary
conjunctions C as atomic subgoals.

286

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

Algorithm 1: Relaxed plan extraction from h1 .
1
2
3
4
5
6
7
8
9
10

11
12

compute h1 ({ p}) for all p  F
m := max pG h1 ({ p})
if m =  then
return 
for i := m, . . . , 1 do
Gi := { p | p  G , h1 ({ p}) = i }
 := 
for i := m, . . . , 1 do
while ex. p  Gi s.t. p not TRUE at i do
select a  A where  6= { p  Gi | p not TRUE at i }  add( a),
and h1 (pre( a)) = i  1
foreach p  Gi  add( a) do
mark p TRUE at i

14

foreach q  pre( a) do
Gh1 ({q}) := Gh1 ({q})  {q}

15

 :=   { a}

13

16

return 

with Gr0 defined as Gr0 := pG0 R({ p}, a).
Relative to Equation 9, this is a simple reformulation, using regression notation which
S
trivializes for singleton conjunctions. In particular, the subgoal Gr0 = pG0 R({ p}, a) generated by the action simplifies to pre( a) here. Relative to Equation 5, instead of a heuristic
value, we compute a relaxed plan. Instead of minimizing over all action choices which
corresponds to h+ , we impose the use of best supporters which corresponds to relaxed
plan extraction from h1 .
S

4.2 Relaxed Plan Extraction from hC
From Equation 10, we obtain  CFF by similar generalizations as we made to get from h+
to hC+ . Extending hC to sets G of conjunctions by hC ( G ) := maxcG hC (c), our definition
reads:
Definition 3 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing all singleton conjunctions. A hC -based critical-path delete-relaxed plan, short C-relaxed
plan, is a set  CFF of action occurrences ( a, G 0 ) where  CFF =  (G C ), with  being a partial
function on conjunction sets G that is defined on G C and satisfies  ( G ) =


c  G : c  I



C
0
0
0
 (( G \ G )  Gr )  {( a, G )} where a  A,
(11)

 6= G 0  {c | c  G, R(c, a) 6= , hC ( R(c, a)) = hC (c)  1},


and hC ( Gr0 ) = hC ( G 0 )  1
else
287

fiF ICKERT & H OFFMANN & S TEINMETZ

with Gr0 defined as Gr0 := cG0 R(c, a).
A hC -based no cross-context critical-path delete-relaxed plan, short nc-C-relaxed plan,
CFF of action occurrences with the same property, except that we define G 0 : = { R ( c, a ) |
is a set nc
r
0
c  G }.
S

This definition parallels the definition of hC+ (Definition 2). The subgoaling structure
is the same, over sets of conjunctions from C each of which must be achieved through
regression. Instead of a heuristic value, we compute a relaxed plan (consisting of action
occurrences, action a plus supported subgoals G 0 as in hC+ and C , as opposed to actions as
in the standard case). Instead of minimizing over all action occurrence choices, we impose
the use of best supporters according to hC . The major new source of complexity, relative
to Equation 10, is that we allow G 0 to be a subset of the best-supported atomic subgoals,
similarly as in Definition 2. Because a relaxed plan does not always exist, we allow  to be
partial and define  CFF only if  is defined on the goal. As we show below, this is possible
iff hC < , i. e., a C-relaxed plan exists iff any relaxed plan for C exists.
Observe that, relative to Equation 10, we have added the new additional condition
S
hC ( Gr0 ) = hC ( G 0 )  1. To understand this condition, consider that (a) Gr0 = cG0 R(c, a) is
the union over the regressions from each individual supported subgoal c  G 0 , and that (b)
for every such subgoal c  G 0 we have hC ( R(c, a)) = hC (c)  1, i. e., the action a selected is
a best supporter for c. From (b), one would surmise that hC ( Gr0 ) = hC ( G 0 )  1, because the
regressions R(c, a) in Gr0 each are one step easier to solve than their original counterparts
c  G. That is only so, however, if there are no cross-context conditions: otherwise, the
union (a) may be more difficult to achieve than each of its components. We get back to
this in detail below in Section 4.3. For now, just keep in mind that the additional condition
hC ( Gr0 ) = hC ( G 0 )  1 is required due to possible cross-context conditions. (We remark that
the condition is equivalent to hC ( Gr0 ) < hC ( G 0 ), as the hC value cannot decrease by more
than 1 in a single regression step; we have written it as hC ( Gr0 ) = hC ( G 0 )  1 merely to use
the most specific write-up.)
It is instructive to consider  CFF from a procedural perspective. Algorithm 2 provides
a corresponding extension of Algorithm 1. Where previously we computed h1 for all facts,
now we compute hC for all conjunctions in C. Where previously our subgoal sets Gi were
sets of facts, now they are sets of conjunctions from C. Where previously the new subgoals
generated were the selected actions precondition facts, now they are the atomic conjunctions contained in the regressed subgoal Gr0 . Note here the pointwise interpretation for ncC-relaxed plans, where Gr0 = { R(c, a) | c  G 0 } is a set of fact sets. In line 10, we now select
an action occurrence ( a, G 0 ) instead of just an action a, resulting in the additional choice of
supported atomic subgoals G 0 , and the accordingly more complicated structure of the regressed subgoal Gr0 . Note here that, for every c  Gi , we have hC (c) = i and, for any action
a0 , hC ( R(c, a0 ))  hC (c)  1 = i  1. Furthermore, if hC ( Gr0 ) = i  1 then hC ( R(c, a))  i  1
for every c  G 0 . Putting these observations together, we get hC ( R(c, a)) = hC (c)  1 for
every c  G 0 , and the choice of G 0 in Algorithm 2 is equivalent to that in Equation 11.
Example 6 Consider, as in Example 5 (page 281), our car-driving example with C containing the
singleton conjunctions as well as c = carY  fuel. We have hC ({carX }) = 0, hC ({fuel}) = 0,
hC ({carY }) = 1, hC ({carY, fuel}) = 2, and hC ({carZ }) = 3.
288

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

Algorithm 2: Relaxed plan extraction from hC . For C-relaxed plan, use Gr0 =
S
0
0
c G 0 R ( c, a ); for nc-C-relaxed plan, use Gr = { R ( c, a ) | c  G }.
1
2
3
4
5
6
7
8
9
10

11
12

compute hC (c) for all c  C
m := maxcC,cG hC (c)
if m =  then
return 
for i := m, . . . , 1 do
Gi := {c | c  C, c  G , hC (c) = i }
 := 
for i := m, . . . , 1 do
while ex. c  Gi s.t. c not TRUE at i do
select ( a, G 0 ) where a  A,  6= G 0  {c  Gi | R(c, a) 6= , c not TRUE at i },
and hC ( Gr0 ) = i  1
foreach c0  G 0 do
mark c0 TRUE at i

14

foreach c0  C s.t. ex. c  Gr0 with c0  c do
G h C ( c 0 ) : = Gh C ( c 0 )  { c 0 }

15

 :=   {( a, G 0 )}

13

16

return 

Tracing Equation 11 from the initializing call  CFF =  ({carZ }), we get the exact same
recursive development as in Example 5. First, carZ is supported only by ( aYZ , {carZ }), where
hC ({carY, fuel}) = 2 = 3  1 = hC ({carZ })  1 as required for the supported conjunction
c = carZ. Similarly, hC ( Gr0 ) = hC ( G 0 )  1 for Gr0 = {carY, fuel} and G 0 = {carZ } as for single
supported conjunctions there is no difference. The recursive subgoal is {carY, fuel, carY  fuel}.
The only supporting action for carY  fuel is arefuel . That action is a best supporter for carY  fuel
as hC ({carY }) = 1 = hC ({carY, fuel})  1. The action is not a best supporter for fuel though,
because fuel is true initially hC ({fuel}) = 0. So the only possible choice for supporting carY  fuel
is arefuel with G 0 := {carY  fuel}. We get Gr0 = {carY } and hC ( Gr0 ) = 1 = hC ( G 0 )  1 as
desired. The recursive subgoal is {fuel, carY }, supported by ( a XY , {carY }) yielding Gr0 = {carX }
with hC ( Gr0 ) = 0 = hC ( G 0 )  1.
Taking the procedural perspective in Algorithm 2, we start by inserting carZ into G3 . At layer
i = 3 we support this by ( aYZ , {carZ }) with the same Gr0 = {carY, fuel} and hC ( Gr0 ) = 2 = i  1.
Similarly, layers 2 and 1 mirror exactly the respective recursive invocations of Equation 11.
We next prove that C-relaxed plans and nc-C-relaxed plans do indeed correspond to
C . We start with C-relaxed plans:
relaxed plans for C respectively nc
Theorem 4 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing
all singleton conjunctions. Then any C-relaxed plan  CFF can be sequentialized to form a relaxed
plan for C .
289

fiF ICKERT & H OFFMANN & S TEINMETZ

Proof Sketch: Sequencing  CFF as h( a0 , G00 ), . . . , ( an1 , Gn0 1 )i in inverse order of action
occurrence selection in Equation 11, i. e., placing the outcome of recursive invocations up
front, h a0 [ G00 ], . . . , an1 [ Gn0 1 ]i is a relaxed plan for C . This is easy to show by induction
over the length of the sequence. With G0 , . . . , Gn being the recursive subgoals generated in
Equation 11, and si being the state after applying ai [ Gi0 ] in C , it holds that {c | c  Gi } 
si . This is obvious for i = 0. If it holds at i, it also holds at i + 1 because (a) the Gi+1 \ Gi0
part of Gi+1 is also part of Gi and hence true by induction hypothesis; and (b) the Gi0 part
of Gi+1 is made true by ai [ Gi0 ], which is applicable to si by induction hypothesis because its
precondition conjunctions are contained in Gi .
An almost identical proof shows the corresponding property for nc-C-relaxed plans:
Theorem 5 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing
CFF can be sequentialized to form a relaxed
all singleton conjunctions. Then any nc-C-relaxed plan nc
C.
plan for nc
C exists if and only if a C-relaxed plan
Finally, a relaxed plan for C respectively nc
respectively an nc-C-relaxed plan exists. This is simply because all of these properties are
fully determined by the critical-path component. Our proof shows this via deriving an
intermediate equation, Equation 12 below, which characterizes the behavior of  CFF and
CFF when restricting the choice of supported subgoal sets G 0 to singletons. Equation 12
nc
will play an important role in the comparison to related work, and in our experiments.

Theorem 6 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing
all singleton conjunctions. Then a C-relaxed plan exists if and only if an nc-C-relaxed plan exists
if and only if hC < .
Proof Sketch: We show the claim in two parts, (a) a C-relaxed plan exists if and only
if hC < , and (b) an nc-C-relaxed plan exists if and only if hC < . The only if
directions follow as corollaries of (a) Theorems 3 and 4 respectively (b) Theorems 3 and 5:
if hC = , neither a C-relaxed plan nor an nc-C-relaxed plan can exist, because otherwise
C would exist by Theorem 4 respectively Theorem 5,
a relaxed plan for C respectively nc
C ) as per Theorem 3.
in contradiction to hC =  = h1 (C ) = h1 (nc
CFF restricting the choice of
For the if directions, we consider versions of  CFF and nc
supported subgoal sets G 0 to singletons, i. e., to single conjunctions G 0 = {c}. Each of  CFF
CFF then simplifies to S
and nc
cG C  ( c ), with  (.) being a partial function on conjunctions
c that satisfies


cI
 S
0 )  {( a, { c })} where a  A,

(
c
 (c) =
(12)
0
C
 c  R(c,a)
C
C
R(c, a) 6= , and h ( R(c, a)) = h (c)  1 else
Note the similarity to Equation 8 (page 285): we are now back to a more common notation
for relaxed plan extraction (over C instead of singleton facts), extracting best supporters
one-by-one.
By changing the subgoaling structure, one can transform Equation 12 into the form
 (G) where  is a partial function on fact sets G that satisfies
290

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION



GI



 ( R( G, a))  {( a, G )} where a  A,
 (G) =
R( G, a) 6= , and hC ( R( G, a)) = hC ( G )  1 G  C


 S
0
else
G 0  G,G 0 C  ( G )

(13)

Comparing this to the hC equation (Equation 3, page 278), it is clear that the subgoaling
structure of the two equations coincides for subgoals c with hC (c) < , and in particular,
if hC <  then Equation 13 has a solution  defined on G . Therefore, Equation 12 has a
solution defined on all c  G C . As Equation 12 captures a restricted version of  CFF and
CFF , C-relaxed and nc-C-relaxed plans exist as desired.
nc

4.3 The Subgoal-Support Selection Problem
We have so far shown how C-relaxed plans and nc-C-relaxed plans can be extracted. We
CFF are defined. Given
have not yet explained how our actual heuristic functions hCFF and hnc
CFF
CFF
C
Theorem 6, both h
and hnc return  in the case h = . For the case hC < , our
description of relaxed plan extraction so far does not specify how to choose the supported
CFF .
subgoal sets G 0 . Taking that choice in particular ways yields the functions hCFF and hnc
The choice is non-trivial because the number of possible action occurrences is worst-case
exponential in |C |. We refer to this choice as the subgoal-support selection problem.
We start by discussing the optimization objective for that problem. Then we fix soluCFF and  CFF in this order, defining the desired heuristics hCFF and hCFF through
tions, for nc
nc
corresponding specializations of Equation 11. We close the section with a brief discussion
of prior work in the light of our findings.
4.3.1 T HE O PTIMIZATION O BJECTIVE
Assume that hC < . As argued in the proof of Theorem 6, we know that Equation 12 has a
solution, so in principle we could restrict ourselves to | G 0 | = 1, resulting in at most |A|  |C |
different action occurrence choices. However, this can result in dramatic overestimation:
Example 7 Consider the task  = (F , A, I , G) where F = { g1 , . . . , gn }, I = , G =
{ g1 , . . . , gn } and A contains the single action a whose precondition and delete list are empty and
whose add list is { g1 , . . . , gn }. Obviously, h = h+ = hFF = 1. However, even with C containing only the singleton conjunctions { gi }, Equation 12 results in dramatic overestimation: the
C-relaxed plan will collect a separate occurrence ( a, { gi }) for every gi , resulting in relaxed plan
length n. If C also contains all fact-pair conjunctions { gi , g j } then we get a C-relaxed plan of size
n(n1)

n+
. In general, we get a C-relaxed plan of size |C |.
2
While this is an extreme example, similar situations arise whenever conjunctions overlap, because an action a adding a single fact p then is a possible supporter of all conjunctions that contain
p. With, e. g., the conjunctions containing all fact pairs, this means that the number of top-level
goal conjunctions supported by a is at least |G|. In domains with many top-level goal facts  including most current IPC benchmarks and, more generally, e. g. typical transportation, construction,
puzzle problems  this is clearly detrimental. (Replacing G by a single fact and a new goal-achiever
action only moves the problem to the precondition of that action.)
291

fiF ICKERT & H OFFMANN & S TEINMETZ

This is essentially the same observation made by Haslum (2009, 2012), non-admissibility
of h+ (m ) as every conjunction must be achieved separately, which prompted the design
of C where every action may achieve arbitrary subsets of conjunctions. What is new here
is the particular context in which we consider this issue, namely the choice of G 0 in  CFF
CFF as per Equation 11: We moved the issue from the generic planning-task level to
and nc
the specific subgoal-support selection level. This more specific perspective identifies the
precise source of complexity, as far as relaxed plan extraction is concerned: How to choose
the sets G 0 in Equation 11  equivalently, how to implement line 10 in Algorithm 2  in a manner
avoiding overestimation to the extent possible?
The intuitive answer to this question, given Example 7, certainly is choose G 0 to be as large
as possible. This intuition is not entirely correct. As we detail in Example 9 (Appendix A),
there are cases where supporting a conjunction c, even though it is feasible, is better done
later on in the recursion, with an action whose precondition is easier to combine with c.
Nevertheless, we employ | G 0 | maximization here, deeming it safe to presume that overlapping conjunctions as per Example 7 are much more practically relevant than contrived
situations as per Example 9.
It will be convenient to introduce a terminology for feasible choices of G 0 . As per
Equation 11, the possible choices of G 0 are those where a is a best supporter for every
c  G 0 , i. e., hC ( R(c, a)) = hC (c)  1, and where the overall regressed subgoal is feasible,
S
hC ( Gr0 ) = hC ( G 0 )  1. In this case, we say in the  CFF context, i. e., with Gr0 = cG0 R(c, a),
CFF context, i. e., with G 0 = { R ( c, a ) | c  G 0 }, that G 0 is
that G 0 is C-feasible. We say in the nc
r
nc-C-feasible.
Our maximization problems then are:
Definition 4 By C-SubgoalSupport we denote the following problem:
Given a planning task , a set of conjunctions C in  containing all singleton conjunctions,
G  C, an action a in , and K  N. Does there exist G 0  {c  G | R(c, a) 6= , h1 ( R(c, a)) =
h1 (c)  1} such that G 0 is C-feasible and | G 0 |  K?
We define nc-C-SubgoalSupport accordingly for nc-C-feasible G 0 .
CFF H EURISTIC
4.3.2 T HE hnc
CFF , i. e., nc-C-SubgoalSupport, is easy to
The subgoal-support selection problem for nc
0
solve. Indeed, any choice of G is nc-C-feasible:

Proposition 2 Let  be a planning task, C a set of conjunctions in  containing all singleton
conjunctions, G  C, and a an action in . Then any G 0  {c  G | R(c, a) 6= , h1 ( R(c, a)) =
h1 (c)  1} is nc-C-feasible.
Proof: By definition, G 0 is nc-C-feasible if hC ( Gr0 ) = hC ({ R(c, a) | c  G 0 }) = hC ( G 0 ) 
1. Now, hC ({ R(c, a) | c  G 0 }) = maxcG0 hC ( R(c, a)) which by construction equals
maxcG0 (hC (c)  1). The latter equals (maxcG0 hC (c))  1 = hC ( G 0 )  1 as desired.
CFF , the additional condition hC ( G 0 ) = hC ( G 0 )  1 in Definition 3
In other words, for nc
r
is redundant. To maximize | G 0 |, we can simply include into G 0 all c where hC ( R(c, a)) =
CFF as:
hC ( G 0 )  1. Accordingly, we define our heuristic function hnc

292

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

Definition 5 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  conCFF =  if
taining all singleton conjunctions. The nc-C-relaxed plan heuristic is defined as hnc
CFF = |  CFF | where  CFF =  (G C ) and  satisfies  ( G ) =
hC = , and otherwise hnc
nc
nc

c  G : c  I
 
C
0
0
0
(14)
 (( G \ G )  Gr )  {( a, G )} where a  A,

0
C
C
 6= G = {c | c  G, R(c, a) 6= , h ( R(c, a)) = h (c)  1} else
with Gr0 defined as Gr0 := { R(c, a) | c  G 0 }.
In words, we restrict Equation 11 to a maximal choice of G 0 in the middle case, using
G 0 = {c  G | R(c, a) 6= , hC ( R(c, a)) = hC (c)  1} instead of G 0  {c  G | R(c, a) 6=
C variant of the regressed subgoal G 0 , and we
, hC ( R(c, a)) = hC (c)  1}. We use the nc
r
C
0
C
0
drop the condition h ( Gr ) = h ( G )  1 which is redundant for that variant.
4.3.3 T HE hCFF H EURISTIC
Matters are not that simple for  CFF , i. e., C-SubgoalSupport, which requires C-feasible sets
CFF setting is trivial
G 0 as opposed to nc-C-feasible ones. The feasible choice of G 0 in the nc
CFF ignores cross-context conditions. Not ignoring these conditions,
(Proposition 2) because nc
CFF
in  , this is no longer true:
Example 8 Consider, as in Example 5 (page 281), our abstract example with conjunctions cq1p =
q1  p, cq2p = q2  p, and cq1q2 = q1  q2 . After supporting each of the goal facts, we get the
subgoal G = {q1 , q2 , p, q1  p, q2  p}. Ignore, like in Example 5, the subsumed subgoals q1 , q2 , p
which can be tackled as a side effect of tackling the non-subsumed ones q1  p and q2  p. The
only possible supporting action for the latter subgoals is a p which adds p (q1 is true initially, and
the action adding q2 deletes p so cannot support q2  p). There are three possible choices of G 0 :
0 : = { q  p, q  p }, G 0 : = { q  p }, or G 0 : = { q  p }.
G12
2
2
1
1
2
1
S
For G10 , Gr0 = cG10 R(c, a) = {q1 } and hC ({q1 }) = 0 = hC ({q1 , p})  1. So G10 is CS
feasible. For G20 , Gr0 = cG20 R(c, a) = {q2 } and hC ({q2 }) = 1 = hC ({q2 , p})  1. So G20
0 is not C-feasible, because G 0 = S
0 R ( c, a ) = { q1 , q2 },
is C-feasible as well. However, G12
c G12
r
corresponding to the atomic conjunction q1  q2 . Selecting both atomic subgoals q1  p and q2  p,
even though each is feasible individually, incurs the cross-context condition q1  q2 , an atomic
conjunction not present in the regression from either of q1  p or q2  p individually. The example
0 )1 =
is constructed so that hC ({q1 , q2 }) = , hence in particular hC ({q1 , q2 }) =  6= hC ( G12
C
h ({{q1 , p}, {q2 , p}})  1 = 1.
0 in  CFF , we get G 0 = { R ( c, a ) | c  G 0 } = {{ q }, { q }} instead. In
Note here that, for G12
2
1
nc
r
12
other words, we get a set containing two small conjunctions q1 and q2 , instead of a set containing
one big conjunction q1  q2 . We have hC ({{q1 }, {q2 }}) = 1 = hC ({{q1 , p}, {q2 , p}})  1, so
0 is (not C-feasible but) nc-C-feasible.
G12
As the example shows, cross-context conditions may render particular combinations of supported conjunctions in G 0 infeasible. Having used this formulation, it should come as no
surprise that maximizing | G 0 | while avoiding such combinations is computationally hard:
Theorem 7 C-SubgoalSupport is NP-complete.
293

fiF ICKERT & H OFFMANN & S TEINMETZ

Proof Sketch: Membership by guess and check. Hardness via a reduction of Hitting Set:
Given a set of elements E and a collection of subsets b  E of elements, the construction
is such that, at a particular point during C-relaxed plan extraction, choosing G 0 amounts
to choosing E0  E, where E0 is C-feasible (results in a hC value 6= ) iff there exists no b
with b  E0 . Given this, E0 \ E is a hitting set, and maximizing | E0 | is equivalent to finding
a minimum-size such set.
So it is hard to find a cardinality-maximal feasible set of supported conjunctions in
 CFF . Presuming that we do not want to invest the effort to solve that problem exactly
(many times during the extraction of a C-relaxed plan on every search state), we need
an approximate solution. A canonical choice for approximating cardinality-maximality is
subset-maximality. We say that G 0  {c  G | R(c, a) 6= , h1 ( R(c, a)) = h1 (c)  1} is
subset-maximally C-feasible if G 0 is C-feasible and, for every G 00 such that G 0 ( G 00  {c 
G | R(c, a) 6= , h1 ( R(c, a)) = h1 (c)  1}, G 00 is not C-feasible. Our heuristic function hCFF
is defined using the corresponding restriction of Equation 11:
Definition 6 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing all singleton conjunctions. The C-relaxed plan heuristic is defined as hCFF =  if hC = ,
and otherwise hCFF = | CFF | where  CFF =  (G C ) and  satisfies  ( G ) =


c  G : c  I



C
0
0
0
 (( G \ G )  Gr )  {( a, G )} where a  A,
(15)

 6= G 0  {c | c  G, R(c, a) 6= , hC ( R(c, a)) = hC (c)  1},


and G is subset-maximally C-feasible
else
with Gr0 defined as Gr0 :=

S

c G0

R(c, a).

A subset-maximally C-feasible set G 0 can be found through simple greedy algorithms,
adding conjunctions one-by-one as shown in Algorithm 3. The candidate conjunctions are
those c  G with R(c, a) 6=  and hC ( R(c, a)) = hC (c)  1. Starting with empty G 0 , we just
try each candidate c exactly once. This suffices to get a subset-maximal G 0 because, as G 0
can only grow, if adding c was not feasible the first time around then adding c cannot be
feasible later on either.
Algorithm 3: Greedy selection of a subset-maximally C-feasible set of supported subgoals G 0 in C-relaxed plan extraction. Implements line 10 in Algorithm 2 to obtain the
heuristic function hCFF .
1 select c  Gi , c not TRUE at i
2 select a  A with R ( c, a ) 6 =  and hC ( R ( c, a )) = hC ( c )  1
3 G0 := {c}
4 foreach c0  Gi s.t. c0 not TRUE at i, R ( c0 , a ) 6 = , and hC ( R ( c0 , a )) = hC ( c0 )  1 do
5
if G 0  {c0 } is C-feasible then
6
G 0 := G 0  {c0 }

We remark that, as Example 9 (Appendix A) shows, there are cases where selecting
a non-subset-maximally C-feasible G 0 leads to a strictly smaller C-relaxed plan. In other
words, like cardinality maximization, subset-maximization is not fail-safe.
294

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

4.3.4 P REVIOUS W ORKS R ELATED TO THE S UBGOAL -S UPPORT S ELECTION P ROBLEM
Interestingly, while the subgoal-support selection problem has never previously been identified, it has already been solved. More plainly put, the previous works in this area can be
viewed as solving the problem at a very abstract level, not identifying what precisely the
problem is, and thus ending up with solutions that do solve the problem, but using unnecessarily drastic measures. Mostly this is due to the compilation view, where relaxed
plan extraction becomes a standard technique, yet the subgoal-support selection problem
has to be solved at the STRIPS level, in the form of the compiled task. The single noncompilation-view prior work, by Alcazar et al. (2013), was conducted as part of a much
broader scope, and does not address the subgoal-support selection problem in detail.
Let us have a closer look at Alcazar et al.s heuristic, FFm (which they implement for
m = 2). This extracts a relaxed plan from hm , restricting C to contain exactly the conjunctions of size  m. That restriction is easily removed. In our notation, FFm then corresponds
S
to a C-relaxed plan extracted using this equation: FFm = cG C  (c) where


cI
 S
0
 (c) =
(16)
0
C  ( c )  { a } where a  A,
 c  R(c,a)
R(c, a) 6= , and hC ( R(c, a)) = hC (c)  1 else
CFF simplify to when reThis is almost exactly what the definitions of both  CFF and nc
0
stricting the choice of G to support only a single conjunction G 0 = {c}, i. e., it is almost
identical to Equation 12 as derived in the proof to Theorem 6. Repeating Equation 12 for
CFF = S
convenience:  CFF = nc
cG C  ( c ) where


cI
 S
0 )  {( a, { c })} where a  A,

(
c
 (c) =
0
C
 c  R(c,a)
R(c, a) 6= , and hC ( R(c, a)) = hC (c)  1 else

The only difference between these two equations is that FFm collects a set of actions as in
CFF collect a set of (single-supported-subgoal) action
the standard setting, while  CFF and nc
occurrences.
In this sense, Alcazar et al.s approach over-simplifies the choice of G 0 , to singleton sets.
C rather than C because, with | G 0 | = 1, cross-context conditions
It effectively tackles nc
never occur. It would furthermore run the risk of excessive overestimation as pointed out
in Example 7  if it did actually collect action occurrences, rather than actions. The latter
might be viewed as a trick to avoid overestimation, yet from a theoretical perspective it
rather defeats the purpose of using explicit conjunctions in the first place. Whereas relaxed
planning on C converges to h , FFm is bounded from above by the number of actions, |A|.
Altogether, our findings allow to understand prior work on this subject as follows:
0

 C Compilation (Haslum, 2012): Includes one compiled action aG for every possible
pair of action a and possible set of supported subgoals G 0 . In this sense, it solves the
NP-complete problem C-SubgoalSupport enumeratively, in-memory.7
Lesson learned in hCFF : There is no need to pre-generate all possible conjunction subsets an
action could support. We can focus on the subgoals that actually arise during relaxed plan
extraction.
0

7. Plus, without actually giving an optimality guarantee: the optimal aG will be in the set of choices for
relaxed plan extraction/the best-supporter function, but there is no guarantee that it will be selected.

295

fiF ICKERT & H OFFMANN & S TEINMETZ

C Compilation (Keyder et al., 2012, 2014): Includes one conditional effect for every
 ce
pair of action a and possibly supported conjunction c. This ignores cross-context
conditions and hence trivializes C-SubgoalSupport into nc-C-SubgoalSupport.
Lesson learned in hCFF : There is no need to ignore cross-context conditions completely. We
can greedily select supported conjunctions whose cross-context conditions are feasible.

 FFm (Alcazar et al., 2013): Restricts the conjunction set C to the size- m conjunctions
as in hm . Restricts the supported subgoals G 0 to single conjunctions, thus trivializing
C compilation.
C-SubgoalSupport and ignoring cross-context conditions like the ce
Collects actions instead of action occurrences, losing convergence to h .
Lesson learned in hCFF : All of these weaknesses can be avoided.

5. Experiments
CFF heuristic functions relative to the most closely
We evaluate the benefits of the hCFF and hnc
related previous heuristics. We state the key issues that we will consider in terms of four
hypotheses, formulating our major expectations regarding algorithm behavior on IPC benchmarks, the standard means for evaluation in the planning community:8

(H1) For hCFF relative to hFF (C ), the hypothesis is that (H1) avoiding the exponential blowup in |C | typically yields a faster heuristic and thus improved performance.
C ), the hypothesis is that (H2) accounting for cross-context
(H2) For hCFF relative to hFF (ce
conditions can yield a more informed heuristic and thus improved performance.
The difference between typically and can in (H1) vs. (H2) is intended. Crosscontext conditions presumably are important only in particular cases, whereas the
advantage of hCFF s smaller representation presumably helps in most cases.
CFF relative to hFF ( C ), the hypothesis is that (H3) the implemen(H3) For both hCFF and hnc
ce
CFF typically is more effective and thus yields improved performance.
tation of hCFF and hnc
CFF are direct, not using a compilation, and thus
We expect this to be so as hCFF and hnc
FF
C ).
are more specialized than that of h (ce
CFF and hFF ( C ) are equivalent except for the implementation, in
Note here that hnc
ce
that they use the same information and have the same scaling behavior in |C |. (In
contrast to the comparison between hCFF vs. hFF (C ), which is dominated by (H1)
the drastically different scaling behavior in |C |.)

(H4) We furthermore compare hCFF to a variant we denote hCFF
, as per Equation 12
| G 0 |=1
0
where we restrict to | G | = 1, the hypothesis being that (H4) the non-trivial subgoal
support selection in hCFF typically yields a more informed heuristic and thus improved performance.
We finally include a variant we denote hCFF
, as per Equation 16 where | G 0 | = 1 and a
| G 0 |=1A
set of actions (as opposed to action occurrences) is selected. This serves as a comparison to
Alcazar et al.s (2013) work.
8. The hypotheses are not intended as formal statements that we will statistically accept or reject; nor are
they intended as an exhaustive representation of all issues we will discuss. They merely serve as a red
thread in the discussion of our large-scale experiments.

296

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

Section 5.1 describes some key points of our implementation, Section 5.2 explains our
experimental setup. Section 5.3 provides comprehensive results, across planner variants,
for small conjunction sets C which turn out to be best in terms of overall performance.
Section 5.4 then analyzes behavior as a function of growing C.
5.1 Implementation
CFF in FD (Helmert, 2006). We furthermore implemented
We implemented hC , hCFF , and hnc
Cadd
the C-additive heuristic h
, defined exactly like hC (Equation 3) except that the maximization over atomic subgoals is replaced by a summation over these subgoals:

GI
 0
1 + minaA,R(G,a)6= h( R( G, a)) G  C
(17)
h( G ) =

0
else
G0 G,G0 C h( G )

In other words, the step from hC to hCadd parallels that from hmax to hadd (Bonet & Geffner,
2001). We dont use hCadd as a heuristic function per se: in contrast to the standard setting,
atomic subgoals overlap in the general case, so that summation doesnt make sense. We
use hCadd as an alternative best-supporter function for relaxed plan extraction. For that
purpose, it turns out to be fairly useful empirically.
As computing critical-path heuristics becomes expensive with many conjunctions, a
key to practicality is an efficient implementation of hC . To that end, we extend the counterbased algorithm originally implemented in FF (Hoffmann & Nebel, 2001) for computing h1
(aka a relaxed planning graph). Our extended algorithm is easily described as a modification of the original algorithm. Assume an input state s. FFs original algorithm associates
the precondition of each action a  A with a counter, denoted here count( a), initialized to
|pre( a)|. Facts p are maintained in a priority queue ordered by an associated v( p) value,
which equals h1 ( p) once p has been dequeued. The queue is initialized with the facts p
true in s, each associated with value v( p) = 0. The main loop dequeues facts, activates
new actions, and maintains the v values. When a fact p is dequeued, a loop over all actions
a with p  pre( a) decrements count( a). If this results in count( a) = 0 then the action is activated, enqueuing every q  add( a) with value v(q) = 1 + max p0 pre(a) v( p0 ), or reducing
v(q) to that value in case q is already in the queue with a higher value.9 The algorithm
stops if either all goal facts have been dequeued and h1 (s) = max pG v( p), or the queue
has become empty and h1 (s) = .
Our extension to hC works in much the same way. We just need to maintain the values
v(.) for conjunctions c  C instead of single facts, and we need to maintain counters for pairs
of action and supported conjunction instead of just actions. Precisely, we create a counter
count(c, a) for every c  C and a  A where R(c, a) 6=  and R(c, a) does not contain a
mutex, i. e., a fact pair known to be unreachable. The latter corresponds to mutex pruning
C . It reduces computational effort as well
as discussed by Keyder et al. (2014) for C and ce
as strengthens the heuristic.
In the extended algorithm, each counter count(c, a) is initialized to |{c0 | c0  C, c0 
R(c, a)}|, i. e., to the number of sub-conjunctions we need to make true in order to be
able to achieve c using a (remember here that C contains all singleton conjunctions). The
9. For general action costs c( a), one can simply use v(q) = c( a) + max p0 pre( a) v( p0 ) here.

297

fiF ICKERT & H OFFMANN & S TEINMETZ

queue now contains conjunctions c0  C instead of facts. It is initialized with the conjunctions c0  s, with v(c0 ) = 0. Dequeueing a conjunction c0 , we loop over all counters
count(c, a) where c0  R(c, a), decrementing count(c, a). If this results in count(c, a) = 0,
we enqueue/upgrade c with value 1 + maxc0 C,c0  R(c,a) v(c0 ).
To compute hCadd , we use the exact same algorithm except that maximization is replaced by summation, i. e., instead of 1 + maxc0 C,c0  R(c,a) v(c0 ) we use 1 + c0 C,c0  R(c,a) v(c0 ).
Based on the conjunction values v(c) computed for hC respectively hCadd , the impleCFF follows Algorithm 2 (page 288). In particular, we select the
mentation of hCFF and hnc
CFF we fix G 0 = { c | c 
action/supported subgoals ( a, G 0 ) as previously discussed. For hnc
CFF
0
G, R(c, a) 6= , v( R(c, a)) = v(c)  1}. For h , we select G as per Algorithm 3 (page 294).
Here, the v(c) values of single conjunctions c are readily available. Values v( X ) for a fact
set X are required for X := R(c, a), as well as during the check for C-feasibility in hCFF
S
(Algorithm 3 line 5), where X := c00 G0 {c0 } c00 with G 0 being the current set of supported
subgoals and c0 being a candidate for inclusion into that set. We compute v( X ) by a loop
over the facts p  X, using lists C [ p] containing the c  C where p  c, and maximizing
respectively summing over v(c) for those c  C [ p] where c  X.
Helpful actions (Hoffmann & Nebel, 2001), i. e., FD preferred operators, are defined
similarly as for hFF . An action a applicable to state s is preferred in s if the C-relaxed plan
contains a, i. e., an action/supported subgoals pair of the form ( a, G 0 ). This corresponds to
C ), based on compiled actions
the selection of preferred operators a in hFF (C ) and hFF (ce
a[C 0 ] occurring in relaxed plans in the respective compilations (Keyder et al., 2014).
The performance of satisficing search in planning is known to be brittle with respect to
minor differences in the heuristic functions (e. g. Valenzano, Sturtevant, Schaeffer, & Xie,
2014). This is important also in our setting. The unavoidable implementation differences
between our new heuristics and their predecessors turn out to be a major complication for
a fair comparison. All heuristics extract relaxed plans from a hC or hCadd best-supporter
C ) do so via a compilation, while hCFF and hCFF do not.
function, yet hFF (C ) and hFF (ce
nc
The relaxed plan extraction algorithms work on different representations. In particular,
the choice of an action in hFF (C ), i. e., of a compiled action a[C 0 ], corresponds to the
choice of an action/supported subgoals pair ( a, G 0 ) in hCFF . By design, hFF (C ) cannot
distinguish between choosing a vs. choosing G 0 . In contrast, by design hCFF chooses first
only the action a and then assembles G 0 by greedy C-feasible maximization.
To offset these unavoidable differences in relaxed plan extraction, we experiment across
C , the tiea variety of tie-breaking strategies in the choice of best supporters. In C and ce
CFF it applies to actions a supportbreaking applies to compiled actions a[C 0 ], in hCFF and hnc
C
C
ing the same conjunction c and where h ( R(c, a)) = h (c)  1 respectively hadd ( R(c, a)) =
hadd (c)  1. Our strategies are:
Arbitrary: Choose an arbitrary best supporter, i. e., the first one we find. This is used (with
C ).
hadd best supporters) in FDs implementation of hFF , as well as hFF (C ) and hFF (ce
Random: Choose a random best supporter. We use 3 different random seeds in our experiments to gauge the performance variance incurred by this criterion. It turns out
that, in almost all cases, the variance is small and the performance change relative
to arbitrary tie-breaking is consistent across random seeds, i. e., consistently positive
298

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

or consistently negative. Thus random tie-breaking exhibits reliable behavior as an
algorithm option.
Difficulty (hC only): This is the tie breaking mechanism used in FF (Hoffmann & Nebel,
2001). It selects a best supporter that minimizes the summed-up hmax values of the
CFF , this translates to summing up the hC
supporters preconditions. In hCFF and hnc
values of the conjunctions contained in the regressed subgoals R(c, a). Remaining
ties are broken arbitrarily.
We use each of these 3 tie-breaking strategies with hC , and the first 2 strategies with hCadd ,
CFF , hFF ( C ), hFF ( C ), hCFF , and hCFF
.
in each of the 6 heuristics hCFF , hnc
ce
| G 0 |=1
| G 0 |=1A
5.2 Experiments Setup and Design
To be able to compare the different heuristic functions directly, in all such comparisons we
use the same conjunction set C for every heuristic. We find these sets C using the exact
same methods, and implementation, as used by Keyder et al. (2014). The motivation is
that our contribution here does not pertain to methods finding C, and re-using the established methods provides for better comparability. To understand our experiments, it is not
necessary to understand the generation of C in detail, so we give a brief summary only.
Keyder et al.s (2014) method is a variant of the method proposed earlier on by Haslum
(2012). In a pre-process to the actual search, C is learned by iteratively refining a deleterelaxed plan on the initial state. Starting with empty C, a relaxed plan  + for C is generated. If  + is a real plan (a plan for the original input task), the process stops. Else, an
analysis step finds a set C 0 of new conjunctions which exclude  + , i. e., such that  + is no
longer a relaxed plan for C when setting C := C  C 0 . Then the process iterates. Running
this ad infinitum, one will eventually find a plan for the input task. But that is typically
not feasible. To find instead a set C for heuristic search, the algorithm applies both, a time
limit T, and a size limit x on C relative action set size increase, i. e., on |AC |/|A|. If either
of the two criteria applies, the process stops and hands over the current set C to the search.
(Each limit may be set to , meaning that this termination criterion is disabled.)
As all heuristics in our experiments use explicit conjunctions, and all use the same
set C, we separate the generation of C from the actual experiments. We apply separate
runtime limits for C-generation and search respectively, and we will report only about the
performance of search not about that of C-learning. Given this, T merely serves as a means
to keep the experiments feasible even for large size limits x. We fix T to 30 minutes.
Throughout, we use FDs lazy-greedy best-first search with a dual open queue for preferred operators (Helmert, 2006), which profits from the search space pruning afforded
by preferred operators, yet preserves completeness by keeping the pruned nodes in the
second open queue. This is the canonical search algorithm for satisficing planning with
delete-relaxation heuristics, widely used as a baseline that yields competitive performance
while being reasonably simple. (Textbook single-queue greedy best-first search lags far behind the state of the art, as it can either not use preferred operators, or loses the solutions
in those cases where preferred operators are too restrictive.) The experiments were run on
a cluster of machines with Intel Xeon E5-2660 processors running at 2.2 GHz. The memory limit was set to 4 GB. We used the benchmarks from the satisficing tracks of the two
most recent International Planning Competitions, IPC11 and IPC14. We do not include
299

fiF ICKERT & H OFFMANN & S TEINMETZ

400000

104

hFF (C )
hCFF

370
360

103

300000

350
340
330

102

320

200000

310

101

300
290

100000

280

100

270
260

01
2

22

23

24

25

26

27

28

29

210

101 1
10

100

(a)

101

(b)
hCFF

hFF (C ).

102

103

104

21

hFF (hadd arb)
hFF (hadd rnd)
22

23

24

25

26

27

28

29

210

(c)
hCFF

Figure 1: Data preview for
vs.
(a) Number of counters in
vs. number of
C
C
actions |A | in  , as a function of the size limit x. (b) States per second with
x = , x-axis hCFF , y-axis hFF (C ). (c) Total coverage as a function of x. In (b)
and (c), each of hCFF and hFF (C ) is run with hCadd using random tie-breaking.
In (c), we also include hFF as a baseline, with two tie-breaking variants: hadd
with arbitrary tie-breaking corresponds to the FD default, hadd with random tiebreaking has better performance on these benchmarks. Recall in this comparison
that the effort for C-learning is not included in hCFF and hFF (C ) (see text).
the IPC14 CityCar domain, in which the FD translator generates actions with conditional
effects, not supported by our implementation. For each domain, each test suite has 20 instances (some domains have been used in both IPC11 and IPC14 so have two test suites).
With 6 heuristic functions, 5 best-supporter definitions (hC vs. hCadd , tie breaking), and
the numeric size-limit parameter x, the experiments space is large. To motivate how we
organize our exploration of that space in what follows, Figure 1 gives a data preview.
The major benefit of hCFF over hFF (C ) is the better scaling in |C |. One would expect this to manifest itself, for large C, in (a) a smaller representation and thus (b) a faster
heuristic function. Figure 1 (a) and (b) confirm that this is indeed so.10 For x = , where
C contains the conjunctions learned within 30 minutes, we get speed-ups of 14 orders of
magnitude. Now, while this is good news, it turns out that in most cases large C is detrimental. While search space size generally does decrease when increasing the size limit
x, all heuristic functions also become slower. The slowdown is dramatic for hFF (C ). It
is much less dramatic for hCFF , but still typically enough to outweigh the search space
reduction. Figure 1 (c) shows the effect: overall coverage becomes worse with growing
x, dramatically for hFF (C ), in a more benign manner but still almost monotonically for
hCFF . The best overall coverage is most often (across heuristics, configurations, domains)
obtained at x = 2, which is also the best setting of x in Keyder et al.s (2014) experiments.
The hFF baselines in Figure 1 (c) are based on hCFF using only the singleton conjunctions (x = 1), for better comparability with our methods, and to have the same 5 tiebreaking strategies at our disposal. In comparison to these baselines, hCFF consistently out10. In Figure 1 (a), for x = 2 the number of counters in hCFF exceeds |AC |. This cannot happen in theory,
as per Definition 1, because C includes an action a[{c}] for every counter count(c, a). It does happen in
practice only due to the handling of facts, i. e., the actions original pre/add/del lists: while Definition 1
handles these as singleton conjunctions, our implementation of C uses the more effective special-case
handling as per Haslums (2012) definition.

300

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

hCFF
hC
domain
Barman11
Barman14
CaveDiving14
ChildSnack14
Elevators11
Floortile11
Floortile14
GED14
Hiking14
Maintenance14
Nomystery11
Openstacks11
Openstacks14
Parcprinter11
Parking11
Parking14
Pegsol11
Scanalyzer11
Sokoban11
Tetris14
Thoughtful14
Tidybot11
Transport11
Transport14
Visitall11
Visitall14
Woodworking11


hCadd

arb rnd dif arb rnd
3
1
7
2
11
20
20
20
15
10
7
19
17
16
5
1
20
17
18
7
13
11
0
0
2
0
20

2
0
7
0
19
20
20
20
17
13
6
19
17
10
20
17
20
18
16
7
13
15
2
0
18
4
20

8
3
7
14
18
20
20
20
14
10
10
19
15
13
8
1
20
20
17
8
9
13
6
2
3
0
20

8
3
7
2
20
20
20
20
14
9
7
19
15
12
8
4
20
20
17
8
10
14
12
9
16
4
20

12
7
7
1
20
20
20
20
16
14
7
20
16
12
14
9
20
20
17
8
11
16
13
7
17
4
20

282 340 318 338 368

hFF (C )
hadd (C )
arb rnd dif arb rnd
h1 (  C )

0
1
7
2
8
20
20
20
16
11
7
20
15
12
2
0
20
18
18
4
13
10
2
0
15
4
20

0
0
7
0
12
20
20
20
10
10
6
16
8
12
19
9
20
18
16
7
10
15
1
0
13
4
20

1
0
7
0
15
20
20
20
16
11
10
20
15
13
4
0
20
20
18
3
15
15
5
2
16
4
20

14
7
7
0
19
20
20
20
15
11
12
20
14
12
11
5
20
19
17
5
12
14
10
7
16
4
20

1
0
7
0
18
20
20
20
13
11
6
20
16
7
20
20
20
20
17
8
10
19
11
7
14
4
20

285 293 310 351

349

C)
hFF (ce
hadd (Cce )
arb rnd dif arb
rnd

h1 (Cce )

1
1
7
2
9
20
20
20
16
11
7
20
14
8
2
0
20
18
18
4
13
13
2
0
3
0
20

0
0
7
0
12
20
20
20
11
9
6
16
9
11
16
8
20
19
16
9
9
17
3
0
13
4
20

2
1
7
0
15
20
20
20
15
11
11
20
13
10
4
0
20
20
16
4
15
15
4
1
3
0
20

15
8
7
0
20
20
20
20
15
11
12
19
12
9
12
5
20
19
17
4
11
14
10
10
16
4
20

2
1
7
0
19
20
20
20
12
12
6
20
16
10
20
19
20
20
17
9
10
18
11
9
14
4
20

269 295 287 350

356

hC

CFF
hnc

hCadd
arb rnd dif arb rnd
2
1
7
2
11
20
20
20
15
10
7
20
17
16
5
1
20
18
18
7
12
11
0
0
2
0
20

2
0
7
0
20
20
20
20
16
12
6
19
16
10
20
20
20
20
17
9
12
15
2
0
18
4
20

8
3
7
14
18
20
20
20
12
11
10
19
15
14
7
1
20
20
17
8
9
13
5
2
3
0
20

11
4
7
2
20
20
20
20
15
10
7
20
13
12
8
5
20
20
17
8
10
14
14
6
18
4
20

13
7
7
1
20
20
20
20
15
14
7
20
17
13
14
9
20
20
17
7
11
16
13
9
18
4
20

282 345 316 345 372

C ), and hCFF , with differTable 1: Coverage results with x = 2, for hCFF , hFF (C ), hFF (ce
nc
C
Cadd
ent best-supporter functions (h vs. h
) and tie-breaking strategies. Best results highlighted in boldface. Abbreviations: arb arbitrary tie-breaking; rnd
random tie-breaking (per-instance median seed, see also Table 2); dif difficulty
tie-breaking.

performs even the more competitive, non-standard hFF variant with random tie-breaking.
For hFF (C ), the same is true with small x values. Recall, however, that we report only
the performance of search, not that of C-learning: the data here evaluates exclusively the
merits of the respective heuristic functions, not the overhead required to obtain them in the
first place. We will stick to this rationale throughout, as the differences between explicitconjunction heuristics are our contribution here. For completeness, Appendix B shows
coverage plots counting C-learning as part of the solving effort, i. e., with a 30-minute limit
on the time taken by C-learning and search together.
Given the typically detrimental effect of large x, in what follows we first (Section 5.3)
explore the case x = 2, examining in detail the space of heuristic functions and best supporters. Subsequently (Section 5.4), we examine in more detail what happens as we scale x.
To make the latter experiments feasible, we will fix for each heuristic function the most performant best-supporter method; as it will turn out, for scaling x, this is the same method
for every heuristic, namely hCadd with random tie-breaking.
301

fiF ICKERT & H OFFMANN & S TEINMETZ

5.3 Small C: Heuristics, Best Supporters, and Tie Breaking for x = 2
We examine first the performance of the main heuristic functions, i. e., hCFF vs. the comC ), as well as hCFF which can essentially be
peting previous variants hFF (C ) and hFF (ce
nc
C ). We will discuss
perceived as an alternative, no-compilation, implementation of hFF (ce
CFF
CFF
CFF
the behavior of h|G0 |=1 and h|G0 |=1A , relative to h , below. Consider Table 1.
The most striking observation in this data is that the differences between heuristic functions
are dominated by those between tie-breaking strategies. As a function of tie-breaking, the range
C ), and 282
of overall coverage is 282368 for hCFF , 285351 for hFF (C ), 269356 for hFF (ce
CFF . This relatively small role of heuristic function differences, for x = 2, makes
372 for hnc
CFF  different scaling in C, cross-context conditions,
sense as the advantages of hCFF and hnc
non-compilation implementation  naturally have more impact the larger C is. There are
cases though where even small C makes a difference.
Comparing tie-breaking strategies, hCadd best supporters are superior to hC best supporters, typically per domain and almost consistently in the total. This makes sense in
that all heuristics here run the risk of over-estimation, and hCadd is better than hC at finding cheap relaxed plans. There are several cases where some combination of heuristic and
CFF with hC and difficulty tietie-breaking method works exceptionally well, e. g. hCFF /hnc
CFF with hC and arbitrary tie-breaking in Parcprinter11,
breaking in ChildSnack14, hCFF /hnc
FF
C
FF
C
add
h ( )/h (ce ) with h
and arbitrary tie-breaking in Barman11. As these performance peaks are not consistent across tie-breaking methods for the respective heuristics,
we consider them to be outliers caused by the brittleness of search.
Comparing heuristic functions h vs. h0 , a way of identifying strong advantages is to
consider those domains in Table 1 where h has a consistent advantage over h0 , i. e., h is at
least as good as h0 for all tie-breaking methods, and is strictly better for at least one method.
Call such an advantage strict if h is strictly better for all tie-breaking methods. In the comparison hCFF vs. hFF (C ), hCFF has a consistent advantage in 5 domains (ChildSnack14,
Elevators11, Openstacks14, Tetris14, and Transport14), while hFF (C ) has a consistent
advantage in 2 domains (Sokoban11 and Visitall14). The advantage is strict only for hCFF
in Elevators: in all other cases, some tie-breaking methods work equally well for both
heuristics. Overall, despite the noise the data is (somewhat) in favor of hCFF .
C ) yields a similar picture, with 4 consistent (non-strict)
The comparison hCFF vs. hFF (ce
CFF
advantages for h
(ChildSnack14, Elevators11, Openstacks14, Sokoban11) vs. 1 conC ) (Tidybot11). It is illuminating to offset these
sistent (non-strict) advantage for hFF (ce
CFF
observations against the data for hnc : in every domain where hCFF has a consistent adC ), hCFF also has a consistent advantage over hFF ( C ), and the only
vantage over hFF (ce
nc
ce
CFF has a consistent disadvantage vs. hFF ( C ) is the same as for hCFF ,
domain where hnc
ce
C ) here are not
Tidybot11. Hence the reason for the differences between hCFF and hFF (ce
the cross-context conditions. Presumably, as cross-context conditions occur only in very
specific situations, with small C they just do not play a role.
CFF .
Further evidence towards this conclusion comes from the comparison hCFF vs. hnc
Actually, regarding cross-context conditions, that comparison is more informative than
C ): after all, hCFF and hCFF differ only in accounting respecthat between hCFF vs. hFF (ce
nc
tively not accounting for cross-context conditions. In terms of consistent advantages, the
CFF , with 8 consistent (non-strict) advantages for hCFF
comparison is clearly in favor of hnc
nc
302

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

hCFF
worst med best

domain

s1

s2

s3

Barman11
Barman14
CaveDiving14
ChildSnack14
Elevators11
Floortile11
Floortile14
GED14
Hiking14
Maintenance14
Nomystery11
Openstacks11
Openstacks14
Parcprinter11
Parking11
Parking14
Pegsol11
Scanalyzer11
Sokoban11
Tetris14
Thoughtful14
Tidybot11
Transport11
Transport14
Visitall11
Visitall14
Woodworking11

13
8
7
2
20
20
20
20
18
13
6
20
18
13
14
9
20
20
16
8
11
15
14
9
17
4
20

12
5
7
1
20
20
20
20
14
13
7
20
16
10
14
11
20
20
17
9
10
16
12
7
18
4
20

12
7
7
2
19
20
20
20
15
13
7
20
16
12
16
8
20
20
18
7
12
16
11
7
17
4
20

10
5
7
0
19
20
20
20
13
10
6
20
16
8
13
7
20
20
16
6
9
14
9
7
17
4
20

12
7
7
1
20
20
20
20
16
14
7
20
16
12
14
9
20
20
17
8
11
16
13
7
17
4
20

15
8
7
4
20
20
20
20
18
15
7
20
18
15
17
12
20
20
18
10
13
17
15
9
18
4
20

375 363 366

336

368

400



hFF (C )
worst med best

var



s1

s2

s3

1
3
0
1
1
0
0
0
4
0
1
0
2
3
2
3
0
0
2
2
2
1
3
2
1
0
0

+5
+5
0
1
1
0
0
0
+4
+4
1
+1
+3
2
+8
+7
0
0
1
1
+2
+2
2
2
+2
0
0

3
0
7
1
18
20
20
20
13
11
6
20
16
9
20
20
20
20
17
11
11
17
12
8
14
4
20

1
0
7
1
17
20
20
20
13
8
6
20
16
9
20
17
20
20
17
9
10
18
11
6
14
4
20

1
1
7
2
19
20
20
20
13
12
6
19
14
9
20
19
20
20
17
7
11
17
11
7
13
4
20

1
0
7
0
17
20
20
20
11
5
6
19
14
6
20
16
20
20
17
7
8
14
9
5
13
4
20

1
0
7
0
18
20
20
20
13
11
6
20
16
7
20
20
20
20
17
8
10
19
11
7
14
4
20

3
1
7
4
19
20
20
20
15
15
6
20
16
14
20
20
20
20
17
12
14
19
14
9
14
4
20

358 344 349

319

349

383

var



2 13
1 7
0
0
1 +2
2 2
0
0
0
0
0
0
0 2
4 3
0 6
1 1
2 +2
0 3
0 +9
3 +15
0
0
0 +1
0
0
4 +6
1 2
1 +4
1 +2
2 1
1 3
0
0
0
0

Table 2: Coverage results with x = 2, showing the effect of different random seeds
in random tie-breaking for hCadd best-supporters. s1, s2, s3 denote
the 3 random seeds (fixed throughout the experiment). The best, med,
and worst columns assess per-instance aggregation methods, selecting the
best/median/worst seed per instance respectively. var assesses the per-domain
performance variance, in terms of the difference between the best and worst coverage.  assesses the per-domain consistency of performance change relative
to the baseline, i. e., relative to hCadd best-supporters with arbitrary tie-breaking.
It shows the maximum absolute difference, with + if coverage is better for all
seeds,  if coverage is worse for all seeds, and  otherwise, i. e., if coverage
actually gets better or worse depending on the seed.
vs. 2 consistent (non-strict) advantages for hCFF . However, examining this more closely, all
these advantages are at a very small scale. Whereas, in the comparisons above, the average
coverage difference in consistent-advantage domains is typically between 2 and 3, in the
CFF it is usually 0.2 and its maximum is 0.8.
comparison between hCFF and hnc
What to conclude regarding our experimental hypotheses, (H1) advantage of hCFF over
C ) thanks to
thanks to better scaling in |C |, (H2) advantage of hCFF over hFF (ce
CFF over hFF ( C ) thanks to
cross-context conditions, and (H3) advantage of hCFF and hnc
ce
implementation? Table 1 provides evidence in favor of (H1) and (H3), though only in few
domains, and subject to substantial noise from tie-breaking. There is no support for (H2).
The evidence suggests that, for x = 2, taking cross-context conditions into account has
neither substantial positive effects nor substantial negative effects.
hFF (C )

303

fiF ICKERT & H OFFMANN & S TEINMETZ

Some words are in order regarding our use of random tie-breaking. The crucial observations are that, per domain, (a) the variance over random seeds is typically small, while
(b) the performance change relative to the baseline typically is consistent. This makes random tie-breaking comparable to other non-randomized algorithm options. That said, in
some domains either (a) or (b) are false, and in the total the differences would sum up.
We counteract this with a per-instance aggregation method to obtain per-instance data that
reduces variance relative to the individual seeds, and that interpolates between the seeds
in terms of overall performance. The per-instance median seed, of the 3 random seeds ran in
our experiments, turns out to be suitable (for coverage, this counts an instance as solved
if at least 2 of the 3 randomized runs solved it). We used the per-instance median seed
in Figure 1 and Table 1, and will use it below in all cases where random tie-breaking is
employed. Table 2 shows the data supporting these observations and design decisions.
A quick look at the var columns in Table 2 confirms observation (a). The difference
between the best and the worst per-seed coverage is  2 except in 5 domains for hCFF and
in 3 domains for hFF (C ). On the other hand, looking at the bottom row and comparing
the seeds, the differences do add up. This would be especially so if we were to select
the best or worst seed per instance (best and worst columns), resulting in coverage
differences of around 70 in the total. However, using the median (med column) seed
results in a per-instance aggregation with the desired properties.
Regarding observation (b), consider the  columns in Table 2. Those domains where
performance relative to the baseline is not consistent, i. e., gets better or worse depending
on the random seed, are marked with a  symbol. These symbols are sparse in the
table. In all but 4 of the 27 domains for hCFF , and in all but 2 of them for hFF (C ), the
randomization changes performance consistently. (It rarely deteriorates performance for
hCFF , while for hFF (C ) the picture is more mixed depending on the domain.) This shows
clearly that random tie-breaking is reliable against the baseline.
Indeed, these findings contradict Keyder et al.s (2014) use of random tie-breaking as a
measure of noise. Keyder et al.s idea was to account for the brittleness of search by randomizing the baseline heuristic h (in their case, hFF with hadd best supporters and arbitrary
tie-breaking), measuring  as in Table 2 yet ignoring the distinctions +, , , i. e.,
considering only the absolute maximum difference. They deem a heuristic h0 to be significantly better than h only if its improvement over h is larger than   intuitively, larger than
the random noise. However, this approach assumes that random tie-breaking yields a distribution around the baseline average, which is very much not so in our data. Consider for
example hFF (C ) in Barman11. According to Keyder et al.s method, the random noise
here is 13, and for any other heuristic to be significantly better than hFF (C ) it must hence
increase coverage by at least 14. But the noise is just the effect of random tie-breaking
being consistently detrimental. Similar examples abound.
We conclude that, in the specific context of our experiments, Keyder et al.s measure is
not appropriate because random tie-breaking is typically not a source of noise. To the contrary, the performance of 3 separate runs of random tie-breaking can be reliably reported
like that of a single planner run, through per-instance median seed aggregation.
Let us finally consider the behavior of hCFF relative to hCFF
which trivializes the sub| G 0 |=1
goal support selection, and relative to hCFF
which also trivializes the C-relaxed plan
| G 0 |=1A
(into a set of actions instead of action occurrences). Table 3 shows the data.
304

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

hCFF
hC
domain
Barman11
Barman14
CaveDiving14
ChildSnack14
Elevators11
Floortile11
Floortile14
GED14
Hiking14
Maintenance14
Nomystery11
Openstacks11
Openstacks14
Parcprinter11
Parking11
Parking14
Pegsol11
Scanalyzer11
Sokoban11
Tetris14
Thoughtful14
Tidybot11
Transport11
Transport14
Visitall11
Visitall14
Woodworking11


hCadd

arb rnd dif arb rnd
3
1
7
2
11
20
20
20
15
10
7
19
17
16
5
1
20
17
18
7
13
11
0
0
2
0
20

8
3
7
14
18
20
20
20
14
10
10
19
15
13
8
1
20
20
17
8
9
13
6
2
3
0
20

2
0
7
0
19
20
20
20
17
13
6
19
17
10
20
17
20
18
16
7
13
15
2
0
18
4
20

8
3
7
2
20
20
20
20
14
9
7
19
15
12
8
4
20
20
17
8
10
14
12
9
16
4
20

12
7
7
1
20
20
20
20
16
14
7
20
16
12
14
9
20
20
17
8
11
16
13
7
17
4
20

282 318 340 338 368

hC

hCFF
| G 0 |=1

hCadd

arb rnd dif arb rnd
2
2
7
2
10
20
20
20
10
4
7
19
17
13
2
1
20
19
16
7
9
10
0
0
12
4
20

3
1
7
10
17
20
20
19
10
5
6
19
17
11
2
0
20
19
17
4
14
11
4
1
11
3
20

0
0
7
1
18
20
20
20
14
4
5
20
20
3
14
3
20
20
17
14
10
12
2
0
20
11
20

13
6
7
1
20
20
20
20
14
6
6
20
16
8
2
0
20
20
17
4
13
14
13
8
20
7
20

13
4
7
2
20
20
20
20
13
5
6
19
15
7
2
0
20
20
16
11
14
16
13
7
20
5
20

273 291 315 335 335

hCFF
| G 0 |=1A

hC

hCadd
arb rnd dif arb rnd
0
1
7
2
8
20
20
20
12
7
6
20
16
16
4
2
20
17
16
8
9
10
1
0
1
0
20

3
1
7
10
12
20
20
20
11
10
9
19
17
11
3
0
20
20
17
10
13
12
3
2
1
0
20

0
0
7
0
19
20
20
20
15
10
5
20
16
5
20
14
20
19
17
11
9
16
1
0
20
9
20

6
1
7
2
20
20
20
20
15
10
8
19
16
7
15
6
20
20
17
8
14
13
11
9
20
8
20

1
1
7
0
20
20
20
20
14
10
7
20
16
8
18
13
20
20
16
10
12
15
9
5
19
5
20

263 291 333 352 346

Table 3: Coverage results with x = 2, for hCFF vs. hCFF
and hCFF
, with different best
| G 0 |=1A
| G 0 |=1
supporter functions and tie-breaking strategies. Best results highlighted in boldface. Abbreviations as in Table 1.
Like in Table 1, there is a lot of noise due to tie-breaking strategies. Note though that
all heuristics shown here work on the same representation and are based on the same
implementation. The differences are only in the subgoal support selection/relaxed plan
definition. Comparisons of tie-breaking strategies across heuristics hence are direct.
The data shows a clear advantage of hCFF over hCFF
. For every tie-breaking strategy,
| G 0 |=1
hCFF is strictly better in the total (the margin varying from 3 for hCadd with arbitrary tiebreaking to 33 for hCadd with random tie-breaking). Using the per-domain comparison
across tie-breaking strategies, hCFF has a consistent advantage in 10 domains (3 of which
are strict), and a consistent disadvantage only in 2 domains (both strict, namely the two
Visitall variants). Comparing search space size and states per second on commonly solved
instances, the advantages of hCFF are partly due to quality (e. g. Parking11 994.4 vs. 8, 091.1
geometric mean of state evaluations), supporting our hypothesis (H4) that the non-trivial
subgoal support selection in hCFF yields a more informed heuristic than hCFF
. There
| G 0 |=1
also are several cases where the advantage is in speed (e. g. Elevators11 4713.0 vs. 4591.1
states per second). The only possible cause for this lies in the different states evaluated:
those for hCFF are easier to evaluate, which typically is the case for states closer to the goal.
(We remark that, like for the standard relaxed plan heuristic, most of the heuristic function
305

fiF ICKERT & H OFFMANN & S TEINMETZ

runtime, typically 90% or more, is spent on the computation of the best-supporter function,
hC respectively hCadd in our case.)
Relative to hCFF
, hCFF is still in the advantage but the picture is more mixed. In
| G 0 |=1A
terms of total coverage, hCFF is dominant except for hCadd with arbitrary tie-breaking. Per
domain, hCFF has a consistent advantage in 6 cases (1 strict), vs. a consistent disadvantage
in 4 cases (none strict). It appears that, at least for this setting of x and the IPC benchmarks, relative to hCFF
which is prone to over-estimation, the trivialized C-relaxed plan
| G 0 |=1
in hCFF
does result in a better heuristic. Comparing hCFF
vs. hCFF
directly, hCFF
| G 0 |=1A
| G 0 |=1A
| G 0 |=1
| G 0 |=1A
dominates in the total except for hC with arbitrary tie-breaking, has a consistent advantage in 5 domains (3 strict), and a consistent disadvantage in 3 domains (none strict). In
terms of search space size and states per second on commonly solved instances, the advantages of hCFF
are mostly due to quality (most notably in Parking11, 1102.7 vs. 8091.1
| G 0 |=1A
state evaluations), except in Hiking14 where hCFF
is faster (526.7 vs. 639.8 states per
| G 0 |=1A
second).
5.4 Scaling C: Performance as a Function of x
We now examine search behavior as C becomes larger. This is naturally presented in terms
of plots of performance measures as a function of the size limit x. Keep in mind, especially
in the comparison to the hFF baselines, that the C-learning is conducted separately from
the search, with a separate 30-minute time limit. Appendix B shows the same coverage
plots included in the below, but when counting C-learning as part of the solving effort. In
the following discussions, we include brief summaries of this data.
Figure 2 shows total coverage as a function of x. Consider first Figure 2 (b) which settles the question about the most competitive tie-breaking strategies. As we have seen in
Tables 1 and 2, total coverage at x = 2 is maximal using hadd with random tie-breaking,
for almost all heuristic functions. The two exceptions are hFF (C ) and hCFF
. For both
| G 0 |=1A
of these, hadd with arbitrary tie-breaking works better than hadd with random tie-breaking
at x = 2 (2 more instances solved for hFF (C ), 6 more instances solved for hCFF
). How| G 0 |=1A
ever, as Figure 2 (b) shows, the advantage of arbitrary tie-breaking at x = 2 turns into a
substantial disadvantage for larger values of x. Hence, for the remainder of the experiments, we fix hadd with random tie-breaking as the best-supporter method throughout.
Consider now Figure 2 (a). As previously hinted (Figure 1 (c)), total coverage tends
to decrease as a function of x. All heuristics consistently outperform both hFF baselines
though, except for hFF (C ) whose per-state runtime overhead drags coverage below that
of hFF once x  32 (and except for a temporary dip of hCFF
below hFF with random
| G 0 |=1
tie-breaking at x = 16).
CFF , hFF ( C ), and hFF ( C ) decrease, relatively speaking, more
Note that all of hCFF , hnc
ce
5
steeply up to x = 2 , and less steeply afterwards. This is because, around this point, in
many domains the C-learning reaches the time limit before the size limit. While hFF (C )
in the remaining domains still becomes substantially worse, for the other heuristics this is
less pronounced. Observe here that the coverage difference between x = 2 and x = 210 ,
except for hFF (C ), is small, around 20 instances. Indeed, most of this decrease is caused
by a few domains only, namely Barman, Parking, Sokoban, and Visitall.
306

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

370

370

360

360

350

350

340

340

330

330

320

320

310

310

300
290
280
270
260
250
240
21

300

CFF
hnc
hCFF

290

C)
hFF (ce
FF
h (C )
hCFF
| G 0 |=1A
hCFF
| G 0 |=1
hFF (arb)
hFF (rnd)

22

280
270
260
250
240

23
24
add

(a) h

25

26

27

28

29

210

21

hFF (C ) (rnd)
hFF (C ) (arb)
hCFF
(rnd)
| G 0 |=1A
hCFF
(arb)
0
| G |=1A
22

23

24

25

26

27

28

29

210

(b) hadd , random vs. arbitrary tie-breaking

, random tie-breaking

Figure 2: Total coverage as a function of the size bound x. (a) All heuristics using hadd with
random tie-breaking (median per-instance seed; heuristic functions listed topdown by order of their coverage for x = 2). (b) hadd with random vs. arbitrary
tie-breaking for hFF (C ) and hCFF
, the only two heuristics where arbitrary
| G 0 |=1A
tie-breaking yields higher total coverage for x = 2. In (a), for comparison we
include hFF as a baseline, using hadd with arbitrary tie-breaking (FD default), and
using hadd with random tie-breaking. Recall in this comparison that the effort for
C-learning is not included in the explicit-conjunction heuristics.
CFF , and hFF ( C ) are fairly close to each other, and follow a simThe curves of hCFF , hnc
ce
CFF
C ), and hCFF is consistently better than
ilar pattern. h
is consistently better than hFF (ce
nc
FF
C
9
6
CFF ,
h (ce ) except for x = 2 . For x  2 , there is a consistent advantage for hCFF over hnc
indicating a beneficial impact of cross-context conditions.
Regarding hCFF
and hCFF
, the latter is consistently much better than the former.
| G 0 |=1
| G 0 |=1A

hCFF
achieves very competitive performance for x  25 . Both heuristics exhibit no
| G 0 |=1A
clear trend over x. The latter is not due to a difference in heuristic function speed (as we
shall see below, the speed of hCFF
and hCFF
is similar to that of hCFF , across x values).
| G 0 |=1
| G 0 |=1A
Rather, it is caused by particular behaviors in the few domains causing the coverage decline tendency in Figure 2 (a). Compared to the other heuristics, hCFF
and hCFF
scale
| G 0 |=1
| G 0 |=1A
better over x in Barman (only hCFF
) and Visitall (both hCFF
and hCFF
). There also
| G 0 |=1A
| G 0 |=1
| G 0 |=1A
are some cases, e. g. Barman for hCFF
and Parking for both hCFF
and hCFF
, where
| G 0 |=1
| G 0 |=1
| G 0 |=1A
these heuristics are bad from begin with, not solving in the first place those instances lost
by other heuristics for larger x, and hence suffering less from large x.
In most domains other than Barman, Parking, Sokoban, and Visitall, the only heuristic
suffering from large x is hFF (C ), if any heuristic suffers at all. On the other heuristics,
growing x has only a marginally negative effect, an inconclusive effect, or no effect at all.
There also are 4 domains where most heuristics tend to improve in coverage as x grows.
Figure 3 shows the data for these. The coverage growth over x is most consistent across
307

fiF ICKERT & H OFFMANN & S TEINMETZ

20

20

15

15

10

10

5

5

01
2

22

23

24

25

26

27

28

29

01
2

210

22

23

(a) Maintenance
20

15

15

10

10

5

5

22

23

24

25

26

27

25

26

27

28

29

210

28

29

210

(b) Parcprinter

20

01
2

24

28

29

01
2

210

(c) Tetris

22

23

24

25

26

27

(d) Thoughtful

Figure 3: Coverage in individual domains. All explicit-conjunction heuristics use hadd
with random tie-breaking. Recall in the comparison to the hFF baselines that
the effort for C-learning is not included in the explicit-conjunction heuristics.
heuristics in Parcprinter. In the other domains, the picture is more mixed, with a lot of
CFF
variance in Maintenance and Thoughtful, and with mainly hCFF
, hCFF
, hCFF , and hnc
| G 0 |=1 | G 0 |=1A
profiting from large x in Tetris. (The curves remain flat in Tetris for x  27 because then
the C-learning time-limit applies and no more new conjunctions are added.)
How does this picture change when imposing a 30-minute limit on the time taken by Clearning and search together? Naturally, the tendency of coverage to decline over growing
x becomes stronger, yet the relative performance of explicit-conjunction heuristics remains
very similar.
For total coverage, shown in Figure 7 (page 323), performance is substantially worse
than for search-only already at x = 2 (by 20-30 instances), and declines more steeply over
x for all heuristics. For the relative performance of heuristics, however, our conclusions
CFF beat
remain exactly the same as above. With respect to the baselines, only hCFF and hnc
FF
the non-default h (random tie-breaking), and only at x = 2. The inferior default hFF
308

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

baseline is beat by all explicit-conjunction heuristics up to medium-large x values (x = 16
or x = 32), except for hFF (C ) which is worse for x  8, and for hCFF
which marginally
| G 0 |=1
beats default hFF only at x = 2.
For the individual domains in Figure 3, the questions are whether (a) x > 2 still improves coverage when including the effort for C-learning, and whether (b) the explicitconjunction heuristics can still beat the hFF baselines. As Figures 8 and 9 (pages 324
and 325) show, the answer to both (a) and (b) is yes. In Maintenance and Thoughtful, not much changes with respect to Figure 3. In Parcprinter and Tetris, very large values
of x are detrimental, but moderate ones arent. Coverage increases up to a certain point,
namely x = 23 in Parcprinter and x = 24 in Tetris, then decreases after that point.
Let us get back to our hypotheses (H1)(H4). Figure 2 (a) confirms (H1) that hCFF typically has an advantage over hFF (C ); somewhat supports that (H2) the cross-context conC ) (and, more directly, over hCFF ); confirms
ditions in hCFF yield an advantage over hFF (ce
nc
CFF
CFF
C ); and (H4) confirms that hCFF has
and hnc have an advantage over hFF (ce
that (H3) h
an advantage over hCFF
. To examine the reason for these advantages, and thus evaluate
| G 0 |=1
the specific claim of each hypothesis, we now consider more fine-granular performance
measures, namely search space size (number of state evaluations), states per second, and
search runtime, on commonly solved instances.
The specific claim of hypothesis (H1) is that, thanks to avoiding the exponential blowup in |C |, hCFF is typically faster than hFF (C ) and thus improves performance. Figure 4
confirms this. The top row of plots shows the main data (the data for overall performance).
As we see in the top left plot, in terms of quality the two heuristics are similar. In terms
of speed, hFF (C ) suffers with growing x, in the overall and consistently in individual
domains, to the effect that runtime, like coverage discussed above, suffers as well. That is
much less so for hCFF , leading to a dramatic performance advantage for large x.11
On the other hand, as hCFF also suffers itself from large x, though less than hFF (C ),
its advantage over hFF (C ) in the overall is mute. Most relevant are the domains where,
thanks to its speed advantage, hCFF benefits from growing x, and hence improves over the
best performance obtainable with hFF (C ) for any value of x. For coverage, this happens
in Maintenance and Tetris (both, with and without including C-learning, cf. Figures 3, 8,
and 9). For search runtime on commonly solved instances, it happens in Hiking, Pegsol,
Tetris, and Thoughtful. Figure 4 showcases Hiking and Tetris, which we will also use as
showcases below as they nicely illustrate most of our main points.
CFF , and
We next compare three heuristic functions with each other, namely hCFF , hnc
FF
C
h (ce ). This serves to examine hypotheses (H2) and (H3). The specific claim of the
former asserts that, thanks to accounting for cross-context conditions, hCFF can be more
C ). The latter asserts that the implementation of hCFF and hCFF is typiinformed than hFF (ce
nc
C ). It is of advantage to compare all three heuristics together
cally faster than that of hFF (ce
as, to evaluate the importance of cross-context conditions, the comparison between hCFF
CFF is more direct. Figure 5 shows the data.
and hnc
CFF are faster
Hypothesis (H3) is confirmed very consistently, at a small scale. hCFF and hnc
FF
C
than h (ce ) across all x values in the overall, with a small advantage that grows in x.
11. We remark that, for x = 1 where hCFF and hFF (C ) both are variants of hFF , there are hardly any speed
differences, neither between hCFF and hFF (C ) nor compared to FDs standard implementation of hFF .

309

fiF ICKERT & H OFFMANN & S TEINMETZ

105

104

hCFF
hCFF (*)
hFF (C )
hFF (C ) (*)

103

102
21

22

23

24

25

26

27

28

29

210

103

103

102

102

101

101

100 1
2

22

23

24

25

26

27

28

29

210

100 1
2

22

23

24

25

26

27

28

29

210

28

29

Overall: search space size, states per second, search runtime
105

103

103

104

102

102

103

101

101

102
21

22

23

24

25

26

27

28

29

100 1
2

22

23

24

25

26

27

28

29

100 1
2

22

23

24

25

26

27

Hiking: search space size, states per second, search runtime
105

103

103

104

102

102

103

101

101

102
21

22

23

24

25

26

27

100 1
2

22

23

24

25

26

27

100 1
2

22

23

24

25

26

27

Tetris: search space size, states per second, search runtime
Figure 4: Data for hCFF vs. hFF (C ). Geometric means. All curves use only those instances
solved for all values of x, for the curves with a (*) solved by both heuristics (174
instances overall), for those without (*) solved by the respective heuristic.
Essentially the same behavior occurs in every individual domain, with a single exception,
C ) is consistently faster. Hiking and Tetris in Figure 5 are two
namely Tidybot where hFF (ce
CFF consistently (across all or almost all
typical examples. In terms of coverage, hCFF and hnc
FF
C
values of x) dominate h (ce ) in Barman, Elevators, Hiking, Maintenance, Sokoban, and
Visitall; the opposite happens only in Parking and Tidybot.
Regarding (H2), as the top left plot in Figure 5 shows, all three heuristics yield similar
search space sizes overall. There are no domains where hCFF consistently, across all values
CFF . However, there are domains where hCFF has
of x, yields smaller search spaces than hnc
a notable advantage for large values of x. This is mainly so for Hiking and Tetris, shown
in Figure 5. In Tetris, the advantage is basically consistent beyond x = 24 . Hiking behaves
similarly except for a degradation at the largest two x values. In both domains, hCFF also
CFF . Overall, the support for hypothesis
has corresponding coverage advantages over hnc
310

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

104

103

103

102

105

104
hCFF
hCFF
nc

103 1
2

hFF (C
ce )
22

23

24

25

26

27

28

29

210

102 1
2

22

23

24

25

26

27

28

29

210

101 1
2

22

23

24

25

26

27

28

29

210

26

27

28

29

210

26

27

28

29

210

Overall: search space size, states per second, search runtime
10

103

103

102

102

5

104

103 1
2

22

23

24

25

26

27

28

29

210

101 1
2

22

23

24

25

26

27

28

29

210

101 1
2

22

23

24

25

Hiking: search space size, states per second, search runtime
103

103

102

102

105

104

103 1
2

22

23

24

25

26

27

28

29

210

101 1
2

22

23

24

25

26

27

28

29

210

101 1
2

22

23

24

25

Tetris: search space size, states per second, search runtime
CFF vs. hFF ( C ). Geometric means. All curves use only those
Figure 5: Data for hCFF vs. hnc
ce
instances solved for all values of x by all heuristics (225 instances overall). Note
that, for better readability, the y-scales show only 2 orders of magnitude (in difference to Figure 4).

(H2) is weak, but the data does give evidence that cross-context conditions can, in some
cases, be of advantage.
In this context, it is worth coming back briefly to the discussion of Table 1, for x =
CFF was somewhat in favor of hCFF (many
2, where the comparison between hCFF and hnc
nc
per-domain advantages, but at a small scale). For growing x, the picture becomes more
CFF consistently
favorable for hCFF , though still at a small scale. There is no domain where hnc
CFF
is faster, or of higher quality, or yields better coverage, than h . On the other hand,
hCFF is consistently faster in Barman, GED, and Openstacks, plus the favorable behavior in
Hiking and Tetris as shown in Figure 5.
311

fiF ICKERT & H OFFMANN & S TEINMETZ

103

105

103

hCFF
hCFF
0

|G =1|

hCFF
|G0 =1|A
104

102

102

103
21

101
22

23

24

25

26

27

28

29

210

10

1

21

22

23

24

25

26

27

28

29

210

21

22

23

24

25

26

27

28

29

210

26

27

28

29

210

26

27

28

29

210

Overall: search space size, states per second, search runtime
105

103

103

104

102

102

103
21

101
22

23

24

25

26

27

28

29

210

10

1

21

22

23

24

25

26

27

28

29

210

21

22

23

24

25

Hiking: search space size, states per second, search runtime
10

103

5

103

104

102

102

103
21

101
22

23

24

25

26

27

28

29

210

10

1

21

22

23

24

25

26

27

28

29

210

21

22

23

24

25

Tetris: search space size, states per second, search runtime
Figure 6: Data for hCFF vs. hCFF
vs. hCFF
. Geometric means. All curves use only those
| G 0 |=1
| G 0 |=1A
instances solved for all values of x by all heuristics (208 instances overall). Note
that, for better readability, the y-scales show only 2 orders of magnitude (in difference to Figure 4).
Let us finally consider hypothesis (H4), which asserts that, thanks to its non-trivial
subgoal support selection, hCFF typically yields a more informed heuristic than hCFF
. We
| G 0 |=1
into the comparison for completeness. Figure 6 shows the data.
include hCFF
| G 0 |=1A
All three heuristics perform very similarly in the overall. This is partly due to particularities of the common instance basis. Several domains are not at all, or hardly, contained
in the instance basis of Figure 6. This pertains in particular to Barman, Maintenance, Parcprinter, and Parking, where hCFF has large coverage advantages over hCFF
.
| G 0 |=1
Nevertheless, Figure 6 allows to confirm (H4), albeit at a small scale. In the overall,
consistently across values of x, hCFF has a slightly smaller search space than hCFF
. (It
| G 0 |=1
also is slightly faster than hCFF
, and consequently results in slightly better runtime.) Per
| G 0 |=1
312

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

domain, hCFF has search advantages in 9 cases, and disadvantages in only 3 cases. Figure 6 showcases Hiking and Tetris, which respectively represent these domain classes, and
where the heuristics benefit from growing x.
In most domains, like in the overall hCFF has a slight speed advantage over hCFF
. As
| G 0 |=1
we already observed in our discussion of Table 3, these must be caused by the different
states evaluated, with hCFF evaluating more states close to the goal and thus being faster.
In terms of coverage, hCFF clearly dominates hCFF
in the overall (Figure 2), and has
| G 0 |=1
strong advantages in 8 domains (e. g. Maintenance and Parcprinter, cf. Figure 3), while
hCFF
has advantages only in 3 domains (Tetris cf. Figure 3, Thoughtful, and Visitall).
| G 0 |=1
Let us finally compare hCFF
with hCFF
. Their overall coverage difference is clearly
| G 0 |=1
| G 0 |=1A
in favor of hCFF
. Per domain, hCFF
has a coverage advantage in 5 domains and a
| G 0 |=1A
| G 0 |=1A
disadvantage in 9, yet the disadvantages are typically marginal whereas the advantages
are substantial. Regarding speed and search space size on commonly solved instances,
speed is very similar almost universally. Search space size also is often very similar for both
(in the overall, hCFF
and hCFF
are almost indistinguishable). There are exceptions in
| G 0 |=1
| G 0 |=1A
individual domains, specifically Elevators, Pegsol, and Visitall where hCFF
is better, and
| G 0 |=1
GED, Hiking, and Parking where hCFF
is better.
| G 0 |=1A
Summing up our observations, the data confirms (H1) impressively, with the caveat
that there are only few IPC domains where large C is beneficial. (H3) and (H4) are confirmed consistently, in the overall and across most domains. The evidence for (H2) is
weaker, with good cases only in Hiking and Tetris. This is not entirely unexpected given
that cross-context conditions occur only in specific situations, important in theory but, evidently, rare in practice as far as reflected by the IPC benchmarks.

6. Contribution Summary and Future Work
Our work contributes a new understanding of recent compilation-based partial delete relaxation heuristics, in terms of a combination of the delete relaxation with critical-path
heuristics. The key insight is to view each of these a priori unrelated relaxations as being
defined through an underlying set of atomic subgoals, where the relaxation consists in decomposing non-atomic conjunctive goals into their atomic subgoals. Critical-path heuristics require to achieve only the most costly atomic subgoal, the delete relaxation requires
to achieve all atomic subgoals. The standard delete-relaxation framework now becomes
the special case where the atomic subgoals are singleton facts, and the entire standard machinery  h+ , relaxed plan existence testing based on h1 , the additive heuristic hadd , relaxed
plan extraction from a best-supporter function  extends naturally along the dimension of
allowing arbitrary atomic subgoals C instead.
Our direct characterization identifies the precise new source of complexity in the relaxed plan extraction process, namely selecting the subset of atomic subgoals to support
with a given action. Thanks to this, we design new C-relaxed plan heuristics, hCFF and
CFF , avoiding the shortcomings of previous compilation-based heuristics. The theoretical
hnc
advantages of hCFF are reflected empirically in IPC benchmarks. The improvement over
the state of the art, overall, is marginal though, and relates more to the new heuristics
implementation advantages than to their theoretical ones.
313

fiF ICKERT & H OFFMANN & S TEINMETZ

In our view, the main value of this work lies in understanding what the compilation
heuristics actually do, spelling out the framework of C-delete relaxation, and replacing
C ) with the more direct and natural hCFF respectively hCFF . While the
hFF (C ) and hFF (ce
nc
new heuristics may not yield dramatic benefits in most cases, they are certainly more reliable and somewhat more efficient than their predecessors, and there is no reason not to
use them. A nice side benefit is the simple yet useful generalization from hm to hC .
We believe that there are still many exciting avenues of future research in this area. We
expect that our results will help with many of them, through the more efficient and direct
implementation, or through the alternate and less opaque formulation.
An obvious topic is to use backward search instead of forward search, paralleling the
design of HSP-r (Bonet & Geffner, 1999) where we need to compute hC only once on the
initial state. Alcazar et al. (2013) already took this direction, but didnt explore it in detail.
There is still a glaring hole in the understanding of C-relaxation heuristics, namely the
role of the conjunction set C. What are good sets C? How to find them? The literature so
far offers preliminary answers to the second question, and offers no answer at all to the
first one. In particular, the proof of convergence is via hm : if we select m large enough
then hm = h , and if we simulate m via C then hC = hm , and hC  hC+ so we can get
hC+ = h QED. But this completely ignores that (a) we are free to choose any set C, not just
the size-m conjunctions, and (b) while hC is a lower bound on hC+ , it is a trivial one and
hC+ typically is much higher (similarly as for the well-known relation between h1 and h+ ).
So how many/which conjunctions are actually needed to render hC+ perfect?
Preliminary results have been obtained with a bottom-up approach trying to identify
planning fragments where small sets C suffice to obtain hC+ = h (Hoffmann, Steinmetz, &
Haslum, 2014). This approach has proved to be exceedingly difficult though, with complex
case considerations already in trivial fragments. Can we instead explore this top-down,
identifying conjunctions not needed to render hC+ perfect, and thus guide the C-learning
mechanisms? In which IPC benchmarks does it suffice to use all fact pairs, and how does
h+ topology (Hoffmann, 2005) change in the other ones? Can we learn something from
that about how particular planning sub-structures should be handled?
A more practical approach is the design of alternate C-learning methods. In particular,
can we learn C during search? Learning from mistakes as has proved extremely successful in constraint-satisfaction problems like SAT (e. g. Marques-Silva & Sakallah, 1999;
Moskewicz, Madigan, Zhao, Zhang, & Malik, 2001)? Recent work (Steinmetz & Hoffmann,
2016) has devised an approach doing so for dead-end detection, refining hC on dead-ends
as they become known during the search (i. e., once the search has explored all their descendants). In a depth-first search, this algorithm approaches the elegance of clause learning in SAT, learning generalizing knowledge from refuted search subtrees.
But what about search guidance on non-dead-end states? Can we usefully refine hCFF
during search? A difficulty is that, whenever C was increased, to re-adjust the relative
ordering of states in principle we would need to re-evaluate hCFF on the entire open list.
An interesting option is local search: use hill-climbing until a local minimum is reached,
then refine C to eliminate that local minimum from hCFF s search surface. In other words:
if caught in a local minimum, rather than giving up on the heuristic and relying on search
instead  as is commonly done across AI sub-areas  refine the heuristic to exhibit the exit.
314

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

In summary, we are now satisfied with the understanding of C-relaxation heuristics,
and we believe that the key to fully exploiting their power lies in a better understanding
and design of methods finding the atomic subgoals C.

Acknowledgments
This work was partially supported by the German Research Foundation (DFG), under
grant HO 2169/5-1. We thank the anonymous reviewers, whose comments helped to improve the paper.

Appendix A. Proofs
Theorem 1 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing
all singleton conjunctions. Then h+ (C ) = hC+ ().
Proof: Denote  = (F , A, I , G). Our proof is via comparing two equations. Equation I
simply is Equation 7 (page 280, the hC+ equation), characterizing hC+ (). We derive Equation II by applying Equation 5 (page 279, our non-standard characterization of h+ ) to C ,
characterizing h+ (C ).
Repeating Equation 7, for convenience: hC+ () = h(G C ), where h is the function on
conjunction sets G that satisfies h( G ) =
(
0
c  G : c  I
C
0
0
1 + minaA,6=G0 {c|cG,R(c,a)6=} h(( G \ G )  Gr ) else
with Gr0 defined as Gr0 := cG0 R(c, a).
Reconsider now Equation 5, which can be written as: h+ () = h(G), where h is the
function on fact sets G that satisfies h( G ) =

0
GI
1 + minaA,6=G0 ={ p| pG,R({ p},a)6=} h(( G \ G 0 )  Gr0 ) else
S

with Gr0 defined as Gr0 := pG0 R({ p}, a).
We now apply the previous equation to C . Making explicit that the individual facts
in C all are -fluents, we obtain: h+ (C ) = h({c | c  G C }), where h is the function
on fact sets G that satisfies h( G ) =
S



0
c  G : c  I C
0
0
1 + mina[C0 ]AC ,6=G0 ={c |c G,R({c },a[C0 ])6=} h(( G \ G )  Gr ) else

with Gr0 defined as Gr0 := c G0 R({c }, a[C 0 ]).
One can already see the correspondence here to Equation I, with conjunctions c corresponding to -fluents c . The only major difference is the set of action/supportedsubgoal-set pairs minimized over in the bottom cases.
Consider the set G 0 = {c | c  G, R({c }, a[C 0 ]) 6= } of supported atomic subgoals
as per the last equation. The condition R({c }, a[C 0 ]) 6=  simplifies to c  C 0 , because
S

315

fiF ICKERT & H OFFMANN & S TEINMETZ

these are exactly the -fluents added by a[C 0 ]. Thus, removing the G 0 variable which is
fixed anyway, the equation simplifies to: h( G ) =


0
c  G : c  I C
0
0
1 + mina[C0 ]AC ,6={c |c G,cC0 } h(( G \ G )  Gr ) else

Now what we minimize over are the actions a[C 0 ] in C , where C 0 needs to support a
non-empty subset of subgoal -fluents/conjunctions c . What are the possible choices of
C 0 ? The c we can in principle include into C 0 , i. e. the subgoals that we can in principle
support using the action a are, by the definition of C , exactly those where R(c, a) 6= .
Observe that there is no point in including c where c 6 G: This will support the same
subgoals yet can only result in a larger precondition. Hence the choice of C 0 is exactly
 6= C 0  {c | c  G, R(c, a) 6= }. Renaming C 0 into G 0 in order to unify notation with
Equation I, this yields our final Equation II: h+ (C ) = h({c | c  G C }), where h is the
function on fact sets G that satisfies h( G ) =


0
c  G : c  I C
1 + mina[G0 ]AC ,6=G0 {c|c G,R(c,a)6=} h(( G \ G 0 )  Gr0 ) else

with Gr0 defined as Gr0 := cG0 R({c }, a[ G 0 ]).
To spell out the correspondence between Equations I and I I, view each of them as a
tree whose root node is the initializing call on the respective input tasks goal. Then
the two trees are isomorphic in the sense that there is a one-to-one mapping between tree
nodes, and, using the suffixes [ I ] and [ I I ] to identify the respective tree, at any pair G [ I ]
and G [ I I ] of corresponding tree nodes we have:
S

() G [ I I ] = {c | c  G [ I ]}
This is true by definition for the root nodes ( I ) hC+ () = h(G C ) respectively ( I I ) h+ (C ) =
h({c | c  G C }).
Consider corresponding bottom-case nodes with (*). Observe that the choice of atomic
subgoals c for G 0 is the same on both sides: Equation I allows those c  G [ I ] where
R(c, a) 6= , Equation I I allows those c  G [ I I ] where R(c, a) 6= .
We map children nodes using the same action a and the same supported subgoal set
0
G [ I ] = G 0 [ I I ] =: G 0 on both sides. We use action a in Equation I and action a[ G 0 ] in
S
Equation I I. Consider the recursive subgoals, ( G [ I ] \ G 0 )  [ cG0 R(c, a)]C in Equation I,
S
and ( G [ I I ] \ {c | c  G 0 })  cG0 R({c }, a[ G 0 ]) in Equation I I.
The G \ G 0 parts of these expressions are in exact match by (*) and construction, so it
S
S
remains to consider [ cG0 R(c, a)]C vs. cG0 R({c }, a[ G 0 ]). As regression over singleton
S
fact sets just yields the action precondition, cG0 R({c }, a[ G 0 ]) simplifies to pre( a[ G 0 ]).
S
By the definition of C , this equals [ cG0 (pre( a)  (c \ add( a)))]C . As R(c, a) = pre( a) 
S
S
(c \ add( a)), this equals [ cG0 R(c, a)]C . The desired match with [ cG0 R(c, a)]C is now
obvious, showing that (*) is preserved, which concludes our argument.
Theorem 2 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing
C ) = h C + (  ).
all singleton conjunctions. Then h+ (nc
nc
316

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

Proof: The proof is very similar to that of Theorem 1. Equation I is the same, except that
C + . Equation II is exactly the same,
Gr0 := { R(c, a) | c  G 0 } as per the definition of hnc
S
the only difference being that Gr0 := cG0 R({c }, a[ G 0 ]) now is interpreted as per the
C , as opposed to that of C . The arguments then are exactly the same, exdefinition of nc
cept for the last part of the proof showing the correspondence of { R(c, a) | c  G 0 }C vs.
S
({c }, a[ G 0 ]). As regression over singleton fact sets just yields the action preconc G 0 RS
C , this equals
dition, cG0 R({c }, a[ G 0 ]) simplifies to pre( a[ G 0 ]). By the definition of nc
{pre( a)  (c \ add( a)) | c  G 0 }C . As R(c, a) = pre( a)  (c \ add( a)), this equals { R(c, a) |
c  G 0 }C , matching { R(c, a) | c  G 0 }C as desired.
Theorem 3 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing
C ) = h C (  ).
all singleton conjunctions. Then h1 (C ) = h1 (nc
Proof: Denote  = (F , A, I , G). Consider first C . We compare respective characterizing
equations. First, Equation 6 (page 279) characterizes h1 ; applying it to C , we get that
h1 (C ) = h(G C ) where h is the function on fact sets G that satisfies

G  I C
 0
0
1 + mina[C0 ]AC ,R(G,a[C0 ])6= h( R( G, a[C ])) G = {c }, c  C
h( G ) =

maxc G h({c })
else
Observe that, in the middle case, we must have c  C 0 because otherwise c 6 add( a[C 0 ]);
and that there is no point in including any other conjunctions into C 0 , i. e., C 0 ) {c}, because this can only yield a larger recursive subgoal R( G, a[C 0 ]). Hence we can re-write the
previous equation to:

G  I C
 0
1 + mina[{c}]AC ,R(G,a[{c}])6= h( R( G, a[{c}])) G = {c }, c  C
h( G ) =

maxc G h({c })
else
Refer to this as Equation I.
Recall that hC () = h(G) where h is the function on fact sets G that satisfies

GI
 0
1
+
min
h
(
R
(
G,
a
))
G
C
h( G ) =
aA,R( G,a)6=

0
maxG0 G,G0 C h( G )
else
Refer to this as Equation II.
Similarly as in the proof of Theorem 1, view each of these equations as a tree whose
root node is the initializing call (I) h1 (C ) = h(G C ) respectively (II) hC () = h(G).
Then the two trees are isomorphic in the sense that there is a one-to-one mapping between
tree nodes, and, using the suffixes [I] and [II] to identify the respective tree, at any pair G [ I ]
and G [ I I ] of corresponding tree nodes we have:

() G [ I ] = {c | c  C, c  G [ I I ]}
This is obviously true for the root nodes, and is obviously invariant over the bottom case
where we can map the children node pairs corresponding to the same c  G [ I ] respectively c  G [ I I ].
317

fiF ICKERT & H OFFMANN & S TEINMETZ

Consider now corresponding middle-case nodes where G [ I ] = {c } and G [ I I ] = c.
First, the C actions a[{c}] all by definition satisfy R( G [ I ], a[{c}]) = R({c }, a[{c}]) 6= .
The choice of a[{c}] thus corresponds to the choice of actions a from the original task 
for which an action a[{c}] is included into C . These are exactly the actions a over which
c can be regressed, R(c, a) 6= , and hence those where R( G [ I I ], a) = R(c, a) 6= . So the
choice of actions minimized over is the same on both sides, and we can map the children
node pairs corresponding to the same a.
For any such pair, the recursive subgoal Gr [ I I ] generated in (II) is R(c, a) = (c \ add( a)) 
pre( a). The recursive subgoal Gr [ I ] generated in (I) is R({c }, a[{c}]) = pre( a[{c}]), which
by the definition of C equals [(c \ add( a))  pre( a)]C . The latter is defined as {c0 | c0 
C, c0  (c \ add( a))  pre( a)}. This equals {c0 | c0  C, c0  Gr [ I I ], showing (*) and
concluding our argument.
C is identical because, for single-conjunction sets C 0 = { c }, the two
The argument for nc
C ).
compilations coincide (specifically, the precondition of a[{c}] is the same in C and nc
Theorem 4 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing
all singleton conjunctions. Then any C-relaxed plan  CFF can be sequentialized to form a relaxed
plan for C .
Proof: Denote  = (F , A, I , G). Recall the definition of  CFF , as  CFF =  (G C ), with 
being a partial function on conjunction sets G that is defined on G C and satisfies  ( G ) =


c  G : c  I



C
0
0
0
 (( G \ G )  Gr )  {( a, G )} where a  A,

 6= G 0  {c | c  G, R(c, a) 6= , hC ( R(c, a)) = hC (c)  1},


and hC ( Gr0 ) = hC ( G 0 )  1
else
with Gr0 defined as Gr0 := cG0 R(c, a).
Starting at  (G C ), say we keep tracing the recursive invocations of the equation, using a suitable ( a, G 0 ) choice in  CFF whenever the bottom case of the equation applies. By
construction (because  CFF =  (G C )), we can make these choices in a way so that we
eventually reach the top case, where we terminate. Denote by h( a0 , G00 ), . . . , ( an1 , Gn0 1 )i
the inverted sequence of action occurrences selected along our trace, i. e., deeper recursion steps correspond to smaller indices, ( a0 , G00 ) is the action occurrence whose selection
lead to the terminating top case, and ( an1 , Gn0 1 ) is the action occurrence selected in the
initializing call. We show that h a0 [ G00 ], . . . , an1 [ Gn0 1 ]i is a relaxed plan for C .
Denote by Gi , for 1  i  n, the subgoal tackled by the selection of ( ai1 , Gi01 ) in the
middle case, and denote by G0 the final subgoal tackled by the top case. Denote by si the
state resulting from applying h a0 [ G00 ], . . . , ai1 [ Gi01 ]i in C . We show by induction over i
that (*) {c | c  Gi }  si . For i = n, where Gn = G C , this shows that sn  G C as desired.
Induction base case, i = 0: Here, (*) follows directly from definition because, the top
case having fired on G0 , for all c  G0 we have that c  I , and hence c  I C = s0 .
For the induction step, assume that (*) is true up to i. We show that it holds for i + 1.
S
By construction, Gi is the recursive subgoal ( Gi+1 \ Gi0 )  [ cGi0 R(c, ai )]C . Denote the left
S
half of this expression by LH := Gi+1 \ Gi0 , and the right half by RH := [ cGi0 R(c, ai )]C .
S

318

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

By induction hypothesis, we have () {c | c  LH  RH }  si . Consider now Gi+1
and si+1 . First, those atomic subgoals not achieved by ( ai , Gi0 ), namely Gi+1 \ Gi0 , are tackled
by LH: By () {c | c  LH } = {c | c  Gi+1 \ Gi0 }  si . As the planning is delete-free
this immediately yields {c | c  Gi+1 \ Gi0 }  si+1 . Second, those atomic subgoals that are
achieved by ( ai , Gi0 ), namely Gi0 , clearly will be true in si+1 as well: This is simply because
add( a[ Gi0 ]) = {c | c  Gi0 }.
It remains to show that ai [ Gi0 ] is applicable in si . By the definition of C , its preconS
dition is [ cGi0 (pre( a)  (c \ add( a)))]C . As R(c, a) = pre( a)  (c \ add( a)), this equals
S
[ cGi0 R(c, a)]C . The latter is exactly pre( ai [ Gi0 ]) = {c | c  RH }, so we are done by
() {c | c  RH }  si which concludes the proof.
Theorem 5 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing
CFF can be sequentialized to form a relaxed
all singleton conjunctions. Then any nc-C-relaxed plan nc
C
plan for nc .
CFF , as  CFF =  (G C ), with 
Proof: Denote  = (F , A, I , G). Recall the definition of nc
being a partial function on conjunction sets G that is defined on G C and satisfies  ( G ) =


c  G : c  I



C
0
0
0
 (( G \ G )  Gr )  {( a, G )} where a  A,

 6= G 0  {c | c  G, R(c, a) 6= , hC ( R(c, a)) = hC (c)  1},


and hC ( Gr0 ) = hC ( G 0 )  1
else

with Gr0 defined as Gr0 := { R(c, a) | c  G 0 }.
The proof of Theorem 4 remains valid exactly as written, except that now RH =
C,
{ R(c, a) | c  Gi0 }C . We need to show that ai [ Gi0 ] is applicable in si . By the definition of nc
its precondition is {pre( a)  (c \ add( a)) | c  Gi0 }C . As R(c, a) = pre( a)  (c \ add( a)), this
equals { R(c, a) | c  Gi0 }C . The latter is exactly pre( ai [ Gi0 ]) = {c | c  RH }, so again we
are done by () {c | c  RH }  si .
Theorem 6 Let  = (F , A, I , G) be a planning task, and C a set of conjunctions in  containing
all singleton conjunctions. Then a C-relaxed plan exists if and only if an nc-C-relaxed plan exists
if and only if hC < .
Proof: Denote  = (F , A, I , G). We show the claim in two parts, (a) a C-relaxed plan
exists if and only if hC < , and (b) an nc-C-relaxed plan exists if and only if hC < . We
consider first part (a).
The only if direction is a corollary of Theorems 3 and 4: If hC = , then by Theorem 3
h1 (C ) =  so a relaxed plan for C does not exist, which by Theorem 4 implies that a
C-relaxed plan cannot exist either.
For the if direction, say that hC < . We need to show that there exists a C-relaxed
plan. To this end, we consider a simpler version of the equation defining  CFF (Equation 11), restricting the choice of G 0 to singletons G 0 = {c}. After easy simplifications, we
get:  ( G ) =

c  G : c  I
 
 (( G \ {c})  R(c, a)C )  {( a, {c})} where a  A,

c  G, R(c, a) 6= , and hC ( R(c, a)) = hC (c)  1 else
319

fiF ICKERT & H OFFMANN & S TEINMETZ

Recall, here and in all equations below, that the function  is partial, which defines a Crelaxed plan only if it is defined on (the atomic conjunctions of) the global goal G C .
Observe that, in the previous equation, as we always support only a single atomic
subgoal anyhow, there is no need to recurse over sets of atomic subgoals. We can instead
recurse over single atomic subgoals, and replace the initializing and recursive calls, now
over sets of atomic subgoals, by the union over a call to each of their elements. This results
S
in the characterization given by Equation 12:  CFF = cG C  (c), with  being a partial
function on conjunctions c that satisfies  (c) =


cI
 S
0 )  {( a, { c })} where a  A,

(
c
0
C
 c  R(c,a)
R(c, a) 6= , and hC ( R(c, a)) = hC (c)  1 else
Note the similarity to Equation 8 (page 285): We are now back to a more common notation
for relaxed plan extraction (over C instead of singleton facts), extracting best supporters
one-by-one.
Towards proving our claim, we now transform the equation in a way making the link
to hC obvious. Instead of the union operations in the initial and recursive calls, which
enumerate all atomic subgoals contained in a given set of facts (G respectively R(c, a)), we
can recurse directly over these fact sets, G, and introduce a third case performing the union
over all atomic conjunctions contained in G. We hence get the characterization given by
Equation 13:  CFF =  (G), with  being a partial function on fact sets G that satisfies
 (G) =


GI



 ( R( G, a))  {( a, G )} where a  A,
R( G, a) 6= , and hC ( R( G, a)) = hC ( G )  1 G  C


 S
0
else
G 0  G,G 0 C  ( G )
Compare this to Equation 3 defining hC : h( G ) =

GI
 0
1 + minaA,R(G,a)6= h( R( G, a)) G  C

maxG0 G,G0 C h( G 0 )
else
The bottom cases in both equations are in obvious correspondence. On G with h( G ) < ,
the middle cases are in correspondence, too, in the sense that the choice of action occurrences in Equation 13 is exactly the choice of minimizing actions in Equation 3: if hC ( G ) <
, then the actions minimizing 1 + hC ( R( G, a)) are those where hC ( R( G, a)) = hC ( G )  1.
So, on finite-value subgoals, the subgoaling structure of the two equations coincides, and
in particular, if hC < , then there exists a solution  to Equation 13 such that  is defined
on G . Therefore, Equation 12 has a solution defined on all c  G C . As Equation 12 captures
a restricted version of  CFF , applying the same conditions to a smaller choice of action occurrences, this implies that there exists a C-relaxed plan as desired, concluding part (a) of
the proof.
For part (b), the only if direction follows in the same manner as a corollary of Theorems 3 and 5. The if direction also follows in the same manner, because the only difference lies in the definition of Gr0 , but for singleton G 0 that difference disappears: for G 0 = {c}
320

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

CFF , vs. G 0 = R ( c, a ) in  CFF . The atomic conjunctions conwe have Gr0 = { R(c, a)} in nc
r
tained in these expressions are the same. Hence, restricting the choice of G 0 to singletons,
CFF simplifies to Equation 12 exactly as above, and from there the
the equation defining nc
proof is identical.

Example 9 We construct an example where it is of advantage to select a smaller set of supported
subgoals G 0 , even though a larger set  a strict superset  would be feasible. The construction
extends our abstract example (Example 3).
Consider the planning task  = (F , A, I , G) defined as follows. F = { g1 , g2 , p, q1 , q2 , r1 ,
N
1
N
1
r2 , r10 , r20 , q10 , . . . , q10 , q20 , . . . , q20 }, I = {q1 , q2 }, G = { g1 , g2 }. A consists of: (abbreviating
each action a in the form a : precondition facts  positive and negative effect literals)
 Achieving the goals. a[ g1 ]: p, q1  g1 . a[ g2 ]: p, q2  g2 .
 Achieving p. a1 [ p]: r1  p. a2 [ p]: r2  p.
 Achieving preconditions for p. a[r1 ]: r10  r1 , q2 . a[r2 ]: r20  r2 , q1 .
 Achieving preconditions for ri . a[r10 ]:  r10 . a[r20 ]:  r20 .
1

N

 r2 , q1 . For 1  i  N: a[q10 i ]:  q10 i .

1

N

 r1 , q2 . For 1  i  N: a[q20 i ]:  q20 i .

 Reachieving q1 . a[r2 , q1 ]: q10 , . . . , q10
 Reachieving q2 . a[r1 , q2 ]: q20 , . . . , q20

In this construction, achieving r1  q1 takes 2 steps, while achieving r1  q2 takes N + 1 steps; and
symmetrically, achieving r2  q2 takes 2 steps, while achieving r2  q1 takes N + 1 steps. We now
CFF must
use these properties to construct a case where any smallest-possible nc-C-relaxed plan nc
use a1 [ p] to achieve p for g1 , and use a2 [ p] to achieve p for g2 , thus relying on non-maximal sets
of best-supported subgoals during relaxed plan extraction. The same arguments apply to C-relaxed
plans  CFF which, in this example, behave identically.
Say that C contains the singleton conjunctions as well as c pq1 = p  q1 , c pq2 = p  q2 ,
cr1q2 = r1  q2 , and cr2q1 = r2  q1 .
CFF =
Constructing an nc-C-relaxed plan according to Definition 3 (page 287), we start by nc
 ({ g1 , g2 }) requiring to support each of the two (atomic-singleton-conjunction) goal facts. This
can be done only by ( a[ g1 ], { g1 }) and ( a[ g2 ], { g2 }) respectively. After using these in the bottom
case of Equation 11, we get the recursive subgoal G = {c pq1 , c pq2 } (as well as the subsumed
conjunctions p, q1 , q2 which are irrelevant to the following discussion). Each of a1 [ p] or a2 [ p]
can support each of these conjunctions. Indeed, each of them is a best supporter for each of these
conjunctions.
To see this, observe first that we have hC (c pq1 ) = 3 e. g. via a[r10 ], a[r1 ], a1 [ p]; and hC (c pq2 ) = 3
e. g. via a[r20 ], a[r2 ], a2 [ p]; as is clear from this already, ai [ p] is a best supporter for c pqi . Regarding the cross-over combinations, R(c pq2 , a1 [ p]) = cr1q2 ; as a[r1 ] deletes q2 , this can only
1
N
be regressed via a[r1 , q2 ], leading to the subgoal {q20 , . . . , q20 } whose hC value clearly is 1, so
hC ( R(c pq2 , a1 [ p])) = 2 = hC (c pq2 )  1 as desired. Similarly, R(c pq1 , a2 [ p]) = cr2q1 whose hC
value is 2 as desired.
So, at the subgoal G = {c pq1 , c pq2 }, we can choose among six action occurrences, using either
0 := {c
0
0
of a1 [ p] or a2 [ p] to support either of G12
pq1 , c pq2 }, G1 : = { c pq1 }, or G2 : = { c pq2 }.
321

fiF ICKERT & H OFFMANN & S TEINMETZ

Now, while the cross-over combinations are suitable as far as hC is concerned, they are not
suitable to obtain shortest relaxed plans. Say we include c pq2 into the supported subgoal set for
i
a1 [ p]. Then the regressed subgoal is cr1q2 , requiring us to use a[r1 , q2 ] as well as the N actions a[q20 ],
so N + 1 actions in total. On the other hand, using a1 [ p] to support c pq1 , the regressed subgoal
is {r1 , q1 }  two singleton conjunctions  which can be supported using the action occurrences
( a[r10 ], {r10 }), ( a[r1 ], {r1 }) (recall here that q1 is true initially). Similarly, using a2 [ p] to support
c pq1 incurs cost N + 1 while using a2 [ p] to support c pq2 incurs cost 2.
Getting back to our choice at the subgoal G = {c pq1 , c pq2 }, if we use ( a1 [ p], G10 ), we can
thereafter use ( a2 [ p], G20 ) and get an nc-C-relaxed plan of cost 8: 2 for previously achieving the
facts gi , 2 for these two occurrences achieving c pqi , 4 for afterwards achieving the facts ri . Similarly
if we use ( a2 [ p], G20 ) first. If, however, we start with any other action occurrence, then we incur
cost N + 1 for at least one of the c pqi , exceeding the optimal cost 8 for sufficiently large N. In
0 ) is feasible, and supports a strict superset of the
particular, while the action occurrence ( a1 [ p], G12
atomic subgoals supported by ( a1 [ p], G10 ), it leads to a strictly larger relaxed plan.
Theorem 7 C-SubgoalSupport is NP-complete.
Proof: Membership is obvious by guess and check. For hardness, we show a polynomial
reduction from the Hitting Set problem with a set B of subsets b  E of a finite set of
elements E, the question being whether there exists a hitting set of size at most L.
Denote E = {e1 , . . . , en }. We construct a planning task  = (F , A, I , G) as follows.
F := E  { p0 , p1 , p2 }  { g1 , . . . , gn }, I := { p0 }, G := { g1 , . . . , gn }. The action set A contains a[ p1 ] with precondition p0 , add p1 , and empty delete, as well as a[ p2 ] with precondition p1 , add p2 , and empty delete. The action set furthermore contains an action a[ei ]
for every ei  E, with pre( a[ei ]) = { p0 }, add( a[ei ]) = {ei }, and del( a[ei ]) = { p2 }  {e j |
ex. b  B : {ei , e j }  b}. Finally, the action set contains the actions a[ g1 ], . . . , a[ gn ] where
pre( a[ gi ]) = {ei , p2 }, add( a[ gi ]) = { gi }, and the delete is empty. We set C := {{ p} | p 
F }  B  {{ei , p2 } | ei  E}.
We think of hC now in terms of a (C-)relaxed planning graph (RPG), where layer t
corresponds to the conjunctions g with hC ( g)  t. None of the conjunctions b  B can
be achieved, as there exists no action through which b can be regressed. However, all the
facts ei  E can be achieved in isolation. Consider layer 1 of the RPG. The key property we
exploit below is that (*) any subset E0 = {e1 , . . . , ek }  E is feasible at layer 1, i. e. hC ( E0 )  1,
iff there does not exist b  B s.t. b  E0 . From right to left, if b  E0 then E0 is infeasible
simply because hC (b) = . Vice versa, say there does not exist b  B s.t. b  E0 . Then
c0  E0 , c0  C is just the set of singleton conjunctions {ei }, and we get hC ( E0 ) = 1 as each
{ei } is achieved by a single action.
At RPG layer 1, we can apply a[ p2 ]. As each ei is already present, we get each of the
conjunctions {e1 , p2 }, . . . , {en , p2 } at layer 2. With this, the a[ gi ] actions become feasible, so
that the goal is reached at layer 3.
Consider now relaxed plan extraction. To get the goal, we must select all a[ gi ] actions.
Say all those are selected in sequence. Then we get the subgoal {{e1 , p2 }, . . . , {en , p2 }} at
layer 2 (plus the subsumed singleton conjunctions, which we omit for readability). The
only action through which these can be regressed is a[ p2 ]: recall that the a[ei ] actions delete
p2 . But what is the maximal subset G 0 := {{ei1 , p2 }, . . . , {eik , p2 }}  {{e1 , p2 }, . . . , {en , p2 }}
that we can choose to support?
322

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

Any such subset yields the new generated subgoal {ei1 , . . . , eik , p1 } at RPG layer 1. Here
p1 is achieved by a[ p1 ] which does not interact with anything so is not critical: hC ({ei1 , . . . , eik ,
p1 }) = hC ({ei1 , . . . , eik }). Denote E0 := {ei1 , . . . , eik }. Then the action occurrence ( a[ p2 ], G 0 )
is C-feasible at RPG layer 1 iff E0 is feasible at RPG layer 1. By (*), the latter is the case iff
there does not exist b  B s.t. b  E0 . But then, consider E \ E0 . By construction, this is a
hitting set iff E0 is feasible: if E \ E0 is a hitting set then no b can be fully contained in E0 , and
if no b is fully contained in E0 then E \ E0 must hit every b. Setting K := n  L, we thus get
that there exists a C-feasible G 0 with | G 0 |  K iff there exists a feasible E0 with | E0 |  n  L
iff there exists a hitting set of size  n  (n  L) = L. This concludes the proof.

370

370

350

350

330

330

310

310

290

290

270

270
CFF
hnc
hCFF

250
230
210
190
170
150
21

250
230

C)
hFF (ce
FF
h (C )
hCFF
| G 0 |=1A
hCFF
| G 0 |=1
hFF arb
hFF rnd

22

23

210
190
170
24

25

26

27

28

29

210

150
21

22

(a) Search-only coverage

23

24

25

26

27

28

29

210

(b) Inclusive coverage

Figure 7: Total coverage.

Appendix B. Coverage when Including C-Learning into the Time Limit
We give the same coverage plots as in Section 5.4, but imposing a 30-minute limit on Clearning and search together (inclusive in Figures 7, 8, and 9). For convenience, we also
include the search-only plots from Section 5.4.

References
Alcazar, V., Borrajo, D., Fernandez, S., & Fuentetaja, R. (2013). Revisiting regression in
planning. In Rossi, F. (Ed.), Proceedings of the 23rd International Joint Conference on
Artificial Intelligence (IJCAI13), pp. 22542260. AAAI Press/IJCAI.
Baier, J. A., & Botea, A. (2009). Improving planning performance using low-conflict relaxed
plans. In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings of the
19th International Conference on Automated Planning and Scheduling (ICAPS09), pp. 10
17. AAAI Press.
323

fiF ICKERT & H OFFMANN & S TEINMETZ

20

20

15

15

10

10

5

5

01
2

22

23

24

25

26

27

28

29

01
2

210

22

Maintenance search-only
20

15

15

10

10

5

5

22

23

24

25

26

27

28

24

25

26

27

28

29

210

29

210

Maintenance inclusive

20

01
2

23

29

01
2

210

Parcprinter search-only

22

23

24

25

26

27

28

Parcprinter inclusive

Figure 8: Coverage in individual domains.
Bonet, B., & Geffner, H. (1999). Planning as heuristic search: New results. In Biundo, S.,
& Fox, M. (Eds.), Proceedings of the 5th European Conference on Planning (ECP99), pp.
6072. Springer-Verlag.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129(12),
533.
Bonet, B., & Helmert, M. (2010). Strengthening landmark heuristics via hitting sets. In
Coelho, H., Studer, R., & Wooldridge, M. (Eds.), Proceedings of the 19th European Conference on Artificial Intelligence (ECAI10), pp. 329334, Lisbon, Portugal. IOS Press.
Bylander, T. (1994). The computational complexity of propositional STRIPS planning. Artificial Intelligence, 69(12), 165204.
Cai, D., Hoffmann, J., & Helmert, M. (2009). Enhancing the context-enhanced additive
heuristic with precedence constraints. In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings of the 19th International Conference on Automated Planning and
Scheduling (ICAPS09), pp. 5057. AAAI Press.
Coles, A. J., Coles, A., Fox, M., & Long, D. (2013). A hybrid LP-RPG heuristic for modelling
numeric resource flows in planning. Journal of Artificial Intelligence Research, 46, 343
412.
324

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

20

20

15

15

10

10

5

5

01
2

22

23

24

25

26

27

28

29

01
2

210

22

23

Tetris search-only
20

15

15

10

10

5

5

22

23

24

25

26

27

25

26

27

28

29

210

28

29

210

Tetris inclusive

20

01
2

24

28

29

01
2

210

Thoughtful search-only

22

23

24

25

26

27

Thoughtful inclusive

Figure 9: Coverage in individual domains.
Do, M. B., & Kambhampati, S. (2001). Sapa: A domain-independent heuristic metric temporal planner. In Cesta, A., & Borrajo, D. (Eds.), Proceedings of the 6th European Conference on Planning (ECP01), pp. 109120. Springer-Verlag.
Domshlak, C., Hoffmann, J., & Katz, M. (2015). Red-black planning: A new systematic
approach to partial delete relaxation. Artificial Intelligence, 221, 73114.
Fox, M., & Long, D. (2001). Hybrid STAN: Identifying and managing combinatorial optimisation sub-problems in planning. In Nebel, B. (Ed.), Proceedings of the 17th International Joint Conference on Artificial Intelligence (IJCAI-01), pp. 445450, Seattle, Washington, USA. Morgan Kaufmann.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning through stochastic local search and
temporal action graphs. Journal of Artificial Intelligence Research, 20, 239290.
Haslum, P. (2009). hm ( P) = h1 ( Pm ): Alternative characterisations of the generalisation from hmax to hm . In Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.),
Proceedings of the 19th International Conference on Automated Planning and Scheduling
(ICAPS09), pp. 354357. AAAI Press.
325

fiF ICKERT & H OFFMANN & S TEINMETZ

Haslum, P. (2012). Incremental lower bounds for additive cost planning problems. In
Bonet, B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings of the 22nd
International Conference on Automated Planning and Scheduling (ICAPS12), pp. 7482.
AAAI Press.
Haslum, P., & Geffner, H. (2000). Admissible heuristics for optimal planning. In Chien, S.,
Kambhampati, R., & Knoblock, C. (Eds.), Proceedings of the 5th International Conference
on Artificial Intelligence Planning Systems (AIPS-00), pp. 140149, Breckenridge, CO.
AAAI Press, Menlo Park.
Helmert, M. (2004). A planning heuristic based on causal graph analysis. In Koenig, S.,
Zilberstein, S., & Koehler, J. (Eds.), Proceedings of the 14th International Conference on
Automated Planning and Scheduling (ICAPS04), pp. 161170, Whistler, Canada. Morgan Kaufmann.
Helmert, M. (2006). The Fast Downward planning system. Journal of Artificial Intelligence
Research, 26, 191246.
Helmert, M., & Geffner, H. (2008). Unifying the causal graph and additive heuristics. In
Rintanen, J., Nebel, B., Beck, J. C., & Hansen, E. (Eds.), Proceedings of the 18th International Conference on Automated Planning and Scheduling (ICAPS08), pp. 140147. AAAI
Press.
Hoffmann, J. (2005). Where ignoring delete lists works: Local search topology in planning
benchmarks. Journal of Artificial Intelligence Research, 24, 685758.
Hoffmann, J. (2011). Analyzing search topology without running any search: On the connection between causal graphs and h+ . Journal of Artificial Intelligence Research, 41,
155229.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253302.
Hoffmann, J., Steinmetz, M., & Haslum, P. (2014). What does it take to render h+ ( c )
perfect?. In ICAPS 2014 Workshop on Heuristics and Search for Domain-Independent
Planning (HSDIP14).
Keyder, E., & Geffner, H. (2008). Heuristics for planning with action costs revisited. In
Ghallab, M. (Ed.), Proceedings of the 18th European Conference on Artificial Intelligence
(ECAI-08), pp. 588592, Patras, Greece. Wiley.
Keyder, E., & Geffner, H. (2009). Trees of shortest paths vs. Steiner trees: Understanding
and improving delete relaxation heuristics. In Boutilier, C. (Ed.), Proceedings of the
21st International Joint Conference on Artificial Intelligence (IJCAI 2009), pp. 17341739,
Pasadena, California, USA. Morgan Kaufmann.
Keyder, E., Hoffmann, J., & Haslum, P. (2012). Semi-relaxed plan heuristics. In Bonet,
B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings of the 22nd International Conference on Automated Planning and Scheduling (ICAPS12), pp. 128136. AAAI
Press.
Keyder, E., Hoffmann, J., & Haslum, P. (2014). Improving delete relaxation heuristics
through explicitly represented conjunctions. Journal of Artificial Intelligence Research,
50, 487533.
326

fiC OMBINING h+ WITH hm : A D IRECT C HARACTERIZATION

Marques-Silva, J., & Sakallah, K. (1999). GRASP: A search algorithm for propositional
satisfiability. IEEE Transactions on Computers, 48(5), 506521.
McDermott, D. V. (1999). Using regression-match graphs to control search in planning.
Artificial Intelligence, 109(1-2), 111159.
Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering
an efficient SAT solver. In Proceedings of the 38th Conference on Design Automation
(DAC-01), Las Vegas, Nevada, USA. IEEE Computer Society.
Richter, S., & Westphal, M. (2010). The LAMA planner: Guiding cost-based anytime planning with landmarks. Journal of Artificial Intelligence Research, 39, 127177.
Steinmetz, M., & Hoffmann, J. (2016). Towards clause-learning state space search: Learning
to recognize dead-ends. In Schuurmans, D., & Wellman, M. (Eds.), Proceedings of the
30th AAAI Conference on Artificial Intelligence (AAAI16). AAAI Press.
Valenzano, R. A., Sturtevant, N. R., Schaeffer, J., & Xie, F. (2014). A comparison of
knowledge-based GBFS enhancements and knowledge-free exploration. In Chien,
S., Do, M., Fern, A., & Ruml, W. (Eds.), Proceedings of the 24th International Conference
on Automated Planning and Scheduling (ICAPS14). AAAI Press.
van den Briel, M., Benton, J., Kambhampati, S., & Vossen, T. (2007). An LP-based heuristic
for optimal planning. In Bessiere, C. (Ed.), Proceedings of the Thirteenth International
Conference on Principles and Practice of Constraint Programming (CP07), Vol. 4741 of
Lecture Notes in Computer Science, pp. 651665. Springer-Verlag.

327

fiJournal of Artificial Intelligence Research 56 (2016) 119-152

Submitted 07/15; published 05/16

Budgeted Optimization with Constrained Experiments
Javad Azimi

jaazimi@microsoft.com

Microsoft, Sunnyvale, CA, USA

Xiaoli Z. Fern

xfern@eecs.oregonstate.edu

School of EECS, Oregon State University

Alan Fern

afern@eecs.oregonstate.edu

School of EECS, Oregon State University

Abstract
Motivated by a real-world problem, we study a novel budgeted optimization problem
where the goal is to optimize an unknown function f () given a budget by requesting a
sequence of samples from the function. In our setting, however, evaluating the function
at precisely specified points is not practically possible due to prohibitive costs. Instead,
we can only request constrained experiments. A constrained experiment, denoted by Q,
specifies a subset of the input space for the experimenter to sample the function from. The
outcome of Q includes a sampled experiment x, and its function output f (x). Importantly,
as the constraints of Q become looser, the cost of fulfilling the request decreases, but the
uncertainty about the location x increases. Our goal is to manage this trade-off by selecting
a set of constrained experiments that best optimize f () within the budget. We study this
problem in two different settings, the non-sequential (or batch) setting where a set of
constrained experiments is selected at once, and the sequential setting where experiments
are selected one at a time. We evaluate our proposed methods for both settings using
synthetic and real functions. The experimental results demonstrate the efficacy of the
proposed methods.

1. Introduction
This work is motivated by the experimental design problem of optimizing the power output
of nano-enhanced microbial fuel cells. Microbial fuel cells (MFCs) (Bond & Lovley, 2003;
Fan, Hu, & Liu, 2007; Park & ZeikusG, 2003; Reguera, McCarthy, Mehta, Nicoll, Tuominen,
& Lovley, 2005) use micro-organisms to break down organic matter and generate electricity.
For a particular MFC design, it is critical to optimize the biological energetics and the
microbial/electrode interface of the system, which research has shown to depend strongly
on the surface properties of the anodes (Park & ZeikusG, 2003; Reguera et al., 2005). This
motivates the design of nano-enhanced anodes, where nano-structures (e.g., carbon nanowire) are grown on the anode surface to improve the MFCs power output. Unfortunately,
there is little understanding of the interaction between various possible nano-enhancements
and MFC capabilities for different micro-organisms. Thus, optimizing the anode design for
a particular application is largely guesswork. Our goal is to develop algorithms to aid this
process.
Traditional experimental design, Bayesian optimization and response surface methods
(Myers, Montgomery, & Anderson-Cook, 1995; Jones, 2001; Brochu, Cora, & De Freitas,
2010) commonly assume that the experimental inputs can be specified precisely and attempt
c
2016
AI Access Foundation. All rights reserved.

fiAzimi, Fern, & Fern

to optimize a design by requesting specific experiments. For example, requesting an anode
to be tested with nano-wire of specific length and density. However, these parameters are
unlike usual experimental control parameters (such as temperature) that can be easily set
at precise values. Manufacturing nano-structures is rather an art and it is very difficult
to achieve a precise parameter setting. Instead, it is more practical to request constrained
experiments, which place constraints on these parameters. For example, we may specify
intervals for the length and density of the nano-wire. Given such a request, nano-materials
that satisfy the given set of constraints can be produced at some cost, which will typically
increase with tighter constraints.
Note that a possible alternative to requesting constrained experiments would be to treat
the nano-structure manufacturing parameters as the experimental inputs. Such inputs can
be precisely specified, and hence standard methods can be used. However, this approach will
not yield a satisfactory solution for our problem. In particular, the mapping between the
manufacturing parameters and the produced nano-structures is extremely noisy. This makes
it difficult to find the manufacturing parameters that optimize the expected MFC power
output. Further, the scientists are primarily interested in learning what nano-structure
properties optimize the MFC power output, rather than knowing the specific manufacturing parameters, which can vary significantly from lab to lab. Thus we focus on directly
optimizing in the space of nano-structure properties via constrained experiments.
Based on the above motivation, in this paper, we study the associated budgeted optimization problem where, given a budget, our goal is to optimize an unknown function f ()
by requesting a set of constrained experiments. Solving this problem requires careful consideration of the trade-off between the cost and the uncertainty of a constrained experiment:
weakening the constraints will lower the cost of an experiment, but increase the uncertainty
about the location of the next observation. Prior work on experimental design, stochastic
optimization, and active learning do not directly apply to constrained experiments because
they typically assume precise experiments.
This problem can be formulated in the theoretical framework of partially observable
Markov decision processes (POMDPs), where the optimal solution corresponds to finding
an optimal POMDP policy. However, solving for optimal or even near-optimal policies is
computationally intractable, even in the case of traditional optimization problems. This
has led researchers to develop a variety of myopic policies in the context of traditional
optimization, which have been observed to achieve good performance, even in comparison to
more sophisticated, less myopic strategies (Moore & Schneider, 1995; Jones, 2001; Madani,
Lizotte, & Greiner, 2004; Brochu et al., 2010).
Our problem can be considered in two different settings, non-sequential and sequential.
In the non-sequential setting, which is also referred to as the batch setting, all constrained
experiments must be selected at once. This setting is appropriate for applications where
there are multiple experimental facilities and the experiments are too time consuming to
be run sequentially. In contrast, the sequential setting allows us to request one constrained
experiment at a time, and wait for the outputs of previous experiments before making the
next request. The sequential setting has the advantage that it allows us to use the maximum
available information for selecting each experiment, and is generally expected to outperform
the non-sequential setting when the total running time is not a concern. In this paper, we
study both settings.
120

fiBudgeted Optimization with Constrained Experiments

For the non-sequential setting, we introduce a non-decreasing submodular objective function to select a batch of constrained experiments within the given budget. For a given set of
constrained experiments, the objective measures its expected maximum output. We present
a computationally efficient greedy algorithm that approximately optimizes the proposed objective.
For the sequential setting, we build on a set of classic myopic policies that have been
shown to achieve good performance in traditional optimization (Moore & Schneider, 1995;
Jones, 2001; Madani et al., 2004; Brochu et al., 2010) and introduce non-trivial extensions
to make them applicable to constrained experiments.
We present experimental evaluations for both settings using synthetic functions and
functions generated from real-world experimental data. The results indicate that, in both
settings, our proposed methods outperform competing baselines.
The remainder of the paper is organized as follows. We will introduce the background
and related work in Section 2. Section 3 describes the problem setup. The non-sequential
setting is studied in Section 4. Section 5 introduces our proposed methods for selecting constrained experiments in the sequential setting. Section 6 presents the empirical evaluation
of the proposed methods. We conclude the paper and discuss future work in Section 7.

2. Background and Related Work
Given an unknown black box function that is costly to evaluate, we are interested in finding
the extreme point (minimizer or maximizer) of the function via a small number of function
evaluations. To solve this problem, Bayesian Optimization (BO) approaches have been
heavily studied (Jones, 2001; Brochu et al., 2010) and demonstrated significant promises.
There are two key components in the basic framework of Bayesian Optimization. The first
component is a probabilistic model of the underlying function that is built based on the prior
information (i.e., the existing observed experiments). Gaussian process (GP) regression has
been widely used in the literature of BO for this purpose (Brochu et al., 2010). For any
unobserved point, a GP models its function output as a normal random variable, with its
mean predicting the expected function output of the point and the variance capturing the
uncertainty associated with the prediction.
The second key component of BO is the selection criterion that is used to determine
what experiment to select based on the learned model. In the existing literature, various
selection criteria have been proposed and most of them are a combination of exploring
the unexplored input space of the function (i.e., areas of high variance) and exploiting the
promising area (i.e., area with large mean). A selection criterion can be either sequential
(Jones, 2001; Locatelli, 1997; Moore, Schneider, Boyan, & Lee, 1998; Srinivas, Krause,
Kakade, & Seeger, 2010) in which only one experiment is asked at each iteration or nonsequential (Schonlau, 1997; Azimi, Fern, & Fern, 2010; Ginsbourger, Riche, & Carrarog,
2010) where a batch of experiments are requested at each iteration.
Below we review some of the most successful sequential selection criteria in the literature
of BO. One of the first sequential policies is based on selecting the sample with maximum
probability of improving (MPI) the best current observation, ymax (assuming we want to
maximize f ), by a given margin  (Elder, 1992; Stuckman, 1988). Let the best current
observation be ymax . The goal of MPI is to select the next experiment that has the highest
121

fiAzimi, Fern, & Fern

probability of producing an output no smaller than (1 + )ymax . One issue of this approach
is that the performance can often be sensitive to the value of the margin parameter  (Jones,
2001). For small values of , MPI will focus on the most promising area at first and then
move onto unexplored areas. In contrast, for large values of , MPI will primarily explore
and converge very slowly. Selecting a proper value for  can be challenging in practice.
The maximum expected improvement (MEI) (Locatelli, 1997) criterion avoids this issue and
selects the experiment that directly maximizes the expected improvement over the current
best observation. Heuristics based on upper-confidence bounds have also been explored
(Srinivas et al., 2010), which has been shown to be competitive with MEI given appropriate
parameter selection. However, selecting the best parameters for a particular application
empirically can be a challenge. An alternative approach that has received attention is
Thompson Sampling (Chapelle & Li, 2011), which is a randomized strategy for managing the
exploration-exploitation trade-off. This approach first samples the underlying uncertainty,
in our case the unknown function f , and then returns the experiment that maximizes the
sample. In this work, we have focused on extending the above deterministic methods for
BO to constrained experiments. Extending alternatives such as Thompson sampling is a
potentially interesting future direction.
Recently, researchers have begun to consider non-sequential or batch Bayesian optimization (Azimi et al., 2010; Ginsbourger et al., 2010; Desautels, Krause, & Burdick, 2014),
which selects multiple experiments at once. Non-sequential BO is considered more appropriate for applications where there is a need and capability to run multiple experiments
simultaneously. One non-sequential approach (Azimi et al., 2010) selects k > 1 experiments
at once by matching the behavior of executing a given sequential policy (e.g., MEI) for k
steps. In another non-sequential approach (Ginsbourger et al., 2010), the authors tried to
select a batch of experiments that will lead to the highest expected improvement. However, it was shown that the expected improvement over a set of jointly normal random
variables does not have any closed form solution when k > 2, nor it can be solved efficiently using numerical methods. Instead, simple heuristics were proposed to approximate
the expected improvement and select a batch accordingly. More recently, an algorithm
based on upper-confidence bounds has also been introduced (Desautels et al., 2014), which
is computationally cheaper than prior work but requires careful parameter selection.
Note that all of the aforementioned approaches assume that the unknown function we
aim to optimize can be sampled at precisely specified points, making them unsuitable for
tasks such as our motivating nano application, where sampling the function at exact locations is impractical. The proposed sequential approaches in this paper, have been previously
presented in less detail (Azimi et al., 2010). In this paper, we provide a more complete
and formal description of the sequential approaches with additional empirical results. In
addition, we introduce and evaluate a batch selection algorithm that chooses a batch of
constrained experiments at each iteration.

3. Problem Setup
Let X  Rd be a d-dimensional input space, where each dimension i is bounded in [Ai ,
Bi ]. We often refer to the elements of X as experiments. We assume there is an unknown
real-valued function f (x) : X  <, which represents the expected value of the dependent
122

fiBudgeted Optimization with Constrained Experiments

variable after running experiment x. In our motivating application, f (x) is the expected
power output produced by a particular nano-structure x. Conducting an experiment x
produces a noisy outcome y = f (x) + , where  is a noise term.
In traditional optimization settings (Jones, 2001; Brochu et al., 2010), the goal is to find
an x  X that approximately optimizes f () by requesting a set of experiments and observing
their outcomes. Since sampling the function at exactly specified points is prohibitively
expensive in our application, we request constrained experiments, which define a subset of
experiments in X . Specifically, we define a constrained experiment as a hyper-rectangle in
X , denoted by Q = (q1 , q2 ,   , qd ), where qi = (ai , bi ) with Ai  ai < bi  Bi defines
the range of values that is considered admissible for each input dimension i. Note that
for computational reasons, in this work we consider a discretized input space, where each
input dimension is divided into equal-sized intervals. As such, a constrained experiment Q
will indicate for each dimension i the first (specified by ai ) and the last (specified by bi )
intervals to be included in the hyper-rectangle. For the remainder of this paper, we will
interchangeably use the terms constrained experiment, hyper-rectangle and query.
Given a constrained experiment Q, the experimenter will first construct an experiment
x (we assume that x can be precisely measured after being produced) that satisfies the
given constraints of Q, run the experiment, and return the noisy observation of f (x). Note
that x is a random variable given Q, and we assume this conditional distribution, px (x|Q),
is known a priori as part of the problem inputs. More precisely, for any query Q, the
experimenter will return a 2-tuple (x, y), where:
 x = (x1 , x2 ,   , xd ) is an experiment that satisfies the constraints of Q,
 y is the noisy observation of the function f () at x, y = f (x) + .
In practice, the cost c of fulfilling a constrained experiment can be variable depending on the size of the hyper-rectangle. In particular, higher cost will be associated with
tighter constraints or smaller hyper-rectangles. We assume that this cost is modeled by a
deterministic function fc (), which is provided to us as part of the inputs. For example, in
our motivating application, fc () is dominated by the time required to produce the nano
material that satisfies the given constraints, which is inversely correlated with the size of
the constraints. In addition, we must operate within a total budget B. Thus, the objective
is to find a set of queries within budget B that leads to the best estimate of the maximizer
of the function over the input space X .
To summarize, the inputs to our problem include a set of prior experiments D (which
could potentially be empty), a budget B, a deterministic cost function fc () of fulfilling a
constrained experiment Q, and a conditional probability density function px (x|Q) of the
specific experiment x generated for any given constrained experiment Q.
Given the inputs, our task is to select a set of constrained experiments Q = {Q1 , Q2 ,  
, Qk } whose total cost is within budget B. Running the selected constrained experiments
will result in a set of k tuples (xi , yi )ki=1 , with which we must determine a final output
x  {x1 , . . . , xk }, which is our prediction of the maximizer of f () among all observed
experiments. Note that we restrict ourselves to returning an experiment that was actually
observed, even in cases where we might predict some other non-observed experiment to be
better. This formulation matches the objective of our motivating application to produce a
123

fiAzimi, Fern, & Fern

good nano-structure x using the given budget, rather than to make a prediction of what
nano-structure might be good.
We study this problem in two different settings, non-sequential (or batch) and sequential.
In the non-sequential setting, we must decide the entire set of queries at the same time.
In contrast, the sequential setting requests constrained experiments sequentially one at a
time: only after receiving the result of the previous request, another query is selected and
presented to the experimenter. This procedure is repeated until we reach the budget limit.
In the following two sections, we will introduce our proposed solutions for both settings.

4. Non-sequential Approach
In this section, we consider the non-sequential setting, in which we must select the entire
batch of queries Q within the given budget B at once. Note that in general these batches
can be multi-sets that have repeated queries, which may be desirable in noisy settings. This
is also called the non-adaptive (Goel et al., 2006; Krause et al., 2008) or Batch (Azimi et al.,
2010) setting. This setting is commonly used in applications where we must start multiple
experiments at once and cannot wait for the outputs of the previous queries to decide the
next queries (Tatbul et al., 2003).
4.1 The Objective Function
Let QB be the set of feasible solutions such that for any Q  QB the total cost of Q
is no greater than the budget B. Our goal is to find the optimal multi-set of queries
Q = {Q1 , Q2 ,   , Qk }  QB . To define what we mean by optimal, let us consider the
outcome of the queries, which are a set of tuples: (xi , yi ) , i = 1, 2, . . . , k. The xi s are the
experiments produced by the experimenter given the queries and the yi s represent their
experimental output (i.e., the noisy observation of f (xi )). We will then select a final output
x  {x1 , . . . , xk } that is believed to achieve the maximal f () value. As such, for any
Q  QB , we can measure how good Q is based on the maximal y value resulting from this
set of queries. Specifically, this is captured by:
h
h
ii

	 fifi
J(Q) = E(x1 ,,x|Q| ) E(y1 ,,y|Q| ) max y1 , . . . , y|Q| fiD, (x1 ,   , x|Q| ) ,
(1)
where the first expectation is taken over all possible values of the xi s, which represent the
individual experiments created for each query in Q, and the second expectation is taken
over all possible yi s, which represents the experimental outcomes of the xi s. As mentioned
previously, the xi s are distributed according to px (xi |Qi ), which is part of the inputs. The
distribution of yi s given the xi s depends on the posterior distribution of f () given D. In
our work, we use Gaussian processes to model the distribution of f (). Consequently, the
set of yi s are jointly normal conditioned on all xi s and D.
Since our input space is discretized, we can enumerate all possible constrained experiments and denote them as QM = {Q1 , Q2 , ..., QM }, where M is the total number of possible
constrained experiments, and let c1 , . . . , cM be their corresponding cost (i.e., ci = fc (Qi )).
Let S  S = {1, ..., M } be a subset of indices and QS denote the collection of queries
indexed by S, i.e., QS = {Qi : i  S}. Our goal can then be stated as selecting an S such
that the corresponding QS maximizes the objective (Equation 1) subject to the constraint
124

fiBudgeted Optimization with Constrained Experiments

P

iS ci  B. Unfortunately, optimizing this objective is intractable due to the combinatorial nature of the problem and exponentially many possible solutions to consider. Below
we will reformulate the objective to demonstrate that it is a non-decreasing submodular set
function and introduce an algorithm with an approximation guarantee.
Specifically, we will consider a slightly different but equivalent view of the querying
process. So far our view is that after S is chosen, each query Q  QS will result in
an experiment x, which can be viewed as a random sample drawn from the distribution
px (x|Q) (note that in this work px is uniform within the hyper-rectangle defined by the
query). From the process point of view, it clearly does not matter whether this random
draw happens after Q is chosen, or at the very beginning of the whole process before Q is
chosen. Following this reasoning, we could assume that for every possible query in QM , a
random experiment is drawn at the very beginning of the process and the results are stored
and used later when S is selected. Let XM = {x1 , . . . , xM } denote the random variables
representing the outcome of the random draw for Q1 , ..., QM respectively. The objective
can be then reformulated as:
h
h
ii

	 fifi
J(S) = EXS E(y1 ,...,y|S| ) max y1 , . . . , y|S| fiD, XS ,
(2)

where XS = {xi : i  S} is the subset of XM defined by S, and the yi s are the noisy
outcomes of the xi s in XS .
4.2 Approximation Algorithm
Since standard batch Bayesian Optimization is a special case of optimizing J(S), the hardness of optimizing J(S) follows from NP-hardness of Bayesian Optimization. Thus, below
we will show that J(S) is a non-decreasing submodular set function and present an algorithm with a bounded approximation factor.
Definition 1. Suppose S is a finite set, g : 2S  R+ is a submodular set function if for all
S1  S2  S and x  S \ S2 , it holds that g(S1  {x})  g(S1 )  g(S2  {x})  g(S2 ).
Thus, a set function is submodular if adding an element to a smaller set provides no
less improvement than adding the element to a larger superset. Also, a set function is
non-decreasing if for any set S and element x we have g(S)  g(S  {x}).
To show that J(S) is submodular, we will rewrite the objective function by defining
JXM (S) to be the inner expectation of Equation 2 for a fixed realization of random variable
XM :
h
i

	 fifi
JXM (S) = E(y1 ,...,y|S| ) max y1 , . . . , y|S| fiD, XS .
Lemma 1. For any given XM , JXM (S), which returns the expected maximum over a set
of jointly distributed random variables, is a monotonically non-decreasing submodular set
function.
The proof is in the Appendix.
The proposed objective function, J(S) = EXM [JXM (S)] takes the expectation of JXM (S)
over all possible values of XM . Because JXM (S) is non-decreasing, it is easy to verify
that J(S) is also non-decreasing. Further note that the set of submodular functions is
125

fiAzimi, Fern, & Fern

closed under expectation, we can thus conclude that the proposed objective, J(S), is a
non-decreasing submodular function.
We now present our proposed algorithm for optimizing J(S). The inputs to our algorithm include the set of all possible constrained experiments, QM = {Q1 , ..., QM }, their associated costs
P c1 , ..., cM , and a total budget B, and the output is a subset S  S = {1, ..., M }
such that iS ci  B. We first introduce a simple greedy algorithm, which begins with
an initial empty set S =  and greedily adds one constrained experiment (its index) at a
time until the total cost of S reaches B. In each step, let S be the current set and C be the
total cost of S, the greedy algorithm selects an index i  S such that:
i = argmax
iS;c
/
i BC

J(S  i)  J(S)
.
ci

(3)

In other words, at each step, the algorithm greedily selects a new constrained experiment
that is within the budget and leads to the largest cost-normalized improvement of the
objective.
It is known that this simple greedy algorithm does not have any bounded approximation
factor (Khuller, Moss, & Naor, 1999). Previous work (Khuller et al., 1999; Krause &
Guestrin, 2005) introduced a small change to the greedy algorithm that provides us with a
bounded approximation factor. In particular, one just needs to consider, as an alternative to
the output of the greedy algorithm, the single query that is within the budget and achieves
the best objective value (denoted by Sa ). By comparing this alternative with the output
of the greedy algorithm, we are guaranteed to achieve a bounded approximation factor.
The complete algorithm is summarized in Algorithm 1. The approximation bound for this
algorithm follows from the following theorem.
Theorem 1. (Khuller et al., 1999) Let J() be a monotonically non-decreasing submodular
set function such that J() = 0, and S  is the output of our Algorithm 1. Suppose OPT is
the optimal solution, the following bound is guaranteed
"

|S  |+1 #
1
1
J(S ) 
1 1 
J(OPT)
2
|S | + 1


1 e1

J(OPT).
2
e


(4)

The dominating factor of the run time is the linear dependence on M , the number of
possible queries. Note that in the discretized setting that we consider, M will be exponential
in the number of dimensions. In the scientific application domains that motivate our work,
the number of dimensions is typically small (e.g., 2 or 3). However, when working with
a fine resolution discretization, the computation time can still be significant. To address
this issue, in the next section we describe a simple strategy for soundly pruning candidate
queries from consideration, which yields significant speedups. Problems with significantly
higher dimensions, however, will require continuous rather than discretized optimization.
126

fiBudgeted Optimization with Constrained Experiments

Algorithm 1 The Greedy Non-Sequential Algorithm
Input: D, B > 0, {Q1 , ..., QM }, {c1 , ..., cM }
P
Output: A set of indices S  S = {1, ..., M } such that iS ci  B
- i = argmaxiS,ci B J({i})
Sa  {i }
- S  , C  0
while (C < B) do
J(S  {i})  J(S)
Select i such that:
i = argmax
ci
iS,c
/ i BC
- C  C + ci
- S  S  {i }
end while
if J(S)  J(Sa ) then
- return S
else
- return Sa
end if

Algorithm 2 Accelerated Greedy Algorithm
Input: D, B > 0, {Q1 , ..., QM }, {c1 , ..., cM }
P
Output: A set of indices S  S = {1, ..., M }, s.t., sS ci  B
- i = argmaxiS,ci B J({i})
- Sa  {i }
-S  , C = 0, (i) = J({i})/ci , for i = 1, . . . , M
while (C < B) do
while true do
Set z = argmaxi:iS\S (i), ci  B  C, then re-calculate (z) such that
J(S  {z})  J(S)
(z) =
cz
if (z)  maxiS\{Sz} (i) then
Break
end if
end while
- C  C + cz , S  S  {z}
end while
if J(S)  J(Sa ) then
- return S
else
- return Sa
end if

127

fiAzimi, Fern, & Fern

4.3 Accelerated Greedy Algorithm
In this section, following prior applications of submodular optimization (e.g. Krause &
Guestrin, 2005), we describe an accelerated greedy algorithm, which yields significant gains
in computational efficiency. At each greedy step, let S represent the set of queries that
have been selected so far. To make another greedy selection, we need to compute the cost
normalized incremental difference (i) = J(Si)J(S)
for each candidate query i, such that
ci
i
/ S and ci  B  C. This computation can be expensive because the number of candidate
queries is often very large. Fortunately, by carefully maintaining the normalized incremental
differences calculated in the first greedy step, we can avoid recomputing a large majority of
them in later iterations.
Specifically, the first iteration will compute the (i) values for all i  S. We then sort
them in decreasing order based on their  values, and select the first query and remove
it from the list. For the next iteration, we move on to the next query in the sorted list
and recompute its  value. If the value remains the largest, we will immediately select this
query and remove it from the list, and proceed to the next iteration without recomputing
any other  values. Otherwise, we proceed to evaluate the next query in the sorted list
until finding one whose recomputed  value is greater than the other stored values and
select that query. The submodular property of our objective guarantees that this approach
makes the same choices as the full greedy algorithm, but effectively avoids a large number of
computations of the  values in practice. The proposed accelerated algorithm is summarized
in Algorithm 2.
4.4 Computation of the Expected Maximum
For any given set S, to compute J(S), we need to compute the expected maximum value
of a set of jointly distributed random variables (y1 , ..., y|S| ). Unfortunately, the expected
maximum of a set of dependent random variables generally does not have a closed-form
solution (Ross, 2008). Instead, we use a Monte-Carlo simulation approach for computing
the expected maximum value. Specifically, given S, to compute J(S), we first sample one
experiment for each Q  QS , resulting in {x1 , ..., x|S| }. We then sample the yi s from their
posterior distribution py (y1 , ..., y|S| |x1 , ..., x|S| , D) and take the maximum of the sampled
yi s. This is repeated l independent times and the expected maximum is then obtained
by averaging across the l results. Note that our computation of the expected maximum
 standard
value with simulation will not be exact. Denoting the simulated results by J,

Chernoff bounds can be used to bound the error of J(S) with high probability. Assuming

a bounded error, that is |J(S)  J(S)|
  for some   0, the following theorem holds for
non-decreasing submodular objective functions:
Theorem 2. (Krause & Guestrin, 2005) Let J() be a non-decreasing submodular function
 such that
and S  = maxS:c(S)B J(S  ) be the cost constrained optimizer of J. For any J()

|J(S)  J(S)|
  for all S, if Algorithm 1 is run using J in place of J, then the returned
set S satisfies the following approximation bound, where cmin = mini ci :




1 e1
1 c(S  )


J(S) 
J(S ) 
+ |S | .
(5)
2
e
2 cmin

128

fiBudgeted Optimization with Constrained Experiments

5. Sequential Approach
In this section, we consider the sequential setting (Deshpande et al., 2004; Silberstein et al.,
2006) where each query is selected one at a time after the result for the previous query
becomes available. This is the most commonly studied setting for Bayesian Optimization
and is appropriate for many applications where there is only a single experimental facility
to process the queries.
The inputs to our problem remain the same, which include B, the total budget, fc ()
the cost function, px (x|Q), the distribution of the constructed experiment x given query Q,
and D, the set of observed experiments. In the sequential setting, given the inputs we must
request a sequence (one at a time) of constrained experiments whose total cost is within
the budget.
Leveraging the extensive body of research on traditional Bayesian Optimization, we design our sequential selection policies by extending a number of well-established myopic sequential selection policies from the literature. Most existing policies for traditional Bayesian
Optimization can be viewed as defining a greedy heuristic that assigns a score to each candidate experiment x based on the current experimental state, which we denote by (Dc , Bc ),
where Dc represent the current set of prior experiments, and Bc represents the current
remaining budget. As reviewed in Section 2, many of the existing heuristics have been observed to perform well for traditional Bayesian Optimization problems. Unfortunately they
cannot be directly used for our problem since they select individual specific experiments
rather than constrained experiments, as we require.
5.1 Model-Free Policies
Model-free policies do not consider statistical models of the data in making the selection. In
this work we consider two model-free policies, Round Robin (RR) and Biased Round Robin
(BRR), which are motivated by previous work on budgeted multi-armed bandit problems
(Lizotte et al., 2003; Madani et al., 2004).
5.1.1 Round Robin (RR)
In the multi-armed bandit setting, the RR policy seeks to keep the number of pulls of each
arm as uniform as possible. In the context of constrained experiments, we apply the same
principle to keep the experiments as uniformly distributed as possible in the experimental
space X . Given the current experimental state (Dc , Bc ), we define the RR policy to return
the largest hyper-rectangle or the least costly query Q that does not contain any previous
experiment in Dc . If the cost Q exceeds the current budget Bc , we return the constrained
experiment with cost Bc that contains the fewest experiments in Dc . Ties are broken
randomly. Note that in RR the outputs of previous queries do not have any effect in
selecting the next queries. However, the exact location of the previous experiments do have
a significant effect in the next query selection. Therefore, we can not consider RR as a
non-sequential approach.
129

fiAzimi, Fern, & Fern

5.1.2 Biased Round Robin (BRR)
BRR policy behaves identically to RR, except that it repeats the previously selected constrained experiment as long as the outcome of the constrained experiment has improved the
performance and it does not exceed the budget. In particular, given the current experimental state (Dc , Bc ), the query Q is repeated as long as it results in an outcome that improves
over the current best outcome in the set Dc , and fc (Q)  Bc . Otherwise, the RR policy is
followed. This policy is analogous to BRR in multi-armed bandit problems (Madani et al.,
2004) where an arm is pulled as long as it has a positive outcome.
5.2 Model-Based Policies
For model-based policies, it is assumed that a conditional posterior distribution p(y|Dc , x)
over the outcome y of each individual experiment x  Q is learned from the set of currently
observed experiments Dc . Existing model-based myopic policies for traditional experimental
design typically select the experiment x that maximizes certain heuristics computed from
statistics of the posterior (Jones, 2001). The heuristics provide different mechanisms for
trading off between exploration (probing unexplored regions of the experimental space) and
exploitation (probing areas that appear promising) given Dc . Note that the experiment x
is a fixed and known point in the experimental design literature before running the real
experiment in the lab since it is assumed that we can ask for a particular fixed point.
However, in our constrained experiment application, we ask for a hyper-rectangle query
Q rather than a fixed experiment point x. Therefore the conditional posterior distribution
for each constrained experiment Q is defined as p(y|Q, Dc ) , Ex|Q [p(y|x, Dc )]. This definition corresponds to the process of drawing an experiment x from Q and then drawing an
outcome for x from p(y|x, Dc ). p() effectively allows us to treat constrained experiments as
if they were individual experiments in a traditional optimization problem. Thus, we can define heuristics for constrained experiments by computing the same statistics of the posterior
p(), as used in traditional optimization. In this work we consider four such heuristics.
5.2.1 Maximum Mean (MM)
In the context of traditional optimization with individual experiments, the MM heuristic,
also known as PMAX (Moore & Schneider, 1995; Moore et al., 1998; Schneider & Moore,
2002), simply selects the experiment that has the largest expected outcome according to the
current posterior. In our constrained experiments, the MM heuristic computes the expected
outcome of a given query according to the current posterior p(). The MM of any arbitrary
query Q is computed as follows:

MM(Q|Dc ) = E [y|Q, Dc ] , where y  p(y|Q, Dc ).

(6)

MM is purely an exploitative heuristic and has the weakness that it can often be too
greedy and get stuck in a poor local maximum point before exploring the rest of the experimental space.
130

fiBudgeted Optimization with Constrained Experiments

5.2.2 Maximum Upper Interval (MUI)
The MUI heuristic, also known as IEMAX in previous work (Moore & Schneider, 1995;
Moore et al., 1998; Schneider & Moore, 2002), attempts to overcome the greedy nature of
MM by exploring areas with non-trivial probability of achieving a good result as measured
by the upper bound of the 95% confidence interval of output prediction. Thus, the MUI
heuristic for any arbitrary constrained experiments Q is calculated as follow (assuming that
Gaussian process is used for estimating the posterior distribution of f ()):
p
(7)
MUI(Q|Dc ) = E [y|Q, Dc ] + 1.96 Var [y|Q, Dc ], where y  p(y|Q, Dc ).
Intuitively, MUI will aggressively explore untouched regions of the experimental space
since the outcomes in such regions will have high posterior variance. However, as experimentation continues for a long time and uncertainty decreases, MUI will focus on the most
promising areas and behaves like MM. Note that MUI is a specific case of a more general
heuristic GP-UCB (Cox & John, 1992, 1997), where the constant 1.96 is replaced by a
varying parameter that results in certain theoretical guarantees. Empirically GP-UCB has
been observed to perform comparatively to the MEI heuristic which we introduce later in
this section.
5.2.3 Maximum Probability of Improvement (MPI)
In the context of individual experiments, the MPI heuristic corresponds to selecting the
experiment that has the highest probability of generating an outcome y that outperforms
the best current outcome in Dc . An issue with the basic MPI strategy is that it has a
tendency to behave similar to MM and focuses on the areas that currently look promising,
rather than exploring unknown areas. The reason for this behavior is that the basic MPI
does not take into consideration the amount of improvement over the current best outcome.
In particular, it is typical for the posterior to assign small amounts of variances to the
outcomes in well explored regions. It means while there might be a good probability of
observing a small amounts of improvement, the probability of a substantial improvement
is small. Hence, it is common to consider the use of a margin  when using MPI, which
we will refer to as MPI(). Let yc represent the current best outcome that was observed
in Dc , then MPI (Q|Dc ) is equal to the probability that the outcome of the constrained
experiment Q will be greater than ((1 + )yc ) (assuming non-negative yc values). The MPI
heuristic is given by:
MPI (Q|Dc ) = p (y  (1 + )yc |Q, Dc ) ,

where y  p(y|Q, Dc ).

(8)

The MPI() heuristic is sensitive to the  margin parameter. Adjusting the margin 
from small to large causes the heuristic to change its behavior from more exploitive to more
explorative.
5.2.4 Maximum Expected Improvement (MEI)
The maximum expected improvement (Locatelli, 1997) heuristic seeks to improve on the
basic MPI heuristic without requiring a margin parameter . Rather than focus on the
probability of improvement, it considers the expected amount of improvement according to
131

fiAzimi, Fern, & Fern

the current posterior. In particular let I(y, yc ) = max(y  yc , 0). Then, the MEI heuristic
is defined as:
MEI(Q|Dc ) = Ey [I(y, yc )|Dc , x] , where y  p(y|Q, Dc ).

(9)

5.3 Cost-Sensitive Policies
The above introduced sequential heuristics do not take the variable cost of a constrained
experiment into account. If only the heuristic value is used as our selection criterion, the
most costly constrained experiment might be selected. In fact, the nature of our heuristics
will typically assign the highest score to the constrained experiments that are maximally
constrained and centered around the individual experiment that maximizes the heuristic.
Unfortunately, such constrained experiments are also maximally costly. More generally,
assume that the cost of a constrained experiment Q monotonically decreases with the size
of its region of support, which is the most natural case to consider. It is easy to show
that for all of our heuristics, the value of a constrained experiment Q is monotonically
non-decreasing with respect to the cost of Q. This is true when reducing the size of a
constrained experiment would remove the points which have the heuristic values less than
the constrained experiment value.
Thus, maximizing the above defined heuristics leads to the selection of the most costly
experiments, which might consume more budget than is warranted. This suggests that
there is a fundamental trade-off between the heuristic values and the cost of the constrained
experiments that we must address. Below, we introduce two approaches that attempt to
address this trade off by defining cost-sensitive policies from the cost insensitive heuristics.
5.3.1 Cost Normalized (CN) Policies
Cost normalized policies have been widely used in budgetd optimization settings where the
costs are non-uniform across experiments, (e.g., see Krause et al., 2008; Snoek, Larochelle,
& Adams, 2012). It simply selects the constrained experiment that achieves the highest
expected improvement per unit cost, or rate of the improvement.
Suppose H is our heuristic. We can define a corresponding CN policy for any heuristic
on constrained experiment Q given the current experimental state {Dc , Bc } as follows:


H(Q|Dc )
CNH (Dc , Bc ) = argmax
,
(10)
fc (Q)
Q:fc (Q)<Bc
where H(Q|Dc ) assigns a score to constrained experiment Q given a set of observed experiments Dc .
This cost normalization approach is a natural baseline and has been suggested in the
context of other optimization problems (e.g., Krause et al., 2008). However, in most such
prior work, the actual empirical evaluations involved uniform cost models and thus there
is little empirical data regarding the performance of normalization. In our setting of constrained experiments, a uniform cost model is not an option, since selecting among constrained experiments of varying variable cost is a fundamental aspect of the problem. Thus,
our empirical evaluation, in Section 6, necessarily provides a substantial evaluation of this
normalization principle.
132

fiBudgeted Optimization with Constrained Experiments

Unfortunately, experimental results show that the proposed cost normalized approach
can be outperformed by random policy in some cases. This prompts us to introduce a
constrained minimum cost(CMC) approach which will only select a constrained experiment
if it is expected to perform better than a random policy when spending the same amount
of budget.
5.3.2 Constrained Minimum Cost (CMC) Policies
For any heuristic on constrained experiments H(Q|Dc ), which assigns a score to constrained
experiment Q given a set of observed experiments Dc , we can define an associated CMC
policy. The principle behind CMC is to select the least cost constrained experiment that
satisfies the following two conditions:
 Condition 1: It approximately optimizes the heuristic value,
 Condition 2: It has an expected improvement (EI) that is no worse than the random
policy after spending the same amount of budget.
The first condition encourages the selection of constrained experiments that look promising according to H, but it might result in the selection of an overly costly experiment. The
second condition helps to place a limit on how much we are willing to pay to achieve a
good heuristic value. Specifically, we will only be willing to pay a cost of c for a single
constrained experiment Q if its expected improvement is no worse than that achieved by a
set of random experiments whose total cost is c.
To formalize this policy, we first make the notion of approximately optimize more precise
by introducing a parameterized version of the CMC policy and then show how the parameter
will be automatically selected via condition 2. For a given heuristic H, let h be the value
of the highest scoring constrained experiment that fits within the current budget. Note
that this will necessarily be one of the most constrained (i.e. most expensive) experiments
within the budget. For a given parameter   [0, 1], the CMCH, policy selects the leastcost constrained experiment that achieves a heuristic value of at least   h . Formally, this
is defined as
(11)
CMCH, (Dc , Bc ) = argmin {fc (Q) | H(Q|Dc )  h }
Q:fc (Q)Bc

The value of  controls the trade off between the cost of Q and its heuristic value.
Smaller/larger values of  will result in less/more costly experiments, but smaller/larger
heuristic values. In our preliminary work, we experimented with the CMCH, policy and
found that it was difficult to select a value of  that worked well across a wide range of
optimization problems, cost structures, and budgets. This motivated the introduction of
condition 2 above to help us adaptively select an appropriate value of  at each decision
point.
We now formalize the CMC class of policies. The objective is to select the largest
value of  such that the experiment suggested by CMCH, satisfies condition 2. This will
guarantee that the selected constrained experiment will: 1) achieve a heuristic value as close
as possible to h , and 2) outperforms the random policy given the same cost allocation. In
the following, we define EIR(Dc , C) to be the expected improvement of a set of random
133

fiAzimi, Fern, & Fern

experiments that have a total cost of C and Q to be the constrained experiment returned
by CMCH, . Also, let EI(Q ) be the expected improvement of constraint experiment Q
and c be its cost. Our parameter-free CMC policy is now defined as:
CMCH (Dc , Bc ) = CMCH, (Dc , Bc )


 = arg max{  [0, 1] | EI(Q |Dc )  EIR(Dc , dc e)}.

(12)

In practice we compute EIR(Dc , C) and EI(Q ) via Monte Carlo simulation. This is a
straightforward process in both cases. For EIR to compute one EIR sample, we randomly
select a set of experiments within the budget C and then sample outcomes for those experiments via the Gaussian Process conditioned on Dc . The improvement of the best outcome
is taken to be the result of the EIR sample. The estimate of EIR is the average of L
EIR samples. A similar process is used for EI, except that rather than drawing random
experiments for each sample we use the experiments in Q .
The following steps summarize the overall computational process of CMC-MEI.
1. Compute h by maximizing H over constrained experiments that fall within the current budget. Note that this only requires optimizing over the set of minimum-sized
constrained experiments that have a cost less than the budget.
2. Perform a discretized line search to find  according to Equation 12.
3. Return CMCH, (Dc , Bc ) according to Equation 11.

6. Experimental Results
Our goal is to evaluate the performance of the proposed policies in scenarios that resemble typical real-world scientific applications. In particular, the experimental domains that
motivate our work in this paper focus on low-dimensional optimization problems. This
choice is based on two reasons. First, with typical budgets the number of total experiments
is often limited, which makes optimizing over many dimensions impractical. In practice,
the scientists often have to carefully select a few key dimensions to consider. Second, in
real-world applications, such as our motivating problem, it is prohibitively difficult to satisfy constraints on more than a couple of experimental variables. Thus, the most relevant
scenarios for us and many other problems are moderate numbers of experiments and small
dimensionality.
6.1 Experimental Setup
Below we describe the set up of our experiments.
6.1.1 Test Functions
We evaluate our policies using five 2-dimensional functions in [0, 1]2 . The first three functions: Cosines, Rosenbrock, and Discontinuous are benchmarks that have been widely used
in previous studies on stochastic optimization (Anderson, Moore, & Cohn, 2000; Brunato,
Battiti, & Pasupuleti, 2006; Azimi et al., 2010). The mathematical expressions of the
functions are listed in Table 1 and their contour plots are given in Figure 1.
134

fiBudgeted Optimization with Constrained Experiments

Cosines

Rosenbrock

Hydrogen

Fuel Cell

Discontinuous
Figure 1: Contour plots of the test functions.
135

fiAzimi, Fern, & Fern

Table 1: Benchmark Functions.
Function
Cosines
Rosenbrock
Discontinuous

Mathematical representation
1  (u2 + v 2  0.3cos(3u)  0.3 cos(3v)),

u = 1.6x  0.5, v = 1.6y  0.5

10  100(y  x2 )2  (1  x)2
1  2((x  0.5)2 + (y  0.5)2 ) if x < 0.5,

0 otherwise

The two remaining functions are derived from real-world experimental data sets involving hydrogen production and our motivating fuel cell application. For the former we
utilize data collected as part of a study on biosolar hydrogen production (Burrows, Wong,
Fern, Chaplen, & Ely, 2009), where the goal was to maximize hydrogen production of the
cyanobacteria Synechocystis sp. PCC 6803 by optimizing the pH and Nitrogen levels of the
growth media. The data set contains 66 samples uniformly distributed over the 2-d input
space. This data is used to simulate the true function by fitting a Gaussian Process (GP)
model with RBF kernel, picking kernel parameters via standard validation techniques such
as cross validation. With this model we then simulated the experimental design process by
sampling from the posterior of this GP to obtain noisy outcomes for requested experiments.
For this purpose, we used a zero-mean Gaussian noise model with variance equal to 0.01.
See Figure 1 for the contour plot.
For our motivating application (described in the introduction), we utilize data from a
set of initial microbial fuel cell experiments using anodes with different nano-enhancements.
In particular, each anode was coated with gold nano-particles under different fabrication
conditions leading to varying particle densities, shapes, and sizes. The construction of each
anode required approximately two days.1 Each anode was then installed in a microbial fuel
cell and run using pure Shewanella oneidensis bacterial cultures grown in fed-batch mode for
one week while recording the current density at regular intervals. The temporally averaged
current density was taken to be the dependent variable to be optimized by modifying the
nano-structure. To characterize the nano-structure on each anode, we captured images using
scanning electron microscopy and used standard image processing software to compute two
features: average area of individual particles, and average circularity of individual particles.
Those features were selected to be the independent variables of the design since they can
be roughly controlled during the fabrication process and appear to influence the current
density. Unfortunately, due to the high cost of running these experiments, which is precisely
the motivation for this paper, our data set currently consists of just 16 data points, which
are relatively uniformly distributed over the experimental space. Due to the sparse data,
we utilize polynomial Bayesian regression with degree 4, rather than Gaussian processes
with RBF kernels, to simulate the true function. See Figure 1 for the contour plot.
1. For this first round of experiments no constraints were provided to the scientist constructing the anodes.
Rather the goal was to generate a diverse set of anodes to provide a good set of data for seeding the
experimental design process. The construction time would likely be more than two days in the presence
of constraints since a number of growth conditions and trials would be necessary.

136

fiBudgeted Optimization with Constrained Experiments

6.1.2 Model Definitions
In this work, we assume that the density px (x|Q) over experiments given a query Q is
uniform over the ranges specified by Q.
To compute the conditional posterior p(y|x, D) required by our non-sequential approach
and model-based sequential heuristics, we use Gaussian process (Rasmussen & Williams,
2006) with zero mean prior and covariance specified by RBF kernel function:
1
|xi  xj |2 ),
(13)
2
where  is the length scale parameter that can be considered as the distance we have to
move in the input space before the function value changes significantly, and f is the signal
variance which specifies the maximum possible variance at each point. In this paper we
2
set  = 0.02 and signal variance f = ymax
, where ymax is an upper bound on the output
values (this is typically easy to elicit from scientists and serves to set our prior uncertainty
so that there is non-trivial probability assigned to the expected range of the output). We did
not optimize these parameters, but did empirically verify that the GP behaved reasonably
for our test functions.
cov (xi , xj ) = k(xi , xj ) = f exp(

6.1.3 Cost Function
In our motivating application, the cost of setting up and running a fuel cell experiment
given a constrained experiment request can be roughly considered to have two components.
The first component corresponds to the cost of setting up an experiment (producing a
nano-structure) that satisfies the given constraints, which is variable depending on the
size of the constraints. The tighter the constraints, the more costly they will be. The
second component corresponds to the cost of running the constrained experiment, which
is generally constant. This two-component cost structure is very common in real-world
applications where a portion of the experimental process can be controlled more precisely
and has uniform costs across different queries, while other portions are less controllable and
have a cost that is inversely proportional to the size of the constraints we place on them.
To capture this structure, we define the following cost function fc () : Q  <+ :
d
Y
slope
fc (Q) = 1 +
.
(bi  ai )

(14)

i=1

In this formulation, the constant of one captures the stationary part of the cost, and
the second term captures the variable portion that is inversely proportional to the size
of the constrains of query Q. The value of the slope parameter dictates how quickly the
cost increases as the size of a constrained experiment decreases. We evaluate our proposed
approaches considering three different slope values; slope = 0.1, 0.15, 0.30. Note that all of
our proposed approaches can be readily applied to other cost functions.
6.1.4 Discretizing the Input Space
As mentioned previously, our policies assume that the input space is discretized. In particular, we divide each input dimension into 100 equal-length subintervals. Note that this
137

fiAzimi, Fern, & Fern

implementation is most appropriate for low dimensional optimization problems, which as
described previously is the situation we often encounter in real-world applications.
6.1.5 Evaluation Settings
In our evaluation, we test all of the proposed policies in comparison to a random policy
(i.e., a policy that always selects the entire input space as the constrained experiment).
Given a budget B and a function f () to be optimized, each run of a policy results in a
set of observed experiments Dc . Let x be the experiment in Dc that is predicted to have
the maximum expected outcome y  . The regret of the policy for a particular run is then
defined to be ymax  y  , where ymax is the maximum value of f (). For each test function
and choice of budget and cost structure (i.e., choice of slope), we evaluate each policy by
averaging the regret over 200 runs. Each run starts with n = 5 randomly selected initial
points D = {(x0 , y0 ), , , (x5 , y5 )}, and then the policies are used to select constrained
experiments until the budget runs out, at which point the regret is measured. In order to
ease the comparison of the regret values across different functions, we report normalized
regret values, which are computed by dividing the regret of each policy by the mean regret
achieved by the random policy. A normalized regret less than one indicates that an approach
outperforms random, while a value greater than one indicates that an approach is worse
than random. In the first round of experiments, we fixed the total budget to B = 15 and
examine the effect of the cost-model slope parameter over values 0.1, 0.15 and 0.3. In later
experiments, we will consider larger budgets.
Note that our non-sequential policy can be used to consume all the experimental budget
at once. However, in practice there is typically a limit on the number of constrained
experiments that can be processed simultaneously due to limited resources. As such, in the
non-sequential setting our policy is used to select up to five simultaneous queries subject to
the budget constraint. We will repeat this process until the budget is consumed.
The run time for selecting a single experiment in the sequential setting is on the order
of minutes (generally under five) in our experiments with an un-optimized matlab implementation. The run time for selecting a batch of five or fewer queries was never more than
30 minutes.
6.2 Results and Discussions
Our results for individual functions are shown in Table 2, and their corresponding standard
deviations are shown inside the parentheses. The first row of each table presents the results
of our non-sequential greedy algorithm (NS-Greedy). Rows 2 to 6 show the performance of
the model-based sequential policies for both CMC and CN cost policies. Note that, for the
CN policy, we report the results of CN-MEI, as it performed the best among all CN policies.
In addition, it has a nice interpretation as maximizing the rate of expected improvement per
unit cost. Finally, the last row shows the performance of our model-free sequential policies.
In order to provide an assessment of the overall performance of different methods, Table 3
presents the normalized regrets for each policy averaged across our five functions. The
different columns of the table correspond to different slope values for the cost function.
Below we discuss the results of different methods in detail.
138

fiBudgeted Optimization with Constrained Experiments

Table 2: Normalized regrets on individual functions for varying cost models (i.e., slopes)
slope = 0.1
slope = 0.15
slope = 0.30
Cosines, Normalized Regret (95% Confidence Interval)
NS-Greedy
0.767 (0.04)
0.838 (0.05)
0.841 (0.05)
CN-MEI
0.569 (0.05)
0.714 (0.06)
0.826 (0.06)
CMC-MEI
0.417 (0.04)
0.514 (0.06)
0.794 (0.06)
CMC-MPI(0.2)
0.535 (0.05)
0.584 (0.06)
0.616 (0.06)
CMC-MUI
0.797 (0.06)
0.804 (0.06)
0.817 (0.06)
CMC-MM
0.708 (0.07)
0.767 (0.07)
0.736 (0.06)
RR/BRR
0.84(0.06)/0.83(0.06)0.86(0.06)/0.86(0.06)0.89(0.06)/0.88(0.06)
Discontinuous, Normalized Regret (95% Confidence Interval)
NS-Greedy
0.528 (0.06)
0.690 (0.06)
0.748 (0.05)
CN-MEI
0.527 (0.06)
0.497 (0.06)
0.626 (0.08)
CMC-MEI
0.564 (0.06)
0.677 (0.08)
0.779 (0.09)
0.954 (0.11)
0.940 (0.10)
0.951 (0.11)
CMC-MPI(0.2)
CMC-MUI
0.710 (0.10)
0.709 (0.11)
0.693 (0.09)
CMC-MM
1.289 (0.15)
1.225 (0.16)
1.116 (0.16)
RR/BRR
0.60(0.07)/0.58(0.07)0.61(0.07)/0.60(0.07)0.63(0.08)/0.63(0.08)
Rosenbrock, Normalized Regret (95% Confidence Interval)
NS-Greedy
0.650 (0.05)
0.877 (0.06)
0.930 (0.06)
CN-MEI
0.602 (0.06)
0.665 (0.07)
0.736 (0.08)
CMC-MEI
0.547 (0.35)
0.556 (0.39)
0.630 (0.47)
CMC-MPI(0.2)
0.503 (0.05)
0.594 (0.06)
0.608 (0.07)
CMC-MUI
0.805 (0.11)
0.974 (0.16)
0.913 (0.14)
CMC-MM
0.721 (0.09)
0.740 (0.10)
0.662 (0.08)
RR/BRR
0.89(0.12)/0.88(0.12)0.93(0.12)/0.92(0.12)0.96(0.14)/0.95(0.14)
Hydrogen, Normalized Regret (95% Confidence Interval)
NS-Greedy
0.879 (0.06)
0.969 (0.08)
0.993 (0.09)
CN-MEI
0.176 (0.04)
0.354 (0.06)
0.852 (0.09)
CMC-MEI
0.129 (0.04)
0.233 (0.06)
0.420 (0.07)
CMC-MPI(0.2)
0.408 (0.09)
0.449 (0.10)
0.613 (0.10)
CMC-MUI
0.716 (0.08)
0.695 (0.08)
0.868 (0.09)
0.728 (0.11)
0.605 (0.10)
0.691 (0.11)
CMC-MM
RR/BRR
1.10(0.09)/1.06(0.09) 1.16(0.10)/1.23(0.10) 1.17(0.09)/1.14(0.09)
Fuel Cell, Normalized Regret (95% Confidence Interval)
NS-Greedy
0.980 (0.02)
0.985 (0.02)
0.995 (0.03)
CN-MEI
0.929 (0.02)
0.950 (0.02)
0.986 (0.03)
CMC-MEI
0.931 (0.02)
0.908 (0.02)
0.940 (0.02)
CMC-MPI(0.2)
0.932 (0.02)
0.930 (0.03)
0.943 (0.03)
CMC-MUI
0.971 (0.03)
0.973 (0.03)
0.995 (0.03)
CMC-MM
0.945 (0.03)
0.963 (0.04)
0.963 (0.05)
RR/BRR
1.03(0.03)/1.02(0.03)1.04(0.03)/1.04(0.03)1.04(0.03)/1.04(0.03)

139

fiAzimi, Fern, & Fern

Table 3: Normalized Overall Regrets.
slope = 0.1 slope = 0.15 slope = 0.30
NS-Greedy
0.760
0.871
0.901
CN-MEI
0.560
0.636
0.805
CMC-MEI
0.517
0.578
0.712
CMC-MPI(0.2)
0.666
0.698
0.746
CMC-MUI
0.800
0.831
0.857
CMC-MM
0.874
0.889
0.834
RR
0.897
0.925
0.944
BR
0.879
0.911
0.934

6.2.1 Non-Sequential
We will first examine the performance of our non-sequential greedy policy (NS-Greedy).
Recall that we present the normalized regret in our results, thus smaller value indicates
better performance. Further, a policy outperforms random whenever its normalized regret
is less than 1.
From Table 2, we observe that the proposed greedy algorithm (NS-Greedy) performs
consistently better than the random policy for all functions. Among these functions, it can
be seen that the performance advantage of NS-Greedy is more significant when the slope
parameter of the cost function is smaller. This is consistent with our expectation: with a
smaller slope, the cost of our query grows slower as we tighten the constraints. This will
allow our algorithm to more aggressively select tighter constraints based on the posterior
model of the function. In fact, if the slope is large enough, one would expect the optimal
policy to be completely random.
Comparing with sequential approaches, we first observe that NS-Greedy compared favorably to the two model-free methods. This is not surprising because RR/BRR do not
consider the posterior model of the function in selecting queries. On the other hand, we also
observe that the NS-Greedy algorithm performs significantly worse than the best modelbased sequential policies, such as CMC-MEI. This result is expected because sequential
policies allow us to update and improve the model of the function with each query. Therefore, we generally expect sequential policies to perform better than non-sequential methods
which is a common phenomenon in the active learning literature (Azimi, Fern, Fern, Borradaile, & Heeringa, 2012).
6.2.2 Sequential
In this section we examine the performance of the sequential policies, including both modelfree and model-based methods.
Model-Free Policies. From Table 3 we see that RR and BRR achieve an improvement
over random by approximately 10% across the different slopes. This shows that the heuristic
of trying to evenly cover the space pays off compared to random. BRR is also observed
to perform slightly better than RR, which indicates that the additional exploitive behavior
of BRR pays off overall. Looking at the individual results in Table 2, we see that for the
140

fiBudgeted Optimization with Constrained Experiments

Hydrogen and Fuel Cell functions, both BRR and RR perform worse than random. Further
investigation reveals that the reason for this poor performance is that RR/BRR have a bias
toward experiments near the center of the input space. This bias is a result of the fact
that constrained experiments (hyper-rectangles) are required to fall completely within the
experimental space and there are fewer such hyper-rectangles that contain points near the
edges and corners. The Hydrogen and Fuel Cell functions have their optimal points near
corners of the space, explaining the poor performance.
Model-Based Policies. We now focus on the performance of the proposed model-based
sequential policies. From the averaged overall results (Table 3), our first observation is that
the model-based policies in general perform better than the random policy. Specifically,
looking at the results of individual functions, we see that all model-based policies outperform
random, with the exception of CMC-MM on the Discontinuous function. This shows that
the two proposed approaches for considering cost are able to avoid catastrophic choices that
expend the budget more quickly than is warranted.
Our analysis of the poor performance of CMC-MM on the Discontinuous function revealed that CMC-MM would often get stuck in poor local optima and cease to explore the
space adequately. Although at each step the CMC-MM policy determined that its selection
was better than random in the near term, this did not translate to long term improvement
over random due to the lack of exploration. The Discontinuous function is particularly
prone to elicit this behavior due to the fact that it has a large sub-optimal and nearly
uniform region, which is difficult for CMC-MM to escape from. This overly greedy performance is consistent with prior observations of the MM heuristic and is largely addressed by
the other heuristics that provide some measure of exploratory value. In fact, CMC-MM is
highly dependent on its initial given random points. For example, if the initial given points
D have been chosen from a non-optimal region, which is more than 50% of the input space
for the Discontinuous function, the CMC-MM approach cannot give a satisfactory performance. This can be seen by the standard deviation of CMC-MM, which is higher than
other model-based and model-free methods. It shows that the performance of CMC-MM
changes significantly in each iteration which is because of its initial points.
In addition, from Table 3, it can be seen that all of the model-based approaches outperform the model free approaches. This indicates that the heuristics we are considering
and our GP probabilistic model are providing useful information for effectively guiding the
constrained experiment selection.
Comparing different model-based heuristics, we see that the MEI-based methods (CNMEI and CMC-MEI) are the top contenders among all methods. Examining the results
for individual functions, we can see that this holds for all functions except for Rosenbrock,
where the CMC-MPI is slightly better than MEI-based methods. Upon closer examination
of the behavior of the MPI and MEI heuristics, we found that MPI often selects slightly
fewer experiments than MEI, which we believe is due to fact that the MEI heuristic tends
to be smoother than MPI over the experimental space. The smoothness of MEI allows the
CMC policy to select less constrained queries and but still achieve approximately optimal
heuristic value, leading to more constrained experiments. In general we recommend CMCMEI as the most preferable heuristic to use based on its consistently superior performance
and the fact that it is parameter free.
141

fiAzimi, Fern, & Fern

0.5

CMCMEI
CNMEI
CMCMPI
NSGreedy
Random

0.4

Regret

0.4

Regret

0.5

CMCMEI
CNMEI
CMCMPI
NSGreedy
Random

0.3

0.3

0.2
0.2
0.1

0.1
10

20

30

Budget

40

50

0
10

60

20

Cosines

Budget

40

50

60

Rosenbrock

0.35

0.6

CMCMEI
CNMEI
CMCMPI
NSGreedy
Random

0.3
0.25

CMCMEI
CNMEI
CMCMPI
NSGreedy
Random

0.55

Regret

0.2
0.15
0.1

0.5

0.45

0.05
0
10

20

30

Budget

40

50

0.4
10

60

20

30

Hydrogen

Budget

40

50

Fuel Cell
CMCMEI
CNMEI
CMCMPI
NSGreedy
Random

0.1

0.08

Regret

Regret

30

0.06

0.04

0.02
10

20

30

Budget

40

50

60

Discontinuous
Figure 2: Un-normalized regret as a function of the budget (slope=0.1).
142

60

fiBudgeted Optimization with Constrained Experiments

We are also interested in comparing the performance of the two proposed schemes for
handling the cost, namely CN and CMC. Focusing on CMC-MEI and CN-MEI, we can see
that CMC-MEI generally outperforms CN-MEI. While the differences in the behavior of
these two policies appear subtle, experimental investigation show that CN-MEI tends to
be overly conservative toward selecting costly experiments in comparison with CMC-MEI,
especially at the later stages of the experimental process.
6.3 Varying Budget
In this round of experiments, we fixed the cost model slope to 0.1 and varied the budget from
10 to 60 units in increments of 10. We are interested in examining the relative performance
of different model-based policies (including both sequential and non-sequential) compared
to the random policy as we increase the budget.
Figure 2 plots the absolute regret (rather than the normalized regret) versus budget
for the best sequential policies including CMC-MEI, CN-MEI and CMC-MPI, and the
proposed non-sequential policy (NS-Greedy). We have also plotted the performance of
random policy as a reference baseline. We use the same experimental setting as used
previously. Specifically, for sequential methods, in each iteration we select one query until
the budget is completely consumed. For the proposed non-sequential approach, we select
up to five queries in each iteration until the budget is consumed.
First, we observe that the performance of NS-Greedy continues to dominate Random as
we increase the budget. This suggests that the performance advantage of NS-Greedy over
Random is robust to the amount of experimental budget. We also observe that NS-Greedy
is generally outperformed by the lead sequential policies, such as CMC-MEI, and CMCMPI. This is consistent with our previous observations with fixed budget and varying slope.
Finally, we see that polices based on the MEI and MPI heuristics generally achieve the best
performance across a wide range of budgets. In particular, they consistently maintain a
significant advantage over Random. The MEI-based and CMC-MPI policies are roughly
comparable for all functions except for the Fuel Cell function. In that case CMC-MPI
slightly outperforms CMC-MEI for large budgets.
Overall, given the results from the previous experiments, CMC-MEI can still be considered as a recommended method, due to its combination of good performance, smoothness
and robustness. CMC-MEI is also preferable in that it does not require the selection of a
margin parameter.
6.4 Comparison with Precise Experiments
In this section, we compare our performance using constrained experiments to the performance achieved using precisely specified experiments. In particular, we compare CMC-MEI
with its precise counterpart MEI. To do this, we use CMC-MEI to select up to fifteen
constrained experiments (with infinite budget) for each function, and at each step evaluate
the regret. This is repeated for 100 times to generate an average performance curve for
CMC-MEI as a function of the number of constrained experiments. This is done for two
different cost models with slope set to 0.1 and 0.3 respectively, resulting in two curves for
CMC-MEI. Similarly, we use MEI to select a sequence of fifteen precisely specified experiments and generate the same average performance curve (over 100 random runs). Finally,
143

fiAzimi, Fern, & Fern

0.8
0.7

MEI
CMCMEI(0.1)
CMCMEI(0.3)
Random

1

Regret

0.6

Regret

1.5

MEI
CMCMEI(0.1)
CMCMEI(0.3)
Random

0.5
0.4

0.5

0.3
0.2
0.1
1

5

10

0
1

15

# of Experiments

5

Cosines
0.5

MEI
CMCMEI(0.1)
CMCMEI(0.3)
Random

0.7

Regret

0.2

0.6
0.5
0.4

0.1

5

10

1

15

# of Experiments

5

10

# of Experiments

Hydrogen

Fuel Cell

0.2

MEI
CMCMEI(0.1)
CMCMEI(0.3)
Random

0.15

Regret

Regret

0.8

0.3

0
0

15

Rosenbrock

MEI
CMCMEI(0.1)
CMCMEI(0.3)
Random

0.4

10

# of Experiments

0.1

0.05

0
1

5

10

# of Experiments

15

Discontinuous
Figure 3: Un-normalized regret as a function of the number of experiments.
144

15

fiBudgeted Optimization with Constrained Experiments

as a reference point, we also plot the performance when experiments are selected randomly.
Figure 3 shows the performance curves of MEI, CMC-MEI (with slope = 0.1 and 0.3 respectively) and random.
From the figure we can see that in most cases CMC-MEI performed comparably to MEI.
For these cases, we observe no detrimental effective from the use of constrained experiments.
If we compare the efficiency of CMC-MEI with slopes 0.3 and 0.1 (larger slopes yield higher
experimental costs), we see that in most cases they are comparable. However, for the Fuel
Cell and Hydrogen functions, the smaller slope is consistently better (by a small margin).
Further, these are also the two functions where precise experiments show the most significant
advantage over CMC-MEI (in particular for slope =0.3). A likely explanation for this is
that the optimal regions for these two functions are fairly small, highly peaked and near the
boundaries. This can make it difficult to effectively explore this region using constrained
experiments, especially with larger slopes.
6.5 Comparison with Constant Window Experiments
In our final experiments, we compare the performance of the CMC-MEI approach with a
Constant Window (CW) approach, where constant constraint sizes are used throughout the
optimization process. The goal is to understand the importance of dynamically selecting
the window size as done by CMC-MEI. We consider three different window sizes, denoted
by CW5, CW20, CW50, which correspond to constraint sizes of 5%, 20% and 50% in each
dimension respectively. Thus, the cost of CW5 is significantly more than CW50 while it
has more precision and control over the final selected samples. We compare these CW
approaches against CMC-MEI. The cost model parameter is set to slope = 0.1 and the
budget is varied from 10 to 60 in denomination of 10. The results are provided in Figure 4.
First, we observe that the best performing CW approach varies significantly across
benchmarks and budgets. This indicates that choosing the window size for a particular
application is non-trivial. Second, we see that CMC-MEI, which adaptively selects the
window size, performs the best or is competitive with the best CW approach. This is
another indication that CMC-MEI is an effective strategy for choosing window sizes. Further
analysis of these experiments indicates that CMC-MEI tends to select experiments close to
CW50 at the beginning and then decreases the window size after several experiments.

7. Summary and Future Directions
Motivated by a real-world application, this paper introduced a novel framework for budgeted Bayesian optimization with constrained experiments. In this framework, instead of
asking for samples of the unknown function at precisely specified inputs, we ask for a constrained experiment and the cost of the constrained experiments is variable depending on
the tightness of the constraints. We studied this problem in two different settings.
In the non-sequential setting, multiple constrained experiments are selected at once. For
this setting, we introduced a non-decreasing submodular objective function and presented a
greedy algorithm for approximately optimizing the proposed objective. Empirical evaluation
indicates that the proposed non-sequential algorithm consistently outperforms a baseline
random policy across different budget and cost configurations.
145

fiAzimi, Fern, & Fern

0.6

0.4

CMCMEI
CW 5
CW 20
CW 50

0.3

0.4

Regret

Regret

0.5

0.3

0.2

0.1

0.2

0.1
10

CMCMEI
CW 5
CW 20
CW 50

20

30

40

50

0
10

60

20

30

Budget

40

50

60

Budget

Cosines

Rosenbrock

0.25
CMCMEI
CW 5
CW 20
CW 50

Regret

0.15

0.1

0.6

0.5
0.05

0
10

20

30

40

50

0.4
10

60

20

30

40

50

Budget

Budget

Hydrogen

Fuel Cell

0.12
CMCMEI
CW 5
CW 20
CW 50

0.1
0.08

Regret

Regret

0.2

CMCMEI
CW 5
CW 20
CW 50

0.7

0.06
0.04
0.02
0
10

20

30

40

50

60

Budget

Discontinuous
Figure 4: CMC-MEI performance versus constant window size experiments.
146

60

fiBudgeted Optimization with Constrained Experiments

In the sequential setting of the problem, one constrained experiment is selected in each
iteration. We extended a number of classic Bayesian optimization and experiment design
heuristics for constrained experiments. Direct use of such heuristics to select constrained
experiments will select overly tight constraints and consume all of the budget at once. Thus,
we introduced two general cost policies, namely CN and CMC, to achieve a balance between
moderating the cost of the experiments and optimizing the heuristics. The experiments show
that sequential policies generally outperform the non-sequential policy, and the proposed CN
and CMC cost policies are effective at dispensing the budget rationally. Overall we found
that CMC used with the MEI heuristic (CMC-MEI) demonstrated robust performance and
is parameter-free, making it a recommended method.
The work described here focused on methods for optimizing low-dimensional functions,
which is typical of the types of scientific and engineering applications that motivated this
work. Extending our methods to higher dimensions requires optimizing the selection criteria
over continuous rather than discrete input spaces. There are a number of straightforward
approaches to doing this and future work could include evaluating those approaches and
designing more sophisticated ones. The most interesting direction for future work is to
continue enriching the cost and action models supported by Bayesian Optimization methods
to more closely match the needs of real-world applications. Solutions for these extended
models will require a tighter integration of planning and scheduling techniques with the
ideas developed so far for traditional Bayesian Optimization.

Acknowledgments
This research was supported by NSF grant IIS 1320943.
Appendix A. Proof of Lemma 1
Lemma 1. Let XM = {x1 , . . . , xM } denote the random variables representing the outcome
of the random draw for QM = {Q1 , ..., QM } respectively where QM is the set of all possible
queries. For any given XM , JXM (S), which returns the expected maximum over a set
of jointly distributed random variables, is a monotonically non-decreasing submodular set
function.

Proof. Suppose S is a finite set. Then g : 2S  R+ is a submodular set function if for all
S1  S2  S and x  S \ S2 , it holds that g(S1  {x})  g(S1 )  g(S2  {x})  g(S2 ). In
addition a set function g() is called monotonically non-decreasing if g(S1 )  g(S2 ).
We first prove that E[max()] is monotonic function and then we show that it is a
submodular objective function.
Assume that S1 = {x1 , x2 ,   , xp } with p  k. We need to prove that
fi i
fi i
h
h
fi
fi
E max (y1 , y2 ,   , yp ,   , yk ) fiD  E max (y1 , y2 ,   , yp ) fiD .
147

(15)

fiAzimi, Fern, & Fern

We use the definition of the expectation to prove the result.

fi i
fi
E max (y1 , y2 ,   , yp ,   , yk ) fiD
Z
Z
=    max (y1 , y2 ,   , yp ,   , yk ) py1 ,y2 ,,yp ,,yk |D dy1 dy2    dyp    dyk
Z
Z
    max (y1 , y2 ,   , yp ) py1 ,y2 ,,yp ,,yk |D dy1 dy2    dyp    dyk
Z

Z
Z
Z
=    max (y1 , y2 ,   , yp )
   py1 ,y2 ,,yp ,,yk |D dyp+1    dyk dy1 dy2    dyp
Z
Z
=    max (y1 , y2 ,   , yp ) py1 ,y2 ,,yp |D dy1 dy2    dyp
fi i
h
fi
= E max (y1 , y2 ,   , yp ) fiD .
(16)
h

This shows that E[max()] is a nondecreasing monotonic function.
To prove the submodularity property, We need to show that

fi i
fi i
h
h
fi
fi
E max (y1 , y2 ,   , yp , y  ) fiD  E max (y1 , y2 ,   , yp ) fiD
fi i
fi i
h
h
fi
fi
 E max (y1 , y2 ,   , yp ,   , yk , y  ) fiD  E max (y1 , y2 ,   , yp ,   , yk ) fiD .

(17)

To prove this, we start from the right hand side of the inequality and the basic definition
of the expectation.

fi i
fi i
h
h
fi
fi
E max (y1 , y2 ,   , yp ,   , yk , y  ) fiD  E max (y1 , y2 ,   , yp ,   , yk ) fiD
Z
Z
=    max (y1 , y2 ,   , yp ,   , yk , y  ) py1 ,y2 ,,yp ,,yk ,y |D dy1 dy2    dyp    dyk dy 
Z
Z
    max (y1 , y2 ,   , yp ,   , yk ) py1 ,y2 ,,yp ,,yk |D dy1 dy2    dyp    dyk
Z
Z
=    max (y1 , y2 ,   , yp ,   , yk , y  ) py1 ,y2 ,,yp ,,yk ,y |D dy1 dy2    dyp    dyk dy 
Z
Z
    max (y1 , y2 ,   , yp ,   , yk ) py1 ,y2 ,,yp ,,yk ,y |D dy1 dy2    dyp    dyk dy 
Z
Z
=    [max (y1 , y2 ,   , yp ,   , yk , y  )  max (y1 , y2 ,   , yp ,   , yk )]
(18)
148

fiBudgeted Optimization with Constrained Experiments

py1 ,y2 ,,yp ,,yk ,y |D dy1 dy2    dyp    dyk dy 
Z


Z


[max (y1 , y2 ,   , yp , y  )  max (y1 , y2 ,   , yp )]
py1 ,y2 ,,yp ,,yk ,y |D dy1 dy2    dyp    dyk dy 

Z

Z


=
Z

[max (y1 , y2 ,   , yp , y  )  max (y1 , y2 ,   , yp )] py1 ,y2 ,,yp ,y |D dy1 dy2    dyp dy 

Z

max (y1 , y2 ,   , yp , y  ) py1 ,y2 ,,yp ,y |D dy1 dy2    dyp dy 
Z
Z
    max (y1 , y2 ,   , yp ) py1 ,y2 ,,yp |D dy1 dy2    dyp
fi i
fi i
h
h
fi
fi
= E max (y1 , y2 ,   , yp , y  ) fiD  E max (y1 , y2 ,   , yp ) fiD

=



(19)
Notice that the inequality holds if we can prove:
max (y1 , y2 ,   , yp ,   , yk , y  )  max (y1 , y2 ,   , yp ,   , yk )
 max (y1 , y2 ,   , yp , y  )  max (y1 , y2 ,   , yp )

(20)

There are two possible cases as follows:
(


max (y1 , y2 ,   , yp ,   , yk , y ) =

y
max (y1 , y2 ,   , yp ,   , yk ) .

(21)

1. In the first case, if max (y1 , y2 ,   , yp ,   , yk , y  ) = y  ,
then we also have max (y1 , y2 ,   , yp , y  ) = y  . Hence,
max (y1 , y2 ,   , yp ,   , yk , y  )  max (y1 , y2 ,   , yp ,   , yk )
= y   max (y1 , y2 ,   , yp ,   , yk )
 y   max (y1 , y2 ,   , yp )

(22)

= max (y1 , y2 ,   , yp , y  )  max (y1 , y2 ,   , yp )
2. In the second case, if max (y1 , y2 ,   , yp ,   , yk , y  ) = max (y1 , y2 ,   , yp ,   , yk ),then
we have
max (y1 , y2 ,   , yp ,   , yk , y  )  max (y1 , y2 ,   , yp ,   , yk )
=0
 max (y1 , y2 ,   , yp , y  )  max (y1 , y2 ,   , yp )
= max (y1 , y2 ,   , yp , y  )  max (y1 , y2 ,   , yp )
Notice that max (y1 , y2 ,   , yp , y  )  max (y1 , y2 ,   , yp ) is always non-negative.

149

(23)

fiAzimi, Fern, & Fern

References
Anderson, B., Moore, A., & Cohn, D. (2000). A nonparametric approach to noisy and costly
optimization. In ICML.
Azimi, J., Fern, A., & Fern, X. (2010). Batch bayesian optimization via simulation matching.
In NIPS, pp. 109117.
Azimi, J., Fern, A., Fern, X. Z., Borradaile, G., & Heeringa, B. (2012). Batch active learning
via coordinated matching. In ICML.
Azimi, J., Fern, X., Fern, A., Burrows, E., Chaplen, F., Fan, Y., Liu, H., Jaio, J., & Schaller,
R. (2010). Myopic policies for budgeted optimization with constrained experiments.
In AAAI.
Bond, D. R., & Lovley, D. R. (2003). Electricity production by geobacter sulfurreducens
attached to electrodes. Applications of Environmental Microbiology, 69, 15481555.
Brochu, E., Cora, V. M., & De Freitas, N. (2010). A tutorial on bayesian optimization of
expensive cost functions, with application to active user modeling and hierarchical
reinforcement learning. arXiv preprint arXiv:1012.2599.
Brunato, M., Battiti, R., & Pasupuleti, S. (2006). A memory-based rash optimizer. In AAAI06 Workshop on Heuristic Search, Memory Based Heuristics and Their applications.
Burrows, E. H., Wong, W.-K., Fern, X., Chaplen, F. W., & Ely, R. L. (2009). Optimization
of ph and nitrogen for enhanced hydrogen production by synechocystis sp. pcc 6803
via statistical and machine learning methods. Biotechnology Progress, 25, 10091017.
Chapelle, O., & Li, L. (2011). An empirical evaluation of thompson sampling. In Advances
in neural information processing systems, pp. 22492257.
Cox, D. D., & John, S. (1992). A statistical method for global optimization. In IEEE
Conference on Systems, Man and Cybernetics, pp. 12411246.
Cox, D. D., & John, S. (1997). Sdo: A statistical method for global optimization. In in
Multidisciplinary Design Optimization: State-of-the-Art, pp. 315329.
Desautels, T., Krause, A., & Burdick, J. W. (2014). Parallelizing exploration-exploitation
tradeoffs in gaussian process bandit optimization. The Journal of Machine Learning
Research, 15 (1), 38733923.
Deshpande, A., Guestrin, C., Madden, S. R., Hellerstein, J. M., & Hong, W. (2004). Modeldriven data acquisition in sensor networks. In VLDB 04: Proceedings of the Thirtieth
international conference on Very large data bases, pp. 588599. VLDB Endowment.
Elder, J.F., I. (1992). Global rd optimization when probes are expensive: the grope algorithm. In IEEE International Conference on Systems, Man and Cybernetics, pp.
577582.
Fan, Y., Hu, H., & Liu, H. (2007). Enhanced coulombic efficiency and power density of
air-cathode microbial fuel cells with an improved cell configuration. Journal of Power
Sources, In press.
Ginsbourger, D., Riche, R. L., & Carrarog, L. (2010). Kriging is well-suited to parallelize
optimization..
150

fiBudgeted Optimization with Constrained Experiments

Goel, A., Guha, S., & Munagala, K. (2006). Asking the right questions: model-driven optimization using probes. In PODS 06: Proceedings of the twenty-fifth ACM SIGMODSIGACT-SIGART symposium on Principles of database systems, pp. 203212.
Jones, D. R. (2001). A taxonomy of global optimization methods based on response surfaces.
Journal of Global Optimization, 21, 345383.
Khuller, S., Moss, A., & Naor, J. (1999). The budgeted maximum coverage problem. Inf.
Process. Lett., 70 (1), 3945.
Krause, A., & Guestrin, C. (2005). A note on the budgeted maximization of submodular
functions. Technical Report, CMU-CALD-05-103.
Krause, A., Singh, A., & Guestrin, C. (2008). Near-optimal sensor placements in Gaussian processes: Theory, Efficient Algorithms and Empirical Studies. The Journal of
Machine Learning Research, 9, 235284.
Lizotte, D., Madani, O., & Greiner, R. (2003). Budgeted learning of naive-bayes classifiers.
In UAI.
Locatelli, M. (1997). Bayesian algorithms for one-dimensional global optimization. Journal
of Global Optimization, 10 (1), 5776.
Madani, O., Lizotte, D., & Greiner, R. (2004). Active model selection. In UAI.
Moore, A., & Schneider, J. (1995). Memory-based stochastic optimization. In NIPS.
Moore, A., Schneider, J., Boyan, J., & Lee, M. S. (1998). Q2: Memory-based active learning
for optimizing noisy continuous functions. In ICML, pp. 386394.
Myers, R. H., Montgomery, D. C., & Anderson-Cook, C. M. (1995). Response surface
methodology: process and product optimization using designed experiments. Wiley.
Park, D. H., & ZeikusG, J. G. (2003). Improved fuel cell and electrode designs for producing
electricity from microbial degradation. Biotechnol Bioeng, 81 (3), 348355.
Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian Processes for Machine Learning.
MIT.
Reguera, G., McCarthy, K. D., Mehta, T., Nicoll, J. S., Tuominen, M. T., & Lovley, D. R.
(2005). Extracellular electron transfer via microbial nanowires. Nature, 10981101.
Ross, A. M. (2008). Computing Bounds on the Expected Maximum of Correlated Normal
Variables . Methodology and Computing in Applied Probability.
Schneider, J., & Moore, A. (2002). Active learning in discrete input spaces. In Interface
Symposium.
Schonlau, M. (1997). Computer Experiments and Global Optimization. Ph.D. thesis, University of Waterloo.
Silberstein, A., Braynardand, R., Ellis, C., Munagala, K., & Yang, J. (2006). A samplingbased approach to optimizing top-k queries in sensor networks. In ICDE 06: Proceedings of the 22nd International Conference on Data Engineering, p. 68, Washington,
DC, USA. IEEE Computer Society.
151

fiAzimi, Fern, & Fern

Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pp. 2951
2959.
Srinivas, N., Krause, A., Kakade, S., & Seeger, M. (2010). Gaussian process optimization
in the bandit setting: No regret and experimental design. In Proc. International
Conference on Machine Learning (ICML).
Stuckman, B. E. (1988). A global search method for optimizing nonlinear systems. In IEEE
transactions on systems, man, and cybernetic, Vol. 18, pp. 965977.
Tatbul, N., Cetintemel, U., Zdonik, S. B., Cherniack, M., & Stonebraker, M. (2003). Load
shedding in a data stream manager. In VLDB 2003: Proceedings of the 29th international conference on Very large data bases, pp. 309320. VLDB Endowment.

152

fiJournal of Artificial Intelligence Research 56 (2016) 61-87

Submitted 09/15; published 05/16

Automatic Wordnet Development for Low-Resource
Languages using Cross-Lingual WSD
Nasrin Taghizadeh

nsr.taghizadeh@ut.ac.ir

School of Electrical and Computer Engineering
College of Engineering, University of Tehran, Tehran, Iran

Hesham Faili

hfaili@ut.ac.ir

School of Electrical and Computer Engineering
College of Engineering, University of Tehran, Tehran, Iran

Abstract
Wordnets are an eective resource for natural language processing and information
retrieval, especially for semantic processing and meaning related tasks. So far, wordnets
have been constructed for many languages. However, the automatic development of wordnets for low-resource languages has not been well studied. In this paper, an ExpectationMaximization algorithm is used to create high quality and large scale wordnets for poorresource languages. The proposed method benefits from possessing cross-lingual word sense
disambiguation and develops a wordnet by only using a bi-lingual dictionary and a monolingual corpus. The proposed method has been executed with Persian language and the
resulting wordnet has been evaluated through several experiments. The results show that
the induced wordnet has a precision score of 90% and a recall score of 35%.

1. Introduction
One of the most important projects in natural language processing over the years has been
the construction of an English wordnet (WordNet) at Princeton University under the direction of George A. Miller (1995). WordNet consists of a lexical database, in which English
words are grouped into sets of cognitive synonyms called synsets. The eectiveness of WordNet in a wide range of language technology applications inspired many researchers to create
wordnets for other languages. The first attempts at this led to the construction of EuroWordNet (Vossen, 1998) and BalkaNet (Tufis, Cristea, & Stamou, 2004). EuroWordNet
deals with European languages such as English, Dutch, German, French, Spanish, Italian,
Czech and Estonian; while BalkaNet covers languages from the Balkan zone. To interconnect wordnets of dierent languages, EuroWordNet links synsets of each language to an
interlingual index (ILI). The ILI allows it to find equivalent synsets across all languages
connected to the ILI.
Although the first wordnet was created manually, several automatic and semi-automatic
techniques have been used for developing the other wordnets. These methods are usually
divided into merge and expansion approaches (Fellbaum & Vossen, 2012; Oliver & Climent, 2012; Erjavec & Fiser, 2006). However, there are methods that combine the merge
and expansion models and benefit from the advantages of both approaches (Prabhu, Desai,
Redkar, Prabhugaonkar, Nagvenkar, & Karmali, 2012; Apidianaki & Sagot, 2014). With
the merge approach, a small wordnet is created manually, which contains high-level and
c
2016
AI Access Foundation. All rights reserved.

fiTaghizadeh & Faili

basic concepts. Next, this small wordnet is developed using automatic and semi-automatic
techniques. In this process, mono-lingual resources and language-specific properties are
employed. Wordnets created in this manner later are mapped onto either the WordNet or
ILI. When using the expansion approach, a multilingual wordnet is constructed by translating words inside the synsets of the WordNet (or other existing wordnets) into the target
language using multi-lingual resources. Therefore the structure of the original wordnet is
preserved and the words are translated.
Among the dierent methods proposed for wordnet construction, few of them are applicable to low-resource languages. Methods that follow the merge approach are labourintensive and time-consuming. Moreover, they need to have vast knowledge about the language and also require many resources, which is the main obstacle of low-resource languages
- so it makes this approach inapplicable for them in practice. On the other hand, methods
that follow the expansion approach usually adopt WordNet structure and find the correct
translation of the associated words with the WordNet synsets in the target language. In
this process, multilingual resources such as comparable corpora (Kaji & Watanabe, 2006),
parallel corpora (Oliver & Climent, 2012; Kazakov & Shahid, 2009; Fiser, 2009; Diab, 2004),
thesaurus (Gunawan & Saputra, 2010), machine translators (Saveski & Trajkovski, 2010)
and multiple bi-lingual machine readable dictionaries (Atserias, Climent, Farreres, Rigau,
& Rodrguez, 2000; Patanakul & Charnyote, 2005; Bond, Isahara, Kanzaki, & Uchimoto,
2008; Lam, Al Tarouti, & Kalita, 2014) are used, which causes a bottleneck for low-resource
languages.
Taking a deeper look at the expansion-based methods, each synset from the WordNet
is kept and words associated with it are translated into the target language. A bi-lingual
dictionary is usually employed and English words inside the WordNet synsets are translated.
Since dictionaries do not translate word sense to word sense, but rather word to word,
translations are ambiguous and should be disambiguated. Looking more carefully, after
translating English words inside a WordNet synset, a set of candidate words in the target
language is obtained; some of these are equivalent to the other senses of the English words
and should thus be omitted. Methods that following the expansion approach rank the
candidate words and omit low-rated ones from the candidate sets. If the task of scoring
candidate words for the WordNet synsets is considered to be an optimization problem, (sub)optimal values can be found using algorithms such as Expectation-Maximization (Montazery
& Faili, 2011). The proposed method is an extension of this work for low-resource languages.
In this paper, the problem of automatically constructing large scale and high quality
wordnets for low-resource languages is studied. Between the two major approaches, merge
and expansion, the first one is not suitable; because it requires vast knowledge about the
target language and also many language resources. So the preferred approach is to utilize wordnets in other languages by adopting their structure and translating their content.
Finding the correct senses of the target language words is an AI-complete problem (Mallery,
1988), that is, by analogy to the NP-completeness in the complexity theory, it is a problem
whose diculty is equivalent to solving the central problems of AI (Navigli, 2009). In this
paper, an iterative optimization method based on cross-lingual WSD is proposed to find
the local optimum of the problem in a reasonable time. The main idea is to iteratively
improve the estimation of the probability of selecting WordNet synsets for the words of the
target language. Additionally, the proposed method needs few resources and so it is suitable
62

fiAutomatic Wordnet Development for Low-Resource Languages

for poor-resource languages. To investigate the performance of the proposed method, Persian has been selected as a poor-resource language and the resulting wordnet is examined
through conducting several experiments.
The roadmap of the paper is as follows: Section 2 presents related works; Section 3
explains the wordnet construction problem and the proposed formulation; Section 4 presents
a case study of the Persian language and error analysis; and conclusions are given and future
works suggested in the last section, Section 5.

2. Related Work
In this section, some automatic methods for constructing wordnets are reviewed that are
based on the expansion approach. The main stage of the expansion-based methods is finding
a set of words that lexicalizes the concept captured by a given synset of an existing wordnet
in another language. All candidate words are usually extracted by a dictionary and a scoring
system is utilized to find the correct words.
In the work of Kaji and Watanabe (2006), the gloss information in WordNet has been
used for the automatic construction of a Japanese wordnet. Given an English synset, it
calculates a score for each of its Japanese translation candidates according to the gloss
appended to the synset. The score is defined as the sum of correlations between the translation candidates and the associated words that appear in the gloss. A pair of words are
deemed associated if the amount of mutual information between them is above a predefined threshold. Since the availability of bi-lingual corpora is limited, an iterative approach
has been proposed for calculating pair-wise correlations.
Another study on creating a wordnet by automatically expanding WordNet describes a
Romanian wordnet. In the work of Barbu and Barbu Mititelu (2005), in order to identify
the Romanian words corresponding to a WordNet synset, several heuristics have been proposed. According to the first heuristic, words related to a synset share a common meaning.
Therefore, the intersection of translations of words associated with the WordNet synsets
is considered. The second heuristic states that a synset and its hypernym share the same
meaning. Therefore, the intersection of word translations from a given WordNet synset
and its hypernym is selected as a Romanian synset. According to the third heuristic, those
translations that have the same domain label are selected for a given WordNet synset. By
the fourth heuristic, a Romanian word is selected if English translations of words based on
its definition have maximum similarity with words in the gloss of the given synset.
In the research conducted by Patanakul and Charnyote (2005), a semi-automatic expanding approach has been presented to construct a Thai wordnet. Candidates for links
between Thai words and WordNet synsets have been derived from WordNet and its translations. To rank these links, 13 criteria are used that have been categorized into three groups:
monosemic, polysemic, and structural criteria. Monosemic criteria focus on English words
that have only one meaning and assume that such English words have only one synset in
the WordNet. Polysemic criteria focus on English words that have multiple meanings, and
believe that such English words have multiple synsets in the WordNet. Structural criteria
focus on structural relations among synsets with respect to the wordnet 1.7.
Another idea for creating wordnet is to use a word-aligned parallel corpus with n languages, annotate each word with a lexical sense tag that consists of the n-tuple of aligned
63

fiTaghizadeh & Faili

words. As a result, all occurrences of a given word in the text for language L are considered
to have the same sense, provided they are tagged with the same multi-lingual synset. However, this kind of corpus is not easily available in most languages. In the research, which was
conducted by Oliver and Climent (2012), two strategies for the automatic construction of
these corpora are proposed: (i) by the machine translation of sense-tagged corpora, and (ii)
by the automatic sense tagging of bi-lingual word-aligned corpora. The results for Spanish
language showed that the first strategy works better than the second. This suggests that
lexical selection errors made by the machine translation systems are less important than
the sense tagging errors.
In the BabelNet project, which was undertaken by Navigli and Ponzetto (2010, 2012a), a
very large multi-lingual semantic network was constructed. In this project, original wordnet
was used as its lexicographic resource as well as Wikipedia pages in dierent languages for
its encyclopedic knowledge. First a mapping between the English Wikipedia pages and
the synsets in the original wordnet was established. Given a Wikipedia page w and its
mapping, a Babel synset was created using the wordnet synset s, page w, all inter-language
links, which are translation of w to the other languages. In this project, the coverage of the
resulting network has been analyzed by comparing it with the gold-standard wordnets in
terms of synset coverage, word coverage, and synset extra coverage. The results show that
the synset coverage varies for dierent languages from 52% for Italian to 86% for French.
In the work of Bond and Foster (2013), an open multi-lingual wordnet for more than
eighty languages was developed. In this project, a common interface for accessing multiple
wordnets was created through gathering existing freely available wordnets of dierent languages and automatically linking them to the WordNet. Next, the wordnets were extended
using the Unicode Common Locale Data Repository (UCLDR) and Wiktionary. To rank
candidate links between WordNet synsets and Wiktionary, several similarity measures were
employed. The results show that the precision score was 85%-99% when measured on sense.
An Arabic wordnet was created that follows the EuroWordNet methodology of manually
encoding a set of base concepts while maximizing compatibility across Arabic and English
wordnets (Black, Elkateb, & Vossen, 2006; Elkateb, Black, Rodrguez, Alkhalifa, Vossen,
Pease, & Fellbaum, 2006). Next, in the project, which was performed by Rodrquez et al.
(2008), a machine learning algorithm was employed for extending the Arabic wordnet and
augmenting formal specification to the senses of its synsets. In order to associate Arabic
words with the WordNet synsets, a Bayesian network with four layers was proposed. Four
layers respectively represent: Arabic words; the corresponding English translation of these
Arabic words in the first layer; all the synsets of the English words in the second layer; and
other WordNet synsets linked to the synsets in layer three. A set of candidates word-synset
is built with pairs <x, y>, where x is an Arabic word and y is a WordNet synset in the third
layer of the Bayesian network that has a non-null probability and so there is a path from
x to y. The score of each link is calculated with the posterior probability of y, given the
evidence provided by the network. Only the tuples that score over a threshold are selected
for inclusion in the final set of candidates word-synset. The best results of the method
proposed in this study noted a score of 71% precision.
In the work of Boudabous et al. (2013), an Arabic wordnet was enriched via adding
semantic relations between synsets. The method consisted of two main phases; the first
phase consisted of defining morpho-lexical patterns using a study corpora extracted from
64

fiAutomatic Wordnet Development for Low-Resource Languages

the Arabic Wikipedia. The second phase consisted of using morpho-lexical patterns, defined
in the previous phase, in order to extract new semantic relations from the Arabic Wikipedia.
Extracted relations were validated, then added to the Arabic wordnet data base.
Piasecki et al. (2011) proposed an algorithm for automatically expanding the Polish
wordnet. This method uses heterogeneous knowledge sources, which are extracted from a
large corpus, and combines them based on a weighted voting scheme. This method extracts
potential instances of lexicon-semantic relations from a corpus and measures the semantic
similarity of lexical units. It analyzes the eect of using dierent knowledge resources on
the performance of the algorithm. Due to the high accuracy of the results, this approach
can be said to be a good basis for semi-automatic methods of constructing wordnets using
human knowledge to correct the output of the automatic approaches.
Lam et al. (2014) proposed an automatic method for constructing wordnet synsets that
uses the publicly available wordnets, a machine translator and bi-lingual dictionaries. For
this purpose, each synset of an existing wordnet is translated into the target language,
then a ranking method is applied to the resulting translation candidates to find the best
translations. To generate candidate synsets, three approaches were proposed; The first one
directly translates synsets in WordNet into the target language. The second one uses intermediate wordnets to handle ambiguities in synset translations. In the case of dictionaries
being available, in addition to the wordnets in the intermediate languages, a third approach
can be used. The experimental results showed that the resulting wordnets have a coverage
of 19%, 65%, 37%, 21% and 83% for Karbi, Arabic, Assamese, Dimasa and Vietnamese
languages, respectively.
In the project, which was conducted by Hasanuzzaman et al. (2014), a method for
constructing a Tempo-wordnet was suggested. According to this method, the WordNet
was augmented with temporal information by following a two-step process: in the first
step, synsets of the WordNet are classified as atemporal or temporal. Next, all synsets are
associated with past, present and future probabilities. The obtained Tempo-wordnet can
be used in time-related applications.
In the work of Shamsfard (2008), a semi-automated method was proposed for developing a Persian lexical ontology called FarsNet. About 1,500 verbs and 1,500 nouns were
gathered manually to make the wordnets core. After that, two heuristics and a Word Sense
Disambiguation (WSD) method were used to find the most likely related Persian synsets.
A practical evaluation of the proposed automatic method used in this studt shows a score
of 70% correctness and covers about 6,500 entries on WordNet. The extension of this work
(Shamsfard, Hesabi, Fadaei, Mansoory, Famian, Bagherbeigi, Fekri, Monshizadeh, & Assi,
2010a), is known as being the first published Persian wordnet, FarsNet, which contains
about 18,000 Persian words and covers about 6,500 WordNet synsets.
In the research, which was performed by Montazery and Faili (2010), an automatic
approach for Persian wordnet construction based on the WordNet has been introduced.
The proposed approach uses two mono-lingual corpora for English and Persian, and a bilingual dictionary in order to construct mapping between WordNet synsets and Persian
words using two dierent methods; some links were selected directly by using heuristics
that recognize these links as unambiguous. Other types of links are ambiguous, in which a
scoring method is used to select the appropriate synset. The practical evaluation of the links
for 500 randomly selected Persian words shows about 76.4% quality in terms of accuracy.
65

fiTaghizadeh & Faili

By augmenting the Persian wordnet with unambiguous words, the total accuracy of the
automatically extracted Persian wordnet becomes 82.6%.

3. Iterative Method for Wordnet Construction
To construct a multi-lingual wordnet, several methods have been presented; however, few
of them have paid attention to low-resource languages. Creating a wordnet from scratch for
such languages is a time-consuming and expensive process. Instead, new wordnets could be
developed by adopting the structure of existing wordnets in other languages (usually WordNet) and translating the words associated with their synsets into the target language. One
important advantage of this approach is that the resulting wordnet is aligned to the WordNet and the ILI, and thus is interesting for contrastive semantic analysis and is particularly
useful in multi-lingual tasks such as multi-lingual information retrieval (Dini, Peters, Liebwald, Schweighofer, Mommers, & Voermans, 2005; Otegi, Arregi, Ansa, & Agirre, 2015)
and multi-lingual semantic web (Buitelaar & Cimiano, 2014). The main assumption on
which one can develop a wordnet using the expansion approach is that most of the concepts
and semantic relations are common among dierent languages. Therefore, language-specific
concepts and relations may not be covered in the resulting wordnet.
In general, and regardless of the approach taken, the main step toward constructing a
complete wordnet is to generate synonym sets. In this section, an automatic method for
extracting synsets for languages with limited resources is proposed. The proposed method
follows the expansion approach; at the start, wordnet is initialized with WordNet synsets.
For every WordNet synset s, all translations of English words inside s are extracted from bilingual dictionary and links between translation words and WordNet synsets are established.
Since dictionaries translate word to word, not word sense to word sense, translations are
ambiguous. Therefore, the task is to score links and find incorrect ones. We consider these
scores to be the probability of selecting each candidate synset for each word in the target
language.
In this paper, the task of finding correct the translation of words associated with the
WordNet synsets is regarded as an optimization problem. If a sensed-tagged corpus similar
to the English SemCor (Landes, Leacock, & Tengi, 1998) exists in the target language, the
problem of creating wordnet is converted to the maximum likelihood estimation (MLE).
The English SemCor corpus is a sense-tagged corpus created at Princeton University by the
wordnet project research team. The corpus consists of a subset of the Brown Corpus and
contains about 700,000 words. In SemCor all the words are POS tagged and more than
200,000 content words are sense-tagged with reference to the WordNet lexical database.
Since such resources may not exist, we use a word sense disambiguation method to find
correct sense of each word in a raw corpus. As shown in the research, which was conducted
by Mallery (1988), WSD is an AI-complete problem whose diculty is equivalent to solving
the central problems of AI. This class of problems is analogous with NP-complete problems
in complexity theory, which are classified as being the most dicult problems. The proposed
idea is to use an iterative algorithm that finds the local optima of the problem with few
iterations in a reasonable time. Our work can be regarded as an extension of the work
which was performed by Montazery and Faili (2011). The proposed method adopts this
66

fiAutomatic Wordnet Development for Low-Resource Languages

Mono-lingual
corpus

extract
unique words

Bi-lingual
Dictionary

w

extract English
translations

WordNet

(w, e)

extract
WordNet
synsets

(w, s)

EM algorithm
(w, s, p)

Synsets in
the target
language

deleting
low-rated links

Figure 1: The overview of the proposed approach for constructing wordnet

work for low-resource languages; and our method additionally attempts to solve its major
drawbacks.
The idea proposed in the work of Montazery and Faili (2011) for wordnet construction,
is to use a bi-lingual dictionary as well as a raw-corpus. First, for each Farsi word in the
corpus, all translations are extracted from the bi-lingual dictionary. Next, all synsets of
the English translations are considered as the candidate synsets for the Farsi word. A
score is calculated for each pair of Farsi words and WordNet synsets using the expectationmaximization (EM) algorithm. In the expectation step, they use a relative-based WSD
method (PMI), in which the co-occurrence frequency of pairs of words in the Farsi language
have been used to disambiguate words of a corpus. Experimental results showed that the
precision of this method varies for dierent POS tags. The highest precision is shown for
adjectives which is 89.7%; next for adverbs, which is 65.6%; and the lowest precision is for
nouns at 61.6%.
The major drawbacks of the above method are that calculating the co-occurrence between each pair of words in the target language usually requires a large corpus, which may
not be easily found in low-resource languages; this is important because the quality of the
resulting wordnet highly depends on the co-occurrence values. As a result, we propose to
change the expectation step of the PMI-based algorithm so that the WSD procedure can be
performed without needing an additional corpus or any other language resources. Figure 1
represents an overview of the proposed method. Next, in the experimental analysis, we will
re-implement this work as the baseline and compare the proposed method with it.
EM is an iterative algorithm for finding the maximum likelihood parameters of a statistical model in cases where the equations cannot be directly solved. These models typically
consist of latent variables in addition to unknown parameters and known data observations.
That is, either there are missing values among the data, or the model can be formulated
more simply by assuming the existence of additional unobserved data points. The basic
idea of the EM is as follows:
1. If we have the actual sucient statistics for the data, we can compute the parameter
values that maximize the likelihood of the data. This is just the problem of learning
a probabilistic model from complete data.
67

fiTaghizadeh & Faili

Maximization Step

initial values

Parameters w,s

sense-tagged corpus

Expectation Step

Figure 2: Expectation-Maximization algorithm for wordnet construction
2. If we actually succeed in learning the model parameters, we could then compute a
probability distribution over the values of the missing attributes.
In the case of our problem, the EM algorithm should find the probability of mapping each
word in the target language to each of its candidate synsets. If a candidate synset represents
a correct sense for a word in the target language, it is expected that this sense occurs in a
corpus containing that word. So the observed data is the words of a corpus in the target
language; the unseen part of each data is the WordNet sense tag of the words.
Th EM algorithm switches between two stages: 1) finding an approximate distribution
of missing data given the parameters; and 2) finding better parameters given the approximation. The first step is known as the expectation or E-step, while the second step is
called the maximization or M-step. Figure 2 represents an overview of the EM algorithm
used for learning words connected to the WordNet synsets. Next, details of each step in the
proposed algorithm are presented.
3.1 E-Step
Similar to the work of Montazery and Faili (2011), for each word in the target language,
w, and each a WordNet synset, s, w,s is defined as the probability of choosing WordNet
synset s for word w, P (s|w). In other words, the number of times that word w appears in
a large corpus with sense s divided by total number of appearance w. That is:
w, s :
w :

w,s  [0, 1].


w,s = 1.

(1)

(2)

s

At this step, current values of parameters w,s are used to label the corpus with sense
tags. For each word w appearing in the corpus, an appropriate sense among the candidate
WordNet synsets should be chosen. To do this task, an unsupervised cross-lingual word
sense disambiguation (WSD) could be employed. WSD algorithms aim to resolve word
ambiguity without the use of annotated corpora. Unsupervised WSD is a well-studied task
in the literature. Among these, two categories of knowledge-based algorithms have gained
popularity: overlap- and graph-based methods. The former owns its success to the simple
intuition that underlies that family of algorithms, while the diusion of the latter started
growing after the development of semantic networks (Basile, Caputo, & Semeraro, 2014).
68

fiAutomatic Wordnet Development for Low-Resource Languages

Within the graph-based framework for WSD, a graph is built from a lexical knowledge
base (usually WordNet) representing all possible senses of the word sequence that is being
disambiguated. Graph nodes correspond to word senses, whereas edges represent dependencies between senses. These dependencies include hypernymy, synonymy, antonymy, etc.
Next, the graph structure is analyzed to determine the importance of each node. Finding
the right sense for each word in the sequence amounts to identifying the most important
node among the set of graph nodes representing its candidate senses. The main challenge of
the graph-based WSD methods is how to create the graph, especially which dependencies
should be chosen as the graphs edges, and which connectivity measure should be used to
score the nodes of the graph.
In the research, which was conducted by Navigli and Lapata (2010), a comprehensive
study on unsupervised graph-based WSD was conducted. They evaluated a wide range of
local and global measures of graph connectivity with the aim of isolating those that are
particularly suited for this task. Local measures include degree, page-rank, HITS, KPP
and betweenness, whereas global measures consist of compactness, graph entropy, and edge
density. Their results indicate that local measures yield a better performance than global
ones. The best local measures are Degree and PageRank.
For the task of wordnet development, we adapt a graph-based WSD method as presented
in work of Navigli and Lapata (2010), for the problem of the sense labelling of the corpus
using the current parameters w,s . It is assumed that the true sense of each word in the
corpus is determined through senses of other words in the same sentence. For every sentence
of the corpus, the following procedure is executed:
 For each word w in the sentence, candidate WordNet synsets are picked, and one
terminal node for each synset s in the graph is created. This set of terminal nodes is
called Vw .
 For each terminal node v, a depth-first search (DFS) on the WordNet graph is performed. Every time a node v   Vw (w = w ) along a path of length  L is encountered,
all intermediate nodes and edges on the path from v to v  are added to the graph. L
is a parameter of the algorithm and usually takes small values such as 3, 4 or 5.
 Terminal nodes of the graph are scored according to their degree as follows: For node
v  Vw ,
deg(v)
C(v) =
,
(3)
maxuVw (deg(u))
where deg(v) is the number of edges terminating in v in graph G = (V, E):
deg(v) = |{(u, v)  E : u, v  V }|,

(4)

Relations chosen as the graphs edges consist of all the lexical and semantic relations
defined in WordNet in addition to the gloss relation. A pair of synsets s and s is connected
via a gloss relation if an unambiguous word w  s occurs in the gloss of s. The word
w must be unambiguous; otherwise, s should have been connected with the appropriate
sense of w (Navigli & Lapata, 2010). To use gloss relation in the WSD procedure, sense
disambiguated glosses of the WordNet are utilized (Semantically Tagged glosses, 2016), in
69

fiTaghizadeh & Faili

which word forms from the glosses in WordNets synsets are manually linked to the contextappropriate sense in WordNet. Therefore, gloss relation is established between s and s , if
s appears as the correct sense of any word in the gloss of the s .
The time complexity of calculating a degree measure is less than PageRank, and its
performance has been shown to be better; so in the last step of the WSD procedure, a
degree measure is preferred for scoring nodes of the graph. To illustrate the steps of the
WSD procedure, we provide an example in the next section.
3.1.1 WSD of a Persian Sentence
In order to better understand WSD procedure, an example is presented. Consider the following Persian sentence which means Workers with thirty years of service become retired.

.

|{z}
punc

	 JPA
	
 
 J 	PAK
 AK. @Y
 g	 K . A A  	 @X
	 . IY
YK	 
| {z } | {z } | {z } | {z } |{z} |{z} | {z } |{z} | {z }
verb

adj

noun

noun noun num

noun

prep

noun

Preposition, number and punctuation tags are not involved in the wordnet and so are
	 	 /retired in the above sentence. According to the Aryanignored. Consider the word J PAK
.
pour dictionary, this word has three translations: emeritus; pensionary; retired. According
to the wordnet 3.0: the first translation has one noun synset and one adjective synset;
the second one has two noun synsets; and the third one has eleven verb synsets and one
adjective synset. Since this word can be a noun or an adjective in a Persian corpus, verb
synsets are ignored. The definitions of the other synsets are as follows:
 {10051861} (noun.person) emeritus#1  (a professor or minister who is retired from
assigned duties)
 {01645490} (adj.all) emeritus#1  (honorably retired from assigned duties and retaining your title along with the additional title emeritus as in professor emeritus)
 {10414612} (noun.person) pensioner#1, pensionary#1  (the beneficiary of a pension
fund)
 {10176913} (noun.person) hireling#1, pensionary#2  (a person who works only for
money)
 {00035368} (adj.all) retired#1  (no longer active in your work or profession)

	

	 . /retired consists of these five
Therefore, the candidate set for the Persian word J PAK
synsets. In general, each of these synsets could be the correct sense in the above sentence.
However, the POS tag of this word in the given sentence can come to our aid during the
WSD procedure in order to filter some synsets. Indeed in the WSD procedure, only those
70

fiAutomatic Wordnet Development for Low-Resource Languages

Table 1: Persian words and their candidate synsets.
Persian word

	
YJPA

	 @X
A
K . A

 g	
IY
	
J 	PAK
	Y .

POS
noun
noun
noun
noun

noun
adjective
verb

Translations
employee,
worker,
member
relieve, own, have
year
background,
antecedent,
history,
record, service
work, job, activity,
profession
retired, emeritus,
wind, grow, lapse,
branch, become, be

candidate synsets

selected synset

correct

10

workern1

3

1
4
40

have1n
yearn1
record1n

3
3
7

30

job1n

3

2
42

retired1a
growv3

3
7

synsets which have the same POS as the given POS in the sentence should be involved.
	 	 /retired has an adjective POS in the above sentence, only adjective
Since the word J PAK
.
synsets are involved in the graphs construction. Following the above steps for the other
words of the sentence leads to finding the candidate synsets of each word that should be
accounted for in the WSD graph. Table 1 represents Persian words, their translations, and
the number of candidate synsets regarding the POS tag of the Persian words. All of these
candidate synsets represent the terminal nodes of the WSD graph. As Figure 3 shows, the
candidate synsets of each Persian word of the given sentence have been grouped in a dotted
box.
In the next step, a DFS algorithm is run for each terminal node on the WordNet graph
with the length being at most three. Upon finding a path from one terminal node to another,
all intermediate nodes and edges are added to the WSD graph. Part of the WSD graph is
shown in Figure 3. Each word in this graph is associated with a POS, which is denoted with
a subscript: n stands for noun, v for verb, a for adjective, and r for adverb. The superscript
denotes the sense number associated with that word in WordNet 3.0. This graph has three
	 
 
 /become and A/year and the
separate components; one component for each word Y K
other component for remaining words. This means no word in the given sentence indicates
the sense of these words.
After the construction of the WSD graph, the correct sense of each Persian word should
be determined. To do this, the synset with the most degree among the candidate set of
	 	 /retired;
each word is chosen as the correct synset for that word. Consider the word J PAK
.
1
in the WSD graph of Figure 3, the node retireda has a degree of one; whereas the node
emeritus1a has a degree of zero. So the selected sense for this word is retired1a . Using the
degree measure, the selected sense for each word of the given sentence is determined, which
is represented in the bold box. Table 1 summarizes the steps taken in the WSD procedure
of the given sentence. As the last column shows, the selected sense for all of the words is

	 /become.
correct except for K. A/background and Y
71

fiTaghizadeh & Faili

	.
J 	PAK
retired1a

workn3

workn1

record1n

move2n

wind3n

historyn2
photographyn1

unf ortunate1n

grown3

ancendent1n
relative1n

job10
n
job6n

wind1n

be1n

...

processorn1

have1n

wind2n

person1n

job7n

employee1n
...

...

decade1n

yearn1

period1n

yearn3

season1n

yearn2

A

workern1

	
YJPA

job1n

...


	 @X

 g	
IY

activityn1

traveln1

 

YK	 

occupation1n employment2n

service5n

K . A

prof ession1n

emeritus1a

...

	 
	 JPA
	 .
 
 J 	PAK
 g	 K . A A  	 @X
 AK. @Y
	 . IY
Figure 3: Part of WSD graph for the sentence YK
3.2 M-Step
In the maximization step, a new estimation of the models parameters should be calculated
based on the sense-tagged corpus that resulted from the expectation step. Similar to the
work of Montazery and Faili (2011), on iteration j, the new value for parameter w,s , which
denotes the probability of assigning a sense tag s to the word w, is equal to averaging the
conditional probability P (s|j1 ) over dierent occurrences of w in the corpus, where j1
is the set of all parameters w,s on iteration j  1. In formal notation,
n
P (si |w1n , j1 )
i=1
w
=w,s
=s
i
i
j
w,s
=
,
(5)
N (w)
j
where w,s
denotes the value of w,s on iteration j, w1n presents sequence of corpus words
and N (w) is number of occurrence of w in w1n .
In each iteration of the EM algorithm, the likelihood of the data given the new parameter
values is at least as great as the likelihood given the old ones. So EM behaves similar to
the gradient descent; at each step, it adjusts the parameter values so as to improve the
likelihood of the data. It follows that EM converges to a set of parameter values that
locally maximizes the likelihood.
The proposed EM method is repeated until the changes in the probability of selecting
a candidate synset for a word in the target language becomes negligible. So, at the end
of each iteration, the maximum change of probabilities is computed. If this value is less
than t, the algorithm stops. After execution of the EM algorithm, all links with a score of
below the threshold tremove (w,s  tremove ) will be deleted from the wordnet. Also in each

72

fiAutomatic Wordnet Development for Low-Resource Languages

	

	 . /retired per iteration.
Table 2: Assigned probabilities for word  J PAK
Synset ID
Noun:10051861
Adjective:01645490
Noun:10414612
Noun:10176913
Adjective:00035368
Entropy

Correct
7
7
7
7
3

Itr #0
0.2
0.2
0.2
0.2
0.2
2.1502

Itr #1
0.11111
0.29885
0.11111
0.11111
0.36781
1.8340

Itr # 2
0.11111
0.08315
0.11111
0.11111
0.58350
1.7880

Itr #3
0.11111
0
0.11111
0.11111
0.66666
1.7797

Itr #4
0.11111
0
0.11111
0.11111
0.66666
1.7781

Itr #5
0.11111
0
0.11111
0.11111
0.66666
1.7768

iteration, those links with a current score below t are ignored and the corresponding senses
are not presented in the graphs construction and the WSD procedure. At the end, those
words in the target language that are mapped onto the same synset in the WordNet make
synsets of the resulting wordnet.
To better follow the process of updating probabilities of each word per iteration, an
example is presented here. For demonstrating the probability adjustment in each iteration,
	 	 /retired. In the expectation step, all words of the corpus
consider again the word  J  PAK
.
should be disambiguated. Next in the maximization step, the new value of the probabilities
		
is computed. Table 2 represents the probabilities of synsets assigned to the word  J  PAK
.
/retired in each iteration. The first and the second columns show the synset ID and the
correction of synsets for the specified word, respectively. The following columns represent
the probability values of the first five iterations. Values less than 0.005 were considered to
be 0. This table shows that the probabilities start out uniformly; then in each iteration,
the probability of correct synsets increases and the probability of incorrect synsets or those
that are not frequent enough in the corpus decreases or does not change. Indeed, if the
	 	 /retired in the corpus, which are tagged with a
number of occurrences of word  J  PAK
.
specific WordNet sense in iteration i are the same as the iteration i  1, the probability of
	 	 /retired does not change in the iteration i. If this
that sense of the given word  J  PAK
.
value becomes greater, that probability increases and so if this value becomes smaller, that
probability decreases. In this particular example, after five iterations, the synset achieving
the highest probability is the correct synset. In iteration three, the probability of the word
	 . /retired being assigned to the second synset goes down to 3.9E-7, which is below
 J  	PAK
the threshold. So in the next iterations, this synset is not considered in WSD procedure
and its probability will be zero. The last row of the table presents the entropy value in
respect to the iteration. The steady decrease in entropy indicates that in each iteration,
the distinction between candidates synsets for each word becomes more clear, which leads
to identification of the correct synsets. The subject of analysis of the entropy for each word
per iteration is discussed later in Section 4.2.1.

4. Case Study: Persian Language
In this section, the proposed method for automatic wordnet construction is applied to
Persian as a low-resource language. In the following subsections, the experimental setup
and evaluation methods are described; after that, the results are presented.
73

fiTaghizadeh & Faili

4.1 Experimental Setup and Data
In this section, the required resources and setup of the experiments are explained1 . To
construct a wordnet for the Persian language, the Bijankhan Persian corpus2 has been
used. This collection has been gathered from daily news and common texts, in which
all documents are categorized into dierent subjects such as political, cultural and so on.
Bijankhan contains about ten million manually-tagged words with a tag set containing 550
fine-grained Persian POS tags (Oroumchian, Tasharofi, Amiri, Hojjat, & Raja, 2006).
Although POS tags are not explicitly used in the proposed method, to get better WSD
results, one can use POS tags to prune synsets along with the other tags from the candidate
set of each word as explained in Section 3.1.1. As a result, in the WSD procedure, just
those synsets with the same POS tag as the word of the corpus are taken part. In WordNet,
four categories of tags are included: noun, verb, adverb and adjective. Thus the words of
the corpus with other tags such as pronoun and preposition are ignored.
Bijankhan is a large corpus. Most low-resource languages may not have such a large
corpus. In order to evaluate the behaviour of the proposed method when the corpus size is
limited, a part of the Bijankhan has been picked for training Persian wordnet. So both the
PMI-based and the graph-based method have been conducted using this part. This part
includes nearly 13% of the total size of the corpus. The remaining 87% has been used in
the testing phase in which coverage of the wordnet over the corpus was evaluated. More
details about the coverage analysis are presented in Section 4.2.4. Also, a complete analysis
on the eect of the corpus size on the quality of the final wordnet is presented in Section
4.4.
Those words in the corpus that appear in their inflected forms may not be found in the
dictionary. Therefore before the beginning of the proposed algorithm, a lemmatizer should
be used so that dierent inflected forms of words are converted to their base form. For
example, plural nouns should be converted to their singular form. To do this, STeP-1 tool
(Shamsfard, Jafari, & Ilbeygi, 2010b) has been utilized. The STeP-1 package is a set of
fundamental tools for Persian text processing that provides support for tokenization, spell
checking, morphological analysis, and POS tagging.
Another required resource for the proposed method is a bi-lingual machine readable
dictionary. An electronic version of the Aryanpour dictionary3 has been used to extract
the English equivalent for Bijankhan words. Also, WordNet version 3.0 has been used to
extract synsets of their English equivalents.
In the WSD procedure, the context of each word is the sentence containing that word.
A depth-first search in WSD has been performed up to a maximum depth of 3 similar to the
work of Navigli and Ponzetto (2012b). As mentioned before in Section 3.2, if the probability
of the WordNet sense s given for the word w is less than or equal to t, that sense is ignored
in the WSD process of the EM algorithm. In our experiments, we have set t = 0.005.

1. The source code is freely available for download at http://ece.ut.ac.ir/en/node/940
2. See http://ece.ut.ac.ir/dbrg/bijankhan/
3. See http://www.aryanpour.com

74

fiAutomatic Wordnet Development for Low-Resource Languages

Iteration
Entropy

Table 3: Entropy values with respect to the iteration
0
1
2
3
4
5
2.15025 1.83406 1.78804 1.77978 1.77813 1.77680

6
1.77677

4.2 Evaluation Results
In this section, the results of the evaluation of the proposed method in various experiments
are presented.
4.2.1 Convergence of the Proposed Method
The EM algorithm iterates between the expectation and maximization steps, until some
criteria are satisfied. In our experiment, after each iteration, the entropy of synset probabilities per word is calculated and the average of the entropy of all the words is considered.
If the changing of this value in two consecutive iterations becomes near zero, the EM algorithm stops. Formally, the entropy of a probability distribution is defined as equation
6:
H(w) =



w,s log(w,s ).

(6)

s

Entropy is best understood as a measure of uncertainty, as entropy is larger for more
random values. Indeed at first, all links for a Persian word have equal probability, and so
maximum entropy is granted. After each iteration, some links sink under the threshold
probability and thus the probability of the other links increases. It is expected at the final
step that all incorrect links obtain a very low probability and that correct links obtain a
high probability. Therefore, entropy analysis can demonstrate the behaviour of the EM
method in changing probabilities. In Table 3, the entropy values per iteration are shown.
At iteration 6, changing the entropy values reaches the predetermined threshold of 0.001
and the EM algorithm stops.
4.2.2 Precision and Recall of the Wordnet
The primary goal of this work is to construct a high quality wordnet for low-resource
languages. After execution of the EM algorithm, the probability of assigning each candidate
synset to each word in the target language is finalized. These probabilities are sorted and
those links with a probability under the threshold tremove should be removed from the final
wordnet. The value of tremove determines the size of the wordnet and aects the quality
of the wordnet. So, experiments were conducted that used dierent values for the tremove
including 0.1, 0.05, 0.02, 0.01, 0.005 and 0.0.
To evaluate the resulting wordnet, we re-implemented the PMI-based method (Montazery & Faili, 2011) and compared our wordnet with it as a baseline. In all experiments,
our wordnet is referred to as the graph-based wordnet, in contrast to the PMI-based
wordnet. In the evaluation process, two data sets were used: 1) FarsNet 2) Manual judges.
FarsNet is a semi-manually created wordnet in Persian, which is available in two versions;
the second release of FarsNet contains more than 36,000 Persian words and phrases that
are organized into about 20,000 synsets of nouns, adjectives, adverbs and verbs. FarsNet 2
75

fiTaghizadeh & Faili

has also inter-lingual relations that connect some of the Persian synsets to English ones in
the Princeton wordnet 3.0.
The second data set consists of a subset of 1,750 links in the resulting wordnet, which
were selected randomly and judged manually. Each link (w, s) was given to two annotators
to decide if the Persian word w is semantically equal to the WordNet s. To ensure the
accuracy of the judges, annotators were selected among people who are native speakers of
Persian and at the same time learn English professionally. In the case of disagreement between two judges, a third annotator was asked to decide about the link. The inter-annotator
agreement was 80%, which means that in 80% of judgements, the two annotators agreed.
Additionally, we computed Cohens Kappa coecient (Cohen, 1960), for two annotators,
which takes into account the amount of agreement that could be expected to occur through
chance. Kappa is computed as follows:
=

po  pe
,
1  pe

(7)

where po is the relative observed agreement among annotators, and pe is the hypothetical
probability of chance agreement. For our two annotators, the Kappa value was 0.55. In
general, if the annotators are in complete agreement, then  = 1. If no agreement between
annotators other than what would be expected by chance (as given by pe ), then   0.
After carrying out the manual judgements, the precision and recall of the resulting wordnet
were measured on this set.
The precision of the resulting wordnet is defined as the number of correct links in the
wordnet that also exist in the test data as correct links, divided by the total number of links
in the wordnet that exist in the test data. Also, the recall of the wordnet is defined as the
number of correct links in the wordnet that also exist in the test set as correct links, divided
by the total number of correct links in the test set. The accuracy of the wordnet is another
measure, which is defined as the number of correct links in the wordnet that also exist in the
test set plus the number of incorrect links in the test set that do not exist in the wordnet,
divided by the total number of links in the test set. These definitions of precision, recall,
and accuracy of the wordnet were also used in the BabelNet project (Navigli & Ponzetto,
2010).
Figure 4a and Figure 4b represent the precision and recall of the PMI-based method
and the proposed method according to FarsNet. As shown, the precision and recall of our
wordnet is better than the PMI-based method. In these figures, precision is at most 18%,
which seems low for a wordnet to be considered as a reliable resource for that language.
Additionally, recall is at most 49%. This is due to the lack of correct links in FarsNet. In
the evaluation of the resulting wordnet according to FarsNet each link (w, s) can be placed
in one of these categories:
 Persian word w does not exist in FarsNet. This link is ignored and is not counted.
 Persian word w exists in FarsNet; however no WordNet synset is given for it. This
link is ignored and not counted.
 Persian word w exists in FarsNet and at least one WordNet synset is given for it. If
s is one of these WordNet synsets, this link is counted as correct or else it is counted
as incorrect.
76

fiAutomatic Wordnet Development for Low-Resource Languages

The WordNet sense distinctions are too fine-grained, meaning that several WordNet
synsets may be mapped onto one synset in FarsNet; while most of them are not given in
FarsNet. Therefore, some correct links in our wordnet are counted as incorrect. Figure 4c
shows the accuracy of the wordnets according to FarsNet, which shows that the graph-based
wordnet surpasses the PMI-based wordnet.
Some reasons for low precision according to FarsNet are as follows:
 Translations of the Persian words are inaccurate or incomplete, meaning that the
correct WordNet synset according to FarsNet does not exist in the candidate set. For
   J/motaalleqAt/possession, three equivalent
example, for the Persian word  HA
English words are written in the Aryanpour dictionary: Appurtenance, Paraphernalia,
  J/possession are determined
Belongings. In our wordnet, the correct synsets for HA
as follows: {13244109} (noun.possession), property#1, belongings#1, holding#2 
(something owned; any tangible or intangible possession that is owned by someone;
that hat is my property; he is a man of property). However according to FarsNet,
the correct synset is {00032613} (noun.Tops) possession#2  (anything owned or
  J/possession to synset Noun-02671421
possessed). In this evaluation, the link HA
is considered to be incorrect and is penalized.
 The Persian word is not lemmatized correctly; so the English translations and consequently candidate set does not contain the correct synset. For example, the Persian
word  @P A K. /bArAk/Barak is a proper noun, while the stemmer recognizes PA K.
/bAr/load as its stem, which means load.
To resolve the above problems, a set of manually judged links were used in the second
experiment. Figure 5 represents the precision and recall of the resulting wordnet for dierent
values of tremove according to manual judges. Parameter tremove demonstrates a threshold,
so those links with a score lower than it should be deleted from the final wordnet. High
values for tremove result in a wordnet with high precision but low recall. On the other hand,
low values for tremove cause a low precision but high recall wordnet. Thus there is a trade-o
between precision and recall. For tremove = 0.1, the precision of the PMI-based wordnet
is 86%, while precision of the wordnet created by the proposed method is 90% according
to manual judges. If tremove = 0, which means that all links are contained in the final
wordnet, the precision is 74%. Therefore, the initial wordnet seen without executing the
EM algorithm has 74% precision. Figures 4d and 5c show another quality measure for both
wordnets, which is F -measure. Definition of the F -measure and a complete analysis about
it is presented in Section 4.3.
4.2.3 Size and Polysemy Rate of the Wordnet
One of the important aspects of wordnets is their size. Large wordnets may have tens
of thousands of sysnsets (Miller, 1995; Patanakul & Charnyote, 2005; Black et al., 2006;
Piasecki et al., 2011). On the other hand, wordnets with more polysemic words are more
useful in NLP and IR tasks. Polysemic words are those words that have more than one
sense in the wordnet. Finding the correct sense of polysemic words is of great significance
to automatic wordnet construction.
77

fiTaghizadeh & Faili

Graph-based
PMI-based

0.15

Recall

Precision

0.8

0.6

Graph-based
PMI-based

0.1
0

2  102 4  102 6  102 8  102

0.4

0.1

0

2  102 4  102 6  102 8  102

tremove

0.1

tremove

(a) Precision

(b) Recall

0.8

F1

Accuracy

0.25
0.7
0.6
Graph-based
PMI-based

0.5
0

2  102 4  102 6  102 8  102

0.2
Graph-based
PMI-based

0.15

0.1

0

2  102 4  102 6  102 8  102

tremove

0.1

tremove

(c) Accuracy

(d) F-measure

Figure 4: Comparison between the wordnets according to the FarsNet.
0.8

Recall

0.85
Graph-based
PMI-based

0.8
0

2  102 4  102 6  102 8  102

Graph-based
PMI-based

0.6

0.4

0.1

0

2  102 4  102 6  102 8  102

tremove

tremove

(a) Precision

(b) Recall
0.8

Graph-based
PMI-based

0.7
F1

Precision

0.9

0.6
0.5
0

2  102 4  102 6  102 8  102

0.1

tremove

(c) F-measure

Figure 5: Comparison between the wordnets according to the manual judges.
78

0.1

fiAutomatic Wordnet Development for Low-Resource Languages

Table 4: Comparison between size of the wordnets

Threshold
0.1
0.05
0.02
0.01
0.005
0

PMI-based wordnet
unique words word-synset polysemy
11,880
27,358
0.63
11,969
36,922
0.71
11,974
49,070
0.76
11,974
58,874
0.78
11,974
71,761
0.80
11,974
141,103
0.85

Graph-based wordnet
unique words word-synset polysemy
11,899
29,944
0.73
11,972
43,690
0.79
11,972
61,823
0.80
11,972
74,619
0.80
11,972
86,879
0.83
11,972
141,103
0.85

In this section, the size of the resulting wordnet and polysemy rate for two wordnets,
PMI-based and graph-based wordnets, are reported. Table 4 presents the number of unique
words, the number of Persian word-WordNet synset links and the proportion of the polysemic words based on dierent values for tremove . As tremove decreases from 0.1 to 0.01, more
unique words will be contained in wordnets, the number of word-synset links increases, and
also the proportion of polysemic words to unique words in the wordnet increases. As can be
seen, the wordnet created as a result of the graph-based method surpasses the PMI-based
wordnet.
If all links were included in the wordnet, then polysemic words are 85% of the unique
words. However, in our wordnet, by removing those links with a probability of less than
0.1, 73% of words are polysemic, which is 10% better than the PMI-base wordnet. For
tremove = 0.1, both wordnets have about 12,000 unique words. Since both methods were
executed on the same corpus, there is no significant dierence in their sizes.
4.2.4 Coverage of the Wordnet
To evaluate the coverage of the resulting wordnet, we are interested in observing the coverage
over WordNet synsets and also the coverage over language words. In this section, three
experiments were performed: 1) core concepts coverage, 2) WordNet synset coverage, and
3) corpus coverage.
In the first experiment, the coverage of our wordnet over core synsets is evaluated. BoydGraber et al. (2006) published a list of about 5,000 word-senses in WordNet 3.0, which
contains the 5,000 most frequently used word-senses (Core WordNet, 2015). Coverage of
a wordnet over this list can be regarded as covering the most common concepts of the
language. So this core wordnet was used to measure the percentage of synsets from this list
covered by PMI-based and graph-based wordnets. Figure 6a represents the core coverage
for dierent values of tremove . Selecting all links, (tremove = 0), causes coverage of 88% of
the core wordnet, while choosing links that are more probable than 0.1, leads to coverage of
53% and 34% of the core wordnet for graph-based and PMI-based wordnets, respectively.
In the second experiment, the coverage of wordnets over all WordNet synsets is studied.
Since the resulting wordnet is a multi-lingual wordnet, its coverage of it over WordNet
synsets is a measure of its quality. Figure 6b represents the coverage of PMI-based and
graph-based wordnets over WordNet 3.0 synsets for dierent values of tremove . This figure
shows that the graph-based wordnet covers more WordNet synsets than PMI-based wordnet
for all values of tremove . For example, by selecting links with a probability higher than 0.1,
79

fiGraph-based
PMI-based

0.8
Core Coverage

WordNet synsets Coverage

Taghizadeh & Faili

0.6

0.4
0

2  102 4  102 6  102 8  102

0.1

0.25

Graph-based
PMI-based

0.2
0.15
0.1
2  102 4  102 6  102 8  102

0

tremove

0.1

tremove

(a) Core Coverage

(b) WordNet Coverage

Figure 6: Coverage of the wordnets over the core synsets and all synsets of the WordNet.
Table 5: Comparison between coverage of the wordnets.

FarsNet
PMI-based wordnet
graph-based wordnet

Coverage over the Bijankhan (unique words)
3,050
11,523
11,543

the graph-based wordnet covers 14% of WordNet synsets; while the PMI-based wordnet
covers 10% of WordNet synsets.
In the third experiment, the coverage of wordnets over the Bijankhan corpus is evaluated.
Bijankhan is a large corpus and the proposed method was trained over 13% of it. The rest
of this corpus was used for measuring the word coverage of wordnets. Table 5 demonstrates
the number of unique words of the corpus, covered by PMI-based and graph-based wordnets,
when tremove = 0.1. The same evaluation was also performed on FarsNet as the baseline
and is also presented in the Table 5. Although the training and testing corpus are separate,
there is a significance dierence between FarsNet and EM-based wordnets coverage.
4.3 Parameter Selection
In the proposed method for wordnet construction after convergence of the EM algorithm,
a set of links between words of the target language and synsets of the source language is
obtained. The links that scored lower than the threshold tremove are removed from the final
wordnet. As the previous experiments showed, the value of the tremove aects the quality
of the resulting wordnet. The experiments in section 4.2.2 illustrated that changing tremove
from 0.005 to 0.1 has a positive eect on the precision but a negative eect on the recall
of the resulting wordnet. Indeed, there is a trade-o between the precision and the recall.
Here a question may arise; what is the best value for the tremove .
In this section, F -measure is used to investigate the quality of the wordnet, considering
both precision and recall. The formula of the F1 is as follows:
F1 = 2.

precision.recall
,
precision + recall
80

(8)

fiAutomatic Wordnet Development for Low-Resource Languages

F1 is the harmonic mean of the precision and the recall. In order to gain some insight
into the optimum value of tremove , the F1 for the resulting wordnet has been calculated for
dierent values of tremove . As for precision and recall, the F1 is calculated against both
the manual judgement and the FarsNet. Figure 5c shows that F1 decreases from 77% to
50% when tremove increases from 0.005 to 0.1 for the graph-based wordnet according to the
manual judgement. This means that the precision value is more important than that of the
recall and the rate at which precision is decreasing is higher than the rate at which recall is
increasing. Therefore, to gain a more precise wordnet, we should increase tremove ; however,
we must accept loosing the recall.
On the other hand, Figure 4d shows that the highest value of the F1 for the graph-based
wordnet is obtained when tremove = 0.1 according to FarsNet. This fact means that the
recall value has more eect on the F1 than the precision value. The reason for this dierence
is due to the low precision values that have been obtained in the evaluation according to
FarsNet, as reported in Section 4.2.2. FarsNet lacks most of the correct mappings between
the Persian words and the WordNet synsets. Indeed in wordnet construction, the precision
of the final wordnet is more important than its recall.
Finally, choosing the threshold tremove has important eect on the quality of the resulting
wordnet. However, this matter depends on the application. In most applications, having
a more precise wordnet is preferential to having a large but not accurate enough one. In
these cases, greater values for tremove is preferential. Although, in applications that high
recall is needed, one should choose low values for tremove .
4.4 The Eect of Corpus Size and Dictionary
In this section, the eect of the required resources on the final wordnet is looked at. The
proposed method needs a bi-lingual dictionary and a mono-lingual corpus. In the previous
experiments, the Aryanpour dictionary and the Bijankhan corpus were used. Since the
Bijankhan is a large corpus only 13% of it was used in the previous experiments. To
investigate the eect of corpus size on the quality of the resulting wordnet, the proposed
method has been executed using four other sizes of the Bijankhan: 5%, 10%, 20% and 50%.
Additionally, to examine the eect of the dictionary on the quality of the final wordnet, the
Google translator4 is used in another experiment instead of the Aryanpour; the resulting
wordnet is then compared against the wordnet created using Aryanpour on the same size
of the Bijankhan. The link removal threshold tremove for all experiments in this section is
0.1. The resulting wordnets have been evaluated for precision, recall, accuracy, coverage
over the WordNet core synsets, coverage over all synsets of the WordNet, and the number
of the Persian words.
As it is shown in Figure 7, when the size of the corpus increases from 5% to 50% of
the Bijankhan corpus using the same dictionary, all measures increase except for precision,
which either does not change or changes only slightly. This result is not beyond expectation. Indeed, the precision of the resulting wordnet depends on the precision of the WSD
procedure and so does not depend on the size of the corpus. However, new possible senses
of the words are discovered by increasing the size of the corpus and therefore the recall,
accuracy, coverage and the size of the wordnet increase with growth of the corpus size. As
4. http://translate.google.com/

81

fiTaghizadeh & Faili

Aryanpour dictionary
Google translator
0.5

0.38

0.9

0.89

accuracy

recall

precision

0.36
0.34
0.32

0.48

0.46

0.3
0.28

0.88
5 10

20

5 10

50

20

5 10

50

20

(a) precision

50
size

size

size

(b) recall

(c) accuracy
104

0.56

0.16

1.4

0.52
0.5
0.48
0.46

number of words

synset coverage

core coverage

0.54
0.14

0.12

1.2

1

0.1

0.8

0.44
5 10

20

50

5 10

20

size

(d) core coverage

50
size

(e) WordNet synset coverage

5 10

20

50
size

(f) number of words

Figure 7: Evaluation of the resulting wordnet trained on dierent sizes of Bijankhan.

Figure 7f demonstrates, to have a wordnet with at least 10,000 words, the corpus size should
be at least 10% of the Bijankhan corpus. Figure 7 also illustrates that the wordnet trained
on the Aryanpour dictionary excels the wordnet derived from the Google translator. This
experiment demonstrates that the dictionary heavily aects the final wordnet even more
than the corpus size. As a result, having a small corpus but a large dictionary results in a
more precise wordnet than having a large corpus but a small dictionary.
In the last experiment, the proposed method has been executed using the full Bijankhan
corpus and the Aryanpour dictionary. The precision, recall and accuracy of the resulting
wordnet are 90%, 41% and 52%, respectively. Comparing to the wordnet, which was created
using 13% of the Bijankhan and the same dictionary, recall and accuracy increased 6% and
3%, accordingly; while the precision does not change. This wordnet has 15,406 Persian
word and covers 61% of the core synsets of the WordNet. Considering all synsets of the
WordNet, it covers 20% of them.

5. Conclusion
In this paper, an EM algorithm was employed in order to develop a wordnet for lowresourced languages. We successfully applied unsupervised cross lingual WSD in the expectation step of the algorithm. The proposed method does not use any features specific
82

fiAutomatic Wordnet Development for Low-Resource Languages

to the target language, and so it can be used for other languages to generate wordnets.
Resources needed for this proposed algorithm include a bi-lingual dictionary and a monolingual corpus. The proposed method belongs to the expansion approach and so creates a
multi-lingual wordnet in which for each word in the target language, the equivalent synset
in WordNet is known.
The proposed method was applied on the Persian language and the quality of the resulting wordnet was examined through several experiments. Its precision was 18% according
to FarsNet and 90% according to the manual judgement. The reason for this dierence is
that the WordNet synsets are too fine-grained in comparison to the FarsNet synsets, and so
most of the synsets in FarsNet should be mapped onto more than one synset in WordNet;
however FarsNet provides one or at most two WordNet synsets for most of FrasNet synsets.
This problem means that most of the correct links in the resulting wordnet are considered
to be incorrect and thus the reported precision becomes low. Also, the resulting wordnet
contains about 12,000 words of the Persian language from only using 13% of the Bijankhan
corpus, which is more than several wordnets in other languages. Additionally, 53% of core
synsets and 14% of all synsets of WordNet are covered. Analysis of the eects of corpus
size and dictionary size of the resulting wordnet showed that the dictionary size can aect
the precision of the wordnet more than the corpus size and therefore it is important to use
large-enough dictionaries.

Acknowledgements
This research was in part supported from Institute for Research in Fundamental Sciences
(No. CS1395-4-19).

References
Apidianaki, M., & Sagot, B. (2014). Data-driven synset induction and disambiguation for
wordnet development. Language Resources and Evaluation, 48 (4), 655677.
Atserias, J., Climent, S., Farreres, X., Rigau, G., & Rodrguez, H. (2000). Combining
multiple methods for the automatic construction of multilingual wordnets. Amsterdam
studies in the theory and history of linguistic science series 4, 327340.
Barbu, E., & Barbu Mititelu, V. (2005). A case study in automatic building of wordnets.
In Proceedings of OntoLex 2005- Ontologies and Rexical Resources, pp. 8590, Jeju
Island, Korea. Asian Federation of Natural Language Processing.
Basile, P., Caputo, A., & Semeraro, G. (2014). An enhanced lesk word sense disambiguation
algorithm through a distributional semantic model. In Proceedings of COLING 2014,
the 25th International Conference on Computational Linguistics: Technical Papers,
pp. 15911600, Dublin, Ireland. International Committee on Computational Linguistics.
Black, W., Elkateb, S., & Vossen, P. (2006). Introducing the Arabic wordnet project. In
Proceedings of the Third International WordNet Conference (GWC-06), pp. 295299,
South Jeju Island, Korea. Global WordNet Association.
83

fiTaghizadeh & Faili

Bond, F., & Foster, R. (2013). Linking and extending an open multilingual wordnet. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,
pp. 13521362, Sofia, Bulgaria. Association for Computational Linguistics.
Bond, F., Isahara, H., Kanzaki, K., & Uchimoto, K. (2008). Boot-strapping a wordnet using
multiple existing wordnets. In Proceedings of the Sixth International Conference on
Language Resources and Evaluation (LREC08), pp. 16191624, Marrakech, Morocco.
European Language Resources Association (ELRA).
Boudabous, M. M., Chaaben Kammoun, N., Khedher, N., Belguith, L. H., & Sadat, F.
(2013). Arabic wordnet semantic relations enrichment through morpho-lexical patterns. In Proceeding of 1st International Conference on Communications, Signal Processing, and their Applications (ICCSPA), pp. 16, American University of Sharjah,
United Arab Emirates. IEEE.
Boyd-Graber, J., Fellbaum, C., Osherson, D., & Schapire, R. (2006). Adding dense, weighted
connections to WordNet. In Proceedings of the third International WordNet Conference (GWC-06), pp. 2935, South Jeju Island, Korea. Global WordNet Association.
Buitelaar, P., & Cimiano, P. (2014). Towards the Multilingual Semantic Web. Springer
Berlin Heidelberg.
Cohen, J. (1960). A coecient of agreement for nominal scales. Educational and Psychological Measurement, 20 (1), 3746.
Core

WordNet (2015)
core-wordnet.txt.

http://wordnetcode.princeton.edu/standoff-files/

Diab, M. (2004). The feasibility of bootstrapping an Arabic wordnet leveraging parallel
corpora and an English WordNet. In Proceedings of the Arabic Language Technologies
and Resources, Cairo, NEMLAR.
Dini, L., Peters, W., Liebwald, D., Schweighofer, E., Mommers, L., & Voermans, W. (2005).
Cross-lingual legal information retrieval using a WordNet architecture. In Proceedings
of the 10th international conference on Artificial intelligence and law (ACAIL), pp.
163167, Bologna, Italy. ACM.
Elkateb, S., Black, W., Rodrguez, H., Alkhalifa, M., Vossen, P., Pease, A., & Fellbaum, C.
(2006). Building a wordnet for Arabic. In Proceedings of The 5th international conference on Language Resources and Evaluation (LREC 2006), Genoa, Italy. European
Language Resources Association (ELRA).
Erjavec, T., & Fiser, D. (2006). Building Slovene wordnet. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006), Genoa,
Italy. European Language Resources Association (ELRA).
Fellbaum, C., & Vossen, P. (2012). Challenges for a multilingual wordnet. Language Resources and Evaluation, 46 (2), 313326.
Fiser, D. (2009). Human language technology. In Leveraging Parallel Corpora and Existing
Wordnets for Automatic Construction of the Slovene Wordnet, pp. 359368. Springer
Berlin Heidelberg.
84

fiAutomatic Wordnet Development for Low-Resource Languages

Gunawan, G., & Saputra, A. (2010). Building synsets for Indonesian wordnet with monolingual lexical resources. In Proceedings of International Conference on Asian Language
Processing (IALP), pp. 297300, Harbin, China. IEEE.
Hasanuzzaman, M., Caen, F., Dias, G., Ferrari, S., & Mathet, Y. (2014). Propagation strategies for building temporal ontologies. In Proceedings of 14rd conference on European
Chapter of the Association for Computational Linguistics, pp. 611, Guthenburg, Sweden. Association for Computational Linguistics.
Kaji, H., & Watanabe, M. (2006). Automatic construction of Japanese wordnet. In Proceedings of the 5th International Conference on Language Resources and Evaluation
(LREC 2006), Genoa, Italy. European Language Resources Association (ELRA).
Kazakov, D., & Shahid, A. R. (2009). Unsupervised construction of a multilingual wordnet
from parallel corpora. In Proceedings of the Workshop on Natural Language Processing
Methods and Corpora in Translation, Lexicography, and Language Learning, pp. 912,
Borovets, Bulgaria. Association for Computational Linguistics.
Lam, K. N., Al Tarouti, F., & Kalita, J. (2014). Automatically constructing wordnet synsets.
In 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014),
pp. 106111, Baltimore, USA. Association for Computational Linguistics.
Landes, S., Leacock, C., & Tengi, R. I. (1998). Building semantic concordances. WordNet:
an electronic lexical database, 199 (216), 199216.
Mallery, J. C. (1988). Thinking about foreign policy: Finding an appropriate role for artificially intelligent computers. Ph.D. thesis, MIT Political Science Department.
Miller, G. A. (1995). WordNet: a lexical database for english. Communications of the ACM,
38 (11), 3941.
Montazery, M., & Faili, H. (2010). Automatic Persian wordnet construction. In Proceedings
of the 23rd International Conference on Computational Linguistics: Posters, pp. 846
850, Beijing, China. Association for Computational Linguistics.
Montazery, M., & Faili, H. (2011). Unsupervised learning for Persian wordnet construction.
In Proceedings of Recent Advances in Natural Language Processing (RANLP), pp.
302308, Hissar, Bulgaria. Association for Computational Linguistics.
Navigli, R. (2009). Word sense disambiguation: A survey.
(CSUR), 41 (2), 10.

ACM Computing Surveys

Navigli, R., & Lapata, M. (2010). An experimental study of graph connectivity for unsupervised word sense disambiguation. Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 32 (4), 678692.
Navigli, R., & Ponzetto, S. P. (2010). BabelNet: Building a very large multilingual semantic network. In Proceedings of the 48th annual meeting of the association for computational linguistics, pp. 216225, Uppsala, Sweden. Association for Computational
Linguistics.
Navigli, R., & Ponzetto, S. P. (2012a). BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193, 217250.
85

fiTaghizadeh & Faili

Navigli, R., & Ponzetto, S. P. (2012b). Multilingual WSD with just a few lines of code: the
BabelNet API. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pp. 6772, Jeju, Republic of Korea. Association
for Computational Linguistics.
Oliver, A., & Climent, S. (2012). Parallel corpora for wordnet construction: machine translation vs. automatic sense tagging. In Proceedings of the 13th International Conference
on Intelligent Text Processing and Computational Linguistics, pp. 110121, New Delhi,
India. Springer.
Oroumchian, F., Tasharofi, S., Amiri, H., Hojjat, H., & Raja, F. (2006). Creating a feasible
corpus for Persian POS tagging. Tech. rep. TR3/06, University of Wollongong, Dubai.
Otegi, A., Arregi, X., Ansa, O., & Agirre, E. (2015). Using knowledge-based relatedness for
information retrieval. Knowledge and Information Systems, 44 (3), 689718.
Patanakul, S., & Charnyote, P. (2005). Construction of Thai wordnet lexical database
from machine readable dictionary. In Conference Proceedings: the tenth Machine
Translation Summit, pp. 8792, Phuket, Thailand. Language Technology World.
Piasecki, M., Kurc, R., & Broda, B. (2011). Heterogeneous knowledge sources in graph-based
expansion of the Polish wordnet. In Intelligent Information and Database Systems,
Vol. 6591, pp. 307316. Springer.
Prabhu, V., Desai, S., Redkar, H., Prabhugaonkar, N., Nagvenkar, A., & Karmali, R. (2012).
An ecient database design for IndoWordNet development using hybrid approach. In
Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language
Processing (SANLP), pp. 229236, Mumbai, India. International Committee on Computational Linguistics.
Rodrquez, H., Farwell, D., Ferreres, J., Bertran, M., Alkhalifa, M., & Mart, M. A.
(2008). Arabic wordnet: Semi-automatic extensions using Bayesian inference. In
Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC08), Marrakech, Morocco. European Language Resources Association
(ELRA).
Saveski, M., & Trajkovski, I. (2010). Automatic construction of wordnets by using machine translation and language modeling. In Proceedings of the 13th International
Multiconference, pp. 7883, Ljubljana, Slovenia. Information Society.
Semantically Tagged glosses (2016) http://wordnet.princeton.edu/glosstag.shtml.
Shamsfard, M. (2008). Towards semi automatic construction of a lexical ontology for Persian. In Proceedings of the 6th International Conference on Language Resources and
Evaluation (LREC 2008), Marrakech, Morocco. European Language Resources Association (ELRA).
Shamsfard, M., Hesabi, A., Fadaei, H., Mansoory, N., Famian, A., Bagherbeigi, S., Fekri, E.,
Monshizadeh, M., & Assi, S. M. (2010a). Semi automatic development of FarsNet; the
Persian wordnet. In Proceedings of 5th Global WordNet Conference, Mumbai, India.
Global WordNet Association.
86

fiAutomatic Wordnet Development for Low-Resource Languages

Shamsfard, M., Jafari, H. S., & Ilbeygi, M. (2010b). STeP-1: A set of fundamental tools for
Persian text processing. In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC 2010), Valletta, Malta. European Language
Resources Association (ELRA).
Tufis, D., Cristea, D., & Stamou, S. (2004). BalkaNet: Aims, methods, results and perspectives. a general overview. Romanian Journal of Information Science and Technology,
7 (1-2), 943.
Vossen, P. (1998). Introduction to EuroWordNet. In EuroWordNet: A multilingual database
with lexical semantic networks, pp. 117. Springer.

87

fi