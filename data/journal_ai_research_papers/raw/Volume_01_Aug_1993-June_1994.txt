Journal of Articial Intelligence Research 1 (1993) 91-107

Submitted 8/93; published 11/93

The Diculties of Learning Logic Programs with Cut
Francesco Bergadano

bergadan@di.unito.it

Daniele Gunetti
Umberto Trinchero

gunetti@di.unito.it
trincher@di.unito.it

Universita di Catania, Dipartimento di Matematica,
via Andrea Doria 6, 95100 Catania, Italy
Universita di Torino, Dipartimento di Informatica,
corso Svizzera 185, 10149 Torino, Italy

Abstract

As real logic programmers normally use cut (!), an eective learning procedure for logic
programs should be able to deal with it. Because the cut predicate has only a procedural
meaning, clauses containing cut cannot be learned using an extensional evaluation method,
as is done in most learning systems. On the other hand, searching a space of possible
programs (instead of a space of independent clauses) is unfeasible. An alternative solution
is to generate rst a candidate base program which covers the positive examples, and then
make it consistent by inserting cut where appropriate. The problem of learning programs
with cut has not been investigated before and this seems to be a natural and reasonable
approach. We generalize this scheme and investigate the diculties that arise. Some of the
major shortcomings are actually caused, in general, by the need for intensional evaluation.
As a conclusion, the analysis of this paper suggests, on precise and technical grounds, that
learning cut is dicult, and current induction techniques should probably be restricted to
purely declarative logic languages.

1. Introduction

Much recent research in AI and Machine Learning is addressing the problem of learning
relations from examples, especially under the title of Inductive Logic Programming (Muggleton, 1991). One goal of this line of research, although certainly not the only one, is the
inductive synthesis of logic programs. More generally, we are interested in the construction
of program development tools based on Machine Learning techniques. Such techniques now
include ecient algorithms for the induction of logical descriptions of recursive relations.
However, real logic programs contain features that are not purely logical, most notably the
cut (!) predicate. The problem of learning programs with cut has not been studied before
in Inductive Logic Programming, and this paper analyzes the diculties involved.

1.1 Why Learn Programs with Cut?

There are two main motivations for learning logic programs with cut:
1. ILP should provide practical tools for developing logic programs, in the context of
some general program development methodology (e.g., (Bergadano, 1993b)); as real
size logic programs normally contain cut, learning cut will be important for creating
an integrated Software Engineering framework.

c 1993 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Bergadano, Gunetti, & Trinchero

2. Extensive use of cut can make programs sensibly shorter, and the diculty of learning
a given logic program is very much related to its length.
For both of these objectives, we need not only cuts that make the programs more
ecient without changing their input-output behavior (\green cuts"), but also cuts that
eliminate some possible computed results (\red cuts"). Red cuts are sometimes considered
bad programming style, but are often useful. Moreover, only the red cuts are eective in
making programs shorter. Green cuts are also important, and less controversial. Once a
correct program has been inferred via inductive methods, it could be made more ecient
through the insertion of green cuts, either manually or by means of automated program
transformation techniques (Lau & Clement, 1993).

1.2 Why Standard Approaches Cannot be Used?
Most Machine Learning algorithms generate rules or clauses one at a time and independently
of each other: if a rule is useful (it covers some positive example) and correct (it does not
cover any negative example), then it is added to the description or program which is being
generated, until all positive examples have been covered. This means that we are searching
a space of possible clauses, without backtracking. This is obviously a great advantage, as
programs are sets of clauses, and therefore the space of possible programs is exponentially
larger.
The one principle which allows this simplication of the problem is the extensional
evaluation of possible clauses, used to determine whether a clause C covers an example
e. The fact that a clause C covers an example e is then used as an approximation of the
fact that a logic program containing C derives e. Consider, for instance, the clause C =
\p(X,Y) ", and suppose the example e is p(a,b). In order to see whether C covers e, the
extensionality principle makes us evaluate any literal in  as true if and only if it matches
some given positive example. For instance, if  = q(X,Z) ^ p(Z,Y), then the example p(a,b)
is extensionally covered i there is a ground term c such that q(a,c) and p(c,b) are given
as positive examples. In particular, in order to obtain the truth value of p(c,b), we will
not need to call other clauses that were learned previously. For this reason, determining
whether C covers e only depends on C and on the positive examples. Therefore, the learning
system will decide whether to accept C as part of the nal program P independently of the
other clauses P will contain.
The extensionality principle is found in Foil (Quinlan, 1990) and its derivatives, but is
also used in bottom-up methods such as Golem (Muggleton & Feng, 1990). Shapiro's MIS
system (Shapiro, 1983) uses it when rening clauses, although it does not when backtracing
inconsistencies. We have also used an extensional evaluation of clauses in the FILP system
(Bergadano & Gunetti, 1993).
When learning programs with cut, clauses are no longer independent and their standalone extensional evaluation is meaningless. When a cut predicate is evaluated, other possible clauses for proving the same goal will be ignored. This changes the meaning of these
other clauses. Even if a clause extensionally covers some example e, it may be the case that
the nal program does not derive e, because some derivation paths have been eliminated
by the evaluation of a cut predicate.
92

The Difficulties of Learning Logic Programs with Cut

However, an exhaustive search in a space of programs is prohibitive. Learning methods,
even if based on extensionality, are often considered inecient if sucient prior information
is not available; searching for sets of clauses will be exponentially worse. This would amount
to a brute-force enumeration of all possible logic programs containing cut, until a program
that is consistent with the given examples is found.

1.3 Is there an Alternative Method?

Cut will only eliminate some computed results, i.e., after adding cut to some program, it
may be the case that some example is no longer derived. This observation suggests a general
learning strategy: a base program P is induced with standard techniques, given the positive
and maybe some of the negative examples, then the remaining negative examples are ruled
out by inserting cut in some clause of P. Obviously, after inserting cut, we must make sure
that the positive examples may still be derived.
Given the present technology and the discussion above, this seems to be the only viable
path to a possible solution. Using standard techniques, the base program P would be generated one clause at a time, so that the positive examples are extensionally covered. However,
we think this view is too restrictive, as there are programs which derive all given positive
examples, although they do not cover them extensionally (Bergadano, 1993a; DeRaedt,
Lavrac, & Dzeroski, 1993). More generally, we consider traces of the positive examples:

Denition 1 Given a hypothesis space S of possible clauses, and an example e such that S
`

e, the set of clauses TS which is used during the derivation of e is called a trace for e.

We will use as a candidate base program P any subset of S which is the union of some
traces for the positive examples. If PS extensionally covers the positive examples, then it
will also be the union of such traces, but the converse is not always true. After a candidate
program has been generated, an attempt is made to insert cuts so that the negative examples
are not derived. If this is successful, we have a solution, otherwise, we backtrack to another
candidate base program. We will analyze the many problems inherent in learning cut with
this class of trace-based learning methods, but, as we discuss later (Section 4), the same
problems need to be faced in the more restrictive framework of extensional evaluation. In
other words, even if we choose to learn the base program P extensionally, and then we
try to make it consistent by using cut, the same computational problems would still arise.
The main dierence is that standard approaches based on extensionality do not allow for
backtracking and do not guarantee that a correct solution is found (Bergadano, 1993a).
As far as computational complexity is concerned, trace-based methods have a complexity
standing between the search in a space of independent clauses (for the extensional methods)
and the exhaustive search in a space of possible programs. We need the following:

Denition 2 Given a hypothesis space S, the depth of an example e is the maximum

number of clauses in S successfully used in the derivation of e.

For example, if we are in a list processing domain, and S only contains recursive calls of
the type \P([HjT]) :- ..., P(T), ..." then the depth of an example P(L) is the length of L.
For practical program induction tasks, it is often the case that the depth of an example is
93

Bergadano, Gunetti, & Trinchero

related to its complexity, and not to the hypothesis space S. If d is the maximum depth for
the given m positive examples, then the complexity of trace-based methods is of the order
of jS jmd, while extensional methods will just enumerate possible clauses with a complexity
which is linear in jS j, and enumerating all possible programs is exponential in jS j.

2. A Simple Induction Procedure
The trace-based induction procedure we analyze here takes as input a nite set of clauses
S and a set of positive and negative examples E+ and E- and tries to nd a subset T of S
such that T derives all the positive examples and none of the negative examples. For every
positive example e+ 2 E+, we assume that S is large enough to derive it. Moreover, we
assume that all clauses in S are attened1 . If this is not the case, clauses are attened as a
preprocessing step.
We consider one possible proof for S ` e+, and we build an intermediate program T  S
containing a trace of the derivation. The same is done for the other positive examples, and
the corresponding traces T are merged. Every time T is updated, it is checked against the
negative examples. If some of them are derived from T, cut (!) is inserted in the antecedents
of the clauses in T, so that a consistent program is found, if it exists. If this is not the case,
the procedure backtracks to a dierent proof for S ` e+. The algorithm can be informally
described as follows:
input: a set of clauses S
a set of positive examples E+
a set of negative examples ES := atten(S)
T ;
For each positive example e+ 2 E+
nd T1  S such that T1 `SLD e+ (backtracking point 1)
T T [ T1
if T derives some negative example e- then trycut(T,e-)
if trycut(T,e-) fails then backtrack
output the clauses listed in T
trycut(T,e-):
insert ! somewhere in T (backtracking point 2) so that
1. all previously covered positive examples are still derived from T, and
2. T 6`SLD e-

The complexity of adding cut somewhere in the trace T, so that the negative example eis no longer derived, obviously only depends on the size of T. But this size depends on the
depth of the positive examples, not on the size of the hypothesis space S. Although more
1. A clause is flattened if it does not contain any functional symbol. Given an unattened clause, it is alway
possible to atten it (by turning functions into new predicates with an additional argument representing
the result of the function) and vice versa (Rouveirol, in press).

94

The Difficulties of Learning Logic Programs with Cut

clever ways of doing this can be devised, based on the particular example e-, we propose a
simple enumerative technique in the implementation described in the Appendix.

3. Example: Simplifying a List

In this section we show an example of the use of the induction procedure to learn the logic
program \simplify ". Simplify takes as input a list whose members may be lists, and
transforms it into a \attened" list of single members, containing no repetitions and no
lists as members. This program appears as exercise number 25 in (Coelho & Cotta, 1988),
is composed of nine clauses (plus the clauses for append and member); six of them are
recursive, one is doubly-recursive and cut is extensively used. Even if simplify is a not a
very complex logic program, it is more complex than usual ILP test cases. For instance,
the quicksort and partition program, which is very often used, is composed of only ve
clauses (plus those for append), and three of them are recursive. Moreover, note that the
conciseness of simplify is essentially due to the extensive use of cut. Without cut, this
program would be much longer. In general, the longer a logic program, the more dicult
to learn it.
As a consequence, we start with a relatively strong bias; suppose that the following
hypothesis space of N=8449 possible clauses is dened by the user:



The clause \simplify(L,NL) :- atten(L,L1), remove(L1,NL)."
All clauses whose head is \atten(X,L)" and whose body is composed of a conjunction
of any of the following literals:
head(X,H), tail(X,L1), equal(X,[L1,T]), null(T), null(H), null(L1), equal(X,[L1]),
atten(H,X1), atten(L1,X2),
append(X1,X2,L), assign(X1,L), assign(X2,L), list(X,L).



All clauses whose head is \remove(IL,OL)" and whose body is composed of a conjunction of any of the following literals:
cons(X,N,OL), null(IL), assign([],OL),
head(IL,X), tail(IL,L), member(X,L), remove(L,OL), remove(L,N).



The correct clauses for null, head, tail, equal, assign, member, append are given:
null([]).
head([Hj ],H).
tail([ jT],T).
equal(X,X).
assign(X,X).
member(X,[Xj ]).
member(X,[ jT]) :- member(X,T).
95

Bergadano, Gunetti, & Trinchero

append([],Z,Z).
append([HjX],Y,[HjZ]) :- append(X,Y,Z).
By using various kinds of constraints, the initial number of clauses can be strongly reduced.
Possible constraints are the following:
 Once an output is produced it must not be instantiated again. This means that any
variable cannot occur as output in the antecedent more than once.
 Inputs must be used: all input variables in the head of a clause must also occur in its
antecedent.
 Some conjunctions of literals are ruled out because they can never be true, e.g.
null(IL)^head(IL,X).
By applying various combination of these constraints it is possible to strongly restrict the
initial hypothesis space, which is then given in input to the learning procedure. The set of
positive and negative examples used in the learning task is:
simplify pos([[[],[b,a,a]],[]],[b,a]). remove pos([a,a],[a]).
(simplify neg([[[],[b,a,a]],[]],X),not equal(X,[b,a])).
simplify neg([[a,b,a],[]],[a,[b,a]]). remove neg([a,a],[a,a]).
Note that we dene some negative examples of simplify to be all the examples with
the same input of a given positive example and a dierent output, for instance simplify neg([[[],[b,a,a]],[]],[a,b]). Obviously, it is also possible to give negative examples as
normal ground literals. The learning procedure outputs the program for simplify reported
below, which turns out to be substantially equivalent to the one described in (Coelho &
Cotta, 1988) (we have kept clauses unattened).
simplify(L,NL) :- atten(L,L1), remove(L1,NL).
atten(X,L) :- equal(X,[L1,T]), null(T), !, atten(L1,X2), assign(X2,L).
atten(X,L) :- head(X,H), tail(X,L1), null(H), !, atten(L1,X2), assign(X2,L).
atten(X,L) :- equal(X,[L1]), !, atten(L1,X2), assign(X2,L).
atten(X,L) :- head(X,H), tail(X,L1), !,
atten(H,X1), !, atten(L1,X2), append(X1,X2,L).
atten(X,L) :- list(X,L).
remove(IL,OL) :- head(IL,X), tail(IL,L), member(X,L), !, remove(L,OL).
remove(IL,OL) :- head(IL,X), tail(IL,L), remove(L,N), cons(X,N,OL).
remove(IL,OL) :- null(IL), assign([],OL).
The learning task takes about 44 seconds on our implementation. However, This is obtained
at some special conditions, which are thoroughly discussed in the next sections:
 All the constraints listed above are applied, so that the nal hypothesis space is
reduced to less than one hundred clauses.
96

The Difficulties of Learning Logic Programs with Cut





Clauses in the hypothesis space are generated in the correct order, as they must appear
in the nal program. Moreover, literals in each clause are in the correct position. This
is important, since in a logic program with cut the relative position of clauses and
literals is signicant. As a consequence, we can learn simplify without having to test
for dierent clause and literal orderings (see subsections 4.2 and 4.5).
We tell the learning procedure to use at most two cuts per clause. This seems to be
quite an intuitive constraint since, in fact, many classical logic programs have no more
than one cut per clause (see subsections 4.1 and 5.4).

4. Problems

Experiments with the above induction procedure have shown that many problems arise when
learning logic programs containing cut. In the following, we analyze these problems, and
this is a major contribution of the present paper. As cut cannot be evaluated extensionally,
this analysis is general, and does not depend on the specic induction method adopted.
Some possible partial solutions will be discussed in Section 5.

4.1 Problem 1: Intensional Evaluation, Backtracking and Cut

The learning procedure of Section 2 is very simple, but it can be inecient. However,
we believe this is common to every intensional method, because clauses cannot be learned
independently of one another. As a consequence, backtracking cannot be avoided and this
can have some impact on the complexity of the learning process. Moreover, cut must be
added to every trace covering negative examples. If no constraints are in force, we can
range from only one cut in the whole trace to a cut between each two literals of each clause
in the trace. Clearly, the number of possibilities is exponential in the number of literals in
the trace. Fortunately, this number is usually much smaller than the size of the hypothesis
space, as it depends on the depth of the positive examples.
However, backtracking also has some advantages; in particular, it can be useful to search
for alternative solutions. These alternative programs can then be confronted on the basis of
any required characteristic, such as simplicity or eciency. For example, using backtracking
we discovered a version of simplify equivalent to the one given but without the cut predicate
between the two recursive calls of the fourth clause of flatten.

4.2 Problem 2: Ordering of Clauses in the Trace

In a logic program containing cut, the mutual position of clauses is signicant, and a dierent ordering can lead to a dierent (perhaps wrong) behavior of the program. For example,
the following program for intersection:

c1) int(X,S2,Y) :- null(X), null(Y).
c2) int(X,S2,Y) :- head(X,H), tail(X,Tail), member(H,S2), !, int(Tail,S2,S), cons(H,S,Y).
c3) int(X,S2,Y) :- head(X,H), tail(X,Tail), int(Tail,S2,Y).
behaves correctly only if c2 comes before c3. Suppose the hypothesis space given in input
to the induction procedure consists of the same three clauses as above, but with c3 before
97

Bergadano, Gunetti, & Trinchero

c2. If :int([a],[a],[]) is given as a negative example, then the learning task fails, because
clauses c1 and c3 derive that example.

In other words, learning a program containing cut means not only to learn a set of
clauses, but also a specic ordering for those clauses. In terms of our induction procedure
this means that for every trace T covering some negative example, we must check not only
every position for inserting cuts, but also every possible clause ordering in the trace. This
\generate and test" behavior is not dicult to implement, but it can dramatically decrease
the performance of the learning task. In the worst case all possible permutations must be
generated and checked, and this requires a time proportional to (md)! for a trace of md
clauses2 .
The necessity to test for dierent permutations of clauses in a trace is a primary source
of ineciency when learning programs with cut, and probably the most dicult problem
to solve.

4.3 Problem 3: Kinds of Given Examples

Our induction procedure is only able to learn programs which are traces, i.e. where every
clause in the program is used to derive at least one positive example. When learning denite
clauses, this is not a problem, because derivation is monotone, and for every program P,
complete and consistent w.r.t. the given examples, there is a program P0P which is also
complete and consistent and is a trace3. On the other hand, when learning clauses containing cut, it may happen that the only complete and consistent program(s) in the hypothesis
space is neither a trace, nor contains it as a subset. This is because derivation is no longer
monotone and it can be the case that a negative example is derived by a set of clauses, but
not by a superset of them, as in the following simple example:
S = fsum(A,B,C) :- A>0, !, M is A-1, sum(M,B,N), C is N+1.
sum(A,B,C) :- C is B.g
sum pos(0,2,2), sum neg(2,2,2).
The two clauses in the hypothesis space represent a complete and consistent program for
the given examples, but our procedure is unable to learn it. Observe that the negative
example is derived by the second clause, which is a trace for the positive example, but not
by the rst and the second together.
This problem can be avoided if we require that, for every negative example, a corresponding positive example with the same input be given (in the above case, the example
required is sum pos(2,2,4)). In this way, if a complete program exists in the hypothesis
space, then it is also a trace, and can be learned. Then it can be made consistent using
cut, in order to rule out the derivation of negative examples. The constraint on positive
and negative examples seems to be quite intuitive. In fact, when writing a program, a
2. it must be noted that if we are learning programs for two dierent predicates, of j and k clauses
respectively (that is, md = j +k), then we have to consider not (j +k)! dierent programs, but only
j !+k!. We can do better if, inside a program, it is known that non-recursive clauses have a xed
position, and can be put before or after of all the recursive clauses.
3. a learned program P is complete if it derives all the given positive examples, and it is consistent if it
does not derive any of the given negative examples

98

The Difficulties of Learning Logic Programs with Cut

programmer usually thinks in terms of what a program should compute on given inputs,
and then tries to avoid wrong computations for those inputs.

4.4 Problem 4: Ordering of Given Examples
When learning clauses with cut, even the order of the positive examples may be signicant.
In the example above, if sum pos(2,2,4) comes after sum pos(0,2,2) then the learning task
fails to learn a correct program for sum, because it cannot nd a program consistent w.r.t.
the rst positive example and the negative one(s).
In general, for a given set of m positive examples this problem can be remedied by
testing dierent example orderings. Again, in the worst case k! dierent orderings of a set
of k positive examples must be checked. Moreover, in some situations a favorable ordering
does not exist. Consider the following hypothesis space:

c1) int(X,Y,W) :- head(X,A), tail(X,B), notmember(A,Y), int(B,Y,W).
c2) int(X,Y,W) :- head(X,A), tail(X,B), notmember(A,Y), !, int(B,Y,W).
c3) int(X,Y,Z) :- head(X,A), tail(X,B), int(B,Y,W), cons(A,W,Z).
c4) int(X,Y,Z) :- head(X,A), tail(X,B), !, int(B,Y,W), cons(A,W,Z).
c5) int(X,Y,Z) :- null(Z).
together with the set of examples:

e1 ) int pos([a],[b],[ ]).
e2 ) int pos([a],[a],[a]).
e3 ) int neg([a],[b],[a]).
e4 ) int neg([a],[a],[ ]).
Our induction procedure will not be able to nd a correct program for any ordering of the
two positive examples, even if such a program does exist ([c2,c4,c5]). This program is the
union of two traces: [c2,c5], which covers e1 , and [c4,c5], which covers e2 . Both of these traces
are inconsistent, because the rst covers e4 , and the second covers e3 . This problem can
be remedied only if all the positive examples are derived before the check against negative
examples is done.
However, in that case we have a further loss of eciency, because some inconsistent
traces are discarded only in the end. In other words, we would need to learn a program
covering all the positive examples, and then make it consistent by using cut and by reordering clauses. Moreover, there can be no way to make a program consistent by using cut and
reorderings. As a consequence, all the time used to build that program is wasted. As an
example, suppose we are given the following hypothesis space:

c01) int(X,Y,Z) :- head(X,A), tail(X,B), int(B,Y,W), cons(A,W,Z).
c02) int(X,Y,Z) :- null(X), null(Z).
c03) int(X,Y,Z) :- null(Z).
99

Bergadano, Gunetti, & Trinchero

with the examples:

e01) int pos([a],[a],[a]).
e02 ) int pos([a,b],[c],[]).
e03 ) int neg([a],[b],[a]).
Then we can learn the trace [c01,c02] from e01 and the trace [c03] from e02 . But [c01,c02,c03] covers
e03, and there is no way to make it consistent using cut or by reordering its clauses. In fact,
the rst partial trace is responsible for this inconsistency, and hence the time used to learn
[c03] is totally wasted.
Here it is also possible to understand why we need attened clauses. Consider the following program for intersection, which is equivalent to [c2,c4,c5], but with the three clauses
unattened:

u2 ) int([AjB],Y,W) :- notmember(A,Y), !, int(B,Y,W).
u4 ) int([AjB],Y,[AjW]) :- !, int(B,Y,W).
u5 ) int( , ,[]).
Now, this program covers int neg([a],[a],[]), i.e. [u2 ,u4,u5 ] ` int([a],[a],[]). In fact, clause
u2 fails on this example because a is a member of [a]. Clause u4 fails because the empty
list cannot be matched with [AjW]. But clause u5 succeeds because its arguments match
those of the negative example. As a consequence, this program would be rejected by the
induction procedure.
The problem is that, if we use unattened clauses, it may happen that a clause body is
not evaluated because an example does not match the head of the clause. As a consequence,
possible cuts in that clause are not evaluated and cannot inuence the behavior of the entire
program. In our example, the cut in clause u4 has no eect because the output argument of
int([a],[a],[]) does not match [AjW], and the body of u4 is not evaluated at all. Then u5 is
red and the negative example is covered. In the attened version, clause c4 fails only when
cons(a,[],[]) is reached, but at that point a cut is in force and clause c5 cannot be activated.
Note that program [u2 ,u4,u5] behaves correctly on the query int([a],[a],X), and gives X=[a]
as the only output.

4.5 Problem 5: Ordering of Literals

Even the relative position of literals and cut in a clause is signicant. Consider again the
correct program for intersection as above ([c2,c4,c5]), but with c4 modied by putting the
cons literal in front of the antecedent:

c04) int(X,Y,Z) :- cons(A,W,Z), head(X,A), tail(X,B), int(B,Y,W).
Then, there is no way to get a correct program for intersection using this clause. To rule
out the negative example int neg([a],[a],[]) we must put a cut before the cons predicate,
in order to prevent the activation of c5. But, then, some positive examples are no longer
covered, such as int pos([a],[],[]). In fact, we have a wrong behavior every time clause c04 is
100

The Difficulties of Learning Logic Programs with Cut

called and fails, since it prevents the activation on c5 . In general, this problem cannot be
avoided even by reordering clauses: if we put c04 after c2 and c5 , then int neg([a],[a],[]) will
be covered. As a consequence, we should also test for every possible permutation of literals
in every clause of a candidate program.

5. Situations where Learning Cut is still Practical
From the above analysis, learning cut appears to be dicult since, in general, a learning
procedure should be able to backtrack on the candidate base programs (e.g., traces), on
the position of cut(s) in the program, on the order of the clauses in the program, on the
order of literals in the clauses and on the order of given positive examples. However, we
have spotted some general conditions at which learning cut could still be practical. Clearly,
these conditions cannot be a nal solution to learning cut, but, if applicable, can alleviate
the computational problems of the task.

5.1 Small Hypothesis Space

First of all, a restricted hypothesis space is necessary. If clauses cannot be learned independently of one another, a small hypothesis space would help to limit the backtracking
required on candidate traces (problem 1). Moreover, even the number of clauses in a trace
would be probably smaller, and hence also the number of dierent permutations and the
number of dierent positions for inserted cuts (problems 2 and 1). A small trace would also
have a slight positive impact on the need to test for dierent literal orderings in clauses
(problem 5).
In general, many kinds of constraints can be applied to keep a hypothesis space small,
such as ij-determinism (Muggleton & Feng, 1990), rule sets and schemata (Kietz & Wrobel,
1991; Bergadano & Gunetti, 1993), determinations (Russell, 1988), locality (Cohen, 1993),
etc (in fact, some of these restrictions and others, such as those listed in Section 3, are
available in the actual implementation of our procedure - see the Appendix4 ). Moreover,
candidate recursive clauses must be designed so that no innite chains of recursive calls
can take place (Bergadano & Gunetti, 1993) (otherwise the learning task itself could be
non-terminating). In general, the number of possible recursive calls must be kept small, in
order to avoid too much backtracking when searching for possible traces. However, general
constraints may not be sucient. The hypothesis space must be designed carefully from
the very beginning, and this can be dicult. In the example of learning simplify an initial
hypothesis space of \only" 8449 clauses was obtained specifying not only the set of required
predicates, but even the variables occurring in every literal.
If clauses cannot be learned independently, experiments have shown to us that a dramatic improvement of the learning task can be obtained by generating the clauses in the
hypothesis space so that recursive clauses, and in general more complex clauses, are taken
into consideration after the simpler and non-recursive ones. Since simpler and non recursive
clauses require less time to be evaluated, they will have a small impact on the learning time.
Moreover, learning simpler clauses (i.e. shorter) also alleviates problem 5.
4. We found these constraints particularly useful. By using them we were often able to restrict a hypothesis
space of one order of magnitude without ruling out any possible solution.

101

Bergadano, Gunetti, & Trinchero

Finally, it must be noted that our induction procedure does not necessarily require that
the hypothesis space S of possible clauses be represented explicitly. The learning task could
start with an empty set S and an implicit description of the hypothesis space, for example
the one given in Section 3. When a positive example cannot be derived from S, a new clause
is asked for to a clause generator and added to S. This step is repeated until the example
is derivable from the updated S, and then the learning task can proceed normally.

5.2 Simple Examples

Another improvement can be achieved by using examples that are as simple as possible.
In fact, each example which may involve a recursive call is potentially responsible for the
activation of all the corresponding clauses in the hypothesis space. The more complex the
example, the larger the number of consecutive recursive activations of clauses and the larger
the number of traces to be considered for backtracking (problem 1). For instance, to learn
the append relation, it may be sucient to use an example like append([a],[b],[a,b]) instead
of one like append([a,b,c,d],[b],[a,b,c,d,b]). Since simple examples would probably require
a smaller number of dierent clauses to be derived, this would result in smaller traces,
alleviating the problem of permutation of clauses and literals in a trace (problems 2 and 5)
and decreasing the number of positions for cuts (problem 1).

5.3 Small Number of Examples

Since a candidate program is formed by taking the union of partial traces learned for single
examples, if we want a small trace (problems 2 and 5) we must use as few examples as
possible, while still completely describing the required concept. In other words, we should
avoid redundant information. For example, if we want to learn the program for append, it
will be normally sucient to use only one of the two positive examples append([a],[b],[a,b])
and append([c],[d],[c,d]). Obviously it may happen that dierent examples are derived by
the same set of clauses, and in this case the nal program does not change.
Having to check for all possible orderings of a set of positive examples, a small number of
examples is also a solution to problem 4. Fortunately, experiments have shown that normally
very few positive examples are needed to learn a program, and hence the corresponding
number of dierent orderings is, in any case, a small number. Moreover, since in our
method a positive example is sucient to learn all the clauses necessary to derive it, most
of the time a complete program can be learned using only one well chosen example. If such
an example can be found (as in the case of the learning task of section 3, where only one
example of simplify and one of remove are given), the computational problem of testing
dierent example orderings is automatically solved.
However, it must be noted that, in general, a small number of examples may not be
sucient, except for very simple programs. In fact, if we want to learn logic programs
such as member, append, reverse and so on, then any example involving recursion will be
sucient. But for more complex programs the choice may not be trivial. For example, our
procedure is able to learn the quicksort (plus partition) program with only one \good"
example. But if one does not know how quicksort and partition work, it is likely that
she or he will provide an example allowing to learn only a partial description of partition.
This is particularly clear in the example of simplify . Had we used the positive example
102

The Difficulties of Learning Logic Programs with Cut

simplify pos([[[],[b,a,a]]],[b,a]) (which is very close to the one eectively used), the rst clause
of flatten would not have been learned. In other words, to give few examples we must give
good examples, and often this is possible only by having in mind (at least partially and in
an informal way) the target program. Moreover, for complex programs, good examples can
mean complex examples, and this is in contrast with the previous requirement. For further
studies of learning from good examples we refer the reader to the work of Ling (1991) and
Aha, Ling, Matwin and Lapointe (1993).

5.4 Constrained Positions for Cut and Literals

Experiments have shown that it is not practical to allow the learning procedure to test all
possible positions of cut in a trace, even if we are able to keep the number of clauses in
a trace small. The user must be able to indicate the positions where a cut is allowed to
occur, e.g., at the beginning of a clause body, or before a recursive call. In this case, many
alternative programs with cut are automatically ruled out and thus do not have to be tested
against the negative examples. It may also be useful to limit the maximum number of cuts
per clause or per trace. For example, most of the time one cut per clause can be sucient
to learn a correct program. In the actual implementation of our procedure, it is in fact
possible to specify the exact position of cut w.r.t. a literal or a group of literals within each
clause of the hypothesis space, when this information is known.
To eliminate the need to test for dierent ordering of literals (problem 5), we may also
impose a particular global order, which must be maintained in every clause of the hypothesis
space. However this requires a deep knowledge of the program we want, otherwise some
(or even all) solutions will be lost. Moreover, this solution can be in contrast with a use of
constrained positions for cut, since a solution program for a particular literal ordering and
for particular positions for cuts may not exist.

6. Conclusion

Our induction procedure is based on an intensional evaluation of clauses. Since the cut
predicate has no declarative meaning, we believe that intensional evaluation of clauses
cannot be abandoned, independently of the kind of learning method adopted. This can
decrease the performance of the learning task, compared with extensional methods, which
examine clauses one at a time without backtracking. However, the computational problems
outlined in Section 4 remain even if we choose to learn a complete program extensionally,
and then we try to make it consistent by inserting cut. The only dierence is that we do
not have backtracking (problem 1), but the situation is probably worse, since extensional
methods can fail to learn a complete program even if it exists in the hypothesis space.
(Bergadano, 1993a).
Even if the ability to learn clauses containing procedural predicates like cut seems to be
fundamental to learning \real" logic programs, in particular short and ecient programs,
many problems inuencing the complexity of the learning task must be faced. These include
the number and the relative ordering of clauses and literals in the hypothesis space, the kind
and the relative ordering of given examples. Such problems seem to be related to the need
for an intensional evaluation of clauses in general, and not to the particular learning method
adopted. Even just to alleviate these problems, it seems necessary to know a lot about the
103

Bergadano, Gunetti, & Trinchero

target program. An alternative solution is simply to ignore some of the problems. That is,
avoid testing for dierent clause and/or literal and/or example orderings. Clearly, in this
way the learning process can become feasible, but it can fail to nd a solution even when
it exists. However, many ILP systems (such as Foil) adopt such an \incomplete-but-fast"
approach, which is guided by heuristic information.
As a consequence, we view results presented in this paper as, at least partially, negative. The problems we raised appear computationally dicult, and suggest that attention
should be restricted to purely declarative logic languages, which are, in any case, suciently
expressive.

Acknowledgements
This work was in part supported by BRA ESPRIT project 6020 on Inductive Logic Programming.

Appendix A
The induction procedure of Section 2 is written in C-prolog (interpreted) and runs on a
SUNsparcstation 1. We are planning to translate it in QUINTUS prolog. This Appendix
contains a simplied description of its implementation. As a preliminary step, in order to
record a trace of the clauses deriving a positive example e+, every clause in the hypothesis
space5 S must be numbered and modied by adding to its body two literals. The rst
one, allowed(n,m) is used to activate only the clauses which must be checked against the
negative examples. The second one, marker(n), is used to remember that clause number n
has been successfully used while deriving e+. Hence, in general, a clause in the hypothesis
space S takes the following form:

P (X1 ,: : : ,Xm) :- allowed(n,m), ,marker(n).
where  is the actual body of the clause, n is the number of the clause in the set and m is a
number used to deal with cuts. For every clause n, the one without cut is augmented with
allowed(n,0), while those containing a cut somewhere in their body are augmented with
allowed(n,1), allowed(n,2), ..., and so on. Moreover, for every augmented clause as above,
a fact \alt(n,m)." is inserted in S, in order to implement an enumeration mechanism.
A simplied (but running) version of the learning algorithm is reported below. In the
algorithm, the output, if any, is the variable Trace containing the list of the (numbers of the)
clauses representing the learned program P. By using the backtracking mechanism of Prolog,
more than one solution (trace) can be found. We assume the two predicates listpositive
and listnegative build a list of the given positive and negative examples, respectively.
consult(le containing the set of clauses S).
5. We assume clauses in the hypothesis space to be attened

104

The Difficulties of Learning Logic Programs with Cut

allowed(X,0).
marker(X) :- assert(trace(X)).
marker(X) :- retract(trace(X)), !, fail.
main :- listpositive(Posexamplelist), tracer([],Posexamplelist,Trace).
tracer(Covered,[ExamplejCdr],Trace) :- Example, /? backtracking point 1 ?/
setof(L,trace(L),Trace1),
notneg(Trace1,[ExamplejCovered],Cdr),
tracer([ExamplejCovered],Cdr,Trace).
tracer( ,[],Trace) :- setof((I,J),allowed(I,J),Trace), asserta((marker(X) :- true, !)).
assertem([]).
assertem([IjCdr]) :- alt(I,J), backassert(allowed(I,J)), assertem(Cdr).
prep(T) :- retract(allowed(X,0)), assertem(T).
backassert(X) :- assert(X).
backassert(X) :- retract(X), !, fail.
resetallowed([]) :- !.
resetallowed( ) :- abolish(allowed,2), assert(allowed(X,0)), !.
notneg(T,Covered,Remaining) :- listnegative([]).
notneg(T,Covered,Remaining) :- listnegative(Negexamplelist),
asserta((marker(X) :- true,!)),
prep(T), /? backtracking point 2 ?/
trypos(Covered), trynegs(Negexamplelist),
resetallowed(Remaining),
retract((marker(X) :- true,!)).
notneg(T,Covered,Remaining) :- resetallowed(Remaining),
retract((marker(X) :- true,!)), !, fail.
trypos([ExamplejCdr]) :- Example, !, trypos(Cdr).
trypos([]) :- !.
trynegs([ExamplejCdr]) :- Example,!,fail.
trynegs([ExamplejCdr]) :- trynegs(Cdr).
trynegs([]) :- !.
Actually, our complete implementation is more complex, also in order to achieve greater
eciency. The behavior of the learning task is quite simple. Initially, the set S of clauses is
read into the Prolog interpreter, together with the learning algorithm. Then the learning
task can be started by calling the predicate main. A list of the positive examples is formed
105

Bergadano, Gunetti, & Trinchero

and the tracer procedure is called on that list. For every positive example, tracer calls
the example itself, ring all the clauses in S that may be resolved against that example.
Observe that, initially, an allowed(X,0) predicate is asserted in the database: in this way
only clauses not containing a cut are allowed to be used (this is because clauses with cut are
employed only if some negative example is derived). Then, a trace, if any, of (the numbers
associated to) the clauses successfully used in the derivation of that example is built, using
the setof predicate.
The trace is added to the traces found for the previous examples, and the result is
checked against the set of the negative examples calling the notneg procedure. If notneg
does not fail (i.e. no negative examples are covered by this trace) then a new positive
example is taken into consideration. Otherwise notneg modies the trace with cut and
tests it again. If also this fails, backtracking occurs and a new trace for the current example
(and possibly for the previous ones) is searched for.
The notneg procedure works as follows. First, only the clauses in the trace are allowed
to be checked against the negative examples, by retracting the allowed(X,0) clause and
asserting an allowed(n,0) if the n-th clause (without cut) is in the trace. This is done with
the prep and assertem predicates. Then a list of the negative examples is formed and we
check if they can be derived from the clauses in the trace. If at least one negative example is
covered, (i.e., if trynegs fails) then we backtrack to the prep procedure (backtracking point
2) where a clause of the trace is substituted with an equivalent one but with cut inserted
somewhere (or in a dierent position). If no correct program can be found in such a way
by trying all possible alternatives (i.e. by using cut in all possible ways), notneg fails, and
backtracking to backtracking point 1 occurs, where another trace is searched for. Otherwise,
all clauses in S without cut are reactivated by asserting again allowed(X,0), and the next
positive example is considered. Note that trypos is used in notneg to verify if a modied
trace still derives the set of positive examples derived initially. The possibility to substitute
clauses in the current trace with others having cut inserted somewhere is achieved through
the alt predicate in the assertem procedure. Finally, note that this simplied version of
the learning procedure is not able to generate and test for dierent orderings of clauses in
a trace or for dierent ordering of literals in each clause, nor to use dierent orderings for
the set of positive examples.
In order to derive all the positive examples before the check against the negative ones
(see subsection 4.4), we must change the rst clause of the tracer procedure into:
tracer([Pos1, ... ,Posn]):-Pos1, ... ,Posn, setof(L,trace(L),T), notneg(T).
The actual implementation of the above induction procedure is available through ftp. For
further information contact gunetti@di.unito.it.

References

Aha, D., Ling, C., Matwin, S., & Lapointe, S. (1993). Learning Singly Recursive Relations
from Small Datasets. In Proceedings of the IJCAI-93 workshop on ILP.
Bergadano, F. (1993a). Inductive database relations. IEEE Transactions on Data and
Knowledge Engineering, 5 (6).
106

The Difficulties of Learning Logic Programs with Cut

Bergadano, F. (1993b). Test Case Generation by Means of Learning Techniques. In Proceedings of ACM SIGSOFT-93.
Bergadano, F., & Gunetti, D. (1993). An interactive system to learn functional logic programs. In Proceedings of IJCAI-93.
Coelho, H., & Cotta, J. C. (1988). Prolog by Example: how to learn teach and use it. Berlin:
Springer-Verlag.
Cohen, W. (1993). Rapid Prototyping of ILP Systems Using Explicit Bias. In Proceedings
of the IJCAI-93 workshop on ILP.
DeRaedt, L., Lavrac, N., & Dzeroski, S. (1993). Multiple predicate learning. In Proceedings
of IJCAI-93.
Kietz, J. U., & Wrobel, S. (1991). Controlling the Complexity of Learning in Logic through
Syntactic and Task-Oriented Models. In Muggleton, S. (Ed.), Inductive Logic Programming. London: Academic Press.
Lau, K. K., & Clement, T. (Eds.). (1993). Logic Program Synthesis and Transformation.
Berlin: Springer-Verlag.
Ling, X. C. (1991). Learning from Good Examples. In Proceedings of IJCAI-91.
Muggleton, S. (Ed.). (1991). Inductive Logic Programming. London: Academic Press.
Muggleton, S., & Feng, C. (1990). Ecient Induction of Logic Programs. In Proceedings of
the rst conference on Algorithmic Learning Theory.
Quinlan, R. (1990). Learning Logical Denitions from Relations. Machine Learning, 5,
239{266.
Rouveirol, C. (in press). Flattening: a representation change for generalization. Machine
Learning.
Russell, S. (1988). Tree-structured bias. In Proceedings of AAAI-88.
Shapiro, E. Y. (1983). Algorithmic Program Debugging. Cambridge, CA: MIT Press.

107

Journal of Articial Intelligence Research 1 (1994) 257-275

Submitted 11/93; published 3/94

Exploring the Decision Forest: An Empirical Investigation
of Occam's Razor in Decision Tree Induction
Patrick M. Murphy
Michael J. Pazzani

Department of Information & Computer Science
University of California, Irvine, CA 92717

pmurphy@ics.uci.edu
pazzani@ics.uci.edu

Abstract

We report on a series of experiments in which all decision trees consistent with the
training data are constructed. These experiments were run to gain an understanding of the
properties of the set of consistent decision trees and the factors that aect the accuracy
of individual trees. In particular, we investigated the relationship between the size of a
decision tree consistent with some training data and the accuracy of the tree on test data.
The experiments were performed on a massively parallel Maspar computer. The results of
the experiments on several articial and two real world problems indicate that, for many
of the problems investigated, smaller consistent decision trees are on average less accurate
than the average accuracy of slightly larger trees.
1. Introduction

The top-down induction of decision trees is an approach to machine learning that has been
used on a variety of real world tasks. Decision trees are well-suited for such tasks since they
scale fairly well with the number of training examples and the number of features, and can
represent complex concepts in a representation that is fairly easy for people to understand.
Decision tree induction algorithms (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1986; Fayyad & Irani, 1992) typically operate by choosing a feature that partitions the
training data according to some evaluation function (e.g., the purity of the resulting partitions). Partitions are then further partitioned recursively until some stopping criterion is
reached (e.g., the partitions contain training examples of a single class). Nearly all decision
tree induction algorithms create a single decision tree based upon local information of how
well a feature partitions the training data. However, this decision tree is only one of a set of
decision trees consistent with the training data. In this paper, we experimentally examine
the properties of the set of consistent decision trees. We will call the set of decision trees
that are consistent with the training data a decision forest.
Our experiments were run on several articial concepts for which we know the correct
answer and two naturally occurring databases from real world tasks available from the UCI
Machine Learning Repository (Murphy & Aha, 1994) in which the correct answer is not
known. The goal of the experiments were to gain insight into how factors such as the
size of a consistent decision tree are related to the error rate on classifying unseen test
instances. Decision tree learners, as well as most other learners, attempt to produce the

c 1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Murphy & Pazzani
smallest consistent hypothesis.1 Occam's razor is often used to justify this bias. Here,
we experimentally evaluate this bias towards simplicity by investigating the relationship
between the size of a consistent decision tree and its accuracy. If the average error of
decision trees with N test nodes is less than the average error of decision trees of size N + i
(for i > 0), an appropriate bias for a learner attempting to minimize average error would
be to return the smallest decision tree it can nd within its resource constraints.
In this paper, we restrict our attention to decision trees that are consistent with the
training data and ignore issues such as pruning which trade o consistency with the training
data and the simplicity of the hypothesis. For the purposes of this paper, a consistent
decision tree is one that correctly classies every training example.2 We also place two
additional constraints on decision trees. First, no discriminator can pass all instances down
a single branch. This insures that the test made by the decision tree partitions the training
data. Second, if all of the training instances at a node are of the same class, no additional
discriminations are made. In this case, a leaf is formed with class label specied by the class
of the instances at the leaf. These two constraints are added to insure that the decision
trees analyzed in the experiments correspond to those that could be formed by top down
decision tree induction algorithms. In this paper, we will not investigate problems that have
continuous-valued features or missing feature values.
In Section 2 (and the appendix), we will report on some initial exploratory experiments
in which the smallest consistent decision trees tend to be less accurate than the average
accuracy of those slightly larger. Section 3 provides results of additional experiments that
address this issue. Section 4 addresses the implication of our ndings to the policy a learner
should take in deciding which of the many consistent hypotheses it should prefer. Section
5 relates this work to previous empirical and theoretical research.
2. Initial Experiments

We will investigate the relationship between various tree characteristics and error. In particular, we will look at node cardinality (i.e., the number of internal nodes in a tree) and
leaf cardinality (i.e., the total number of leaves in a tree).
It should be noted that even when using a powerful massively parallel computer, the
choice of problems is severely constrained by the computational complexity of the task.
The number of trees of any node cardinality that might be generated is O(dc ) where d is
the number of discriminators and c is the node cardinality. Even on a massively parallel
computer, this precluded the use of problems with many features or any continuous-valued
features.
The rst experiment considered learning from training data in which there are 5 boolean
features. The concept learned was XY Z _ AB . This concept was chosen because it was of
moderate complexity, requiring a decision tree with at least 8 nodes to represent correctly.
With 5 boolean features, the smallest concept (e.g., True) would require 0 test nodes and
the largest (e.g., parity) would require 31.
1. We say \attempt to produce the smallest consistent hypothesis" because most systems use some form of
limited look-ahead or greedy search. As a result, the smallest consistent tree is rarely found.
2. The articial and natural problems we study here have consistent training sets.

258

Exploring the Decision Forest

0.6

100

0.5

80

Error

0.4

60

0.3
40

0.2
Error
Trials

0.1

20

0.0

Number of Trials

We ran 100 trials, creating a training set by randomly choosing without replacement
20 of the 32 possible training examples and using the remaining 12 examples as the test
set. For each trial, every consistent decision tree was created, and we computed the average
error rate made by trees with the same node cardinality. Figure 1 plots the mean and 95%
condence interval of these average errors as a function of the node cardinality. Figure 1
also plots the number of trials on which at least one decision tree of a given node cardinality
is consistent with the training data.

0
0

2

4

6
8 10 12 14
Node Cardinality

16

18

20

Figure 1. The average error of 100 trials as a function of node cardinality and the number
of trials for each node cardinality.
From node cardinality 7 to node cardinality 16, there is a monotonic increase in error
with increasing node cardinality. For the range from 2 to 3 nodes, the error is varied;
however there is little evidence for these error values because they are based on only 2 and
1 trials, respectively. For the range of node cardinalities between 4 and 7, average error is
denitely not a monotonically increasing function of node cardinality. As seen in the curve,
5 node trees are on the average more accurate than 4 node trees, and 7 node trees are on
the average more accurate than trees with 6 nodes. This last result is somewhat surprising
since one gets the impression from reading the machine learning literature (Muggleton,
Srinivasan, & Bain, 1992) that the smaller hypothesis (i.e., the one that provides the most
compression of the data (Rissanen, 1978)) is likely to be more accurate. We will explore
this issue in further detail in Section 3. Appendix 1 presents data showing that this result
is not unique to this particular concept. A nal, interesting nding that we will not explore
further in this paper is that for very large node cardinalities, error begins to decrease as the
node cardinality increases.
Table 1 lists the average number of consistent trees for each node cardinality and the
average number of correct trees (i.e., those trees consistent with the training data that make
no errors on the unseen test examples). There are no correct trees with fewer than 8 nodes,
since at least 8 nodes are required to represent this concept. Clearly, since there are many
trees consistent with the training data, a learner needs some policy to decide which tree to
return. We will return to this issue in Section 4.
259

Murphy & Pazzani
Nodes

2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

Number of
Number of
Consistent Trees Correct Trees
2.0
0.0
4.0
0.0
3.3
0.0
12.3
0.0
27.6
0.0
117.1
0.0
377.0
17.8
879.4
37.8
1799.9
50.2
3097.8
41.6
4383.0
95.4
5068.9
66.6
4828.3
37.7
3631.5
31.3
1910.6
14.8
854.4
4.0
308.6
3.6
113.8
0.0

Table 1. The average number of trees consistent with 20 training examples of the XY Z_AB
concept.
3. Further Experimentation

For most of the problems studied, we found that on average, the smallest decision trees
consistent with the training data had more error on unseen examples than slightly larger
trees. We ran additional experiments to make sure that this result is not an artifact of the
experimental methodology that we used, as reported in the next sections.
3.1 Representative Train/Test Partitions

One possible explanation for the nding of the previous section is that the smaller decision
trees are formed from unrepresentative samples. For example, there are 11 positive and 21
negative examples of the concept XY Z _ AB . If all or most of the examples in the training
set are negative, a very small tree may be learned which would probably do very poorly on
the mostly positive test set. To insure that the results are not caused by unrepresentative
training sets, we eliminated all training data that was not reasonably representative. In
11 probability that a training instance is positive, a representative
particular, since there is a 32
training set
have about 7 positive instances. Since one standard deviation
q of size11 20 would
11
would be 20 3 32 3 (1 0 32 ), we eliminated from analysis those training sets with greater
than 8 or fewer than 5 positive instances. Similarly, there is a 0.5 probability that each
binary feature takes on a true value, so we eliminated from analysis any training data which
has any feature that is true in greater than 13 or fewer than 7 instances. Figure 2 is based
260

Exploring the Decision Forest

0.6

100

0.5

80

Error

0.4

60

0.3
40

0.2
Error
Trials

0.1

20

0.0

Number of Trials

on the 69 of 100 trials of the XY Z _ AB concept that met this representative test. Notice
that the two trials that formed the only 2 and 3 node trees were removed. Even when only
the more representative training sets are considered, the average error of trees of size 4 is
greater than the average error of size 5 trees.

0
0

2

4

6
8 10 12 14
Node Cardinality

16

18

20

Figure 2. Error rate of consistent trees from representative training sets as a function of
node cardinality.
By regrouping the results of 100 trials for the XY Z _AB concept so that trials with the
same minimum-sized trees are grouped together, a set of ve curves, each associated with
a subgroup, was formed (Figure 3). The intent of the grouping is to allow us to determine
whether the minimum-sized trees for any given trial are on average more accurate than
larger trees.
0.6
0.5
Error

0.4
0.3
Min. Tree Size = 2
Min. Tree Size = 4
Min. Tree Size = 5
Min. Tree Size = 6
Min. Tree Size = 7

0.2
0.1
0.0
0

2

4

6

8 10 12 14
Node Cardinality

16

18

Figure 3. Error as a function of node cardinality for the
grouped by minimum-sized trees built.

20

XY Z _ AB concept when rst

Note that in Figure 3, for most minimum tree sizes, error is not a monotonically increasing function of node cardinality. Furthermore, the average error of the smallest trees found
is not the most accurate when the smallest tree has 4 or 6 nodes. In addition, regardless

261

Murphy & Pazzani

0.6

100

0.5

80

Error

0.4

60

0.3
40

0.2

Error
Trials

0.1

20

0.0

Number of Trials

of the size of the smallest tree found, the average accuracy of trees of size 8 (the size of the
smallest correct tree) rarely has the minimum average error.
Another interesting nding becomes apparent with this way of viewing the data: the
average error rates of trees for training sets that allow creation of smaller consistent trees
tends to be higher than for those training sets that can only form larger trees. For example,
the error rate for those training sets whose minimum-sized trees have 4 nodes is higher than
the error rate on trials whose minimum-sized trees has 7 nodes.

0
0

2

4

6
8 10 12 14
Node Cardinality

16

18

20

Figure 4. Error rate of consistent trees with 2 examples per leaf of some correct 8 node tree
as a function of node cardinality.
The denition of representative that we used earlier in this section used global characteristics of the training data to determine representativeness. Here, we consider a more
detailed view of representativeness that takes the structure of the correct concept into account. It is unreasonable to expect a decision tree learner to learn an accurate concept if
there are no examples that correspond to some of the leaves of some correct decision tree.
To generate training data for the next experiment, we rst randomly selected one of the
72 trees with 8 nodes that is consistent with all the data. Next, for each leaf of the tree,
we randomly selected two examples (if possible) to include in the training set. If a leaf
only had one example, that example was included in the training set. Finally, we randomly
selected from the remaining examples so that there were 20 training examples and 12 test
examples. We had anticipated that with representative training sets formed in this manner,
very small consistent trees would be rare and perhaps the error rate would monotonically
increase with node cardinality. However, the results of 100 trials, as displayed in Figure 4,
indicate the same general pattern as before. In particular, the average error of trees with 7
nodes is substantially less than the average error of those with 6 nodes. Another experiment
with one randomly selected example per leaf had similar results.
3.2 Training Set Size and Concept Complexity

The minimum-sized decision tree for the concept XY Z _AB has 8 tests and 9 leaves. Since
the correct tree does not provide much compression3 of a set of 20 examples used to induce
3. The exact amount of compression provided depends upon the particular scheme chosen for encoding the
training data. See (Quinlan & Rivest, 1989; Wallace & Patrick, 1993) for two such schemes.

262

Exploring the Decision Forest
the tree, one might argue that the sample used was too small for this complex a concept.
Therefore, we increased the number of training examples to the maximum possible. Figure
5 plots the average error of 32 trials in which we formed all decision trees consistent with 31
examples. Each tree was evaluated on the remaining unseen example. Figure 5 shows that
the smaller trees formed from samples of size 31 have more error than the slightly larger
trees. Since the minimum correct decision tree has 8 nodes and the consistent trees classify
all 31 training examples correctly, any decision tree with fewer than 8 nodes classies the
test example incorrectly.
32
28

Error

0.8

Error
Trials

24
20
16
12
8
4
0

0.6
0.4
0.2
0.0
0

2

4

6

Number of Trials

1.0

8 10 12 14 16 18 20 22 24 26
Node Cardinality

Figure 5. Error rate of consistent trees with leave-one-out testing as a function of node
cardinality.
To refute further the hypothesis that the results obtained so far were based on using
too small a training set for a given concept complexity, we considered two less complex
concepts. In particular, we investigated a single attribute discrimination, A with four
irrelevant features (Figure 6) and a simple conjunction, AB with three irrelevant features
(Figure 7).
0.6

100

Error

0.4

80

0.3
0.2

60
Error
Trials

0.1
0.0

Number of Trials

0.5

40
0

2

4

6

8 10 12 14
Node Cardinality

16

18

20

Figure 6. Error as a function of node cardinality for the single attribute discrimination
concept.

263

A

Murphy & Pazzani

100
80

Error

0.3

60
0.2
40
0.1

Error
Trials

20

0.0

Number of Trials

0.4

0
0

2

4

6

8 10 12 14
Node Cardinality

16

18

20

Figure 7. Error as a function of node cardinality for the simple conjunction

AB concept.

For each concept, 100 trials were run in which 20 examples were used for training and
the remaining 12 for testing. For these simpler concepts, though the smallest trees are the
most accurate, error again is not a monotonically increasing function of node cardinality.
3.3 Training and Testing using the Same Probability Distribution.

In our previous experiments, we used a methodology that is typical in empirical evaluations
of machine learning systems: the training data and the test data are disjoint. In contrast,
most theoretical work on the PAC model (Valiant, 1984) assumes that the training and
test data are generated from the same probability distribution over the examples. For
this section, we ran an experiment in which training and test examples were selected with
replacement from the same distribution to ensure that our results were not dependent on a
particular experimental methodology.
100
80

Error

0.15

60
0.10
40
0.05

Error
Trials

0.00

20

Number of Trials

0.20

0
0

2

4

6

8 10 12 14 16 18 20 22
Node Cardinality

Figure 8. Error as a function of node cardinality when the training and test examples are
generated by the same distribution for the XY Z _ AB concept.
Once again, the target concept was XY Z _ AB . By randomly choosing 31 training
examples with replacement from the set of 32 possible instances, on average approximately
20 distinct training examples are selected. Error is estimated by randomly choosing 1000
264

Exploring the Decision Forest
examples with replacement from the set of possible instances. Figure 8 graphs the mean
error (averaged over 100 trials) as a function of node cardinality.
This testing methodology produces much smaller values for the proportion of test examples misclassied than the disjoint training and test set methodology because those test
examples which also were training examples are always classied correctly. However, the
same basic pattern of results is observed. Error is not at a minimum for the smallest decision trees nor at decision trees with 8 nodes (the minimum-sized correct tree). Error
monotonically increases starting at trees with 7 nodes and then begins to decrease again
for very large node cardinalities. Note that on some trials, it is possible to build decision
trees with up to 21 nodes since some training sets contained 22 distinct examples.
3.4 Average Path Length

The information gain metric of ID3 is intended to minimize the number of tests required to
classify an example. Figure 9 reanalyzes the data from Figure 1 by graphing average error
as a function of average path length for the XY Z _ AB concept.
100
80
0.3
Error

60
40

0.2
Error
Trials

20

0.1

Number of Trials

0.4

0
1.0

2.0

3.0
4.0
Average Path Length

5.0

Figure 9. Error as a function of average path length for the XY Z _ AB concept.
The results are similar to those obtained when relating the number of test nodes to the
error rate: error is not a monotonically increasing function of average path length. Similar
analyses were performed and similar results have been obtained for other concepts which
are presented in the Appendix.
4. The Minimum-Sized Decision Tree Policy

A designer of a learning algorithm either explicitly or implicitly must decide which hypothesis to prefer when multiple hypotheses are consistent with the training data. As Table 1
shows, there can be many consistent decision trees. Should the learner always prefer the
smallest consistent decision tree? A learner that adopts this strategy can be said to be
following the minimum-sized decision tree policy.

265

Murphy & Pazzani
In this section, we present results from additional experiments to evaluate this policy.
In particular, we gather evidence to address two related questions:



Given any two consistent decision trees with dierent node cardinalities, what is the
probability that the smaller decision tree is more accurate?



Given the minimum-sized decision tree and a larger consistent decision tree, what is
the probability that the smallest decision tree is more accurate?

The rst question is of more interest to the current practice of decision tree induction
since, for eciency reasons, no algorithm attempts to nd the smallest consistent decision
tree for large data sets. Nonetheless, most algorithms are biased toward favoring trees with
fewer nodes.
0.8

Probability

0.6
Prob(Smaller > Larger)
Prob(Larger > Smaller)
Prob(Smaller = Larger)

0.4
0.2
0.0
0

2
4
6
8
10
12
Difference in Node Cardinality

14

16

Number of trials

1000
800
600
400
200
0
0

2
4
6
8
10
12
Difference in Node Cardinality

14

16

Figure 10. The probability that the accuracy of a smaller decision tree is greater than,
equal to, or less than the accuracy of a larger tree as a function of the dierence of node
cardinalities for the XY Z _AB concept (upper). The number of trials out of 1000 on which
at least 2 trees had a given dierence in node cardinality (lower).
To address the question of whether a learner should prefer the smaller of two randomly
selected consistent trees, we ran 1000 trials of learning the concept XY Z _ AB from 20
training examples. For each trial, we recorded the node cardinality and accuracy (on the
12 test examples) of every consistent tree. For each pair of consistent trees (with dierent
266

Exploring the Decision Forest
node cardinalities), we computed the dierence in node cardinality and indicated whether
the accuracy of the smaller tree was greater than, equal to, or less than the accuracy of the
larger tree. From this data, we computed the observed probability that one decision tree
was more accurate than another as a function of the dierence in node cardinalities (see
Figure 10 upper). The graph shows that on this concept, the probability that the smaller
of two randomly chosen consistent decision trees will be more accurate is greater than the
probability that the larger tree will be more accurate. Furthermore, the probability that
the smaller tree is more accurate increases as the dierence in node cardinality increases.
An exception to this trend occurs for very large dierences in node cardinality. However,
as Figure 10 lower shows, these exceptions are quite rare. Consistent decision trees whose
node cardinalities diered by 16 were found in only 6 of the 1000 trials.4 The results of
this experiment indicate that on average, a learner that prefers the smaller of two randomly
selected decision trees has a higher probability of being more accurate on this concept than
a learner that selects the larger tree.

Probability

0.8
0.6
Prob(Smallest > Larger)
Prob (Larger > Smallest)
Prob(Smallest = Larger)

0.4
0.2
0.0
0

2

4
6
8
10
12
Difference in Node Cardinality

14

16

Figure 11. The probability that the accuracy of a minimum-sized decision is greater than,
equal to, or less than the accuracy of a larger tree as a function of the dierence of node
cardinalities for the XY Z _ AB concept.
To address the question of whether a learner should prefer the smallest consistent decision over a randomly selected consistent tree with more test nodes, we reanalyzed the data
from the previous experiment. Figure 11 graphs the observed probability that a consistent
decision tree with the minimum node cardinality is more accurate than a larger tree as a
function of the dierence in node cardinalities between the two trees. The graph shows that
a learner that chooses randomly among the consistent decision trees with minimum node
cardinalities is more likely to nd a tree that is more accurate than a learner that randomly
selects among larger trees.5
Figure 11 clearly shows that for this particular concept, preferring the minimum-sized
decision tree policy is on average a better policy than preferring a decision tree that is any
4. Four trials had minimum-sized trees with 2 nodes and maximally sized trees with 18 nodes. Two trials
had minimum-sized trees with 3 nodes and maximally sized trees with 19 nodes.
5. Except for the rare case when an extremely small and an extremely large decision trees is found on the
same trial.

267

Murphy & Pazzani
xed size larger than the smallest decision tree. However, it is not clear that the minimumsized decision tree is the best possible policy for this concept. Indeed, by looking at the
data from Figure 3, it is apparent that a better strategy for this concept would be to nd
the minimum-sized tree and then decide whether to return the minimum-sized tree or a tree
of a dierent node cardinality as a function of the node cardinality of the minimum-sized
consistent tree. Table 2 shows which node cardinality has the highest probability of being
most accurate as a function of the minimally sized tree, together with the number of trials
(out of 1000) on which the minimum-sized tree had a particular node cardinality.
Minimum
Preferred
Number of
Node Cardinality Node Cardinality
Trials
2
2
49
3
5
17
4
5
300
5
5
351
6
8
211
7
8
71
8
8
1

Table 2. A policy of returning a larger decision tree as a function of the minimum-sized
tree for the XY Z _ AB concept.
Figure 11 provides some of the data that illustrates that the policy in Table 2 will
perform better than preferring the minimum-sized decision tree on this concept. Figure
12 graphs the observed probability that a consistent decision tree with a minimum node
cardinality of 5 (upper), 6 (middle), or 7 (lower) is more accurate than a larger tree as a
function of the dierence in node cardinalities between the two trees. The graph shows
that when the minimum-sized decision tree has 5 nodes, the probability that a larger tree
is more accurate is less than the probability that the smaller tree is more accurate for all
node cardinalities. This is particularly interesting because it shows that giving a decision
tree learner the size of the correct tree and having the decision tree learner produce an
hypothesis of this size is not the best strategy for this concept. However, when the smallest
consistent tree has 6 nodes, there is a 0.560 probability that a randomly chosen tree with
8 nodes will be more accurate and a 0.208 probability that a tree with 8 test nodes will
have the same accuracy. In addition, when the minimum-sized tree has 7 test nodes, the
probability that a tree with 8 nodes is more accurate is 0.345 while the probability that it
is less accurate is 0.312.
Note that we do not believe that the policy in Table 2 is uniformly superior to preferring
the minimum-sized decision tree. Rather, there is probably some interaction between the
complexity of the concept to be learned, the number of training examples, and the size of
the smallest consistent decision tree. Furthermore, a learner should not be tuned to learn a
particular concept, but should perform well on a variety of concepts. Clearly, if extremely
simple concepts are to be learned suciently frequently, the minimum-sized decision tree
policy will be better than the policy in Table 2. Indeed, the minimum-sized decision tree
268

Exploring the Decision Forest
policy would work well on the simple concepts A and AB discussed in Section 3.2. However,
if simple concepts are rarely encountered, there may be better policies. The best policy must
depend upon the distribution of concepts that are encountered. Clearly, if the only concept
Minimum Node Cardinality = 5
1.0

Probability

0.8
Prob(Smallest>Larger)
Prob(Larger>Smallest)
Prob(Smallest=Larger)

0.6
0.4
0.2
0.0
0

5
10
Difference in Node Cardinality

15

Minimum Node Cardinality = 6
1.0

Probability

0.8
Prob(Smallest>Larger)
Prob(Larger>Smallest)
Prob(Smallest=Larger)

0.6
0.4
0.2
0.0
0

5
10
Difference in Node Cardinality

15

Minimum Node Cardinality = 7
1.0

Probability

0.8
Prob(Smallest>Larger)
Prob(Larger>Smallest)
Prob(Smallest=Larger)

0.6
0.4
0.2
0.0
0

5
10
Difference in Node Cardinality

15

Figure 12. The probability that the accuracy of a minimum-sized decision tree is greater
than, equal to, or less than the accuracy of a larger tree as a function of the dierence of
node cardinalities for the XY Z _ AB concept when the minimum-sized decision tree has 5
(upper), 6 (middle), or 7 (lower) test nodes.

269

Murphy & Pazzani
to be learned is XY Z _AB , the best policy would be to ignore the training data and return
the decision tree representation for XY Z _ AB . It may be that Occam's razor should
be viewed as a philosophical statement about the distribution of concepts one is likely
to encounter. Occam's razor has not been shown to be a guarantee that when learning
a complex concept, the simplest hypothesis consistent with the data is likely to be more
accurate than the randomly-chosen more complex hypothesis consistent with the training
data.
5. Analysis

Schaer (1992, 1993) presents a series of experiments on overtting avoidance algorithms.
Overtting avoidance algorithms prefer simpler decision trees over more complex ones, even
though the simpler decision trees are less accurate on the training data, in hopes that the
trees will be more accurate on the test data. Schaer shows that these overtting avoidance
algorithms are a form of bias. Rather than uniformly improving performance, the overtting
avoidance algorithms improve performance on some distributions of concepts and worsen
performance on other distributions of concepts.
The results of our experiments go a step further than Schaer's. We have shown that
for some concepts, the preference for simpler decision trees does not result in an increase
in predictive accuracy on unseen test data, even when the simple trees are consistent with
the training data. Like Schaer, we do not dispute the theoretical results on Occam's
razor (Blumer, Ehrenfeucht, Haussler, & Warmuth, 1987), minimum description length
(Quinlan & Rivest, 1989; Muggleton et al., 1992), or minimizing the number of leaves of
a decision tree (Fayyad & Irani, 1990). Rather, we point out that for a variety of reasons,
the assumptions behind these theoretical results mean that the results do not apply to the
experiments reported here. For example, (Blumer et al., 1987) indicates that if one nds an
hypothesis in a suciently small hypothesis space (and simpler hypotheses are one example
of a small hypothesis space) and this hypothesis is consistent with a suciently large sample
of training data, one can be fairly condent that it will be fairly accurate on unseen data
drawn from the same distribution of examples. However, it does not say that on average
this hypothesis will be more accurate than other consistent hypotheses not in this small
hypothesis space.
The (Fayyad & Irani, 1990) paper explicitly states that the results on minimizing the
number of leaves of decision trees are worst case results and should not be used to make
absolute statements concerning improvements in performances. Nonetheless, informal arguments in the paper state: \This may then serve as a basis for provably establishing that one
method for inducing decision trees is better than another by proving that one algorithm
always produces a tree with a smaller number of leaves, given the same training data."
Furthermore, other informal arguments imply that this result is probabilistic because of the
existence of \pathological training sets." However, as we have shown in Figures 2 and 4
(as well as a reanalysis of the mux6 data in the Appendix), eliminating pathological (i.e.,
unrepresentative) training sets does not change the qualitative result that on some concepts,
the smaller trees are less accurate predictors than slightly larger trees.

270

Exploring the Decision Forest
6. Conclusion

We have reported on a series of experiments in which we generated all decision trees on
a variety of articial concepts and two naturally occurring data sets. We found that for
many of the concepts, the consistent decision trees that had a smaller number of nodes were
less accurate on unseen data than the slightly larger ones. These results do not contradict
existing theoretical results. Rather, they serve to remind us to be cautious when informally
using the intuitions derived from theoretical results on problems that are not covered by the
theorems or when using intuitions derived from worst-case results to predict average-case
performance.
We stress that our results are purely experimental. Like the reader, we too would be
pleased if there were theoretical results that indicated, for a given sample of training data,
which decision tree is likely to be most accurate. However, it is not clear whether this can be
done without knowledge of the distribution of concepts one is likely to encounter (Schaer,
1994).
We also note that our results may be due to the small size of the training sets relative to
the size of the correct tree. We tried to rule out this possibility by using larger training sets
(31 of the 32 possible examples) and by testing simpler concepts. For the simpler concepts,
the smallest decision trees were the most accurate, but error did not monotonically increase
with node cardinality. Since most decision tree learners that greedily build decision trees
do not return the smallest decision tree, our results may be of practical interest even for
simple concepts. In the future, experiments with more features and more examples could
help to answer this question, but considerably more complex problems cannot be handled
even by future generations of parallel supercomputers. In addition, we note that in our
experiments, we did not build decision trees in which a test did not partition the training
data. This explains why we found relatively few extremely large decision trees and may
explain why very large trees made few errors. To our knowledge, all decision tree algorithms
have this constraint. However, the theoretical work on learning does not make use of this
information. We could rerun all of our experiments without this constraint, but we would
prefer that some future theoretical work take this constraint into account.
Although we have found situations in which the smallest consistent decision tree is not
on average the most accurate and cases in which there is a greater than 0.5 probability that a
larger decision tree is more accurate than the smallest, we believe that learning algorithms
(and people) with no relevant knowledge of the concept and no information about the
distribution of concepts that are likely to be encountered should prefer simpler hypotheses.
This bias is appropriate for learning simple concepts. For more complex concepts, the
opposite bias, preferring the more complex hypotheses, is unlikely to produce an accurate
hypothesis (Blumer et al., 1987) and (Fayyad & Irani, 1990) due to the large number of
consistent complex hypotheses. We believe that the only way to learn complex hypotheses
reliably is to have some bias (e.g., prior domain knowledge) which favors particular complex
hypotheses such as combinations of existing hypotheses learned inductively as in OCCAM
(Pazzani, 1990). Indeed, (Valiant, 1984) advocates a similar position: \If the class of
learnable concepts is as severely limited as suggested by our results, then it would follow
that the only way of teaching more complex concepts is to build them up from simpler
ones."
271

Murphy & Pazzani
Acknowledgements

We thank Ross Quinlan, Georey Hinton, Michael Cameron-Jones, Cullen Schaer, Dennis Kibler, Steve Hampson, Jason Catlett, Haym Hirsh, Anselm Blumer, Steve Minton,
Michael Kearns, Tom Dietterich, Pat Langley, and David Schulenburg for commenting on
various aspects of this research. The research reported here was supported in part by
NSF infrastructure grant number MIP-9205737, NSF Grant INT-9201842, AFOSR grant
F49620-92-J-0430, and AFOSR AASERT grant F49620-93-1-0569.
Appendix A. Experiments on Additional Problems

In this appendix, we provide data on experiments which we ran on additional problems.
The experiments show that the basic ndings in this paper are not unique to the articial
concept, XY Z _ AB .
Mux6

The multiplexor concept we consider, mux6, has a total of 8 binary features. Six features
represent the functionality of a multiplexor and 2 features are irrelevant. The minimum sized
tree has 7 nodes. This particular concept was chosen because it is dicult for a top-down
inductive decision tree learner with limited look ahead to nd a small hypothesis (Quinlan,
1993). On each trial, we selected 20 examples randomly and tested on the remaining
examples. Since most of the computational cost of building consistent trees is for larger
node cardinalities and we are primarily interested in trees with small node cardinalities, we
only computed consistent trees with up to 10 nodes for 10 trials and up to 8 nodes for 340
0.45

350
Error
Trials

Error

250
200

0.35

150
100

0.30

50
0.25

Number of Trials

300

0.40

0
2

4

6
Node Cardinality

8

10

Figure 13. Error as a function of node cardinality for the mux6 concept.
trials. Figure 13 presents the average error as a function of the node cardinality for these
trials. This graph again shows that average error does not monotonically increase with node
cardinality. Trees of 4 nodes are on the average 4% less accurate than trees of 5 nodes.
272

Exploring the Decision Forest
Lenses

0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

Error

Error

The lenses domain has one 3-valued and three binary features, three classes, and 24 instances. Since the lenses domain has one non-binary feature, trees with a range of leaf
cardinalities are possible for a particular node cardinality. The minimum-sized tree has
6 nodes and 9 leaves. Separate analyses for leaf and node cardinalities were performed.
We used training set sizes of 8, 12, and 18 for this domain, built all consistent trees, and
measured the error rate on all unseen examples.

0.3
0.2

0.2

Size = 8
Size = 12
Size = 18

0.1

0.3
Size = 8
Size = 12
Size = 18

0.1

0.0

0.0
0

2

4

6 8 10 12
Node Cardinality

14 16 18

0

2

4

6 8 10 12 14 16 18 20
Leaf Cardinality

Figure 14. Error as a function of node cardinality (left) and error as a function of leaf
cardinality (right).
Figure 14 (left) shows the error as a function of the node cardinality for the 3 training
set sizes averaged over 50 trials. These curves indicate that the smallest consistent trees are
not always the most accurate. When observing the larger node cardinalities for the training
set sizes 12 and 18, error monotonically decreases with increasing node cardinality. Similar
statements can be said for the curve in Figure 14 (right), which relates average error as a
function of leaf cardinality.
Shuttle Landing

The shuttle landing domain has four binary and two 4-valued features, two classes, and 277
instances. The minimum-sized consistent tree has 7 nodes and 14 leaves. We used training
sets of size 20, 50, and 100 for the shuttle domain, generating all consistent decision trees
with fewer than 8, 10, and 12 nodes, and measured the error of these trees on all unseen
examples. Figure 15 presents the error as a function of leaf cardinality, averaged over

273

Murphy & Pazzani
10 trials. For this domain, there is a monotonically increasing relationship between node
cardinality and error.
0.4

size = 20
size = 50
size = 100

Error

0.3
0.2
0.1
0.0
0

2

4

6
8
10
Node Cardinality

12

14

Figure 15. Error as a function of node cardinality for the Shuttle concept.
References

Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. (1987). Occam's razor. Information Processing Letters, 24, 377{380.
Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classication and Regression
Trees. Pacic Grove, CA: Wadsworth & Brooks.
Fayyad, U., & Irani, K. (1990). What should be minimized in a decision tree?. In Proceedings
of the Eighth National Conference on Articial Intelligence, AAAI-90.
Fayyad, U., & Irani, K. (1992). The attribute selection problem in decision tree generation..
In Proceedings of the Tenth National Conference on Articial Intelligence, AAAI-92.
Muggleton, S., Srinivasan, A., & Bain, M. (1992). Compression, signicance and accuracy.
In Machine Learning: Proceedings of the Ninth International Workshop.
Murphy, P., & Aha, D. (1994). UCI Repository of machine learning databases [Machinereadable data repository]. Irvine, CA: University of California, Department of Information and Computer Science.
Pazzani, M. (1990). Creating a memory of causal relationships: An integration of empirical
and explanation-based learning methods. Hillsdale, NJ: Lawrence Erlbaum Associates.
Quinlan, J. (1986). Induction of decision trees. Machine Learning, 1 (1), 81{106.
Quinlan, J. (1993). C4.5 Programs for Machine Learning. San Mateo,CA: Morgan Kaufmann.
Quinlan, J., & Rivest, R. (1989). Inferring decision trees using the minimum description
length principle. Information and Computation, 80, 227{248.
274

Exploring the Decision Forest
Rissanen, J. (1978). Modeling by shortest data description. Automatica, 14, 465{471.
Schaer, C. (1992). Sparse data and the eect of overtting avoidance in decision tree
induction. In Proceedings of the Tenth National Conference on Articial Intelligence,
AAAI-92.
Schaer, C. (1993). Overtting avoidance as bias. Machine Learning, 10 (2), 153{178.
Schaer, C. (1994). A conservation law for generalization performance. Unpublished
Manuscript.
Valiant, L. (1984). A theory of the learnable. Communications of the ACM, 27 (11), 1134{
1142.
Wallace, C., & Patrick, J. (1993). Coding decision trees. Machine Learning, 11 (1), 7{22.

275

Journal of Articial Intelligence Research 1 (1994) 231-255

Submitted 12/93; published 2/94

Substructure Discovery Using Minimum Description
Length and Background Knowledge
Diane J. Cook
Lawrence B. Holder

Department of Computer Science Engineering
Box 19015
University of Texas at Arlington
Arlington, TX 76019 USA

cook@cse.uta.edu
holder@cse.uta.edu

Abstract

The ability to identify interesting and repetitive substructures is an essential component to discovering knowledge in structural data. We describe a new version of our Subdue substructure discovery system based on the minimum description length principle.
The Subdue system discovers substructures that compress the original data and represent
structural concepts in the data. By replacing previously-discovered substructures in the
data, multiple passes of Subdue produce a hierarchical description of the structural regularities in the data. Subdue uses a computationally-bounded inexact graph match that
identies similar, but not identical, instances of a substructure and nds an approximate
measure of closeness of two substructures when under computational constraints. In addition to the minimum description length principle, other background knowledge can be used
by Subdue to guide the search towards more appropriate substructures. Experiments in
a variety of domains demonstrate Subdue's ability to nd substructures capable of compressing the original data and to discover structural concepts important to the domain.

1. Introduction
The large amount of data collected today is quickly overwhelming researchers' abilities to
interpret the data and discover interesting patterns within the data. In response to this
problem, a number of researchers have developed techniques for discovering concepts in
databases. These techniques work well for data expressed in a non-structural, attributevalue representation, and address issues of data relevance, missing data, noise and uncertainty, and utilization of domain knowledge. However, recent data acquisition projects
are collecting structural data describing the relationships among the data objects. Correspondingly, there exists a need for techniques to analyze and discover concepts in structural
databases.
One method for discovering knowledge in structural data is the identication of common substructures within the data. The motivation for this process is to nd substructures
capable of compressing the data and to identify conceptually interesting substructures that
enhance the interpretation of the data. Substructure discovery is the process of identifying
concepts describing interesting and repetitive substructures within structural data. Once
discovered, the substructure concept can be used to simplify the data by replacing instances
of the substructure with a pointer to the newly discovered concept. The discovered substructure concepts allow abstraction over detailed structure in the original data and provide
c 1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Cook & Holder
new, relevant attributes for interpreting the data. Iteration of the substructure discovery
and replacement process constructs a hierarchical description of the structural data in terms
of the discovered substructures. This hierarchy provides varying levels of interpretation that
can be accessed based on the goals of the data analysis.
We describe a system called Subdue (Holder, Cook, & Bunke, 1992; Holder & Cook,
1993) that discovers interesting substructures in structural data based on the minimum
description length principle. The Subdue system discovers substructures that compress
the original data and represent structural concepts in the data. By replacing previouslydiscovered substructures in the data, multiple passes of Subdue produce a hierarchical description of the structural regularities in the data. Subdue uses a computationally-bounded
inexact graph match that identies similar, but not identical, instances of a substructure and
nds an approximate measure of closeness of two substructures when under computational
constraints. In addition to the minimum description length principle, other background
knowledge can be used by Subdue to guide the search towards more appropriate substructures.
The following sections describe the approach in detail. Section 2 describes the process of
substructure discovery and introduces needed denitions. Section 3 compares the Subdue
discovery system to other work found in the literature. Section 4 introduces the minimum
description length encoding used by this approach, and Section 5 presents the inexact
graph match algorithm employed by Subdue. Section 6 describes methods of incorporating
background knowledge into the substructure discovery process. The experiments detailed
in Section 7 demonstrate Subdue's ability to nd substructures that compress the data and
to re-discover known concepts in a variety of domains. Section 8 details the hierarchical
discovery process. We conclude with observations and directions for future research.

2. Substructure Discovery
The substructure discovery system represents structured data as a labeled graph. Objects
in the data map to vertices or small subgraphs in the graph, and relationships between
objects map to directed or undirected edges in the graph. A substructure is a connected
subgraph within the graphical representation. This graphical representation serves as input
to the substructure discovery system. Figure 1 shows a geometric example of such an input
graph. The objects in the gure (e.g., T1, S1, R1) become labeled vertices in the graph, and
the relationships (e.g., on(T1,S1), shape(C1,circle)) become labeled edges in the graph.
The graphical representation of the substructure discovered by Subdue from this data is
also shown in Figure 1.
An instance of a substructure in an input graph is a set of vertices and edges from
the input graph that match, graph theoretically, to the graphical representation of the
substructure. For example, the instances of the substructure in Figure 1 are shown in
Figure 2.
The substructure discovery algorithm used by Subdue is a computationally-constrained
beam search. The algorithm begins with the substructure matching a single vertex in the
graph. Each iteration through the algorithm selects the best substructure and expands the
instances of the substructure by one neighboring edge in all possible ways. The new unique
generated substructures become candidates for further expansion. The algorithm searches
232

Substructure Discovery

Input Graph

Substructure

T1

pe

S1

triangle

a
sh

C1
R1

T2

T3

S2

S3

on

T4

pe

square

a
sh

S4

Figure 1: Example substructure in graph form.
Instance

1

Instance

2

Instance

3

Instance

T1

T2

T3

T4

S1

S2

S3

S4

4

Figure 2: Instances of the substructure.
for the best substructure until all possible substructures have been considered or the total
amount of computation exceeds a given limit. The evaluation of each substructure is guided
by the MDL principle and other background knowledge provided by the user.
Typically, once the description length of an expanding substructure begins to increase,
further expansion of the substructure will not yield a smaller description length. As a
result, Subdue makes use of an optional pruning mechanism that eliminates substructure
expansions from consideration when the description lengths for these expansions increases.

3. Related Work
Several approaches to substructure discovery have been developed. Winston's Arch program (Winston, 1975) discovers substructures in order to deepen the hierarchical description
of a scene and to group objects into more general concepts. The Arch program searches for
two types of substructure in the blocks-world domain. The rst type involves a sequence
of objects connected by a chain of similar relations. The second type involves a set of
objects each having a similar relationship to some \grouping" object. The main dierence
between the substructure discovery procedures used by the Arch program and Subdue is
that the Arch program is designed specically for the blocks-world domain. For instance,
the sequence discovery method looks for supported-by and in-front-of relations only.
Subdue's substructure discovery method is domain independent, although the inclusion of
domain-specic knowledge would improve Subdue's performance.
Motivated by the need to construct a knowledge base of chemical structures, Levinson
(Levinson, 1984) developed a system for storing labeled graphs in which individual graphs
233

Cook & Holder
are represented by the set of vertices in a universal graph. In addition, the individual graphs
are maintained in a partial ordering dened by the subgraph-of relation, which improves
the performance of graph comparisons. The universal graph representation provides a
method for compressing the set of graphs stored in the knowledge base. Subgraphs of
the universal graph used by several individual graphs suggest common substructure in the
individual graphs. One dierence between the two approaches is that Levinson's system
is designed to incrementally process smaller individual graphs; whereas, Subdue processes
larger graphs all at once. Also, Levinson's system discovers common substructure only
as an indirect result of the universal graph construction; whereas, Subdue's main goal
is to discover and output substructure denitions that reduce the minimum description
length encoding of the graph. Finally, the subgraph-of partial ordering used by Levinson's
system is not included in Subdue, but maintaining this partial ordering would improve the
performance of the graph matching procedure by pruning the number of possible matching
graphs.
Segen (Segen, 1990) describes a system for storing graphs using a probabilistic graph
model to represent subsets of the graph. Alternative models are evaluated based on a minimum description length measure of the information needed to represent the stored graphs
using the model. In addition, Segen's system clusters the graphs into classes based on
minimizing the description length of the graphs according to the entire clustering. Apart
from the probabilistic representation, Segen's approach is similar to Levinson's system in
that both methods take advantage of commonalities in the graphs to assist in graph storage and matching. The probabilistic graphs contain information for identifying common
substructure in the exact graphs they represent. The portion of the probabilistic graph
with high probability denes a substructure that appears frequently in the exact graphs.
This notion was not emphasized in Segen's work, but provides an alternative method to
substructure discovery by clustering subgraphs of the original input graphs. As with Levinson's approach, graphs are processed incrementally, and substructure is found across several
graphs, not within a single graph as in Subdue.
The Labyrinth system (Thompson & Langley, 1991) extends the Cobweb incremental
conceptual clustering system (Fisher, 1987) to handle structured objects. Labyrinth uses
Cobweb to form hierarchical concepts of the individual objects in the domain based on
their primitive attributes. Concepts of structured objects are formed in a similar manner
using the individual objects as attributes. The resulting hierarchy represents a componential
model of the structured objects. Because Cobweb's concepts are probabilistic, Labyrinth
produces probabilistic models of the structured objects, but with an added hierarchical
organization. The upper-level components of the structured-object hierarchy produced by
Labyrinth represent substructures common to the examples. Therefore, although not the
primary focus, Labyrinth is discovering substructure, but in a more constrained context
than the general graph representation used by Subdue.
Conklin et al. (Conklin & Glasgow, 1992) have developed the i-mem system for constructing an image hierarchy, similar to that of Labyrinth, used for discovering common
substructures in a set of images and for ecient retrieval of images similar to a given image.
Images are expressed in terms of a set of relations dened by the user. Specic and general
(conceptual) images are stored in the hierarchy based on a subsumption relation similar
234

Substructure Discovery
to Levinson's subgraph-of partial ordering. Image matching utilizes a transformational
approach (similar to Subdue's inexact graph match) as a measure of image closeness.
As with the approaches of Segen and Levinson, i-mem is designed to process individual
images. Therefore, the general image concepts that appear higher in i-mem's hierarchy
will represent common substructures across several images. Subdue is designed to discover
common substructures within a single image. Subdue can mimic the individual approach
of these systems by processing a set of individual images as one disconnected graph. The
substructures found will be common to the individual images. The hierarchy also represents
a componential view of the images. This same view can be constructed by Subdue using
multiple passes over the graph after replacing portions of the input graph with substructures
discovered during previous passes. i-mem has performed well in a simple chess domain
and molecular chemistry domains (Conklin & Glasgow, 1992). However, i-mem requires
domain-specic relations for expressing images in order for the hierarchy to nd relevant
substructures and for image matching to be ecient. Again, maintaining the concepts
(images, graphs) in a partially-ordered hierarchy improves the eciency of matching and
retrieval, and suggests a possible improvement to Subdue.
The CLiP system (Yoshida, Motoda, & Indurkhya, 1993) for graph-based induction is
more similar to Subdue than the previous systems. CLiP iteratively discovers patterns in
graphs by expanding and combining patterns discovered in previous iterations. Patterns
are grouped into views based on their collective ability to compress the original input
graph. During each iteration CLiP uses existing views to contract the input graph and
then considers adding to the views new patterns consisting of two vertices and an edge from
the contracted graph. The compression of the new proposed views is estimated, and the
best views (according to a given beam width) are retained for the next iteration.
CLiP discovers substructures (patterns) dierently than Subdue. First, CLiP produces
a set of substructures that collectively compress the input graph; whereas, Subdue produces
only single substructures evaluated using the more principled minimum description length.
CLiP has the ability to grow substructures agglomeratively (i.e., merging two substructures
together); whereas, Subdue always produces new substructures using incremental growth
along one new edge. CLiP initially estimates the compression value of new views based on
the compression value of the parent view; whereas, Subdue performs an expensive exact
measurement of compression for each new substructure. Finally, CLiP employs an ecient
graph match based on graph identity, not graph isomorphism as in Subdue. Graph identity
assumes an ordering over the incident edges of a vertex and does not consider all possible
mappings when looking for occurrences of a pattern in an input graph. These dierences
in CLiP suggest possible enhancements to Subdue.
Research in pattern recognition has begun to investigate the use of graphs and graph
grammars as an underlying representation for structural problems (Schalko, 1992). Many
results in grammatical inference are applicable to constrained classes of graphs (e.g., trees)
(Fu, 1982; Miclet, 1986). The approach begins with a set of sample graphs and produces a
generalized graph grammar capable of deriving the original sample graphs and many others.
The production rules of this general grammar capture regularities (substructures) in the
sample graphs. Jeltsch and Kreowski (Jeltsch & Kreowski, 1991) describe an approach that
begins with a maximally-specic grammar and iteratively identies common subgraphs in
the right-hand sides of the production rules. These common subgraphs are used to form
235

Cook & Holder
new, more general production rules. Although their method does not address the underlying
combinatorial nondeterminism, heuristic approaches could provide a feasible method for
extracting substructures in the form of graph grammars. Furthermore, the graph grammar
production-rule may provide a suitable representation for background knowledge during the
substructure discovery process.

4. Minimum Description Length Encoding of Graphs

The minimum description length principle (MDLP) introduced by Rissanen (Rissanen,
1989) states that the best theory to describe a set of data is that theory which minimizes
the description length of the entire data set. The MDL principle has been used for decision
tree induction (Quinlan & Rivest, 1989), image processing (Pednault, 1989; Pentland, 1989;
Leclerc, 1989), concept learning from relational data (Derthick, 1991), and learning models
of non-homogeneous engineering domains (Rao & Lu, 1992).
We demonstrate how the minimum description length principle can be used to discover
substructures in complex data. In particular, a substructure is evaluated based on how well
it can compress the entire dataset using the minimum description length. We dene the
minimum description length of a graph to be the number of bits necessary to completely
describe the graph.
According to the minimum description length (MDL) principle, the theory that best
accounts for a collection of data is the one that minimizes I (S ) + I (GjS ), where S is the
discovered substructure, G is the input graph, I (S ) is the number of bits required to encode
the discovered substructure, and I (GjS ) is the number of bits required to encode the input
graph G with respect to S .
The graph connectivity can be represented by an adjacency matrix. Consider a graph
that has n vertices, which are numbered 0; 1; : : :; n , 1. An n  n adjacency matrix A can
be formed with entry A[i; j ] set to 0 or 1. If A[i; j ] = 0, then there is no connection from
vertex i to vertex j . If A[i; j ] = 1, then there is at least one connection from vertex i to
vertex j . Undirected edges are recorded in only one entry of the matrix. The adjacency
matrix for the graph in Figure 3 is shown below.
x 20 1 1 0 0 03
triangle 66 0 0 0 0 0 0 77
y 666 0 0 0 1 1 0 777
square 66 0 0 0 0 0 0 77
r 40 0 0 0 0 15
rectangle 0 0 0 0 0 0
The encoding of the graph consists of the following steps. We assume that the decoder
has a table of the lu unique labels in the original graph G.
1. Determine the number of bits vbits needed to encode the vertex labels of the graph.
First, we need (lg v ) bits to encode the number of vertices v in the graph. Then,
encoding the labels of all v vertices requires (v lg lu ) bits. We assume the vertices are
specied in the same order they appear in the adjacency matrix. The total number
of bits to encode the vertex labels is
vbits = lg v + v lg lu
236

Substructure Discovery

triangle

e

p
ha

s
x

on

pe

square

a
sh
y

on

pe

rectangle

a
sh
r

Figure 3: MDL example graph.
For the example in Figure 3, v = 6, and we assume that there are lu = 8 unique
labels in the original graph. The number of bits needed to encode these vertices is
lg 6 + 6 lg 8 = 20:58 bits.
2. Determine the number of bits rbits needed to encode the rows of the adjacency matrix
A. Typically, in large graphs, a single vertex has edges to only a small percentage of
the vertices in the entire graph. Therefore, a typical row in the adjacency matrix will
have much fewer than v 1s, where v is the total number of vertices in the graph. We
apply a variant of the coding scheme used by (Quinlan & Rivest, 1989) to encode bit
strings with length n consisting of k 1s and (n , k) 0s, where k  (n , k). In our
case, row i (1  i  v ) can be represented as a bit string of length v containing ki
1s. If we let b = maxi ki , then the ith row of the adjacency matrix can be encoded as
follows.
(a) Encoding the value of ki requires lg(b + 1) bits.
 
(b) Given that only ki 1s occur in the row bit string of length v , only kvi strings
of 0s and 1s are
Since all of these strings have equal probability of
 possible.

v
occurrence, lg ki bits are needed to encode the positions of 1s in row i. The
value of v is known from the vertex encoding.
Finally, we need an additional lg(b + 1) bits to encode the number of bits needed to
specify the value of ki for each row. The total encoding length in bits for the adjacency
matrix is

rbits = lg(b + 1) +

v
X
i=1

= (v + 1) lg(b + 1)
237

 

lg(b + 1) + lg kvi
v
X
i=1

 

lg kvi

Cook & Holder
For the example in Figure 3, b = 2, and
of
the
 the number
 
 bits
 needed
  to encode

6
6
6
6
6
6
adjacency matrix is (7 lg 3)+lg 2 +lg 0 +lg 2 +lg 0 +lg 1 +lg 0 = 21:49
bits.
3. Determine the number of bits ebits needed to encode the edges represented by the
entries A[i; j ] = 1 of the adjacency matrix A. The number of bits needed to encode
entry A[i; j ] is (lg m) + e(i; j )[1 + lg lu ], where e(i; j ) is the actual number of edges
between vertex i and j in the graph and m = maxi;j e(i; j ). The (lg m) bits are needed
to encode the number of edges between vertex i and j , and [1 + lg lu ] bits are needed
per edge to encode the edge label and whether the edge is directed or undirected. In
addition to encoding the edges, we need to encode the number of bits (lg m) needed
to specify the number of edges per entry. The total encoding of the edges is

ebits = lg m +

v X
v
X
i=1 j =1

lg m + e(i; j )[1 + lg lu ]

= lg m + e(1 + lg lu ) +

v X
v
X
i=1 j =1

A[i; j ] lg m

= e(1 + lg lu ) + (K + 1) lg m
where e is the number of edges in the graph, and K is the number of 1s in the adjacency
matrix A. For the example in Figure 3, e = 5, K = 5, m = 1, lu = 8, and the number
of bits needed to encode the edges is 5(1 + lg 8) + 6 lg 1 = 20.
The total encoding of the graph takes (vbits + rbits + ebits) bits. For the example in
Figure 3, this value is 62:07 bits.
Both the input graph and discovered substructure can be encoded using the above
scheme. After a substructure is discovered, each instance of the substructure in the input
graph is replaced by a single vertex representing the entire substructure. The discovered
substructure is represented in I (S ) bits, and the graph after the substructure replacement is
represented in I (GjS ) bits. Subdue searches for the substructure S in graph G minimizing
I (S ) + I (GjS ).

5. Inexact Graph Match

Although exact structure match can be used to nd many interesting substructures, many
of the most interesting substructures show up in a slightly dierent form throughout the
data. These dierences may be due to noise and distortion, or may just illustrate slight
dierences between instances of the same general class of structures. Consider the image
shown in Figure 9. The pencil and the cube would make ideal substructures in the picture,
but an exact match algorithm may not consider these as strong substructures, because they
rarely occur in the same form and level of detail throughout the picture.
Given an input graph and a set of dened substructures, we want to nd those subgraphs
of the input graph that most closely resemble the given substructures. Furthermore, we want
to associate a distance measure between a pair of graphs consisting of a given substructure
and a subgraph of the input graph. We adopt the approach to inexact graph match given
by Bunke and Allermann (Bunke & Allermann, 1983).
238

Substructure Discovery

g1

g2

b

B

a
A

B

1

2

A

3

4
a
a

b

b
5
B

Figure 4: Two similar graphs g1 and g2 .
In this inexact match approach, each distortion of a graph is assigned a cost. A distortion
is described in terms of basic transformations such as deletion, insertion, and substitution
of vertices and edges. The distortion costs can be determined by the user to bias the match
for or against particular types of distortions.
An inexact graph match between two graphs g1 and g2 maps g1 to g2 such that g2 is
interpreted as a distorted version of g1. Formally, an inexact graph match is a mapping
f : N1 ! N2 [ fg, where N1 and N2 are the sets of vertices of g1 and g2, respectively. A
vertex v 2 N1 that is mapped to  (i.e., f (v ) = ) is deleted. That is, it has no corresponding
vertex in g2. Given a set of particular distortion costs as discussed above, we dene the cost
of an inexact graph match cost(f ), as the sum of the cost of the individual transformations
resulting from f , and we dene matchcost(g1 ; g2) as the value of the least-cost function that
maps graph g1 onto graph g2.
Given g1 , g2, and a set of distortion costs, the actual computation of matchcost(g1 ; g2)
can be determined using a tree search procedure. A state in the search tree corresponds to
a partial match that maps a subset of the vertices of g1 to a subset of the vertices in g2.
Initially, we start with an empty mapping at the root of the search tree. Expanding a state
corresponds to adding a pair of vertices, one from g1 and one from g2, to the partial mapping
constructed so far. A nal state in the search tree is a match that maps all vertices of g1 to
g2 or to . The complete search tree of the example in Figure 4 is shown in Figure 5. For
this example we assign a value of 1 to each distortion cost. The numbers in circles in this
gure represent the cost of a state. As we are eventually interested in the mapping with
minimum cost, each state in the search tree gets assigned the cost of the partial mapping
that it represents. Thus the goal state to be found by our tree search procedure is the
nal state with minimum cost among all nal states. From Figure 5 we conclude that the
minimum cost inexact graph match of g1 and g2 is given by the mapping f (1) = 4, f (2) = 3.
The cost of this mapping is 4.
Given graphs g1 with n vertices and g2 with m vertices, m  n, the complexity of the
full inexact graph match is O(nm+1 ). Because this routine is used heavily throughout the
239

Cook & Holder

(1, 3) 1

(1, 5) 1

(1, 4) 0

(1,

)1

(2,4) (2,5) (2, ) (2,3) (2,5) (2, ) (2,3) (2,4) (2, ) (2,3) (2,4) (2,5) (2, )
7

6

10

3

6

9

7

7

10

9

10

9

11

Figure 5: Search tree for computing matchcost(g1,g2) from Figure 4.
discovery and evaluation process, the complexity of the algorithm can signicantly degrade
the performance of the system.
To improve the performance of the inexact graph match algorithm, we extend Bunke's
approach by applying a branch-and-bound search to the tree. The cost from the root of the
tree to a given node is computed as described above. Nodes are considered for pairings in
order from the most heavily connected vertex to the least connected, as this constrains the
remaining match. Because branch-and-bound search guarantees an optimal solution, the
search ends as soon as the rst complete mapping is found.
In addition, the user can place a limit on the number of search nodes considered by
the branch-and-bound procedure (dened as a function of the size of the input graphs).
Once the number of nodes expanded in the search tree reaches the dened limit, the search
resorts to hill climbing using the cost of the mapping so far as the measure for choosing the
best node at a given level. By dening such a limit, signicant speedup can be realized at
the expense of accuracy for the computed match cost.
Another approach to inexact graph match would be to encode the dierence between
two graphs using the MDL principle. Smaller encodings would indicate a lower match cost
between the two graphs. We leave this as a future research direction.

6. Guiding the Discovery Process with Background Knowledge
Although the principle of minimum description length is useful for discovering substructures that maximize compression of the data, scientists may realize more benet from the
discovery of substructures that exhibit other domain-specic and domain-independent characteristics.
To make Subdue more powerful across a wide variety of domains, we have added the
ability to guide the discovery process with background knowledge. Although the minimum
description length principle still drives the discovery process, the background knowledge can
be used to input a bias toward certain types of substructures. This background knowledge
is encoded in the form of rules for evaluating substructures, and can represent domainindependent or domain-dependent rules. Each time a substructure is evaluated, these input
240

Substructure Discovery
rules are used to determine the value of the substructure under consideration. Because
only the most-favored substructures are kept and expanded, these rules bias the discovery
process of the system.
Each background rule can be assigned a positive, zero, or negative weight, that biases
the procedure toward a type of substructure, eliminates the use of the rule, or biases the
procedure away from a type of substructure, respectively. The value of a substructure is
dened as the description length (DL) of the input graph using the substructure multiplied by the weighted value of each background rule from a set of rules R applied to the
substructure.

value(s) = DL(G; s) 

jRj
Y
r=1

ruler (s)er

(1)

Three domain-independent heuristics that have been incorporated as rules into the Subdue system are compactness, connectivity, and coverage. For the denitions of these rules,

we will let G represent the input graph, s represent a substructure in the graph, and I
represent the set of instances of the substructure s in G. The instance weight w of an
instance i 2 I of a substructure s is dened to be
(i; s)
(2)
w(i; s) = 1 , matchcost
size(i) ;
where size(i) = #vertices(i) + #edges(i). If the match cost is greater than the size of the
larger graph, then w(i; s) = 0. The instance weights are used in these rules to compute a
weighted average over instances of a substructure. A value of 1 is added to each formula so
that the exponential weights can be used to control the rule's signicance.
The rst rule, compactness, is a generalization of Wertheimer's Factor of Closure, which
states that human attention is drawn to closed structures (Wertheimer, 1939). A closed
substructure has at least as many edges as vertices, whereas a non-closed substructure
has fewer edges than vertices (Prather, 1976). Thus, closed substructures have a higher
compactness value. Compactness is dened as the weighted average of the ratio of the
number of edges in the substructure to the number of vertices in the substructure.

compactness(s) = 1 + j1I j

X

i2I

edges(i)
w(i; s)  ##vertices
(i)

(3)

The second rule, connectivity, measures the amount of external connection in the instances of the substructure. The connectivity rule is a variant of Wertheimer's Factor
of Proximity (Wertheimer, 1939), and is related to earlier numerical clustering techniques
(Zahn, 1971). These works demonstrate the human preference for \isolated" substructures,
that is, substructures that are minimally related to adjoining structure. Connectivity measures the \isolation" of a substructure by computing the inverse of the average number of
external connections over all the weighted instances of the substructure in the input graph.
An external connection is dened here as an edge that connects a vertex in the substructure
to a vertex outside the substructure. The formula for determining the connectivity of a
substructure s with instances I in the input graph G is given below.
241

Cook & Holder
"

connectivity(s) = 1 + 1

X

jI j i2I w(i; s)  num external conns(i)

#,1

(4)

The third rule, coverage, measures the fraction of structure in the input graph described
by the substructure. The coverage rule is motivated from research in inductive learning and
provides that concept descriptions describing more input examples are considered better
(Michalski & Stepp, 1983). Although MDL measures the amount of structure, the coverage
rule includes the relevance of this savings with respect to the size of the entire input graph.
Coverage is dened as the number of unique vertices and edges in the instances of the
substructure divided by the total number of vertices and edges in the input graph. In this
formula, the unique structure(i) of an instance i is the number of vertices and edges in i
that have not already appeared in previous instances in the summation.

coverage(s) = 1 +

P
i2I w(i; s)  unique

size(G)

structure(i)

(5)

Domain-dependent rules can also be used to guide the discovery process in a domain
where scientists can contribute their expertise. For example, CAD circuits generally consist
of two types of components, active and passive components. The active components are
the main driving components. Identifying the active components is the rst step in understanding the main function of the circuit. To add this knowledge to Subdue we include
a rule that assigns higher values to substructures (circuit components) representing active
components and lower values to substructures representing passive components. Since the
active components have higher scores, they are expected to be selected. The system can
then focus the attention on the active components which will be expanded to the functional
substructures.
Another method of biasing the discovery process with background knowledge is to let
background rules aect the prior probabilities of possible substructures. However, choosing
the appropriate prior probabilities to express desired properties of substructures is dicult, but indicates a future direction for the inclusion of background knowledge into the
substructure discovery process.

7. Experiments
The experiments in this section evaluate Subdue's substructure discovery capability in
several domains, including chemical compound analysis, scene analysis, CAD circuit design
analysis, and analysis of an articially-generated structural database.
Two goals of our substructure discovery system are to nd substructures that can reduce
the amount of information needed to describe the data, and to nd substructures that are
considered interesting for the given database. As a result, we evaluate the Subdue system
in this section along these two criteria. First, we measure the amount of compression that
Subdue provides across a variety of databases. Second, we use the Subdue system with the
additional background knowledge rules to re-discover substructures that have been identied
as interesting by experts in each specic domain. Section 7.1 describes the domains used
in these experiments, and Section 7.2 presents the experimental results.
242

Substructure Discovery

CH 2OH

O
CH 3

CH 3

O

C

O
OH

Figure 6: Cortisone.
CH 3
C
CH
2

CH 3

H

C

C
CH

2

CH

CH

2
C

CH 3

2

CH 3

H

C

C
CH

CH

2

2

CH

CH

2

2

C

C
CH 3

H

H

CH

2

C
CH

2

C
H

Figure 7: Natural rubber (all-cis polyisoprene).

7.1 Domains
7.1.1 Chemical Compound Analysis

Chemical compounds are rich in structure. Identication of the common and interesting
substructures can benet scientists by identifying recurring components, simplying the data
description, and focusing on substructures that stand out and merit additional attention.
Chemical compounds are represented graphically by mapping individual atoms, such as
carbon and oxygen, to labeled vertices in the graph, and by mapping bonds between the
atoms onto labeled edges in the graph. Figures 6, 7, and 8 show the graphs representing
the chemical compound databases for cortisone, rubber, and a portion of a DNA molecule.
7.1.2 Scene Analysis

Images and scene descriptions provide a rich source of structure. Images that humans
encounter, both natural and synthesized, have many structured subcomponents that draw
our attention and that help us to interpret the data or the scene.
Discovering common structures in scenes can be useful to a computer vision system.
First, automatic substructure discovery can help a system interpret an image. Instead of
working from low-level vertices and edges, Subdue can provide more abstract structured
components, resulting in a hierarchical view of the image that the machine can analyze at
many levels of detail and focus, depending on the goal of the analysis. Second, substructure
discovery that makes use of an inexact graph match can help identify objects in a 2D image
of a 3D scene where noise and orientation dierences are likely to exist. If an object appears often in the scene, the inexact graph match driving the Subdue system may capture
slightly dierent views of the same object. Although an object may be dicult to identify
243

Cook & Holder

O
CH2 O
N

adenine

N
N

N
O

O

N

P

O

O
H
N

OH

O

N

CH 3

O

H
P

HO

O
N

H

N

H

O

N

guanine
N
O
P

O

O

CH2 O

O

thymine

CH2

O

H

N

N

O

O

N
H

OH

N

cytosine

CH2
CH 3

O

H

P

HO

O

O

O
O

CH2 O
N

thymine
P

N

H

N

O

N

O
O

N
N

CH 3

O

H

OH

CH2

adenine

N
H

O
P

HO

O
O

Figure 8: Portion of a DNA molecule.

Figure 9: Scene analysis example.
244

O

Substructure Discovery

f

a

k

x

l

t

p

m

Figure 10: Possible vertices and labels.

l

l
l

l

l

l

l

l

t

l

l
t
t
t

l

t
m

t
t
a
l

l

l

l

l

l

l
t

t

l

a

l

a

f
l

l

a

Figure 11: Portion of graph representing scene in Figure 4.
from just one 2D picture, Subdue will match instances of similar objects, and the dierences between these instances can provide additional information for identication. Third,
substructure discovery can be used to compress the image. Replacing common interesting
substructures by a single vertex simplies the image description and reduces the amount of
storage necessary to represent the image.
To apply Subdue to image data, we extract edge information from the image and
construct a graph representing the scene. The graph representation consists of eight types
of vertices and two types of arcs (edge and space). The vertex labels (f , a, l, t, k, x, p, and
m) follow the Waltz labelings (Waltz, 1975) of junctions of edges in the image and represent
the types of vertices shown in Figure 10. An edge arc represents the edge of an object in the
image, and a space arc links non-connecting objects together. The edge arcs represent an
edge in the scene that connects two vertices, and the space arcs connect the closest vertices
from two disjoint neighboring objects. Distance, curve, and angle information has not been
included in the graph representation, but can be added to give additional information about
the scene. Figure 11 shows the graph representation of a portion of the scene depicted in
Figure 9. In this gure, the edge arcs are solid and the space arcs are dashed.
245

Cook & Holder

VCC

ext_pin

drain
drain
gate
n_mosfet

gate
source

source
connect

drain

drain
gate
connect

gate

n_mosfet

source

ext_pin

GND

Figure 12: Amplier circuit and graph representation.
7.1.3 CAD Circuit Analysis

In this domain, we employ Subdue to nd circuit components in CAD circuit data. Discovery of substructures in circuit data can be a valuable tool to an engineer who is attempting to
identify common reusable parts in a circuit layout. Replacing individual components in the
circuit description by larger substructure descriptions will also simplify the representation
of the circuit.
The data for the circuit domain was obtained from National Semiconductor, and consists of a set of components making up a circuit as output by the Cadence Design System.
The particular circuit used for this experiment is a portion of an analog-to-digital converter. Figure 12 presents a circuit for an amplier and gives the corresponding graph
representation.
7.1.4 Artificial Domain

In the nal domain, we articially generate graphs to evaluate Subdue's ability to discover
substructures capable of compressing the graph. Four substructures are created of varying
sizes with randomly-selected vertices and edges (see Figure 13). The name of a substructure
reects the number of vertices and edges in its graph representation. Next, these substructures are embedded in larger graphs whose size is 15 times the size of the substructure.
The graphs vary across four parameters: number of possible vertex and edge labels (one
times and two times the number of labels used in the substructure), connectivity of the
substructure (1 or 2 external connections), coverage of the instances (60% and 80%), and
246

Substructure Discovery

e1
e2

e3

n4

n1

e1
n3

e2

n2

n4

n3
e3

n2

e3
e6
n7

e3
n5

e1
n2

e4
n5

e6

e3

n1

n6

n3

n1

e9

e3
e5
n3

e2
n3

n1

e2

e3
n1

n7

e8
n2

n4

e6

e1
e7
e8

Figure 13: Four articial substructures used to evaluate Subdue.

the amount of distortion in the instances (0, 1 or 2 distortions). This yields a total of 96
graphs (24 for each dierent substructure).

7.2 Experimental Results
7.2.1 Experiment 1: Data compression

In the rst experiment, we test Subdue's ability to compress a structural database. Using
a beam width of 4 and Subdue's pruning mechanism, we applied the discovery algorithm to
each of the databases mentioned above. We repeat the experiment with match thresholds
ranging from 0.0 to 1.0 in increments of 0.1. Table 1 shows the description length (DL) of the
original graph, the description length of the best substructure discovered by Subdue, and
graph
the value of compression. Compression here is dened as DLDLofofcompressed
original graph . Figure 14,
shows the actual discovered substructures for the rst four datasets.
As can be seen from Table 1, Subdue was able to reduce the database to slightly
larger than 14 of its original size in the best case. The average compression value over
all of these domains (treating the articial graphs as one value) is 0.62. The results of
this experiment demonstrate that the substructure discovered by Subdue can signicantly
reduce the amount of data needed to represent an input graph. We expect that compressing
the graph using combinations of substructures and hierarchies of substructures will realize
even greater compression in some databases.
247

Cook & Holder

Database
DLoriginal Thresholdoptimal DLcompressed Compression
Rubber
371.78
0.1
95.20
0.26
Cortisone
355.03
0.3
173.25
0.49
DNA
2427.93
1.0
2211.87
0.91
Pencils
1592.33
1.0
769.18
0.48
CAD { M1
4095.73
0.7
2148.8
0.52
CAD { S1SegDec
1860.14
0.7
1149.29
0.62
CAD { S1DrvBlk
12715.12
0.7
9070.21
0.71
CAD { BlankSub
8606.69
0.7
6204.74
0.72
CAD { And2
427.73
0.1
324.52
0.76
Articial (avg. over 96 graphs) 1636.25
0.0: : :1.0
1164.02
0.71
Table 1: Graph compression results.

CH 3

H

CH2

O

a

O

C
C

C

C

C

CH

CH
2

2

(a)

C

C

(b)

C
l

O

(c)

a

(d)

Figure 14: Best substructure for (a) rubber database, (b) cortisone database, (c) DNA
database, and (d) image database.

248

Substructure Discovery

Figure 15: Benzene ring discovered by Subdue.
7.2.2 Experiment 2: Re-discovery of known substructures using background
knowledge

Another way of evaluating the discovery process is to evaluate the interestingness of the
discovered substructures. The determination of this value will change from domain to
domain. As a result, in this second set of experiments we test Subdue's ability to discover
substructures that have already been labeled as important by experts in the domains under
consideration.
In the chemical compound domain, chemists frequently describe compounds in terms of
the building-block components that are heavily used. For example, in the rubber compound
database shown in Figure 7, the compound is made up of a chain of structures that are
labeled by chemists as isoprene units. Subdue's ability to re-discover this structure is
exemplied in Figure 14a. This substructure, which was discovered using the MDL principle
with no extra background knowledge, represents an isoprene unit.
Although Subdue was able to re-discover isoprene units without extra background
knowledge, the substructure aording the most compression will not always be the most interesting or important substructure in the database. For example, in the cortisone database
the benzene ring which consists of a ring of carbons is not discovered using only the MDL
principle. However, the additional background rules can be used to increase the chance of
nding interesting substructures in these domains. In the case of the cortisone compound,
we know that the interesting structures exhibit a characteristic of closure. Therefore, we
give a strong weight (8.0) to the compactness background rule and use a match threshold of
0.2 to allow for deviations in the benzene ring instances. In the resulting output, Subdue
nds the benzene ring shown in Figure 15.
In the same way, we can use the background rules to nd the pencil substructure in
the image data. When the image in Figure 9 is viewed, the substructure of interest is the
pencil in its various forms. However, the substructure that aorded the most compression
does not make up an entire pencil. We know that the pencils have a high degree of closure
and of coverage, so the weights for these rules are set to 1.0. With these weights, Subdue
is able to nd the pencil substructure shown in Figure 16 for all tested match thresholds
between 0.0 and 1.0.

8. Hierarchical Concept Discovery

After a substructure is discovered, each instance of the substructure in the input graph can
be replaced by a single vertex representing the entire substructure. The discovery procedure
can then be repeated on the compressed data set, resulting in new interesting substructures.
If the newly-discovered substructures are dened in terms of existing substructure concepts,
the substructure denitions form a hierarchy of substructure concepts.
249

Cook & Holder

l

l

a

a
l

Figure 16: Pencil substructure discovered by Subdue.

Hierarchical concept discovery also adds the capability to improve Subdue's performance. When Subdue is applied to a large input graph, the complexity of the algorithm
prevents consideration of larger substructures. Using hierarchical concept discovery, Subdue can rst discover those smaller substructures which best compress the data. Applying
the compression reduces the graph to a more manageable size, increasing the chance that
Subdue will nd the larger substructures on the subsequent passes through the database.
Once Subdue selects a substructure, all vertices that comprise the exact instances of
the substructure are replaced in the graph by a single vertex representing the discovered
substructure. Edges connecting vertices outside the instance to vertices inside the instance
now connect to the new vertex. Edges internal to the instance are removed. The discovery
process is then applied to the compressed data. If a hierarchical description of concepts is
particularly desired, heavier weight can be given to substructures which utilize previously
discovered substructures. The increased weight reects increased attention to this substructure. Figure 17 illustrates the compressed rubber compound graph using the substructure
shown in Figure 14a.
To demonstrate the ability of Subdue to nd a hierarchy of substructures, we let the system make multiple passes through a database that represents a portion of a DNA molecule.
Figure 8 shows a portion of two chains of a double helix, using three pairs of bases which
are held together by hydrogen bonds. Figure 18 shows the substructures found by Subdue
after each of three passes through the data. Note that, on the third pass, Subdue linked
together the instances of the substructure in the second pass to nd the chains of the double
helix.
Although replacing portions of the input graph with the discovered substructures compresses the data and provides a basis for discovering hierarchical concepts in the data, the
substructure replacement procedure becomes more complicated when concepts with inexact
instances are discovered. When inexact instances of a discovered concept are replaced by
a single vertex in the data, all distortions of the graph (dierences between the instance
graph and the substructure denition) must be attached as annotations to the vertex label.
250

Substructure Discovery

Highest-valued substructure

CH 3

H

S =
1

C

C
CH

CH

2

2

Compressed graph using discovered substructure

G

S1

=

CH

CH

C
2

C
2

CH

CH

2

2
C

CH 3

CH

H

3

C
CH

S1

S1

CH

H

3

=

S1

S
1

C

C
CH

CH

2

2

CH

CH

2

2

C

C
CH 3

H

H

3

CH

2

C
H

Figure 17: Compressed graph for rubber compound data.

251

C
CH

2

Cook & Holder

Highest-valued substructure
after First Pass

CH2 O

S1 =

O
O

Highest-valued substructure
after Second Pass

S1

C

CH2 O

O

S2 =

=
O

P
P

O
CH2 O

Highest-valued substructure
after Third Pass

O
O
O

S2

P

OH

OH
O

=

S3 =

CH2 O

O

S2

OH

O

S2

OH

O
O

P

O

OH
O
CH2 O

O
O

P

OH
O

Figure 18: Hierarchical discovery in DNA data.

252

Substructure Discovery

9. Conclusions

Extracting knowledge from structural databases requires the identication of repetitive substructures in the data. Substructure discovery identies interesting and repetitive structure
in structural data. The substructures represent concepts found in the data and a means of
reducing the complexity of the representation by abstracting over instances of the substructure. We have shown how the minimum description length (MDL) principle can be used to
perform substructure discovery in a variety of domains. The substructure discovery process
can also be guided by background knowledge. The use of an inexact graph match allows
deviation in the instances of a substructure. Once a substructure is discovered, instances
of the substructure can be replaced by the concept denition, aording compression of the
data description and providing a basis for discovering hierarchically-dened structures.
Future work will combine structural discovery with discovery of concepts using a linearbased representation such as AutoClass (Cheeseman, Kelly, Self, Stutz, Taylor, & Freeman,
1988). In particular, we will use Subdue to compress the data fed to AutoClass, and
let Subdue evaluate the interesting structures in the classes generated by AutoClass. In
addition, we will be developing a parallel implementation of the AutoClass / Subdue
system that will enable application of substructure discovery to larger structural databases.

Acknowledgements

This project is supported by NASA grant NAS5-32337. The authors would like to thank
Mike Shay at National Semiconductor for providing the circuit data. We would also like
to thank Surnjani Djoko and Tom Lai for their help with this project. Thanks also to the
reviewers for their numerous insightful comments.

References

Bunke, H., & Allermann, G. (1983). Inexact graph matching for structural pattern recognition. Pattern Recognition Letters, 1 (4), 245{253.
Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. (1988). Autoclass:
A bayesian classication system. In Proceedings of the Fifth International Workshop
on Machine Learning, pp. 54{64.
Conklin, D., & Glasgow, J. (1992). Spatial analogy and subsumption. In Proceedings of the
Ninth International Machine Learning Workshop, pp. 111{116.
Derthick, M. (1991). A minimal encoding approach to feature discovery. In Proceedings of
the Ninth National Conference on Articial Intelligence, pp. 565{571.
Fisher, D. H. (1987). Knowledge acquisition via incremental conceptual clustering. Machine
Learning, 2 (2), 139{172.
Fu, K. S. (1982). Syntactic Pattern Recognition and Applications. Prentice-Hall.
Holder, L. B., Cook, D. J., & Bunke, H. (1992). Fuzzy substructure discovery. In Proceedings
of the Ninth International Machine Learning Conference, pp. 218{223.
253

Cook & Holder
Holder, L. B., & Cook, D. J. (1993). Discovery of inexact concepts from structural data.
IEEE Transactions on Knowledge and Data Engineering, 5 (6), 992{994.
Jeltsch, E., & Kreowski, H. J. (1991). Grammatical inference based on hyperedge replacement. In Fourth International Workshop on Graph Grammars and Their Application
to Computer Science, pp. 461{474.
Leclerc, Y. G. (1989). Constructing simple stable descriptions for image partitioning. International journal of Computer Vision, 3 (1), 73{102.
Levinson, R. (1984). A self-organizing retrieval system for graphs. In Proceedings of the
Second National Conference on Articial Intelligence, pp. 203{206.
Michalski, R. S., & Stepp, R. E. (1983). Learning from observation: Conceptual clustering.
In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning:
An Articial Intelligence Approach, Vol. I, pp. 331{363. Tioga Publishing Company.
Miclet, L. (1986). Structural Methods in Pattern Recognition. Chapman and Hall.
Pednault, E. P. D. (1989). Some experiments in applying inductive inference principles
to surfa ce reconstruction. In Proceedings of the International Joint Conference on
Articial Intelligence, pp. 1603{1609.
Pentland, A. (1989). Part segmentation for object recognition. Neural Computation, 1,
82{91.
Prather, R. (1976). Discrete Mathemetical Structures for Computer Science. Houghton
Min Company.
Quinlan, J. R., & Rivest, R. L. (1989). Inferring decision trees using the minimum description length principle. Information and Computation, 80, 227{248.
Rao, R. B., & Lu, S. C. (1992). Learning engineering models with the minimum description length principle. In Proceedings of the Tenth National Conference on Articial
Intelligence, pp. 717{722.
Rissanen, J. (1989). Stochastic Complexity in Statistical Inquiry. World Scientic Publishing
Company.
Schalko, R. J. (1992). Pattern Recognition: Statistical, Structural and Neural Approaches.
John Wiley & Sons.
Segen, J. (1990). Graph clustering and model learning by data compression. In Proceedings
of the Seventh International Machine Learning Workshop, pp. 93{101.
Thompson, K., & Langley, P. (1991). Concept formation in structured domains. In Fisher,
D. H., & Pazzani, M. (Eds.), Concept Formation: Knowledge and Experience in Unsupervised Learning, chap. 5. Morgan Kaufmann Publishers, Inc.
Waltz, D. (1975). Understanding line drawings of scenes with shadows. In Winston, P. H.
(Ed.), The Psychology of Computer Vision. McGraw-Hill.
254

Substructure Discovery
Wertheimer, M. (1939). Laws of organization in perceptual forms. In Ellis, W. D. (Ed.), A
Sourcebook of Gestalt Psychology, pp. 331{363. Harcourt, Brace and Company.
Winston, P. H. (1975). Learning structural descriptions from examples. In Winston, P. H.
(Ed.), The Psychology of Computer Vision, pp. 157{210. McGraw-Hill.
Yoshida, K., Motoda, H., & Indurkhya, N. (1993). Unifying learning methods by colored
digraphs. In Proceedings of the Learning and Knowledge Acquisition Workshop at
IJCAI-93.
Zahn, C. T. (1971). Graph-theoretical methods for detecting and describing gestalt clusters.
IEEE Transactions on Computers, 20 (1), 68{86.

255

Journal of Articial Intelligence Research 1 (1994) 139-158

Submitted 9/93; published 1/94

Teleo-Reactive Programs for Agent Control
Nils J. Nilsson

nilsson@cs.stanford.edu

Robotics Laboratory, Department of Computer Science
Stanford University, Stanford, CA 94305 USA

Abstract

A formalism is presented for computing and organizing actions for autonomous agents
in dynamic environments. We introduce the notion of teleo-reactive (T-R) programs whose
execution entails the construction of circuitry for the continuous computation of the parameters and conditions on which agent action is based. In addition to continuous feedback,
T-R programs support parameter binding and recursion. A primary dierence between
T-R programs and many other circuit-based systems is that the circuitry of T-R programs
is more compact; it is constructed at run time and thus does not have to anticipate all
the contingencies that might arise over all possible runs. In addition, T-R programs are
intuitive and easy to write and are written in a form that is compatible with automatic
planning and learning methods. We briey describe some experimental applications of T-R
programs in the control of simulated and actual mobile robots.
1. Introduction

Autonomous agents, such as mobile robots, typically operate in dynamic and uncertain
environments. Such environments can be sensed only imperfectly, eects on them are not
always completely predictable, and they may be subject to changes not under the agent's
control. Designing agents to operate in these environments has presented challenges to the
standard methods of articial intelligence, which are based on explicit declarative representations and reasoning processes. Prominent among the alternative approaches are the
so-called behavior-based, situated, and animat methods (Brooks, 1986; Maes, 1989; Kaelbling & Rosenschein, 1990; Wilson, 1991), which convert sensory inputs into actions in a
much more direct fashion than do AI systems based on representation and reasoning. Many
of these alternative approaches share with control theory the central notion that continuous
feedback from the environment is a necessary component of eective action.
Perhaps it is relatively easier for control theorists than it is for computer scientists
to deal with continuous feedback because control theorists are accustomed to thinking of
their controlling mechanisms as composed of analog electrical circuits or other physical
systems rather than as automata with discrete read-compute-write cycles. The notions of
goal-seeking servo-mechanisms, homeostasis, feedback, ltering, and stability|so essential
to control in dynamic environments|were all developed with analog circuitry in mind.
Circuits, by their nature, are continously responsive to their inputs.
In contrast, some of the central ideas of computer science, namely sequences, events,
discrete actions, and subroutines, seem at odds with the notion of continuous feedback.
For example, in conventional programming when one program calls another, the calling
program is suspended until the called program returns control. This feature is awkward
in applications in which the called program might encounter unexpected environmental

c 1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Nilsson

circumstances with which it was not designed to cope. In such cases, the calling program
can regain control only through interrupts explicitly provided by the programmer.
To be sure, there have been attempts to blend control theory and computer science. For
example, the work of Ramadge and Wonham (Ramadge & Wonham, 1989) on discrete-event
systems has used the computer science notions of events, grammars, and discrete states to
study the control of processes for which those ideas are appropriate. A book by Dean
and Wellman (Dean & Wellman, 1991) focusses on the overlap between control theory and
articial intelligence. But there has been little eort to import fundamental control-theory
ideas into computer science. That is precisely what I set out to do in this paper.
I propose a computational system that works dierently than do conventional ones. The
formalism has what I call circuit semantics (Nilsson, 1992); program execution produces
(at least conceptually) electrical circuits, and it is these circuits that are used for control.
While importing the control-theory concept of continuous feedback, I nevertheless want to
retain useful ideas of computer science. My control programs will have parameters that can
be bound at run time and passed to subordinate routines. They can have a hierarchical
organization, and they can be recursive. In contrast with some of the behavior-based
approaches, I want the programs to be responsive to stored models of the environment as
well as to their immediate sensory inputs.
The presentation of these ideas will be somewhat informal in line with my belief that
formalization is best done after a certain amount of experience has been obtained. Although
preliminary experiments indicate that the formalism works quite well, more work remains
to be done to establish its place in agent control.
2. Teleo-Reactive Sequences
2.1 Condition-Action Rules

A teleo-reactive (T-R) sequence is an agent control program that directs the agent toward a
goal (hence teleo) in a manner that takes into account changing environmental circumstances
(hence reactive). In its simplest form, it consists of an ordered set of production rules:

K1
K2

!
!

a1
a2

111

Ki

!

ai

111

Km

!

am

The Ki are conditions (on sensory inputs and on a model of the world), and the ai
are actions (on the world or which change the model). A T-R sequence is interpreted in a
manner roughly similar to the way in which some production systems are interpreted. The
list of rules is scanned from the top for the rst rule whose condition part is satised, and
the corresponding action is executed. T-R sequences dier substantively from conventional
production systems, however. T-R actions can be durative rather than discrete. A durative
140

Teleo-Reactive Programs

action is one that continues indenitely. For example, a mobile robot is capable of executing
the durative action move, which propels the robot ahead (say at constant speed) indenitely.
Such an action contrasts with a discrete one, such as move forward one meter. In a T-R
sequence, a durative action continues so long as its corresponding condition remains the rst
true condition. When the rst true condition changes, the action changes correspondingly.
Thus, unlike production systems in computer science, the conditions must be continuously
evaluated; the action associated with the currently rst true condition is always the one
being executed. An action terminates only when its energizing condition ceases to be the
rst true condition.
Indeed, rather than thinking of T-R sequences in terms of the computer science idea of
discrete events, it is more appropriate to think of them as being implemented by circuitry.
For example, the sequence above can be implemented by the circuit shown in gure 1.
Furthermore, we imagine that the conditions, Ki , are also being continuously computed.
sensors
and
model

conditioncomputing
circuits

K1

a1


^

K2

a2



K3

^

a3

^

am



Km

Figure 1: Implementing a T-R Sequence in Circuitry
The actions, ai , of a T-R sequence can either be primitive actions, or they can be T-R
sequences themselves. Thus, programs written in this formalism can be hierarchical (even
recursive, as we shall see later). In the case of hierarchical programs, it is important to
realize that all conditions at all levels of the hierarchy are continuously being evaluated; a
high level sequence can redirect control through a dierent path of lower level sequences as
dictated by the values of the conditions at the various levels.
141

Nilsson

In writing a T-R sequence, a programmer ordinarily works backward from whatever goal
condition the sequence is being designed to achieve. The condition K1 is taken to be the
goal condition, and the corresponding action, a1, is the null action. The condition K2 is
the weakest condition such that when it is satised (and K1 is not), the durative execution
of a2 will (all other things being equal) eventually achieve K1 . And so on. Each non-null
action, ai , is supposed to achieve a condition, Kj , strictly higher in the list (j < i). The
conditions are therefore regressions (Nilsson, 1980) of higher conditions through the actions
that achieve those higher conditions.
Formally, we say that a T-R sequence satises the regression property if each condition,
 i > 1), is the regression of some higher condition in the sequence, Kj (j < i),
through the action ai . We say that a T-R sequence is complete if and only if K1 _ 1 1 1 _ Ki _
1 1 1 _ Km is a tautology. A T-R sequence is universal if it satises the regression property
and is complete. It is easy to see that a universal T-R sequence will always achieve its goal
condition, K1 , if there are no sensing or execution errors.

Ki (m

Sometimes an action does not have the eect that was anticipated by the agent's designer
(the normal eect), and sometimes exogenous events (separate from the actions of the
agent) change the world in unexpected ways. These phenomena, of course, are the reason
continuous feedback is required. Universal T-R sequences, like universal plans (Schoppers,
1987), are robust in the face of occasional deviations from normal execution. They can
also exploit serendipitous eects; it may accidentally happen that an action achieves a
condition higher in the list of condition/action rules than normally expected. Even if an
action sometimes does not achieve its normal eect (due to occasional sensing or execution
errors), nevertheless some action will be executed. So long as the environment does not
too often frustrate the achievement of the normal eects of actions, the goal condition of a
universal T-R sequence will ultimately be achieved.

2.2 An Example

The following rather simple example should make these ideas more concrete. Consider the
simulated robots in gure 2. Let's suppose that these robots can move bars around in
their two-dimensional world. The robot on the right is holding a bar, and we want the
other robot to go to and grab the bar marked A. We presume that this robot can sense its
environment and can evaluate conditions which tell it whether or not it is already grabbing
bar A (is-grabbing), facing toward bar A (facing-bar), positioned with respect to bar A
so that it can reach and grab it (at-bar-center), on the perpendicular bisector of bar A
(on-bar-midline), and facing a zone on the perpendicular bisector of bar A from which it
would be appropriate to move toward bar A (facing-midline-zone). Let's assume also that
these conditions have some appropriate amount of hysteresis so that hunting behavior is
dampened. Suppose the robot is capable of executing the primitive actions grab-bar, move,
and rotate with the obvious eects. Execution of the following T-R sequence will result in
the robot grabbing bar A:
142

bar-midline

A

bar-center

midline-zone

Figure 2: Robots and Bars
Notice how each properly executed action in this sequence achieves the condition in
the rule above it. In this way, the actions inexorably proceed toward the goal. Occasional
setbacks merely cause delays in achieving the goal so long as the actions usually1 achieve
their normal eects.
3. Teleo-Reactive Programs
3.1 Rules with Variables

We can generalize the notion of a T-R sequence by permitting the rules to contain free
variables that are bound when the sequence is \called." We will call such a sequence a T-R
program. Additional generality is obtained if we assume that the variables are not necessarily
bound to constants but to quantities whose values are continuously being computed (as if
by circuitry) as the environment changes.
A simple example involving having a robot go to a designated goal location in two
dimensions will serve to illustrate. Suppose the goal location is given by the value of the
variable loc. At run time, loc will be bound to a pair of X; Y coordinates, although we allow
the binding to change during run time. At any time during the process, the robot's X; Y
position is given by the value of the variable position. (We assume that the robot has some
kind of navigational aid that reliably and continuously computes the value of position.)
From the instantaneous values of loc and position, the robot can compute the direction that
1. We do not choose to dene usually more precisely here, although a probabilistic analysis could be given.

143

Nilsson

it should face to proceed in a straight line toward loc. Let the value of this direction at any
time be given by the value of the function course(position, loc). At any time during the
process, the robot's angular heading is given by the value of the variable heading. Using
these variables, the T-R program to drive the robot to loc is:
goto(loc)
equal(position, loc)
equal(heading, course(position, loc))
T

!
!
!

nil
move
rotate

Implementing goto(loc) in circuitry is straightforward. The single parameter of the
program is loc whose (possibly changing) value is specied at run time by a user, by a
higher level program, or by circuitry. The other (global) parameters, position and heading,
are provided by circuitry, and we assume that the function course is continuously being
computed by circuitry. Given the values of all of these parameters, computing which action
to energize is then computed by circuitry in the manner of gure 1.
3.2 Hierarchical Programs

Our formalism allows writing hierarchical and recursive programs in which the actions in
the rules are themselves T-R programs. As an example, we can write a recursive navigation
program that calls goto. Our new navigation program requires some more complex sensory
functions. Imagine a function clear-path(place1, place2) that has value T if and only if
the direct path is clear between place1 and place2. (We assume the robot can compute
this function, continuously, for place1 = position, and place2 equal to any target location.)
Also imagine a function new-point(place1, place2) that computes an intermediate position
between place1 and place2 whenever clear-path does not have value T . The value of newpoint lies appropriately to the side of the obstacle determined to be between place1 and
place2 (so that if the robot heads toward new-point rst and then toward place2, it can
navigate around the obstacle). Both clear-path and new-point are continuously computed
by perceptual systems with which we endow the robot. We'll name our new navigation
program amble(loc). Here is the code:
amble(loc)
equal(position, loc)
clear-path(position, loc)
T

!
!
!

nil
goto(loc)
amble(new-point(position, loc))

We show in gure 3 the path that a robot controlled by this program might take in
navigating around the obstacles shown. (The program doesn't necessarily compute shortest
paths; we present the program here simply as an illustration of recursion.) Note that if the
obstacle positions or goal location change during execution, these changes will be reected
in the values of the parameters used by the program, and program execution will proceed in
a manner appropriate to the changes. In particular, if a clear path ever becomes manifest
144

goal location

Figure 3: Navigating using amble
The continuous computation of parameters involved in T-R programs and the ability of
high level programs to redirect control account for the great robustness of this formalism.
A formal syntax for T-R programs is given in (Nilsson, 1992).
3.3 Implementational Issues

The T-R formalism, with its implicit assumption of continuous computation of conditions
and parameters, should be thought of as a fully legitimate \level" in the hierarchy of program
structure controlling the agent, regardless of how this level is implemented by levels below|
just as computer scientists think of list processing as a level of actual operation even though
it is implemented by more primitive logical operations below. If we assume (as we do) that
the pace of events in the agent's environment is slow compared with the amount of time
taken to perform the \continuous" computations required in a T-R program, then the T-R
programmer is justied in assuming \real" continuous sensing as s/he writes programs (even
though the underlying implentation may involve discrete sampling). We recommend the
T-R formalism only for those applications for which this assumption is justied. For those
applications, the T-R level shields the programmer from having to worry about how that
level is implemented and greatly facilitates program construction.
There are several dierent ways in which T-R programs can be interpreted into lower
level implementations. It is beyond the scope of this paper to do more than point out some
obvious methods, and we leave important questions about the properties of these methods
to subsequent research. One method of implementation involves the construction of actual
or simulated circuits according to the basic scheme of gure 1. First, the top level conditioncomputing circuits (including circuits for computing parameters used in the conditions) are
constructed and allowed to function. A specic action, say ai , is energized as a result. If ai
145

Nilsson

is primitive, it is turned on, keeping the circuitry in place and functioning until some other
top-level action is energized, and so on. If ai is a T-R sequence, the circuitry needed to
implement it is constructed (just as was done at the top level), an action is selected, and
so on|and all the while levels of circuitry above are left functioning. As new lower level
circuitry is constructed, any circuitry no longer functioning (that is, circuitry no longer
\called" by functioning higher level circuitry) can be garbage collected.
There are important questions of parameter passing and of timing in this process which
I do not deal with here|relying on the assumption that the times needed to create circuitry and for the circuitry to function are negligible compared to the pace of events in the
world. This assumption is similar to the synchrony hypothesis in the ESTEREL programming language (Berry & Gonthier, 1992) where it is assumed that a program's reaction \. . .
takes no time with respect to the external environment, which remains invariant during [the
reaction]."
Although there is no reason in principle that circuitry could not be simulated or actually
constructed (using some sort of programmable network of logic gates), it is also straightforward to implement a T-R program using more standard computational techniques. T-R
programs can be written as LISP cond statements, and durative actions can be simulated
by iterating very short action increments. For example, the increment for the move action
for a simulated robot might move the robot ahead by a small amount. After each action
increment, the top level LISP cond is executed anew, and of course all of the functions and
parameters that it contains are evaluated anew. In our simulations of robots moving in
two-dimensional worlds (to be discussed below), the computations involved are suciently
fast to eect a reasonable pace with apparent smooth motion.
This implementation method essentially involves sampling the environment at irregular
intervals. Of course, there are questions concerning how the computation times (and thus
the sampling rate) aect the real-time aspects of agent behavior which we do not address
here|again assuming the sampling rate to be very short.
Whatever method is used to interpret T-R programs, care must be taken not to conate
the T-R level with the levels below. The programmer ought not to have to think about
circuit simulators or sampling intervals but should imagine that sensing is done continuously
and immediately.
3.4 Graphical Representations

The goto program can be represented by a graph as well as by the list of rules used earlier.
The graphical representation of this program is shown in gure 4. The nodes are labeled
by conditions, and the arcs by actions. To execute the graphical version of the program, we
look for the shallowest true node (taking the goal condition as the root) and execute the
action labeling the arc leading out from that node.
In the graph of gure 4, each action normally achieves the condition at the head of its arc
(when the condition at the tail of the arc is the shallowest true condition). If there is more
than one action that can achieve a condition, we would have a tree instead of a single-path
graph. A more general graph, then, is a teleo-reactive tree such as that depicted in gure 5.
T-R trees are executed by searching for the shallowest true node and executing the action
labeling the arc leaving that node. Alternatively, we could search for that true node judged
146

Teleo-Reactive Programs

equal(position, loc)

move
equal(heading, course(position, loc))

rotate
T

Figure 4: Graphical Representation of goto
to be on a path of least cost to the goal, where some appropriate heuristic measure of cost
is used. [For simplicity, the phrase \shallowest true node" will be taken to mean either the
shallowest true node (literally) or the true node on a path of least cost to the goal.] Ties
among several equally shallow true nodes are broken according to a xed tie-breaking rule.
In gure 5 we see that, in particular, there are at least two ways to achieve condition K1 .
One way uses action a2 (when K2 is the shallowest true node), and one way uses action a3
(when K3 is the shallowest true node).
In analogy with the denitions given for T-R sequences, a T-R tree satises the regression
property if every non-root node is the regression of its parent node through the action linking
it with its parent. A T-R tree is complete if the disjunction of all of its conditions is a
tautology. A T-R tree is universal if and only if it satises the regression property and is
also complete. With a xed tie-breaking rule, a T-R tree becomes a T-R sequence. If a
T-R tree is universal, then so will be the corresponding T-R sequence.
One might at rst object to this method for executing a T-R tree on the grounds that
the sequence of actions that emerge will hop erratically from one path to another. But
if the tree satises the regression property, and if the heuristic for measuring cost to the
goal is reasonable, then (however erratic the actions may appear to be), each successfully
executed action brings the agent closer to the goal.
4. Experiments

We have carried out several preliminary experiments with agents programmed in this language (using LISP cond statements and short action increments). One set of experiments
uses simulated robots acting in a two-dimensional space, called Botworld 2, of construction
2. The original Botworld interface, including the primitive perceptual functions and actions for its robots,
was designed and implemented by Jonas Karlsson for the NeXT computer system (Karlsson, 1990). Sub-

147

Nilsson

K1
a2

a3

K2

K3

Km -1
am
Km

Figure 5: A T-R Tree
materials, structures made from these materials, and other robots. The construction materials are bars, and the robots are to build structures by connecting the bars in various ways.
A robot can turn and move, can grab and release a suitably adjacent bar, can turn and move
a grabbed bar, and can connect a bar to other bars or structures. The robots continuously
sense whether or not they are holding a bar, and they \see" in front of them (giving them
information about the location of bars and structures). Because of the existence of other
robots which may change the world in sometimes unexpected ways, it is important for each
robot to sense certain critical aspects of its environment continuously.
A typical Botworld graphical display is shown in gure 6.
We have written various T-R programs that cause the robots to build structures of
various kinds (like the triangle being constructed in gure 6). A robot controlled by one
of these programs exhibits homeostatic behavior. So long as the main goal (whatever it is)
is satised, the robot is inactive. Whenever the goal (for whatever reason) is not satised,
the robot becomes active and persists until it achieves the goal. If another agent achieves
part or all of the goal, the robot carries on appropriately from the situation it nds itself
in to complete the process.
In our experiments, the conditions used in the T-R rules are conditions on a model of
the environment that the robot constructs from its sensory system and maintains separately
from the T-R mechanism. The use of a model permits a robot to perform its actions in
response to all the sensory stimuli (past and present) that have been used to help construct
the model. But, if the T-R actions include direct changes to the model (in addition to those
sequently, Patrick Teo implemented a version that runs under X-windows on any of several dierent workstations (Teo, 1991, 1992). The latter version allows the simulation of several robots simultaneously|
each under the control of its own independently running process.

148

Teleo-Reactive Programs

Figure 6: Botworld Display
changes resulting from perceived changes to the environment), then there is a potential for
undesirable instabilities (as with any system with positive feedback). (The problem of how
to model the environment and how this model should be updated in response to sensory
data is a separate major research problem outside the scope of the work reported here.)
In other experiments, we have used the Nomadic Technologies 100 series mobile robot.
The robot is equipped with a ring of 16 infrared sensors and a ring of 16 sonar sensors.
It is controlled via a radio modem by a Macintosh II running Allegro Common Lisp. We
have implemented robust T-R programs for some simple oce-environment tasks, such as
wall-following and corridor-following (Galles, 1993). The programs were initially developed
and debugged using the Nomadics simulator of the actual robot; very few changes had to be
made in porting the programs from the simulator to the robot. In performing these tasks,
the robot is highly reactive and persistent even in the face of occasional extreme sonar or
infrared range errors and deliberate attempts to confuse it. The robot quickly adapts to
sudden changes in the environment, such as those caused by people sharing the hallways.
In writing T-R programs, one need only be concerned with inventing the appropriate
predicates using the available perceptual functions and model database. One does not need
to worry about providing interrupts of lower level programs so higher level ones can regain
control. We have found that debugging T-R programs presents some challenges, though.
Since they are designed to be quite robust in the face of environmental uncertainty, they
also sometimes work rather well even though they are not completely debugged. These
residual errors might not have undesirable eects until the programs are used in higher
level programs|making the higher ones more dicult to debug.
149

Nilsson

5. Other Approaches for Specifying Behavior

There have been several formalisms proposed for prescribing sensory-directed, real-time
activity in dynamic environments. Some of these are closely related to the T-R formalism
proposed here. In this section I point out the major similarities and dierences between T-R
programs and a representative, though not complete, sample of their closest relatives. The
other reactive formalisms are of two types, namely, those that sample their environments
at discrete intervals (perhaps rapidly enough to be suciently reactive), and those that
create circuitry (like T-R programs). The discrete-sampling systems do not abstract this
activity into a higher level in which the environment is monitored continuously, and most
of the circuitry-creating systems do so prior to run time (unlike T-R programs which create
circuitry at run time).
5.1 Discrete-Sampling Systems
5.1.1 Production Systems

As has already been mentioned, T-R programs are similar to production systems (Waterman & Hayes-Roth, 1978). The intermediate-level actions (ILAs) used in the SRI robot
Shakey (Nilsson, 1984) were programmed using production rules and were very much like
T-R programs. A T-R program also resembles a plan represented in triangle-table form
constructed by STRIPS (Fikes, Hart & Nilsson, 1972). Each of the conditions of a T-R
sequence corresponds to a triangle table kernel. In the PLANEX execution system for triangle tables, the action corresponding to the highest-numbered satised kernel is executed.
A major dierence between all of these previous production-system style programs and TR programs is that T-R programs are continuously responsive to the environment while
ordinary production systems are not.
5.1.2 Reactive Plans

Several researchers have adopted the approach of using the current situation to index into
a set of pre-arranged action sequences (George & Lansky, 1987; Schoppers, 1987; Firby,
1987). This set can either be large enough to cover a substantial number of the situations
in which an agent is likely to nd itself or it can cover all possible situations. In the latter
case, the plan set is said to be universal. Unlike T-R programs, these systems explicitly
sample their environments at discrete time steps rather than continuously. As with T-R
programs, time-space trade-os must be taken into account when considering how many
dierent conditions must be anticipated in providing reactive plans. Ginsberg has noted
that in several domains, the number of situations likely to be encountered by the agent is
so intractably large that the agent is forced to postpone most of its planning until run time
when situations are actually encountered (Ginsberg, 1989). (For further discussion of this
point, see (Selman, 1993).) T-R programs have the advantage that at least a rudimentary
form of planning, namely parameter binding, is done at run time. The PRS system (George
& Lansky, 1987) is capable of more extensive planning at run time as well as reacting
appropriately to its current situation.
150

Teleo-Reactive Programs

5.1.3 Situated Control Rules

Drummond (Drummond, 1989) introduces the notion of a plan net which is a kind of Petri
net (Reisig, 1985) for representing the eects of actions (which can be executed in parallel).
Taking into account the possible interactions of actions, he then projects the eects of all
possible actions from a present state up to some horizon. These eects are represented in
a structure called a plan projection. The plan projection is analyzed to see, for each state
in it, which states possibly have a path to the goal state. This analysis is a forward version
of the backward analysis used by a programmer in producing a T-R tree. Situated control
rules are the result of this analysis; they constrain the actions that might be taken at any
state to those which will result in a state that still possibly has a path to the goal. Plan
nets and Petri nets are based on discrete events and thus are not continuously responsive
to their environments in the way that T-R programs are.
5.2 Circuit-Based Systems

Kaelbling has proposed a formalism called GAPPS (Kaelbling, 1988; Kaelbling & Rosenschein, 1990), involving goal reduction rules, for implicitly describing how to achieve goals.
The GAPPS programmer denes the activity of an agent by providing sucient goal reduction rules to connect the agent's goals with the situations in which it might nd itself.
These rules are then compiled into circuitry for real-time control of the agent. Rosenschein
and Kaelbling (Rosenschein & Kaelbling, 1986) call such circuitry situated automata.
A collection of GAPPS rules for achieving a goal can be thought of as an implicit
specication of a T-R program in which the computations needed to construct the program
are performed when the rules are compiled. The GAPPS programmer typically exerts less
specic control over the agent's activity|leaving some of the work to the search process
performed by the GAPPS compiler. For example, a T-R program to achieve a goal, p , can
be implicitly specied by the following GAPPS rule:
(defgoalr (ach ?p)
(if ((holds ?p) (do nil))
((holds (regress ?a ?p)) (do ?a))
(T ach (regress ?a ?p)) ))

The recursion dened by this rule bottoms out in rules of the form:
(defgoalr (ach )
((holds

) (do

)) )

where  and are conditions and  is a specic action.
GAPPS compiles its rules into circuitry before run time, whereas the circuit implementation of a T-R program depends on parameters that are bound at run time. Both systems
result in control that is continuously responsive to the environment.
In implementing a system to play a video game, Chapman (Chapman, 1990) compiles
production-like rules into digital circuitry for real-time control using an approach that he
calls \arbitration macrology." As in situated automata, the compilation process occurs
prior to run time.
Brooks has developed a behavior language, BL, (Brooks, 1989), for writing reactive
robot control programs based on his \subsumption architecture" (Brooks, 1986). A similar
language, ALFA, has been implemented by Gat (Gat, 1991). Programs written in these
151

Nilsson

languages compile into structures very much like circuits. Again, compilation occurs prior
to run time. It has been relatively straightforward to translate examples of subsumptionarchitecture programs into T-R programs.
In all of these circuit-based systems, pre-run-time compiling means that more circuitry
must be built than might be needed in any given run because all possible contingencies
must be anticipated at compile time.3 But in T-R programs, parameters are bound at run
time, and only that circuitry required for these specic bindings is constructed.
6. Future Work

The T-R formalism might easily be augmented to embody some features that have not been
discussed in this paper. Explicit reference to time in specifying actions might be necessary.
For example, we might want to make sure that some action a is not initiated until after some
time t1 and ceases after some time t2. Time predicates, whose time terms are evaluated
using an internal clock, may suce for this purpose.
Also, in some applications we may want to control which conditions in a T-R program
are actually tested. It may be, for example, that some conditions won't have to be checked
because their truth or falsity can be guessed with compelling accuracy.
Simultaneous and asynchronous execution of multiple actions can be achieved by allowing the right-hand side of rules to contain sets of actions. Each member of the set is
then duratively executed asynchronously and independently (so long as the condition in the
rule that sustains this set remains the highest true condition). Of course, the programmer
must decide under what conditions it is appropriate to call for parallel actions. Future
work on related formalisms might reveal ways in which parallel actions might emerge from
the interaction of the program and its environment rather than having to be explicitly
programmed.
Although we intend that T-R programs for agent control be written by human programmers, we are also interested in methods for modifying them by automatic planning
and machine learning. We will briey discuss some of our preliminary ideas on planning
and learning here.
T-R trees resemble the search trees constructed by those planning systems that work
backwards from a goal condition. The overall goal is the root of the tree; any non-root
node gi is the regression of its parent node, gj through the action, ak , connecting them.
This similarity suggests that T-R trees can be constructed (and modied) by an automatic
planning system capable of regressing conditions through durative actions. Indeed triangle
tables (Fikes, Hart & Nilsson, 1972), a degenerate form of T-R tree consisting of only a
single path, were constructed by an automatic planning system and an EBL-style generalizer
(Mitchell, Keller & Kedar-Cabelli, 1986).
The reader might object that there is no reason to suppose that the search trees produced by an automatic planning process will contain nodes whose conditions are those that
the agent is likely to encounter in its behavior. A process of incremental modication, however, should gradually make these constructed trees more and more matched to the agent's
environment. If a tree for achieving a desired goal has no true nodes in a certain situation,
3. Agre's \running arguments" construct (Agre, 1989) is one example of a circuit-based system that can
add circuitry at run time as needed.

152

Teleo-Reactive Programs

it is as if the search process employed by an automatic planner had not yet terminated
because no subgoal in the search tree was satised in the current state. In this case, the
planning system can be called upon to continue to search; that is, the existing T-R tree will
be expanded until a true node is produced. Pruning of T-R trees can be accomplished by
keeping statistics on how often their nodes are satised. Portions of the trees that are never
or seldom used can be erased. Early unpublished work by Scott Benson indicates that T-R
programs can be eectively generated by automatic planning methods (Benson, 1993).
In considering learning mechanisms, we note rst that T-R sequences are related to a
class of Boolean functions that Rivest has termed k-decision lists (Rivest, 1987; Kohavi &
Benson, 1993). A k-decision list is an ordered list of condition-value pairs in which each
condition is a conjunction of Boolean variables of length at most k, and each value is a truth
value (T or F ). The value of the Boolean function represented by a k-decision list is that
value associated with the highest true condition. Rivest has shown that such functions are
polynomially PAC learnable and has presented a supervised learning procedure for them.
We can see that a T-R sequence whose conditions are limited to k-length conjunctions of
Boolean features is a slight generalization of k-decision lists. The only dierence is that
such a T-R sequence can have more than two dierent \values" (that is, actions). We
observe that such a T-R sequence (with, say, n dierent actions) is also PAC learnable
since its actions can be encoded with log2 n decision lists. George John (John, 1993) has
investigated a supervised learning mechanism for learning T-R sequences.
Typically, the conditions used in T-R programs are conjunctions of propositional features of the robot's world and/or model. Because a linear threshold function can implement
conjunctions, one is led to propose a neural net implementation of a T-R sequence. The neural net implementation, in turn, evokes ideas about possible learning mechanisms. Consider
the T-R sequence:

K1
K2

!
!

a1
a2

111

Ki

!

ai

111

Km

!

am

Suppose we stipulate that the Ki are linear threshold functions of a set of propositional
features. The ai are not all necessarily distinct; in fact we will assume that there are only
k  m distinct actions. Let these be denoted by b1; 1 1 1 ; bk . The network structure in gure
7 implements such a T-R sequence.
The propositional features tested by the conditions are grouped into an n-dimensional
binary (0,1) vector, X called the input vector. The m conditions are implemented by m
threshold elements having weighted connections to the components of the input vector. The
process of nding the rst true condition is implemented by a layer containing appropriate
inhibitory weights and AND units such that only one AND unit can ever have an output
value of 1, and that unit corresponds to the rst true condition. A unique action is associated
with each condition through a layer of binary-valued weights and OR-unit associators. Each
153

Nilsson

inhibitory weights
1 or 0 weights

X

K1

V

b1

K2

V

b2

...

...

bi

V

Ki

...

...

Km
input
vector

...
...
bk

V
AND
units

conditions

associators

actions

(OR
units)

Figure 7: A Neural Net that Implements a T-R Sequence
AND unit is connected to one and only one associator by a non-zero weight. Since only
one AND unit can have a non-zero output, only that unit's associator can have a non-zero
output. (But each associator could be connected to multiple AND units.) For example, if
action bi is to be associated with conditions Kj and Kl , then there will be unit weights from
the j-th and l-th AND units to the associator representing action bi and zero-valued weights
from all other AND units to that associator. The action selected for execution is the action
corresponding to the single associator having the non-zero output. We are investigating
various learning methods suggested by this neural net implementation.
Work must also be done on the question of what constitutes a goal. I have assumed
goals of achievement. Can mechanisms be found that continously avoid making certain
conditions true (or false) while attempting to achieve others? Or suppose priorities on a
number of possibly mutually contradictory conditions are specied; what are reasonable
methods for attending to those achievable goals having the highest priorities?
Also, it will be interesting to ask in what sense T-R programs can be proved to be correct.
It would seem that verication would have to make assumptions about the dynamics of the
environment; some environments might be so malevolent that agents in them could never
achieve their goals. Even so, a verier equipped with a model of the eects of actions could
at least check to see that the regression property was satised and note any lapses.
More work remains on methods of implementing or interpreting T-R programs and
the real-time properties of implementations. These properties will, of course, depend on
the depth of the T-R program hierarchy and on the conditions and features that must be
evaluated.
154

Teleo-Reactive Programs

Finally, it might be worthwhile to investigate \fuzzy" versions of T-R trees. One could
imagine fuzzy predicates that would energize actions with a \strength" that depends on
the degree to which the predicates are true. The SRI robot, Flakey, uses a fuzzy controller
(Saotti, Ruspini & Konolige, 1993).
7. Conclusions

I have presented a formalism for specifying actions in dynamic and uncertain domains. Since
this work rests on ideas somewhat dierent than those of conventional computer science, I
expect that considerably more analysis and experimentation will be required before the T-R
formalism can be fully evaluated. The need in robotics for control-theoretic ideas such as
homeostasis, continuous feedback, and stability appears to be suciently strong, however,
that it seems appropriate for candidate formalisms embodying these ideas to be put forward
for consideration.
Experiments with the language will produce a stock of advice about how to write T-R
programs eectively. Already, for example, it is apparent that a sustaining condition in a
T-R sequence must be carefully specied so that it is no more restrictive than it really needs
to be; an overly restrictive condition is likely to be rendered false by the very action that
it is supposed to sustain before that action succeeds in making a higher condition in the
sequence true. But, of course, overly restrictive conditions won't occur in T-R programs
that satisfy the regression property.
To be usefully employed, T-R programs (or any programs controlling agent action)
need to be embodied in an overall agent architecture that integrates perceptual processing,
goal selection, action computation, environmental modeling, and planning and learning
mechanisms. Several architectural schemes have been suggested, and we will not summarize
them here except to say that three layers of control are often delineated. A typical example
is the SSS architecture of Connell (Connell, 1993). His top (Symbolic) layer does overall
goal setting and sequencing, the middle (Subsumption) level selects specic actions, and
the lower (Servo) level exerts standard feedback control over the eectors. We believe T-R
programs would most appropriately be used in the middle level of such architectures.
The major limitation of T-R programs is that they involve much more computation
than do programs that check only relevant conditions. Most of the conditions computed by
a T-R program in selecting an action are either irrelevant to the situation at hand or have
values that might be accurately predicted (if the programmer wanted to take the trouble
to do so). We are essentially trading computing time for ease of programming, and our
particular trade will only be advantageous in certain applications. Among these, I think, is
the mid-level control of robots and (possibly) software agents.
In conclusion, there are three main features embodied in the T-R formalism. One is
continuous computation of the parameters and conditions on which action is based. TR programs allow for continuous feedback while still supporting parameter binding and
recursion. The second feature is the regression relationship between conditions in a T-R
program. Each condition is the regression of some condition closer to the goal through an
action that normally achieves that closer-to-the-goal condition. The regression property
assures robust goal-seeking behavior. Third, the conceptual circuitry controlling action is
constructed at run time, and this feature permits programs to be universal while still being
155

Nilsson

compact. In addition, T-R programs are intuitive and easy to write and are written in a
formalism that is compatible with automatic planning and learning methods.
Acknowledgements

I trace my interest in reactive, yet purposive, systems to my early collaborative work on
triangle tables and ILAs. Several former Stanford students, including Jonas Karlsson, Eric
Ly, Rebecca Moore, and Mark Torrance, helped in the early stages of this work. I also
want to thank my sabbatical hosts, Prof. Rodney Brooks at MIT, Prof. Barbara Grosz at
Harvard, and the people at the Santa Fe Institute. More recently, I have benetted from
discussions with Scott Benson, George John, and Ron Kohavi. I also thank the anonymous
referees for their helpful suggestions. This work was performed under NASA Grant NCC2494 and NSF Grant IRI-9116399.
References

Agre, P. (1989). The Dynamic Structure of Everyday Life. Tech. rep. TR 1085, AI Lab.,
Massachusetts Institute of Technology.
Benson, S. (1993). Unpublished working paper. Robotics Laboratory, Stanford University.
Berry, G., & Gonthier, G. (1992). The ESTEREL Synchronous Programming Language.
Science of Computer Programming, 19, no. 2, 87-152, November.
Brooks, R. (1986). A Robust Layered Control System for a Mobile Robot. IEEE Journal of
Robotics and Automation, March.
Brooks, R. (1989). The Behavior Language User's Guide. Seymour Implementation Note 2,
AI Lab., Massachusetts Institute of Technology.
Chapman, D. (1990). Vision, Instruction and Action. Tech. rep. 1204, AI Lab., Massachusetts Institute of Technology.
Connell, J. (1993). SSS: A Hybrid Architecture Applied to Robot Navigation. Research
Report, IBM Research Division, T. J. Watson Research Center, Yorktown Heights,
NY 10598.
Dean, T., & Wellman, M. (1991). Planning and Control. San Francisco, CA: Morgan Kaufmann.
Drummond, M. (1989). Situated Control Rules. In Proc. First International Conf. on Principles of Knowledge Representation and Reasoning. San Francisco, CA: Morgan Kaufmann.
Fikes, R., Hart, P., & Nilsson, N. (1972). Learning and Executing Generalized Robot Plans.
Articial Intelligence, 3, 251-288.
Firby, R. (1987). An Investigation into Reactive Planning in Complex Domains. In Proc.
AAAI-87. San Francisco, CA: Morgan Kaufmann.
156

Teleo-Reactive Programs

Galles, D. (1993). Map Building and Following Using Teleo-Reactive Trees. In Intelligent
Autonomous Systems: IAS-3, Groen, F. C. A., Hirose, S. & Thorpe, C. E. (Eds.),
390-398. Washington: IOS Press.
Gat, E. (1991). ALFA: A Language for Programming Reactive Robotic Control Systems.
In Proceedings 1991 IEEE Robotics and Automation Conference.
George, M., & Lansky, A. (1989). Reactive Reasoning and Planning. In Proc. AAAI-87.
San Francisco, CA: Morgan Kaufmann.
Ginsberg, M. L. (1989). Universal Planning: An (Almost) Universally Bad Idea. AAAI
Magazine, 10, no. 4, 40-44, Winter.
John, G. (1993). `SQUISH: A Preprocessing Method for Supervised Learning of T-R Trees
from Solution Paths, (unpublished). Robotics Laboratory, Stanford University.
Kaelbling, L. P. (1988). Goals as Parallel Program Specications. In Proceedings AAAI-88,
60-65. Menlo Park, CA: American Association for Articial Intelligence.
Kaelbling, L. P., & Rosenschein, S. J. (1990). Action and Planning in Embedded Agents.
Robotics and Autonomous Systems, 6, nos. 1 and 2, 35-48, June.
Karlsson, J. (1990). Building a Triangle Using Action Nets. Unpublished project paper.
Computer Science Dept., Stanford University. June.
Kohavi, R., & Benson, S. (1993). Research Note on Decision Lists. Machine Learning, 13,
131-134.
Maes, P. (1989). How to Do the Right Thing. Connection Science, 1, no.3, 291-323.
Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. (1986). Explanation-based Generalization: A Unifying View. Machine Learning, 1, 47-80.
Nilsson, N. J. (1980). Principles of Articial Intelligence. San Francisco, CA: Morgan Kaufmann.
Nilsson, N. (Ed.) (1984). Shakey the Robot. Tech. Note 323, Articial Intelligence Center,
SRI International, Menlo Park, CA 94025.
Nilsson, N. (1992). Toward Agent Programs with Circuit Semantics. Tech. rep. STAN-CS92-1412, Department of Computer Science, Stanford University.
Ramadge, P. J. G., & Wonham, W. M. (1989). The Control of Discrete Event Systems.
Proceedings of the IEEE, 77, no. 1, 81-98, January.
Reisig, W. (1985). Petri Nets: An Introduction, Springer Verlag.
Rivest, R. L. (1987). Learning Decision Lists. Machine Learning, 2, 229-246.
157

Nilsson

Rosenschein, S. J. & Kaelbling, L.P. (1986). The Synthesis of Machines with Provable
Epistemic Properties. In Proceedings of the 1986 Conference on Theoretical Aspects
of Reasoning about Knowledge. Halpern, J. (Ed.), 83-98, San Francisco, CA: Morgan
Kaufmann. (Updated version: Technical Note 412, Articial Intelligence Center, SRI
International, Menlo Park, CA.)
Saotti, A., Ruspini, E., & Konolige, K. (1993). Integrating Reactivity and Goaldirectedness in a Fuzzy Controller. In Proc. of the 2nd Fuzzy-IEEE Conference, San
Francisco, CA.
Schoppers, M. J. (1987). Universal Plans for Reactive Robots in Unpredictable Domains.
In Proceedings of IJCAI-87. San Francisco, CA: Morgan Kaufmann.
Selman, B. (1993). Near-Optimal Plans, Tractability, and Reactivity. Tech. rep., AI Dept.,
AT&T Bell Laboratories.
Teo, P. C-S. (1991). \Botworld," (unpublished). Robotics Laboratory, Computer Science
Dept., Stanford University, December.
Teo, P. C-S. (1992). Botworld Structures, (unpublished). Robotics Laboratory, Computer
Science Dept., Stanford University, June.
Waterman, D. A. & Hayes-Roth, F. (1978). An Overview of Pattern-Directed Inference
Systems. In Pattern-Directed Inference Systems, Waterman, D. A. & Hayes-Roth, F.
(Eds.), 3-22. New York:Academic Press.
Wilson, S. (1991). The Animat Path to AI. In From Animals to Animats; Proceedings of
the First International Conference on the Simulation of Adaptive Behavior, Meyer, J.
A., & Wilson, S. (Eds.). Cambridge, MA: The MIT Press/Bradford Books.

158

Journal of Articial Intelligence Research 1 (1994) 309-314

Submitted 1/94; published 6/94

Research Note

Applying GSAT to Non-Clausal Formulas

Roberto Sebastiani

Mechanized Reasoning Group, DIST, viale Causa 13, 16145 Genova, Italy.
Mechanized Reasoning Group, IRST, loc. Pante, 38050 Povo, Trento, Italy.

rseba@dist.unige.it

Abstract

In this paper we describe how to modify GSAT so that it can be applied to non-clausal
formulas. The idea is to use a particular \score" function which gives the number of clauses
of the CNF conversion of a formula which are false under a given truth assignment. Its
value is computed in linear time, without constructing the CNF conversion itself. The
proposed methodology applies to most of the variants of GSAT proposed so far.

1. Introduction

GSAT (Selman, Levesque, & Mitchell, 1992; Selman & Kautz, 1993) is an incomplete
model-nding algorithm for clausal propositional formulas which performs a randomized
local search. GSAT has been shown to solve many \hard" problems much more eciently
than other traditional algorithms like, e.g., DP (Davis & Putnam, 1960). Since GSAT
applies only to clausal formulas, using it to nd models for ordinary propositional formulas
requires some previous clausal-form conversion. This requires extra computation (which can
be extremely heavy if the \standard" clausal conversion is used). Much worse, clausal-form
conversion causes either a large increase in the size of the input formula or an enlargement
of the search space.
In this paper we describe how to modify GSAT so that it can be applied to non-clausal
formulas directly , i.e., with no previous clausal form conversion. An extended version of the
paper (Sebastiani, 1994) provides the proofs of the theorems and a detailed description of
the algorithm introduced.
This achievement could enlarge GSAT's application domain. Selman et al. (1992) suggest that some traditional AI problems can be formulated as model-nding tasks; e.g., visual
interpretation (Reiter & Mackworth, 1989), planning (Kautz & Selman, 1992), generation
of \vivid" knowledge representation (Levesque, 1986). It is often the case that non-clausal
representations are more compact for such problems. For instance, each rule in the form
\Vi i  " gives rise to several distinct clauses if some i are disjuncts or is a conjunct. In automated theorem proving (a.t.p.) some applications of model-nding have been
proposed (see, e.g., (Artosi & Governatori, 1994; Klingerbeck, 1994)). For instance, some
decision procedures for decidable subclasses of rst-order logic iteratively perform nonclausal model-nding for propositional instances of the input formulas (Jeroslow, 1988).
More generally, some model-guided techniques for proof search, like goal deletion (Ballantyne & Bledsoe, 1982), false preference , or semantic resolution (Slaney, 1993), seem to be
applicable to non-clausal a.t.p. as well.
c 1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Sebastiani

procedure GSAT()
for j := 1 to Max-tries do

T := initial()
for k := 1 to Max-ips do

if T j= 
then return T
else Poss-ips := hill-climb(; T )
V := pick(Poss-ips)
T := ip(V,T)
UpdateScores(; V )

end
end
return \no satisfying assignment found".
Figure 1: A general schema for GSAT.

2. GSAT

If  is a clausal propositional formula and T is a truth assignment for the variables of
, then the number of clauses of  which are falsied by T is called the score of T for 
(score(T; )). T satises  i score(T; ) = 0. The notion of score plays a key role in
GSAT, as it is considered as the \distance" from a truth assignment to a satisfying one.
The schema of Figure 2 describes GSAT as well as many of its possible variants. We use
the notation from (Gent & Walsh, 1993). GSAT performs an iterative search for a satisfying
truth assignment for , starting from a random assignment provided by initial() . At each
step, the successive assignment is obtained by ipping (inverting) the truth value of one
single variable V in T . V is chosen to minimize the score. Let Ti be the assignment obtained
from T by ipping its i-th variable Vi . hill-climb() returns the set Poss-ips of the variables
Vr which minimize score(Tr ; ). If the current values of si = score(Ti; ) , score(T; )
are stored for every variable Vi , then hill-climb() simply returns the set of the variables Vr
with the best sr . pick() chooses randomly one of such variables. ip() returns T with V 's
value ipped. After each ipping, UpdateScores() updates the values of si , for all i.
This paper exploits the observation that the functions initial() , hill-climb() , pick() and
ip() do not depend on the structure of the input formula , and that the computation
of the scores is the only step where the input formula  is required to be in clausal form.
The idea is thus to nd a suitable notion of score for non-clausal formulas, and an ecient
algorithm computing it.

3. An extended notion of score

Let cnf(') be the result of converting a propositional formula ' into clausal form by the
standard method (i.e., by applying the rules of De Morgan). Then the following denition
extends the notion of score to all propositional formulas.

Denition 3.1 The score of a truth assignment T for a propositional formula ' is the
number of the clauses of cnf(') which are falsied by T .
310

Applying GSAT to Non-Clausal Formulas

mPP
PP
PP
PP [2,-]
[1,-]
P m
m
[14,-]






m
" b
"

[7,-] 

......
... ....

... ...
......
..

.....
.... ....

.......
.... ....

# @

 A
b
................. "
#
b [2,-]

A
@
[2,-] #
... .. [4,-]"".....
b
#
@[1,-]
A

.
... -C
... .
D -E B , S
A
... [1,-]
 J [0,-] [1,-] [0,-]

 S
[1,-]
 A
E
... ..
,
J

S
SS [2,-] ...

E
[1,-]
[2,0]
[2,-]
,
... [2,-] 


J
 AA

...
S
[1,-]
[0,1]
...
..
.
-F
-B
...  A -D
 B
B
[1,-]  C ...
 A
 A [1,-]
 A [1,-]
.
...  A

B
 B

A

A

A
 C .
B
 C ... 
... 
A
A


A

 B
A
.
... -A -B C -E -F ... -D A -E C F D -A -F D -B E -C F
[0,-] [1,-] [0,-] [1,-] [0,-]
...[1,-]
. . . . . .[1,-]
. . . . .[0,-]
. . . . . .[1,-]
. . . . . .[1,-]
. . . . . ... [1 , 0][0,1][1 , 0] [0,1] [0,1] [0,-] [1,-] [1,-]

m

m

... ...
........

m

m

.
......
... ....

m

m

m

m

.... ...
.......

m

...
......
... ...

.......
... ...

.
......
... ....

.......
.... ...

m

... ..
......
..

m

........
... ...

.
......
... ....

Figure 2: The computation tree of s(T; ').
cnf() represents the \natural" clausal form conversion. cnf(') has the same number of
propositional variables as ' and it is logically equivalent to '. The problem with cnf() is
the exponential size growth of cnf(') , that is, jcnf (')j = O(2j'j). Denition 3.1 overcomes
such a problem, for it is possible to introduce a linear-time computable function s(T; ')
which gives the score of T for a formula '. This is done directly, i.e., without converting '
into clausal form. We dene s(T; ') recursively as follows: 1
'
s((T; ')
s(,(T; ')
0 if T j= '
1 if T j= '
' literal
1 otherwise
0 otherwise
:V'1
sP, (T; '1)
sQ(T; '1)
, 'k )
'
s
(
T;
'
)
Wk 'k
Q k s(T; ' k)
Pk ss,((T;
T; 'k)
k
k k
k
k
'1  '2 s, (T; '1)  s(T; '2)
s(T; '1) + s, (T; '2)
,
'2)+ (s(T; '1) + s, (T; '2))
'1  '2 ss((T;T;''1))ss,(T;
(T; ' )
(s, (T; ' ) + s(T; ' ))
1

2

1

2

s, (T; 'k) is s(T; :'k ). The distinction between s(T; 'k ) and s, (T; 'k) is due to the polarity
of the current subformula 'k . During the computation of s(T; '), a call to the function
s(T; 'j ) [s, (T; 'j )] is invoked i 'j is a positive [negative] subformula of '.
Example 3.1 Figure 2 represents the computation tree of the score of a truth assignment
T for the formula ' :
(D

((( A

^:

^:

B C ) D ( E F )) C (( D A E ) (C F )))
E B ) (((D A) ( F D B ) F ) A ((E C F ) B )):
^

:

^

_

_:

^:

_

:

_

^:

:

^

^:

^:

^

_:

:

^

^

^:

^



^:

^

^

_

_:

T assigns \true" to all the variables of '. The information in square brackets associated
to any subformula 'j represents [s(T; 'j ); s, (T; 'j )]. For instance, if we consider the small
subtree in the left of Figure 2, then the score is computed in the following way:
1. Notice that the denition of s(T; ') can be easily extended to formulas involving other connectives (e.g.,
nand , nor , xor , if-then-else , : : : ) or more complicate boolean functions.

311

Sebastiani

s(T; ( A B C ) D ( E F ) ) =
s(T; A B C ) s(T; D) s(T; E F ) =
(s(T; A) + s(T; B ) + s(T; C )) s(T; D) (s(T; E ) + s(T; F )) =
(1 + 1 + 0) 1 (1 + 1) = 4:
:

:

^:

^:

^

^

:



:



_:

_

:

:

^:



:



^:

:



:

:

W

Q

; s(T; Vk 'k ) = Pk s(T; 'k )
; s(T; k 'k ) = k s(T; 'k )
; literals



2

Notice that cnf (') is 360 clauses long.

Theorem 3.1 Let ' be a propositional formula and T a truth assignment for the variables
of '. Then the function s(T; ') gives the score of T for '.

The proof follows from the consideration that, for any truth assignment T , the set of the
false clauses of cnf('1 _ '2) is the cross product between the two sets of the false clauses
of cnf('1) and cnf('2) .
Theorem 3.2 Let ' be a propositional formula and T a truth assignment for the variables
of '. Then the number of operations required for calculating s(T; ') grows linearly with the
size of '.
The proof follows from the fact that, if Time(s ('i; T )) is the number of operations required
for computing both s(T; 'i) and s, (T; 'i), and if Time(s ('i; T ))  ai  j'ij + bi , then
Time(s ('1  '2; T ))  maxi (ai)  j'1  '2j + 2  maxi (bi) + 6, for any  2 f^; _; ;  g.
The number of operations required for computing the score of an assignment T for
a clausal formula  is O(jj). If  = cnf ('), then jj = O(2j'j). Thus the standard
computation of the score of T for  requires O(2j'j) operations, while s(T; ') performs the
same result directly in linear time.

4. GSAT for non-clausal formulas

It follows from Sections 2, 3 that we can extend GSAT to non-clausal formulas ' by simply
using the extended notion of score of Denition 3.1. Let NC-GSAT (non-clausal GSAT)
be a new version of GSAT in which the scores are computed by some implementation of
the function s() . Then it follows from Theorem 3.1 that in NC-GSAT(') the function hillclimb() always returns the same sets of variables as in GSAT(cnf(')), so that NC-GSAT(')
performs the same ips and returns the same result as GSAT(cnf(')). Theorem 3.2 ensures
that every score computation is performed in linear time.
The current implementation of GSAT (Selman & Kautz, 1993) provides a highlyoptimized implementation of Updatescores(; V ) , which analyzes only the clauses which
the last-ipped variable V occurs in. This allows a strong reduction in computational cost.
In (Sebastiani, 1994) we describe in detail an analogous optimized version of the updating
procedure for NC-GSAT, called NC-Updatescores('; V ) , and prove the following properties:
(i) if ' is in clausal form, i.e., ' = cnf ('), then NC-UpdateScores('; V ) has the same
complexity as UpdateScores('; V ) ;
(ii) if  = cnf ('), then NC-UpdateScores('; V ) is O(j'j). UpdateScores(; V ) is O(2j'j).
The latter mirrors the complexity issues presented in Section 3.
312

Applying GSAT to Non-Clausal Formulas

The idea introduced in this paper can be applied to most variants of GSAT. In \CSAT"
(Cautious SAT) hill-climb() returns all the variables which cause a decrease of the score;
in \DSAT" (Deterministic SAT) the function pick() performs a deterministic choice; in
\RSAT" (Random walk SAT) the variable is picked randomly among all the variables; in
\MSAT" (Memory SAT) pick() remembers the last ipped variable and avoids picking it.
All these variants, proposed in (Gent & Walsh, 1992, 1993), can be transposed into NCGSAT as well, as they are independent of the structure of the input formula. Selman and
Kautz (1993) suggest some variants which improve the performance and overcome some
problems, such as that of escaping local minima. The strategy \Averaging in " suggests a
dierent implementation of the function initial() : instead of a random assignment, initial()
returns a bitwise average of the best assignments of the two latest cycles. This is independent
of the form of the input formula. In the strategy \random walk " the sequence hill-climb()
- pick() is substituted with probability p by a simpler choice function: \choose randomly a
variable occurring in some unsatised clause". This idea can be transposed into NC-GSAT
as well: \choose randomly a branch passing only for nodes whose score is dierent from
zero, and pick the variable at the leaf".
One nal observation is worth making. In order to overcome the exponential growth of
CNF formulas, some algorithms have been proposed (Plaisted & Greenbaum, 1986; de la
Tour, 1990) which convert propositional formulas ' into polynomial-size clausal formulas .
Such methods are based on the introduction of new variables, each representing a subformula
of the original input '. Unfortunately, the issue of size-polynomiality is valid only if no \"
occurs in ', as the number of clauses of grows exponentially with the number of \" in
'. Even worse, the introduction of k new variables enlarges the search space by a 2k factor
and reduces strongly the solution ratio. In fact, any model for is also a model for ', but
for any model of ' we only know that one of its 2k extensions is a model of (Plaisted &
Greenbaum, 1986).

Acknowledgements
Fausto Giunchiglia and Enrico Giunchiglia have given substantial and continuous feedback
during the whole development of this paper. Toby Walsh provided important feedback
about a previous version of this paper. Aaron Noble, Paolo Pecchiari, and Luciano Serani
helped with the nal revision. Bart Selman and Henry Kautz are thanked for assistance
with the GSAT code.

References

Artosi, A., & Governatori, G. (1994). Labelled Model Modal Logic. In Proc. of CADE12
Workshop on Automated Model Building.

Ballantyne, M., & Bledsoe, W. (1982). On Generating and Using Examples in Proof Discovery. In Michie, D. (Ed.), Machines intelligence, Vol. 10, pp. 3{39. Halsted Press.
Davis, M., & Putnam, H. (1960). A computing procedure for quantication theory. Journal
of the ACM, 7, 201{215.
313

Sebastiani

de la Tour, T. B. (1990). Minimizing the Number of Clauses by Renaming. In Proc. of the
10th Conference on Automated Deduction, pp. 558{572. Springer-Verlag.
Gent, I. P., & Walsh, T. (1992). The Enigma of SAT Hill-climbing Procedures. Tech. rep.
605, University of Edinburgh, Dept. of Articial Intelligence.
Gent, I. P., & Walsh, T. (1993). Towards an Understanding of Hill-climbing Procedures for
SAT. In Proc. of the 11th National Conference on Articial Intelligence, pp. 28{33.
Jeroslow, R. (1988). Computation-Oriented Reduction of Predicate to Propositional Logic.
Decision Support System, 4, 183{197.
Kautz, H., & Selman, B. (1992). Planning as Satisability. In Proc. 10th European Conference on Articial Intelligence, pp. 359{363.
Klingerbeck, S. (1994). Generating Finite Counter Examples with Semantic Tableaux and
Interpretation Revision. In Proc. of CADE12 Workshop on Automated Model Building.

Levesque, H. (1986). Making believers out of computers. Articial Intelligence., 30, 81{108.
Plaisted, D., & Greenbaum, S. (1986). A Structure-preserving Clause Form Translation.
Journal of Symbolic Computation, 2, 293{304.
Reiter, R., & Mackworth, A. (1989). A logical framework for depiction and image interpretation. Articial Intelligence., 41 (2), 125{155.
Sebastiani, R. (1994). Applying GSAT to Non-Clausal Formulas. Tech. rep. 94-0018,
DIST, University of Genova, Italy. Available via anonimous ftp from mrg.dist.unige.it,
/pub/mrg-ftp/.
Selman, B., & Kautz, H. (1993). Domain-Independent Extension to GSAT: Solving Large
Structured Satisability Problems. In Proc. of the 13th International Joint Conference
on Articial Intelligence, pp. 290{295.
Selman, B., Levesque, H., & Mitchell, D. (1992). A New Method for Solving Hard Satisability Problems. In Proc. of the 10th National Conference on Articial Intelligence,
pp. 440{446.
Slaney, J. (1993). SCOTT: A Model-Guided Theorem Prover. In Proc. of the 13th International Joint Conference on Articial Intelligence, pp. 109{114. Morgan Kaufmann.

314

Journal of Articial Intelligence Research 1 (1993) 1-23

Submitted 5/93; published 8/93

A Market-Oriented Programming Environment and its
Application to Distributed Multicommodity Flow Problems
Michael P. Wellman

wellman@engin.umich.edu

University of Michigan, Dept. of Electrical Engineering and Computer Science,
Ann Arbor, MI 48109 USA

Abstract

Market price systems constitute a well-understood class of mechanisms that under
certain conditions provide eective decentralization of decision making with minimal communication overhead. In a market-oriented programming approach to distributed problem
solving, we derive the activities and resource allocations for a set of computational agents
by computing the competitive equilibrium of an articial economy. Walras provides basic
constructs for dening computational market structures, and protocols for deriving their
corresponding price equilibria. In a particular realization of this approach for a form of
multicommodity ow problem, we see that careful construction of the decision process according to economic principles can lead to ecient distributed resource allocation, and that
the behavior of the system can be meaningfully analyzed in economic terms.

1. Distributed Planning and Economics
In a distributed or multiagent planning system, the plan for the system as a whole is a composite of plans produced by its constituent agents. These plans may interact signicantly in
both the resources required by each of the agents' activities (preconditions) and the products resulting from these activities (postconditions). Despite these interactions, it is often
advantageous or necessary to distribute the planning process because agents are separated
geographically, have dierent information, possess distinct capabilities or authority, or have
been designed and implemented separately. In any case, because each agent has limited
competence and awareness of the decisions produced by others, some sort of coordination is
required to maximize the performance of the overall system. However, allocating resources
via central control or extensive communication is deemed infeasible, as it violates whatever
constraints dictated distribution of the planning task in the rst place.
The task facing the designer of a distributed planning system is to dene a computationally ecient coordination mechanism and its realization for a collection of agents. The
agent conguration may be given, or may itself be a design parameter. By the term agent,
I refer to a module that acts within the mechanism according to its own knowledge and
interests. The capabilities of the agents and their organization in an overall decision-making
structure determine the behavior of the system as a whole. Because it concerns the collective behavior of self-interested decision makers, the design of this decentralized structure is
fundamentally an exercise in economics or incentive engineering. The problem of developing
architectures for distributed planning ts within the framework of mechanism design (Hurwicz, 1977; Reiter, 1986), and many ideas and results from economics are directly applicable.
In particular, the class of mechanisms based on price systems and competition has been
deeply investigated by economists, who have characterized the conditions for its eciency
c 1993 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Wellman

and compatibility with other features of the economy. When applicable, the competitive
mechanism achieves coordination with minimal communication requirements (in a precise
sense related to the dimensionality of messages transmitted among agents (Reiter, 1986)).
The theory of general equilibrium (Hildenbrand & Kirman, 1976) provides the foundation for a general approach to the construction of distributed planning systems based on
price mechanisms. In this approach, we regard the constituent planning agents as consumers
and producers in an articial economy, and dene their individual activities in terms of production and consumption of commodities. Interactions among agents are cast as exchanges,
the terms of which are mediated by the underlying economic mechanism, or protocol. By
specifying the universe of commodities, the conguration of agents, and the interaction
protocol, we can achieve a variety of interesting and often eective decentralized behaviors.
Furthermore, we can apply economic theory to the analysis of alternative architectures, and
thus exploit a wealth of existing knowledge in the design of distributed planners.
I use the phrase market-oriented programming to refer to the general approach of deriving solutions to distributed resource allocation problems by computing the competitive
equilibrium of an articial economy.1 In the following, I describe this general approach
and a primitive programming environment supporting the specication of computational
markets and derivation of equilibrium prices. An example problem in distributed transportation planning demonstrates the feasibility of decentralizing a problem with nontrivial
interactions, and the applicability of economic principles to distributed problem solving.

2. WALRAS: A Market-Oriented Programming Environment
To explore the use of market mechanisms for the coordination of distributed planning modules, I have developed a prototype environment for specifying and simulating computational
markets. The system is called walras, after the 19th-century French economist Leon Walras, who was the rst to envision a system of interconnected markets in price equilibrium.
Walras provides basic mechanisms implementing various sorts of agents, auctions, and
bidding protocols. To specify a computational economy, one denes a set of goods and
instantiates a collection of agents that produce or consume those goods. Depending on the
context, some of the goods or agents may be xed exogenously, for example, they could correspond to real-world goods or agents participating in the planning process. Others might
be completely articial ones invented by the designer to decentralize the problem-solving
process in a particular way. Given a market conguration, walras then runs these agents
to determine an equilibrium allocation of goods and activities. This distribution of goods
and activities constitutes the market solution to the planning problem.
1. The name was inspired by Shoham's use of agent-oriented programming to refer to a specialization of
object-oriented programming where the entities are described in terms of agent concepts and interact
via speech acts (Shoham, 1993). Market-oriented programming is an analogous specialization, where the
entities are economic agents that interact according to market concepts of production and exchange. The
phrase has also been invoked by Lavoie, Baetjer, and Tulloh (1991) to refer to real markets in software
components.

2

Market-Oriented Programming

2.1 General Equilibrium

The walras framework is patterned directly after general-equilibrium theory. A brief exposition, glossing over many ne points, follows; for elaboration see any text on microeconomic
theory (e.g., (Varian, 1984)).
We start with k goods and n agents. Agents fall in two general classes. Consumers can
buy, sell, and consume goods, and their preferences for consuming various combinations or
bundles of goods are specied by their utility function. If agent i is a consumer, then its
utility function, u : <+ ! <, ranks the various bundles of goods according to preference.
Consumers may also start with an initial allocation of some goods, termed their endowment. Let e denote agent i's endowment of good j , and x the amount of good j that i
ultimately consumes. The objective of consumer i is to choose a feasible bundle of goods,
(x 1; : : :; x ) (rendered in vector notation as x ), so as to maximize its utility. A bundle
is feasible for consumer i if its total cost at the going prices does not exceed the value of
i's endowment at these prices. The consumer's choice can be expressed as the following
constrained optimization problem:
k

i

i;j

i;

i;j

i;k

i

max
u (x ) s.t. p  x  p  e ;
x
i

i

i

i

(1)

i

where p = (p1; : : :; p ) is the vector of prices for the k goods.
Agents of the second type, producers, can transform some sorts of goods into some
others, according to their technology. The technology species the feasible combinations of
inputs and outputs for the producer. Let us consider the special case where there is one
output good, indexed j , and the remaining goods are potential inputs. In that case, the
technology for producer i can be described by a production function,
k

y = ,x = f (x 1; : : :; x
i

i;j

i

i;

i;j

,1; x +1 ; : : :; x );
i;j

i;k

specifying the maximum output producible from the given inputs. (When a good is an
input in its own production, the production function characterizes net output.) In this
case, the producer's objective is to choose a production plan that maximizes prots subject
to its technology and the going price of its output and input goods. This involves choosing a
production level, y , along with the levels of inputs that can produce y at the minimum cost.
Let x  and p denote the consumption and prices, respectively, of the input goods. Then
the corresponding constrained optimization problem is to maximize prots, the dierence
between revenues and costs:
i

i;|

i

|







max p y , min
p  x  s.t. y  f (x ) ;
x  
yi

or equivalently,

j

i

i;|

|

i;|

i

i

min
p  x s.t. , x  f (x ):
x
i

i

i;j

i

i;|

i;|

(2)

An agent acts competitively when it takes prices as given, neglecting any impact of its
own behavior on prices. The above formulation implicitly assumes perfect competition, in
that the prices are parameters of the agents' constrained optimization problems. Perfect
competition realistically reects individual rationality when there are numerous agents, each
small with respect to the entire economy. Even when this is not the case, however, we can
3

Wellman

implement competitive behavior in individual agents if we so choose. The implications of
the restriction to perfect competition are discussed further below.
A pair (p; x) of a price vector and vector of demands for each agent constitutes a
competitive equilibrium for the economy if and only if:
1. For each agent i, x is a solution to its constrained optimization problem|(1) or
(2)|at prices p, and
i

2. the net amount of each good produced and consumed equals the total endowment,
n
X

=1

i

x =

n
X

i;j

=1

e ; for j = 1; : : :; k:
i;j

(3)

i

In other words, the total amount consumed equals the total amount produced (counted
as negative quantities in the consumption bundles of producers), plus the total amount
the economy started out with (the endowments).
Under certain \classical" assumptions (essentially continuity, monotonicity, and concavity of the utility and production functions; see, e.g., (Hildenbrand & Kirman, 1976; Varian,
1984)), competitive equilibria exist, and are unique given strictness of these conditions.
From the perspective of mechanism design, competitive equilibria possess several desirable
properties, in particular, the two fundamental welfare theorems of general equilibrium theory: (1) all competitive equilibria are Pareto optimal (no agent can do better without some
other doing worse), and (2) any feasible Pareto optimum is a competitive equilibrium for
some initial allocation of the endowments. These properties seem to oer exactly what
we need: a bound on the quality of the solution, plus the prospect that we can achieve
the most desired behavior by carefully engineering the conguration of the computational
market. Moreover, in equilibrium, the prices reect exactly the information required for
distributed agents to optimally evaluate perturbations in their behavior without resorting
to communication or reconsideration of their full set of possibilities (Koopmans, 1970).

2.2 Computing Competitive Equilibria

Competitive equilibria are also computable, and algorithms based on xed-point methods (Scarf, 1984) and optimization techniques (Nagurney, 1993) have been developed. Both
sorts of algorithms in eect operate by collecting and solving the simultaneous equilibrium equations (1), (2), and (3)). Without an expressly distributed formulation, however,
these techniques may violate the decentralization considerations underlying our distributed
problem-solving context. This is quite acceptable for the purposes these algorithms were
originally designed, namely to analyze existing decentralized structures, such as transportation industries or even entire economies (Shoven & Whalley, 1992). But because our purpose
is to implement a distributed system, we must obey computational distributivity constraints
not relevant to the usual purposes of applied general-equilibrium analysis. In general, explicitly examining the space of commodity bundle allocations in the search for equilibrium
undercuts our original motive for decomposing complex activities into consumption and
production of separate goods.
4

Market-Oriented Programming

Another important constraint is that internal details of the agents' state (such as utility
or production functions and bidding policy) should be considered private in order to maximize modularity and permit inclusion of agents not under the designers' direct control. A
consequence of this is that computationally exploiting global properties arising from special features of agents would not generally be permissible for our purposes. For example,
the constraint that prots be zero is a consequence of competitive behavior and constantreturns production technology. Since information about the form of the technology and
bidding policy is private to producer agents, it could be considered cheating to embed the
zero-prot condition into the equilibrium derivation procedure.
Walras's procedure is a decentralized relaxation method, akin to the mechanism of
tatonnement originally sketched by Leon Walras to explain how prices might be derived.
In the basic tatonnement method, we begin with an initial vector of prices, p0 . The agents
determine their demands at those prices (by solving their corresponding constrained optimization problems), and report the quantities demanded to the \auctioneer". Based on
these reports, the auctioneer iteratively adjusts the prices up or down as there is an excess
of demand or supply, respectively. For instance, an adjustment proportional to the excess
could be modeled by the dierence equation
n
X

p +1 = p + ( x ,
t

t

=1

i

i

n
X

=1

e ):
i

i

If the sequence p0 ; p1; : : : converges, then the excess demand in each market approaches zero,
and the result is a competitive equilibrium. It is well known, however, that tatonnement
processes do not converge to equilibrium in general (Scarf, 1984). The class of economies in
which tatonnement works are those with so-called stable equilibria (Hicks, 1948). A sucient
condition for stability is gross substitutability (Arrow & Hurwicz, 1977): that if the price
for one good rises, then the net demands for the other goods do not decrease. Intuitively,
gross substitutability will be violated when there are complementarities in preferences or
technologies such that reduced consumption for one good will cause reduced consumption
in others as well (Samuelson, 1974).

2.3 WALRAS Bidding Protocol

The method employed by walras successively computes an equilibrium price in each separate market, in a manner detailed below. Like tatonnement, it involves an iterative adjustment of prices based on reactions of the agents in the market. However, it diers from
traditional tatonnement procedures in that (1) agents submit supply and demand curves
rather than single point quantities for a particular price, and (2) the auction adjusts individual prices to clear, rather than adjusting the entire price vector by some increment
(usually a function of summary statistics such as excess demand).2
Walras associates an auction with each distinct good. Agents act in the market by
submitting bids to auctions. In walras, bids specify a correspondence between prices and
2. This general approach is called progressive equilibration by Dafermos and Nagurney (1989), who applied
it to a particular transportation network equilibrium problem. Although this model of market dynamics
does not appear to have been investigated very extensively in general-equilibrium theory, it does seem
to match the kind of price adjustment process envisioned by Hicks in his pioneering study of dynamics
and stability (Hicks, 1948).

5

Wellman

quantities of the good that the agent oers to demand or supply. The bid for a particular
good corresponds to one dimension of the agent's optimal demand, which is parametrized
by the prices for all relevant goods. Let x (p) be the solution to equation (1) or (2), as
appropriate, for prices p. A walras agent bids for good j under the assumption that prices
for the remaining goods are xed at their current values, p. Formally, agent i's bid for
good j is a function x : <+ ! <, from prices to quantities satisfying
i

|

i;j

x (p ) = x (p ; p) ;
i;j

j

i

j

|

j

where the subscript j on the right-hand side selects the quantity demanded of good j from
the overall demand vector. The agent computes and sends this function (encoded in any of
a variety of formats) to the auction for good j .
Given bids from all interested agents, the auction derives a market-clearing price, at
which the quantity demanded balances that supplied, within some prespecied tolerance.
This clearing price is simply the zero crossing of the aggregate demand function, which is the
sum of the demands from all agents. Such a zero crossing will exist as long as the aggregate
demand is suciently well-behaved, in particular, if it is continuous and decreasing in price.
Gross substitutability, along with the classical conditions for existence of equilibrium, is
sucient to ensure the existence of a clearing price at any stage of the bidding protocol.
Walras calculates the zero crossing of the aggregate demand function via binary search.
If aggregate demand is not well-behaved, the result of the auction may be a non-clearing
price.
When the current price is clearing with respect to the current bids, we say the market
for that commodity is in equilibrium. We say that an agent is in equilibrium if its set of
outstanding bids corresponds to the solution of its optimization problem at the going prices.
If all the agents and commodity markets are in equilibrium, the allocation of goods dictated
by the auction results is a competitive equilibrium.
Figure 1 presents a schematic view of the walras bidding process. There is an auction
for each distinct good, and for each agent, a link to all auctions in which it has an interest.
There is also a \tote board" of current prices, kept up-to-date by the various auctions. In
the current implementation the tote board is a global data structure, however, since price
change notications are explicitly transmitted to interested agents, this central information
could be easily dispensed with.
Each agent maintains an agenda of bid tasks, specifying the markets in which it must
update its bid or compute a new one. In Figure 1, agent A has pending tasks to submit
bids to auctions G1 , G7, and G4. The bidding process is highly distributed, in that each
agent need communicate directly only with the auctions for the goods of interest (those in
the domain of its utility or production function, or for which it has nonzero endowments).
Each of these interactions concerns only a single good; auctions never coordinate with each
other. Agents need not negotiate directly with other agents, nor even know of each other's
existence.
As new bids are received at auction, the previously computed clearing price becomes
obsolete. Periodically, each auction computes a new clearing price (if any new or updated
bids have been received) and posts it on the tote board. When a price is updated, this
may invalidate some of an agent's outstanding bids, since these were computed under the
assumption that prices for remaining goods were xed at previous values. On nding out
i

6

Market-Oriented Programming

G1

A1

G2

Gk

A2

Ai
Task Agenda
[1], [7], [4]

Figure 1:

Walras's bidding process. G

tote board p1
p2

}
An

}
pk

denotes the auction for the j th good, and A the
ith trading agent. An item [j ] on the task agenda denotes a pending task to
compute and submit a bid for good j .
j

i

about a price change, an agent augments its task agenda to include the potentially aected
bids.
At all times, walras maintains a vector of going prices and quantities that would be
exchanged at those prices. While the agents have nonempty bid agendas or the auctions new
bids, some or all goods may be in disequilibrium. When all auctions clear and all agendas
are exhausted, however, the economy is in competitive equilibrium (up to some numeric
tolerance). Using a recent result of Milgrom and Roberts (1991, Theorem 12), it can be
shown that the condition sucient for convergence of tatonnement|gross substitutability|
is also sucient for convergence of walras's price-adjustment process. The key observation
is that in progressive equilibration (synchronous or not) the price at each time is based on
some set of previous supply and demand bids.
Although I have no precise results to this eect, the computational eort required for
convergence to a xed tolerance seems highly sensitive to the number of goods, and much
less so to the number of agents. Eydeland and Nagurney (1989) have analyzed in detail
the convergence pattern of progressive equilibration algorithms related to walras for particular special cases, and found roughly linear growth in the number of agents. However,
general conclusions are dicult to draw as the cost of computing the equilibrium for a particular computational economy may well depend on the interconnectedness and strength of
interactions among agents and goods.

2.4 Market-Oriented Programming

As described above, walras provides facilities for specifying market congurations and
computing their competitive equilibrium. We can also view walras as a programming
environment for decentralized resource allocation procedures. The environment provides
constructs for specifying various sorts of agents and dening their interactions via their
7

Wellman

relations to common commodities. After setting up the initial conguration, the market
can be run to determine the equilibrium level of activities and distribution of resources
throughout the economy.
To cast a distributed planning problem as a market, one needs to identify (1) the goods
traded, (2) the agents trading, and (3) the agents' bidding behavior. These design steps
are serially dependent, as the denition of what constitutes an exchangeable or producible
commodity severely restricts the type of agents that it makes sense to include. And as
mentioned above, sometimes we have to take as xed some real-world agents and goods
presented as part of the problem specication. Once the conguration is determined, it
might be advantageous to adjust some general parameters of the bidding protocol. Below, I
illustrate the design task with a walras formulation of the multicommodity ow problem.

2.5 Implementation

Walras is implemented in Common Lisp and the Common Lisp Object System (CLOS).

The current version provides basic infrastructure for running computational economies,
including the underlying bidding protocol and a library of CLOS classes implementing a
variety of agent types. The object-oriented implementation supports incremental development of market congurations. In particular, new types of agents can often be dened as
slight variations on existing types, for example by modifying isolated features of the demand
behavior, bidding strategies (e.g., management of task agenda), or bid format. Wang and
Slagle (1993) present a detailed case for the use of object-oriented languages to represent
general-equilibrium models. Their proposed system is similar to walras with respect to
formulation, although it is designed as an interface to conventional model-solving packages,
rather than to support a decentralized computation of equilibrium directly.
Although it models a distributed system, walras runs serially on a single processor.
Distribution constraints on information and communication are enforced by programming
and specication conventions rather than by fundamental mechanisms of the software environment. Asynchrony is simulated by randomizing the bidding sequences so that agents
are called on unpredictably. Indeed, articial synchronization can lead to an undesirable
oscillation in the clearing prices, as agents collectively overcompensate for imbalances in
the preceding iteration.3
The current experimental system runs transportation models of the sort described below, as well as some abstract exchange and production economies with parametrized utility
and production functions (including the expository examples of Scarf (1984) and Shoven
and Whalley (1984)). Customized tuning of the basic bidding protocol has not been necessary. In the process of getting walras to run on these examples, I have added some
generically useful building blocks to the class libraries, but much more is required to ll out
a comprehensive taxonomy of agents, bidding strategies, and auction policies.
3. In some formal dynamic models (Huberman, 1988; Kephart, Hogg, & Huberman, 1989), homogeneous
agents choose instantaneously optimal policies without accounting for others that are simultaneously
making the same choice. Since the value of a particular choice varies inversely with the number of agents
choosing it, this delayed feedback about the others' decisions leads to systematic errors, and hence
oscillation. I have also observed this phenomenon empirically in a synchronized version of WALRAS.
By eliminating the synchronization, agents tend to work on dierent markets at any one time, and hence
do not suer as much from delayed feedback about prices.

8

Market-Oriented Programming

3. Example: Multicommodity Flow
In a simple version of the multicommodity ow problem, the task is to allocate a given
set of cargo movements over a given transportation network. The transportation network
is a collection of locations, with links (directed edges) identifying feasible transportation
operations. Associated with each link is a specication of the cost of moving cargo along it.
We suppose further that the cargo is homogeneous, and that amounts of cargo are arbitrarily
divisible. A movement requirement associates an amount of cargo with an origin-destination
pair. The planning problem is to determine the amount to transport on each link in order to
move all the cargo at the minimum cost. This simplication ignores salient aspects of real
transportation planning. For instance, this model is completely atemporal, and is hence
more suitable for planning steady-state ows than for planning dynamic movements.
A distributed version of the problem would decentralize the responsibility for transporting separate cargo elements. For example, planning modules corresponding to geographically or organizationally disparate units might arrange the transportation for cargo
within their respective spheres of authority. Or decision-making activity might be decomposed along hierarchical levels of abstraction, gross functional characteristics, or according
to any other relevant distinction. This decentralization might result from real distribution
of authority within a human organization, from inherent informational asymmetries and
communication barriers, or from modularity imposed to facilitate software engineering.
Consider, for example, the abstract transportation network of Figure 2, taken from
Harker (1988). There are four locations, with directed links as shown. Consider two movement requirements. The rst is to transport cargo from location 1 to location 4, and the
second in the reverse direction. Suppose we wish to decentralize authority so that separate
agents (called shippers) decide how to allocate the cargo for each movement. The rst shipper decides how to split its cargo units between the paths 1 ! 2 ! 4 and 1 ! 2 ! 3 ! 4,
while the second gures the split between paths 4 ! 2 ! 1 and 4 ! 2 ! 3 ! 1. Note that
the latter paths for each shipper share a common resource: the link 2 ! 3.
2

4

1

3

Figure 2: A simple network (from Harker (1988)).
Because of their overlapping resource demands, the shippers' decisions appear to be
necessarily intertwined. In a congested network, for example, the cost for transporting a
unit of cargo over a link is increasing in the overall usage of the link. A shipper planning
its cargo movements as if it were the only user on a network would thus underestimate its
costs and potentially misallocate transportation resources.
9

Wellman

For the analysis of networks such as this, transportation researchers have developed
equilibrium concepts describing the collective behavior of the shippers. In a system equilibrium, the overall transportation of cargo proceeds as if there were an omniscient central
planner directing the movement of each shipment so as to minimize the total aggregate
cost of meeting the requirements. In a user equilibrium, the overall allocation of cargo
movements is such that each shipper minimizes its own total cost, sharing proportionately
the cost of shared resources. The system equilibrium is thus a global optimum, while the
user equilibrium corresponds to a composition of locally optimal solutions to subproblems.
There are also some intermediate possibilities, corresponding to game-theoretic equilibrium
concepts such as the Nash equilibrium, where each shipper behaves optimally given the
transportation policies of the remaining shippers (Harker, 1986).4
From our perspective as designer of the distributed planner, we seek a decentralization
mechanism that will reach the system equilibrium, or come as close as possible given the
distributed decision-making structure. In general, however, we cannot expect to derive a
system equilibrium or globally optimal solution without central control. Limits on coordination and communication may prevent the distributed resource allocation from exploiting
all opportunities and inhibiting agents from acting at cross purposes. But under certain
conditions decision making can indeed be decentralized eectively via market mechanisms.
General-equilibrium analysis can help us to recognize and take advantage of these opportunities.
Note that for the multicommodity ow problem, there is an eective distributed solution
due to Gallager (1977). One of the market structures described below eectively mimics this
solution, even though Gallager's algorithm was not formulated expressly in market terms.
The point here is not to crack a hitherto unsolved distributed optimization problem (though
that would be nice), but rather to illustrate a general approach on a simply described yet
nontrivial task.

4. WALRAS Transportation Market

In this section, I present a series of three transportation market structures implemented in
walras. The rst and simplest model comprises the basic transportation goods and shipper
agents, which are augmented in the succeeding models to include other agent types. Comparative analysis of the three market structures reveals the qualitatively distinct economic
and computational behaviors realized by alternate walras congurations.

4.1 Basic Shipper Model

The resource of primary interest in the multicommodity ow problem is movement of cargo.
Because the value and cost of a cargo movement depends on location, we designate as a
distinct good the capacity on each origin-destination pair in the network (see Figure 2). To
capture the cost or input required to move cargo, we dene another good denoting generic
transportation resources. In a more concrete model, these might consist of vehicles, fuel,
labor, or other factors contributing to transportation.
4. In the Nash solution, shippers correctly anticipate the eect of their own cargo movements on the average
cost on each link. The resulting equilibrium converges to the user equilibrium as the number of shippers
increases and the eect of any individual's behavior on prices diminishes (Haurie & Marcotte, 1985).

10

Market-Oriented Programming

To decentralize the decision making, we identify each movement requirement with a
distinct shipper agent. These shippers, or consumers, have an interest in moving various
units of cargo between specied origins and destinations.
The interconnectedness of agents and goods denes the market conguration. Figure 3
depicts the walras conguration for the basic shipper model corresponding to the example
network of Figure 2. In this model there are two shippers, S1 4 and S4 1, where S denotes
a shipper with a requirement to move goods from origin i to destination j . Shippers connect
to goods that might serve their objectives: in this case, movement along links that belong to
some simple path from the shipper's origin to its destination. In the diagram, G denotes
the good representing an amount of cargo moved over the link i ! j . G0 denotes the special
transportation resource good. Notice that the only goods of interest to both shippers are
G0, for which they both have endowments, and G2 3, transportation on the link serving
both origin-destination pairs.
;

;

i;j

i;j

;

G

G 2,3

G 2,4

S4,1

S1,4

G0

G 1,2

Figure 3:

G 3,1

3,4

G

G

2,1

4,2

Walras basic shipper market conguration for the example transportation network.

The model we employ for transportation costs is based on a network with congestion,
thus exhibiting diseconomies of scale. In other words, the marginal and average costs (in
terms of transportation resources required) are both increasing in the level of service on a
link. Using Harker's data, we take costs to be quadratic. The quadratic cost model is posed
simply for concreteness, and does not represent any substantive claim about transportation
networks. The important qualitative feature of this model (and the only one necessary
for the example to work) is that it exhibits decreasing returns, a dening characteristic of
congested networks. Note also that Harker's model is in terms of monetary costs, whereas
we introduce an abstract input good.
Let c (x) denote the cost in transportation resources (good G0 ) required to transport
x units of cargo on the link from i to j . The complete cost functions are:
c1 2(x) = c2 1(x) = c2 4(x) = c4 2(x) = x2 + 20x;
c3 1(x) = c2 3(x) = c3 4(x) = 2x2 + 5x:
Finally, each shipper's objective is to transport 10 units of cargo from its origin to its
destination.
i;j

;

;

;

;

;

;

;

11

Wellman

In the basic shipper model, we assume that the shippers pay proportionately (in units
of G0 ) for the total cost on each link. This amounts to a policy of average cost pricing.
We take the shipper's objective to be to ship as much as possible (up to its movement
requirement) in the least costly manner. Notice that this objective is not expressible in
terms of the consumer's optimization problem, equation (1), and hence this model is not
technically an instance of the general-equilibrium framework.
Given a network with prices on each link, the cheapest cargo movement corresponds to
the shortest path in the graph, where distances are equated with prices. Thus, for a given
link, a shipper would prefer to ship its entire quota on the link if it is on the shortest path,
and zero otherwise. In the case of ties, it is indierent among the possible allocations. To
bid on link i; j , the shipper can derive the threshold price that determines whether the link
is on a shortest path by taking the dierence in shortest-path distance between the networks
where link i; j 's distance is set to zero and innity, respectively.
In incrementally changing its bids, the shipper should also consider its outstanding bids
and the current prices. The value of reserving capacity on a particular link is zero if it
cannot get service on the other links on the path. Similarly, if it is already committed to
shipping cargo on a parallel path, it does not gain by obtaining more capacity (even at a
lower price) until it withdraws these other bids.5 Therefore, the actual demand policy of
a shipper is to spend its uncommitted income on the potential ow increase (derived from
maximum-ow calculations) it could obtain by purchasing capacity on the given link. It is
willing to spend up to the threshold value of the link, as described above. This determines
one point on its demand curve. If it has some unsatised requirement and uncommitted
income it also indicates a willingness to pay a lower price for a greater amount of capacity.
Boundary points such as this serve to bootstrap the economy; from the initial conditions it
is typically the case that no individual link contributes to overall ow between the shipper's
origin and destination. Finally, the demand curve is completed by a smoothing operation
on these points.
Details of the boundary points and smoothing operation are rather arbitrary, and I
make no claim that this particular bidding policy is ideal or guaranteed to work for a broad
class of problems. This crude approach appears sucient for the present example and some
similar ones, as long as the shippers' policies become more accurate as the prices approach
equilibrium.
Walras successfully computes the competitive equilibrium for this example, which
in the case of the basic shipper model corresponds to a user equilibrium (UE) for the
transportation network. In the UE for the example network, each shipper sends 2.86 units
of cargo over the shared link 2 ! 3, and the remaining cargo over the direct link from
location 2 to the destination. This allocation is inecient, as its total cost is 1143 resource
5. Even if a shipper could simultaneously update its bids in all markets, it would not be a good idea to do
so here. A competitive shipper would send all its cargo on the least costly path, neglecting the possibility
that this demand may increase the prices so that it is no longer cheapest. The outstanding bids provide
some sensitivity to this eect, as they are functions of price. But they cannot respond to changes in
many prices at once, and thus the policy of updating all bids simultaneously can lead to perpetual
oscillation. For example, in the network considered here, the unique competitive equilibrium has each
shipper splitting its cargo between two dierent paths. Policies allocating all cargo to one path can never
lead to this result, and hence convergence to competitive equilibrium depends on the incrementality of
bidding behavior.

12

Market-Oriented Programming

units, which is somewhat greater than the global minimum-cost solution of 1136 units. In
economic terms, the cause of the ineciency is an externality with respect to usage of the
shared link. Because the shippers are eectively charged average cost|which in the case
of decreasing returns is below marginal cost|the price they face does not reect the full
incremental social cost of additional usage of the resource. In eect, incremental usage of
the resource by one agent is subsidized by the other. The steeper the decreasing returns,
the more the agents have an incentive to overutilize the resource.6 This is a simple example
of the classic tragedy of the commons.
The classical remedy to such problems is to internalize the externality by allocating
ownership of the shared resource to some decision maker who has the proper incentives to
use it eciently. We can implement such a solution in walras by augmenting the market
structure with another type of agent.

4.2 Carrier Agents
We extend the basic shipper model by introducing carriers, agents of type producer who
have the capability to transport cargo units over specied links, given varying amounts
of transportation resources. In the model described here, we associate one carrier with
each available link. The production function for each carrier is simply the inverse of the
cost function described above. To achieve a global movement of cargo, shippers obtain
transportation services from carriers in exchange for the necessary transportation resources.
Let C denote the carrier that transports cargo from location i to location j . Each
carrier C is connected to the auction for G , its output good, along with G0|its input
in the production process. Shipper agents are also connected to G0 , as they are endowed
with transportation resources to exchange for transportation services. Figure 4 depicts the
walras market structure when carriers are included in the economy.
i;j

i;j

i;j

G

C 2,4

Figure 4:

3,4

C 3,4

G 2,4

S1,4

G 1,2

C 1,2

G 2,3

C 2,3

G0

C 3,1

G 3,1

S4,1

G

C

G

4,2

2,1

C 2,1

4,2

Walras market conguration for the example transportation network in an economy with shippers and carriers.

6. Average-cost pricing is perhaps the most common mechanism for allocating costs of a shared resource.
Shenker (1991) points out problems with this scheme|with respect to both eciency and strategic
behavior|in the context of allocating access to congested computer networks, a problem analogous to
our transportation task.

13

Wellman

In the case of a decreasing returns technology, the producer's (carrier's) optimization
problem has a unique solution. The optimal level of activity maximizes revenues minus costs,
which occurs at the point where the output price equals marginal cost. Using this result,
carriers submit supply bids specifying transportation services as a function of link prices
(with resource price xed), and demand bids specifying required resources as a function of
input prices (for activity level computed with output price xed).
For example, consider carrier C1 2. At output price p1 2 and input price p0 , the carrier's
prot is
p1 2y , p0c1 2(y);
where y is the level of service it chooses to supply. Given the cost function above, this
expression is maximized at y = (p1 2 , 20p0)=2p0. Taking p0 as xed, the carrier submits a
supply bid with y a function of p1 2. On the demand side, the carrier takes p1 2 as xed and
submits a demand bid for enough good G0 to produce y , where y is treated as a function
of p0.
With the revised conguration and agent behaviors described, walras derives the system equilibrium (SE), that is, the cargo allocation minimizing overall transportation costs.
The derived cargo movements are correct to within 10% in 36 bidding cycles, and to 1%
in 72, where in each cycle every agent submits an average of one bid to one auction. The
total cost (in units of G0 ), its division between shippers' expenditures and carriers' prots,
and the equilibrium prices are presented in Table 1. Data for the UE solution of the basic shipper model are included for comparison. That the decentralized process produces a
global optimum is perfectly consistent with competitive behavior|the carriers price their
outputs at marginal cost, and the technologies are convex.
;

;

;

;

;

;

;

TC expense prot p1 2 p2 1 p2 3 p2 4 p3 1 p3 4 p4 2
pricing
MC (SE) 1136
1514 378 40.0 35.7 22.1 35.7 13.6 13.6 40.0
1143
0 30.0 27.1 16.3 27.1 10.7 10.7 30.0
AC (UE) 1143
;

;

;

;

;

;

;

Table 1: Equilibria derived by walras for the transportation example. TC, MC, and AC
stand for total, marginal, and average cost, respectively. TC = shipper expense ,
carrier prot.
As a simple check on the prices of Table 1, we can verify that p2 3 + p3 4 = p2 4 and
p2 3 + p3 1 = p2 1. Both these relationships must hold in equilibrium (assuming all links have
nonzero movements), else a shipper could reduce its cost by rerouting some cargo. Indeed,
for a simple (small and symmetric) example such as this, it is easy to derive the equilibrium
analytically using global equations such as these. But as argued above, it would be improper
to exploit these relationships in the implementation of a truly distributed decision process.
The lesson from this exercise is that we can achieve qualitatively distinct results by simple variations in the market conguration or agent policies. From our designers' perspective,
we prefer the conguration that leads to the more transportation-ecient SE. Examination
of Table 1 reveals that we can achieve this result by allowing the carriers to earn nonzero
prots (economically speaking, these are really rents on the xed factor represented by the
;

;

;

;

14

;

;

Market-Oriented Programming

congested channel) and redistributing these prots to the shippers to cover their increased
expenditures. (In the model of general equilibrium with production, consumers own shares
in the producers' prots. This closes the loop so that all value is ultimately realized in
consumption. We can specify these shares as part of the initial conguration, just like the
endowment.) In this example, we distribute the prots evenly between the two shippers.

4.3 Arbitrageur Agents

The preceding results demonstrate that walras can indeed implement a decentralized
solution to the multicommodity ow problem. But the market structure in Figure 4 is not
as distributed as it might be, in that (1) all agents are connected to G0, and (2) shippers
need to know about all links potentially serving their origin-destination pair. The rst of
these concerns is easily remedied, as the choice of a single transportation resource good was
completely arbitrary. For example, it would be straightforward to consider some collection
of resources (e.g., fuel, labor, vehicles), and endow each shipper with only subsets of these.
The second concern can also be addressed within walras. To do so, we introduce yet
another sort of producer agent. These new agents, called arbitrageurs, act as specialized
middlemen, monitoring isolated pieces of the network for ineciencies. An arbitrageur
A produces transportation from i to k by buying capacity from i to j and j to k. Its
production function simply species that the amount of its output good, G , is equal to
the minimum of its two inputs, G and G . If p + p < p , then its production
is protable. Its bidding policy in walras is to increment its level of activity at each
iteration by an amount proportional to its current protability (or decrement proportional
to the loss). Such incremental behavior is necessary for all constant-returns producers in
walras, as the prot maximization problem has no interior solution in the linear case.7
To incorporate arbitrageurs into the transportation market structure, we rst create new
goods corresponding to the transitive closure of the transportation network. In the example
network, this leads to goods for every location pair. Next, we add an arbitrageur A for
every triple of locations such that (1) i ! j is in the original network, and (2) there exists a
path from j to k that does not traverse location i. These two conditions ensure that there
is an arbitrageur A for every pair i; k connected by a path with more than one link, and
eliminate some combinations that are either redundant or clearly unprotable.
The revised market structure for the running example is depicted in Figure 5, with new
goods and agents shaded. Some goods and agents that are inactive in the market solution
have been omitted from the diagram to avoid clutter.
Notice that in Figure 5 the connectivity of the shippers has been signicantly decreased,
as the shippers now need be aware of only the good directly serving their origin-destination
pair. This dramatically simplies their bidding problem, as they can avoid all analysis of the
price network. The structure as a whole seems more distributed, as no agent is concerned
with more than three goods.
i;j;k

i;k

i;j

j;k

i;j

j;k

i;k

i;j;k

i;j;k

7. Without such a restriction on its bidding behavior, the competitive constant-returns producer would
choose to operate at a level of innity or zero, depending on whether its activity were protable or
unprotable at the going prices (at break-even, the producer is indierent among all levels). This
would lead to perpetual oscillation, a problem noticed (and solved) by Paul Samuelson in 1949 when he
considered the use of market mechanisms to solve linear programming problems (Samuelson, 1966).

15

Wellman

A
G 2,4

A2,3,1

2,3,4

G 2,3
C 2,4

A1,2,4

G

3,4

C 3,4

G1,4

G 1,2

C 1,2

C 2,1
C 2,3

G0

S1,4

G2,1

C 3,1

G3,1

A 4,2,1

C 4,2

G 4,2

G4,1

S4,1

Figure 5: The revised walras market conguration with arbitrageurs.
Despite the simplied shipper behavior, walras still converges to the SE, or optimal
solution, in this conguration. Although the resulting allocation of resources is identical,
a qualitative change in market structure here corresponds to a qualitative change in the
degree of decentralization.
In fact, the behavior of walras on the market conguration with arbitrageurs is virtually identical to a standard distributed algorithm (Gallager, 1977) for multicommodity
ow (minimum delay on communication networks). In Gallager's algorithm, distributed
modules expressly dierentiate the cost function to derive the marginal cost of increasing
ow on a communication link. Flows are adjusted up or down so to equate the marginal
costs along competing subpaths. This procedure provably converges to the optimal solution
as long as the iterative adjustment parameter is suciently small. Similarly, convergence
in walras for this model requires that the arbitrageurs do not adjust their activity levels
too quickly in response to prot opportunities or loss situations.

4.4 Summary

The preceding sections have developed three progressively elaborate market congurations
for the multicommodity ow problem. Table 2 summarizes the size and shape of the conguration for a transportation network with V locations and E links, and M movement
requirements. The basic shipper model results in the user equilibrium, while both of the
augmented models produce the globally optimal system equilibrium. The carrier model requires E new producer agents to produce the superior result. The arbitrageur model adds
O(V E ) more producers and potentially some new goods as well, but reduces the number of
goods of interest to any individual agent from O(E ) to a small constant.
These market models represent three qualitatively distinct points on the spectrum of
potential congurations. Hybrid models are also conceivable, for example, where a partial
set of arbitrageurs are included, perhaps arranged in a hierarchy or some other regular
16

Market-Oriented Programming

model
Basic shipper
: : : plus carriers
: : : plus arbitrageurs

goods

shippers

E + 1 M [O(E )]
E + 1 M [O(E )]
O(V 2) M [2]

carriers arbitrageurs
|
|
E [2]
|
E [2] O(V E ) [3]

Table 2: Numbers of goods and agents for the three market congurations. For each type of
agent, the gure in brackets indicates the number of goods on which each individual
bids.
structure. I would expect such congurations to exhibit behaviors intermediate to the
specic models studied here, with respect to both equilibrium produced and degree of
decentralization.

5. Limitations

One serious limitation of walras is the assumption that agents act competitively. As
mentioned above, this behavior is rational when there are many agents, each small with
respect to the overall economy. However, when an individual agent is large enough to aect
prices signicantly (i.e., possesses market power), it forfeits utility or prots by failing to
take this into account. There are two approaches toward alleviating the restriction of perfect
competition in a computational economy. First, we could simply adopt models of imperfect
competition, perhaps based on specic forms of imperfection (e.g., spatial monopolistic
competition) or on general game-theoretic models. Second, as architects we can congure
the markets to promote competitive behavior. For example, decreasing the agent's grain size
and enabling free entry of agents should enhance the degree of competition. Perhaps most
interestingly, by controlling the agents' knowledge of the market structure (via standard
information-encapsulation techniques), we can degrade their ability to exploit whatever
market power they possess. Uncertainty has been shown to increase competitiveness among
risk-averse agents in some formal bidding models (McAfee & McMillan, 1987), and in a
computational environment we have substantial control over this uncertainty.
The existence of competitive equilibria and ecient market allocations also depends
critically on the assumption of nonincreasing returns to scale. Although congestion is a
real factor in transportation networks, for example, for many modes of transport there
are often other economies of scale and density that may lead to returns that are increasing
overall (Harker, 1987). Note that strategic interactions, increasing returns, and other factors
degrading the eectiveness of market mechanisms also inhibit decentralization in general,
and so would need to be addressed directly in any approach.
Having cast walras as a general environment for distributed planning, it is natural to
ask how universal \market-oriented programming" is as a computational paradigm. We can
characterize the computational power of this model easily enough, by correspondence to the
class of convex programming problems represented by economies satisfying the classical conditions. However, the more interesting issue is how well the conceptual framework of market
17

Wellman

equilibrium corresponds to the salient features of distributed planning problems. Although
it is too early to make a denitive assertion about this, it seems clear that many planning
tasks are fundamentally problems in resource allocation, and that the units of distribution
often correspond well with units of agency. Economics has been the most prominent (and
arguably the most successful) approach to modeling resource allocation with decentralized
decision making, and it is reasonable to suppose that the concepts economists nd useful
in the social context will prove similarly useful in our analogous computational context.
Of course, just as economics is not ideal for analyzing all aspects of social interaction, we
should expect that many issues in the organization of distributed planning will not be well
accounted-for in this framework.
Finally, the transportation network model presented here is a highly simplied version of the actual planning problem for this domain. A more realistic treatment would
cover multiple commodity types, discrete movements, temporal extent, hierarchical network structure, and other critical features of the problem. Some of these may be captured
by incremental extensions to the simple model, perhaps applying elaborations developed
by the transportation science community. For example, many transportation models (including Harker's more elaborate formulation (Harker, 1987)) allow for variable supply and
demand of the commodities and more complex shipper-carrier relationships. Concepts of
spatial price equilibrium, based on markets for commodities in each location, seem to oer
the most direct approach toward extending the transportation model within walras.

6. Related Work
6.1 Distributed Optimization
The techniques and models described here obviously build on much work in economics,
transportation science, and operations research. The intended research contribution here is
not to these elds, but rather in their application to the construction of a computational
framework for decentralized decision making in general. Nevertheless, a few words are in
order regarding the relation of the approach described here to extant methods for distributed
optimization.
Although the most elaborate walras model is essentially equivalent to existing algorithms for distributed multicommodity ow (Bertsekas & Tsitsiklis, 1989; Gallager, 1977),
the market framework oers an approach toward extensions beyond the strict scope of this
particular optimization problem. For example, we could reduce the number of arbitrageurs,
and while this would eliminate the guarantees of optimality, we might still have a reasonable
expectation for graceful degradation. Similarly, we could realize conceptual extensions to
the structure of the problem, such as distributed production of goods in addition to transportation, by adding new types of agents. For any given extension, there may very well be
a customized distributed optimization algorithm that would outperform the computational
market, but coming up with this algorithm would likely involve a completely new analysis.
Nevertheless, it must be stated that speculations regarding the methodological advantages
of the market-oriented framework are indeed just speculations at this point, and the relative
exibility of applications programming in this paradigm must ultimately be demonstrated
empirically.
18

Market-Oriented Programming

Finally, there is a large literature on decomposition methods for mathematical programming problems, which is perhaps the most common approach to distributed optimization.
Many of these techniques can themselves be interpreted in economic terms, using the close
relationship between prices and Lagrange multipliers. Again, the main distinction of the
approach advocated here is conceptual. Rather than taking a global optimization problem and decentralizing it, our aim is to provide a framework for formulating a task in a
distributed manner in the rst place.

6.2 Market-Based Computation
The basic idea of applying economic mechanisms to coordinate distributed problem solving
is not new to the AI community. Starting with the contract net (Davis & Smith, 1983),
many have found the metaphor of markets appealing, and have built systems organized
around markets or market-like mechanisms (Malone, Fikes, Grant, & Howard, 1988). The
original contract net actually did not include any economic notions at all in its bidding
mechanism, however, recent work by Sandholm (1993) has shown how cost and price can
be incorporated in the contract net protocol to make it more like a true market mechanism. Miller and Drexler (Drexler & Miller, 1988; Miller & Drexler, 1988) have examined
the market-based approach in depth, presenting some underlying rationale and addressing
specic issues salient in a computational environment. Waldspurger, Hogg, Huberman,
Kephart, and Stornetta (1992) investigated the concepts further by actually implementing
market mechanisms to allocate computational resources in a distributed operating system.
Researchers in distributed computing (Kurose & Simha, 1989) have also applied specialized
algorithms based on economic analyses to specic resource-allocation problems arising in
distributed systems. For further remarks on this line of work, see (Wellman, 1991).
Recently, Kuwabara and Ishida (1992) have experimented with demand adjustment
methods for a task very similar to the multicommodity ow problem considered here. One
signicant dierence is that their method would consider each path in the network as a
separate resource, whereas the market structures here manipulate only links or location
pairs. Although they do not cast their system in a competitive-equilibrium framework, the
results are congruent with those obtained by walras.
Walras is distinct from these prior eorts in two primary respects. First, it is constructed expressly in terms of concepts from general equilibrium theory, to promote mathematical analysis of the system and facilitate the application of economic principles to
architectural design. Second, walras is designed to serve as a general programming environment for implementing computational economies. Although not developed specically
to allocate computational resources, there is no reason these could not be included in market structures congured for particular application domains. Indeed, the idea of grounding
measures of the value of computation in real-world values (e.g., cargo movements) follows
naturally from the general-equilibrium view of interconnected markets, and is one of the
more exciting prospects for future applications of walras to distributed problem-solving.
Organizational theorists have studied markets as mechanisms for coordinating activities
and allocating resources within rms. For example, Malone (1987) models information
requirements, exibility and other performance characteristics of a variety of market and
non-market structures. In his terminology, walras implements a centralized market, where
19

Wellman

the allocation of each good is mediated by an auction. Using such models, we can determine
whether this gross form of organization is advantageous, given information about the cost
of communication, the exibility of individual modules, and other related features. In this
paper, we examine in greater detail the coordination process in computational markets,
elaborating on the criteria for designing decentralized allocation mechanisms. We take the
distributivity constraint as exogenously imposed; when the constraint is relaxable, both
organizational and economic analysis illuminate the tradeos underlying the mechanism
design problem.
Finally, market-oriented programming shares with Shoham's agent-oriented programming (Shoham, 1993) the view that distributed problem-solving modules are best designed
and understood as rational agents. The two approaches support dierent agent operations
(transactions versus speech acts), adopt dierent rationality criteria, and emphasize different agent descriptors, but are ultimately aimed at achieving the same goal of specifying
complex behavior in terms of agent concepts (e.g., belief, desire, capability) and social organizations. Combining individual rationality with laws of social interaction provides perhaps
the most natural approach to generalizing Newell's \knowledge level analysis" idea (Newell,
1982) to distributed computation.

7. Conclusion

In summary, walras represents a general approach to the construction and analysis of
distributed planning systems, based on general equilibrium theory and competitive mechanisms. The approach works by deriving the competitive equilibrium corresponding to a
particular conguration of agents and commodities, specied using walras's basic constructs for dening computational market structures. In a particular realization of this
approach for a simplied form of distributed transportation planning, we see that qualitative dierences in economic structure (e.g., cost-sharing among shippers versus ownership
of shared resources by prot-maximizing carriers) correspond to qualitatively distinct behaviors (user versus system equilibrium). This exercise demonstrates that careful design of
the distributed decision structure according to economic principles can sometimes lead to
eective decentralization, and that the behaviors of alternative systems can be meaningfully
analyzed in economic terms.
The contribution of the work reported here lies in the idea of market-oriented programming, an algorithm for distributed computation of competitive equilibria of computational
economies, and an initial illustration of the approach on a simple problem in distributed
resource allocation. A great deal of additional work will be required to understand the precise capabilities and limitations of the approach, and to establish a broader methodology
for conguration of computational economies.

Acknowledgements
This paper is a revised and extended version of (Wellman, 1992). I have beneted from
discussions of computational economies with many colleagues, and would like to thank in
particular Jon Doyle, Ed Durfee, Eli Gafni, Daphne Koller, Tracy Mullen, Anna Nagurney,
20

Market-Oriented Programming

Scott Shenker, Yoav Shoham, Hal Varian, Carl Waldspurger, Martin Weitzman, and the
anonymous reviewers for helpful comments and suggestions.

References

Arrow, K. J., & Hurwicz, L. (Eds.). (1977). Studies in Resource Allocation Processes.
Cambridge University Press, Cambridge.
Bertsekas, D. P., & Tsitsiklis, J. N. (1989). Parallel and Distributed Computation. PrenticeHall, Englewood Clis, NJ.
Dafermos, S., & Nagurney, A. (1989). Supply and demand equilibration algorithms for a
class of market equilibrium problems. Transportation Science, 23, 118{124.
Davis, R., & Smith, R. G. (1983). Negotiation as a metaphor for distributed problem
solving. Articial Intelligence, 20, 63{109.
Drexler, K. E., & Miller, M. S. (1988). Incentive engineering for computational resource
management. In Huberman (1988), pp. 231{266.
Eydeland, A., & Nagurney, A. (1989). Progressive equilibration algorithms: The case of
linear transaction costs. Computer Science in Economics and Management, 2, 197{
219.
Gallager, R. G. (1977). A minimum delay routing algorithm using distributed computation.
IEEE Transactions on Communications, 25, 73{85.
Harker, P. T. (1986). Alternative models of spatial competition. Operations Research, 34,
410{425.
Harker, P. T. (1987). Predicting Intercity Freight Flows. VNU Science Press, Utrecht, The
Netherlands.
Harker, P. T. (1988). Multiple equilibrium behaviors on networks. Transportation Science,
22, 39{46.
Haurie, A., & Marcotte, P. (1985). On the relationship between Nash-Cournot and Wardrop
equilibria. Networks, 15, 295{308.
Hicks, J. R. (1948). Value and Capital (second edition). Oxford University Press, London.
Hildenbrand, W., & Kirman, A. P. (1976). Introduction to Equilibrium Analysis: Variations on Themes by Edgeworth and Walras. North-Holland Publishing Company,
Amsterdam.
Huberman, B. A. (Ed.). (1988). The Ecology of Computation. North-Holland.
Hurwicz, L. (1977). The design of resource allocation mechanisms. In Arrow and Hurwicz
(1977), pp. 3{37. Reprinted from American Economic Review Papers and Proceedings,
1973.
21

Wellman

Kephart, J. O., Hogg, T., & Huberman, B. A. (1989). Dynamics of computational ecosystems. Physical Review A, 40, 404{421.
Koopmans, T. C. (1970). Uses of prices. In Scientic Papers of Tjalling C. Koopmans, pp.
243{257. Springer-Verlag. Originally published in the Proceedings of the Conference
on Operations Research in Production and Inventory Control, 1954.
Kurose, J. F., & Simha, R. (1989). A microeconomic approach to optimal resource allocation
in distributed computer systems. IEEE Transactions on Computers, 38, 705{717.
Kuwabara, K., & Ishida, T. (1992). Symbiotic approach to distributed resource allocation:
Toward coordinated balancing. In Pre-Proceedings of the 4th European Workshop on
Modeling Autonomous Agents in a Multi-Agent World.
Lavoie, D., Baetjer, H., & Tulloh, W. (1991). Coping with complexity: OOPS and the
economists' critique of central planning. Hotline on Object-Oriented Technology, 3 (1),
6{8.
Malone, T. W., Fikes, R. E., Grant, K. R., & Howard, M. T. (1988). Enterprise: A marketlike task scheduler for distributed computing environments. In Huberman (1988), pp.
177{205.
Malone, T. W. (1987). Modeling coordination in organizations and markets. Management
Science, 33, 1317{1332.
McAfee, R. P., & McMillan, J. (1987). Auctions and bidding. Journal of Economic Literature, 25, 699{738.
Milgrom, P., & Roberts, J. (1991). Adaptive and sophisticated learning in normal form
games. Games and Economic Behavior, 3, 82{100.
Miller, M. S., & Drexler, K. E. (1988). Markets and computation: Agoric open systems. In
Huberman (1988), pp. 133{176.
Nagurney, A. (1993). Network Economics: A Variational Inequality Approach. Kluwer
Academic Publishers.
Newell, A. (1982). The knowledge level. Articial Intelligence, 18, 87{127.
Reiter, S. (1986). Information incentive and performance in the (new)2 welfare economics. In
Reiter, S. (Ed.), Studies in Mathematical Economics. MAA Studies in Mathematics.
Samuelson, P. A. (1966). Market mechanisms and maximization. In Stiglitz, J. E. (Ed.),
The Collected Scientic Papers of Paul A. Samuelson, Vol. 1, pp. 415{492. MIT Press,
Cambridge, MA. Originally appeared in RAND research memoranda, 1949.
Samuelson, P. A. (1974). Complementarity: An essay on the 40th anniversary of the HicksAllen revolution in demand theory. Journal of Economic Literature, 12, 1255{1289.
22

Market-Oriented Programming

Sandholm, T. (1993). An implementation of the contract net protocol based on marginal
cost calculations. In Proceedings of the National Conference on Articial Intelligence,
pp. 256{262 Washington, DC. AAAI.
Scarf, H. E. (1984). The computation of equilibrium prices. In Scarf, H. E., & Shoven, J. B.
(Eds.), Applied General Equilibrium Analysis, pp. 1{49. Cambridge University Press,
Cambridge.
Shenker, S. (1991). Congestion control in computer networks: An exercise in cost-sharing.
Prepared for delivery at Annual Meeting of the American Political Science Association.
Shoham, Y. (1993). Agent-oriented programming. Articial Intelligence, 60, 51{92.
Shoven, J. B., & Whalley, J. (1984). Applied general-equilibrium models of taxation and
international trade: An introduction and survey. Journal of Economic Literature, 22,
1007{1051.
Shoven, J. B., & Whalley, J. (1992). Applying General Equilibrium. Cambridge University
Press.
Varian, H. R. (1984). Microeconomic Analysis (second edition). W. W. Norton & Company,
New York.
Waldspurger, C. A., Hogg, T., Huberman, B. A., Kephart, J. O., & Stornetta, S. (1992).
Spawn: A distributed computational economy. IEEE Transactions on Software Engineering, 18, 103{117.
Wang, Z., & Slagle, J. (1993). An object-oriented knowledge-based approach for formulating
applied general equilibrium models. In Third International Workshop on Articial
Intelligence in Economics and Management Portland, OR.
Wellman, M. P. (1991). Review of Huberman (1988). Articial Intelligence, 52, 205{218.
Wellman, M. P. (1992). A general-equilibrium approach to distributed transportation planning. In Proceedings of the National Conference on Articial Intelligence, pp. 282{289
San Jose, CA. AAAI.

23

Journal of Articial Intelligence Research 1 (1994) 209-229

Submitted 11/93; published 2/94

Learning the Past Tense of English Verbs:
The Symbolic Pattern Associator vs. Connectionist Models
Charles X. Ling

ling@csd.uwo.ca

Department of Computer Science
The University of Western Ontario
London, Ontario, Canada N6A 5B7

Abstract

Learning the past tense of English verbs | a seemingly minor aspect of language acquisition | has generated heated debates since 1986, and has become a landmark task
for testing the adequacy of cognitive modeling. Several articial neural networks (ANNs)
have been implemented, and a challenge for better symbolic models has been posed. In
this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based upon
the decision-tree learning algorithm ID3. We conduct extensive head-to-head comparisons
on the generalization ability between ANN models and the SPA under dierent representations. We conclude that the SPA generalizes the past tense of unseen verbs better than
ANN models by a wide margin, and we oer insights as to why this should be the case.
We also discuss a new default strategy for decision-tree learning algorithms.

1. Introduction
Learning the past tense of English verbs, a seemingly minor aspect of language acquisition,
has generated heated debates since the rst connectionist implementation in 1986 (Rumelhart & McClelland, 1986). Based on their results, Rumelhart and McClelland claimed that
the use and acquisition of human knowledge of language can best be formulated by ANN
(Articial Neural Network) models without symbol processing that postulates the existence
of explicit symbolic representation and rules. Since then, learning the past tense has become a landmark task for testing the adequacy of cognitive modeling. Over the years a
number of criticisms of connectionist modeling appeared (Pinker & Prince, 1988; Lachter &
Bever, 1988; Prasada & Pinker, 1993; Ling, Cherwenka, & Marinov, 1993). These criticisms
centered mainly upon the issues of high error rates and low reliability of the experimental results, the inappropriateness of the training and testing procedures, \hidden" features of the
representation and the network architecture that facilitate learning, as well as the opaque
knowledge representation of the networks. Several subsequent attempts at improving the
original results with new ANN models have been made (Plunkett & Marchman, 1991; Cottrell & Plunkett, 1991; MacWhinney & Leinbach, 1991; Daugherty & Seidenberg, 1993).
Most notably, MacWhinney and Leinbach (1991) constructed a multilayer neural network
with backpropagation (BP), and attempted to answer early criticisms. On the other hand,
supporters of the symbolic approach believe that symbol structures such as parse trees,
propositions, etc., and the rules for their manipulations, are critical at the cognitive level,
while the connectionist approach may only provide an account of the neural structures
in which the traditional symbol-processing cognitive architecture is implemented (Fodor
& Pylyshyn, 1988). Pinker (1991) and Prasada and Pinker (1993) argue that a proper

c 1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Ling

accounting for regular verbs should be dependent upon production rules, while irregular
past-tense inections may be generalized by ANN-like associative memory.
The proper way of debating the adequacy of symbolic and connectionist modeling is by
contrasting competitive implementations. Thus, a symbolic implementation is needed that
can be compared with the ANN models. This is, in fact, a challenge posed by MacWhinney
and Leinbach (1991), who assert that no symbolic methods would work as well as their own
model. In a section titled \Is there a better symbolic model?" they claim:
If there were some other approach that provided an even more accurate
characterization of the learning process, we might still be forced to reject the
connectionist approach, despite its successes. The proper way of debating conceptualizations is by contrasting competitive implementations. To do this in the
present case, we would need a symbolic implementation that could be contrasted
with the current implementation. (MacWhinney & Leinbach, 1991, page 153)
In this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based
upon the symbolic decision tree learning algorithm ID3 (Quinlan, 1986). We have shown
(Ling & Marinov, 1993) that the SPA's results are much more psychologically realistic than
ANN models when compared with human subjects. On the issue of the predictive accuracy,
MacWhinney and Leinbach (1991) did not report important results of their model on unseen
regular verbs. To reply to our criticism, MacWhinney (1993) re-implemented the ANN
model, and claimed that its raw generalization power is very close to that of our SPA. He
believed that this should be the case because both systems learn from the same data set:
There is a very good reason for the equivalent performance of these two
models. [...] When two computationally powerful systems are given the same
set of input data, they both extract every bit of data regularity from that input.
Without any further processing, there is only so much blood that can be squeezed
out of a turnip, and each of our systems [SPA and ANN] extracted what they
could. (MacWhinney, 1993, page 295)
We will show that this is not the case; obviously there are reasons why one learning
algorithm outperforms another (otherwise why do we study dierent learning algorithms?).
The Occam's Razor Principle | preferring the simplest hypothesis over more complex
ones | creates preference biases for learning algorithms. A preference bias is a preference
order among competitive hypotheses in the hypothesis space. Dierent learning algorithms,
however, employ dierent ways of measuring simplicity, and thus concepts that they bias
to are dierent. How well a learning program generalizes depends upon the degree to which
the regularity of the data ts with its bias. We study and compare the raw generalization
ability of symbolic and ANN models on the task of learning the past tense of English
verbs. We perform extensive head-to-head comparisons between ANN and SPA, and show
the eects of dierent representations and encodings on their generalization abilities. Our
experimental results demonstrate clearly that
1. the distributed representation, a feature that connectionists have been advocating,
does not lead to better generalization when compared with the symbolic representation, or with arbitrary error-correcting codes of a proper length;
210

Learning the Past Tense: Symbolic vs Connectionist Models

2. ANNs cannot learn the identity mapping that preserves the verb stem in the past
tense as well as the SPA can;
3. a new representation suggested by MacWhinney (1993) improves the predictive accuracy of both SPA and ANN, but SPA still outperforms ANN models;
4. in sum, the SPA generalizes the past tense of unseen verbs better than ANN models
by a wide margin.
In Section 5 we discuss reasons as to why the SPA is a better learning model for the
the task of English past-tense acquisition. Our results support the view that many such
rule-governed cognitive processes should be better modeled by symbolic, rather than connectionist, systems.

2. Review of Previous Work
In this section, we review briey the two main connectionist models of learning the past
tenses of English verbs, and the subsequent criticisms.

2.1 Rumelhart and McClelland's Model

Rumelhart and McClelland's model is based on a simple perceptron-based pattern associator interfaced with an input/output encoding/decoding network which allows the model
to associate verb stems with their past tenses using a special Wickelphone/Wickelfeature
phoneme-representation format. The learning algorithm is the classical perceptron convergence procedure. The training and the testing sets are mutually disjoint in the experiments.
The errors made by the model during the training process broadly follow the U-shaped learning curve in the stages of acquisition of the English past tense exhibited by young children.
The testing sample consists of 86 \unseen" low frequency verbs (14 irregular and 72 regular)
that are not randomly chosen. The testing sample results have a 93% error rate for the
irregulars. The regulars fare better with a 33.3% error rate. Thus, the overall error rate for
the whole testing sample is 43% | 37 wrong or ambiguous past tense forms out of 86 tested.
Rumelhart and McClelland (1986) claim that the outcome of their experiment disconrms
the view that there exist explicit (though inaccessible) rules that underlie human knowledge
of language.

2.2 MacWhinney and Leinbach's Model

MacWhinney and Leinbach (1991) report a new connectionist model on the learning of the
past tenses of English verbs. They claim that the results from the new simulation are far
superior to Rumelhart and McClelland's results, and that they can answer most of the criticisms aimed at the earlier model. The major departure from Rumelhart and McClelland's
model is that the Wickelphone/Wickelfeature representational format is replaced with the
UNIBET (MacWhinney, 1990) phoneme representational system which allows the assignment of a single alphabetic/numerical letter to each of the total 36 phonemes. MacWhinney
and Leinbach use special templates with which to code each phoneme and its position in a
word. The actual input to the network is created by coding the individual phonemes as sets
211

Ling

of phonetic features in a way similar to the coding of Wickelphones as Wickelfeatures (cf
Section 4.3). The network has two layers of 200 \hidden" units fully connected to adjacent
layers. This number was arrived at through trial and error. In addition, the network has a
special-purpose set of connections that copy the input units directly onto the output units.
Altogether, 2062 regular and irregular English verbs are selected for the experiment
| 1650 of them are used for training (1532 regular and 118 irregular), but only 13 low
frequency irregular verbs are used for testing (MacWhinney & Leinbach, 1991, page 144).
Training the network takes 24,000 epochs. At the end of training there still are 11 errors
on the irregular pasts. MacWhinney and Leinbach believe that if they allow the network
to run for several additional days and give it additional hidden unit resources, it probably
can reach complete convergence (MacWhinney & Leinbach, 1991, page 151). The only
testing error rate reported is based on a very small and biased test sample of 13 unseen
irregular verbs; 9 out of 13 are predicted incorrectly. They do not test their model on any
of the unseen regular verbs: \Unfortunately, we did not test a similar set of 13 regulars."
(MacWhinney & Leinbach, 1991, page 151).

2.3 Criticism of the Connectionist Models

Previous and current criticisms of the connectionist models of learning the past tenses of
English verbs center mainly on several issues. Each of these issues is summarized in the
following subsections.

2.3.1 Error Rates

The error rate in producing the past tenses of the \unseen" test verbs is very high in
both ANN models, and important tests were not carried out in MacWhinney and Leinbach
(1991) model. The experimental results indicate that neither model reaches the level of
adult competence. In addition, relatively large numbers of the errors are not psychologically
realistic since humans rarely make them.

2.3.2 Training and Testing Procedures

In both Rumelhart and McClelland's model, and MacWhinney and Leinbach's model, the
generalization ability is measured on only one training/testing sample. Further, the testing
sets are not randomly chosen, and they are very small. The accuracy in testing irregular
verbs can vary greatly depending upon the particular set of testing verbs chosen, and thus
multiple runs with large testing samples are necessary to assess the true generalization
ability of a learning model. Therefore, the results of the previous connectionist models are
not reliable. In Section 4, we set up a reliable testing procedure to compare connectionist
models with our symbolic approach. Previous connectionist simulations have also been
criticized for their crude training processes (for example, the sudden increase of regular
verbs in the training set), which create such behavior as the U-shaped learning curves.

2.3.3 Data Representation and Network Architecture

Most of the past criticisms of the connectionist models have been aimed at the datarepresentation formats employed in the simulations. Lachter and Bever (1988) pointed
212

Learning the Past Tense: Symbolic vs Connectionist Models

out that the results achieved by Rumelhart and McClelland's model would have been impossible without the use of several TRICS (The Representations It Crucially Supposes)
introduced with the adoption of the Wickelphone/Wickelfeature representational format.
MacWhinney and Leinbach claim that they have improved upon the earlier connectionist
model by getting rid of the Wickelphone/Wickelfeature representation format, and thus
to have responded to the many criticisms that this format entailed. However, MacWhinney and Leinbach also introduce several TRICS in their data-representation format. For
example, instead of coding predecessor and successor phonemes as Wickelphones, they introduce special templates with which to code positional information. This means that the
network will learn to associate patterns of phoneme/positions within a predetermined consonant/vowel pattern. Further, the use of restrictive templates gets rid of many English
verbs that do not t the chosen template. This may bias the model in favour of shorter
verbs, predominantly of Anglo-Saxon origin, and against longer verbs, predominantly composite or of Latin and French origin. Another TRICS introduced is the phonetic feature
encoding (a distributed representation). It is not clear why phonetic features such as front,
centre, back, high, etc. are chosen. Do they represent ner grained \microfeatures" that
help to capture the regularities in English past tenses? In Section 4.5, we will show that
the straightforward symbolic representation leads to better generalization than does the
carefully engineered distributed representation. This undermines the claimed advantages of
the distributed representation of connectionist models.

2.3.4 Knowledge Representation and Integration of Acquired Knowledge
Pinker and Prince (1988), and Lachter and Bever (1988) point out that Rumelhart and
McClelland try to model the acquisition of the production of the past tense in isolation
from the rest of the English morphological system. Rumelhart and McClelland, as well
as MacWhinney and Leinbach, assume that the acquisition process establishes a direct
mapping from the phonetic representation of the stem to the phonetic representation of
the past tense form. This direct mapping collapses some well-established distinctions such
as lexical item vs. phoneme string, and morphological category vs. morpheme. Simply
remaining at the level of phonetic patterns, it is impossible to express new categorical
information in rst-order (predicate/function/variable) format. One of the inherent decits
of the connectionist implementations is that there is no such thing as a variable for verb
stem, and hence there is no way for the model to attain the knowledge that one could
add sux to a stem to get its past tense (Pinker & Prince, 1988, page 124). Since the
acquired knowledge in such networks is a large weight matrix, which usually is opaque to the
human observer, it is unclear how the phonological levels processing that the connectionist
models carry out can be integrated with the morphological, lexical, and syntactical level
of processing. Neither Rumelhart and McClelland nor MacWhinney and Leinbach address
this issue. In contrast to ANNs whose internal representations are entirely opaque, the
SPA can represent the acquired knowledge in the form of production rules, and allow for
further processing, resulting in higher-level categories such as the verb stem and the voiced
consonants, linguistically realistic production rules using these new categories for regular
verbs, and associative templates for irregular verbs (Ling & Marinov, 1993).
213

Ling

3. The Symbolic Pattern Associator

We take up MacWhinney and Leinbach's challenge for a better symbolic model for learning
the past tense of English verbs, and present a general-purpose Symbolic Pattern Associator
(SPA)1 that can generalize the past tense of unseen verbs much more accurately than
connectionist models in this section. Our model is symbolic for several reasons. First,
the input/output representation of the learning program is a set of phoneme symbols,
which are the basic elements governing the past-tense inection. Second, the learning
program operates on those phoneme symbols directly, and the acquired knowledge can be
represented in the form of production rules using those phoneme symbols as well. Third,
those production rules at the phonological level can easily be further generalized into rstorder rules that use more abstract, high-level symbolic categories such as morphemes and
the verb stem (Ling & Marinov, 1993). In contrast, the connectionist models operate
on a distributed representation (phonetic feature vectors), and the acquired knowledge is
embedded in a large weight matrix; it is therefore hard to see how this knowledge can be
further generalized into more abstract representations and categories.

3.1 The Architecture of the Symbolic Pattern Associator

The SPA is based on C4.5 (Quinlan, 1993) which is an improved implementation of the ID3
learning algorithm (cf. (Quinlan, 1986)). ID3 is a program for inducing classication rules in
the form of decision trees from a set of classied examples. It uses information gain ratio as
a criterion for selecting attributes as roots of the subtrees. The divide-and-conquer strategy
is recursively applied in building subtrees until all remaining examples in the training set
belong to a single concept (class); then a leaf is labeled as that concept. The information
gain guides a greedy heuristic search for the locally most relevant or discriminating attribute
that maximally reduces the entropy (randomness) in the divided set of the examples. The
use of this heuristic usually results in building small decision trees instead of larger ones
that also t the training data.
If the task is to learn to classify a set of dierent patterns into a single class of several
mutually exclusive categories, ID3 has been shown to be comparable with neural networks
(i.e., within about 5% range on the predictive accuracy) on many real-world learning tasks
(cf. (Shavlik, Mooney, & Towell, 1991; Feng, King, Sutherland, & Henery, 1992; Ripley,
1992; Weiss & Kulikowski, 1991)). However, if the task is to classify a set of (input) patterns
into (output) patterns of many attributes, ID3 cannot be applied directly. The reason is
that if ID3 treats the dierent output patterns as mutually exclusive classes, the number of
classes would be exponentially large and, more importantly, any generalization of individual
output attributes within the output patterns would be lost.
To turn ID3 or any similar N-to-1 classication system into general purpose N-to-M
symbolic pattern associators, the SPA applies ID3 on all output attributes and combines
individual decision trees into a \forest", or set of trees. A similar approach was proposed for
dealing with the distributed (binary) encoding in multiclass learning tasks such as NETtalk
(English text-to-speech mapping) (Dietterich, Hild, & Bakiri, 1990). Each tree takes as
input the set of all attributes in the input patterns, and is used to determine the value of
1. The SPA programs and relevant datasets can be obtained anonymously from ftp.csd.uwo.ca under
pub/SPA/ .

214

Learning the Past Tense: Symbolic vs Connectionist Models

one attribute in its output pattern. More specically, if a pair of input attributes (1 to n )
and output attributes (!1 to !m ) is represented as:

1; :::; n ! !1; :::; !m

then the SPA will build a total of m decision trees, one for each output attribute !i (1 
i  m) taking all input attributes 1; :::; n per tree. Once all of m trees are built, the SPA
can use them jointly to determine the output pattern !1 ; :::; !m from any input pattern
1; :::; n.

An important feature of the SPA is explicit knowledge representation. Decision trees for
output attributes can easily be transformed into propositional production rules (Quinlan,
1993). Since entities of these rules are symbols with semantic meanings, the acquired
knowledge often is comprehensible to the human observer. In addition, further processing
and integration of these rules can yield high-level knowledge (e.g., rules using verb stems)
(Ling & Marinov, 1993). Another feature of the SPA is that the trees for dierent output
attributes contain identical components (branches and subtrees) (Ling & Marinov, 1993).
These components have similar roles as hidden units in ANNs since they are shared in the
decision trees of more than one output attribute. These identical components can also be
viewed as high-level concepts or feature combinations created by the learning program.

3.2 Default Strategies

An interesting research issue is how decision-tree learning algorithms handle the default
class. A default class is the class to be assigned to leaves which no training examples are
classied into. We call these leaves empty leaves. This happens when the attributes have
many dierent values, or when the training set is relatively small. In these cases, during the
tree construction, only a few branches are explored for some attributes. When the testing
examples fall into the empty leaves, a default strategy is needed to assign classes to those
empty leaves.
For easier understanding, we use the spelling form of verbs in this subsection to explain
how dierent default strategies work. (In the actual learning experiment the verbs are
represented in phonetic form.) If we use consecutive left-to-right alphabetic representation,
the verb stems and their past tenses of a small training set can be represented as follows:
a,f,f,o,r,d,_,_,_,_,_,_,_,_,_
e,a,t,_,_,_,_,_,_,_,_,_,_,_,_
l,a,u,n,c,h,_,_,_,_,_,_,_,_,_
l,e,a,v,e,_,_,_,_,_,_,_,_,_,_

=>
=>
=>
=>

a,f,f,o,r,d,e,d,_,_,_,_,_,_,_
a,t,e,_,_,_,_,_,_,_,_,_,_,_,_
l,a,u,n,c,h,e,d,_,_,_,_,_,_,_
l,e,f,t,_,_,_,_,_,_,_,_,_,_,_

where is used as a ller for empty space. The left-hand 15 columns are the input patterns
for the stems of the verbs; the right-hand 15 columns are the output patterns for their
corresponding correct past tense forms.
As we have discussed, 15 decision trees will be constructed, one for each output attribute.
The decision tree for the rst output attribute can be constructed (see Figure 1 (a)) from
the following 4 examples:
a,f,f,o,r,d,_,_,_,_,_,_,_,_,_ => a
e,a,t,_,_,_,_,_,_,_,_,_,_,_,_ => a
l,a,u,n,c,h,_,_,_,_,_,_,_,_,_ => l

215

Ling

l,e,a,v,e,_,_,_,_,_,_,_,_,_,_ => l

where the last column is the classication of the rst output attribute. However, many
other branches (such as 1 = c in Figure 1 (a)) are not explored, since no training example
has that attribute value. If a testing pattern has its rst input attribute equal to c, what
class should it be assigned to? ID3 uses the majority default. That is, the most popular
class in the whole subtree under 1 is assigned to the empty leaves. In the example above,
either class a or l will be chosen since they each have 2 training examples. However, this is
clearly not the right strategy for this task since a verb such as create would be output as
l...... or a......, which is incorrect. Because it is unlikely for a small training set to have all
variations of attribute values, the majority default strategy of ID3 is not appropriate for
this task.

1
a

e

6
l

a

c

z

<= Passthrough

__

d

5
a:1

a:1

l:2

c:0

o:2

z:0

d:20
p

x:n indicates that there are n examples
classified in the leaf labelled as x.
x:0 (boxed) indicates the empty leaves.

t:10

Figure 1: (a) Passthrough default

<= Majority

r

d:2

l

d:5

k

t:0

(b) Various default

For applications such as verb past-tense learning, a new default heuristic | passthrough
| may be more suitable. That is, the classication of an empty leaf should be the same
as the attribute value of that branch. For example, using the passthrough default strategy,
create will be output as c....... The passthrough strategy gives decision trees some rst-order
avor, since the production rules for empty leaves can be represented as If Attribute = X
then Class = X where X can be any unused attribute values. Passthrough is a domaindependent heuristic strategy because the class labels may have nothing to do with the
attribute values in other applications.
Applying the passthrough strategy alone, however, is not adequate for every output
attribute. The endings of the regular past tenses are not identical to any of the input
patterns, and the irregular verbs may have vowel and consonant changes in the middle of
the verbs. In these cases, the majority default may be more suitable than the passthrough.
In order to choose the right default strategy | majority or passthrough | a decision is
made based upon the training data in the corresponding subtree. The SPA rst determines
the majority class, and counts the number of examples from all subtrees that belong to
this class. It then counts the number of examples in the subtrees that coincide with the
216

Learning the Past Tense: Symbolic vs Connectionist Models

passthrough strategy. These two numbers are compared, and the default strategy employed
by more examples is chosen. For instance, in the example above (see Figure 1 (a)), the
majority class is l (or a) having 2 instances. However, there are 3 examples coinciding with
the passthrough default: two l and one a. Thus the passthrough strategy takes over, and
assigns all empty leaves at this level. The empty attribute branch c would then be assigned
the class c. Note that the default strategy for empty leaves of attribute X depends upon
training examples falling into the subtree rooted at X . This localized method ensures that
only related objects have an inuence on calculating default classes. As a result, the SPA
can adapt the default strategy that is best suited at dierent levels of the decision trees. For
example, in Figure 1 (b), two dierent default strategies are used at dierent levels in the
same tree. We use the SPA with the adaptive default strategy throughout the remainder of
this paper. Note that the new default strategy is not a TRICS in the data representation;
rather, it represents a bias of the learning program. Any learning algorithm has a default
strategy independent of the data representation. The eect of dierent data representations
on generalization is discussed in Sections 4.3, 4.5, and 4.6. The passthrough strategy can
be imposed on ANNs as well by adding a set of copy connections between the input units
and the twin output units. See Section 4.4 for detail.

3.3 Comparisons of Default Strategies of ID3, SPA, and ANN
Which default strategy do neural networks tend to take in generalizing default classes
when compared with ID3 and SPA? We conducted several experiments to determine neural
networks' default strategy. We assume that the domain has only one attribute X which
may take values a, b, c, and d. The class also can be one of the a, b, c, and d. The training
examples have attribute values a, b, and c but not d | it is reserved for testing the default
class. The training set contains multiple copies of the same example to form a certain
majority class. Table 1 shows two sets of training/testing examples that we used to test
and compare default strategies of ID3, SPA and neural networks.
Data set 1
Data set 2
Training examples
Training examples
Values of X Class # of copies Values of X Class # of copies
a
a
10
a
c
10
b
b
2
b
b
6
c
c
3
c
c
7
Testing example
Testing example
d
?
1
d
?
1
Table 1: Two data sets for testing default strategies of various methods.
The classication of the testing examples by ID3 and SPA is quite easy to decide. Since
ID3 takes only the majority default, the output class is a (with 10 training examples) for
the rst data set, and c (with 17 training examples) for the second data set. For SPA, the
number of examples using passthrough is 15 for the rst data set, and 13 for the second
217

Ling

data set. Therefore, the passthrough strategy wins in the rst case with the output class
d, and the majority strategy wins in the second case with the output class c.
For neural networks, various coding methods were used to represent values of the attribute X . In the dense coding, we used 00 to represent a, 01 for b, 10 for c and 11 for
d. We also tried the standard one-per-class encoding, and real number encoding (0.2 for a,
0.4 for b, 0.6 for c and 0.8 for d). The networks were trained using as few hidden units as
possible in each case. We found that in most cases the classication of the testing example is not stable; it varies with dierent random seeds that initialize the networks. Table
2 summarises the experimental results. For ANNs, various classications obtained by 20
dierent random seeds are listed with the rst ones occurring most frequently. It seems
that not only do neural networks not have a consistent default strategy, but also that it
is neither the majority default as in ID3 nor the passthrough default as in SPA. This may
explain why connectionist models cannot generalize unseen regular verbs well even when
the training set contains only regular verbs (see Section 4.4). The networks have diculty
(or are underconstrained) in generalizing the identity mapping that copies the attributes of
the verb stems into the past tenses.
The classication for the testing example
Data set 1 Data set 2
ID3
a
c
SPA
d
c
ANN, dense coding
b; c
b
ANN, one-per-class
b; c; a
c; b
ANN, real numbers
c; d
d; c
Table 2: Default strategies of ID3, SPA and ANN on two data sets.

4. Head-to-head Comparisons between Symbolic and ANN Models

In this section, we perform a series of extensive head-to-head comparisons using several
dierent representations and encoding methods, and demonstrate that the SPA generalizes
the past tense of unseen verbs better than ANN models do by a wide margin.

4.1 Format of the data

Our verb set came from MacWhinney's original list of verbs. The set contains about
1400 stem/past tense pairs. Learning is based upon the phonological UNIBET representation (MacWhinney, 1990), in which dierent phonemes are represented by dierent
alphabetic/numerical letters. There is a total of 36 phonemes. The source le is transferred
into the standard format of pairs of input and output patterns. For example, the verbs in
Table 3 are represented as pairs of input and output patterns (verb stem => past tense):
6,b,&,n,d,6,n
=>
6,b,&,n,d,6,n,d
I,k,s,E,l,6,r,e,t => I,k,s,E,l,6,r,e,t,I,d

218

Learning the Past Tense: Symbolic vs Connectionist Models

6,r,3,z => 6,r,o,z
b,I,k,6,m => b,I,k,e,m

See Table 3 (The original verb set is available in Online Appendix 1). We keep only one
form of the past tense among multiple past tenses (such as hang-hanged and hang-hung)
in the data set. In addition, no homophones exist in the original data set. Consequently,
there is no noise (contradictory data which have the same input pattern but dierent output
patterns) in the training and testing examples. Note also that information as to whether
the verb is regular or irregular is not provided in training/testing processes.
base (stem)
UNIBET
b=base
1 = irregular
spelling form phonetic form d= past tense 0 = regular
abandon
6b&nd6n
b
0
abandoned
6b&nd6nd
d
0
benet
bEn6fIt
b
0
beneted
bEn6fItId
d
0
arise
6r3z
b
0
arose
6roz
d
1
become
bIk6m
b
0
became
bIkem
d
1
......
Table 3: Source le from MacWhinney and Leinbach.

4.2 Experiment Setup

To guarantee unbiased and reliable comparison results, we use training and testing samples
randomly drawn in several independent runs. Both SPA and ANN are provided with the
same sets of training/testing examples for each run. This allows us to achieve a reliable
estimate of the inductive generalization capabilities of each model on this task.
The neural network program we used is a package called Xerion, which was developed
at the University of Toronto. It has several more sophisticated search mechanisms than
the standard steepest gradient descent method with momentum. We found that training
with the conjugate-gradient method is much faster than with the standard backpropagation
algorithm. Using the conjugate-gradient method also avoids the need to search for proper
settings of parameters such as the learning rate. However, we do need to determine the
proper number of hidden units. In the experiments with ANNs, we rst tried various
numbers of hidden units and chose the one that produced the best predictive accuracy in
a trial run, and then use the network with that number of hidden units in the actual runs.
The SPA, on the other hand, has no parameters to adjust.
One major dierence in implementation between ANNs and SPA is that SPA can take
(symbolic) phoneme letters directly while ANNs normally encode each phoneme letter to
binary bits. (Of course, SPA also can apply to the binary representation). We studied
various binary encoding methods and compared results with SPA using symbolic letter
219

Ling

representation. Since outputs of neural networks are real numbers, we need to decode the
network outputs back to phoneme letters. We used the standard method of decoding: the
phoneme letter that has the minimal real-number Hamming distance (smallest angle) with
the network outputs was chosen. To see how binary encoding aects the generalization,
the SPA was also trained with the binary representation. Since the SPA's outputs are
binary, the decoding process may tie with several phoneme letters. In this case, one of
them is chosen randomly. This reects the probability of the correct decoding at the level
of phoneme letters. When all of the phoneme letters are decoded, if one or more letters are
incorrect, the whole pattern is counted as incorrect at the word level.

4.3 Templated, Distributed Representation

This set of experiments was conducted using the distributed representation suggested by
MacWhinney and Leinbach (1991). According to MacWhinney and Leinbach, the output is
a left-justied template in the format of CCCVVCCCVVCCCVVCCC, where C stands for
consonant and V for vowel space holders. The input has two components: a left-justied
template in the same format as the input, and a right-justied template in the format of
VVCCC. For example, the verb bet, represented in UNIBET coding as bEt, is shown in the
template format as follows ( is the blank phoneme):
INPUT
bEt
template:
OUTPUT
bEt
template:

b__E_t____________
CCCVVCCCVVCCCVVCCC
(left-justified)

_E__t
VVCCC
(right-justified)

b__E_t____________
CCCVVCCCVVCCCVVCCC
(left-justified)

A specic distributed representation | a set of (binary) phonetic features | is used
to encode all phoneme letters for the connectionist networks. Each vowel (V in the above
templates) is encoded by 8 phonetic features (front, centre, back, high, low, middle, round,
and diphthong) and each consonant (C in the above templates) by 10 phonetic features
(voiced, labial, dental, palatal, velar, nasal, liquid, trill, fricative and interdental). Note
that because the two feature sets of vowels and consonants are not identical, templates are
needed in order to decode the right type of the phoneme letters from the outputs of the
network.
In our experimental comparison, we decided not to use the right-justied template
(VVCCC) since this information is redundant. Therefore, we used only the left-justied
template (CCCVVCCCVVCCCVVCCC) in both input and output. (The whole verb set
in the templated phoneme representation is available in Online Appendix 1. It contains
1320 pairs of verb stems and past tenses that t the template). To ease implementation,
we added two extra features that always were assigned to 0 in the vowel phonetic feature
set. Therefore, both vowels and consonants were encoded by 10 binary bits. The ANN
thus had 18  10 = 180 input bits and 180 output bits, and we found that one layer of 200
hidden units (same as MacWhinney (1993) model) reached the highest predictive accuracy
in a trial run. See Figure 2 for the network architecture used.
220

Learning the Past Tense: Symbolic vs Connectionist Models

(180 output units)

C

...

...

...

...

......

...

...

...

C

C

V

V

CCCVVCCCVV

C

C

C

(full connection between the two layers)
......

(200 hidden units)

......

(full connection between the two layers)

C

...

...

...

...

......

...

...

...

C

C

V

V

CCCVVCCCVV

C

C

C

(180 input units)

Figure 2: The architecture of the network used in the experiment.
The SPA was trained and tested on the same data sets but with phoneme letters directly;
that is, 18 decision trees were built for each of the phoneme letters in the output templates.
To see how phonetic feature encoding aects the generalization, we also trained the SPA
with the the same distributed representation | binary bit patterns of 180 input bits and
180 output bits | exactly the same as those in the ANN simulation. In addition, to see how
the \symbolic" encoding works in ANN, we also train another neural network (with 120
hidden units) with the \one-per-class" encoding. That is, each phoneme letter (total of 37;
36 phoneme letters plus one for blank) is encoded by 37 bits, one for each phoneme letter.
We used 500 verb pairs (including both regular and irregular verbs) in the training and
testing sets. Sampling was done randomly without replacement, and training and testing
sets were disjoint. Three runs of SPA and ANN were conducted, and both SPA and ANN
were trained and tested on the same data set in each run. Training reached 100% accuracy
for SPA and around 99% for ANN.
Testing accuracy on novel verbs produced some interesting results. The ANN model
and the SPA using the distributed representation have very similar accuracy, with ANN
slightly better. This may well be caused by the binary outputs of SPA that suppress the
ne dierences in prediction. On the other hand, the SPA using phoneme letters directly
produces much higher accuracy on testing. The SPA outperforms neural networks (with
either distributed or one-per-class representations) by 20 percentage points! The testing
results of ANN and SPA can be found in Table 4. Our ndings clearly indicate that the
SPA using symbolic representation leads to much better generalization than ANN models.

4.4 Learning Regular Verbs

Predicting the past tense of an unseen verb, which can be either regular or irregular, is
not an easy task. Irregular verbs are not learned by rote as traditionally thought since
221

Ling

Distributed representation
ANN: % Correct
SPA: % Correct
Reg Irrg Comb Reg Irrg Comb
65.3 14.6 60.4 62.2 18.8 58.0
59.7 8.6 53.8 57.9 8.2 52.2
60.0 16.0 55.6 58.0 8.0 53.0
61.7 13.1 56.6 59.4 11.7 54.4

Symbolic representation
ANN: % Correct
SPA: % Correct
Reg Irrg Comb Reg Irrg Comb
63.3 18.8 59.2 83.0 29.2 77.8
58.8 10.3 53.2 83.3 22.4 76.2
58.7 16.0 54.4 80.9 20.0 74.8
60.3 15.0 55.6 82.4 23.9 76.3

Table 4: Comparisons of testing accuracy of SPA and ANN with distributed and symbolic
representations.
children and adults occasionally extend irregular inection to irregular-sounding regular
verbs or pseudo verbs (such as cleef | cleft) (Prasada & Pinker, 1993). The more similar
the novel verb is to the cluster of irregular verbs with similar phonological patterns, the
more likely the prediction of an irregular past-tense form. Pinker (1991) and Prasada and
Pinker (1993) argue that regular past tenses are governed by rules, while irregulars may
be generated by the associated memory which has this graded eect of irregular past-tense
generalization. It is would be interesting, therefore, to compare SPA and ANN on the
past-tense generalization of regular verbs only. Because both SPA and ANN use the same,
position specic, representation, learning regular past tenses would require learning dierent
suxes2 at dierent positions, and to learn the identity mapping that copies the verb stem
to the past tenses for verbs of dierent lengths.
We used the same templated representation as in the previous section, but both training
and testing sets contained only regular verbs. Again samples were drawn randomly without
replacement. To maximize the size of the testing sets, testing sets simply consisted of all
regular verbs that were not sampled in the training sets. The same training and testing sets
were used for each of the following methods compared. To see the eect of the adaptive
default strategy (as discussed in Section 3.2) on generalization, the SPA with the majority
default only and with the adaptive default were both tested. The ANN models were similar
to those used in the previous section (except with 160 one-layer hidden units, which turned
out to have the best predictive accuracy in a test run). The passthrough default strategy can
be imposed on neural networks by adding a set of copy connections that connect directly
from the input units to the twin output units. MacWhinney and Leinbach (1991) used
such copy connections in their simulation. We therefore tested the networks with the copy
connection to see if generalization would be improved as well.
The results on the predictive accuracy of the SPA and ANNs on one run with with
randomly sampled training and testing sets are summarized in Table 5. As we can see,
the SPA with the adaptive default strategy, which combines the majority and passthrough
default, outperforms the SPA with only the majority default strategy used in ID3. The
2. In phonological form there are three dierent suxes for regular verbs. When the verb stem ends with
t or d (UNIBET phonetic representations), then the sux is Id. For example, extend | extended (in
spelling form). When the verb stem ends with a unvoiced consonant, the sux is t. For example, talk
| talked. When the verb stem ends with a voiced consonant or vowel, the sux is d. For example,
solve | solved.

222

Learning the Past Tense: Symbolic vs Connectionist Models

ANNs with copy connections do generalize better than the ones without. However, even
ANN models with copy connections have a lower predictive accuracy than the SPA (majority). In addition, the dierences in the predictive accuracy are larger with smaller sets
of training examples. Smaller training sets make the dierence in testing accuracy more
evident. When the training set contains 1000 patterns (out of 1184), the testing accuracy
becomes very similar, and would approach asymptotically to 100% with larger training sets.
Upon examination, most of the errors made in ANN models occur in the identity mapping
(i.e., strange phoneme change and drop); the verb stems cannot be preserved in the past
tense if the phonemes are not previously seen in the training examples. This contradicts the
ndings of Prasada and Pinker (1993), which show that native English speakers generate
regular sux-adding past tenses equally well with unfamiliar-sounding verb stems (as long
as these verb stems do not sound close to irregular verbs). This also indicates that the bias
of the ANN learning algorithms is not suitable to this type of task. See further discussion
in Section 5.
Training
Percent correct on testing
size
SPA (adaptive) SPA (majority) ANN (copy con.) ANN (normal)
50
55.4
30.0
14.6
7.3
100
72.9
58.6
34.6
24.9
300
87.0
83.7
59.8
58.2
500
92.5
89.0
82.6
67.9
1000
93.5
92.4
92.0
87.3
Table 5: Predictive accuracy on learning the past tense of regular verbs

4.5 Error Correcting Codes
Dietterich and Bakiri (1991) reported an increase in the predictive accuracy when errorcorrecting codes of large Hamming distances are used to encode values of the attributes.
This is because codes with larger Hamming distance (d) allow for correcting fewer than d=2
bits of errors. Thus, learning programs are allowed to make some mistakes at the bit level
without their outputs being misinterpreted at the word level.
We wanted to nd if performances of the SPA and ANNs are improved with the errorcorrecting codes encoding all of the 36 phonemes. We chose error-correcting codes ranging
from ones with small Hamming distance to ones with very large Hamming distance (using
the BHC codes, see Dietterich and Bakiri (1991)). Because the number of attributes for
each phoneme is too large, the data representation was changed slightly for this experiment.
Instead of 18 phoneme holders with templates, 8 consecutive, left-to-right phoneme holders
were used. Verbs with stems or past tenses of more than 8 phonemes were removed from the
training/testing sets. (The whole verb set in the this representation is available in Online
Appendix 1. It contains 1225 pairs of verb stems and past tenses whose lengths are shorter
than 8). Both SPA and ANN take exactly the same training/testing sets, each contains 500
pairs of verb stems and past tenses, with the error-correcting codes encoding each phoneme
223

Ling

letter. Still, training networks with 92-bit or longer error-correcting codes takes too long to
run (there are 8  92 = 736 input attributes and 736 output attributes). Therefore, only
two runs with 23- and 46-bit codes were conducted. Consistent with Dietterich and Bakiri
(1991)'s ndings, we found that the testing accuracy generally increases when the Hamming
distance increases. However, we also observed that the testing accuracy decreases very
slightly when the codes become too long. The accuracy using 46-bit codes (with Hamming
distance of 20) reaches the maximum value (77.2%), which is quite close to the accuracy
(78.3%) of SPA using the direct phoneme letter representation. It seems there is a trade-o
between tolerance of errors with large Hamming distance and diculty in learning with
longer codes. In addition, we found the testing accuracy of ANNs to be lower than the one
of SPA for both 23 bit- and 46-bit error-correcting codes. The results are summarized in
Table 6.
ANN
Hamming Distance Correct at bit level Correct at word level
23-bit codes
10
93.5%
65.6%
46-bit codes
20
94.1%
67.4%
SPA
Hamming Distance Correct at bit level Correct at word level
23-bit codes
10
96.3%
72.4%
46-bit codes
20
96.3%
77.2%
92-bit codes
40
96.1%
75.6%
127-bit codes
54
96.1%
75.4%
Table 6: Comparisons of the testing accuracy of SPA and ANNs with error-correcting codes
Our results in this and the previous two subsections undermine the advantages of the
distributed representation of ANNs, a unique feature advocated by connectionists. We have
demonstrated that, in this task, the distributed representation actually does not allow for
adequate generalization. Both SPA using direct symbolic phoneme letters and SPA with
error-correcting codes outperform ANNs with distributed representation by a wide margin.
However, neither phoneme symbols nor bits in the error-correcting codes encode, implicitly
or explicitly, any micro-features as in the distributed representation. It may be that the
distributed representation used was not optimally designed. Nevertheless, straightforward
symbolic format requires little representation engineering compared with the distributed
representation in ANNs.

4.6 Right-justied, Isolated Sux Representation

MacWhinney and Leinbach (1991) did not report important results of the predictive accuracy of their model on unseen regular verbs. In his reply (MacWhinney, 1993) to our
paper (Ling & Marinov, 1993), MacWhinney re-implemented the ANN model. In his new
implementation, 1,200 verb stem and past-tense pairs were in the training set, among which
1081 were regular and 119 were irregular. Training took 4,200 epochs, and reached 100%
correct on regulars and 80% on irregulars. The testing set consisted of 87 regulars and 15
irregulars. The percent correct on testing at epoch 4,200 was 91% for regulars and 27% for
irregulars, with a combined 80.0% on the testing set. MacWhinney claimed that the raw
224

Learning the Past Tense: Symbolic vs Connectionist Models

generalization power of ANN model is very close to that of our SPA. He believes that this
should be the case simply because both systems were trained on the same data set.
We realize (via private communication) that a new representation used in MacWhinney's
recent implementation plays a critical role in the improved performance. In MacWhinney's
new representation, the input (for verb stems) is coded by the right-justied template
CCCVVCCCVVCCCVVCCC. The output contains two parts: a right-justied template
that is the same as the one in the input, and a coda in the form of VVCCC. The rightjustied template in the output is used to represent the past tense without including the
sux for the regular verbs. The sux of the regular past tense always stays in the coda,
which is isolated from the main, right-justied templates. For the irregular past tense, the
coda is left empty. For example, the input and output templated patterns for the past tense
of verbs in Table 3 are represented as:
INPUT
(right-justified)
CCCVVCCCVVCCCVVCCC
___6_b__&_nd_6_n__
b__E_n__6_f__I_t__
________6_r__3_z__
_____b__I_k__6_m__

OUTPUT
(right-justified)
CCCVVCCCVVCCCVVCCC
___6_b__&_nd_6_n__
b__E_n__6_f__I_t__
________6_r__o_z__
_____b__I_k__e_m__

(suffix only)
VVCCC
__d__ (for abandon-abandoned)
I_d__ (for benefit-benefited)
_____ (for arise-arose)
_____ (for become-became)

Such data representation clearly facilitates learning. For the regular verbs, the output
patterns are always identical to the input patterns. In addition, the verb-ending phoneme
letters always appear at a few xed positions (i.e., the right most VVCCC section in the
input template) due to the right-justied, templated representation. Furthermore, the sux
always occupies the coda, isolated from the right-justied templates.
We performed a series of experiments to see how much improvement we could accomplish using the new representation over MacWhinney's recent ANN model and over the
left-justied representation discussed in Section 4.3. Our SPA (with an averaged predictive
accuracy of 89.0%) outperforms MacWhinney's recent ANN implementation (with the predictive accuracy of 80.0%) by a wide margin. In addition, the predictive accuracy is also
improved from an average of 76.3% from the left-justied representation to 82.8% of the
right-justied, isolated sux one. See results in Table 7.

5. General Discussion and Conclusions

Two factors contribute to the generalization ability of a learning program. The rst is
the data representation, and the other is the bias of the learning program. Arriving at
the right, optimal, representation is a dicult task. As argued by Prasada and Pinker
(1993), regular verbs should be represented in a coarse grain in terms of the verb stem
and suxes; while irregular verbs in a ner grain in terms of phonological properties.
Admittedly, SPA works uniformly at the level of phoneme letters, as ANNs do. However,
because SPA produces simple production rules that use these phoneme letters directly, those
rules can be further generalized to rst-order rules with new representations such as stems
and the voiced consonants which can be used across the board in other such rule-learning
modules (Ling & Marinov, 1993). This is one of the major advantages over ANN models.
225

Ling

Predictive accuracy with right-justied, isolated sux representation
SPA
MacWhinney's ANN model
training/testing training/testing
training/testing
500/500
1200/102
1200/102
Run 1
81.3
89.2
Run 2
84.1
90.4
Run 3
83.1
87.4
Average
82.8
89.0
80.0 (one run)
Table 7: Comparisons of testing accuracy of SPA and ANN (with right-justied, isolated
sux representation)
It seems quite conceivable that children acquire these high-level concepts such as stems
and voiced consonants through learning noun plurals, verb past tense, verb third-person
singular, comparative adjectives, and so on. With a large weight matrix as the result of
learning, it is hard to see how this knowledge can be further generalized in ANN models
and shared in other modules.
Even with exactly the same data representation, there exist some learning tasks that
symbolic methods such as the SPA generalize categorically better than ANNs. The converse also is true. This fact reects the dierent inductive biases of the dierent learning
algorithms. The Occam's Razor Principle | preferring the simplest hypothesis over more
complex ones | creates a preference bias, a preference of choosing certain hypotheses over
others in the hypothesis space. However, dierent learning algorithms choose dierent hypotheses because they use dierent measurements for simplicity. For example, among all
possible decision trees that t the training examples, ID3 and SPA induce simple decision
trees instead of complicated ones. Simple decision trees can be converted to small sets of
production rules. How well a learning algorithm generalizes depends upon the degree to
which the underlying regularities of the target concept t its bias. In other words, if the
underlying regularities can be represented compactly in the format of hypotheses produced
by the learning algorithm, the data can be generalized well, even with a small set of training
examples. Otherwise, if the underlying regularities only have a large hypothesis, but the
algorithm is looking for compact ones (as per the Occam's Razor Principle), the hypotheses inferred will not be accurate. A learning algorithm that searches for hypotheses larger
than necessary (i.e., that does not use the Occam's Razor Principle) is normally \underconstrained"; it does not know, based on the training examples only, which of the many
competitive hypotheses of the large size should be inferred.
We also can describe the bias of a learning algorithm by looking at how training examples
of dierent classes are separated in the n-dimensional hyperspace where n is the number
of attributes. A decision node in a decision tree forms a hyperplane as described by a
linear function such as X = a. Not only are these hyperplanes perpendicular to the axis,
they are also partial-space hyperplanes that extend only within the subregion formed by
the hyperplanes of their parents' nodes. Likewise, hidden units with a threshold function in
ANNs can be viewed as forming hyperplanes in the hyperspace. However, unlike the ones
in the decision trees, they need not be perpendicular to any axis, and they are full-space
226

Learning the Past Tense: Symbolic vs Connectionist Models

hyperplanes that extend through the whole space. If ID3 is applied to the concepts that t
ANN's bias, especially if their hyperplanes are not perpendicular to any axis, then many
zigzag hyperplanes that are perpendicular to axes would be needed to separate dierent
classes of the examples. Hence, a large decision tree would be needed, but this does not t
ID3's bias. Similarly, if ANN learning algorithms are applied to the concepts that t ID3's
bias, especially if their hyperplanes form many separated, partial-space regions, then many
hidden units may be needed for these regions.
Another major dierence between ANNs and ID3 is that ANNs have a larger variation
and a weaker bias (cf. (Geman, Bienenstock, & Doursat, 1992)) than ID3. Many more
Boolean functions (e.g., linearly separable functions) can t a small network (e.g., one with
no hidden units) than they can a small decision tree. This is sometimes attributed to the
claimed versatility and exibility of ANNs; they can learn (but not necessarily predict reliably well) many functions, while symbolic methods are brittle. However, it is my belief that
we humans are versatile, not because we have a learning algorithm with a large variation,
but rather because we have a set of strong-biased learning algorithms, and we can somehow
search in the bias space and add new members into the set for the new learning tasks. Symbolic learning algorithms have clear semantic components and explicit representation, and
thus we can more easily construct strong-based algorithms motivated from various specic
learning tasks. The adaptive default strategy in the SPA is such an example. On the other
hand, we still largely do not know how to eectively strengthen the bias of ANNs for many
specic tasks (such as the identity mapping, k-term DNF, etc.). Some techniques (such
as adding copy connections and weight decaying) exist, but their exact eects on biasing
towards classes of functions are not clear.
From our analyses (Ling & Marinov, 1993), the underlying regularities governing the
inection of the past tense of English verbs do form a small set of production rules with
phoneme letters. This is especially so for regular verbs; all the rules are either identity
rules or the sux-adding rules. For example, decision trees can be converted into a set of
precedence-ordered production rules with more complicated rules (rules with more conditions) listed rst. As an example, using consecutive, left-to-right phonetic representation, a
typical sux-adding rule for verb stems with 4 phoneme letters (such as talk | talked) is:
If 4 = k and 5 = , then !5 = t
That is, if the fourth input phoneme is k and the fth is blank (i.e., if we are at a verb
ending) then the fth output phoneme is t. On the other hand, the identity-mapping rules
have only one condition. A typical identity rule looks like:
If 3 = l, then !3 = l
In fact, the passthrough default strategy allows all of the identity-mapping rules to be represented in a simple rst-order format:
If 3 = X, then !3 = X
where X can be any phoneme. Clearly, the knowledge of forming regular past tenses can
thus be expressed in simple, conjunctive rules which t the bias of the SPA (ID3), and
therefore, the SPA has a much better generalization ability than the ANN models.
To conclude, we have demonstrated, via extensive head-to-head comparisons, that the
SPA has a more realistic and better generalization capacity than ANNs on learning the
past tense of English verbs. We have argued that symbolic decision-tree/production-rule
learning algorithms should outperform ANNs. This is because, rst, the domain seems to be
227

Ling

governed by a compact set of rules, and thus ts the bias of our symbolic learning algorithm;
second, the SPA directly manipulates on a representation better than ANNs do (i.e., the
symbolic phoneme letters vs. the distributed representation); and third, the SPA is able
to derive high-level concepts used throughout English morphology. Our results support the
view that many such high-level, rule-governed cognitive tasks should be better modeled by
symbolic, rather than connectionist, systems.

Acknowledgements
I gratefully thank Steve Pinker for his constant encouragement, and Marin Marinov, Steve
Cherwenka and Huaqing Zeng for discussions and for help in implementing the SPA. I
thank Brian MacWhinney for providing the verb data used in his simulation. Discussions
with Tom Dietterich, Dave Touretzky and Brian MacWhinney, as well as comments from
reviewers, have been very helpful. The research is conducted with support from the NSERC
Research Grant and computing facilities from our Department.

References

Cottrell, G., & Plunkett, K. (1991). Using a recurrent net to learn the past tense. In
Proceedings of the Cognitive Science Society Conference.
Daugherty, K., & Seidenberg, M. (1993). Beyond rules and exceptions: A connectionist
modeling approach to inectional morphology. In Lima, S. (Ed.), The Reality of
Linguistic Rules. John Benjamins.
Dietterich, T., & Bakiri, G. (1991). Error-correcting output codes: A general method for
improving multiclass inductive learning programs. In AAAI-91 (Proceedings of Ninth
National Conference on Articial Intelligence).
Dietterich, T., Hild, H., & Bakiri, G. (1990). A comparative study of ID3 and backpropagation for English text-to-speech mapping. In Proceedings of the 7th International
Conference on Machine Learning. Morgan Kaufmann.
Feng, C., King, R., Sutherland, A., & Henery, R. (1992). Comparison of symbolic, statistical and neural network classiers. Manuscript. Department of Computer Science,
University of Ottawa.
Fodor, J., & Pylyshyn, Z. (1988). Connectionism and cognitive architecture: A critical
analysis. In Pinker, S., & Mehler, J. (Eds.), Connections and Symbols, pp. 3 { 71.
Cambridge, MA: MIT Press.
Geman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks and the bias/variance
dilemma. Neural Computation, 4, 1 { 58.
Lachter, J., & Bever, T. (1988). The relation between linguistic structure and associative
theories of language learning { a constructive critique of some connectionist learning
models. In Pinker, S., & Mehler, J. (Eds.), Connections and Symbols, pp. 195 { 247.
Cambridge, MA: MIT Press.
228

Learning the Past Tense: Symbolic vs Connectionist Models

Ling, X., Cherwenka, S., & Marinov, M. (1993). A symbolic model for learning the past
tenses of English verbs. In Proceedings of IJCAI-93 (Thirteenth International Conference on Articial Intelligence), pp. 1143{1149. Morgan Kaufmann Publishers.
Ling, X., & Marinov, M. (1993). Answering the connectionist challenge: a symbolic model
of learning the past tense of English verbs. Cognition, 49 (3), 235{290.
MacWhinney, B. (1990). The CHILDES Project: Tools for Analyzing Talk. Hillsdale, NJ:
Erlbaum.
MacWhinney, B. (1993). Connections and symbols: closing the gap. Cognition, 49 (3),
291{296.
MacWhinney, B., & Leinbach, J. (1991). Implementations are not conceptualizations: Revising the verb model. Cognition, 40, 121 { 157.
Pinker, S. (1991). Rules of language. Science, 253, 530 { 535.
Pinker, S., & Prince, A. (1988). On language and connectionism: Analysis of a parallel
distributed processing model of language acquisition. In Pinker, S., & Mehler, J.
(Eds.), Connections and Symbols, pp. 73 { 193. Cambridge, MA: MIT Press.
Plunkett, K., & Marchman, V. (1991). U-shaped learning and frequency eects in a multilayered perceptron: Implications for child language acquisition. Cognition, 38, 43 {
102.
Prasada, S., & Pinker, S. (1993). Generalization of regular and irregular morphological
patterns. Language and Cognitive Processes, 8 (1), 1 { 56.
Quinlan, J. (1986). Induction of decision trees. Machine Learning, 1 (1), 81 { 106.
Quinlan, J. (1993). C4.5 Programs for Machine Learning. Morgan Kaufmann: San Mateo,
CA.
Ripley, B. (1992). Statistical aspects of neural networks. Invited lectures for SemStat
(Seminaire Europeen de Statistique, Sandbjerg, Denmark, 25-30 April 1992).
Rumelhart, D., & McClelland, J. (1986). On learning the past tenses of English verbs.
In Rumelhart, D., McClelland, J., & the PDP Research Group (Eds.), Parallel Distributed Processing Vol 2, pp. 216 { 271. Cambridge, MA: MIT Press.
Shavlik, J., Mooney, R., & Towell, G. (1991). Symbolic and neural learning algorithms: An
experimental comparison. Machine Learning, 6 (2), 111 { 144.
Weiss, S., & Kulikowski, C. (1991). Computer Systems that Learn: classication and prediction methods from statistics, neural networks, machine learning, and expert systems.
Morgan Kaufmann, San Mateo, CA.

229

Journal of Articial Intelligence Research 1 (1993) 47-59

Submitted 6/93; published 9/93

An Empirical Analysis of Search in GSAT
Ian P. Gent

I.P.Gent@edinburgh.ac.uk

Toby Walsh

walsh@loria.fr

Department of Articial Intelligence, University of Edinburgh
80 South Bridge, Edinburgh EH1 1HN, United Kingdom
INRIA-Lorraine, 615, rue du Jardin Botanique,
54602 Villers-les-Nancy, France

Abstract

We describe an extensive study of search in GSAT, an approximation procedure for
propositional satisability. GSAT performs greedy hill-climbing on the number of satised
clauses in a truth assignment. Our experiments provide a more complete picture of GSAT's
search than previous accounts. We describe in detail the two phases of search: rapid hillclimbing followed by a long plateau search. We demonstrate that when applied to randomly
generated 3-SAT problems, there is a very simple scaling with problem size for both the
mean number of satised clauses and the mean branching rate. Our results allow us to
make detailed numerical conjectures about the length of the hill-climbingphase, the average
gradient of this phase, and to conjecture that both the average score and average branching
rate decay exponentially during plateau search. We end by showing how these results can
be used to direct future theoretical analysis. This work provides a case study of how
computer experiments can be used to improve understanding of the theoretical properties
of algorithms.

1. Introduction

Mathematicians are increasingly recognizing the usefulness of experiments with computers
to help advance mathematical theory. It is surprising therefore that one area of mathematics
which has benetted little from empirical results is the theory of algorithms, especially those
used in AI. Since the objects of this theory are abstract descriptions of computer programs,
we should in principle be able to reason about programs entirely deductively. However,
such theoretical analysis is often too complex for our current mathematical tools. Where
theoretical analysis is practical, it is often limited to (unrealistically) simple cases. For
example, results presented in (Koutsoupias & Papadimitriou, 1992) for the greedy algorithm
for satisability do not apply to interesting and hard region of problems as described in x3.
In addition, actual behaviour on real problems is sometimes quite dierent to worst and
average case analyses. We therefore support the calls of McGeoch (McGeoch, 1986), Hooker
(Hooker, 1993) and others for the development of an empirical science of algorithms. In
such a science, experiments as well as theory are used to advance our understanding of
the properties of algorithms. One of the aims of this paper is to demonstrate the benets
of such an empirical approach. We will present some surprising experimental results and
demonstrate how such results can direct future eorts for a theoretical analysis.
The algorithm studied in this paper is GSAT, a randomized hill-climbing procedure for
propositional satisability (or SAT) (Selman, Levesque, & Mitchell, 1992; Selman & Kautz,
1993a). Propositional satisability is the problem of deciding if there is an assignment for
c 1993 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Gent & Walsh

the variables in a propositional formula that makes the formula true. Recently, there has
been considerable interest in GSAT as it appears to be able to solve large and dicult satisability problems beyond the range of conventional procedures like Davis-Putnam (Selman
et al., 1992). We believe that the results we give here will actually apply to a larger family
of procedures for satisability called GenSAT (Gent & Walsh, 1993). Understanding such
procedures more fully is of considerable practical interest since SAT is, in many ways, the
archetypical (and intractable) NP-hard problem. In addition, many AI problems can be
encoded quite naturally in SAT (eg. constraint satisfaction, diagnosis and vision interpretation, refutational theorem proving, planning).
This paper is structured as follows. In x2 we introduce GSAT, the algorithm studied
in the rest of the paper. In x3 we dene and motivate the choice of problems used in our
experiments. The experiments themselves are described in x4. These experiments provide
a more complete picture of GSAT's search than previous informal accounts. The results
of these experiments are analysed more closely in x5 using some powerful statistical tools.
This analysis allow us to make various experimentally veriable conjectures about GSAT's
search. For example, we are able to conjecture: the length of GSAT's initial hill-climbing
phase; the average gradient of this phase; the simple scaling of various important features
like the score (on which hill-climbing is performed) and the branching rate. In x6 we suggest
how such results can be used to direct future theoretical analysis. Finally, in x7 we describe
related work and end with some brief conclusions in x8.

2. GSAT

GSAT is a random greedy hill-climbing procedure. GSAT deals with formulae in conjunct-

ive normal form (CNF); a formula,  is in CNF i it is a conjunction of clauses, where a
clause is a disjunction of literals. GSAT starts with a randomly generated truth assignment,
then hill-climbs by \ipping" the variable assignment which gives the largest increase in
the number of clauses satised (called the \score" from now on). Given the choice between
several equally good ips, GSAT picks one at random. If no ip can increase the score,
then a variable is ipped which does not change the score or (failing that) which decreases
the score the least. Thus GSAT starts in a random part of the search space and searches
for a global solution using only local information. Despite its simplicity, this procedure has
been shown to give good performance on hard satisability problems (Selman et al., 1992).
procedure GSAT()
for i := 1 to Max-tries
T := random truth assignment
for j := 1 to Max-ips
if T satises  then return T
else Poss-ips := set of vars which increase satisability most
V := a random element of Poss-ips
T := T with V's truth assignment ipped
end
end
return

\no satisfying assignment found"
48

An Empirical Analysis of Search in GSAT

In (Gent & Walsh, 1993) we describe a large number of experiments which suggest
that neither greediness not randomness is important for the performance of this procedure.
These experiments also suggest various other conjectures. For instance, for random 3-SAT
problems (see x3) the log of the runtime appears to scale with a less than linear dependency
on the problem size. Conjectures such as these could, as we noted in the introduction,
be very protably used to direct future eorts to analyse GSAT theoretically. Indeed,
we believe that the experiments reported here suggest various conjectures which would be
useful in a proof of the relationship between runtime and problem size (see x6 for more
details)

3. Problem Space
To be able to perform experiments on an algorithm, you need a source of problems on which
to run the algorithm. Ideally the problems should come from a probability distribution
with some well-dened properties, contain a few simple parameters and be representative of
problems which occur in real situations. Unfortunately, it is often dicult to meet all these
criteria. In practice, one is usually forced to accept either problems from a well-dened
distribution with a few simple parameters or a benchmark set of real problems, necessarily
from some unknown distribution. In these experiments we adopt the former approach and
use CNF formulae randomly generated according to the random k-SAT model.
Problems in random k-SAT with N variables and L clauses are generated as follows:
a random subset of size k of the N variables is selected for each clause, and each variable is made positive or negative with probability 21 . For random 3-SAT, there is a phase
transition from satisable to unsatisable when L is approximately 4.3N (Mitchell, Selman,
& Levesque, 1992; Larrabee & Tsuji, 1992; Crawford & Auton, 1993). At lower L, most
problems generated are under-constrained and are thus satisable; at higher L, most problems generated are over-constrained and are thus unsatisable. As with many NP-complete
problems, problems in the phase transition are typically much more dicult to solve than
problems away from the transition (Cheeseman, Kanefsky, & Taylor, 1991). The region
L=4.3N is thus generally considered to be a good source of hard SAT problems and has
been the focus of much recent experimental eort.

4. GSAT's search

When GSAT was rst introduced, it was noted that search in each try is divided into two
phases. In the rst phase of a try, each ip increases the score. However, this phase is
relatively short and is followed by a second phase in which most ips do not increase the
score, but are instead sideways moves which leave the same number of clauses satised.
This phase is a search of a \plateau" for the occasional ip that can increase the score.1
One of the aims of this paper is to improve upon such informal observations by making
quantitative measurements of GSAT's search, and by using these measurements to make
several experimentally testable predictions.
1. Informal observations to this eect were made by Bart Selman during the presentation of (Selman et al.,
1992) at AAAI-92. These observations were enlarged upon in (Gent & Walsh, 1992).

49

Gent & Walsh

In our experiments, we followed three methodological principles from (McGeoch, 1986).
First, we performed experiments with large problem sizes and many repetitions, to reduce
variance and allow for emergent properties. Second, we sought good views of the data. That
is, we looked for features of performance which are meaningful and which are as predictable
as possible. Third, we analysed our results closely. Suitable analysis of data may show
features which are not clear from a simple presentation. In the rest of this paper we show
how these principles enabled us to make very detailed conjectures about GSAT's search.
Many features of GSAT's search space can be graphically illustrated by plotting how
they vary during a try. The most obvious feature to plot is the score, the number of satised
clauses. In our quest for a good view of GSAT's search space, we also decided to plot \possips" at each ip: that is, the number of equally good ips between which GSAT randomly
picks. This is an interesting measure since it indicates the branching rate of GSAT's search
space.
We begin with one try of GSAT on a 500 variable random 3-SAT problem in the
dicult region of L = 4.3N (Figure 1a). Although there is considerable variation between
tries, this graph illustrates features common to all tries. Both score (in Figure 1a) and
poss-ips (in Figure 1b) are plotted as percentages of their maximal values, that is L and N
respectively. The percentage score starts just above 87.5%, which might seem surprisingly
high.
Theoretically, however, we expect a random truth assignment in k-SAT to satisfy
2k ,1 of all clauses (in this instance, 7 ). As expected from the earlier informal description,
8
2k
the score climbs rapidly at rst, and then attens o as we mount the plateau. The graph
is discrete since positive moves increase the score by a xed amount, but some of this
discreteness is lost due to the small scale. To illustrate the discreteness, in Figure 1b we
plot the change in the number of satised clauses made by each ip (as its exact value,
unscaled). Note that the x-axis for both plots in Figure 1b is the same.
Change in score

Percentage score
100

6
5

97.5

4
3

95

2
1

92.5

0

% poss-flips

-1
90

20
10

87.5

0
0

40

80

120

160

200

240
flips

0

40

80

120

160

200

240
flips

(a) Score
(b) Change in score, and poss-ips
Figure 1: GSAT's behaviour during one try, N = 500, L = 2150, rst 250 ips
The behaviour of poss-ips is considerably more complicated than that of the score. It
is easiest rst to consider poss-ips once on the plateau. The start of plateau search, after
115 ips, coincides with a very large increase in poss-ips, corresponding to a change from
50

An Empirical Analysis of Search in GSAT

the region where a small number of ips can increase the score by 1 to a region where a
large number of ips can be made which leave the score unchanged. Once on the plateau,
there are several sharp dips in poss-ips. These correspond to ips where an increase by 1
in the score was eected, as can be seen from Figure 1b. It seems that if you can increase
the score on the plateau, you only have a very small number of ways to do it. Also, the
dominance of ips which make no change in score graphically illustrates the need for such
\sideways" ips, a need that has been noted before (Selman et al., 1992; Gent & Walsh,
1993).
Perhaps the most fascinating feature is the initial behaviour of poss-ips. There are
four well dened wedges starting at 5, 16, 26, and 57 ips, with occasional sharp dips.
These wedges demonstrate behaviour analogous to the that of poss-ips on the plateau.
The plateau spans the region where ips typically do not change the score: we call this
region H0 since hill-climbing typically makes zero change to the score. The last wedge
spans the region H1 where hill-climbing typically increases the score by 1, as can be seen
very clearly from Figure 1b. Again Figure 1b shows that the next three wedges (reading
right to left) span regions H2, H3 , and H4 . As with the transition onto the plateau, the
transition between each region is marked by a sharp increase in poss-ips. Dips in the
wedges represent unusual ips which increase the score by more than the characteristic
value for that region, just as the dips in poss-ips on the plateau represent ips where an
increase in score was possible. This exact correlation can be seen clearly in Figure 1b. Note
that in this experiment, in no region H did a change in score of j + 2 occur, and that there
was no change in score of ,1 at all. In addition, each wedge in poss-ips appears to decay
close to linearly. This is explained by the facts that once a variable is ipped it no longer
appears in poss-ips (ipping it back would decrease score), that most of the variables in
poss-ips can be ipped independently of each other, and that new variables are rarely
added to poss-ips as a consequence of an earlier ip. On the plateau, however, when a
variable is ipped which does not change the score, it remains in poss-ips since ipping it
back also does not change the score.
To determine if this behaviour is typical, we generated 500 random 3-SAT problems
with N=500 and L=4.3N, and ran 10 tries of GSAT on each problem. Figure 2a shows the
mean percentage score2 while Figure 2b presents the mean percentage poss-ips together
with the mean change in score at each ip. (The small discreteness in this gure is due to
the discreteness of Postscript's plotting.)
The average percentage score is very similar to the behaviour on the individual run of
Figure 1, naturally being somewhat smoother. The graph of average poss-ips seems quite
dierent, but it is to be expected that you will neither observe the sharply dened dips
in poss-ips from Figure 1b, nor the very sharply dened start to the wedges, since these
happen at varying times. It is remarkable that the wedges are consistent enough to be
visible when averaged over 5,000 tries; the smoothing in the wedges and the start of the
plateau is caused by the regions not starting at exactly the same time in each try.
Figure 2 does not distinguish between satisable and unsatisable problems. There
is no current technique for determining the satisability of 500 variable 3-SAT problems
in feasible time. From instances we have been able to test, we do not believe that large
j

2. In this paper we assign a score of 100% to ips which were not performed because a satisfying truth
assignment had already been found.

51

Gent & Walsh

Mean percentage score

Mean change in score

100

6
5

97.5

4
3

95

2
1

92.5

0
Mean percent poss-flips

90

-1

20
10

87.5

0
0

40

80

120

160

200

240
flips

0

40

80

120

160

200

240
flips

(a) Mean score
(b) Mean change in score, poss-ips
Figure 2: Mean GSAT behaviour, N = 500, L = 4.3N, rst 250 ips

dierences from Figure 2 will be seen when it is possible to plot satisable and unsatisable
problems separately, but this remains an interesting topic to investigate in the future.
Experiments with other values of N with the same ratio of clauses to variables demonstrated qualitatively similar behaviour. More careful analysis shows the remarkable fact that
not only is the behaviour qualitatively similar, but quantitatively similar, with a simple linear dependency on N. If graphs similar to Figure 2 are plotted for each N with the x-axis
scaled by N, behaviour is almost identical. To illustrate this, Figure 3 shows the mean
percentage score, percentage poss-ips, and change in score, for N = 500, 750, and 1000, for
L = 4.3N and for the rst 0.5N ips (250 ips at N = 500). Both Figure 3a and Figure 3b
demonstrate the closeness of the scaling, to the extent that they may appear to contain just
one thick line. In Figure 3b there is a slight tendency for the dierent regions of hill-climbing
to become better dened with increasing N.
The gures we have presented only reach a very early stage of plateau search. To
investigate further along the plateau, we performed experiments with 100, 200, 300, 400,
and 500 variables from 0 to 2.5N ips.3 In Figure 4a shows the mean percentage score in
each case, while Figure 4b shows the mean percentage poss-ips, magnied on the y -axis
for clarity. Both these gures demonstrate the closeness of the scaling on the plateau. In
Figure 4b the graphs are not quite so close together as in Figure 4a. The phases of hillclimbing become much better dened with increasing N. During plateau search, although
separate lines are distinguishable, the dierence is always considerably less than 1% of the
total number of variables.
The problems used in these experiments (random 3-SAT with L=4.3N) are believed to
be unusually hard and are satisable with probability approximately 12 . Neither of these
facts appears to be relevant to the scaling of GSAT's search. To check this we performed
a similar range of experiments with a ratio of clauses to variables of 6. Although almost all
such problems are unsatisable, we observed exactly the same scaling behaviour. The score
3. At 100 variables, 2.5N ips is close to the optimal value for Max-ips. However, experiments have
suggested that Max-ips may need to vary quadratically for larger N (Gent & Walsh, 1993).

52

An Empirical Analysis of Search in GSAT

Mean change in score

Mean percentage score
100

6
5

97.5

4
3

95

2
1

92.5

0
Mean percent poss-flips

90

-1

20
10

87.5

0
0

0.08N 0.16N 0.24N 0.32N 0.40N 0.48N
flips

0.08N 0.16N 0.24N 0.32N 0.40N 0.48N
flips

0

(a) Mean score
(b) Mean change in score, poss-ips
Figure 3: Scaling of mean GSAT behaviour, N = 500, 750, 1000, rst 0.5N ips
Mean percentage score

Mean percentage poss-flips

100

12.5

97.5

10

95

7.5

92.5

5

90

2.5

87.5

0
0

0.4N

0.8N

1.2N

1.6N

2.0N

2.4N
flips

0

0.4N

0.8N

1.2N

1.6N

2.0N

2.4N
flips

(a) mean score, L = 4.3N
(b) mean poss-ips, L = 4.3N
Figure 4: Scaling of mean GSAT behaviour, N = 100, 200, 300, 400, 500

does not reach such a high value as in Figure 4a, as is to be expected, but nevertheless shows
the same linear scaling. On the plateau, the mean value of poss-ips is lower than before.
We again observed this behaviour for L = 3N, where almost all problems are satisable.
The score approaches 100% faster than before, and a higher value of poss-ips is reached
on the plateau, but the decay in the value of poss-ips seen in Figure 4b does not seem to
be present.
To summarise, we have shown that GSAT's hill-climbing goes through several distinct
phases, and that the average behaviour of certain important features scale in linear fashion
with N. These results provide a considerable advance on previous informal descriptions of
GSAT's search.
53

Gent & Walsh

5. Numerical Conjectures
In this section, we will show that detailed numerical conjectures can be made if the data
presented graphically in x4 is analysed numerically. We divide our analysis into two parts:
rst we deal with the plateau search, where behaviour is relatively simple, then we analyse
the hill-climbing search.
On the plateau, both average score and poss-ips seem to decay exponentially with a
simple linear dependency on problem size. To test this, we performed regression analysis
on our experimental data, using the models

S (x) = N  (B , C  e, AxN )
P (x) = N  (E + F  e, DxN )




(1)
(2)

where x represents the number of ips, S (x) the average score at ip x and P (x) the average
number of possible ips. To determine GSAT's behaviour just on the plateau, we analysed
data on mean score, starting from 0.4N ips, a time when plateau search always appears to
have started (see x5). Our experimental data tted the model very well. Detailed results for
N = 500 are given in Table 1 to three signicant gures. The values of A, B, and C change
only slightly with N, providing further evidence for the scaling of GSAT's behaviour. For L
= 3N the asymptotic mean percentage score is very close to 100% of clauses being satised,
while for L = 4.3N it is approximately 99.3% of clauses and for L = 6N it is approximately
98.2% of clauses. A good t was also found for mean poss-ips behaviour (see Table 2 for
N = 500), except for L = 3N, where the mean value of poss-ips on the plateau may be
constant. It seems that for L = 4.3N the asymptotic value of poss-ips is about 10% of N
and that for 6 it is about 5% of N.
It is important to note that the behaviour we analysed was for mean behaviour over
both satisable and unsatisable problems. It is likely that individual problems will exhibit
similar behaviour with dierent asymptotes, but we do not expect even satisable problems
to yield a mean score of 100% asymptotically. Note that as N increases a small error in
percentage terms may correspond to a large error in the actual score. As a result, our
predictions of asymptotic score may be inaccurate for large N, or for very large numbers of
ips. Further experimentation is necessary to examine these issues in detail.
L/N N
A
B
C
R2
3 500 0.511 2.997 0.0428 0.995
4.3 500 0.566 4.27 0.0772 0.995
6 500 0.492 5.89 0.112 0.993
Table 1: Regression results for average score of GSAT.4
4. The value of R2 is a number in the interval [0; 1] indicating how well the variance in data is explained by
the regression formula. 1 , R2 is the ratio between variance of the data from its predicted value, and the
variance of the data from the mean of all the data. A value of R2 close to 1 indicates that the regression
formula ts the data very well.

54

An Empirical Analysis of Search in GSAT

L/N N
D
E
F
R2
4.3 500 0.838 0.100 0.0348 0.996
6 500 0.789 0.0502 0.0373 0.999
Table 2: Regression results on average poss-ips of GSAT.
We have also analysed GSAT's behaviour during its hill-climbing phase. Figure 1b
shows regions where most ips increase the score by 4, then by 3, then by 2, then by 1.
Analysis of our data suggested that each phase lasts roughly twice the length of the previous
one. This motivates the following conjectures: GSAT moves through a sequence of regions
H for j = :::; 3; 2; 1 in which the majority of ips increase the score by j , and where the
length of each region H is proportional to 2, (except for the region H0 which represents
plateau search).
To investigate this conjecture, we analysed 50 tries each on 20 dierent problems for
random 3-SAT problems at N=500 and L=4.3N. We very rarely observe ips in H that
increase the score by less than j , and so dene H as the region between the rst ip which
increases the score by exactly j and the rst ip which increases the score by less than
j (unless the latter actually appears before the former, in which case H is empty). One
simple test of our conjecture is to compare the total time spent in H with the total time up
to the end of H ; we predict that this ratio will be 12 . For j = 1 to 4 the mean and standard
deviations of this ratio, and the length of each region are shown in Table 3.5 This data
supports our conjecture although as j increases each region is slightly longer than predicted.
The total length of hill-climbing at N=500 is 0.22N ips, while at N=100 it is 0.23N. This
is consistent with the scaling behaviour observed in x4.
j

j

j

j

j

j

j

j

Region
mean ratio
All climbing
|
H1
0.486
H2
0.513
H3
0.564
H4
0.574

s.d. mean length
|
112
0.0510
54.7
0.0672
29.5
0.0959
15.7
0.0161
7.00

s.d.
7.59
7.69
5.12
3.61
2.48

Table 3: Comparative and Absolute Lengths of hill-climbing phases
Our conjecture has an appealing corollary. Namely, that if there are i non-empty hillclimbing regions, the average change in score per ip during hill-climbing is:
1  1 + 1  2 + 1  3 + 1  4 +    + 1  i  2:
(3)
2
4
8
16
2
It follows from this that mean gradient of the entire hill-climbing phase is approximately 2.
At N=500, we observed a mean ratio of change in score per ip during hill-climbing of 1.94
i

5. The data for \All climbing" is the length to the start of H0 .

55

Gent & Walsh

with a standard deviation of 0.1. At N=100, the ratio is 1.95 with a standard deviation of
0.2.
The model presented above ignores ips in H which increase the score by more than
j . Such ips were seen in Figure 1b in regions H3 to H1. In our experiment 9.8% of ips
in H1 were of size 2 and 6.3% of ips in H2 were of size 3. However, ips of size j + 2
were very rare, forming only about 0.02% of all ips in H1 and H2 . We conjectured that
an exponential decay similar to that in H0 occurs in each H . That is, we conjecture that
the average change in number of satised clauses from ip x to ip x + 1 in H is given by:
j

j

j

j+E

x
 e, Dj N

(4)
This might correspond to a model of GSAT's search in which there are a certain number
of ips of size j + 1 in each region H , and the probability of making a j + 1 ip is merely
dependent on the number of such ips left; the rest of the time, GSAT is obliged to make
a ip of size j . Our data from 1000 tries tted this model well, giving values of R2 of 96.8%
for H1 and 97.5% for H2 . The regression gave estimates for the parameters of: D1 = 0:045,
E1 = 0:25, D2 = 0:025, E2 = 0:15. Not surprisingly, since the region H3 is very short,
data was too noisy to obtain a better t with the model (4) than with one of linear decay.
These results support our conjecture, but more experiments on larger problems are needed
to lengthen the region H for j  3.
j



j

j

6. Theoretical Conjectures

Empirical results like those given in x5 can be used to direct eorts to analyse algorithms
theoretically. For example, consider the plateau region of GSAT's search. If the model (1)
applies also to successful tries, the asymptotic score is L, giving
S (x) = L , C  N  e, AxN
Dierentiating with respect to x we get,
dS (x) = C  e, axN = L , S (x)
dx
A
AN
The gradient is a good approximation for D , the average size of a ip at x. Hence,
D = L A, SN(x)
Our experiments suggest that downward ips and those of more than +1 are very rare on the
plateau. Thus, a good (rst order) approximation for D is as follows, where prob(D = j )
is the probability that a ip at x is of size j .




x

x

x

XL

D =
x

j

Hence,

=,L

x

j  prob(D = j ) = prob(D = 1)
x

prob(D = 1) = L A, SN(x)
x

56

x

An Empirical Analysis of Search in GSAT

That is, on the plateau the probability of making a ip of size +1 may be directly proportional to L , S (x), the average number of clauses remaining unsatised and inversely
proportional N, to the number of variables. A similar analysis and result can be given for
prob(D = j +1) in the hill-climbing region H , which would explain the model (4) proposed
in x5.
If our theoretical conjecture is correct, it can be used to show that the mean number
of ips on successful tries will be proportional to N ln N. Further investigation, both experimental and theoretical, will be needed to determine the accuracy of this prediction.
Our conjectures in this section should be seen as conjectures as to what a formal theory
of GSAT's search might look like, and should be useful in determining results such as average runtime and the optimal setting for a parameter like Max-ips. In addition, if we
can develop a model of GSAT's search in which prob(D = j ) is related to the number
of unsatised clauses and N as in the above equation, then the experimentally observed
exponential behaviour and linear scaling of the score will follow immediately.
x

j

x

7. Related Work
Prior to the introduction of GSAT in (Selman et al., 1992), a closely related set of procedures were studied by Gu (Gu, 1992). These procedures have a dierent control structure
to GSAT which allows them, for instance, to make sideways moves when upwards moves
are possible. This makes it dicult to compare our results directly. Nevertheless, we are
condent that the approach taken here would apply equally well to these procedures, and
that similar results could be expected. Another \greedy algorithm for satisability" has
been analysed in (Koutsoupias & Papadimitriou, 1992), but our results are not directly
applicable to it because, unlike GSAT, it disallows sideways ips.
In (Gent & Walsh, 1993) we describe an empirical study of GenSAT, a family of procedures related to GSAT. This study focuses on the importance of randomness, greediness
and hill-climbing for the eectiveness of these procedures. In addition, we determine how
performance depends on parameters like Max-tries and Max-ips. We showed also that
certain variants of GenSAT could outperform GSAT on random problems. It would be
very interesting to perform a similar analysis to that given here of these closely related
procedures.
GSAT is closely related to simulated annealing (van Laarhoven & Aarts, 1987) and
the Metropolis algorithm, which both use greedy local search with a randomised method of
allowing non-optimal ips. Theoretical work on these algorithms has not applied to SAT
problems, for example (Jerrum, 1992; Jerrum & Sorkin, 1993), while experimental studies of
the relationship between GSAT and simulated annealing have as yet only reached tentative
conclusions (Selman & Kautz, 1993b; Spears, 1993).
Procedures like GSAT have also been successfully applied to constraint satisfaction
problems other than satisability. For example, (Minton, Johnston, Philips, & Laird, 1990)
proposed a greedy local search procedure which performed well scheduling observations on
the Hubble Space Telescope, and other constraint problems like the million-queens, and
3-colourability. It would be very interesting to see how the results given here map across
to these new problem domains.
57

Gent & Walsh

8. Conclusions

We have described an empirical study of search in GSAT, an approximation procedure for
satisability. We performed detailed analysis of the two basic phases of GSAT's search,
an initial period of fast hill-climbing followed by a longer period of plateau search. We
have shown that the hill-climbing phases can be broken down further into a number of
distinct phases each corresponding to progressively slower climbing, and each phase lasting
twice as long as the last. We have also shown that, in certain well dened problem classes,
the average behaviour of certain important features of GSAT's search (the average score
and the average branching rate at a given point) scale in a remarkably simple way with
the problem size We have also demonstrated that the behaviour of these features can be
modelled very well by simple exponential decay, both in the plateau and in the hill-climbing
phase. Finally, we used our experiments to conjecture various properties (eg. the probability
of making a ip of a certain size) that will be useful in a theoretical analysis of GSAT. These
results illustrate how carefully performed experiments can be used to guide theory, and how
computers have an increasingly important r^ole to play in the analysis of algorithms.

Acknowledgements
This research is supported by a SERC Postdoctoral Fellowship to the rst author and
a HCM Postdoctoral fellowship to the second. We thank Alan Bundy, Ian Green, and
the members of the Mathematical Reasoning Group for their constructive comments and
for the quadrillion CPU cycles donated to these and other experiments from SERC grant
GR/H/23610. We also thank Andrew Bremner, Judith Underwood, and the reviewers of
this journal for other help.

References
Cheeseman, P., Kanefsky, B., & Taylor, W. (1991). Where the really hard problems are.
In Proceedings of the 12th IJCAI, pp. 163{169. International Joint Conference on
Articial Intelligence.
Crawford, J., & Auton, L. (1993). Experimental results on the crossover point in satisability problems. In Proceedings of the Eleventh National Conference on Articial
Intelligence, pp. 21{27. AAAI Press/The MIT Press.
Gent, I. P., & Walsh, T. (1993). Towards an Understanding of Hill-climbing Procedures for
SAT. In Proceedings of the Eleventh National Conference on Articial Intelligence,
pp. 28{33. AAAI Press/The MIT Press.
Gent, I. P., & Walsh, T. (1992). The enigma of SAT hill-climbing procedures. Research
paper 605, Dept. of Articial Intelligence, University of Edinburgh.
Gu, J. (1992). Ecient local search for very large-scale satisability problems. SIGART
Bulletin, 3 (1).
58

An Empirical Analysis of Search in GSAT

Hooker, J. N. (1993). Needed: An empirical science of algorithms. Tech. rep., Graduate
School of Industrial Administration, Carnegie Mellon University, Pittsburgh PA.
Jerrum, M. (1992). Large cliques elude the Metropolis process. Random Structures and
Algorithms, 3 (4), 347{359.
Jerrum, M., & Sorkin, G. (1993). Simulated annealing for graph bisection. Tech. rep.
ECS-LFCS-93-260, Department of Computer Science, University of Edinburgh.
Koutsoupias, E., & Papadimitriou, C. H. (1992). On the greedy algorithm for satisability.
Information Processing Letters, 43, 53{55.
Larrabee, T., & Tsuji, Y. (1992). Evidence for a Satisability Threshold for Random 3CNF
Formulas. Tech. rep. UCSC-CRL-92-42, Baskin Center for Computer Engineering and
Information Sciences, University of California, Santa Cruz.
McGeoch, C. (1986). Experimental Analysis of Algorithms. Ph.D. thesis, Carnegie Mellon
University. Also available as CMU-CS-87-124.
Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1990). Solving large-scale constraint satisfaction and scheduling problems using a heuristic repair method. In
AAAI-90, Proceedings Eighth National Conference on Articial Intelligence, pp. 17{
24. AAAI Press/MIT Press.
Mitchell, D., Selman, B., & Levesque, H. (1992). Hard and easy distributions of SAT
problems. In Proceedings, 10th National Conference on Articial Intelligence. AAAI
Press/The MIT Press.
Selman, B., & Kautz, H. (1993a). Domain-independent extensions to GSAT: Solving large
structured satisability problems. In Proceedings, IJCAI-93. International Joint Conference on Articial Intelligence.
Selman, B., & Kautz, H. (1993b). An empirical study of greedy local search for satisability
testing. In Proceedings of the Eleventh National Conference on Articial Intelligence,
pp. 46{51. AAAI Press/The MIT Press.
Selman, B., Levesque, H., & Mitchell, D. (1992). A new method for solving hard satisability
problems. In Proceedings, 10th National Conference on Articial Intelligence. AAAI
Press/The MIT Press.
Spears, W. M. (1993). Simulated annealing for hard satisability problems. Tech. rep.
AIC-93-015, AI Center, Naval Research Laboratory.
van Laarhoven, P., & Aarts, E. (1987). Simulated Annealing: Theory and Applications. D.
Reidel Publishing Company, Dordrecht, Holland.

59

Journal of Articial Intelligence Research 1 (1993) 109-138

Submitted 7/93; published 12/93

Decidable Reasoning in Terminological Knowledge
Representation Systems
Martin Buchheit

German Research Center for Articial Intelligence (DFKI)
Stuhlsatzenhausweg 3, D-66123 Saarbrucken, Germany

Francesco M. Donini
Andrea Schaerf

buchheit@dfki.uni-sb.de
donini@assi.dis.uniroma1.it
aschaerf@assi.dis.uniroma1.it

Dipartimento di Informatica e Sistemistica
Universita di Roma \La Sapienza", Via Salaria 113, I-00198 Roma, Italy

Abstract

Terminological knowledge representation systems (TKRSs) are tools for designing and
using knowledge bases that make use of terminological languages (or concept languages).
We analyze from a theoretical point of view a TKRS whose capabilities go beyond the
ones of presently available TKRSs. The new features studied, often required in practical
applications, can be summarized in three main points. First, we consider a highly expressive terminological language, called ALCNR, including general complements of concepts,
number restrictions and role conjunction. Second, we allow to express inclusion statements between general concepts, and terminological cycles as a particular case. Third, we
prove the decidability of a number of desirable TKRS-deduction services (like satisability,
subsumption and instance checking) through a sound, complete and terminating calculus
for reasoning in ALCNR-knowledge bases. Our calculus extends the general technique
of constraint systems. As a byproduct of the proof, we get also the result that inclusion
statements in ALCNR can be simulated by terminological cycles, if descriptive semantics
is adopted.

1. Introduction
A general characteristic of many proposed terminological knowledge representation systems
(TKRSs) such as krypton (Brachman, Pigman Gilbert, & Levesque, 1985), nikl (Kaczmarek, Bates, & Robins, 1986), back (Quantz & Kindermann, 1990), loom (MacGregor &
Bates, 1987), classic (Borgida, Brachman, McGuinness, & Alperin Resnick, 1989), kris
(Baader & Hollunder, 1991), k-rep (Mays, Dionne, & Weida, 1991), and others (see Rich,
editor, 1991; Woods & Schmolze, 1992), is that they are made up of two dierent components. Informally speaking, the rst is a general schema concerning the classes of individuals
to be represented, their general properties and mutual relationships, while the second is a
(partial) instantiation of this schema, containing assertions relating either individuals to
classes, or individuals to each other. This characteristic, which the mentioned proposals
inherit from the seminal TKRS kl-one (Brachman & Schmolze, 1985), is shared also by
several proposals of database models such as Abrial's (1974), candide (Beck, Gala, &
Navathe, 1989), and taxis (Mylopoulos, Bernstein, & Wong, 1980).
Retrieving information in actual knowledge bases (KBs) built up using one of these systems is a deductive process involving both the schema (TBox) and its instantiation (ABox).
c 1993 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Buchheit, Donini, & Schaerf

In fact, the TBox is not just a set of constraints on possible ABoxes, but contains intensional
information about classes. This information is taken into account when answering queries
to the KB.
During the realization and use of a KB, a TKRS should provide a mechanical solution
for at least the following problems (from this point on, we use the word concept to refer to
a class):
1. KB-satisability : are an ABox and a TBox consistent with each other? That is, does
the KB admit a model? A positive answer is useful in the validation phase, while the
negative answer can be used to make inferences in refutation-style. The latter will be
precisely the approach taken in this paper.
2. Concept Satisability : given a KB and a concept C , does there exist at least one
model of the KB assigning a non-empty extension to C ? This is important not only
to rule out meaningless concepts in the KB design phase, but also in processing the
user's queries, to eliminate parts of a query which cannot contribute to the answer.
3. Subsumption : given a KB and two concepts C and D, is C more general than D in
any model of the KB? Subsumption detects implicit dependencies among the concepts
in the KB.
4. Instance Checking : given a KB, an individual a and a concept C , is a an instance
of C in any model of the KB? Note that retrieving all individuals described by a
given concept (a query in the database lexicon) can be formulated as a set of parallel
instance checkings.
The above questions can be precisely characterized once the TKRS is given a semantics
(see next section), which denes models of the KB and gives a meaning to expressions
in the KB. Once the problems are formalized, one can start both a theoretical analysis
of them, and|maybe independently|a search for reasoning procedures accomplishing the
tasks. Completeness and correctness of procedures can be judged with respect to the formal
statements of the problems.
Up to now, all the proposed systems give incomplete procedures for solving the above
problems 1{4, except for kris1. That is, some inferences are missed, in some cases without
a precise semantical characterization of which ones are. If the designer or the user needs
(more) complete reasoning, she/he must either write programs in a suitable programming
language (as in the database proposal of Abrial, and in taxis), or dene appropriate inference rules completing the inference capabilities of the system (as in back, loom, and
classic). From the theoretical point of view, for several systems (e.g., loom) it is not even
known if complete procedures can ever exist|i.e., the decidability of the corresponding
problems is not known.
Recent research on the computational complexity of subsumption had an inuence in
many TKRSs on the choice for incomplete procedures. Brachman and Levesque (1984)
1. Also the system classic is complete, but only w.r.t. a non-standard semantics for the treatment of
individuals. Complete reasoning w.r.t. standard semantics for individuals is not provided, and is coNPhard (Lenzerini & Schaerf, 1991).

110

Decidable Reasoning in Terminological KR Systems

started this research analyzing the complexity of subsumption between pure concept expressions, abstracting from KBs (we call this problem later in the paper as pure subsumption). The motivation for focusing on such a small problem was that pure subsumption is
a fundamental inference in any TKRS. It turned out that pure subsumption is tractable
(i.e., worst-case polynomial-time solvable) for simple languages, and intractable for slight
extensions of such languages, as subsequent research denitely conrmed (Nebel, 1988;
Donini, Lenzerini, Nardi, & Nutt, 1991a, 1991b; Schmidt-Schau & Smolka, 1991; Donini,
Hollunder, Lenzerini, Marchetti Spaccamela, Nardi, & Nutt, 1992). Also, beyond computational complexity, pure subsumption was proved undecidable in the TKRSs U (Schild,
1988), kl-one (Schmidt-Schau, 1989) and nikl (Patel-Schneider, 1989).
Note that extending the language results in enhancing its expressiveness, therefore the
result of that research could be summarized as: The more a TKRS language is expressive,
the higher is the computational complexity of reasoning in that language|as Levesque
(1984) rst noted. This result has been interpreted in two dierent ways, leading to two
dierent TKRSs design philosophies:
1. `General-purpose languages for TKRSs are intractable, or even undecidable, and
tractable languages are not expressive enough to be of practical interest'. Following this interpretation, in several TKRSs (such as nikl, loom and back) incomplete
procedures for pure subsumption are considered satisfactory (e.g., see (MacGregor &
Brill, 1992) for loom). Once completeness is abandoned for this basic subproblem,
completeness of overall reasoning procedures is not an issue anymore; but other issues
arise, such as how to compare incomplete procedures (Heinsohn, Kudenko, Nebel,
& Protlich, 1992), and how to judge a procedure \complete enough" (MacGregor,
1991). As a practical tool, inference rules can be used in such systems to achieve the
expected behavior of the KB w.r.t. the information contained in it.
2. `A TKRS is (by denition) general-purpose, hence it must provide tractable and
complete reasoning to a user'. Following this line, other TKRSs (such as krypton
and classic) provide limited tractable languages for expressing concepts, following
the \small-can-be-beautiful" approach (see Patel-Schneider, 1984). The gap between
what is expressible in the TKRS language and what is needed to be expressed for the
application is then lled by the user, by a (sort of) programming with inference rules.
Of course, the usual problems present in program development and debugging arise
(McGuinness, 1992).
What is common to both approaches is that a user must cope with incomplete reasoning.
The dierence is that in the former approach, the burden of regaining useful yet missed
inferences is mostly left to the developers of the TKRS (and the user is supposed to specify
what is \complete enough"), while in the latter this is mainly left to the user. These
are perfectly reasonable approaches in a practical context, where incomplete procedures
and specialized programs are often used to deal with intractable problems. In our opinion
incomplete procedures are just a provisional answer to the problem|the best possible up to
now. In order to improve on such an answer, a theoretical analysis of the general problems
1{4 is to be done.
Previous theoretical results do not deal with the problems 1{4 in their full generality.
For example, the problems are studied in (Nebel, 1990, Chapter 4), but only incomplete
111

Buchheit, Donini, & Schaerf

procedures are given, and cycles are not considered. In (Donini, Lenzerini, Nardi, & Schaerf,
1993; Schaerf, 1993a) the complexity of instance checking has been analyzed, but only KBs
without a TBox are treated. Instance checking has also been analyzed in (Vilain, 1991),
but addressing only that part of the problem which can be performed as parsing.
In addition, we think that the expressiveness of actual systems should be enhanced
making terminological cycles (see Nebel, 1990, Chapter 5) available in TKRSs. Such a
feature is of undoubtable practical interest (MacGregor, 1992), yet most present TKRSs
can only approximate cycles, by using forward inference rules (as in back, classic, loom).
In our opinion, in order to make terminological cycles fully available in complete TKRSs, a
theoretical investigation is still needed.
Previous theoretical work on cycles was done in (Baader, 1990a, 1990b; Baader, Burkert,
Hollunder, Nutt, & Siekmann, 1990; Dionne, Mays, & Oles, 1992, 1993; Nebel, 1990, 1991;
Schild, 1991), but considering KBs formed by the TBox alone. Moreover, these approaches
do not deal with number restrictions (except for Nebel, 1990, Section 5.3.5) |a basic feature
already provided by TKRSs| and the techniques used do not seem easily extensible to
reasoning with ABoxes. We compare in detail several of these works with ours in Section 4.
In this paper, we propose a TKRS equipped with a highly expressive language, including constructors often required in practical applications, and prove decidability of problems
1{4. In particular, our system uses the language ALCNR, which supports general complements of concepts, number restrictions and role conjunction. Moreover, the system allows
one to express inclusion statements between general concepts and, as a particular case,
terminological cycles. We prove decidability by means of a suitable calculus, which is developed extending the well established framework of constraint systems (see Donini et al.,
1991a; Schmidt-Schau & Smolka, 1991), thus exploiting a uniform approach to reasoning
in TKRSs. Moreover, our calculus can easily be turned into a decision procedure.
The paper is organized as follows. In Section 2 we introduce the language, and we
give it a Tarski-style extensional semantics, which is the most commonly used. Using this
semantics, we establish relationships between problems 1{4 which allow us to concentrate
on KB-satisability only. In Section 3 we provide a calculus for KB-satisability, and show
correctness and termination of the calculus. Hence, we conclude that KB-satisability is
decidable in ALCNR, which is the main result of this paper. In Section 4 we compare our
approach with previous results on decidable TKRSs, and we establish the equivalence of
general (cyclic) inclusion statements and general concept denitions using the descriptive
semantics. Finally, we discuss in detail several practical issues related to our results in
Section 5.

2. Preliminaries

In this section we rst present the basic notions regarding concept languages. Then we
describe knowledge bases built up using concept languages, and reasoning services that
must be provided for extracting information from such knowledge bases.

2.1 Concept Languages

In concept languages, concepts represent the classes of objects in the domain of interest,
while roles represent binary relations between objects. Complex concepts and roles can be
112

Decidable Reasoning in Terminological KR Systems

dened by means of suitable constructors applied to concept names and role names. In
particular, concepts and roles in ALCNR can be formed by means of the following syntax
(where Pi (for i = 1; : : :; k) denotes a role name, C and D denote arbitrary concepts, and
R an arbitrary role):

C; D ,! A j

>j
?j

(C u D) j
(C t D) j

:C j
8R.C j
9R.C j
( n R) j ( n R)
R ,! P1 u    u Pk

(concept name)
(top concept)
(bottom concept)
(conjunction)
(disjunction)
(complement)
(universal quantication)
(existential quantication)
(number restrictions)
(role conjunction)

When no confusion arises we drop the brackets around conjunctions and disjunctions.
We interpret concepts as subsets of a domain and roles as binary relations over a domain.
More precisely, an interpretation I = (I ; I ) consists of a nonempty set I (the domain
of I ) and a function I (the extension function of I ), which maps every concept to a subset
of I and every role to a subset of I  I . The interpretation of concept names and
role names is thus restricted by AI  I , and P I  I  I , respectively. Moreover,
the interpretation of complex concepts and roles must satisfy the following equations (]fg
denotes the cardinality of a set):

>I = I
?I = ;

(C u D)I
(C t D)I
(:C )I
(8R.C )I
(9R.C )I
( n R)I
( n R)I
(P1 u    u Pk )I

=
=
=
=
=
=
=
=

C I \ DI
C I [ DI
I n C I
fd1 2 I j 8d2 : (d1; d2) 2 RI ! d2 2 C I g
fd1 2 I j 9d2 : (d1; d2) 2 RI ^ d2 2 C I g
fd1 2 I j ]fd2 j (d1; d2) 2 RI g  ng
fd1 2 I j ]fd2 j (d1; d2) 2 RI g  ng
P1I \    \ PkI

2.2 Knowledge Bases

(1)

A knowledge base built by means of concept languages is generally formed by two components: The intensional one, called TBox, and the extensional one, called ABox.
We rst turn our attention to the TBox. As we said before, the intensional level species the properties of the concepts of interest in a particular application. Syntactically,
such properties are expressed in terms of what we call inclusion statements. An inclusion
113

Buchheit, Donini, & Schaerf

statement (or simply inclusion) has the form

CvD
where C and D are two arbitrary ALCNR-concepts. Intuitively, the statement species
that every instance of C is also an instance of D. More precisely, an interpretation I satises
the inclusion C v D if C I  DI .
A TBox is a nite set of inclusions. An interpretation I is a model for a TBox T if I
satises all inclusions in T .
In general, TKRSs provide the user with mechanisms for stating concept introductions
(e.g., Nebel, 1990, Section 3.2) of the form A =: D (concept denition, interpreted as set
equality), or A _ D (concept specication, interpreted as set inclusion), with the restrictions
that the left-hand side concept A must be a concept name, that for each concept name
at most one introduction is allowed, and that no terminological cycles are allowed, i.e.,
no concept name may occur|neither directly nor indirectly|within its own introduction.
These restrictions make it possible to substitute an occurrence of a dened concept by its
denition.
We do not impose any of these restrictions to the form of inclusions, obtaining statements
that are syntactically more expressive than concept introductions. In particular, a denition
of the form A =: D can be expressed in our system using the pair of inclusions A v D
and D v A and a specication of the form A _ D can be simply expressed by A v D.
Conversely, an inclusion of the form C v D, where C and D are arbitrary concepts, cannot
be expressed with concept introductions. Moreover, cyclic inclusions are allowed in our
statements, realizing terminological cycles.
As shown in (Nebel, 1991), there are at least three types of semantics for terminological cycles, namely the least xpoint, the greatest xpoint, and the descriptive semantics.
Fixpoint semantics choose particular models among the set of interpretations that satisfy a
statement of the form A =: D. Such models are chosen as the least and the greatest xpoint
of the above equation. The descriptive semantics instead considers all interpretations that
satisfy the statement (i.e., all xpoints) as its models.
However, xpoint semantics naturally apply only to xpoint statements like A =: D,
where D is a \function" of A, i.e., A may appear in D, and there is no obvious way to
extend them to general inclusions. In addition, since our language includes the constructor
for complement of general concepts, the \function" D may be not monotone, and therefore
the least and the greatest xpoints may be not unique. Whether there exists or not a
denitional semantics that is suitable for cyclic denitions in expressive languages is still
unclear.
Conversely, the descriptive semantics interprets statements as just restricting the set of
possible models, with no denitional import. Although it is not completely satisfactory in all
practical cases (Baader, 1990b; Nebel, 1991), the descriptive semantics has been considered
to be the most appropriate one for general cyclic statements in powerful concept languages.
Hence, it seems to be the most suitable to be extended to our case and it is exactly the one
we have adopted above.
Observe that our decision to put general inclusions in the TBox is not a standard one. In
fact, in TKRS like krypton such statements were put in the ABox. However, we conceive
114

Decidable Reasoning in Terminological KR Systems

inclusions as a generalization of traditional TBox statements: acyclic concept introductions,
with their denitional import, can be perfectly expressed with inclusions; and cyclic concept
introductions can be expressed as well, if descriptive semantics is adopted. Therefore, we
believe that inclusions should be part of the TBox.
Notice that role conjunction allows one to express the practical feature of subroles. For
example, the role ADOPTEDCHILD can be written as CHILD u ADOPTEDCHILD0, where ADOPTEDCHILD' is a role name, making it a subrole of CHILD. Following such idea, every hierarchy
of role names can be rephrased with a set of role conjunctions, and vice versa.
Actual systems usually provide for the construction of hierarchies of roles by means of
role introductions (i.e., statements of the form P =: R and P _ R) in the TBox. However,
in our simple language for roles, cyclic denitions of roles can be always reduced to acyclic
denitions, as explained in (Nebel, 1990, Sec.5.3.1). When role denitions are acyclic, one
can always substitute in every concept each role name with its denition, obtaining an
equivalent concept. Therefore, we do not consider role denitions in this paper, and we
conceive the TBox just as a set of concept inclusions.
Even so, it is worth to notice that concept inclusions can express knowledge about roles.
In particular, domain and range restrictions of roles can be expressed, in a way similar to
the one in (Catarci & Lenzerini, 1993). Restricting the domain of a role R to a concept C
and its range to a concept D can be done by the two inclusions

9R.> v C; > v 8R.D
It is straightforward to show that if an interpretation I satises the two inclusions, then
RI  C I  D I .

Combining subroles with domain and range restrictions it is also possible to partially
express the constructor for role restriction, which is present in various proposals (e.g.,
the language FL in Brachman & Levesque, 1984). Role restriction, written as R : C , is
dened by (R : C )I = f(d1; d2) 2 I  I j (d1; d2) 2 RI ^ d2 2 C I g. For example the
role DAUGHTER, which can be formulated as CHILD:Female, can be partially simulated by
CHILD u DAUGHTER0, with the inclusion > v 8DAUGHTER0.Female. However, this simulation
would not be complete in number restrictions: E.g., if a mother has at least three daughters,
then we know she has at least three female children; if instead we know that she has three
female children we cannot infer that she has three daughters.
We can now turn our attention to the extensional level, i.e., the ABox. The ABox
essentially allows one to specify instance-of relations between individuals and concepts, and
between pairs of individuals and roles.
Let O be an alphabet of symbols, called individuals. Instance-of relationships are expressed in terms of membership assertions of the form:

C (a);

R(a; b);

where a and b are individuals, C is an ALCNR-concept, and R is an ALCNR-role. Intuitively, the rst form states that a is an instance of C , whereas the second form states that
a is related to b by means of the role R.
115

Buchheit, Donini, & Schaerf

In order to assign a meaning to membership assertions, the extension function I of an
interpretation I is extended to individuals by mapping them to elements of I in such a
way that aI 6= bI if a 6= b. This property is called Unique Name Assumption; it ensures
that dierent individuals are interpreted as dierent objects.
An interpretation I satises the assertion C (a) if aI 2 C I , and satises R(a; b) if
I
(a ; bI ) 2 RI . An ABox is a nite set of membership assertions. I is a model for an ABox
A if I satises all the assertions in A.
An ALCNR-knowledge base  is a pair  = hT ; Ai where T is a TBox and A is an
ABox. An interpretation I is a model for  if it is both a model for T and a model for A.
We can now formally dene the problems 1{4 mentioned in the introduction. Let  be
an ALCNR-knowledge base.
1. KB-satisability :  is satisable, if it has a model;
2. Concept Satisability : C is satisable w.r.t , if there exists a model I of  such that
C I 6= ;;
3. Subsumption : C is subsumed by D w.r.t. , if C I  DI for every model I of ;
4. Instance Checking : a is an instance of C , written  j= C (a), if the assertion C (a) is
satised in every model of .
In (Nebel, 1990, Sec.3.3.2) it is shown that the ABox plays no active role when checking
concept satisability and subsumption. In particular, Nebel shows that the ABox (subject
to its satisability) can be replaced by an empty one without aecting the result of those
services. Actually, in (Nebel, 1990), the above property is stated for a language less expressive than ALCNR. However, it is easy to show that it extends to ALCNR. It is important
to remark that such a property is not valid for all concept languages. In fact, there are
languages that include some constructors that refer to the individuals in the concept language, e.g., the constructor one-of (Borgida et al., 1989) that forms a concept from a set of
enumerated individuals. If a concept language includes such a constructor the individuals
in the TBox can interact with the individuals in the ABox, as shown in (Schaerf, 1993b).
As a consequence, both concept satisability and subsumption depend also on the ABox.

Example 2.1 Consider the following knowledge base  = hT ; Ai:
T = f9TEACHES.Course v (Student u 9DEGREE.BS) t Prof;
Prof v 9DEGREE.MS;
9DEGREE.MS v 9DEGREE.BS;
MS u BS v ?g
A = fTEACHES(john; cs156); ( 1 DEGREE)(john); Course(cs156)g
 is a fragment of a hypothetical knowledge base describing the organization of a university.
The rst inclusion, for instance, states that the persons teaching a course are either graduate
students (students with a BS degree) or professors. It is easy to see that  is satisable. For
example, the following interpretation I satises all the inclusions in T and all the assertions
116

Decidable Reasoning in Terminological KR Systems

in A, and therefore it is a model for :
I = fjohn; cs156; csbg; johnI = john; cs156I = cs156
StudentI = fjohng; ProfI = ;; CourseI = fcs156g; BSI = fcsbg
MSI = ;; TEACHESI = f(john; cs156)g; DEGREEI = f(john; csb)g
We have described the interpretation I by giving only I , and the values of I on
concept names and role names. It is straightforward to see that all values of I on complex
concepts and roles are uniquely determined by imposing that I must satisfy the Equations 1
on page 113.
Notice that it is possible to draw several non-trivial conclusions from . For example, we
can infer that  j= Student(john). Intuitively this can be shown as follows: John teaches
a course, thus he is either a student with a BS or a professor. But he can't be a professor
since professors have at least two degrees (BS and MS) and he has at most one, therefore
he is a student.
Given the previous semantics, the problems 1{4 can all be reduced to KB-satisability
(or to its complement) in linear time. In fact, given a knowledge base  = hT ; Ai, two
concepts C and D, an individual a, and an individual b not appearing in , the following
equivalences hold:

C is satisable w:r:t  i hT ; A [ fC (b)gi is satisable:
C is subsumed by D w:r:t:  i hT ; A [ f(C u :D)(b)gi is not satisable:
 j= C (a) i hT ; A [ f(:C )(a)gi is not satisable:
A slightly dierent form of these equivalences has been given in (Hollunder, 1990). The
equivalences given here are a straightforward consequence of the ones given by Hollunder.
However, the above equivalences are not valid for languages including constructors that refer
to the individuals in the concept language. The equivalences between reasoning services in
such languages are studied in (Schaerf, 1993b).
Based on the above equivalences, in the next section we concentrate just on KBsatisability.

3. Decidability Result

In this section we provide a calculus for deciding KB-satisability. In particular, in Subsection 3.1 we present the calculus and we state its correctness. Then, in Subsection 3.2, we
prove the termination of the calculus. This will be sucient to assess the decidability of all
problems 1{4, thanks to the relationships between the four problems.

3.1 The calculus and its correctness

Our method makes use of the notion of constraint system (Donini et al., 1991a; SchmidtSchau & Smolka, 1991; Donini, Lenzerini, Nardi, & Schaerf, 1991c), and is based on a
tableaux-like calculus (Fitting, 1990) that tries to build a model for the logical formula
corresponding to a KB.
117

Buchheit, Donini, & Schaerf

We introduce an alphabet of variable symbols V together with a well-founded total
ordering `' on V . The alphabet V is disjoint from the other ones dened so far. The
purpose of the ordering will become clear later. The elements of V are denoted by the
letters x; y; z; w. From this point on, we use the term object as an abstraction for individual
and variable (i.e., an object is an element of O [ V ). Objects are denoted by the symbols
s; t and, as in Section 2, individuals are denoted by a; b.
A constraint is a syntactic entity of one of the forms:

s: C; sPt;

8x.x: C; s =6 : t;

where C is a concept and P is a role name. Concepts are assumed to be simple, i.e., the
only complements they contain are of the form :A, where A is a concept name. Arbitrary
ALCNR-concepts can be rewritten into equivalent simple concepts in linear time (Donini
et al., 1991a). A constraint system is a nite nonempty set of constraints.
Given an interpretation I , we dene an I -assignment  as a function that maps every
variable of V to an element of I , and every individual a to aI (i.e., (a) = aI for all
a 2 O).
A pair (I ; ) satises: the constraint s: C if (s) 2 C I , the constraint sPt if ((s); (t))
2 P I , the constraint s =6 t if (s) 6= (t), and nally, the constraint 8x.x: C if C I = I
(notice that  does not play any role in this case). A constraint system S is satisable if
there is a pair (I ; ) that satises every constraint in S .
An ALCNR-knowledge base  = hT ; Ai can be translated into a constraint system
S by replacing every inclusion C v D 2 T with the constraint 8x.x: :C t D, every
membership assertion C (a) with the constraint a: C , every R(a; b) with the constraints
aP1 b; : : :; aPk b if R = P1 u : : : u Pk , and including the constraint a =
6 : b for every pair (a; b)
of individuals appearing in A. It is easy to see that  is satisable if and only if S is
satisable.
In order to check a constraint system S for satisability, our technique adds constraints
to S until either an evident contradiction is generated or an interpretation satisfying it can
be obtained from the resulting system. Constraints are added on the basis of a suitable set
of so-called propagation rules.
Before providing the rules, we need some additional denitions. Let S be a constraint
system and R = P1 u : : : u Pk (k  1) be a role. We say that t is an R-successor of s in S
if sP1 t; : : :; sPk t are in S . We say that t is a direct successor of s in S if for some role R,
t is an R-successor of s. We call direct predecessor the inverse relation of direct successor.
If S is clear from the context we omit it. Moreover, we denote by successor the transitive
closure of the relation direct successor, and we denote by predecessor its inverse.
We assume that variables are introduced in a constraint system according to the ordering
`'. This means, if y is introduced in a constraint system S then x  y for all variables x
that are already in S .
We denote by S [x=s] the constraint system obtained from S by replacing each occurrence
of the variable x by the object s.
We say that s and t are separated in S if the constraint s =
6 : t is in S .
Given a constraint system S and an object s, we dene the function  (; ) as follows:
 (S; s) := fC j s: C 2 S g. Moreover, we say that two variables x and y are S -equivalent,
118

Decidable Reasoning in Terminological KR Systems

written x s y , if  (S; x) =  (S; y ). Intuitively, two S-equivalent variables can represent the
same element in the potential interpretation built by the rules, unless they are separated.
The propagation rules are:
1. S !u fs: C1; s: C2g [ S
if 1. s: C1 u C2 is in S ,
2. s: C1 and s: C2 are not both in S
2. S !t fs: Dg [ S
if 1. s: C1 t C2 is in S ,
2. neither s: C1 nor s: C2 is in S ,
3. D = C1 or D = C2
3. S !8 ft: C g [ S
if 1. s: 8R.C is in S ,
2. t is an R-successor of s,
3. t: C is not in S
4. S !9 fsP1 y; : : :; sPk y; y : C g [ S
if 1. s: 9R.C is in S ,
2. R = P1 u : : : u Pk ,
3. y is a new variable,
4. there is no t such that t is an R-successor of s in S and t: C is in S ,
5. if s is a variable there is no variable w such that w  s and s s w
5. S ! fsP1 yi ; : : :; sPk yi j i 2 1::ng [ fyi 6=: yj j i; j 2 1::n; i 6= j g [ S
if 1. s: ( n R) is in S ,
2. R = P1 u : : : u Pk ,
3. y1 ; : : :; yn are new variables,
4. there do not exist n pairwise separated R-successors of s in S ,
5. if s is a variable there is no variable w such that w  s and s s w
6. S ! S [y=t]
if 1. s: ( n R) is in S ,
2. s has more than n R-successors in S ,
3. y; t are two R-successors of s which are not separated
7. S !8x fs: C g [ S
if 1. 8x.x: C is in S ,
2. s appears in S ,
3. s: C is not in S .
We call the rules !t and ! nondeterministic rules, since they can be applied in
dierent ways to the same constraint system (intuitively, they correspond to branching
rules of tableaux). All the other rules are called deterministic rules. Moreover, we call the
rules !9 and ! generating rules, since they introduce new variables in the constraint
system. All other rules are called nongenerating ones.
119

Buchheit, Donini, & Schaerf

The use of the condition based on the S -equivalence relation in the generating rules
(condition 5) is related to the goal of keeping the constraint system nite even in presence
of potentially innite chains of applications of generating rules. Its role will become clearer
later in the paper.
One can verify that rules are always applied to a system S either because of the presence
in S of a given constraint s: C (condition 1), or, in the case of the !8x-rule, because of the
presence of an object s in S . When no confusion arises, we will say that a rule is applied
to the constraint s: C or the object s (instead of saying that it is applied to the constraint
system S ).
Proposition 3.1 (Invariance) Let S and S 0 be constraint systems. Then:
1. If S 0 is obtained from S by application of a deterministic rule, then S is satisable if
and only if S 0 is satisable.
2. If S 0 is obtained from S by application of a nondeterministic rule, then S is satisable if S 0 is satisable. Conversely, if S is satisable and a nondeterministic rule is
applicable to an object s in S , then it can be applied to s in such a way that it yields
a satisable constraint system.
Proof. The proof is mainly a rephrasing of typical soundness proofs for tableaux methods (e.g., Fitting, 1990, Lemma 6.3.2). The only non-standard constructors are number
restrictions.
1. \(" Considering the deterministic rules one can directly check that S is a subset of S 0.
So it is obvious that S is satisable if S 0 is satisable.
\)" In order to show that S 0 is satisable if this is the case for S we consider in turn
each possible deterministic rule application leading from S to S 0. We assume that (I ; )
satises S .
If the !u -rule is applied to s: C1 u C2 in S , then S 0 = S [ fs: C1; s: C2g. Since (I ; )
satises s: C1 u C2, (I ; ) satises s: C1 and s: C2 and therefore S 0.
If the !8-rule is applied to s: 8R.C , there must be an R-successor t of s in S such that
S 0 = S [ft: C g. Since (I ; ) satises S , it holds that ((s); (t)) 2 RI . Since (I ; ) satises
s: 8R.C , it holds that (t) 2 C I . So (I ; ) satises t: C and therefore S 0.
If the !8x-rule is applied to an s because of the presence of 8x.x: C in S , then S 0 =
S [ fs: C g. Since (I ; ) satises S it holds that C I = I . Therefore (s) 2 C I and so
(I ; ) satises S 0 .
If the !9 -rule is applied to s: 9R.C , then S 0 = S [ fsP1 y; : : :; sPk y; y : C g. Since (I ; )
satises S , there exists a d such that ((s); d) 2 RI and d 2 C I . We dene the I -assignment
0 as 0 (y) := d and 0(t) := (t) for t 6= y . It is easy to show that (I ; 0) satises S 0.
If the ! -rule is applied to s: ( n R), then S 0 = S [ fsP1 yi ; : : :; sPk yi j i 2 1::ng [
fyi 6=: yj j i; j 2 1::n; i 6= j g. Since (I ; ) satises S , there exist n distinct elements
d1; : : :; dn 2 I such that ((s); di) 2 RI . We dene the I -assignment 0 as 0 (yi) := di
for i 2 1::n and 0 (t) := (t) for t 62 fy1 ; : : :; yn g. It is easy to show that (I ; 0) satises S 0.
2. \(" Assume that S 0 is satised by (I ; 0). We show that S is also satisable. If S 0
is obtained from S by application of the !t -rule, then S is a subset of S 0 and therefore
satised by (I ; 0).
120

Decidable Reasoning in Terminological KR Systems

If S 0 is obtained from S by application of the ! -rule to s: ( n R) in S , then there
are y; t in S such that S 0 = S [y=t]. We dene the I -assignment  as (y ) := 0 (t) and
(v ) := 0(v ) for every object v with v 6= y. Obviously (I ; ) satises S .
\)" Now suppose that S is satised by (I ; ) and a nondeterministic rule is applicable
to an object s.
If the !t -rule is applicable to s: C1 t C2 then, since S is satisable, (s) 2 (C1 t C2)I .
It follows that either (s) 2 C1I or (s) 2 C2I (or both). Hence, the !t -rule can obviously
be applied in a way such that (I ; ) satises the resulting constraint system S 0.
If the ! -rule is applicable to s: ( n R), then|since (I ; ) satises S |it holds that
(s) 2 ( n R)I and therefore the set fd 2 I j ((s); d) 2 RI g has at most n elements.
On the other hand, there are more than n R-successors of s in S and for each R-successor t
of s we have ((s); (t)) 2 RI . Thus, we can conclude by the Pigeonhole Principle (see e.g.,
Lewis & Papadimitriou, 1981, page 26) that there exist at least :two R-successors t; t0 of s
such that (t) = (t0 ). Since (I ; ) satises S , the constraint t 6= t0 is not in S . Therefore
one of the two must be a variable, let's say t0 = y . Now obviously (I ; ) satises S [y=t].
Given a constraint system S , more than one rule might be applicable to it. We dene
the following strategy for the application of rules:
1. apply a rule to a variable only if no rule is applicable to individuals;
2. apply a rule to a variable x only if no rule is applicable to a variable y such that y  x;
3. apply generating rules only if no nongenerating rule is applicable.
The above strategy ensures that the variables are processed one at a time according to
the ordering `'.
From this point on, we assume that rules are always applied according to this strategy
and that we always start with a constraint system S coming from an ALCNR-knowledge
base . The following lemma is a direct consequence of these assumptions.

Lemma 3.2 (Stability) Let S be a constraint system and x be a variable in S . Let a

generating rule be applicable to x according to the strategy. Let S 0 be any constraint system
derivable from S by any sequence (possibly empty) of applications of rules. Then
1. No rule is applicable in S 0 to a variable y with y  x
2.  (S; x) =  (S 0; x)
3. If y is a variable in S with y  x then y is a variable in S 0, i.e., the variable y is not
substituted by another variable or by a constant.
1. By contradiction: Suppose S  S0 ! S1 !    ! Sn  S 0, where  2
ft; u; 9; 8; ; ; 8xg and a rule is applicable to a variable y such that y  x in S 0. Then
there exists a minimal i, where i  n, such that this is the case in Si . Note that i 6= 0; in
fact, because of the strategy, if a rule is applicable to x in S no rule is applicable to y in S .
So no rule is applicable to any variable z such that z  x in S0; : : :; Si,1. It follows that
from Si,1 to Si a rule is applied to x or to a variable w such that x  w. By an exhaustive
Proof.

121

Buchheit, Donini, & Schaerf

analysis of all rules we see that|whichever is the rule applied from Si,1 to Si |no new
constraint of the form y : C or yRz can be added to Si,1 , and therefore no rule is applicable
to y in Si , contradicting the assumption.
2. By contradiction: Suppose  (S; x) 6=  (S 0; x). Call y the direct predecessor of x, then a
rule must have been applied either to y or to x itself. Obviously we have y  x, therefore
the former case cannot be because of point 1. A case analysis shows that the only rules
which can have been applied to x are generating ones and the !8 and the ! rules. But
these rules add new constraints only to the direct successors of x and not to x itself and
therefore do not change  (; x).
3. This follows from point 1. and the strategy.
Lemma 3.2 proves that for a variable x which has a direct successor,  (; x) is stable,
i.e., it will not change because of subsequent applications of rules. In fact, if a variable
has a direct successor it means that a generating rule has been applied to it, therefore
(Lemma 3.2.2) from that point on  (; x) does not change.
A constraint system is complete if no propagation rule applies to it. A complete system
derived from a system S is also called a completion of S . A clash is a constraint system
having one of the following forms:

 fs: ?g
 fs: A; s: :Ag, where A is a concept name.
 fs: ( n R)g [ fsP1: ti ; : : :; sPk ti j i 2 1::n + 1g
[ fti =6 tj j i; j 2 1::n + 1; i =6 j g,
where R = P1 u : : : u Pk .
A clash is evidently an unsatisable constraint system. For example, the last case
represents the situation in which an object has an at-most restriction and a set of Rsuccessors that cannot be identied (either because they are individuals or because they
have been created by some at-least restrictions).
Any constraint system containing a clash is obviously unsatisable. The purpose of the
calculus is to generate completions, and look for the presence of clashes inside. If S is a
completion of S and S contains no clash, we prove that it is always possible to construct
a model for  on the basis of S . Before looking at the technical details of the proof, let us
consider an example of application of the calculus for checking satisability.

Example 3.3 Consider the following knowledge base  = hT ; Ai:
T = fItalian v 9FRIEND.Italiang
A = fFRIEND(peter; susan);
8FRIEND.:Italian(peter);
9FRIEND.Italian(susan)g
The corresponding constraint system S is:
S = f8x.x: :Italian t 9FRIEND.Italian;
peterFRIENDsusan

;

122

Decidable Reasoning in Terminological KR Systems

8
9:

.:
.
g

;

peter: FRIEND Italian
susan: FRIEND Italian
peter = susan

6

A sequence of applications of the propagation rules to S is as follows:
S1 = S [ fsusan: :Italiang (!8-rule)
S2 = S1 [ fpeter: :Italian t 9FRIEND.Italiang (!8x-rule)
S3 = S2 [ fsusan: :Italian t 9FRIEND.Italiang (!8x-rule)
S4 = S3 [ fpeter: :Italiang (!t -rule)
S5 = S4 [ fsusanFRIENDx; x: Italiang (!9 -rule)
S6 = S5 [ fx: :Italian t 9FRIEND.Italiang (!8x-rule)
S7 = S6 [ fx: 9FRIEND.Italiang (!t-rule)
S8 = S7 [ fxFRIENDy; y: Italiang (!9-rule)
S9 = S8 [ fy: :Italian t 9FRIEND.Italiang (!8x-rule)
S10 = S9 [ fy: 9FRIEND.Italiang (!t -rule)
One can verify that S10 is a complete clash-free constraint system. In particular, the !9 rule is not applicable to y . In fact, since x S10 y condition 5 is not satised. From S10 one
can build an interpretation I , as follows (again, we give only the interpretation of concept
and role names):
I = fpeter; susan; x; yg
peterI = peter, susanI = susan, (x) = x, (y) = y,
ItalianI = fx; yg
FRIENDI = f(peter; susan); (susan; x); (x; y); (y; y)g
It is easy to see that I is indeed a model for .
In order to prove that it is always possible to obtain an interpretation from a complete
clash-free constraint system we need some additional notions. Let S be a constraint system
and x, w variables in S . We call w a witness of x in S if the three following conditions hold:
1. x s w
2. w  x
3. there is no variable z such that z  w and z satises conditions 1. and 2., i.e., w is
the least variable w.r.t.  satisfying conditions 1. and 2.
We say x is blocked (by w) in S if x has a witness (w) in S . The following lemma states a
property of witnesses.

Lemma 3.4 Let S be a constraint system, x a variable in S . If x is blocked then
1. x has no direct successor and
2. x has exactly one witness.
123

Buchheit, Donini, & Schaerf

1. By contradiction: Suppose that x is blocked in S and xPy is in S . During the
completion process leading to S a generating rule must have been applied to x in a system
S 0. It follows from the denition of the rules that in S 0 for every variable w  x we had
x6s w. Now from Lemma 3.2 we know, that for the constraint system S derivable from
S 0 and for every w  x in S we also have x6s w. Hence there is no witness for x in S ,
contradicting the hypothesis that x is blocked.
2. This follows directly from condition 3. for a witness.
As a consequence of Lemma 3.4, in a constraint system S , if w1 is a witness of x then w1
cannot have a witness itself, since both the relations `' and S -equivalence are transitive.
The uniqueness of the witness for a blocked variable is important for dening the following
particular interpretation out of S .
Let S be a constraint system. We dene the canonical interpretation IS and the canonical IS -assignment S as follows:
Proof.

0

1.
2.
3.
4.

IS := fs j s is an object in S g
S (s) := s
s 2 AIS if and only if s: A is in S
(s; t) 2 P IS if and only if
(a) sPt is in S or
(b) s is a blocked variable, w is the witness of s in S and wPt is in S .

We call (s; t) a P-role-pair of s in IS if (s; t) 2 P IS , we call (s; t) a role-pair of s in IS
if (s; t) is a P-role-pair for some role P . We call a role-pair explicit if it comes up from case
4.(a) of the denition of the canonical interpretation and we call it implicit if it comes up
from case 4.(b).
From Lemma 3.4 it is obvious that a role-pair cannot be both explicit and implicit.
Moreover, if a variable has an implicit role-pair then all its role-pairs are implicit and they
all come from exactly one witness, as stated by the following lemma.

Lemma 3.5 Let S be a completion and x a variable in S . Let IS be the canonical interpretation for S . If x has an implicit role-pair (x; y ), then
1. all role-pairs of x in IS are implicit
2. there is exactly one witness w of x in S such that for all roles P in S and all P -rolepairs (x,y) of x, the constraint wPy is in S .

The rst statement follows from Lemma 3.4 (point 1 ). The second statement follows
from Lemma 3.4 (point 2 ) together with the denition of IS .
We have now all the machinery needed to prove the main theorem of this subsection.
Proof.

Theorem 3.6 Let S be a complete constraint system. If S contains no clash then it is

satisable.

124

Decidable Reasoning in Terminological KR Systems

Proof. Let IS and S be the canonical interpretation and canonical I -assignment for S .
We: prove that the pair (IS ; S ) satises every constraint c in S . If c has the form sPt or
s 6= t, then (IS ; S ) satises them by denition of IS :and S . Considering the ! -rule and
the ! -rule we see that a constraint of the form s =
6 s can not be in S . If c has the form
s: C , we show by induction on the structure of C that s 2 C IS .
We rst consider the base cases. If C is a concept name, then s 2 C IS by denition
of IS . If C = >, then obviously s 2 >IS . The case that C = ? cannot occur since S is
clash-free.
Next we analyze in turn each possible complex concept C . If C is of the form :C1 then
C1 is a concept name since all concepts are simple. Then the constraint s: C1 is not in S
since S is clash-free. Then s 62 C1IS , that is, s 2 IS n C1IS . Hence s 2 (:C1)IS .
If C is of the form C1 u C2 then (since S is complete) s: C1 is in S and s: C2 is in S . By
induction hypothesis, s 2 C1IS and s 2 C2IS . Hence s 2 (C1 u C2)IS .
If C is of the form C1 t C2 then (since S is complete) either s: C1 is in S or s: C2 is in
S . By induction hypothesis, either s 2 C1IS or s 2 C2IS . Hence s 2 (C1 t C2)IS .
If C is of the form 8R.D, we have to show that for all t with (s; t) 2 RIS it holds that
t 2 DIS . If (s; t) 2 RIS , then according to Lemma 3.5 two cases can occur. Either t is an
R-successor of s in S or s is blocked by a witness w in S and t is an R-successor of w in S .
In the rst case t: D must also be in S since S is complete. Then by induction hypothesis
we have t 2 DIS . In the second case by denition of witness, w: 8R.D is in S and then
because of completeness of S , t: D must be in S . By induction hypothesis we have again
t 2 DIS .
If C is of the form 9R.D we have to show that there exists a t 2 IS with (s; t) 2 RIS
and t 2 DIS . Since S is complete, either there is a t that is an R-successor of s in S and
t: D is in S , or s is a variable blocked by a witness w in S . In the rst case, by induction
hypothesis and the denition of IS , we have t 2 DIS and (s; t) 2 RIS . In the second case
w: 9R.D is in S . Since w cannot be blocked and S is complete, we have that there is a
t that is an R-successor of w in S and t: D is in S . So by induction hypothesis we have
t 2 DIS and by the denition of IS we have (s; t) 2 RIS .
If C is of the form ( n R) we show the goal by contradiction. Assume that s 62 (
n R)IS . Then there exist atleast n + 1 distinct objects t1 ; : : :; tn+1 with (s; ti ) 2 RIS ; i 2
1::n + 1. This means that, since R = P1 u : : : u Pk , there are pairs (s; ti) 2 PjIS , where
i 2 1::n + 1 and j 2 1::k. Then according to Lemma 3.5 one of the two following cases must
occur. Either all sPj ti for j 2 1::k; i 2 1::n + 1 are in S or there exists a witness w of s in
S with all wPiti for j 2 1::k and i 2 1::n + 1 are in S . In the rst case the ! -rule can not
be applicable because of completeness. :This means that all the ti 's are pairwise separated,
i.e., that S contains the constraints ti 6= tj ; i; j 2 1::n + 1; i 6= j . This contradicts the fact
that S is clash-free. And the second case leads to an analogous contradiction.
If C is of the form ( n R) we show the goal by contradiction. Assume that s 62 (
n R)IS . Then there exist atmost m < n (m possibly 0) distinct objects t1; : : :; tm with
(s; ti ) 2 RIS ; i 2 1::m. We have to consider two cases. First case: s is not blocked in
S . Since there are only m R-successors of s in S , the ! -rule is applicable to s. This
contradicts the fact that S is complete. Second case: s is blocked by a witness w in S .
Since there are m R-successors of w in S , the ! -rule is applicable to w. But this leads to
the same contradiction.

125

Buchheit, Donini, & Schaerf

If c has the form 8x.x: D then, since S is complete, for each object t in S , t: D is in
S |and, by the previous cases, t 2 DIS . Therefore, the pair (IS ; S ) satises 8x.x: D.
Finally, since (IS ; S ) satises all constraints in S , (IS ; S ) satises S .

Theorem 3.7 (Correctness) A constraint system S is satisable if and only if there exists
at least one clash-free completion of S .

\(" Follows immediately from Theorem 3.6. \)" Clearly, a system containing
a clash is unsatisable. If every completion of S is unsatisable, then from Proposition 3.1
S , is unsatisable.
Proof.

3.2 Termination and complexity of the calculus

Given a constraint system S , we call nS the number of concepts appearing in S , including
also all the concepts appearing as a substring of another concept. Notice that nS is bounded
by the length of the string expressing S .

Lemma 3.8 Let S be a constraint system and let S 0 be derived from S by means of the

propagation rules. In any set of variables in S 0 including more than 2nS variables there are
at least two variables x,y such that x s y .
0

Each constraint x: C 2 S 0 may contain only concepts of the constraint system S .
Since there are nS such concepts, given a variable x there cannot be more than 2nS dierent
sets of constraints x: C in S 0 .
Proof.

Lemma 3.9 Let S be a constraint system and let S 0 be any constraint system derived from

S by applying the propagation rules with the given strategy. Then, in S 0 there are at most
2nS non-blocked variables.

Suppose there are 2nS + 1 non-blocked variables. From Lemma 3.8, we know that
in S 0 there are at least two variables y1 , y2 such that y1 s y2 . Obviously either y1  y2 or
y2  y1 holds; suppose that y1  y2 . From the denitions of witness and blocked either y1
is a witness of y2 or there exists a variable y3 such that y3  y1 and y3 is a witness of y2 .
In both cases y2 is blocked, contradicting the hypothesis.
Proof.

Theorem 3.10 (Termination and space complexity) Let  be an ALCNR-knowledge
base and let n be its size. Every completion of S is nite and its size is O(24n ).

Let S be a completion of S . From Lemma 3.9 it follows that there are at most 2n
non-blocked variables in S . Therefore there are at most m  2n total variables in S , where
m is the maximum number of direct successors for a variable in S .
Observe that m is bounded by the number of 9R.C concepts (at most n) plus the sum of
all numbers appearing in number restrictions. Since these numbers are expressed in binary,
their sum is bounded by 2n . Hence, m  2n + n. Since the number of individuals is also
bounded by n, the total number of objects in S is at most m  (2n + n)  (2n + n)  (2n + n),
that is, O(22n).
Proof.

126

Decidable Reasoning in Terminological KR Systems

The number of dierent constraints of the form s: C , 8x.x: C in which each object s can
be involved is bounded by n, and each constraint has size linear in n. Hence, the total size
of these constraints is bounded by n  n  22n , that is O(23n).
The number of constraints of the form sPt, s =
6 : t is bounded by (22n)2 = 24n, and each
constraint has constant size.
In conclusion, we have that the size of S is O(24n).
Notice that the above one is just a coarse upper bound, obtained for theoretical purposes.
In practical cases we expect the actual size to be much smaller than that. For example,
if the numbers involved in number restrictions were either expressed in unary notation, or
limited by a constant (the latter being a reasonable restriction in practical systems) then
an argumentation analogous to the above one would lead to a bound of 23n .

Theorem 3.11 (Decidability) Given an ALCNR-knowledge base , checking whether 
is satisable is a decidable problem.

This follows from Theorems 3.7 and 3.10 and the fact that  is satisable if and
only if S is satisable.
We can rene the above theorem, by giving tighter bounds on the time required to
decide satisability.
Proof.

Theorem 3.12 (Time complexity) Given an ALCNR-knowledge base , checking
whether  is satisable can be done in nondeterministic exponential time.

In order to prove the claim it is sucient to show that each completion is obtained
with an exponential number of applications of rules. Since the number of constraints of
each completion is exponential (Theorem 3.10) and each rule, but the ! -rule, adds new
constraints to the constraint system, it follows that all such rules are applied at most an
exponential number of times. Regarding the ! -rule, it is applied for each object at most as
many times as the number of its direct successors. Since such number is at most exponential
(if numbers are coded in binary) w.r.t. the size of the knowledge base, the claim follows.
A lower bound of the complexity of KB-satisability is obtained exploiting previous
results about the language ALC , which is a sublanguage of ALCNR that does not include
number restrictions and role conjunction. We know from McAllester (1991), and (independently) from an observation by Nutt (1992) that KB-satisability in ALC -knowledge bases
is EXPTIME-hard (see (Garey & Johnson, 1979, page 183) for a denition) and hence it
is hard for ALCNR-knowledge bases, too. Hence, we do not expect to nd any algorithm
solving the problem in polynomial space, unless PSPACE=EXPTIME. Therefore, we do
not expect to substantially improve space complexity of our calculus, which already works
in exponential space. We now discuss possible improvements on time complexity.
The proposed calculus works in nondeterministic exponential time, and hence improves
the one we proposed in (Buchheit, Donini, & Schaerf, 1993, Sec.4), which works in deterministic double exponential time. The key improvement is that we showed that a KB has
a model if and only if it has a model of exponential size. However, it may be argued that
as it is, the calculus cannot yet be turned into a practical procedure, since such a procedure would simply simulate nondeterminism by a second level of exponentiality, resulting
Proof.

127

Buchheit, Donini, & Schaerf

in a double exponential time procedure. However, the dierent combinations of concepts
are only exponentially many (this is just the cardinality of the powerset of the set of concepts). Hence, a double exponential time procedure wastes most of the time re-analyzing
over and over objects with dierent names yet with the same  (; ), in dierent constraint
systems. This could be avoided if we allow a variable to be blocked by a witness that is
in a previously analyzed constraint system. This technique would be similar to the one
used in (Pratt, 1978), and to the tree-automata technique used in (Vardi & Wolper, 1986),
improving on simple tableaux methods for variants of propositional dynamic logics. Since
our calculus considers only one constraint system at a time, a modication of the calculus
would be necessary to accomplish this task in a formal way, which is outside the scope of
this paper. The formal development of such a deterministic exponential time procedure will
be a subject for future research.
Notice that, since the domain of the canonical interpretation IS is always nite, we
have also implicitly proved that ALCNR-knowledge bases have the nite model property,
i.e., any satisable knowledge base has a nite model. This property has been extensively
studied in modal logics (Hughes & Cresswell, 1984) and dynamic logics (Harel, 1984). In
particular, a technique, called ltration, has been developed both to prove the nite model
property and to build a nite model for a satisable formula. This technique allows one to
build a nite model from an innite one by grouping the worlds of a structure in equivalence
classes, based on the set of formulae that are satised in each world. It is interesting to
observe that our calculus, based on witnesses, can be considered as a variant of the ltration
technique where the equivalence classes are determined on the basis of our S -equivalence
relation. However, because of number restrictions, variables that are S -equivalent cannot
be grouped, since they might be separated (e.g., they might have been introduced by the
same application of the ! -rule). Nevertheless, they can have the same direct successors,
as stated in point 4.(b) of the denition of canonical interpretation on page 124. This would
correspond to grouping variables of an innite model in such a way that separations are
preserved.

4. Relation to previous work
In this section we discuss the relation of our paper to previous work about reasoning with inclusions. In particular, we rst consider previously proposed reasoning techniques that deal
with inclusions and terminological cycles, then we discuss the relation between inclusions
and terminological cycles.

4.1 Reasoning Techniques
As mentioned in the introduction, previous results were obtained by Baader et al. (1990),
Baader (1990a, 1990b), Nebel (1990, 1991), Schild (1991) and Dionne et al. (1992, 1993).
Nebel (1990, Chapter 5) considers the language T F , containing concept conjunction,
universal quantication and number restrictions, and TBoxes containing (possibly cyclic)
concept denitions, role denitions and disjointness axioms (stating that two concept names
are disjoint). Nebel shows that subsumption of T F -concepts w.r.t. a TBox is decidable.
However, the argument he uses is non-constructive: He shows that it is sucient to con128

Decidable Reasoning in Terminological KR Systems

sider nite interpretations of a size bounded by the size of the TBox in order to decide
subsumption.
In (Baader, 1990b) the eect of the three types of semantics|descriptive, greatest xpoint and least xpoint semantics|for the language FL0, containing concept conjunction
and universal quantication, is described with the help of nite automata. Baader reduces
subsumption of FL0 -concepts w.r.t. a TBox containing (possibly cyclic) denitions of the
form A =: C (which he calls terminological axioms) to decision problems for nite automata.
In particular, he shows that subsumption w.r.t. descriptive semantics can be decided in polynomial space using Buchi automata. Using results from (Baader, 1990b), in (Nebel, 1991)
a characterization of the above subsumption problem w.r.t. descriptive semantics is given
with the help of deterministic automata (whereas Buchi automata are nondeterministic).
This also yields a PSPACE-algorithm for deciding subsumption.
In (Baader et al., 1990) the attention is restricted to the language ALC . In particular,
that paper considers the problem of checking the satisability of a single equation of the
form C = >, where C is an ALC -concept. This problem, called the universal satisability problem, is shown to be equivalent to checking the satisability of an ALC -TBox (see
Proposition 4.1).
In (Baader, 1990a), an extension of ALC , called ALC reg , is introduced, which supports
a constructor to express the transitive closure of roles. By means of transitive closure of
roles it is possible to replace cyclic inclusions of the form A v D with equivalent acyclic
ones. The problem of checking the satisability of an ALC reg -concept is solved in that
paper. It is also shown that using transitive closure it is possible to reduce satisability
of an ALC -concept w.r.t. an ALC -TBox T = fC1 v D1; : : :; Cn v Dn g into the concept
satisability problem in ALC reg (w.r.t. the empty TBox). Since the problem of concept
satisability w.r.t. a TBox is trivially harder than checking the satisability of a TBox,
that paper extends the result given in (Baader et al., 1990).
The technique exploited in (Baader et al., 1990) and (Baader, 1990a) is based on the
notion of concept tree. A concept tree is generated starting from a concept C in order
to check its satisability (or universal satisability). The way a concept tree is generated
from a concept C is similar in avor to the way a complete constraint system is generated
from the constraint system fx: C g. However, the extension of the concept tree method to
deal with number restrictions and individuals in the knowledge base is neither obvious, nor
suggested in the cited papers; on the other hand, the extension of the calculus based on
constraint systems is immediate, provided that additional features have a counterpart in
First Order Logic.
In (Schild, 1991) some results more general than those in (Baader, 1990a) are obtained
by considering languages more expressive than ALC reg and dealing with the concept satisability problem in such languages. The results are obtained by establishing a correspondence
between concept languages and Propositional Dynamic Logics (PDL), and reducing the
given problem to a satisability problem in PDL. Such an approach allows Schild to nd
several new results exploiting known results in the PDL framework. However, it cannot be
used to deal with every concept language. In fact, the correspondence cannot be established
when the language includes some concept constructors having no counterpart in PDL (e.g.,
number restrictions, or individuals in an ABox).
129

Buchheit, Donini, & Schaerf

Recently, an algebraic approach to cycles has been proposed in (Dionne et al., 1992), in
which (possibly cyclic) denitions are interpreted as determining an equivalence relation over
the terms describing concepts. The existence and uniqueness of such an equivalence relation
derives from Aczel's results on non-well founded sets. In (Dionne et al., 1993) the same
researchers prove that subsumption based on this approach is equivalent to subsumption in
greatest xpoint semantics. The language analyzed is a small fragment of the one used in the
TKRS k-rep, and contains conjunction and existential-universal quantications combined
into one construct (hence it is similar to FL0 ). The diculty of extending these results
lies in the fact that it is not clear how individuals can be interpreted in this algebraic
setting. Moreover, we believe that constructive approaches like the algebraic one, give
counterintuitive results when applied to non-constructive features of concept languages|as
negation and number restrictions.
In conclusion, all these approaches, i.e., reduction to automata problems, concept trees,
reduction to PDL and algebraic semantics, deal only with TBoxes and they don't seem to be
suitable to deal also with ABoxes. On the other hand, the constraint system technique, even
though it was conceived for TBox-reasoning, can be easily extended to ABox-reasoning, as
also shown in (Hollunder, 1990; Baader & Hollunder, 1991; Donini et al., 1993).

4.2 Inclusions versus Concept Denitions
Now we compare the expressive power of TBoxes dened as a set of inclusions (as done in
this paper) and TBoxes dened as a set of (possibly cyclic) concept introductions of the
form A _ D and A =: D.
Unlike (Baader, 1990a) and (Schild, 1991), we consider reasoning problems dealing with
TBox and ABox together. Moreover, we use the descriptive semantics for the concept introductions, as we do for inclusions. The result we have obtained is that inclusion statements
and concept introductions actually have the same expressive power. In detail, we show that
the satisability of a knowledge base  = hA; T i, where T is a set of inclusion statements,
can be reduced to the satisability of a knowledge base 0 = hA0; T 0i such that T 0 is a set
of concept introductions. The other direction, from concept introductions to inclusions, is
trivial since introductions of the form A =: D can be expressed by the pair of inclusions
A v D and D v A, while a concept name specication A _ D can be rewritten as the
inclusion A v D (as already mentioned in Section 2).
As a notation, given a TBox T = fC1 v D1 ; : : :; Cn v Dn g, we dene the concept CT
as CT = (:C1 t D1) u    u (:Cn t Dn ). As pointed out in (Baader, 1990a) for ALC , an
interpretation satises a TBox T if and only if it satises the equation CT = >. This result
easily extends to ALCNR, as stated in the following proposition.

Proposition 4.1 Given an ALCNR-TBox T = fC1 v D1; : : :; Cn v Dng, an interpretation I satises T if and only if it satises the equation CT = >.
An interpretation I satises an inclusion C v D if and only if it satises the equation
:C t D = >; I satises the set of equations :C1 t D1 = >,: : : , :Cn t Dn = > if and only
if I satises (:C1 t D1) u    u (:Cn t Dn ) = >. The claim follows.
Proof.

130

Decidable Reasoning in Terminological KR Systems

Given a knowledge base  = hA; T i and a concept A not appearing in , we dene the
knowledge base 0 = hA0 ; T 0 i as follows:
A0 = A [ fA(b) j b is an individual in g
T 0 = fA _ CT u 8P1.A u    u 8Pn .Ag
where P1 ; P2; : : :; Pn are all the role names appearing in . Note that T 0 has a single
inclusion, which could be also thought of as one primitive concept specication.
Theorem 4.2  = hA; T i is satisable if and only if 0 = hA0; T 0i is satisable.
0 the following
Proof. In order to simplify the machinery of the proof, we will use for T
(logically equivalent) form:
T 0 = fA v CT ; A v 8P1.A; : : :; A v 8Pn .Ag
(Note that we use the symbol `v' instead of `_ ' because now the concept name A appears
as the left-hand side of many statements, we must consider these statements as inclusions).
\)" Suppose  = hA; T i satisable. From Theorem 3.7, there exists a complete
constraint system S without clash, which denes a canonical interpretation IS which is a
model of . Dene the constraint system S 0 as follows:
S 0 = S [ fw: A j w is an object in S g
and call IS the canonical interpretation associated to S 0. We prove that IS is a model of
0 .
First observe that every assertion in A is satised by IS since IS is equal to IS except
for the interpretation of A, and A does not appear in A. Therefore, every assertion in A0
is also satised by IS , either because it is an assertion of A, or (if it is an assertion of the
form A(b)) by denition of S 0.
Regarding T 0 , note that by denition of S 0, we have AIS = IS = IS ; therefore both
sides of the inclusions of the form A v 8Pi .A (i = 1; : : :; n) are interpreted as IS , hence
they are satised by IS . Since A does not appear in CT , we have that (CT )IS = (CT )IS .
Moreover, since IS satises T , we also have, by Proposition 4.1, that (CT )IS = IS ,
therefore (CT )IS = (CT )IS = IS = IS . It follows that also both sides of the inclusion
A v CT are interpreted as IS . In conclusion, IS satises T 0 .
\(" Suppose 0 = hA0 ; T 0 i satisable. Again, because of Theorem 3.7, there exists a
complete constraint system S 0 without clash, which denes a canonical interpretation IS
which is a model of 0. We show that IS is also a model of .
First of all, the assertions in A are satised because A  A0 , and IS satises every
assertion in A0 . To prove that IS satises T , we rst prove the following equation:
AIS = IS
(2)
Equation 2 is proved by showing that, for every object s 2 IS , s is in AIS . In order to do
that, observe a general property of constraint systems: Every variable in S 0 is a successor of
an individual. This comes from the denition of the generating rules, which add variables
to the constraint system only as direct successors of existing objects, and at the beginning
S contains only individuals.
Then, Equation 2 is proved by observing the following three facts:
0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

0

131

0

Buchheit, Donini, & Schaerf

1. for every individual b in IS , b 2 AIS ;
0

0

2. if an object s is in AIS , then because IS satises the inclusions AIS  (8P1.A)IS ; : : :;
AIS  (8Pn .A)IS , every direct successor of s is in AIS ;
3. the successor relation is closed under the direct successor relation
0

0

0

0

0

0

0

From the Fundamental Theorem on Induction (see e.g., Wand, 1980, page 41) we conclude that every object s of IS is in AIS . This proves that Equation 2 holds.
From Equation 2, and the fact that IS satises the inclusion AIS  (CT )IS , we derive
that (CT )IS = IS , that is IS satises the equation CT = >. Hence, from Proposition
4.1, IS satises T , and this completes the proof of the theorem.
The machinery present in this proof is not new. In fact, realizing that the inclusions
A v 8P1 .A; : : :; A v 8Pn .A simulate a transitive closure on the roles P1 ; : : :; Pn , one can
recognize similarities with the proofs given by Schild (1991) and Baader (1990a). The dierence is that their proofs rely on the notion of connected model (Baader uses the equivalent
notion of rooted model). In contrast, the models we obtain are not connected, when the
individuals in the knowledge base are not. What we exploit is the weaker property that
every variable in the model is a successor of an individual.
Note that the above reduction strongly relies on the fact that disjunction `t' and complement `:' are within the language. In fact, disjunction and complement are necessary
in order to express all the inclusions of a TBox T inside the concept CT . Therefore, the
proof holds for ALC -knowledge bases, but does not hold for TKRSs not allowing for these
constructors of concepts (e.g., back).
Furthermore, for the language FL0 introduced in Section 4.1, the opposite result holds.
In fact, McAllester (1991) proves that computing subsumption w.r.t. a set of inclusions is
EXPTIME-hard, even in the small language FL0 . Conversely, Nebel (1991) proves that
subsumption w.r.t. a set of cyclic denitions in FL0 can be done in PSPACE. Combining
the two results, we can conclude that for FL0 subsumption w.r.t. a set of inclusions and
subsumption w.r.t. a set of denitions are in dierent complexity classes, hence (assuming
EXPTIME 6= PSPACE) inclusion statements are strictly more expressive than concept
denitions in FL0.
It is still open whether inclusions and denitions are equivalent in languages whose
expressivity is between FL0 and ALC .
0

0

0

0

0

0

0

0

0

5. Discussion

In this paper we have proved the decidability of the main inference services of a TKRS based
on the concept language ALCNR. We believe that this result is not only of theoretical
importance, but bears some impact on existing TKRSs, because a complete procedure can
be easily devised from the calculus provided in Section 3. From this procedure, one can build
more ecient (but still complete) ones, as described at the end of Section 3.2, and also by
applying standard optimization techniques such as those described in (Baader, Hollunder,
Nebel, Protlich, & Franconi, 1992). An optimized procedure can perform well for small
sublanguages where reasoning is tractable, while still being complete when solving more
complex tasks. However, such a complete procedure will still take exponential time and
132

Decidable Reasoning in Terminological KR Systems

space in the worst case, and it may be argued what could be its practical applicability. We
comment in following on this point.
Firstly, a complete procedure (possibly optimized) oers a benchmark for comparing
incomplete procedures, not only in terms of performance, but also in terms of missed inferences. Let us illustrate this point in detail, by providing a blatant paradox: consider the
mostly incomplete constant-time procedure, answering always \No" to any check. Obviously this useless procedure outperforms any other one, if missed inferences are not taken
into account. This paradox shows that incomplete procedures can be meaningfully compared only if missed inferences are considered. But to recognize missed inferences over large
examples, one needs exactly a complete procedure|even if not an ecient one|like ours.
We believe that a fair detection of missed inferences would be of great help even when the
satisfaction of end users is the primary criterion for judging incomplete procedures.
Secondly, a complete procedure can be used for \anytime classication", as proposed
in (MacGregor, 1992). The idea is to use a fast, but incomplete algorithm as a rst step
in analyzing the input knowledge, and then do more reasoning in background. In the
cited paper, resolution-based theorem provers are proposed for performing this background
reasoning. We argue that any specialized complete procedure will perform better than a
general theorem prover. For instance, theorem provers are usually not specically designed
to deal with ltration techniques.
Moreover, our calculus can be easily adapted to deal with rules. As outlined in the
introduction, rules are often used in practical TKRSs. Rules behave like one-way concept
inclusions|no contrapositive is allowed|and they are applied only to known individuals.
Our result shows that rules in ALCNR can be applied also to unknown individuals (our
variables in a constraint system) without endangering decidability. This result is to be
compared with the negative result in (Baader & Hollunder, 1992), where it is shown that
subsumption becomes undecidable if rules are applied to unknown individuals in classic.
Finally, the calculus provides a new way of building incomplete procedures, by modifying
some of the propagation rules. Since the rules build up a model, modications to them
have a semantical counterpart which gives a precise account of the incomplete procedures
obtained. For example, one could limit the size of the canonical model by a polynomial in
the size of the KB. Semantically, this would mean to consider only \small" models, which
is reasonable when the intended models for the KB are not much bigger than the size of the
KB itself. We believe that this way of designing incomplete procedures \from above", i.e.,
starting with the complete set of inferences and weakening it, is dual to the way incomplete
procedures have been realized so far \from below", i.e., starting with already incomplete
inferences and adding inference power by need.
Further research is still needed to address problems issuing from practical systems. For
example, to completely express role restrictions inside number restrictions, qualied number
restrictions (Hollunder & Baader, 1991) should be taken into account. Also, the language
resulting from the addition of enumerated sets (called one-of in classic), and role llers
to ALCNR is still to be studied, although it does not seem to endanger the ltration
method we used. Instead, a dierent method might be necessary if inverse roles are added
to ALCNR, since the nite model property is lost (as shown in Schild, 1991). Finally, the
addition of concrete domains (Baader & Hanschke, 1991) remains open.
133

Buchheit, Donini, & Schaerf

Acknowledgements
We thank Maurizio Lenzerini for the inspiration of this work, as well as for several discussions that contributed to the paper. Werner Nutt pointed out to us the observation mentioned at the end of Section 3, and we thank him and Franz Baader for helpful comments
on earlier drafts. We thank also the anonymous reviewers, whose stimulating comments
helped us in improving on the submitted version.
The research was partly done while the rst author was visiting the Dipartimento di Informatica e Sistemistica, Universita di Roma \La Sapienza". The third author also acknowledges Yoav Shoham for his hospitality at the Computer Science Department of Stanford
University, while the author was developing part of this research.
This work has been supported by the ESPRIT Basic Research Action N.6810 (COMPULOG 2) and by the Progetto Finalizzato Sistemi Informatici e Calcolo Parallelo of the
CNR (Italian Research Council), LdR \Ibridi".

References

Abrial, J. (1974). Data semantics. In Klimbie, J., & Koeman, K. (Eds.), Data Base
Management, pp. 1{59. North-Holland Publ. Co., Amsterdam.
Baader, F. (1990a). Augmenting concept languages by transitive closure of roles: An alternative to terminological cycles. Tech. rep. RR-90-13, Deutsches Forschungszentrum
fur Kunstliche Intelligenz (DFKI), Kaiserslautern, Germany. An abridged version appeared in Proc. of the 12th Int. Joint Conf. on Articial Intelligence IJCAI-91, pp.
446{451.
Baader, F. (1990b). Terminological cycles in KL-ONE-based knowledge representation languages. Tech. rep. RR-90-01, Deutsches Forschungszentrum fur Kunstliche Intelligenz
(DFKI), Kaiserslautern, Germany. An abridged version appeared in Proc. of the 8th
Nat. Conf. on Articial Intelligence AAAI-90, pp. 621{626.
Baader, F., Burkert, H.-J., Hollunder, B., Nutt, W., & Siekmann, J. H. (1990). Concept
logics. In Lloyd, J. W. (Ed.), Computational Logics, Symposium Proceedings, pp.
177{201. Springer-Verlag.
Baader, F., & Hanschke, P. (1991). A schema for integrating concrete domains into concept
languages. In Proc. of the 12th Int. Joint Conf. on Articial Intelligence IJCAI-91,
pp. 452{457 Sydney.
Baader, F., & Hollunder, B. (1991). A terminological knowledge representation system with
complete inference algorithm. In Proc. of the Workshop on Processing Declarative
Knowledge, PDK-91, Lecture Notes in Articial Intelligence, pp. 67{86. SpringerVerlag.
Baader, F., & Hollunder, B. (1992). Embedding defaults into terminological knowledge
representation formalisms. In Proc. of the 3rd Int. Conf. on Principles of Knowledge
Representation and Reasoning KR-92, pp. 306{317. Morgan Kaufmann, Los Altos.
134

Decidable Reasoning in Terminological KR Systems

Baader, F., Hollunder, B., Nebel, B., Protlich, H.-J., & Franconi, E. (1992). An empirical
analisys of optimization techniques for terminological representation systems. In Proc.
of the 3rd Int. Conf. on Principles of Knowledge Representation and Reasoning KR92, pp. 270{281. Morgan Kaufmann, Los Altos.
Beck, H. W., Gala, S. K., & Navathe, S. B. (1989). Classication as a query processing
technique in the CANDIDE semantic data model. In Proc. of the 5th IEEE Int. Conf.
on Data Engineering.
Borgida, A., Brachman, R. J., McGuinness, D. L., & Alperin Resnick, L. (1989). CLASSIC:
A structural data model for objects. In Proc. of the ACM SIGMOD Int. Conf. on
Management of Data, pp. 59{67.
Brachman, R. J., & Levesque, H. J. (1984). The tractability of subsumption in framebased description languages. In Proc. of the 4th Nat. Conf. on Articial Intelligence
AAAI-84, pp. 34{37.
Brachman, R. J., Pigman Gilbert, V., & Levesque, H. J. (1985). An essential hybrid
reasoning system: Knowledge and symbol level accounts in KRYPTON. In Proc. of
the 9th Int. Joint Conf. on Articial Intelligence IJCAI-85, pp. 532{539 Los Angeles.
Brachman, R. J., & Schmolze, J. G. (1985). An overview of the KL-ONE knowledge representation system. Cognitive Science, 9 (2), 171{216.
Buchheit, M., Donini, F. M., & Schaerf, A. (1993). Decidable reasoning in terminological
knowledge representation systems. Tech. rep. RR-93-10, Deutsches Forschungszentrum fur Kunstliche Intelligenz (DFKI), Saarbrucken, Germany. An abridged version
appeared in Proc. of the 13th Int. Joint Conf. on Articial Intelligence IJCAI-93 pp.
704{709.
Catarci, T., & Lenzerini, M. (1993). Representing and using interschema knowledge in
cooperative information systems. Journal of Intelligent and Cooperative Inf. Syst. To
appear.
Dionne, R., Mays, E., & Oles, F. J. (1992). A non-well-founded approach to terminological
cycles. In Proc. of the 10th Nat. Conf. on Articial Intelligence AAAI-92, pp. 761{766.
AAAI Press/The MIT Press.
Dionne, R., Mays, E., & Oles, F. J. (1993). The equivalence of model theoretic and structural
subsumption in description logics. In Proc. of the 13th Int. Joint Conf. on Articial
Intelligence IJCAI-93, pp. 710{716 Chambery, France. Morgan Kaufmann, Los Altos.
Donini, F. M., Hollunder, B., Lenzerini, M., Marchetti Spaccamela, A., Nardi, D., & Nutt,
W. (1992). The complexity of existential quantication in concept languages. Articial
Intelligence, 2{3, 309{327.
Donini, F. M., Lenzerini, M., Nardi, D., & Nutt, W. (1991a). The complexity of concept
languages. In Allen, J., Fikes, R., & Sandewall, E. (Eds.), Proc. of the 2nd Int.
Conf. on Principles of Knowledge Representation and Reasoning KR-91, pp. 151{162.
Morgan Kaufmann, Los Altos.
135

Buchheit, Donini, & Schaerf

Donini, F. M., Lenzerini, M., Nardi, D., & Nutt, W. (1991b). Tractable concept languages.
In Proc. of the 12th Int. Joint Conf. on Articial Intelligence IJCAI-91, pp. 458{463
Sydney.
Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1991c). A hybrid system integrating
datalog and concept languages. In Proc. of the 2nd Conf. of the Italian Association
for Articial Intelligence, No. 549 in Lecture Notes in Articial Intelligence. SpringerVerlag. An extended version appeared also in the Working Notes of the AAAI Fall
Symposium \Principles of Hybrid Reasoning".
Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1993). Deduction in concept languages: From subsumption to instance checking. Journal of Logic and Computation.
To appear.
Fitting, M. (1990). First-Order Logic and Automated Theorem Proving. Springer-Verlag.
Garey, M., & Johnson, D. (1979). Computers and Intractability|A guide to NPcompleteness. W.H. Freeman and Company, San Francisco.
Harel, D. (1984). Dynamic logic. In Handbook of Philosophical Logic, Vol. 2, pp. 497{640.
D. Reidel, Dordrecht, Holland.
Heinsohn, J., Kudenko, D., Nebel, B., & Protlich, H.-J. (1992). An empirical analysis of
terminological representation systems. In Proc. of the 10th Nat. Conf. on Articial
Intelligence AAAI-92, pp. 767{773. AAAI Press/The MIT Press.
Hollunder, B. (1990). Hybrid inferences in KL-ONE-based knowledge representation systems. In Proc. of the German Workshop on Articial Intelligence, pp. 38{47. SpringerVerlag.
Hollunder, B., & Baader, F. (1991). Qualifying number restrictions in concept languages.
Tech. rep. RR-91-03, Deutsches Forschungszentrum fur Kunstliche Intelligenz (DFKI),
Kaiserslautern, Germany. An abridged version appeared in Proc. of the 2nd Int. Conf.
on Principles of Knowledge Representation and Reasoning KR-91.
Hughes, G. E., & Cresswell, M. J. (1984). A Companion to Modal Logic. Methuen, London.
Kaczmarek, T. S., Bates, R., & Robins, G. (1986). Recent developments in NIKL. In Proc.
of the 5th Nat. Conf. on Articial Intelligence AAAI-86, pp. 978{985.
Lenzerini, M., & Schaerf, A. (1991). Concept languages as query languages. In Proc. of the
9th Nat. Conf. on Articial Intelligence AAAI-91, pp. 471{476.
Levesque, H. J. (1984). Foundations of a functional approach to knowledge representation.
Articial Intelligence, 23, 155{212.
Lewis, H. R., & Papadimitriou, C. H. (1981). Elements of the Theory of Computation.
Prentice-Hall, Englewood Clis, New Jersey.
MacGregor, R. (1991). Inside the LOOM description classier. SIGART Bulletin, 2 (3),
88{92.
136

Decidable Reasoning in Terminological KR Systems

MacGregor, R. (1992). What's needed to make a description logic a good KR citizen. In
Working Notes of the AAAI Fall Symposium on Issues on Description Logics: Users
meet Developers, pp. 53{55.
MacGregor, R., & Bates, R. (1987). The Loom knowledge representation language. Tech.
rep. ISI/RS-87-188, University of Southern California, Information Science Institute,
Marina del Rey, Cal.
MacGregor, R., & Brill, D. (1992). Recognition algorithms for the LOOM classier. In
Proc. of the 10th Nat. Conf. on Articial Intelligence AAAI-92, pp. 774{779. AAAI
Press/The MIT Press.
Mays, E., Dionne, R., & Weida, R. (1991). K-REP system overview. SIGART Bulletin,
2 (3).
McAllester, D. (1991). Unpublished manuscript.
McGuinness, D. L. (1992). Making description logic based knowledge representation systems
more usable. In Working Notes of the AAAI Fall Sysmposium on Issues on Description
Logics: Users meet Developers, pp. 56{58.
Mylopoulos, J., Bernstein, P., & Wong, E. (1980). A language facility for designing databaseintensive applications. ACM Trans. on Database Syst., 5 (2), 185{207.
Nebel, B. (1988). Computational complexity of terminological reasoning in BACK. Articial
Intelligence, 34 (3), 371{383.
Nebel, B. (1990). Reasoning and Revision in Hybrid Representation Systems. Lecture Notes
in Articial Intelligence. Springer-Verlag.
Nebel, B. (1991). Terminological cycles: Semantics and computational properties. In Sowa,
J. F. (Ed.), Principles of Semantic Networks, pp. 331{361. Morgan Kaufmann, Los
Altos.
Nutt, W. (1992). Personal communication.
Patel-Schneider, P. F. (1984). Small can be beautiful in knowledge representation. In Proc.
of the IEEE Workshop on Knowledge-Based Systems. An extended version appeared
as Fairchild Tech. Rep. 660 and FLAIR Tech. Rep. 37, October 1984.
Patel-Schneider, P. (1989). Undecidability of subsumption in NIKL. Articial Intelligence,
39, 263{272.
Pratt, V. R. (1978). A practical decision method for propositional dynamic logic. In Proc.
of the 10th ACM SIGACT Symp. on Theory of Computing STOC-78, pp. 326{337.
Quantz, J., & Kindermann, C. (1990). Implementation of the BACK system version 4. Tech.
rep. KIT-Report 78, FB Informatik, Technische Universitat Berlin, Berlin, Germany.
Rich, editor, C. (1991). SIGART bulletin. Special issue on implemented knowledge representation and reasoning systems. (2)3.
137

Buchheit, Donini, & Schaerf

Schaerf, A. (1993a). On the complexity of the instance checking problem in concept languages with existential quantication. Journal of Intelligent Information Systems, 2,
265{278. An abridged version appeared in Proc. of the 7th Int. Symp. on Methodologies for Intelligent Systems ISMIS-93.
Schaerf, A. (1993b). Reasoning with individuals in concept languages. Tech. rep. 07.93,
Dipartimento di Informatica e Sistemistica, Universita di Roma \La Sapienza". An
abridged version appeared in Proc. of the 3rd Conf. of the Italian Association for
Articial Intelligence AI*IA-93.
Schild, K. (1988). Undecidability of subsumption in U . Tech. rep. KIT-Report 67, FB
Informatik, Technische Universitat Berlin, Berlin, Germany.
Schild, K. (1991). A correspondence theory for terminological logics: Preliminary report.
In Proc. of the 12th Int. Joint Conf. on Articial Intelligence IJCAI-91, pp. 466{471
Sydney.
Schmidt-Schau, M. (1989). Subsumption in KL-ONE is undecidable. In Brachman, R. J.,
Levesque, H. J., & Reiter, R. (Eds.), Proc. of the 1st Int. Conf. on Principles of
Knowledge Representation and Reasoning KR-89, pp. 421{431. Morgan Kaufmann,
Los Altos.
Schmidt-Schau, M., & Smolka, G. (1991). Attributive concept descriptions with complements. Articial Intelligence, 48 (1), 1{26.
Vardi, M., & Wolper, P. (1986). Automata-theoretic techniques for modal logics of programs. Journal of Computer and System Science, 32, 183{221. A preliminary version
appeared in Proc. of the 16th ACM SIGACT Symp. on Theory of Computing STOC84.
Vilain, M. (1991). Deduction as parsing: Tractable classication in the KL-ONE framework.
In Proc. of the 9th Nat. Conf. on Articial Intelligence AAAI-91, pp. 464{470.
Wand, M. (1980). Induction, Recursion, and Programming. North-Holland Publ. Co.,
Amsterdam.
Woods, W. A., & Schmolze, J. G. (1992). The KL-ONE family. In Lehmann, F. (Ed.),
Semantic Networks in Articial Intelligence, pp. 133{178. Pergamon Press. Published
as a special issue of Computers & Mathematics with Applications, Volume 23, Number
2{9.

138

Journal of Articial Intelligence Research 1 (1994) 277{308

Submitted 3/94; published 6/94

A Semantics and Complete Algorithm for
Subsumption in the CLASSIC Description Logic
Alex Borgida

borgida@cs.rutgers.edu

Department of Computer Science
Rutgers University
New Brunswick, NJ 08904 U. S. A.

Peter F. Patel-Schneider

pfps@research.att.com

AT&T Bell Laboratories
600 Mountain Avenue
Murray Hill, NJ 07974 U. S. A.

Abstract

This paper analyzes the correctness of the subsumption algorithm used in classic,
a description logic-based knowledge representation system that is being used in practical
applications. In order to deal eciently with individuals in classic descriptions, the developers have had to use an algorithm that is incomplete with respect to the standard,
model-theoretic semantics for description logics. We provide a variant semantics for descriptions with respect to which the current implementation is complete, and which can
be independently motivated. The soundness and completeness of the polynomial-time subsumption algorithm is established using description graphs, which are an abstracted version
of the implementation structures used in classic, and are of independent interest.

1. Introduction to Description Logics

Data and knowledge bases are models of some part of the natural world. Such models
are often built from individual objects that are inter-related by relationships and grouped
into classes that capture commonalities among their instances. Description logics (DLs),
also known as terminological logics, form a class of languages used to build and access such
models; their distinguishing feature is that classes (usually called concepts) can be dened
intensionally|in terms of descriptions that specify the properties that objects must satisfy
to belong to the concept. These descriptions are expressed using some language that allows
the construction of composite descriptions, including restrictions on the binary relationships
(usually called roles) connecting objects.
As an example, consider the description
GAME u 4 participants u 8participants:(PERSON u gender : Female):1
This description characterizes objects in the intersection (u) of three sub-descriptions:
GAME|objects that belong to the atomic concept; 4 participants|objects with at least
four llers for the participants role; and 8participants:(PERSON u gender : Female)|objects
all of whose participants llers are restricted to belong to PERSONs, which themselves have
gender role lled by the value Female.
1. The notation used for descriptions here is the standard notation in the description logic community
(Baader et al., 1991). The classic notation is not used because it is more verbose.

c 1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Borgida & Patel-Schneider

A key dierence between DLs and the standard representation formalisms based on
First-Order Logic, e.g., relational and deductive databases, is that DLs provide an arena
for exploring new sets of \logical connectives"|the constructors used to form composite
descriptions|that are dierent from the standard connectives such as conjunction, universal
quantiers, etc.. Therefore, DLs provide a new space in which to search for expressive
yet eectively computable representation languages. Moreover, although it is possible to
translate many aspects of DLs currently encountered into First Order Logic, reasoning with
such a translation would be a very poor substitute because DL-based systems reason in a
way that does not resemble standard theorem proving (e.g., by making use of imperative
programming features).
Descriptions such as the one above can be used in several ways in a knowledge base
management system (KBMS) based on a description logic:
1. To state queries: The KBMS can locate all the objects that satisfy the description's
properties.
2. To dene and classify concepts: Identiers can be attached to descriptions, in the manner of views in relational DBMSs. The system can in addition automatically determine
the \subclass" relationship between pairs of such concepts based on their denitions.
For example, a concept dened by the above description would be subsumed by a
concept dened by \games with at least two participants" (GAME u 2 participants).
3. To provide partial information about objects: It is important to understand that distinct DL descriptions can be ascribed to arbitrary individuals (e.g., \today's game
of cards|individual Bgm#467|will have exactly two participants from the following
set of three : : : , all of whom like tea and rum"). Note that unlike database systems, DL-based KBMSs do not require descriptions to be predened. This provides
considerable power in recording partial knowledge about objects.
4. To detect errors: It is possible to determine whether two descriptions are disjoint,
whether a description is incoherent or not, and whether ascribing a description to an
individual leads to an inconsistency.
Quite a number of KBMSs based on description logics have been built, including classic
(Resnick et al., 1992), loom (MacGregor & Bates, 1987), and back (Peltason et al., 1987).
Such systems have been used in several practical situations, including software information
bases (Devanbu et al., 1991), nancial management (Mays et al., 1987), conguration management (Owsnicki-Klewe, 1988; Wright et al., 1993), and data exploration. Additional
signs that DLs are signicant subjects of study are the several recent workshops on DLs
(Nebel et al., 1991; Peltason et al., 1991; AAAI, 1992).

1.1 On the Tractability and Completeness of DL Implementations

The fundamental operation on descriptions is determining whether one description is more
general, or subsumes, another, in the sense that any object satisfying the latter would also
satisfy the conditions of the former. In parallel with the surge of work on nding tractable
yet expressive subsets of rst order logic, the DL research community has been investigating
the complexity of reasoning with various constructors. The rst result in this area (Levesque
278

Subsumption in CLASSIC

& Brachman, 1987) showed that even a seemingly simple addition to a very small language
can lead to subsumption determination becoming NP-hard. A more recent, striking pair of
results (Patel-Schneider, 1989b; Schmidt-Schauss, 1989) shows that adding the ability to
represent equalities of role compositions makes the complexity of the subsumption problem
leap from quadratic to undecidable.
There are three possible responses to these intractability results:
 Provide an incomplete implementation of the DL reasoner, in the sense that there
are inferences sanctioned by the standard semantics of the constructors that are not
performed by the algorithm. This approach, explicitly adopted by the loom system
implementers (MacGregor & Bates, 1987), and advocated by some users (Doyle &
Patil, 1991), has one major diculty: how can one describe to users the inferences
actually drawn by the implementation so that systems with known properties can be
implemented on top of such KBMS? Two solutions to this problem have been suggested: alternative semantic accounts (based on weaker, 4-valued logics, for example)
(Patel-Schneider, 1989a), and proof-theoretic semantics (Borgida, 1992).
 Provide a complete implementation of a specic DL reasoner, acknowledging that in
certain circumstances it may take an inordinate amount of time. This approach,
followed in systems such as kris (Baader & Hollunder, 1991), has the problem of
unpredictability: when will the system \go o into the wild blue yonder"? And of
course, in some circumstances this is impossible to even attempt since the reasoning
problem is undecidable.
 Carefully devise a language of limited expressive power for which reasoning is tractable,
and then provide a complete implementation for it. This was the approach chosen
by the designers of such languages as kandor (Patel-Schneider, 1984) and krypton
(Brachman et al., 1983), and is close to the approach in classic (Borgida et al.,
1989).
A hidden diculty in the second and third approach is to produce an implementation
that is correct (\complete") with respect to the semantics. This diculty is illustrated by
the discovery, several years later, that the implementation of kandor, as well as candide
(Beck et al., 1989), was in fact incomplete, and its subsumption problem is NP-hard (Nebel,
1988), rather than polynomial, as was claimed; this happened despite the fact that Kandor
is a very \small" language in comparison with other DLs, and its implementation appeared
to be evidently correct. To avoid such problems, it is necessary to produce convincing
demonstrations that the algorithm is correct; several such proofs have in fact already appeared in the DL literature (e.g., (Patel-Schneider, 1987; Hollunder & Nutt, 1990; Donini
et al., 1991)), albeit only for languages that have not seen use in practical applications.

1.2 Outline

The classic 12 system is a reasoner based on a moderately complicated DL. It is being
used in commercial (Wright et al., 1993) and prototype applications at AT&T, and is made
available to academic researchers by AT&T Bell Laboratories.
2. classic 1 is the rst released version of classic. A new version, classic 2, with a more expressive DL,
has recently been released.

279

Borgida & Patel-Schneider

One purpose of this paper is to provide a rigorous formal analysis of the correctness
and eciency for the classic DL subsumption algorithm.3 We start by presenting such
a result for a subset of the language, which we call Basic classic. The subsumption
algorithm relies on the transformation of descriptions into a data structure, which we call
description graphs, and which are a generalization of A-Kaci's psi-terms (1984). In the
process of normalizing such a graph to a canonical form, we remove obvious redundancies
and explicate certain implicit facts, encoding in particular the innite set of inferences that
can be drawn from so-called \coreference constraints". The correctness of the subsumption
algorithm is demonstrated rigorously by showing how to construct (inductively) a countermodel in case the algorithm returns the answer \no".
Next, we explore the eect of adding individuals to descriptions. We show that, using
individuals, one can encode disjunctive information leading to the need to examine combinatorially many possibilities. The classic implementation is in fact incomplete with respect
to the standard semantics. The second contribution of this paper is then a well-motivated,
understandable, and small change to the standard semantics that alleviates this problem.
We extend the subsumption algorithm and its proof of correctness to deal with individuals
under the modied semantics, thereby characterizing in some sense the \incompleteness"
of the reasoner.
This paper therefore illustrates all three paradigms described above, albeit in a nonstandard manner for the second paradigm, and does so for the rst time on a realistic
language with signicant practical use.

2. Basic CLASSIC
Descriptions in Basic classic are built up from a collection of atomic concept names, role
names, and attribute names. Roles and attributes are always atomic but descriptions can
be built up using operators/constructors such as value restrictions and number restrictions,
as we indicate below.
Basic classic incorporates objects from the host programming language,4 called host
individuals, which form a distinct group from classic individuals; only the latter can have
roles or attributes of their own, the former being restricted to be role or attribute llers.
The denotational semantics of classic descriptions starts, as usual, with a domain of
values, , subsets of which are extensions for descriptions, while subsets of    are
extensions of roles and attributes. This domain is in fact disjointly divided into two realms,
the host realm, H , containing objects corresponding to host language individuals, and the
classic realm C , containing the other objects. Every description, except for THING, which
denotes the entire domain has as its extension a subset of either the classic realm or the
host realm. (NOTHING denotes the empty set, which is therefore both a classic and host
concept.) The extension of a role in a possible world is a relation from the classic realm to
the entire domain, while the extension of an attribute is a function from the classic realm
into the entire domain.
3. In empirical tests (Heinsohn et al., 1992), classic has emerged as the fastest of the current DL
implementations.
4. A general scheme for incorporating such host objects is described in (Baader & Hanschke, 1991).

280

Subsumption in CLASSIC

Host descriptions are relatively simple: (i) HOST-THING, denoting the entire host realm,
H ; (ii) special, pre-dened names corresponding to the types in the host programming language; and (iii) conjunctions of the above descriptions. The descriptions corresponding to
the host programming language types have pre-dened extensions and subsumption relationships, mirroring the subtype relationship in the host programming language. This
subtype relationship is satised in all possible worlds/interpretations. We require that (i)
all host concepts have an extension that is either of innite size or is empty; (ii) that if
the extensions of two host concepts overlap, then one must be subsumed by the other, i.e.,
types are disjoint, unless they are subtypes of each other; and (iii) that a host concept has
an innite number of extra instances than each of its child concepts. (These conditions are
needed to avoid being able to infer conclusions from the size of host descriptions.) This
allows for host concepts like INTEGER, REAL, COMPLEX, and STRING, but not BOOLEAN
or NON-ZERO-INTEGER .
Non-host (classic) descriptions in Basic classic are formed according to the following
syntax:
Syntax
Constructor Name
CLASSIC-THING
E
Atomic Concept Name
CuD
Intersection
8R:C
Role Value Restriction
8A:C
Attribute Value Restriction
n R
Minimum Number Restriction
m R
Maximum Number Restriction
A1  : : : Ak = B1  : : : Bh Equality Restriction
where E is an atomic concept name; C and D are classic descriptions; R is a role; A, Ai ,
and Bj are attributes; n,k,h are positive integers; and m is a non-negative integer. The set
of constructors in Basic classic was judiciously chosen to result in a language in which
subsumption is easy to compute.
The denotational semantics for descriptions in Basic classic is recursively built on the
extensions assigned to atomic names by a possible world:
Denition 1 A possible world/interpretation, I , consists of a domain, , and an interpretation function :I . The domain is disjointly divided into a classic realm, C , and a host
realm, H . The interpretation function assigns extensions to atomic identiers as follows:
 The extension of an atomic concept name E is some subset EI of the classic realm.
 The extension of an atomic role name R is some subset RI of C  .
 The extension of an atomic attribute name A is some total function AI from C to
.
The extension CI of a non-atomic classic description is computed as follows:
 CLASSIC-THINGI = C .
 (C u D)I = CI \ DI .
281

Borgida & Patel-Schneider

 (8p:C)I = fd 2 C j 8x (d; x) 2 pI ) x 2 CI g, i.e., those objects in C all of
whose p-role or p-attribute llers are in the extension of C;

 (n p)I (resp. (n p)I ) is those objects in C with at least (resp. at most) n llers
for role p.

 (A1 : : : Ak = B1 : : : Bh )I = fd 2 C j Ak I (: : : A1I (d)) = BhI (: : : B1I (d))g, i.e.,

those objects in C with the property that applying the composition of the extension
of the Ai s and the composition of the extension of the Bj s to the object both result in
the same value.5

A description, D1, is then said to subsume another, D2 , if for all possible worlds I , D2I 
D1 I .

Of key interest is the computation of the subsumption relationship between descriptions
in Basic classic. Subsumption computation is a multi-part process. First, descriptions
are turned into description graphs. Next, description graphs are put into canonical form,
where certain inferences are explicated and other redundancies are reduced by combining
nodes and edges in the graph. Finally, subsumption is determined between a description
and a canonical description graph.
To describe in detail the above process, we start with a formal denition of the notion
of description graph (Denition 2), and then present techniques for

 translating a description to a description graph (Section 2.2), which requires merging
pairs of nodes, and pairs of graphs (Denitions 4 and 5);

 putting a description graph into canonical form (Section 2.3);
 determining whether a description subsumes a description graph (Algorithm 1).
To prove the correctness of this approach, we need to show that the rst two steps
lead us in the right direction, i.e., that the following three questions are equivalent: \Does
description D subsume description C?", \Does description D subsume graph GC ?", and
\Does description D subsume graph canonical(GC )?". To do this, we need to dene the
formal semantics of both descriptions and graphs (Denitions 1 and 3), and then prove the
results (Theorems 1 and 2). To prove the \completeness" of the subsumption algorithm, we
show that if the algorithm does not indicate that D subsumes canonical(GC ), then we can
construct an interpretation (\graphical world") in which some object is in the denotation
of canonical(GC ) but not that of D.

2.1 Description Graphs

One way of developing a subsumption algorithm is to rst transform descriptions into a
canonical form, and then determine subsumption relationships between them. Canonical
descriptions can normally be thought of as trees since descriptions are terms in a rst order
term language. The presence of equality restrictions in classic signicantly changes the
5. Note that both attribute chains must have a denite value, and that all but the last cannot evaluate to
host individuals, since these cannot have attributes.

282

Subsumption in CLASSIC

fCLASSIC-THING
g
t

fTHING
t g
:




,


,


captain,
coach



,


participants - t fPERSONg
t 

,

fGAMEg

father

[0; 1]

Figure 1: A description graph.
handling of subsumption because they introduce relationships between dierent pieces of
the normal form. Most signicantly, in the presence of equalities, a small description, such
as 8friend:TALL u friend = friendfriend, can be subsumed by descriptions of arbitrary size,
such as
8friend:(8friend:(: : : (8friend:TALL) : : :)):
In order to record such sets of inferences in the canonical form, we will resort to a graphbased representation, suggested by the semantic-network origins of description logics, and
the work of A-Kaci (1984).
Intuitively, a description graph is a labelled, directed multigraph, with a distinguished
node. Nodes of the graph correspond to descriptions, while edges of the graph correspond
to restrictions on roles or attributes. The edges of the graph are labelled with the role name
and the minimum and maximum number of llers associated with the edge, or just with the
attribute name. The nodes of the graph are labelled with concept names associated with
the node concept. For example, Figure 1 is a description graph, which, as we shall see later,
corresponds to the description GAME u 8participants: PERSON u coach = (captainfather).
Because equality restrictions (and hence the non-tree portions of the graph) involve only
attributes, edges labelled with roles are all cut-edges, i.e., their removal increases by one the
number of connected components of the graph. This restriction is important because if the
graph is in tree form, there is really no dierence between a graphical and a linear notation,
and a semantics is simple to develop. If the graph is a general directed acyclic graph,
then there is the problem of relating the semantics generated by two dierent paths in the
graph that share the same beginning and ending nodes. If the graph contains cycles, the
problem of developing a correct semantics is even more dicult, as a simplistic semantics
will be non-well-founded, and some sort of xed-point or model-preference semantics will be
required. Fortunately, any non-tree parts of our graphical notation will involve attributes
only, and because attributes are functional, our job will be much easier.
As a result of the above restrictions, it is possible to view a description graph as having
the following recursive structure: (i) There is a distinguished node r, which has an \island"
of nodes connected to it by edges labelled with attributes. (ii) Nodes in this island may
have 0 or more edges labelled with roles leaving them, pointing to distinguished nodes of
other description graphs. (iii) These graphs share no nodes or edges in common with each
other, nor with the islands above them.
283

Borgida & Patel-Schneider

Because of this recursive structure, it is easier to represent description graphs using a
recursive denition, instead of the usual graph denition. This recursive denition is similar
to the recursive denition of a tree, which states that a tree consists of some information
(the information on the root of the tree) plus a set of trees (the children of the root of the
tree). As description graphs are more complex than simple trees, we will have to use a
two-part denition.

Denition 2 A description graph is a triple, hN; E; ri, consisting of a set N of nodes; a

bag E of edges (a-edges) labelled with attribute names; and a distinguished node r in N .
Elements of E will be written hn1 ; n2; Ai where n1 and n2 are nodes and A is an attribute
name.
A node in a description graph is a pair, hC; H i consisting of a set C of concept names
(the atoms of the node), and a bag H of tuples (the r-edges of the node). An r-edge is a
tuple, hR; m; M; Gi, of a role name, R; a min, m, which is a non-negative integer; a max,
M , which is a non-negative integer or 1; and a (recursively nested) description graph G,
representing the restriction on the llers of the role. (G will often be called the restriction
graph of the node.)
Concept names in a description graph are atomic concept names, host concept names,
THING, CLASSIC-THING, or HOST-THING.

Descriptions graphs are provided extensions starting from the same possible worlds I
as used for descriptions. However, in addition we need a way of identifying the individuals
to be related by attributes, which will be given by the function .

Denition 3 Let G = hN; E; ri be a description graph and let I be a possible world. Then

the interpretation GI of G, and the interpretation nI of each of the nodes in N , are recursively (and mutually) dened as follows:
An element, d, of  is in GI , i there is some function, , from N into  such that
1. d = (r);
2. for all n 2 N (n) 2 nI ;
3. for all hn1 ; n2; Ai 2 E we have h(n1 ); (n2)i 2 AI , (which is equivalent to (n2 ) =
AI ((n1)), since AI is a function).
An element, d, of  is in nI , where n = hC; H i, i
1. for all C 2 C , we have d 2 CI ; and
2. for all hR; m; M; Gi 2 H ,
(a) there are between m and M elements, d0, of the domain such that hd; d0i 2 RI
and
(b) d0 2 GI for all d0 such that hd; d0i 2 RI .
284

Subsumption in CLASSIC

2.2 Translating Descriptions to Description Graphs

A Basic classic description is turned into a description graph by a recursive process,
working from the \inside out". In this process, description graphs and nodes are often
merged.

Denition 4 The merge of two nodes, n1  n2, is a new node whose atoms are the union

of the atoms of the two nodes and whose r-edges are the union of the r-edges of the two
nodes6.

Denition 5 The merge of two description graphs, G1  G2, is a description graph whose

nodes are the disjoint union7 of the non-distinguished nodes of G1 and G2 plus a new
distinguished node. The edges of the merged graph are the union of the edges of G1 and G2,
except that edges touching on the distinguished nodes of G1 or G2 are modied to touch the
new distinguished node. The new distinguished node is the merge of the two distinguished
nodes of G1 and G2.

The rules for translating a description C in Basic classic into a description graph GC
are as follows:
1. A description that consists of a concept name is turned into a description graph with
one node and no a-edges. The atoms of the node contains only the concept name.
The node has no r-edges.
2. A description of the form n R is turned into a description graph with one node and
no a-edges. The node has as its atoms CLASSIC-THING and has a single r-edge with
role R, min n, max 1, and restriction GTHING .
3. A description of the form n R is turned into a description graph with one node and
no a-edges. The node has as its atoms CLASSIC-THING and a single r-edge with role
R, min 0, max n, and restriction GTHING .
4. A description of the form 8R:C, with R a role, is turned into a description graph with
one node and no a-edges. The node has as its atoms CLASSIC-THING and has a single
r-edge with role R, min 0, max 1, and restriction GC.
5. To turn a description of the form C u D into a description graph, construct GC and
GD and merge them.
6. To turn a description of the form 8A:C, with A an attribute, into a description graph,
rst construct the description graph hNC ; EC ; rC i for C. The description graph for
8A:C is hNC [ ftg; EC [ fht; rC ; Aig; ti, where t is the node hfCLASSIC-THINGg; fgi.
7. To turn a description of the form A1  : : : An = B1  : : : Bm into a description graph
rst create a distinguished node, node r, with CLASSIC-THING as its atoms, and a
node e, with THING as its atoms. For 1  i  n , 1 create a node ai , with its atoms
6. Note that duplicate edges, such as ones joining ni to ni , are not removed, since the edges form a bag.
7. In taking the disjoint union of two sets, elements of one may be systematically renamed rst to make
sure that the sets are non-overlapping.

285

Borgida & Patel-Schneider

being CLASSIC-THING. For 1  j  m , 1 create a node bj , with its atoms being
CLASSIC-THING. None of the ai or bj have r-edges.
If n = 1, create the edge hr; e; A1i; if n > 1 then create edges hr; a1; A1i, han,1 ; e; An i,
and hai,1 ; ai; Ai i for 2  i  n , 1.
Similarly, if m = 1, create the edge hr; e; B1i; if m > 1 then create edges hr; b1; B1 i,
hbm,1; e; Bmi, and hbi,1; bi; Bii for 2  i  m , 1.
This creates two disjoint paths, one for the Ai and one for the Bj , from the distinguished node to the end node.
Figure 1 presents a view of a description graph constructed in this fashion from the
description GAME u 8participants:PERSON u coach = captainfather:
Now we want to show that this process preserves extensions. As we use the merge
operations we rst show that they work correctly.

Lemma 1 If n1 and n2 are nodes then (n1  n2)I = nI1 \ nI2 . If D1 and D2 are description
graphs then (D1  D2)I = D1I \ D2I .
Proof: Since the components (atoms and r-edges) of the merged node are obtained by

unioning the components of the respective nodes, and since the interpretation of a node
is the intersection of the interpretation of its components, the result is obviously true for
nodes.
For merging graphs, the only dierence is that the root nodes are replaced by their
merger in all edges, as well as the root. But then an element of (D1  D2)I is clearly an
element of both D1I and D2I . Conversely, since we take the disjoint union of the other nodes
in the two graphs, the mapping functions 1 and 2 in Denition 3 can simply be unioned,
so that an element of both D1I and D2I is an element of the merged root node, and hence
of (D1  D2 )I .

Theorem 1 For all possible worlds, the extension of a description is the same as the ex-

tension of its description graph.
Proof: The proof is by structural induction on descriptions.
The extension of concept names, cardinality restrictions, and 8-restrictions on roles
can be easily seen to agree with the extension of description graphs formed from them.
Lemma 1 shows that conjunction is properly handled. For 8-restrictions on attributes, the
construction is correct because attributes are functional.
For equalities A1  : : : An = B1  : : : Bm the construction forms a description graph with
two disjoint paths from the distinguished node to an end node, one labelled by the Ai ,
through nodes ai , and the other labelled by the Bj , through nodes bj . If

d 2 (A1  : : : An = B1  : : : Bm)I = fd 2 C j Ak I (: : : A1 I (d)) = Bh I (: : : B1 I (d))g;
then dening (ai ) = Ai I (: : : A1 I (d)) and (bj ) = Bj I (: : : B1 I (d))g, yields the mapping

required by Denition 3. The converse is satised by the requirement in Denition 3 that
for each a-edge hn1 ; n2; Ai 2 E , we have (n2 ) = AI ((n1 )).
286

Subsumption in CLASSIC

2.3 Canonical Description Graphs

In the following sections we will occasionally refer to \marking a node incoherent"; this
consists of replacing it with a special node having no outgoing r-edges, and including in
its atoms NOTHING, which always has the empty interpretation. Marking a description
graph as incoherent consists of replacing it with a description graph consisting only of an
incoherent node. (Incoherent graphs are to be thought of as representing concepts with
empty extension.)
Description graphs are transformed into canonical form by repeating the following normalization steps whenever possible for the description graph and all its descendants.
1. If some node has in its atoms a pre-dened host concept, add HOST-THING to its
atoms. If some node has an atomic concept name in its atoms, add CLASSIC-THING
to its atoms. For each pre-dened host concept in the atoms of the node, add all the
more-general pre-dened host concepts to its atoms.
2. If some node has both HOST-THING and CLASSIC-THING in its atoms, mark the
node incoherent. If some node has in its atoms a pair of host concepts that are not
related by the pre-dened subsumption relationship, mark the node incoherent, since
their intersection will be empty.
3. If any node in a description graph is marked incoherent, mark the description graph
as incoherent. (Reason: Even if the node is not a root, attributes must always have a value,
and this value cannot belong to the empty set.)
4. If some r-edge in a node has its min greater than its max, mark the node incoherent.
5. If some r-edge in a node has its description graph marked incoherent, change its max
to 0. (Reason: It cannot have any llers that belong to the empty set.)
6. If some r-edge in a node has a max of 0, mark its description graph as incoherent.
(Reason: This normalization step records the equivalence between 0 R and 8R:NOTHING,
and is used then to infer that a concept with 8R:C for arbitrary C subsumes 0 R.)
7. If some node has two r-edges labelled with the same role, merge the two edges, as
described below.
8. If some description graph has two a-edges from the same node labelled with the same
attribute, merge the two edges.
To merge two r-edges of a node, which have identical roles, replace them with one redge. The new r-edge has the role as its role, the maximum of the two mins as its min, the
minimum of the two maxs as its max, and the merge of the two description graphs as its
restriction.
To merge two a-edges hn; n1 ; Ai and hn; n2; Ai, replace them with a single new edge
hn; n0; Ai, where n0 results from merging n1 and n2, i.e., n0 = n1  n2. (If n1 = n2 then
n0 = n1.) In addition, replace n1 and n2 by n0 in all other a-edges of this description graph.
287

Borgida & Patel-Schneider

We need to show that the transformations to canonical form do not change the extension
of the graph. The main diculty is in showing that the two edge-merging processes do not
change the extension.

Lemma 2 Let G = hN; E; ri be a description graph with two mergeable a-edges and let
G0 = hN 0; E 0; r0i be the result of merging these two a-edges. Then GI = G0I .
Proof: Let the two edges be hn; n1; Ai and hn; n2; Ai and the new node n0 be n1  n2.
Choose d 2 GI , and let  be a function from N into the domain satisfying the conditions

for extensions (Denition 3) such that (r) = d. Then (n1 ) = (n2 ) because both are
equal to AI ((n)). Let 0 be the same as  except that 0 (n0 ) = (n1 ) = (n2 ). Then
0 satises Denition 3, part 3, for G0, because we replace n1 and n2 by n0 everywhere.
Moreover, 0 (n0) = (n1 ) 2 nI1 \ nI2 , which, by Lemma 1, equals (n1  n2 )I ; so part 2 is
satised too, since n0 = n1  n2 . Finally, if the root is modied by the merger, i.e., n1 or
n2 is r, say n1, then d = (n1) = 0 (n0), so part 1 of the denition is also satised.
Conversely, given arbitrary d 2 G0I , let 0 be the function stipulated by Denition 3 such
that 0 (r0) = d. Let  be the same as 0 except that (n1 ) = (n0 ) and (n2 ) = 0 (n0 ).
Then the above argument can be traversed in reverse to verify that  satises Denition 3,
so that d 2 GI .

Lemma 3 Let n be a node with two mergeable r-edges and let n0 be the node with these

edges merged. Then nI = n0I .
Proof: Let the two r-edges be hR; m1; M1; G1i and hR; m2; M2; G2i.
Let d 2 nI . Then there are between m1 (m2) and M1 (M2) elements of the domain, d0,
such that hd; d0i 2 RI . Therefore there are between the maximum of m1 and m2 and the
minimum of M1 and M2 elements of the domain, d0, such that hd; d0i 2 RI . Also, all d0 such
that hd; d0i 2 RI are in GI1 (GI2 ). Therefore, all d0 such that hd; d0i 2 RI are in GI1 \ GI2 ,
which equals (G1  G2)I by Lemma 1. Thus d 2 n0I .
Let d 2 n0I . Then there are between the maximum of m1 and m2 and the minimum of
M1 and M2 elements of the domain, d0 , such that hd; d0i 2 RI . Therefore there are between
m1 (m2) and M1 (M2) elements of the domain, d0, such that hd; d0i 2 RI . Also, all d0 such
that hd; d0i 2 RI are in (G1  G2 )I = GI1 \ GI2 . Therefore, all d0 such that hd; d0i 2 RI are
in GI1 (GI2 ). Therefore d 2 nI .

Having dealt with the issue of merging, we can now return to our desired result: showing
that \normalization" does not aect the meaning of description graphs.

Theorem 2 For all possible worlds I , the extension of the canonical form of a description

graph, G, resulting from a Basic classic description is the same as the extension of the
description.
Proof: Steps 1 and 2 are justied since GI is a subset of either H or C , which are
disjoint.
Step 3 is justied by the fact that, by the denition of description graphs, there must
be an element of the domain in the extension of each node in a description graph.
Steps 4, 5, and 6 are easily derived from Denition 3.
Steps 7 and 8 are dealt with in the preceding two lemmas.
288

Subsumption in CLASSIC

2.4 Subsumption Algorithm
The nal part of the subsumption process is checking to see if a canonical description graph
is subsumed by a description. It turns out that it is possible to carry out the subsumption
test without the expense of normalizing the candidate subsumer concept.

Algorithm 1 (Subsumption Algorithm) Given a description D and description graph
G = hN; E; ri, subsumes?(D; G) is dened to be true if and only if any of the following

conditions hold:

1. The description graph G is marked incoherent.
2. D is equivalent to THING. (This is determined by checking rst if D=THING, or by
recursively testing whether D subsumes the canonical description graph GTHING .)
3. D is a concept name and is an element of the atoms of r.
4. D is n R and some r-edge of r has R as its role and min greater than or equal to n.
5. D is n R and some r-edge of r has R as its role and max less than or equal to n.
6. D is 8R:C and some r-edge of r has R as its role and G0 as its restriction graph and
subsumes?(C; G0).
7. D is 8R:C and subsumes?(C; GTHING) and r has CLASSIC-THING in its atoms. (Reason: 8R:THING only requires the possibility that R be applicable to an object, which is absent
for host values.)
8. D is 8A:C and some a-edge of G is of the form hr; r0; Ai, and subsumes?(C; hN; E; r0i).
9. D is 8A:C and subsumes?(C; GTHING) and r has CLASSIC-THING in its atoms.
10. D is A1  : : : An = B1  : : : Bm and the paths A1 ; : : :; An and B1 ; : : :; Bm exist in G
starting from r and end at the same node.
11. D is A1  : : : An = B1  : : : Bm with An the same as Bm and the paths A1 ; : : :; An,1
and B1 ; : : :; Bm,1 exist in G starting from r and end at the same node, which has
CLASSIC-THING in its atoms. (Reason: If AiI ( A1 I ( )) = Bj I ( B1 I ( )) then
:::

d

:::

d

FI (AiI (: : : A1I (d))) = FI (Bj I (: : : B1 I (d)))

for any attribute F, as long as the attribute is applicable (i.e., the value is not in the host
domain).)

12. D is C u E and both subsumes?(C; G) and subsumes?(E; G) are true.
289

Borgida & Patel-Schneider

2.5 Correctness of Subsumption Algorithm

The soundness of this algorithm is fairly obvious, so we shall not dwell on it. The completeness of the algorithm is, as usual, more dicult to establish. First we have to show that
for any canonical description graph or node that is not marked as incoherent, a possible
world having a non-empty extension for the description graph or node can be constructed.
We will do this in a constructive, inductive manner, constructing a collection of such possible worlds, called the graphical worlds of a description graph. A graphical world has a
distinguished domain element that is in the extension of the description graph or node.
A common operation is to merge two possible worlds.
Denition 6 Let I1 and I2 be two possible worlds. The merge of I1 and I2, I1  I2, is
a possible world with classic realm the disjoint union of the classic realm of I1 and the
classic realm of I2 . The extension of atomic names in I1  I2 is the disjoint union of their
extensions in I1 and I2 .
It is easy to show that the extension of a description, a description graph, or a node in
I1  I2 is the union (disjoint union for the classic realm, regular union for the host realm)
of its extensions in I1 and I2 .
Another operation is to add new domain elements to a possible world. These new domain
elements must be in the classic realm. The extension of all atomic identiers remain the
same except that the new domain elements belong to some arbitrary set of atomic concept
names and have some arbitrary set of llers (ller) for each role (attribute). Again, it is
easy to show that a domain element of the original world is in an extension in the original
world i it is in the extension in the augmented world.
Given a node, n, that is not marked as incoherent, we construct the graphical worlds
for n as follows:
1. If the atoms of n are precisely THING, then n can have no r-edges, because the only
constructs that cause r-edges to be created also add CLASSIC-THING to the atoms.
Any possible world, with any domain element the distinguished domain element, is a
graphical world for n.
2. If the atoms of n include HOST-THING, then n can have no r-edges. Any possible
world, with distinguished element any domain element in the extension of all the
atoms of n and in no other host concepts, is a graphical world for n. (Because of
the requirements on the host domain, there are an innite number of these domain
elements.)
3. If the atoms of n include CLASSIC-THING, then for each r-edge, hR; m; M; Gi, in n,
construct between m and M graphical worlds for G. This can be done for any number
between m and M because if m > 0 then G is not marked incoherent, and if G is
marked incoherent then M = 0.
No two of these graphical worlds should have the same host domain element as their
distinguished element. (Again, this is possible because the extension of a host concept
is either empty or innite.) Now merge all the graphical worlds for each r-edge into
one possible world. Add some new domain elements such that one of them is in exactly
290

Subsumption in CLASSIC

the extensions of the atoms of n and has as llers for each R exactly the distinguished
elements of the appropriate graphical worlds. This domain element will have the
correct number of llers for each r-edge, because of the disjoint union of the classic
realms in the merge process and because of the dierent host domain elements picked
above; therefore it is in the extension of n. Thus the resulting world is a graphical
world for n.
Given a description graph, G = hN; E; ri, that is not marked incoherent, we construct
the graphical worlds for G as follows: For each node n 2 N construct a graphical world for
n. This can be done because none of them are marked incoherent. Merge these graphical
worlds. Modify the resulting world so that for each hn1 ; n2; Ai 2 E the A-ller for the
distinguished node of the graphical world from n1 is the distinguished node of the graphical
world from n2 . It is easy to show that the distinguished node of the graphical world of r is
in the extension of G, making this a graphical world for G.
Now we can show the nal part of the result.

Theorem 3 If the subsumption algorithm indicates that the canonical description of some
graph G is not subsumed by the Basic classic description D, then for some possible world
there is a domain element in the extension of the graph but not in the extension of D.
Therefore G is not subsumed by D.
Proof: The proof actually shows that if the subsumption algorithm indicates that some
canonical description graph, G, is not subsumed by some description, D, then there are
some graphical worlds for G such that their distinguished domain elements are not in the
extension of D. Remember that the subsumption algorithm indicates that G is not subsumed
by D, so G must not be marked as incoherent and thus there are graphical worlds for G.
The proof proceeds by structural induction on D. Let G = hN; E; ri.
 If D is an atomic concept name or a pre-dened host concept, then D does not occur
in the atoms of r. By construction, in any graphical world for G the distinguished
domain element will not be in the extension of D. Similarly, if D is CLASSIC-THING
or HOST-THING, then the distinguished domain elements will be in the wrong realm.
If D is THING, then it is not possible for the subsumption algorithm to indicate a
non-subsumption. In each case any graphical world for G has the property that its
distinguished domain element is not in the extension of D.

 If D is of the form D1 u D2 then the subsumption algorithm must indicate that G

is not subsumed by at least one of D1 or D2 . By the inductive hypothesis, we get
some graphical worlds of G where the distinguished domain elements are not in the
extension of D1 or not in the extension of D2, and thus are not in the extension of D.

 If D is the form n R then either the r-edge from r labelled with R has min less than
n or there is no such r-edge.
In the former case there are graphical worlds for G in which the distinguished node
has n , 1 llers for R, because n is greater than the min on the r-edge for R, and thus
the distinguished node is not in the extension of D.
291

Borgida & Patel-Schneider

In the latter case, there are graphical worlds for G in which its distinguished node
has any number of llers for R. Those with n , 1 llers have the property that their
distinguished node is not in the extension of D.

 If D is of the form n R then either the r-edge from r labelled with R has max greater
than n (including 1) or there is no such r-edge.
In the former case there are graphical worlds for G in which the distinguished node
has n + 1 llers for R, because n is less than the max on the r-edge for R, and thus
the distinguished node is not in the extension of D.
In the latter case, there are graphical worlds for G in which its distinguished node
has any number of llers for R. Those with n + 1 llers have the property that their
distinguished node is not in the extension of D.

 If D is of the form 8R:C, where R is a role, then two cases arise.
1. If subsumes?(C; GTHING) then CLASSIC-THING is not in the atoms of r. Then
there are some graphical worlds for G whose distinguished element is in the host

realm, and thus not in the extension of D.
2. Otherwise, either there is an r-edge from r with role R and description graph H
such that subsumes?(C; H ) is false or there is no r-edge from r with role R. Note
that the extension of C is not the entire domain, and thus must be a subset of
either the host realm or the classic realm.
In the former case H is not marked incoherent (or else the subsumption could
not be false) and the max on the r-edge cannot be 0. Thus there are graphical
worlds for H whose distinguished element is not in the extension of C and there
are graphical worlds for G that use these graphical worlds for H as distinguished
domain element R-llers. In these graphical worlds for G the distinguished element is not in the extension of D.
In the latter case, pick graphical worlds for G that have some distinguished node
R-ller in the wrong realm. In these graphical worlds for G the distinguished
element is not in the extension of D.

 If D is of the form 8A:C where A is an attribute then two cases arise.
1. If subsumes?(C; GTHING) then CLASSIC-THING is not in the atoms of r. Then
there are some graphical worlds for G whose distinguished element is in the host
realm, and thus not in the extension of D.
2. Otherwise, either there is an a-edge from r with attribute A to some other node
r0 such that subsumes?(C; H ) is false, where H = hN; E; r0i; or there is no a-edge
from r with attribute A. Note that the extension of C is not the entire domain,
and thus must be a subset of either the host realm or the classic realm.
In the former case H is not marked incoherent, because G is not marked incoherent. Thus there are graphical worlds for H whose distinguished element is
not in the extension of C. Given any graphical world for H , a graphical world
for G can be formed simply by changing the distinguished domain element. If
292

Subsumption in CLASSIC

the original graphical world's distinguished element is not in the extension of C,
then the new graphical world's distinguished element will not be in the extension
of D, as required.
In the latter case, pick graphical worlds for G that have their distinguished node
A-ller in the wrong realm. In these graphical worlds for G the distinguished
element is not in the extension of D.
 If D is of the form A1 : : : An = B1  : : : Bm several cases again arise.
1. If one of the paths A1 ; : : :; An,1 or B1 ; : : :; Bm,1 does not exist in G starting
from r, then nd the end of the partial path and use graphical worlds in which
the domain element for this node has an element of the host domain as its ller
for the next attribute in the path. Then one of the full paths will have no ller.
2. If the paths A1 ; : : :; An and B1 ; : : :; Bm exist in G starting from r but end at
dierent nodes, then use graphical worlds in which the domain elements for
these two nodes are dierent.
3. If one of the paths A1 ; : : :; An and B1 ; : : :; Bm does not exist in G starting from
r but the paths A1 ; : : :; An,1 and B1; : : :; Bm,1 both exist in G starting from r
and end at the same node then either CLASSIC-THING is not in the atoms of this
node or An 6= Bm. In the former case use graphical worlds in which the domain
element for this node is in the host realm. In the latter case use graphical worlds
that have dierent llers for An and Bm for the domain element for this node.
4. If one of the paths A1 ; : : :; An and B1 ; : : :; Bm does not exist in G starting from
r but the paths A1 ; : : :; An,1 and B1; : : :; Bm,1 both exist in G starting from r
and end at dierent nodes then use graphical worlds that have dierent llers
for the domain elements of these nodes or that have the domain elements in the
host realm.
In all cases we have that either one of An I (: : : A1 I )(d) or BmI (: : : B1 I )(d) does not
exist or An I (: : : A1 I )(d) 6= BmI (: : : B1 I )(d), so the distinguished domain element is
not in the extension of D.

2.6 Implementing the subsumption algorithm

In this section we provide some further comments about the actual subsumption algorithm
used by the classic system, including a rough analysis of its complexity.
As we have described it, deciding whether description C subsumes D is accomplished in
three phases:
1. Convert D into a description graph GD .
2. Normalize GD .
3. Verify whether C subsumes GD .
Step 1: Conversion is accomplished by a simple recursive descent parser, which takes
advantage of the fact that the syntax of description logics (i.e., the leading term constructor) makes them amenable to predictive parsing. Clearly, constructing graphs for xed sized
293

Borgida & Patel-Schneider

terms (like at-least) is constant time (if we measure size so that an integer is size 1 no matter how large), while the time for non-recursive terms (like same-as) is proportional to their
length. Finally, recursive terms (like all, and) only require a xed amount of additional
work, on top of the recursive processing. Therefore, the rst stage can be accomplished in
time proportional to the size of the input description. In order to speed up later processing,
it will be useful to maintain various lists, such as the lists of atomic concept identiers,
or roles/attributes, in sorted order. This sorting needs to be done initially (later, ordering
will be maintained by performing list merges) and this incurs, in the worst case a quadratic
overhead in processing8 . In any case, the total size of the graph constructed (including the
sizes of the nodes, etc.) is proportional to the size of the original concept description.
Step 3: Checking whether a description C subsumes a description graph GD , can be
seen to run in time proportional to the size of the subsuming concept, modulo the cost
of lookups in various lists. Since these are sorted, the lookup costs are bounded by the
logarithm of the size of the candidate subsumee graph, so the total cost is bounded by
O(j C j  log j GD j).
Step 2: Normalization is accomplished by a post-order traversal of the description
graph: in processing a description graph hN; E; ri, each node in N is normalized rst independently (see details below), and afterwards the attribute edges E are normalized. This
later task involves identifying multiple identically-labelled attribute edges leaving a node
(this is done in one pass since the attribute edges are grouped by source node, and sorted
by attribute name), and \merging" them. Merging two edges is quite easy in and of itself, but when merging the nodes at their tips, we must be careful because node mergers
may cascade; for example, if a concept has the form a1 = b1 u a2 = b2 u : : : u an = bn u
a1 = a2 u a2 = a3 u : : : u an,1 = an then the original graph will have 2n + 1 nodes, but 2n
of these are collapsed by normalization step 8. To discover this eciently, we use a version
of A-Kaci's algorithm for unifying 	-terms (At-Kaci, 1984; At-Kaci & Nasr, 1986); the
algorithm relies on the UNION-FIND technique to identify nodes to be merged, and runs
in time just slightly more than linear in the number of nodes in N . Therefore the cost of
the non-recursive portion of graph normalization is roughly linear in the number of nodes
in it.
The merging of two description graph nodes is quite similar to the normalization of
a single node: the atomic concept identier lists need to sorted/merged, with duplicates
eliminated on the y. This can be done in time proportional to the size of the nodes
themselves, if we make the size of the node include the size of the various lists in it, such as
atoms. The processing of role edges leaving a node is, again, one of identifying and merging
identically-labelled edges. (But in this case the mergers of labelled edges do not interact, so
a single pass over the role-edge list is sucient.) The cost of non-recursive aspects of any
such merger is once again proportional to the size of the local information.
We are therefore left with the problem of bounding the total number of procedure calls
to NormalizeGraph, NormalizeNode, MergeEdge, and MergeNode, and then bounding the
sizes of the nodes being merged.
NormalizeGraph and NormalizeNode are called exactly once on every (sub)graph and
node in the original graph, as part of the depth-rst traversal, and as argued above, on
8. We tend not to use fancy sorting techniques since these lists are not likely to be very long.

294

Subsumption in CLASSIC

their own they contribute at most time proportional to the total size of the original graph,
which was proportional to the size of the original description.
The number of calls to MergeEdge and MergeNode is not so simply bounded however {
the same node may be merged several times with others. However, these calls are paired,
and each invocation of MergeNode reduces the number of nodes in the graph by one. Therefore, since the number of nodes is not incremented elsewhere, the total number of calls to
MergeEdge and MergeNode is bounded by the number of nodes in the original graph. The
only problem is that the non-recursive cost of a call to MergeNode depends on the size of
the argument nodes, and each call may increase the size of the remaining node to be the
sum of the sizes of the two original nodes.
Therefore, if the original concept had size S, with the graph having n nodes, each of size
vi, then the worst case cost would result from the iterative summation of sizes:
(vi1 + vi2 ) + (vi1 + vi2 + vi3 ) + (vi1 + vi2 + vi3 + vi4 ) + : : :
= n  vi1 + (n , 1)  vi2 + : : : + 1  vi
n

Given that n and all vj are bounded
by S , clearly the above is in the worst case O(S 3).
P
In fact, given the constraint that j =1 nvj = S , it is possible to argue that the worst case
cost will occur when vj = 1 for every j, (i.e., when n = S ), in which case the cost is really
just O(S 2).
There are other theoretical improvements that could be attempted for the algorithm
(e.g., merging nodes in the correct order of increasing size) as well as its analysis (e.g., only
nodes in graphs at the same depth in the tree can be merged).
We remark that like all other description logics, classic permits identiers to be associated with complex descriptions and then these identiers can be used in other descriptions
(though no recursion is allowed). The expansion of identiers is a standard operation which
can lead to exponential growth in size in certain pathological cases (Nebel, 1990), making
the subsumption problem inherently intractable. As with the type system of the programming language Standard ML, such pathological cases are not encountered in practice, and
the correct algorithm is simple, straightforward and ecient in normal cases (unlike the
correct algorithm for reasoning with the set constructor, say).
Because users rarely ask only whether some concept subsumes another, but rather are
interested in the relationship between pairs of concepts, classic in fact constructs the
normalized description graph of any description given to it. This suggests that it might be
better to check whether one description graph subsumes another one, rather than checking
whether a description subsumes a graph. In general, this works quite well, except that we
would have to verify that the attribute edges in the subsumer graph form a subgraph of the
subsumee's attribute edges. Since edges are uniquely labelled after normalization, this is not
inherently hard, but it still requires a complete traversal (and hence marking/unmarking)
of the upper graph. We have therefore found it useful to encode as part of the description
graph's root the same-as restrictions that lead to the construction of the corresponding aedges; then, during subsumption testing, the only aspect of the subsumer related to same-as
which is checked is this list of same-as pairs.
Also, the above description of the algorithm has tried to optimize the cost of normalization, which dominates when checking a single subsumption. If in the overall use of a
295

Borgida & Patel-Schneider

system (e.g., processing individuals), inquiries about the restrictions on roles/attributes are
frequent, and space usage is not a problem, then it may be practically advantageous to
maintain the r-edges and a-edges of a node in a hash table, rather than a sorted list, in
order to speed up access. (Note that for merging r-edges, one must however still have some
way of iterating through all the values stored in the hash table.)

3. Individuals in Descriptions

In practical applications where DLs have been used, such as integrity constraint checking, it
is often very useful to be able to specify ranges of atomic values for roles. The most common
examples of this involve integers, e.g., \the year of a student can be 1,2,3 or 4", or what are
called enumerated types in Pascal, e.g., \the gender of a person is either M or F". One way
to allow such constraints is to introduce a new description constructor, a set description,
which creates a description from a list of individual names, and whose obvious extension is
the set consisting of the extensions of the individuals that appear in the list. This construct
could be used in terms like 8year:f1 2 3 4g. Another useful constructor involving individuals
is a lls restriction, p : I, which denotes objects that have the extension of the individual I
as one of the llers for the relationship denoted by role or attribute p. (Note that for an
attribute, q, 8q:fIg is the same as q : I.)
Within the paradigm of DLs, these constructors are quite useful and can in fact be used
to express new forms of incomplete information. For example, if we only know that Ringo
is in his early fties, we can simply assert that Ringo is described by 8age:f50 51 52 53 54g.
The constructors can also be used to ask very useful queries. For example, to nd all the
male persons it suces to determine the instances of gender : M.
The new constructors do interact with previous ones, such as cardinality constraints:
clearly the size of a set is an upper cardinality bound for any role it restricts. This interaction
is not problematic as long as the individuals in the set are host values, since such individuals
have properties that are xed and known ahead of time. However, once we allow classic
individuals as members of sets, then the properties of these individuals might themselves
aect subsumption. As a simple example, if we know that Ringo is an instance of the
concept ROCK-SINGER (which we shall write as Ringo 2 ROCK-SINGER ) then the extension
of 8friends:ROCK-SINGER is always a superset of the extension of 8friends:fRingog.
This is disturbing because then the classication hierarchy of denitions would change as
new facts about individuals are added to the knowledge base. Denitions are not meant to
be contingent of facts about the current world. Therefore, subsumption is usually dened to
be independent of these \contingent" assertions. As we shall see below, the use of individual
properties in description subsumption also leads to intractability.

3.1 Complex Subsumption Reasoning: An Example

Traditional proofs of intractability (e.g. (Levesque & Brachman, 1987)) have occasionally
left users of DLs puzzled over the intuitive aspects of a language which make reasoning
dicult. For this reason we present an example that illustrates the complexity of reasoning
with the set description.
Suppose that we have the concept of JADED-PERSON as being one who wants only to
visit the Arctic and/or the Antarctic, wherever there are penguins:
296

Subsumption in CLASSIC

JADED-PERSON =: 8wantsToVisit:(fArctic Antarcticg u 8hasPenguins!: fYesg)

Suppose we do not remember which is the Arctic and which the Antarctic; but we do know
that the South Pole is located in one of these two places, and that there are penguins there,
while the North Pole is located in one of these two places, and there are no penguins there.
Assuming that isLocatedIn! and hasPenguins! are attributes|roles with exactly one ller,
we can record
Southpole 2 8isLocatedIn!:(fArctic Antarcticg u 8hasPenguins!: fYesg)
Northpole 2 8isLocatedIn!:(fArctic Antarcticg u 8hasPenguins!: fNog)

We are thus unable to distinguish the exact location of the Southpole and Northpole; however,
since hasPenguins! has a single ller, exactly one of Arctic and Antarctic can (and in fact
must) have Yes as ller for hasPenguins!, and therefore exactly one of them is the location
of Southpole .
As a result of these facts, we know that the extension of JADED-PERSON must be a
subset of the extension of 1 wantsToVisit in any database containing the above facts about
Southpole and Northpole.
Observe that we have here not just an occasional worse-case behavior, but a generalized
diculty in reasoning with set descriptions. Because subsumption ignores assertions about
individuals, this does not (yet) show that subsumption per se must perform these inferences.
A simple transformation, given in the appendix, establishes this fact, by converting the
recognition of individuals into a question about the subsumption of two descriptions by
making all the individuals involved attribute-llers for new dummy attributes, and their
descriptions as restrictions on these attributes. As a result, if the description is non-empty
then these attribute values must satisfy the corresponding restrictions.

3.2 A Modied Semantics for Individuals

We have seen two problems with individuals appearing in descriptions: (1) the eect of
\mutable facts" on extensional relationships between \immutable" descriptions, and (2)
the computational intractability of subsumption caused by the appearance of individuals in
descriptions.
To deal with the rst problem, it is reasonable to restrict the computation of subsumption so that it cannot access \database facts" about individuals, such as their role llers, so
that all individuals are treated like host identiers. This is a procedural description of some
aspect of reasoning, in the same sense as negation-by-failure is in Prolog. As with Prolog,
it would be desirable to nd a semantic account of this phenomenon.
A semantics that ignores mutable facts when determining subsumption is not hard to
devise|all that is required is to have two dierent sets of possible worlds corresponding
to a KB containing both concepts and individuals. One set consists of all possible worlds
that model all the information in the KB; the second consists of all possible worlds that
model only the information about concepts (and roles and attributes). When asking questions about individuals, the rst set of possible worlds must be considered; when asking
subsumption questions, the second, larger, set must be considered, thus ignoring any eects
of the mutable facts.
297

Borgida & Patel-Schneider

However, this semantics does not solve the computational problem with individuals
in descriptions. To deal with this problem, the semantics of individuals are modied as
follows: instead of mapping individuals into separate elements of the domain, as is done in
a standard semantics, individuals are mapped into disjoint subsets of the domain, intuitively
representing dierent possible realizations of that (Platonic) individual.
Therefore, the semantics of the set constructor is now stated as follows: Domain value
d belongs to the extension of fB1 : : : Bn g i d belongs to the extension of one of the Bi .
An associated change in the notion of cardinality is required|two elements of the domain
are considered congruent if they belong to the extension of the same individual or if they
are identical. The cardinality of a set of elements of the domain is then the size of the set
modulo this congruence relationship. This means that occurrences of dierent identiers in
description(s) are guaranteed to be unequal, but distinct occurrences of the same individual
identier are not guaranteed to denote the same individual.
Here are two consequences of this stance:
1. Looking at the descriptions of Southpole and Northpole in Section 3.1, the distinct
occurrences of Arctic might be satised by distinct domain elements, with dierent role
llers. (In greater detail: the extension of Arctic might include domain elements d1 and
d2, with d1 satisfying condition hasPenguins! : Yes, while d2 satises hasPenguins! : No.
If Southpole is then located in d1, while Northpole is located in d2 , then we still have
both satisfying isLocatedIn! : Arctic. Similarly for domain elements d3 and d4 in the
extension of Antarctic. Therefore one could have two places to visit where there are
penguins, d1 and d3.)
2. Even though an individual may have a description that includes
isLocatedIn! : Arctic u originatesIn! : Arctic;
it need not satisfy the condition isLocatedIn! = originatesIn!, since the equality restriction requires identity of domain values.

4. Adding Individuals to CLASSIC

Individuals can occur in both classic and host descriptions. The following constructs create
classic descriptions:
R:I
A:I
fI1 : : : Ing
where A is an attribute, R is a role, I is the name of a classic individual or a host value,
collectively called individuals, and Ij are names of classic individuals. New host descriptions
can be constructed using fI1 : : : In g, where the Ij are host values.
The interpretation function :I is extended to individual identiers, by requiring that II
be a non-empty subset of C , if I is syntactically not recognized to be a host individual, and
making II = fIg for host values I. As stated earlier, the interpretations of distinct identiers
must be non-overlapping.
The interpretation CI of non-atomic descriptions is modied as follows:
298

Subsumption in CLASSIC

 p : II = fd 2 C j 9x (d; x) 2 pI ^ x 2 II g
 fI1 : : : IngI = Sk IIk if the Ik are all classic individuals; fI1 : : : IngI = fI1 : : : In g if Ik
are all host individuals; empty otherwise.

 (n p)I (resp. (n p)I ) is those objects in C with at least (resp. at most) n noncongruent llers for role p

The development of the subsumption algorithm in Section 2 is then modied to take
into account the added constructs with the modied semantics introduced earlier.
First description graphs are extended. A node of a description graph is given a third
eld, which is either a nite set of individuals or a special marker denoting the \universal"
set. This eld is often called the dom of the node. Both a-edges and r-edges are given an
extra eld, called the llers of the edge. This eld is a nite set of individuals. Where
unspecied, as in constructions in previous sections, the dom of a node is the universal set
and the llers of an a-edge or an r-edge is the empty set.
The semantics of description graphs in Denition 3 are extended to the following:

Denition 7 Let G = hN; E; ri be a description graph and let I be a possible world.

An element, d, of  is in GI , i there is some function, , from N into  such that

1. d = (r);
2. for all n 2 N (n) 2 nI ;
3. for all hn1 ; n2; A; F i 2 E we have h(n1 ); (n2)i 2 AI , and for all f 2 F , (n2 ) 2 f I .
An element, d, of  is in nI , where n = hC; H; S i, i
1. for all C 2 C , we have d 2 CI ;
2. for all hR; m; M; G; F i 2 H ,
(a)
(b)
(c)

there are between m and M elements, d0, of the domain such that hd; d0i 2 RI ;
d0 2 GI for all d0 such that hd; d0i 2 RI ; and
for all f 2 F there is a domain element, d0 , such that hd; d0i 2 RI and d0 2 f I

3. If the S is not the universal set then 9f 2 S such that d 2 f I .

When merging nodes, the dom sets are intersected. Merging description graphs is unchanged. When merging a-edges and r-edges, the sets of llers are unioned.
The translation of descriptions into description graphs is extended by the following rules:
8. A description of the form R : I is turned into a description graph with one node and
no a-edges. The node has as its atoms CLASSIC-THING and a single r-edge with role
R, min 0, max 1, and llers fIg. The description graph restricting this r-edge is
GCLASSIC-THING if I is a classic individual, and GHOST-THING otherwise.
299

Borgida & Patel-Schneider

9. A description of the form A : I is turned into a description graph with two nodes with
a single a-edge between them. The distinguished node of the graph is the source of
the a-edge. It has no r-edges and has as atoms CLASSIC-THING. The other node
also has no r-edges. It has as atoms CLASSIC-THING if I is a classic individual, and
HOST-THING otherwise. The a-edge has as its single ller I.
10. A description of the form fI1 : : : In g is turned into a description graph with one node.
The node has as dom the set containing I1 through In , and no r-edges. The atoms of
the node is HOST-THING if all of the individuals are host values, and CLASSIC-THING
if all of the individuals are classic individual names. (Note that the parser ensures
that individuals either must all be host values or must all be classic individual names.)
A short examination shows that Theorem 1 is true for these graphs, i.e., the extension of
description graphs formed using these rules is the same as the extension of the description
from which they were formed.
The following transformations are added to the canonicalization algorithm:
9. If the dom of a node is empty, mark the node incoherent.
10. If a host value in the dom of a node is not in all the atoms of the node, remove it from
the dom.
11. If an a-edge has more than one ller, then mark the description graph as incoherent.
12. If an a-edge has a ller and the node at its end has the universal dom, make the dom
be the ller.
13. If the ller of an a-edge is not included in the dom of the node at its end, mark the
description graph as incoherent.
14. If a node has only one element in its dom, make this element be the ller for all the
a-edges pointing to it.
15. If the llers of some r-edge are not a subset of the dom of the distinguished node of
the restriction graph of the edge, mark the node of the r-edge incoherent.
16. If the min on an r-edge is less than the cardinality of llers on it, let the min be this
cardinality.
17. If the max on an r-edge is greater than the cardinality of the dom on the distinguished node of the description graph of the r-edge, make the max of this edge be the
cardinality of the dom.
18. If the min on an r-edge is greater than or equal to the cardinality of the dom on the
distinguished node of the restriction graph of the r-edge, let the llers of the edge be
the union of its llers and the dom above. (If min is greater than the cardinality, then
steps 4 and 17 detect the inconsistency.)
300

Subsumption in CLASSIC

19. If the max on an edge is equal to the cardinality of llers on the edge, let the dom
on the distinguished node of the description graph of the r-edge be the intersection of
the dom and the llers. (If max is less than the cardinality, steps 18 and 4 detect the
inconsistency.)
Note that in the new canonical form all a-edges pointing to a single node have the same
value for their llers, and that if this is not the empty set, then the node has this set as the
value for its dom.
The proofs of Lemmas 3 and 2 also work for this extension of description graphs. The
proof of Theorem 2 can then be extended for these graphs.
The subsumption algorithm from page 289 is extended as follows:
13. D is R : I and some r-edge of r has role R and llers including I.
14. D is A : I and some a-edge from r has attribute A and llers including I.
15. D is fI1 : : : In g and the dom of r is a subset of fI1 : : : In g.
Again, the soundness of the extended algorithm is fairly obvious. The completeness
proof has the following additions to the construction of graphical worlds:

 The extension of classic individual names starts out empty.
 When constructing graphical worlds for a node that includes HOST-THING in its
atoms and has a non-universal dom, pick only those domain elements corresponding
to the elements of its dom.

 When constructing graphical worlds for a node that includes CLASSIC-THING in its
atoms and has a non-universal dom, add the distinguished domain element to the
extension of one of its dom elements.

 When constructing graphical worlds for the r-edges of a node, ensure that each element

of the llers of the r-edge has the distinguished element of at least one of the graphical
worlds in its extension by either adding them to the extension or using appropriate
host domain elements. (This can be done because the llers must be a subset of the
dom of the distinguished node of the graphical world and any host values must belong
to its atoms.)

The llers for a-edges need not be considered here because they are \pushed" onto the nodes
in the canonicalization process.
The proof of Theorem 3 is then extended with the following cases:

 If D is of the form fI1 : : : Ing then the dom of r is not a subset of fI1; : : :; Ing. Thus
there are graphical worlds for G in which the distinguished domain element is not in
the extension of any of the Ij.

 If D if of the form A : I then either the a-edge from r labelled with A does not have
ller I or there is no such a-edge.

301

Borgida & Patel-Schneider

In the former case the node pointed to by the a-edge cannot have as its domain the
singleton consisting of I. Therefore there are graphical worlds for G that have their
distinguished node A-ller not in the extension of I, as required.
In the latter case, pick graphical worlds for G that have their distinguished node Aller in the wrong realm. In these graphical worlds for G the distinguished element is
not in the extension of D.

 If D is of the form R : I then either the r-edge from r labelled with R does not have
ller I or there is no such r-edge.
In the former case either the cardinality of the dom of the distinguished node of the
description graph of this r-edge is greater than the min, m, of the r-edge, or the dom
does not include I. If the dom does not include I, then all graphical worlds for the
node have their distinguished element not in the extension of I, as required. If the
dom does include I, then there are at least m elements of the dom besides I, and the
llers of the r-edge are a subset of the set of these elements. There are thus graphical
worlds for G that use only these elements, as required.
In the latter case, pick graphical worlds for G that have some distinguished node Rller in the wrong realm. In these graphical worlds for G the distinguished element is
not in the extension of D.

This shows that the subsumption algorithm given here is sound and complete for the
modied semantics presented here.

5. Complete CLASSIC

We now make a nal pass to deal with some less problematic aspects of classic descriptions
that have not been appropriately covered so far.
classic allows primitive descriptions of the form (PRIMITIVE D T), where D is a
description, and T is a symbol. The extension of this is some arbitrary subset of the
extension of D, but is the same as the extension of (PRIMITIVE E T), provided that D
and E subsume each other. In this way one can express EMPLOYEE, a kind of a person
who must have an employee number, as
(PRIMITIVE (PERSON u 1 employeeNr) employee)
This construct can be removed by creating for every such primitive an atomic concept (e.g.,
EMPLOYEEHOOD) and then replacing the denition of the concept by the conjunction
of the necessary conditions and this atom, in this case EMPLOYEEHOOD u (PERSON u
1 employeeNr). Care has to be taken to use the same atomic concept for equivalent primitives.
classic permits the declaration of disjoint primitives, essentially allowing one to state
that the extensions of various atomic concepts must be disjoint in all possible worlds. To
deal with such declarations, we need only modify the algorithm for creating canonical graphs
by adding a step that marks a node as incoherent whenever its atoms contains two identiers
that have been declared to be disjoint.
302

Subsumption in CLASSIC

To allow an approximate representation for ideas that cannot be encoded using the
constructors expressly provided, classic allows the use of test-dened concepts, using the
following syntax:
(TEST [host-language Boolean function])
e.g., (TEST Prime-Number-Testing-Function).9 For the purposes of subsumption, these are
treated as \black-boxes", with semantics assigned as for atomic concepts. (Test concepts
have a real eect on reasoning at the level of individuals, where they can perform constraint
checking.)
With these simple additions, the above algorithm is a sound and complete subsumption
algorithm for descriptions in classic 1, under the modied semantics introduced in this
paper.

6. Summary, Related Work, and Conclusions
We believe this paper makes two kinds of contributions: First, the paper presents an abstracted form of the subsumption algorithm for the classic description logic, and shows
that it is ecient and correct under the modied semantics. This is signicant because
previous claims of correct and ecient subsumption algorithms in implemented DLs such
as kandor (Patel-Schneider, 1984) and candide (Beck et al., 1989) have turned out to be
unfounded (Nebel, 1988).
A tractability proof for a language like Basic classic is claimed to exist (but is not
proven) in (Donini et al., 1991), and an alternate proof technique may be found by considering a restriction of the (corrected) subsumption algorithm in (Hollunder & Nutt, 1990).
Description graphs have also turned out to be of interest because they support further
theoretical results about DLs, concerning their learnability (Cohen & Hirsh, 1994; Pitt &
Frazier, 1994)|results which would seem harder to obtain using the standard notation for
DLs.
Second, this paper investigates the eect of allowing individuals to appear in descriptions of DLs. As independently demonstrated in (Lenzerini & Schaerf, 1991), adding a
set description introduces yet another source of intractability, and we have provided an
intuitive example illustrating the source of diculties. The implementers of the classic
system, like others who do not use refutation/tableaux theorem-proving techniques, chose
not to perform all inferences validated by a standard semantics, not just because of the
formal intractability result but because no obvious algorithm was apparent, short of enumerating all possible ways of lling roles. The subset of inferences actually performed was
initially described procedurally: \facts" about individuals were not taken into account in
the subsumption algorithm. This paper provides a denotational semantic account for this
incomplete set of inferences. The formal proof of this being a correct account is a corollary
of the completeness proof for the subsumption algorithm in Section 4, and the observation
that the graph construction and subsumption algorithms in that section do indeed ignore
9. In order to deal with the two realms, classic in fact provides two constructors: H-TEST and CTEST, for host and classic descriptions, but this does not cause any added complications besides
keeping track of the correct realm.

303

Borgida & Patel-Schneider

the properties of the individuals involved. The one dierence between the original implementation of classic and the current semantics is that attribute paths ending with the
same ller were used to imply an equality condition. As noted in Section 3.2, the modied
semantics does not support this inference, and it was taken out of the implementation of
classic. It is signicant that the change to the standard semantics is small, easy to explain
to users (either procedurally or semantically), and only aects the desired aspects of the
language (i.e., all reasoning with Basic classic remains exactly as before).

Acknowledgments

We wish to thank Ronald Brachman and our other colleagues in the classic project for
their collaboration, and the JAIR referees for their excellent suggestions for improving the
paper. In particular, one of the referees deserves a medal for the thoroughness and care
taken in locating weaknesses in our arguments, and we are most thankful. Any remaining
errors are of course our own responsibility.

References

At-Kaci, H. (1984). A Lattice Theoretic Approach to Computation Based on a Calculus of
Partially-Ordered Type Structures. Ph.D. thesis, University of Pennsylvania.
At-Kaci, H., & Nasr, R. (1986). LOGIN: A logic programming language with built-in
inheritance. Journal of Logic Programming, 3, 187{215.
American Association for Articial Intelligence (1992). Issues in Description Logics: Users
Meet Developers. Working Notes of the AAAI 1992 Fall Symposium.
Baader, F., Burckert, H.-J., Heinsohn, J., Hollunder, B., Muller, J., Nebel, B., Nutt, W.,
& Protlich, H.-J. (1991). Terminological knowledge representation: A proposal for a
terminological logic. German Research Center for Articial Intelligence (DFKI).
Baader, F., & Hanschke, P. (1991). A scheme for integrating concrete domains into concept
languages. In Proceedings of the Twelfth International Joint Conference on Articial
Intelligence, pp. 452{457. International Joint Committee on Articial Intelligence. A
long version appears as Research Report RR-91-10 from the German Research Center
for Articial Intelligence (DFKI), April 1991.
Baader, F., & Hollunder, B. (1991). KRIS: Knowledge Representation and Inference System.
SIGART Bulletin, 2 (2), 8{15.
Beck, H. W., Gala, S. K., & Navathe, S. B. (1989). Classication as a query processing
technique in the CANDIDE semantic data model. In Proceedings of the Fifth International Data Engineering Conference, pp. 572{581. Institute of Electric and Electronic
Engineers.
Borgida, A., Brachman, R. J., McGuinness, D. L., & Resnick, L. A. (1989). CLASSIC: A
structural data model for objects. In Proceedings of the 1989 ACM SIGMOD International Conference on Mangement of Data, pp. 59{67. Association for Computing
Machinery.
304

Subsumption in CLASSIC

Borgida, A. (1992). From type systems to knowledge representation: Natural semantics
specications for description logics. International Journal of Intelligent and Cooperative Information Systems, 1 (1), 93{126.
Brachman, R. J., Fikes, R. E., & Levesque, H. J. (1983). KRYPTON: A functional approach
to knowledge representation. IEEE Computer, 16 (10), 67{73.
Cohen, W. W., & Hirsh, H. (forthcoming). Learnability of description logics with equality
constraints. Machine Learning. A preliminary version appears in the Proceedings of
the Fourth Annual Workshop on Computational Learning Theory.
Devanbu, P., Brachman, R. J., Ballard, B., & Selfridge, P. G. (1991). LaSSIE: A knowledgebased software information system. Communications of the ACM, 34 (5), 35{49.
Donini, F. M., Lenzerini, M., Nardi, D., & Nutt, W. (1991). Tractable concept languages.
In Proceedings of the Twelfth International Joint Conference on Articial Intelligence,
pp. 458{453. International Joint Committee on Articial Intelligence.
Doyle, J., & Patil, R. (1991). Two theses of knowledge representation: Language restrictions, taxonomic classication, and the utility of representation services. Articial
Intelligence, 48 (3), 261{297.
Pitt, L., & Frazier, M. (1994). Classic learning. In Proceedings of the Seventh Annual ACM
Conference on Computational Learning Theory New Brunswick, NJ. ACM Press.
Heinsohn, J., Kudenko, D., Nebel, B., & Protlich, H.-J. (1992). An empirical analysis of terminological representation systems. In Proceedings of the Tenth National
Conference on Articial Intelligence, pp. 767{773. American Association for Articial
Intelligence.
Hollunder, B., & Nutt, W. (1990). Subsumption algorithms for concept languages. Research
report RR-90-04, German Research Center for Articial Intelligence (DFKI).
Lenzerini, M., & Schaerf, A. (1991). Concept languages and query languages. In Proceedings
of the Ninth National Conference on Articial Intelligence, pp. 471{476. American
Association for Articial Intelligence.
Levesque, H. J., & Brachman, R. J. (1987). Expressiveness and tractability in knowledge
representation and reasoning. Computational Intelligence, 3 (2), 78{93.
MacGregor, R. M., & Bates, R. (1987). The Loom knowledge representation language. Tech.
rep. ISI/RS-87-188, Information Sciences Institute, University of Southern California.
Mays, E., Apte, C., Griesmer, J., & Kastner, J. (1987). Organizing knowledge in a complex
nancial domain. IEEE Expert, 2, 61{70.
Nebel, B. (1988). Computational complexity of terminological reasoning in BACK. Articial
Intelligence, 34 (3), 371{383.
Nebel, B. (1990). Terminological reasoning is inherently intractable. Articial Intelligence,
43 (2), 235{249.
305

Borgida & Patel-Schneider

Nebel, B., Peltason, C., & von Luck, K. (Eds.). (1991). International Workshop on Terminological Logics. Document D-91-13, German Research Center for Articial Intelligence
(DFKI).
Owsnicki-Klewe, B. (1988). Conguration as a consistency maintenance task. In Hoeppner, W. (Ed.), Proceedings of GWAI-88|the 12th German Workshop on Articial
Intelligence, pp. 77{87. Springer Verlag.
Patel-Schneider, P. F. (1984). Small can be beautiful in knowledge representation. In
Proceedings of the IEEE Workshop on Principles of Knowledge-Based Systems, pp.
11{16. IEEE Computer Society.
Patel-Schneider, P. F. (1987). Decidable, Logic-Based Knowledge Representation. Ph.D.
thesis, Department of Computer Science, University of Toronto.
Patel-Schneider, P. F. (1989a). A four-valued semantics for terminological logics. Articial
Intelligence, 38 (3), 319{351.
Patel-Schneider, P. F. (1989b). Undecidability of subsumption in NIKL. Articial Intelligence, 39 (2), 263{272.
Peltason, C., von Luck, K., & Kindermann, C. (Eds.). (1991). Terminological Logic Users
Workshop. Fachbereich Informatik, Technische Universitat Berlin.
Peltason, C., von Luck, K., Nebel, B., & Schmiedel, A. (1987). The user's guide to the
BACK system. Kit-report 42, Fachbereich Informatik, Technische Universitat Berlin.
Resnick, L. A., Borgida, A., Brachman, R. J., McGuinness, D. L., & Patel-Schneider,
P. F. (1992). CLASSIC description and reference manual for the COMMON LISP
implementation. AI Principles Research Department, AT&T Bell Laboratories.
Schmidt-Schauss, M. (1989). Subsumption in KL-ONE is undecidable. In Proceedings of
the First International Conference on Principles of Knowledge Representation and
Reasoning, pp. 421{431. Morgan Kaufmann.
Wright, J. R., Weixelbaum, E. S., Brown, K., Vesonder, G. T., Palmer, S. R., Berman,
J. I., & Moore, H. H. (1993). A knowledge-based congurator that supports sales,
engineering, and manufacturing at AT&T network systems. In Proceedings of the
Innovative Applications of Articial Intelligence Conference, pp. 183{193. American
Association for Articial Intelligence.

A. Intractability of Reasoning with ONE-OF

We present here a formal proof that subsumption with set descriptions is in fact NP-hard.10
We will show that if a term language allows a set description then it will need to
do \case analysis" in order to check whether the extension of an individual belongs to a
description or not; this is because this constructor behaves like disjunction if its elements can
10. Our original result was submitted for publication in 1990. A dierent, independent, proof of the same
result has since been outlined in (Lenzerini & Schaerf, 1991).

306

Subsumption in CLASSIC

be extensions of individuals whose membership in all terms is not known a priori, i.e., nonhost individuals. In particular, we will show how to encode the testing of unsatisability of a
formula in 3CNF as the question of recognizing an individual as an instance of a description.
Since this problem is known to be NP-hard, we have strong indication of its intractability.
Start with a formula F , in 3CNF. Using DeMorgan's laws, construct formula G, which is
the negation of F , and which is in 3DNF. Testing the validity of G is equivalent to checking
the unsatisability of F .
Construct for every propositional symbol p used in F , two individual names P and P^ .
(Here P^ will represent the negation of p.) Each individual will have attribute truthValue,
with possible llers True and False
P; P^ 2 8truthValue: fTrue Falseg:
To make sure that P and P^ have exactly one, and opposite, truth values, we create two more
individual names, Yesp and Nop, with additional attributes approve and deny respectively,
whose llers need to have truth value True and False respectively:
Yesp 2 8approve:(fP P^ g u 8truthValue: fTrueg)
Nop 2 8deny:(fP P^ g u 8truthValue: fFalseg)
Now, given the formula G = C 1 _ C 2 _ : : : _ Cn, create individual names C1, C2,
: : : , Cn, each with role conjuncts containing the propositions that are its conjuncts. For
example, if C 1 = p ^ :q ^ :r then
C1 2 8conjuncts: fP Q^ R^ g u 3 conjuncts:
Finally, construct individual G to have C1, C2, : : : , Cn as possible llers for a new role

disjunctsHolding :

G 2 8disjunctsHolding: fC1 C2 : : : Cng:

The formula G will then be valid i there is always at least one disjunct that holds.
This is equivalent to membership in the concept VALID-FORMULAE dened as

1 disjunctsHolding u 8disjunctsHolding:(8conjuncts:(8truthValue:fTrueg)):
The above shows that recognizing whether individuals are instances of descriptions is
intractable in the presence of set descriptions, minimum number restrictions, and value
restrictions.
We can convert this into a question concerning the subsumption of two descriptions by
essentially making all the individuals involved attribute-llers for new dummy attributes,
and their descriptions as restrictions on these attributes. Then if the description is nonempty then these attribute values must satisfy the corresponding restrictions.
So, dene concept UPPER to be

8formula:VALID-FORMULAE
and dene concept LOWER to be
307

Borgida & Patel-Schneider

8dummy1-p:(fPg u [P 0s concept descriptor ]) u
8dummy2-p:(fP^g u [P^ 0s concept descriptor ]) u
8dummy3-p:(fYespg u : : :) u
8dummy4-p:(fNopg u : : :) u
:::
8dummy5-ci:(fCig u : : :) u
:::
8formula:(fGg u : : :)
Then in any database state either concept LOWER has no instances, in which case it is
a subset of the extension of UPPER, or it has at least one instance, in which case the
individual names lling the various dummy attributes must have the properties ascribed to
them, whence C will be in VALID-FORMULAE (and hence UPPER will subsume LOWER) i
C is valid, which completes the proof.

308

Journal of Artificial Intelligence Research 1 (1994) 159-208

Submitted 8/93; published 2/94

Bias-Driven Revision of Logical Domain Theories

Moshe Koppel
Ronen Feldman

KOPPEL@BIMACS.CS.BIU.AC.IL
FELDMAN@BIMACS.CS.BIU.AC.IL

Department of Mathematics and Computer Science, Bar-Ilan University,
Ramat-Gan, Israel

Alberto Maria Segre

SEGRE@CS.CORNELL.EDU

Department of Computer Science, Cornell University,
Ithaca, NY 14853, USA

Abstract
The theory revision problem is the problem of how best to go about revising a deficient
domain theory using information contained in examples that expose inaccuracies. In this paper we
present our approach to the theory revision problem for propositional domain theories. The
approach described here, called PTR, uses probabilities associated with domain theory elements to
numerically track the flow of proof through the theory. This allows us to measure the precise
role of a clause or literal in allowing or preventing a (desired or undesired) derivation for a given
example. This information is used to efficiently locate and repair flawed elements of the theory.
PTR is proved to converge to a theory which correctly classifies all examples, and shown
experimentally to be fast and accurate even for deep theories.

1. Introduction
One of the main problems in building expert systems is that models elicited from experts tend to
be only approximately correct. Although such hand-coded models might make a good first
approximation to the real world, they typically contain inaccuracies that are exposed when a fact
is asserted that does not agree with empirical observation. The theory revision problem is the
problem of how best to go about revising a knowledge base on the basis of a collection of
examples, some of which expose inaccuracies in the original knowledge base. Of course, there
may be many possible revisions that sufficiently account for all of the observed examples; ideally,
one would find a revised knowledge base which is both consistent with the examples and as
faithful as possible to the original knowledge base.
Consider, for example, the following simple propositional domain theory, . This theory,
although flawed and incomplete, is meant to recognize situations where an investor should buy
stock in a soft drink company.
buy-stock  increased-demand  product-liability
product-liability  popular-product  unsafe-packaging
increased-demand  popular-product  established-market
increased-demand  new-market  superior-flavor.
The theory  essentially states that buying stock in this company is a good idea if demand for its
product is expected to increase and the company is not expected to face product liability lawsuits.
In this theory, product liability lawsuits may result if the product is popular (and therefore may
present an attractive target for sabotage) and if the packaging is not tamper-proof. Increased
product demand results if the product is popular and enjoys a large market share, or if there are

 1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

KOPPEL, FELDMAN, & SEGRE

new market opportunities and the product boasts a superior flavor. Using the closed world
assumption, buy-stock is derivable given that the set of true observable propositions is precisely,
say,
{popular-product, established-market, celebrity-endorsement}, or
{popular-product, established-market, colorful-label}
but not if they are, say,
{unsafe-packaging, new-market}, or
{popular-product, unsafe-packaging, established-market}.
Suppose now that we are told for various examples whether buy-stock should be derivable.
For example, suppose we are told that if the set of true observable propositions is:
(1)

{popular-product, unsafe-packaging, established-market} then buy-stock is false,

(2)

{unsafe-packaging, new-market} then buy-stock is true,

(3)

{popular-product, established-market, celebrity-endorsement} then buy-stock is true,

(4)

{popular-product, established-market, superior-flavor} then buy-stock is false,

(5)

{popular-product, established-market, ecologically-correct} then buy-stock is false, and

(6)

{new-market, celebrity-endorsement} then buy-stock is true.

Observe that examples 2, 4, 5 and 6 are misclassified by the current theory . Assuming that the
explicitly given information regarding the examples is correct, the question is how to revise the
theory so that all of the examples will be correctly classified.
1.1. Two Paradigms
One approach to this problem consists of enumerating partial proofs of the various examples in
order to find a minimal set of domain theory elements (i.e., literals or clauses) the repair of which
will satisfy all the examples (EITHER, Ourston & Mooney, in press). One problem with this
approach is that proof enumeration even for a single example is potentially exponential in the size
of the theory. Another problem with this approach is that it is unable to handle negated internal
literals, and is restricted to situations where each example must belong to one and only one class.
These problems suggest that it would be worthwhile to circumvent proof enumeration by
employing incremental numerical schemes for focusing blame on specific elements.
A completely different approach to the revision problem is based on the use of neural
networks (KBANN, Towell & Shavlik, 1993). The idea is to transform the original domain theory
into network form, assigning weights in the graph according to some pre-established scheme.
The connection weights are then adjusted in accordance with the observed examples using
standard neural-network backpropagation techniques. The resulting network is then translated
back into clausal form. The main disadvantage of this method that it lacks representational
transparency; the neural network representation does not preserve the structure of the original
knowledge base while revising it. As a result, a great deal of structural information may be lost
translating back and forth between representations. Moreover, such translation imposes the
limitations of both representations; for example, since neural networks are typically slow to
converge, the method is practical for only very shallow domain theories. Finally, revised domain
theories obtained via translation from neural networks tend to be significantly larger than their
corresponding original domain theories.
160

BIAS DRIVEN REVISION

Other approaches to theory revision which are much less closely related to the approach we
will espouse here are RTLS (Ginsberg, 1990), KR-FOCL (Pazzani & Brunk, 1991), and
ODYSSEUS (Wilkins, 1988).
1.2. Probabilistic Theory Revision
Probabilistic Theory Revision (PTR) is a new approach to theory revision which combines the
best features of the two approaches discussed above. The starting point for PTR is the
observation that any method for choosing among several possible revisions is based on some
implicit bias, namely the a priori probability that each element (clause or literal) of the domain
theory requires revision.
In PTR this bias is made explicit right from the start. That is, each element in the theory is
assigned some a priori probability that it is not flawed. These probabilities might be assigned by
an expert or simply chosen by default.
The mere existence of such probabilities solves two central problems at once. First, these
probabilities very naturally define the best (i.e., most probable) revision out of a given set of
possible revisions. Thus, our objective is well-defined; there is no need to impose artificial
syntactic or semantic criteria for identifying the optimal revision. Second, these probabilities can
be adjusted in response to newly-obtained information. Thus they provide a framework for
incremental revision of the flawed domain theory.
Briefly, then, PTR is an algorithm which uses a set of provided examples to incrementally
adjust probabilities associated with the elements of a possibly-flawed domain theory in order to
find the most probable set of revisions to the theory which will bring it into accord with the
examples.1 Like KBANN, PTR incrementally adjusts weights associated with domain theory
elements; like EITHER, all stages of PTR are carried out within the symbolic logic framework
and the obtained theories are not probabilistic.
As a result PTR has the following features:
(1)

it can handle a broad range of theories including those with negated internal literals and
multiple roots.

(2)

it is linear in the size of the theory times the number of given examples.

(3)

it produces relatively small, accurate theories that retain much of the structure of the
original theory.

(4)

it can exploit additional user-provided bias.

In the next section of this paper we formally define the theory revision problem and discuss
issues of data representation. We lay the foundations for any future approach to theory revision
by introducing very sharply defined terminology and notation. In Section 3 we propose an
efficient algorithm for finding flawed elements of a theory, and in Section 4 we show how to
revise these elements. Section 5 describes how these two components are combined to form the
PTR algorithm. In Section 5, we also discuss the termination and convergence properties of PTR
and walk through a simple example of PTR in action. In Section 6 we experimentally evaluate
PTR and compare it to other theory revision algorithms. In Section 7, we sum up our results and
1

In the following section we will make precise the notion of most probable set of revisions.

161

KOPPEL, FELDMAN, & SEGRE

indicate directions for further research.
The formal presentation of the work described here is, unfortunately, necessarily dense. To
aid the more casual reader, we have moved all formal proofs to three separate appendices. In
particular, in the third appendix we prove that, under appropriate conditions, PTR converges.
Reading of these appendices can safely be postponed until after the rest of the paper has been
read. In addition, we provide in Appendix D, a quick reference guide to the notation used
throughout the paper. We would suggest that a more casual reader might prefer to focus on
Section 2, followed by a cursory reading of Sections 3 and 4, and a more thorough reading of
Section 5.

2. Representing the Problem
A propositional domain theory, denoted , is a stratified set of clauses of the form C i : H i  Bi
where C i is a clause label, H i is a proposition (called the head of C i ) and Bi is a set of positive
and negative literals (called the body of C i ). As usual, the clause C i : H i  Bi represents the
assertion that the proposition H i is implied by the conjunction of literals in Bi . The domain theory
is simply the conjunction of its clauses. It may be convenient to think of this as a propositional
logic program without facts (but with negation allowed).
A proposition which does not appear in the head of any clause is said to be observable. A
proposition which appears in the head of some clause but does not appear in the body of any
clause is called a root. An example, E, is a truth assignment to all observable propositions. It is
convenient to think of E as a set of true observable propositions.
Let  be a domain theory with roots r 1 , . . . , r n . For an example, E, we define the vector
(E) =  1 (E), . . . , n (E)  where i (E) = 1 if E |  r i (using resolution) and i (E) = 0 if
E |/  r i . Intuitively, (E) tells us which of the conclusions r 1 , . . . , r n can be drawn by the expert
system when given the truth assignment E.
Let the target domain theory, , be some domain theory which accurately models the domain
of interest. In other words,  represents the correct domain theory. An ordered pair,  E, (E)  ,
is called an exemplar of the domain: if i (E) = 1 then the exemplar is said to be an IN exemplar
of r i , while if i (E) = 0 then the exemplar is said to be an OUT exemplar of r i . Typically, in
theory revision, we know (E) without knowing .
Let  be some possibly incorrect theory for a domain which is in turn correctly modeled by
the target theory . Any inaccuracies in  will be reflected by exemplars for which (E)  (E).
Such exemplars are said to be misclassified by . Thus, a misclassified IN exemplar for r i , or false
negative for r i , will have i (E) = 1 but i (E) = 0, while a misclassified OUT exemplar for r i , or
false positive for r i , will have i (E) = 0 but i (E) = 1.2 Typically, in theory revision we know
(E) without knowing .
Consider, for example, the domain theory, T , and example set introduced in Section 1. The
theory T has only a single root, buy-stock. The observable propositions mentioned in the
examples are popular-product, unsafe-packaging, established-market, new-market, celebrity2

We prefer the new terminology IN/OUT to the more standard positive/negative because the latter is often used to refer to the classification of the example by the given theory, while we use IN/OUT to
refer specifically to the actual classification of the example.

162

BIAS DRIVEN REVISION

endorsement,
superior-flavor,
and
ecologically-correct.
For
the
example
E = {unsafe-packaging, new-market} we have (E) =  1 (E)  =  0  . Nevertheless, we are told
that (E) =  1 (E)  =  1  . Thus, E =  {unsafe-packaging, new-market},  1  
is a
misclassified IN exemplar of the root buy-stock.
Now, given misclassified exemplars, there are four revision operators available for use with
propositional domain theories:
(1)

add a literal to an existing clause,

(2)

delete an existing clause,

(3)

add a new clause, and

(4)

delete a literal from an existing clause.

For negation-free domain theories, the first two operations result in specializing , since they may
allow some IN exemplars to become OUT exemplars. The latter two operations result in
generalizing , since they may allow some OUT exemplars to become IN exemplars.3
We say that a set of revisions to  is adequate for a set of exemplars if, after the revision
operators are applied, all the exemplars are correctly classified by the revised domain theory .
Note that we are not implying that  is identical to , but rather that for every exemplar
 E, (E)  , (E) = (E). Thus, there may be more than one adequate revision set. The goal of
any theory revision system, then, is to find the best revision set for , which is adequate for a
given a set of exemplars.
2.1. Domain Theories as Graphs
In order to define the problem even more precisely and to set the stage for its solution, we will
show how to represent a domain theory in the form of a weighted digraph. We begin by defining a
more general version of the standard ANDOR proof tree, which collapses the distinction between
AND nodes and OR nodes.
For any set of propositions {P 1 , . . . , P n }, let NAND({P 1 , . . . , P n }) be a Boolean formula
which is false if and only if {P 1 , . . . , P n } are all true. Any domain theory  can be translated into
an equivalent domain theory  consisting of NAND equations as follows:
(1)

For each clause C i : H i  Bi  , the equation C i = NAND(Bi ) is in .

(2)

For each non-observable proposition P appearing in  the equation P = NAND(C P ) is in
, where C P = {C i H i = P}, i.e., the set consisting of the label of each clause in  whose
head is P.

(3)

For each negative literal P appearing in , the equation P = NAND({P}) is in .

 contains no equations other than these. Observe that the literals of  are the literals of 
together with the new literals {C i } which correspond to the clauses of . Most important,  is
equivalent to  in the sense that for each literal l in  and any assignment E of truth values to the
observable propositions of , E |  l if and only if E |  l.
3

In the event that negative literals appear in the domain theory, the consequences of applying these
operators are slightly less obvious. This will be made precise in the second part of this section.

163

KOPPEL, FELDMAN, & SEGRE

Consider, for example, the domain theory  of Section 1. The set of NAND equations  is
buy-stock = NAND({C 1 }),
C 1 = NAND({increased-demand, product-liability}),
product-liability = NAND({product-liability}),
increased-demand = NAND({C 3 , C 4 }),
product-liability = NAND({C 2 }),
C 2 = NAND({popular-product, unsafe-packaging}),
C 3 = NAND({popular-product, established-market}), and
C 4 = NAND({new-market, superior-flavor}).
Observe that buy-stock is true in  for precisely those truth assignments to the observables for
which buy-stock is true in T .
We now use  to obtain a useful graph representation of . For an equation i in , let h(i )
refer to the left side of i and let b(i ) refer to the set of literals which appear on the right side of
i . In other words, h(i ) = NAND(b(i )).
Definition: A dt-graph  for a domain theory  consists of a set of nodes which
correspond to the literals of  and a set of directed edges corresponding to the set
of ordered pairs {  x, y  x = h(i ), y  b(i ), i  }. In addition, for each root
r we add an edge, er , leading into r (from some artificial node).
In other words,  consists of edges from each literal in  to each of its antecedents. The dtgraph representation of  is shown in Figure 1.
Let n e be the node to which the edge e leads and let n e be the node from which it comes. If
n e is a clause, then we say that e is a clause edge; if n e is a root, then we say that e is a root edge;
if n e is a literal and n e is a clause, then we say that e is a literal edge; if n e is a proposition and n e
is its negation, then we say that e is a negation edge.
The dt-graph  is very much like an ANDOR graph for . It has, however, a very significant
advantage over ANDOR graphs because it collapses the distinction between clause edges and
literal edges which is central to the ANDOR graph representation. In fact, even negation edges
(which do not appear at all in the ANDOR representation) are not distinguished from literal edges
and clause edges in the dt-graph representation.
In terms of the dt-graph  , there are two basic revision operators  deleting edges or adding
edges. What are the effects of adding or deleting edges from  ? If the length of every path from
a root r to a node n is even (odd) then n is said to be an even (odd) node for r i . If n e is even (odd)
for r i , then e is said to be even (odd) for r i . (Of course it is possible that the depth of an edge is
neither even nor odd.) Deleting an even edge for r i specializes the definitions of r i in the sense
that if  is the result of the deletion, then i (E)  i (E) for all exemplars  E, (E)  ; likewise,
adding an even edge for r i generalizes the definition of r i in the sense that if  is the result of
adding the edge to  then i (E)  i (E). Analogously, deleting an odd edge for r i generalizes
the definition of r i , while adding an odd edge for r i specializes the definition of r i . (Deleting or
adding an edge which is neither odd nor even for r i might result in a new definition of r i which is
neither strictly more general nor strictly more specific.)
To understand this intuitively, first consider the case in which there are no negation edges in
 . Then an even edge in  represents a clause in , so that deleting is specialization and adding
is generalization. An odd edge in  represents a literal in the body of a clause in  so that
deleting is generalization and adding a specialization. Now, if an odd number of negation edges
164

BIAS DRIVEN REVISION

buy-stock

C1

increased-demand

product-liability

C4

new-market

C3

product-liability

established-market
superior-flavor

C2

popular-product
unsafe-packaging

Figure 1: The dt-graph,  , of the theory .

are present on the path from r i to an edge then the role of the edge is reversed.
2.2. Weighted Graphs
A weighted dt-graph is an ordered pair   , w  where  is a dt-graph w and is an assignment
of values in (0, 1] to each node and edge in  . For an edge e, w(e) is meant to represent the
users degree of confidence that the edge e need not be deleted to obtain the correct domain
theory. For a node n, w(n) is the users degree of confidence that no edge leading from the node
n need be added in order to obtain the correct domain theory. Thus, for example, the assignment
w(n) = 1 means that it is certain that no edge need be added to the node n and the assignment
w(e) means that it is certain that e should not be deleted. Observe that if the node n is labeled by
a negative literal or an observable proposition then w(n) = 1 by definition, since graphs obtained
by adding edges to such nodes do not correspond to any domain theory. Likewise, if e is a rootedge or a negation-edge, then w(e) = 1.
165

KOPPEL, FELDMAN, & SEGRE

For practical reasons, we conflate the weight w(e) of an edge e and the weight, w(n e ), of the
node n e , into a single value, p(e) = w(e)  w(n e ), associated with the edge e. The value p(e) is
the users confidence that e need not be repaired, either by deletion or by dilution via addition of
child edges.
There are many ways that these values can be assigned. Ideally, they can be provided by the
expert such that they actually reflect the experts degree of confidence in each element of the
theory. However, even in the absence of such information, values can be assigned by default; for
example, all elements can be assigned equal value. A more sophisticated method of assigning
values is to assign higher values to elements which have greater semantic impact (e.g., those
closer to the roots). The details of one such method are given in Appendix A. It is also, of
course, possible for the expert to assign some weights and for the rest to be assigned according to
some default scheme. For example, in the weighted dt-graph,   , p  , shown in Figure 2, some
edges have been assigned weight near 1 and others have been assigned weights according to a
simple default scheme.
The semantics of the values associated with the edges can be made clear by considering the
case in which it is known that the correct dt-graph is a subset of the given dt-graph, . Consider a
probability function on the space of all subgraphs of . The weight of an edge is simply the sum
of the probabilities of the subgraphs in which the edge appears. Thus the weight of an edge is the
probability that the edge does indeed appear in the target dt-graph. We easily extend this to the
case where the target dt-graph is not necessarily a subgraph of the given one.4
Conversely, given only the probabilities associated with edges and assuming that the deletion
of different edges are independent events, we can compute the probability of a subgraph, .
Since p(e) is the probability that e is not deleted and 1  p(e) is the probability that e is deleted, it
follows that
p() =



e  

p(e) 



e  

1  p(e).

Letting S =   , we rewrite this as
p() =



e  S

p(e) 



e S

1  p(e).

We use this formula as a basis for assigning a value to each dt-graph  obtainable from  via
revision of the set of edges S, even in the case where edge-independence does not hold and even
in the case in which  is not a subset of . We simply define
w() =



e  S

p(e) 



e S

1  p(e).

(In the event that  and  are such that S is not uniquely defined, choose S such that w() is
maximized.) Note that where independence holds and  is subgraph of , we have
4

In order to avoid confusion it should be emphasized that the meaning of the weights associated with
edges is completely different than that associated with edges of Pearls Bayesian networks (1988). For us,
these weights represent a meta-domain-theory concept: the probability that this edge appears in some unknown target domain theory. For Pearl they represent conditional probabilities within a probabilistic domain theory. Thus, the updating method we are about to introduce is totally unrelated to that of Pearl.

166

BIAS DRIVEN REVISION

.999
buy-stock
.99
C1
1.0

.95

increased-demand
.9

product-liabilty
.9

C4
.8
new-market

.9
C3

.8

product-liability
.8

.8

.9

established-market
superior-flavor

C2
.8

.99

popular-product
unsafe-packaging

Figure 2: The weighted dt-graph,   , p  .

w() = p().
2.3. Objectives of Theory Revision
Now we can formally define the proper objective of a theory revision algorithm:
Given a weighted dt-graph  , p  and a set of exemplars , find a dt-graph  such that
 correctly classifies every exemplar in  and w() is maximal over all such dt-graphs.
Restating this in the terminology of information theory, we define the radicality of a dt-graph 
relative to an initial weighted dt-graph  =  , p  as
Rad  () =



e  S

log( p(e)) +



e S

log(1  p(e))

where S is the set of edges of  which need to be revised in order to obtain . Thus given a
weighted dt-graph  and a set of exemplars , we wish to find the least radical dt-graph relative
167

KOPPEL, FELDMAN, & SEGRE

to  which correctly classifies the set of exemplars .
Note that radicality is a straightforward measure of the quality of a revision set which neatly
balances syntactic and semantic considerations. It has been often noted that minimizing syntactic
change alone can lead to counter-intuitive results by giving preference to changes near the root
which radically alter the semantics of the theory. On the other hand, regardless of the distribution
of examples, minimizing semantic change alone results in simply appending to the domain theory
the correct classifications of the given misclassified examples without affecting the classification
of any other examples.
Minimizing radicality automatically takes into account both these criteria. Thus, for example,
by assigning higher initial weights to edges with greater semantic impact (as in our default
scheme of Appendix A), the syntactic advantage of revising close to the root is offset by the
higher cost of such revisions. For example, suppose we are given the theory  of the introduction
and the single misclassified exemplar
 {unsafe-packaging, new-market},  1   .
There are several possible revisions which would bring  into accord with the exemplar. We
could, for example, add a new clause
buy-stock  unsafe-packaging  new-market,
delete superior-flavor from clause C4, delete popular-product and established-market from clause
C3, or delete increased-demand from clause C1. Given the weights of Figure 2, the deletion of
superior-flavor from clause C4 is clearly the least radical revision.
Observe that in the special case where all edges are assigned identical initial weights,
regardless of their semantic strength, minimization of radicality does indeed reduce to a form of
minimization of syntactic change. We wish to point out, however, that even in this case our
definition of syntactic change differs from some previous definitions (Wogulis &
Pazzani, 1993). Whereas those definitions count the number of deleted and added edges, we
count the number of edges deleted or added to. To understand why this is preferable, consider the
case in which some internal literal, which happens to have a large definition, is omitted from one
of the clause in the target theory. Methods which count the number of added edges will be
strongly biased against restoring this literal, prefering instead to make several different repairs
which collectively involve fewer edges than to make a single repair involving more edges.
Nevertheless, given the assumption that the probabilities of the various edges in the given theory
being mistaken are equal, it is far more intuitive to repair only at a single edge, as PTR does. (We
agree, though, that once an edge has been chosen for repair, the chosen repair should be minimal
over all equally effective repairs.)

3. Finding Flawed Elements
PTR is an algorithm which finds an adequate set of revisions of approximately minimum
radicality. It achieves this by locating flawed edges and then repairing them. In this section we
give the algorithm for locating flawed edges; in the next section we show how to repair them.
The underlying principle of locating flawed edges is to process exemplars one at a time, in
each case updating the weights associated with edges in accordance with the information
contained in the exemplars. We measure the flow of a proof (or refutation) through the edges
of the graph. The more an edge contributes to the correct classification of an example, the more
its weight is raised; the more it contributes to the misclassification of the example, the more its
168

BIAS DRIVEN REVISION

weight is lowered. If the weight of an edge drops below a prespecified revision threshold  , it is
revised.
The core of the algorithm is the method of updating the weights. Recall that the weight
represents the probability that an edge appears in the target domain theory. The most natural way
to update these weights, then, is to replace the probability that an edge need not be revised with
the conditional probability that it need not be revised given the classification of an exemplar. As
we shall see later, the computation of conditional probabilities ensures many desirable properties
of updating which ad hoc methods are liable to miss.
3.1. Processing a Single Exemplar
One of the most important results of this paper is that under certain conditions the conditional
probabilities of all the edges in the graph can be computed in a single bottom-up-then-top-down
sweep through the dt-graph. We shall employ this method of computation even when those
conditions do not hold. In this way, updating is performed in highly efficient fashion while, at the
same time, retaining the relevant desirable properties of conditional probabilities.
More precisely, the algorithm proceeds as follows. We think of the nodes of  which
represent observable propositions as input nodes, and we think of the values assigned by an
example E to each observable proposition as inputs. Recall that the assignment of weights to the
edges is associated with an implicit assignment of probabilities to various dt-graphs obtainable
via revision of  . For some of these dt-graphs, the root r i is provable from the example E, while
for others it is not. We wish to make a bottom-up pass through  =   , p  in order to compute
(or at least approximate) for each root r i , the probability that the target domain theory is such that
r i is true for the example E. The obtained probability can then be compared with the desired
result, i (E), and the resulting difference can be used as a basis for adjusting the weights, w(e),
for each edge e.
Let
1 if P is true in E
E(P) = 
0 if P is false in E.
We say that a node n   is true if the literal of  which labels it is true. Now, a node passes the
value true up the graph if it is either true or deleted, i.e., if it is not both undeleted and false.
Thus, for an edge e such that n e is the observable proposition P, the value
u E (e) = 1  [ p(e)  (1  E(P))] is the probability of the value true being passed up the graph
from e.5
Now, recalling that a node in  represents a NAND operation, if the truth of a node in  is
independent of the truth of any of its brothers, then for any edge e, the probability of true being
passed up the graph is

5

Note that, in principle, the updating can be performed exactly the same way even if 0 < E(P) < 1.
Thus, the algorithm extends naturally to the case in which there is uncertainty regarding the truth-value of
some of the observable propositions.

169

KOPPEL, FELDMAN, & SEGRE

u E (e) = 1  p(e)



s  children(e)

u E (s).

We call u E (e) the flow of E through e.
We have defined the flow u E (e) such that, under appropriate independence conditions, for any
node n e , u E (e) is in fact the probability that n e is true given   , w  and E. (For a formal proof
of this, see Appendix B.) In particular, for a root r i , the flow u E (er i ) is, even in the absence of the
independence conditions, a good approximation of the probability that the target theory is such
that r i is true given   , w  and E.
In the second stage of the updating algorithm, we propagate the difference between each
computed value u E (er i ) (which lies somewhere between 0 and 1) and its target value i (E)
(which is either 0 or 1) top-down through  in a process similar to backpropagation in neural
networks. As we proceed, we compute a new value v E (e) as well as an updated value for p(e),
for every edge e in  . The new value v E (e) represents an updating of u E (e) where the correct
classification, (E), of the example E has been taken into account.
Thus, we begin by setting each value v E (r i ) to reflect the correct classification of the
example. Let  > 0 be some very small constant6 and let

if i (E) = 0
v E (er i ) = 
1


if i (E) = 1.

Now we proceed top down through  , computing v E (e) for each edge in  . In each case we
compute v E (e) on the basis of u E (e), that is, on the basis of how much of the proof (or refutation)
of E flows through the edge e. The precise formula is
v E (e) = 1  (1  u E (e)) 

v E ( f (e))
u E ( f (e))


max[v E ( f (e)), u E ( f (e))] 
where f (e) is that parent of e for which 1 
 is greatest. We show in
min[v
(
f
(e)),
u
(
f
(e))]
E
E


Appendix B why this formula works.
Finally, we compute p new (e), the new values of p(e), using the current value of p(e) and the
values of v E (e) and u E (e) just computed:
p new (e) = 1  (1  p(e)) 

v E (e)
.
u E (e)

If the deletion of different edges are independent events and  is known to be a subgraph of
, then p new (e) is the conditional probability that the edge e appears in , given the exemplar
 E, (E)  (see proof in Appendix B). Figure 3 gives the pseudo code for processing a single
exemplar.

Strictly speaking, for the computation of conditional probabilities, we need to use  = 0. However, in
order to ensure convergence of the algorithm in all cases, we choose  > 0 (see Appendix C). In the experiments reported in Section 6, we use the value  = . 01.
6

170

BIAS DRIVEN REVISION

function BottomUp(  , p  : weighted dt-graph, E: exemplar): array of real;
begin
S   ; V  Leaves();
for e  Leaves() do
begin
if e  E then u(e)  1;
else u(e)  1  p(e);
S  Merge(S, Parents(e, ));
end
while S   do
begin
e  PopSuitableParent(S, V ); V  AddElement(e, V );
u(e)  1  ( p(e)
 u(d));
d  Children(e,)

S  Merge(S, Parents(e, ));
end
return u;
end

function TopDown(  , p  : weighted dt-graph, u: array of real,
E: exemplar,  : real): weighted dt-graph;
begin
S   ; V  Roots();
for r i  Roots() do
begin
if i (E) = 1 then v(r i )   ;
else v(r i )  1   ;
S  Merge(S, Children(r i , ));
end
while S   do
begin
e  PopSuitableChild(S, V ); V  AddElement(e, V ); f  MostChangedParent(e, );
v( f )
v(e)  1  (1  u(e)) 
;
u( f )
v(e)
p(e)  1  (1  p(e)) 
;
u(e)
S  Merge(S, Children(e, ));
end
return  , p  ;
end
Figure 3: Pseudo code for processing a single exemplar. The functions BottomUp and TopDown
sweep the dt-graph. BottomUp returns an array on edges representing proof flow, while TopDown
returns an updated weighted dt-graph. We are assuming the dt-graph datastructure has been defined and initialized appropriately. Functions Children, Parents, Roots, and Leaves return sets of
edges corresponding to the corresponding graph relation on the dt-graph. Function Merge and AddElement operate on sets, and functions PopSuitableParent and PopSuitableChild return an element of its first argument whose children or parents, respectively, are all already elements of its
second argument while simultaneously deleting the element from the first set, thus guaranteeing
the appropriate graph traversal strategy.

171

KOPPEL, FELDMAN, & SEGRE

Consider the application of this updating algorithm to the weighted dt-graph of Figure 2. We
are given the exemplar  {unsafe-packaging, new-market},  1   , i.e., the example in which
unsafe-packaging and new-market are true (and all other observables are false) should yield a
derivation of the root buy-stock. The weighted dt-graph obtained by applying the algorithm is
shown in Figure 4.
This example illustrates some important general properties of the method.
(1)

Given an IN exemplar, the weight of an odd edge cannot decrease and the weight of an
even edge cannot increase. (The analogous property holds for an OUT exemplar.) In the
case where no negation edge appears in  , this corresponds to the fact that a clause
cannot help prevent a proof, and literals in the body of a clause cannot help complete a

.998
buy-stock
.999
C1
.94

1.0
increased-demand
.98

product-liability
.91

C4
.8
new-market

1.0
C3

.15

product-liability
.69

.69

.88

established-market
superior-flavor

C2
.96

.99

popular-product
unsafe-packaging

Figure 4: The weighted dt-graph of
 {unsafe-packaging, new-market},  1   .

Figure

172

2

after

processing

the

exemplar

BIAS DRIVEN REVISION

proof. Note in particular that the weights of the edges corresponding to the literals
popular-product and established-market in clause C3 dropped by the same amount,
reflecting the identical roles played by them in this example. However, the weight of the
edge corresponding to the literal superior-flavor in clause C4 drops a great deal more than
both those edges, reflecting the fact that the deletion of superior-flavor alone would allow
a proof of buy-stock, while the deletion of either popular-product alone or establishedmarket alone would not allow a proof of buy-stock.
(2)

An edge with initial weight 1 is immutable; its weight remains 1 forever. Thus although an
edge with weight 1, such as that corresponding to the literal increased-demand in clause
C1, may contribute to the prevention of a desired proof, its weight is not diminished since
we are told that there is no possibility of that literal being flawed.

(3)

If the processed exemplar can only be correctly classified if a particular edge e is revised,
then the updated probability of e will approach 0 and e will be immediately revised.7
Thus, for example, were the initial weights of the edge corresponding to establishedmarket and popular-product in C3 to approach 1, the weight of the edge corresponding to
superior-flavor in C4 would approach 0. Since we use weights only as a temporary
device for locating flawed elements, this property renders our updating method more
appropriate for our purposes then standard backpropagation techniques which adjust
weights gradually to ensure convergence.

(4)

The computational complexity of processing a single exemplar is linear in the size of the
theory . Thus, the updating algorithm is quite efficient when compared to revision
techniques which rely on enumerating all proofs for a root. Note further that the
computation required to update a weight is identical for every edge of  regardless of
edge type. Thus, PTR is well suited for mapping onto fine-grained SIMD machines.

3.2. Processing Multiple Exemplars
As stated above, the updating method is applied iteratively to one example at a time (in random
order) until some edge drops below the revision threshold,  . If after a complete cycle no edge
has dropped below the revision threshold, the examples are reordered (randomly) and the
updating is continued.8
For example, consider the weighted dt-graph of Figure 2. After processing the exemplars
 {unsafe-packaging, new-market},  1   ,
 {popular-product, established-market, superior-flavor},  0   , and
 {popular-product, established-market, celebrity-endorsement},  0  
we obtain the dt-graph shown in Figure 5. If our threshold is, say,  = . 1, then we have to revise
the edge corresponding to the clause C3. This reflects the fact that the clause C3 has contributed
7

If we were to choose  = 0 in the definition of v E (er ), then the updated probability would equal 0.

8

Of course, by processing the examples one at a time we abandon any pretense that the algorithm is
Bayesian. In this respect, we are proceeding in the spirit of connectionist learning algorithms in which it is
assumed that the sequential processing of examples in random order, as if they were actually independent,
approximates the collective effect of the examples.

173

KOPPEL, FELDMAN, & SEGRE

.999...
buy-stock
.998
C1
.95

1.0
increased-demand
.98

product-liability
.02

C4
.99
new-market

1.0
product-liability

C3
.15

.69

.69

established-market
superior-flavor

.89
C2
.96

.88

popular-product
unsafe-packaging

Figure 5: The weighted dt-graph of Figure 2 after processing exemplars
 {unsafe-packaging, new-market},  1   ,
 {popular-product, established-market, superior-flavor},  0   , and
 {popular-product, established-market, celebrity-endorsement},  0   .
The clause C3 has dropped below the threshold.

substantially to the misclassification of the second and third examples from the list above while
not contributing substantially to the correct classification of the first.

4. Revising a Flawed Edge
Once an edge has been selected for revision, we must decide how to revise it. Recall that p(e)
represents the product of w(e) and w(n e ). Thus, the drop in p(e) indicates either that e needs to
be deleted or that, less dramatically, a subtree needs to be appended to the node n e . Thus, we need
to determine whether to delete an edge completely or to simply weaken it by adding children;
intuitively, adding edges to a clause node weakens the clause by adding conditions to its body,
174

BIAS DRIVEN REVISION

while adding edges to a proposition node weakens the propositions refutation power by adding
clauses to its definition. Further, if we decide to add children, then we need to determine which
children to add.
4.1. Finding Relevant Exemplars
The first stage in making such a determination consists of establishing, for each exemplar, the role
of the edge in enabling or preventing a derivation of a root. More specifically, for an IN
exemplar,  E, (E)  , of some root, r, an edge e might play a positive role by facilitating a proof
of r, or play a destructive role by preventing a proof of r, or may simply be irrelevant to a proof
of r.
Once the sets of exemplars for which e plays a positive role or a destructive role are
determined, it is possible to append to e an appropriate subtree which effectively redefines the
role of e such that it is used only for those exemplars for which it plays a positive role.9 How,
then, can we measure the role of e in allowing or preventing a proof of r from E?
At first glance, it would appear that it is sufficient to compare the graph  with the graph e
which results from deleting e from . If E |  r and E |/ e r (or vice versa) then it is clear that e
is responsible for r being provable or not provable given the exemplar  E, (E)  . But, this
criterion is too rigid. In the case of an OUT exemplar, even if it is the case that E |/ e r, it is still
necessary to modify e in the event that e allowed an additional proof of r from E. And, in the
case of an IN exemplar, even if it is the case that E |  r it is still necessary not to modify e in
such a way as to further prevent a proof of r from E, since ultimately some proof is needed.
Fortunately, the weights assigned to the edges allow us the flexibility to not merely determine
whether or not there is a proof of r from E given  or e but also to measure numerically the flow
of E through r both with and without e. This is just what is needed to design a simple heuristic
which captures the degree to which e contributes to a proof of r from E.
Let  =  , p  be the weighted dt-graph which is being revised. Let e =  , p  where p
is identical with p, except that p(e) = 1. Let e =  , p  where p is identical with p, except
that p(e) = 0; that is, e is obtained from  by deleting the edge e.
Then define for each root r i
1   (E)  ue (e )
i
ri
E

Ri (  E, (E)  , e, ) = 
.
1   (E)  ue (e )
i
r
E
i


Then if Ri (  E, (E)  , e, ) > 2, we say that e is needed for E and r i and if
Ri (  E, (E)  , e, ) < 1/2 we say that e is destructive for E and r i .
9

PTR is not strictly incremental in the sense that when an edge is revised its role in proving or refuting each exemplar is checked. If strict incrementality is a desideratum, PTR can be slightly modified so
that an edge is revised on the basis of only those exemplars which have already been processed. Moreover,
it is generally not necessary to check all exemplars for relevance. For example, if e is an odd edge and E is
a correctly classified IN exemplar, then e can be neither needed for E (since odd edges can only make
derivations more difficult) nor destructive for E (since E is correctly classified despite e).

175

KOPPEL, FELDMAN, & SEGRE

Intuitively, this means, for example, that the edge e is needed for an IN exemplar, E, of r i , if
most of the derivation of r i from E passes through the edge e. We have simply given formal
definition to the notion that most of the derivation passes through e, namely, that the flow,


u E e (er i ), of E through r i without e is less than half of the flow, u E e (er i ), of E through r i with e.
For negation-free theories, this corresponds to the case where the edge e represents a clause
which is critical for the derivation of r i from E. The intuition for destructive edges and for OUT
exemplars is analogous. Figure 6 gives the pseudo code for computing the needed and destructive
sets for a given edge e and exemplar set .
In order to understand this better, let us now return to our example dt-graph in the state in
which we left it in Figure 5. The edge corresponding to the clause C3 has dropped below the
threshold. Now let us check for which exemplars that edge is needed and for which it is
destructive. Computing R(  E, (E)  , C3, ) for each example E we obtain the following:
R(  {popular-product, unsafe-packaging, established-market},  0   , C3, ) = 0. 8
R(  {unsafe-packaging, new-market},  1   , C3, ) = 1. 0
R(  {popular-product, established-market, celebrity-endorsement},  1   , C3, ) = 136. 1
R(  {popular-product, established-market, superior-flavor},  0   , C3, ) = 0. 1
R(  {popular-product, established-market, ecologically-correct},  0   , C3, ) = 0. 1
R(  {new-market, celebrity-endorsement},  1   , C3, ) = 1. 0
function Relevance(  , p  : weighted dt-graph , : exemplar set, e: edge): tuple of set;
begin
N  ;
D ;
p saved  Copy( p);
for E   do
for r i  Roots() do

p(e)  1; u  BottomUp(, p, E); u E e  u(r i ); p  p saved ;

p(e)  0; u  BottomUp(, p, E); u E e  u(r i ); p  p saved ;
e
u
if i (E) = 1 then Ri  E e ;
uE

1  uEe
else Ri 
;

1  uEe
if Ri > 2 then N  N  {E};
1
if Ri < then D  D  {E};
2
end
end
return  N , D  ;
end
Figure 6: Pseudo code for computing the relevant sets (i.e., the needed and destructive sets) for a
given edge e and exemplar set . The general idea is to compare proof flow (computed using
function BottomUp) both with and without the edge in question for each exemplar in the exemplar
set. Note that the original weights are saved and later restored at the end of the computation.

176

BIAS DRIVEN REVISION

The high value of
R(  {popular-product, established-market, celebrity-endorsement},  1   , C3, )
reflects the fact that without the clause C3, there is scant hope of a derivation of buy-stock for this
example. (Of course, in principle, both new-market and superior-flavor might still be deleted from
the body of clause C4, thus obviating the need for C3, but the high weight associated with the
literal new-market in C4 indicates that this is unlikely.) The low values of
R(  {popular-product, established-market, superior-flavor},  0   , C3, ) and
R(  {popular-product, established-market, ecologically-correct},  0   , C3, )
reflect the fact that eliminating the clause C3 would greatly diminish the currently undesirably
high flow through buy-stock (i.e., probability of a derivation of buy-stock) from each of these
examples.
An interesting case to examine is that of
 {popular-product, unsafe-packaging, established-market},  0   .
It is true that the elimination of C3 is helpful in preventing an unwanted derivation of buy-stock
because it prevents a derivation of increased-demand which is necessary for buy-stock in clause
C1. Nevertheless, R correctly reflects the fact that the clause C3 is not destructive for this
exemplar since even in the presence of C3, buy-stock is not derivable due to the failure of the
literal product-liability.
4.2. Appending a Subtree
Let N be the set of examples for which e is needed for some root and let D be the set of examples
for which e is destructive for some root (and not needed for any other root). Having found the
sets N and D, how do we repair e?
At this point, if the set D is non-empty and the set N is empty, we simply delete the edge
from  . We justify this deletion by noting that no exemplars require e, so deletion will not
compromise the performance of the theory. On the other hand, if N is not empty, we apply some
inductive algorithm10 to produce a disjunctive normal form (DNF) logical expression constructed
from observable propositions which is true for each exemplar in D but no exemplar in N . We
reformulate this DNF expression as a conjunction of clauses by taking a single new literal l as the
head of each clause, and using each conjunct in the DNF expression as the body of one of the
clauses. This set of clauses is converted into dt-graph n with l as its root. We then suture n to e
by adding to  a new node t, an edge from e to t, and another edge from t to the root, l, of n .
In order to understand why this works, first note the important fact that (like every other
subroutine of PTR), this method is essentially identical whether the edge, e, being repaired is a
clause edge, literal edge or negation edge. However, when translating back from dt-graph form to
domain theory form, the new node t will be interpreted differently depending on whether n e is a
clause or a literal. If n e is a literal, then t is interpreted as the clause n e  l. If n e is a clause,
10

Any standard algorithm for constructing decision trees from positive and negative examples can be
used. Our implementation of PTR uses ID3 (Quinlan, 1986). The use of an inductive component to add
new substructure is due to Ourston and Mooney (Ourston & Mooney, in press).

177

KOPPEL, FELDMAN, & SEGRE

then t is interpreted as the negative literal l.11
Now it is plain that those exemplars for which e is destructive will use the graph rooted at t to
overcome the effect of e. If n e is a literal which undesirably excludes E, then E will get by n e by
satisfying the clause t; if n e is a clause which undesirably allows E, then E will be stopped by the
function Revise(  , p  : weighted dt-graph , : set of exemplars, e: edge,  : real): weighted dt-graph;
begin
 N , D   Relevance(  , p  , , e);
if D   then
begin
if N =  then p(e)  0;
else
begin
p(e)   ;
l  NewLiteral();
 ID3 = DTGraph(l, DNF-ID3(D, N ));
t  NewNode();   AddNode(, t);
if Clause?(n e ) then Label(t)  l;
else Label(t)  NewClause();
  AddEdge(,  n e , t  ); p(  n e , t  )   ;
  AddEdge(,  t, Root( ID3 )  ); p(  t, Root( ID3 )  )  1;
     ID3 ; for e   ID3 do p(e)  1;
end
end
return  , p  ;
end
Figure 7: Pseudo code for performing a revision. The function Revise takes a dt-graph, a set of exemplars , an edge to be revised e, and a parameter  as inputs and produces a revised dt-graph as
output. The function DNF-ID3 is an inductive learning algorithm that produces a DNF formula
that accepts elements of D but not of N , while the function DTGraph produces a dt-graph with the
given root from the given DNF expression as described in the text. For the sake of expository simplicity, we have not shown the special cases in which n e is a leaf or e is a negation edge, as discussed in Footnote 11.

11

Of course, if we were willing to sacrifice some elegance, we could allow separate sub-routines for
the clause case and the literal case. This would allow us to make the dt-graphs to be sutured considerably
more compact. In particular, if n e is a literal we could suture the children of l in n directly to n e . If n e is a
clause, we could use the inductive algorithm to find a DNF expression which excludes examples in D and
includes those in N (rather than the other way around as we now do it). Translating this expression to a dtgraph  with root l, we could suture n to  by simply adding an edge from the clause n e to the root l.
Moreover, if n represents a single clause l  l 1 , . . . , l m then we can simply suture each of the leaf-nodes
l 1 , . . . , l m directly to n e . Note that if n e is a leaf or a negative literal, it is inappropriate to append child
edges to n e . In such cases, we simply replace n e with a new literal l and append to l both n and the graph
of the clause l  n e .

178

BIAS DRIVEN REVISION

new literal t = l.
Whenever a graph n is sutured into  , we must assign weights to the edges of n . Unlike
the original domain theory, however, the new substructure is really just an artifact of the inductive
algorithm used and the current relevant exemplar set. For this reason, it is almost certainly
inadvisable to try to revise it as new exemplars are encountered. Instead, we would prefer that
this new structure be removed and replaced with a more appropriate new construct should the
need arise. To ensure replacement instead of revision, we assign unit certainty factors to all edges
of the substructure. Since the internal edges of the new structure have weights equal to 1, they
will never be revised. Finally, we assign a default weight  to the substructure root edge  n e , t  ,
that connects the new component to the existing  and we reset the weight of the revised edge,
e, to the same value  . Figure 7 gives the pseudo code for performing the revision step just
described.
Consider our example from above. We are repairing the clause C3. We have already found
that the set D consists of the examples
{popular-product, established-market, superior-flavor} and
{popular-product, established-market, ecologically-correct}
while the set N consists of the single example
{popular-product, established-market, celebrity-endorsement}.
Using ID3 to find a formula which excludes N and includes D, we obtain { celebrityendorsement} which translates into the single clause, {l  celebrity-endorsement}. Translating
into dt-graph form and suturing (and simplifying using the technique of Footnote 11), we obtain
the dt-graph shown in Figure 8.
Observe now that the domain theory  represented by this dt-graph correctly classifies the
examples
{popular-product, established-market, superior-flavor} and
{popular-product, established-market, ecologically-correct}
which were misclassified by the original domain theory .

5. The PTR Algorithm
In this section we give the details of the control algorithm which puts the pieces of the previous
two sections together and determines termination.
5.1. Control
The PTR algorithm is shown in Figure 9. We can briefly summarize its operation as follows:
(1)

PTR process exemplars in random order, updating weights and performing revisions
when necessary.

(2)

Whenever a revision is made, the domain theory which corresponds to the newly revised
graph is checked against all exemplars.

(3)

PTR terminates if:
(i) All exemplars are correctly classified, or
(ii) Every edge in the newly revised graph has weight 1.

179

KOPPEL, FELDMAN, & SEGRE

.999...
buy-stock
.998
C1
.95

1.0
increased-demand

product-liability
.70

.98

1.0

C4
.99
new-market

C3
.15

.69

.70

product-liability
.69

established-market
celebrity-endorsement
superior-flavor

.89
C2
.96

.88

popular-product
unsafe-packaging

Figure 8: The weighted dt-graph of Figure 2 after revising the clause C3 (the graph has been
slightly simplified in accordance with the remark in Footnote 11).

(4)

If, after a revision is made, PTR does not terminate, then it continues processing
exemplars in random order.

(5)

if, after a complete cycle of exemplars has been processed, there remain misclassified
exemplars, then we
(i) Increment the revision threshold  so that  = min[ +   , 1], and
(ii) Increment the value  assigned to a revised edge and to the root edge of an added
component, so that  = min[ +   , 1].

(6)

Now we begin anew, processing the exemplars in (new) random order.

It is easy to see that PTR is guaranteed to terminate. The argument is as follows. Within
1 1
max  ,  cycles, both  and  will reach 1. At this point, every edge with weight less than
   
180

BIAS DRIVEN REVISION

1 will be revised and will either be deleted or have its weight reset to  = 1. Moreover, any edges
added during revision will also be assigned certainty factor  = 1. Thus all edges will have weight
1 and the algorithm terminates by the termination criterion (ii).
Now, we wish to show that PTR not only terminates, but that it terminates with every
exemplar correctly classified. That is, we wish to show that, in fact, termination criterion (ii) can
never be satisfied unless termination criterion (i) is satisfied as well. We call this property
convergence. In Appendix C we prove that, under certain very general conditions, PTR is
guaranteed to converge.
5.2. A Complete Example
Let us now review the example which we have been considering throughout this paper.
We begin with the flawed domain theory and set of exemplars introduced in Section 1.
C1: buy-stock  increased-demand  product-liability
C2: product-liability  popular-product  unsafe-packaging
C3: increased-demand  popular-product  established-market
C4: increased-demand  new-market  superior-flavor.
We translate the domain theory into the weighted dt-graph   , p  of Figure 2, assigning
weights via a combination of user-provided information and default values. For example, the user
has indicated that their confidence in the first literal (increased-demand) in the body of clause C1
is greater than their confidence in the second literal ( product-liability).
function PTR(  , p  : weighted dt-graph, : set of exemplars,
  0 ,  0 ,   ,   ,   : five tuple of real): weighted dt-graph;
begin
   0;
   0;
while  E   such that (E)  (E) do
begin
for E  RandomlyPermute() do
begin
u  BottomUp(  , p  , E);
 , p   TopDown(  , p  , u, E,  );
if  e   such that p(e)   then  , p   Revise(  , p  , ,  ,  );
if  e  , p(e) = 1 or  E  , (E) = (E) then return  , p  ;
end
  max[  +   , 1];
  max[ +   , 1];
end
end
Figure 9: The PTR control algorithm. Input to the algorithm consists of a weighted dt-graph
 , p  , a set of exemplars , and five real-valued parameters  0 ,  0 ,   ,   , and  . The algorithm
produces a revised weighted dt-graph whose implicit theory correctly classifies all exemplars in .

181

KOPPEL, FELDMAN, & SEGRE

We set the revision threshold  to .1, the reset value  initially to .7 and their respective
increments   and   to . 03. We now start updating the weights of the edges by processing the
exemplars in some random order.
We first process the exemplar
 {unsafe-packaging, new-market},  1   .
First, the leaves of the dt-graph are labeled according to their presence or absence in the exemplar.
Second, u E (e) values (proof flow) are computed for all edges of the dt-graph in bottom up
fashion. Next, v E (er i ) values are set to reflect the vector of correct classifications for the example
(E). New values for v E (e) are computed in top down fashion for each edge in the dt-graph. As
these values are computed, new values for p(e) are also computed. Processing of this first
exemplar produces the updated dt-graph shown in Figure 3.
Processing of exemplars continues until either an edge weight falls below  (indicating a
flawed domain theory element has been located), a cycle (processing of all known exemplars) is
completed, or the PTR termination conditions are met. For our example, after processing the
additional exemplars
 {popular-product, established-market, superior-flavor},  0   and
 {popular-product, established-market, ecologically-correct},  0  
the weight of the edge corresponding to clause C3 drops below  (see Figure 5), indicating that
this edge needs to be revised.
We proceed with the revision by using the heuristic in Section 4.2 in order to determine for
which set of exemplars the edge in question is needed and for which it is destructive. The edge
corresponding to the clause C3 is needed for
{  {popular-product, established-market, celebrity-endorsement},  1   }
and is destructive for
{  {popular-product, established-market, ecologically-correct},  0   ,
 {popular-product, established-market, superior-flavor},  0   }.
Since the set for which the edge is needed is not empty, PTR chooses to append a subtree
weakening clause C3 rather than simply deleting the clause outright. Using these sets as input to
ID3, we determine that the fact celebrity-endorsement suitably discriminates between the needed
and destructive sets. We then repair the graph to obtain the weighted dt-graph shown in Figure 8.
This graph corresponds to the theory in which the literal celebrity-endorsement has been added to
the body of C3.
We now check the newly-obtained theory embodied in the dt-graph of Figure 8 (i.e., ignoring
weights) against all the exemplars and determine that there are still misclassified exemplars,
namely
 {unsafe-packaging, new-market},  1   and
 {new-market, celebrity-endorsement},  1   .
Thus, we continue processing the remaining exemplars in the original (random) order.
After processing the exemplars
 {popular-product, unsafe-packaging, established-market},  0   ,
 {popular-product, established-market, celebrity-endorsement},  1   , and
 {new-market, celebrity-endorsement},  1   ,
182

BIAS DRIVEN REVISION

the weight of the edge corresponding to the literal superior-flavor in clause C4 drops below the
revision threshold  . We then determine that this edge is not needed for any exemplar and thus
the edge is simply deleted.
At this point, no misclassified exemplars remain. The final domain theory is:
C1: buy-stock  increased-demand  product-liability
C2: product-liability  popular-product  unsafe-packaging
C3: increased-demand  popular-product  established-market  celebrity-endorsement
C4: increased-demand  new-market.
This theory correctly classifies all known exemplars and PTR terminates.

6. Experimental Evaluation
In this section we will examine experimental evidence that illustrates several fundamental
hypotheses concerning PTR. We wish to show that:
(1)

theories produced by PTR are of high quality in three respects: they are of low radicality,
they are of reasonable size, and they provide accurate information regarding exemplars
other than those used in the training.

(2)

PTR converges rapidly  that is, it requires few cycles to find an adequate set of
revisions.

(3)

well-chosen initial weights provided by a domain expert can significantly improve the
performance of PTR.

More precisely, given a theory  obtained by using PTR to revise a theory  on the basis of a
set of training examplars, we will test these hypotheses as follows.
Radicality. Our claim is that Rad  () is typically close to minimal over all theories which
correctly classify all the examples. For cases where the target theory, , is known, we measure
Rad  ()
. If this value is less than 1, then PTR can be said to have done even better than
Rad  ()
finding the target theory in the sense that it was able to correctly classify all training examples
using less radical revisions than those required to restore the target theory. If the value is greater
than 1, then PTR can be said to have over-revised the theory.
Cross-validation. We perform one hundred repetitions of cross-validation using nested sets
of training examples. It should be noted that our actual objective is to minimize radicality, and
that often there are theories that are less radical than the target theory which also satisfy all
training examples. Thus, while cross-validation gives some indication that theory revision is
being successfully performed, it is not a primary objective of theory revision.
Theory size. We count the number of clauses and literals in the revised theory merely to
demonstrate that theories obtained using PTR are comprehensible. Of course, the precise size of
the theory obtained by PTR is largely an artifact of the choice of inductive component.
Complexity. Processing a complete cycle of exemplars is O(n  d) where n is the number of
edges in the graph and d is the number of exemplars. Likewise repairing an edge is O(n  d). We
will measure the number of cycles and the number of repairs made until convergence. (Recall
1 1
that the number of cycles until convergence is in any event bounded by max  , . We will
   
show that, in practice, the number of cycles is small even if   =   = 0.
183

KOPPEL, FELDMAN, & SEGRE

Utility of Bias. We wish to show that user-provided guidance in choosing initial weights
leads to faster and more accurate results. For cases in which the target theory, , is known, let S
be the set of edges of  which need to be revised in order to restore the target theory . Define
1

1

/ S, p  (e) = ( p(e))  .
p  (e) such that for each e  S, 1  p  (e) = (1  p(e))  and for each e 
That is, each edge which needs to be revised to obtain the intended theory has its initial weight
diminished and each edge which need not be revised to obtain the intended theory has its weight
increased. Let   =   , p   . Then, for each  ,
1

Rad   () =  log(  (1  p(e))  
e S

1

 ( p(e))  ) =

e
/ S

1


Rad  ().

Here, we compare the results of cross-validation and number-of-cycles experiments for  = 2
with their unbiased counterparts (i.e.,  = 1).
6.1. Comparison with other Methods
In order to put our results in perspective we compare them with results obtained by other
methods.12
(1)

ID3 (Quinlan, 1986) is the inductive component we use in PTR. Thus using ID3 is
equivalent to learning directly from the examples without using the initial flawed domain
theory. By comparing results obtained using ID3 with those obtained using PTR we can
gauge the usefulness of the given theory.

(2)

EITHER (Ourston & Mooney, in press) uses enumeration of partial proofs in order to find
a minimal set of literals, the repair of which will satisfy all the exemplars. Repairs are
then made using an inductive component. EITHER is exponential in the size of the
theory. It cannot handle theories with negated internal literals. It also cannot handle
theories with multiple roots unless those roots are mutually exclusive.

(3)

KBANN (Towell & Shavlik, 1993) translates a symbolic domain theory into a neural net,
uses backpropagation to adjust the weights of the nets edges, and then translates back
from net form to partially symbolic form. Some of the rules in the theory output by
KBANN might be numerical, i.e., not strictly symbolic.

(4)

RAPTURE (Mahoney & Mooney, 1993) uses a variant of backpropagation to adjust
certainty factors in a probabilistic domain theory. If necessary, it can also add a clause to
a root. All the rules produced by RAPTURE are numerical. Like EITHER, RAPTURE
cannot handle negated internal literals or multiple roots which are not mutually exclusive.

Observe that, relative to the other methods considered here, PTR is liberal in terms of the
theories it can handle, in that (like KBANN, but unlike EITHER and RAPTURE) it can handle
negated literals and non-mutually exclusive multiple roots; it is also strict in terms of the theories
it yields in that (like EITHER, but unlike KBANN and RAPTURE) it produces strictly symbolic
theories.
12

There are other interesting theory revision algorithms, such as RTLS (Ginsberg, 1990), for which no
comparable data is available.

184

BIAS DRIVEN REVISION

We have noted that both KBANN and RAPTURE output numerical rules. In the case of
KBANN, a numerical rule is one which fires if the sum of weights associated with satisfied
antecedents exceeds a threshold. In the case of RAPTURE, the rules are probabilistic rules using
certainty factors along the lines of MYCIN (Buchanan & Shortliffe, 1984). One might ask, then,
to what extent are results obtained by theory revision algorithms which output numerical rules
merely artifacts of the use of such numerical rules? In other words, can we separate the effects of
using numerical rules from the effects of learning?
To make this more concrete, consider the following simple method for transforming a
symbolic domain theory into a probabilistic domain theory and then reclassifying examples using
the obtained probabilistic theory. Suppose we are given some possibly-flawed domain theory .
Suppose further that we are not given the classification of even a single example. Assign a weight
p(e) to each edge of  according to the default scheme of Appendix A. Now, using the bottomup subroutine of the updating algorithm, compute u E (er ) for each test example E. (Recall that
u E (er ) is a measure of how close to a derivation of r from E there is, given the weighted dt-graph
  , p  .) Now, for some chosen cutoff value 0  n  100, if E 0 is such that u E 0 (er ) lies in
the upper n% of the set of values {u E (er )} then conclude that  is true for E 0 ; otherwise conclude
that  is false for E 0 .
This method, which for the purpose of discussion we call PTR*, does not use any training
examples at all. Thus if the results of theory revision systems that employ numerical rules can be
matched by PTR*  which performs no learning  then it is clear that the results are merely
artifacts of the use of numerical rules.
6.2. Results on the PROMOTER Theory
We first consider the PROMOTER theory from molecular biology (Murphy & Aha, 1992), which
is of interest solely because it has been extensively studied in the theory revision literature
(Towell & Shavlik, 1993), thus enabling explicit performance comparison with other algorithms.
The PROMOTER theory is a flawed theory intended to recognize promoters in DNA nucleotides.
The theory recognized none of a set of 106 examples as promoters despite the fact that precisely
half of them are indeed promoters.13
Unfortunately, the PROMOTER theory (like many others used in the theory revision
literature) is trivial in that it is very shallow. Moreover, it is atypical of flawed domains in that it
is overly specific but not overly general. Given the shortcomings of the PROMOTER theory, we
will also test PTR on a synthetically-generated theory in which errors have been artificially
introduced. These synthetic theories are significantly deeper than those used to test previous
methods. Moreover, the fact that the intended theory is known will enable us to perform
experiments involving radicality and bias.

13

In our experiments, we use the default initial weights assigned by the scheme of Appendix A. In addition, the clause whose head is the proposition contact is treated as a definition not subject to revision but
only deletion as a whole.

185

KOPPEL, FELDMAN, & SEGRE

6.2.1. Cross-validation
In Figure 10 we compare the results of cross-validation for PROMOTER. We distinguish
between methods which use numerical rules (top plot) and those which are purely symbolic
(bottom plot).
The lower plot in Figure 10 highlights the fact that, using the value n = 50, PTR* achieves
better accuracy, using no training examples, than any of the methods considered here achieve
using 90 training examples. In particular, computing u E (er ) for each example, we obtain that of
the 53 highest-ranking examples 50 are indeed promoters (and, therefore, of the 53 lowestranking examples 50 are indeed non-promoters). Thus, PTR* achieves 94. 3% accuracy. (In fact,
all of the 47 highest-ranking examples are promoters and all of the 47 lowest-ranking are not
promoters. Thus, a more conservative version of PTR* which classifies the, say, 40% highestranking examples as IN and the 40% lowest-ranking as OUT, would indeed achieve 100%
accuracy over the examples for which it ventured a prediction.)
This merely shows that the original PROMOTER theory is very accurate provided that it is
given a numerical interpretation. Thus we conclude that the success of RAPTURE and KBANN
for this domain is not a consequence of learning from examples but rather an artifact of the use of
numerical rules.
As for the three methods  EITHER, PTR and ID3  which yield symbolic rules, we see in
the top plot of Figure 10 that, as reported in (Ourston & Mooney, in press; Towell &
Shavlik, 1993), the methods which exploit the given flawed theory do indeed achieve better
results on PROMOTER than ID3, which does not exploit the theory. Moreover, as the size of the
training set grows, the performance of PTR is increasingly better than that of EITHER.14
Finally, we wish to point out an interesting fact about the example set. There is a set of 13
out of the 106 examples which each contain information substantially different than that in the
rest of the examples. Experiments show that using ten-fold cross-validation on the 93 good
examples yields 99. 2% accuracy, while training on all 93 of these examples and testing on the 13
bad examples yields below 40% accuracy.
6.2.2. Theory size
The size of the output theory is an important measure of the comprehensibility of the output
theory. Ideally, the size of the theory should not grow too rapidly as the number of training
examples is increased, as larger theories are necessarily harder to interpret. This observation
holds both for the number of clauses in the theory as well as for the average number of
antecedents in each of those clauses.
Theory sizes for the theories produced by PTR are shown in Figure 11. The most striking
aspect of these numbers is that all measures of theory size are relatively stable with respect to
training set size. Naturally, the exact values are to a large degree an artifact of the inductive
learning component used. In contrast, for EITHER, theory size increases with training set size
14

Those readers familiar with the PROMOTER theory should note that the improvement over EITHER is a consequence of PTR repairing one flaw at a time and using a sharper relevance criterion. This
results in PTR always deleting the extraneous conformation literal, while EITHER occasionallly fails to do
so, particularly as the number of training exmaple increases.

186

BIAS DRIVEN REVISION

60
% Misclassified
50

ID3
EITHER
PTR

40
30
20
10
0
0

20

40

60

60
% Misclassified
50

80
100
# Training Exemplars

RAPTURE
KBANN
PTR*

40
30
20
10
0
0

20

40

60

80
100
# Training Exemplars

Figure 10: PROMOTER: Error rates using nested training sets for purely symbolic theories (top
plot) and numeric theories (bottom plot). Results for EITHER, RAPTURE, and KBANN are taken
from (Mahoney & Mooney, 1993), while results for ID3 and PTR were generated using similar experimental procedures. Recall that PTR* is a non-learning numerical rule system; the PTR* line is
extended horizontally for clarity.

187

KOPPEL, FELDMAN, & SEGRE

Training
Set Size

Mean
Clauses in
Output

Mean
Literals in
Output

Original
Theory

14

83

20
40
60
80
100

11
11
11
11
12

39
36
35
32
36

Mean
Revisions to
Convergence

Mean
Exemplars to
Convergence

10.7
15.2
18.2
22.1
22.0

88
140
186
232
236

Figure 11: PROMOTER: Results. Numbers reported for each training set size are average values
over one hundred trials (ten trials for each of ten example partitions).

(Ourston, 1991). For example, for 20 training examples the output theory size (clauses plus
literals) is 78, while for 80 training examples, the output theory size is 106.
Unfortunately, making direct comparisons with KBANN or RAPTURE is difficult. In the
case of KBANN and RAPTURE, which allow numerical rules, comparison is impossible given
the differences in the underlying representation languages. Nevertheless, it is clear that, as
expected, KBANN produces significantly larger theories than PTR. For example, using 90
training examples from the PROMOTER theory, KBANN produces numerical theories with, on
average, 10 clauses and 102 literals (Towell & Shavlik, 1993). These numbers would grow
substantially if the theory were converted into strictly symbolic terms. RAPTURE, on the other
hand, does not change the theory size, but, like KBANN, yields numerical rules (Mahoney &
Mooney, 1993).
6.2.3. Complexity
EITHER is exponential in the size of the theory and the number of training examples. For
KBANN, each cycle of the training-by-backpropagation subroutine is O(d  n) (where d is the
size of the network and n is the number of exemplars), and the number of such cycles typically
numbers in the hundreds even for shallow nets.
Like backpropagation, the cost of processing an example with PTR is linear in the size of the
theory. In contrast, however, PTR typically converges after processing only a tiny fraction of the
number of examples required by standard backpropagation techniques. Figure 11 shows the
average number of exemplars (not cycles!) processed by PTR until convergence as a function of
training set size. The only other cost incurred by PTR is that of revising the theory. Each such
revision in O(d  n). The average number of revisions to convergence is also shown in Figure 11.
6.3. Results on Synthetic Theories
The character of the PROMOTER theory make it less than ideal for testing theory revision
algorithms. We wish to consider theories which (i) are deeper, which (ii) make substantial use of
negated internal literals and which (iii) are overly general as well as overly specific. As opposed
to shallow theories which can generally be easily repaired at the leaf level, deeper theories often
188

BIAS DRIVEN REVISION

require repairs at internal levels of the theory. Therefore, a theory revision algorithm which may
perform well on shallow theories will not necessarily scale up well to larger theories. Moreover,
as theory size increases, the computational complexity of an algorithm might preclude its
application altogether. We wish to show that PTR scales well to larger, deeper theories.
Since deeper, propositional, real-world theories are scarce, we have generated them
synthetically. As an added bonus, we now know the target theory so we can perform controlled
experiments on bias and radicality. In (Feldman, 1993) the aggregate results of experiments
performed on a collection of synthetic theories are reported. In order to avoid the dubious
practice of averaging results over different theories and in order to highlight significant features of
a particular application of PTR, we consider here one synthetic theory typical of those studied in
(Feldman, 1993).

r  A, B
r  C, D
A  E, F
A  p0 , G, p1 , p2 , p3
B   p0
B  p1 , H
B  p4 ,  p11
C  I, J
C  p2 , K
C   p8 ,  p9
D  p10 ,  p12 , L
D  p3 ,  p9 , M
E  N , p5 , p6
E  O,  p7 ,  p8
F  p4
F  Q, R
G  S,  p3 , p8
G   p10 , p12
H  U, V
H  p1 , p2 ; p3 , p4
I W
I  p6
J  X, p5
J Y
K  P,  p5 , p9
K   p6 , p9

L  T , p1
L  p2 , p12 , p16
M  Z ,  p17
M  p18 ,  p19
N   p0 , p1
N  p3 , p4 , p6
N  p10 ,  p12
Z  p2 , p3
Z   p2 , p3 , p17 , p18 , p20
O   p3 , p4 , p5 , p11 ,  p12
O   p13 , p18
Y  p4 , p5 p6
P   p6 , p7 , p8
X  p7 , p9
Q  p0 , p4
Q  p3 ,  p13 , p14 , p15
W  p10 , p11
W  p3 , p9
R  p12 ,  p13 , p14
V   p14 , p15
S  p3 , p6 ,  p14 , p15 , p16
U  p11 , p12
U  p13 , p14 ,  p15 ,  p16 ,  p17
T  p7
T   p7 , p8 , p9 ,  p16 ,  p17 ,  p18

Figure 12: The synthetic domain theory  used for the experiments of Section 6.

189

KOPPEL, FELDMAN, & SEGRE

The theory  is shown is Figure 12. Observe that  includes four levels of clauses and has
many negated internal nodes. It is thus substantially deeper than theories considered before in
testing theory revision algorithms. We artificially introduce, in succession, 15 errors into the
theory . The errors are shown in Figure 13. For each of these theories, we use the default initial
weights assigned by the scheme of Appendix A.
Let i be the theory obtained after introducing the first i of these errors. In Figure 14 we
show the radicality, Rad i (), of  relative to each of the flawed theories, i for i = 3, 6, 9, 12, 15,
as well as the number of examples misclassified by each of those theories. Note that, in general,
the number of misclassified examples cannot necessarily be assumed to increase monotonically
with the number of errors introduced since introducing an error may either generalize or
specialize the theory. For example, the fourth error introduced is undone by the fifth error.
Nevertheless, it is the case that for this particular set of errors, each successive theory is more
radical and misclassifies a larger number of examples with respect to .
To measure radicality and accuracy, we choose 200 exemplars which are classified according
to . Now for each i (i = 3, 6, 9, 12, 15), we withhold 100 test examples and train on nested sets
of 20, 40, 60, 80 and 100 training examples. We choose ten such partitions and run ten trials for
each partition.
Rad i ()
, where  is the theory produced by
In Figure 15, we graph the average value of
Rad i ()
PTR. As can be seen, this value is consistently below 1. This indicates that the revisions found

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

Added clause A   p6
Added clause S   p5
Added clause A  p8 ,  p15
Added literal  p6 to clause B  p4 ,  p11
Deleted clause B  p4 ,  p6 ,  p11
Added clause D   p14
Added clause G   p12 , p8
Added literal p2 to clause A  E, F
Added clause L  p16
Added clause M   p13 ,  p7
Deleted clause Q  p3 ,  p13 , p14 , p15
Deleted clause L  p2 , p12 , p16
Added clause J  p11
Deleted literal p4 from clause F  p4
Deleted literal p1 from clause B  p1 , H

Figure 13: The errors introduced into the synthetic theory  in order to produce the flawed synthetic theories i . Note that the fifth randomly-generated error obviates the fourth.

190

BIAS DRIVEN REVISION

3

6

9

Number of Errors

3

6

9

Rad()

7.32

12

15

12

15

17.53

22.66

27.15

33.60

Misclassified IN
Misclassified OUT

0
50

26
45

34
45

34
46

27
64

Initial Accuracy

75%

64.5%

60.5%

60%

54.5%

Figure 14: Descriptive statistics for the flawed synthetic theories i (i = 3, 6, 9, 12, 15).

1
Normalized
Radicality
0.8

0.6

0.4

15
12
9
6
3

0.2

0
20

40

60

80
100
# Training Exemplars

Rad i ()
, for the output theories  produced by PTR from
Rad i ()
i (i = 3, 6, 9, 12, 15). Error bars reflect 1 standard error.

Figure 15: The normalized radicality,

by PTR are less radical than what is needed to restore the original . Thus by the criterion of
success that PTR set for itself, minimizing radicality, PTR does better than restoring . As is to
be expected, the larger the training set the closer this value is to 1. Also note that as the number
of errors introduced increases, the saving in radicality achieved by PTR increases as well, since a
larger number of opportunities are created for more parsimonious revision. More precisely, the
191

KOPPEL, FELDMAN, & SEGRE

average number of revisions made by PTR to 3 , 6 , 9 , 12 , and 15 with a 100 element training
set are 1.4, 4.1, 7.6, 8.3, and 10.4, respectively.
An example will show how PTR achieves this. Note from Figure 13 that the errors introduced
in 3 are the additions of the rules:
A   p6
S   p5
S  p8 ,  p15 .
In most cases, PTR quickly locates the extraneous clause A   p6 , and discovers that deleting it
results in the correct classification of all exemplars in the training set. In fact, this change also
results in the correct classification of all test examples as well. The other two added rules do not
affect the classification of any training examples, and therefore are not deleted or repaired by
PTR. Thus the radicality of the changes made by PTR is lower than that required for restoring the
original theory. In a minority of cases, PTR first deletes the clause B   p0 and only then deletes
the clause A  p6 . Since the literal B is higher in the tree than the literal S, the radicality of these
changes is marginally higher that that required to restore the original theory.
In Figure 16, we graph the accuracy of  on the test set. As expected, accuracy degenerates
somewhat as the number of errors is increased. Nevertheless, even for 15 , PTR yields theories
which generalize accurately.
Figure 17 shows the average number of exemplars required for convergence. As expected,
the fewer errors in the theory, the fewer exemplars PTR requires for convergence. Moreover, the

60
% Misclassified
50

15
12
9
6
3

40
30
20
10
0
0

20

40

60

80
100
# Training Exemplars

Figure 16: Error rates for the output theories produced by PTR from i (i = 3, 6, 9, 12, 15).

192

BIAS DRIVEN REVISION

300
Exemplars to
Convergence
250

15
12
9
6
3

200
150
100
50
0
0

20

40

60

80
100
# Training Exemplars

Figure 17: Number of exemplars processed until convergence for i (i = 3, 6, 9, 12, 15).

number of exemplars processed grows less than linearly with the training set size. In fact, in no
case was the average number of examples processed greater than 4 times the training set size. In
comparison, backpropagation typically requires hundreds of cycles when it converges.
Next we wish show the effects of positive bias, i.e., to show that user-provided guidance in
the choice of initial weights can improve speed of convergence and accuracy in cross-validation.
For each of the flawed theories 3 and 15 , we compare the performance of PTR using default
initial weights and biased initial weights (  = 2). In Figure 18, we show how cross-validation
accuracy increases when bias is introduced. In Figure 19, we show how the number of examples
which need to be processed until convergence decreases when bias is introduced.
Returning to the example above, we see that the introduction of bias allows PTR to
immediately find the flawed clause A  p6 and to delete it straight away. In fact, PTR never
requires the processing of more than 8 exemplars to do so. Thus, in this case, the introduction of
bias both speeds up the revision process and results in the consistent choice of the optimal
revision.
Moreover, it has also been shown in (Feldman, 1993) that PTR is robust with respect to
random perturbations in the initial weights. In particular, in tests on thirty different syntheticallygenerated theories, introducing small random perturbations to each edge of a dt-graph before
training resulted in less than 2% of test examples being classified differently than when training
was performed using the original initial weights.

193

KOPPEL, FELDMAN, & SEGRE

60
% Misclassified
50

15
3
15 + bias
3 + bias

40
30
20
10
0
0

20

40

60

80
100
# Training Exemplars

Figure 18: Error rates for the output theories produced by PTR from i (i = 3, 6, 9, 12, 15), using
favorably-biased initial weights.

6.4. Summary
Repairing internal literals and clauses is as natural for PTR as repairing leaves. Moreover, PTR
converges rapidly. As a result, PTR scales up to deep theories without difficulty. Even for very
badly flawed theories, PTR quickly finds repairs which correctly classify all known exemplars.
These repairs are typically less radical than restoring the original theory and are close enough to
the original theory to generalize accurately to test examples.
Moreover, although PTR is robust with respect to initial weights, user guidance in choosing
these weights can significantly improve both speed of convergence and cross-validation accuracy.

7. Conclusions
In this paper, we have presented our approach, called PTR, to the theory revision problem for
propositional theories. Our approach uses probabilities associated with domain theory elements
to numerically track the flow of proof through the theory, allowing us to efficiently locate and
repair flawed elements of the theory. We prove that PTR converges to a theory which correctly
classifies all examples, and show experimentally that PTR is fast and accurate even for deep
theories.
There are several ways in which PTR can be extended.
First-order theories. The updating method at the core of PTR assumes that provided
exemplars unambiguously assign truth values to each observable proposition. In first-order
theory revision the truth of an observable predicate typically depends on variable assignments.
194

BIAS DRIVEN REVISION

300
Exemplars to
Convergence
250

15
15 + bias
3
3 + bias

200
150
100
50
0
0

20

40

60

80
100
# Training Exemplars

Figure 19: Number of exemplars processed until convergence using favorably-biased initial
weights.

Thus, in order to apply PTR to first-order theory revision it is necessary to determine optimal
variable assignments on the basis of which probabilities can be updated. One method for doing so
is discussed in (Feldman, 1993).
Inductive bias. PTR uses bias to locate flawed elements of a theory. Another type of bias can
be used to determine which revision to make. For example, it might be known that a particular
clause might be missing a literal in its body but should under no circumstances be deleted, or that
only certain types of literals can be added to the clause but not others. Likewise, it might be
known that a particular literal is replaceable but not deletable, etc. It has been shown (Feldman et
al., 1993) that by modifying the inductive component of PTR to account for such bias, both
convergence speed and cross-validation accuracy are substantially improved.
Noisy exemplars. We have assumed that it is only the domain theory which is in need of
revision, but that the exemplars are all correctly classified. Often this is not the case. Thus, it is
necessary to modify PTR to take into account the possibility of reclassifying exemplars on the
basis of the theory rather than vice-versa. The PTR* algorithm (Section 6) suggests that
misclassed exemplars can sometimes be detected before processing. Briefly, the idea is that an
example which allows multiple proofs of some root is almost certainly IN for that root regardless
of the classification we have been told. Thus, if u E (er ) is high, then E is probably IN regardless
of what we are told; analogously, if u E (er ) is low. A modified version of PTR based on this
observation has already been successfully implemented (Koppel et al., 1993).
In conclusion, we believe the PTR system marks an important contribution to the domain
theory revision problem. More specifically, the primary innovations reported here are:
195

KOPPEL, FELDMAN, & SEGRE

(1)

By assigning bias in the form of the probability that an element of a domain theory is
flawed, we can clearly define the objective of a theory revision algorithm.

(2)

By reformulating a domain theory as a weighted dt-graph, we can numerically trace the
flow of a proof or refutation through the various elements of a domain theory.

(3)

Proof flow can be used to efficiently update the probability that an element is flawed on
the basis of an exemplar.

(4)

By updating probabilities on the basis of exemplars, we can efficiently locate flawed
elements of a theory.

(5)

By using proof flow, we can determine precisely on the basis of which exemplars to revise
a flawed element of the theory.

Acknowledgments
The authors wish to thank Hillel Walters of Bar-Ilan University for his significant contributions to
the content of this paper. The authors also wish to thank the JAIR reviewers for their
exceptionally prompt and helpful remarks. Support for this research was provided in part by the
Office of Naval Research through grant N00014-90-J-1542 (AMS, RF) and the Air Force Office
of Scientific Research under contract F30602-93-C-0018 (AMS).

196

BIAS DRIVEN REVISION

Appendix A: Assigning Initial Weights
In this appendix we give one method for assigning initial weights to the elements of a domain
theory. The method is based on the topology of the domain theory and assumes that no userprovided information regarding the likelihood of errors is available. If such information is
available, then it can be used to override the values determined by this method.
The method works as follows. First, for each edge e in  we define the semantic impact
of e, (e). (e) is meant to signify the proportion of examples whose classification is directly
affected by the presence of e in  .
One straightforward way of formally defining (e) is the following. Let  I be the pair
  , I  such that I assigns all root and negation edges the weight 1 and all other edges the
1
weight . Let I (e) be identical to I except that e and all its ancestor edges have been assigned
2
the weight 1. Let E be the example such that for each observable proposition P in , E(P) is the
a priori probability that P is true in a randomly selected example.15 In particular, for the typical
1
case in which observable propositions are Boolean and all example are equiprobable, E(P) = .
2
E can be thought of as the average example. Then, if no edge of  has more than one parentedge, we formally define the semantic significance, (e), of an edge e in  as follows:


(e) = uE (er )  u E e
I (e)

I (e)

(er ).

That is, (e) is the difference of the flow of E through the root r, with and without the edge e.
Note that (e) can be efficiently computed by first computing uE (e) for every edge e in a
single bottom-up traversal of  , and then computing (e) for every edge e in a single top-down
traversal of  , as follows:
I

For a root edge r, (r) = 1  uE (r).
I

(1)

2(1  uE (e))
, where f (e) is the parent edge of e.
I
uE (e)
If some edge in  has more than one parent-edge then we define (e) for an edge by
using this method of computation, where in place of ( f (e)) we use max ( f (e)).

f 
I

For all other edges, (e) = ( f (e)) 

(2)

Finally, for a set, R, of edges in G, we define (R) =



e R

(e).16

Now, having computed (e) we compute the initial weight assignment to e, p(e), in the
following way. Choose some large C.17 For each e in  define:
15

Although we have defined an example as a {0, 1} truth assignment to each observable proposition,
we have already noted in Footnote 4 that we can just as easily process examples which assign to observables any value in the interval [0, 1].
16

Observe that the number of examples reclassified as a result of edge-deletion is, in fact, superadditive, a fact not reflected by this last definition.
17

We have not tested how to choose C optimally. In the experiments reported in Section 6, the value C = 106 was used.

197

KOPPEL, FELDMAN, & SEGRE

C (e)
.
C (e) + 1
Now, regardless of how (e) is defined, the virtue of this method of computing p(e) from (e) is
the following: for such an initial assignment, p, if two sets of edges   , p  are of equal total
strength then as revision sets they are of equal radicality. This means that all revision sets of
equal strength are a priori equally probable.
p(e) =

For a set of edges of  , define
1 if e  S
S(e) = 
/ S
0 if e 
Then the above can be formalized as follows:
Theorem A1: If R and S are sets of elements of  such that (R) = (S) then it
follows that Rad(R) = Rad(S).
Proof of Theorem A1: Let R and S be sets of edges such that (R) = (S).
Recall that
Rad(S) =  log



[1  p(e)]S(e)  [ p(e)]1S(e) .

e


Then
[1  p(e)]S(e)  p(e)1S(e)
exp(Rad(S))
= 
exp(Rad(R)) e   [1  p(e)] R(e)  p(e)1R(e)
=

  p(e)1  p(e)
e 

=

C (e) 


e 

R(e)S(e)

R(e)S(e)

= C (R)(S) = 1.
It follows immediately that Rad(R) = Rad(S).
A simple consequence which illustrates the intuitiveness of this theorem is the following:
suppose we have two possible revisions of , each of which entails deleting a simple literal.
Suppose further that one literal, l 1 , is deep in the tree and the other, l 2 , is higher in the tree so that
(l 2 ) = 4  (l 1 ). Then, using default initial weights as assigned above, the radicality of
deleting l 2 is 4 times as great as the radicality of deleting l 1 .

198

BIAS DRIVEN REVISION

Appendix B: Updated Weights as Conditional Probabilities
In this appendix we prove that under certain limiting conditions, the algorithm computes the
conditional probabilities of the edges given the classification of the example.
Our first assumption for the purpose of this appendix is that the correct dt-graph  is known
to be a subgraph of the given dt-graph  . This means that for every node n in  , w(n) = 1 (and,
consequently, for every edge e in  , p(e) = w(e)). A pair   , w  with this property is said to
be deletion-only.
Although we informally defined probabilities directly on edges, for the purposes of this
appendix we formally define our probability function on the space of all subgraphs of  . That is,
the elementary events are of the form  =  where    . Then the probability that e  
is simply  { p( =  )|e   }.
  

We say that a deletion-only, weighted dt-graph   , p  is edge-independent if for any
  ,
p( =  ) =



e  

p(e) 



e
/ 

1  p(e).

Finally, we say that  is tree-like if no edge e   has more than one parent-edge. Observe that
any dt-graph which is connected and tree-like has only one root.
We will prove results for deletion-only, edge-independent, tree-like weighted dt-graphs.18
First we introduce some more terminology. Recall that every node in  is labeled by one of
the literals in  and that by definition, this literal is true if not all of its children in  are true.
Recall also that the dt-graph    represents the sets of NAND equations,   . A literal l in
 forces its parent in  to be true, given the set of equations  and the example E, if l appears in
 and is false given  and E. (This follows from the definition of NAND.) Thus we say that an
edge e in  is used by E in  if e   and  | E n e .
If e is not used by E in  we write N E (e). Note that N E (er ) if and only if (E) = 1.
Note that, given the probabilities of the elementary events  =  , the probability p(N E (e))
that the edge e is not used by E in the target domain theory  is simply




 p( =  )|N E (e). Where there is no ambiguity we will use N E (e) to refer to N E (e).

   

Theorem B1: If   , w  is a deletion-only, edge-independent, tree-like weighted
dt-graph, then for every edge e in  , u E (e) = p(N E (e)).
Proof of Theorem B1: We use induction on the distance of n e from its deepest
descendant. If n e is an observable proposition P then e is used by E in 
precisely if e   and P is false in E. Thus the probability that e is not used by E
in  is [1  p(e)]  [1  E(P)] = u E (e).

18

Empirical results show that our algorithm yields reasonable approximations of the conditional probabilities even when these conditions do not hold.

199

KOPPEL, FELDMAN, & SEGRE

If n e is not a observable proposition then  | E n e precisely if all its
children in  are true in , that is, if all its children are unused in . But then
p(N E (e)) = p(e)  p( |
= p(e) 

E



p(N E (s))



u E (s)

s  children(e)

= p(e) 

(edge independence)

n e )

s  children(e)

(induction hypothesis)

= u E (e).
This justifies the bottom-up part of the algorithm. In order to justify the top-down part we need
one more definition.
Let p(e|  E, (E)  ) be the probability that e   given   , p  and the exemplar
 E, (E)  . Then
p(e|  E, (E)  ) =



  

{ p( =  )|e   , (E) = (E)}

 { p( =  )|(E) = (E)}
  

.

Now we have
Theorem B2: If   , w  is deletion-only, edge-independent and tree-like, then for
every edge e in  , p new (e) = p(e|  E, (E)  ).
In order to prove the theorem we need several lemmas:
Lemma B1: For every example E and every edge e in 
p( N E (e)) = p( N E (e), N E ( f (e))) = p( N E (e)|N E ( f (e)))  p(N E ( f (e))).
This follows immediately from the fact that if an edge, e, is used, then its parent-edge, f (e), is not
used.
Lemma B2: For every example E and every edge e in  ,
p(N E (E)|N E ( f (e)),  E, (E)  ) = p(N E (e)|N E ( f (e))).
This lemma states that N E (e) and  E, (E)  are conditionally independent given N E ( f (e))
(Pearl, 1988). That is, once N E ( f (e)) is known,  E, (E)  adds no information regarding
N E (e). This is immediate from the fact that p(  E, (E)  |N E ( f (e))) can be expressed in terms of
the probabilities associated with non-descendants of f (e), while p(N E (e)) can be expressed in
terms of the probabilities associated with descendants of r(e).
Lemma B3: For every example E and every edge e in  ,
v E (e) = p(N E (e)|  E, (E)  ).
Proof of Lemma B3: The proof is by induction on the depth of the edge, e. For
the root edge, er , we have

200

BIAS DRIVEN REVISION

v E (er ) = (E) = p((E) = 1|  E, (E)  ) = p(N E (er )|  E, (E)  ).
Assuming that the theorem is known for f (e), we show that it holds for e as
follows:
v ( f (e))
1  v E (e) = 1  u E (e) E

 u E ( f (e))
= p( N E (e)) 

(definition of v)

v E ( f (e))
p(N E ( f (e))

= p(N E (e)|  E, (E)  ) 

p( N E (e))
p(N E ( f (e))

(Theorem B1)
(induction hypothesis)

= p(N E (e)|  E, (E)  )  p( N E (e)|N E ( f (e))

(Lemma B1)

= p(N E (e)|  E, (E)  )
 p( N E (e)|N E ( f (e)),  E, (E)  )

(Lemma B2)

= p( N E (e), N E ( f (e))|  E, (E)  )

(Bayes rule)

= p( N E (e)|  E, (E)  )

(Lemma B1)

= 1  p(N E (e)|  E, (E)  ).
Let e be short for the event e 
/  . Then we have
Lemma B4: For every example E and every edge e in  ,
p( e) = p( e, N E (e)) = p( e|N E (e))  p(N E (e)).
This lemma, which is analogous to Lemma B1, follows from the fact that if e is deleted, then e is
unused.
Lemma B5: For every example E and every edge e in  ,
p( e| N E (e),  E, (E)  ) = p( e| N E (e)).
This lemma, which is analogous to Lemma B2, states that e and  E, (E)  are conditionally
independent given N E (e). That is, once N E (e) is known,  E, (E)  adds no information
regarding the probability of e. This is immediate from the fact that p(  E, (E)  | N E (e)) can
be expressed in terms of the probabilities of edges other than e.
We now have all the pieces to prove Theorem B2.
Proof of Theorem B2:
v (e)
1  p new (e) = 1  p(e) E

 u E (e)
= p( e) 

(definition of pnew)

v E (e)
p(N E (e))

(Theorem B1)

201

KOPPEL, FELDMAN, & SEGRE

= p(N E (e)|  E, (E)  ) 

p( e)
p(N E (e))

(Lemma B3)

= p(N E (e)|  E, (E)  )  p( e|N E (e))

(Lemma B4)

= p(N E (e)|  E, (E)  )  p( e|N E (e),  E, (E) 

(Lemma B5)

= p( e, N E (e)|  E, (E)  )

(Bayes rule)

= p( e|  E, (E)  )

(Lemma B4)

= 1  p(e|  E, (E)  ).

202

BIAS DRIVEN REVISION

Appendix C: Proof of Convergence
We have seen in Section 5 that PTR always terminates. We wish to show that when it does, all
exemplars are classified correctly. We will prove this for domain theories which satisfy certain
conditions which will be made precise below. The general idea of the proof is the following: by
definition, the algorithm terminates either when all exemplars are correctly classified or when all
edges have weight 1. Thus, it is only necessary to show that it is not possible to reach a state in
which all edges have weight 1 and some exemplar is misclassified. We will prove that such a
state fails to possess the property of consistency which is assumed to hold for the initial
weighted dt-graph , and which is preserved at all times by the algorithm.
Definition (Consistency): The weighted dt-graph  =  , p  is consistent with
exemplar  E, (E)  if, for every root r i in , either:
(i) i (E) = 1 and uE (r i ) > 0, or
(ii) i (E) = 0 and uE (r i ) < 1.
Recall that an edge e is defined to be even if it is of even depth along every path from a root and
odd if is of odd depth along every path from a root. A domain theory is said to be unambiguous if
every edge is either odd or even. Note that negation-free domain theories are unambiguous. We
will prove our main theorem for unambiguous, single-root domain theories.
Recall that the only operations performed by PTR are:
(1)

updating weights,

(2)

deleting even edges,

(3)

deleting odd edges,

(4)

adding a subtree beneath an even edge, and

(5)

adding a subtree beneath an odd edge.

We shall show that each of these operations is performed in such a way as to preserve
consistency.
Theorem C1 (Consistency): If  =  , p  is a single-rooted, unambiguous
weighted dt-graph which is consistent with the exemplar  E, (E)  and
 =  , p  is obtained from  via a single operation performed by PTR, then 
is also a single-rooted, unambiguous dt-graph which is consistent with E.
Before we prove this theorem we show that it easily implies convergence of the algorithm.
Theorem C2 (Convergence): Given a single-rooted, unambiguous weighted dtgraph  and a set of exemplars  such that  is consistent with every exemplar in
, PTR terminates and produces a dt-graph  which classifies every exemplar in 
correctly.
Proof of Theorem C2: If PTR terminates prior to each edge being assigned the
weight 1, then by definition, all exemplars are correctly classified. Suppose then
that PTR produces a weighted dt-graph  =  , p  such that p(e) = 1 for every
e  . Assume, contrary to the theorem, that some exemplar  E, (E)  is
misclassified by  for the root r. Without loss of generality, assume that
 E, (E)  is an IN exemplar of r. Since p(e) = 1 for every edge, this means that
u
E (er ) = 0. But this is impossible since the consistency of  implies that
u KE (er ) > 0 and thus it follows from Theorem C1 that for any  obtainable form
203

KOPPEL, FELDMAN, & SEGRE

, u
E (er ) > 0. This contradicts the assumption that E is misclassified by .
Let us now turn to the proof of Theorem C1. We will use the following four lemmas, slight
variants of which are proved in (Feldman, 1993).
Lemma C1: If  =  , p  is obtained from  =  , p  via updating of weights,
then for every edge e   such that 0 < p(e) < 1, we have 0 < p(e) < 1.19
Lemma C2: Let  =  , p  be a weighted dt-graph such that 0 < uE (er ) < 1 and
let  =  , p  . Then if for every edge e in  such that 0 < p(e) < 1, we have
0 < p(e) < 1, it follows that 0 < u
E (er ) < 1.
Lemma C3: Let  =  , p  be a weighted dt-graph such that uE (er ) > 0 and let
 =  , p  . The, if for every edge e in , it holds that either:
(i) p(e) = p(e), or
(ii) depth(e) is odd and u
E (e) > 0, or
(iii) depth(e) is even and u
E (e) < 1
then u
E (e) > 0.
An analogous lemma holds where the roles of > 0 and < 1 are reversed.




Lemma C4: If e is even edge in , then u E e (er )  uE (er )  u E e (r). In addition, if


e is an odd edge in , then u E e (er )  uE (er )  u E e (r).
We can now prove consistency (Theorem C1). We assume, without loss of generality, that
 E, (E)  is an IN exemplar of the root r and prove that for each one of the five operations
(updating and four revision operators) of PTR, that if  is obtained by that operation from  and
uE (er ) > 0, then u
E (er ) > 0.
Proof of Theorem C1: The proof consists of five separate cases, each
corresponding to one of the operations performed by PTR.
Case 1:  is obtained from  via updating of weights.
By Lemma C1, for every edge e in , if 0 < p(e) < 1 then 0 < p(e) < 1. But then
by Lemma C2, if uE (er ) > 0 then u
E (er ) > 0.
Case 2:  is obtained from  via deletion of an even edge, e.


From Lemma C4(i), we have u E e (er )  uE (er ) > 0.
Case 3:  is obtained from  via deletion of an odd edge, e.
The edge e is deleted only if it is not needed for any exemplar. Suppose that,
contrary to the theorem, there is an IN exemplar  E, (E)  such that uE (er ) > 0
but u
E (er ) = 0. Then
19

Recall that in the updating algorithm we defined


if i (E) = 0
v E (er i ) = 
.
1   if i (E) = 1
The somewhat annoying presence of  > 0 is necessary for the proof of Lemma C1.

204

BIAS DRIVEN REVISION

R(  E, (E)  , e, ) =

=



u E e (er )


u E e (er )


u E e (er )
u
E (er )


u E e (er )
> 2.
0
But then e is needed for E, contradicting the fact that e is not needed for any
exemplar.
=

Case 4:  is obtained from  via appending a subtree beneath an even edge, e.
If p(e) < 1, then the result is immediate from Lemma C2. Otherwise, let f be the
root edge of the subtree a which is appended to , beneath e. Then | f = e .
Suppose that, contrary to the theorem, there is some IN exemplar  E, (E)  such
that
uE (er ) > 0
but
u
Then
by
Lemma
C4(ii),
E (er ) = 0.
e
|e

u E (er ) = u E (er )  u E (er ) = 0. But then,
R(  E, (E)  , e, ) =




u E e (er )


u E e (er )
0

u E e (er )

= 0.

Thus e is destructive for E in . But then, by the construction of a , u
E ( f ) = 1.
(e)
=
0
<
1.
The
result
follows
immediately
from
Lemma
C3.
Thus, u
E
Case 5:  is obtained from  via appending a subtree to  beneath the odd edge,
e.
Suppose that, contrary to the theorem, some IN exemplar  E, (E)  , uE (er ) > 0
but u
E (er ) = 0. Since e = e , it follows that
R(  E, (E)  , e, ) =

=



u E e (er )


u E e (er )


u E e (er )


u E e (er )

.

Now, using Lemma C4(ii) on both numerator and denominator, we have


u E e (er )


u E e (er )

 uE (er )u
E (er ) =  > 2.

Thus, e is needed for E in . Now, let f be the root edge of the appended subtree,
a . Then, by the construction of a , it follows that u
E ( f ) < 1 and, therefore
u
(e)
>
0.
The
result
is
immediate
from
Lemma
C3.
E

205

KOPPEL, FELDMAN, & SEGRE

This completes the proof of the theorem.
It is instructive to note why the proof of Theorem C1 fails if  is not restricted to
unambiguous single-rooted dt-graphs. In case 4 of the proof of Theorem C1, we use the fact that
if an edge e is destructive for an exemplar  E, (E)  then the revision algorithm used to
construct the subgraph, a , appended to e will be such that u
E ( f ) = 1. However, this fact does
not hold in the case where e is simultaneously needed and destructive. This can occur if e is a
descendant of two roots where E is IN for one root and OUT for another root. It can also occur
when one path from e to the root r is of even length and another path is of odd length.

206

BIAS DRIVEN REVISION

Appendix D: Guide to Notation


A domain theory consisting of a set of clauses of the form C i : H i  Bi .

Ci

A clause label.

Hi

A clause head; it consists of a single positive literal.

Bi

A clause body; it consists of a conjunction of positive or negative literals.

E

An example; it is a set of observable propositions.

i (E)

The classification of the example E for the ith root according to domain
theory .

i (E)

The correct classification of the example E for the ith root.

 E, (E) 

An exemplar, a classified example.



The set of NAND clauses equivalent to .



The dt-graph representation of .

ne

The node to which the edge e leads.

n

e

The node from which the edge e comes.

p(e)

The weight of the edge e; it represents the probability that the edge e
needs to be deleted or that edges need to be appended to the node n e .

 =  , p 

A weighted dt-graph.

e

Same as  but with the weight of the edge e equal to 1.

e

Same as  but with the edge e deleted.

u E (e)

The flow of proof from the example E through the edge e.

v E (e)

The adjusted flow of proof through e taking into account the correct
classification of the example E.

Ri (  E, (E)  , e, )

The extent (ranging from 0 to ) to which the edge e in the weighted dtgraph  contributes to the correct classification of the example E for the
ith root. If Ri is less/more than 1, then e is harmful/helpful; if Ri = 1 then
e is irrelevant.



The revision threshold; if p(e) <  then e is revised.



The weight assigned to a revised edge and to the root of an appended
component.



The revision threshold increment.



The revised edge weight increment.

Rad  ()

The radicality of the changes required to  in order to obtain a revised
theory .

207

KOPPEL, FELDMAN, & SEGRE

References
Buchanan, B. & Shortliffe, E.H. (1984). Rule-Based Expert Systems: The MYCIN Experiments of
the Stanford Heuristic Programming Project. Reading, MA: Addison Wesley.
Feldman, R. (1993). Probabilistic Revision of Logical Domain Theories. Ithaca, NY: Ph.D.
Thesis, Department of Computer Science, Cornell University.
Feldman, R., Koppel, M. & Segre, A.M. (August 1993). The Relevance of Bias in the Revision
of Approximate Domain Theories. Working Notes of the 1993 IJCAI Workshop on Machine
Learning and Knowledge Acquisition: Common Issues, Contrasting Methods, and Integrated
Approaches, 44-60.
Ginsberg, A. (July 1990). Theory Reduction, Theory Revision, and Retranslation. Proceedings
of the National Conference on Artificial Intelligence, 777-782.
Koppel, M., Feldman, R. & Segre, A.M. (December 1993). Theory Revision Using Noisy
Exemplars. Proceedings of the Tenth Israeli Symposium on Artificial Intelligence and Computer
Vision, 96-107.
Mahoney, J. & Mooney, R. (1993). Combining Connectionist and Symbolic Learning to Refine
Certainty-Factor Rule-Bases. Connection Science, 5, 339-364.
Murphy, P.M. & Aha, D.W. (1992). UCI Repository of Machine Learning Databases [Machinereadable data repository]. Irvine, CA: Department of Information and Computer Science,
University of California at Irvine.
Ourston, D. (August 1991). Using Explanation-Based and Empirical Methods in Theory
Revision. Austin, TX: Ph.D. Thesis, University of Texas at Austin.
Ourston, D. & Mooney, R. (in press). Theory Refinement Combining Analytical and Empirical
Methods. Artificial Intelligence.
Pazzani, M. & Brunk, C. (June 1991). Detecting and Correcting Errors in Rule-Based Expert
Systems: An Integration of Empirical and Explanation-Based Learning. Knowledge Acquisition,
3(2), 157-173.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. San Mateo, CA: Morgan
Kaufmann.
Quinlan, J.R. (1986). Induction of Decision Trees. Machine Learning, 1(1), 81-106.
Towell, G.G. & Shavlik, J.W. (October 1993). Extracting Refined Rules From Knowledge-Based
Neural Networks. Machine Learning, 13(1), 71-102.
Wilkins, D.C. (July 1988). Knowledge Base Refinement Using Apprenticeship Learning
Techniques. Proceedings of the National Conference on Artificial Intelligence, 646-653.
Wogulis, J. & Pazzani, M.J. (August 1993). A Methodology for Evaluating Theory Revision
Systems: Results with Audrey II. Proceedings of the Thirteenth International Joint Conference
on Artificial Intelligence, 1128-1134.

208

Journal of Artificial Intelligence Research 1 (1993) 6189

Submitted 8/93; published 11/93

Software Agents: Completing Patterns and
Constructing User Interfaces
Jeffrey C. Schlimmer
Leonard A. Hermens
School of Electrical Engineering & Computer Science,
Washington State University, Pullman, WA 99164-2752, U.S.A.

SCHLIMMER@EECS.WSU.EDU
LHERMENS@EECS.WSU.EDU

Abstract
To support the goal of allowing users to record and retrieve information, this paper
describes an interactive note-taking system for pen-based computers with two distinctive
features. First, it actively predicts what the user is going to write. Second, it automatically
constructs a custom, button-box user interface on request. The system is an example of a
learning-apprentice software-agent. A machine learning component characterizes the
syntax and semantics of the users information. A performance system uses this learned
information to generate completion strings and construct a user interface.

1. Introduction and Motivation
People like to record information for later consultation. For many, the media of choice is
paper. It is easy to use, inexpensive, and durable. To its disadvantage, paper records do not
scale well. As the amount of information grows, retrieval becomes inefficient, physical storage becomes excessive, and duplication and distribution become expensive. Digital media
offers better scaling capabilities. With indexing and sub-linear algorithms, retrieval is efficient; using high density devices, storage space is minimal; and with electronic storage and
high-speed networks, duplication and distribution is fast and inexpensive. It is clear that our
computing environments are evolving as several vendors are beginning to market inexpensive, hand-held, highly portable computers that can convert handwriting into text. We view
this as the start of a new paradigm shift in how traditional digital information will be gathered and used. One obvious change is that these computers embrace the paper metaphor,
eliminating the need for typing. It is in this paradigm that our research is inspired, and one of
our primary goals is to combine the best of both worlds by making digital media as convenient as paper.
This document describes an interactive note-taking software system for computers with
pen-based input devices. Our software has two distinctive features: first, it actively predicts
what the user is going to write and provides a default that the user may select; second, the
software automatically constructs a graphical interface at the users request. The purpose of
these features is to speed up information entry and reduce user errors. Viewed in a larger
context, the interactive note-taking system is a type of self-customizing software.
To clarify this notion, consider a pair of dimensions for characterizing software. As
Figure 1 depicts, one dimension is task specificity. Software that addresses a generic task
(e.g., a spreadsheet) lies between task independent software (e.g., a compiler) and task
specific software (e.g., a particular companys accounting software). Another dimension is
the amount of user customization required to make the software useful. Task generic software lies between the two extremes, requiring modest programming in a specialized
 1993 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

S CHLIMMER & H ERMENS

High

Visual BASIC

User
Customization
Required

Spreadsheets

Self-Customizing

Custom Software

Low
Generic

Low

Task Specificity of Product

Development Cost / User

Specific

High

Figure 1: Continuum of software development depicting the traditional trade-off
between the development cost per user and the amount of user customization required.
Self-customizing software eliminates the need for user customization by starting with
partially-specified software and applying machine learning methods to complete any
remaining customization.
language. Self-customizing software uses machine learning techniques to automatically customize task generic software to a specific user. Because the software learns to assist the user
by watching them complete tasks, the software is also a learning apprentice. Similarly,
because the user does not explicitly program the defaults or the user interface for the note
taking system, it is a type of software agent. Agents are a new user interface paradigm that
free the user from having to explicitly command the computer. The user can record information directly and in a free-form manner. Behind the interface, the software is acting on behalf
of the user, helping to capture and organize the information.
Next we will introduce the performance component of the note-taking software in more
detail, then describe the representations and algorithms used by the learning methods. We
also present empirical results, comparing the performance of seven alternate methods on
nine realistic note-taking domains, and finally, we describe related research and identify
some of the systems limitations.

62

S OFTWARE A GENTS : C OMPLETING P ATTERNS & C ONSTRUCTING U SER I NTERFACES

Figure 2: Screen snapshot of the note-taking software in contextual prompting mode
for a PowerBook note. The two triangles in the lower left are scroller buttons.

2. Performance Task
The primary function of the note-taking software is to improve the users speed and accuracy
as they enter notes about various domains of interest. A note is a short sequence of descriptive terms that describe a single object of interest. Example 1 shows a note describing a particular personal computer (recorded by the first author from a Usenet newsgroup during
1992):
4096K PowerBook 170, 1.4MB and 40MB Int. Drives, 2400/9600 Baud FAX Modem

(Example 1)
Example 2 is a note describing a fabric pattern (recorded by the first authors wife):
Butterick 3611 Size 10 dress, top

(Example 2)

Tables 5 through 11 later in the paper list sample notes drawn from seven other domains.
The user may enter notes from different domains at their convenience and may use whatever
syntactic style comes naturally.
From the users point of view, the software operates in one of two modes: a contextual
prompting mode, and an interactive graphical interface mode. In the first mode, the software
continuously predicts a likely completion as the user writes out a note. It offers this as a
default for the user. The location and presentation of this default must balance conflicting
requirements to be convenient yet unobtrusive. For example, the hand should not hide the
indicated default while the user is writing. Our solution is to have a small, colored completion button follow to the left and below where the user is writing. In this location, it is visible
to either right- or left-handed people as they write out notes. The user can reposition the button to another location if they prefer. The default text is displayed to the immediate right of
this button in a smaller font. The completion button is green; the text is black. The completion button saturation ranges from 1 (appearing green), when the software is highly confident
of the predicted value, to 0 (appearing white), when the software lacks confidence. The button has a light gray frame, so it is visible even when the software has no prediction. Figure 2
63

S CHLIMMER & H ERMENS

Figure 3: Screen snapshot of the note-taking software in button-box mode for a
PowerBook note.
portrays a screen snapshot of the software operating in the contextual prompting mode for a
PowerBook note.
The softwares second mode presents an interactive graphical interface. Instead of
requiring the user to write out the text of a note, the software presents a radio-button and
check-box interface (what we call a button-box interface). With this, the user may select
from text fragments, portions of notes called descriptive terms , by tapping on radio-buttons
or check-boxes with the pen interface device. Each selection from the button-box interface is
added to the current note. Intuitively, check boxes are generated to depict optional descriptive terms, whereas radio-button panels are generated to depict alternate, exclusive descriptive terms. For user convenience, the radio-buttons are clustered into panels and are sorted
alphabetically in ascending order from top to bottom. To allow the user to add new descriptive terms to a button-box panel, an additional blank button is included at the bottom of each.
When the user selects a radio button item, the graphical interface is expanded to depict additional choices corresponding to descriptive terms that follow syntactically. The software
indicates its predictions by preselecting the corresponding buttons and highlighting them in
green. The user may easily override the default selection by tapping the desired button.
Figure 3 portrays a screen snapshot of the software operating in the interactive graphical
interface mode for a PowerBook note.
The software is in prompting mode when a user begins to write a note. If the learned syntax for the domain of the note is sufficiently mature (see Section 6, Constructing a ButtonBox Interface), then the software can switch into the button-box mode. To indicate this to the
user, a mode switch depicted as a radio button is presented for the users notice. A convenient and unobtrusive location for this switch is just below the completion button. In keeping
with the color theme, the mode switch also has a green hue. If the user taps this switch, the
written text is removed, and the appropriate radio buttons and check boxes are inserted. The
system automatically selects buttons that match the user-written text. As the user makes
additional selections, the interface expands to include additional buttons. When the user
finishes a note, in either mode, the software returns to prompting mode in anticipation of
another note.1 Because the interface is constructed from a learned syntax, as the software

64

S OFTWARE A GENTS : C OMPLETING P ATTERNS & C ONSTRUCTING U SER I NTERFACES

refines its representation of the domains of the notes, the button-box interface also improves.
On-line Appendix 1 is a demonstration of the systems operation in each of its two modes.

3. Learning a Syntax
To implement the two modes of the note taking software, the system internally learns two
structures. To characterize the syntax of users notes, it learns finite-state machines (FSMs).
To generate predictions, it learns decision tree classifiers situated at states within the FSMs.
In order to construct a graphical user interface, the system converts a FSM into a set of
buttons. This section describes the representation and method for learning FSMs. The next
section discusses learning of the embedded classifiers.
3.1

Tokenization

Prior to learning a finite-state machine, the users note must first be converted into a
sequence of tokens. Useful tokenizers can be domain independent. However, handcrafted
domain-specific tokenizers lead to more useful representations. The generic tokenizer used
for the results reported here uses normal punctuation, whitespace, and alpha-numeric character boundaries as token delimiters. For example, our generic tokenizer splits the sample
PowerBook note in Example 1 into the following 16 tokens:
:NULL
"4096"
" K"
" PowerBook"
" 170"
", 1.4"
"MB"
" and"
" 40"
"MB"
" Int."
" Drives"
", 2400/9600"
" Baud"
" FAX"
" Modem" .

The token :NULL is prepended by the tokenizer. This convention simplifies the code for
constructing a FSM.
3.2

Learning a Finite-State Machine

Deterministic finite-state machines (FSMs) are one candidate approach for describing the
syntax of a users notes because they are well understood and relatively expressive. Moreover, Angluin (1982) and Berwick and Pilato (1987) present a straightforward algorithm for
learning a specific subclass of FSMs called k-reversible FSMs. The algorithm is incremental
1. Of the functionality described here, our prototype implements all but the transition from button-box to contextual prompting. The mechanism for such a transition is machine dependent and is not germane to this research.

65

S CHLIMMER & H ERMENS

start

start

:NULL

:NULL

:NULL

Butterick

Butterick

Butterick

3035

3035

3611

Size

Size

Size

11/12

11/12

10

dress

dress

dress

terminal

terminal
top

terminal
(a)

(b)

Figure 4: (a) Degenerate finite-state machine after processing a single fabric pattern
note, and (b) prefix tree finite-state machine after adding a second fabric pattern note
(cf. Example 2).
and does not suffer from presentation order effects. Berwick and Pilato define a k-reversible
FSM as:
A regular language is k-reversible, where k is a non-negative integer, if whenever two
prefixes whose last k words [tokens] match have a tail in common, then the two prefixes
have all tails in common. In other words, a deterministic finite-state automaton (DFA)
[FSM] is k-reversible if it is deterministic with lookahead k when its sets of initial and
final states are swapped and all of its arcs [transitions] are reversed.

Given a list of tokens, the k-reversible FSM algorithm first constructs a prefix tree, where
all token sequences with common k-leaders share a k-length path through the FSM. For
example, Figure 4a depicts a simple FSM constructed for a single fabric pattern note. The
text of the users note was converted into a sequence of tokens. Then a transition was created
for each token and a sequence of states was created to link them together. One state serves as
the initial state, and another indicates the completion of the sequence. For convenience, this
latter, terminal state is depicted with a double circle. If the FSM is able to find a transition
for each token in the sequence, and it arrives at the terminal state, then the FSM accepts the
token sequence as an instance of the language it defines. Figure 4b depicts the same FSM
after another path has been added corresponding to a second fabric pattern note (Example 2).
Now the FSM will accept either note if expressed as a sequence of tokens. This FSM is a
trivial prefix tree because only the first state is shared between the two paths.
66

S OFTWARE A GENTS : C OMPLETING P ATTERNS & C ONSTRUCTING U SER I NTERFACES

A k-leader is defined as a path of length k that accepts in the given state.
Merge any two states if either of the following is true:
1. Another state transitions to both states on the same token; or
(This enforces determinism.)
2. Both states have a common k-leader and
a. Both states are accepting states, or
b. Both states transition to a common state via the same token.
Table 1: FSM state merging rules from (Angluin, 1982).
A prefix tree is minimal for observed token sequences, but it may not be general enough
for use in prediction. (The prefix tree is, in essence, an expensive method for memorizing
token sequenceswhich is not the desired result.) For the sake of prediction, it is desirable
to have a FSM that can accept new, previously unseen combinations of tokens. The prefix
tree automaton can be converted into a more general FSM by merging some of its states. A
particular method for doing this converts a prefix tree into a k-reversible FSM via Angluins
(1982) algorithm. The algorithm merges states that have similar transitions, and it creates a
FSM that accepts all token sequences in the prefix tree, as well as other candidate sequences.
Table 1 lists the three rules for deciding when to merge a pair of states in a prefix tree to form
a k-reversible FSM. In the special case where k equals zero, all states have a common kleader, and Rule 2a ensures that there will be only one accepting state.
Because the rules in Table 1 must be applied to each pair of states in the FSM, and
because each time a pair of states is merged the process must be repeated, the asymptotic
complexity of the process is O( n3 ), where n is the number of states in the FSM.
Applying these rules to the prefix tree in Figure 4b with k equal to zero results in a FSM
depicted in Figure 5a. Notice that the first two states have been merged to make the FSM
deterministic (Rule 1). The accepting states have also been merged in compliance with
Rule 2a. The resulting FSM has fewer states but is not more general. It only accepts the two
token sequences originally seen. Extending this example, Figure 5b illustrates the addition of
a third fabric pattern note as a prefix tree path to the FSM. Reapplying the rules results in the
FSM shown in Figure 6. The first two states have been merged as before through the action
of the determinism Rule 1. Note that a pair of latter states have also been merged because
they share a common zero-leader (true of all pairs of states) and because they transition to
the common terminal state on the token "dress".
Figure 7 depicts a more sophisticated result; it shows a learned zero-reversible FSM for
notes about PowerBook computers. This example shows that the model number "100" is
never followed by a specification for an internal floppy drive, but that other model numbers
are. Any model may have an external floppy drive. Note that there is a single terminal state.
Whitespace and punctuation have been eliminated for clarity in the figure.
The rules listed in Table 1 are generalization operators that allow the FSM to accept
previously unobserved sequences. Whenever two or more states are merged into one, the
FSM will accept more sequences than before if the new state is at the tail end of more transitions than one of the previous states and if the new state is at the head end of at least one
transition. For example, the state just after State 1 in Figure 7 was merged from several
previous states and generalizes memory sizes for PowerBook models. These rules comprise a
heuristic bias and may be too conservative. For example, Figure 8 depicts a FSM for notes
67

S CHLIMMER & H ERMENS

start

start

:NULL

:NULL

Butterick

:NULL

Butterick

Butterick

3035

3611

3035

3611

3674

Size

Size

Size

Size

Size

11/12

10

11/12

10

10

dress

dress

dress

dress

dress

terminal
top

top

terminal

terminal

(a)

(b)

Figure 5: (a) Finite-state machine after processing two fabric pattern notes and
applying state merging rules in Table 1, and (b) prefix tree finite-state machine after
adding a third fabric pattern note.
about fabric patterns. Many of the states prior to the accepting state could be usefully
merged, but using only the rules listed in Table 1, many more notes will have to be processed
before this happens. If the FSM in Figure 8 were rendered as a button-box interface, it would
reflect little of the true structure of the domain of fabric patterns. Table 2 lists specializations
of Rules 2a and 2b and an additional pair of rules we developed to make the FSM generalize
more readily. Note that the parameter k has been set to zero in Rule 2 and to one in Rule 3.
Effectively, two states are merged by Rules 3a or 2b' if they share an incoming or outgoing
transition. Rule 3b is a Kleene rule that encourages the FSM to generalize the number of
times a token may appear in a sequence. If one state has a transition to another, then merging
them will result in a transition that loops from and to the newly merged state. Figure 9
depicts a FSM for notes about fabric patterns learned using all three generalization rules in
Table 2. The resulting FSM accurately captures the syntax of the users fabric pattern notes
and correctly indicates the syntactically optional tokens that may appear at the end of note.
When rendered as a button-box interface, it clearly depicts the users syntax (as illustrated
later by Figure 12). The added generalization rules may have only marginal effects on the
systems ability to accurately predict a completion as the user writes out a note (as Table 14
below indicates). Their purpose is to improve the quality of the custom interface.
Cohen (1988) uses an interesting alternative representation for learning a syntactic form.
The goal in his work is to guide the generation of proof structures. Intuitively, the representation is a finite-state machine that accepts a tree rather than a sequence, and for this reason it
is termed a tree automaton. Like the rules in Tables 1 and 2, tree automatons are generalized
68

S OFTWARE A GENTS : C OMPLETING P ATTERNS & C ONSTRUCTING U SER I NTERFACES

start
:NULL

Butterick

3035

3674

3611

Size

Size

Size

11/12

10

10

dress

dress

top

terminal

Figure 6: Sample finite-state machine after processing three fabric pattern notes.

Merge any two states if any of the following are true:
1. Another state transitions to both states on the same token; or
(This enforces determinism.)
2'. Both states have a common 0-leader and
a. Both states are accepting states, or
b. Both states transition to a common state via the same token; or
3. Both states have a common 1-leader and
a. Both states transition to a common state via any token, or
b. One transitions to the other via any token.
Table 2: Extended FSM state merging rules.
by merging states that share similar transitions. Oddly enough, one motivation for using tree
automatons is that they are less likely to introduce extraneous loops, the opposite of the
problem with the original FSM merging rules in Table 1. It is not clear how to map the
sequence of tokens in the users notes into a tree structure, but the less sequential nature of
the tree automaton may help alleviate sequencing problems in rendering the custom user
interface (see Section 9, Observations/Limitations).
3.3

Parsing

To use the finite-state machine for prediction, the software needs a strategy for dealing with
novel tokens. For example, when the user takes a note about a PowerBook computer with a
69

S CHLIMMER & H ERMENS

start
:NULL
1
2048

4096

6144

8192

K

PowerBook
2
100

140

145

160

170

3
80

1.4

MB

MB

Int

and
4

and

20

40

20

1.4

MB

MB

Int

40

80

120

5
Ext

Drive

Drives

and

6

terminal

Drives
2xBattery,
Battery,
Case,
Charger,
FPU,
Video Output

14.4

v

1.4

K

32

MB

9.6

9600

2400/4800

2400/9600

4800/9600

Ext

K

Baud

bis

7
FAX

Modem

Figure 7: Zero-reversible FSM characterizing PowerBook notes (cf. Example 1).
70

S OFTWARE A GENTS : C OMPLETING P ATTERNS & C ONSTRUCTING U SER I NTERFACES

start
:NULL

Butterick

McCall's

4198

3722

4352

6171

3674

3035

3611

Size

Sizes

Size

Size

Size

Size

12

8-10-12

12

10

10

11/12

Dress

Jumper

Top

Dress

4864

5057

5377

Suze

Size

10

12

Dress

Dress

Skirt

Simplicity

5906

5424

5465

Size

Size

Size

12

11/12

11/12

Jumper

Dress

Jumper

Skirt

Top

terminal

Figure 8: Zero-reversible finite-state machine characterizing fabric pattern notes
learned using merging rules listed in Table 1.
new memory configuration, the FSM will not have a transition for the first token. If the software is to prompt the user, then it must have a means for deciding where novel tokens lie in
a notes syntaxwhich state to predict from. Without such a mechanism, no meaningful
prediction can be generated after novel tokens.
A state may not have a transition for the next token. In general, this is a single symptom
with three possible causes: (1) a novel token has been inserted, (2) a suitable token has been
omitted and the next token would be accepted by a subsequent state, or (3) a token has been
simply replaced by another in the syntax. For example, in the sequence of tokens {:NULL,
"12288", "K", "PB"}, "12288" is a novel token, a familiar memory size has been omitted, and
"PowerBook" has been replaced by "PB".
An optimal solution would identify the state requiring a minimum number of insertions,
omissions, and replacements necessary to parse the new sequence. An efficient, heuristic
approximation does a greedy search using a special marker. Each time the marked state in the
FSM has a transition for the next token written by the user, the marker is moved forward, and
a prediction is generated from that state. When there is no transition for the next token, a
greedy search is conducted for some state (including the marked one and those reachable
from it) that has a transition for some token (including the next one and those following). If
such a state is found, the marker is moved forward to that state, tokens for the transitions of
skipped states are assumed omitted, and novel tokens are assumed inserted. If no state past
the marker has a transition for any of the remaining tokens, the remaining tokens are
assumed to be replacements for the same number of the most likely transitions; the marker is
not moved. If the user writes a subsequent token for which some state has a transition, the
71

S CHLIMMER & H ERMENS

start
:NULL

Butterick

3722

4198

4352

6171

3674

McCall's

3035

Sizes

3611

4864

5057

5377

Simplicity

5906

5424

5465

Size

8-10-12

10

11/12

Dress

12

Jumper

terminal
Jumper

Skirt

Top

Figure 9: Finite-state machine characterizing fabric pattern notes learned using
extended rules in Table 2. Compare to zero-reversible finite-state machine for the
same domain in Figure 8.
marker is moved as described above, and the syntax of the users note is realigned with the
learned syntax. Continuing with the simple PowerBook example, the marker is moved to
State 1 of the FSM in Figure 7 because the initial state had a transition for the first token
:NULL. Because State 1 doesnt have a transition for the next token "12288", a greedy search
is conducted to find a nearby state that accepts either "12288", "K", or "PB". The state just
before State 2 accepts "K", so the marker is moved to that state. Another greedy search is
started to find a state that accepts "PB". Because one cannot be found, the heuristic parsing
assumes that it should skip to the next transition. In this case the one labeled "PowerBook".
Consequently, the system generates a prediction from State 2 to prompt the user.
3.4

Multiple Finite-State Machines

If the user decides to take notes about multiple domains, it may be necessary to learn a separate syntax for each domain. For example, a single syntax generalized over both the PowerBook and fabric pattern notes is likely to yield confusing predictions and an unnatural user
interface. Maintenance of multiple finite-state machines is an instance of the clustering problemdeciding which notes should be clustered together to share a FSM. As Fisher (1987)
discusses, this involves a trade-off between maximizing similarity within a cluster and minimizing similarity between clusters. Without the first criteria, all notes would be put into a
single cluster. Without the second criteria, each note would be put into its own cluster.
One obvious approach would be to require the user to prepend each note with a unique
token to identify each notes domain. This simplifies the clustering computation. All notes
sharing the first token would share a FSM. However, with this scheme, the user would have
72

S OFTWARE A GENTS : C OMPLETING P ATTERNS & C ONSTRUCTING U SER I NTERFACES

start
terminal

X

Figure 10: Simple finite-state machine with one state.
to remember the identifying token or name for each domain. An interface could provide a
pop-up list of all previously used domain identifiers. This is not satisfactory because it
requires overhead not needed when taking notes on paper.
An alternative approach doesnt require any extra effort on the part of the user. A new
note is grouped with the FSM that skips the fewest of its tokens. This heuristic encourages
within cluster similarity because a FSM will accept new token sequences similar to those it
summarizes. To inhibit the formation of single-note FSMs, a new FSM is constructed only if
all other FSMs skip more than half of the new notes tokens. This is a parametrized solution
to encourage between-cluster dissimilarity.

4. Learning Embedded Classifiers
Finite-state machines are useful representations for capturing the syntax of a users notes,
and they are easy to learn. When predicting a notes completion, it is essential that a prediction be made from the correct state in the FSM (as discussed above). It is also necessary to
decide whether to terminate (indicating acceptance of the note) or continue prediction, and,
in the later case, which transition to predict. To facilitate these decisions, the FSM can maintain a count of how many times parsing terminated and how many times each transition was
taken. Prediction can then return the option with the maximum frequency.
Figure 10 depicts a FSM for which this method will prove insufficient. There is only one
state, an accepting state, and the transition corresponding to the token "X" is optional. (This
corresponds to a check box interface item.) There are two problems with a frequency-based
prediction. First, the FSM does not indicate that the transition is to be taken at most once, yet
this is quite clear from the user interface. Second, simple frequency-based prediction would
always recommend termination and never the transition. The FSM accepts whether the box is
checked or not, thus the frequency of termination is greater than or equal to the frequency of
the transition. This problem arises whenever there is a loop.
Embedding general classifiers in a FSM can alleviate some of the FSMs representational
shortcomings. For example, in the FSM depicted in Figure 10, a decision tree embedded in
this state easily tests whether the transition has already been taken and can advise against
repeating it. Moreover, a classifier can predict based on previous transitions rather than just
the frequency of the current states transitions. Therefore, a decision tree embedded in the
state of Figure 10 can predict when the transition should be taken as a function of other,
earlier tokens in the sequence. Table 3 lists sample decision trees embedded in states of the
FSM depicted in Figure 7. The first tree tests which token was parsed by a distant state, in
effect augmenting the FSM representation. It relates memory size to hard disk capacity
(small amounts of memory correlate with a small hard disk). The second tree prevents an
optional loop from being taken a second time by testing to see if the state has yet been
visited during a parse of the note. After processing additional notes, this second decision tree
73

S CHLIMMER & H ERMENS

Decision tree embedded in State 3:
If State 1 exited with "2048"
Then predict " 20"
Else if with "4096"
Then predict " 40"
Else if with "6144"
Then predict " 40"
Else if with "8192"
Then predict " 40" .

Decision tree embedded in State 7:
If State 7 has not been visited
Then predict " FAX"
Else if State 7 exited with " FAX"
Then predict " Modem" .

Table 3: Sample decision trees embedded in the finite-state machine depicted in
Figure 7.
becomes more complex as the system tries to predict which PowerBooks have FAX modems
and which do not.
A classifier is trained for each state in the FSM which: (a) has more than one transition,
or (b) is marked as a terminal state but also has a transition. The classifiers are updated incrementally after the user finishes each note. The classifiers training data are token sequences
parsed at this state. The class value of the data is the transition taken from, or termination at,
this state by the token sequences. Only those classifiers whose states are used in a parse are
updated. The attributes of the data are the names of states prior to this one, and the values of
the attributes are the transitions taken from those states. A distinct attribute is defined each
time a state is visited during a given parse, so when a loop transition is taken a specific
attribute reflects this fact. For any of the attributes, if the corresponding state was not visited
while parsing the token sequence, the attribute has a special, empty value.
Consider the PowerBook FSM shown in Figure 7. A classifier would be embedded at
States 1, 2, 3, 4, 5, 6, 7. A training example corresponding to the note in Example 1 for the
classifier at State 6 would be:
Attributes:
S1
=
S2
=
S3
=
S4
=
S5
=
S6
=
S7
=
S7-1
=
Class:
=

Values:
"4096"
" 170"
NIL
" 40"
" Drives"
", 2400/9600"
" FAX"
" Modem"
:TERMINATE .

Note that there is no value for State 3, denoting that it wasnt visited during the parse of
Example 1. Also there are two attributes for State 7 denoting that it has been visited twice.
The classifier gives informed advice about which transition to take or whether to terminate. The FSM in turn gives the classifier a specific context for operation. If only a single
classifier were used to predict the next token, it would be hard pressed to represent the different predictions required. The domain is naturally narrowed by the FSM and therefore
reduces the representational demands on the classifier. Later, we present empirical results
74

S OFTWARE A GENTS : C OMPLETING P ATTERNS & C ONSTRUCTING U SER I NTERFACES

comparing a single classifier to a set of classifiers embedded in a FSM. The findings there
show that the latter outperforms the former, confirming the intuition that learning is more
effective if situated within a narrow context.
From the classifiers point of view, the learning task is non-stationary. The concept to be
learned is changing over time because the structure of the FSM is changing. When two states
are merged, one of the two classifiers is discarded. The other is now embedded in a different
position in the FSM, and it sees different training data. Similarly, when other states are
merged, the attributes of the training data also change. To help mitigate this effect, the new
state takes the oldest identifier assigned to the two merged states. Empirical results in
Table 14 illustrate that the FSM does not have to be fixed before the classifier can learn
useful information.

5. Contextual Prompting
In the prompting mode, the software continuously predicts a likely completion as the user
writes out a note. It presents this as a default next to the completion button. The buttons
saturation ranges from white to green in proportion to the confidence of the prediction. If the
user taps the completion button, the prompt text is inserted at the end of the current note.
A completion is generated by parsing the tokens already written by the user, finding the
last state visited in the FSM, and predicting the next most likely transition (or termination).
This process is repeated until a stopping criterion is satisfied, which is discussed below. If
the last token written by the user is incomplete, matching only a prefix of a states transition,
then the remainder of that transition is predicted. If the last token matches more than one
transition, a generalized string is predicted using special characters to indicate the type and
number of characters expected. If a digit is expected, a "#" is included; if a letter, an "a" is
included; if either are possible, a "?" is included; and if some transitions tokens are longer
than others, a "" is appended to the end. For example, if the user has written
"4096K PowerBook 1", the possible values for PowerBook models of "100", "140",
"160C", and "170" are generalized, and the prompt is "#0".
A simple calculation is used to compute the confidence of the prediction and set the
buttons color saturation. It is the simple ratio
f ( prediction )
f ( total )  ( 1 + skipped )

where f ( prediction ) is the frequency of the predicted arc (or terminate) [i.e., the number of
times this choice was taken while parsing previously observed notes], f ( total ) is the total
frequency of all arcs (and terminate), and skipped is the number of tokens skipped during
heuristic parsing (cf. Section 3.3, Parsing). Confidence is directly proportional to the simple
likelihood of the prediction and is degraded in proportion to the number of tokens the FSM
had to skip to get to this point. This information is used in a simple way, so it is unclear if
more sophisticated measures are needed.
The stopping criterion is used to determine how much of a prompt to offer the user. At
one extreme, only a single token can be predicted. This gives the user little context and may
not provide much assistance. At the other extreme, a sequence of tokens that completes the
note can be predicted. This may be too lengthy, and the user would have to edit the prompt if
selected. The stopping criterion in Table 4 balances these two extremes and attempts to limit
prompts to a consistent set of tokens. In particular, Condition 3 stops expanding the prompt
75

S CHLIMMER & H ERMENS

Stop expanding the prompt if any of the following are true:
1. The next prediction is to terminate; or
2. The next prediction is a generalized string; or
3. At least one token has already been predicted and
a. The prediction starts with punctuation, or
b. The confidence of the prediction is lower; or
4. The next prediction is the same as the last prediction; or
5. More than 10 tokens have already been predicted.
Table 4: Stopping criterion for contextual prompting.
upon reaching a syntactic boundary (leading punctuation) or upon reaching a semantic
boundary (falling confidence).

6. Constructing a Button-Box Interface
In the button-box mode, the software presents an interactive graphical interface. Instead of
writing out the note, the user may select note fragments by tapping buttons. To switch from
contextual mode to button-box mode, a green radio button indicator is displayed below the
completion button when the software is confident about the users syntax. If the user taps this
indicator, the existing text is removed, and the corresponding buttons in the button-box interface are selected. As the user selects additional buttons, the interface dynamically expands to
reveal additional choices. Because the interface reflects an improving syntactic representation, it also improves with successive notes.
The button-box interface is a direct presentation of a finite-state machine. After the user
has written out a token or so of the note, the software finds the FSM that best parses these
tokens. The mode switch is presented if the syntax is sufficiently matureif the average
number of times each state has been used to parse earlier notes is greater than 2. If the user
selects this indicator, the FSM is incrementally rendered as a set of radio buttons and check
boxes.
The two user interface item types correspond to optional choices (check boxes) and
exclusive choices (radio buttons). Mapping a FSM into these two item types proceeds one
state at a time. Given a particular state to be rendered, any transition that starts a path that
does not branch and eventually returns back to the state is rendered as a check box (a loop).
The loop corresponds to syntactically optional information. The label for the check box consists of each of the transition labels along the looping path. Other non-looping transitions are
rendered as buttons in a single radio button panel along with an extra, unlabeled button. They
correspond to syntactically exclusive information. The label for each radio button consists of
each transition label up to the point of a subsequent branch or termination. For example,
compare the FSM depicted in Figure 7 and the corresponding button-box interface in
Figure 3.
Because the transitions for different radio buttons lead to different parts of the FSM, it
may confuse the user to render the entire FSM at once. So, each branching state is rendered
as it is visited. Initially, the first state in the FSM is rendered. Then, when a radio button is
selected, the branching state at the end of its transition path is rendered. Note that check
boxes do not trigger additional rendering because the branching state at the end of their loop

76

S OFTWARE A GENTS : C OMPLETING P ATTERNS & C ONSTRUCTING U SER I NTERFACES

has already been rendered. This interactive process is repeated as long as the user selects
radio buttons that lead to branching states.

7. Empirical Results
We tested the interactive note taking software on notes drawn from a variety of domains.
Tables 5 through 11 list sample notes from seven domains (in addition to the PowerBook and
fabric pattern sample notes listed above).
CVA-62 8/6/63 to 3/4/64 Mediterranean A-5A AG 60X
CVA-61 8/5/64 to 5/6/65 Vietnam RA-5C NG 10X

Table 5: Sample notes from the airwing domain. Listed above are 2 of the 78 notes
about airwing assignments aboard aircraft carriers collected from (Grove & Miller,
1989).

B, 81, 5, 151 (2.5), Cyl. 4, 2-bbl., Pontiac
C, 82, X, 173 (2.8), Cyl. 6, 2-bbl., Chevrolet

Table 6: Sample notes from the engine code domain. Listed above are 2 of the 20 notes
about the meaning of engine codes stamped on automobile identification plates
collected from Chiltons Repair & Tune-Up Guide (1985).

90, Mazda MPV, 40K MI, 7 Pass, V6, Auto
ABS, PL/PW, Cruise, Dual Air
87, Grand Caravan, 35K MI, 7 Pass, V6, Auto
Cruise, Air, Tilt, Tinting

Table 7: Sample notes from the minivan domain. Listed above are 2 of the 22 notes
about minivan automobiles collected by the first author.

Lorus Disney Oversize Mickey Mouse Watch.
Genuine leather strap.
Seiko Disney Ladies' Minnie Mouse Watch.
Leather strap.

Table 8: Sample notes from the watch domain. Listed above are 2 of the 89 notes
about personal watches collected from the Best catalog (a department store).

77

S CHLIMMER & H ERMENS

azatadine maleate
Blood: thrombocytopenia.
CNS: disturbed coordination, dizziness, drowsiness, sedation,
vertigo.
CV: palpitations, hypotension.
GI: anorexia, dry mouth and throat, nausea, vomiting.
GU: Urinary retention.
Skin: rash, urticaria.
Other: chills, thickening of bronchial secretions.
brompheniramine maleate
Blood: aganulocytosis, thrombocytopenia.
CNS: dizziness, insomnia, irritability, tremors.
CV: hypotension, palpitations.
GI: anorexia, dry mouth and throat, nausea, vomiting.
GU: urinary retention.
Skin: rash, urticaria.
After parenteral administration:
local reaction, sweating, syncope may occur.

Table 9: Sample notes from the antihistamine domain. Listed above are 2 of the 17
notes on the side effects of antihistamines collected from the Nurses Guide to Drugs
(1979).

Canon FD f/1.8, 6oz., f/22, 13in.,
good sharpness, poor freedom from flare,
better freedom from distortion,
focal length marked on sides as well as
on front of lens
Chinon f/1.7, 6oz., f/22, 9in.,
poor sharpness, good freedom from flare,
good freedom from distortion,
cannot be locked in program mode, which
is only a problem, of course, when lens is
used on program-mode cameras

Table 10: Sample notes from the lens domain. Listed above are 2 of the 31 notes about
35mm SLR camera normal lenses collected from the Consumer Reports (1988).

78

S OFTWARE A GENTS : C OMPLETING P ATTERNS & C ONSTRUCTING U SER I NTERFACES

22in. W. 48in.
A very large falcon. Three color phases occur:
blackish, white, and gray-brown. All
are more uniformly colored than the
Peregrine Falcon, which has dark
mustaches and hood.
16-24in. W. 42in.
Long-winged, long-tailed hawk with a
white rump, usually seen soaring
unsteadily over marshes with its wings
held in a shallow 'V'. Male has a pale
gray back, head, and breast. Female
and young are brown above, streaked
below, young birds with a rusty tone.

Table 11: Sample notes from the raptor domain. Listed above are 2 of the 21 notes
about North American birds of prey collected from (Bull & Farrand, 1977).
Summary characteristics of the nine domains are listed in Table 12 together with some
simple measures to indicate prediction difficulty. For instance, Column 1 shows the number
of notes in the domain. With a larger number of notes, the easier it should be to accurately
train a predictive method. Column 4 shows the standard deviation (STD) of the length of all
notes in each domain. It is more likely that a well-behaved FSM can be discovered when
STD is low. In this and successive tables, the domains are ranked by STD. Column 5
presents the percentage of unique tokens in the notes. The fewer novel tokens a note has, the
more likely that successive tokens can be predicted. This measure places an upper bound on
predictive accuracy. Column 6 shows the percentage of constant tokens, ones that always
appear in a fixed position. It is easier to predict these constant tokens. Finally, Column 7
indicates the percentage of repeated tokens. When fewer tokens are repeated verbatim within
a note, the more likely that the predictive method will not become confused about its locale
within a note during prediction.
The first six domains are natural for the interactive note taking task because they exhibit
a regular syntax. The last three domains are included to test the softwares ability on less
suitable domains. Notes from the Antihistamine, Lens, and Raptor domains contain highlyvariable lists of terms or natural language sentences. Learned FSMs for notes in these
domains are unlikely to converge, and, in the experiments reported here, only the FSM for
the Lens data exceeded the maturity threshold (average state usage greater than 2).
7.1

Contextual Prediction Accuracy

Column 7 of Table 13 lists the accuracy of next-token predictions made by the software in
prompting mode. The first nine rows list predictive accuracy over all tokens as notes from
each of the nine domains are independently processed in the order they were collected. The
last row lists predictive accuracy over all tokens as notes from all nine domains are collectively processed. This simulates a user taking notes about several domains simultaneously.
To put these results in context, the table also lists predictive accuracies for several other
methods. Column 1 lists the accuracy for a lower bound method. It assumes that each note
shares a fixed sequence of tokens. Termed common , this method initializes its structure to the
79

S CHLIMMER & H ERMENS

Domain
Airwing
Pattern
Engine Code
Minivan
PowerBook
Watch
Antihistamine
Lens
Raptor

1
2
3
4
5
6
7
N Notes N Tokens Tokens/Note STD % Unique % Constant % Repeated
78
936
12.0 0.3
18
8
0
13
75
5.8 0.7
21
0
0
20
222
11.1 0.8
0
0
0
22
335
15.2 1.7
9
17
0
95
1238
13.0 2.6
1
31
15
89
832
9.3 5.1
13
0
1
17
421
24.8 9.4
17
8
1
31
1066
34.4 9.6
1
26
19
21
878
41.8 11.5
33
7
22

Table 12: Quantitative properties of the nine domains used to test alternative methods.
first note. It then removes each token in this sequential structure that cannot be found in
order in other notes. At best, this method can only predict the constant, delimiter-like tokens
that may appear regularly in notes. Its performance is limited by the percentage of constant
tokens reported in Column 6 of Table 12. It performs best for the PowerBook notes where it
learns the following note syntax:
* :NULL * "K" * " PowerBook" * "MB" * "MB" * " Int." * .

(Example 3)

(The asterisks indicate Kleene star notation.) This reads as some sequence of zero or more
tokens then the token :NULL, followed by zero or more tokens then "K", followed by zero or
more tokens then "PowerBook", and so on. It is less successful for the minivan notes where it
learns a simpler syntax:
* :NULL * "K" * " MI" * " Pass" * .

(Example 4)

Columns 2 and 3 of Table 13 list the accuracy of using a classifier to directly predict the
next token without explicitly learning a syntax. In this paradigm, examples are prefixes of
token sequences. Attributes are the last token in the sequence, the second to last token, the
third to last token, and so on. Class values are the next token in the sequencethe one to be
predicted. Column 2 lists the performance of a simple Bayes classifier, and Column 3 lists
the performance of an incremental variant of ID3 (Schlimmer & Fisher, 1986). Perhaps
surprisingly, these methods perform considerably worse than the simple conjunctive method.
Without the benefit of a narrow context provided by the FSM, these methods must implicitly
construct representations to detect differences between similar situations that arise within a
single note. For example, in the PowerBook notes, a classifier-only approach must learn to
discriminate between the first and second occurrence of the "MB" token.
Column 4 of Table 13 lists the accuracy of a more viable prediction mechanism. Based
on simple ideas of memorization and termed digram, the method maintains a list of tokens
that have immediately followed each observed token. For example, in the fabric pattern
domain, this method retains the list of tokens {"8-10-12", "10", "11/12", "12"} as those
that follow the token "Size". Each list of follow tokens are kept in order from most to least
frequent. To predict the next token, the system looks for the last token written and predicts
80

S OFTWARE A GENTS : C OMPLETING P ATTERNS & C ONSTRUCTING U SER I NTERFACES

Domain
Airwing
Pattern
Engine Code
Minivan
PowerBook
Watch
Antihistamine
Lens
Raptor
Combined

1
2
3
4
5
Common Bayes ID4 Digram FSM
19
8
8
47
62
25
15 16
34
43
18
8
8
59
64
29
6
7
54
46
40
7
8
73
70
21
10 14
44
39
11
4
6
40
24
22
3
3
68
63
9
2
3
11
12

 
48
46

6
FSM+Bayes
44
43
63
44
76
33
22
60
9
45

7
8
FSM+ID4 Upper
62
79
51
68
69
87
47
80
82
96
42
78
24
68
63
91
12
55
49


Table 13: Percentage of tokens correctly predicted as a function of the learning
method.
the most frequent follow token. This method is nearly as effective as any other in Table 13,
especially on the combined task when notes from each domain are entered in random order.
Laird (1992) describes an efficient algorithm for maintaining higher-dimensional n-grams, in
effect increasing the context of each prediction and effectively memorizing longer sequences
of tokens. Lairds algorithm builds a Markov tree and incorporates heuristics that keep the
size of the tree from growing excessively large. Regrettably, these methods are unsuitable for
the interactive note-taking software because of the difficulty of using them to construct a
custom user interface. It is plausible to construct a panel of exclusive choices based directly
on the set of follow tokens, but it is unclear how to identify optional choices corresponding
to loops in finite-state machines. Moreover, if notes are drawn from different domains, and
those domains share even a single token, then some follow set will include tokens from
different domains. Using these follow sets to construct a user interface will unnecessarily
confuse the user by introducing options from more than one domain at a time.
Column 5 of Table 13 lists the accuracy of prediction based solely on the learned FSMs.
Without an embedded classifier, this method must rely on prediction of the most common
transition (or termination) from each state. Because the prediction is based on simple counts
(as noted in Section 4, Learning Embedded Classifiers), this method never predicts optional
transitions.
Columns 6 and 7 of Table 13 list the accuracy of predicting using FSMs and embedded
classifiers. The classifiers used are simple Bayes and the incremental ID3, respectively. The
latter outperforms either the FSM alone or the FSM with embedded Bayes classifiers. If the
system only makes predictions when its confidence measure is greater than 0.25, the accuracy is significantly different for the Engine Code, Minivan, Lens, and Raptor domains,
ranging between 10 and 22 percentage points of improvement.
Column 8 of Table 13 lists an estimate of the upper-bound on predictive accuracy. This
was calculated by assuming that prediction errors were only made the first time each distinct
token was written.

81

S CHLIMMER & H ERMENS

1
Domain
Airwing
Pattern
Engine Code
Minivan
PowerBook
Watch
Antihistamine
Lens
Raptor

Norm
62
51
69
47
82
42
24
63
12

2
Diff
Tokens
62
51
71
48
80
42
25
66
11

3
Rules
2a,b
63
53
72
48
83
43
24
64
12

4
5
6
7
8
9
10
Rules
No
Accept Accept Repeat Drop
New IDs
2ab,3a Restart = 1/4
= 3/4
Atts Classr
62
62
62
62
62
61
63
52
50
51
51
51
51
53
69
43
69
69
69
67
72
47
28
47
47
52
45
48
83
77
82
82
81
80
82
43
28
42
42
42
41
43
24
9
24
24
24
24
24
63
46
63
63
63
63
64
12
11
12
12
12
12
12

Table 14: Percentage of tokens correctly predicted as a function of design variations.
7.2

Design Decisions

The note taking software embodies a number of design decisions. Table 14 lists the effects of
these decisions on predictive accuracy by comparing versions of the software with and without each design feature. The first column lists the predictive accuracy for the softwares
nominal configuration. Column 2 lists the accuracy data for a slightly different generic
tokenizer. Accuracy is higher for some domains, lower for others. A custom-built tokenizer
is one way to incorporate knowledge about the domain. Columns 3 and 4 show the accuracy
for the system using only the original two FSM merging rules (cf. Table 1) and all but the
last merging rule (cf. Table 2), respectively. The decreased structural generality tends to
lower predictive accuracy, but the embedded classifiers help compensate for the reduced
accuracy. Column 5 lists the accuracy for when the FSM does not heuristically continue
parsing upon encountering a token for which there is no immediate transition. As expected,
accuracy suffers considerably in some domains because a novel token in a sequence
completely foils any subsequent prediction. Columns 6 and 7 list accuracy for different
values of the free parameter controlling the clustering of notes together into a FSM. There is
little effect on predictive accuracy in this case. Column 8 shows the accuracy for when
embedded classifiers do not use information about repeated states in the FSM. Without this
information, the classifiers cannot predict that a loop transition should be taken exactly once.
Surprisingly, elimination of this feature has little effect on accuracy. Column 9 lists the accuracy for when the embedded classifiers associated with a pair of FSM states are discarded
when the states are merged. Finally, Column 10 lists the accuracy for when a new FSM state
is assigned a unique ID rather than the ID of the oldest of the two merged states.
7.3

Sample Button-Box Interfaces

In addition to Figure 3, Figures 11 through 15 depict button-box interfaces for the five other
well-behaved note taking domains listed at the top of Table 12. These interfaces are visual
and offer the user an organized view of their notes, presenting options in a natural way.
However, whenever unique tokens are involved, the current software makes no attempt to
explicitly generalize tokens. This effect is reflected in the tour dates for the Airwing notes in
Figure 11. Note that the radio button panel consists of a long series of dates, none of which is
likely to be selected for a new note.
82

S OFTWARE A GENTS : C OMPLETING P ATTERNS & C ONSTRUCTING U SER I NTERFACES

Figure 11: Screen snapshot of the note-taking software in button-box mode for an
airwing note.

Figure 12: Screen snapshot of the note-taking software in button-box mode for a
fabric pattern note.

8. Related Work
Self-customizing software agents have several subjective dimensions on which they can be
evaluated and compared:
 AnticipationDoes the system present alternatives without the user having to
request them?
 User interfaceIs the system graphical, or is it command-line oriented?
 User controlCan the user override or choose to ignore predictive actions?
 ModalityIf the system has a number of working modes, can the user work in any
mode without explicitly selecting one of them?
 Learning updateIs learning incremental, continuous and/or real-time?
83

S CHLIMMER & H ERMENS

Figure 13: Screen snapshot of the note-taking software in button-box mode for an
engine code note.

Figure 14: Screen snapshot of the note-taking software in button-box mode for a
minivan note.
 User adjustableCan the user tune the system parameters manually?
Here we describe related systems that exhibit properties in each of these agent dimensions.
Our note taking software utilizes the anticipation user interface technique pioneered by
Eager (Cypher, 1991). Eager is a non-intrusive system that learns to perform iterative procedures by watching the user. As such, it is a learning apprentice, a software agent, and an
example of programming by example or demonstration. Situated within the HyperCard environment, it continuously watches a users actions. When it detects the second cycle of an
iteration, it presents an execute icon for the users notice. It also visually indicates the anticipated next action by highlighting the appropriate button, menu item, or text selection in
green. As the user performs their task, they can verify that Eager has learned the correct
84

S OFTWARE A GENTS : C OMPLETING P ATTERNS & C ONSTRUCTING U SER I NTERFACES

Figure 15: Screen snapshot of the note-taking software in button-box mode for a
watch note.
procedure by comparing its anticipations to their actions. When the user is confident enough,
they can click on the execution icon, and Eager will run the iterative procedure to completion. Eager is highly anticipatory, uses a graphical interface, is non-obtrusive, non-modal,
and learns in real-time, but is not user adjustable.
CAP is an apprenticeship system that learns to predict default values (Dent, et al., 1992).
Its domain of operation is calendar management, and it learns preferences as a knowledgable
secretary might. For example, a professor may prefer to hold a regular group meeting in a
particular room at a particular time of day for a particular durationinformation that a
secretary would know from experience. CAP collects information as the user manages their
calendar, learns from previous meetings, and uses the regularities it learns to offer default
values for meeting location, time, and duration. The learning system is re-run each night on
the most recent meeting data, and the learned rules are applied for prediction the following
day. CAP is also designed to utilize an extensible knowledge base that contains calendar
information and a database of personnel information. The system continues to be used to
manage individual faculty calendars. Though offering some intelligence, CAPs user interface is line-oriented and is based on the Emacs editor. Questions asked of the user about
meetings are presented using a command-line dialog, and the default predictions are
displayed one-at-a-time. CAP can be characterized as anticipatory, command-line oriented
and modal with user control (but not user adjustable), where learning is done in batch.
Another related system addresses the task of learning to fill out a form (Hermens &
Schlimmer, 1993). The system recreates a paper form as an on-screen facsimile, allowing the
user to view all of the pertinent information at a glance. Input typed by the user into the electronic form is processed by a central form-filling module. When the user completes a form
copy, it is printed, and each field value on the form is forwarded to a learning module (a decision tree learning method). The learned representations predict default values for each field
on the form by referring to values observed on other fields and on the previous form copy.
From the users point of view, it is as if spreadsheet functions have been learned for each
field of the form. Empirical studies indicate that this system reduced the number of keystrokes required of the user by 87% on 269 forms processed over the 8 month period in

85

S CHLIMMER & H ERMENS

which it was actually used by office personnel. This system is unobtrusive, non-modal and
anticipatory, uses a graphical interface, and updates learning in real-time.
Maes and Kozierok (1993) are addressing the problem of self-customizing software at a
much more task-independent level. They identify three learning opportunities for a software
agent: observing the users actions and imitating them, receiving user feedback upon error,
and incorporating explicit training by the user. To illustrate the generality of their framework, they demonstrate simple learning apprentices that help sort the users electronic mail
and schedule meetings. Their initial systems use an instance-based (case- or memory-based)
approach primarily because it allows efficient update and because it naturally generates a
confidence in each of its predictions. Users may set thresholds on these predictions, corresponding to a minimum confidence for when the agent should prompt the user (a tell-me
threshold) and a higher minimum confidence for the agent to act immediately on behalf of
the user (a do-it threshold). The framework for learning in this case is anticipatory, utilizes
a graphical user interface, is devoted to user control, is non-modal, learns in real-time, and is
user adjustable.
A system developed for Macintosh Common Lisp (MCL) provides a word-completion
mechanism for word prefixes typed by the user in any window. J. Salem and A. Ruttenberg
(unpublished) have devised MCL methods to display a word completion in the status bar of
the each window. If the user desires to add the completion to the window, they simply press
the CLEAR key. This word completion mechanism is similar to file-name completion in
EMACS and the C-shell in UNIX systems, except that the word is displayed for the user
before it is added. This system is anticipatory (unlike the UNIX file completion), is command
line oriented (but displays the default completion in a graphical window), can be fully
controlled by the user, is non-modal, learns in real time, is not intended to be user adjustable
(though knowledgeable MCL programmers could easily make changes to the code).
The interactive note taking software we have devised does not require any user programming. It only receives implicit user feedback when the user chooses to complete a note in a
different way than prompted. It does not have any mechanisms for direct user instruction or
threshold tuning. In a system designed to be as easy to use as paper, such explicit adjustment
may be inappropriate. We characterize our system as anticipatory, graphically-oriented, and
modal (due to the switching that takes place when a user wishes to display the button-box
interface). It allows the user to override default prompts and predictions, and it learns in
real-time. We have not included features that allow the user to configure the performance of
the agent.

9. Observations/Limitations
The interactive note-taking software is designed to help users capture information digitally,
both to speed entry and improve accuracy, and to support the longer term goal of efficient
retrieval. The software incorporates two distinctive features. First, it actively predicts what
the user is going to write. Second, it automatically constructs a custom radio-button, checkbox user interface.
This research explores the extremes of FSM learning and prediction, where the system
has no explicit a priori knowledge of the note domains. We have tried to design the system
so that it can learn quickly, yet adapt well to semantic and syntactic changes, all without a
knowledge store from which to draw. It is clear that knowledge in the form of a domain-specific tokenizer would aid FSM learning by chunking significant phrases and relating similar
notations and abbreviations. Some preliminary work has shown that, after a few notes have
86

S OFTWARE A GENTS : C OMPLETING P ATTERNS & C ONSTRUCTING U SER I NTERFACES

been written, users may create abbreviations instead of writing out whole words. A domainspecific tokenizer would be able to relate an abbreviation and a whole word as being in the
same class, and therefore allow for more flexibility during note taking. For example, a
domain-specific tokenizer may recognize that "Megabytes", "Meg", "MB", and "M" all represent the same token for memory sizes. One could imagine a framework that would allow for
domain-specific tokenizers to be simply plugged in.
The prototype built to demonstrate these ideas was implemented on a conventional,
micro computer with keyboard input. As a consequence, it was impossible to evaluate user
acceptance of the new interface or the adaptive agent. With newly available computing
devices incorporating pen input and handwriting recognition, it should be possible to reengineer the user interface and field test these ideas with actual users.
One aspect of note learning, related to tokenization and the button-box user interface
display, is the difficulty of generalizing numeric strings or unique tokens. The cardinality of
the range of model numbers, telephone numbers, quantities, sizes, other numeric values, and
even proper names is very large in some note domains. The finite-state machine learning
method presented here is incapable of generalizing over transitions from a particular state,
and, as a consequence, the current system has the problem of displaying a very lengthy
button-box interface list. (A button is displayed for each value encountered in the syntax of
notes, and there may be many choices.) For example, a large variety of pattern numbers may
be available in the fabric pattern note domain. An appropriate mechanism is desired to determine when the list of numeric choices is too large to be useful as a button-box interface. The
system can then generalize the expected number, indicating the number of digits to prompt
the user: ####, for example. This may be helpful to remind the user that a number is
expected without presenting an overbearing list of possibilities.
Another limitation of the current effort lies in the choice of finite-state machines to
represent the syntax of the users notes. Notes may not be regular expressions with the
consequence that the FSMs may become too large as the learning method attempts to acquire
a syntax. This may place an unreasonable demand on memory and lead to reduced prompting
effectiveness.
The choice of finite-state machines also apparently constraints the custom user interface.
Because FSMs branch in unpredicable ways, button-box interfaces must be rendered incrementally. After the user indicates a particular transition (by selecting a button), the system
can render states reachable from that transition for the user. Ideally, the user should be able
to select buttons corresponding to note fragments in any order, allowing them to write down
the size before the pattern number, for example. To construct a non-modal user interface, a
more flexible syntactic representation is needed.
Several of the low-level design decisions employed in this system are crude responses to
technical issues. For instance, the decision to render a syntax as a button-box interface only
if the average number of times each state has been used to parse notes is greater than 2. This
ignores the fact that some parts of the state machine have been used frequently for parsing
notes while other parts have rarely been used. Similarly, the particular measure for estimating prompting confidence (and setting the saturation of the completion button) is simplistic
and would benefit from a more sound statistical basis.

Acknowledgments
Anonymous reviewers suggested an additional example in Section 3, offered some refinements to the user interface, graciously identified some limitations of the work listed in
87

S CHLIMMER & H ERMENS

Section 9, and pointed out some additional related work. Mike Kibler, Karl Hakimian, and
the EECS staff provided a consistent and reliable computing environment. Apple Cambridge
developed and supports the Macintosh Common Lisp programming environment. Allen
Cypher provided the tokenizer code. This work was supported in part by the National
Science Foundation under grant number 92-1290 and by a grant from Digital Equipment
Corporation.

References
Angluin, D. (1982). Inference of reversible languages. Journal of the Association for
Computing Machinery, 29, 741765.
Berwick, R. C., & Pilato, S. (1987). Learning syntax by automata induction. Machine Learning, 2, 938.
Bull, J., & Farrand, J., Jr. (1977). The Audubon Society Field Guide to North American Birds
(Eastern Edition). NY: Alfred A. Knopf (pp. 401682).
Chiltons Repair & Tune-Up Guide: GM X-Body 1980-1985 (1985). Randor, PA: Chilton
Book (p. 7).
Cohen, W. W. (1988). Generalizing number and learning from multiple examples in explanation based learning. Proceedings of the Fifth International Conference on Machine
Learning (pp. 256269). Ann Arbor, MI: Morgan Kaufmann.
Consumer Reports (1988), 53 (12), 302303. Mount Vernon, NY: Consumers Union.
Cypher, A. (1991). Eager: Programming repetitive tasks by example. Proceedings of CHI
(pp. 3339). New Orleans, LA: ACM.
Dent, L., Boticario, J., McDermott, J., Mitchell, T., & Zabowski, D. (1992). A personal
learning apprentice. Proceedings of the Tenth National Conference on Artificial Intelligence (pp. 96103). San Jose, CA: AAAI Press.
Fisher, D. H. (1987). Knowledge acquisition via incremental conceptual clustering. Machine
Learning, 2, 139172.
Grove, M., & Miller, J. (1989). North American Rockwell A3J/A-5 Vigilante. Arlington, TX:
Aerofax (pp. 1315).
Hermens, L. A., & Schlimmer, J. C. (1993). A machine-learning apprentice for the completion of repetitive forms. Proceedings of the Ninth IEEE Conference on Artificial Intelligence for Applications. Orlando, FL.
Laird, P. (1992). Discrete sequence prediction and its applications. Proceedings of the Tenth
National Conference on Artificial Intelligence (pp. 135140). San Jose, CA: AAAI Press.

88

S OFTWARE A GENTS : C OMPLETING P ATTERNS & C ONSTRUCTING U SER I NTERFACES

Maes, P., & Kozierok, R. (1993). Learning interface agents. Proceedings of the Eleventh
National Conference on Artificial Intelligence (pp. 459465). Washington, D. C.: AAAI
Press.
Nurses Guide to Drugs (1979). Horsham, PA: Intermed Communications (pp. 454462).
Schlimmer, J. C., & Fisher, D. H. (1986). A case study of incremental concept induction.
Proceedings of the Fifth National Conference on Artificial Intelligence (pp. 496501).
Philadelphia, PA: AAAI Press.

89

Journal of Articial Intelligence Research 1 (1993) 25-46

Submitted 7/93; published 8/93

Dynamic Backtracking
Matthew L. Ginsberg

ginsberg@cs.uoregon.edu

CIRL, University of Oregon,
Eugene, OR 97403-1269 USA

Abstract

Because of their occasional need to return to shallow points in a search tree, existing
backtracking methods can sometimes erase meaningful progress toward solving a search
problem. In this paper, we present a method by which backtrack points can be moved
deeper in the search space, thereby avoiding this diculty. The technique developed is
a variant of dependency-directed backtracking that uses only polynomial space while still
providing useful control information and retaining the completeness guarantees provided
by earlier approaches.

1. Introduction
Imagine that you are trying to solve some constraint-satisfaction problem, or csp. In the
interests of deniteness, I will suppose that the csp in question involves coloring a map of
the United States subject to the restriction that adjacent states be colored dierently.
Imagine we begin by coloring the states along the Mississippi, thereby splitting the
remaining problem in two. We now begin to color the states in the western half of the
country, coloring perhaps half a dozen of them before deciding that we are likely to be able
to color the rest. Suppose also that the last state colored was Arizona.
At this point, we change our focus to the eastern half of the country. After all, if we can't
color the eastern half because of our coloring choices for the states along the Mississippi,
there is no point in wasting time completing the coloring of the western states.
We successfully color the eastern states and then return to the west. Unfortunately, we
color New Mexico and Utah and then get stuck, unable to color (say) Nevada. What's more,
backtracking doesn't help, at least in the sense that changing the colors for New Mexico
and Utah alone does not allow us to proceed farther. Depth-rst search would now have
us backtrack to the eastern states, trying a new color for (say) New York in the vain hope
that this would solve our problems out West.
This is obviously pointless; the blockade along the Mississippi makes it impossible for
New York to have any impact on our attempt to color Nevada or other western states.
What's more, we are likely to examine every possible coloring of the eastern states before
addressing the problem that is actually the source of our diculties.
The solutions that have been proposed to this involve nding ways to backtrack directly
to some state that might actually allow us to make progress, in this case Arizona or earlier.
Dependency-directed backtracking (Stallman & Sussman, 1977) involves a direct backtrack
to the source of the diculty; backjumping (Gaschnig, 1979) avoids the computational overhead of this technique by using syntactic methods to estimate the point to which backtrack
is necessary.

c 1993 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Ginsberg

In both cases, however, note that although we backtrack to the source of the problem,
we backtrack over our successful solution to half of the original problem, discarding our
solution to the problem of coloring the states in the East. And once again, the problem is
worse than this { after we recolor Arizona, we are in danger of solving the East yet again
before realizing that our new choice for Arizona needs to be changed after all. We won't
examine every possible coloring of the eastern states, but we are in danger of rediscovering
our successful coloring an exponential number of times.
This hardly seems sensible; a human problem solver working on this problem would
simply ignore the East if possible, returning directly to Arizona and proceeding. Only if the
states along the Mississippi needed new colors would the East be reconsidered { and even
then only if no new coloring could be found for the Mississippi that was consistent with the
eastern solution.
In this paper we formalize this technique, presenting a modication to conventional
search techniques that is capable of backtracking not only to the most recently expanded
node, but also directly to a node elsewhere in the search tree. Because of the dynamic way
in which the search is structured, we refer to this technique as dynamic backtracking.
A more specic outline is as follows: We begin in the next section by introducing a
variety of notational conventions that allow us to cast both existing work and our new
ideas in a uniform computational setting. Section 3 discusses backjumping, an intermediate
between simple chronological backtracking and our ideas, which are themselves presented
in Section 4. An example of the dynamic backtracking algorithm in use appears in Section
5 and an experimental analysis of the technique in Section 6. A summary of our results and
suggestions for future work are in Section 7. All proofs have been deferred to an appendix
in the interests of continuity of exposition.

2. Preliminaries
Denition 2.1 By a constraint satisfaction problem (I; V; ) we will mean a set I of vari-

ables; for each i 2 I , there is a set Vi of possible values for the variable i.  is a set of
constraints, each a pair (J; P ) where J = (j1; . . . ; jk ) is an ordered subset of I and P is a
subset of Vj1      Vjk .
A solution to the csp is a set vi of values for each of the variables in I such that vi 2 Vi
for each i and for every constraint (J; P ) of the above form in , (vj1 ; . . . ; vjk ) 2 P .

In the example of the introduction, I is the set of states and Vi is the set of possible
colors for the state i. For each constraint, the rst part of the constraint is a pair of adjacent
states and the second part is a set of allowable color combinations for these states.
Our basic plan in this paper is to present formal versions of the search algorithms
described in the introduction, beginning with simple depth-rst search and proceeding to
backjumping and dynamic backtracking. As a start, we make the following denition of a
partial solution to a csp:

Denition 2.2 Let (I; V; ) be a csp. By a partial solution to the csp we mean an ordered
subset J  I and an assignment of a value to each variable in J .
26

Dynamic Backtracking

We will denote a partial solution by a tuple of ordered pairs, where each ordered pair

(i; v ) assigns the value v to the variable i. For a partial solution P , we will denote by P the
set of variables assigned values by P .

Constraint-satisfaction problems are solved in practice by taking partial solutions and
extending them by assigning values to new variables. In general, of course, not any value can
be assigned to a variable because some are inconsistent with the constraints. We therefore
make the following denition:

Denition 2.3 Given a partial solution P to a

csp, an eliminating explanation for a
variable i is a pair (v; S ) where v 2 Vi and S  P . The intended meaning is that i
cannot take the value v because of the values already assigned by P to the variables in S .
An elimination mechanism  for a csp is a function that accepts as arguments a partial
solution P , and a variable i 62 P . The function returns a (possibly empty) set (P; i) of
eliminating explanations for i.

For a set E of eliminating explanations, we will denote by Eb the values that have been
identied as eliminated, ignoring the reasons given. We therefore denote by b(P; i) the set
of values eliminated by elements of (P; i).
Note that the above denition is somewhat exible with regard to the amount of work
done by the elimination mechanism { all values that violate completed constraints might
be eliminated, or some amount of lookahead might be done. We will, however, make the
following assumptions about all elimination mechanisms:
1. They are correct. For a partial solution P , if the value vi 62 b(P; i), then every
constraint (S; T ) in  with S  P [fig is satised by the values in the partial solution
and the value vi for i. These are the constraints that are complete after the value vi
is assigned to i.
2. They are complete. Suppose that P is a partial solution to a csp, and there is some
solution that extends P while assigning the value v to i. If P 0 is an extension of P
with (v; E ) 2 (P 0 ; i), then
E \ (P 0 , P ) 6= 
(1)
In other words, whenever P can be successfully extended after assigning v to i but
P 0 cannot be, at least one element of P 0 , P is identied as a possible reason for the
problem.
3. They are concise. For a partial solution P , variable i and eliminated value v , there
is at most a single element of the form (v; E ) 2 (P; i). Only one reason is given why
the variable i cannot have the value v .

Lemma 2.4 Let  be a complete elimination mechanism for a csp, let P be a partial solution to this csp and let i 62 P . Now if P can be successfully extended to a complete solution
after assigning i the value v , then v 62 b(P; i).
I apologize for the swarm of denitions, but they allow us to give a clean description of
depth-rst search:
27

Ginsberg

Algorithm 2.5 (Depth-rst search) Given as inputs a constraint-satisfaction problem

and an elimination mechanism :
1. Set P = . P is a partial solution to the csp. Set Ei =  for each i 2 I ; Ei is the
set of values that have been eliminated for the variable i.
2. If P = I , so that P assigns a value to every element in I , it is a solution to the
original problem. Return it. Otherwise, select a variable i 2 I , P . Set Ei = b(P; i),
the values that have been eliminated as possible choices for i.
3. Set S = Vi , Ei, the set of remaining possibilities for i. If S is nonempty, choose an
element v 2 S . Add (i; v ) to P , thereby setting i's value to v , and return to step 2.
4. If S is empty, let (j; vj ) be the last entry in P ; if there is no such entry, return failure.
Remove (j; vj ) from P , add vj to Ej , set i = j and return to step 3.

We have written the algorithm so that it returns a single answer to the csp; the modication to accumulate all such answers is straightforward.
The problem with Algorithm 2.5 is that it looks very little like conventional depth-rst
search, since instead of recording the unexpanded children of any particular node, we are
keeping track of the failed siblings of that node. But we have the following:

Lemma 2.6 At any point in the execution of Algorithm 2.5, if the last element of the partial
solution P assigns a value to the variable i, then the unexplored siblings of the current node
are those that assign to i the values in Vi , Ei .
Proposition 2.7 Algorithm 2.5 is equivalent to depth-rst search and therefore complete.

As we have remarked, the basic dierence between Algorithm 2.5 and a more conventional description of depth-rst search is the inclusion of the elimination sets Ei. The
conventional description expects nodes to include pointers back to their parents; the siblings of a given node are found by examining the children of that node's parent. Since we
will be reorganizing the space as we search, this is impractical in our framework.
It might seem that a more natural solution to this diculty would be to record not the
values that have been eliminated for a variable i, but those that remain to be considered.
The technical reason that we have not done this is that it is much easier to maintain
elimination information as the search progresses. To understand this at an intuitive level,
note that when the search backtracks, the conclusion that has implicitly been drawn is
that a particular node fails to expand to a solution, as opposed to a conclusion about the
currently unexplored portion of the search space. It should be little surprise that the most
ecient way to manipulate this information is by recording it in approximately this form.

3. Backjumping

How are we to describe dependency-directed backtracking or backjumping in this setting?
In these cases, we have a partial solution and have been forced to backtrack; these more
sophisticated backtracking mechanisms use information about the reason for the failure to
identify backtrack points that might allow the problem to be addressed. As a start, we need
to modify Algorithm 2.5 to maintain the explanations for the eliminated values:
28

Dynamic Backtracking

Algorithm 3.1 Given as inputs a constraint-satisfaction problem and an elimination mechanism :
1. Set P = Ei =  for each i 2 I . Ei is a set of eliminating explanations for i.
2. If P = I , return P . Otherwise, select a variable i 2 I , P . Set Ei = (P; i):
3. Set S = Vi , Ebi. If S is nonempty, choose an element v 2 S . Add (i; v ) to P and
return to step 2.
4. If S is empty, let (j; vj ) be the last entry in P ; if there is no such entry, return failure.
Remove (j; vj ) from P . We must have Ebi = Vi, so that every value for i has been
eliminated; let E be the set of all variables appearing in the explanations for each
eliminated value. Add (vj ; E , fj g) to Ej , set i = j and return to step 3.

Lemma 3.2 Let P be a partial solution obtained during the execution of Algorithm 3.1,

and let i 2 P be a variable assigned a value by P . Now if P 0  P can be successfully
extended to a complete solution after assigning i the value v but (v; E ) 2 Ei , we must have

E \ (P , P 0) 6= 

In other words, the assignment of a value to some variable in P , P 0 is correctly identied
as the source of the problem.
Note that in step 4 of the algorithm, we could have added (vj ; E \ P ) instead of (vj ; E ,
fj g) to Ej ; either way, the idea is to remove from E any variables that are no longer assigned
values by P .
In backjumping, we now simply change our backtrack method; instead of removing a
single entry from P and returning to the variable assigned a value prior to the problematic
variable i, we return to a variable that has actually had an impact on i. In other words, we
return to some variable in the set E .

Algorithm 3.3 (Backjumping) Given as inputs a constraint-satisfaction problem and an
elimination mechanism :
1. Set P = Ei =  for each i 2 I .

2. If P = I , return P . Otherwise, select a variable i 2 I , P . Set Ei = (P; i):
3. Set S = Vi , Ebi. If S is nonempty, choose an element v 2 S . Add (i; v ) to P and
return to step 2.
4. If S is empty, we must have Ebi = Vi . Let E be the set of all variables appearing in
the explanations for each eliminated value.
5. If E = , return failure. Otherwise, let (j; vj ) be the last entry in P such that j 2 E .
Remove from P this entry and any entry following it. Add (vj ; E \ P ) to Ej , set i = j
and return to step 3.
29

Ginsberg

In step 5, we add (vj ; E \ P ) to Ej , removing from E any variables that are no longer
assigned values by P .

Proposition 3.4 Backjumping is complete and always expands fewer nodes than does depthrst search.

Let us have a look at this in our map-coloring example. If we have a partial coloring
P and are looking at a specic state i, suppose that we denote by C the set of colors that
are obviously illegal for i because they conict with a color already assigned to one of i's
neighbors.
One possible elimination mechanism returns as (P; i) a list of (c; P ) for each color
c 2 C that has been used to color a neighbor of i. This reproduces depth-rst search, since
we gradually try all possible colors but have no idea what went wrong when we need to
backtrack since every colored state is included in P . A far more sensible choice would take
(P; i) to be a list of (c; fng) where n is a neighbor that is already colored c. This would
ensure that we backjump to a neighbor of i if no coloring for i can be found.
If this causes us to backjump to another state j , we will add i's neighbors to the eliminating explanation for j 's original color, so that if we need to backtrack still further, we
consider neighbors of either i or j . This is as it should be, since changing the color of one of
i's other neighbors might allow us to solve the coloring problem by reverting to our original
choice of color for the state j .
We also have:

Proposition 3.5 The amount of space needed by backjumping is o(i2v), where i = jI j is

the number of variables in the problem and v is the number of values for that variable with
the largest value set Vi .

This result contrasts sharply with an approach to csps that relies on truth-maintenance
techniques to maintain a list of nogoods (de Kleer, 1986). There, the number of nogoods
found can grow linearly with the time taken for the analysis, and this will typically be
exponential in the size of the problem. Backjumping avoids this problem by resetting the
set Ei of eliminating explanations in step 2 of Algorithm 3.3.
The description that we have given is quite similar to that developed in (Bruynooghe,
1981). The explanations there are somewhat coarser than ours, listing all of the variables
that have been involved in any eliminating explanation for a particular variable in the csp,
but the idea is essentially the same. Bruynooghe's eliminating explanations can be stored
in o(i2) space (instead of o(i2v )), but the associated loss of information makes the technique
less eective in practice. This earlier work is also a description of backjumping only, since
intermediate information is erased as the search proceeds.

4. Dynamic backtracking

We nally turn to new results. The basic problem with Algorithm 3.3 is not that it backjumps to the wrong place, but that it needlessly erases a great deal of the work that has
been done thus far. At the very least, we can retain the values selected for variables that
are backjumped over, in some sense moving the backjump variable to the end of the partial
30

Dynamic Backtracking

solution in order to replace its value without modifying the values of the variables that
followed it.
There is an additional modication that will probably be clearest if we return to the
example of the introduction. Suppose that in this example, we color only some of the eastern
states before returning to the western half of the country. We reorder the variables in order
to backtrack to Arizona and eventually succeed in coloring the West without disturbing the
colors used in the East.
Unfortunately, when we return East backtracking is required and we nd ourselves
needing to change the coloring on some of the eastern states with which we dealt earlier.
The ideas that we have presented will allow us to avoid erasing our solution to the problems
out West, but if the search through the eastern states is to be ecient, we will need to
retain the information we have about the portion of the East's search space that has been
eliminated. After all, if we have determined that New York cannot be colored yellow, our
changes in the West will not reverse this conclusion { the Mississippi really does isolate one
section of the country from the other.
The machinery needed to capture this sort of reasoning is already in place. When we
backjump over a variable k, we should retain not only the choice of value for k, but also k's
elimination set. We do, however, need to remove from this elimination set any entry that
involves the eventual backtrack variable j , since these entries are no longer valid { they
depend on the assumption that j takes its old value, and this assumption is now false.

Algorithm 4.1 (Dynamic backtracking I) Given as inputs a constraint-satisfaction problem and an elimination mechanism :
1. Set P = Ei =  for each i 2 I .
2. If P = I , return P . Otherwise, select a variable i 2 I , P . Set Ei = Ei [ (P; i).
3. Set S = Vi , Ebi. If S is nonempty, choose an element v 2 S . Add (i; v ) to P and
return to step 2.
4. If S is empty, we must have Ebi = Vi ; let E be the set of all variables appearing in the
explanations for each eliminated value.
5. If E = , return failure. Otherwise, let (j; vj ) be the last entry in P such that j 2 E .
Remove (j; vj ) from P and, for each variable k assigned a value after j , remove from
Ek any eliminating explanation that involves j . Set

Ej = Ej [ (P; j ) [ f(vj ; E \ P )g
(2)
so that vj is eliminated as a value for j because of the values taken by variables in
E \ P . The inclusion of the term (P; j ) incorporates new information from variables
that have been assigned values since the original assignment of vj to j . Now set i = j
and return to step 3.

Theorem 4.2 Dynamic backtracking always terminates and is complete. It continues to

satisfy Proposition 3.5 and can be expected to expand fewer nodes than backjumping provided
that the goal nodes are distributed randomly in the search space.
31

Ginsberg

The essential dierence between dynamic and dependency-directed backtracking is that
the structure of our eliminating explanations means that we only save nogood information
based on the current values of assigned variables; if a nogood depends on outdated information, we drop it. By doing this, we avoid the need to retain an exponential amount of
nogood information. What makes this technique valuable is that (as stated in the theorem)
termination is still guaranteed.
There is one trivial modication that we can make to Algorithm 4.1 that is quite useful
in practice. After removing the current value for the backtrack variable j , Algorithm 4.1
immediately replaces it with another. But there is no real reason to do this; we could
instead pick a value for an entirely dierent variable:

Algorithm 4.3 (Dynamic backtracking) Given as inputs a constraint-satisfaction problem and an elimination mechanism :
1. Set P = Ei =  for each i 2 I .
2. If P = I , return P . Otherwise, select a variable i 2 I , P . Set Ei = Ei [ (P; i).
3. Set S = Vi , Ebi. If S is nonempty, choose an element v 2 S . Add (i; v ) to P and
return to step 2.
4. If S is empty, we must have Ebi = Vi ; let E be the set of all variables appearing in the
explanations for each eliminated value.
5. If E = , return failure. Otherwise, let (j; vj ) be the last entry in P that binds a
variable appearing in E . Remove (j; vj ) from P and, for each variable k assigned
a value after j , remove from Ek any eliminating explanation that involves j . Add
(vj ; E \ P ) to Ej and return to step 2.

5. An example
In order to make Algorithm 4.3 a bit clearer, suppose that we consider a small mapcoloring problem in detail. The map is shown in Figure 1 and consists of ve countries:
Albania, Bulgaria, Czechoslovakia, Denmark and England. We will assume (wrongly!) that
the countries border each other as shown in the gure, where countries are denoted by nodes
and border one another if and only if there is an arc connecting them.
In coloring the map, we can use the three colors red, yellow and blue. We will typically
abbreviate the country names to single letters in the obvious way.
We begin our search with Albania, deciding (say) to color it red. When we now look at
Bulgaria, no colors are eliminated because Albania and Bulgaria do not share a border; we
decide to color Bulgaria yellow. (This is a mistake.)
We now go on to consider Czechoslovakia; since it borders Albania, the color red is
eliminated. We decide to color Czechoslovakia blue and the situation is now this:
32

Dynamic Backtracking

Denmark
s

s

Czechoslovakia

,,@@
,
@@
,
,
@
,
@@
,
Albanias,
@,sBulgaria
@@
,
,
@@
,
,
@
,
@@ ,
@,s
England

Figure 1: A small map-coloring problem
country
color red yellow blue
Albania
red
Bulgaria
yellow
Czechoslovakia blue A
Denmark
England
For each country, we indicate its current color and the eliminating explanations that mean
it cannot be colored each of the three colors (when such explanations exist). We now look
at Denmark.
Denmark cannot be colored red because of its border with Albania and cannot be colored
yellow because of its border with Bulgaria; it must therefore be colored blue. But now
England cannot be colored any color at all because of its borders with Albania, Bulgaria
and Denmark, and we therefore need to backtrack to one of these three countries. At this
point, the elimination lists are as follows:
country
color red yellow blue
Albania
red
Bulgaria
yellow
Czechoslovakia blue A
Denmark
blue A
B
England
A
B
D
We backtrack to Denmark because it is the most recent of the three possibilities, and
begin by removing any eliminating explanation involving Denmark from the above table to
get:
33

Ginsberg

color red yellow blue
country
Albania
red
Bulgaria
yellow
Czechoslovakia blue A
Denmark
A
B
England
A
B
Next, we add to Denmark's elimination list the pair
(blue; fA; B g)
This indicates correctly that because of the current colors for Albania and Bulgaria, Denmark cannot be colored blue (because of the subsequent dead end at England). Since every
color is now eliminated, we must backtrack to a country in the set fA; B g. Changing
Czechoslovakia's color won't help and we must deal with Bulgaria instead. The elimination
lists are now:
country
color red yellow blue
Albania
red
Bulgaria
Czechoslovakia blue A
Denmark
A
B A,B
England
A
B
We remove the eliminating explanations involving Bulgaria and also add to Bulgaria's elimination list the pair
(yellow; A)
indicating correctly that Bulgaria cannot be colored yellow because of the current choice of
color for Albania (red).
The situation is now:
color red yellow blue
country
Albania
red
Czechoslovakia blue A
Bulgaria
A
Denmark
A
England
A
We have moved Bulgaria past Czechoslovakia to reect the search reordering in the algorithm. We can now complete the problem by coloring Bulgaria red, Denmark either yellow
or blue, and England the color not used for Denmark.
This example is almost trivially simple, of course; the thing to note is that when we
changed the color for Bulgaria, we retained both the blue color for Czechoslovakia and the
information indicating that none of Czechoslovakia, Denmark and England could be red.
In more complex examples, this information may be very hard-won and retaining it may
save us a great deal of subsequent search eort.
Another feature of this specic example (and of the example of the introduction as
well) is that the computational benets of dynamic backtracking are a consequence of
34

Dynamic Backtracking

the automatic realization that the problem splits into disjoint subproblems. Other authors
have also discussed the idea of applying divide-and-conquer techniques to csps (Seidel, 1981;
Zabih, 1990), but their methods suer from the disadvantage that they constrain the order in
which unassigned variables are assigned values, perhaps at odds with the common heuristic
of assigning values rst to those variables that are most tightly constrained. Dynamic
backtracking can also be expected to be of use in situations where the problem in question
does not split into two or more disjoint subproblems.1

6. Experimentation

Dynamic backtracking has been incorporated into the crossword-puzzle generation program
described in (Ginsberg, Frank, Halpin, & Torrance, 1990), and leads to signicant performance improvements in that restricted domain. More specically, the method was tested
on the problem of generating 19 puzzles of sizes ranging from 2  2 to 13  13; each puzzle
was attempted 100 times using both dynamic backtracking and simple backjumping. The
dictionary was shued between solution attempts and a maximum of 1000 backtracks were
permitted before the program was deemed to have failed.
In both cases, the algorithms were extended to include iterative broadening (Ginsberg
& Harvey, 1992), the cheapest-rst heuristic and forward checking. Cheapest-rst has
also been called \most constrained rst" and selects for instantiation that variable with
the fewest number of remaining possibilities (i.e., that variable for which it is cheapest to
enumerate the possible values (Smith & Genesereth, 1985)). Forward checking prunes the
set of possibilities for crossing words whenever a new word is entered and constitutes our
experimental choice of elimination mechanism: at any point, words for which there is no legal
crossing word are eliminated. This ensures that no word will be entered into the crossword
if the word has no potential crossing words at some point. The cheapest-rst heuristic
would identify the problem at the next step in the search, but forward checking reduces
the number of backtracks substantially. The \least-constraining" heuristic (Ginsberg et al.,
1990) was not used; this heuristic suggests that each word slot be lled with the word that
minimally constrains the subsequent search. The heuristic was not used because it would
invalidate the technique of shuing the dictionary between solution attempts in order to
gather useful statistics.
The table in Figure 2 indicates the number of successful solution attempts (out of 100)
for each of the two methods on each of the 19 crossword frames. Dynamic backtracking is
more successful in six cases and less successful in none.
With regard to the number of nodes expanded by the two methods, consider the data
presented in Figure 3, where we graph the average number of backtracks needed by the
two methods.2 Although initially comparable, dynamic backtracking provides increasing
computational savings as the problems become more dicult. A somewhat broader set of
experiments is described in (Jonsson & Ginsberg, 1993) and leads to similar conclusions.
There are some examples in (Jonsson & Ginsberg, 1993) where dynamic backtracking
leads to performance degradation, however; a typical case appears in Figure 4.3 In this
1. I am indebted to David McAllester for these observations.
2. Only 17 points are shown because no point is plotted where backjumping was unable to solve the problem.
3. The worst performance degradation observed was a factor of approximately 4.

35

Ginsberg

Dynamic
Dynamic
Frame backtracking Backjumping Frame backtracking Backjumping
1
100
100
11
100
98
100
100
12
100
100
2
3
100
100
13
100
100
100
100
14
100
100
4
5
100
100
15
99
14
100
100
16
100
26
6
7
100
100
17
100
30
100
100
18
61
0
8
9
100
100
19
10
0
10
100
100
Figure 2: Number of problems solved successfully

400
r
r

dynamic 200
backtracking

r
r

r rr
rrr
rr

r
r

rr

200

400
600
backjumping

Figure 3: Number of backtracks needed

36

800

1000

Dynamic Backtracking

Region
1
s

,,
,
,
,
,
,
B
sa
s,
@@
A aaaa
aaa
aaa @@
aaa @
aa@a
@a@s

Region 2

Figure 4: A dicult problem for dynamic backtracking
gure, we rst color A, then B , then the countries in region 1, and then get stuck in region
2.
We now presumably backtrack directly to B , leaving the coloring of region 1 alone. But
this may well be a mistake { the colors in region 1 will restrict our choices for B , perhaps
making the subproblem consisting of A, B and region 2 more dicult than it might be. If
region 1 were easy to color, we would have been better o erasing it even though we didn't
need to.
This analysis suggests that dependency-directed backtracking should also fare worse
on those coloring problems where dynamic backtracking has trouble, and we are currently
extending the experiments of (Jonsson & Ginsberg, 1993) to conrm this. If this conjecture
is borne out, a variety of solutions come to mind. We might, for example, record how
many backtracks are made to a node such as B in the above gure, and then use this to
determine that exibility at B is more important than retaining the choices made in region
1. The diculty of nding a coloring for region 1 can also be determined from the number
of backtracks involved in the search.

7. Summary
7.1 Why it works
There are two separate ideas that we have exploited in the development of Algorithm 4.3
and the others leading up to it. The rst, and easily the most important, is the notion
that it is possible to modify variable order on the y in a way that allows us to retain the
results of earlier work when backtracking to a variable that was assigned a value early in
the search.
37

Ginsberg

This reordering should not be confused with the work of authors who have suggested a
dynamic choice among the variables that remain to be assigned values (Dechter & Meiri,
1989; Ginsberg et al., 1990; P. Purdom & Robertson, 1981; Zabih & McAllester, 1988); we
are instead reordering the variables that have been assigned values in the search thus far.
Another way to look at this idea is that we have found a way to \erase" the value given
to a variable directly as opposed to backtracking to it. This idea has also been explored
by Minton et.al. in (Minton, Johnston, Philips, & Laird, 1990) and by Selman et.al. in
(Selman, Levesque, & Mitchell, 1992); these authors also directly replace values assigned
to variables in satisability problems. Unfortunately, the heuristic repair method used is
incomplete because no dependency information is retained from one state of the problem
solver to the next.
There is a third way to view this as well. The space that we are examining is really a
graph, as opposed to a tree; we reach the same point by coloring Albania blue and then
Bulgaria red as if we color them in the opposite order. When we decide to backjump from a
particular node in the search space, we know that we need to back up until some particular
property of that node ceases to hold { and the key idea is that by backtracking along a
path other than the one by which the node was generated, we may be able to backtrack
only slightly when we would otherwise need to retreat a great deal. This observation is
interesting because it may well apply to problems other than csps. Unfortunately, it is not
clear how to guarantee completeness for a search that discovers a node using one path and
backtracks using another.
The other idea is less novel. As we have already remarked, our use of eliminating
explanations is quite similar to the use of nogoods in the atms community; the principal
dierence is that we attach the explanations to the variables they impact and drop them
when they cease to be relevant. (They might become relevant again later, of course.) This
avoids the prohibitive space requirements of systems that permanently cache the results of
their nogood calculations; this observation also may be extensible beyond the domain of
csps specically. Again, there are other ways to view this { Gashnig's notion of backmarking
(Gaschnig, 1979) records similar information about the reason that particular portions of a
search space are known not to contain solutions.

7.2 Future work
There are a variety of ways in which the techniques we have presented can be extended; in
this section, we sketch a few of the more obvious ones.
7.2.1 Backtracking to older culprits

One extension to our work involves lifting the restriction in Algorithm 4.3 that the variable
erased always be the most recently assigned member of the set E .
In general, we cannot do this while retaining the completeness of the search. Consider
the following example:
Imagine that our csp involves three variables, x, y and z , that can each take the value 0
or 1. Further, suppose that this csp has no solutions, in that after we pick any two values
for x and for y , we realize that there is no suitable choice for z .
38

Dynamic Backtracking

We begin by taking x = y = 0; when we realize the need to backtrack, we introduce the
nogood
x = 0  y 6= 0
(3)
and replace the value for y with y = 1.
This fails, too, but now suppose that we were to decide to backtrack to x, introducing
the new nogood
y = 1  x 6= 0
(4)
We change x's value to 1 and erase (3).
This also fails. We decide that y is the problem and change its value to 0, introducing
the nogood
x = 1  y 6= 1
but erasing (4). And when this fails, we are in danger of returning to x = y = 0, which we
eliminated at the beginning of the example. This loop may cause a modied version of the
dynamic backtracking algorithm to fail to terminate.
In terms of the proof of Theorem 4.2, the nogoods discovered already include information
about all assigned variables, so there is no dierence between (7) and (8). When we drop
(3) in favor of (4), we are no longer in a position to recover (3).
We can deal with this by placing conditions on the variables to which we choose to
backtrack; the conditions need to be dened so that the proof of Theorem 4.2 continues to
hold.4 Experimentation indicates that loops of the form we have described are extremely
rare in practice; it may also be possible to detect them directly and thereby retain more
substantial freedom in the choice of backtrack point.
This freedom of backtrack raises an important question that has not yet been addressed
in the literature: When backtracking to avoid a diculty of some sort, to where should one
backtrack?
Previous work has been constrained to backtrack no further than the most recent choice
that might impact the problem in question; any other decision would be both incomplete and
inecient. Although an extension of Algorithm 4.3 need not operate under this restriction,
we have given no indication of how the backtrack point should be selected.
There are several easily identied factors that can be expected to bear on this choice.
The rst is that there remains a reason to expect backtracking to chronologically recent
choices to be the most eective { these choices can be expected to have contributed to
the fewest eliminating explanations, and there is obvious advantage to retaining as many
eliminating explanations as possible from one point in the search to the next. It is possible, however, to simply identify that backtrack point that aects the fewest number of
eliminating explanations and to use that.
Alternatively, it might be important to backtrack to the choice point for which there
will be as many new choices as possible; as an extreme example, if there is a variable i
for which every value other than its current one has already been eliminated for other
reasons, backtracking to i is guaranteed to generate another backtrack immediately and
should probably be avoided if possible.
4. Another solution appears in (McAllester, 1993).

39

Ginsberg

Finally, there is some measure of the \directness" with which a variable bears on a
problem. If we are unable to nd a value for a particular variable i, it is probably sensible
to backtrack to a second variable that shares a constraint with i itself, as opposed to some
variable that aects i only indirectly.
How are these competing considerations to be weighed? I have no idea. But the framework we have developed is interesting because it allows us to work on this question. In
more basic terms, we can now \debug" partial solutions to csps directly, moving laterally
through the search space in an attempt to remain as close to a solution as possible. This
sort of lateral movement seems central to human solution of dicult search problems, and
it is encouraging to begin to understand it in a formal way.
7.2.2 Dependency pruning

It is often the case that when one value for a variable is eliminated while solving a csp,
others are eliminated as well. As an example, in solving a scheduling problem a particular
choice of time (say t = 16) may be eliminated for a task A because there then isn't enough
time between A and a subsequent task B ; in this case, all later times can obviously be
eliminated for A as well.
Formalizing this can be subtle; after all, a later time for A isn't uniformly worse than an
earlier time because there may be other tasks that need to precede A and making A later
makes that part of the schedule easier. It's the problem with B alone that forces A to be
earlier; once again, the analysis depends on the ability to maintain dependency information
as the search proceeds.
We can formalize this as follows. Given a csp (I; V; ), suppose that the value v has
been assigned to some i 2 I . Now we can construct a new csp (I 0; V 0 ; 0) involving the
remaining variables I 0 = I ,fig, where the new set V 0 need not mention the possible values
Vi for i, and where 0 is generated from  by modifying the constraints to indicate that i
has been assigned the value v . We also make the following denition:

Denition 7.1 Given a csp, suppose that i is a variable that has two possible values u and

v. We will say that v is stricter than u if every constraint in the csp induced by assigning
u to i is also a constraint in the csp induced by assigning i the value v.
The point, of course, is that if v is stricter than u is, there is no point to trying a
solution involving v once u has been eliminated. After all, nding such a solution would
involve satisfying all of the constraints in the v restriction, these are a superset of those in
the u restriction, and we were unable to satisfy the constraints in the u restriction originally.
The example with which we began this section now generalizes to the following:

Proposition 7.2 Suppose that a csp involves a set S of variables, and that we have a
partial solution that assigns values to the variables in some subset P  S . Suppose further
that if we extend this partial solution by assigning the value u to a variable i 62 P , there is
no further extension to a solution of the entire csp. Now consider the csp involving the
variables in S , P that is induced by the choices of values for variables in P . If v is stricter
than u as a choice of value for i in this problem, the original csp has no solution that both
assigns v to i and extends the given partial solution on P .
40

Dynamic Backtracking

This proposition isn't quite enough; in the earlier example, the choice of t = 17 for A
will not be stricter than t = 16 if there is any task that needs to be scheduled before A is.
We need to record the fact that B (which is no longer assigned a value) is the source of the
diculty. To do this, we need to augment the dependency information with which we are
working.
More precisely, when we say that a set of variables fxi g eliminates a value v for a variable
x, we mean that our search to date has allowed us to conclude that
(v1 = x1) ^    ^ (vk = xk )  v 6= x
where the vi are the current choices for the xi . We can obviously rewrite this as
(v1 = x1 ) ^    ^ (vk = xk ) ^ (v = x)  F
(5)
where F indicates that the csp in question has no solution.
Let's be more specic still, indicating in (5) exactly which csp has no solution:
(v1 = x1 ) ^    ^ (vk = xk ) ^ (v = x)  F (I )
(6)
where I is the set of variables in the complete csp.
Now we can address the example with which we began this section; the csp that is
known to fail in an expression such as (6) is not the entire problem, but only a subset of it.
In the example, we are considering, the subproblem involves only the two tasks A and B .
In general, we can augment our nogoods to include information about the subproblems on
which they fail, and then measure strictness with respect to these restricted subproblems
only. In our example, this will indeed allow us to eliminate t = 17 from consideration as a
possible time for A.
The additional information stored with the nogoods doubles their size (we have to store a
second subset of the variables in the csp), and the variable sets involved can be manipulated
easily as the search proceeds. The cost involved in employing this technique is therefore that
of the strictness computation. This may be substantial given the data structures currently
used to represent csps (which typically support the need to check if a constraint has been
violated but little more), but it seems likely that compile-time modications to these data
structures can be used to make the strictness question easier to answer. In scheduling
problems, preliminary experimental work shows that the idea is an important one; here,
too, there is much to be done.
The basic lesson of dynamic backtracking is that by retaining only those nogoods that
are still relevant given the partial solution with which we are working, the storage diculties
encountered by full dependency-directed methods can be alleviated. This is what makes
all of the ideas we have proposed possible { erasing values, selecting alternate backtrack
points, and dependency pruning. There are surely many other eective uses for a practical
dependency maintenance system as well.

Acknowledgements
This work has been supported by the Air Force Oce of Scientic Research under grant
number 92-0693 and by DARPA/Rome Labs under grant number F30602-91-C-0036. I
41

Ginsberg

would like to thank Rina Dechter, Mark Fox, Don Geddis, Will Harvey, Vipin Kumar,
Scott Roy and Narinder Singh for helpful comments on these ideas. Ari Jonsson and
David McAllester provided me invaluable assistance with the experimentation and proofs
respectively.

A. Proofs
Lemma 2.4 Let  be a complete elimination mechanism for a csp, let P be a partial solution

to this csp and let i 62 P . Now if P can be successfully extended to a complete solution after
assigning i the value v , then v 62 b(P; i).

Proof. Suppose otherwise, so that (v; E ) 2 (P; i). It follows directly from the completeness
of  that

E \ (P , P ) 6= 

a contradiction.

Lemma 2.6 At any point in the execution of Algorithm 2.5, if the last element of the partial

solution P assigns a value to the variable i, then the unexplored siblings of the current node
are those that assign to i the values in Vi , Ei .

Proof. We rst note that when we decide to assign a value to a new variable i in step 2

of the algorithm, we take Ei = b(P; i) so that Vi , Ei is the set of allowed values for this
variable. The lemma therefore holds in this case. The fact that it continues to hold through
each repetition of the loop in steps 3 and 4 is now a simple induction; at each point, we
add to Ei the node that has just failed as a possible value to be assigned to i.

Proposition 2.7 Algorithm 2.5 is equivalent to depth-rst search and therefore complete.
Proof. This is an easy consequence of the lemma. Partial solutions correspond to nodes

in the search space.
Lemma 3.2 Let P be a partial solution obtained during the execution of Algorithm 3.1, and
let i 2 P be a variable assigned a value by P . Now if P 0  P can be successfully extended
to a complete solution after assigning i the value v but (v; E ) 2 Ei , we must have

E \ (P , P 0) 6= 

Proof. As in the proof of Lemma 2.6, we show that no step of Algorithm 3.1 can cause
Lemma 3.2 to become false.
That the lemma holds after step 2, where the search is extended to consider a new
variable, is an immediate consequence of the assumption that the elimination mechanism
is complete.
In step 4, when we add (vj ; E , fj g) to the set of eliminating explanations for j , we
are simply recording the fact that the search for a solution with j set to vj failed because
we were unable to extend the solution to i. It is a consequence of the inductive hypothesis
that as long as no variable in E , fj g changes, this conclusion will remain valid.
Proposition 3.4 Backjumping is complete and always expands fewer nodes than does depthrst search.

42

Dynamic Backtracking

Proof. That fewer nodes are examined is clear; for completeness, it follows from Lemma
3.2 that the backtrack to some element of E in step 5 will always be necessary if a solution
is to be found.
Proposition 3.5 The amount of space needed by backjumping is o(i2v), where i = jI j is
the number of variables in the problem and v is the number of values for that variable with
the largest value set Vi .
Proof. The amount of space needed is dominated by the storage requirements of the elimination sets Ej ; there are i of these. Each one might refer to each of the possible values for
a particular variable j ; the space needed to store the reason that the value j is eliminated
is at most jI j, since the reason is simply a list of variables that have been assigned values.
There will never be two eliminating explanations for the same variable, since  is concise
and we never rebind a variable to a value that has been eliminated.
Theorem 4.2 Dynamic backtracking always terminates and is complete. It continues to

satisfy Proposition 3.5 and can be expected to expand fewer nodes than backjumping provided
that the goal nodes are distributed randomly in the search space.

Proof. There are four things we need to show: That dynamic backtracking needs o(i2v)

space, that it is complete, that it can be expected to expand fewer nodes than backjumping,
and that it terminates. We prove things in this order.
Space This is clear; the amount of space needed continues to be bounded by the structure
of the eliminating explanations.
Completeness This is also clear, since by Lemma 3.2, all of the eliminating explanations
retained in the algorithm are obviously still valid. The new explanations added in (2) are
also obviously correct, since they indicate that j cannot take the value vj as in backjumping
and that j also cannot take any values that are eliminated by the variables being backjumped
over.
Eciency To see that we expect to expand fewer nodes, suppose that the subproblem
involving only the variables being jumped over has s solutions in total, one of which is given
by the existing variable assignments. Assuming that the solutions are distributed randomly
in the search space, there is at least a 1=s chance that this particular solution leads to a
solution of the entire csp; if so, the reordered search { which considers this solution earlier
than the other { will save the expense of either assigning new values to these variables or
repeating the search that led to the existing choices. The reordered search will also benet
from the information in the nogoods that have been retained for the variables being jumped
over.
Termination This is the most dicult part of the proof.
As we work through the algorithm, we will be generating (and then discarding) a variety
of eliminating explanations. Suppose that e is such an explanation, saying that j cannot
take the value vj because of the values currently taken by the variables in some set eV .
We will denote the variables in eV by x1 ; . . . ; xk and their current values by v1; . . . ; vk . In
declarative terms, the eliminating explanation is telling us that
(x1 = v1) ^    ^ (xk = vk )  j 6= vj
43

(7)

Ginsberg

Dependency-directed backtracking would have us accumulate all of these nogoods; dynamic
backtracking allows us to drop any particular instance of (7) for which the antecedent is no
longer valid.
The reason that dependency-directed backtracking is guaranteed to terminate is that
the set of accumulated nogoods eliminates a monotonically increasing amount of the search
space. Each nogood eliminates a new section of the search space because the nature of the
search process is such that any node examined is consistent with the nogoods that have been
accumulated thus far; the process is monotonic because all nogoods are retained throughout
the search. These arguments cannot be applied to dynamic backtracking, since nogoods are
forgotten as the search proceeds. But we can make an analogous argument.
To do this, suppose that when we discover a nogood like (7), we record with it all of the
variables that precede the variable j in the partial order, together with the values currently
assigned to these variables. Thus an eliminating explanation becomes essentially a nogood
n of the form (7) together with a set S of variable/value pairs.
We now dene a mapping (n; S ) that changes the antecedent of (7) to include assumptions about all the variables bound in S , so that if S = fsi ; vi g,

(n; S ) = [(s1 = v1) ^    ^ (sl = vl)  j 6= vj ]

(8)

At any point in the execution of the algorithm, we denote by N the conjunction of the
modied nogoods of the form (8).
We now make the following claims:
1. For any eliminating explanation (n; S ), n j= (n; S ) so that (n; S ) is valid for the
problem at hand.
2. For any new eliminating explanation (n; S ), (n; S ) is not a consequence of N .
3. The deductive consequences of N grow monotonically as the dynamic backtracking
algorithm proceeds.
The theorem will follow from these three observations, since we will know that N is a valid
set of conclusions for our search problem and that we are once again making monotonic
progress toward eliminating the entire search space and concluding that the problem is
unsolvable.
That (n; S ) is a consequence of (n; S ) is clear, since the modication used to obtain
(8) from (7) involves strengthening that antecedent of (7). It is also clear that (n; S ) is
not a consequence of the nogoods already obtained, since we have added to the antecedent
only conditions that hold for the node of the search space currently under examination. If
(n; S ) were a consequence of the nogoods we had obtained thus far, this node would not
be being considered.
The last observation depends on the following lemma:

Lemma A.1 Suppose that x is a variable assigned a value by our partial solution and that

x appears in the antecedent of the nogood n in the pair (n; S ). Then if S 0 is the set of
variables assigned values no later than x, S 0  S .
44

Dynamic Backtracking

Proof. Consider a y 2 S 0, and suppose that it were not in S . We cannot have y = x, since

y would then be mentioned in the nogood n and therefore in S . So we can suppose that
y is actually assigned a value earlier than x is. Now when (n; S ) was added to the set of
eliminating explanations, it must have been the case that x was assigned a value (since it
appears in the antecedent of n) but that y was not. But we also know that there was a
later time when y was assigned a value but x was not, since y precedes x in the current
partial solution. This means that x must have changed value at some point after (n; S ) was
added to the set of eliminating explanations { but (n; S ) would have been deleted when this
happened. This contradiction completes the proof.
Returning to the proof the Theorem 4.2, suppose that we eventually drop (n; S ) from
our collection of nogoods and that when we do so, the new nogood being added is (n0; S 0). It
follows from the lemma that S 0  S . Since xi = vi is a clause in the antecedent of (n; S ), it
follows that (n0 ; S 0) will imply the negation of the antecedent of (n; S ) and will therefore
imply (n; S ) itself. Although we drop (n; S ) when we drop the nogood (n; S ), (n; S )
continues to be entailed by the modied set N , the consequences of which are seen to be
growing monotonically.

References

Bruynooghe, M. (1981). Solving combinatorial search problems by intelligent backtracking.
Information Processing Letters, 12 (1), 36{39.
de Kleer, J. (1986). An assumption-based truth maintenance system. Articial Intelligence,
28, 127{162.
Dechter, R., & Meiri, I. (1989). Experimental evaluation of preprocessing techniques in
constraint satisfaction problems. In Proceedings of the Eleventh International Joint
Conference on Articial Intelligence, pp. 271{277.
Gaschnig, J. (1979). Performance measurement and analysis of certain search algorithms.
Tech. rep. CMU-CS-79-124, Carnegie-Mellon University.
Ginsberg, M. L., Frank, M., Halpin, M. P., & Torrance, M. C. (1990). Search lessons learned
from crossword puzzles. In Proceedings of the Eighth National Conference on Articial
Intelligence, pp. 210{215.
Ginsberg, M. L., & Harvey, W. D. (1992). Iterative broadening. Articial Intelligence, 55,
367{383.
Jonsson, A. K., & Ginsberg, M. L. (1993). Experimenting with new systematic and nonsystematic search techniques. In Proceedings of the AAAI Spring Symposium on AI
and NP-Hard Problems Stanford, California.
McAllester, D. A. (1993). Partial order backtracking. Journal of Articial Intelligence
Research, 1. Submitted.
Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1990). Solving large-scale constraint satisfaction and scheduling problems using a heuristic repair method. In Proceedings of the Eighth National Conference on Articial Intelligence, pp. 17{24.
45

Ginsberg

P. Purdom, C. B., & Robertson, E. (1981). Backtracking with multi-level dynamic search
rearrangement. Acta Informatica, 15, 99{114.
Seidel, R. (1981). A new method for solving constraint satisfaction problems. In Proceedings
of the Seventh International Joint Conference on Articial Intelligence, pp. 338{342.
Selman, B., Levesque, H., & Mitchell, D. (1992). A new method for solving hard satisability
problems. In Proceedings of the Tenth National Conference on Articial Intelligence.
Smith, D. E., & Genesereth, M. R. (1985). Ordering conjunctive queries. Articial Intelligence, 26 (2), 171{215.
Stallman, R. M., & Sussman, G. J. (1977). Forward reasoning and dependency-directed
backtracking in a system for computer-aided circuit analysis. Articial Intelligence,
9 (2), 135{196.
Zabih, R. (1990). Some applications of graph bandwidth to constraint satisfaction problems.
In Proceedings of the Eighth National Conference on Articial Intelligence, pp. 46{51.
Zabih, R., & McAllester, D. A. (1988). A rearrangement search strategy for determining
propositional satisability. In Proceedings of the Seventh National Conference on
Articial Intelligence, pp. 155{160.

46

