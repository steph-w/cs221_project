Journal of Artificial Intelligence Research 49 (2014) 501-525

Submitted 09/13; published 03/14

Information-Theoretic Multi-view Domain Adaptation: A Theoretical
and Empirical Study
Pei Yang

YANGPEI @ SCUT. EDU . CN

South China University of Technology
Guangzhou, China

Wei Gao

WGAO @ QF. ORG . QA

Qatar Computing Research Institute
Qatar Foundation, Doha, Qatar

Abstract
Multi-view learning aims to improve classification performance by leveraging the consistency
among different views of data. The incorporation of multiple views was paid little attention in the
studies of domain adaptation, where the view consistency based on source data is largely violated in
the target domain due to the distribution gap between different domain data. In this paper, we leverage multiple views for cross-domain document classification. The central idea is to strengthen the
views‚Äô consistency on target data by identifying the associations of domain-specific features from
different domains. We present an Information-theoretic Multi-view Adaptation Model (IMAM)
using a multi-way clustering scheme, where word and link clusters can draw together seemingly unrelated features across domains, which boosts the consistency between document clusterings
that are based on the respective word and link views. Moreover, we demonstrate that IMAM can
always find the document clustering with the minimal disagreement rate to the overlap of viewbased clusterings. We provide both theoretical and empirical justifications of the proposed method.
Our experiments show that IMAM significantly outperforms traditional multi-view algorithm cotraining, the co-training-based adaptation algorithm CODA, the single-view transfer model CoCC
and the large-margin-based multi-view transfer model MVTL-LM.

1. Introduction
In many mission-critical applications of data mining, natural language processing and information
retrieval, it is typically expensive and time-consuming to obtain appropriate training data to learn
the needed models. For example, sentiment classifiers for online reviews need to work properly on
data of different types of products; search engines must provide consistent quality of service on the
Web data in the markets of different languages or verticals. However, the training data commonly
exist only in a limited number of domains. Collecting and annotating data for all different domains
would become practically prohibitive.
Domain adaptation is a task that utilizes the training data out of the domain (i.e., out-of-domain
or source domain) to effectively transform the relevant knowledge to the domain where the task is
performed (i.e., in-domain or target domain). Abundant labeled data may exist in a source domain
such as webpage data for training a general Web search ranker, but they are not readily available in
target domains such as the ranking systems for image search or music search. The out-of-domain
data are commonly drawn from some form of feature distribution that is different from that of the
in-domain counterpart. Bridging the domain gap is a challenging issue for the model learned from
source domain to be generalized well in target domain. For practical reasons, domain adaptation
c
2014
AI Access Foundation. All rights reserved.

YANG & G AO

is of great importance to many real-world applications, such as entity mention detection (DaumeÃÅ
III & Marcu, 2006), document classification (Sarinnapakorn & Kubat, 2007), sentiment classification (Blitzer, Dredze, & Pereira, 2007), part-of-speech tagging (Jiang & Zhai, 2007), and more
recently Web search ranking (Gao, Cai, Wong, & Zhou, 2010; Cai, Gao, Zhou, & Wong, 2011a,
2011b; Gao & Yang, 2014).
Many types of data can be represented by multiple independent sets of features, reflecting the
different views of the data. For example, in document classification, Web document features consist
of not only the word-based features but also the features based on link structures among the documents (Blum & Mitchell, 1998); in Web search, document rankers accept both query-dependent
features (e.g., tfidf, BM25, language-modeling IR scores, etc.) as well as query-independent features (e.g., page rank, inlink/outlink numbers, url click count, etc.) (Gao, Blitzer, Zhou, & Wong,
2009). Traditionally, the learning scheme called multi-view learning aims to improve classifiers by
leveraging the redundancy and consistency among these distinct views (Blum & Mitchell, 1998;
RuÃàping & Scheffer, 2005; Abney, 2002). Existing methods of multi-view learning were designed
for the data from a single domain, which assumes that either view alone can predict the in-domain
class consistently and accurately. However, this view-consistency assumption is largely violated in
the setting of domain adaptation where training and test data are drawn from different distributions
(which is empirically justified in the experiment section). In such a case, domain adaptation with
multiple views of data needs to be investigated carefully.
Little research has been done on multi-view domain adaptation in the literature. Zhang, He,
Liu, Si, and Lawrence (2011) proposed an instance-based multi-view transfer learning approach
that integrates the loss of cross-domain classification and multi-view consistency in a large margin
framework. However, the instance-level approach assumes that some useful source training examples can be identified and reused to train the target model. It cannot mine the relationships at feature
level such as the correlation between source-specific and target-specific features, and may perform
poorly since target-specific features are the key for good adaptation performance (Blitzer, Kakade,
& Foster, 2011).
In this work, we present an Information-theoretical Multi-view Adaptation Model (IMAM) that
combines the paradigms of multi-view learning and domain adaptation based on an co-clustering
framework (Dhillon, Mallela, & Modha, 2003) and aims to transfer knowledge across domains in
multiple subspaces of features complementarily. IMAM exploits a multi-way-clustering-based classification scheme to simultaneously cluster documents, words and links into their respective clusters.
The word and link clusterings can automatically associate the specific features from different domains that seemingly may not be directly correlated. Such correlations can bridge the domain gap
and then enhance the consistency of distinct views when clustering (i.e., classifying) the target data.
The more consistent the views, the better the document clustering, and then the better the word and
link clustering, which creates a cycle of positive feedback and gradually improves the adaptation
performance. In essence, the enhanced consistency of views helps to bridge the domain gap (i.e.,
by finding more cross-domain feature correlations), and vice versa. We also provide theoretical justifications for the proposed approach regarding the objective, convergence property and the optimal
solution. Our experimental results demonstrate that IMAM significantly outperforms the state-ofthe-art baselines including the traditional single-domain multi-view algorithm co-training (Blum &
Mitchell, 1998), the co-training-based domain adaptation algorithm CODA (Chen, Weinberger, &
Blitzer, 2011), the single-view transfer learning algorithm CoCC (Dai, Xue, Yang, & Yu, 2007a)
and the instance-level multi-view transfer learning algorithm MVTL-LM (Zhang et al., 2011).
502

I NFORMATION -T HEORETIC M ULTI - VIEW D OMAIN A DAPTATION

The rest of the paper is organized as follows: Section 2 reviews the related work; Section 3
describes the background concepts on which we build our model; Section 4 presents the proposed
the IMAM model and the corresponding algorithm; Section 5 analyses the realization of consistency
between distinct views in our model; Section 6 discusses the experiments and results; Finally, we
conclude in Section 7 with prospects on future work.

2. Literature Review
Domain adaptation assumes that multiple tasks can benefit from certain structures of data shared between different distributions. Existing methods can be divided into instance-based approach (Jiang
& Zhai, 2007; Dai, Yang, Xue, & Yu, 2007b), feature-based approach (Blitzer et al., 2007; Dai et
al., 2007a) and parameter-based approach (Dayanik, Lewis, Madigan, Menkov, & Genkin, 2006).
Pan and Yang (2010) presented a comprehensive survey of transfer learning which described domain adaptation as a sub-category of transfer learning. We would not give comprehensive review
on domain adaptation for this reason. Interested readers may refer to the survey paper (Pan & Yang,
2010) for details.
The work closely related to ours was done by Dai et al. (2007a), where they proposed a coclustering-based classification (CoCC) algorithm to learn from the out-of-domain data and apply
the learned classifier to the in-domain task. CoCC extended the information-theoretic co-clustering
method proposed by Dhillon et al. (2003), where in-domain constraints were added to word clusters
to provide a class structure and partial categorization knowledge. However, CoCC is a single-view
algorithm which cannot leverage the complementary nature of multiple views. Our framework is an
extension from single-view CoCC, and our algorithm is focused on strengthening the consistency of
predictions between distinct views across two domains, which is considered the key to the success
of multi-view domain adaptation.
Multi-view learning has been studied extensively under single-domain setting. Co-training is
the first multi-view algorithm, which trained a learner on each view of labeled examples and then
let each learner label the unlabeled examples that receive the highest confidence (Blum & Mitchell,
1998). It was proved that the two independent yet consistent views can be used to learn a concept in the PAC framework based on few labeled and many unlabeled examples. Many extensions
were proposed following the idea of co-training. Collins and Singer (1999) introduced an explicit
objective function that measures the compatibility of learned hypotheses and used boosting to optimize the function. Dasgupta, Littman, and McAllester (2001) provided PAC-like guarantees for
co-training providing an upper bound for the error of classifiers learned from two views. Abney
(2002) relaxed the view independence assumption and suggested that there may be an underlying
principle which gives rise to a family of new methods: the disagreement rate of two independent
hypotheses upper bounds the error rate of either hypothesis. Sridharan and Kakade (2008) proposed
an information-theoretic framework for multi-view learning. They showed how to derive incompatibility functions for certain loss functions of interest so that minimizing this incompatibility over
unlabeled data helps reduce expected loss on the test data. Nevertheless, multi-view learning generally is not effective for domain adaptation since they treat the domain divergence indiscriminately,
which is empirically justified in our experiments (see Experiments and Results section).
Multi-view adaptation is not well studied in the literature. DaumeÃÅ III, Kumar, and Saha (2010)
proposed a co-regularization based approach (EA++) to semi-supervised domain adaptation. EA++
builds on the feature augmentation and harnesses unlabeled data in target domain to assist the trans503

YANG & G AO

fer of information from source to target. Different from EA++ that aims to make the different
hypotheses learned from different distributions agree on unlabeled data, we consider a true multiview setting and try to make the hypotheses learned from different views consistent with each other.
Furthermore, EA++ builds the classifier on the transformed feature space via feature augmentation,
while our proposed method learns the hypotheses on the mapped feature space via multi-way clustering. Chen et al. (2011) proposed CODA for adaptation based on co-training (Blum & Mitchell,
1998), which is however a pseudo multi-view algorithm for the original data that have only one
view. In order to apply CODA for the real multi-view data, the views have to be first concatenated and then split into multiple pseudo-views. Therefore, it is not suitable nor natural for the true
multi-view case as ours. He and Lawrence (2011) proposed a graph-based learning framework to
tackle the problems with both feature heterogeneity and task heterogeneity. Their algorithm is a
transductive learning approach. Zhang and Huan (2012) proposed an inductive multi-view learning
algorithm for multiple related tasks. They used co-regularization to obtain view-based classifiers that agree with each other on unlabeled data and ensure that the learned functions are similar
in each view across different tasks. Both of these two algorithms were designed for multi-task
learning rather than transfer learning. Zhang et al. (2011) proposed an instance-level multi-view
transfer algorithm that integrates classification loss and view consistency terms based on large margin framework. The instance-level approach assumes that some similar source training examples
can be identified and reused to train the target model. However, the performance of instance-based
approach is generally poor when new target features lack support from source data (Blitzer et al.,
2011). We focus on feature-level multi-view adaptation, where adaptation takes place in the multiple transformed feature spaces simultaneously and complementarily. To the best of our knowledge,
there are no existing work focused on the feature-level multi-view domain adaptation except for
our preliminary study recently published (Yang, Gao, Tan, & Wong, 2012). This paper extends the
work of Yang et al. (2012) substantially by providing the detailed algorithm, theoretical justification
and comprehensive empirical evaluation, which were not specifically presented in the preliminary
version.

3. Background Concepts
Our multi-view approach is based on the co-clustering (Dhillon et al., 2003) and co-clustering-based
classification (CoCC) model (Dai et al., 2007a) for building the underlying clusters of each view.
Before going to the details of our model, we will briefly describe some background concepts and
lemmas related to the co-clustering techniques in this section.
Mutual information is a fundamental measure to quantify the mutual dependence of two random variables. Let I(X, Y ) be the mutual information of variables X and Y , which is defined as
P P
p(x,y)
I(X, Y ) = x y p(x, y)log p(x)p(y)
(Cover & Thomas, 1991). Mutual information can also be
expressed in the form of Kullback-Leibler (KL) divergence, i.e., I(X, Y ) = D (p(x, y)||p(x)p(y)).
Given two discrete random variables X and Y with joint probability distribution p(x, y), co-clustering
approach (Dhillon et al., 2003) aims to simultaneously cluster X into disjoint clusters XÃÇ, and Y into
disjoint clusters YÃÇ . The quality of co-clustering is measured by the resulting loss based on mutual
information:
I(X, Y ) ‚àí I(XÃÇ, YÃÇ )
For the given X and Y , since I(X, Y ) is fixed, minimizing the above equation is equivalent to
maximizing I(XÃÇ, YÃÇ ).
504

I NFORMATION -T HEORETIC M ULTI - VIEW D OMAIN A DAPTATION

p(x) p(y)
For the simplicity of expression, a joint distribution q(x, y) = p(xÃÇ, yÃÇ) p(xÃÇ)
p(yÃÇ) is defined to

approximate the probability p(x, y) under co-clustering (XÃÇ, YÃÇ ). Note that the distribution q(x, y)
preserves the marginals of p(x, y). That is, for any x ‚àà xÃÇ, y ‚àà yÃÇ, we have q(x) = p(x) because
q(x) =

X

q(x, y) =

y

XX
yÃÇ

p(xÃÇ, yÃÇ)

y‚ààyÃÇ

p(x) p(y) X
p(x)
=
p(xÃÇ, yÃÇ)
= p(x).
p(xÃÇ) p(yÃÇ)
p(xÃÇ)
yÃÇ

Likewise we have q(y) = p(y).
Dhillon et al. (2003) proved that the loss in mutual information between pre- and post-clustering
can be reformulated as the KL-divergence between p(x, y) and an approximation q(x, y), which is
given as the following lemma:
Lemma 3.1. For a fixed co-clustering (XÃÇ, YÃÇ ), the loss in mutual information can be expressed as
I(X, Y ) ‚àí I(XÃÇ, YÃÇ ) = D (p(x, y)||q(x, y)) ,
where D(¬∑||¬∑) is the KL-divergence, and q(x, y) is the distribution of the form
q(x, y) = p(xÃÇ, yÃÇ)

p(x) p(y)
,
p(xÃÇ) p(yÃÇ)

where x ‚àà xÃÇ and y ‚àà yÃÇ.
For completeness and clarity, we reproduce the illustrative example given by Dhillon et al.
(2003) for interpreting Lemma 3.1. Consider the joint distribution of (X, Y ) represented by a 6*6
matrix below:
Ô£∂
Ô£´
.05 .05 .05 0
0
0
Ô£¨ .05 .05 .05 0
0
0 Ô£∑
Ô£∑
Ô£¨
Ô£∑
Ô£¨ 0
0
0
.05
.05
.05
Ô£∑
Ô£¨
Ô£¨ 0
0
0 .05 .05 .05 Ô£∑
Ô£∑
Ô£¨
Ô£≠ .04 .04 0 .04 .04 .04 Ô£∏
.04 .04 .04

0

.04 .04

It follows naturally that the rows are divided into three clusters: xÃÇ1 = {x1 , x2 }, xÃÇ2 = {x3 , x4 } and
xÃÇ3 = {x5 , x6 }, and the columns clustering is: yÃÇ1 = {y1 , y2 , y3 }, yÃÇ2 = {y4 , y5 , y6 }. The resulting
joint distribution of (XÃÇ, YÃÇ ) is given by:
Ô£´

Ô£∂
.3 0
Ô£≠ 0 .3 Ô£∏
.2 .2
It can be verified that the mutual information loss in this co-clustering is .0957, which is the minimum among all the possible co-clusterings.

4. Information-Theoretic Multi-view Adaptation Model (IMAM)
We will first introduce the motivation, and then will describe our model and its algorithm.
505

YANG & G AO

4.1 Motivation
Traditional multi-view learning such as co-training framework (Blum & Mitchell, 1998) employs
two basic assumptions: (1) the target functions in each view agree on the labels of most examples
(consistency assumption); and (2) the views are independent given the class label (independence
assumption). The first assumption reduces the complex learning problem to the search of compatible
functions; and the second assumption allows the model to achieve high-confidence predictions since
it becomes unlikely for consistent classifiers trained on independent views to agree on an incorrect
label.
Considering the training and test data drawn from different distributions, nonetheless, the consistency assumption is mostly violated because the distinct views agreeing on the labels of source
data are unnecessarily compatible on the labels of target examples due to the domain gap. Therefore, it can be expected that traditional multi-view learning framework will not work effectively
across different domains, which can be empirically justified in the comparison experiments. Hence,
how to enhance the consistency among multiple views and bridge the gap among different domains
simultaneously is the key issue for the multi-view domain adaptation approach to succeed.
Without loss of generality, we will focus on cross-domain document classification in this paper
where the document representation consists of two views such as word and link. Given text documents from two domains, there would be a set of common word features available on both domains,
considered as domain-independent features, and the remaining words would be regarded as either
source-specific or target-specific features. The same taxonomy regarding domain-independent and
domain-specific features also apply to the inter-document links, e.g., the hyperlinks or citations
features.
From a single view‚Äôs perspective, source-specific and target-specific features can be drawn together by mining their co-occurrence with domain-independent features. IMAM exploits multiway clustering to correlate those seemingly unrelated domain-specific features via the domainindependent features which act as a bridge. Such correlations help bridge the domain gap and facilitate the adaptation (Dai et al., 2007a). From multiple view‚Äôs perspective, if the word and link clusters
constructed over the two domains are of high quality, the corresponding target document clustering
resulted from either view can be subsequently improved due to the effect of co-clustering (Dhillon
et al., 2003). It can be expected that the predictive power of distinct views on the target data tends
to become more concordant and approaches to the optimal solution. Our model leverages complementary cooperation between different views to yield better adaptation performance.
Next, we will present some representational preliminaries and the objective function of our
multi-view adaptation model, and then an iterative two-phase algorithm is presented to optimize the
objective.
4.2 The Graphical Representation
Let DS be the training documents of source domain and DT be the unlabeled documents of the
target domain. The source and target data are assumed to draw from different feature spaces where
the i.i.d. assumption no longer holds. Some features are defined in source or target domain only
while some others are defined in both domains. We simply expand the feature space to include all
features of both domains where the missing features in either domain are replenished as 0. Let W be
the vocabulary of the entire document collection D = DS ‚à™ DT . Each d ‚àà D can be represented by
a bag-of-words set {w|w ‚àà d ‚àß w ‚àà W }. Let L be the set of all links (hyperlinks or citations) in the
506

I NFORMATION -T HEORETIC M ULTI - VIEW D OMAIN A DAPTATION

collection. Each d can be also represented by a bag-of-links set {l|l ‚àà d‚àßl ‚àà L}. D and L naturally
form independent sets of features respectively corresponding to word view and link view. Let C
denote the set of class labels shared between the two domains. Each source document ds ‚àà DS is
labeled with a unique class label c ‚àà C. Our objective is to assign the appropriate class label to
target document dt ‚àà DT as accurately as possible. Note that we assume there is no labeled data
available in target domain, which follows the transductive learning scheme. Transductive approach
is a typical domain adaptation setting, which is more general and widely applicable to different
scenarios including the inductive setting where only a small number of labeled target data exist.
Figure 1 shows the graphical multi-view adaption model representation, where DÃÇ, WÃÇ and LÃÇ are
the respective clusterings of documents, words and links. Additionally, the multi-way clusterings
mutually constrain each other and are subject to various explicit and implicit association relationships. Explicit association includes two types of constraints: (1) Document clustering is constrained
by word clustering and link clustering; (2) Word or link clustering is constrained by document clustering and class labels. Implicit association means that the class label knowledge is transferred from
source documents to target documents through word and link clusters.

Figure 1: The graphical representation of the proposed multi-view adaptation model.
Our model incorporates such a multi-way clustering scheme that simultaneously clusters documents, words and links. The clustering functions are defined as CD (d) = dÀÜ for documents,
ÀÜ wÃÇ and ÀÜl represent the corresponding
CW (w) = wÃÇ for words and CL (l) = ÀÜl for links, where d,
clusters.
4.3 Preliminaries ‚Äì Co-clustering-Based Classification
Dai et al. (2007a) proposed a co-clustering-based classification framework, namely CoCC, to learn
a classifier from source-domain documents and then use it to classify target-domain documents. In
their approach, co-clustering was leveraged as a bridge to transfer the knowledge from source to
target.
Co-clustering aims to simultaneously cluster target documents DT into clusters DÃÇT and words
W into clusters WÃÇ . Since the problem is to classify target-domain documents, the key is to make use
of the knowledge about classes in the data of source domain for the co-clustering process. Such kind
of correlation between the source document class knowledge and the target document clustering can
be established by considering their respective relationship with the word clusters as an intermediary.
A good word clustering should minimize the loss in mutual information between class labels and
words before and after clustering for the source data, and meanwhile it should minimize the same
507

YANG & G AO

loss between documents and words for the target data. Therefore, the loss function of CoCC (Dai et
al., 2007a) is formulated as follows:
h
i
I(DT , W ) ‚àí I(DÃÇT , WÃÇ ) + Œª I(C, W ) ‚àí I(C, WÃÇ )
where Œª is a trade-off parameter that balances the effect to word clusters from co-clustering and
word clustering.
4.4 Objective Function
We extend the information-theoretic framework for co-clustering (Dhillon et al., 2003) and coclustering-based classification (Dai et al., 2007a) by incorporating the loss terms from multiple
views. Co-clustering aims to minimize the loss of mutual information between pre- and postclustering with respect to a pair of clustering variables, such as documents and words. The objective of our Information-theoretic Multi-view Adaptation Model (IMAM) is to minimize the loss
by trading off different views:
Œò = Œ±ŒòW + (1 ‚àí Œ±)ŒòL
(1)
where
h
i
ŒòW = I(DT , W ) ‚àí I(DÃÇT , WÃÇ ) + Œª I(C, W ) ‚àí I(C, WÃÇ )
h
i
ŒòL = I(DT , L) ‚àí I(DÃÇT , LÃÇ) + Œª I(C, L) ‚àí I(C, LÃÇ) .

(2)
(3)

ŒòW and ŒòL are the loss terms based on word view and link view, respectively, and Œ± is the trade-off
coefficient. In Eq. 2, I(DT , W ) ‚àí I(DÃÇT , WÃÇ ) measures the loss of word-document co-clustering,
I(C, W ) ‚àí I(C, WÃÇ ) measures the loss between vocabulary and class labels, and Œª is the weight of
the loss for word clustering. Class labels act as indirect constraints added on vocabulary via source
documents and are propagated to target documents through co-clustering. In Eq. 3, we have the
similar loss term for the link view. When Œ± = 1, the function relies on text information only, which
reduces to CoCC (Dai et al., 2007a). But unlike CoCC (Dai et al., 2007a), we aim to learn the
cross-domain classifiers for multi-view data.
It is worth noting that by substituting Eq. 2 and 3 in Eq. 1 and ignoring the constant terms, we
can reformulate the problem as the following maximization, which is kind of easier to interpret:
h
i
Œ±I(DÃÇT , WÃÇ ) + (1 ‚àí Œ±)I(DÃÇT , LÃÇ) + Œª Œ±I(C, WÃÇ ) + (1 ‚àí Œ±)I(C, LÃÇ)
where the first two terms enforce that the view consistency on DÃÇT , which means that the document
clusters DÃÇT should preserve their mutual information with both words and links as much as possible,
and the last two terms enforce transfer of information from source to target via agreement with labels
C, which indicates that the source label knowledge should be maximally preserved by both word
and link clusters.
Given the multi-view data data from different domains, the central problem would be how different views could cooperate each other to form consistent target class output in the scenario where
different domain data follow different distributions. This is challenging because the view consistency based on source data is largely violated in the target domain due to the domain gap. To tackle
this problem, we aim to simultaneously enhance the consistency among multiple views and bridge
508

I NFORMATION -T HEORETIC M ULTI - VIEW D OMAIN A DAPTATION

the gap among different domains in a unified objective. IMAM exploits multi-way clustering to enrich common words (and links) by drawing together those seemingly unrelated source-specific and
target-specific words (and links). Such correlations bridge the domain gap and facilitate the adaptation process. On the other hand, IMAM takes the weighted combination of view-based loss of
mutual information. As pointed out in Section 5 (Consistency of Multiple Views), the optimal document clustering is to optimize the weighted sum of word-view and link-view document clustering
functions, and try to minimize the disagreement between different views. Moreover, the multi-way
clustering scheme imposes the constraints on all of document and word/link clustering, which can
make them mutually benefit from each other. In summary, IMAM uses such a boosting procedure
to enhance the view consistency and bridge domain gap simultaneously, and can be expected to
improve the adaptation performance on the multi-view data.
4.5 IMAM Algorithm
Based on q(x, y) defined in Section 3, we can also define the corresponding conditional distribution q(x|yÃÇ) = q(x,y)
p(y) under co-clustering. For any x ‚àà xÃÇ, we can easily prove that q(x|yÃÇ) =
p(x|xÃÇ)p(xÃÇ|yÃÇ). Therefore, for any w ‚àà wÃÇ, l ‚àà ÀÜl, d ‚àà dÀÜ and c ‚àà C, we can calculate a set of
ÀÜ q(d|wÃÇ), q(l|d),
ÀÜ q(d|ÀÜl), q(c|wÃÇ), q(c|ÀÜl).
conditional distributions including q(w|d),
The objective of Eq. 1 is hard to optimize directly because it contains mutual information of
two clusterings, which is a combinatorial optimization problem. Therefore, we transform it to the
form of KL-divergence between two conditional distributions in Lemma 4.1 in order to facilitate
our search for the optimal value. Let D(p(x|y)||q(x|y)) denote KL-divergence between p(x|y) and
q(x|y), which is defined as
D(p(x|y)||q(x|y)) =

X

p(x|y)log

x

p(x|y)
.
q(x|y)

We have the following lemma, and using the similar technique as in Dhillon et al. (2003), we provide
its proof in the Appendix A.
Lemma 4.1 (Objective functions). Equation 1 can be turned into the form of alternate minimization
between two objectives:
(i) For document clustering while keeping word and link clustering fixed, we minimize
X
ÀÜ + œÜC (WÃÇ , LÃÇ)
Œò=
p(d)œÜD (d, d)
d

where œÜC (WÃÇ , LÃÇ) is a constant1 and
ÀÜ = Œ±D(p(w|d)||q(w|d))
ÀÜ + (1 ‚àí Œ±)D(p(l|d)||q(l|d)).
ÀÜ
œÜD (d, d)
(ii) For word and link clustering while keeping document clustering fixed, we minimize
X
X
Œò=Œ±
p(w)œÜW (w, wÃÇ) + (1 ‚àí Œ±)
p(l)œÜL (l, ÀÜl)
w

l

h
i
1. We can prove that œÜC (WÃÇ , LÃÇ) = Œª Œ±(I(C, W ) ‚àí I(C, WÃÇ )) + (1 ‚àí Œ±)(I(C, L) ‚àí I(C, LÃÇ)) , where MI between
class label and other variables is constant.

509

YANG & G AO

Algorithm 1 Algorithm for IMAM
Input:
Document-term matrices DS √ó W and DT √ó W ;
Document-link matrices DS √ó L and DT √ó L;
Class label c ‚àà C assigned to each doc d ‚àà DS ;
# of document clusters (i.e., # of classes);
Output:
Class label assigned to each document d ‚àà DT ;
(0)
(0)
1: Set t = 0. Initialize document clustering CD using NBC. Initialize word clustering CW and link clus(0)
tering CL randomly;
ÀÜ q (0) (l|d),
ÀÜ q (0) (d|wÃÇ), q (0) (d|ÀÜl), q (0) (c|wÃÇ), q (0) (c|ÀÜl);
2: Initialize distributions q (0) (w|d),
3: repeat
4:
Document clustering: For each d, find its new cluster index using Eq. 4;
5:
Keep q (t+1) (c|wÃÇ) = q (t) (c|wÃÇ) and q (t+1) (c|ÀÜl) = q (t) (c|ÀÜl);
ÀÜ q (t+1) (l|d),
ÀÜ q (t+1) (d|wÃÇ), q (t+1) (d|ÀÜl);
Update q (t+1) (w|d),
6:
Word clustering: For each word w, find its new cluster index using Eq. 5;
Link clustering: For each link l, find its new cluster index using Eq. 6;
ÀÜ q (t+2) (l|d),
ÀÜ q (t+2) (d|wÃÇ), q (t+2) (d|ÀÜl), q (t+2) (c|wÃÇ) and q (t+2) (c|ÀÜl);
7:
Update q (t+2) (w|d),
8:
t = t + 2;
9: until no document‚Äôs cluster index needs to adjust
10: for each unlabeled d ‚àà DT do
11:
Assign d the class label based on Eq. 7;
12: end for

where for any feature v (e.g., w and l) in feature set V (e.g., W and L)
œÜV (v, vÃÇ) = D(p(d|v)||q(d|vÃÇ)) + ŒªD(p(c|v)||q(c|vÃÇ)).
The intuition of the optimization is that given the document-word and document-link matrices,
let us simultaneously re-order documents in the two matrices such that all documents mapping to the
first document cluster are arranged first, followed by all documents mapping to the second cluster,
and so on. A good document clustering tries to ensure the consistency between different views.
Next, let us simultaneously re-order words and links in document-word and document-link matrices
in a similar way. A good word (or link) clustering draws indirectly related domain-specific words
(or links) together since both of them may co-occur with domain-independent words (or links) in
the documents. The document-word-link interaction helps finding an optimal multi-way clustering.
Lemma 4.1 allows us to alternately reorder either documents or both words and links, which is
shown as Algorithm 1, in such a way that the mutual information loss decreases monotonically (see
Lemma 4.2).
4.5.1 A LGORITHM
(0)

(0)

(0)

The algorithm starts with an initial multi-clustering (CD , CW , CL ) and iteratively refines it
until the algorithm converges. The algorithm uses a two-phase iterative procedure to minimize
the loss, in which it first searches for the best document clustering while keeping word and link
clustering unchanged, and then clusters words and links while document clustering remains fixed.
In step 1, Naive Bayes classifier (NBC) is trained on source data DS and used to predict the
class of target data DT , which produces the initial document clustering of entire D. Note that the
510

I NFORMATION -T HEORETIC M ULTI - VIEW D OMAIN A DAPTATION

cluster index of source documents is fixed with class labels. Thus, the allocation of each target
document to certain cluster also means that the document is assigned with the corresponding class
label. It is worth noting since the objective Eq. 1 is non-convex, it will be somewhat sensitive to the
initialization. Hence, instead of random initialization, we use NBC to generate the initial document
clusterings so as to keep it start from some good points.
Step 4 updates the cluster index for each d:
h
i
(t+1)
ÀÜ + (1 ‚àí Œ±)D(p(l|d)||q (t) (l|d))
ÀÜ
CD (d) = arg min Œ±D(p(w|d)||q (t) (w|d))
(4)
dÀÜ

Step 6 updates the cluster index of each w:
h
i
(t+2)
CW (w) = arg min D(p(d|w)||q (t+1) (d|wÃÇ)) + ŒªD(p(c|w)||q (t+1) (c|wÃÇ))

(5)

wÃÇ

and then updates the cluster index of each l:
h
i
(t+2)
CL (l) = arg min D(p(d|l)||q (t+1) (d|ÀÜl)) + ŒªD(p(c|l)||q (t+1) (c|ÀÜl))

(6)

lÃÇ

Note that Algorithm 1 does not separately update the membership of each word and link since
there are implicit association relationships between the word clustering and link clustering via document clustering. The document clustering acts as the bridge to make word clustering and link
clustering mutually affect each other.
After finishing the multi-way clustering procedure, we assign each target document d ‚àà DT
with the class label predicted by
h
i
ÀÜ + (1 ‚àí Œ±)D(p(l|c)||q(l|d))
ÀÜ
c‚àó = arg min Œ±D(p(w|c)||q(w|d))
(7)
c‚ààC

Lemma 4.2 below guarantees the convergence of the algorithm, and its proof is given in the
Appendix B by borrowing the similar technique from Dhillon et al. (2003). Note that finding a
global minimum for multi-way clustering is NP-hard, and IMAM uses a greedy approach to find a
local minimum, which does not guarantee the global optimum. But usually we can run experiments
multiple times and then average over the performance of different runs.
Lemma 4.2 (Convergence). IMAM monotonically reduces the objective given in Equation 1. That
is,
Œò(t) ‚â• Œò(t+1)
Œò(t+1) ‚â• Œò(t+2)
where t = 0, 2, 4, . . .

5. Consistency of Multiple Views
In this section, we present how the consistency of document clustering on target data could be enhanced among multiple views, which is the key issue of our multi-view adaptation method. We
511

YANG & G AO

particularly discuss the relationship between the disagreement rate of views and the optimal document clustering function.
(t+1)
In each iteration of Algorithm 1, the optimal document clustering function CD
(see Eq. 4)
is to minimize the weighted sum of KL-divergences used in optimal word-view and link-view document clustering functions as shown above. The optimal word-view clustering functions can be
denoted as follows:
(t+1)
ÀÜ
CDW (d) = arg min D(p(w|d)||q (t) (w|d))
(8)
dÀÜ

and similarly the link-view function as
(t+1)

ÀÜ
CDL (d) = arg min D(p(l|d)||q (t) (l|d))

(9)

dÀÜ

(t+1)

(t+1)

Our central idea is that the document clusterings CDW and CDL based on the two views are
drawn closer in each iteration due to the word and link clusterings (Eq. 5 and 6) that bring together
(t+1)
seemingly unrelated source-specific and target-specific features. Meanwhile, CD
combines the
two views and reallocates the documents so that it maintains the consistency with the view-based
clusterings as much as possible.
5.1 Disagreement Rate of Views
ÀÜ dÀÜ ‚àà DÃÇ} is the set of all document clustering functions where the
Suppose ‚Ñ¶ = {Fi |Fi (d) = d,
number of clusters is fixed. For any document, a consistency indicator function with respect to any
two clustering functions can be defined as follows (Round indicator t is omitted for simplicity):
Definition 1 (Indicator function) For any d ‚àà D, and any Fi ‚àà ‚Ñ¶, Fj ‚àà ‚Ñ¶

1, if Fi (d) = Fj (d);
Œ¥Fi ,Fj (d) =
0, otherwise
Then we define the disagreement rate between two view-based clustering functions:
Definition 2 (View disagreement rate) For any Fi ‚àà ‚Ñ¶ and Fj ‚àà ‚Ñ¶
P
Œ¥F ,F (d)
Œ∑(Fi , Fj ) = 1 ‚àí d‚ààD i j
|D|

(10)

Obviously, Œ∑(CDW , CDL ) denotes the disagreement rate between the word-view and link-view
clustering functions. Abney (2002) suggests that the disagreement rate of two independent hypotheses upper-bounds the error rate of either hypothesis. By minimizing the disagreement rate on
unlabeled data, the error rate of each view can be minimized (so does the overall error). However,
the disagreement rate function is not continuous nor convex, which is difficult to optimize directly2 .
Alternatively, we minimize the mutual information loss in Eq. 1 as a surrogate for the disagreement
rate function. We believe that the mutual information loss is a good surrogate because, as discussed
in Section 4.4, Eq. 1 aims to enhance the view consistency, which is equivalent to minimizing the
disagreement rate of views. Moreover, we show empirically that by optimizing Eq. 1 the disagreement rate Œ∑(CDW , CDL ) is indeed monotonically decreased with the iterations in our experiments
(see Section 6).
2. Abney (2002) used a greedy approach.

512

I NFORMATION -T HEORETIC M ULTI - VIEW D OMAIN A DAPTATION

5.2 View Combination
Note that in practice the view-based document clusterings in Eq. 8 and Eq. 9 are not computed
explicitly. Instead, Eq. 4 directly optimizes the view combination and produces the document clustering. Therefore, it is necessary to disclose how consistent the combined view-based clustering
could be with the individual view-based clusterings.
For any Fi ‚àà ‚Ñ¶, we obtain the disagreement rate Œ∑(Fi , CDW ‚à© CDL ), where CDW ‚à© CDL denotes the clustering resulting from the overlap of the individual view-based clusterings. Note that
the co-training style algorithms usually assume that the multiple views are redundant. Thus, the
intersection of them would not be empty. We obtain Lemma 5.1 as below, and its proof is given in
the Appendix C.
Lemma 5.1. The optimal document clustering function CD in IMAM model always minimizes the
disagreement rate for any Fi ‚àà ‚Ñ¶ such that
Œ∑(CD , CDW ‚à© CDL ) = min Œ∑(Fi , CDW ‚à© CDL )
Fi ‚àà‚Ñ¶

And meanwhile, Œ∑(CD , CDW ‚à© CDL ) = Œ∑(CDW , CDL ).
Lemma 5.1 suggests that IMAM always finds the document clustering with the minimal disagreement rate to the overlap of the individual view-based clusterings, and the minimal value of
disagreement rate equals to the disagreement rate of the individual view-based clusterings.

6. Experiments and Results
In this section, we empirically evaluate the IMAM algorithm for the cross-domain document classification tasks in comparison with the state-of-the-art baselines.
6.1 Data and Setup
Cora (McCallum, Nigam, Rennie, & Seymore, 2000) is an online archive which contains approximately 37,000 computer science research papers and over 1 million links among documents. The
documents are categorized into a hierarchical structure. We selected a subset of Cora, which contains 5 top categories and 10 sub-categories (the numbers are in the parenthesis):
- DA 1: /data structures algorithms and theory/computational complexity/ (711)
- DA 2: /data structures algorithms and theory/computational geometry/ (459)
- EC 1: /encryption and compression/encryption/ (534)
- EC 2: /encryption and compression/compression/ (530)
- NT 1: /networking/protocols/ (743)
- NT 2: /networking/routing/ (477)
- OS 1: /operating systems/realtime/ (595)
- OS 2: /operating systems/memory management/ (1,102)
- ML 1: /machine learning/probabilistic methods/ (687)
- ML 2: /machine learning/genetic algorithms/ (670)

Based on this dataset, we used a similar way as Dai et al. (2007a) to construct our training
and test sets. For each set, we chose two top categories, one as positive class and the other as
the negative. Different sub-categories were deemed as different domains. The task is defined as
top category classification. For example, the subset denoted as DA-EC consists of source domain:
513

YANG & G AO

DA 1(+), EC 1(-); and target domain: DA 2(+), EC 2(-). The method ensures the domains of
labeled and unlabeled data related due to same top categories, but the domains are different because
they are drawn from different sub-categories. Such preprocessing is a common practice for data
preparation for adaptation purpose. Some previous work (Ling, Dai, Xue, Yang, & Yu, 2008; Dai
et al., 2007a) found that baseline SVM as well as transductive SVM classifiers trained on sourcedomain data performed much worse on the target domain, implying large domain gap between them.
We have the same finding on this dataset by using transductive SVM.
We preprocessed the data for both text and link information. For the texts, we removed stop
words and low-frequency words with count less than 5. For the links, we removed the links with less
than 5 citation counts. Then the standard TF-IDF (Salton & Buckley, 1988) technique was applied
to both the text and link datasets. Moreover, we generated the merged dataset by concatenating both
the word and link features together.
Reuters-21578 (Lewis, 2004) is widely used for the evaluation of automatic text categorization
algorithms. Reuters-21578 corpus also has a hierarchical structure, which contains 5 top categories.
We used the pre-processed version of the corpus that is public accessible3 . The statistics of this
dataset can be seen in Table 1. Based on these data, we generated separate information representing
two views: the first view corresponds to the features using the TF-IDF scores of terms; the second
view corresponds to the topic-based features (i.e. document-topic distributions) obtained by applying probabilistic Latent Semantic Analysis (pLSA)4 on the term counts information, where the topic
number was set to 200.
Subset
Orgs-People
Orgs-Places
People-Places

Source
OrgsPeople.src (1,237)
OrgsPlaces.src (1,016)
PeoplePlaces.src (1,077)

Target
OrgsPeople.tar (1,208)
OrgsPlaces.tar (1,043)
PeoplePlaces.tar (1,077)

Table 1: The statistics of Reuters-21578 dataset.
Using Cora dataset, we conducted experiments with IMAM for studying the influence of different parameters and the manifestation of view disagreement rate. Also, we compared IMAM with
various state-of-the-art domain adaptation algorithms on both Cora and Reuters datasets. In order
to avoid the infinity values, we applied Laplacian smoothing when computing the KL-divergence.
6.2 Parameter Sensitivity
We first studied the influence of some important parameters, i.e., the number of word/link clusters,
Œ±, and Œª.
6.2.1 I NFLUENCE OF C LUSTER N UMBER
Figure 2 shows the error rate curves varying with different number of word (and link) clusters on
the 4 subsets: DA-EC, DA-NT, DA-OS and EC-NT. The X-axis represents the number of word (and
link) clusters which is tuned from 32 to 512. According to the performance shown in the figure, we
empirically set the number of word (and link) clusters to 128.
3. http://www.cse.ust.hk/TL/dataset/Reuters.zip.
4. http://lear.inrialpes.fr/people/verbeek/code/plsa.tar.gz.

514

I NFORMATION -T HEORETIC M ULTI - VIEW D OMAIN A DAPTATION

Figure 2: Error rate curves varying with different number of word/link clusters.
6.2.2 I NFLUENCE OF Œ±
Figure 3 shows that the performance curves vary with different values of Œ±. The error rate generally
decreases first and then increases when Œ± is augmented. As always, the algorithm performs worst
when the model heavily relies on either the text information (0.9 ‚â§ Œ± ‚â§ 1.0) or the link structure
(0 ‚â§ Œ± ‚â§ 0.1). And setting Œ± between 0.5 and 0.8 achieved the best results on most of the subsets. This implies that the two views of document are complementary. Therefore, in the remaining
experiments, we set the value of Œ± to 0.7.

Figure 3: Error rate curves varying with different settings of Œ±.
6.2.3 I NFLUENCE OF Œª
Œª is used for propagating class labels from source document class to target document clustering
through word and link clusters. Surprisingly, we did not observe its significant influence on most
of the subsets. This is because we used NBC to initialize document clusterings for a good starting
point, and the class information, though not accurately, could be largely propagated to the words
and link clusters at the next iteration. This observation is similar to that of Dai et al. (2007a) when
the number of their word clusters was appropriately provided. We empirically set Œª to 0.5 after
trying 0, 0.25, 0.5, 1, 2 and 4.
515

YANG & G AO

Œ∑ on source
Œ∑ on target

DA-EC
0.179
0.251

DA-NT
0.157
0.224

DA-OS
0.188
0.275

DA-ML
0.184
0.211

EC-NT
0.210
0.234

Average
0.184
0.239

Table 2: The view disagreement rates under different domains using co-training.
Iteration
DA-EC
DA-NT
DA-OS
DA-ML
EC-NT


Œ∑

Œ∑

Œ∑

Œ∑

Œ∑

1
0.194
0.340
0.147
0.295
0.129
0.252
0.166
0.306
0.311
0.321

2
0.153
0.132
0.083
0.100
0.064
0.092
0.102
0.107
0.250
0.137

3
0.149
0.111
0.071
0.076
0.052
0.068
0.071
0.076
0.228
0.112

4
0.144
0.101
0.065
0.069
0.047
0.060
0.065
0.062
0.219
0.096

5
0.144
0.095
0.064
0.064
0.041
0.052
0.064
0.054
0.217
0.089

Œ≥
0.998
0.996
0.998
0.984
0.988

Table 3: View disagreement rate (Œ∑) and error rate () both decrease with iterations. Their correlation is denoted as Œ≥.

6.3 View Disagreement Rate Œ∑
In this section, we studied the view disagreement rate for two different purposes: (1) we experimentally verified that the view consistency assumption was violated due to distinct domains for the
traditional multi-view learning using co-training, which justified our motivation to reduce the view
disagreement rate; (2) we examined the property of view disagreement rate based on our method
and revealed its relationship with the cross-domain classification performance.
6.3.1 Œ∑ WITH C O -T RAINING
In this experiment, for each subset, the source data were splitted into two portions, one portion
for training and the other for testing. The traditional multi-view algorithm co-training (Blum &
Mitchell, 1998) was trained on the source training set, and then the model was evaluated on the
source test set and the target test set separately. The first result corresponds to the single-domain
performance and the second corresponds to cross-domain performance.
As shown in Table 2, it is clear that the view disagreement rate on the target domain is considerably higher than that on the source domain. It implies that the domain gap is likely to deteriorate
view consistency. As Abney (2002) pointed out, view consistency is directly related to classification
error rate, which is upper bounded by the view disagreement rate. Our finding from this experiment
seems consistent with this claim, and furthermore, it implies that it would be helpful to overcome
domain gap by enhancing the view consistency on target data.
6.3.2 Œ∑ WITH IMAM
Here we examined the variance of disagreement rate Œ∑(CDW , CDL ) between view-based clusterings
and its correlation with the error rate .
516

I NFORMATION -T HEORETIC M ULTI - VIEW D OMAIN A DAPTATION

We used the Pearson‚Äôs correlation to measure the dependence of the disagreement rate and error
rate, which takes the value between -1 (perfect negative correlation) and 1 (perfect positive correlation). Table 3 shows the monotonic decrease of disagreement rate Œ∑ and error rate  with the
iterations, and their correlation Œ≥ is nearly perfectly positive. This indicates that IMAM may gradually improves adaptation performance by strengthening the consistency between different views,
and alternatively, IMAM increases classification performance, which then causes the different views
to be more consistent. Both procedures are therefore reciprocal causation. This is achieved by
the mutual reinforcement of word and link clustering that draws together those target-specific and
source-specific features, which are originally unrelated but could co-occur with the common features across the two domains.
6.4 Convergence
The convergence property of IMAM is shown as Figure 4. IMAM uses a two-phase iterative procedure to find a local optimal point. The convergence is guaranteed by Lemma 4.2. We can see
that the number of documents needed to be reassigned into different clusters decreases very fast
during the first 5 iterations and reaches 0 after 10 iterations. Thus, we terminate the algorithm after
a maximum of 15 iterations.

Figure 4: Number of documents needed to be reassigned into different clusters varies with iterations.
6.5 Algorithms for Comparison
We compared IMAM with a variety of the state-of-the-art algorithms including Transductive SVM5
(TSVM) (Joachims, 1999) which is a semi-supervised classifier, co-training (Co-Train) (Blum &
Mitchell, 1998), the co-clustering-based single-view transfer learning CoCC (Dai et al., 2007a),
the large-margin-based multi-view transfer learning MVTL-LM (Zhang et al., 2011) and the cotraining-based adaptation algorithm CODA6 (Chen et al., 2011). We used both Cora and Reuters
datasets for the comparative study.
On both datasets, for the ease of presentation, we used the postfix -C, -L and -CL to denote that
the classifier was fed with data of different views. For Cora dataset, -C, -L and -CL represent the text
5. http://svmlight.joachims.org/.
6. http://www1.cse.wustl.edu/Àúmchen/code/coda.tar.

517

YANG & G AO

Subset
DA-EC
DA-NT
DA-OS
DA-ML
EC-NT
EC-OS
EC-ML
NT-OS
NT-ML
OS-ML
Average

TSVM-C
0.293
0.175
0.276
0.217
0.305
0.355
0.333
0.364
0.205
0.202
0.272

TSVM-L
0.157
0.137
0.261
0.114
0.220
0.201
0.205
0.501
0.106
0.170
0.207

TSVM-CL
0.214
0.114
0.262
0.107
0.177
0.245
0.168
0.396
0.101
0.179
0.196

Co-Train
0.230
0.163
0.175
0.171
0.296
0.175
0.206
0.220
0.132
0.128
0.190

MVTL-LM
0.192
0.108
0.068
0.183
0.261
0.176
0.264
0.288
0.071
0.126
0.174

CODA
0.234
0.076
0.109
0.150
0.178
0.187
0.322
0.240
0.025
0.087
0.161

CoCC-C
0.149
0.106
0.075
0.109
0.225
0.137
0.203
0.107
0.054
0.051
0.122

CoCC-L
0.227
0.132
0.086
0.098
0.296
0.116
0.269
0.142
0.094
0.051
0.151

CoCC-CL
0.187
0.115
0.067
0.095
0.239
0.125
0.237
0.115
0.047
0.062
0.129

IMAM
0.138
0.069
0.039
0.047
0.191
0.074
0.173
0.070
0.031
0.021
0.085

Table 4: Error rate of classification adaptation on Cora dataset.
Subset
OrgsPeople
OrgsPlaces
PeoplePlaces
Average

TSVM-C
0.246
0.278
0.294
0.273

TSVM-L
0.263
0.304
0.335
0.301

TSVM-CL
0.227
0.263
0.286
0.259

Co-Train
0.251
0.270
0.318
0.280

MVTL-LM
0.230
0.249
0.260
0.246

CODA
0.177
0.226
0.275
0.226

CoCC-C
0.185
0.214
0.245
0.215

CoCC-L
0.219
0.235
0.262
0.239

CoCC-CL
0.191
0.221
0.248
0.220

IMAM
0.153
0.192
0.218
0.188

Table 5: Error rate of classification adaptation on Reuters-21578 dataset.
view, link view and two views, respectively; for Reuters dataset, they correspond to term view, topic
view and two views. If the examined classifier is inherently multi-view, both of the views‚Äôs data
were fed to it. Such algorithm include TSVM-CL, co-training, MVTL-LM, CoCC-CL, and IMAM.
Since CODA is a pseudo multi-view adaptation algorithm, to fit our scenario, the CODA was fed
with the merged view which could be automatically split into the sub-views. For each algorithm,
the parameters were tuned by using five-fold cross-validation on training data. To cancel out local
optimal results, we repeated the algorithms five times for each subset and reported the average error
rate.
All the algorithms were trained on the source data and then tested on the target data. The
classification error rate on target data is used as evaluation metric, which is defined as the ratio of
the number of misclassified documents to that of total documents.
6.6 Performance Comparison
Table 4 shows the results of comparison on Cora dataset, and Table 5 shows the same on Reuters21578. We have consistent findings on the two datasets.
On both datasets, TSVM performed poorly for adaptation when using either content or link features alone. Simply merging the two sets of features makes some improvements, implying that text
and link in Cora data (or, term and topic in Reuters data) can be complementary, but it may degrade
the confidence of the classifier on some instances whose features become conflicting because of
merging. Co-training can avoid this problem by boosting the confidence of classifiers built on the
distinct views in a complementary way, and its performance is comparable with TSVM though it
uses a weaker base classifier. Since both TSVM and co-training do not consider the distribution
gap, they performed clearly worse than CoCC even though CoCC is a single-view approach.
On both datasets, CODA outperformed co-training and MVTL-LM by splitting the feature space
into multiple pseudo views and iteratively adding the shared source and target features based on
their compatibility across domains. However, it could not be comparably effective than IMAM. It
seems that the pseudo views automatically generated by CODA are not as complementary as the
original view partition on these two datasets. It performed even worse than the COCC under singleview setting, indicating that sometimes pseudo views might be detrimental. The relatively lower
performance of CODA may be explained as follows. It might happen that the original formation
of the two views on our data was reasonably good, but after they were combined into one view,
it was likely that CODA could be stuck in a poor locally optimal decomposition of features due
518

I NFORMATION -T HEORETIC M ULTI - VIEW D OMAIN A DAPTATION

to the non-smooth, non-convex nature of its objective function. Since its model parameters were
initialized randomly, repeating the algorithm did not guarantee a better solution. In contrast, the
objective function of IMAM, although non-convex, is smooth, and also, instead of using random
initialization we used NBC to initialize the document clusters to ensure a good starting point.
IMAM significantly outperformed both CoCC-C and CoCC-L on all the subsets. In average,
the error rate of IMAM is 30.3% lower than that of CoCC-C (or 43.7% lower than that of CoCCL). This is because IMAM effectively leverages distinct and complementary views. Compared to
CoCC, using source training data to improve the view consistency on target data is the key competency of IMAM. Moreover, IMAM performed much better than the CoCC-CL. Unlike CoCC-CL
which simply concatenates the two-view data, our technique is to strengthen the view consistency
by bootstrapping two CoCC models iteratively and complementarily. In our model the two CoCC
models communicate complementarily in each iteration, which consequently boosts the consistency
between the two views.
The result shows that multi-view adaptation using MVTL-LM performs worse than IMAM on
most subsets. A general explanation suggests that instance-based approach relying on instance
weighting are not effective when the data of different domains are drawn from different feature
spaces. Although MVTL-LM regulates view consistency on both domains‚Äô instances, it cannot
identify the useful correlation between the target-specific and source-specific features, which is the
key to the success of adaptation especially when the domain gap is large and little commonality
could be found. In contrast, CoCC and IMAM can use co-clustering or multi-way clustering to find
such correlation.
Note that we use different ways to generate the multi-view data for the two datasets. Different
from Cora dataset which has natural multiple views, i.e., text and link, we generate the term and
topic views for Reuters-21578 dataset based on the text information only. Nevertheless, the results
on both datasets show that IMAM works well on different types of multi-view data by using the
multi-way clustering to enhance the view consistency.

7. Conclusion and Future Work
We presented a novel feature-level multi-view adaptation approach called IMAM for cross-domain
document classification. The thrust of our technique is to incorporate distinct views of document
features into the multi-way clustering framework and gradually strengthen the view consistency for
classifying target documents. The improvements over the state-of-the-art baselines are substantial.
We provided both theoretical and empirical justifications regarding the properties of the proposed
algorithm. Experiments show that it considerably outperforms the state-of-the-art baselines including the multi-view single-domain algorithm co-training, the co-training-based adaptation CODA,
the single-view adaptation CoCC as well as the instance-level multi-view adaptation MVLT-LM.
Multi-view domain adaptation is a promising direction since its underlying principle and practice are still open questions. As part of our ongoing work, we will further explore the foundations
and limitations of multi-view domain adaptation. For example, multiple views might hurt adaptation performance when domains or views are very ‚Äúdissimilar‚Äù. Although it was not observed in
our experiments, it needs to be analyzed more deeply. In addition, due to practical reasons, we did
not directly optimize the consistency measure, i.e., view disagreement rate. Instead, we adopted the
information-theoretical framework to optimize the mutual information loss, which worked well but
may not be the ideal solution. In the future, we will study the techniques of directly optimize the
consistency measure of views.
519

YANG & G AO

Appendix A. Proof of Lemma 4.1
Proof. The proof of Lemma 4.1 can be divided into two parts.
(i) For document clustering:
Note that the word and link clusterings keep fixed in this phase. Thus the mutual information
between class label and word (or link) clusters remains unchanged during the document clustering
phase, that is,
h
i
œÜC (WÃÇ , LÃÇ) = Œª Œ±(I(C, W ) ‚àí I(C, WÃÇ )) + (1 ‚àí Œ±)(I(C, L) ‚àí I(C, LÃÇ))
is a constant. By using Eq. 1, we can obtain
Œò ‚àí œÜC (WÃÇ , LÃÇ)
= Œ±ŒòW + (1 ‚àí Œ±)ŒòL ‚àí œÜC (WÃÇ , LÃÇ)
h
i
h
i
= Œ± I(DT ; W ) ‚àí I(DÃÇT ; WÃÇ ) + (1 ‚àí Œ±) I(DT ; L) ‚àí I(DÃÇT ; LÃÇ)
Ô£Æ
Ô£´
Ô£∂
Ô£π
XXX X
XX X X
ÀÜ wÃÇ)
p(d,
w)
p(
d,
Ô£≠
Ô£ª
p(d, w) log
= Œ±Ô£∞
p(d, w)Ô£∏ log
‚àí
ÀÜ wÃÇ)
p(d)p(w)
p(
d)p(
dÀÜ wÃÇ d‚ààdÀÜ w‚ààwÃÇ
dÀÜ wÃÇ
d‚ààdÀÜ w‚ààwÃÇ
Ô£´
Ô£Æ
Ô£∂
Ô£π
X
X
XXXX
X
X
ÀÜ
ÀÜ
p(d, l)
p(d, l) Ô£ª
Ô£≠
p(d, l) log
+ (1 ‚àí Œ±) Ô£∞
p(d, l)Ô£∏ log
‚àí
ÀÜ ÀÜl)
p(d)p(l)
p(d)p(
dÀÜ

lÃÇ

=Œ±

XXX X

=Œ±

XXX X

=Œ±

XXX X

=Œ±

XX

dÀÜ

dÀÜ

dÀÜ

dÀÜ

p(d, w) log

wÃÇ d‚ààdÀÜ w‚ààwÃÇ

wÃÇ d‚ààdÀÜ w‚ààwÃÇ

p(d)

XX

d‚ààdÀÜ l‚ààlÃÇ

lÃÇ

XXXX
ÀÜ wÃÇ)
ÀÜ ÀÜl)
p(d, w)p(d)p(
p(d, l)p(d)p(
+ (1 ‚àí Œ±)
p(d, l) log
ÀÜ wÃÇ)p(d)p(w)
ÀÜ ÀÜl)p(d)p(l)
p(d,
p(d,
ÀÜ
ÀÜ
d

lÃÇ

d‚ààd l‚ààlÃÇ

XXXX
p(d, w)
p(d, l)
p(d, w) log
+ (1 ‚àí Œ±)
p(d, l) log
q(d, w)
q(d, l)
dÀÜ

p(d)p(w|d) log

wÃÇ d‚ààdÀÜ w‚ààwÃÇ

dÀÜ d‚ààdÀÜ

=

d‚ààdÀÜ l‚ààlÃÇ

XX

p(w|d) log

wÃÇ w‚ààwÃÇ

p(w|d)
+ (1 ‚àí Œ±)
ÀÜ
q(w|d)

lÃÇ

d‚ààdÀÜ l‚ààlÃÇ

XXXX

p(w|d)
+ (1 ‚àí Œ±)
ÀÜ
q(w|d)

dÀÜ

p(d)p(l|d) log

d‚ààdÀÜ l‚ààlÃÇ

lÃÇ

XX

p(d)

dÀÜ d‚ààdÀÜ

XX
lÃÇ

h

ÀÜ + (1 ‚àí Œ±)D(p(l|d)||q(l|d))
ÀÜ
p(d) Œ±D(p(w|d)||q(w|d))

p(l|d)
ÀÜ
q(l|d)

p(l|d) log

l‚ààlÃÇ

p(l|d)
ÀÜ
q(l|d)

i

dÀÜ d‚ààdÀÜ

=

X

ÀÜ
p(d)œÜD (d, d)

d

(ii) For word and link clustering:
Note that the document clusterings remains unchanged in this phase. Using the similar technique
as above, we can obtain
Œò=Œ±

X

p(w)œÜW (w, wÃÇ) + (1 ‚àí Œ±)

w

X
l

By combining steps (i) and (ii), Lemma 4.1 can be proved.
520

p(l)œÜL (l, ÀÜl)

I NFORMATION -T HEORETIC M ULTI - VIEW D OMAIN A DAPTATION

Appendix B. Proof of Lemma 4.2
Proof. The proof of Lemma 4.2 can be divided into two parts.
(i) For document clustering: Note that the word and link clusterings keep fixed in this phase.
(t)

Œò

(a)

(t)

X

‚àí œÜC (WÃÇ , LÃÇ) =

h
i
(t)
ÀÜ + (1 ‚àí Œ±)D(p(l|d)||q (t) (l|d))
ÀÜ
p(d) Œ±D(p(w|d)||q (w|d))

X

dÃÇ d:C (t) (d)=dÃÇ
D

Ô£Æ
=

X

X

p(d) Ô£∞Œ±

X

p(w|d) log

w

dÃÇ d:C (t) (d)=dÃÇ
D

Ô£π
X
p(l|d)
p(w|d)
Ô£ª
+ (1 ‚àí Œ±)
p(l|d) log
ÀÜ
ÀÜ
q (t) (w|d)
q (t) (l|d)
l

Ô£Æ

(b)

X

‚â•

X

p(d) Ô£∞Œ±

X

p(w|d)

p(w|d) log

w

dÃÇ d:C (t) (d)=dÃÇ
D

Ô£Æ
(c)

X

=

X

p(d) Ô£∞Œ±

X

p(w|d) log

w

dÃÇ d:C (t+1) (d)=dÃÇ
D

+ (1 ‚àí Œ±)

(t+1)
q (t) (w|CD
(d))

X

Ô£π

p(l|d)

p(l|d) log

(t+1)
q (t) (l|CD
(d))

l

Ô£ª

Ô£π
X
p(w|d)
p(l|d)
Ô£ª
+ (1 ‚àí Œ±)
p(l|d) log
ÀÜ
ÀÜ
q (t) (w|d)
q (t) (l|d)
l

Ô£Æ
(d)

X

=

X

dÃÇ d:C (t+1) (d)=dÃÇ
D

Ô£π

Ô£Ø X
Ô£Ø
p(d) Ô£ØŒ±
Ô£∞
wÃÇ

X

p(w|d)

p(w|d) log

ÀÜ
q (t) (w|wÃÇ)q (t) (wÃÇ|d)

(t+1)
w:C
(w)=wÃÇ
W

+ (1 ‚àí Œ±)

X

X

p(l|d) log

lÃÇ l:C (t+1) (l)=lÃÇ
L

=

X

dÃÇ d:C (t+1) (d)=dÃÇ
D

Ô£∫
Ô£∫
Ô£∫
ÀÜ Ô£ª
q (t) (l|lÃÇ)q (t) (lÃÇ|d)

Ô£π

Ô£Æ
X

p(l|d)

Ô£Ø X
Ô£Ø
p(d) Ô£ØŒ±
Ô£∞
wÃÇ

X

p(w|d)

p(w|d) log

q (t) (w|wÃÇ)

(t+1)
w:C
(w)=wÃÇ
W

+ (1 ‚àí Œ±)

X

X

lÃÇ l:C (t+1) (l)=lÃÇ
L

p(l|d) Ô£∫
Ô£∫
p(l|d) log
Ô£∫
q (t) (l|lÃÇ) Ô£ª

{z

|

}

I

Ô£π

Ô£Æ
+

X

X

dÃÇ d:C (t+1) (d)=dÃÇ
D

Ô£Ø X
Ô£Ø
p(d) Ô£ØŒ±
Ô£∞
wÃÇ

X

p(w|d) log

(t+1)
w:C
(w)=wÃÇ
W

Ô£´
"
=I+

X

Œ±

XÔ£¨
Ô£¨
Ô£¨
Ô£≠
wÃÇ

dÃÇ

X
d:C

X

(t+1)
(t+1)
(d)=dÃÇ w:C
(w)=wÃÇ
D
W

X

d:C

X

(t+1)
(t+1)
(d)=dÃÇ l:C
(l)=lÃÇ
D
L

Ô£Æ
X

Ô£∞Œ±

X (t+1)
ÀÜ wÃÇ) log
q
(d,
wÃÇ

dÃÇ
(e)

‚â• I+

1
ÀÜ
q (t) (wÃÇ|d)

Ô£Æ
X (t+1)
X (t+1)
ÀÜ Ô£∞Œ±
ÀÜ log
q
(d)
q
(wÃÇ|d)
wÃÇ

dÃÇ

X

X

p(d) Œ±

dÃÇ d:C (t+1) (d)=dÃÇ
D

+ (1 ‚àí Œ±)

Ô£∑
1
Ô£∑
p(d)p(l|d)Ô£∑ log
ÀÜ
Ô£∏
q (t) (lÃÇ|d)

+ (1 ‚àí Œ±)

=

X

1
ÀÜ
q (t+1) (wÃÇ|d)

X

wÃÇ

(t+1)
w:C
(w)=wÃÇ
W

X

p(l|d) log

X

p(d) Œ±

dÃÇ d:C (t+1) (d)=dÃÇ
D

(1 ‚àí Œ±)

X

Ô£∫
Ô£∫
Ô£∫
ÀÜ Ô£ª
q (t) (lÃÇ|d)

X

+ (1 ‚àí Œ±)

p(w|d) log

ÀÜ
q (t) (lÃÇ|d)

Ô£π
Ô£ª

X (t+1)
ÀÜ log
q
(lÃÇ|d)
lÃÇ

1
ÀÜ
q (t+1) (lÃÇ|d)

Ô£π
Ô£ª

p(w|d)
ÀÜ
q (t) (w|wÃÇ)q (t+1) (wÃÇ|d)
#

p(l|d)

X

wÃÇ

(t+1)
w:C
(w)=wÃÇ
W

lÃÇ l:C (t+1) (l)=lÃÇ
L

1

ÀÜ
q (t) (l|lÃÇ)q (t+1) (lÃÇ|d)

X

p(l|d) log

#

X (t+1)
ÀÜ lÃÇ) log
q
(d,

"
X

lÃÇ l:C (t+1) (l)=lÃÇ
L

lÃÇ

X

lÃÇ l:C (t+1) (l)=lÃÇ
L
(f )

p(l|d) log

p(w|d)
+
ÀÜ
q (t+1) (w|wÃÇ)q (t+1) (wÃÇ|d)

p(w|d) log

#

p(l|d)
ÀÜ
q (t+1) (l|lÃÇ)q (t+1) (lÃÇ|d)

Ô£Æ
=

X

=

X

X

dÃÇ d:C (t+1) (d)=dÃÇ
D

Ô£π

Ô£Ø X
Ô£Ø
p(d) Ô£ØŒ±
Ô£∞
wÃÇ

X
w:C

(t+1)
(w)=wÃÇ
W

p(w|d) log

p(w|d)
ÀÜ
q (t+1) (w|d)

(t+1)

= Œò

+ (1 ‚àí Œ±)

X

(t+1)

‚àí œÜC

(WÃÇ , LÃÇ)

521

X

lÃÇ l:C (t+1) (l)=lÃÇ
L

h
i
(t+1)
ÀÜ + (1 ‚àí Œ±)D(p(l|d)||q (t+1) (l|d))
ÀÜ
p(d) Œ±D(p(w|d)||q
(w|d))

X

dÃÇ d:C (t+1) (d)=dÃÇ
D
(g)

1

Ô£∑
1
Ô£∑
p(d)p(w|d)Ô£∑ log
ÀÜ
Ô£∏
q (t) (wÃÇ|d)

"
=

+ (1 ‚àí Œ±)

X

Ô£∂

XÔ£¨
Ô£¨
+ (1 ‚àí Œ±)
Ô£¨
Ô£≠

=I+

ÀÜ
q (t) (wÃÇ|d)

X

Ô£∂

Ô£´

lÃÇ

1

p(l|d) log

Ô£∫
p(l|d)
Ô£∫
Ô£∫
ÀÜ Ô£ª
q (t+1) (l|d)

YANG & G AO

where (a) follows from Lemma 4.1, (b) follows from Step 4 of the IMAM algorithm, (c) follows by rearranging the summation, (d) and (f) follow since we hold the word and link clusters
fixed in Step 4, (e) follows by non-negativity of the KL-divergence, and (g) follows from Lemma 4.1. Since the word and link clusters remain unchanged during the document clustering, i.e.,
(t)
(t+1)
œÜC (WÃÇ , LÃÇ) = œÜC (WÃÇ , LÃÇ), we can prove that Œò(t) ‚â• Œò(t+1) .
(ii) For word and link clustering: Note that the document clusterings remains unchanged in this
phase. By using the properties of Step 6 and the similar technique as above, we can prove that
Œò(t+1) = Œ±

X

X

(t+1)

p(w)œÜW

(w, wÃÇ) + (1 ‚àí Œ±)

X

X

lÃÇ

(t+1)
l:CÃÇL
(l)=lÃÇ

wÃÇ w:C (t+1) (w)=wÃÇ
W

‚â•Œ±

X

=Œò

(t+2)

X

(t+2)
p(w)œÜW (w, wÃÇ)

+ (1 ‚àí Œ±)

X

wÃÇ w:C (t+2) (w)=wÃÇ
W

lÃÇ

X
(t+2)

l:CL

(t+1)

p(l)œÜL
(t+2)

p(l)œÜL

(l, ÀÜ
l)

(l, ÀÜ
l)

(l)=lÃÇ

By combining steps (i) and (ii), it follows that in every iteration the algorithm IMAM monotonically decreases the objective function.

Appendix C. Proof of Lemma 5.1
Proof. For any document d ‚àà D and any document cluster dÀÜ ‚àà DÃÇ,
(i) If CDW (d) = CDL (d)
For brevity, we denote the cluster by dÀÜ‚àó , i.e., CDW (d) = CDL (d) = dÀÜ‚àó . By using Eq. 4, Eq. 8
and Eq. 9, we can obtain
ÀÜ
D(p(w|d)||q(w|dÀÜ‚àó )) ‚â§ D(p(w|d)||q(w|d))
‚àó
ÀÜ
ÀÜ
D(p(l|d)||q(l|d ))
‚â§ D(p(l|d)||q(l|d))
‚àó
ÀÜ
ÀÜ + (1 ‚àí Œ±)D(p(l|d)||q(l|d))
ÀÜ
‚áí Œ±D(p(w|d)||q(w|d )) + (1 ‚àí Œ±)D(p(l|d)||q(l|dÀÜ‚àó )) ‚â§ Œ±D(p(w|d)||q(w|d))



‚áí CD (d) = CDW (d) = CDL (d) = dÀÜ‚àó
‚áí Œ¥CD ,CDW ‚à©CDL (d) = 1

(ii) If CDW (d) 6= CDL (d)
Obviously, we have Œ¥CD ,CDW ‚à©CDL (d) = 0. By combining (i) and (ii), we can obtain

1, if CDW (d) = CDL (d);
Œ¥CD ,CDW ‚à©CDL (d) = Œ¥CDW ,CDL (d) =
0, otherwise
For any Fi ‚àà ‚Ñ¶, the indicator function Œ¥Fi ,CDW ‚à©CDL (d) can be rewritten as
Ô£±
Ô£≤ 1, if CDW (d) = CDL (d) = Fi (d);
0, if CDW (d) = CDL (d) 6= Fi (d);
Œ¥Fi ,CDW ‚à©CDL (d) =
Ô£≥
0, otherwise
Thus, we have Œ¥CD ,CDW ‚à©CDL (d) ‚â• Œ¥Fi ,CDW ‚à©CDL (d). This inequation holds true for any Fi ‚àà ‚Ñ¶.
Therefore, based on the definition of disagreement rate we can obtain
P
Œ∑(Fi , CDW ‚à© CDL ) = 1 ‚àí

d‚ààD

Œ¥Fi ,CDW ‚à©CDL (d)
|D|

P
‚â•1‚àí

d‚ààD

Œ¥CD ,CDW ‚à©CDL (d)
|D|

Meanwhile, we can obtain Œ∑(CD , CDW ‚à© CDL ) = Œ∑(CDW , CDL ).
522

= Œ∑(CD , CDW ‚à© CDL )

I NFORMATION -T HEORETIC M ULTI - VIEW D OMAIN A DAPTATION

References
Abney, S. (2002). Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pp. 360‚Äì367.
Blitzer, J., Dredze, M., & Pereira, F. (2007). Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics, June 23-30, 2007, Prague, Czech Republic,
pp. 440‚Äì447.
Blitzer, J., Kakade, S., & Foster, D. P. (2011). Domain adaptation with coupled subspaces. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, April
11-13, 2011, Ft. Lauderdale, FL, USA, pp. 173‚Äì181.
Blum, A., & Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory, Madison, Wisconsin, USA, July 24-26, 1998, pp. 92‚Äì100.
Cai, P., Gao, W., Zhou, A. Y., & Wong, K. F. (2011a). Relevant knowledge helps in choosing right
teacher: Active query selection for ranking adaptation. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, July
24-28, 2011, Beijing, China, pp. 115‚Äì124.
Cai, P., Gao, W., Zhou, A. Y., & Wong, K. F. (2011b). Query weighting for ranking model adaptation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, June 19-24, Portland, Oregon, USA, pp. 112‚Äì122.
Chen, M.M., Weinberger, K. Q., & Blitzer, J. (2011). Co-training for domain adaptation. In Proceedings of Advances in Neural Information Processing Systems 24, December 12-14, 2011,
Granada, Spain, pp. 1‚Äì9.
Collins, M., & Singer, Y. (1999). Unsupervised models for named entity classification. In Proceedings of 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pp. 100‚Äì110.
Cover, T. M., & Thomas, J. A. (1991). Elements of information theory. Wiley-Interscience.
Dai, W. Y., Xue, G. R., Yang, Q., & Yu, Y. (2007a). Co-clustering based classification for outof-domain documents. In Proceedings of the 13th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, San Jose, California, USA, August 12-15, 2007,
pp. 210‚Äì219.
Dai, W. Y., Yang, Q., Xue, G. R., & Yu, Y. (2007b). Boosting for transfer learning. In Proceedings
of the 24th International Conference on Machine Learning, Corvallis, Oregon, USA, June
20-24, 2007, pp. 193‚Äì200.
Dasgupta, S., Littman, M. L., & McAllester, D. (2001). PAC generalization bounds for co-training.
In Proceedings of Advances in Neural Information Processing Systems 14, December 9-14,
2002, Vancouver, British Columbia, Canada, pp. 375‚Äì382.
DaumeÃÅ III, H., & Marcu, D. (2006). Domain adaptation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(2006):101‚Äì126.
523

YANG & G AO

DaumeÃÅ III, H., Kumar, A., & Saha, A. (2010). Co-regularization based semi-supervised domain
adaptation. In Proceedings of Advances in Neural Information Processing Systems 23, December 6-9, 2010, Vancouver, Canada, pp. 478‚Äì496.
Dayanik, A. A., Lewis, D. D., Madigan, D., Menkov, V., & Genkin, A. (2006). Constructing informative prior distributions from domain knowledge in text classification. In Proceedings
of the 29th Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval, Seattle, Washington, USA, August 6-11, 2006, pp. 493‚Äì500.
Dhillon, I. S., Mallela, S., & Modha, D. S. (2003). Information-theoretic co-clustering. In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, Washington, DC, USA, August 24 - 27, 2003, pp. 210‚Äì219.
Gao, W., Blitzer, J., Zhou, M., & Wong, K. F. (2009). Exploiting bilingual information to improve
web search. In Proceedings of the 47th Annual Meeting of the Association for Computational
Linguistics and the 4th International Joint Conference on Natural Language Processing of
the AFNLP, August 2-7, 2009, Singapore, pp. 1075‚Äì1083.
Gao, W., Cai, P., Wong, K. F., & Zhou, A. Y. (2010). Learning to rank only using training data from
related domain. In Proceedings of the 33rd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, July 19-23, 2010, Geneva, Switzerland,
pp. 162‚Äì169.
Gao, W., & Yang, P. (2014). Democracy is good for ranking: Towards multi-view rank learning and
adaptation in web search. In Proceedings of the 7th International ACM Conference on Web
Search and Data Mining, Feburary 25-27, 2014, New York City, USA, pp. 63‚Äì72.
He, J. R., & Lawrence, R. (2011) A graph-based framework for multi-task multi-view learning.
In Proceedings of the 28th International Conference on Machine Learning, Washington, Jun
28-Jul 2, 2011, pp. 25‚Äì32.
Lewis, D. D. (2004). Reuters-21578 test collection. http://www.daviddlewis.com/.
Joachims, T. (1999). Transductive inference for text classification using support vector machines.
In Proceedings of the 16th International Conference on Machine Learning, Bled, Slovenia,
June 27-30, 1999, pp. 200‚Äì209.
Jiang, J., & Zhai, C. X. (2007). Instance weighting for domain Adaptation in NLP. In Proceedings of
the 45th Annual Meeting of the Association for Computational Linguistics, June 23-30, 2007,
Prague, Czech Republic, pp. 264‚Äì271.
Ling, X., Dai, W. Y., Xue, G. R., Yang, Q., & Yu, Y. (2008). Spectral domain-transfer learning. In
Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, Las Vegas, Nevada, USA, August 24-27, 2008, pp. 488‚Äì496.
McCallum, A. K., Nigam, K., Rennie, J., & Seymore, K. (2000). Automating the construction of
Internet portals with machine learning. Information Retrieval, 3(2):127‚Äì163.
Pan, S. J., & Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and
Data Engineering, 22(10):1345‚Äì1359.
RuÃàping, S., & Scheffer, T. (2005). Learning with multiple views. In Proceedings of 2005 ICML
Workshop on Learning with Multiple Views.
524

I NFORMATION -T HEORETIC M ULTI - VIEW D OMAIN A DAPTATION

Salton, G., & Buckley, C. (1988). Term-weighting approaches in automatic text retrieval. Information Processing & Management, 24(5):513‚Äì523.
Sarinnapakorn, K., & Kubat, M. (2007). Combining sub-classifiers in text categorization: A DSTbased solution and a case study. IEEE Transactions Knowledge and Data Engineering,
19(12):1638‚Äì1651.
Sridharan, K., & Kakade, S. M. (2008). An information theoretic framework for multi-view learning. In Proceedings of the 21st Annual Conference on Learning Theory, Helsinki, Finland,
July 9-12, 2008, pp. 403‚Äì414.
Yang, P., Gao, W., Tan, Q., & Wong, K. F. (2012). Information-theoretic multi-view domain adaptation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, July 8-11, 2012, Jeju Island, Korea, pp. 270‚Äì274.
Zhang, D., He, J. R., Liu, Y., Si, L., & Lawrence, R. D. (2011). Multi-view transfer learning with
a large margin approach. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Diego, CA, USA, August 21-24, 2011,
pp. 1208‚Äì1216.
Zhang, J. T., & Huan, J. (2012). Inductive multi-task learning with multiple view data. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, Beijing, China, August 12-16, 2012, pp. 543‚Äì551.

525

Journal of Artificial Intelligence Research 49 (2014) 601-633

Submitted 10/13; published 04/14

Algorithms and Applications for the
Same-Decision Probability
Suming Chen
Arthur Choi
Adnan Darwiche

suming@cs.ucla.edu
aychoi@cs.ucla.edu
darwiche@cs.ucla.edu

Computer Science Department
University of California, Los Angeles
Los Angeles, CA 90095

Abstract
When making decisions under uncertainty, the optimal choices are often difficult to
discern, especially if not enough information has been gathered. Two key questions in this
regard relate to whether one should stop the information gathering process and commit
to a decision (stopping criterion), and if not, what information to gather next (selection
criterion). In this paper, we show that the recently introduced notion, Same-Decision
Probability (SDP), can be useful as both a stopping and a selection criterion, as it can provide additional insight and allow for robust decision making in a variety of scenarios. This
query has been shown to be highly intractable, being PPPP -complete, and is exemplary of
a class of queries which correspond to the computation of certain expectations. We propose the first exact algorithm for computing the SDP, and demonstrate its effectiveness on
several real and synthetic networks. Finally, we present new complexity results, such as the
complexity of computing the SDP on models with a Naive Bayes structure. Additionally,
we prove that computing the non-myopic value of information is complete for the same
complexity class as computing the SDP.

1. Introduction
Probabilistic graphical models have often been used to model a variety of decision problems,
e.g., in medical diagnosis (Pauker & Kassirer, 1980; Kahn, Roberts, Shaffer, & Haddawy,
1997; van der Gaag & CoupeÃÅ, 1999), fault diagnosis (Lu & Przytula, 2006), classification
(Friedman, Geiger, & Goldszmidt, 1997; Ramoni & Sebastiani, 2001; Jordan, 2002), troubleshooting (Heckerman, Breese, & Rommelse, 1995), educational diagnosis (Butz, Hua, &
Maguire, 2004; Arroyo & Woolf, 2005; MillaÃÅn, Descalco, Castillo, Oliveira, & Diogo, 2013),
and in intrusion detection (Kruegel, Mutz, Robertson, & Valeur, 2003; Modelo-Howard,
Bagchi, & Lebanon, 2008). In these and similar applications, a decision maker is typically
in a position where they must decide which tests to perform or observations to make in
order to make a better informed decision. Perhaps more critically, a decision maker must
also decide when to stop making observations and commit to a particular decision.
The Same-Decision Probability (SDP) was recently proposed by Darwiche and Choi
(2010), in order to help quantify the robustness of a decision, in the context of decisionmaking with Bayesian networks. In short, the SDP is the probability that we would make
the same decision, if we were to perform further observations that have yet to be made.
As such, the SDP can be treated as a measure for a decision‚Äôs robustness with respect to
c
2014
AI Access Foundation. All rights reserved.

Chen, Choi & Darwiche

unknown variables, quantifying our confidence that we would make the same decision, even
if we made further observations.
In this paper, we show how we can apply the SDP as a tool for information gathering, in
particular, as a way to determine if we should stop information gathering (as a stopping criterion), and if not, which pieces of information to gather next (as a selection criterion). We
compare the SDP to classical stopping and selection criteria through illustrative examples.
For instance, we demonstrate that the SDP can distinguish between stable and unstable
decisions that are indistinguishable by classical criteria. Additionally, we also show that
there are scenarios where classical criteria may call for performing further observations, but
where the SDP indicates that our decision is unlikely to change.
Notably, the SDP has been shown to be highly intractable, by Choi, Xue, and Darwiche
(2012), and the exact computation of the SDP has been limited to toy examples, with
few variables, through brute-force enumeration. In this paper, we propose the first exact
algorithm for computing the SDP. This algorithm can be applied to real-world networks that
are out of the scope of both brute-force enumeration and previously proposed approximation
algorithms, and can be further applied to synthetic networks with as many as 100 variables.
We further provide new complexity results on the SDP, which both highlight its relative
intractability (even in Naive Bayes networks), but also its relationship to a broader class
of expectation computation problems, emphasizing the broader importance of developing
effective algorithms for the SDP and related problems.
Our paper is thus structured as follows. We first introduce notation and discuss some
common stopping and selection criteria in Section 2. We then review our previously introduced work on the SDP in Section 3. In Section 4, we discuss how the SDP can be applied
as both a stopping criterion and as a selection criterion. In Section 5, we present a novel
exact algorithm for computing the SDP and discuss experimental results in Section 6. In
Section 7, we present some recent complexity results on the SDP. We then conclude our
paper in Section 8.

2. Related Work
When making decisions under uncertainty, it may be difficult to finalize a decision in the
presence of unobserved variables. Given these unobserved variables, there are two fundamental questions. The first question is whether, given the current observations, the decision
maker is ready to commit to a decision. We will refer to this as the stopping criterion for
making a decision. Assuming the stopping criterion is not met, the second question is what
additional observations should be made before the decision maker is ready to make a decision. This typically requires a selection criterion based on some measure for quantifying an
observation‚Äôs value of information (VOI). In this section, we first introduce some necessary
notation, and then review some commonly used stopping and selection criteria.
2.1 Notation
Throughout this paper, we use standard notation for variables and their instantiations,
where variables are denoted by upper case letters X and their instantiations by lower case
letters x. Additionally, sets of variables are denoted by bold upper case letters X and
their instantiations by bold lower case letters x. We assume that the state of the world is
602

Algorithms and Applications for the Same-Decision Probability

described over random variables X, where the evidence E ‚äÜ X includes all known variables,
and where hidden variables U ‚äÜ X include all unknown variables. By definition, E ‚à© U = ‚àÖ
and E‚à™U = X. We often discuss the ramifications of observing a subset of hidden variables
H ‚äÜ U on decision making. Furthermore, we use D ‚àà U to denote the main hypothesis
variable that forms the basis for a decision.1
2.2 Stopping Criterion
Given that there are hidden variables in our model and we have the choice of whether or
not to observe some subset, a stopping criterion determines when we stop the process of
information gathering and commit to a decision. Note that we are concerned with making
a decision based on some hypothesis variable, such as the state of a patient‚Äôs health. For
a stopping criterion, the most basic approach used in a variety of domains is to commit
to a decision once the belief about a certain event crosses some threshold, as is done by
Pauker and Kassirer (1980), Kruegel et al. (2003), and Lu and Przytula (2006). However,
this approach may not be robust, as further observations may cause the belief about the
event to fall below the threshold. Van der Gaag and Bodlaender (2011) note the possibility
of this and pose the STOP problem, which asks whether or not the present evidence gathered
is sufficient for diagnosis, or if there exists further relevant evidence that can and should be
gathered.
Other approaches involve ensuring that the uncertainty surrounding the decision variable
is sufficiently reduced. For instance, Gao and Koller (2011) stop information gathering when
1) the conditional entropy of the interest variable is reduced beyond some threshold or 2)
the margin between the first and second most likely states of the interest variable is above
some threshold. In any case, it is clear that threshold-based stopping criteria are ubiquitous
for decision making under uncertainty.
Alternatively, there are also several stopping criteria that involve the existence of a
budget, which can be an abstract quantity to represent the available resources that can
be used for information gathering. The budget may be representative of the number of
observations that are allowed (Modelo-Howard et al., 2008; Munie & Shoham, 2008; Yu,
Krishnapuram, Rosales, & Rao, 2009; Chen, Low, Tan, Oran, Jaillet, Dolan, & Sukhatme,
2012a), or in terms of a ‚Äúmonetary‚Äù amount that may be spent on observations of varying
cost (Greiner, Grove, & Roth, 2002; Krause & Guestrin, 2009; Bilgic & Getoor, 2011). In the
context of a budget, the general stopping criterion is then to continue to make observations
until the budget is completely expended, as is done by Modelo-Howard et al. (2008) and
Munie and Shoham (2008). Krause and Guestrin (2009) and Bilgic and Getoor (2011) note
that the budget should be expended with the caveat that the value of information of an
observation is at least the cost of the observation.
2.3 Selection Criterion: Value of Information
Should the stopping criterion determine that further observations are necessary, a selection
criterion is then used to determine which variables should be selected for observation.
Ideally, we want to observe all variables that will give us additional information with regards
1. The work presented in this paper can be extended to the case of multiple hypothesis variables, but we
focus here on the case of one hypothesis variable for simplicity.

603

Chen, Choi & Darwiche

to our decision variable. However, due to resource constraints (such as a limited budget) this
is often not possible. In this basic approach, a common selection criterion is to then select
observations that will minimize the conditional entropy of the decision variable (Vomlel,
2004; Lu & Przytula, 2006; Krause & Guestrin, 2009; Yu et al., 2009; Zhang & Ji, 2010;
Gao & Koller, 2011; Ognibene & Demiris, 2013; Shann & Seuken, 2013). The entropy of a
variable X is defined as:
H(X) = ‚àí

X

Pr (x) log Pr (x)

(1)

x

and is a measure of the uncertainty of the variable‚Äôs state ‚Äî if the entropy of a variable is
high, that means that there is much uncertainty on what value that variable takes.2 The
uncertainty of the decision variable‚Äôs true state makes it difficult to make a decision. Thus, a
natural selection criterion is to observe variables to minimize the conditional entropy of the
decision variable, where the conditional entropy of variable D given variable X is defined
as:
H(D | X) =

X

H(D | x)Pr (x)

(2)

x

The conditional entropy is thus an expectation of what the entropy would be after
observing X. A similar selection criterion is to observe variables that will most greatly
increase the margin between the posterior probabilities of the first and second most-likely
states of the decision variable (Krause & Guestrin, 2009).
These selection criteria involve utilizing the notion of value of information (VOI) in order
to quantify the value of various observations (Lindley, 1956; Stratonovich, 1965; Howard,
1966; Raiffa, 1968). The VOI of a set of variables can depend on various measures. In
the two example selection criteria we have discussed, those measures would be entropy and
margins of confidence. For instance, if observing a variable X would reduce the conditional
entropy H(D | X) more than observing variable X ‚Ä≤ would (H(D | X) < H(D | X ‚Ä≤ )), then
the value of observing X would be higher.
Krause and Guestrin (2009) define a general notion of VOI that is based on different
reward functions. In particular, given an arbitrary reward function R,3 a hypothesis variable
D, and evidence e, the VOI of observing hidden variables H is:
V(R, D, H, e) = ER(R, D, H, e) ‚àí R(Pr (D | e))

(3)

where
ER(R, D, H, e) =

X

R(Pr (D | h, e))Pr (h | e)

(4)

h

is the expected reward of observing variables H and R(Pr (D | e)) is the reward had
we not observed variables H. By this definition, the reward function used by Lu and
2. In information theory, the logarithm is typically assumed to be base-2 (Cover & Thomas, 1991), which
we also assume throughout the paper for convenience.
3. A reward function is assumed to take as input the probability distribution of the hypothesis variable,
Pr (D), and return some numeric value. We discuss reward functions further in Section 7.2.

604

Algorithms and Applications for the Same-Decision Probability

D

D
+
‚àí

Pr (D | E1 = +, E2 = +)
0.880952
0.119048

X1

X2

E1

E2

H1

H2

Figure 1: A simple Bayesian network, under sensor readings {E1 = +, E2 = +}. Variables H1
and H2 represent the health of sensors E1 and E2 . On the left is the posterior on
the decision variable D. Network CPTs can be found in Appendix C in Figure 13.

Przytula (2006) and Krause and Guestrin (2009) to select variables in order to minimize
the conditional entropy is then R(Pr (D | e)) = ‚àíH(D | e), so maximizing the expected
reward of observing variables H is then equivalent to minimizing the conditional entropy
H(D | H). Some other possible reward functions involve utility-based reward functions or
threshold-based reward functions (Munie & Shoham, 2008).4
Note that the vast majority of selection criteria use a myopic approach, in which out of
all possible observations, just one observation is considered at a time, and the observation
with the highest VOI is selected each time. This approach is greedy and short-sighted ‚Äî
the optimal VOI can only be computed by computing it non-myopically (Bilgic & Getoor,
2011). We discuss the usage of non-myopic VOI in Appendix A.1.

3. Same-Decision Probability
The Same-Decision Probability (SDP) was initially introduced by Darwiche and Choi (2010)
as a confidence measure for threshold-based decisions in Bayesian networks under noisy
sensor readings. Prior to formally defining the SDP, we first show an example to provide
intuition. Consider now the Bayesian network in Figure 1, which models a scenario involving
a hypothesis variable D, and two noisy sensors E1 and E2 that influence our belief in some
hypothesis d. Networks such as this are typically used to compute the belief in the hypothesis
given some sensor readings, Pr (d | e). The basis of whether or not to make a decision often
then depends on whether or not the posterior probability of that hypothesis d surpasses
some threshold T (Hamscher, Console, & de Kleer, 1992; Heckerman et al., 1995; Kruegel
et al., 2003; Lu & Przytula, 2006).
Figure 1 shows a particular reading of two sensors and the resulting belief Pr (D = + |
E1 = +, E2 = +). Suppose our threshold is T = 0.6, then as Pr (d | e) ‚â• T , we would make a
certain decision. Notice in Figure 1 that the health of those sensors is modeled by variables
H1 and H2 . A sensor can either be truthful, stuck positive (readings always display as +),
or lying (readings show the opposite value of the actual value) (Darwiche & Choi, 2010).
If those variables had been observed, they could have informed us of the trustworthiness of
4. For more on reward functions, see the list provided by Krause and Guestrin (2009).

605

Chen, Choi & Darwiche

H1
t
p
l
t
p
l
t
p
l

H2
t
t
t
p
p
p
l
l
l

Pr (h | e)
0.781071
0.096429
0.001071
0.096429
0.021429
0.001190
0.001071
0.001190
0.000119

Pr (d | h, e)
0.90
0.82
0.10
0.90
0.50
0.10
0.90
0.18
0.10

Table 1: Scenarios h for sensor readings e = {E1 = +, E2 = +} for the network in Figure 1,
where H = {H1 , H2 }. Cases above the threshold T = 0.6 are in bold. Note t, p, l
respectively represent a truthful, stuck positive, and lying sensor.

the sensors E1 and E2 and thus allow us to make a better decision. We want to make a
more-informed decision based on the probability Pr (d | h, e) instead of making a decision
based on just Pr (d | e).
Consider Table 1, which enumerates all of the possible health states of the sensors. In
only four of these cases does the probability of the hypothesis pass the threshold (in bold),
leading to the same decision. In the other five scenarios, a different decision would have
been made. The SDP is thus the probability of the four scenarios in which the same decision
would have been made. For this example, the SDP is:
0.781071 + 0.096429 + 0.096429 + 0.001071 = 0.975
indicating a relatively robust decision.
Choi et al. (2012) define the SDP formally as:
Definition 1 (Same-Decision Probability). Let N be a Bayesian network that is conditioned
on evidence e, where we are further given a hypothesis d, a threshold T , and a set of
unobserved variables H. Suppose we are making a decision that is confirmed by the threshold
Pr (d | e) ‚â• T . The Same-Decision Probability in this scenario is
X
[Pr (d | e, h) ‚â• T ]Pr (h | e),
(5)
SDP (d, H, e, T ) =
h

where [Pr (d | h, e) ‚â• T ] is an indicator function such that

1 if Pr (d | e, h) ‚â• T
[Pr (d | h, e) ‚â• T ] =
0 otherwise.
The SDP is notably hard to compute. Choi et al. (2012) prove that computing the SDP
is in general PPPP -complete.5 From previous work on the SDP (Darwiche & Choi, 2010;
Choi et al., 2012), the two options for computing the SDP are
5. The class PPPP can be thought of as a counting variant of the NPPP class, which contains the polynomial
time hierarchy PH and for which the MAP problem is complete (Park & Darwiche, 2004).

606

Algorithms and Applications for the Same-Decision Probability

D
+
‚àí

Pr (D)
0.5
0.5

D

S1

S2

S3

S4

Figure 2: A Bayesian network for intrusion detection, with its CPTs given in Table 2
1. An approximate algorithm developed by Choi et al. (2012). This algorithm uses an
augmented variable elimination algorithm that produces a potentially weak bound
based on the one-sided Chebyshev inequality.
2. A naive brute-force method that enumerates over all possible instantiations.

4. Applying the Same-Decision Probability
We investigate the use of the SDP as a stopping criterion and a selection criterion. We
contrast the usage of the SDP with traditional methods discussed in Section 2, and find
that using the SDP can provide more insight to a decision maker in some scenarios.
4.1 SDP as a Stopping Criterion
By the definition of SDP, we can see that calculating a high SDP, in contrast to calculating
a low SDP, would indicate a higher degree of readiness to make a decision, as the chances
of the decision changing given further evidence gathering is lower. In this section we show
that computing the SDP can provide additional insight and thus can distinguish scenarios
that are otherwise indistinguishable based on standard stopping criteria.
The threshold-based decision is a classical notion in decision making under uncertainty,
and it is commonly used as it requires no utilities to be elicited. Examples of thresholdbased decisions are very prevalent in educational diagnosis (Gertner, Conati, & VanLehn,
1998; Conati, Gertner, & VanLehn, 2002; Butz et al., 2004; Xenos, 2004; Arroyo & Woolf,
2005; Munie & Shoham, 2008), intrusion detection (Kruegel et al., 2003; Modelo-Howard
et al., 2008), fault diagnosis (Heckerman et al., 1995; Lu & Przytula, 2006), and medical
diagnosis (Pauker & Kassirer, 1980; Kahn et al., 1997; van der Gaag & CoupeÃÅ, 1999).
Consider the sensor network in Figure 2, which may correspond to an intrusion detection
application as discussed by Kruegel et al. (2003). Here, the hypothesis variable is D =
{+, ‚àí} with D = + implying an intrusion. Suppose we commit to a decision, and stop
performing observations, when our belief in the event D = + surpasses some threshold T ,
say T = 0.55. There are four sensors in this model, S1 , S2 , S3 and S4 , whose readings may
affect this decision.
Consider the two following scenarios:
1. S1 = + and S2 = +.
2. S3 = + and S4 = +.
607

Chen, Choi & Darwiche

D S1 Pr (S1 | D)
+ +
0.55
+ ‚àí
0.45
‚àí +
0.45
0.55
‚àí ‚àí

D S3 Pr (S3 | D)
+ +
0.60
+ ‚àí
0.40
‚àí +
0.40
0.60
‚àí ‚àí

D S2 Pr (S2 | D)
+ +
0.55
0.45
+ ‚àí
‚àí +
0.45
‚àí ‚àí
0.55

D S4 Pr (S4 | D)
+ +
0.65
0.35
+ ‚àí
‚àí +
0.35
‚àí ‚àí
0.65

Table 2: CPTs for the network in Figure 2. Parameterization 1.
Since Pr (D = + | S1 = +, S2 = +) = 0.60 > 0.55 and Pr (D = + | S3 = +, S4 = +) =
0.74 > 0.55, it is clear that in both cases that the threshold has been crossed. We deem
that no further observations are necessary based on our beliefs surpassing our threshold.
Hence, when using thresholds as a stopping criterion (as is commonly done, see Kruegel
et al., 2003; Lu & Przytula, 2006; Gao & Koller, 2011), the two scenarios are identical in
that no more information is gathered and a decision is made.
From the viewpoint of SDP, however, these two scenarios are very different. In particular, the first scenario leads to an SDP of 52.97%. This means that there is a 47.03% chance
that a different decision would be made if we were to further observe the two unobserved
sensors S3 and S4 . The second scenario, however, leads to an SDP of 100%. That is, we
would with certainty know that we would make the same decision if we were to also observe
the two unobserved sensors S1 and S2 : no matter what the readings of S1 and S2 could be,
our beliefs in the event D = + would always surpass our threshold 0.55. Indeed, as we can
see in Table 2, the sensors S1 and S2 are not as strong as sensors S3 and S4 , and in this
example, they are not strong enough to reverse our decision.
This example provides a clear illustration of the utility of the SDP as a stopping criterion.
However, some may argue that it is clear that in the second case, we should stop gathering
information as Pr (D = + | S3 = +, S4 = +) = 0.74 has a larger margin from the threshold
than Pr (D = + | S1 = +, S2 = +) = 0.60.6 However, we show with the following example
that deciding to stop based solely on the margin is not robust. Consider once again the
sensor network in Figure 2 and the parameterizations of the sensor network shown in Table 5
and Table 6 (found in Appendix C), which we respectively refer to as Case 1 and Case 2.7
Note that in this example, we use a threshold of T = 0.5.
In both cases, when S3 = + and S4 = + are observed, Pr (D = + | S3 = +, S4 = +) ‚â• T .
In particular,
1. Case 1: Pr (D = + | S3 = +, S4 = +) = 0.775.
6. Thus using the aforementioned margins of confidence stopping criterion used by Gao and Koller (2011).
7. Note that the exact numbers of the CPTs are not necessary to grasp these examples ‚Äî CPTs are
provided so that readers may reconstruct these networks.

608

Algorithms and Applications for the Same-Decision Probability

2. Case 2: Pr (D = + | S3 = +, S4 = +) = 0.599.
By using the previously discussed margin stopping criterion, it would seem that in Case
1 we could stop information gathering, whereas for Case 2 more information gathering is
necessary. However, we can compute the SDP for both cases for more insights on the
nature of robustness in these settings. For Case 1, we find that the SDP is 0.781, whereas
for Case 2, we find that the SDP is 1.0 ‚Äî even though in Case 1 the margin is higher,
there is a greater chance that the decision would change given further information. This
demonstrates that we cannot use solely the margin to determine whether or not to stop
information gathering.
It is clear from these examples that SDP is a useful stopping criterion. First, the SDP
can pinpoint situations where further observations are unnecessary as they would never
reverse the decision under consideration. Second, the SDP can also identify situations
where the decision to be made is not robust, and is likely to change upon making further
observations. In addition to these examples, in Appendix A.2 we show how the SDP can be
a useful stopping criterion in the context of utility-based decisions (e.g. influence diagrams).
4.2 SDP as a Selection Criterion
We now turn our attention to the use of SDP as a criterion for deciding which variables to
observe next, assuming that some stopping criterion indicates that further observations are
necessary. Our proposal is based on using VOI as the selection criterion (see Equation 3),
while choosing the SDP as the reward function. We call this the SDP gain, and it is formally
defined as:
Definition 2. Given Definition 4 of an SDP, the SDP gain of observing variables G out
of variables H is defined as the expected SDP of observing G ‚äÜ H subtracted by the SDP
over H:
G(G) = E(G, H, e, T ) ‚àí SDP (d, H, e, T ),
(6)
where the expected SDP is defined as:
E(G, H, e, T ) =

X

SDP (d, H \ G, ge, T )Pr (g|e)

(7)

g

and d is defined as the decision made given the current evidence.
Note that if we observe some variables G ‚äÜ H such that the expected SDP is 1.0, that
indicates that after observing G and making a decision, the remaining variables H \ G will
be rendered completely redundant ‚Äî their observation will have no effect on the decision.
The goal of using the SDP gain as a selection criterion is to observe those variables which,
on average, will allow for the most stable decision given the collected observations.
We will next provide an example of using SDP as a selection criterion, contrasting it
with two other selection criteria: One based on reducing entropy of the hypothesis variable
D, and another based on maximizing the gap between the decision probability Pr (d|e) and
the given threshold T (Krause & Guestrin, 2009). While both criteria can be motivated as
reducing uncertainty, we show that both can indeed lead to less stable decisions than if the
SDP were to be used.
609

Chen, Choi & Darwiche

D

D
+
‚àí

Pr (D)
0.5
0.5
S1

S2

Figure 3: A Bayesian network with its CPTs given in Appendix C.

The example is given by the Bayesian network in Figure 3, where D is the hypothesis
variable and S1 /S2 are sensors. A decision is triggered when Pr (D = + | e) ‚â• .80, where
evidence e is over sensors S1 and S2 . With no observations (empty evidence e), the SDP
is 0.595, suggesting that further observations may be needed. Assuming a limited number
of observations (Heckerman et al., 1995), and using a myopic approach of observing one
variable at a time (Dittmer & Jensen, 1997), we need now to select the next variable to
observe.
Note that maximizing VOI with negative entropy as the reward function amounts to
maximizing mutual information, as H(D, X) = H(D) ‚àí H(D | X) (Cover & Thomas, 1991;
Krause & Guestrin, 2005). The mutual information between variable D and sensor S2 is
0.53 whereas the mutual information between D and sensor S1 is 0.278. Hence, observing
S2 will reduce the entropy of D the most. In terms of margin of confidence, another reward
function used by Krause and Guestrin (2009), observing S2 will on average lead to a 0.7
margin between the states D = + and D = ‚àí, whereas observing S1 will only lead to a 0.6
margin between the two states.
However, if we compute the corresponding SDP gains, G(S1 ) and G(S2 ), we find that
observing S1 will, on average, lead to improving the decision stability the most. In particular, observing S1 would give us an SDP of either 1 or 0.81 ‚Äî for an expected SDP
of 0.905, whereas observing S2 would give us an SDP of either 0.7625, 0.5, or 1 ‚Äî for an
expected SDP of 0.805. Therefore, G(S1 ) = 0.31 and G(S2 ) = 0.21. Hence, observing S1
will on average allow us to make a decision that is less likely to change due to additional
information (beyond S1 ).
Some intuition to why this occurs is that although observing S2 leads to greater information gain than observing S1 , it is superfluous information. Note that Pr (D = + | S2 = ‚àí) =
0.0625, whereas Pr (D = + | S1 = ‚àí) = 0.2. Clearly, we can see that observing S2 can lead
to a more skewed distribution with minimal conditional entropy. However, in the context
of threshold-based decisions, we make a decision based solely on whether Pr (D = + | e) is
above or below the threshold, meaning that we may not put as much emphasis on how much
below or above the threshold Pr (D = + | e) is. In this case, although observing S2 can on
average lead to a more extreme distribution, observing S2 = o leads to making an extremely
nonrobust decision (a decision that would change 50% of the time with observation of S1 ).
Observing S1 before making a decision leads to a much more robust decision. This example
demonstrates the usefulness of SDP as a selection criterion for threshold-based decisions,
as the SDP can be used to select observations that lead to more robust decisions.
610

Algorithms and Applications for the Same-Decision Probability

D
+
‚àí

Pr (D)
0.3
0.7

D

E1

H1

H3

H2

Figure 4: A Naive Bayes network (CPTs defined in Appendix C).

5. Computing the Same-Decision Probability
Computing the SDP involves computing an expectation over the hidden variables H. The
naive brute-force algorithm would enumerate and check whether Pr (d | h, e) ‚â• T for all
instantiations h of H. We now present an algorithm that can save us the need to explore
every possible instantiation of h. To make the algorithm easier to understand, we will first
describe how to compute the SDP in a Naive Bayes network. This is no trivial problem ‚Äî
we show in Section 7 that computing the SDP in a Naive Bayes network is NP-hard. We
then generalize our algorithm to arbitrary networks.
5.1 Computing the SDP in Naive Bayes Networks
We will find it more convenient to implement the test Pr (d | h, e) ‚â• T in the log-odds
domain, where:
log O(d | h, e) = log

Pr (d | h, e)
Pr (d | h, e)

(8)

T
We then define the log-odds threshold as Œª = log 1‚àíT
and, equivalently, test whether
log O(d | h, e) ‚â• Œª.
In a Naive Bayes network with D as the class variable, H and E as the leaf variables,
and Q ‚äÜ H, the posterior log-odds after observing a partial instantiation q = {h1 , . . . , hj }
can be written as:

log O(d | q, e) = log O(d | e) +

j
X

w hi

(9)

i=1

where whi is the weight of evidence hi and defined as:
whi = log

Pr (hi | d, e)
Pr (hi | d, e)

(10)

The weight of evidence whi is then the contribution of evidence hi to the quantity
log O(d | q, e) (Chan & Darwiche, 2003). Note that all weights can be computed in time
and space linear in |H| using a floating point representation.8 Table 3 depicts the weights
of evidence for the network in Figure 4.
8. Additionally, note that for Equation 10, since in Naive Bayes networks Hi is d-separated from E given
d, the term e can be dropped from the equation. We leave the term in because for general networks, Hi
may not be d-separated from E.

611

Chen, Choi & Darwiche

i
1
2
3

w hi
3.0
1.22
1.22

w hi
-2.17
-1.22
-1.22

Table 3: Weights of evidence for the attributes in Figure 4.
H1
0.0
H2
3.0

H2
-2.17

H3
4.22
5.44

H3
1.78
3.0

3.0

H3
‚àí0.95
0.56

0.27

‚àí2.17

H3
‚àí3.39
‚àí2.17

‚àí4.61

Figure 5: The search tree for the network of Figure 4. A solid line indicates + and a dashed
line indicates ‚àí. The quantity log O(d | q, e) is displayed next to each node q in
the tree. Nodes with log O(d | q, e) ‚â• Œª = 0 are shown in bold.

One can then compute the SDP by enumerating the instantiations of variables H and
then using Equation 9 to test whether log O(d | h, e) ‚â• Œª. Figure 5 depicts a search tree
for the Naive Bayes network in Figure 4, which can be used for this purpose. The leaves of
this tree correspond to instantiations h of variables H. More generally, every node in the
tree corresponds to an instantiation q, where Q ‚äÜ H.
A brute-force computation of the SDP would then entail:
1. Initializing the total SDP to 0.
2. Visiting every leaf node h in the search tree.
3. Checking whether log O(d | h, e) ‚â• Œª and if so, adding Pr (h | e) to the total SDP.
Figure 5 depicts the quantity log O(d | q, e) for each node q in the tree, indicating that five
leaf nodes (i.e., five instantiations of variables H) will indeed contribute to the SDP.
We now state the key observation underlying our proposed algorithm. Consider the node
corresponding to instantiation H1 = + in the search tree, with log O(d | H1 = +, e) = 3.0.
All four completions h of this instantiation (i.e., the four leaf nodes below it) are such that
log O(d | h, e) ‚â• Œª = 0. Hence, we really do not need to visit all such leaves and add their
contributions Pr (h|e) individually to the SDP. Instead, we can simply add Pr (H1 = +|e)
to the SDP, which equals the sum of Pr (h|e) for these leaves. More importantly, we can
detect that all such leaves will contribute to the SDP by computing a lower bound using the
weights depicted in Table 3. That is, there are two weights for variable H2 , the minimum
of which is ‚àí1.22. Moreover, there are two weights for variable H3 , the minimum of which
‚àí1.22. Hence, the lowest contribution to the log-odds made by any leaf below node H1 = +
612

Algorithms and Applications for the Same-Decision Probability

H1
0.0
3.0

H2
-2.17
H3
‚àí0.95
0.27

‚àí3.39

‚àí2.17

Figure 6: The reduced search tree for the network of Figure 5.
will be ‚àí1.22 ‚àí 1.22 = ‚àí2.44. Adding this contribution to the current log-odds of 3.0 will
lead to a log-odds of .56, which still passes the given threshold.
A similar technique can be used to compute upper bounds, allowing us to detect nodes
in the search tree where no leaf below them will contribute to the SDP. Consider for example
the node corresponding to instantiation H1 = ‚àí, H2 = ‚àí, with log O(d | H1 = ‚àí, H2 =
‚àí, e) = ‚àí3.39. Neither of the leaves below this node will contribute to the SDP as their
log-odds do not pass the threshold. This can be detected by considering the weights of
evidence for variable H3 and computing the maximum of these weights (1.22). Adding this
to the current log-odds of ‚àí3.39 gives ‚àí2.17, which is still below the threshold. Hence, no
leaf node below H1 = ‚àí, H2 = ‚àí will contribute to the SDP and this part of the search tree
can also be pruned.
If we apply this pruning technique based on lower and upper bounds, we will actually
end up exploring only the portion of the tree shown in Figure 6. The pseudocode of our
final algorithm is shown in Algorithm 1. Note that it takes linear time to compute the
upper and lower bounds. Additionally, note that the specific ordering of H in which the
search tree is constructed is directly linked to the amount of pruning. We use an ordering
heuristic that ranks each query variable Hi by the difference of its corresponding upper and
lower bound ‚Äî H is then ordered from greatest difference to lowest difference as to allow
for earlier pruning.
5.2 Computing the SDP in Arbitrary Networks
We will generalize our algorithm to arbitrary networks by viewing such networks as Naive
Bayes networks but with aggregate attributes. For this, we first need the following notion.
Definition 3. A partition of H given D and E is a set S1 , . . . , Sk such that: Si ‚äÜ H;
Si ‚à© Sj = ‚àÖ; S1 ‚à™ . . . ‚à™ Sk = H; and Si is independent (d-separated) from Sj , i 6= j, given
D and E.
Figure 7 depicts an example partition.
The intuition behind a partition is that it allows us to view an arbitrary network as a
Naive Bayes network, with class variable D and aggregate attributes S1 , . . . , Sk . That is,
each aggregate attribute Si is viewed as a variable with states si , allowing us to view each
instantiation h as a set of values s1 , . . . , sk . We now have:

613

Chen, Choi & Darwiche

Algorithm 1 Computing the SDP in a Naive Bayes network. Note: For q = {h1 , . . . , hj },
P
wq is defined as ji=1 whi .

input:
N : Naive Bayes network with class variable D
H: attributes {H1 , . . . , Hk }
Œª: log-odds threshold
e: evidence
output: Same-Decision Probability p

main:
global p ‚Üê 0.0 (initial probability)
q ‚Üê {} (initial instantiation is empty set)
depth ‚Üê 0 (initial depth of search tree)
DFS SDP(q, H, depth)
return p
1: procedure DFS SDP(q, H, depth)
P
2:
U pperBound ‚Üê log O(d | e) + wq + ki=depth+1 maxhi whi
P
3:
LowerBound ‚Üê log O(d | e) + wq + ki=depth+1 minhi whi
4:
if (U pperBound < Œª) then return
5:
else if (LowerBound ‚â• Œª) then
6:
add Pr (q | e) to p, return
7:
else
8:
if depth < k then
9:
for each value hdepth+1 of attribute Hdepth+1 do
10:
DFS SDP(qhdepth+1 , H \ Hdepth+1 , depth + 1)
Proposition 1. For a partial instantiation q = {s1 , . . . , sj },
log O(d | q, e) = log O(d | e) +

j
X

w si ,

(11)

i=1

where
wsi = log

Pr (si , | d, e)
Pr (si | d, e)

Proof.
Pr (d | q, e)
Pr (d | q, e)
Pr (d | e)Pr (s1 | d, e) . . . Pr (sj | d, e)
= log
Pr (d | e)Pr (s1 | d, e) . . . Pr (sj | d, e)

log O(d | q, e) = log

= log O(d | e) +

j
X
i=1

614

w si

(12)

Algorithms and Applications for the Same-Decision Probability

E1

X1

H1

H4

D

X2

H3

H5

E2

X3

H6

H2

Figure 7: The partition of H given D and E is: S1 = {H1 , H2 , H3 } S2 = {H4 }, S3 =
{H5 , H6 }.

Since Equations 11 and 12 are analogous to Equations 9 and 10, we can now use Algorithm 1 on an arbitrary network. This usage, however, requires some auxiliary computations
that were not needed or were readily available for Naive Bayes networks. We discuss these
computations next.
5.2.1 Finding a Partition
We first need to compute a partition S1 , . . . , Sk , which is done by pruning the network
structure as follows: we delete edges outgoing from nodes in evidence E and hypothesis D,
and delete (successively) all leaf nodes that are neither in H, E or D. We then identify the
components X1 , . . . , Xk of the resulting network and define each non-empty Si = H ‚à© Xi
as an element of the partition. This guarantees that in the original network structure, Si
is d-separated from Sj by D and E for i 6= j (see (Darwiche, 2009)). In Figure 7, network
pruning leads to the components X1 = {X1 , X2 , E2 , H1 , H2 , H3 }, X2 = {D, E1 , H4 } and
X3 = {X3 , H5 , H6 }.
5.2.2 Computing Posterior Log-Odds, Probability and Weights of Evidence
The quantities O(d | e), Pr (q | e) and wsi , which are referenced on Lines 2, 3, and 6 of
the algorithm, have simple closed forms in Naive Bayes networks. For arbitrary networks,
however, computing these quantities requires inference which we do using the algorithm
of variable elimination as described by Darwiche (2009). Note that the network pruning
of deleting edges and removing the leaf nodes, as discussed above, guarantees that each
factor used by variable elimination will have all its variables in some component Xi . Hence,
variable elimination can be applied to each component Xi in isolation, which is sufficient
to obtain all needed quantities.
615

Chen, Choi & Darwiche

5.2.3 Computing the Min and Max of Evidence Weights
We finally show how to compute the upper and lower bounds, maxsi wsi and minsi wsi ,
which are referenced on Lines 2 and 3 of the algorithm. These quantities can also be
computed using variable elimination, applied to each component Xi in isolation. In this
case, however, we must eliminate variables Xi \ Si first and then variables Si . Moreover,
the first set of variables is summed-out, while the second set of variables is max‚Äôd-out or
min‚Äôd-out, depending on whether we need maxsi wsi or minsi wsi . Finally, this elimination
process is applied twice, once with evidence d, e and a second time with evidence d, e.
More precisely, for every component Xi we have a set of factors for the case where D = d
and where D = d. Using the same variable ordering, we perform variable elimination on both
sets of factors to eliminate
so that
are left with a
Q iany nonquery (intermediary) variables
Q we
i
i
i
set of factors œàd where œàd = Pr (Si , d, e), and a set of factors œÜd where œÜd = Pr (Si , d, e).
Since the elimination order was the same, there is thus a one-to-one matching between
œài
Pr i (ei ,S )
factors from both sets, and we can define a new set of factors œái = œÜid = Pr id (ei ,Si ) . We
d

d

i

can then calculate wsi and wsi by respectively maximizing and minimizing out variables.
Note that summing out variables and then maximizing variables is the variable elimination
algorithm used by Dechter (1999) in order to solve MAP. Our algorithm here differs as we
perform both maximization and minimization (to both calculate wsi and wsi ), and do so on
the set of factors œái instead of on the factors (œàdi or œÜdi ) that result from simply summing
out the intermediary variables.
Note that similarly to Dechter (1999), as we are first summing out variables and then
performing some maximization (and minimization in our case), the elimination order in
this case is constrained, meaning that we may be forced to use a poor ordering for variable
elimination that results in a high treewidth.
5.3 Complexity Analysis
Let n be the number of variables in the network, h = |H|, and w = maxi wi , where
wi is the width of constrained elimination order used
on component Xi . The best-case

time complexityof our algorithm is then O n exp w and the worst-case time complexity is
O n exp (w + h) . The intuition behind these bounds is that computing
 the maximum and
minimum weights for each aggregate attribute takes time O n exp w . This also bounds
the complexity of computing O(d|e), Pr (q|e) and corresponding weights wsi . Moreover,
depending on the weights and the
 threshold T , traversing the search tree can take anywhere
from constant time to O exp h . Since depth-first
search can be implemented with linear

space, the space complexity is O n exp w .

6. Experimental Results
We performed several experiments on both real and synthetic networks to test the performance of our algorithm across a wide variety of network structures, ranging from simple
Naive Bayes networks to highly connected networks. Real networks were either learned from
datasets provided by the UCI Machine Learning Repository (Bache & Lichman, 2013) or
616

Algorithms and Applications for the Same-Decision Probability

Network
car
emdec6g
tcc4e
ttt
caa
voting
nav
fire
chess

source
UCI
HRL
HRL
UCI
CRESST
UCI
CRESST
CRESST
UCI

|H|
|h|
6
144
8
256
9
512
9
19683
14
16384
16
65536
20 1572864
24 16777216
30 1610612736

naive
0.131
0.407
0.470
6.234
6.801
21.35
642.88
œÜ
œÜ

approx
0.118
0.245
0.257
0.133
0.145
0.176
0.856
0.183
*

new
0.049
0.294
0.149
0.091
0.167
0.128
0.178
0.508
15.53

Table 4: Algorithm comparison on real networks. We show the time, in seconds, it takes
each algorithm, naive, approx, and new to compute the SDP in different networks.
Note that œÜ indicates that the computation did not complete in the 20 minute
time limit that we constrained. Moreover, * indicates that there was not sufficient
memory to complete the computation.

provided by HRL Laboratories and CRESST.9 The majority of the real networks used were
diagnostic networks, which made it clear which variable should be selected as the decision
variable as it would either be the ‚Äúknowledge‚Äù or ‚Äúfault‚Äù variable. For the unclear cases, the
decision variable was picked at random. Both query and evidence variables were selected
at random for all real networks.
Besides this algorithm, there are two other options available to compute the SDP: 1. the
naive method to brute-force the computation by enumerating over all possible instantiations
or 2. the approximate algorithm developed by Choi et al. (2012). To compare our algorithm
with these two other approaches, we compute the SDP over the real networks. For each
network we selected at least 80% of the total network variables to be query variables so
that we could emphasize how the size of the query set greatly influences the computation
time. Each computation was given 20 minutes to complete. As we believe that the value of
the threshold can greatly affect running time, we computed the SDP with thresholds T =
[0.01, 0.1, 0.2, . . . , 0.8, 0.9, 0.99] and took the worst-case time. The results of our experiments
with the three algorithms are shown in Table 4. Note that |H| is the number of query
variables and |h| is the number of instantiations the naive algorithm must enumerate over.
Moreover, œÜ indicates that the computation did not complete in the 20 minute time limit and
* indicates that there was not sufficient memory to complete the computation. The networks
{car,ttt,voting,nav,chess} are Naive Bayes networks whereas the networks {caa,fire} are
polytree networks and the others are more general networks.
Given the real networks that we tested our algorithm on, it is clear that the algorithm
outperforms both the naive implementation and the approximate algorithm for both Naive
Bayes networks and polytree networks. Note that the approximation algorithm is based
on variable elimination but can only use certain constrained orders. For a Naive Bayes
9. http://www.cse.ucla.edu/

617

Chen, Choi & Darwiche

4000
3500

Average Explored Instantiations and Running Time
Number of instantiations (x 10e3)

3000
2500
2000
1500
1000
500
01

Time (s)
2

3

4
5
Number of subnetworks

6

7

8

Figure 8: Synthetic network average running time and average number of instantiations
explored by number of connected components.

network with hypothesis D being the root, the approximation algorithm will be forced to
use a particularly poor ordering, which explains its failure on the chess network.
To analyze how a more general network structure and the selected threshold affects
the performance of our algorithm, we generated synthetic networks with 100 variables and
varying treewidth using BNGenerator (Ide, Cozman, & Ramos, 2004). For each network,
we randomly selected the decision variable, 25 query variables, and evidence variables.10 We
then generated a partition for each network and grouped the networks by the size of obtained
partition (k). Our goal was to test how our algorithm‚Äôs running time and ability to prune
the search-space depends on k. The average time and average number of instantiations
explored are shown in Figure 8.
In general, we can see that as k increases, the number of instantiations explored by
the algorithm decreases and its runtime improves. The network becomes more similar to a
Naive Bayes structure with increasing k. Moreover, the larger k is, the more levels there are
in the search tree, which means that our algorithm will have more opportunities to prune.
In the worst case, a network may be unable to be disconnected at all (k = 1). However, even
in this case our algorithm is still, on average, more efficient compared to the brute-force
implementation ‚Äî for some cases, after computing the maximum and minimum weight of
observing H, it will find that there does not exist any h that will change the decision. We
found that, given a time limit of 2 hours, the brute-force algorithm could not solve any
synthetic networks, whereas our approach solved more than 70% of such networks.
We also test how the threshold affects computation time. Here, we calculate the posterior
probability of the decision variable and then run repeatedly our algorithm with thresholds
that are varying increments away. The average running time for all increments can be seen
in Figure 9. It is evident that when the threshold is set to be further away from the initial
10. As the synthetic networks are binary, a brute-force approach would need to explore 225 instantiations.

618

Algorithms and Applications for the Same-Decision Probability

1800
1600

Average Explored Instantiations and Running Time
Number of instantiations (x 10e3)

1400
1200
1000
800
600
400
200
00.0

Time (s)
0.1

0.2
0.3
0.4
0.5
Threshold distance from initial posterior

0.6

Figure 9: Synthetic network average running time and average number of instantiations
explored by threshold distance from the initial posterior probability.

posterior probability, the algorithm finishes much faster, which is perhaps expected since
the usage of more extreme thresholds would allow for more search space pruning.
Overall, our experimental results show that our algorithm is able to solve many SDP
problems that are out of reach of existing methods. We also confirm that our algorithm
completes much faster when the network can be disconnected or when the threshold is far
away from the initial posterior probability of the decision variable.

7. The Complexity of Computing the Same-Decision Probability
We present here new complexity results for the SDP. We first prove that the complexity of
computing the SDP in Naive Bayes structures is NP-hard. We then show that the general
complexity of computing the SDP lies in the same complexity class as a general expectation
computation problem that is applicable to a wide variety of queries in graphical models,
such as the computation of non-myopic value of information.
7.1 Computing the SDP in Naive Bayes
SDP is known to be PPPP -complete (Choi et al., 2012). We now show that SDP remains
hard for Naive Bayes networks.
Theorem 1. Computing the Same-Decision Probability in a Naive Bayes network is NPhard.
Proof. We reduce the number partition problem defined by Karp (1972) to computing the
SDP in a Naive Bayes model. Suppose we are given a set of positive P
integers c1P
, . . . , cn , and
we wish to determine whether there exists I ‚äÜ {1, . . . , n} such that j‚ààI ci = j6‚ààI cj . We
can solve this by considering a Naive Bayes network with a binary class variable D having
uniform probability, and binary attributes H1 , . . . , Hn having CPTs leading to weights of
619

Chen, Choi & Darwiche

evidence wHi =T = ci and wHi =F = ‚àíci . The construction of these CPTs can be done by
solving the following system of equations:
Pr (Hi = T | D = T )
Pr (Hi = T | D = F )
Pr (Hi = F | D = T )
‚àíci = log
Pr (Hi = F | D = F )
1 = Pr (Hi = T | D = T ) + Pr (Hi = F | D = T )
ci = log

1 = Pr (Hi = T | D = F ) + Pr (Hi = F | D = F )

We leave the exact derivations out (see Exercise 3.27 in Darwiche, 2009). We get the
result that:
Pr (Hi = T | D = F ) = Pr (Hi = F | D = T ) =

2ci

1
+1

Pr (Hi = T | D = T ) = Pr (Hi = F | D = F ) = 1 ‚àí

2ci

1
+1

Note that given that these CPTs have been defined such that wHi =T = ci and wHi =F =
‚àíci , the set of integers can be partitioned if there is an instantiation h = {h1 , . . . , hn } with
P
n
i=1 whi = 0 since I would then include all indices i with hi = T in this case.
First,
PnThe Naive Bayes network satisfies a number of properties that we shall use
Pnext.
n
whi is either 0, ‚â• 1, or ‚â§ ‚àí1 since all weights whi are integers. Next, if i=1 whi = c,
i=1P
then ni=1 wh‚Ä≤i = ‚àíc where h‚Ä≤i 6= hi . Finally, Pr (h1 , . . . , hn ) = Pr (h‚Ä≤1 , . . . , h‚Ä≤n ) when h‚Ä≤i 6= hi ,
as D has a uniform probability distribution and each leaf Hi has been defined with a
symmetric CPT.
Consider now the following SDP (the last step below is based on the above properties):
SDP (D = T, {H1 , . . . , Hn }, {}, 2/3)
X
[Pr (D = T | h1 , . . . , hn ) ‚â• 2/3]Pr (h1 , . . . , hn )
=
h1 ,...,hn

=

X

[log O(D = T | h1 , . . . , hn ) ‚â• 1]Pr (h1 , . . . , hn )

h1 ,...,hn

=

X

h1 ,...,hn

"

1 X
=
2

n
X

h1 ,...,hn

We then have

Pn

i=1 whi

#

whi ‚â• 1 Pr (h1 , . . . , hn )

i=1

"

n
X

#

whi 6= 0 Pr (h1 , . . . , hn )

i=1

= 0 for some instantiation h1 , . . . , hn iff
" n
#
X X
whi 6= 0 Pr (h1 , . . . , hn ) < 1
h1 ,...,hn

i=1

620

Algorithms and Applications for the Same-Decision Probability

Hence, the partitioning problem can be solved iff
SDP (D = T, {H1 , . . . , Hn }, {}, 2/3) < 1/2

7.2 The Complexity of Computing the Non-myopic VOI
The SDP was shown to be a PPPP -complete problem by Choi et al. (2012). The class PPPP is
essentially the counting variant of the NPPP class, which contains the polynomial hierarchy
PH and for which the MAP problem is complete (Park & Darwiche, 2004). We show in this
section that a general problem of computing expectations is also PPPP -complete, with the
non-myopic VOI and SDP being an instance of such an expectation. Thus, the development
of algorithms to compute the SDP will be beneficial to problems in the PPPP class, which
in turn benefits computing an assortment of expectations, including non-myopic VOI.
The proposed expectation computation is based on using a reward function R with
some properties that we review next. In particular, the function R is assumed to map a
probability distribution Pr (D | e) to a numeric value. We also assume that the minimum
l and maximum u of this range are polytime computable. These assumptions are not too
limiting‚Äîfor example, both entropy and utility can be expressed using reward functions
that fall in this category (Krause & Guestrin, 2009).
We now consider the following computation of expectations.
D-EPT: Given a polynomial-time computable reward function R, hypothesis variable
D, unobserved variables H, evidence e, a real number N , and a distribution Pr induced
by a Bayesian network over variables X,11 the expectation decision problem asks: Is
E=

X

R(Pr (D | h, e))Pr (h | e)

h

greater than N ?
Note that the SDP falls as a special case when the reward function R is the SDP indicator
function (see Definition 4). For example, in the definition used by Choi et al. (2012), the
decision function outputs one of two decisions depending on whether Pr (d|e) > T for some
value d of D and some threshold T .
We now have the following theorems, with proofs in Appendix B.
Theorem 2. D-EPT is PPPP -hard.
Theorem 3. D-EPT is in PPPP .
This shows that D-EPT is PPPP -complete and implies that computational problems
such as computing the non-myopic VOI using a variety of reward functions is also PPPP complete.
11. This proof also holds for influence diagrams constrained to have only one decision node.

621

Chen, Choi & Darwiche

8. Conclusion
In this paper, we have discussed some commonly used information gathering criteria for
graphical models such as value of information and have reviewed the recently introduced
notion of the Same-Decision Probability (SDP). In this paper, we have proposed the usage
of the SDP as a decision making tool by showing concrete examples of its usefulness as both
a stopping criterion and a selection criterion. As a stopping criterion, the SDP can allow
us to determine when no further observations are necessary. As a selection criterion, usage
of the SDP can allow us to select observations that allow us to increase decision robustness.
As we have justified the usage of the SDP, we have proposed an exact algorithm for its
computation. Experimental results show that this algorithm has comparable running time
to the previous approximate algorithm and is also much faster than the naive brute-force
algorithm. Finally, we have presented several new complexity results.

Acknowledgements
This paper combines and extends the work presented by Chen, Choi, and Darwiche (2012b,
2013). This work has been partially supported by ONR grant #N00014-12-1-0423, NSF
grant #IIS-1118122, and NSF grant #IIS-0916161. We would also like to thank the National
Center for Research on Evaluation, Standards, & Student Testing and Hughes Research Lab
for contributing sample diagnostic networks.

Appendix A. Miscellaneous Topics
In this section we go into more details about notions that were mentioned earlier in the
paper. In particular, we continue our discussion from Section 2.3 and go over the notion of
non-myopic value of information. Additionally, we also continue from where we left off in
Section 4.1 and expand upon the notion of SDP as a stopping criterion in the context of
utility-based decisions.
Appendix A.1 Non-myopic Value of Information
Myopic value of information is often used in many applications as it is easy to compute
(Dittmer & Jensen, 1997; Vomlel, 2004; Gao & Koller, 2011). However, the problem with
myopic selection is that it is not optimal, as at times the ‚Äúwhole is greater than the sum of
its parts‚Äù, as each individual observation in a set H seemingly may not provide significant
value, but the VOI of observing H can be very high. For instance, take the function
D = X1 ‚äï X2 , where alone neither X1 nor X2 is useful, but together they are determinative
of D (Bilgic & Getoor, 2011). Only by computing the non-myopic VOI can the the optimal
VOI be obtained.
Due to the aforementioned problems with using myopic VOI, more recently, researchers
have recently suggested using the non-myopic VOI instead of myopic VOI and have proposed various methods to compute the non-myopic VOI (Heckerman, Horvitz, & Middleton,
1993; Liao & Ji, 2008; Krause & Guestrin, 2009; Zhang & Ji, 2010; Bilgic & Getoor, 2011).
Computing the non-myopic VOI of some hidden variables H is difficult as it involves com622

Algorithms and Applications for the Same-Decision Probability

puting an expectation over the possible values of H, which quickly becomes intractable as
H becomes larger.
Existing algorithms for computing the non-myopic VOI are approximate algorithms
(Heckerman et al., 1993; Liao & Ji, 2008) or relatively limited algorithms that are restricted
to tree networks with few leaf variables (Krause & Guestrin, 2009). Bilgic and Getoor
(2011) have developed the Value of Information Lattice (VOILA), a framework in which
all subsets of hidden variables H are examined, and the optimal subset of features can be
found to increase classification accuracy while meeting some budget constraint.
Appendix A.2 SDP as a Stopping Criterion for Utility-based Decisions
In some cases the expected utility of different decisions, as well as the cost of reducing
uncertainty (making observations), is quantified. This is common in the decision-theoretic
setting (Howard, 1966; Howard & Matheson, 1984), where influence diagrams are commonly
used. Influence diagrams can be seen as Bayesian networks that incorporate decision and
utility nodes (Howard & Matheson, 1984; Zhang, 1998; Kj√¶rulff & Madsen, 2008). The
selection criterion for the decision-theoretic setting is clear: the observations that lead to
the greatest increase of expected utility are selected. The usage of utilities and observation
costs is now prevalent; however, numerous researchers have noted the difficulty of coming
up with the actual numerical quantities (Glasziou & Hilden, 1989; Lu & Przytula, 2006;
Bilgic & Getoor, 2011).
To show how the SDP can be used as a stopping criterion in the decision-theoretic context of expected-utility decisions and influence diagrams (Howard & Matheson, 1984), we
extend the definition of the SDP to a more general setting to allow for more applications.
In particular, we assume that F is a polytime computable decision function that outputs
a decision d based on the distribution Pr (D | e). For instance, the decision function most
commonly used in classification is to select the class with the highest posterior probability arg maxd Pr (d | e) (Friedman et al., 1997), whereas for threshold-based decisions, the
decision function would simply be to select a decision d if Pr (D = d | e) ‚â• T .
SDP is thus defined as the probability that the same decision would be made if the
hidden states of variables H were known (Chen et al., 2012b).
Definition 4 (Same-Decision Probability ‚Äî Generalized ). Given a decision function F,
hypothesis variable D, unobserved variables H, and evidence e, the Same-Decision Probability (SDP) is defined as
X
[F(Pr (D | h, e))]h Pr (h | e)
(13)
SDP (F, D, H, e) =
h

where [F(Pr (D | h, e))]h is an indicator function that

1 if F(Pr (D | h, e)) = F(Pr (D | e))
=
0 otherwise.
The original SDP definition, however, assumed that D is a binary variable, where
F(Pr (D | e)) = d when Pr (d | e) ‚â• T for some threshold T (Darwiche & Choi, 2010).
We now consider the use of SDP as a stopping criterion in the context of expectedutility decisions and influence diagrams (Howard & Matheson, 1984). In particular, we
623

Chen, Choi & Darwiche

Q

C

S
I

P

Figure 10: An influence diagram for an investment problem.
show that by using the SDP, we can distinguish high-risk, high-reward scenarios from lowrisk, low-reward scenarios that are otherwise indistinguishable when we consider the usage
of VOI/utilities alone.
Consider the influence diagram in Figure 10, which consists of a Bayesian network with
three variables (C, Q and S), a decision node I, and a utility node P that is a direct
function of the utility function u. This influence diagram models an investment problem
in which a venture capital firm is deciding whether to invest an amount of $5 million in a
tech startup (I = T ) or allowing the money to collect interest in the bank (I = F ). In this
example, the profit of the investment (P ) depends on the decision (I) and the success of the
company (S), which in turn depends on two factors: (1) whether the existing competitor
companies are successful (C) and (2) whether the the co-founders of the startup have a high
quality, original idea (Q). Both C and Q are unobserved initially and independent of each
other. Variable S is the latent hypothesis variable in this case and thus cannot be observed.
Variables C and Q, however, can be observed for a price.
The goal here is to choose the decision I = i with the maximum expected utility:
X
EU (i | e) =
Pr (s | e)u(i, s),
s

where u(i, s) is the utility of decision I = i given evidence e on variables C and Q.
Figures 11 and 12 contain two different parameterizations of the influence diagram in
Figure 10. We will refer to these as different scenarios of the investment problem.
In both scenarios, given no evidence on variables C and Q, the best decision is I = F ,
with an expected utility of $500K. A decision maker may commit to this decision or decide
to observe variables C and Q, with the hope of finding a better decision in light of the
additional information. The classical stopping criterion here is to compute the maximum
expected utility given that we observe variables C and Q (Heckerman et al., 1993; Dittmer
& Jensen, 1997):
X
max
EU (i | c, q)Pr (c, q).
i

c,q

In both scenarios, the maximum expected utility comes out to $1, 180K, showing that
further observations may lead to a better decision.12
12. According to the formulation of Krause and Guestrin (2009), we have computed the VOI for variables
C and Q using the reward function.

624

Algorithms and Applications for the Same-Decision Probability

Q
T
F

Pr (Q)
0.4
0.6

C
T
F

Pr (C)
0.6
0.4

Q
T
T
F
F

Pr (S = T | .)
0.60
0.90
0.20
0.30

C
T
F
T
F
I
T
T
F
F

S
T
F
T
F

u(I, S)
$5 √ó 106
‚àí$5 √ó 106
$5 √ó 105
$5 √ó 105

Figure 11: A parameterization of the influence diagram in Figure 10.

Q
T
F

Pr (Q)
0.1
0.9

C
T
F

Pr (C)
0.9
0.1

Q
T
T
F
F

Pr (S = T | .)
0.05
0.98
0.01
0.05

C
T
F
T
F
I
T
T
F
F

S
T
F
T
F

u(I, S)
$7 √ó 107
‚àí$5 √ó 106
$5 √ó 105
$5 √ó 105

Figure 12: A parameterization of the influence diagram in Figure 10.
Up to this point, the above two scenarios are indistinguishable from the viewpoint of
classical decision making tools. Remember that Krause and Guestrin (2009) and Bilgic and
Getoor (2011) remark that the budget for observations should be expended so long as the
value of information of the observation is greater than the cost of observation. According
to those selection criteria, both of the variables should thus be observed, as the expected
financial gain could very well increase.
The SDP, however, finds that these two scenarios are very different. In particular, with
respect to variables C and Q, the SDP is 60% in the first scenario and is 99% in the second
scenario. That is, even though we stand to make a better decision in both scenarios upon
observing variables C and Q (at least with respect to financial gain), and even though the
expected benefit from such observations is the same in both scenarios, it is very unlikely that
we would change the current decision of I = F in the second scenario in comparison to the
first. Hence, given the additional information provided by the SDP, a decision maker may
act quite differently in these two scenarios. Indeed, when we take a closer look at the second
scenario, there is a state of the world (when S = T ) where deciding to invest would yield a
very large financial gain. However, the chance of this state manifesting itself is extremely
625

Chen, Choi & Darwiche

small (analogous to a lottery), meaning that a risk-conscious decision maker may be more
averse to ‚Äúgamble‚Äù in the second scenario and even waste resources to observe the variables
C and D. Note that for this example we have assumed that the utility does not incorporate
any ‚Äúrisk-factor‚Äù, as if it did a rational decision maker would then always choose to gather
more information despite a low probability of changing the current decision.
This illustrates the usefulness of SDP as a stopping criterion in the context of expectedutility decisions and influence diagrams. Namely, using SDP, we can distinguish between
two very different scenarios, that are otherwise indistinguishable when we consider utilities
alone.

Appendix B. Proofs
In this section we provide proofs for Theorems 2 and 3.
Proof of Theorem 2. We show D-EPT is PPPP -hard by reduction from the following decision problem D-SDP, which corresponds to the originally proposed notion of same-decision
probability for threshold-based decisions (Darwiche & Choi, 2010).
D-SDP: Given a decision based on probability Pr (d | e) surpassing a threshold T , a set
of unobserved variables H, and a probability p, is the same-decision probability:
X
[Pr (d | h, e) ‚â• T ]Pr (h | e)
(14)
h

greater than p?
Here, [.] denotes an indicator function which evaluates to 1 if the enclosed expression is
satisfied, and 0 otherwise. D-SDP was shown to be PPPP -complete by Choi et al. (2012).
This same-decision probability corresponds to an expectation with respect to the distribution Pr (H | e), using the reward function:

1 if Pr (d | h, e) ‚â• T
R(Pr (D | h, e)) =
0 otherwise.
Thus the same-decision probability is ‚â• T iff this expectation is ‚â• T .
Proof of Theorem 3. To show that D-EPT is in PPPP , we provide a probabilistic polynomialtime algorithm, with access to a PP oracle, that answers the decision problem D-EPT
correctly with probability greater than 21 . This proof generalizes and simplifies the proof
given by Choi et al. (2012) for D-SDP.
Consider the following probabilistic algorithm that determines if E > N :
1. Sample a complete instantiation x from the Bayesian network, with probability Pr (x).
We can do this in linear time, using forward sampling (Henrion, 1986).
2. If x is compatible with e, we can use a PP-oracle to compute t = R(Pr (D | h, e)).
First, the reward function R can be computed in polynomial time, by definition.
Second, Pr (D | h, e) can be computed using a PP-oracle, since the inference is #Pcomplete (Roth, 1996), and since PPP = P#P .
626

Algorithms and Applications for the Same-Decision Probability

3. Define a function a(t) = 12 + 21 t‚àíN
u‚àíl , which defines a probability used by our probabilistic
algorithm to guess whether E > N (see Lemma 1).
4. Declare that E > N with probability:
‚Ä¢ a(t) if x is compatible with e;
‚Ä¢

1
2

if x is not compatible with e.

The probability of declaring E > N is:
r=

X
h

which is greater than

1
2

1
a(t)Pr (h, e) + (1 ‚àí Pr (e))
2

(15)

iff the following set of equivalent statements hold:
X

a(t)Pr (h, e) >

h

X

a(t)Pr (h | e) >

h

1
2


1
1t‚àíN
Pr (h | e) >
+
2 2 u‚àíl
2


X 1t‚àíN
Pr (h | e) > 0
2 u‚àíl
h
X
(t ‚àí N )Pr (h | e) > 0

X 1
h

Pr (e)
2

h

X

R(Pr (D | h, e))Pr (h | e) > N.

h

Thus r >

1
2

iff E > N .

Lemma 1. The function a(t) =

1
2

+

1 t‚àíN
2 u‚àíl

maps a reward t to a probability in [0, 1].

Proof. Values u and l are given, and denote upper and lower bounds on the reward t, but
also the threshold N . Thus t‚àíN
u‚àíl is in [‚àí1, 1].
Note that a(t) denotes a probability used by our algorithm to declare whether E > N ,
which is higher or lower depending on the value of the reward t = R(Pr (D | h, e)).

Appendix C. Conditional Probability Tables
In this section we provide conditional probability tables for the networks in Figures 1, 2, 3,
and 4.

627

Chen, Choi & Darwiche

D
+
‚àí

Pr (D)
0.5
0.5

Hi
t
t
p
p
n
n
l
l

D
+
+
‚àí
‚àí

Xi
+
‚àí
+
‚àí
+
‚àí
+
‚àí

X1
+
‚àí
+
‚àí

Ei
+
+
+
+
+
+
+
+

Pr (X1 | D)
0.9
0.1
0.1
0.9

X1
+
+
‚àí
‚àí

Pr (Ei | Hi , Xi )
1.0
0.0
1.0
1.0
0.0
0.0
0.0
1.0

Hi
t
p
n
l

X2
+
‚àí
+
‚àí

Pr (X2 | X1 )
0.9
0.1
0.1
0.9

Pr (Hi )
0.81
0.09
0.09
0.01

Figure 13: The CPTs for the Bayesian network given in Figure 1. Note that for the
CPTs of variables Ei , only the lines for the case Ei = + are given, since
Pr (Ei = ‚àí|Hi , Xi ) = 1 ‚àí Pr (Ei = +|Hi , Xi ).

D S1 Pr (S1 | D)
+ +
0.65
+ ‚àí
0.35
0.30
‚àí +
‚àí ‚àí
0.70

D S3 Pr (S3 | D)
+ +
0.65
+ ‚àí
0.35
0.35
‚àí +
‚àí ‚àí
0.65

D S2 Pr (S2 | D)
+ +
0.60
+ ‚àí
0.40
‚àí +
0.30
‚àí ‚àí
0.70

D S4 Pr (S4 | D)
+ +
0.65
+ ‚àí
0.35
‚àí +
0.35
‚àí ‚àí
0.65

Table 5: CPTs for the network in Figure 2. Parameterization 2.

628

Algorithms and Applications for the Same-Decision Probability

D S1 Pr (S1 | D)
+ +
0.50
0.50
+ ‚àí
‚àí +
0.40
‚àí ‚àí
0.60

D S3 Pr (S3 | D)
+ +
0.55
0.45
+ ‚àí
‚àí +
0.45
‚àí ‚àí
0.55

D S2 Pr (S2 | D)
+ +
0.50
+ ‚àí
0.50
‚àí +
0.40
‚àí ‚àí
0.60

D S4 Pr (S4 | D)
+ +
0.55
+ ‚àí
0.45
‚àí +
0.45
‚àí ‚àí
0.55

Table 6: CPTs for the network in Figure 2. Parameterization 3.

D S2 Pr (S2 | D)
+ +
0.75
+ o
0.2
0.05
+ ‚àí
‚àí +
0.05
‚àí o
0.2
‚àí ‚àí
0.75

D S1 Pr (S1 | D)
+ +
0.8
0.2
+ ‚àí
‚àí +
0.2
‚àí ‚àí
0.8

Table 7: CPTs for the Bayesian network in Figure 3.

D H1 Pr (H1 | D)
+ +
0.80
+ ‚àí
0.20
‚àí +
0.10
‚àí ‚àí
0.90

D H2 Pr (H2 | D)
+ +
0.70
+ ‚àí
0.30
‚àí +
0.30
‚àí ‚àí
0.70

Table 8: CPTs for the network in Figure 4. Pr (H3 | D), Pr (E1 | D) and Pr (H2 |D) are
equal.

629

Chen, Choi & Darwiche

References
Arroyo, I., & Woolf, B. (2005). Inferring learning and attitudes from a Bayesian network
of log file data. In Proceedings of the 12th International Conference on Artificial
Intelligence in Education, pp. 33‚Äì40.
Bache, K., & Lichman, M. (2013). UCI machine learning repository..
Bilgic, M., & Getoor, L. (2011). Value of information lattice: Exploiting probabilistic independence for effective feature subset acquisition. Journal of Artificial Intelligence
Research (JAIR), 41, 69‚Äì95.
Butz, C. J., Hua, S., & Maguire, R. B. (2004). A web-based intelligent tutoring system for
computer programming. In Web Intelligence, pp. 159‚Äì165. IEEE Computer Society.
Chan, H., & Darwiche, A. (2003). Reasoning about Bayesian network classifiers. In Proceedings of the 19th Conference in Uncertainty in Artificial Intelligence, pp. 107‚Äì115.
Chen, J., Low, K. H., Tan, C. K.-Y., Oran, A., Jaillet, P., Dolan, J., & Sukhatme, G.
(2012a). Decentralized data fusion and active sensing with mobile sensors for modeling
and predicting spatiotemporal traffic phenomena. In Proceedings of the Twenty-Eighth
Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-12), pp.
163‚Äì173, Corvallis, Oregon. AUAI Press.
Chen, S., Choi, A., & Darwiche, A. (2012b). The Same-Decision Probability: A new tool
for decision making. In Proceedings of the Sixth European Workshop on Probabilistic
Graphical Models, pp. 51‚Äì58.
Chen, S., Choi, A., & Darwiche, A. (2013). An exact algorithm for computing the SameDecision Probability. In Proceedings of the 23rd International Joint Conference on
Artificial Intelligence, pp. 2525‚Äì2531.
Choi, A., Xue, Y., & Darwiche, A. (2012). Same-Decision Probability: A confidence measure for threshold-based decisions. International Journal of Approximate Reasoning
(IJAR), 2, 1415‚Äì1428.
Conati, C., Gertner, A., & VanLehn, K. (2002). Using Bayesian networks to manage uncertainty in student modeling. User Modeling and User-Adapted Interaction, 12 (4),
371‚Äì417.
Cover, T. M., & Thomas, J. A. (1991). Elements of Information Theory. Wiley-Interscience.
Darwiche, A. (2009). Modeling and Reasoning with Bayesian Networks (1st edition). Cambridge University Press.
Darwiche, A., & Choi, A. (2010). Same-Decision Probability: A confidence measure for
threshold-based decisions under noisy sensors. In Proceedings of the Fifth European
Workshop on Probabilistic Graphical Models, pp. 113‚Äì120.
Dechter, R. (1999). Bucket elimination: A unifying framework for reasoning. Artificial
Intelligence, 113 (1), 41‚Äì85.
Dittmer, S., & Jensen, F. (1997). Myopic value of information in influence diagrams. In Proceedings of the Thirteenth Conference Annual Conference on Uncertainty in Artificial
Intelligence (UAI-97), pp. 142‚Äì149.
630

Algorithms and Applications for the Same-Decision Probability

Friedman, N., Geiger, D., & Goldszmidt, M. (1997). Bayesian network classifiers. Machine
Learning, 29 (2-3), 131‚Äì163.
Gao, T., & Koller, D. (2011). Active classification based on value of classifier. In Advances
in Neural Information Processing Systems (NIPS 2011).
Gertner, A. S., Conati, C., & VanLehn, K. (1998). Procedural help in Andes: Generating hints using a Bayesian network student model. In Proceedings of the National
Conference on Artificial Intelligence, pp. 106‚Äì111.
Glasziou, P., & Hilden, J. (1989). Test selection measures. Medical Decision Making, 9 (2),
133‚Äì141.
Greiner, R., Grove, A. J., & Roth, D. (2002). Learning cost-sensitive active classifiers.
Artificial Intelligence, 139 (2), 137‚Äì174.
Hamscher, W., Console, L., & de Kleer, J. (Eds.). (1992). Readings in Model-Based Diagnosis. Morgan Kaufmann Publishers Inc.
Heckerman, D., Breese, J. S., & Rommelse, K. (1995). Decision-theoretic troubleshooting.
Communications of the ACM, 38 (3), 49‚Äì57.
Heckerman, D., Horvitz, E., & Middleton, B. (1993). An approximate nonmyopic computation for value of information. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 15 (3), 292‚Äì298.
Henrion, M. (1986). Propagating uncertainty in Bayesian networks by probabilistic logic
sampling. In Proceedings of the Second Annual Conference on Uncertainty in Artificial
Intelligence (UAI-86), pp. 149‚Äì163.
Howard, R. A. (1966). Information value theory. IEEE Transactions on Systems Science
and Cybernetics, 2 (1), 22‚Äì26.
Howard, R. A., & Matheson, J. E. (Eds.). (1984). Readings on the Principles and Applications of Decision Analysis. Strategic Decision Group.
Ide, J. S., Cozman, F. G., & Ramos, F. T. (2004). Generating random Bayesian networks
with constraints on induced width. In Proceedings of the 16th European Conference
on Artificial Intelligence, pp. 323‚Äì327.
Jordan, A. (2002). On discriminative vs. generative classifiers: A comparison of logistic
regression and naive Bayes. Advances in Neural Information Processing Systems, 14,
841.
Kahn, C. E., Roberts, L. M., Shaffer, K. A., & Haddawy, P. (1997). Construction of a
Bayesian network for mammographic diagnosis of breast cancer. Computers in Biology
and Medicine, 27 (1), 19‚Äì29.
Karp, R. M. (1972). Reducibility among combinatorial problems. In Complexity of Computer Computations. Springer.
Kj√¶rulff, U. B., & Madsen, A. L. (2008). Bayesian Networks and Influence Diagrams: A
Guide to Construction and Analysis. Springer.
Krause, A., & Guestrin, C. (2005). Near-optimal nonmyopic value of information in graphical models. In 21st Conference on Uncertainty in Artificial Intelligence, pp. 324‚Äì331.
631

Chen, Choi & Darwiche

Krause, A., & Guestrin, C. (2009). Optimal value of information in graphical models.
Journal of Artificial Intelligence Research (JAIR), 35, 557‚Äì591.
Kruegel, C., Mutz, D., Robertson, W., & Valeur, F. (2003). Bayesian event classification
for intrusion detection. In Proceedings of the Annual Computer Security Applications
Conference (ACSAC).
Liao, W., & Ji, Q. (2008). Efficient non-myopic value-of-information computation for influence diagrams. International Journal of Approximate Reasoning, 49 (2), 436‚Äì450.
Lindley, D. V. (1956). On a measure of the information provided by an experiment. Annals
of Mathematical Statistics, 27 (4), 986‚Äì1005.
Lu, T.-C., & Przytula, K. W. (2006). Focusing strategies for multiple fault diagnosis. In
Proceedings of the 19th International FLAIRS Conference, pp. 842‚Äì847.
MillaÃÅn, E., Descalco, L., Castillo, G., Oliveira, P., & Diogo, S. (2013). Using Bayesian
networks to improve knowledge assessment. Computers & Education, 60 (1), 436‚Äì447.
Modelo-Howard, G., Bagchi, S., & Lebanon, G. (2008). Determining placement of intrusion detectors for a distributed application through Bayesian network modeling. In
Proceedings of the 11th International Symposium on Recent Advances in Intrusion
Detection, pp. 271‚Äì290.
Munie, M., & Shoham, Y. (2008). Optimal testing of structured knowledge. In AAAI‚Äô08:
Proceedings of the 23rd National Conference on Artificial intelligence, pp. 1069‚Äì1074.
Ognibene, D., & Demiris, Y. (2013). Towards active event recognition. In Proceedings of
the 23rd International Joint Conference on Artificial Intelligence, pp. 2495‚Äì2501.
Park, J. D., & Darwiche, A. (2004). Complexity results and approximation strategies for
MAP explanations. Journal of Artificial Intelligence Research (JAIR), 21, 101‚Äì133.
Pauker, S. G., & Kassirer, J. P. (1980). The threshold approach to clinical decision making..
The New England Journal of Medicine, 302 (20), 1109‚Äì17.
Raiffa, H. (1968). Decision Analysis ‚Äì Introductory Lectures on Choices under Uncertainty.
Addison-Wesley.
Ramoni, M., & Sebastiani, P. (2001). Robust Bayes classifiers. Artificial Intelligence, 125 (12), 209‚Äì226.
Roth, D. (1996). On the hardness of approximate reasoning. Artificial Intelligence, 82, 273
‚Äì 302.
Shann, M., & Seuken, S. (2013). An active learning approach to home heating in the
smart grid. In Proceedings of the 23rd International Joint Conference on Artificial
Intelligence, pp. 2892‚Äì2899.
Stratonovich, R. (1965). On value of information. Izvestiya of USSR Academy of Sciences,
Technical Cybernetics, 5, 3‚Äì12.
van der Gaag, L. C., & CoupeÃÅ, V. M. H. (1999). Sensitivity analysis for threshold decision
making with Bayesian belief networks. In AI*IA, pp. 37‚Äì48.
Vomlel, J. (2004). Bayesian networks in educational testing. International Journal of
Uncertainty, Fuzziness and Knowledge-Based Systems, 12 (supp01), 83‚Äì100.
632

Algorithms and Applications for the Same-Decision Probability

Xenos, M. (2004). Prediction and assessment of student behaviour in open and distance
education in computers using Bayesian networks. Computers & Education, 43 (4),
345‚Äì359.
Yu, S., Krishnapuram, B., Rosales, R., & Rao, R. B. (2009). Active sensing. In International
Conference on Artificial Intelligence and Statistics, pp. 639‚Äì646.
Zhang, N. L. (1998). Probabilistic inference in influence diagrams. In Computational Intelligence, pp. 514‚Äì522.
Zhang, Y., & Ji, Q. (2010). Efficient sensor selection for active information fusion. IEEE
Transactions on Systems, Man, and Cybernetics, Part B, 40 (3), 719‚Äì728.

633

Journal of Artificial Intelligence Research 49 (2014) 143-170

Submitted 10/13; published 02/14

A Procedural Characterization of
Solution Concepts in Games
Joseph Y. Halpern

halpern@cs.cornell.edu

Computer Science Department
Cornell University
Ithaca, NY 14853, USA

Yoram Moses

moses@ee.technion.ac.il

Department of Electrical Engineering
Technion‚ÄîIsrael Institute of Technology
Haifa, 32000, Israel

Abstract
We show how game-theoretic solution concepts such as Nash equilibrium, correlated
equilibrium, rationalizability, and sequential equilibrium can be given a uniform definition
in terms of a knowledge-based program with counterfactual semantics. In a precise sense,
this program can be viewed as providing a procedural characterization of rationality.

1. Introduction
There is a general intuition that, in many situations, what players do depends on what
they know. This leads to the hope that we can describe players‚Äô actions procedurally using
knowledge-based (kb) programs (Fagin, Halpern, Moses, & Vardi, 1995, 1997) of the form ‚Äúif
I know (or do not know) X then I should do Y ‚Äù. For example, a kb program could say ‚ÄúIf
you don‚Äôt know that Ann received the information, then send her a text message‚Äù, which
can be written
if ¬¨Ki (Ann received info) then send Ann a text message.
This kb program has the form of a standard if . . . then statement, except that the test in
the if clause involves i‚Äôs knowledge (expressed using the modal operator Ki ).
Knowledge-based programs have been successfully applied in distributed computing,
both to help in the design of new protocols and to clarify our understanding of existing
protocols (see, e.g., Fagin et al., 1997; Dwork & Moses, 1990; Hadzilacos, 1987; Halpern,
Moses, & Waarts, 2001; Halpern & Zuck, 1992; Mazer & Lochovsky, 1990; Mazer, 1990;
Moses & Kislev, 1993; Moses & Tuttle, 1988; Neiger & Bazzi, 1992; Neiger & Toueg, 1993).
They have also been applied successfully in planning (Brafman, Latombe, Moses, & Shoham,
1997; Lang & Zanuttini, 2012, 2013; Reiter, 2001). In this paper, we initiate a project on
the use of kb programs in game theory. This seems like a particularly fruitful application
area, since this seems to be just the kind of reasoning that people employ in games and
decision-making problems.
We focus on one application of kb programs to game theory: characterizing solution concepts. Many solution concepts have been considered in the game-theory literature, ranging
from Nash equilibrium and correlated equilibrium to refinements of Nash equilibrium such
c
2014
AI Access Foundation. All rights reserved.

Halpern & Moses

as sequential equilibrium and weaker notions such as rationalizability (see Osborne & Rubinstein, 1994, for an overview).
Typically, these solution concepts assume that players are rational, in the sense that each
player‚Äôs strategy represents a best response to the beliefs that he has about the strategies
that other players are using. Indeed, a number of epistemic characterizations of various
solution concepts have been provided in the literature. Of particular interest to us are
characterizations in terms of common knowledge of rationality. These characterizations
all have the following flavor: ~œÉ satisfies solution concept X iff there is a state œâ in a
model M where (a) it is common knowledge that players are rational, (b) the players
are playing strategy profile ~œÉ , and (possibly) (c) M satisfies some additional property. No
additional properties are needed if X is rationalizability (Brandenburger & Dekel, 1987); we
get correlated equilibrium if we assume that players have a common prior in M (Aumann,
1987); we get Nash equilibrium if we assume that this prior takes strategy choices to be
uncorrelated (Aumann, 1987).1 Similar results can be proved for sequential equilibrium and
perfect equilibrium (Halpern & Moses, 2010).
The standard semantics of kb programs (see Section 2) essentially ensures that the kb
programs being run by the players are common knowledge. Thus, we might hope that
if we could find a kb program that captures rationality, then we could use it to give a
procedural characterization of various solution concepts. In this paper we show that this
goal is attainable.
Consider the following kb program EQŒìi for player i, where Ai (Œì) denotes all the possible
actions available to player i in game Œì. (This program applies both to normal-form and
to extensive-form games. In an extensive-form game Œì, Ai (Œì) is the union of the actions
available at each of i‚Äôs information sets; we assume without loss of generality that the sets
of actions at different information sets are disjoint.)
for each action a ‚àà Ai (Œì) do
V
if Ki (intendi (a) ‚àß a0 ‚ààAi (Œì) EUi (a) ‚â• EUi (a0 ))
then play a.
Intuitively, we can think of this program as one that ensures that players are expected utility
maximizers: Player i follows an action only if playing that action maximizes i‚Äôs expected
utility. More literally, EQŒìi says that player i should play action a if she currently intends
to do so (that is the intended semantics of intendi (a)), and her utility from doing so is no
less than her utility would be from playing an alternative action. Here EUi (a0 ) represents i‚Äôs
expected utility if she were to play a0 (conditional on i‚Äôs beliefs at the current information
set when we consider extensive-form games).2 The utility is taken with respect to player
i‚Äôs current probability distribution (which can be viewed as a distribution over the strategy
profiles of the other players, and thus represents i‚Äôs beliefs about what other players are
doing).
1. Although Aumann (1987) does not state the result for Nash equilibrium explicitly, it follows easily from
his results on correlated equilibrium.
2. Note that EUi (a0 ) incorporates a counterfactual. We are in a situation where i is actually intending to
play a, but EUi (a0 ) is what her expected utility would be were she to play a0 instead. In the preliminary
version of this paper (Halpern & Moses, 2007), we made this counterfactual reasoning explicit by having
a counterfactual operator in the language. Following the suggestion of a reviewer, we have suppressed
the counterfactual here, so as to focus on the issues of most interest to us.

144

A Procedural Characterization of Game Concepts

Programs are generally viewed as describing when various actions are supposed to be
performed, thereby providing a procedural specification of behavior. Knowledge-based programs are, in general, somewhat different. For example, consider the program EQŒìi . It
is written assuming that each player has already chosen her strategy. One possible way
‚àí‚àí‚ÜíŒì
to view EQ (i.e., (EQŒì1 , . . . , EQŒìn )) is as specifying that, given a last-minute chance to
change their minds, none of the (utility-maximizing) players would have a reason to deviate
from their individual choices.
The test in EQŒìi can be seen as embodying (one standard form of) rationality: it says
that i should act so as to maximize i‚Äôs expected utility. If it is common knowledge that
each player i follows EQŒìi (recall that the semantics of kb programs essentially ensures
that the programs being run are commonly known), then, intuitively, the players should
have common knowledge of rationality, and so be in equilibrium. Indeed, as we show,
‚àí‚àí‚ÜíŒì
under appropriate assumptions, what is played when players act according to EQ is an
Œì
‚àí‚àí‚Üí
instance of a standard solution concept. EQ captures many solution concepts in a uniform
way. Solution concepts differ in the assumptions that are made about players‚Äô beliefs. The
assumptions are essentially the ones that arose in the logical characterizations of solution
concepts mentioned above. Thus, for example, for correlated equilibrium, rather than
requiring common knowledge of rationality and that there is a common prior, we now
‚àí‚àí‚ÜíŒì
require that players use EQ and there is a common prior; similarly for the other solution
concepts.
The semantics of kb programs allows us to capture these assumptions about players‚Äô
beliefs by restricting to appropriate systems. The upshot is that we have a single kb program
that arguably gives a procedural embodiment of rationality. Moreover, common knowledge
that this program is being run provides us with a characterization of perhaps the most
common solution concepts used in game theory: Nash equilibrium, correlated equilibrium,
rationalizability, sequential equilibrium, and perfect equilibrium.
There has been considerable work in the last two decades focusing on the interplay between modal logic and game theory, and more specifically on epistemic logic and solution
concepts in games (see, e.g., Aumann & Brandenburger, 1995; Benthem, 2007, 2010; Bruin,
2010; Bonanno, 2002; Harrenstein, Hoek, Meyer, & Witteveen, 2002; Lorini & Schwarzentruber, 2010). This paper differs from that line of work in that it relates equilibrium notions
to knowledge-based programs, and shows the procedural commonality among equilibrium
notions, as well as how slightly varying the epistemic assumptions gives rise to the different
notions.
The rest of this paper is devoted to making these claims precise. In Section 2, we
review the relevant background on game theory and knowledge-based programs. To give
formal semantics to kb programs, we use the runs-and-systems framework (Fagin et al.,
1995), which has been used in the computer science literature to represent complex systems.
We specialize the framework so that it can represent the games that are of interest here.
In Section 3, we show that EQŒì characterizes Nash equilibrium, correlated equilibrium,
rationalizability, and sequential equilibrium in a game Œì, each in an appropriate context.
We conclude in Section 4 with a discussion of our results, their implications and possible
extensions, and a general discussion of the potential use of kb programs in game theory.
In particular, we argue that despite the non-negligible overhead involved in dealing with
145

Halpern & Moses

knowledge-based programs, using them gives us a flexible and powerful tool for capturing
intuitions such as best response.

2. Background and Definitions
In this section, we review the relevant background on games and knowledge-based programs,
and define the semantics of the knowledge-based program EQŒìi formally. We describe only
what we need for proving our results. The reader is encouraged to consult a standard game
theory text (e.g., Osborne & Rubinstein, 1994) for more on game theory, and the work of
Fagin et al. (1995, 1997) for more on the runs-and-systems framework and on knowledgebased programs.
2.1 Games and Strategies
A game Œì in extensive form is described by a game tree T = TŒì . A utility ui (h) is defined
for each terminal history h in the game tree (where a terminal history in a game tree is
just a path leading from the root to a leaf), specifying player i‚Äôs utility when that history is
played. Let ZŒì denote the set of all terminal histories. (We omit the subscript Œì if it is clear
from context.) Although it is typical to assume that only one player at a time moves in an
extensive-form game, we allow arbitrary subsets of players to move. This added generality
allows us to view normal-form games as a special case of extensive-form games. Thus,
associated with each non-leaf node is the subset of players whose move it is at that node.3
For each non-leaf node w of T , there is a bijection between the possible sets of moves that
can be played at w and the successors of w. If w0 is the successor of w that corresponds to
a particular set of moves, then w0 can be thought of as the outcome of playing those moves
at w. The nodes where a player i moves are partitioned into information sets.
A behavioral strategy œÉ for player i in an extensive-form game associates with each
information set I a distribution œÉ(I) over the actions that can be played at I. Thus,
a strategy for player i tells player i what to do at each node in the game tree where i
is supposed to move. The fact that a strategy determines actions as a function of the
information sets captures the intuition that, at all the nodes that player i cannot tell apart,
player i must do the same thing. A pure strategy Si for i is deterministic, specifying a single
action per information set. (Of course, a pure strategy can be viewed as a special case of
behavioral strategy; it is a behavioral strategy that puts probability 1 on a particular
action at each information set.) Since the game tree is assumed to be finite, there are
only finitely many pure strategy profiles. A mixed strategy œÉi for i is a distribution over
pure strategies.4 Note that a player using a mixed strategy randomizes only once, at the
beginning of the game; by way of contrast, a player using a behavioral strategy randomizes
at each information set. A pure (resp., mixed; behavioral) strategy profile is a tuple ~œÉ =
(œÉ1 , . . . , œÉn ) specifying a pure (resp., mixed; behavioral) strategy for each player. As usual,
given a profile ~x, we denote by ~x‚àíi the partial profile containing a component for all players
3. For ease of exposition, at this point we consider only games where there are no moves by nature. There
is no difficulty in dealing with moves by nature; we discuss this in Section 4.
4. We consistently use Si to denote a pure strategy and œÉi to denote a mixed strategy or a behavioral
strategy.

146

A Procedural Characterization of Game Concepts

~ = (S1 , . . . , Sn )
other than i. We denote by S = S(T ) the set of all pure strategy profiles S
for the game tree T .
A normal-form game can be viewed as a special case of an extensive-form game where
each player makes only one move, and all players move simultaneously. The tree corresponding to such a game has a depth of one: all nodes other than the root are leaves.
2.2 The Runs-and-Systems Framework
To explain kb programs, we must first describe the runs-and-systems framework. We assume
that, at any given point in time, a player in a game is in some local state. The local state
could include the history of the game up to this point, the strategy being used by the player,
and perhaps some other features of the player‚Äôs type, such as beliefs about the strategies
being used by other players. As we shall see, for the purposes of this paper, to model games,
a player‚Äôs local state will essentially consist of his strategy and his information set. A global
state is a profile of local states: one local state for each player.
A run is a sequence of global states; formally, a run is a function from times to global
states. Thus, r(m) is the global state in run r at time m. For definiteness, we assume that
time ranges over the natural numbers here. A point is a pair (r, m) consisting of a run r
and time m. Let ri (m) be i‚Äôs local state at the point (r, m); that is, if r(m) = (s1 , . . . , sn ),
then ri (m) = si . A system is a set of runs. A probabilistic system is a tuple PS = (R, ¬µ
~ ),
where R is a system and ¬µ
~ = (¬µ1 , . . . , ¬µn ) associates a probability ¬µi on the runs of R with
each player i. Intuitively, ¬µi represents player i‚Äôs prior beliefs. In the special case where
¬µ1 = ¬∑ ¬∑ ¬∑ = ¬µn = ¬µ, the players have a common prior ¬µ on R. In this case, we write just
(R, ¬µ).
2.3 Modeling a Game as a System
With each game Œì, we associate a system RŒì . In describing the system RŒì , we have to
decide how to model the players‚Äô local states, that is, what they know at each point in the
system? For the most part, the details do not matter for the analysis we do in this paper,
but they can have a critical effect on other analyses. We believe that one of the advantages
of the runs-and-systems approach is that it forces the modeler to think through carefully
what the players‚Äô local states should be.
In the case of a normal-form game Œì, at time 0, we can take a player‚Äôs local state to
consist of the pure strategy that she is intending to play; the player‚Äôs local state at times
m ‚â• 1 consists of the strategy she played (which we take to be the same as the one that
she was intending to play) and her utility. Even if we think of a player as having a mixed
strategy, we can think of the pure strategy in her local state as the pure strategy that the
player chooses after tossing her coin. We could, of course, also include the mixed strategy
in the player‚Äôs state. It turns out that doing so would make no difference; player i‚Äôs strategy
is encoded by the distribution ¬µi on runs. The upshot of this approach to modeling things
~
is that we can identify a run in RŒì with a pure strategy profile in Œì; we denote by rS the
~
run coresponding to strategy profile S.
To take a simple example, consider the normal-form game Œìn in Figure 1, between
two players, Alice (the row player) and Bob (the column player): There are four runs in
RŒìn , corresponding to the four strategy profiles in the game. In the run r(T,L) , we have
147

Halpern & Moses

T
B

L
(3, 3)
(4, 1)

R
(1, 4)
(0, 0)

Figure 1: A simple 2-player game Œìn .

r(T,L) (0) = (T, L) and r(T,L) (1) = ((T, 3), (L, 3)). In this run, Alice‚Äôs initial state is T , her
intended strategy, and her state at all times m ‚â• 1 is (T, 3), her strategy and utility.
In an extensive-form game, we take the runs in RŒì to correspond to the terminal histories
in the game tree; there is one run rh corresponding to each history h. The points on a run
rh correspond to the nodes in the history h. That is, the global state rh (m) corresponds to
the mth node w in history h. (If m is greater than the length of h, then rh (m) = rh (|h|).)
Suppose that i moves at node w. Then we can take i‚Äôs local state at rh (m) to have the form
(Iw , a), where Iw is i‚Äôs information set at w, and a is the move that i makes at information
set Iw in history h. Intuitively, this is saying that i knows his information set, and the move
that he is intending to make. For the solution concepts we analyze in extensive-form games,
we assume that i is using a behavioral strategy. Thus, we can think of a as the outcome
of the coin toss at information set I in i‚Äôs behavioral strategy. We still need to represent
i‚Äôs local state at points that correspond to nodes w where i does not move. The details of
i‚Äôs local state at points where i does not move do not matter much. For definiteness, if i
does not move at the node w corresponding to (rh , m), then we take i‚Äôs local state to be
Iw0 , where w0 is the most recent node preceding w where i does move; if i has not moved
prior to w, then we take i‚Äôs local state to be just h i. If w is a final node in a terminal
history, we also encode i‚Äôs utility in i‚Äôs local state, just as we did for normal-form games.
Thus, i‚Äôs state encodes i‚Äôs information about what has happened thus far (this is the Iw
component), whether or not it is i‚Äôs move (this is captured by whether or not there is an
action component for i in the local state), what i intends to do if it is his move, and (at
points that correspond to the end of the game), each player‚Äôs utility.
In the solution concepts we focus on here for extensive-form games, we think of the
players as using a behavioral strategy. However, just as in normal-form games, we do
not encode i‚Äôs behavioral strategy in his local state. And again, it will turn out that the
behavioral strategy is encoded in the probability distribution on runs.5 We could have also
included the game Œì itself in each player‚Äôs local states, since we have implicitly assuming
that the game Œì is common knowledge. Doing so would not change anything in the analysis;
we have not done it simply to avoid cluttering the notation. Of course, this would be an
appropriate thing to do in games where players are not fully aware of what game is being
played (see, e.g., Halpern & ReÃÇgo, 2013).
5. The alert reader may have spotted a potential problem here. Runs off the equilibrium path (and hence,
information sets off the equilibrium path) get probability 0, so it may seem that we cannot use the
probability on runs to infer i‚Äôs behavioral strategy at such information sets. But, as we shall see, the
probability on runs that we actually use is a nonstandard probability that is infinitesimally close to
the actual probability generated by the strategy profile. This nonstandard probability gives positive
probability to all runs, and hence can be used to infer i‚Äôs behavioral strategy. But this is an artifact of
our approach. In other contexts, we may well want to include i‚Äôs behavioral strategy in i‚Äôs local state.

148

A Procedural Characterization of Game Concepts

Consider the 2-player extensive-form game Œìe in Figure 2, where player 1 moves at w1 ,
and player 2 moves at the information set {w2 , w3 }:
b1 ! !

!r (3,4)
!

!

!!
!
r!
#aaa
#
w
2
2
ab
#
aa
a1 #
aa
ar (‚àí3,2)
#
#
#
#
#
w1rc
c
c

ca2
c
c

b1 ! !

!
c
!!
w
!
3
c
c!
ra
aa


2
ab
aa
a

!r (1,1)
!

a
ar (1,2)

Figure 2: The extensive-form game Œìe .
There are four runs in RŒìe , corresponding to the four terminal histories. Call these
histories h1 ‚Äìh4 , going from top to bottom. Thus, in rh1 (where the utility is (3,4)), 1‚Äôs local
state at rh1 (0) is ({w1 }, a1 ), while 2‚Äôs local state is h i. At rh1 (1), 1‚Äôs local state is {w1 },
while 2‚Äôs is ({w2 , w3 }, b1 ), and at rh1 (2), i‚Äôs local state is ({w1 }, 3), while 2‚Äôs local state is
({w2 , w3 }, 4).
Since there is a bijection between runs in RŒì for a normal-form game Œì and pure strategy
profiles, a distribution ¬µ on pure strategies in a normal-form game Œì can be identified with
a distribution on the runs in RŒì . Thus, we can associate with each mixed strategy profile ~œÉ
in a normal-form game Œì the probabilistic system (RŒì , ¬µ~œÉ ), where ¬µ~œÉ is the distribution on
strategy profiles (and hence also on runs) induced by ~œÉ . Note that according to ¬µ~œÉ , players‚Äô
strategy choices are uncorrelated; the probability that player i chooses Si and player j
chooses Sj is just the product of the probability that i chooses Si and the probability
that j chooses Sj . Similarly, in an extensive-form game Œì, a behavioral strategy profile ~œÉ
induces a distribution on histories, and hence also on the runs in RŒì . We again denote this
distribution ¬µ~œÉ .
2.4 Knowledge-Based Programs
A knowledge-based program is a syntactic object. For our purposes, a knowledge-based
program for player i is taken to have the form
if Œ∫1 then a1
if Œ∫2 then a2
...,
149

Halpern & Moses

where each aj is an action for i, and each Œ∫j is a Boolean combination of formulas of the
form Ki œï, in which the œï‚Äôs can have nested occurrences of K` operators. We assume that
the tests Œ∫1 , Œ∫2 , . . . are mutually exclusive, so that, in a kb program for player i, at most one
of the tests evaluates to true in each local state for player i at which i moves. The program
EQŒìi can be written in this form by simply replacing the for . . . do statement by one line
for each possible action of i in the game Œì. That is, for each action a ‚àà Ai (Œì), there is a
line in EQŒìi of the form
if Ki (intendi (a) ‚àß

^

EUi (a) ‚â• EUi (a0 )) then play a.

a0 ‚ààSi (A)

Since a player i intends to play at most one action at a point (r, m) in (RŒì , ¬µ
~ ), the tests
in EQŒìi are mutually exclusive. The tests are not necessarily exhaustive, since at a point
in which the strategy S that player i uses is not a best response, no test of the form above
is satisfied. Roughly speaking, if none of the tests is satisfied, then i does nothing (and
performs the null action skip).
We want to define what it means for a (probabilistic) system PS to be compatible
with a kb program. Intuitively, this is the case when all the moves made in PS are the
ones recommended by the kb program. For simplicity, we give just enough of the required
definitions here to be able to handle the case that PS has the form (RŒì , ¬µ
~ ) and and the kb
program is EQŒìi . For further details, the interested reader can consult Fagin et al. (1995,
1997).
As a first step to making this precise, for each standard system PS = (RŒì , ¬µ
~ ), we
associate with each formula œï a set [[œï]]PS of points in PS. Intuitively, [[œï]]PS is the set of
points of RŒì where œï is true. For intendi (a) this is easy:
‚Ä¢ [[intendi (a)]]PS is the set of points (r, m) of PS at which i moves at the node w in the
game tree associated with (r, m) and a is the action encoded in i‚Äôs local state.
Note that a kb program Pgi for player i can attempt to ‚Äúoverride‚Äù i‚Äôs intentions; that is,
the program can have a line of the form ‚Äúif Œ∫ then play a0 ‚Äù such that, at a point (r, m) in
the probabilistic system PS, Œ∫ is true (i.e., (r, m) ‚àà [[Œ∫]]PS ), but the action in i‚Äôs local state
is a, not a0 . In this case, as we shall see, Pg would not be compatible with PS.
The semantics of knowledge is defined as usual: the formula Ki œï is true if œï is true at
all the points that i considers possible. We view all of i‚Äôs information at a point (r, m) as
being encapsulated by i‚Äôs local state at (r, m), which we denote ri (m). Thus, the set of
points that i considers possible at a point (r, m) is Ki (r, m) = {(r0 , m0 ) : ri0 (m0 ) = ri (m)};
Ki (r, m) just consists of all the points where i has the same local state as at (r, m).
‚Ä¢ [[Ki œï]]PS is the set of points (r, m) such that Ki (r, m) ‚äÜ [[œï]]PS .
It remains to give semantics to formulas of the form EUi (a0 ) ‚â• EUi (a). Clearly the
expected utility that i would obtain if i were to play a0 at the point (r, m) depends on i‚Äôs
beliefs about what the other players are doing at (r, m). Roughly speaking, these beliefs
are obtained by conditioning i‚Äôs prior beliefs ¬µi on Ki (r, m). But there is a small technical
problem here. The probability distribution ¬µi is a distribution on runs; Ki (r, m) is a set of
150

A Procedural Characterization of Game Concepts

points. We cannot condition ¬µi on Ki (r, m). To enable conditioning, we first associate with
Ki (r, m) the set R[Ki (r, m)] of those runs that go through points in Ki (r, m); that is,
R[Ki (r, m)] = {r0 : (r0 , m) ‚àà Ki (r, m)).
We then define ¬µi,r,m = ¬µi | R[Ki (r, m)]. (For the purposes of this paper, we do not specify
¬µi,r,m if ¬µi (R[Ki (r, m)]) = 0. It turns out to be irrelevant to our discussion.) Recall that
there is a bijection between the runs in RŒì and pure strategy profiles. Moreover, since player
i knows his strategy, at all the runs in R[Ki (r, m)], player i is using the same strategy. Thus,
~‚àíi . We use this distribution to compute EUi (a) and
¬µi,r,m determines a distribution on S
0
EUi (a ). In the case of a normal-form game Œì, this suffices to compute i‚Äôs expected utility
if he were to play a0 (although a0 may not in fact be the strategy that i intends to use in
the runs in R[Ki (r, m)]), under the assumption that all other players do use the strategy
that they were intending to use; only i‚Äôs strategy changes. In an extensive-form game,
when considering a change from a to a0 in a run r, we keep all other actions fixed (i.e., the
actions of all other players throughout the run, and i‚Äôs actions off the information set I),
and consider the utility of the resulting run.
‚Ä¢ If Œì is a normal-form game and PS = (RŒì , ¬µ
~ ), then [[EUi (a) ‚â• EUi (a0 )]]PS consists
of those points (r, m) at which the expected utility for i of using a is at least as high
as that of using a0 , where the expectation is taken with respect to the distribution
on strategy profiles a‚àíi of the other players, as determined by ¬µi,r,m . If Œì is an
extensive-form game, and at the point (r, m) player i intends to play action a, i is in
information set I, and it is i‚Äôs move. To compute EUi (a0 ), for each run r0 in Ki (r, m),
let hr0 [a/a0 ] be the history in Œì where the same sequence of actions is played by each
player as in r0 , except that at information set I, i plays a0 rather than a. Then
P
EUi (a0 ) = r0 ‚ààKi (r,m) ¬µi,r,m (r0 )ui (hr0 [a/a0 ]). Again, [[EUi (a) ‚â• EUi (a0 )]]PS consists of
all those points (r, m) where i moves and EUi (a0 ) (computed as above) is no higher
than EUi (a).
Since player i knows his strategy (it appears in his local state), if (r, m) ‚àà [[intendi (a)]]PS ,
then Ki (r, m) ‚äÜ [[intendi (a)]]PS . Similarly, since ¬µi,r,m = ¬µi,r0 ,m0 if (r0 , m0 ) ‚àà Ki (r, m), player
i knows his probability distribution, so if (r, m) ‚àà [[EUi (a) ‚â• EUi (a0 )]]PS , then Ki (r, m) ‚äÜ
V
0
[[EUi (a) ‚â• EUi (a0 )]]PS . Hence, the formula intendi (a) ‚àß
a0 ‚ààAi (Œì) EUi (a) ‚â• EUi (a ) is
V
0
equivalent to the epistemic formula Ki (intendi (a) ‚àß
a0 ‚ààAi (Œì) EUi (a) ‚â• EUi (a )); that is,
V
0
intendi (a) ‚àß a0 ‚ààAi (Œì) EUi (a) ‚â• EUi (a ) is true iff player i knows it. We have kept the Ki
in the kb program just to emphasize that this is a formula whose truth depends only on
what i knows and believes, and thus is a test that i can act on.
‚àí
‚Üí
Intuitively, a system PS is compatible with a kb program profile Pg if PS could have
arisen if each player i uses Pgi . We formalize this as follows.
~ if for all r ‚àà R and m ‚â• 0,
Definition 2.1 PS is compatible with the kb program profile Pg
there is an action profile ~a such that both (a) applying ~a to (r, m) results in (r, m + 1) (see
below) and (b) for each i such that ¬µi (R[Ki (r, m)]) > 0 and ¬µi (r | R[Ki (r, m)]) > 0, either
there is a line if Œ∫ then ai in Pgi and (r, m) ‚àà [[Œ∫]]PS , or there is no such line and ai is the
null move skip.
151

Halpern & Moses

We have not explained what it means to apply an action profile ~a to a point (r, m).
The general definition involves viewing action profiles as transformers of global states (see
Fagin et al., 1995, 1997). Rather than going through the details of the general definition
‚àí
‚Üí ‚àí‚àí‚ÜíŒì
here, we just give the definition in the case that PS has the form (RŒì , ¬µ) and Pg is EQ .
That is all we need for this paper, and in this case, the definition is quite simple. If Œì is
~
~
~
a normal-form game, then the action profile ~a applied to rS (0) results in rS (1) iff ~a = S;
~
~
if m ‚â• 1, then ~a applied to rS (m) results in rS (m + 1) iff a1 = ¬∑ ¬∑ ¬∑ an = skip. If Œì is an
extensive-form game, then ~a applied to rh (m) results in rh (m + 1) iff ai = skip if i does
not move at the information set I associated with rh (m) (which means that, in particular,
if m is greater or equal to the length of h, then ai = skip for all players i) and, if i does
move at I, then ai is the move encoded in rih (m). Roughly speaking, this means that for a
‚àí‚àí‚ÜíŒì
normal-form game Œì, PS = (RŒì , ¬µ) is compatible with EQ iff, for each player i and run r
‚àí‚àí‚ÜíŒì
such that ¬µ(r) > 0, what does according to EQ at r(0) is what i intends to do according
to his local state ri (0). Thus, if i intends to play strategy S, then it must be the case that
EUi (S) ‚â• EUi (S 0 ) for all other strategies S 0 for player i. The analogous statement is true
in extensive-form games, although to compute whether EUi (a) ‚â• EUi (a0 ) at a point (r, m),
we use the probability conditioned on R[Ki (r, m)]. That is, if ai is the action played by
i at (r, m), then ai really is a best response for i, given i‚Äôs beliefs at (r, m). While this
observation makes all the proofs of the results relatively straightforward, it is important to
note that this is really an instance of the general semantics of kb programs.

3. The Main Results
In this section, we show that EQŒì captures a number of standard solution concepts. We
start by considering solution concepts in normal-form games, and then move to extensiveform games.
3.1 Capturing Solution Concepts in Normal-Form Games
We show EQŒì captures three of the most studied solution concepts in normal-form games:
Nash equilibrium, correlated equilibrium, and rationalizability. The differences in how they
are captured highlights the distinctions between the notions.
3.1.1 Nash Equilibrium
Recall that (R, ¬µ) is a probabilistic system in which all players have a common prior ¬µ on
runs.
Theorem 3.1 The mixed strategy profile ~œÉ is a Nash equilibrium of the normal-form game
‚àí‚àí‚ÜíŒì
Œì iff (RŒì , ¬µ~œÉ ) is compatible with EQ .
Proof

First suppose that ~œÉ = (œÉ1 , . . . , œÉn ) is a Nash equilibrium of the game Œì. To see
‚àí‚àí‚ÜíŒì
~
that PS = (RŒì , ¬µ~œÉ ) is compatible with EQ , it suffices to show that if ¬µ~œÉ (rS ) > 0, then
~
~
Si is a best response with respect to ¬µ~œÉ | R[Ki (rS , 0)]. (We need to consider only (rS , 0)
because Œì is a normal-form game, so there are no moves after time 0.)
152

A Procedural Characterization of Game Concepts

~

Note that ¬µ~œÉ | R[Ki (rS , 0)] = ~œÉ‚àíi (under the obvious identification of ¬µ~œÉ | Ki (r, 0) with
a distribution on S‚àíi ). Since ~œÉ is a Nash equilibrium, Si must be a best response to ~œÉ‚àíi .
~
Thus, for all strategies S 0 ‚àà Si (Œì), we must have that (rS , 0) ‚àà [[EUi (S) ‚â• EUi (S 0 )]]PS . It
‚àí‚àí‚ÜíŒì
follows that PS = (RŒì , ¬µ~œÉ ) is compatible with EQ .
‚àí‚àí‚ÜíŒì
For the converse, suppose that (RŒì , ¬µ~œÉ ) is compatible with EQ . We want to show that
~œÉ is a Nash equilibrium. It suffices to show that each pure strategy Si in the support of œÉi
~‚àíi in
is a best response to ~œÉ‚àíi . Let Si be in the support of œÉi . Choose a strategy profile S
~
~
S
S
Œì
the support of ~œÉ‚àíi . Then ¬µ~œÉ (r ) > 0. Moreover, ¬µ~œÉ | R[Ki (r , 0)] = ~œÉ‚àíi . Since (R , ¬µ~œÉ ) is
‚àí‚àí‚ÜíŒì
~
compatible with EQ and ¬µ~œÉ (rS ) > 0, it must be the case that, for all strategies S 0 ‚àà Si (Œì),
~
(rS , 0) ‚àà [[EUi (S) ‚â• EUi (S 0 )]]PS . That is, S is indeed a best response to ~œÉ‚àíi .
Consider the game Œìn described in Figure 1. It is easy to check that this game has three
Nash equilibria: there are two equilibria in pure strategies: (B, L) and (T, R). There is
also an equilibrium in mixed strategies where Alice randomizes (uniformly) between T and
B, and Bob randomizes between L and R. That means that there are three probabilistic
systems of the form (RŒìn , ¬µ) compatible with EQŒìn . In the first, ¬µ puts probability 1
on r(B,L) ; in the second, ¬µ puts probability 1 on r(T,R) , and in the third, ¬µ puts uniform
probability on the four runs in the system.
3.1.2 Correlated Equilibrium
As is well known, players can sometimes achieve better outcomes than a Nash equilibrium
if they have access to a helpful mediator. Consider the simple 2-player game Œìn described
in Figure 1. Recall that the total utility in each of the three Nash equilibria of the games
(that is, the sum of the utilities of the two players) is at most 5. We get a higher total
utility by using a trusted mediator, who makes a recommendation by choosing at random
between (T, L), (T, R), and (B, L). This gives each player an expected utility of 8/3; thus,
the total utility is 16/3. This is an example of a correlated equilibrium since, for example,
if the mediator chooses (T, L), and thus sends recommendation T to Alice and L to Bob,
then Alice considers it equally likely that Bob was told L and R, and thus has no incentive
to deviate; similarly, Bob has no incentive to deviate. In general, a distribution ¬µ over pure
strategy profiles is a correlated equilibrium if players cannot do better than following a
mediator‚Äôs recommendation if the mediator makes recommendations according to ¬µ. (Note
that, as in our example, if a mediator chooses a (pure) strategy profile (S1 , . . . , Sn ) according
to ¬µ, the mediator recommends Si to player i; player i is told nothing about the strategy
profile except for Si .) Roughly speaking, a correlated equilibrium is a distribution Œ∑ over
(pure) strategy profiles in which every strategy T for player i that has a positive probability
is a best response to the conditional probability Œ∑ | T projected onto S‚àíi . (Note that the
~ 0 such that S 0 = T , so Œ∑ | T can be viewed
support of Œ∑ | T consists of strategy profiles S
i
as a distribution on S‚àíi . Intuitively, if player i knows that the prior probability on pure
strategy profiles is Œ∑ and is told to play T , then he believes that the probability on the
strategy profiles in S‚àíi played by the other players is described by Œ∑ | T (projected onto
S‚àíi ). Conversely, if Œ∑ is a distribution over pure strategy profiles such that, for each player
i and every strategy T for player i that is given positive probability by Œ∑ is a best response
to Œ∑ | T , then Œ∑ is a correlated equilibrium. Formally, we have the following definition.
153

Halpern & Moses

Definition 3.2 (Aumann, 1974) A distribution Œ∑ on pure strategy profiles is a correlated
equilibrium if for each player i, each strategy S for player i such that Œ∑(S) > 0 (where we
~ 0 such that S 0 = S), and each strategy S 0
identify S with the set of pure strategy profiles S
i
for player i, we have
X

ui (S, S‚àíi )Œ∑(S‚àíi | S) ‚â•

S‚àíi ‚ààS‚àíi

X

ui (S 0 , S‚àíi )Œ∑(S‚àíi | S).

S‚àíi ‚ààS‚àíi

That is, Œ∑ is a correlated equilibrium when, for every player i, if the mediator tells i to play a
strategy S that has positive probability according to Œ∑, then i does not gain from switching
to S 0 , given his beliefs about what the other players will do, conditional on player i being
told S.
Clearly a distribution Œ∑ on strategy profiles in game Œì can be identified with a distribution on the runs in RŒì . We can easily capture correlated equilibrium using EQŒì in a
way that generalizes Theorem 3.1. The only difference between Theorem 3.1 and Theorem 3.3 is that while ¬µ~œÉ in Theorem 3.1 is a product measure, the distribution Œ∑ on runs in
Theorem 3.3 is not necessarily a product measure (indeed, it is a product measure iff the
correlated equilibrium is a Nash equilibrium).
Theorem 3.3 The distribution Œ∑ on strategy profiles is a correlated equilibrium of the
‚àí‚àí‚ÜíŒì
(normal-form) game Œì iff (RŒì , Œ∑) is compatible with EQ .
Proof The proof proceeds along lines similar to that of Theorem 3.1.
~
Suppose that Œ∑ is a correlated equilibrium in Œì and Œ∑(rS ) > 0. Again, we must show
~
~
that Si is a best response to Œ∑ | R[Ki (rS , 0)]. But R[Ki (rS , 0)] consists precisely of the runs
where player i plays Si . Thus, ¬µi,rS~ ,m = Œ∑ | Si . Since Œ∑ is a correlated equilibrium, Si is a
‚àí‚àí‚ÜíŒì
best response to Œ∑ | Si ; it thus easily follows that (RŒì , Œ∑) is compatible with EQ .
For the converse, suppose that Œ∑ is a distribution on strategy profiles such that (RŒì , Œ∑)
‚àí‚àí‚ÜíŒì
is compatible with EQ . We want to show that Œ∑ is a correlated equilibrium. Suppose
that T is a strategy for player i that has positive probability according to Œ∑. Thus, there
~
is some run r = rS such that Œ∑(r) > 0 and Si = T . As we have seen, ¬µi,r,0 = Œ∑ | T . Since
‚àí‚àí‚ÜíŒì
(RŒì , Œ∑) is compatible with EQ , it must be the case that T is a best response to Œ∑ | T ,
which determines i‚Äôs beliefs at (r, 0). Thus, Œ∑ is a correlated equilibrium.
Note that the fact that i‚Äôs intended strategy is included in i‚Äôs local state for normal-form
games ensures that ¬µi,rS~ ,0 = Œ∑ | Si . Intuitively, in a correlated equilibrium, the mediator
tells i what strategy to follow, and i uses this information in determining a best response.
Thus, i‚Äôs local state should model this information. By way of contrast, Theorem 3.1 would
hold even if the strategy were not part of i‚Äôs local state. Since ~œÉ is a product measure in
Theorem 3.1, it would still be the case that ¬µi,r,0 = ~œÉ‚àíi for all runs r.
3.1.3 Rationalizability
Our characterization of both Nash equilibrium and correlated equilibrium involves a common prior on runs. Dropping this assumption gives rise to another standard solution concept: rationalizability (Bernheim, 1984; Pearce, 1984). Intuitively, a strategy for player i
154

A Procedural Characterization of Game Concepts

is rationalizable if it is a best response to some beliefs that player i may have about the
strategies that other players are following, assuming that each of these strategies is itself a
best response to beliefs that one of the other players has about strategies that other players
are following, and so on.
Following Osborne and Rubinstein (1994), say that a strategy S for player i in game Œì
is rationalizable if, for each player j, there is a set Zj ‚äÜ Sj (Œì) and, for each strategy T ‚àà Zj ,
a probability measure Œ∑j,T on S‚àíj (Œì) whose support is a subset of Z‚àíj such that
‚Ä¢ S ‚àà Zi ; and
‚Ä¢ for each player j and strategy T ‚àà Zj , strategy T is a best response to (the beliefs)
Œ∑j,T .
Intuitively, the strategies in Zi are the rationalizable strategies for player i. Player i can
justify playing a strategy T ‚àà Zi because, by assumption, there is a distribution on Z‚àíi
(representing i‚Äôs beliefs about the strategies that other players are using) against which T
is a best response. Moreover, each of the strategies to which i assigns positive probability
are themselve justifiable, since they are in Z‚àíi , and so are best responses to beliefs that
place positive probability on strategies that are justifiable, and so on.
For ease of exposition, we consider only pure rationalizable strategies. This is essentially
without loss of generality. It is easy to see that a mixed strategy œÉi for player i is a best
response to some beliefs Œ∑i of player i iff each pure strategy in the support of œÉi is a best
response to Œ∑i . Moreover, we can assume without loss of generality that the support of Œ∑i
consists only of pure strategy profiles.
Notice that in the game Œìn of Figure 1, all strategies are rationalizable. Alice playing
T is justified if Alice believes that Bob will play R; Bob playing R is justified if he believes
that Alice will T ; Alice playing B is justified if Alice believes that Bob will play L; and Bob
playing L is justified if he believes that Alice will play R.
The following theorem characterizes rationalizability in our framework. Note that we
now do not assume a common prior, so that there is a vector ¬µ
~ = (¬µ1 , . . . , ¬µn ), in which the
¬µi ‚Äôs are not necessarily identical, describing the players‚Äô beliefs.
Theorem 3.4 A pure strategy S for player i in the (normal-form) game Œì is rationalizable
iff there exists a probabilistic system PS = (RŒì , ¬µ
~ ) and, for each player j, there exists a set
Zj ‚äÜ Sj such that (a) ¬µj gives every strategy in Zj positive probability; (b) the support of
‚àí‚àí‚ÜíŒì
¬µj is contained in Z = Z1 √ó ¬∑ ¬∑ ¬∑ √ó Zn , (c) S ‚àà Zi , and (d) (RŒì , ¬µ
~ ) is compatible with EQ .
Proof Suppose that PS = (RŒì , ¬µ
~ ) is a probabilistic system satisfying the four properties
above. We want to show that S is rationalizable. Take the sets Zi guaranteed to exist
by the assumptions of the theorem to be the sets Zi in the definition of rationalizability.
For T ‚àà Zj , let Œ∑j,T be ¬µj | T projected onto S‚àíj . Note that ¬µj | T is well defined,
since ¬µj (T ) > 0. Moreover, the support of Œ∑j,T is contained in Z‚àíj , since the support of
¬µj is contained in Z. We now show that every T ‚àà Zj is a best response to Œ∑j,T . Since
~
~
¬µj (T ) > 0, there must be a run rS such that Sj = T and ¬µj (rS ) > 0. It is easy to see that
‚àí‚àí‚ÜíŒì
¬µj,rS~ ,0 = ¬µj | T = Œ∑j,T . Since (RŒì , ¬µ
~ ) is compatible with EQ , it must be the case that T
is a best response to Œ∑j,T . Thus, S is rationalizable.
155

Halpern & Moses

For the converse, suppose that S ‚àà Si is rationalizable. Thus, for each player j, there
exist a set Zj and, for each strategy T ‚àà Zj , a measure Œ∑j,T on S‚àíj (Œì) such that T is a
~ = Œ∑j,S (S~‚àíj )/|Zj | if Sj ‚àà Zj , and taking
best response to Œ∑j,T . Define ¬µj by taking ¬µj (S)
j
~ = 0 otherwise. First, observe that ¬µj is a probability on S: For each strategy Sj ‚àà Zj ,
¬µj (S)
we have ¬µj (Sj √ó S‚àíj ) = Œ∑j,Sj (S‚àíj )/|Zj | = 1/|Zj |; the result easily follows. Moreover, for
S ‚àà Sj , we have that ¬µj (S) > 0 iff S ‚àà Zj (of course, ¬µj (S) is just ¬µj (S √ó S‚àíj ). And since
the support of ¬µj,T is contained in Z‚àíj for each strategy T ‚àà Sj , it easily follows that the
support of ¬µj is contained in Z. Finally, S ‚àà Zi , by construction. Since T is a best response
‚àí‚àí‚ÜíŒì
to Œ∑j,T for all T ‚àà Zj , it easily follows that PS = (RŒì , ¬µ
~ ) is compatible with EQ .
To see that all strategies in Œìn are rationalizable, we can actually take ¬µ1 = ¬µ2 to be
distributions that assign probability 1/2 to each of (T, R) and (B, L). It is easy to see
that this satisfies the conditions of Theorem 3.4, taking Z1 = {T, B} and Z2 = {L, R}.
However, we do not have to take ¬µ1 = ¬µ2 . As long as the support of both ¬µ1 and ¬µ2 is
{(T, R), (B, L)}, any choice of ¬µ1 and ¬µ2 works.
Osborne and Rubinstein‚Äôs definition of rationalizability allows ¬µj,T to be such that j
believes that other players‚Äô strategy choices are correlated. In most of the literature, players
are assumed to believe that other players‚Äô choices are made independently. If we add the
latter requirement, then we must impose the same requirement on the probability measures
¬µ1 , . . . , ¬µn in Theorem 3.4.
It is important in the characterization of rationalizability that i‚Äôs strategy be part of
i‚Äôs local state. Intuitively, i‚Äôs strategy together with his beliefs about the strategies of the
remaining players determine i‚Äôs type. In modeling rationalizability, it suffices to assume that
i‚Äôs strategy determines i‚Äôs beliefs, so we can identify i‚Äôs type with his strategy. By including
the strategy in the local state, we are basically allowing different types of player i.
3.2 Capturing Solution Concepts in Extensive-Form Games
We now consider solution concepts in extensive-form games. Recall that, in this case, we
assume that players are using behavioral strategies.
3.2.1 Nash Equilibrium
Here we get essentially the same result as Theorem 3.1: the behavioral strategy profile ~œÉ
is a Nash equilibrium in the extensive-form game Œì iff (RŒì , ¬µ~œÉ ) is compatible with EQŒì .
However, a number of new subtleties arise in extensive-form games. First, Nash equilibrium in an extensive-form game does not require that players make a best response off the
equilibrium path. This is dealt with in our definition of compatibility since the fact that we
consider only points (r, m) such that ¬µi (Ki (r, m)) > 0 means that we are considering only
points on the equilibrium path.
Another subtlety arises from the fact that, in determining whether œÉi is a best response
to œÉ‚àíi at a point (r, m), player i is allowed to change to a completely different strategy œÉi0 .
But the definition of EQŒì in extensive-form games only considers changing to a different
action. To show that this suffices, we appeal to a result known in the literature as the
one-deviation property (Osborne & Rubinstein, 1994). The one-deviation property holds
if, in order to check that a behavioral strategy œÉ is a best response to œÉ‚àíi , it suffices to
156

A Procedural Characterization of Game Concepts

check local changes to œÉ; that is, it suffices to check behavioral strategies that differ from œÉ
by just modifying what œÉ does at one information set.
Let œÉ[I/a] be the behavioral strategy that is just like œÉ except that it assigns probability
1 to the action a at the information set I. Strategies of the form œÉ[I/a] are what we consider
to show that the one-deviation property holds at information set I. But a best response
at information set I allows changes not just at I, but at all information sets preceded
by I. Our analysis considers only what are called games with perfect recall. Roughly
speaking, in a game of perfect recall, all the players recall what moves they have made
and what information sets they have passed through. This recollection is formalized by
putting conditions on information sets. We omit the formal definition of perfect recall here
(see Osborne & Rubinstein, 1994). In a finite extensive-form game Œì with perfect recall,
for each player i, we can define a partial order i on player i‚Äôs information sets such that
I i I 0 if, for every history h ‚àà I, there is a prefix h0 of h in I 0 . Thus, I i I 0 if I 0 is
preceded by I, or, equivalently, appears below I in the game tree. Given two information
sets, we say that I precedes I 0 , and write I  I 0 , if I = I 0 or I 0 comes after I in some history
of the game (i.e., if some node in I 0 is preceded by a node in I in the game tree). In games
of perfect recall,  is a partial order; there cannot be two distinct information sets I and
I 0 such that I  I 0 and I 0  I. Given an information set I for player i, denote by [œÉi0 , I, œÉi ]
the strategy for player i that agrees with œÉi on all information sets I 0 for player i such that
I  I 0 and agrees with œÉi0 on all other information sets.
We now recall the notion of best response for behavioral strategies given by Halpern
(2013). A belief system (Kreps & Wilson, 1982) is a function ¬µ that associates with each
information set I a probability, denoted ¬µI , on the histories in I. Given a behavioral strategy
~œÉ and a belief system ¬µ in an extensive-form game Œì, let Pr~œÉ denote the distribution on
terminal histories induced by ~œÉ and define
EUi ((~œÉ , ¬µ) | I) =

XX

¬µI (h)Pr~œÉ (z | h)ui (z).

h‚ààI z‚ààZ

Thus, the expected utility of (~œÉ , ¬µ) conditional on reaching I captures the expected payoff
to player i if ~œÉ is played from information set I on, given that the relative likelihood of
histories in I is determined by ¬µ. Finally, if ~œÉ is a completely-mixed behavioral strategy
profile, let ¬µ~œÉ be the belief system determined by ~œÉ in the obvious way:
¬µ~IœÉ (h) = Pr~œÉ (h | I).
Definition 3.5 (Halpern, 2013) If Œµ ‚â• 0 and I is an information set for player i that is
0 for i conditional
reached with positive probability by ~œÉ 0 , then œÉi is an Œµ-best response to ~œÉ‚àíi
on having reached I using ~œÉ 0 if, for every strategy œÑ for player i, we have
0

0

0
0
EUi (((œÉi , ~œÉ‚àíi
), ¬µ~IœÉ ) | I) ‚â• EUi (((œÑi , ~œÉ‚àíi
), ¬µ~IœÉ ) | I) ‚àí Œµ.
0 for
The strategy œÉi is an Œµ-best response for i relative to ~œÉ 0 if œÉi is an Œµ-best response to ~œÉ‚àíi
0
i conditional on having reached I using ~œÉ for all information sets I for i that are reached
with positive probability by ~œÉ 0 . The strategy ~œÉi is a best response for i relative to ~œÉ 0 (resp.,
best response for i conditional on having reached I using ~œÉ 0 ) if œÉi is a 0-best response for i
relative to ~œÉ 0 (resp., 0-best response for i conditional on having reached I).

157

Halpern & Moses

0
Thus, œÉi is a best response for i relative to ~œÉ 0 if œÉi is a best response to œÉ‚àíi
at each
0
information set I for player i that is reached with positive probability by ~œÉ , where we
assume that ~œÉ 0 determines the probability of the histories in I and, in best responding,
we allow player i to make arbitrary changes after I has been reached. determines the
probability of reaching I.
The next result is basically the one-deviation property; it shows that a strategy that is
optimal with respect to local changes is in fact a best response.

Theorem 3.6 Let Œì be a game of perfect recall. The strategy œÉi is a best response response
to œÉ‚àíi relative to ~œÉ iff EUi (~œÉ ) ‚â• EUi (œÉi [I/a], ~œÉ‚àíi ) for each information set I for player i
that is reached with positive probability by ~œÉ and each action a that i can play at I.
Proof This is essentially proved by Selten (1975), so we just briefly sketch the argument
here. Clearly, if œÉi is a best response to œÉ‚àíi relative to ~œÉ and I is reached by ~œÉ with
positive probability, then EUi (((~œÉ , ¬µ~IœÉ ) | I) ‚â• EUi ((œÉi [I/a], ~œÉ‚àíi ), ¬µ~IœÉ ) | I) for all information sets I that are reached with positive probability according to ¬µ~œÉ at which player i
moves, and all actions a it can take at I. For the converse, suppose that EUi (((~œÉ , ¬µ~IœÉ ) |
I) ‚â• EUi ((œÉi [I/a], ~œÉ‚àíi ), ¬µ~IœÉ ) | I) for all information sets I that are reached with positive
probability according to ¬µ~œÉ at which player i moves, and all actions a it can take at I. By
way of contradiction, suppose that œÉi is not a best response to œÉ‚àíi relative to ~œÉ . Then
there must be some information set I that is reached with positive probability by ~œÉ and
strategy œÑi for player i such that EUi (([œÉi , I, œÑi ], ~œÉ‚àíi ), ¬µ~IœÉ ) | I) > EUi (((~œÉ , ¬µ~IœÉ ) | I). We get
an easy contradiction by considering a latest information set I for player i that is reached
with positive probability by ~œÉ at which this inequality holds (so that at all information set
I 0 6= I such that I  I 0 , the inequality does not hold). (Since Œì is a game of perfect recall,
the notion of ‚Äúlatest information set‚Äù is well defined.)
Theorem 3.7 The behavioral strategy profile ~œÉ is a Nash equilibrium of the extensive-form
game Œì iff (RŒì , ¬µ~œÉ ) is compatible with EQŒì .
We omit this proof, since it is easier (and similar in spirit) to the the proofs presented
later in this section.
3.2.2 Perfect Equilibrium
We start by considering (trembling-hand) perfect equilibrium (Selten, 1975). This is defined
in both normal-form games and extensive-form games. For ease of exposition, we focus on
the definition in extensive-form games, although essentially the same approach applies to
normal-form games.
The idea is that ~œÉ is a perfect equilibrium if, not only is œÉi a best response to ~œÉ‚àíi ,
but œÉi is a best response even if some players j 6= i ‚Äútremble‚Äù, and (with exceedingly small
probability) play a strategy other than œÉj .
To make this precise, define a completely
mixed (behavioral) strategy for player i to be a strategy where, at each information set
for player i, each action that can be played is played with positive probability. Observe
that if ~œÉ is a completely mixed strategy in an extensive-form game Œì, then it will reach
every information set I of each player in Œì with positive probability. For an extensive-form
158

A Procedural Characterization of Game Concepts

game Œì, the strategy profile ~œÉ is a perfect equilibrium in Œì iff there exists a sequence ~œÉ n
of completely mixed strategies such that ~œÉ n ‚Üí ~œÉ and, for all n and each information set I
n conditional on having reached I. Intuitively, ~
n
of player i, œÉi is a best response to ~œÉ‚àíi
œÉ‚àíi
n
represents a ‚Äútremble‚Äù; ~œÉ‚àíi , n = 1, 2, 3, . . . is a sequence of trembles converging to ~œÉ . The
strategy œÉi must be a best response to each tremble in this sequence. Thus, each strategy œÉi
in a perfect equilibrium profile ~œÉ is not a best response to all possible trembles, but to the
trembles along one particular path converging to ~œÉ . (The definition of perfect equilibrium
is essentially the same in normal-form games, except that there is no need to condition on
the information set.)
Our result depends on a characterization of perfect equilibrium given by Halpern (2009,
2013) that uses nonstandard probabilities, which can assign infinitesimal probabilities to
strategy profiles. A discrete nonstandard probability distribution on a set of runs is a discrete
probability distribution that assigns nonstandard probabilities to runs so that the sum over
all runs adds up to 1.
Halpern (2009) shows that by using nonstandard probability, we can capture Selten‚Äôs
intuition for trembling-hand equilibrium without needing to explicitly refer to sequences of
strategy profiles, as is done in Selten‚Äôs original definition. The idea is that the sequence
converging to ~œÉ in Selten‚Äôs original definition is replaced by a single completely mixed
strategy profile that is infinitesimally close to ~œÉ . To make this precise, we need a few
definitions. It is well known that to every nonstandard real number œÅ, there is a closest
standard real number denoted st(œÅ) ‚àà IR, and called ‚Äúthe standard part of œÅ‚Äù: the difference
|œÅ ‚àí st(œÅ)| is an infinitesimal. Given a nonstandard probability measure ŒΩ, we can define
the standard probability measure st(ŒΩ) by taking st(ŒΩ)(w) = st(ŒΩ(w)) for all states œâ ‚àà ‚Ñ¶.
Two possibly nonstandard distributions ŒΩ and ŒΩ 0 differ infinitesimally if st(ŒΩ) = st(ŒΩ 0 ).
While, as Selten shows, a perfect equilibrium always exists in normal-form games, it
does not necessarily exist in an arbitrary extensive-form game. However, it does exist in
extensive-form games of perfect recall. We can now state Halpern‚Äôs characterization of
trembling-hand equilibrium. We say that two behavioral strategies œÉi and œÉi0 for player
i differ infinitesimally if the distributions œÉi (I) and œÉi0 (I) differ infinitesimally for each
information set I for player I. Two strategy profiles ~œÉ = (œÉ1 , . . . , œÉn ) and ~œÉ 0 = (œÉ10 , . . . , œÉn0 )
differ infinitesimally if œÉi differs infinitesimally from œÉi0 at I, for every i = 1, . . . , n.
Theorem 3.8 (Halpern, 2009, 2013) The behavioral strategy profile ~œÉ = (œÉ1 , . . . , œÉn ) is a
perfect equilibrium in an extensive-form game Œì of perfect recall iff there exists a nonstandard completely mixed behavioral strategy profile ~œÉ 0 that differs infinitesimally from ~œÉ such
0 relative to ~
that œÉi is a best response to œÉ‚àíi
œÉ 0 for each player i.
When dealing with standard probabilities, in the definition of a probabilistic system
‚àí
‚Üí
PS = (R, ¬µ
~ ) being compatible with knowledge-based program profile Pg, we required that
the action played by Pgi at (r, m) be the same as that played in the system PS at (r, m)
only for runs r such that ¬µi (r | R[Ki (r, m)]) > 0. We now want to restrict not just to runs
that have positive probability, but to runs that have ‚Äúnontrivial‚Äù positive probability. The
obvious choice would be to require that st(¬µi (r | R[Ki (r, m)])) > 0. In settings where all
players are following a behavioral strategy, this requirement would would restrict to runs r
where, at all times m0 > m, if player j moves at the information set associated with (r, m0 ),
then the move made by j is given positive standard probability by the behavioral strategy.
159

Halpern & Moses

We would like this to be the case as well for the move made at (r, m). Since we encode
i‚Äôs intended move at (r, m) in i‚Äôs local state ri (m) (recall that the local state models i‚Äôs
information after i has made the coin toss), conditional on Ki (r, m), player i‚Äôs intended
move has probability 1, even if it has infinitesimal probability according to his strategy.
For simplicity, we describe the requirement that we want only in systems of the form
RŒì . In such systems, if I is an information set in Œì, let R[I] consist of all runs that go
through information set I. Note that if i moves at I, then R[I] is the disjoint union of
sets of the form R[Ki (r, m)], for runs r where i‚Äôs local state has the form (I, a). We now
define what it means for PS = (RŒì , ¬µ
~ ), where ¬µ
~ is a profile of nonstandard probability
measures just as we did before, except that in clause (b), we replace the requirement that
¬µi (r | R[Ki (r, m)]) > 0 by st(¬µi (r | R[Ki (r, m)])) > 0, and if i moves at the information set
I associated with (r, m), then we further strengthen this requirement to st(¬µi (r | R[I])) > 0.
Since ¬µ(r | R[I]) is a convex combination of terms of the form ¬µ(r | R[Ki (r0 , m)]), where
the sum is taken over the points (r0 , m) such that the node associated with (r0 , m) is in I
and ¬µ(R[Ki (r0 , m)]) > 0, and ¬µ(r | R[Ki (r0 , m)]) = 0 if (r, m) ‚àà
/ Ki (r0 , m), it easily follows
that if st(¬µi (r | R[I])) > 0 then st(¬µi (r | R[I])) > 0.
For standard probability measures ¬µi , it is easy to see that ¬µi (R[Ki (r, m)]) > 0 and
that ¬µi (r | R[Ki (r, m)]) > 0 iff both ¬µi (R[Ki (r, m)]) > 0 and ¬µi (r | R[I]), so this really is a
generalization of the standard definitions.6
Roughly speaking, this says that compatibility is required only at points on which i
places ‚Äúsignificant‚Äù probability on the moves made by the strategy used by i. (This is made
more precise in the proof of Theorem 3.9.)
Note that since only one player i moves at a node w in an extensive-form game (since we
do not allow moves by nature), in the action profile ~a such that applying ~a to r(m) results
‚àí‚àí‚ÜíŒì
in r(m + 1), we have aj = skip for j 6= i. This is because (r, m) ‚àà
/ [[Œ∫]]PS for a test Œ∫ in EQj ;
these tests Œ∫ are true only at points where j moves.
Theorem 3.9 The strategy profile ~œÉ is a perfect equilibrium of the extensive-form game Œì
of perfect recall iff there exists a (possibly nonstandard) completely mixed behavioral strategy
‚àí‚àí‚ÜíŒì
profile ~œÉ 0 such that œÉi differs infinitesimally from œÉi0 and (RŒì , ¬µ~œÉ0 ) is compatible with EQ .
Proof Suppose that ~œÉ = (œÉ1 , . . . , œÉn ) is a perfect equilibrium of Œì. By Theorem 3.8,
there exists a nonstandard completely mixed strategy profile ~œÉ 0 that differs infinitesimally
0 relative to ~
from ~œÉ such that œÉi is a best response to œÉ‚àíi
œÉ 0 , for each player i = 1, . . . , n.
‚àí‚àí‚ÜíŒì
We show that PS = (RŒì , ¬µ~œÉ0 ) is compatible with EQ . Suppose that the information set
I = Ki (r, m) associated with (r, m) is one where i moves, and that st(¬µ~œÉ0 (r | R[I])) > 0.
(Note that since ~œÉ 0 is completely mixed, it is guaranteed that ¬µ~œÉ0 (R[I]) > 0.) Let a be
6. The reader may wonder why we did not just take ri (m) to be i‚Äôs information set, rather than having
it include the action that i plans to do. While the former choice would have simplified the discussion
above, for the kb program EQŒì to be meaningful, i has to know what action he is about to do. Note
that what we are doing here is considering i‚Äôs information at two stages: before he has tossed the coin to
determine his next action, and after he has tossed it. We are conditioning on his information before he
tossed the coin, even though i‚Äôs local state models only the situation after he has tossed the coin. Using
this intuition we can extend the definition of compatibility beyond the scope of systems of the form RŒì ,
as long as the system is generated by players running randomized programs (like behavioral strategies),
although making this precise is beyond the scope of this paper.

160

A Procedural Characterization of Game Concepts

the action encoded in ri (m); that is, a is the action that i plans to play at (r, m). Since
st(¬µ~œÉ (r | R[I])) > 0, it must be the case that the action a is given positive standard
probability by the (completely mixed) distribution œÉi0 (I). Since ~œÉ is a perfect equilibrium,
we have by Theorem 3.8 that œÉi is a best response to œÉ‚àíi relative to ~œÉ . Let œÑi = [œÉi0 , I, œÉi ].
By definition, œÑi and œÉi agree on their actions at I. Since œÉi gives a positive standard
probability at I, so does œÑi . By Theorem 3.6, œÑi must be at least as good a response as
0 , for any action a0 that i can play at I. Since œÑ gives a positive standard
œÑi [I/a0 ] to ~œÉ‚àíi
i
0 for any action
probability, œÑi [I/a] must be at least as good a response as œÑi [I/a0 ] to ~œÉ‚àíi
0
0
a , conditional on reaching I using ~œÉ . It is easy to see that the expected utility of œÑi [I/a]
(resp., œÑi [I/a0 ]) conditional on reaching I is just the value of EUi (a) (resp., EUi (a0 )) at the
point (r, m). Thus, (r, m) ‚àà [[EUi (a) ‚â• EUi (a0 )]]PS , so PS is compatible with EQŒì .
For the converse, suppose that ~œÉ 0 is a completely mixed behavioral strategy profile
‚àí‚àí‚ÜíŒì
such that œÉi differs infinitesimally from œÉi0 and (RŒì , ¬µ~œÉ0 ) is compatible with EQ . Let
~œÉ = st(~œÉ 0 ). Again, let œÑi = [œÉi0 , I, œÉi ]. By Theorems 3.8 and 3.6, it suffices to show that for
each information set I for player i and each action a0 that i can play at I, the strategy œÑi is
0 conditional on having reached I using ~
at least as good a response as œÑi [I/a0 ] to ~œÉ‚àíi
œÉ 0 . To
do this, it suffices to show that for each action a in the support of œÑi (I) = œÉi (I), strategy
0 conditional on having reached I using
œÑi [I/a] is at least as good a response as œÑi [I/a0 ] to ~œÉ‚àíi
~œÉ 0 . So fix an information set I where player i moves and suppose that a is in the support
of œÉi (I). Let r be a history that reaches I = Ki (r, m) in which i plays a at I and all the
players play an action that is given positive standard probability by ~œÉ (and hence also by
~œÉ 0 ) at all points preceding (r, m). Thus, st(¬µ~œÉ (r | R[I])) > 0. Since PS is compatible with
EQŒì , it must be the case that (r, m) ‚àà [[EUi (a) ‚â• EUi (a0 )]]PS . As in the first half of the
0 conditional
proof, it now follows that œÑi [I/a] is at least as good a response as œÑi [I/a0 ] to ~œÉ‚àíi
on having reached I using ~œÉ 0 . Thus, ~œÉ is a perfect equilibrium.
3.2.3 Sequential Equilibrium
We next characterize sequential equilibrium in terms of EQŒì . Recall that a sequential equilibrium (Kreps & Wilson, 1982) is an assessment, a pair (~œÉ , ¬µ), where ~œÉ is a behavioral
strategy profile and ¬µ is a belief system, that is, a function that determines for every information set I a probability ¬µI over the histories in I. Intuitively, if I is an information set
for player i, then ¬µI is i‚Äôs subjective assessment of the relative likelihood of the histories in
I. Roughly speaking, an assessment is a sequential equilibrium if both (a) at every information set where a player moves he chooses a best response given the beliefs he has about
the histories in that information set and the strategies of other players, and (b) his beliefs
are consistent with the strategy profile being played. We omit the formal definition here,
and instead use a characterization of sequential equilibrium due to Halpern (2009).
Theorem 3.10 (Halpern, 2009, 2013) An assessment (~œÉ , ¬µ) is a sequential equilibrium
in an extensive-form game Œì with perfect recall iff there exist an infinitesimal Œµ and a
nonstandard completely mixed strategy profile ~œÉ 0 that differs infinitesimally from ~œÉ such
that œÉi is an Œµ-best response to œÉ‚àíi relative to ~œÉ , for each player i.
The only difference between sequential equilibrium and perfect equilibrium in this characterization is that with perfect equilibrium œÉi must be a best response to œÉ‚àíi relative to
161

Halpern & Moses

~œÉ while with sequential equilibrium, it need only be an Œµ-best response for some infinitesimal Œµ. To capture this difference, when dealing with sequential equilibrium, we reinterpret
the formula EUi (a) ‚â• EUi (a0 ) so as to ignore infinitesimal differences. Thus, the formula
is true unless st(EUi (a0 ) ‚àí EUi (a)) > 0 (or, equivalently, it is true if the standard part of
i‚Äôs expected utility using a is greater than or equal to the standard part of i‚Äôs expected
‚àí‚àí‚ÜíŒì
utility using a0 ). PS is st-compatible with EQ (standing for compatible with respect to
‚àí‚àí‚ÜíŒì
‚àí‚àí‚ÜíŒì
standard values) if PS is compatible with EQ under this reinterpretation of EQ . Clearly,
when all the probability distributions in PS are standard, the notions compatibility and
‚àí‚àí‚ÜíŒì
st-compatibility of EQ with PS coincide.
Theorem 3.11 The assessment (~œÉ , ¬µ) is a sequential equilibrium of the finite extensiveform game Œì of perfect recall iff there exists (possibly nonstandard) completely mixed strategy
‚àí‚àí‚ÜíŒì
profile ~œÉ 0 such that œÉi differs infinitesimally from œÉi0 and (RŒì , ¬µ~œÉ0 ) is st-compatible with EQ .
Proof The proof is almost identical to that of Theorem 3.9, replacing ‚Äúbest response‚Äù by
‚ÄúŒµ-best response‚Äù, and ‚Äúcompatible‚Äù with ‚Äúst-compatible‚Äù. We leave details to the reader.

3.2.4 Subgame-Perfect Equilibrium
Subgame-perfect equilibrium, defined by Selten (1965), is usually considered in games of
perfect information, where all information sets are singletons. In games of perfect information, subgame perfection, sequential equilibrium, and trembling-hand perfect equilibrium
all agree, so we do not need to provide a separate characterization. However, subgame
perfection is actually defined for arbitrary games of perfect recall.
Given a game Œì of perfect information and a node w in Œì, the subtree of Œì rooted at w
determines a subgame that we denote Œìw if, for every information set I in Œì that includes a
node w0 at or below w in Œì, all the nodes in I are below w in Œì. For example, the subtree of
the game Œìe in Figure 2 does not determine a subgame, since the information set {w2 , w3 }
includes a node that is at or below w2 (namely, w2 itself), but w3 is not below w2 . The
strategy profile ~œÉ is a subgame-perfect equilibrium if, for every subgame Œìw of Œì, ~œÉ restricted
to the nodes in Œìw is a Nash equilibrium in Œìw . Note that subgame perfection places no
requirements on the action played at nodes that do not determine subgames, beyond the
fact that the action must be part of a Nash equilibrium at nodes higher in the tree that do
determine subgames.
We can use a program much like EQŒì to characterize subgame-perfect equilibrium
in arbitrary games of perfect recall. We need to make two changes to EQŒì . First, we
need to say that a best response is required only at points where subgame holds, where
(r, m) ‚àà [[subgame]]PS if the node w associated with (r, m) determines a subgame. (Note
that [[subgame]]PS consists of all points in a game of perfect information.) Second, we need
to say that there are no constraints at points where subgame does not hold. So, for each
action a ‚àà Ai (Œì), we now have two lines of the form
V
if Ki (intendi (a) ‚àß subgame ‚àß a0 ‚ààSi (A) EUi (a) ‚â• EUi (a0 )) then play a
V
if Ki (intendi (a) ‚àß a0 ‚ààSi (A) EUi (a) ‚â• EUi (a0 )) then play a.
Call the resulting program SUBEQŒìi .
162

A Procedural Characterization of Game Concepts

These changes make it clear that subgame perfect is a somewhat awkward notion in
games where players do not have perfect information. In any case, with this change, an
analogue of Theorem 3.9 holds for subgame perfect equilibria.
Theorem 3.12 The strategy profile ~œÉ is a subgame-perfect equilibrium of the extensiveform game Œì of perfect recall iff there exists a (possibly nonstandard) completely mixed
behavioral strategy profile ~œÉ 0 such that œÉi differs infinitesimally from œÉi0 and (RŒì , ¬µ~œÉ0 ) is
subgame compatible with SUBEQŒì .
We omit the proof here, which is similar in spirit to that of Theorem 3.9.

4. Discussion and Conclusions
The essential intuition in many solution concepts is that (it is common knowledge that)
players are making a best response to their beliefs. We have shown that this ‚Äúprocedural‚Äù
‚àí‚àí‚ÜíŒì
intuition can be captured by a single knowledge-based program, denoted EQ . The differences between these solutions concepts lies in differences in assumptions about players‚Äô
beliefs and in what counts as a best response.
‚Ä¢ In Nash equilibrium, players believe that a mixed strategy profile is being played (and
have common belief about which one it is).
‚Ä¢ In correlated equilibrium, the players believe that a correlated strategy profile is being
played (and have common belief about which one it is).
‚Ä¢ In perfect equilibrium, they can be viewed as believing that a nonstandard completely
mixed strategy profile is being played (and having common belief about which one it
is), and caring only about what happens in situations with positive standard probability.
‚Ä¢ In sequential equilibrium, they can similarly be viewed as believing that a nonstandard
completely mixed strategy profile is being played (and have common belief about
which one it is), and caring only about what happens at states with positive standard
probability and about best responses with respect to standard differences (an Œµ better
response for some infinitesimal Œµ is not viewed as being better).
‚Ä¢ In rationalizability, different players may hold different beliefs about the strategy
profile being played.
While the unification given by kb programs arguably does give insight, there is clearly
a significant amount of overhead in the kb program framework. It is certainly reasonable
to ask whether it is worth dealing with the overhead just to get such a unification, given
that the intuitions are certainly well understood in the game-theory literature.
If the sole advantage of using kb programs was to prove the theorems in this paper,
then perhaps the answer is ‚Äúno‚Äù, but we believe that the kb program framework offers
much more to game theory than just this unification. For one thing, kb programs can
capture the intuition of best response more generally. We give a few examples here:
163

Halpern & Moses

‚Ä¢ Dealing with moves by nature: We have assumed for simplicity that there were no
moves by nature in the extensive-form games being analyzed. To deal with moves
by nature, we first expand the notion of a global state so that it includes the local
state of ‚Äúnature‚Äù, not just the local states of the players. We can think of nature‚Äôs
local state as consisting of the current node in the game tree. In addition, we think of
nature, just like the players, as following a behavioral strategy (where nature‚Äôs move
depends on its local state). With these minor changes, all our results still go through,
with no change. In particular, all of the theorems in the paper continue to hold even
for games where nature moves, with no change in the proof.
‚Ä¢ Bayesian games: In a Bayesian game, players have types. We can think of a type as
a description of a player‚Äôs private information. There is assumed to be a commonlyknown distribution over type profiles. A strategy can be viewed as a function from
types to actions. A player‚Äôs utility depends on both the action profile and the type
profile. The standard solution concept considered in Bayesian games is a Bayes-Nash
equilibrium. In a Bayes-Nash equilibrium, no player wants to switch to a different
strategy, since doing so results in a lower expected utility (see Osborne & Rubinstein,
1994, for details). We can again capture a Bayes-Nash equilibrium in our framework.
Now a player‚Äôs local state would include the player‚Äôs type, and a run can be characterized by the strategy profile and type profile. This means that the set of runs in
RŒì is larger. With these changes, an analogue of Theorem 3.1 holds for Bayes-Nash
equilibrium.
‚Ä¢ Beyond expected utility maximization: All of the solution concepts that we have considered in the paper are based on maximizing expected utility. But we can also consider solution concepts based on other decision criteria. For example, Boutilier and
Hyafil (2004) consider minimax-regret equilibria, where each player uses a strategy
that is a best-response in a minimax-regret sense to the choices of the other players.
Similarly, we can use maximin equilibria (Aghassi & Bertsimas, 2006). As pointed
out by Chu and Halpern (2003), all these decision rules can be viewed as instances
of a generalized notion of expected utility, where (a) uncertainty is represented by a
plausibility measure, a generalization of a probability measure, (b) utilities are elements of an arbitrary partially ordered space, and (c) plausibilities and utilities are
combined using ‚äï and ‚äó, generalizations of + and √ó. Just by interpreting ‚ÄúEUi = u‚Äù
appropriately, we can capture these more exotic solution concepts as well. Moreover,
applying the same ideas and essentially the same proof we can capture solution concepts in games in which the game itself is not common knowledge, or where players
are not aware of all available moves, as discussed by Halpern and ReÃÇgo (2013).
All the results mentioned up to now are straightforward, and much in the spirit of the
results we have already shown. A more interesting situation arises when we consider games
of imperfect recall. Part of the overhead in the framework is the need to specify exactly
what the players‚Äô local states are, that is, what they know. In the context of games of
perfect recall, this is perhaps not that important, but when we move to games of imperfect
recall, this becomes highly significant. Consider the single-player game depicted in Figure 3,
first introduced by Piccione and Rubinstein (1997).
164

A Procedural Characterization of Game Concepts

x0
.5

.5
z0

S

x2

x1

z1

S

2

2

B

B
x3
L
z2

R

L

6

2

z3
3

X

x4
R
z4

z5
4

Figure 3: A game of imperfect recall.
It is not hard to show that the strategy that maximizes expected utility in this example
chooses move S at node x1 , move B at node x2 , and move R at the information set X
consisting of x3 and x4 . Call this strategy œÉ. Let œÉ 0 be the strategy of choosing move B
at x1 , move S at x2 , and move L at X. Piccione and Rubinstein argue that if node x1 is
reached, the player should reconsider, and decide to switch from œÉ to œÉ 0 . While this indeed
leads to a better payoff, the resulting strategy (i.e., starting with œÉ and switching to œÉ 0 at
x2 , if x2 is reached) is not a legal strategy in the original game; the player moves left at x3
and right at x4 , although the two nodes are in the same information set.
The question of what the player‚Äôs local state is now becomes critical. We did not include
the player‚Äôs behavior strategy in his local state when we modeled extensive-form, but we
could have done so with no change. Suppose that we do so in this game of imperfect recall.
First note that if we include the player‚Äôs strategy in his local state, then in the system RŒì
corresponding to the game, if h is the history ending in z5 , at the point (rh , 2), the player
knows that he is at x4 , despite the information set. This is an instance of a more general
phenomenon: in a game of imperfect recall, the strategy that a player is using gives him
information about which node in an information set he is at. This cannot happen in a game
of perfect recall.
Suppose for ease of exposition that we include a player‚Äôs strategy in his local state.
What happens if a player switches strategy. How does his local state change then? If the
local state includes the new strategy (whether or not it includes the original strategy), then
in the set of runs that arises if the player sticks to œÉ at x2 , but switches from œÉ to œÉ 0 at x1 ,
when player reaches x3 , he knows that he is at x3 , and if he reaches x4 , he knows that he
is at x4 . The ‚Äúinformation set‚Äù is not correctly representing the player‚Äôs knowledge at all!7
The key point here is that the runs-and-systems framework forces a modeler to consider
questions like whether a player is able to keep track of his changes of strategy; moreover, the
answers must be reflected in the choice of local state. There has been some recent work on
7. This point was already made by Halpern (1997).

165

Halpern & Moses

defining notions like sequential equilibrium in games of imperfect recall (see, e.g., Halpern
& Pass, 2011b; Kline, 2005; Marple & Shoham, 2012). We believe that the program EQŒì
and, more generally, the use of the runs-and-systems framework can provide some insight
into this problem.
Interesting new issues arise when we add computation to the picture. Equilibrium
notions that take computation into account have been considered by Halpern and Pass
(2011a). It seems that the notion of computational Nash equilibrium defined by Halpern and
Pass, where players choose a Turing machine to play for them, can be captured using EQŒì
as well. But that is only because, roughly speaking, there is no charge for the computation
of which Turing machine is a best response to the Turing machines chosen by the other
players. If we were to impose such a cost, then we might need more ‚Äúcomputational‚Äù or
‚Äúalgorithmic‚Äù notions of knowledge (such notions are discussed by Fagin et al., 1995, ch. 10).
We conclude with two other directions for further research. First, although we have
focused on using kb programs to characterize solution concepts here, the idea that an agent‚Äôs
actions depend on her knowledge and beliefs seems like a very natural way to characterize
strategies in games, and ‚Äúmeta-strategies‚Äù for classes of games. Sayings such as ‚Äúlook before
you leap‚Äù and ‚Äútrust, but verify‚Äù are really shorthand for knowledge-based programs. We
believe that useful insights into how agents play games can be gained by thinking at the
knowledge level in this way. Indeed, it is not only preconditions on actions that depend
on knowledge and belief; an agent‚Äôs utility can also depend on her beliefs. This is the
key insight in psychological games (Geanakoplos, Pearce, & Stacchetti, 1989). It would be
interesting to extend knowledge-based programs to knowledge-based utilities.
Finally, although we have talked about kb programs as ‚Äúprocedural‚Äù, in fact, there is
no procedure given for the calculation of the relevant knowledge, which really amounts to a
best-response computation. In a non-probabilistic setting, there are conditions under which
a kb program can be implemented by a unique standard program (i.e., one without tests
for knowledge) as shown by Fagin et al. (1995, Section 7.2). Such results do not carry
over to probabilistic systems (since they give no indication of how to compute the relevant
probabilities). Nevertheless, given beliefs, kb programs can be viewed as defining how an
agent should act. When computing an equilibrium, the beliefs are typically determined by
the strategy profile. That is, we do not start with beliefs and then determine how to act.
Rather, in most of the solutions concepts we have considered here, we have a fixed point:
the beliefs determine the strategies (each player‚Äôs strategy is a best response to her beliefs),
and the strategies determine the beliefs. However, we believe that, in other applications
of kb programs, it may well be possible to view a kb program as providing a procedural
specification. We leave this topic to further research.

Acknowledgments
We thank the reviewers for their perceptive comments, which led to many improvements
in the paper. Some material in this paper appeared in preliminary form in (Halpern &
Moses, 2007). Joe Halpern‚Äôs work on this paper was supported in part by NSF grants
IIS-0534064, IIS-0812045, IIS-0911036, and CCF-1214844, by AFOSR grants FA9550-08-10438 and FA9550-09-1-0266, by the DoD Multidisciplinary University Research Initiative
(MURI) program administered by AFOSR under grant N00014-01-1-0795, and by ARO
166

A Procedural Characterization of Game Concepts

grant W911NF-09-1-0281. Yoram Moses is the Israel Pollak academic chair at the Technion;
his work was supported in part by the Israel Science Foundation under grant ISF 1520/11.

References
Aghassi, M., & Bertsimas, D. (2006). Robust game theory. Mathematical Programming,
Series B, 107 (1‚Äì2), 231‚Äì273.
Aumann, R. J. (1974). Subjectivity and correlation in randomized strategies. Journal of
Mathematical Economics, 1, 67‚Äì96.
Aumann, R. J. (1987). Correlated equilibrium as an expression of Bayesian rationality.
Econometrica, 55, 1‚Äì18.
Aumann, R. J., & Brandenburger, A. (1995). Epistemic conditions for Nash equilibrium.
Econometrica, 63 (5), 1161‚Äì1180.
Benthem, J. van (2007). Rational dynamics and epistemic logic in games. International
Game Theory Review, 9 (1), 13‚Äì45. (Reprinted with corrections in International Game
Theory Review, 9:2, 377-409.).
Benthem, J. van (2010). Modal Logic for Open Minds. Center for Study of Logic and
Information-Lecture Notes.
Bernheim, B. D. (1984). Rationalizable strategic behavior. Econometrica, 52 (4), 1007‚Äì1028.
Bonanno, G. (2002). Modal logic and game theory: two alternative approaches. Risk
Decision and Policy, 7, 309‚Äì324.
Brafman, R. I., Latombe, J.-C., Moses, Y., & Shoham, Y. (1997). Applications of a logic
of knowledge to motion planning under uncertainty. Journal of the ACM, 44 (5),
633‚Äì668.
Brandenburger, A., & Dekel, E. (1987). Rationalizability and correlated equilibria. Econometrica, 55, 1391‚Äì1402.
Bruin, B. de (2010). Explaining Games: The Epistemic Programme in Game Theory, Vol.
346. Synthese Library.
Chu, F., & Halpern, J. Y. (2003). Great expectations. Part I: On the customizability of
generalized expected utility. In Proc. Eighteenth International Joint Conference on
Artificial Intelligence (IJCAI ‚Äô03), pp. 291‚Äì296.
Dwork, C., & Moses, Y. (1990). Knowledge and common knowledge in a Byzantine environment: crash failures. Information and Computation, 88 (2), 156‚Äì186.
Fagin, R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1995). Reasoning About Knowledge.
MIT Press, Cambridge, Mass. A slightly revised paperback version was published in
2003.
167

Halpern & Moses

Fagin, R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1997). Knowledge-based programs.
Distributed Computing, 10 (4), 199‚Äì225.
Geanakoplos, J., Pearce, D., & Stacchetti, E. (1989). Psychological games and sequential
rationality. Games and Economic Behavior, 1 (1), 60‚Äì80.
Hadzilacos, V. (1987). A knowledge-theoretic analysis of atomic commitment protocols. In
Proc. 6th ACM Symposium on Principles of Database Systems, pp. 129‚Äì134.
Halpern, J. Y. (1997). On ambiguities in the interpretation of game trees. Games and
Economic Behavior, 20, 66‚Äì96.
Halpern, J. Y. (2009). A nonstandard characterization of sequential equilibrium, perfect
equilibrium, and proper equilibrium. International Journal of Game Theory, 38 (1),
37‚Äì50.
Halpern, J. Y. (2013). A nonstandard characterization of sequential equilibrium, perfect
equilibrium, and proper equilibrium: Erratum. Unpublished manuscript.
Halpern, J. Y., & Moses, Y. (2007). Characterizing solution concepts in games using
knowledge-based programs. In Proc. Twentieth International Joint Conference on
Artificial Intelligence (IJCAI ‚Äô07), pp. 1300‚Äì1307.
Halpern, J. Y., & Moses, Y. (2010). Characterizing solution concepts in games using common knowledge of rationality. Unpublished manuscript.
Halpern, J. Y., Moses, Y., & Waarts, O. (2001). A characterization of eventual Byzantine
agreement. SIAM Journal on Computing, 31 (3), 838‚Äì865.
Halpern, J. Y., & Pass, R. (2011a). Algorithmic rationality: Game theory with costly computation.. Available at www.cs.cornell.edu/home/halpern/papers/algrationality.pdf;
to appear, Journal of Economic Theory. A preliminary version with the title ‚ÄúGame
theory with costly computation‚Äù appears in Proc. First Symposium on Innovations
in Computer Science, 2010.
Halpern,
J. Y., & Pass,
R. (2011b).
Sequential equilibrium
games of imperfect recall.
Unpublished manuscript;
available
www.cs.cornell.edu/home/halpern/papers/imperfect.pdf.

in
at

Halpern, J. Y., & ReÃÇgo, L. C. (2013). Extensive games with possibly unaware players.
Mathematical Social Sciences. To appear.
Halpern, J. Y., & Zuck, L. D. (1992). A little knowledge goes a long way: knowledge-based
derivations and correctness proofs for a family of protocols. Journal of the ACM,
39 (3), 449‚Äì478.
Harrenstein, P., Hoek, W. van der, Meyer, J.-J. C., & Witteveen, C. (2002). On modal
logic interpretations of games. In ECAI, pp. 28‚Äì32.
168

A Procedural Characterization of Game Concepts

Hyafil, N., & Boutilier, C. (2004). Regret minimizing equilibria and mechanisms for games
with strict type uncertainty. In Proc. Twentieth Conference on Uncertainty in Artificial Intelligence (UAI 2004), pp. 268‚Äì277.
Kline, J. J. (2005). Imperfect recall and the relationships between solution concepts in
extensive games. Economic Theory, 25, 703‚Äì710.
Kreps, D. M., & Wilson, R. B. (1982). Sequential equilibria. Econometrica, 50, 863‚Äì894.
Lang, J., & Zanuttini, B. (2012). Knowledge-based programs as plans‚Äîthe complexity of
plan verification. In Proceedings of the 20th European Conference on AI (ECAI 2012),
pp. 504‚Äì504.
Lang, J., & Zanuttini, B. (2013). Knowledge-based programs as plans: succinctness and the
complexity of plan existence. In Theoretical Aspects of Rationality and Knowledge:
Proc. Fourteenth Conference (TARK 2013), pp. 138‚Äì147.
Lorini, E., & Schwarzentruber, F. (2010). A modal logic of epistemic games. Games, 1 (4),
478‚Äì526.
Marple, A., & Shoham, Y. (2012). Equilibria in finite games with imperfect recall. Unpublished manuscript.
Mazer, M. S. (1990). A link between knowledge and communication in faulty distributed
systems. In Theoretical Aspects of Reasoning about Knowledge: Proc. Third Conference, pp. 289‚Äì304.
Mazer, M. S., & Lochovsky, F. H. (1990). Analyzing distributed commitment by reasoning
about knowledge. Tech. rep. CRL 90/10, DEC-CRL.
Moses, Y., & Kislev, O. (1993). Knowledge-oriented programming. In Proc. 12th ACM
Symposium on Principles of Distributed Computing, pp. 261‚Äì270.
Moses, Y., & Tuttle, M. R. (1988). Programming simultaneous actions using common
knowledge. Algorithmica, 3, 121‚Äì169.
Neiger, G., & Bazzi, R. (1992). Using knowledge to optimally achieve coordination in distributed systems. In Theoretical Aspects of Reasoning about Knowledge: Proc. Fourth
Conference, pp. 43‚Äì59.
Neiger, G., & Toueg, S. (1993). Simulating real-time clocks and common knowledge in
distributed systems. Journal of the ACM, 40 (2), 334‚Äì367.
Osborne, M. J., & Rubinstein, A. (1994). A Course in Game Theory. MIT Press, Cambridge, Mass.
Pearce, D. G. (1984). Rationalizable strategic behavior and the problem of perfection.
Econometrica, 52 (4), 1029‚Äì1050.
Piccione, M., & Rubinstein, A. (1997). On the interpretation of decision problems with
imperfect recall. Games and Economic Behavior, 20 (1), 3‚Äì24.
169

Halpern & Moses

Reiter, R. (2001). On knowledge-based programming with sensing in the situation calculus.
ACM Transactions on Computational Logic, 2 (4), 433‚Äì457.
Selten, R. (1965).
Spieltheoretische Behandlung eines Oligopolmodells mit NachfragetraÃàgheit. Zeitschrift fuÃàr Gesamte Staatswissenschaft, 121, 301‚Äì324 and 667‚Äì689.
Selten, R. (1975). Reexamination of the perfectness concept for equilibrium points in extensive games. International Journal of Game Theory, 4, 25‚Äì55.

170

Journal of Artificial Intelligence Research 49 (2014) 363-402

Submitted 09/13; published 02/14

Multiagent Only Knowing in Dynamic Systems
Vaishak Belle

vaishak@cs.toronto.edu

Dept. of Computer Science, University of Toronto,
Toronto, Ontario, Canada M5S 3H5

Gerhard Lakemeyer

gerhard@cs.rwth-aachen.de

Dept. of Computer Science, RWTH Aachen University,
52056 Aachen, Germany

Abstract
The idea of ‚Äúonly knowing‚Äù a collection of sentences, as proposed by Levesque, has been previously shown to be very useful in characterizing knowledge-based agents: in terms of a specification,
a precise and perspicuous account of the beliefs and non-beliefs is obtained in a monotonic setting.
Levesque‚Äôs logic is based on a first-order modal language with quantifying-in, thus allowing for
de re versus de dicto distinctions, among other things. However, the logic and its recent dynamic
extension only deal with the case of a single agent. In this work, we propose a first-order multiagent framework with knowledge, actions, sensing and only knowing, that is shown to inherit all
the features of the single agent version. Most significantly, we prove reduction theorems by means
of which reasoning about knowledge and actions in the framework simplifies to non-epistemic,
non-dynamic reasoning about the initial situation.

1. Introduction
When considering knowledge-based agents in dynamic worlds, much depends on what is known
and what is not, and how that evolves. Making a telephone call, for example, requires knowing the
referent, and if the number is not known, a lookup in the telephone directory must be attempted,
after which the agent would have sufficient information to complete the task. Essentially, the agent
deliberates on the act of sensing when the agent‚Äôs knowledge base (KB) informs the agent that
it is ignorant of some fact, perhaps one that is necessary for achieving a goal. Moreover, taking
a pragmatic point of view, it is desirable that when designing agents, the modeler would provide
certain facts but may leave others unsaid. Think of a simple blocks world domain where we find a
red block on the table. The agent would be told of the red block, but in the absence of a complete
description of the location of every other block in the domain, the agent has to make do with partial
information. Thus, at the very least, what is needed is a compact way to write down the knowledge
base, thereby providing a full specification of the beliefs and non-beliefs, an account of how that
changes after acting and sensing, and a query language that explicitly refers to this knowledge.1
One way to view the first requirement is to think that the beliefs of the agent are exactly those
that follow from the assumption that a KB is all that is believed. Perhaps the most general account
to capture the beliefs of a KB is OL: the logic of only knowing by Levesque (1990). Levesque‚Äôs
proposal is remarkably simple. He augments a logic of belief (Hintikka, 1962; Kripke, 1963; Fagin,
1. We use the terms ‚Äúknowledge‚Äù and ‚Äúbelief‚Äù interchangeably with the understanding that knowledge need not necessarily be true in the real world.
c
2014
AI Access Foundation. All rights reserved.

Belle & Lakemeyer

Halpern, Moses, & Vardi, 1995), where (say) the modality K denotes knowledge, with a modality
O to capture the notion of ‚Äúonly knowing.‚Äù Beliefs are reasoned about in terms of valid sentences
of the form:
OKB ‚äÉ KŒ±
which is to be read as ‚Äúif KB is all that is believed by the agent, then the agent knows Œ±.‚Äù What
is particularly interesting about the new modality is that it not only allows one to draw conclusions about what is known but also about what is not. That is, O p ‚äÉ ¬¨Kq and, by introspection, O p ‚äÉ K¬¨Kq both come out valid. Note that this is quite different from classical epistemic
logic (Fagin et al., 1995), in the sense that if we replace O by K, then neither of these sentences
is valid. As a consequence, for example, from O(Tel(A, 1234) ‚à® Tel(B, 1234)) the agent concludes
K(‚àÉx. Tel(x, 1234) ‚àß ¬¨KTel(x, 1234)). This says that the agent knows there is someone whose
telephone number is 1234 without knowing who, usually referred to as the de dicto versus de re
distinctions in knowledge (Kaplan, 1968). Thus, an agent is able to reason about its own ignorance,
in a quantificational language, without having to be told explicitly what it does not know.
While OL does capture the desiderata on beliefs, it does not include any notions of actions. To
obtain the many features of OL in a dynamic setting, a logic ES (Lakemeyer & Levesque, 2011)
was proposed that amalgamates OL and the situation calculus (McCarthy & Hayes, 1969; Reiter,
2001; Scherl & Levesque, 2003). The situation calculus is a popular and general formalism for
representing and reasoning about dynamic domains. ES is a (situation-suppressed) modal dialect
of the situation calculus2 that has formulas like those of traditional dynamic logic (Harel, Kozen, &
Tiuryn, 2000), such as
[pickup(obj5)](Holding(obj5) ‚àß ¬¨Holding(obj3))
which says that after picking up obj5, the agent is holding obj5 but not obj3. In ES, one stipulates
the set of axioms capturing the application domain to be all that is known by the agent, and then
obtains entailments regarding beliefs, non-beliefs, and belief expansion that can resolve the agent‚Äôs
ignorance as it acts and perceives in the environment. For example,
œÜ = {SF(senseFragility(x)) ‚â° Fragile(x)},
roughly says that after any sequence of actions, if the agent were to perform a fragility sensing
action, SF would inform the robot whether the object sensed is fragile or not. If obj5 is an object
that is fragile in the real world, ES allows us to reason about entailments of the sort:
1. |= œÜ ‚àß Fragile(obj5) ‚àß OœÜ ‚äÉ ¬¨K(Fragile(obj5));
2. |= œÜ ‚àß Fragile(obj5) ‚àß OœÜ ‚äÉ [senseFragility(obj5)]K(Fragile(obj5));
which, in English, says that although the agent does not know that obj5 is fragile initially, he does
so after sensing.
ES not only allows Reiter-style basic action theories, but is also equipped with an important
result from (Reiter, 2001; Scherl & Levesque, 2003): the regression theorem for knowledge. That
2. Under certain assumptions, valid sentences in ES can be mapped as valid sentences in the classical situation calculus (Lakemeyer & Levesque, 2011). That is, ES can serve as a semantic basis for the situation calculus with a more
workable model theory.

364

Multiagent Only Knowing in Dynamic Systems

is, sentences and goals about the future, even those mentioning belief, are reduced to questions
(perhaps involving knowledge) about the initial state only. More importantly, a significant result
from OL called the representation theorem (Levesque & Lakemeyer, 2001) can be leveraged to
reduce epistemic queries about the initial state to a first-order reasoning task. In effect, no modal
reasoning will be necessary.
However, ES only deals with the single agent case. Many AI applications where such formalisms are needed involve multiple agents. We might imagine a robot following the lead of another agent, perhaps a second robot, and they are to coordinate deliveries of items between rooms.
Similarly, we imagine two agents playing a game of cards against each other. In these and in others,
modeling and reasoning about beliefs and non-beliefs that agents have about the real world and the
other agents in this world is of interest. In the case of a card game, for example, especially a fair
one, agents might believe initially that all their opponents know are the rules of the game. This
might then justify certain strategies that depend on the lack of information on the opponent‚Äôs part.
Before extending ES to the multiagent case, however, we first need an account of only knowing
in the multiagent case. While a number of previous proposals (Lakemeyer, 1993; Halpern, 1993;
Halpern & Lakemeyer, 2001; Waaler & Solhaug, 2005) have attempted multiagent extensions to
OL, they are all propositional. Besides, they significantly deviate from Levesque‚Äôs simple model
theory. In recent work (Belle & Lakemeyer, 2010a), we were able to show that a natural generalization of OL to the n-agent case does exist for a first-order language. In this article, we continue
that line of work and propose a n-agent generalization to ES.3 For the projection problem (Reiter,
2001), where we are interested in reasoning about goals (perhaps involving multiagent beliefs) after
actions, we show that a regression property is provable. Finally, we also obtain a representation
theorem for the n-agent case by means of which no modal reasoning will be necessary. We survey
related literature in greater detail in Section 5 but both of these results differ from existing results
in the epistemic situation calculus (Scherl & Levesque, 2003), which extends the situation calculus
in having a notion of knowledge realized in terms of an accessibility relation between situations.
(That is, situations are viewed as possible worlds.) For instance, the regression property is different
from previous multiagent generalizations (Shapiro, LespeÃÅrance, & Levesque, 2002; Kelly & Pearce,
2008) of the epistemic situation calculus in that the background theory may involve nesting of only
knowing operators, such as ‚Äúall that Alice knows is that Bob only knows the rules of the game.‚Äù
Capturing multiagent only knowing in possible-world models that include explicit accessibility relations between worlds (Fagin et al., 1995), as required by the classical epistemic situation calculus,
is known to be problematic (Halpern & Lakemeyer, 2001; Belle & Lakemeyer, 2010a), and so
such statements do not have obvious counterparts in previous proposals. Similarly, the reduction of
knowledge to first-order reasoning is investigated in a very restricted setting by Reiter (2001), and
for the single agent case only.
The paper is structured as follows. We first introduce the logic, followed by a discussion of basic
action theories. Subsequently, we prove the regression property and a generalized representation
theorem. We end after discussing related work. Appendices contain proofs of the main results, that
is, the regression property and the representation theorem.

3. A preliminary version of this work appears in the proceedings of the Twenty-Fourth AAAI Conference on Artificial
Intelligence, Atlanta, Georgia, USA, July 11-15, 2010 (Belle & Lakemeyer, 2010b).

365

Belle & Lakemeyer

2. The Formalism
We let ESn be a first-order modal language consisting of formulas over symbols from the following
vocabulary:
‚Ä¢ first-order variables of the object sort: x1 , x2 , . . . , y1 , y2 , . . .;
‚Ä¢ first-order variables of the action sort: a1 , a2 , . . .;
‚Ä¢ fluent predicates of arity k: F1 , F2 , . . .; for example, Wet;
‚Ä¢ rigid predicates of arity k: G1 , G2 , . . .; for example, Fragile;
‚Ä¢ fluent function symbols of arity k: f1 , f2 , . . .; for example, distance;
‚Ä¢ rigid function symbols of arity k: g1 , g2 , . . .; for example, pickup, senseColor;
‚Ä¢ countably infinite standard names: # 1, # 2, . . . for objects and actions;
‚Ä¢ connectives and other symbols: =, ‚à®, ¬¨, ‚àÄ, Ki , Oi , [a], , parenthesis, period and comma.
In the following, for ease of exposition, we assume i ‚àà {A, B} in Ki and Oi , that is, there are two
agents A and B. The extension to more agents is straightforward.
We remark that standard names are rigid designators, that is, they mean the same entity in
all possible worlds (see below). They can be thought of as constants but satisfying the unique
name assumption and an infinitary version of domain closure. Having these symbols means that
quantification can be understood substitutionally. Readers familiar with the classical situation calculus (Reiter, 2001) may note that situation terms do not appear in the language. Therefore, we
have to distinguish fluents, whose values change after actions, and rigids, whose values do not, both
syntactically as well as semantically. ESn is also assumed to contain a distinguished predicate Poss
and distinguished functions SFi , both of which take an action as an argument. Essentially, Poss(a)
says that a is executable; SFi (a) refers to agent i‚Äôs sensing outcomes on performing a, as shown for
the single agent case in the previous section using the fragility sensing action. Section 3 will discuss
this in detail for multiple agents.
The terms of ESn are of the sort action or object, and they are the least set such that:
‚Ä¢ every standard name and first-order variable is a term of the corresponding sort;
‚Ä¢ if t1 , . . . , tk are terms (of any sort) and f is a k-ary function, then f (t1 , . . . , tk ) is a term.
By a primitive term, we mean one of the form f (n1 , . . . , nk ) where f is a (fluent or rigid) function
symbol and all of the ni are standard names.
The well-formed formulas of ESn form the least set such that:
‚Ä¢ if t1 , . . . , tk are terms, and F is a k-ary predicate symbol then F(t1 , . . . , tk ) is an (atomic)
formula;
‚Ä¢ if t1 and t2 are terms, then (t1 = t2 ) is a formula;
366

Multiagent Only Knowing in Dynamic Systems

‚Ä¢ if t is an action term and Œ± is a formula then [t]Œ± is a formula;
‚Ä¢ if Œ± and Œ≤ are formulas, and x is a first-order variable then the following are also formulas:
¬¨Œ±, Œ± ‚à® Œ≤, ‚àÄxŒ±, Œ±, Ki Œ±, Oi Œ±.
As usual, we treat other connectives such as ‚äÉ and ‚â° as abbreviations. That is, Œ± ‚äÉ Œ≤ abbreviates ¬¨Œ± ‚à® Œ≤, and Œ± ‚â° Œ≤ abbreviates (Œ± ‚äÉ Œ≤) ‚àß (Œ≤ ‚äÉ Œ±).
ESn has two epistemic modalities. We read Ki Œ± as ‚Äúi knows Œ±,‚Äù and we read Oi Œ± as ‚Äúall that i
knows is Œ±.‚Äù ESn also includes dynamic modalities. We read [a]Œ± as ‚ÄúŒ± holds after doing a‚Äù and
we read Œ± as ‚ÄúŒ± holds after all possible action sequences.‚Äù
A formula without any free variables is called a sentence. We also refer to certain kinds of
formulas with the following terminology:
‚Ä¢ A formula with no  operators is called bounded.
‚Ä¢ A formula with no [t] or  operators is called static.
‚Ä¢ A formula that does not mention Oi for any i is called basic. (The formula may mention KA
or KB .)
‚Ä¢ A formula with no Ki , Oi , [t], Poss or SFi is called fluent.4
For example, P(# 1) ‚à® [t]KA P(# 2) is bounded, but not static; P(# 1) ‚à® KA P(# 2) is static and basic,
but it is not a fluent formula; P(# 1) ‚à® OA P(# 2) ‚à® Poss(t) is a static formula, but neither is it a basic
formula nor is it a fluent formula; (P(# 1) ‚à® P(# 2)) ‚àß Q( f (# 3)) is a fluent formula.
2.1 The Semantics
A semantics is provided in terms of possible worlds. The purpose of the semantics is to determine
the values of fluents, both initially and after any sequence of actions. Therefore, in ESn , similar
to the idea of situation trees (Reiter, 2001), worlds determine the changing values of fluents after
actions; see Figure 1 for the intuition. More precisely,
‚Ä¢ let Z denote all finite sequences of action names, including hi, which is the empty sequence
(corresponding to the initial situation);
‚Ä¢ then a world w ‚àà W is any function from G √ó Z to {0, 1}, where G is the set of primitive
atoms, and from T √ó Z to N (preserving sorts), where T is the set of primitive terms, and
satisfying the rigidity constraint: if g is a rigid function or predicate symbol, then for all z and
z0 in Z, w[g(n1 , ..., nk ), z] = w[g(n1 , ..., nk ), z0 ].
To interpret arbitrary terms, we proceed as follows. As mentioned earlier, names are rigid
designators. Given a term t without variables, a world w and a sequence z, we define |t|zw (to be read
as ‚Äúthe co-referring standard name for t given w and z‚Äù) by:
1. |t|zw = t if t is a name;
4. In the situation calculus (Reiter, 2001), these correspond to formulas that are uniform in a situation term.

367

Belle & Lakemeyer

p, q, . . .

a1

p, ¬¨q, . . .

ak

a1

¬¨p, ¬¨q, . . .

...

..
.

hi

a2

¬¨p, ¬¨q, . . .

ak

...

ak

a1

...

..
.

¬¨p, q, . . .

a1

ak

...

..
.

Figure 1: A possible world.
2. | f (t1 , . . . , tk )|zw = w[ f (n1 , . . . , nk ), z], where |ti |zw = ni .
Agents may, of course, have incomplete knowledge. To distinguish their uncertainty from the real
world, we stipulate epistemic states that model multiple possibilities. Standard accounts of multiagent epistemic states are based on Kripke frames (Fagin et al., 1995). For multiagent only knowing,
however, Kripke-based accounts turn out to be very problematic, as seen in the work of Halpern
(1993), Lakemeyer (1993), Halpern and Lakemeyer (2001) and Waaler and Solhaug (2005). For
example, Lakemeyer (1993) shows that certain types of epistemic states cannot be constructed in
his approach. In the work of Halpern (1993), epistemic operators do not interact in an intuitive
manner (Halpern & Lakemeyer, 2001). In the work of Halpern and Lakemeyer (2001), the semantic
notion of validity is defined directly in the language, making the proposal unnatural. Serious complications are present in later proposals as well (Waaler & Solhaug, 2005). Moreover, none of these
have been extended to a quantified language. A discussion on these issues is not needed for the purposes of this article; interested readers are referred to our earlier work (Belle & Lakemeyer, 2010a).
In that work, we then proposed an alternative called k-structures, which was shown to generalize
Levesque‚Äôs (1990) proposal to the many agent case in an appropriate and intuitive manner. These
structures deviate from Kripke-based accounts in defining epistemic states of increasing depths. As
it turns out, these structures also have a natural extension to the dynamic setting, which we present
below. We first define a notion of depth for formulas in the following way:
Definition 1 The i-depth of Œ± ‚àà ESn , denoted |Œ±|i , is defined inductively as (Mi denotes Ki or Oi ):
‚Ä¢ |Œ±|i = 1 for atomic formulas;
‚Ä¢ |¬¨Œ±|i = |Œ±|i ;
368

Multiagent Only Knowing in Dynamic Systems

‚Ä¢ |‚àÄxŒ±|i = |Œ±|i ;
‚Ä¢ |[a]Œ±|i = |Œ±|i ;
‚Ä¢ |Œ±|i = |Œ±|i ;
‚Ä¢ |Œ± ‚à® Œ≤|i = max(|Œ±|i , |Œ≤|i );
‚Ä¢ |Mi Œ±|i = |Œ±|i ,
‚Ä¢ |M j Œ±|i = |Œ±| j + 1 for j , i.
A formula Œ± has depth k if max(|Œ±|A , |Œ±|B ) = k.
Given a formula of A-depth k and of B-depth j, we say that the formula has A, B-depth of k, j for
brevity. We say Œ± is objective if no epistemic operators are mentioned in Œ±. A formula is called
i-objective if all epistemic operators which do not occur within the scope of another epistemic
operator are of the form M j , j , i, where Mi denotes Ki or Oi . A formula is called i-subjective if
every atom is in the scope of an epistemic operator and all epistemic operators which do not occur
within the scope of another epistemic operator are of the form Mi . Intuitively, i-subjective formulas
represent i‚Äôs beliefs about the world whereas i-objective formulas determine what is true about the
world from i‚Äôs perspective, which may include beliefs of agents other than i.
Example 2 Consider the formula KA KB KA p ‚à® KB [t]q. Here:
‚Ä¢ |KA KB KA p ‚à® KB [t]q|A = max(|KA KB KA p|A , |KB [t]q|A ) = 3 because
1. |KA KB KA p|A = |KA KB KA p|A = |KB KA p|A = 1 + |KA p|B = 2 + |p|A = 3,
2. |KB [t]q|A = 1 + |[t]q|B = 1 + |q|B = 2.
‚Ä¢ |KA KB KA p ‚à® KB [t]q|B = max(|KA KB KA p|B , |KB [t]q|B ) = 4 because
1. |KA KB KA p|B = |KA KB KA p|B = 1 + |KB KA p|A = 1 + 3 (as shown above) = 4,
2. |KB [t]q|B = |[t]q|B = |q|B = 1.
Therefore, the depth of the formula is 4. Consider each of the disjuncts. KA KB KA p is both
A-subjective as well as B-objective. On the other hand, KB [t]q is both B-subjective as well as
A-objective. Moreover, KA KB KA p ‚à® KB [t]q is neither A-subjective nor B-subjective. For that
matter, it is neither A-objective nor B-objective.
The beliefs of an agent are captured by means of a k-structure defined over the set W:
Definition 3 A k-structure ek , where k ‚â• 1, is defined inductively as:
‚àí e1 ‚äÜ W √ó {{}},
‚àí ek ‚äÜ W √ó Ek‚àí1 , where Em is the set of all m-structures.
369

Belle & Lakemeyer

That is, a e1 is simply a set of worlds. A e2 is a set of the form {(w, e1 ), (w0 , e0 1 ), . . .} which states
that at w an agent, say A, believes B to consider worlds from e1 possible, and at w0 she believes B to
consider worlds from e0 1 possible. This captures the intuition that A has partial information about
B, and so her beliefs about B differ at different worlds.5 When modeling a k-structure, say ek , for A
j
we denote it as ekA . Analogously, when modeling a j-structure, say e j , for B we denote it as eB .
Such structures essentially represents the initial beliefs of the agent, that is, the initial state of
knowledge. But when actions occur, perhaps an agent acquires new information and as a result
of this some of the possibilities in an epistemic state may be discarded over the course of doing
actions (Scherl & Levesque, 2003). Following Lakemeyer and Levesque (2011), we capture this
feature by means of a compatibility relation 'iz between worlds (relative to an agent i), which looks
for truth in the real world by means of sensing. We define w0 'iz w inductively by the following:
‚Ä¢ w0 'ihi w for all worlds w0 and w;
‚Ä¢ w0 'iz¬∑r w iff w0 'iz w and w0 [SFi (r), z] = w[SFi (r), z].
j

We define a ek for A, a e j for B and a world w as a (k, j)-model (ekA , eB , w). The idea is that only
formulas with a maximal A-depth of k and with a maximal B-depth of j are to be interpreted wrt
(k, j)-models. To determine whether a formula is true or not after a sequence of actions z given a
j
(k, j)-model, we write ekA , eB , w, z |= Œ±. The definition of truth is as follows:
j

1. ekA , eB , w, z |= P(t1 , . . . , tk ) iff w[P(n1 , . . . , nk ), z] = 1 where |ti |zw = ni ;
j

2. ekA , eB , w, z |= t1 = t2 iff n1 and n2 are the same standard names, where |ti |zw = ni ;
j

j

3. ekA , eB , w, z |= ¬¨Œ± iff ekA , eB , w, z 6|= Œ±;
j

j

j

4. ekA , eB , w, z |= Œ± ‚à® Œ≤ iff ekA , eB , w, z |= Œ± or ekA , eB , w, z |= Œ≤;
j

j

j

j

5. ekA , eB , w, z |= ‚àÄxŒ± iff ekA , eB , w, z |= Œ±nx for every name n of the appropriate sort;
6. ekA , eB , w, z |= [t]Œ± iff ekA , eB , w, z ¬∑ r |= Œ± where |t|zw = r;
j

j

7. ekA , eB , w, z |= Œ± iff ekA , eB , w, z ¬∑ z0 |= Œ± for every z0 ‚àà Z;
j

8. ekA , eB , w, z |= KA Œ± iff for all w0 'Az w, for all ek‚àí1 (for B),
k
k k‚àí1
0
if (w0 , ek‚àí1
B ) ‚àà eA then eA , eB , w , z |= Œ±;
j

9. ekA , eB , w, z |= OA Œ± iff for all w0 'Az w, for all ek‚àí1 (for B),
k
k k‚àí1
0
(w0 , ek‚àí1
B ) ‚àà eA iff eA , eB , w , z |= Œ±.

In an analogous fashion, the semantics for KB Œ± and OB Œ± are specified. Here, Ki is the classical
epistemic operator. We may read Ki Œ± as ‚Äú(at least) Œ± is believed‚Äù because Ki Œ± certainly does not
preclude Ki (Œ± ‚àß Œ≤) from holding in general. On the other hand, if Oi Œ± holds then the epistemic
5. Levesque‚Äôs (1990) notion of an epistemic state is simply a set of worlds. It is easy to see that if there is only a single
agent then we only need 1-structures, which then coincides with Levesque‚Äôs account.

370

Multiagent Only Knowing in Dynamic Systems

state is one which contains all and only the structures satisfying Œ±. In essence (Levesque, 1990), the
definition for Oi differs from that for Ki in using an ‚Äúiff‚Äù rather than an ‚Äúif‚Äù.6
j
j
Given a sentence Œ± of maximal A, B-depth k, j, we write ekA , eB , w |= Œ± to mean ekA , eB , w, hi |= Œ±.
j
We say that a sentence Œ± of maximal A, B-depth k, j is satisfiable if there is a (k, j)-model (ekA , eB , w)
j
such that ekA , eB , w |= Œ±. If Œ£ is any set of sentences of maximal A, B-depth of k, j and Œ± is as above,
j
we write Œ£ |= Œ± (read: ‚ÄúŒ£ entails Œ±‚Äù) iff for every (k, j)-model such that ekA , eB , w |= Œ±0 for every
j
Œ±0 ‚àà Œ£ then ekA , eB , w |= Œ±. We write |= Œ± (read: ‚ÄúŒ± is valid‚Äù) to mean {} |= Œ±.
j

We often write {}, eB , w |= Œ± when Œ± is A-objective because the k-structure for A is irrelevant.
Analogously, for B-objective formulas, we often write ekA , {}, w |= Œ±. When the formula Œ± is objective, we omit the structures for A and B altogether and simply write w |= Œ±.
2.2 Properties
We differ slightly from usual semantical accounts in that the satisfaction relation is undefined for
formulas whose depth exceeds a certain number. Nevertheless, we are able to show that as far as
entailment is concerned, such an account does not present any serious limitations. Let us begin with
a few simple examples.
Example 4 Let p be an atom. Then the following sentences are valid. Our method for proving
these examples will be to look at the sentence to decide on the depth of the models. (We will use
TRUE to denote a tautologous sentence, such as ‚àÄx. (x = x).)
1. OA TRUE ‚äÉ ¬¨KA ¬¨KB p.
The sentence is A-subjective and of A-depth 2. So consider any 2-structure for A that satisfies
OA TRUE. Here is one: let e2A = W √ó 2W . Clearly e2A , {}, w |= OA TRUE. (We reiterate that
when the epistemic state for B is irrelevant, we will simply write (ekA , {}, w) and ignore the
structure for B.) It is easy to verify that no other e2 satisfies OA TRUE. So now e2A , {}, w |=
¬¨KA ¬¨KB p iff there is some (w0 , e1B ) ‚àà e2A such that e2A , e1B , w0 |= KB p. By construction, there
is (w, e‚àó1B ) ‚àà e2A where e‚àó1B = {(w, {}) | w |= p} and e2A , e‚àó1B , w |= KB p.
2. OA TRUE ‚äÉ ¬¨KA KB p.
Construct e2A as in item 1. Then e2A , {}, w |= ¬¨KA KB p iff there is some (w0 , e1B ) ‚àà e2A , such
that e2A , e1B , w0 |= ¬¨KB p. By construction, (w, e‚àó1B ) ‚àà e2A where e‚àó1B = {(w, {}) | w 6|= p} and,
e2A , e‚àó1B , w |= ¬¨KB p.
3. OA (p ‚àß OB p) ‚äÉ KA p.
We will consider any 2-structure for A satisfying OA (p ‚àß OB p) and prove that KA p is also
satisfied at the structure. So let W p = {w | w |= p}. Clearly e1B = {(w, {}) | w ‚àà W p } is the
only 1-structure for B that satisfies OB p. Similarly, the 2-structure e2A = {(w, e1B ) | w ‚àà W p }
is the only 2-structure for A that satisfies OA (p ‚àß OB p). It follows that e2A , {}, w |= KA p since
all w0 in (w0 , e1B ) ‚àà e2A satisfy p by construction.
6. In the literature, for a closer examination of the relationship between these modalities, a third modality to denote
what the agent ‚Äúat most‚Äù knows is often included in the logical language (Halpern & Lakemeyer, 2001; Levesque &
Lakemeyer, 2001). This modality need not concern us here. We refer interested readers to our earlier work on how a
semantics is given for such an operator using k-structures (Belle & Lakemeyer, 2010a).

371

Belle & Lakemeyer

4. OA (p ‚àß OB p) ‚äÉ KA KB p.
A 2-structure e2A is constructed as in item 3. Then it follows that e2A , {}, w |= KA KB p since all
worlds
{w00 | (w00 , {}) ‚àà e1B and (w0 , e1B ) ‚àà e2A for some w0 }
satisfy p by construction.
5. OA (p ‚àß OB p) ‚äÉ (KA ¬¨KB KA p ‚àß KA ¬¨KB ¬¨KA p).
Using ideas from item 1 and 2, it follows that OB p ‚äÉ ¬¨KB KA p ‚àß ¬¨KB ¬¨KA p is valid. Let e3A
be any structure that satisfies OA (p ‚àß OB p). Since for all (w0 , e2B ) ‚àà e3A , e3A , e2B , w0 |= p ‚àß OB p,
it follows that e3A , e2B , w0 |= ¬¨KB KA p ‚àß ¬¨KB ¬¨KA p. Therefore e3A , {}, w |= KA (¬¨KB KA p ‚àß
¬¨KB ¬¨KA p).
Items 1 and 2 tell us if all that A knows is TRUE, then she correctly reasons about her ignorance:
she does not know whether B knows p. Items 3 and 4 tell us if A only knows {p, OB p}, then she
correctly believes that both she and B believe p. Finally, item 5 tells us since A believes B only
knows p, she believes B cannot tell whether A knows p.
In these examples, we (appropriately) chose structures of a certain depth to interpret the sentences of a corresponding depth. However, as far as validity goes, models of any higher depth can
be considered. That is, if a formula of maximal A, B-depth k, j is true at all (k, j)-models, then the
formula is also true at all (k0 , j0 )-models, for k0 ‚â• k and j0 ‚â• j. To demonstrate this property, we
0
0
construct for every ekA , a k-structure eA ‚Üìkk , such that they agree on all formulas of maximal A-depth
k. Analogously, a j-structure that agrees on all formulas of maximal B-depth j can be constructed
j0
for every eB .
j0

0

j0

0

Definition 5 Given ekA and eB , we inductively define a k-structure eA ‚Üìkk and a j-structure eB ‚Üì j for
k0 ‚â• k ‚â• 1 and j0 ‚â• j ‚â• 1, respectively:
‚Ä¢ eA ‚Üì11 = e1A ;
‚Ä¢ eB ‚Üì11 = e1B ;
0

0

0

‚Ä¢ eA ‚Üìk1 = {(w, {}) | (w, ekB ‚àí1 ) ‚àà ekA } for k0 > 1;
j0

j0 ‚àí1

j0

‚Ä¢ eB ‚Üì1 = {(w, {}) | (w, eA ) ‚àà eB } for j0 > 1;
0

0

0

0

‚àí1
‚Ä¢ eA ‚Üìkk = {(w, eB ‚Üìkk‚àí1
) | (w, ekB ‚àí1 ) ‚àà ekA } for k > 1;
j0

j0 ‚àí1

j0 ‚àí1

j0

‚Ä¢ eB ‚Üì j = {(w, eA ‚Üì j‚àí1 ) | (w, eA ) ‚àà eB } for j > 1.
0

With this definition in hand, we get the following property by relating k0 -structures ekA and j0 0
j0
j0
structures eB , and their corresponding k-structures eA ‚Üìkk and j-structures eB ‚Üì j respectively.
Lemma 6 Let k0 ‚â• k and j0 ‚â• j. For all Œ± of maximum A-depth k and maximum B-depth j:
0

j0

0

j0

ekA , eB , w |= Œ± iff eA ‚Üìkk , eB ‚Üì j , w |= Œ±.
372

Multiagent Only Knowing in Dynamic Systems

The proof is not hard, but tedious. The arguments for this result appear elsewhere (Belle & Lakemeyer, 2010a), and so we do not reproduce them here.7
Theorem 7 For all formulas Œ± of A, B-depth of k, j, if Œ± is true at all (k, j)-models, then Œ± is true at
all (k0 , j0 )-models, where k0 ‚â• k and j0 ‚â• j.
0

j0

0

j0

Proof: Suppose Œ± is true at all (k, j)-models. Given any (ekA , eB , w), by assumption eA ‚Üìkk , eB ‚Üì j , w |=
0

j0

Œ±. By the previous lemma, ekA , eB , w |= Œ±.
It follows then that one may speak about the valid sentences of the logic without explicitly speculating what their depths or the depths of their models need to be. That is, we may simply assume
that models have ‚Äúappropriate‚Äù depths, in the sense of having depths that equal or exceed the depth
of the sentences. For example, we obtain the following result that knowledge with k-structures has
K45n properties (Fagin et al., 1995), as well as the universal and existential versions of the Barcan
formula. Moreover, these properties hold after any number of actions have been performed.
Lemma 8 Let Œ± and Œ≤ be ESn -formulas. Then the following sentences are valid:
1. (Ki Œ± ‚àß Ki (Œ± ‚äÉ Œ≤) ‚äÉ Ki Œ≤);
2. (Ki Œ± ‚äÉ Ki Ki Œ±);
3. (¬¨Ki Œ± ‚äÉ Ki ¬¨Ki Œ±);
4. (‚àÄxKi Œ± ‚äÉ Ki ‚àÄxŒ±);
5. (‚àÉxKi Œ± ‚äÉ Ki ‚àÉxŒ±).
Proof: The proofs are very similar. We show item 3 and 4. Let i be A. The other case is symmetric.
j

k
3. Suppose ekA , eB , w, z |= ¬¨KA Œ±. Then there is some w0 'zA w, (w0 , ek‚àí1
B ) ‚àà eA such that
k‚àí1
k
k‚àí1
0
00
00
A
0
00
0
eA , eB , w , z |= ¬¨Œ±. Let w be any world such that w 'z w , (w , eB ) ‚àà ekA . Clearly
j
ekA , e0B k‚àí1 , w00 , z |= ¬¨KA Œ±. Since w00 'zA w, we get that ekA , eB , w, z |= KA ¬¨KA Œ±.
j

j

4. Suppose ekA , eB , w, z |= ‚àÄxKA Œ±. Then ekA , eB , w, z |= (KA Œ±)nx for every name n. That is,
j
k
ekA , eB , w, z |= KA Œ±nx for every n. Then for all w0 'zA w, such that (w0 , ek‚àí1
B ) ‚àà eA we have
j
0
x
k k‚àí1
0
k
ekA , ek‚àí1
B , w , z |= Œ±n for every n iff by definition eA , eB , w , z |= ‚àÄxŒ±. Therefore eA , eB , w, z |=
KA ‚àÄxŒ±.
Apart from K45n belief properties, the relationship between only knowing and knowledge can
also be established using the notion of validity:
Lemma 9 Suppose p and q are atoms, and Œ± is any ESn -formula. Then the following are valid:
7. While actions are not considered in that work, these are interpreted wrt worlds and so the extension of the argument
is straightforward.

373

Belle & Lakemeyer

1. Oi Œ± ‚äÉ Ki Œ±;
2. Oi p ‚äÉ ¬¨Ki q.
Proof: Item 1 is an easy consequence of the semantics. For item 2, observe that by the definition
of only knowing, structures that satisfy p ‚àß ¬¨q, which must exist because p and q are atoms, are
included in an epistemic state where Oi p holds. Therefore q cannot be known.
Item 1 says that whatever is only known is also believed by the agent. Item 2, of course, relates only
knowing and non-beliefs. It is straightforward to generalize the arguments for these properties to
also capture the valid sentences from Example 4 involving multiagent nested beliefs.
Finally, when specifying the agent, we want to allow for agents that have false beliefs. This is
permitted, and that can be demonstrated by means of the following property that shows that it is
possible to know (and only know) a formula that is false in the real world, and it is also possible to
not know (and not only know) a formula that is true in the real world.
Lemma 10 Let p be an atom. Then following sentences are satisfiable (let Mi denote Ki or Oi ):
1. ¬¨p ‚àß Mi p;
2. p ‚àß ¬¨Mi p.
Proof: We show Mi = OA . The case for Mi = OB is symmetric. The arguments for Mi = Ki is
analogous. For item 1, let w be a world such that w |= ¬¨p, and W p = {w0 | w0 |= p}. Let e1A be the
set {(w0 , {}) | w0 ‚àà W p }. It follows then that e1A , {}, w |= ¬¨p ‚àß OA p.
For item 2, suppose w‚àó |= p, and W0p = W p ‚àí {w‚àó }. Let e1A = {(w0 , {}) | w0 ‚àà W0p }. Then, we
get e1A , {}, w‚àó |= p ‚àß ¬¨OA p.
Before concluding this section, let us briefly reflect on the fact that k-structures have a finite
depth. So suppose A only knows Œ£, of depth k. Using k-structures alone allows us to reason
about what is believed and what is not believed, up to depth k. For example, OA P(# 1) entails
KA P(# 1), ¬¨KA P(# 2), ¬¨KA P(# 3), . . . as shown in Lemma 9. Moreover, as already observed in Example 4, the logic correctly captures that A is ignorant about beliefs at depth greater than k. That is,
using the simple example of an agent who only knows TRUE of depth 1, we saw that the sentences
OA TRUE ‚äÉ ¬¨KA ¬¨KB p and OA TRUE ‚äÉ ¬¨KA KB p are valid. So, although the KB has finite
depth, we are able to ask queries Œ± of any depth in the sense of determining whether the sentence
Oi Œ£ ‚äÉ Ki Œ± is valid.
For most purposes, this restriction of having a parameter k seems harmless in the sense that
agents usually have a finite knowledge base with sentences of some maximal depth k and they should
be ignorant about what is known at depths higher than k. But there is one aspect which we cannot
handle: the property of simultaneously satisfying an infinite set of sentences of unbounded depth.
Indeed, k-structures cannot be used for this purpose simply because, for a fixed k, the satisfaction
relation is undefined for formulas beyond depth k.
One prominent application of such a property is the notion of common knowledge (Fagin et al.,
1995). We do not go over the details here, but the common knowledge modality allows the logic to
374

Multiagent Only Knowing in Dynamic Systems

reason about sentences such as (Ki K j )k Œ±, where Œ± appears in the scope of k sequences of Ki K j ,
for any k. Even though the nature of common knowledge is infinitary, in the sense that it essentially
corresponds to an infinite conjunction, it can nonetheless be given a finite axiomatic characterization, making it a useful operator for certain applications (Fagin et al., 1995). Thus, if we were to
include the notion of common knowledge in a logic, then we would get entailments about what is
believed at arbitrary depths. With our current model, however, this cannot be captured. While this
is certainly a restriction, we are willing to pay that price because in return we get, for the first time,
a very simple model theory for multiagent only knowing (Belle & Lakemeyer, 2010a).

3. Basic Action Theories
Let us now consider the equivalent of basic action theories of the situation calculus. Since situations
do not appear in the language, as in ES, the basic action theories do not require foundational axioms
like Reiter‚Äôs second-order induction axiom for situations (Reiter, 2001).
Definition 11 Given a set of fluents F , a set Œ£ ‚äÜ ESn of sentences is called a basic action theory
(BAT) over F iff Œ£ = Œ£0 ‚à™ Œ£pre ‚à™ Œ£post ‚à™ Œ£sense where Œ£ only mentions fluents from F and8
1. Œ£0 is any set of fluent sentences;
2. Œ£pre is a singleton sentence of the form:
Poss(a) ‚â° œÄ
where œÄ is a fluent formula;9
3. Œ£post is a set that includes sentences of the form:
[a]F(~x) ‚â° Œ≥F ,
one for each fluent predicate F, and sentences of the form:
[a] f (~x) = u ‚â° Œ≥ f ,
one for each fluent function f , where Œ≥F and Œ≥ f are fluent formulas;10
4. Œ£sense is a set of sentences similar to the one for Poss of the form:
SFi (a) = x ‚â° œïi ,
one for each agent i, where œïi is a fluent formula.
8. We follow the usual convention that free variables are universally quantified from the outside.
9. We assume that  has lower syntactic precedence than the logical connectives, so that Poss(a) ‚â° œÄ stands for
‚àÄa.(Poss(a) ‚â° œÄ).
10. The [a] construct has higher precedence than the logical connectives. That is, [a] f (x1 , . . . , xk ) = y ‚â° Œ≥ f abbreviates
‚àÄa.([a] f (x1 , . . . , xk ) = y ‚â° Œ≥ f ).

375

Belle & Lakemeyer

The idea is that Œ£0 expresses what is true initially, Œ£pre is one large precondition axiom, Œ£post are the
successor state axioms, one per fluent, which are formulated so as to incorporate Reiter‚Äôs solution
the frame problem. Œ£sense accommodates the intuition that the sensing results for agents may differ
for various actions. For example, when B senses that A is reading a letter, we would not expect B
to learn the contents of that letter. Here, we follow the convention (Scherl & Levesque, 2003) that
every action returns a sensing result. For actions such as forward, which do not return any sensing
information, SFi is defined to return a special standard name NIL.
Knowledge about the initial situation may be incomplete. More precisely, we have to distinguish
between what is true in the real world and what the agents know or believe about the world. Of
course, what A believes about the world may differ from B‚Äôs knowledge. Moreover, what A believes
B to know may differ from what B actually believes. One way to capture such generality is to
first insist on an action theory modeling the real world, say Œ•. Then, we might imagine differing
basic action theories for subsequent levels of beliefs for the agents, as illustrated by the following
theory:11
(1)
Œ• ‚àß OA (Œ£ ‚àß OB (Œ£ ‚àó ‚àß . . .)) ‚àß OB (Œ£ 0 ‚àß OA (Œ£ ‚àó‚àó ‚àß . . .))
where Œ• and Œ£ (with superscripts) are basic action theories that may differ arbitrarily. Here, Œ•
represents what is true in the real world, and Œ£ (with superscripts) represent the agent‚Äôs beliefs. For
example, Œ£ ‚àó represents what A believes B to know. By extension, then, for n agents with k levels,
we would expect n ¬∑ k + 1 action theories, each one perhaps differing arbitrarily from each other.
For ease of exposition, we will consider the following simple case in the remainder of the article.
The simple case stipulates that if Œ£ represents A‚Äôs view of the world, then he believes that Œ£ also
represents B‚Äôs view of the world. This is reasonable for applications such as simple card games,
which we consider below. None of our technical results, including the regression property and the
representation theorem, hinge on this stipulation, however. See Section 4.4 for discussions.
A background theory, then, is a special case of (1), as illustrated by the following sentence:
Œ• ‚àß OA (Œ£ ‚àß OB (Œ£ ‚àß . . .)) ‚àß OB (Œ£ 0 ‚àß OA (Œ£ 0 ‚àß . . .))

(2)

where, again, Œ•, Œ£ and Œ£ 0 may differ arbitrarily.
Formally, in order to prepare for agents that may have beliefs to some arbitrary (but finite) depth,
we introduce the following inductive definition over a basic action theory Œ£:
‚Ä¢ let OKnowŒ£ [A, 1] = OA Œ£;
‚Ä¢ let OKnowŒ£ [B, 1] = OB Œ£;
‚Ä¢ for k > 1, let OKnowŒ£ [A, k] = OA (Œ£ ‚àß OKnowŒ£ [B, k ‚àí 1]);
‚Ä¢ for j > 1, let OKnowŒ£ [B, j] = OB (Œ£ ‚àß OKnowŒ£ [A, j ‚àí 1]).
Given basic action theories Œ•, Œ£ and Œ£ 0 , in the remainder of the article we will be interested in
theories of the form
Œ• ‚àß OKnowŒ£ [A, k] ‚àß OKnowŒ£ 0 [B, j]
(3)
11. In the sequel, background theory stipulations assume the nesting of only knowing operators. There are other possibilities, of course, such as Oi (œÜ ‚àß (K j Œ± ‚à® K j Œ≤)). We defer discussions on these to Section 4.4.

376

Multiagent Only Knowing in Dynamic Systems

which says that A believes the action theory Œ£ to k levels, i.e. he believes B to also believe Œ£ and
so on, while B believes the action theory Œ£ 0 to j levels. Before presenting any technical results
on reasoning about actions, we show with an example how the formalism can be used to model
domains, and that it has appropriate properties regarding knowledge, introspection and sensing.
Example 12 Imagine two agents playing a simple card game. We imagine a deck of cards, numbered 1 through 52. Two face-down cards have been dealt, one to A and the other to B. Player i
picks her card, reads the card and decides to challenge player j ( j , i). When a challenge is posed,
the player with the card that has the highest number wins the game.
We begin by stipulating the preconditions of the domain. Let Œ£pre be the following:
Poss(a) ‚â°
‚àÉx[a = picki (x) ‚àß ‚àÄy(¬¨Holdingi (y))] ‚à®
‚àÉx[a = seei (x) ‚àß Holdingi (x)] ‚à®
a = challengei ‚àß TRUE.
In English: sensing actions seei are explained below, but to sense we assume that i is holding the
object. We let picki be a physical action, and we require that for object x to be picked up, i is not
holding anything else. The other fluent-changing action in the domain is challengei , such that Œ£post
has two elements:
[a]Holdingi (x) ‚â° a = picki (x) ‚à® Holdingi (x).
[a]Losei ‚â° Losei ‚à®
a = challengei ‚àß (num(cardi ) < num(card j )) ‚à®
a = challenge j ‚àß (num(card j ) > num(cardi )).
where cardi is the card that has been dealt to i, num is a rigid function representing the card‚Äôs
number, and Losei indicates that i has lost the game. That is, if i challenges, he would win only with
the higher number.
Let us now formalize the sensing axioms. When A reads his card, we expect her to discover the
number on the card. Actions are public, but despite the fact that B observes A reading her card, B is
not expected to discover the contents of A‚Äôs card. This asymmetry can be captured by letting Œ£sense
contain the following sentence:
[a]SFi (a) = y ‚â°
‚àÉx[a = seei (x) ‚àß y = num(x)] ‚à®
¬¨‚àÉx[a = seei (x) ‚àß y = NIL].
In English: i‚Äôs sensing results for the action seei (x) informs i about the number on card x, while
i‚Äôs sensing results for every other action returns NIL. That is, NIL is obtained when i senses on
physical actions, as well as when i observes j reading a card by means of see j .
Finally, we stipulate the initial theories, after which we are done. We let Œ£0 be the following:
‚Ä¢ ‚àÄx[¬¨Holdingi (x)];
‚Ä¢ num(cardA ) , num(cardB );
377

Belle & Lakemeyer

‚Ä¢ ‚àÄx[num(x) = # 1 ‚à® . . . ‚à® num(x) = # 52];
‚Ä¢ # 1 < # 2 < # 3 < . . . < # 51 < # 52;
‚Ä¢ ¬¨LoseA ‚àß ¬¨LoseB .
Here, we are supposing that cardA is the card that A was dealt, while cardB is the one that B was
dealt. Basically, Œ£0 says that the numbers on cardA and cardB are different but they are one of
{1, . . . , 52} and that initially, no player has lost the game.
Œ£0 represents the initial assumptions of the game. In general, players may have access to additional information. In an unfair setting, for instance, we might imagine that B knows A‚Äôs card before
A does. For our current purposes, however, we will simply assume that Œ£0 is what i believes, as well
as what i believes j to believe, at all levels. To now model the real world, let
Œ•0 = Œ£0 ‚à™ {num(cardA ) = # 1, num(cardB ) = # 52}.
Letting Œ£ = Œ£0 ‚à™ Œ£pre ‚à™ Œ£post ‚à™ Œ£sense , and letting Œ• = Œ£ ‚à™ Œ•0 , our development leads to a theory of
the following form:
Œ• ‚àß OKnowŒ£ [A, k] ‚àß OKnowŒ£ [B, j]
(4)
Prior to analyzing the entailments of (4), it is convenient to state a lemma regarding how a model
of (4) can be constructed. For that, we will use the notion of the modal depth of a formula, which
refers to the epistemic modalities in the formula.
Definition 13 The modal depth of a formula Œ± is defined inductively:
‚Ä¢ modal(Œ±) = 0 for atomic formulas;
‚Ä¢ modal(Œ± ‚à® Œ≤) = max(modal(Œ±), modal(Œ≤));
‚Ä¢ modal(‚àÄxŒ±) = modal(Œ±);
‚Ä¢ modal([t]Œ±) = modal(Œ±);
‚Ä¢ modal(Œ±) = modal(Œ±);
‚Ä¢ modal(¬¨Œ±) = modal(Œ±);
‚Ä¢ modal(Mi Œ±) = 1 + modal(Œ±) where Mi ‚àà {Ki , Oi }.
For example, p ‚à® [t]q, where p and q are atoms, is a formula not mentioning epistemic operators
and so its modal depth is 0. KA KB p, in contrast, has a modal depth of 2. Essentially, the modal
depth simply counts the epistemic modalities in a formula and completely ignores the indices of
these modalities. Not surprisingly, it differs from the i-depth of formulas. For example, the modal
depth of KA p is 1, |KA p|A is 1, but |KA p|B is 2. In contrast, the modal depth of KA KA p is 2, but
its A-depth is 1 and its B-depth is 2, as in the case of KA p.
Suppose œÜ is any objective sentence, possibly a basic action theory. Let us denote the set of
worlds {w | w |= œÜ} as WœÜ . Further, let e1œÜ = WœÜ √ó {{}}. Let ekœÜ = {(w, ek‚àí1
œÜ ) | w ‚àà WœÜ } be defined
inductively. Then,
378

Multiagent Only Knowing in Dynamic Systems

j

Lemma 14 Suppose œÜ is an objective sentence. Suppose w is any world and eœÜ kA and eœÜ B are
j
constructed as above. Then eœÜ kA , eœÜ B , w |= OKnowœÜ [A, k] ‚àß OKnowœÜ [B, j].
Proof: The proof is by an induction on the modal depth of the background theory, which Definition
13 provides. First note that when the modal depth of the background theory is l, then we have a
sentence of the form OKnowœÜ [A, k] ‚àß OKnowœÜ [B, j] such that k ‚â§ l, j ‚â§ l and k or j is l.
Since OKnowœÜ [i, k] is interpreted wrt i‚Äôs epistemic state, we can treat the A-subjective and Bsubjective formulas of the background theory individually. The base case is for theories of modal
depth 1, where we are considering a sentence of the form Oi œÜ. To prove the base case, consider
any world w0 . Clearly w0 'hi w by definition. By construction, (w0 , {}) ‚àà eœÜ 1A iff w0 |= œÜ. Therefore
eœÜ 1A , {}, w |= OA œÜ. Analogously for eœÜ 1B .
Suppose that the lemma holds for background theories of modal depth k ‚àí 1, that is, eœÜ k‚àí1
A
)
be
any
k-structure
in
satisfies OKnowœÜ [A, k ‚àí 1]. This is analogously stated for B. Let (w0 , eœÜ k‚àí1
B
0 |= OKnow [B, k ‚àí 1]. That is,
eœÜ kA . By construction w0 |= œÜ. By induction hypothesis, {}, eœÜ k‚àí1
,
w
œÜ
B
k
k‚àí1
0
k
by construction, (w0 , eœÜ k‚àí1
B ) ‚àà eœÜ A iff {}, eœÜ B , w |= œÜ ‚àß OKnowœÜ [B, k ‚àí 1]. Therefore eœÜ A , {}, w |=
k
OA (œÜ ‚àß OKnowœÜ [B, k ‚àí 1]), that is, eœÜ A , {}, w |= OKnowœÜ [A, k].
Using this lemma, we now consider some properties of (4):
Proposition 15 The following sentences are entailed by the sentence (4), with k > 1 and j > 1.
1. ¬¨KA (num(cardA ) = # 1).
Initially, A does not know the details of her card. (That is, non-beliefs are obtained via
only knowing.)
2. [pickA (cardA )][seeA (cardA )]KA (num(cardA ) = # 1).
After sensing, A knows he has the lowest number.
3. [pickA (cardA )][seeA (cardA )]KB ‚àÉxKA (num(cardA ) = x).
After B observes A reading his card, B knows that A knows what cardA holds for him.
That is, B has de dicto knowledge about A‚Äôs knowledge.
4. [pickA (cardA )][seeA (cardA )]¬¨‚àÉxKB (num(cardA ) = x).
But it is not the case that B knows A‚Äôs card when he observes A sensing. That is, B does
not have de re knowledge about the card.
5. [pickA (cardA )][seeA (cardA )]KA ¬¨‚àÉxKB (num(cardA ) = x).
Moreover, A knows that B does not know her card.
6. [pickA (cardA )][seeA (cardA )][pickB (cardB )][seeB (cardB )]Ki ([challengeB ]LoseA )
where i ‚àà {A, B}.
After sensing, both A and B believe that A would lose the game if challenged by B.
379

Belle & Lakemeyer

3,1

3,2

2,3

2,1

1,3

1,2

3,1

3,2

2,3

2,1

1,3

1,2

3,1

3,2

2,3

2,1

1,3

1,2

X X X X

Figure 2: This depicts the compatibility of worlds after actions, shown wrt a 3-card deck for simplicity‚Äôs sake. Here, worlds are characterized in terms of the numbers on the cards, and
therefore, they are simply labeled (n, m), where n denotes the number on A‚Äôs card and m
denotes the number on B‚Äôs card at the world. The first line represent‚Äôs A‚Äôs uncertainty initially, and the second after A senses # 1, in which case all worlds where she does not have
that card are discarded. The third represents B‚Äôs belief about A‚Äôs epistemic state after he
observes A sensing her card. That is, while he does not know which card A has, he does
know that A considers only two worlds possible, grouped as shown, without knowing
which group represents truth.

j

j

Proof: Let M = (ekA , eB , w) be any model of (4). It is easy to see that ekA and eB would be as in
Lemma 14, and w is any world satisfying Œ• from (4). Below, we let r denote pickA (cardA ) and let
r0 denote seeA (cardA ).
j

1. Assume the contrary. Suppose that ekA , eB , w |= KA (num(cardA ) = # 1). Then for all (w0 , ek‚àí1
B )‚àà
#
k
k
k‚àí1
0
eA , we get eA , eB , w |= (num(cardA ) = 1).
Now, observe that Œ£0 only says that the value of num(cardA ) ‚àà {1, . . . , 52}. Thus, by construction (and the definition of W), there are worlds w‚àó ‚àà WŒ£ where (say) w‚àó |= (num(cardA ) =
#
k
k‚àí1
2), w‚àó 'Ahi w, and (w‚àó , ek‚àí1
B ) ‚àà eA for some eB . This is a contradiction.
2. After A executes r ¬∑ r0 , it follows that only those worlds w0 ‚àà WŒ£ such that w0 [SFA (r0 ), r]
= w[SFA (r0 ), r] = # 1 are considered when evaluating A-subjective formulas. (These are
j
worlds that agree on the number on A‚Äôs card with the real world.) Therefore ekA , eB , w, r ¬∑ r0 |=
k
0 A
k k‚àí1
0
0
KA (num(cardA ) = # 1) since for every (w0 , ek‚àí1
B ) ‚àà eA such that w 'r¬∑r0 w, eA , eB , w , r ¬∑ r |=
#
(num(cardA ) = 1) by the definition of the semantics and the sensing axioms Œ£sense .
380

Multiagent Only Knowing in Dynamic Systems

j‚àí1

j

B w iff w‚àó [SF (r 0 ), r] = w[SF (r 0 ), r]. Since
3. Consider any (w‚àó , eA ) ‚àà eB . We get that w‚àó 'r¬∑r
0
B
B
0
SFB (r) = SFB (r ) = NIL in all worlds satisfying Œ£, for item 3 to hold it must follow that
j
j‚àí1 j
j‚àí1 j
j‚àí1
for every (w0 , eA ) ‚àà eB , eA , eB , w0 , r ¬∑ r0 |= ‚àÉxKA (num(cardA ) = x), and so eA , eB , w0 , r ¬∑
r0 |= KA (num(cardA ) = n) for some n. Using arguments from item 2, this is easily shown
j‚àí1 j
A
0
to be the case. That is, eA , eB , w0 |= KA (num(cardA ) = n) iff for every w00 'r¬∑r
0 w , if
j‚àí2
j‚àí1
j‚àí1 j‚àí2
00
00
00
A
(w , eB ) ‚àà eA , then eA , eB , w |= (num(cardA ) = n). This holds because w 'r¬∑r0 w0
will hold when w00 , r ¬∑ r0 |= (num(cardA ) = n).
j‚àí1

The intuitive argument is as follows. Suppose B only considered j-structures (w, eA ) possible, where w is the real world. Then he would be able to infer cardA ‚Äôs number. But since his
j‚àí1
j‚àí1
epistemic state is {(w0 , eA ), (w00 , eA ), . . .} he believes at each of the worlds w0 that A knows
his number as well as what this is, but he does not know of which of these is the real world.
In effect, there are some structures that inform B that A‚Äôs card is # 1, and there are others that
inform him that A‚Äôs card is a different number, leaving him uncertain. For the case of a 3-card
deck, Figure 2 illustrates this development.
4. This follows from the arguments for the previous item. Basically, for every w0 ‚àà WŒ£ ,
j‚àí1
w0 [SFB (r), r] = NIL = w[SFB (r), r]. When evaluating B-subjective formulas every (w0 , eA ) ‚àà
j
j
eB is considered, including ones where (say) w0 , r¬∑r0 |= (num(cardA ) = # 2). Thus ekA , eB , w, r 6|=
#
KB (num(cardA ) = 1).
k
0 A
5. Consider any (w0 , ek‚àí1
B ) ‚àà eA such that w 'r¬∑r0 w. By the arguments from item 4, it follows
that
0
0
ekA , ek‚àí1
B , w , r ¬∑ r 6|= ‚àÉxKB (num(cardA ) = x).
j

Therefore, ekA , eB , w, r ¬∑ r0 |= KA ¬¨‚àÉxKB (num(cardA ) = x).
6. This property follows from logical deduction. After sensing, both players know their own
cards. A has the lowest number, while B has the highest. Since both agents know that their
numbers are unique, and these numbers are one of {1, . . . , 52}, both infer that A would lose
after being challenged.

4. Regression
A fundamental reasoning task in dynamic domains is projection (Reiter, 2001), where we are to
infer whether Œ± holds after a sequence of actions a1 , . . . , ak is executed:
Œ• |= [a1 ] . . . [ak ]Œ±.
Reiter (2001) developed an important solution to the projection problem in the situation calculus
called regression. The idea is to reduce a query Œ± about the future to a query Œ±0 about the initial
situation by successively replacing fluents in Œ± by the rhs of the successor state axioms until the
resulting sentence Œ±0 contains no more actions. We then need to only verify whether Œ±0 is entailed
by the initial theory.
In the context of multiagent systems, we might, for example, be interested in reasoning about
entailments about a background theory such as (3) that stipulates the beliefs of agents in the application domain:
(3) |= [a1 ] . . . [ak ]œÜ
381

Belle & Lakemeyer

where œÜ perhaps mentions belief operators.
Reiter‚Äôs (2001) results were extended by Scherl and Levesque (2003) to handle knowledge in the
situation calculus, which was further shown to carry over to ES (Lakemeyer & Levesque, 2011). In
this section, we generalize these results for background theories of the form (3) involving multiagent
only knowing operators. We first recap the regression of objective formulas from ES.
4.1 Regressing Objective Formulas
Without any loss of generality, we assume that the query Œ± is syntactically reformulated as follows:
1. quantifiers use distinct variables, and we say such formulas are rectified;
2. formulas are in a certain normal form called NF (defined below).
After applying these transformations, the query becomes amenable to regression. The first syntactic
manipulation is required because of the way regression handles quantifiers, which can lead to incorrect transformations if the variables are not distinct. The second is required for giving a simple
formulation of regression.
Definition 16 A formula Œ± is in NF if every function symbol f in Œ± occurs only in equality expressions of the form f (t1 , . . . , tk ) = t0 , where ti and t0 are either variables or names.
It is immediate to verify that every formula can be rewritten to one in NF, and this transformation
is linear in the size of the formula. For instance, f (g(x)) = f 0 (x) is equivalent to ‚àÉy, u. f (y) =
u ‚àß f 0 (x) = u ‚àß g(x) = y. Further, by this definition, if a term t appears either as an argument for a
function or as an action operator [t], then it follows that it is either an (action) name or a variable.
In the following we will use œÉ to denote sequences that consist of action variables or action names.
Lakemeyer and Levesque (2011) define the regression operator R, which is applicable to any
bounded objective formula.12 If such a formula is not rectified or not in NF, it is transformed to a
formula satisfying these conditions.
Definition 17 Define R[Œ±], the regression of a bounded basic formula Œ± wrt Œ•, to be the fluent
formula R[hi, Œ±]. For any sequence of action names or variables œÉ, R[œÉ, Œ±] is defined inductively:
1. R[œÉ, t1 = t2 ] = (t1 = t2 ) if t1 and t2 do not mention functional fluents;
2. R[œÉ, ‚àÄxŒ±] = ‚àÄxR[œÉ, Œ±];
3. R[œÉ, Œ± ‚à® Œ≤] = R[œÉ, Œ±] ‚à® R[œÉ, Œ≤];
4. R[œÉ, ¬¨Œ±] = ¬¨R[œÉ, Œ±];
5. R[œÉ, [t]Œ±] = R[œÉ ¬∑ t, Œ±];
6. R[œÉ, Poss(t)] = R[œÉ, œÄat ];
12. Roughly speaking, these correspond to the class of formulas that Reiter (2001) deems regressable in the situation
calculus.

382

Multiagent Only Knowing in Dynamic Systems

7. R[œÉ, G(t1 , . . . , tk )] = G(t1 , . . . , Gk ) for rigid predicate G;
8. R[œÉ, F(t1 , . . . , tk )] for fluent predicate F is defined inductively on œÉ:
(a) R[hi, F(t1 , . . . , tk )] = F(t1 , . . . , tk );
(b) R[œÉ ¬∑ t, F(t1 , . . . , tk )] = R[œÉ, Œ≥F ta tx1 1......tkxk ];
9. R[œÉ, f (t1 , . . . , tk ) = t0 ] for fluent function f is defined inductively by:
(a) R[hi, f (t1 , . . . , tk ) = t0 ] = ( f (t1 , . . . , tk ) = t0 );
(b) R[œÉ ¬∑ t, f (t1 , . . . , tk ) = t0 ] = R[œÉ, ‚àÉy. (Œ≥ f )at tx11 ...... txk k ‚àß y = t0 ].
Note that this definition includes œÄ, Œ≥F and Œ≥ f which are the rhs of the precondition axiom and the
successor state axioms from Œ•.
The main result regarding Definition 17 is that the evaluation of objective bounded sentences
reduces to a query about the initial theory.
Theorem 18 (Lakemeyer & Levesque, 2011) Let Œ• be a basic action theory, whose initial theory is
Œ•0 , and let Œ± be any objective bounded sentence. Then R[Œ±] is a fluent sentence and satisfies:
Œ• |= Œ± iff Œ•0 |= R[Œ±].
4.2 Regressing Multiagent Beliefs
Let us now consider the more general case of regression for bounded sentences mentioning belief
operators. This first needs the equivalent of a successor state axiom for knowledge, which will tell
us how knowledge can be regressed wrt an action. The following theorem generalizes a similar
result by Lakemeyer and Levesque (2004) to the many agent case.
Theorem 19 (Successor State Axiom for Knowledge.)
|= [a]Ki (Œ±) ‚â°
‚àÉx. SFi (a) = x ‚àß Ki (SFi (a) = x ‚äÉ [a]Œ±).
Proof: Let i be A, with the other case being symmetric. For the only-if direction, suppose that
j
j
ekA , eB , w, z |= [r]KA Œ±ar for an action name r. Abbreviate Œ±ar as Œ±0 . Suppose that ekA , eB , w, z |=
j
SFA (r) = n. It then suffices to show that ekA , eB , w, z |= KA (SFA (r) = n ‚äÉ [r]Œ±0 ).
k
0
0 A
So suppose (w0 , ek‚àí1
B ) ‚àà eA and w [SFA (r), z] = n. Since w 'z¬∑r w, it follows by assumption that
0
0
k k‚àí1
0
0
k k‚àí1
0
0
ekA , ek‚àí1
B , w , z ¬∑ r |= Œ± , i.e. eA , eB , w , z |= [r]Œ± . Thus eA , eB , w , z |= SFA (r) = n ‚äÉ [r]Œ± , and it
j
follows then that ekA , eB , w0 , z |= KA (SFA (r) = n ‚äÉ [r]Œ±0 ).
j

Conversely, suppose that ekA , eB , w, z |= SFA (r) = n ‚àß KA (SFA (r) = n ‚äÉ [r]Œ±0 ). We now need to
j
k
0 A
k k‚àí1
0
0
show that ekA , eB , w, z |= [r]KA (Œ±0 ), i.e. for all (w0 , ek‚àí1
B ) ‚àà eA such that w 'z w, eA , eB , w , z¬∑r |= Œ± .
k
k‚àí1
Suppose w0 'Az¬∑r w, i.e. w0 [SFA (r), z] = n and (w0 , ek‚àí1
B ) ‚àà eA for some eB . Then by assumption,
j
0
0
k k‚àí1
0
k
0
ekA , ek‚àí1
B , w , z ¬∑ r |= Œ± . So eA , eB , w , z |= [r]Œ±, from which we get eA , eB , w, z |= KA ([r]Œ± ).

383

Belle & Lakemeyer

This theorem essentially says that knowledge after an action depends on what was known before,
and what the future would look like contingent on the sensing result. Note that this theorem is not a
stipulation of the action theory (Scherl & Levesque, 2003), but a theorem of the logic.
We mentioned earlier that in the case of (3), we need three basic action theories Œ•, Œ£ and Œ£ 0 . The
idea behind regression is to transform objective formulas wrt Œ•, while subjective ones are regressed
wrt Œ£ and Œ£ 0 . Consequently, R is defined wrt Œ•, Œ£ and Œ£ 0 . More precisely, we define a regression
operator R[Œ•, Œ£, Œ£ 0 , œÉ, Œ±] wrt a basic action theory Œ• for what is true in the real world, a basic
action theory Œ£ for what A believes at all levels, and a basic action theory Œ£ 0 for what B believes at
all levels, as expected by (3).
Definition 20 We define R[Œ•, Œ£, Œ£ 0 , Œ±], the regression of a bounded basic formula Œ± wrt Œ•, Œ£
and Œ£ 0 , to be R[Œ•, Œ£, Œ£ 0 , hi, Œ±]. For a given sequence of action names or variables œÉ, we define
R[Œ•, Œ£, Œ£ 0 , œÉ, Œ±] inductively by:
1.-9. See Definition 17. (Note that this definition uses the rhs of the precondition axiom and the
successor state axioms from Œ•.)
10. R[Œ•, Œ£, Œ£ 0 , œÉ, SFA (t) = t0 ] = R[Œ•, Œ£, Œ£ 0 , œÉ, œïA at tx0 ] which uses the rhs of the sensing axioms
from Œ£;
11. R[Œ•, Œ£, Œ£ 0 , œÉ, SFB (t) = t0 ] = R[Œ•, Œ£, Œ£ 0 , œÉ, œïB at tx0 ] which uses the rhs of the sensing axioms
from Œ£ 0 ;
12. R[Œ•, Œ£, Œ£ 0 , œÉ, KA Œ±] is defined inductively on œÉ by:
(a) R[Œ•, Œ£, Œ£ 0 , hi, KA Œ±] = KA (R[Œ£, Œ£, Œ£, hi, Œ±]);
(b) R[Œ•, Œ£, Œ£ 0 , œÉ¬∑t, KA Œ±] = R[Œ•, Œ£, Œ£ 0 , œÉ, Œ≤at ], where Œ≤ is rhs of the equivalence in Theorem
19 for the agent index A.
13. R[Œ•, Œ£, Œ£ 0 , œÉ, KB Œ±] is defined inductively on œÉ by:
(a) R[Œ•, Œ£, Œ£ 0 , hi, KB Œ±] = KB (R[Œ£ 0 , Œ£ 0 , Œ£ 0 , hi, Œ±]);
(b) R[Œ•, Œ£, Œ£ 0 , œÉ¬∑t, KB Œ±] = R[Œ•, Œ£, Œ£ 0 , œÉ, Œ≤at ], where Œ≤ is rhs of the equivalence in Theorem
19 for the agent index B.
The regression operator in the multiagent case works as follows. At the initial situation, regressing
KA Œ± is equivalent to regressing Œ± wrt the basic action theory Œ£ that A believes at all levels. Similarly, at the initial situation, regressing KB Œ± is equivalent to regressing Œ± wrt the basic action theory
Œ£ 0 that B believes at all levels. More generally, if we are regressing Ki Œ± wrt an action sequence
œÉ ¬∑ t, then this is equivalent to regressing the rhs of Theorem 19 wrt œÉ by first substituting t.
For simplicity, we often write R[œÉ, Œ±] instead of R[Œ•, Œ£, Œ£ 0 , œÉ, Œ±]. We are now ready to prove
the main result of this section for bounded basic sentences:13
13. Roughly speaking, these correspond to the class of regressable formulas in the epistemic situation calculus (Scherl
& Levesque, 2003).

384

Multiagent Only Knowing in Dynamic Systems

Theorem 21 Suppose Œ± is a bounded basic sentence of maximal A, B-depth k, j. Let Œ•, Œ£ and Œ£ 0 be
basic action theories. Then R[hi, Œ±] is a static sentence and satisfies:
Œ• ‚àß œà |= Œ± iff Œ•0 ‚àß œà0 |= R[hi, Œ±]
where œà = OKnowŒ£ [A, k] ‚àß OKnowŒ£ 0 [B, j]
œà0 = OKnowŒ£0 [A, k] ‚àß OKnowŒ£0 0 [B, j].
That is, we solve projection which is the task of verifying whether Œ± is entailed by regressing Œ±
and verifying that is an entailment of the conjunction of what is true initially and each agent only
knowing their initial beliefs. The proof for this theorem is provided in Appendix A.
Readers will have noticed that the theorem assumes a background theory where A has beliefs to
level k and B has beliefs to level j, given a query whose maximal A, B-depth is k, j. This syntactic
restriction is essential for our relatively simple regression operator to be well-defined. To see that,
suppose we are interested in verifying whether KA KB [r]Œ± is entailed by OA (Œ£), where Œ£ is a basic
action theory. By the definition of the regression operator given above, evaluating the query reduces
to regressing [r]Œ± wrt Œ£, but this is not a correct transformation because A does not have any beliefs
about B‚Äôs knowledge of the world. In fact, the formula KA KB [r]Œ± does not seem amenable to
regression wrt OA (Œ£) since it is simply not clear how one should regress the subformula KB [r]Œ±.
But now note that the formula KA KB [r]Œ± is of depth 2 and that the transformation is indeed correct
wrt initial knowledge for A of at least depth 2, such as OA (Œ£ ‚àß OB Œ£).
Readers will also notice that we are restricting the regression operator to bounded basic sentences. There are at least two reasons for this limitation. First, note that the language is not expressive enough to refer to only knowing in non-initial situations; if an agent only knows a basic action
theory, one presumes that after an action the agent only knows another basic action theory. Regressing the latter should intuitively lead to a sentence that talks about what was only known before the
action was executed, and this currently cannot be expressed in the language. Second, note that a
basic action theory contains sentences such as the successor state axioms which are not bounded.
So, if after an action we are left with a formula of the form Oi (Œ±), where Œ± by the above argument
would contain sentences that are not bounded, then this Œ± would not be regressable. This is because
Theorem 18 is limited to regressing bounded formulas. Nevertheless, the regression operator covers
the same class of formulas as considered by Scherl and Levesque (2003), and is sufficient for most
practical purposes.
Example 22 We illustrate regression using the card game. Suppose we are interested in checking
whether (4) from Section 3 entails the following sentence:
[pickA (cardA )][seeA (cardA )](KA (num(cardA ) = # 1) ‚àß ¬¨KB (num(cardA ) = # 1)).

(5)

That is, after A picks her card and senses, she knows of her own card while B does not learn this.
Use r for pickA (cardA ), r0 for seeA (cardA ) and Œ± for (num(cardA ) = # 1). Begin with
R[Œ•, Œ£, Œ£, r ¬∑ r0 , KA Œ± ‚àß ¬¨KB Œ±]
= R[Œ•, Œ£, Œ£, r ¬∑ r0 , KA Œ±] ‚àß R[Œ•, Œ£, Œ£, r ¬∑ r0 , ¬¨KB Œ±]
385

Belle & Lakemeyer

Consider [r][r0 ]KA Œ±. We have:
R[Œ•, Œ£, Œ£, r ¬∑ r0 , KA Œ±]
= R[Œ•, Œ£, Œ£, r, ‚àÉx(SFA (r0 ) = x ‚àß KA (SFA (r0 ) = x ‚äÉ [r0 ]Œ±))]
= ‚àÉx. R[Œ•, Œ£, Œ£, r, SFA (r0 ) = x] ‚àß R[Œ•, Œ£, Œ£, r, KA (SFA (r0 ) = x ‚äÉ [r0 ]Œ±)]
= ‚àÉx. R[Œ•, Œ£, Œ£, r, num(cardA ) = x] ‚àß
R[Œ•, Œ£, Œ£, hi, ‚àÉy. SFA (r) = y ‚àß KA (SFA (r) = y ‚äÉ [r]Œ≤)]
= ‚àÉx. num(cardA ) = x ‚àß ‚àÉy. y = NIL ‚àß KA (R[Œ£, Œ£, Œ£, hi, SFA (r) = y ‚äÉ [r]Œ≤])
= ‚àÉx. num(cardA ) = x ‚àß ‚àÉy. y = NIL ‚àß
KA (y = NIL ‚äÉ (num(cardA ) = x ‚äÉ num(cardA ) = # 1))
Here, Œ≤ denotes (SFA (r0 ) = x ‚äÉ [r0 ]Œ±).
These reductions mostly involve repeated applications of the knowledge successor state axiom,
over logical connectives. Then, in step 4, when regressing knowledge wrt hi, we replace all basic
action theories in the R operator with the one that A believes, as expected by Rule 12(a) of Definition
20. Regarding R‚Äôs result, since Œ•0 contains num(cardA ) = # 1, it is not hard to see that
Œ•0 ‚àß OKnowŒ£0 [A, k] ‚àß OKnowŒ£0 [B, j] |= R[Œ•, Œ£, Œ£, r ¬∑ r0 , KA Œ±].
Simplifying R[Œ•, Œ£, Œ£, r ¬∑ r0 , ¬¨KB Œ±] is analogous, the only dissimilarity arising from regressing B‚Äôs
beliefs wrt r0 , which obtains the sensing result NIL:
R[Œ•, Œ£, Œ£, r ¬∑ r0 , ¬¨KB Œ±]
= ¬¨R[Œ•, Œ£, Œ£, r ¬∑ r0 , KB Œ±]
= ¬¨[‚àÉx. x = NIL ‚àß ‚àÉy. y = NIL ‚àß KB (y = NIL ‚äÉ (x = NIL ‚äÉ num(cardA ) = # 1))].
One may verify that
Œ•0 ‚àß OKnowŒ£0 [A, k] ‚àß OKnowŒ£0 [B, j] |= ¬¨R[Œ•, Œ£, Œ£, r ¬∑ r0 , KB Œ±].
Therefore,
Œ•0 ‚àß OKnowŒ£0 [A, k] ‚àß OKnowŒ£0 [B, j] |= R[Œ•, Œ£, Œ£, r ¬∑ r0 , KA Œ± ‚àß ¬¨KB Œ±].
By means of Theorem 21, this allows us to conclude that the query (5) is indeed entailed by (4).
Example 23 We will regress nested beliefs in this example. Suppose we are interested in checking
whether (4) from Section 3 entails the following sentence:
[pickA (cardA )][seeA (cardA )]KA ¬¨KB (num(cardA ) = # 1).
386

(6)

Multiagent Only Knowing in Dynamic Systems

As we have done so above, let r denote pickA (cardA ), r0 denote seeA (cardA ), and Œ± denote num(cardA ) =
1. Then:

#

R[Œ•, Œ£, Œ£, r ¬∑ r0 , KA ¬¨KB Œ±]
= R[Œ•, Œ£, Œ£, r, ‚àÉx(SFA (r0 ) = x ‚àß KA (SFA (r0 ) = x ‚äÉ [r0 ]¬¨KB Œ±))]
= ‚àÉx. R[Œ•, Œ£, Œ£, r, num(cardA ) = x] ‚àß R[Œ•, Œ£, Œ£, r, KA (x = num(cardA ) ‚äÉ [r0 ]¬¨KB Œ±)]
= ‚àÉx. num(cardA ) = x ‚àß
R[Œ•, Œ£, Œ£, hi, ‚àÉy(SFA (r) = y ‚àß KA (SFA (r) = y ‚äÉ [r]Œ≤))]
= ‚àÉx. num(cardA ) = x ‚àß ‚àÉy. y = NIL ‚àß KA (R[Œ£, Œ£, Œ£, hi, y = NIL ‚äÉ [r]Œ≤]).
Here, Œ≤ denotes (num(cardA ) = x ‚äÉ [r0 ]¬¨KB Œ±).
The above reduction leads to:
‚àÉx. num(cardA ) = x ‚àß ‚àÉy. y = NIL ‚àß KA (y = NIL ‚äÉ R[Œ£, Œ£, Œ£, hi, [r]Œ≤]).

(7)

Let us consider R[Œ£, Œ£, Œ£, hi, [r]Œ≤]. We have (on simplification):
R[Œ£, Œ£, Œ£, hi, [r]Œ≤]
= (num(cardA ) = x) ‚äÉ R[Œ£, Œ£, Œ£, hi, [r][r0 ]¬¨KB Œ±].
Following the reduction of R[Œ£, Œ£, Œ£, hi, [r][r0 ]¬¨KB Œ±] as done in the previous example, it can be
shown that
Œ•0 ‚àß OKnowŒ£0 [A, k] ‚àß OKnowŒ£0 [B, j] |= (7).
Using the regression property, that is, Theorem 21, we conclude that (6) is entailed by (4). Therefore, we are done. (We reiterate that A has knowledge about B‚Äôs non-beliefs because A has beliefs
about what B only knows.)
Analogously, other entailments from Proposition 15 can be verified using regression.
While regression allows us to reduce questions about knowledge and action to queries about
initial beliefs, in the next section we go even further and replace reasoning about knowledge by
classical first-order reasoning.
4.3 A Representation Theorem
The representation theorem is a result by means of which reasoning about knowledge is reduced to
first-order theorem proving. The presentation below generalizes the single agent variant (Lakemeyer
& Levesque, 2004).
The basic idea is to substitute believed sentences with their instances. For example, suppose all
that i believes is the sentence œÜ:
œÜ = {Smaller(n2 , n1 ), Smaller(n3 , n1 ) ‚à® Smaller(n4 , n1 )}.
That is, in a blocks world domain: n2 is smaller than n1 , and n3 is smaller than n1 or n4 is smaller
than n1 . Supposing we ask:
Ki ‚àÉx. (Smaller(x, n1 ) ‚àß ¬¨Ki Smaller(x, n1 ))
387

(8)

Belle & Lakemeyer

That is, does œÜ know of a block that is smaller than n1 , but does not know which one? The answer
is certainly yes because the list of smaller blocks known is incomplete, except for n2 . The essential
step is to replace Ki Smaller(x, n1 ) with x = n2 . Then, it can be shown that the query reduces to
verifying if ‚àÉx. (Smaller(x, n1 ) ‚àß x , n2 ) is entailed by œÜ.
To make this intuition precise, we first define a procedure Res[Œ±, œÜ], introduced originally by
Levesque (1990), to obtain the known instances of Œ± that are entailed by œÜ, where both Œ± and œÜ are
fluent formulas. When Œ± does not mention free variables, then Res checks whether œÜ entails the
sentence Œ±.
Definition 24 Let Œ± be a fluent formula, and œÜ is a fluent sentence. Let n1 , . . . , nk be all the names
occurring in œÜ and Œ± and n0 is a name not occurring in œÜ or Œ±. Then, Res[Œ±, œÜ] is defined as:
1. If Œ± has no free variables, then Res[Œ±, œÜ] is TRUE if œÜ |= Œ± and FALSE otherwise.
2. If x is a free variable in Œ±, then Res[Œ±, œÜ] is defined as:
((x = n1 ) ‚àß Res[Œ±nx1 , œÜ]) ‚à®
... ‚à®
((x = nk ) ‚àß Res[Œ±nxk , œÜ]) ‚à®
0

((x , n1 ) ‚àß . . . ‚àß (x , nk ) ‚àß Res[Œ±nx0 , œÜ]nx ).
For instance, if Œ± was Smaller(x, n1 ) and œÜ is as above, then Res[Œ±, œÜ] would simplify to:
(x = n2 ) ‚àß Res[Œ±nx2 , œÜ]
where, further, Res[Œ±nx2 , œÜ] is TRUE because œÜ |= Smaller(n2 , n1 ).
Given any formula Ki Œ± and a œÜ that is only known by i, the idea is to reason about knowledge
by utilizing Res. Of course, as discussed earlier, in the multiagent case we have to account for
knowledge bases at the different levels, that is, by addressing background theories of the form (3).
So we proceed as follows. Let œÜ and œÜ0 denote the initial theories (fluent sentences) believed by A
and B at all levels respectively. Then, given any bounded basic sentence, we first use regression to
obtain a static basic sentence. For any static basic Œ±, define kŒ±kœÜ,œÜ0 as follows:
Definition 25 Let œÜ and œÜ0 be fluent sentences, and Œ± and Œ≤ be static basic sentences. Then we
define the fluent sentence kŒ±kœÜ,œÜ0 by:
1. kŒ±kœÜ,œÜ0 = Œ± if Œ± is objective;
2. k¬¨Œ±kœÜ,œÜ0 = ¬¨kŒ±kœÜ,œÜ0 ;
3. kŒ± ‚à® Œ≤kœÜ,œÜ0 = kŒ±kœÜ,œÜ0 ‚à® kŒ≤kœÜ,œÜ0 ;
4. k‚àÄxŒ±kœÜ,œÜ0 = ‚àÄxkŒ±kœÜ,œÜ0 ;
5. kKA Œ±kœÜ,œÜ0 = Res[kŒ±kœÜ,œÜ , œÜ];
6. kKB Œ±kœÜ,œÜ0 = Res[kŒ±kœÜ0 ,œÜ0 , œÜ0 ].
388

Multiagent Only Knowing in Dynamic Systems

Intuitively, given an objective KB œÜ that A believes at all levels and an objective KB œÜ0 that B
believes at all levels, a conceptually simple reduction operator can be obtained. The reader may
notice some similarity to the regression operator, viz. whenever KA Œ± is encountered then the reduction is continued wrt the KB œÜ. Analogously, the reduction is continued wrt œÜ0 whenever KB Œ± is
encountered.
We now present the main result for this section, by relating R and k ¬∑ kœÜ,œÜ0 :
Theorem 26 Let Œ•, Œ£ and Œ£ 0 be basic action theories. Suppose Œ± is a basic bounded sentence of
maximal A, B-depth k, j, then
Œ• ‚àß œà |= Œ± iff

|= Œ•0 ‚äÉ kR[hi, Œ±]kŒ£0 ,Œ£0 0 .

where œà = OKnowŒ£ [A, k] ‚àß OKnowŒ£ 0 [B, j].
That is, a query Œ± perhaps with action operators is entailed by the background theory iff the regressed
query reduced by the representation theorem wrt Œ£0 and Œ£0 0 is entailed by the set of sentences that
are true initially. Thus, no modal reasoning is necessary. The proof for this theorem appears in
Appendix B.
Example 27 Let us consider a projection query from Example 22. Consider, for example, the
question whether (4) from Section 3 entails:
[pickA (cardA )][seeA (cardA )]KA (num(cardA ) = # 1).

(9)

By means of Theorem 26, we get that |= (4) ‚äÉ (9) iff
|= {num(cardA ) = # 1, num(cardB ) = # 52, Œ£0 } ‚äÉ kR[Œ•, Œ£, Œ£, hi, (9)]kŒ£0 ,Œ£0

(10)

So for (10) to be true, first consider that R[Œ•, Œ£, Œ£, hi, (9)] simplifies to
‚àÉx. x = num(cardA ) ‚àß ‚àÉy. y = NIL ‚àß KA Œ±

(11)

where Œ± is
y = NIL ‚äÉ (num(cardA ) = x ‚äÉ num(cardA ) = # 1).
Next, observe that k(11)kŒ£0 ,Œ£0 yields
‚àÉx. x = num(cardA ) ‚àß ‚àÉy. y = NIL ‚àß Res[kŒ±kŒ£0 ,Œ£0 , Œ£0 ].

(12)

We then note that Res[kŒ±kŒ£0 ,Œ£0 , Œ£0 ] reduces to
x = # 1 ‚àß y = NIL.

(13)

The reduction is as follows. Res[kŒ±kŒ£0 ,Œ£0 , Œ£0 ] = Res[Œ±, Œ£0 ] because Œ± is objective. Now, Res[Œ±, Œ£0 ]
has 2 free variables: x and y. By the definition of Res, all possible substitutions n and m are chosen
xy
for x and y respectively from the names in Œ£0 ‚à™ {Œ±}, to check whether Res[Œ±n m , Œ£0 ] is true. But that
389

Belle & Lakemeyer

is the case only for substitutions # 1 and NIL for x and y respectively. Therefore, Res[Œ±, Œ£0 ] yields
(13). Replacing (13) in (12), we get:
‚àÉx[x = num(cardA ) ‚àß ‚àÉy[y = NIL ‚àß (x = # 1 ‚àß y = NIL)]].

(14)

Thus, from (10), we ask is it true that the following first-order formula is valid:
{num(cardA ) = # 1, num(cardB ) = # 52, Œ£0 } ‚äÉ (14).
The answer is clearly yes, and therefore, |= (4) ‚äÉ (9).
So standard first-order theorem proving can be employed for reasoning about multiagent systems in
ESn . There is a caveat, however. Unlike standard theorem proving, the set of basic bounded formulas that follow from a basic action theory by applying the representation theorem is not recursively
enumerable (Rogers Jr., 1987). More precisely, in item 1 of Res‚Äôs definition, note that we appeal
to validity, when returning TRUE, and appeal to falsifiability, when returning FALSE (Levesque &
Lakemeyer, 2001).
4.4 Discussions
Before wrapping up the section, let us reflect on the limitations of the regression property and the
representation theorem. Clearly, they represent a very special case, one of the form (say, for theories
of depth 2):
OA (œÜ ‚àß OB œÜ) ‚àß OB (œà ‚àß OA œà)
where if an agent only knows œÜ, she also believes other agents hold the same beliefs. It is not hard
to generalize both the results to cases of the form:
OA (œÜ ‚àß OB œÜ0 ) ‚àß OB (œà ‚àß OA œà0 )
where, œÜ, œÜ0 , œà and œà0 may differ arbitrarily. The idea, not surprisingly, is to relate the depth of
the formula to the sentence believed by the agent at a corresponding depth. For example, if p is
an atom, one would evaluate [r]KA p wrt œÜ and [r]KB p wrt œà. At the next level, given a formula
[r]KA ([r0 ]KB p), we would evaluate [r0 ]KB p wrt œÜ0 , and given the resultant formula Œ±, we would
evaluate [r]KA Œ± wrt œÜ. More precisely, the regression operator and the representation theorem will
now be defined in terms of the sentences true in the real world, as well as the ones believed: œÜ, œÜ0 , œà
and œà0 . (That is, instead of three theories wrt which R is defined for knowledge bases of arbitrary
depths, as in Section 4, we would have at most 5 theories for knowledge bases of depth 2.) Using
the techniques presented here, it is then not hard to show that, yet again, we would obtain a property
analogous to Theorem 26, where no modal reasoning will be necessary. Because we allow beliefs
to differ arbitrarily after actions, we think expecting an initial specification that makes assumptions
about what agents only know is reasonable. (Note also that œÜ and œÜ0 can be any first-order theory,
that is, no complete knowledge assumption is made here or anywhere else in this paper.) Therefore,
this more general setting would cover a broad range of application domains. In return, only a slightly
involved definition for R and k ¬∑ k is needed.
However, there may be domains that make a case for still other kinds of initial states, such as
OA (œÜ ‚àß (KB œà ‚à® KB œà0 )) ‚àß KB (œà ‚àß KA (¬¨œÜ))
390

Multiagent Only Knowing in Dynamic Systems

is one where A only knows that B knows œà or B knows œà0 . It is also an example where the modeler
has not given a full characterization of B‚Äôs knowledge base. Appealing to the underlying semantics
to reason about properties of knowledge in such examples is well-defined, of course, since we are
simply checking the validity of well-formed formulas in the logic. As far as the effectiveness of
reasoning is concerned, note that most significantly, regression of basic bounded formulas is not
limited to the nature of the initial theory. So this aspect is not the problem. But we have very
little to say about the reduction of knowledge operators in these cases. Indeed, while a version of
Theorem 21 is provable, where after regression, one would replace basic action theories by their
initial components only, Theorem 26 need not hold. Thus, in such cases, modal reasoning will
perhaps be necessary.

5. Related Work
This article focused on only knowing and knowledge in a multiagent dynamic setting. In particular,
the modal dialect ES of the situation calculus, along with associated reduction theorems, were
generalized to the many agent case.
The underlying language of the situation calculus has received a lot of attention in the action
community. There are, of course, alternate formalisms, such as the fluent calculus (Thielscher,
1999) and other closely related approaches, such as those based on dynamic logics (Gerbrandy &
Groeneveld, 1997; Demolombe, 2003; Demolombe, Herzig, & Varzinczak, 2003; Van Ditmarsch,
Herzig, & De Lima, 2007). In particular, the action modality of ES, which we inherit here, is taken
from dynamic logic. However, there are significant differences. For example, Van Ditmarsch et al.
(2007) consider an epistemic extension to dynamic logic with a regression property, but they are
propositional, and do not consider only knowing. Demolombe (2003), on the other hand, considers
a form of only knowing, but his work, like the original ES, is restricted to the single agent case.
Also, there is no notion of regression. For more details on how the single agent version of ES is related to various other action proposals, see the discussion by Lakemeyer and Levesque (2004). It is
worth mentioning that the situation calculus itself has been previously extended to deal with multiple agents (Shapiro et al., 2002). Recently, in fact, Kelly and Pearce (2008) formulate the evaluation
of epistemic queries, including queries about common knowledge (Fagin et al., 1995), by means
of a meta-level operator using regression. In contrast to these ideas, we are mainly concerned with
identifying how regression works in the presence of multiagent only knowing operators. As we have
argued earlier, by being able to define initial knowledge in terms of what is only known one obtains
a natural means of reasoning about both beliefs and non-beliefs. Moreover, the epistemic situation
calculus of Scherl and Levesque does not have an equivalent of the representation theorem. Therefore, the other approaches would require a form of modal reasoning about the initial situation.14 In
other aspects, moreover, the approaches are not comparable. On the one hand, in contrast to Kelly
and Pearce we observed in Section 2.2 that common knowledge cannot be captured with our semantics. On the other hand, integrating only knowing in the situation calculus when situation terms
are explicit, as in the above derivatives of the Scherl-Levesque scheme, is problematic (Lakemeyer,
1996; Lakemeyer & Levesque, 1998, 2004).
14. Special cases for the reduction of knowledge are treated, for example, by Reiter (2001) and Lakemeyer and
LespeÃÅrance (2012).

391

Belle & Lakemeyer

We note that reasoning about knowledge in multiagent systems is an important area in artificial intelligence, and numerous formal systems have been studied (Fagin et al., 1995; Wooldridge,
2009). For example, properties similar to ones obtained for the card game studied in this article are
also considered in the work of Van Ditmarsch (2002). However, many of these systems are propositional. Most significantly, only knowing, and the feature that appropriate beliefs are entailments
of a knowledge base that is only known, are not addressed.
As we pointed out, Levesque (1990) was among the first to propose a notion of only knowing in
the logic OL, but there are a number of related notions (Halpern & Moses, 1984; Hoek & Thijsse,
2002; Pratt-Hartmann, 2000). We do not discuss them here, nor their relationships to OL, which
is treated elsewhere (Rosati, 2000; Halpern & Lakemeyer, 2001; Levesque & Lakemeyer, 2001).
Readers are also referred to the work of Levesque and Lakemeyer (2001) for a more comprehensive
study on OL; for example, it is shown that the compactness property does not hold for the objective
fragment of OL. Halpern and Pass (2009) consider a related probabilistic variant of only knowing
for studying certain kinds of strategies in game theory.
Since Levesque‚Äôs proposal, generalizations to the many agent case has been attempted in a number of papers (Lakemeyer, 1993; Halpern, 1993; Halpern & Lakemeyer, 2001; Waaler & Solhaug,
2005). But as we point out in earlier work (Belle & Lakemeyer, 2010a), these approaches have
undesirable features. Our k-structures approach (Belle & Lakemeyer, 2010a), on the other hand,
was shown to satisfactorily capture multiagent only knowing. In that work, we also discuss a number of other aspects of multiagent only knowing, including, for example, a sound and complete
axiomatization for the propositional case.15
Finally, we remark that the intuition of k-structures seems closely related to the proposal of
knowledge structures (Fagin, Halpern, & Vardi, 1991). Although restricted to a propositional language, and although actions and only knowing are not considered, the proposal is also based on
epistemic states at various depths. (See Kaneko and Suzuki, 2003, for similar semantical notions in
game theory.) For an agent, a 1-world is simply a set of truth assignments to primitive propositions.
This roughly corresponds to a set of worlds, similar to a 1-structure. However, a 2-world considers
the triple: truth assignment to primitive propositions, a 1-world for A, and a 1-world for B. So this
differs from our proposal slightly. They also expect k-worlds to satisfy various constraints, including one about knowledge always being correct. Despite these differences, they are also motivated
by the ease of capturing non-beliefs, and so an investigation on the correspondences between the
two approaches is perhaps worthy of study.

6. Conclusions
This work considered reasoning about only knowing with many agents in dynamic domains. The
language introduced is a first-order formalism that allows us to reason about knowledge, only knowing, actions and sensing. Only knowing has distinctive advantages from the view of a knowledgebased agent where it is possible to specify the sentences that precisely characterize a knowledge
base, and then logically infer corresponding beliefs and non-beliefs (with quantifying-in) from that
characterization. Building on previous work on multiagent only knowing (Belle & Lakemeyer,
2010a) and a modal fragment of the situation calculus (Lakemeyer & Levesque, 2004), a semantical
15. OL‚Äôs axiomatization for the first-order language (Levesque, 1990) was shown to be incomplete by Halpern and
Lakemeyer (1995); they also show that any complete axiomatization cannot be recursive.

392

Multiagent Only Knowing in Dynamic Systems

account was first discussed. We showed that knowledge has appropriate properties, despite the semantics slightly deviating from the usual Kripke-structure account. We then considered the notion
of a basic action theory, and explored projection tasks in terms of a simple card game. In particular,
non-trivial knowledge change mechanisms after sensing was demonstrated in the formalism.
One important methodology for reasoning about actions in the literature is regression, extensively used in planning methodologies (Fritz, 2009), and we proved a version of regression for the
formalism. From this, reasoning about actions and knowledge reduces to reasoning about knowledge in the initial state only. Next, we generalized the representation theorem (Levesque & Lakemeyer, 2001) to further reduce reasoning about knowledge in the initial state to first-order reasoning.
Thus, no modal reasoning is necessary. We believe these results together with the underlying logic
enhance the current paradigms for the logical modeling of intelligent agents (Fagin et al., 1995;
Wooldridge, 2009), especially in the sense of formal specifications for knowledge-based systems.
There are many avenues for future work. An important observation on OL by Levesque (1990)
is that when the knowledge base includes beliefs about itself, a certain flavor of nonmonotonicity is
exhibited. In fact, the beliefs that logically follow can be related in a precise way to the fixed-point
definition of autoepistemic logic (Moore, 1985). We have previously shown (Belle & Lakemeyer,
2010a) that these notions generalize to the multiagent case as well, which can be used for multiagent
autoepistemic reasoning. For example, if Fred tells Sara that he recently bought a bird, Fred might
come to assume that Sara believes then that the bird flies, without him explicitly suggesting such a
fact. ESn , of course, would further allow these notions to be studied in a dynamic setting (Kakas,
Michael, & Miller, 2008; Lakemeyer & Levesque, 2009).
For long-lived agents, regression would become infeasible after millions of actions, and so we
would need to periodically update the knowledge base, which is referred to as progression (Lin &
Reiter, 1997). STRIPS technology, for instance, is a simple form of progression (Reiter, 2001).
Recently, the computational methodology of progression has been studied in the context of only
knowing (Lakemeyer & Levesque, 2009). The idea, roughly, is that if the agent only knows a basic
action theory Œ£0 ‚à™ Œ£pre ‚à™ Œ£post ‚à™ Œ£sense , then after an action, the agent only knows another basic
action theory Œ£0 0 ‚à™ Œ£pre ‚à™ Œ£post ‚à™ Œ£sense , where Œ£0 0 is the progression of Œ£0 . Here, only knowing
characterizes the knowledge base in a precise way after doing actions. Our account might suggest
ways to study these notions in a multiagent setting, where after actions, an agent would update not
only her beliefs about the world but would also update her beliefs about what other agents know.
Finally, extensions for probabilistic nondeterminism (Gabaldon & Lakemeyer, 2007; Belle &
Lakemeyer, 2011) and the development of strategies and coalitions between agents (Alur, Henzinger, & Kupferman, 2002; Giacomo, LespeÃÅrance, & Pearce, 2010) are worth exploring in an only
knowing framework, perhaps along the lines of Halpern and Pass (2009).

Acknowledgements
We thank the reviewers for many helpful comments and suggestions. This work was carried out
when the first author was supported by the graduate school GK 643 at RWTH Aachen University
and funded by a DFG (German Research Foundation) scholarship.
393

Belle & Lakemeyer

Appendix A. Proof of Regression Property
In this section, we prove Theorem 21. We begin with a few useful lemmas before turning to the
main theorem. In what follows, we will make use of the following special construction. Given a
world w, we define another world wŒ£ which is like w except that it satisfies Œ£pre , Œ£post and Œ£sense
sentences of Œ£.
Definition 28 Let w be a world, z ‚àà Z and Œ£ a basic action theory over fluents F . Then wŒ£ is a
world satisfying the following conditions:
1. for f < F , wŒ£ [ f (n1 , . . . , nk ), z] = w[ f (n1 , . . . , nk ), z];
2. for f ‚àà F , wŒ£ is defined inductively by:
(a) wŒ£ [ f (n1 , . . . , nk ), hi] = w[ f (n1 , . . . , nk ), hi];
k
(b) wŒ£ [ f (n1 , . . . , nk ), z ¬∑ r] = m iff wŒ£ , z |= (Œ≥ f )ar my xn11,...,x
,...,nk ;

3. wŒ£ [Poss(r), z] = 1 iff wŒ£ , z |= œÄar ;
4. wŒ£ [SFi (r), z] = m iff wŒ£ , z |= œïi ar mx ;
where Œ≥ f , œÄ and œï are the rhs of the successor state, precondition and sensing axioms respectively,
appearing in the basic action theory Œ£.
The following properties can be shown regarding wŒ£ in relation to w:
Lemma 29 (Lakemeyer & Levesque, 2004)
1. For any w, wŒ£ exists and is unique.
2. If w |= Œ£0 then wŒ£ |= Œ£.
3. If w |= Œ£ then w = wŒ£ .
4. Let Œ± be any bounded objective sentence, and suppose that it is rectified and in NF. Let z ‚àà Z.
Then w |= R[z, Œ±] iff wŒ£ , z |= Œ±.
Proof: The proof for the lemma is given elsewhere (Lakemeyer & Levesque, 2004). Later in this
section, arguments analogous to their proof for item 4 will be needed, so we include the proof for
this item here.
Item 4 is proven by induction on the length of Œ±. We treat the length of Poss(r) and SFi (r) as the
length of œÄar and œïi ar plus 1. We only consider the non-trivial cases below:
case Poss(r).
We have wŒ£ , z |= Poss(r)
iff wŒ£ , z |= œÄar by definition of wŒ£
iff w |= R[z, œÄar ] by induction
394

Multiagent Only Knowing in Dynamic Systems

iff w |= R[z, Poss(r)] by definition of R.
case SFi (r) = m.
We have wŒ£ , z |= SFi (r) = m
iff wŒ£ , z |= œïi ar by definition of wŒ£
iff w |= R[z, œïi ar ] by induction
iff w |= R[z, SFi (r) = m] by definition of R.
case fluents f ‚àà F . Note that, by definition of NF, ground atoms are of the form f (n1 , . . . , nk ) = m.
The proof is by sub-induction on z.
1. wŒ£ |= f (n1 , . . . , nk ) = m
iff w |= f (n1 , . . . , nk ) = m by definition of wŒ£
iff w |= R[hi, f (n1 , . . . , nk ) = m] by definition of R.
2. wŒ£ , z ¬∑ r |= f (n1 , . . . , nk ) = m
k
iff wŒ£ , z |= Œ≥ f ar my xn11,...,x
,...,nk by definition of wŒ£
a
y
x
,...,x
1
iff w |= R[z, Œ≥ f r m n1 ,...,nkk ] by sub-induction
iff w |= R[z ¬∑ r, f (n1 , . . . , nk ) = m] by definition of R.

We now proceed to prove similar properties about epistemic states. Given ek and a basic action
theory Œ£, let us define eŒ£ k inductively by:
1. eŒ£ 1 = {(wŒ£ , {}) | (w, {}) ‚àà e1 };
2. eŒ£ k = {(wŒ£ , eŒ£ k‚àí1 ) | (w, ek‚àí1 ) ‚àà ek }.
In addition, using our notation for nested only knowing operators (see Section 3), for brevity, let
‚Ä¢ œà0 = OKnowŒ£0 [A, k] ‚àß OKnowŒ£0 0 [B, j], and
‚Ä¢ œà = OKnowŒ£ [A, k] ‚àß OKnowŒ£ 0 [B, j].
Then, item 2 of Lemma 29 is extended for knowledge in the following manner:
j

j

Lemma 30 Suppose ekA , eB , w |= œà0 . Then eŒ£ kA , eŒ£ 0 B , w |= œà.
Proof: The proof is a simple induction on the modal depth (Definition 13) of the background theory.
Recall that when the modal depth of the background theory is l, then we have a sentence of the form
OKnowŒ£0 [A, k] ‚àß OKnowŒ£0 0 [B, j] such that k ‚â§ l, j ‚â§ l and k or j is l.
The base case is a theory of modal depth 1. So suppose e1A , e1B , w |= OA (Œ£0 ) ‚àß OB (Œ£0 0 ). We will
now show (w0 , {}) ‚àà eŒ£ 1A iff w0 |= Œ£. From that, we get eŒ£ 1A , {}, w |= OA Œ£. The case of eŒ£ 0 1B is entirely
analogous, by means of which we have shown eŒ£ 1A , eŒ£ 0 1B , w |= OA (Œ£) ‚àß OB (Œ£ 0 ).
Suppose w |= Œ£. Then w |= Œ£0 and therefore, by assumption, (w, {}) ‚àà e1A . By Lemma 29,
w = wŒ£ and therefore, (w, {}) ‚àà eŒ£ 1A .
395

Belle & Lakemeyer

Conversely, let (w, {}) ‚àà eŒ£ 1A . By definition, there is a (w0 , {}) ‚àà e1A such that w0Œ£ = w. But since
|= Œ£0 , it follows from Lemma 29 that w |= Œ£. Thus, eŒ£ 1A , {}, w |= OA (Œ£).
satisfies
Assume that the hypothesis holds for theories of modal depth k ‚àí 1, that is, if ek‚àí1
A
j
k
k
OKnowŒ£0 [A, k‚àí1] then eŒ£ A satisfies OKnowŒ£ [A, k‚àí1], and similarly for B. Now, suppose eA , eB , w |=
k
0
0 k‚àí1
k k‚àí1
k
œà0 . Then, (w0 , ek‚àí1
B ) ‚àà eA iff eA , eB , w |= Œ£0 ‚àßOKnowŒ£0 [B, k ‚àí1]. We now show (w , eB ) ‚àà eŒ£ A iff
0
k
eŒ£ kA , ek‚àí1
B , w |= Œ£ ‚àß OKnowŒ£ [B, k ‚àí 1], from which we get eŒ£ A , {}, w |= OKnowŒ£ [A, k]. The argument
j
is symmetric for eB , and therefore, the lemma‚Äôs claim follows.

w0

0 k‚àí1
k k‚àí1
Consider any ek‚àí1
B and w such that eŒ£ A , eB , w |= Œ£ ‚àß OKnowŒ£ [B, k ‚àí 1]. Now, consider eB
k‚àí1
0
such that {}, eB , w |= OKnowŒ£0 [B, k ‚àí1]. Since w |= Œ£, by Lemma 29 w = wŒ£ and also, w |= Œ£0 . It
follows that (w, e0B k‚àí1 ) ‚àà ekA by assumption. By induction hypothesis, {}, eŒ£ 0 k‚àí1
B , w |= OKnowŒ£ [B, k ‚àí
k‚àí1
k‚àí1
0
k
0
1]. By definition, (w, eŒ£ B ) ‚àà eŒ£ A . An easy argument shows that eŒ£ B = ek‚àí1
B .
k‚àí1
k
k‚àí1
Conversely, consider any (w, eB ) ‚àà eA . By assumption, {}, eB , w |= Œ£0 ‚àß OKnowŒ£0 [B, k ‚àí 1].
By Lemma 14, wŒ£ |= Œ£. By induction hypothesis, {}, eŒ£ k‚àí1
B , w |= OKnowŒ£ [B, k ‚àí 1]. By definition,
k.
(wŒ£ , eŒ£ k‚àí1
)
‚àà
e
Œ£
B
A

We now generalize item 4 of Lemma 29 for knowledge.
j

j

Lemma 31 ekA , eB , w |= R[Œ•, Œ£, Œ£ 0 , z, Œ±] iff eŒ£ kA , eŒ£ 0 B , wŒ• , z |= Œ±.
Proof: The proof is by induction on z, and a sub-induction on Œ±.
Let z = hi. The case of objective formulas proceeds exactly as in Lemma 29. So let us consider
the case of A-subjective formulas.
j
We have eŒ£ kA , eŒ£ 0 B , wŒ• , z |= KA Œ±
k
k k‚àí1
iff for all (w, ek‚àí1
B ) ‚àà eŒ£ A , eŒ£ A , eB , w |= Œ±
k
k
k‚àí1
k
iff for all (w, ek‚àí1
B ) ‚àà eA , eŒ£ A , eŒ£ B , wŒ£ |= Œ± by definition of eŒ£ A
k
k k‚àí1
iff for all (w, ek‚àí1
B ) ‚àà eA , eA , eB , w |= R[hi, Œ±] by sub-induction
j

iff ekA , eB , w |= KA R[hi, Œ±]
j

iff ekA , eB , w |= R[hi, KA Œ±] by definition of R.
The case of B-subjective formulas is symmetric.
Now, we consider the case of z ¬∑ r. The proof is precisely as in the base case, except for subjective formulas, which we prove as follows. We show the argument for A-subjective formulas. The
arguments for B-subjective formulas is symmetric.
j
eŒ£ kA , eŒ£ 0 B , wŒ• , z ¬∑ r |= KA Œ±
j

iff eŒ£ kA , eŒ£ 0 B , wŒ• , z |= [r]KA Œ± by definition
j

iff eŒ£ kA , eŒ£ 0 B , wŒ• , z |= Œ≤ar where Œ≤ is the rhs of Theorem 19 for [r]KA Œ±
j

iff ekA , eB , w |= R[z, Œ≤ar ] by the main induction
396

Multiagent Only Knowing in Dynamic Systems

j

iff ekA , eB , w |= R[z ¬∑ r, KA Œ±] by definition of R.
We are now ready to prove Theorem 21. We restate the claim below:
Theorem 21 Suppose Œ± is a bounded basic sentence of maximal A, B-depth k, j. Let Œ•, Œ£ and Œ£ 0 be
basic action theories. Then R[hi, Œ±] is a static sentence and satisfies:
Œ• ‚àß œà |= Œ± iff Œ•0 ‚àß œà0 |= R[hi, Œ±]
where œà = OKnowŒ£ [A, k] ‚àß OKnowŒ£ 0 [B, j]
œà0 = OKnowŒ£0 [A, k] ‚àß OKnowŒ£0 0 [B, j].
Proof: Let us denote Œ• ‚àß œà as Œì and Œ•0 ‚àß œà0 as Œì0 .
j

For the only-if direction, suppose that Œì |= Œ± and suppose that ekA , eB , w |= Œì0 . That is, w |=
j
Œ•0 and by Lemma 29, wŒ• |= Œ•. Further, by Lemma 30, eŒ£ kA , eŒ£ 0 B , wŒ• |= Œì. By assumption,
j
j
eŒ£ kA , eŒ£ 0 B , wŒ• |= Œ±. Then, by Lemma 31, ekA , eB , w |= R[hi, Œ±].
j

Conversely, suppose that Œì0 |= R[hi, Œ±] and let ekA , eB , w |= Œì. Then w |= Œ•0 . Suppose that
j
j
j
e0 kA , e0 B , w |= œà0 . By assumption e0 kA , e0 B , w |= R[hi, Œ±]. By Lemma 31, eŒ£ 0 kA , eŒ£ 0 0 B , wŒ• |= Œ±. By
j
Lemma 29, wŒ• = w. By Lemma 30, eŒ£ 0 kA , eŒ£ 0 0 B , wŒ• |= Œì. Since both ekA and eŒ£ 0 kA are k-structures
for A where OKnowŒ£ [A, k] holds, a simple induction argument shows that eŒ£ 0 kA = ekA . Analogously,
j
j
j
eŒ£ 0 0 B and eB are the same. Therefore ekA , eB , w |= Œ±.

Appendix B. Proof of Representation Theorem
In this section, we prove Theorem 26. We proceed first by relating valid fluent sentences in ESn to
its non-dynamic fragment OLn (Belle & Lakemeyer, 2010a). For this, we will only go over a few
essential details of OLn . Roughly speaking, OLn is ESn without the dynamic operators {[t], } and
distinguished symbols {Poss, SFi .} A static world w ‚àà W‚àó is any function from primitive sentences
to {0, 1} and from primitive terms to standard names. Epistemic states in OLn are k-structures over
such static worlds. All other notions carry over to OLn , by simply ignoring dynamic aspects. For
j
example, we specify the semantics for KA Œ± wrt the triple (ekA , eB , w) for w ‚àà W‚àó as follows:
j

0 k‚àí1
k
k k‚àí1
0
‚Ä¢ ekA , eB , w |= KA Œ± iff for all w0 ‚àà W‚àó , for all ek‚àí1
B , if (w , eB ) ‚àà eA , then eA , eB , w |= Œ±.

In other words, roughly, we dropped the action sequence z and the compatibility relation 'zA from
the semantical definition of KA Œ± in ESn . Terminology for formulas, such as objective and basic is
analogously defined for OLn .
We now present three formal properties regarding OLn and ESn sentences:
Lemma 32 For any Œ± ‚àà OLn , Œ± is valid in OLn iff Œ± is valid in ESn .
Lemma 33 If Œ± ‚àà ESn is a fluent sentence and z is any action sequence, then R[Œ•, Œ£, Œ£ 0 , z, Œ±] is an
objective OLn -sentence.
397

Belle & Lakemeyer

Both proofs are straightforward generalizations of analogous results regarding OL and ES, appearing as Theorem 6, Lemma 9 and Lemma 10 in the work of Lakemeyer and Levesque (2004), and
therefore not reproduced here. For example, with Lemma 32, the main technical scheme is to relate
ES (and thus ESn ) worlds and OL (and thus OLn ) worlds. For Lemma 33, clearly objective OL
sentences are also objective OLn sentences, and so the claim follows.
Lemma 34 If Œ± is a bounded basic sentence and z is any action sequence, then R[Œ•, Œ£, Œ£ 0 , z, Œ±] is
a basic OLn sentence.
Proof: The proof is by induction on Œ±. If Œ± is a fluent sentence then the argument is immediate
owing to Lemma 33. For Poss, R[z, Poss(t)] = R[z, œÄat ], but œÄat is a fluent formula, so Lemma
ay
ay
33 applies. For SFi , R[z, SFi (t) = t0 ] = R[z, œïi t t0 ], and again, œïi t t0 is a fluent formula. For [t],
R[z, [t]Œ±] = R[z ¬∑ t, Œ±] which is a basic OLn sentence by induction.
For KA , we do a sub-induction on z. The case for KB is analogous. R[hi, KA Œ±] = KA R[hi, Œ±].
Since by the main induction, R[hi, Œ±] is basic, and so KA R[hi, Œ±] is also basic. R[z ¬∑ t, KA Œ±] =
R[z, Œ≤at ], where Œ≤ is the rhs of Theorem 19 for [t]KA Œ±. By the sub-induction hypothesis and Lemma
33, Œ≤at is also basic.
We will now prove three main results that are essential for Theorem 26. To prepare for that, given
static worlds W‚àó and an objective OLn -sentence œÜ, let:
‚Ä¢ WœÜ = {w | w |= œÜ, w ‚àà W‚àó };
‚Ä¢ eœÜ 1 = WœÜ √ó {{}};
‚Ä¢ eœÜ k = {(w, eœÜ k‚àí1 ) | w ‚àà WœÜ }.
In the sequel, benefiting from Lemma 32, we simply argue using OLn -models, that is, by ignoring
dynamic notions.
j

Lemma 35 Let œÜ and œÜ0 be objective OLn sentences and let eœÜ kA and eœÜ0 B be as above. Let Œ± be any
objective formula with free variables x1 , . . . , xk . For any vector of standard names n1 , . . . , nk and
world w:
j
x1 ,...,xk
k
eœÜ kA , eœÜ0 B , w |= KA Œ±nx11 ,...,x
,...,nk iff |= Res[Œ±, œÜ]n1 ,...,nk .
k
Analogously for KB Œ±nx11 ,...,x
,...,nk .

x1 ,...,xk
k
k
Proof: From Lemma 7, it follows that eœÜ kA , {}, w |= KA Œ±nx11 ,...,x
,...,nk iff eœÜ A ‚Üì1 , {}, w |= KA Œ±n1 ,...,nk because
KA Œ± has A-depth 1. So it is sufficient to show that:
x1 ,...,xk
k
eœÜ A ‚Üìk1 , {}, w |= KA Œ±nx11 ,...,x
,...,nk iff |= Res[Œ±, œÜ]n1 ,...,nk .

(15)

Note that eœÜ A ‚Üìk1 = {(w, {}) | w |= œÜ}, and so (15) can be simply proved in OL (Levesque & Lakemeyer, 2001, Lemma 7.2.2).
Theorem 36 Let Œ± be any basic OLn formula of maximal A, B-depth k, j and with free variables
j
x1 , . . . , xk . Let eœÜ kA , eœÜ0 B be as before, w any world, and n1 , . . . , nk be a vector of names. Then
x1 ,...,xk
k
eœÜ kA , eœÜ0 B , w |= Œ±nx11 ,...,x
,...,nk iff w |= kŒ±kœÜ,œÜ0 n1 ,...,nk .
j

398

Multiagent Only Knowing in Dynamic Systems

Proof: The proof is by induction on the structure of Œ±. If Œ± is an atom or an equality, the lemma
clearly holds since Œ± is objective. By induction, the lemma also holds for negations, disjunctions
and quantifiers.
Now, consider KA Œ±. (The case of KB Œ± is symmetric.) We have
k
eœÜ kA , {}, w |= KA Œ±nx11 ,...,x
,...,nk

x1 ,...,xk
k
0 k‚àí1
0
iff eœÜ kA , ek‚àí1
B , w |= Œ±n1 ,...,nk for all (w , eB ) ‚àà eœÜ A

k
by the induction hypothesis
iff w0 |= kŒ±kœÜ,œÜ0 nx11 ,...,x
,...,nk
k
k
is objective
since kŒ±kœÜ,œÜ0 nx11 ,...,x
iff eœÜ kA , {}, w |= KA kŒ±kœÜ,œÜ0 nx11 ,...,x
,...,nk
,...,nk

k
k
, œÜ]nx11 ,...,x
iff |= Res[kŒ±kœÜ,œÜ0 nx11 ,...,x
,...,nk by Lemma 35
,...,nk

k
by definition of Res
iff |= kKA Œ±kœÜ,œÜ0 nx11 ,...,x
,...,nk

k
iff w |= kKA Œ±kœÜ,œÜ0 nx11 ,...,x
because the result of Res is an objective formula that does not use
,...,nk
k
predicates and function symbols. Therefore, kKA Œ±kœÜ,œÜ0 nx11 ,...,x
is either valid or unsatisfi,...,nk
able.

Theorem 37 Suppose Œ± is of maximal A, B-depth k, j. Let œÜ, œÜ0 and Œ∏ be objective OLn sentences.
Then
Œ∏ ‚àß œà |= Œ± iff |= Œ∏ ‚äÉ kŒ±kœÜ,œÜ0 .
where œà = OKnowœÜ [A, k] ‚àß OKnowœÜ0 [B, j].
j

Proof: For the if direction, suppose (ekA , eB , w) is a model of œà ‚àß Œ∏. It is easy to verify that ekA = eœÜ kA
j
j
j
and eB = eœÜ0 B , and so, w is any world satisfying Œ∏. Since œà ‚àß Œ∏ |= Œ±, ekA , eB , w |= Œ± iff w |= kŒ±kœÜ,œÜ0 by
Theorem 36. So any model of Œ∏ satisfies kŒ±kœÜ,œÜ0 . Therefore, Œ∏ |= kŒ±kœÜ,œÜ0 or |= Œ∏ ‚äÉ kŒ±kœÜ,œÜ0 .
j

Conversely, suppose Œ∏ |= kŒ±kœÜ,œÜ0 . Now, let (ekA , eB , w) be any model of œà ‚àß Œ∏. It is easy to
j
j
verify that ekA = eœÜ kA and eB = eœÜ0 B . Further, since w |= Œ∏ we have w |= kŒ±kœÜ,œÜ0 . By Theorem 36,
j
ekA , eB , w |= Œ±.
Finally, we turn to the proof for Theorem 26. We restate the claim below.
Theorem 26 Let Œ•, Œ£ and Œ£ 0 be basic action theories. Suppose Œ± is a basic bounded sentence of
maximal A, B-depth k, j, then
Œ• ‚àß œà |= Œ± iff

|= Œ•0 ‚äÉ kR[hi, Œ±]kŒ£0 ,Œ£0 0 .

where œà = OKnowŒ£ [A, k] ‚àß OKnowŒ£ 0 [B, j].
Proof: We have Œ• ‚àß OKnowŒ£ [A, k] ‚àß OKnowŒ£ 0 [B, j] |= Œ±
iff Œ•0 ‚àß OKnowŒ£0 [A, k] ‚àß OKnowŒ£0 0 [B, j] |= R[hi, Œ±] by the regression property Theorem 21
iff Œ•0 ‚àß OKnowŒ£0 [A, k] ‚àß OKnowŒ£0 0 [B, j] ‚äÉ R[hi, Œ±] is valid in OLn by Lemma 32, owing to the
fact that R[hi, Œ±] is also a (basic) OLn sentence by Lemma 34
iff Œ•0 ‚äÉ kR[hi, Œ±]kŒ£0 ,Œ£0 0 is valid in OLn by Theorem 37.
399

Belle & Lakemeyer

References
Alur, R., Henzinger, T. A., & Kupferman, O. (2002). Alternating-time temporal logic. J. ACM,
49(5), 672‚Äì713.
Belle, V., & Lakemeyer, G. (2010a). Multi-agent only-knowing revisited. In Proc. KR, pp. 49‚Äì60.
Belle, V., & Lakemeyer, G. (2010b). Reasoning about imperfect information games in the epistemic
situation calculus. In Proc. AAAI, pp. 255‚Äì261.
Belle, V., & Lakemeyer, G. (2011). A semantical account of progression in the presence of uncertainty. In Proc. AAAI, pp. 165‚Äì170.
Demolombe, R. (2003). Belief change: from situation calculus to modal logic. In Proc. Nonmonotonic Reasoning, Action, and Change (NRAC).
Demolombe, R., Herzig, A., & Varzinczak, I. (2003). Regression in modal logic. Journal of Applied
Non-Classical Logics, 13(2), 165‚Äì185.
Fagin, R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1995). Reasoning About Knowledge. MIT
Press.
Fagin, R., Halpern, J. Y., & Vardi, M. Y. (1991). A model-theoretic analysis of knowledge. J. ACM,
38(2), 382‚Äì428.
Fritz, C. (2009). Monitoring the Generation and Execution of Optimal Plans. Ph.D. thesis, University of Toronto.
Gabaldon, A., & Lakemeyer, G. (2007). ESP: A logic of only-knowing, noisy sensing and acting.
In Proc. AAAI, pp. 974‚Äì979.
Gerbrandy, J., & Groeneveld, W. (1997). Reasoning about information change. J. of Logic, Lang.
and Inf., 6(2), 147‚Äì169.
Giacomo, G. D., LespeÃÅrance, Y., & Pearce, A. R. (2010). Situation calculus based programs for
representing and reasoning about game structures. In KR.
Halpern, J. Y. (1993). Reasoning about only knowing with many agents. In Proc. AAAI, pp. 655‚Äì
661.
Halpern, J. Y., & Lakemeyer, G. (1995). Levesque‚Äôs axiomatization of only knowing is incomplete.
Artificial Intelligence, 74(2), 381‚Äì387.
Halpern, J. Y., & Moses, Y. (1984). Towards a theory of knowledge and ignorance: Preliminary
report. In Proc. NMR, pp. 125‚Äì143.
Halpern, J. Y., & Pass, R. (2009). A logical characterization of iterated admissibility. In Proc.
TARK, pp. 146‚Äì155.
Halpern, J., & Lakemeyer, G. (2001). Multi-agent only knowing. Journal of Logic and Computation,
11(1), 251‚Äì265.
Harel, D., Kozen, D., & Tiuryn, J. (2000). Dynamic logic. The MIT Press.
Hintikka, J. (1962). Knowledge and belief: an introduction to the logic of the two notions. Cornell
University Press.
Hoek, W. V. D., & Thijsse, E. (2002). A general approach to multi-agent minimal knowledge: With
tools and samples. Studia Logica, 72(1), 61‚Äì84.
400

Multiagent Only Knowing in Dynamic Systems

Kakas, A. C., Michael, L., & Miller, R. (2008). Fred meets tweety. In ECAI, pp. 747‚Äì748.
Kaneko, M., & Suzuki, N.-Y. (2003). Epistemic models of shallow depths and decision making in
games: Horticulture. The Journal of Symbolic Logic, 68(1), pp. 163‚Äì186.
Kaplan, D. (1968). Quantifying in. Synthese, 19(1), 178‚Äì214.
Kelly, R. F., & Pearce, A. R. (2008). Complex epistemic modalities in the situation calculus. In
Proc. KR, pp. 611‚Äì620.
Kripke, S. (1963). Semantical considerations on modal logic. Acta Philosophica Fennica, 16,
83‚Äì94.
Lakemeyer, G. (1996). Only knowing in the situation calculus. In Proc. KR, pp. 14‚Äì25.
Lakemeyer, G., & Levesque, H. J. (2011). A semantic characterization of a useful fragment of the
situation calculus with knowledge. Artificial Intelligence, 175, 142‚Äì164.
Lakemeyer, G., & Levesque, H. J. (2004). Situations, si! situation terms, no!. In Proc. KR, pp.
516‚Äì526.
Lakemeyer, G., & Levesque, H. (1998). AOL: a logic of acting, sensing, knowing, and only knowing. In Proc. KR, pp. 316‚Äì329.
Lakemeyer, G. (1993). All they know: A study in multi-agent autoepistemic reasoning. In Proc.
IJCAI, pp. 376‚Äì381.
Lakemeyer, G., & LespeÃÅrance, Y. (2012). Efficient reasoning in multiagent epistemic logics. In
Proc. ECAI, pp. 498‚Äì503.
Lakemeyer, G., & Levesque, H. (2009). A semantical account of progression in the presence of
defaults. In Conceptual Modeling: Foundations and Applications, pp. 82‚Äì98. Springer.
Levesque, H. J. (1990). All I know: a study in autoepistemic logic. Artificial Intelligence, 42(2-3),
263‚Äì309.
Levesque, H., & Lakemeyer, G. (2001). The logic of knowledge bases. The MIT Press.
Lin, F., & Reiter, R. (1997). How to progress a database. Artificial Intelligence, 92(1-2), 131‚Äì167.
McCarthy, J., & Hayes, P. J. (1969). Some philosophical problems from the standpoint of artificial
intelligence. In Machine Intelligence, pp. 463‚Äì502.
Moore, R. C. (1985). Semantical considerations on nonmonotonic logic. Artificial Intelligence,
25(1), 75‚Äì94.
Pratt-Hartmann, I. (2000). Total knowledge. In Proc. AAAI, pp. 423‚Äì428.
Reiter, R. (2001). Knowledge in action: logical foundations for specifying and implementing dynamical systems. MIT Press.
Rogers Jr., H. (1987). Theory of recursive functions and effective computability. The MIT Press.
Rosati, R. (2000). On the decidability and complexity of reasoning about only knowing. Artificial
Intelligence, 116(1-2), 193‚Äì215.
Scherl, R. B., & Levesque, H. J. (2003). Knowledge, action, and the frame problem. Artificial
Intelligence, 144(1-2), 1‚Äì39.
401

Belle & Lakemeyer

Shapiro, S., LespeÃÅrance, Y., & Levesque, H. (2002). The cognitive agents specification language
and verification environment for multiagent systems. In Proc. AAMAS, pp. 19‚Äì26.
Thielscher, M. (1999). From situation calculus to fluent calculus: state update axioms as a solution
to the inferential frame problem. Artificial Intelligence, 111(1-2), 277‚Äì299.
Van Ditmarsch, H., Herzig, A., & De Lima, T. (2007). Optimal regression for reasoning about
knowledge and actions. In Proc. AAAI, pp. 1070‚Äì1075.
Van Ditmarsch, H. (2002). Descriptions of game actions. Journal of Logic, Language and Information, 11(3), 349‚Äì365.
Waaler, A., & Solhaug, B. (2005). Semantics for multi-agent only knowing: extended abstract. In
Proc. TARK, pp. 109‚Äì125.
Wooldridge, M. (2009). An Introduction to Multiagent Systems (2 edition). Wiley, Chichester, UK.

402

Journal of Artificial Intelligence Research 49 (2014) 527-568

Submitted 10/13; published 03/14

Large-Scale Optimization for Evaluation Functions with
Minimax Search
Kunihito Hoki

hoki@cs.uec.ac.jp

Department of Communication Engineering and Informatics
The University of Electro-Communications

Tomoyuki Kaneko

kaneko@acm.org

Department of Graphics and Computer Sciences
The University of Tokyo

Abstract
This paper presents a new method, Minimax Tree Optimization (MMTO), to learn
a heuristic evaluation function of a practical alpha-beta search program. The evaluation
function may be a linear or non-linear combination of weighted features, and the weights
are the parameters to be optimized. To control the search results so that the move decisions agree with the game records of human experts, a well-modeled objective function
to be minimized is designed. Moreover, a numerical iterative method is used to find local
minima of the objective function, and more than forty million parameters are adjusted by
using a small number of hyper parameters. This method was applied to shogi, a major
variant of chess in which the evaluation function must handle a larger state space than
in chess. Experimental results show that the large-scale optimization of the evaluation
function improves the playing strength of shogi programs, and the new method performs
significantly better than other methods. Implementation of the new method in our shogi
program Bonanza made substantial contributions to the program‚Äôs first-place finish in the
2013 World Computer Shogi Championship. Additionally, we present preliminary evidence
of broader applicability of our method to other two-player games such as chess.

1. Introduction
Heuristic search is a powerful method in artificial intelligence. In 1997, the chess-playing
computer Deep Blue defeated the world chess champion Garry Kasparov (Campbell, Hoane,
& Hsu, 2002). The computer decided its moves after making a large number of searches
of the minimax game tree and using heuristic evaluation functions. In this framework of
artificial intelligence, the heuristic evaluation functions, as well as the search methods, are
crucial for making strong computer players. Thus, researchers working on various games
have made substantial efforts in a quest to create effective evaluation functions by using machine learning techniques (FuÃàrnkranz, 2001). However, fully automated learning of
the heuristic evaluation functions remains a challenging goal in chess variants. For example, developers have reported that the majority of the features and weights in Deep Blue
were created/tuned by hand (Campbell et al., 2002). It is said that recent top-level chess
programs tune some of their parameters automatically, although we have yet to find any
publication describing the methods they use. Moreover, reinforcement learning has been
applied to chess (Baxter, Tridgell, & Weaver, 2000; Veness, Silver, Uther, & Blair, 2009).
c
2014
AI Access Foundation. All rights reserved.

Hoki & Kaneko

However, to the best of the authors‚Äô knowledge, the evaluation functions learned by the
methods reported in the literature are still weaker than the best hand-crafted functions in
terms of chess-playing strength.
In this paper, we revisit the idea behind earlier research on learning chess evaluation
functions (Marsland, 1985; Hsu, Anantharaman, Campbell, & Nowatzyk, 1990; Tesauro,
2001) and reformulate the task as an optimization problem using an alternative learning
method, called Minimax Tree Optimization (MMTO). The objective here is to optimize
a full set of parameters in the evaluation function so that the search results match the
desired move decisions, e.g., the recorded moves of grandmaster games. The evaluation
functions are learned through iteration of two procedures: (1) a shallow heuristic search for
all training positions using the current parameters and (2) a parameter update guided by an
approximation of the gradient of the objective function. To achieve scalability and stability,
we introduce a new combination of optimization techniques: a simplified loss function, gridadjacent update, equality constraint, and l1 -regularization. One of the resulting merits is
that MMTO can ensure the existence of a local minimum within a convenient range of
parameters.
This study demonstrates the performance of MMTO in shogi, a variant of chess where
evaluation functions need to handle a wider variety of features and positions than in Western
chess. Implementation of MMTO in our shogi program Bonanza (described in Section 4.6)
made substantial contribution to the program‚Äôs first-place finish in the 2013 World Computer Shogi Championship. The rules of shogi, as well as a survey of approaches in artificial
intelligence, are described in the literature (Iida, Sakuta, & Rollason, 2002). Basic techniques, such as a minimax search guided by heuristic evaluation functions, are as effective
in shogi as in chess. However, the ‚Äúdrop rule‚Äù that allows a player to reuse captured pieces
significantly changes a few properties: (1) the number of legal moves, as well as average
game length, is greater than in chess, (2) endgame databases are not available, (3) and
the material balance is less important than in chess, especially in the endgame. Thus, the
performance of a shogi program is more dependent on the quality of its evaluation function.
Through experiments, we first show that the full set of parameters in the evaluation
functions can be optimized with respect to the rate of agreement with the training set.
After that, we examine the performance of various learned evaluation functions in terms of
their rates of agreement with the test positions and win rates against references. Scalability
is demonstrated up to about forty million parameters, which is far too many to tune by
hand. The features we used are piece values and extended versions of piece-square tables
that are commonly used to learn evaluation functions in chess (Tesauro, 2001; Baxter et al.,
2000; Veness et al., 2009). We also briefly examine the performance of MMTO in chess to
catch a glimpse of the applicability of MMTO to other games.
The rest of this paper is organized as follows. The next section reviews related research.
The third section presents the MMTO method. The fourth section shows our experimental
results, where forty million of parameters are adjusted for better performance, and compares
the performance of our method with that of existing methods. The last section presents
our concluding remarks. This paper incorporates and extends our previous work (Hoki &
Kaneko, 2012; Kaneko & Hoki, 2012).
528

Large-Scale Optimization for Evaluation Functions with Minimax Search

2. Related Work
This section reviews related research on learning evaluation functions. First, we describe
supervised learning methods that use the desired moves. Second, we discuss other learning
methods, including regression and reinforcement learning. Third, we briefly discuss the difficulty of supervised learning in terms of numerical optimization. Although machine learning
of other components besides evaluation functions in game programs would be an interesting
research topic (BjoÃàrnsson & Marsland, 2002; Tsuruoka, Yokoyama, & Chikayama, 2002;
Coulom, 2007; Silver & Tesauro, 2009), this review only focuses on research that has been
done on learning evaluation functions.
2.1 Learning from Desired Moves in Chess
Grandmaster games are a popular source of information for learning chess. Let us say
that we have a set of positions P and the desired moves for each position in P. Typically,
such positions and moves are sampled from grandmaster games. A chess program has an
evaluation function e(p, w ), where p is a game position and the feature weight vector w
contains the parameters to be adjusted.
Let us assume that the evaluation function e(p, w ) is partially differentiable with respect
to wi for any i. Here, wi is the i-th component of w . For
P example, the function could be
a linear combination of weighted features, i.e., e(p, w ) = i wi fi (p), where fi (p) is the i-th
feature value of position p. The aim of learning is to find a better weight vector w for
strengthening the play of the program. The hypothesis behind this kind of learning is that
the more the computer play agrees with the desired moves, the better it plays.
Let us begin with a simple intuitive goal: make the results of a one-ply search agree
with the desired moves. For simplicity, let us assume that the maximizing player moves first
at the root position p. In a one-ply search, the move with the highest evaluation value is
selected. Thus, w should be adjusted so that the desired move has the highest evaluation of
all the moves. This goal can formally be written as a mathematical minimization problem
with an objective function:
P
w) =
JH
(w

X X

H (e(p.m, w ) ‚àí e(p.dp , w )) .

(1)

p‚ààP m‚ààM0p

Here, p.m is the position after move m in position p, dp is the desired move in position p, M0p
is the set of all legal moves in p excluding dp , and H(x) is the Heaviside step function, i.e.,
H(x) equals 1 if x ‚â• 0, and 0 otherwise. Because this objective function counts the number
of moves that have an evaluation value greater than or equal to that of the desired move,
a better w can be found by minimizing Eq. (1). Although several studies have attempted
machine learning on the basis of this framework (Nitsche, 1982; van der Meulen, 1989;
Anantharaman, 1997), their numerical procedures were complicated, and the adjustment of
a large-scale vector w seemed to present practical difficulties.
Marsland (1985) presented a notable extension wherein a continuous function is used
so that conventional optimization techniques can be exploited. Here, a continuous function
of difference is substituted for the non-continuous step function in Eq. (1). An interesting
529

Hoki & Kaneko

modified function is
w) =
J2P (w

X X

[max {0, e(p.m, w ) ‚àí e(p.dp , w )}]2 .

(2)

p‚ààP m‚ààM0p

The meaning of the function value is different from that in Eq. (1); i.e., the function does
not count the number of moves that have an evaluation value greater than or equal to that
w ) helps to reduce the function
of the desired move. However, the gradient vector ‚àáw J2P (w
value numerically. Marsland also introduced inequality constraints in order to keep the
evaluation in the right range. However, the literature does not provide any experimental
results on practical chess programs.
A second notable extension was proposed early in the development of chess machines
Deep Thought (Nowatzyk, 2000; Hsu et al., 1990). Here, the positions being compared are
not p.m, but rather œÄwp.m , that is, one of the leaves of the principal variations (PVs), possibly
several plies from p.m. This extension carries out a least-square fitting of the evaluation
w ) does. Instead, it biases
values. Therefore, it does not have the max function that J2P (w
p.dp
the value of e(œÄw , w ) before it is used in each least-square fitting, if the evaluation value
p .d
p .m
of the desired move dp , e(œÄw p , w ) is lower than that of another move m, e(œÄw
, w ).
A third notable extension is the comparison training proposed by Tesauro (2001).
Tesauro modified the objective function to
X X
p .d
P
p .m
w) =
Jct
(w
Tct (e(œÄw p , w ) ‚àí e(œÄw
, w )),
p‚ààP m‚ààM0p

Tct (x) = [œÉ(R(x)) ‚àí 1]2 ,

(3)

where œÉ is the standard sigmoid function, and R is a heuristic rescaling factor for positive
differences, i.e., R(x) = x when x ‚â§ 0, and R(x) = cx for a constant c > 1 otherwise. Note
that R(x) is still a continuous function. The important property of this modified objective
function is that the value and the derivative are zero in the limit as the difference x goes
to positive infinity, and they are respectively one and zero in the limit as the difference
x goes to negative infinity. Therefore, Tct (x) in Eq. (3) is a continuous approximation of
H(‚àíx) in Eq. (1). Note that this property is not explicitly stated by Tesauro, but it is
notably distinct from the other work. The number of feature weights adjusted with his
method was less than two hundred. Tesauro also mentioned an application for small-bit
integers, which was used to adjust some of the weights in Deep Blue. However, he neither
clarified its procedure nor mentioned whether the weights were automatically adjusted in
that experiment.
Table 1 summarizes the related work. Each of the existing methods possesses at least
one of three important properties for optimization, i.e., continuity, minimax searches, and
assured local minimum. However, none of them have all three properties. Also, some
of the existing methods (Nowatzyk, 2000; Hsu et al., 1990; Tesauro, 2001) do not try to
decrease the functions through iteration as much as possible. We will revisit these issues in
Section 2.3. On the other hand, our method, MMTO, has scalability in high-dimensional
learning. Moreover, we empirically show that a decrease in the objective function value
leads to an increase in playing strength. The existing methods have not been shown to have
this property.
530

Large-Scale Optimization for Evaluation Functions with Minimax Search

Method
(Nitsche, 1982)
(Marsland, 1985)
(van der Meulen, 1989)
(Hsu et al., 1990)
(Anantharaman, 1997)
Comparison training
MMTO

Continuity

Search

Assured local minimum

No

No
No
No

No
No
No
No

Yes
No

Yes‚àó

Yes
Yes
Yes
Yes

No

Yes‚àó
Yes‚àó

Yes
No

Yes

Table 1: Summary of learning methods using the desired moves in training positions to
adjust the feature weights in the evaluation functions. The first column is the name
of the method or piece of literature. The second column describes the continuity
of the objective functions with respect to the feature weights. Yes‚àó means that
continuity depends on the kind of search method used. The third column indicates
whether the objective functions use minimax searches with depths more than 1,
instead of comparisons of legal moves at the root position. The fourth column
shows whether the hyper parameters of the objective functions can assure a local
minimum can be found.

2.2 Other Methods of Learning Evaluation Functions
Many researchers have utilized information sources other than the desired moves. For
example, some studies on Othello dating from the 1990s compare the desired moves with
other moves (Fawcett, 1993). However, the most practical and famous machine learning
method that has yielded strong programs is based on regression of the desired value by
using 1.5 million features (Buro, 1995, 2002). In Othello, different evaluation functions are
used for game stages determined on the basis of the number of discs in play. Thus, the
desired values of the training positions are obtained through a complete endgame search
as well as a heuristic search with evaluation functions learned in later game stages. This
method has also been successfully applied to card games (Buro, Long, Furtak, & Sturtevant,
2009), but not to chess variants. To the best of the authors‚Äô knowledge, learning based on
regression with win/loss-labeled data has not yielded decent evaluation functions in chess
variants. Except for not using the desired moves, Buro‚Äôs method has properties that are
similar to those listed in Table 1; his objective function has continuity as well as an assured
local minimum, and his method is scalable. Gomboc, Buro, and Marsland (2005) proposed
to learn from game records annotated by human experts; however, the feature weights that
were adjusted in their experiments were only a small part of the full evaluation functions.
Reinforcement learning (Sutton & Barto, 1998), especially temporal difference learning,
of which a famous success is Backgammon (Tesauro, 2002), is considered to be promising
way to avoid the difficulty in finding the desired values for regression. This approach has
been applied to chess and has been shown to improve the strength of programs (Baxter
et al., 2000; Levinson & Weber, 2001; Veness et al., 2009). The KnightCap program
achieved a rating of about 2, 150 points at the Free Internet Chess Server (FICS1 ) and
1. Free Internet Chess Server, http://www.freechess.org, last access: 2013

531

Hoki & Kaneko

Easy
(a)

Single minimum

DiÔ¨Écult
(b)

(c)

(d)

Smooth Non-diÔ¨Äeren able

(e)

Narrow trough Non-con nuous

Figure 1: Example illustrating the difficulties facing any minimization procedure.
2, 575 points at its highest peak at the Internet Chess Club (ICC) (Baxter et al., 2000).
Another program achieved 2, 338 points at its highest peak at ICC (Veness et al., 2009).
However, strong human players have ratings of more than 3, 000 points at ICC, and this
difference means these programs have not reached the top level of chess programs; that
is, evaluation functions tuned by reinforcement learning have not yet reached the level
of the best-handcrafted evaluation functions in chess. Moreover, the number of feature
weights to be adjusted is on the order of thousands. In checkers, evaluation functions
trained by temporal difference learning are reportedly comparable to the best handcrafted
efforts (Schaeffer, Hlynka, & Jussila, 2001). It has also been reported that a player stronger
than expert human checker players was created by using neural networks trained with
an evolutionary strategy (Chellapilla & Fogel, 1999). Here, no features beyond the piece
differentials were given to the neural network a priori.
Many machine learning techniques (Baxter et al., 2000; Veness et al., 2009) have been
applied to shogi. However, despite efforts by many programmers and researchers, the adjustment of the full weight vector in the evaluation function remains a challenging goal.
The studies published so far have adjusted only piece values or a small part of the feature
weights in the evaluation functions (Beal & Smith, 2001; Ugajin & Kotani, 2010).
2.3 Learning and Numerical Optimization
Some learning methods reviewed in Section 2.1 have objective functions to decrease; the
learning process can be extended into a numerical optimization using these functions. The
performance of numerical optimization is sensitive to the surface of the objective function.
Figure 1 shows the properties of particular sorts of functions and their difficulties regarding
numerical minimization. The easiest one among them is the convex function (a); if a local
minimum exists, then it is a global minimum. Function (b) has multiple local minima; however, it can still be thought of as an easy problem, because various minimization algorithms
using gradients and Hessian matrices are effective on it. It would be desirable to design a
learning method using, say, linear or logistic regression, which uses one of these two types
of objective function (Buro, 2002).
In contrast, non-differentiable functions such as (c) and (e) are often more difficult to
minimize than differentiable ones. This is because a quadratic model, such as the Hessian
approximation of the conjugated gradient method (Bertsekas & Bertsekas, 2008), is not
always appropriate for these functions. Function (d) is also a difficult target, because an
important local minimum is hidden inside a deep narrow trough, and it is quite difficult to
find it by using numerical iteration methods. The most difficult example is minimization of
532

Large-Scale Optimization for Evaluation Functions with Minimax Search

the non-continuous function (e); even primitive iterative methods such as gradient decent
are not capable of finding its minimum. The extreme case would be a function for which
an analytical formula for the gradient is unavailable. In that case, a learning method would
not be able to use partial derivatives, and the minima would have to be obtained using
derivative-free methods, e.g., sampling methods (BjoÃàrnsson & Marsland, 2002; Coulom,
2012).
Theorems in Appendix A show that the minimax value is continuous but not always
partially differentiable. Thus, the existing methods that incorporate a minimax search
(Hsu et al., 1990; Tesauro, 2001) and MMTO listed in Table 1 are type (c). Moreover,
certain forward pruning techniques may cause discontinuities. Therefore, even these learning
methods can be type (e). To overcome this difficulty, MMTO has a well-modeled objective
function and updates the feature weights in a careful manner.

3. Minimax Tree Optimization
Minimax Tree Optimization (MMTO) is an extension of comparison training to reach the
first intuitive goal embodied in Eq. (1). The purpose of this extension is to overcome the
practical difficulties and stabilize the mathematical optimization procedure with a largescale feature weight vector w . Given a set of training positions P and the desired move dp
for each position p, MMTO optimizes the weight vector w so that the minimax search with
w better agrees with the desired moves.
The weight vector w is improved through iteration of sub-procedures (see Figure 2).
For each iteration t, the first step consists of tree searches to identify one of the leaves of
PVs œÄw (t) for all legal moves in the training positions P. Because PV leaf œÄw (t) depends
on the feature weights w (t) in an evaluation function, a new PV will be obtained when
w (t) is updated (We discuss this issue in Section 3.5). The second step is the calculation
of the approximate partial derivatives, which depends on both PV and the weight vector.
The last step is the update of the weight vector. For numerical stability, the difference
w (t + 1) ‚àí w (t)| must be kept small so that it will not be distorted by drastic changes in
|w
the partial derivatives. Section 3.4 shows that a grid-adjacent update ensures this.
3.1 Objective Function to be Minimized
The objective function is
P
w ) = J(P, w ) + JC (w
w ) + JR (w
w ),
JMmto
(w

(4)

where the first term J(P, w ) on the right side is the main part. The other terms JC and
JR are constraint and regularization terms, respectively, which are defined in Section 3.2.
The first term is
X X
J(P, w ) =
T (s(p.dp , w ) ‚àí s(p.m, w )) ,
(5)
p‚ààP m‚ààM0p

where s(p, w ) is the minimax value identified by the tree search for position p. T (x) is
1/(1 + exp(ax)), which is a horizontally mirrored sigmoid function. The slope of T (x) is
controlled by a constant parameter a > 0. In the large a limit, T (x) becomes the Heaviside
533

Hoki & Kaneko




m
œÄwp.(t)

1. Perform a game-tree search to identify PV leaves
for all child positions
p.m of position p in training set P, where w (t) is the weight vector at the
t-th iteration and w (0) is the initial guess.
2. Calculate a partial-derivative approximation of the well-modeled objective
p .m
w
function defined in Section 3.1 by using both œÄw
(t) and (t). The objective
function employs a differentiable approximation of H(x) (see Section 3.1),
as well as a constraint and regularization term (see Section 3.2).



3. Obtain a new weight vector w (t+1) from w (t) by using a grid-adjacent update
guided by the partial derivatives computed in step 2 (see Section 3.4). Go
back to step 1, or terminate the optimization when the objective function
value converges (see Section 4).



Figure 2: Minimax Tree Optimization: Iteration of searches and update using partial
derivatives

step function H(x). Thus, the main differences from the first intuitive objective function
P (w
w ) in Eq. (1) are the use of T (x) for a smooth approximation of H(x) and the use of
JH
w ) in
the search result s(p, w ) instead of the raw evaluation e(p, w ). The difference from J2P (w
P
w
w
Eq. (2) and Jct (w ) in Eq. (3) is that J(P, ) is simpler and closer to the first intuitive one
w ) or JR (w
w ) in Eq (4).
in Eq. (1). Moreover, none of the existing studies incorporate JC (w
The minimax value s(p, w ) equals the raw evaluation value e(œÄwp , w ), where e(p, w ) is the
evaluation of position p and œÄwp is one of the PV leaves identified by the tree search rooted
at p with a weight vector w . In most cases, the derivatives of s(p, w ) equal the derivatives
of e(œÄwp , w ). For these reasons, the PV leaves are identified in step 1 in Figure 2.
3.2 Constraint and Regularization Terms
In the computer programs of chess variants, the evaluation values are typically represented
by integers. Signed 16-bit integers are especially preferred because the corresponding transposition tables will be memory efficient. Thus, we will restrict the range of the absolute
value of the evaluation function e(p, w ). Moreover, because the search results do not change
w with a constant factor Œ± > 0, this restriction
when one uses a scaled weight vector Œ±w
stabilizes the numerical optimization procedure if the value of Œ± is uncertain.
w ) = Œª0 g(w
w 0 ) in Eq. (4), where
For this restriction, we introduce a constraint term JC (w
0
w ) = 0 is an equality constraint, and Œª0 is a Lagrange multiplier.
is a subset of w , g(w
w ) (see
In addition to the constraint term, we also introduce a regularization term JR (w
w ) = Œª1 |w
w 00 |, where Œª1 > 0 is a
the last term in Eq. (4)). We use l1 -regularization JR (w
constant variable, and w 00 is a subset of w . l1 -regularization is widely used to deal with highdimensional parameters, whereas l2 -regularization is used to avoid over-fitting (Tibshirani,
1996).

w0

534

Large-Scale Optimization for Evaluation Functions with Minimax Search

P (w
w ) exists
The constraint and regularization terms ensure that a local minimum of JMmto
in a finite range of w . On the other hand, depending on P and the distribution of dp , this
P (w
w ) in Eq. (2), or for Jct
w ) in Eq. (3).
property is not always true for J(P, w ) itself, for J2P (w
The constraint and l1 -regularization terms have similar functionalities; i.e., both restrict
the range of the absolute value of the evaluation function e(p, w ). However, their distinctions are important in practice because l1 -regularization makes the weight vector w 00 sparse
whereas the constraint term does not. Thus, the regularization term is suitable for minor
features that are rarely seen, whereas the constraint term is suitable for major features
that appear often in the training set. Moreover, both terms are useful for controlling the
strength of the restriction. Because major feature values usually change more often than
minor feature values, the magnitudes of the partial derivatives with respect to major feature
weights are usually greater than those with respect to minor feature weights. We can adjust
the strength of l1 -regularization term so that it is weaker than the constraint term.
For example, our experiments used the constraint term for the piece values because their
feature values, i.e., the number of pieces owned by black/white, change in most single games
of shogi. The many other weights were penalized by l1 -regularization. Each weight was
w 0 , w 00 ). Because the
controlled by either the constraint or l1 -regularization term, i.e., w = (w
partial derivatives with respect to the major and minor feature weights differed by several
orders of magnitude, it was difficult to stabilize the optimization procedure by means of a
single hyper parameter Œª1 .

3.3 Partial Derivative Approximation
In each iteration, feature weights are updated on the basis of the partial derivatives of the
P (w
w ) defined by Eq. (4). The partial derivative, if it exists, is
objective function JMmto
‚àÇ P
‚àÇ
‚àÇ
‚àÇ
w) =
w) +
w ).
JMmto (w
J(P, w ) +
JC (w
JR (w
‚àÇwi
‚àÇwi
‚àÇwi
‚àÇwi

(6)

‚àÇ
w ) on the right side is treated in an intuitive manner; sgn(wi )Œª1 for
The last term ‚àÇw
JR (w
i
00
wi ‚àà w , and 0 otherwise. Function sgn(x) is 1 for x > 0, 0 for x = 0, and ‚àí1 for x < 0.
‚àÇ
w ) is 0 for wi ‚àà
JC (w
/ w 0 . The case of wi ‚àà w 0
The partial derivative of the constraint term ‚àÇw
i
is discussed in Section 3.5.
The partial derivative of J(P, w ) does not always exist, because the minimax value
s(p, w ) is not always differentiable. Instead, we can use an approximation,

‚àÇ
J(P, w ) =
‚àÇwi
‚âà

‚àÇ X X
T (s(p.dp , w ) ‚àí s(p.m, w ))
‚àÇwi
0

(7)



‚àÇ X X
p .d
T e(œÄw p , w ) ‚àí e(œÄwp.m , w )
‚àÇwi
0

(8)

p‚ààP m‚ààMp

p‚ààP m‚ààMp

=

X X

p.d

T 0 (e(œÄw p , w ) ‚àí e(œÄwp.m , w )) ¬∑

p‚ààP m‚ààM0p


‚àÇ  p .d p
e(œÄw , w ) ‚àí e(œÄwp.m , w ) ,
‚àÇwi

where T 0 (x) = ddx T (x). The approximation of Eq. (7) by Eq. (8) makes the computation
tractable, because we identify the PV leaves in step 1 in Figure 2. As stated in Appendix A,
535

Hoki & Kaneko

minimax value s(p, w ) found by the Œ±Œ≤ search is continuous, and therefore, the function
J(P, w ) is also continuous. Moreover, the approximate value is equivalent to the partial
derivative when a unique PV exists for each position. Appendix A also discusses the con‚àÇ
ditions under which ‚àÇw
s(p, w ) exists. Note that we have found that the errors caused by
i
this approximation are sufficiently small for the shogi application (Kaneko & Hoki, 2012).
Previous studies (Baxter et al., 2000; Tesauro, 2001) use this approximation as well.
3.4 Grid-Adjacent Update
For numerical stability, the grid-adjacent update in step 3 (see Figure 2) is used to get
w (t + 1) from w (t). Consider a simple n-dimensional grid in which the distance between two
adjacent points is h. Suppose that h is an integer, e.g., h = 1. In the grid-adjacent update,
the feature vector w (t) is always one of the points of the grid, and the i-th component
wi (t + 1) is adjacent to wi (t):
wi (t + 1) = wi (t) ‚àí h sgn(

P (w
w (t))
‚àÇJMmto
).
‚àÇwi

Thus, |wi (t+1)‚àíwi (t)| = |‚àÜwi | = h or 0 for all i. This update should decrease the objective
P (w
P (w
w ) because ‚àÜw
w ¬∑ ‚àáw JMmto
w ) ‚â§ 0 and the errors in the approximation (see
function JMmto
Eq. (8)) are negligible. Moreover, h must be small enough so that the update does not
p
p
change the PV, i.e., œÄw
(t) = œÄw (t+1) for the majority of positions p searched in step 1.
Although MMTO focuses on optimization of weight vectors represented by integers, it
should be noted that the gradient descent update is not suitable even when one uses floatingpoint feature weights. Our preliminary experiments indicate that the partial derivatives of
J(P, w ) with respect to major and minor feature weights differ by more than seven orders
w that is proportional to the gradient vector may
of magnitude. Thus, an update vector ‚àÜw
not be appropriate for updating the minor feature weights with a small step. Thus, the step
size of each component in a weight vector should be fixed as in the grid-adjacent update,
or it might be able to be controlled in other ways (see, e.g., Duchi, Hazan, & Singer, 2011).
3.5 Combination of Techniques and Practical Issues
MMTO is a combination of the above-described techniques. This subsection discusses the
practical issues of this combination and its alternatives; some relate to external constraints
on learning (e.g., how many weeks we can wait for results), and some depend on the properties of the domain to which MMTO is applied.
3.5.1 Lagrange Multiplier in Grid-Adjacent Update
For numerical stability, MMTO explores a restricted parameter space where the constraint
w ) = 0. To do this, the Lagrange multiplier Œª0 in JC (w
w ) is set to the
is satisfied, i.e., JC (w
w)
‚àÇJ(P,w
0
median of the partial derivatives { ‚àÇwi | wi ‚àà w } in order to maintain the constraint
w 0 ) = 0 in each iteration. As a result, ‚àÜwi0 is h for n feature weights, ‚àíh for n feature
g(w
weights, and 0 in one feature weight, where the number of feature weights in w 0 is 2n + 1.
w ) is constant in all iterations.
On the other hand, Œª1 in the regularization term JR (w
536

Large-Scale Optimization for Evaluation Functions with Minimax Search

3.5.2 Search Depth
The game tree searches in step 1 in Figure 2 are the most time-consuming step in MMTO.
Tesauro (2001) has shown that the use of a quiescence search yields better evaluation
functions. Thus, it is expected that deeper searches in MMTO will yield better evaluation
functions. On the other hand, we must handle a large amount of training positions, and the
search time tends to grow exponentially when we increase the search depth. Therefore, most
of our experiments use a 1-ply standard search together with a quiescence search. Here, the
quiescence search is called at every frontier node of the standard search. We observed that
evaluation functions learned with shallow searches are still effective for playing games with
deep searches (see Section 4.4). Similar results were reported by Tesauro.
3.5.3 Reuse of PV for Efficiency of Learning
Because step 1 in Figure 2 is the most time-consuming part, it is worth considering omitting
it by assuming œÄwp (t) = œÄwp (t‚àí1) with a certain frequency. In our experiments, steps 2 and 3
were repeated 32 times without running step 1. We counted the number of iterations in the
run of step 1. That is, each iteration ran a single step 1 and 32 pairs of steps 2 and 3. The
number 32 would be domain dependent and should be set small enough so that the update
does not change the PV for most positions.
3.5.4 Pruning of Trees
Pruning techniques can dramatically reduce the number of searched nodes and hence speed
up learning. Fortunately, Œ±Œ≤ pruning does not introduce any discontinuities in the objective
function. On the other hand, other pruning methods, including futility pruning (Schaeffer,
1986), may introduce discontinuities (see Appendix A.4). Therefore, the robustness of the
whole learning procedure should be examined when such pruning techniques are used. As
far as the authors‚Äô experience goes, the objective function with futility pruning seems to be
continuous (see Section 4.5).
3.5.5 Convergence and Performance Measurement
The termination criteria is usually difficult to determine in iterative computations. In the
case of learning the shogi evaluation function, the convergence of the objective function in
MMTO seems to be a significant criteria, because the rate of agreement with the test set
and the Elo rating of the learned evaluation function also converge when it converges. Note
that the rate of agreement should be measured on a separate test set from the training set
in order to detect overfitting (see Section 4.3).
3.5.6 Duplication in Positions and Alternative Moves
Game records usually have duplications in the positions and desired moves in the opening
phase. Although the ideal distributions of these positions and desired moves are unknown,
we decided to remove the duplications from the training and test sets for simplicity. That
is, we use each pair of hposition, movei at most once in each iteration. These duplications
are detected by Zobrist hashing (1990). Note that two or more different moves may be
suggested for the same position in the training and test sets, and the objective function
537

Hoki & Kaneko

becomes smaller if the tree search rooted at the position matches one of these moves. As
a result, conflicting goals such as ‚Äúmove a should be better than move b and vice versa‚Äù
are independently augmented to the objective function and cancel each other when both
moves can be played in the same position. In our experience, this adaptation seems to work
reasonably well in shogi, but the best solution may depend on the target game.

4. Experiments
We evaluated the effectiveness of MMTO in experiments in which the number of feature
weights in the evaluation function was varied from thirteen to about forty million. We
found that MMTO works better than comparison training and its intuitive modifications
in terms of the rate of agreement, speed of convergence, and game-playing strength. We
w ) and regularization term JR (w
w ) help to inalso observed that the constraint term JC (w
crease the performance of the evaluation functions in terms of the rate of agreement with
the test set. To see numerical convergence, we investigated the surfaces of the objective
function in MMTO with a limited number of feature weights and experimentally found that
MMTO finds local minima in a reasonable range of feature weights. Finally, we carried out
preliminary experiments on chess as well as experiments on data quality dependence.
4.1 Setup: Evaluation Functions, Features, and Game Records
Most of the experiments described in this section used Bonanza, whose source code is
available online (Hoki & Muramatsu, 2012). The performance of Bonanza in major tournaments is discussed in Section 4.6. Bonanza uses techniques such as MMTO, PVS (Pearl,
1980; Marsland & Campbell, 1982; Reinefeld, 1983), a capture search at frontier nodes as
a quiescence search, transposition tables (Zobrist, 1990; Russell & Norvig, 2002), static exchange evaluation (Reul, 2010), killer and history heuristics (Akl & Newborn, 1977; Schaeffer, 1989), null move pruning (Adelson-Velskiy, Arlazarov, & Donskoy, 1975; Heinz, 1999),
futility pruning (Schaeffer, 1986; Heinz, 1998), and late move reductions (Romstad, 2010).
It also uses an opening-book database from which we randomly chose the opening lines
of self-play experiments. The game records in the training and test sets were exclusively
chosen from games played in famous tournaments2 . There were 48, 566 game records in
total. More than 30, 000 of the games were played by professional players using standard
time controls, i.e., from one to ten hours for a side with a ‚Äúbyoyomi‚Äù period (once the time
2. The abbreviated tournament name, number of games we used, and date range of the games are: Juni,
12827, 1946‚Äì2010; Kisei, 3286, 1962‚Äì2010; Ryuo, 3279, 1987‚Äì2010; Osho, 2286, 1950‚Äì2010; Oui, 2017,
1959‚Äì2010; Ouza, 1849, 1952‚Äì2010; NHK-cup, 1745, 1951‚Äì2010; Ginga, 1735, 1991‚Äì2010; Kio, 1620,
1973‚Äì2010; Shinjino, 1332, 1969‚Äì2010; Zen-nihon-proshogi, 1160, 1982‚Äì2001; Hayazashi-shogi-senshuken,
945, 1972‚Äì2003; Judan, 764, 1962‚Äì1987; Meisho, 752, 1973‚Äì1989; Joryu-meijin, 608, 1974‚Äì2010; Meijin,
551, 1935‚Äì2010; All-star-kachinuki, 545, 1978‚Äì2003; Rating-senshuken, 476, 1987‚Äì2007; Asahi-open,
429, 2001‚Äì2007; Heisei-saikyo, 412, 1992‚Äì2007; Teno, 351, 1984‚Äì1992; Joryu-osho, 351, 1979‚Äì2010;
Kurashiki-touka, 314, 1993‚Äì2010; Nihon-series, 304, 1981‚Äì2010; 3-dan-league, 283, 1963‚Äì2009; Ladiesopen, 255, 1987‚Äì2007; Joryu-oui, 253, 1990‚Äì2010; Shoureikai, 217, 1941‚Äì2008; Gakusei-osho, 212, 1972‚Äì
2006; Hayazashi-shinei, 206, 1982‚Äì2002; Gakusei-ouza, 191, 2001‚Äì2006; Asahi-amashogi, 187, 1980‚Äì2009;
Wakajishi, 183, 1953‚Äì1991; Kudan, 182, 1947‚Äì1961; Gakusei-meijin, 177, 1972‚Äì2006; Shogi-Renmei-cup,
172, 1967‚Äì1984; Tatsujin, 160, 1993‚Äì2010; Kinsho-cup, 156, 2002‚Äì2005; Amateur-meijin, 146, 1948‚Äì2009;
Kashima-cup, 119, 1996‚Äì2006; Grand-champion, 111, 1981‚Äì2008; Saikyosya-kettei, 101, 1954‚Äì1973; Miscellaneous, 5317, 1607‚Äì2010.

538

Large-Scale Optimization for Evaluation Functions with Minimax Search

evaluation function
13
X
 A

A
e
fi (p) ‚àí fiA (pÃÉ) wiA
i=1 
X
 B
B
B
eB
fkj
(p) ‚àí fkj
(pÃÉ) wkj
eC
eD

k,j
X

0 ,l
k,k
X

dimension
13
60, 876

 C
C
C
fkk
0 l (p) ‚àí fkk 0 l (pÃÉ) wkk 0 l

2, 425, 950

 D
 D
D
fkjj 0 (p) ‚àí fkjj
0 (pÃÉ) wkjj 0

44, 222, 454

k,j‚â§j 0

Table 2: Dimensions of evaluation functions. Each evaluation function is a linear combination of weighted features. eA evaluates the material balance, and the others
evaluate a variety of positional scores by using extended piece-square tables.

had expired, a player had to move within sixty seconds). Some tournaments employed rapid
time controls such as 30 seconds per move and had top-level amateur players as participants.
Table 2 shows the four basic evaluation functions, where eA is for material balance and
the others are for positional scores. Our experiments used the sum of these functions, i.e.,
eA , eAB = eA + eB , eABC = eAB + eC , and eABCD = eABC + eD . All evaluation functions are
anti-symmetric with respect to the exchange of black and white: e(p, w ) = ‚àí e(pÃÉ, w ). Here,
pÃÉ is a complete reversal of the black and white sides at position p; that is, black plays for
white and white plays for black3 . After this reversal, the pieces owned by black and white in
p are regarded as white and black pieces in pÃÉ, respectively. Also, all evaluation functions are
symmetric with respect to right-and-left mirroring of a position: e(p, w ) = e(pÃÇ, w ), where pÃÇ
is the mirror image of p along file e.
The function eA (p, w A ) was used to evaluate the material balance. There are 13 types
of pieces in shogi (Iida et al., 2002). Each feature fiA (p) represents the number of the i-th
type owned by black in position p, and wiA is the relative value of the i-th type of piece.
The partial derivative of the evaluation function with respect to wiA is ‚àÇ eA (p, w A )/‚àÇwiA =
fiA (p) ‚àí fiA (pÃÉ).
The function eB (p, w B ) is a linear combination of weighted two-piece-square features.
These are natural extensions of one-piece-square features that were employed in recent
machine learning studies of chess evaluations (Baxter et al., 2000; Tesauro, 2001; Veness
et al., 2009). These two-piece-square features were used to evaluate all conditions of the
B (p) is an indicator function that returns one if
king and another piece. Each feature fkj
both of the conditions k and j exist in position p. Otherwise, it returns zero. Condition
k represents the location of the black king (there are 81 squares), and j represents the
type, owner (black or white), and location of the other piece. There were 1, 476 different
conditions for j after some of the minor conditions were merged. Thus, the total number of
the king‚Äìpiece conditions was 81 ¬∑ 1, 476 = 119, 556 before the mirror symmetric conditions
were merged.
3. Following shogi notation, black and white refer to the players who plays first and second, respectively.

539

Hoki & Kaneko

Similarly, the functions eC (p, w C ) and eD (p, w D ) were used to evaluate the king‚Äìking‚Äì
C (p)
piece features and king‚Äìpiece‚Äìpiece features, respectively. The indicator function fkk
0l
represents the location of the two kings (k, k 0 ) and the condition (type and location) of a
D (p) represents the location of black king k and
black piece l. The indicator function fkjj
0
the conditions of the other two black or white pieces (j, j 0 ).
Game tree searches are required to identify PV leaf positions for MMTO and to obtain
best moves to measure the rate of agreement. For these purposes, a nominal depth 1
search was used together with the quiescence search. To normalize the objective function
values, the objective function values were divided by the total number of move pairs, Z P =
P
0
p‚ààP |Mp |. The constraint function was set to
A

w )=
g(w

13
X

!
wiA

‚àí 6, 500.

(9)

i=1

Also, in accordance with the magnitude of the constraint, a in the horizontally mirrored
sigmoid function T (x) = 1/(1 + exp(ax)) was set to 0.0273 so that T (x) would vary signifiw ) = 0.00625 ¬∑ (|w
wB| +
cantly if x changed by a hundred. The regularization term was JR (w
C
D
w | + |w
w |). An intuitive explanation of the penalty strength is that the absolute value
|w
of wi can be increased to 160 if doing so improves the relationship between the evaluation
values of a desired move and another legal move. The sums in eB , eC , and eD were computed
using 32-bit integers, and they were divided by 25 in order to fit the evaluation value into
a 16-bit integer. Step h in the grid-adjacent update was set to the smallest integer value 1.
4.2 Learning the Piece Values
First, the feature weights w = w A of the evaluation function eA were adjusted by MMTO
or by comparison training, starting from the same initial value wiA = 500 for all i. Tesauro
(2001) used floating-point feature weights and the conventional gradient descent method.
That is, the weight vector w was updated by
P
w ),
w (t) = w (t) ‚àí r‚àáw Jct
(w

(10)

where r is a constant training rate hand-tuned to 0.5. The components in w used in the tree
search were rounded to the nearest integer values. The rescaling factor R in Eq. (3) was set
p .d
to 0.0025 in accordance with the range of the difference | e(œÄw p , w ) ‚àí e(œÄwp.m , w )| from 50 to
5, 000. Because this experiment had only 13 piece values to be adjusted in w A , only 1, 000
game records were used to compose the training set P. The set had 101,898 desired moves
and Z P = 7, 936, 180 move pairs after removing duplications and handicapped games.
One problem observed in the comparison training was slow learning: as shown in Figure 3, phase I of the iterative procedure (from iteration 1 to 10) is mainly for adjusting
the pawn value, because the partial differential value of Eq. (3) for pawns is the largest
in this phase. After a good pawn value is found, phase II (from iteration 10 to 100) is
mainly for adjusting the promoted rook and promoted bishop values. These values should
be the highest and second highest for reasonable game play. The long period of time taken
w ) in Eq. (3) scales poorly. This is a general problem in
by phase II indicates that JctP (w
gradient descent methods with multiple degrees of freedom (Nocedal & Wright, 2006), and
540

Large-Scale Optimization for Evaluation Functions with Minimax Search

pro_rook

2500

Phase II

Phase I

Phase III
pro_bishop

Piece weight

2000
gold
bishop
rook
pro_pawn
pro_knight
silver
pro_silver
pro_lance
knight
lance

1500

1000

500

pawn
0

2

1

3

4 5 6

2

3

4 5 6

10

2

3

4 5 6

100

1000

Iteration

Figure 3: Results of comparison training of the piece weights in shogi. The horizontal axis
plots the number of iterations on a logarithmic scale.

to cope with it, the learning rate r cannot be greater than 0.5 in accordance with the largest
partial derivative in these experiments.
The second problem was about convergence: in phase III (after iteration 100) of Figure 3,
all piece values keep increasing without changing the ratio of piece values, even though the
relative ratios of the piece values have room for improvement. This problem is inherent
to the objective function of comparison training, because Eq. (3) has no explicit term
to avoid it. In an extreme case where the training data satisfy the inequality condition
p .d
e(œÄw p , w ) ‚â• e(œÄwp.m , w ) for all moves m in any position p, all piece values diverge to infinity
w ) is minimized. In fact, it was found that the training data in this
when the value JctP (w
experiment satisfied the condition for 94% of the pairs of the best and another legal move.
Moreover, in the other extreme case, where the training does not satisfy the inequality
condition for any move m in any position p in Eq. (3), all piece values shrink to zero.
MMTO deals with these problems by making grid-adjacent updates and keeping the
w ). The weighted vector w A converged
magnitudes constant through its constraint term JC (w
in 40 iterations (see Figure 4); the value of the promoted rook was 945 and that of the
pawn was 122. Note that the number of iterations was counted as the number of step (1)s
throughout the experiments.
4.3 Scalability in Learning Practical Evaluation Functions
After learning the piece values, we adjusted the weight vectors for positional scores. This
time, a large number of training records were used to cope with high-dimensional weight
vectors. The main training set P had 4, 467, 116 desired moves and Z P = 368, 313, 024
541

Hoki & Kaneko

pro_rook
pro_bishop
rook
bishop
gold
pro_knight
silver
pro_pawn
pro_lance
pro_silver
knight
lance
pawn

Piece weight

800

600

400

200
2

1

3

4

5 6 7 8

2

10
Iteration

3

4

5 6 7 8

100

Figure 4: Results of MMTO for the piece weights.
move pairs after removing duplications and handicap games from the 47, 566 game records.
The test set had 103, 105 desired moves after removing duplications and handicap games
from another 1, 000 game records. The feature weights for eAB were adjusted with MMTO
and comparison training and its intuitive modifications. The same initial feature weights
w A (0), w B (0)) were used in all three methods; w A (0) was optimized by MMTO from the
(w
previous experiment, and w B (0) = 0. After that, to show scalability, the feature weights for
eABC and eABCD were optimized by MMTO in this order. To adjust the feature weights for
eABC , the optimized feature weights of eAB were used for the initial feature weights w A (0)
and w B (0), and 0 was used for w C (0). Similarly, in eABCD , the optimized feature weights
in eABC were used for the initial feature weights w A (0), w B (0), and w C (0), and 0 was used
for w D (0).
Comparison training with eABC and eABCD was not tested because learning eAB yielded
only small improvements. The rate r in Eq. (10) was hand tuned to 0.031. As an example
of the intuitive modifications to stabilize the iterative procedure, a constant-step update
was also tested for learning eAB . In this case, the training rate r0 (t) was substituted for r
and
P
w (t))|,
r0 (t) = rc /|‚àáw (t) Jct
(w
where this constant-step modification conservatively updated w by using a constant step rc
that was hand tuned to 1, 000. Each value of r and rc was the best of five trials. Another
intuitive modification was the reuse of PV, as explained in Section 3.5, where the same PVs
were used 32 times and the rate r in Eq. (10) was 0.01. The rescaling factor R in Eq. (3)
was set to 0.0025, because this value was satisfactory in the previous experiment shown in
Figure 3. Although the three methods are different, their iterations consumed almost the
same amount of time. This is because the most time-consuming step of these experiments
was the game tree search to identify the PV leaf positions.
The rate of agreement with the test set is shown in Figure 5. Here, agreement means that
the legal move that obtained the highest value in the tree search is the desired move. The
542

Large-Scale Optimization for Evaluation Functions with Minimax Search

0

b2_c2_h6
b2_c2_b4

35

-20
d2_b2_f3
-40

b2_c2_b3
d2_f3_c4

-60

Positional weight

Agreement (%)

30

25

ABCD

MMTO (e
)
ABC
MMTO (e )
AB
MMTO (e )
AB
CT (e , reuse of PV)
AB
CT (e , constant step)
AB
CT (e )

20

15

2

1

3

4

5 6 7 8

2

10

3

4

5 6 7 8

b2_c2_a3
d2_f3_b3

-80

b2_b1_c2
-100

b2_c2_a2

-120

-140
b2_d1_c2

2

100

Iteration

0

50

100
150
Iteration

200

Figure 5: (Left panel) Improvement in rate of agreement with the test set for MMTO
and comparison training (CT). (Right panel) Improvement in feature weights for
positional features in eD . Feature weight b2 c2 b4 indicates the black king is at
b2 and two gold generals are at c2 and b4. Similarly, feature weights b2 c2 h6,
b2 c2 b3, b2 c2 a3, b2 b1 c2, b2 c2 a2, and b2 d1 c2 indicate two gold generals
with the king at b2. Feature weight d2 b2 f3 indicates the black king is at d2
and the opponent‚Äôs two gold generals are at b2 and f3. Similarly, feature weights
d2 f3 b3 and d2 f3 c4 indicate the opponent‚Äôs two gold generals with the king at
d2. Here, each value has been divided by 25 .

rate is calculated by excluding positions in which there is only one legal move, or positions
in which there is an easy checkmate sequence that can be identified in the shallow-depth
search. Tied values are not counted, either.
The performances of MMTO, comparison training, and its variations were compared in
the case of learning eAB . We can see from the figure that the agreement rates of comparison
training and constant-step modification are unstable and substantially lower than that of
MMTO. We can also see that the reuse-of-PV modification increases stability because it
reduces the step length from 0.031 to 0.01 and reduces the computation time of learning by
almost 32 times because it reduces the number of time-consuming PV updates.
MMTO with the full evaluation function eABCD had the highest rate (37%). The largescale optimization of the weight vector wD increased the level of agreement in 200 iterations.
543

Hoki & Kaneko

without constraint
with constraint

Pawn value

140
120
100
2

1

3 4 5 6

2

10
Iteration

3 4 5 6

2

100

Figure 6: Effect of the constraint term in MMTO (eAB ).
This computation took about a week using an Intel X5690 workstation. The agreement ratio
with the test set converged in 100 iterations. However, the feature weights did not converge.
w ) in Eq. (9) improves the stability of MMTO in
Figure 6 shows how the constraint JC (w
response to pawn-value changes during the eAB learning. We can see that the value keeps
w ) is turned off and that it converges in 100 iterations with the
on increasing when JC (w
constraint turned on. One of the feature weights overflowed in the comparison training of
eABC , and this is another reason why the results of eABC are not shown for comparison
w ) has little effect on the learning of eAB , the
training. As the regularization term JR (w
improvement in the agreement rates of MMTO is mainly due to the use of the constraint
w ) and grid-adjacent updates.
JC (w
w ) is important for optimizing larger weight vectors. FigThe regularization term JR (w
w ) in Eq. (4) improves the weight vector in the enlarged evaluation
ure 7 shows how JR (w
function eABCD . Without a regularization term, the objective function value and the rate
of agreement with the training set increase with the number of iterations. However, there
is also a linear increment in the absolute value of the weight vectors, and it distorts the
rate of agreement with the test set after the 50-th iteration. After the 200-th iteration,
only 0.2% of the components in w are zero. On the other hand, 96.3% of the components
in w are zero with the regularization term. These results indicate that MMTO without the
regularization suffers from overfitting of the training set when a large-scale weight vector
is used. A similar effect of regularization also occurs when MMTO is used in the eABC
learning, though the effect is smaller than that of eABCD .
4.4 Improvements in Strength
To analyze the relationship between the agreement rate and the strength, we had the programs learned by MMTO and by comparison training (Figure 5) play games against a
reference shogi program many times. The reference program was a version of GPS Shogi
released in 2008 (Kaneko, 2009). It is an open source program and was a finalist in past
world computer shogi championships. It has a completely different evaluation function in
which the majority of the parameters have been hand tuned. This version of GPS Shogi
serves as a reference program on a popular game server for shogi programs4 . The matches
were as follows: the reference program (4 ¬∑ 105 nodes/move) vs. all four learned programs,
4. http://wdoor.c.u-tokyo.ac.jp/shogi/, last access: 2013 (in Japanese).

544

Objective function

Large-Scale Optimization for Evaluation Functions with Minimax Search

0.06
0.05
0.04
0.03
0.02

Agreement (%)

60

with l1-regularization
without l1-regularization

55
50

with training set

45

with test set

40

D

10

B

|œâ |+|œâ |+|œâ |

10

C

35
11
10

10

9

2

1

3

4 5 6

2

10

3

4 5 6

2

100

Iteration

Figure 7: Effect of the regularization term in MMTO (eABCD ).
the reference program (2¬∑105 nodes/move) vs. the two learned programs with the evaluation
function eA and eAB , and the reference program (8 ¬∑ 105 nodes/move) vs. the two learned
programs with the evaluation functions eABC and eABCD . 500 games were played in each
match. The weight vectors obtained after 1, 2, 4, 8, 16, 32, 48, 64, 80, 96, 112, 128, and 144
iterations were tested for each learning configuration. Thus, a total of 8 ¬∑ 13 ¬∑ 500 = 52, 000
games were played. The learned programs searched 4 ¬∑ 105 nodes per move. All programs
ran on a single thread and searched similar numbers of nodes in a second.
We measured the playing strength in terms of the Elo rating, which is a popular way
to represent relative strength in two-player games. The winning probability between two
players is estimated as 1/(1 + 10d/400 ), where d is the difference between their ratings. For
example, if the rating of player A is higher than that of player B by 150, the winning
percentage of player A is about 70%. Here, the ratings were determined by using maximum
likelihood estimation on all the games.
Figure 8 shows the Elo rating of each player. We can see that MMTO with eAB significantly outperformed comparison training with the same initial feature weights. When
MMTO used eAB , the winning percentage against the reference (400k/move) stably increased from 11.2% (1, 800 Elo points) to 59.4% (2, 210 Elo points). In contrast, comparison
545

Elo Rating

Hoki & Kaneko

2500
2400
2300
2200
2100
2000
1900
1800
1700
1600
1500

MMTO (ABCD)
MMTO (ABC)
MMTO (AB)
Comparison training (AB)
Reference (800k)
Reference (400k)
Reference (200k)

1

10
Iteration

100

Figure 8: Improvements in strength (Elo rating) achieved by MMTO and comparison training.

Opponent
Player 1
Player 2

Depth 2
94 ¬± 2%
77 ¬± 3%

Depth 3
89 ¬± 3%
75 ¬± 3%

Depth 4
82 ¬± 3%
77 ¬± 3%

Depth 5
85 ¬± 3%
82 ¬± 3%

Depth 6
78 ¬± 3%
81 ¬± 3%

Depth 7
81 ¬± 3%
85 ¬± 3%

Depth 8
78 ¬± 3%
84 ¬± 3%

Table 3: Winning percentages of program learned with game tree search having various
depths. Opponent player 1 is the same program but the search depth is reduced
by 1, and opponent player 2 is also the same program but it uses the weight vector
before the learning.

training won at most 19.5% (1, 910 Elo points) of its games. The results shown in Figs. 5
and 8 indicate that MMTO outperforms comparison training.
The large number of features also contributed to the playing strength of the programs
learned by MMTO. Although eABC showed a small improvement in terms of the agreement
rate and Elo rating, eABCD consistently yielded significant improvements in these two criteria. Thus, we concluded that MMTO scales well to forty million features. Note that the
computational cost of eABCD was reasonably small for practical game play. This is because
the number of features that appear in a position is only 2, 800 or less even when the total
number of features is about forty million. Also, the summations in Table 2 can be maintained in an incremental manner when the program makes or unmakes a move. This sort
of feature design is similar to that of a famous Othello program (Buro, 2002). As a result,
Bonanza using eABCD searched about 3 ¬∑ 106 nodes/sec on an Intel Xeon X5680 with 12
threads. The speed itself is slower than that of many chess programs, but about average
for strong shogi programs. In addition, we found that Bonanza using eABCD trained by
MMTO played better than or was comparable to any of the top shogi programs in actual
tournaments. The details are discussed in Section 4.6.
Two additional fixed-depth self-play experiments were conducted to see if evaluation
functions trained by using shallow searches (depth 1 with quiescence search) are effective on
deep searches. Table 3 shows the winning percentages of the learned program with various
546

Large-Scale Optimization for Evaluation Functions with Minimax Search

search depths of game play. The learned program had the eABCD evaluation function yielded
after the 200-th iteration (Figure 5). The winning percentages against the same program
(player 1) with the search depth reduced by 1 were around 80%. Thus, we see that the
deeper the learned program searched, the stronger the program was. Tesauro (2001) also
reported similar results by using comparison training. In addition, the winning percentage
was about 80% against a program (player 2) that searched to the same depth but used eABC
after the 200-th iteration. Thus, the use of eABCD trained by 200 iterations was effective
even when the program searched deeper. Here, the winning percentages were computed for
a thousand games (Seventy-six games or less ending in draws or exceeding 300 moves were
not counted). Fifty megabytes of memory were assigned to the transposition table of each
program. The uncertainties indicated as ¬±3 was estimated by conducting a two-sided test
at a significance level of 5% on the one-thousand games.
4.5 Numerical Stability and Convergence
We investigated the continuity and partial differentiability of the objective function and
convergence of the feature weights in an empirical manner. While forward pruning techniques in game tree searches can speed up MMTO in practical applications, such methods
do not always maintain continuous search values, as is shown in Appendix A.4. Moreover,
the objective function contains a large number of search values. This means it is difficult
to estimate its properties in a theoretical manner.
To make the empirical investigation manageable, we used only the smallest evaluation
function eA that deals with thirteen shogi piece values. Moreover, we reduced the number
of game records to 1, 000; the game records had 98, 224 desired moves and Z P = 7, 900, 993
move pairs after removing duplications and handicapped games.
4.5.1 Surface of the Objective Function
We investigated the function surface of the main part of the objective function J(P, w A )
of MMTO in Eq. (4) by generating contour maps from millions of sampling vectors w .
Note that a contour line (isovalue surface) is a curve along which the functions take the
same value. The contour lines have certain properties: the gradient of the function is
perpendicular to the lines, and the magnitude of the gradient is large when two lines are
close together. In addition, a closed-loop contour line indicates the location of the local
minimum or maximum.
Two of the thirteen piece values in the weight vector w A were sampled in order to draw
the contour maps of two-dimensional functions at an interval of 5 for each piece value.
The remaining eleven pieces were assigned reasonable values; 118 (pawn), 273 (lance), 318
(knight), 477 (silver general), 562 (gold general), 620 (bishop), 734 (rook), 485 (promoted
pawn), 387 (promoted lance), 445 (promoted knight), 343 (promoted silver), 781 (promoted
w A ) was ignored so that w A
bishop), and 957 (promoted rook). The constraint term JC (w
w ) was turned off for piece values.
could be freely changed and the regularization term JR (w
A nominal depth 1 search, together with the quiescence search, was used.
We analyzed two pairs: hgold, bishopi, and hpawn, promoted lancei. Figure 9 shows
an enlargement of the contour map of J(P, wgold , wbishop ). The contour interval is 5 ¬∑ 10‚àí4 .
The map was computed with the ranges of [200, 1100] for the bishop and [100, 930] for
547

Hoki & Kaneko

(3) bishop=pro_bishop

(4) bishop=dragon
(5) gold=bishop

(2) bishop=rook

(1) bishop=silver

Gold general weight

(6) gold=pro_bishop

(8) gold=rook
700
600

x x

500
400
400

Objective function

(7) gold=pro_rook

800

0.164

(9) gold=silver
500

(1) (5)

600 700 800
Bishop weight

(2)

(9)

0.162
0.160

900

(5) (6)
(8)

(7)

(3) (4)

0.158
400 500 600 700 800
Bishop weight

500 600 700 800 900
Gold general weight

Figure 9: (Upper panel) Enlarged contour map of J(P, wgold , wbishop ). The dashed lines
indicate the critical boundaries at which the two-dimensional function is not
partially differentiable. The two minima are indicated by x. (Bottom panel)
Cross sections of the contour map. The left one shows the intersection of the
map with the line wgold = 560, and the other shows that of wbishop = 620.

the gold general. Note that the function simply increases and there are no interesting
structures outside of the enlarged map. Figure 10 shows an enlargement of the contour
map of J(P, wpawn , wpro lance ). The contour interval is 1 ¬∑ 10‚àí3 . The map was computed
with the ranges of [10, 500] for a pawn and [200, 700] for a promoted lance.
We can see from these maps that there are local minima within reasonable ranges
and no sudden changes in the function values. Although the function depends on a large
number of empirical search values, s(p, w ), it is approximately continuous and amenable to
optimization on the basis of gradients approximated by MMTO.
On the other hand, the maps illustrate three difficulties. The first difficulty is the clear
edges of the contour lines. They indicate that the function is not partially differentiable at
the points on these edges. The dashed lines in these maps are critical boundaries at which
the profit and loss ratio of material exchanges inverts itself. For example, a silver is usually
548

Large-Scale Optimization for Evaluation Functions with Minimax Search

(1) pro_lance=lance (2) pro_lance=knight (3) pro_lance=silver
(4) pawn=silver

Pawn weight

400
(5) pawn=knight
300

(6) pawn=lance
(7) lance promotion=pawn

200
X

100

200

300

400

X

500

600

700

Objective function

Promoted lance weight
0.1584
0.1583
(2) (7) (3)
0.1582
0.1581 (1)
0.1580
0.1579
200 300 400 500 600 700
Promoted lance weight

0.20
0.19
0.18
0.17
0.16

(7) (6) (5)
(4)

100 200 300 400 500
Pawn weight

Figure 10: (Upper panel) Enlarged contour map of J(P, wpawn , wpro lance ). The dashed
lines indicate critical boundaries at which the two-dimensional function is not
partially differentiable. The two minima are indicated by x. (Bottom panel)
Cross sections of the contour map. The left one shows the intersection of the
map with the line of wpawn = 125, and the other shows that of wpro lance = 450.

less valuable than a bishop, but capturing a silver becomes more profitable than capturing a
bishop when the bishop value is smaller than 477. This boundary is labeled ‚Äúbishop=silver‚Äù
in Figure 9. As discussed in Appendix A, the function is not always partially differentiable at
these critical boundaries, where multiple moves share the same best value. Note that there
can be more boundaries in theory, e.g., a ‚Äúbishop=promoted knight‚Äù boundary. Whether a
boundary is visible or not depends on the training set and evaluation features. In addition,
the boundaries become winding curves when a non-linear evaluation function is used instead
of a linear weighted sum.
The second difficulty, the scaling problem, is illustrated in Figure 10. In this map, we
can see that the scales of the two piece values differ by two orders of magnitude. That
is, a pawn-value variation of five hundred changes the function value by 0.04, whereas a
promoted-lance-value variation of five hundred changes the function value by only 4 ¬∑ 10‚àí4 .
Because of the difference in scaling, the surface along a promoted lance is almost flat. This
property explains why the pawn value is optimized earlier than those of the other pieces in
comparison training, as shown in Figure 3. This property of ill-scaling is disadvantageous
when it comes to optimizing the promoted-lance value using a naive gradient decent method.
549

Hoki & Kaneko

Methods based on second-order partial derivatives or approximations of the Hessian matrix
can resolve this problem; however, they behave poorly at non-partially differentiable points
on many boundaries. These two difficulties point to why the grid-adjacent update in MMTO
is effective.
The third difficulty is that there are multiple local minima in the two maps. This
means the results of MMTO depend on the initial values and there is a chance of ending
up with a local rather than global minimum. We will investigate this problem in the next
subsection 4.5.2.
4.5.2 Empirical Convergence and Local-Minima Properties
In the previous subsection, we examined two-dimensional cross sections of the function
J(P, w A ). In this subsection, we loosen the restriction from two to thirteen dimensions,
which is sufficiently large to express all piece values in shogi. The aim of this experiment
is to catch a glimpse of the global map and numerical convergences for arbitrary initial
guesses about the values of all of the pieces.
For this purpose, a Monte Carlo sampling of the initial guess, w A (0), was carried out to
enumerate the local minima and analyze the optimized vectors. We ran 444 MMTO with
randomized initial values. Here, a uniformly distributed integer in the range of [0, 32767]
was assigned to each vector component, and the resulting vector was scaled to satisfy the
w A ) = 0.
equality condition g(w
Figure 11 shows the cosine similarity and objective-function value of a hundred of the 444
runs. Here, the cosine similarity of a weight vector is measured relative to the best vector
whose objective function is the smallest among those of 444 vectors after 100 iterations.
In the majority of the runs, we can see that function values and weight vectors converged
numerically in 50 iterations. Here, we regard the iteration procedure to have converged
when the function values and similarities oscillate and show neither steady increase nor
decrease from the 50-th to 100-th iteration. Although convergence is almost assured for
MMTO with thirteen piece values, it would be difficult to achieve if more feature weights
were to be optimized. For example, Figure 5 shows there was no convergence after twothousand iterations using eABCD . Because 200 iterations took about a week on an Intel
X5690 workstation, we could not afford to investigate the convergence of eABCD with the
current hardware. However, 200 iterations nonetheless achieved a significant improvement
in strength, as shown in Figure 8.
We can also see that these trials of MMTO ended up with multiple local minima.
Although a multiplicity of minima is generally undesirable in an optimization, there were
other, more favorable properties. The first property is that each run of MMTO changed
the weight-vector components by a sufficient amount. That is, the cosine similarity of the
444 optimized vectors was localized in the range of [0.925, 1], while that of the random
initial vectors were widely spread (see the top panel of Figure 12). The second property is
that there was a weak correlation between the cosine similarities of the initial and optimized
vectors. This means that starting from a better initial vector in terms of the cosine similarity
should be beneficial (see the top panel of Figure 12). However, starting from a better initial
vector in terms of the objective function value is not beneficial (see the middle panel of
Figure 12). The third is that the distribution of local minima formed structures (see the
550

Large-Scale Optimization for Evaluation Functions with Minimax Search

0.95
0.90
0.85
0.80
0.98

Similarity

Cosine similarity of weight vector

1.00

0.75
0.70

0.97
0.96
0.95

Objective function

0.65

Objective function

0.30

0.170
0.165

0.25
0.160
60 80
Iteration

0.20

2

1

3

4

5 6 7 89

10
Iteration

2

3

4

5 6 7 89

100

Figure 11: A hundred runs of MMTO for a weight vector w A consisting of thirteen piece
values. The initial vectors were set using pseudo-random numbers. The inset
is an enlargement showing the appearance of the numerical convergences. The
top panel shows the cosine similarities relative to the best weight vector. The
bottom panel shows the values of the objective function.

bottom panel of Figure 12). That is, the lower the local minimum is, the more similar it
becomes to the best vector. Moreover, the number of local minima decreases as the weight
vector gets farther away from the best.
We also investigated the dependence of the performance on the nominal search depth
of step (1) shown in Figure 2. Similar results in terms of convergence and the distribution
of local minima were obtained using a deeper search with a nominal depth of 2. Because
MMTO with a depth of 2 consumes more time than MMTO with a depth of 1, the number
551

Initial objective function

Cosine similarity of initial vector

Hoki & Kaneko

1.0
0.9
0.8
0.7
0.6
0.35

corr = 0.27

0.30

0.25

0.20
corr = -0.06

Optimized objective function

0.180
0.175
0.170
0.165
0.160
corr = -0.55
0.94

0.96

0.98

1.00

Cosine similarity of optimized vector

Figure 12: Scatter plots for 444 trials of thirteen-dimensional weight vectors. The vector
expresses thirteen piece values. The cosine similarity of the vector is measured
relative to the best vector. The initial vector consists of uniform pseudo-random
numbers, and the optimized one is the 100-th vector of the MMTO iterations
starting from the initial one. The inset shows the correlation coefficient of each
scatter plot.

of random initial vectors was reduced to 78, and the number of iterations was reduced
to sixty for the sake of speed. In the majority of runs, the function values and weight
vectors converged in 50 iterations. Figure 13 shows the strength (Elo rating) and objective552

Large-Scale Optimization for Evaluation Functions with Minimax Search

100
depth 1 (corr = -0.60)
depth 2 (corr = -0.86)

Elo rating

50
0
-50
-100
-150
0.150

0.155

0.160

0.165

0.170

0.175

0.180

Objective function

Figure 13: Scatter plots for thirteen-dimensional weight vectors. The 444 vectors indicated
by crosses were learned with the nominal depth 1 search of step (1), and the 78
vectors indicated by squares were learned with a depth 2 search.

function value of the 78 runs with depth 2 (squares) and 444 runs with depth 1 (crosses).
Here, the Elo ratings were identified by using maximum likelihood estimation on 894, 244
random-pairing games (5 ¬∑ 104 nodes/move). The Elo rating with depth 1 was ‚àí17 on
average and that with depth 2 was 41 on average. Also, the correlation coefficient between
the Elo rating and objective function value with depth 2 was ‚àí0.86 and that with depth
1 was ‚àí0.60. Moreover, we compared the performance of two best vectors that gave the
smallest objective function values. Here, we computed the winning probability between the
best results of depth 1 and 2. Each player was allowed to use one second for each move,
and one core of an Intel Xeon X5680 and fifty megabytes of memory were assigned to the
transposition table. After excluding two drawn games and two games exceeding a thousand
moves, we obtained a 43.6% winning rate against the program using the best results of
depth 2. These results indicate that MMTO is better with depth 2 than with depth 1.
4.6 Performance of MMTO under Tournament Conditions
MMTO was invented by the developer of Bonanza and made it one of the best programs
in shogi. Moreover, the ideas behind earlier versions of MMTO published in Japanese
(Hoki, 2006) have been adopted by many developers and have dramatically changed shogi
programs.
One of the authors started developing Bonanza in 2004, published program files on
the web in 2005, and published source codes on the web in 2009 (Hoki, 2013). This paper
gives detailed descriptions of the evaluation-function learning, whereas the literature (Hoki
& Muramatsu, 2012) gives detailed descriptions of the game-tree pruning of Bonanza. In
addition to the learning method MMTO, Bonanza uses the evaluation function eABCD
shown in Table 2. The earlier versions until 2009 used a subset of eABCD with a modified
553

Hoki & Kaneko

1
2
3
4
5

2006 May
Bonanza‚àó‚àó
YSS
KCC Shogi
TACOS
Gekisashi

2007 May
YSS
Tanase Shogi‚àó
Gekisashi
Bonanza‚àó‚àó
Bingo Shogi

2008 May
Gekisashi
Tanase Shogi‚àó
Bonanza‚àó‚àó
YSS
Bingo Shogi

2009 May
GPS Shogi‚àó‚àó
Otsuki Shogi‚àó
Monju‚àó‚àó‚àó
KCC Shogi‚àó
Bonanza‚àó‚àó‚àó

1
2
3
4
5

2010 May
Gekisashi‚àó
Shueso‚àó
GPS Shogi‚àó‚àó
Bonkras‚àó‚àó
Bonanza Feliz‚àó‚àó‚àó

2011 May
Bonkras‚àó‚àó
Bonanza‚àó‚àó‚àó
Shueso‚àó
Gekisashi‚àó
ponanza‚àó‚àó

2012 May
GPS Shogi‚àó‚àó
Puella Œ±‚àó‚àó
Tsutsukana‚àó
ponanza‚àó‚àó
Shueso‚àó

2013 May
Bonanza‚àó‚àó‚àó
ponanza‚àó‚àó
GPS Shogi‚àó‚àó
Gekisashi‚àó
NineDayFever‚àó‚àó

Table 4: Program names and results of the recent World Computer Shogi Championship.
‚àó‚àó‚àó MMTO, ‚àó‚àó an earlier version or a variant of MMTO, or ‚àó a learning method
influenced by MMTO is used.

l2-regularization (Hoki, 2006). Subsequent versions fully evaluate eABCD learned with l1regularization.
Table 4 shows the results of the World Computer Shogi Championships. Since 2006,
the performance of Bonanza has been examined in several computer shogi tournaments,
where each participant connects to a server program and plays shogi under a time control
of 25 minutes a side. Bonanza received the first prize twice, second prize once, and third
prize once. Moreover, players entitled Bonanza Feliz and Monju used the same evaluation functions as obtained by MMTO. Thus, we claim that when Bonanza uses MMTO, it
plays better than or is comparable to any of the top programs in shogi, including commercial ones. This method clearly plays at the level of handcrafted shogi programs. Moreover,
descriptions of the learning shogi evaluation functions and the earlier version of MMTO
were published by Hoki (2006) in Japanese and were quickly recognized as significant advances. In fact, no shogi program with conventional handcrafted evaluation functions has
broken into the top five in during the last five years of tournaments. One interesting case is
the results of GPS Shogi (Kaneko, 2009), the winner of the 2009 and 2012 tournaments,
and source codes are available online (Tanaka and Kaneko, 2013). From 2003 to 2008, this
program uses a handcrafted evaluation function but in 2009 it used a variant of MMTO
and its results dramatically improved. The variants of MMTO used in each program differ
in accordance with the content and policy of each program. For example, Tanase Shogi,
the runner-up program in 2008, used a learning method based on MMTO and handcrafted
evaluation functions. Bonkras, ponanza, Puella Œ±, and NineDayFever also used variants of MMTO. These excellent results make it clear that MMTO outperforms conventional
programs that use handcrafted evaluation functions and has played extremely well in recent
shogi tournaments.
554

Large-Scale Optimization for Evaluation Functions with Minimax Search

It should be noted that some versions of Bonanza add a small amount of randomness
to the grid-adjacent updates. However, we omitted any discussion of using randomness in
this paper because it is not clear whether the added randomness improved the quality of the
evaluation function or not. The source codes of various versions of Bonanza are available
online (Hoki, 2013) and the source code of MMTO are in two files, learn1.c and learn2.c.
4.7 Preliminary Experiments on Chess
So far, we have discussed the performance of MMTO in shogi. We expect that MMTO
would be effective in other two-player perfect information games provided that certain
conditions are met: (1) a sufficient number of game records are available, (2) minimax
searches guided by heuristic evaluations are effective, and (3) the analytic partial derivatives
of the evaluation function with respect to the variables are available. For example, MMTO
would not yield interesting results were it to be applied to a game that has been solved by
other means (e.g., van den Herik, Uiterwijk, & van Rijswijck, 2002). Also, it would not yield
interesting results in the game of Go because Monte-Carlo tree searches are more effective
than minimax searches guided by a heuristic evaluation function (Kocsis & Szepesvari, 2006;
Gelly & Silver, 2011; Browne, Powley, Whitehouse, Lucas, Cowling, Rohlfshagen, Tavener,
Perez, Samothrakis, & Colton, 2012; Gelly, Kocsis, Schoenauer Sebag, Silver, SzepesvaÃÅri, &
TeÃÉytaud, 2012). Moreover, a simpler learning method (e.g., a regression method in Othello,
Buro, 2002) would be preferable to MMTO, if it is sufficiently effective.
We conducted preliminary experiments on chess to catch a glimpse of the applicability
of MMTO to other games. Note that there already are evaluation functions in chess that
can outplay grandmasters, whereas there are none in shogi. Thus, it might be difficult to
improve well-crafted chess evaluation functions. For this experiment, we chose an opensource program (Crafty) as a fair implementation of a chess program (Hyatt, 2013).
The original evaluation function had been tightly tuned and is not a simple multivariable
function. Thus, for the sake of simplicity, we did not modify it in any way except to add
a new linear combination of weighted two-pieces-square features. The features were used
to evaluate all conditions of the king and another piece, such as in eB in Section 4.1. The
mirror symmetric property described in Section 4.1 was not applied and features in which
a pawn exists at the eighth rank were not counted. As a results, the total number of added
weights w B‚Äô was 39, 312. Because a chess position possesses only thirty or fewer two-piecessquare features, the additional computational time due to the above modification became
almost negligible with the help of a pawn hash table and the lazy evaluation technique that
had come with the original.
The training and test sets were composed by using game records at the Free Internet
Chess Server (FICS). These games were played using the standard time control of the server
by two players with ratings of 2, 600 or more. The training set P had 1, 267, 032 desired
moves and Z P = 33, 619, 904 move pairs after removing duplications from the 13, 440 game
records, whereas the test set P had 101, 982 desired moves and Z P = 2, 755, 217 move pairs
after removing duplications from the 1, 000 game records.
Figure 14 shows the rate of agreement with the test set and the number of correct answers
of chess problems through iteration. Here, Œ± in the sigmoid function was set to 0.00341, the
w B‚Äô ) = 0.156|w
w B‚Äô |.
equality constraint was not used, and the regularization term was JR (w
555

Hoki & Kaneko

Agreement (%)

35.2

1270

34.8

1260
1250

Agreement
Number of correct answers

34.4
2

3

4

5

6

7 8 9

1

2

3

4

5

10

6

1240

7 8 9

100

Number of correct answers

1280

Iteration

Figure 14: Improvement in rate of agreement with the test set (solid line) and the number of
correct answers of 2, 180 problems (dashed line) in chess. The two-piece-square
weights w B‚Äô were adjusted using MMTO.

Rating
Win

1010‚Äì1279
33 ¬± 3%

1280‚Äì1489
35 ¬± 3%

1490‚Äì1769
39 ¬± 4%

1770‚Äì2049
43 ¬± 4%

2050‚Äì
42 ¬± 4%

Table 5: Dependence of the strength (winning percentages) of learned programs on the
quality (ratings of players) of the training set. The uncertainty indicated as ¬±3
was estimated by conducting a two-sided test at a significance level of 5% on 1, 000
games.

A total of 2, 180 chess problems from the Encyclopedia of Chess Middlegames (the second
section of the 879 problems), Win at Chess (300 problems), and Winning Chess Sacrifices
(1, 001 problems) were used (Krogius, Livsic, Parma, & Taimanov, 1980; Reinfeld, 2001,
1969). The learned program searched 5 ¬∑ 104 nodes per problem and eight megabytes of
memory were assigned to the transposition table. We see that the agreement rate as well as
the number of correct answers tends to improve as the number of iterations grows, though
the differences are moderate. It means that MMTO found room for improvement in a
well-implemented chess program. These results indicate that MMTO can be a useful way
to learn heuristic evaluation functions in chess, especially when one can design evaluation
features suitable for learning.
4.8 Data Quality Dependence
To assess the importance of the quality of the game records, we conducted additional experiments using game records of players with various levels of experience in shogi. Here,
eABCD was learned by using the results of eABC in Figure 5 as the initial value. The results
are summarized in Table 5. Each training set was composed from the records of 47, 566
rapid time control (30 seconds per move) games played by amateurs on a popular Internet
shogi site, Shogi Club 245 . The first line in the table shows the ratings of the amateur
players. The second line shows the winning percentages of the learned evaluation function
5. Shogi Club 24, http://www.shogidojo.com, last access: 2013.

556

Large-Scale Optimization for Evaluation Functions with Minimax Search

against the evaluation function trained with grandmaster-game records. Here, each evaluation function was learned in 200 iterations. The winning percentages were computed by
averaging the results of a thousand games (About 15 drawn games and games exceeding
300 moves were not counted). Each player was allowed to use one second on one core of
an Intel Xeon X5680 for each move, and fifty megabytes of memory were assigned to the
transposition table. Table 5 shows the significance of the quality of the training set; the use
of game records of stronger players made the program stronger.

5. Conclusion
We presented a method, Minimax Tree Optimization (MMTO), that uses game records to
adjust a full set of feature weights of the evaluation function in a two-player game. The
learning of MMTO has been designed so that the search results match the desired moves,
e.g., the recorded moves of grandmaster games. MMTO consists of two procedures: (1)
a shallow heuristic search for all training positions using the current feature weights and
(2) an update guided by an approximation of the gradient of the objective function. A
new combination of a simple smooth approximation of the step function and grid-adjacent
updates with standard techniques, i.e., gradient guided optimization, constraints, and regularization, contributed to the scalability and stability of MMTO and led to it showing
substantial improvements over existing methods.
The performance of MMTO was demonstrated in experiments on shogi, a variant of chess
that has a larger number of legal moves. MMTO clearly outperformed the existing methods.
In addition, the experimental results on the rate of agreement and playing strength indicate
that MMTO can adjust forty million parameters. Possible future work would be automated
adjustment of the step length and a theoretical convergence analysis.

Acknowledgments
We are grateful to Dr. Masakazu Muramatsu for his support of this work.

Appendix A. Notes on the Continuity and Partial Differentiability of the
Minimax Value
We saw in Section 4.5 that the objective function of MMTO has a piecewise smooth surface.
In this Appendix, we theoretically discuss the continuity and partial differentiability of the
w ) with respect to w ‚àà RN , where w is the vector of parameters in the
minimax value vp (w
evaluation function e(p, w ) and p is the position. The continuity of the minimax value
ensures the continuity of the main part of objective function of MMTO defined in Eq. (5).
The partial differentiability analysis gives conditions under which the approximation inside
MMTO described in Section 3.3 is valid. We first analyze a single minimax tree, assuming
that the tree is known and fixed. Then, we extend our discussion to game-tree-search
methods that possibly explore different trees for different w .
Definition 1. The evaluation function e(¬∑, ¬∑) is a (P, RN ) 7‚Üí R function, where P is the set
of all positions in a target game, R is the set of real numbers, and RN is an N -dimensional
557

Hoki & Kaneko

Euclidean space. The evaluation function e(p, w ) is continuous with respect to the parameters w for any position p ‚àà P and for any w ‚àà RN . Moreover, the evaluation function
e(p, w ) is partially differentiable with respect to any component of w at any w ‚àà RN .
The continuity and partial differentiability of the evaluation function are feasible assumptions. Note that an evaluation based on an ordinary piece-square table has these
properties, and all recent machine learning of evaluation functions have them (Baxter et al.,
2000; Veness et al., 2009; Buro, 2002).
Definition 2. The theoretical game graph G is a finite, directed acyclic, connected graph
representing all possible transitions of states in the target game, where a node (resp. edge)
represents a position (resp. move). The set of nodes in G corresponds to P; V (G) = P. A
minimax graph T is a finite connected sub-graph of G. By convention, we use the term
minimax tree for the minimax graph even when it is not a tree. We denote the set of minimax
trees in G by T. A node is called a maximizing (resp. minimizing) node if the corresponding
position is the maximizing (resp. minimizing) player to move. The destination of an edge is
a maximizing (resp. minimizing) node if and only if the source of the edge is a minimizing
(resp. maximizing) node. We can clearly assume any node n to be a single position p, and
we will denote the evaluation function as e(n, w ).
Let Lr,T be the set of leaf nodes of the entire sub-tree Tr of T and Tr is rooted at node r.
We will omit tree T and use Lr if it is obvious. We denote the set of immediate successors
(or children) at node n in tree T by Cn,T or by Cn . Note that Cn = ‚àÖ if n is a leaf. In the
standard notation, a node (or vertex) in a graph T is denoted by n ‚àà V (T ). However, in
this Appendix, we will omit V (.) and write n ‚àà T because it is obvious.
w ) is a value associated with each node n in a minimax
Definition 3. A minimax value vn,T (w
tree T ‚àà T and it is defined recursively by a tree structure and by a static evaluation function
e(n, w ), as follows:
Ô£±
if n is a leaf,
Ô£≤ e(n, w )
w ) if n is a non-leaf maximizing node,
w) =
maxc‚ààCn,T vc (w
vn,T (w
(11)
Ô£≥
w
minc‚ààCn,T vc (w ) if n is a non-leaf minimizing node.
w ) if it is obvious. For two minimax values a and b of a
We will omit tree T and use vn (w
maximizing (resp. minimizing) node, we say a is better than b if a > b (resp. b < a).
A.1 Continuity of Minimax Value
The continuity of the minimax value follows from the continuity of the evaluation function.
w ) is continuous with respect to w for any minimax
Theorem 4. The minimax value vn,T (w
w ) = vn,T (w
w 0 ), or equivalently, for
tree T ‚àà T and for any w ‚àà RN . That is, limw ‚Üíw
w 0 vn,T (w
w ‚àí w 0 | < Œ¥ logically implies
any w 0 ‚àà RN and for any Œµ > 0, there exists Œ¥ > 0 such that |w
0
w ) ‚àí vn,T (w
w )| < Œµ.
|vn,T (w
The following assertion about the ordinary properties of the basic functions max and
min and is common sense in analysis. It is rather difficult, however, to find a suitable
reference containing it. We therefore give a proof that will be useful in the subsequent
discussion.
558

Large-Scale Optimization for Evaluation Functions with Minimax Search

x), ..., fk (x
x) be a continuous function
Proposition 5. Let k be a natural number and each f1 (x
x)) is a continuous function on RN . Similarly, mini (fi (x
x)) is a
RN 7‚Üí R. Then, maxi (fi (x
N
continuous function on R .
x) is continuous, for any x 0 ‚àà RN and for any Œµ > 0, there exists
Proof. Because each fi (x
x ‚àí x0 | < Œ¥i implies |fi (x
x) ‚àí fi (x
x0 )| < Œµ. Hence, if we choose Œ¥ = mini Œ¥i ,
Œ¥i > 0 such that |x
0
0
x ‚àí x | < Œ¥ implies |fi (x
x) ‚àí fi (x
x )| < Œµ for any i = 1, . . . , k; that is,
then |x
x0 ) ‚àí Œµ < fi (x
x) < fi (x
x0 ) + Œµ,
fi (x

for any i = 1, . . . , k.

Note that ai < bi for any i = 1, . . . , k obviously implies maxi ai < maxi bi . Thus, from the
above inequalities we obtain
x0 ) ‚àí Œµ < max fi (x
x) < max fi (x
x0 ) + Œµ,
max fi (x
i

i

i

that is,

x) ‚àí max fi (x
x0 )| < Œµ.
| max fi (x
i

i

x). The proof is similar for mini fi (x
x).
This implies the continuity of maxi fi (x
Let r be the root of a given tree T . Now, we prove Theorem 4 on the basis of
mathematical induction from the leaf nodes Lr,T to root r. That is, at any leaf node
n ‚àà Lr,T , the minimax value is continuous because of the continuity of the evaluation
w ) = e(n, w ). For an internal node n, we assume that continuity holds for
function; vn,T (w
any child c in Cn,T . This induction hypothesis and Proposition 5 ensure the continuity of
w ).
vn,T (w
A.2 Stability of Principal Variations
In the above subsection, we showed the continuity of minimax values through the continuity
of min and max functions. Here, we show that the best moves and principal variations are
stable when the changes in the leaves are small enough. We analyze the stability in order
to discuss partial differentiability.
+
w ), hereafter called the best children, denotes the set of
(w
Definition 6. The symbol Cn,T
such children at node n in tree T that have the same minimax value as that of n:
+
w ) = {c ‚àà Cn,T |vc (w
w ) = vn (w
w )}.
(w
Cn,T
+
‚àí
w ). Here, A \ B denotes
w ); that is, Cn,T \ Cn,T
(w
(w
We denote the rest of the children as Cn,T
the set difference, i.e., {e|e ‚àà A ‚àß e ‚àà
/ B}.

A child is considered to be the best choice in its parent node if the minimax value of the
child is the same as that of the parent node. When no two children share the same value,
w ) contains only one child. Otherwise, the number of nodes in Cn+ (w
w ) can be greater
Cn+ (w
than one.
Definition 7. Let r be the root of a tree T ‚àà T. The principal variation (abbreviated PV
w ) of tree T is the sub-tree of T obtained as the closure of the best children
for short) T ‚àó (w
from the root:
w ) = {r},
T 0 (w
+
i
w ) = {c ‚àà Cn,T
w ) | n ‚àà T i‚àí1 (w
w )} for i > 0,
T (w
(w
w) =
T ‚àó (w

‚àû
[

w ).
T i (w

i=0

559

Hoki & Kaneko

n0 2

}
!
n1 2 n2 2 n3-1

! 

n4 7 n5 2 n6-1

Figure 15: Example of a minimax tree (graph) with a transposition at n5
+
+
‚àí
w ) = Cn,T
w ) and Cn,T
w ) = ‚àÖ for any n ‚àà T ‚àó (w
w ). Also, we denote leaves
Note that Cn,T
(w
‚àó (w
‚àó (w
‚àó
‚àó
‚àó
w ) by L (w
w ), that is, T (w
w ) ‚à© Lr,T .
in T (w

Example 8. Figure 15 shows a small minimax tree T that has two best children at root n0 ;
the maximizing and minimizing nodes are denoted by boxes and circles, respectively. Here,
Cn+0 = {n1 , n2 } and Cn+1 = {n5 }. The principal variation T ‚àó of this tree is {n0 , n1 , n2 , n5 }.
Lemma 9. For any internal node n in any tree T ‚àà T and for any w 0 ‚àà RN , there exists
w 1 ‚àí w 0 | < Œ¥n , the set of the best
a positive number Œ¥n such that for any w 1 satisfying |w
1
0
children at node n for w is a subset of the one for w :
+
+
w 0 ), for any w 1 s.t. |w
w 1 ‚àí w 0 | < Œ¥n .
w 1 ) ‚äÜ Cn,T
(w
(w
Cn,T

w 0 ) is empty, the assertion is trivial.
Proof. When all child values are the same, i.e., Cn‚àí (w
Otherwise, let Œµ0 be the minimum absolute difference between the best value and any of the
w 0 ) ‚àí vc (w
w 0 )| > 0. The continuity of the minimax
other values, i.e., Œµ0 = minc‚ààCn‚àí (w
w 0 ) |vn (w
w 1 ‚àí w 0 | < Œ¥n , we have
values ensures the existence of Œ¥n such that for any w 1 satisfying |w
1
0
1
0
w ) ‚àí vc (w
w )| < Œµ0 /2 and also |vn (w
w ) ‚àí vn (w
w )| < Œµ0 /2. From the definition of
maxc‚ààCn |vc (w
‚àí
0
w ) satisfies
Œ¥n and triangle inequalities, any c ‚àà Cn,T (w
w 0 ) ‚àí vn (w
w 0 )|
Œµ0 ‚â§ |vc (w
w 0 ) ‚àí vc (w
w 1 )| + |vc (w
w 1 ) ‚àí vn (w
w 0 )|
‚â§ |vc (w
w 0 ) ‚àí vc (w
w 1 )| + |vc (w
w 1 ) ‚àí vn (w
w 1 )| + |vn (w
w 1 ) ‚àí vn (w
w 0 )|
‚â§ |vc (w
Œµ
Œµ
w 1 ) ‚àí vn (w
w 1 )| + 0 + 0
< |vc (w
2
2
1
1
w ) ‚àí vn (w
w )| + Œµ0 .
= |vc (w
w 1 ) ‚àí vn (w
w 1 )| > Œµ0 ‚àí Œµ0 = 0, namely, vc (w
w 1 ) 6= vn (w
w 1 ). This implies by definition
Thus, |vc (w
+
w 1 ).
(irrespective of whether n is a max or min node) that c 6‚àà Cn,T
(w
Definition 10. The tree stability Œ¥T of a tree T is the minimum value of Œ¥n among all the
nodes n ‚àà T , where Œ¥n is a positive number satisfying Lemma 9. Note that the minimum
value Œ¥T > 0 exists because T is finite.
Example 11. In reference to Figure 15, suppose that each leaf value changes by at most 0.1.
w 1 ) ‚àí vn (w
w 0 )| ‚â§ 0.1 for each internal node n of heights 1, 2,
Then, it will be proven that |vn (w
and 3 in order: it is obvious for n4 , n5 , and n6 , and it can be proven for n1 , n2 and n3 , and
finally for n0 . We can see that neither n4 nor n6 can become a new best node as a result of
this change.
560

Large-Scale Optimization for Evaluation Functions with Minimax Search

O
p

vn
Ô£±‚Ä¢
Ô£¥
u
Œµ0Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥

w1)
vn (w

w0)
(w
j

w1)
vc (w
(c ‚àà Cn+ )
/ h
w 1 ) (c ‚àà
vc (w
/ Cn+ )
q



w 1 ) at maximizing-node n, where w 1 changes along the i-th comFigure 16: Sketch of vn (w
0
w 1 ) at n equals vc (w
w 1 ) at one of the old best
ponent wi + h from w 0 . Here, vn (w
+
0
w ).
children c ‚àà Cn (w

A.3 Partial Differentiability
We show that the partial differentiability, as well as the partial derivative, of the minimax
value at a node in tree T depends only on its principal variations. We denote the right and
left partial derivatives of a function RN 7‚Üí R at point x 0 as
‚àÇf 0
x ) =
(x
‚àÇx+
i

f (x01 , . . . , x0i + h, . . . , x0N ) ‚àí f (x01 , . . . , x0N )
,
h‚Üí+0
h

(12)

‚àÇf 0
x ) =
(x
‚àÇx‚àí
i

f (x01 , . . . , x0i + h, . . . , x0N ) ‚àí f (x01 , . . . , x0N )
.
h‚Üí‚àí0
h

(13)

lim
lim

Let us pay attention to the single parameter xi that changes by h under these limit op‚àÇf
0
erations. Hereafter, the other parameters held constant will often be omitted as ‚àÇx
+ (x ),
where x is the one-dimensional parameter of interest. We use the symbol ‚àÇ because of its
analogy to the partial derivative in order not to forget that the other parameters have been
omitted.
w ) of tree T ‚àà T and for any
Theorem 12. For any node n in the principal variation T ‚àó (w
w ) at which the partial derivative of the evaluation
w ‚àà RN , there exists such a leaf la ‚àà L‚àó (w
‚àÇ
w ): ‚àÇvn+ (w
w ) = ‚àÇw
function equals the right partial derivative of vn (w
e(la , w ). Similarly, there
i
‚àÇw
i

w ) at which the partial derivative of the evaluation function equals
exists such a leaf lb ‚àà L‚àó (w
‚àÇ
w ), ‚àÇvn‚àí (w
w ) = ‚àÇw
the left partial derivative of vn (w
e(lb , w ).
i
‚àÇw
i

The proof of the theorem, given at the end of this subsection, is based on the stability
0 ) and |w
w 1 ‚àí w 0 | = |h| are
of the best moves. We assume that w 1 = (w10 , . . . , wi0 + h, . . . , wN
sufficiently small in Appendix A.3. Consequently, we have |h| < Œ¥T , and for any node n in
tree T and for any w 0 ‚àà RN ,
Ô£±
1
(n:leaf)
Ô£¥
Ô£≤ e(n, w )
1 ) (n:maximizing node)
1
w
max
v
(w
+
0
c
w )=
w )
vn (w
c‚ààCn,T (w
Ô£¥
Ô£≥ min +
w 1 ) (n:minimizing node).
w 0 ) vc (w
c‚ààC
(w
n,T

561

(14)

Hoki & Kaneko

w 1 ) changing with h, where n is a maxExample 13. Figure 16 sketches an example of vn (w
w 0 ) when h = 0. Each value
imizing node. There are three best children with value vn (w
continuously (not always linearly) changes with h. While the best child depends on the sign
w 0 ) when h is less than Œ¥T . This is because the minimax
of h, it is always one of c ‚àà Cn+ (w
w 0 ) are sufficiently less (by at least Œµ0 ) than vn (w
w 0 ) at
values of the other children c ‚àà Cn‚àí (w
h = 0.
w ) are given by
The next goal is to show that the right and left partial derivatives of vn (w
w ), respectively. The
the right and left partial derivatives at one of the best children Cn+ (w
following propositions describe the ordinary properties of the right and left limits and the
basic functions max and min. Similar arguments can be found in a comprehensive textbook
of calculus. We will give a detailed proof here, however, because it is rather difficult to find
the precisely same assertion in a textbook.
Proposition 14. Let k be a natural number and any of f1 (x), ..., fk (x) be a continuous
function R 7‚Üí R. Suppose that these functions have the same value at point x0 , i.e.,
‚àÇfi
0
maxi fi (x0 ) = mini fi (x0 ), and all of them have a right partial derivative ‚àÇx
+ (x ). Then,
0
the right partial derivative of the minimum or maximum of fi (x) at point x exists and is
equal to the minimum or maximum of the right partial derivatives of fi (x0 ), respectively.
‚àÇ maxi fi 0
‚àÇfi
‚àÇ mini fi 0
‚àÇfi
(x ) = max + (x0 ),
(x ) = min + (x0 ).
+
+
i ‚àÇx
i ‚àÇx
‚àÇx
‚àÇx
Proof. Let o(h) be Landau‚Äôs symbol, and let us use it to denote residual terms converging
0
0
to 0 faster than h, i.e., limh‚Üí+0 o(h)
h = 0. Recall that fi (x ) = f1 (x ) for any i = 1, . . . , k.
For positive h, we have


‚àÇfi 0
0
max fi (x + h) ‚àí max fi (x ) = max fi (x ) + h + (x ) + o(h) ‚àí max fi (x0 )
i
i
i
i
‚àÇx


‚àÇfi
= max f1 (x0 ) + h + (x0 ) + o(h) ‚àí f1 (x0 )
i
‚àÇx


‚àÇfi 0
= h max + (x ) + o(h)
i ‚àÇx
0

0

From Eq. (12), the function maxi fi (x) at point x0 has a right partial derivative maxi
The same argument applies to the right partial derivative of mini fi (x).

‚àÇfi
(x0 ).
‚àÇx+

Proposition 15. Suppose that functions have the same value at point x0 and all of these
‚àÇfi
0
functions have a left derivative ‚àÇx
‚àí (x ). Then, the left partial derivative of the minimum
or maximum of fi (x) at point x0 is equal to the maximum or minimum of the left partial
derivatives of fi (x0 ):
‚àÇ maxi fi 0
‚àÇfi
‚àÇ mini fi 0
‚àÇfi
(x ) = min ‚àí (x0 ),
(x ) = max ‚àí (x0 ).
‚àí
‚àí
i ‚àÇx
i ‚àÇx
‚àÇx
‚àÇx
562

Large-Scale Optimization for Evaluation Functions with Minimax Search

Proof. Using similar algebra as in the proof of Proposition 14, we find for negative h,


‚àÇfi 0
0
0
max fi (x + h) ‚àí max fi (x ) = h min ‚àí (x ) + o(h).
i
i
i ‚àÇx
‚àÇfi
0
From Eq. (13), the function maxi fi (x) at point x0 has the left partial derivative mini ‚àÇx
‚àí (x ).
Note that min and max are switched in the algebra above because of the negativity of h.
The same argument applies to the left partial derivative of mini fi (x).

Lemma 16. Let gi+ (n, w ) =

‚àÇvn
w)
(w
‚àÇwi+

(resp. gi‚àí (n, w ) =

‚àÇvn
w )) be the right (resp.
(w
‚àÇwi‚àí
w ‚àà RN and for any internal

left)

w ). For any
partial derivative of the minimax value vn (w
node
w ) of tree T ‚àà T, there exist right and left partial derivatives
n in principal variation T ‚àó (w
w ) with respect to any i = 1, . . . , N . The right and left partial
gi+ (n, w ) and gi‚àí (n, w ) of vn (w
derivatives are:
Ô£±
+
Ô£≤ maxc‚ààC + (w
w ) gi (c, w ) (n: maximizing node)
n,T ‚àó
+
gi (n, w ) =
+
Ô£≥ minc‚ààC + ‚àó (w
w ) gi (c, w ) (n: minimizing node)
n,T
Ô£±
‚àí
Ô£≤ minc‚ààC + (w
w ) gi (c, w ) (n: maximizing node)
n,T ‚àó
‚àí
gi (n, w ) =
‚àí
(n: minimizing node).
Ô£≥ maxc‚ààC + ‚àó (w
w ) gi (c, w )
n,T

Proof. We prove these equalities on the basis of mathematical induction from the leaf nodes
w ), by the definition of the evaluation function, the
Lr,T to the root r. For each leaf n in L‚àó (w
w ) is clearly continuous and partially differentiable with respect to any
minimax value vn (w
component in w ‚àà RN . For any internal node n, we assume, as an induction hypothesis,
that the right partial derivative gi+ (c, w ) and left partial derivative gi‚àí (c, w ) exist for any
w 1 ) ‚äÜ Cn+ (w
w 0 ) for any |h| < Œ¥T and Eq. (14). From the
child c ‚àà Cn,T . Recall that Cn+ (w
induction hypothesis with Proposition 14, we have
‚àÇ maxc‚ààCn+ (w
w ) vc
‚àÇwi+

‚àÇ minc‚ààCn+ (w
‚àÇvc
‚àÇvc
w ) vc
w ),
w ) = min
w ).
(w
(w
+
+
+ (w
+
+
‚àÇw
‚àÇw
‚àÇw
w)
w)
c‚ààCn (w
c‚ààCn (w
i
i
i

w ) = max
(w

Similarly, from Proposition 15, we have
‚àÇ maxc‚ààCn+ (w
w ) vc
‚àÇwi‚àí

‚àÇ minc‚ààCn+ (w
‚àÇvc
‚àÇvc
w ) vc
w ),
w ) = max
w ).
(w
(w
‚àí
‚àí
‚àí (w
+
+
‚àÇw
‚àÇw
‚àÇw
w)
w)
c‚ààCn (w
c‚ààCn (w
i
i
i

w ) = min
(w

w ), it is obvious that gi+ (n, w ) =
Now, we prove Theorem 12. For any leaf n ‚àà L‚àó (w
‚àÇ
w ), Lemma 16 ensures that the left
= ‚àÇw
e(n, w ). For any internal node n ‚àà T ‚àó (w
i
and right partial derivatives gi+ (n, w ) and gi‚àí (n, w ) are given by one of the best children.
w ) such that
Thus, for root r, there always exist leaves la and lb ‚àà L‚àó (w
gi‚àí (n, w )

gi+ (r, w ) =

‚àÇ
‚àÇwi

gi‚àí (r, w ) =

e(la , w ),
563

‚àÇ
‚àÇwi

e(lb , w ).

(15)

Hoki & Kaneko

‚Ä¢
g + (n, w 0 ) = 0
g ‚àí (n, w 0 ) = 1
‚àÇ
‚àÇwi

e(a, w 0 ) = 1
e(a, w 0 ) = 0

n


 ‚Ä¢

a ‚Ä¢

g + (r, w 0 ) = g ‚àí (r, w 0 ) = 0
w0) = 0
vr (w

r

‚Ä¢ c
‚Ä¢ &b

‚àÇ
‚àÇwi

e(c, w 0 ) = 0
e(c, w 0 ) = 0

‚àÇ
‚àÇwi

e(b, w 0 ) = 0
e(b, w 0 ) = 0

w ) exists at w 0 , it is not equal to the partial
Figure 17: Although the partial derivative of vr (w
‚àÇ
0
derivative at a PV leaf ‚àÇwi e(a, w ).

w ) with
Remark 17. By definition, if gi+ (n, w 0 ) = gi‚àí (n, w 0 ), the partial derivative of vn (w
0
‚àó
0
w ) satisfying
respect to wi exists at the point w and there is a leaf l ‚àà L (w
‚àÇ
‚àÇ
w0) =
vn (w
e(l, w 0 ).
‚àÇwi
‚àÇwi

(16)

w ) with respect
Remark 18. For any i = 1, . . . , N , the partial derivative of minimax value vn (w
‚àÇ
0
0
‚àó
w 0 ).
to wi exists at w and equals ‚àÇwi e(l, w ), if l is the unique element of L (w
w ) has a partial derivative
Remark 19. There exists a tree Tr for which the minimax value vn (w
0
w 0 )| > 1) and
with respect to wi at w , even when the leaves l in PV are not unique (|L‚àó (w
‚àÇ
0
give different partial derivatives ‚àÇwi e(l, w ). An example is sketched in Figure 17, where
the partial derivative is 1 for a and 0 for b and c.
A.4 Game-Tree Search and Pruning Techniques
Consider a game tree search S be a function that takes the root position r and the evaluationw ) with minimax values
function parameters w as inputs, and yields a minimax tree TrS (w
S (w
w
w
vn,Tr (w
(w
)
for
all
n
‚àà
T
).
We
call
a
game-tree
search
S
static,
provided that it yields
w)
r
S
S
0
w )) = V (Tr (w
w )), for any root r. Then,
a constant tree with respect to w , i.e., V (Tr (w
w ) yielded by such a static game-tree
theorems 4 and 12 apply to the minimax value vr,TrS (w
search. For example, a fixed-depth minimax search or a minimax search considering limited
types of moves (e.g., capture and promotion) is a static game-tree search. A minimax search
with ‚Äústand pat‚Äù used in the quiescence search (Beal, 1990) is static, too. Note that ‚Äústand
pat‚Äù at node n is equivalent to a virtual move adding an evaluation function e(n, w ) as a
w ) in Eq. (11), even when n is not a leaf node.
candidate of the node value vn (w
When pruning techniques are incorporated, part of the tree is pruned and not explored.
0
w ) ‚äÜ TrS (w
w ) yielded by
Consider a static search S, that with a pruning S 0 , and tree TrS (w
0
S . We call a pruning conservative, provided that it yields the same minimax value at
w ) = vr,T S0 (w
w ). Theorem 4 applies to the minimax
any root r for any w ‚àà RN : vr,TrS (w
w ) (w
r
w
value at the root r, vr,T S0 (w
(w
),
yielded
by
such
a
static
game-tree search with conservative
w)
r
pruning. Standard Œ±Œ≤ pruning (Knuth & Moore, 1975) is a conservative pruning. However,
many pruning techniques, e.g., static exchange evaluation (Reul, 2010), (extended) futility
pruning (Heinz, 1998), null move pruning (Adelson-Velskiy et al., 1975), and late move
reductions (Romstad, 2010), can prune a sub-tree without having to prove that the sub564

Large-Scale Optimization for Evaluation Functions with Minimax Search

tree is irrelevant to the minimax value at the root. Thus, these pruning techniques are
generally not conservative.
A.5 Summary
The minimax value of the root of the tree explored by a game-tree search with wellconfigured pruning techniques is continuous. This result suggests the continuity of the
objective function of MMTO in Eq. (4), as was empirically observed in Section 4.5. As for
partial differentiability, Theorem 12 suggest that it is feasible to consider the leaves of the
principal variations in a search tree. When there is only one principal variation, as stated in
Remark 18, the use of the partial derivative at the unique leaf introduced in Section 3.3 is
correct. Otherwise, i.e., when there are multiple principal variations, the partial derivative
may not exist or be different from the partial derivative at one of the leaves, as stated in
Remark 19. Although the frequency of such cases depends on the target game and on the
evaluation features, it is almost negligible in the experiments discussed in our previous work
(Kaneko & Hoki, 2012).

References
Adelson-Velskiy, G. M., Arlazarov, V. L., & Donskoy, M. V. (1975). Some methods of
controlling the tree search in chess programs. Artificial Intelligence, 6 (4), 361 ‚Äì 371.
Akl, S. G., & Newborn, M. M. (1977). The principal continuation and the killer heuristic.
In Proceedings of the 1977 Annual Conference, ACM ‚Äô77, pp. 466‚Äì473, New York, NY,
USA. ACM.
Anantharaman, T. (1997). Evaluation tuning for computer chess: Linear discriminant methods. ICCA Journal, 20 (4), 224‚Äì242.
Baxter, J., Tridgell, A., & Weaver, L. (2000). Learning to play chess using temporaldifferences. Machine Learning, 40 (3), 242‚Äì263.
Beal, D. F. (1990). A generalised quiescence search algorithm. Artificial Intelligence, 43,
85‚Äì98.
Beal, D. F., & Smith, M. C. (2001). Temporal difference learning applied to game playing
and the results of application to shogi. Theoretical Computer Science, 252 (1-2), 105‚Äì
119.
Bertsekas, D. P., & Bertsekas, D. P. (2008). Nonlinear Programming (2nd edition). Athena
Scientific.
BjoÃàrnsson, Y., & Marsland, T. A. (2002). Learning control of search extensions. In Caulfield,
H. J., Chen, S.-H., Cheng, H.-D., Duro, R. J., Honavar, V., Kerre, E. E., Lu, M.,
Romay, M. G., Shih, T. K., Ventura, D., Wang, P. P., & Yang, Y. (Eds.), JCIS, pp.
446‚Äì449. JCIS / Association for Intelligent Machinery, Inc.
Browne, C., Powley, E., Whitehouse, D., Lucas, S., Cowling, P., Rohlfshagen, P., Tavener,
S., Perez, D., Samothrakis, S., & Colton, S. (2012). A survey of monte carlo tree
search methods. Computational Intelligence and AI in Games, IEEE Transactions
on, 4 (1), 1‚Äì43.
565

Hoki & Kaneko

Buro, M. (2002). Improving heuristic mini-max search by supervised learning. Artificial
Intelligence, 134 (1‚Äì2), 85‚Äì99.
Buro, M., Long, J. R., Furtak, T., & Sturtevant, N. R. (2009). Improving state evaluation,
inference, and search in trick-based card games. In IJCAI, pp. 1407‚Äì1413.
Buro, M. (1995). Statistical feature combination for the evaluation of game positions.
Journal of Artificial Intelligence Research, 3, 373‚Äì382.
Campbell, M., Hoane, Jr., A. J., & Hsu, F.-h. (2002). Deep Blue. Artificial Intelligence,
134 (1‚Äì2), 57‚Äì83.
Chellapilla, K., & Fogel, D. (1999). Evolving neural networks to play checkers without
relying on expert knowledge. Neural Networks, IEEE Transactions on, 10 (6), 1382
‚Äì1391.
Coulom, R. (2007). Computing ‚ÄúElo Ratings‚Äù of move patterns in the game of go. ICGA
Journal, 30 (4), 198‚Äì208.
Coulom, R. (2012). Clop: Confident local optimization for noisy black-box parameter tuning.
In Herik, H., & Plaat, A. (Eds.), Advances in Computer Games 13, No. 7168 in LNCS,
pp. 146‚Äì157. Springer-Verlag.
Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research, 12, 2121‚Äì2159.
Fawcett, T. E. (1993). Feature Discovery for Problem Solving Systems. Ph.D. thesis, Department of Computer Science, University of Massachusetts, Amherst.
FuÃàrnkranz, J. (2001). Machine learning in games: a survey. In Machines that learn to play
games, pp. 11‚Äì59. Nova Science Publishers, Commack, NY, USA.
Gelly, S., Kocsis, L., Schoenauer M., Sebag, M., Silver, D., SzepesvaÃÅri, C., & TeÃÉytaud, O.
(2012). The grand challenge of computer go: Monte carlo tree search and extensions.
Commun. ACM, 55 (3), 106‚Äì113.
Gelly, S., & Silver, D. (2011). Monte-carlo tree search and rapid action value estimation in
computer go. Artificial Intelligence, 175 (11), 1856‚Äì1875.
Gomboc, D., Buro, M., & Marsland, T. A. (2005). Tuning evaluation functions by maximizing concordance. Theoretical Computer Science, 349 (2), 202‚Äì229.
Heinz, E. A. (1998). Extended futility pruning. ICCA Journal, 21 (2), 75‚Äì83.
Heinz, E. A. (1999). Adaptive null-move pruning. ICCA Journal, 22 (3), 123‚Äì132.
Hoki, K. Bonanza ‚Äì the computer shogi program.. http://www.geocities.jp/bonanza_
shogi/ Last access: 2013. In Japanese.
Hoki, K. (2006). Optimal control of minimax search results to learn positional evaluation. In
The 11th Game Programming Workshop (GPW2006), pp. 78‚Äì83, Kanagawa, Japan.
In Japanese.
Hoki, K., & Kaneko, T. (2012). The global landscape of objective functions for the optimization of shogi piece values with game-tree search. In van den Herik, H. J., &
Plaat, A. (Eds.), Advances in Computer Games 13, No. 7168 in LNCS, pp. 184‚Äì195.
Springer-Verlag.
566

Large-Scale Optimization for Evaluation Functions with Minimax Search

Hoki, K., & Muramatsu, M. (2012). Efficiency of three forward-pruning techniques in shogi:
Futility pruning, null-move pruning, and late move reduction (LMR). Entertainment
Computing, 3 (3), 51‚Äì57.
Hsu, F.-h., Anantharaman, T. S., Campbell, M. S., & Nowatzyk, A. (1990). Deep Thought.
In Marsland, T. A., & Schaeffer, J. (Eds.), Computers, Chess, and Cognition, pp.
55‚Äì78. Springer-Verlag.
Iida, H., Sakuta, M., & Rollason, J. (2002). Computer shogi. Artificial Intelligence, 134 (1‚Äì
2), 121‚Äì144.
Kaneko, T. (2009). Recent improvements on computer shogi and GPS-Shogi. IPSJ Magazine, 50 (9), 878‚Äì886. In Japanese.
Kaneko, T., & Hoki, K. (2012). Analysis of evaluation-function learning by comparison of
sibling nodes. In van den Herik, H. J., & Plaat, A. (Eds.), Advances in Computer
Games 13, No. 7168 in LNCS, pp. 158‚Äì169. Springer-Verlag.
Knuth, D. E., & Moore, R. W. (1975). An analysis of alpha-beta pruning. Artificial
Intelligence, 6 (4), 293‚Äì326.
Kocsis, L., & Szepesvari, C. (2006). Bandit based monte-carlo planning. In Machine Learning: ECML 2006, Vol. 4212, pp. 282‚Äì293. Springer.
Krogius, N., Livsic, A., Parma, B., & Taimanov, M. (1980). Encyclopedia of Chess Middlegames: Combinations. Chess Informant.
Levinson, R., & Weber, R. (2001). Chess neighborhoods, function combination, and reinforcement learning. In Marsland, T. A., & Frank, I. (Eds.), Computer and Games,
No. 2063 in LNCS, pp. 133‚Äì150. Springer-Verlag.
Marsland, T. A. (1985). Evaluation function factors. ICCA Journal, 8 (2), 47‚Äì57.
Marsland, T. A., & Campbell, M. (1982). Parallel search of strongly ordered game trees.
ACM Computing Surveys, 14 (4), 533‚Äì551.
Nitsche, T. (1982). A learning chess program. In Advances in Computer Chess 3, pp.
113‚Äì120. Pergamon Press.
Nocedal, J., & Wright, S. (2006). Numerical Optimization. Springer-Verlag.
Nowatzyk, A. (2000). http://tim-mann.org/DT_eval_tune.txt.
Pearl, J. (1980). Scout: A simple game-searching algorithm with proven optimal properties.
In In Proceedings of the First Annual National Conference on Artificial Intelligence,
pp. 143‚Äì145.
Reinefeld, A. (1983). An improvement to the scout tree search algorithm. ICCA Journal,
6 (4), 4‚Äì14.
Reinfeld, F. (1969). 1001 Winning Chess Sacrifices and Combinations. Wilshire Book
Company.
Reinfeld, F. (2001). Win at Chess (Dover Books on Chess). Dover Publications.
Reul, F. (2010). Static exchange evaluation with Œ±Œ≤-approach. ICGA Journal, 33 (1), 3‚Äì17.
567

Hoki & Kaneko

Romstad, T. An Introduction to Late Move Reductions. http://www.glaurungchess.com/
lmr.html, Last access: 2010.
Russell, S. J., & Norvig, P. (2002). Artificial Intelligence: A Modern Approach (2nd Edition).
Prentice Hall.
Schaeffer, J. (1986). Experiments in search and knowledge. Ph.D. Thesis, Department of
Computing Science, University of Waterloo, Canada.
Schaeffer, J. (1989). The history heuristic and alpha-beta search enhancements in practice.
IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-11 (1), 1203‚Äì
1212.
Schaeffer, J., Hlynka, M., & Jussila, V. (2001). Temporal difference learning applied to
a high-performance game-playing program. In IJCAI‚Äô01: Proceedings of the 17th
international joint conference on Artificial intelligence, pp. 529‚Äì534, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Silver, D., & Tesauro, G. (2009). Monte-carlo simulation balancing. In ICML ‚Äô09: Proceedings of the 26th Annual International Conference on Machine Learning, pp. 945‚Äì952.
ACM.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction (Adaptive
Computation and Machine Learning). The MIT Press.
Tanaka, T., & Kaneko, T. GPS Shogi.. http://gps.tanaka.ecc.u-tokyo.ac.jp/
gpsshogi/ Last access: 2013. In Japanese.
Tesauro, G. (2001). Comparison training of chess evaluation functions. In Machines that
Learn to Play Games, pp. 117‚Äì130. Nova Science Publishers.
Tesauro, G. (2002). Programming backgammon using self-teaching neural nets. Artificial
Intelligence, 134 (1‚Äì2), 181‚Äì199.
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. J. Royal. Statist.
Soc B, 58 (1), 267‚Äì288.
Tsuruoka, Y., Yokoyama, D., & Chikayama, T. (2002). Game-tree search algorithm based
on realization probability. ICGA Journal, 25 (3), 145‚Äì152.
Ugajin, T., & Kotani, Y. (2010). Learning evaluation function based on tree strap in shogi.
In The 15th Game Programming Workshop, pp. 114‚Äì118. In Japanese.
van den Herik, H. J., Uiterwijk, J. W. H. M., & van Rijswijck, J. (2002). Games solved:
now and in the future. Artif. Intell., 134 (1-2), 277‚Äì311.
van der Meulen, M. (1989). Weight assessment in evaluation functions. In Beal, D. (Ed.),
Advances in. Computer Chess 5, pp. 81‚Äì89.
Veness, J., Silver, D., Uther, W., & Blair, A. (2009). Bootstrapping from game tree search.
In Advances in Neural Information Processing Systems 22, pp. 1937‚Äì1945.
Zobrist, A. L. (1990). A new hashing method with application for game playing. ICCA
Journal, 13 (2), 69‚Äì73.

568

Journal of Artificial Intelligence Research 49 (2014) 323-361

Submitted 08/13; published 02/14

Symmetric Subgame-Perfect Equilibria
in Resource Allocation
Ludek Cigler
Boi Faltings

ludek.cigler@epfl.ch
boi.faltings@epfl.ch

Artificial Intelligence Laboratory
Ecole Polytechnique FeÃÅdeÃÅrale de Lausanne
CH-1015 Lausanne, Switzerland

Abstract
We analyze symmetric protocols to rationally coordinate on an asymmetric, efficient
allocation in an infinitely repeated N -agent, C-resource allocation problems, where the
resources are all homogeneous. Bhaskar proposed one way to achieve this in 2-agent, 1resource games: Agents start by symmetrically randomizing their actions, and as soon as
they each choose different actions, they start to follow a potentially asymmetric ‚Äúconvention‚Äù that prescribes their actions from then on. We extend the concept of convention
to the general case of infinitely repeated resource allocation games with N agents and C
resources. We show that for any convention, there exists a symmetric subgame-perfect
equilibrium which implements it. We present two conventions: bourgeois, where agents
stick to the first allocation; and market, where agents pay for the use of resources, and
observe a global coordination signal which allows them to alternate between different allocations. We define price of anonymity of a convention as a ratio between the maximum
social payoff of any (asymmetric) strategy profile and the expected social payoff of the
subgame-perfect equilibrium which implements the convention. We show that while the
price of anonymity of the bourgeois convention is infinite, the market convention decreases
this price by reducing the conflict between the agents.

1. Introduction
In many situations, agents have to coordinate their use of some resource. One wireless channel can only be used by one device, one parking slot may only be occupied by one vehicle,
etc. The problem is that often, the agents have identical preferences: Everyone prefers to
access rather than yield. Similarly, everyone prefers to have a parking slot rather than leave
their car at home. However, if multiple agents try to use one resource simultaneously, they
collide and everyone loses.
Consider a simple example: two agents want to access a single resource. We can describe
the problem as a game. Both agents have two actions: yield (Y ) and access (A). If agent
Œ± yields, it gets a payoff of 0. When agent Œ± accesses the resource while the other agent
yields, it gets a payoff of 1. But if both agents access the resource at the same time, they
both incur a cost Œ≥ > 0.
The normal form of such a game looks as follows:

Y
A
c
2014
AI Access Foundation. All rights reserved.

Y
0, 0
1, 0

A
0, 1
‚àíŒ≥, ‚àíŒ≥

Cigler & Faltings

This is a symmetric game, but the two efficient Nash equilibria (NE) are asymmetric:
either one agent yields and the other one accesses the resource, or vice versa. The only
symmetric equilibrium outcome is the mixed NE where both agents access the resource
1
with probability Pr(A) := |Œ≥|+1
. However, this mixed equilibrium is not efficient, because
the expected payoff of both agents is 0.
Asymmetric equilibria of symmetric games are undesirable for two reasons: First, they
are not fair. In our example, only one agent can access the resource. Second, coordinating
on an asymmetric equilibrium is difficult. Imagine that the agents are all identical and
anonymous, i.e. they cannot observe their own identity, nor the identity of any other agent.
We cannot prescribe a different strategy for each of the agents. Agents in some peer-to-peer
file-sharing networks are assumed to be anonymous (Chothia & Chatzikokolakis, 2005), as
well as agents in some wireless sensor networks (Durresi, Paruchuri, Durresi, & Barolli,
2005).
Consider the following example: Millions of wireless sensors are produced all by the
same pipeline. We take two of them randomly, and put them in a room. There is only one
frequency on which the sensors can transmit their measurements. How can each sensor know
when to transmit and when to stay quiet? The factory could program half of the sensors
to transmit in odd slots, and the other half to transmit in the even slots. Nevertheless, it
would be just as likely to have an odd-even pair of sensors, as it would be to have a pair
where the sensors transmit at the same time.
Aumann (1974) proposed the notion of correlated equilibria which fixes some of our issues
with the Nash equilibria in the resource allocation game above. A correlated equilibrium
(CE) is a probability distribution over the joint strategy profiles in the game. A correlation
device samples this distribution and recommends an action for each agent to play. The
probability distribution is a CE if agents do not have an incentive to deviate from the
recommended action. The correlation device takes away the burden of coordination from
the anonymous agents. They can all follow the same strategy: ‚Äúdo what the correlation
device has told me‚Äù.
What if such ‚Äúsmart‚Äù correlation device, which can send each agent a different private
signal, is not available? Can we still reach a correlated equilibrium outcome, one in which
anonymous agents can play identical strategies, and yet achieve an efficient and fair allocation? In our previous work (Cigler & Faltings, 2011), we have proposed an algorithm
that allows agents to learn a correlated equilibrium outcome through repeated play. We
considered a special case of a resource allocation problem. We proposed to use a global
coordination signal and multi-agent learning to reach a symmetric, fair and efficient outcome (Wang et al. (2011) later implemented this approach in an actual wireless network
and achieved throughput 3√ó higher than standard ALOHA protocols).
How does the coordination signal from our previous work (Cigler & Faltings, 2011) differ
from the ‚Äúsmart‚Äù correlation device assumed by Aumann (1974)? Firstly, it is public and
cannot send private signals to the agents. Such private signals are necessary for anonymous
agents to implement the desirable correlated equilibrium in a single stage resource allocation
game. The anonymous agents all have to follow the same strategy for each given public signal
value. Secondly, the coordination signal is not specific to the game. The only requirement
is that it is ergodic, i.e. it regularly sends each of its possible values. An example of such
324

Symmetric Subgame-Perfect Equilibria in Resource Allocation

signal is the day of the week, the decimal value of a price of a certain stock, or even a noise
on some frequency.
However, our previous solution had a major limitation: The learning algorithm itself was
not an equilibrium of the repeated game. A selfish agent could force everyone else to yield
by accessing all the time, securing the resource for herself. Therefore, in this paper, we focus
on learning algorithms which are themselves equilibria of the repeated game. We propose a
distributed algorithm to find an allocation of a set of resources which is not only symmetric
and fair, but also an equilibrium. We draw inspiration from the works of Bhaskar (2000)
and Kuzmics, Palfrey, and Rogers (2010) on symmetric equilibria for symmetric repeated
games.
Assume that agents play an infinitely repeated game, and they discount future payoffs
with a common discount factor 0 < Œ¥ < 1. A strategy for an agent is a mapping from
any history of the play to a probability distribution over the actions. Our goal is to find a
symmetric subgame perfect equilibrium. A subgame perfect equilibrium is a strategy profile
(vector of strategies for every agent) which is a NE in any history, including those that
cannot occur on the equilibrium path.
The symmetric subgame perfect equilibria that we study have the following form: The
agents start by choosing their actions randomly, all according to a given probability distribution. As soon as they play an (asymmetric) pure-strategy Nash equilibrium of the
game, they adopt a convention, that prescribes their actions deterministically from then
on. Bhaskar (2000) gives two examples of conventions for symmetric 2-agent, 2-action
games:
Bourgeois Agents keep using the action they played in the last round;
Egalitarian Agents play the action of their opponent from the last round.
In this paper, we extend the notion of convention to arbitrary resource allocation problems with N agents and C homogeneous resources, and we show that for any convention,
there exists a symmetric subgame-perfect equilibrium that reaches this convention. We give
a closed form expression to calculate the subgame-perfect equilibrium for the bourgeois convention, and show that for a small number of resources C, this convention leads to zero
expected payoff. This means that the price of anonymity of the bourgeois convention is ‚àû.
We present the market convention as a generalization of the egalitarian convention of
Bhaskar (2000). The main idea is that 1) agents pay a price for each successful access
of a resource, and 2) before each round of the game, they observe a global coordination
signal k ‚àà {1, . . . , K}, based on which they decide whether and which resource they access.
The agents have a decreasing marginal utility from accessing more often. The price helps
to decrease the demand for the resources, while the global coordination signal effectively
increases the capacity K-times. We show that compared to the bourgeois convention, the
market convention improves the expected payoff. Its price of anonymity is therefore finite.
This paper is structured as follows: In Section 2, we review some basic notions from game
theory, and we present the general definitions of conventions and their implementations. In
Section 3, we formally define the resource allocation game of N players and C resources, and
show that for any convention, there exists a symmetric subgame-perfect equilibrium which
implements it. In Section 4 we present two concrete examples of a convention: bourgeois and
325

Cigler & Faltings

market conventions and discuss their properties. In Section 5 we discuss the relationship of
this work to the work on folk theorems in game theory. Finally, Section 6 concludes.

2. Preliminaries
In this section, we will first introduce some basic concepts of game theory that we are going
to use throughout the paper. Then, we will define the notion of price of anonymity. Finally,
we will give the general definition of a convention and its implementation.
2.1 Game Theory
Game theory is the study of interactions among independent, self-interested agents. An
agent who participates in a game is called a player. Each player has a utility function
associated with each state of the world. Self-interested players take actions so as to achieve
a state of the world that maximizes their utility. Game theory studies and attempts to
predict the behaviour, as well as the final outcome of such interactions. Leyton-Brown and
Shoham (2008) give a more complete introduction to game theory.
The basic way to represent a strategic interaction (game) is using the so-called normal
form.
Definition 1. (Normal form game) A finite, N -person normal-form game is a tuple
G = (N, A, u), where
‚Ä¢ N is a set of N players;
‚Ä¢ A = A1 √ó A2 √ó . . . √ó AN , where Ai is a set of actions available to player i. Each vector
a = (a1 , a2 , . . . , aN ) ‚àà A is called an action profile;
‚Ä¢ u = (u1 , u2 , . . . , uN ), where ui : A ‚Üí R is a utility function for player i that assigns
each action vector a certain utility (payoff).
In this paper, we will be studying symmetric games. In such games, the players are
anonymous, and the only thing that influences the outcome is the number of agents who
took a certain action.
Definition 2. (Symmetric game) We say that a normal-form game G = (N, A, u) is a
symmetric game, if for any permutation of the vector of players Œ∑ : N ‚Üî N, it holds that
for any strategy vector œÉ = (œÉ1 , œÉ2 , . . . , œÉN ) and any i ‚àà N,
ui (œÉ1 , œÉ2 , . . . , œÉN ) = uŒ∑(i) (œÉŒ∑(1) , œÉŒ∑(2) , . . . , œÉŒ∑(N ) ).
Besides playing a single deterministic action, the player can also choose her action
randomly from a certain probability distribution.
Definition 3. (Mixed strategy) A mixed strategy selects a probability distribution over
the entire action space, i.e. œÉi ‚àà ‚àÜ(Ai ). A mixed strategy profile is a vector of mixed
strategies for each player. For a mixed strategy œÉi , we define its support supp(œÉi ) as
supp(œÉi ) = {ai ‚àà Ai : œÉi (ai ) > 0} .
326

Symmetric Subgame-Perfect Equilibria in Resource Allocation

Given a game specified using its normal form, how should the players choose their
strategy? When players know the strategies of the others, they can choose their action
quite easily: just pick the strategy that maximizes the payoff given what everyone else is
playing:
Definition 4. (Best response) We say that a mixed strategy œÉi‚àó of player i is a best
response to the strategy profile of the opponents œÉ‚àíi if for any strategy œÉi0 ,
ui (œÉi‚àó , œÉ‚àíi ) ‚â• ui (œÉi0 , œÉ‚àíi ).
As we mentioned earlier, one of the basic goals of game theory is to predict an outcome
of a strategic interaction. Such outcome should be stable ‚Äì therefore, it is usually called
an equilibrium. One requirement for an outcome to be an equilibrium is that none of the
players has an incentive to change their strategy, i.e. all players play their best-response to
the strategies of the others. This defines perhaps the most important equilibrium concept,
the Nash equilibrium:
Definition 5. (Nash equilibrium) A strategy profile œÉ = (œÉ1 , œÉ2 , . . . , œÉN ) is a Nash
equilibrium (NE) if for every player i, her strategy œÉ‚àíi is a best response to the strategies
of the others œÉ‚àíi .
Correlated equilibrium extends the notion of Nash equilibrium. In the canonical interpretation, it assumes that there is a central correlation device which samples the space of
possible outcomes of the game according to some probability distribution, and then recommends an action to play to each player. No player has an incentive to deviate from the
recommended action. The formal definition is as follows:
Definition 6. (Correlated equilibrium) Given an N -player game G = (N, A, u), a
correlated equilibrium is a tuple (v, œÄ, ¬µ), where v is a tuple of random variables v =
(v1 , v2 , . . . , vN ) with domains D = (D1 , D2 , . . . , DN ), œÄ is a joint probability distribution
over v, ¬µ = (¬µ1 , ¬µ2 , . . . , ¬µN ) is a vector of mappings ¬µi : Di 7‚Üí Ai , and for each player i and
every mapping ¬µ0i : Di 7‚Üí Ai it is the case that
X

œÄ(d)ui (¬µ1 (d1 ), ¬µ2 (d2 ), . . . , ¬µN (dN )) ‚â•

d‚ààD

X


œÄ(d)ui ¬µ01 (d1 ), ¬µ02 (d2 ), . . . , ¬µ0N (dN ) .

d‚ààD

In an equilibrium, each agent chooses the best strategy for himself. Oftentimes, the end
result is not the best for the agents as a whole. To analyze the overall utility of a game
outcome to all of the agents, we define its social payoff:
Definition 7. (Social payoff ) For a (mixed) strategy P
vector (œÉ1 , œÉ2 , . . . , œÉN ), we define
its social payoff as the sum of utilities of all the players, N
i=1 ui (œÉ1 , œÉ2 , . . . , œÉN ).
2.2 Repeated Game
In a repeated game, the same players play a given game (for example specified by its normal
form) repeatedly. We call the normal form game that is being played in each round the
stage game.
327

Cigler & Faltings

(1)

(2)

Definition 8. (Future discounted payoff ) Given an infinite sequence of payoffs ri , ri , . . .
for player i and a discount factor Œ¥, 0 < Œ¥ < 1, the future discounted payoff of player i is
Ei :=

‚àû
X

(j)

Œ¥ j ri .

j=1

Definition 9. (Infinitely repeated game) Let G = (N, A, u) be a normal form game.
An infinitely repeated version G of the game G with discounting is a game where the players
play the normal form game G for an infinite number of rounds. The payoff of player i in
game G is defined as its future discounted reward ri (Œ¥).
In this paper, we will study symmetric equilibria of an extended version of the repeated
game, so-called augmented game. We assume that in every round of the game, the players
can observe a common coordination signal, on which they can condition what strategy
they will use. In general, this coordination signal is just a random integer taken from set
{0, 1, . . . , K ‚àí 1}. In practice, it can be any piece of information observable by everyone:
price of a certain stock at a given time, temperature in the room, day of the week, etc.
Such a signal will allow agents to coordinate more efficiently, while at the same time it is
more realistic than a general correlation device which recommends actions to the agents, as
is assumed in the definition of correlated equilibria.
Definition 10. (Augmented repeated game) Let G = (N, A, u) be a normal form
game, let K := {0, 1, . . . , K ‚àí 1} be a set of coordination signals. An augmented infinitely
repeated version G of the game G with discounting is a game where players play the normal
form game G for an infinite number of rounds. In each round t, the players observe a
coordination signal kt ‚àà K. The coordination signal is chosen from a uniform distribution
over K. The players discount future payoff with a discount factor Œ¥.
W.l.o.g., we always assume that repeated games are augmented, since in an ordinary
repeated game, we can just assume that there is only one coordination signal. Therefore, in
the rest of the paper, whenever we refer to a repeated game or its strategy etc., we always
assume that the game is augmented with a coordination signal.
Definition 11. (History of a repeated game) Let G be an infinitely repeated game
with discounting. We define the history ht of the play in round t ‚â• 0 as

t‚àí1
t‚àí1
ht := ((a01 , a02 , . . . , a0N ), k0 ), . . . , ((at‚àí1
1 , a2 , . . . , aN ), kt‚àí1 )
where ati is the action taken by player i in round t, and kt is the signal that the players
observe in round t.
Definition 12. (Strategy of a repeated game) A strategy in the repeated game of a
player i is a function œái from the history ht and a currently observed coordination signal kt
to a probability distribution over the action space,
œái : (ht , kt ) 7‚Üí ‚àÜ(Ai ).
We can define the Nash equilibrium of the repeated game in the same way as for the
stage game (we can treat the repeated game as if it was just a normal form game where
players commit to their strategy for the entire game up front).
328

Symmetric Subgame-Perfect Equilibria in Resource Allocation

Definition 13. (Nash equilibrium of a repeated game) A strategy profile œá =
(œá1 , œá2 , . . . , œáN ) is a Nash equilibrium of the infinitely repeated game if for each player
i,
Ei (œái , œá‚àíi ) ‚â• Ei (œá0i , œá‚àíi )
(1)
for any alternative strategy of the repeated game œá0i . Here Ei ((œái , œá‚àíi ), ht , kt ) is the future
discounted payoff of player i when she adopts strategy œái and the other players adopt a
strategy vector œá‚àíi .
In the following text, we will use the notion of future discounted social payoff:
Definition 14. (Future discounted social payoff ) Given a strategy profile œá of the
infinitely repeated game G, the future discounted social payoff is defined as
E(œá) :=

N
X

Ei (œá).

(2)

i=1

For the repeated games, there exists a stronger notion of equilibria, which is a refinement
of the standard Nash equilibrium definition.
Definition 15. (Subgame-perfect equilibrium) Let G be an infinitely repeated game
with a discount factor 0 < Œ¥ < 1. A strategy vector œá = (œá1 , œá2 , . . . , œáN ) is a subgame-perfect
equilibrium of the game G if for each player i,
Ei ((œái , œá‚àíi ), ht , kt ) ‚â• Ei ((œá0i , œá‚àíi ), ht , kt )
for any strategy œá0i , history ht and coordination signal kt .
In the subgame-perfect equilibrium, players play a best-response strategy given any
history of the play, including the histories which cannot occur if they follow the equilibrium
strategy from the beginning. The notion of subgame-perfect equilibria eliminates this way
non-credible threats, or equilibria in which a player threatens someone else with a strategy
which the player might be prefer to avoid if it was supposed to be executed.
2.3 Price of Anonymity
In Section 1, we have seen that in the simple resource-allocation game, the symmetric
equilibrium leads to a significantly lower payoff than the asymmetric equilibria. Symmetry
of the equilibria is a natural requirement when players are all the same, i.e. anonymous.
How much social payoff do we have to sacrifice for the requirement of symmetry? Inspired
by the price of anarchy of Koutsoupias and Papadimitriou (1999), we propose the price
of anonymity as a measure of how efficient a given symmetric strategy vector is (the term
‚Äúprice of anonymity‚Äù was used previously in a different context by Bonnet & Raynal, 2011).
For a given symmetric strategy vector of the stage game œÉ, we calculate the ratio between
the social payoff of the most efficient (potentially asymmetric) Nash equilibrium of the
game, and the social payoff of strategy vector œÉ. The formal definition is as follows:
329

Cigler & Faltings

Definition 16. (Price of anonymity of a Nash equilibrium) Let G be a symmetric
game, let œÉ = (œÉ1 , œÉ2 , . . . , œÉN ) be a symmetric Nash equilibrium (that is ‚àÄi, j : œÉi = œÉj ),
and let œÑ be a (mixed) Nash equilibrium of the game G with the maximum social payoff.
We define the price of anonymity of strategy vector œÉ as follows:
RG (œÉ) :=

E(œÑ )
.
E(œÉ)

Definition 17. (Price of anonymity of a stage game) Let G be a symmetric game,
‚àó ) be a symmetric Nash equilibrium with minimal social payoff, and
let œÉ ‚àó = (œÉ1‚àó , œÉ2‚àó , . . . , œÉN
let œÑ be a (mixed) Nash equilibrium of the game G with the maximum social payoff. We
define the price of anonymity of the game G as follows:
RG :=

E(œÑ )
.
E(œÉ ‚àó )

For infinitely repeated games, we define the price of anonymity for their subgame-perfect
equilibria:
Definition 18. (Price of anonymity of a repeated game) Let G be a symmetric game,
let œá‚àó = (œá‚àó1 , œá‚àó2 , . . . , œá‚àóN ) be a symmetric subgame-perfect equilibrium with minimal social
payoff, and let œà be a subgame-perfect equilibrium of the game G with the maximum social
payoff. We define the price of anonymity of the game G as follows:
RG :=

E(œà)
.
E(œá‚àó )

2.4 Conventions and Implementations
As we have shown for the example of the 2-agent, 1-resource allocation game in Section 1,
there exist symmetric games that have nevertheless only asymmetric efficient equilibria. If
we allow for a central coordination device, the agents can play a symmetric and efficient
correlated equilibrium that selects randomly from the set of efficient Nash equilibria. Without such a device, in the stage game, there is no way to reach a symmetric efficient outcome
in an equilibrium.
However, if the agents play the game repeatedly, they can use the history of the play
to condition their strategy. If two agents have different histories, they can take different
actions in the future. In the first round of the game though, the history is empty for
everyone. Therefore, a symmetric strategy for the players has to randomize in order to ever
reach a point when the histories of the agents are distinct.
Bhaskar (2000) considered the problem of playing asymmetric outcomes of the stage
game using a symmetric strategy of the repeated game. His work considers games with 2
players and 2 actions, such as the 1-resource allocation game. The idea is that the two players start by playing randomly, using the same probability distribution over actions. They
randomize until they reach a round t where they happen to play some pure-strategy Nash
equilibrium (that is, they take a different action each). We call this round the asynchrony
round. Then, the agents start following a so-called convention. A convention maps the
asymmetric pure-strategy Nash equilibrium to a (potentially asymmetric) strategy vector
that the agents then adopt.
330

Symmetric Subgame-Perfect Equilibria in Resource Allocation

We have already mentioned the two basic conventions proposed by Bhaskar (2000): the
bourgeois and egalitarian convention. In the 1-resource allocation game, in the asynchrony
round, one agent chooses action A and the other one chooses Y . We will call the agent who
chose A in the asynchrony round the winner. The other agent is the loser. The bourgeois
convention guarantees that the agents will keep playing this NE forever after. This way,
the winner will be forever guaranteed a higher payoff than the loser. In the egalitarian
convention, the players alternate between the two pure-strategy Nash equilibria. That way
the payoffs of the winner and a loser will be closer.
In the infinitely repeated game with discounting, the social payoff will depend on two
things: the discount factor Œ¥, and the probability of a collision, that is the probability that
the players both play action A. When there is a big difference between the winner and loser
payoff, the losers will ‚Äúfight back‚Äù harder, so they will play their most preferred action A
with higher probability. This will increase the probability of a collision. In the egalitarian
convention, the payoffs to the loser are closer to the winner. Therefore, the agents will
collide less often, and they will also reach the asynchrony faster.
As another example of a convention, Kuzmics et al. (2010) analyze the Nash demand
game. The Nash demand game is a game of N players who choose between N actions
labeled 1, . . . , N . If all the players choose a distinct action, each player receives a payoff
equal to the label of her chosen action. If there are any two players who chose the same
action, every player (including those who chose an action alone) receives zero payoff. In a
pure-strategy Nash equilibrium, all the players choose a different action. Naturally, each
player prefers the equilibrium where she is the one who chose action N .
In the Nash demand game, we can also define bourgeois and egalitarian conventions.
Kuzmics et al. (2010) define three notions of payoff symmetry:
Ex-ante All agents have the same expected payoffs before the game starts.
Ex-post All agents have the same expected payoffs when asynchrony occurs (regardless of
who was the winner).
Strong ex-post All agents have the same payoff along any realization of the play.
The bourgeois convention is only ex-ante payoff symmetric, since once asynchrony occurs, the winner gets a higher payoff than the loser. The egalitarian convention is strong
ex-post payoff symmetric. In fact, Kuzmics et al. (2010) show that in the Nash demand
game, if a convention is socially efficient, it must be strong ex-post payoff symmetric. The
intuition is that in order to maximize social efficiency, we want to reach asynchrony as fast
as possible. This is only possible if agents choose their actions uniformly at random. They
will only do that if they are indifferent between which action they choose at the moment
asynchrony occurs.
We will now formally define the convention for an augmented repeated game of N agents.
Definition 19. (Convention) Let G = (N , A, u)
and let G be the repeated version of game G. We
that maps a vector of pure-strategy Nash equilibria
a = (a1 , a2 , . . . , aK ) to a vector of strategies of the
331

be a symmetric normal form game
define a convention as a function Œæ
of the game G for each signal value
repeated game G, such that for any

Cigler & Faltings

permutation Œ∑ : N ‚Üî N of the set of players,
Œæ((Œ∑(a1 ), . . . , Œ∑(aK ))) = Œ∑(Œæ(a1 , a2 , . . . , aK ))

(3)

that is, ‚Äúthe convention of a permutation is a permutation of a convention‚Äù (here Œæi denotes
the strategy for player i). The strategies can be different for each coordination signal value.

We use the notation Œ∑(a) := aŒ∑(1) , . . . , aŒ∑(N ) , and Œ∑(Œæ(a)) := ŒæŒ∑(1) (a), . . . , ŒæŒ∑(N ) (a) to
denote the permutation of the history vector using Œ∑, and the permutation of the strategy
vectors respectively.
Our definition of convention generalizes the definition Bhaskar (2000) gave for symmetric
games of 2 players and two actions Œ±, Œ≤. Bhaskar defined a convention as a mapping from
a set of Nash equilibrium action profiles {(Œ±, Œ≤), (Œ≤, Œ±)} to a set of strategies in which the
players alternate the strategy profiles (Œ±, Œ≤) and (Œ≤, Œ±) in some order. In our definition, a
convention maps any Nash equilibrium of the stage game to any strategy profile, provided
that it satisfies the permutation condition.
Intuitively, a convention prescribes each agent a potentially different role. The problem
for anonymous agents is to learn their role. We will call the learning algorithm they will
use an implementation of a convention.
Definition 20. (Implementation) Let G be an infinitely repeated game, and let Œæ be a
convention defined for this game. An implementation œÄŒæ of a convention Œæ is a strategy
vector of the infinitely repeated game, that is a function that assigns
œÄŒæ : (ht , kt ) 7‚Üí ‚àÜ(A1 ) √ó . . . √ó ‚àÜ(AN ),
and that satisfies the following conditions:
Let ht be the history of the game at time t.
1. If the players have already played some pure-strategy Nash equilibrium for all coordination signals k ‚àà K in some round t0 < t (t0 is the round in which they played
the NE for the last signal), follow the strategy prescribed by the convention Œæ for the
history ht \ ht0 (that is, the history from round t0 + 1 onwards).
2. Otherwise, let kt be the signal observed in the current round, and let vector a =
(a1 , a2 , . . . , aK ) such that ak is the action vector from the last round when the signal
k was observed (if signal k was not observed yet, we define ak = ‚àÖ). Then, the actions
of the players in the current round t only depend on vector a (abusing the notation,
we can write œÄŒæ (ht , kt ) = œÄŒæ (a, kt )), and for any permutation Œ∑ : {1, 2, . . . , N } ‚Üî
{1, 2, . . . , N },

(œÄŒæ,1 (Œ∑(a), kt ), . . . , œÄŒæ,N (Œ∑(a), kt )) = œÄŒæ,Œ∑(1) (a, kt ), . . . , œÄŒæ,Œ∑(N ) (a, kt ) ,
that is the strategy for the current round only depends on the actions a played in the
last round each signal was observed, and on the current coordination signal.
In Section 3, we will be concerned with equilibrium strategies for the resource allocation
game. That is, we will look at its symmetric subgame-perfect equilibria. To construct
such equilibria, we define the concepts of an equilibrium convention, and its equilibrium
implementation.
332

Symmetric Subgame-Perfect Equilibria in Resource Allocation

Definition 21. (Equilibrium convention) Let G be an infinitely repeated game, and let
Œæ be some convention. We say that the convention Œæ is an equilibrium convention when for
every vector of pure-strategy Nash equilibria a = (a1 , . . . , aK ), Œæ(a) is a vector of subgameperfect equilibria of the game G.
Definition 22. (Equilibrium implementation) Let G be an infinitely repeated game, Œæ
some equilibrium convention, and œÄŒæ an implementation of convention Œæ. We say that œÄŒæ is
an equilibrium implementation if it is a subgame-perfect equilibrium.

3. Resource Allocation Game
In this section, we will first formally define the resource allocation game, and discuss its
Nash equilibria. We will then show that for any equilibrium convention of the resource
allocation game, there exists an equilibrium implementation.
3.1 Definitions
We will first define the resource allocation game, and restricted notions of uniform convention and uniform implementation.
Definition 23. (Resource allocation game) A resource allocation game GN,C is a game
of N agents. Each agent i can access one of C identical resources. The agent chooses its
action ai from Ai = {Y, A1 , A2 , . . . , AC }, where action ai = Y means to yield, and action
ai = Ac means to access resource c. Because all resources are identical, we can define a
special meta-action ai = A. To take action A means to choose to access, and then to choose
the resource uniformly at random from the set of available resources.
The payoff function for agent i is defined as follows:
Ô£±
0
if ai = Y
Ô£¥
Ô£¥
Ô£≤
1
if ai 6= Y,
ui (a1 , . . . , ai , . . . , aN ) :=
(4)
‚àÄj
6= i, aj 6= ai
Ô£¥
Ô£¥
Ô£≥
‚àíŒ≥ < 0 otherwise
This game has a set of pure strategy NEs where C agents each access a resource ci and
N ‚àí C agents do not. There is also a symmetric mixed strategy NE in which each agent
decides to play action A with probability
s
(
! )
|Œ≥|
Pr(ai > 0) := min C ¬∑ 1 ‚àí N ‚àí1
,1 .
(5)
1 + |Œ≥|
Note that for high enough values of C, all agents will always choose to access. 1
Since we assume that the resources are identical, when the agents start following a
convention, their expected future payoff shouldn‚Äôt depend on which resource they have
1. The resource allocation game as defined here is an instance of a class of games known as potential games
(Monderer & Shapley, 1996). In an (exact) potential game, there exists a potential function Œ¶ : A ‚Üí R
00
such that ‚àÄa‚àíi ‚àà A‚àíi , ‚àÄa0i , ai ‚àà Ai ,
Œ¶(a0i , a‚àíi ) ‚àí Œ¶(a00i , a‚àíi ) = ui (a0i , a‚àíi ) ‚àí ui (a00i , a‚àíi ).

333

Cigler & Faltings

accessed in the Nash equilibrium. We will therefore restrict ourselves to so-called uniform
conventions:
Definition 24. (Uniform convention) Let GN,C be a resource allocation game, and GN,C
its infinitely repeated version. Let Œæ be a convention. We say that the convention Œæ is a
uniform convention, if the following holds: Let a = (a1 , a2 , . . . , aK ) be a vector of purestrategy Nash equilibria for each coordination signal. Let for each player i, ci the number
of signals for which player i accesses some resource in action vector a. Then
‚àÄi, j : ci = cj =‚áí Ei (Œæ(a)) = Ej (Œæ(a)).
That is, if the number of signals for which the two players access some resource is the
same, their expected payoff in the remainder of the game has to be the same too.
Definition 25. (Losers, winners, claimed and unclaimed resources) Let GN,C be
an infinitely repeated resource allocation game, let ht be the history of play in round t, and
let ak = (ak1 , ak2 , . . . , akN ) be the action vector played in the last round when signal k was
observed.
‚Ä¢ Player i is a winner for signal k if aki = Ai and for all other players j 6= i, akj 6= Ai ;
‚Ä¢ Player i is a loser for signal k otherwise;
‚Ä¢ Resource c is claimed for signal k, if there exists exactly one player i such that aki = Ac ;
‚Ä¢ Resource c is unclaimed for signal k otherwise.
If signal k was never observed before, all the players are losers and all the resources are
unclaimed for signal k.
Definition 26. (Uniform implementation) Let GN,C be an infinitely repeated resource
allocation game, let Œæ be some uniform convention. A uniform implementation œÄŒæ is defined
as follows: Let ht be the history of the game at time t, let kt be the signal observed in the
current round.
1. If the players have already played some pure-strategy Nash equilibrium for all coordination signals follow the strategy prescribed by the convention Œæ.
2. Otherwise, let n be the number of losers for signal kt , and let c be the number of
unclaimed resources for signal k. The strategy prescribed by implementation œÄŒæ in
round t is then the following:
For an action vector a such that there are co occupied resources, and nA agents who access some resource,
the exact potential function of the resource allocation game is
Œ¶(a) := co + Œ≥ ¬∑ (nA ‚àí co ).
Exact potential games are also referred to as congestion games (Rosenthal, 1973). Finite versions
of such games are always guaranteed to have a pure-strategy Nash equilibrium. Moreover, agents can
reach a pure-strategy Nash equilibrium by starting from an arbitrary action vector a0 and iteratively
playing best-response action, one by one. When the players are anonymous and update their strategies
simultaneously, as we study in this paper, this doesn‚Äôt hold. Hence, the theory of potential games cannot
be applied to the scenario we study in this paper.

334

Symmetric Subgame-Perfect Equilibria in Resource Allocation

Agents follow implementation œÄ

‚ÄúAsynchrony‚Äù

Initial state

Agents follow
convention Œæ

n=4
c=3

n=3
c=2

n=2
c=1

n=1
c=0

Figure 1: Learning to play a convention in a resource allocation game with N = 4 agents
and C = 3 resources. Under each state, we denote the number of losers in the
current state n, and the number of unclaimed resources c. Winners are denoted
as black circles, losers as light grey circles. In the asynchrony state, there are
3 winners and one loser. Arrows indicate the possible transitions between the
states. Once the players reach the asynchrony state, they start following the
convention from the next round on.

‚Ä¢ If player i is a winner for signal kt , she will access the same resource as she did
the last time signal kt was observed.
‚Ä¢ If player i is a loser, she will access choose to access an unclaimed resource r
p
with probability 0 ‚â§ (n,c)
‚â§ 1. The probability of accessing a claimed resource
c
is zero.
In the remainder of this section, instead of studying general strategies for the repeated
game, we will limit ourselves to strategies which are a uniform implementation.
Figure 1 shows how the agents learn to follow a convention when N = 4 and C = 3.
Assume that the players adopt a convention Œæ, and they use its implementation œÄ. Initially,
they are all ‚Äúlosers‚Äù, and the implementation prescribes the same strategy to all of them.
Once an agent accesses some resource alone, she becomes a winner and will access the same
resource until the agents reach an asynchrony round (a state where each resource is accessed
by exactly one agent).
Definition 27. (Expected payoff functions EA and EY ) Let GN,C be an infinitely
repeated resource allocation game, let Œæ be some uniform convention and œÄŒæ its equilibrium
implementation. Let ht be the history of the game in round t, such that some k ‚àà K, the
Nash equilibrium has not been reached (and so the convention has not been activated yet).
Let nk the number of losers for signal k ‚àà K and ck the number of unclaimed resources
for signal k ‚àà K. Let p = (pn1 ,c1 , . . . , pnK ,cK ) be the access probability of the losers for
335

Cigler & Faltings

each signal k ‚àà K. Let kt be the currently observed coordination signal. Let wŒæ (nw ) be the
expected payoff of a new winner (player who was a loser in previous rounds and becomes
winner in round t) given that there are nw new winners in round t. Let lŒæ (nw ) be the
expected payoff of a player who stays a loser, when there are nw new winners in round t.
Assume that player Œ± is a loser for signal kt . We define her expected payoff functions EA
and EY when she takes action A (or Y ) for signal kt , and adopts the strategy prescribed
by the implementation œÄŒæ for other signals:
EA (p, kt ) :=
min(n,c)

X

[Pr(Œ± wins & nw winners|A)wŒæ (nw ) + Pr(Œ± loses & nw winners|A)(‚àíŒ≥ + lŒæ (nw ))]

nw =1

Ô£Æ

Ô£´

Ô£∂Ô£π

K
X
Ô£Ø
Ô£∑Ô£∫
Œ¥ Ô£¨
Ô£Ø
Ô£¨
Ô£∑Ô£∫
+ Pr(0 winners|A) ¬∑ Ô£∞‚àíŒ≥ +
E
(p,
k)
+
(p
E
(p,
l)
+
(1
‚àí
p
)E
(p,
l))
n
,c
n
,c
A
A
Y
l
l
l
l
Ô£∏Ô£ª
KÔ£≠
l=1
l6=k

(6)
min(n,c)

EY (p, k) :=

X

Pr(nw winners|Y ) ¬∑ lŒæ (nw )

nw =1

Ô£´

Ô£∂

K
X
Ô£∑
Œ¥ Ô£¨
Ô£¨EY (p, k) +
(pnl ,cl EA (p, l) + (1 ‚àí pnl ,cl )EY (p, l))Ô£∑
+ Pr(0 winners|Y ) ¬∑
Ô£≠
Ô£∏
K

(7)

l=1
l6=k

3.2 Existence of an Equilibrium Implementation
We are now ready to prove that for any uniform equilibrium convention, there exists its
equilibrium implementation.
Lemma 1. For any signal k ‚àà K, the functions EA and EY are continuous in p ‚àà h0, 1iK .
Proof. The probabilities Pr(nw winners|A) and Pr(nw winners|Y ) are continuous. The
functions EA and EY are sums of products of continuous functions, so they must be themselves continuous.
Lemma 2. Functions EA and EY are well-defined for any k ‚àà K and p ‚àà h0, 1iK .
Proof. For fixed p, Œ≥ and Œ¥, the functions EA and EY define a system of 2K linear equations.
We can write this system as (I ‚àí A)E = b, where E = (EA,1 , . . . , EA,K , EY,1 , . . . , EY,K ) is
a vector of variables corresponding to the payoff functions, b ‚àà R2K and I is 2K √ó 2K unit
matrix. The matrix A is defined as follows: The first K rows correspond to variables EA,k
and the second K rows correspond to variables EY,k .
The elements in row k corresponding to EA,k are defined as:
Ô£±
for l = k
Pr(0 winners|A, pnk ,ck ) ¬∑ KŒ¥
Ô£¥
Ô£¥
Ô£≤
0
for l = K + k
Ak,l :=
Œ¥
Pr(0
winners|A,
p
)
¬∑
¬∑
p
for l ‚â§ K, l 6= k
Ô£¥
nk ,ck
nl ,cl
Ô£¥
K
Ô£≥
Œ¥
Pr(0 winners|A, pnk ,ck ) ¬∑ K ¬∑ (1 ‚àí pnl ,cl ) for K < l ‚â§ 2K, l 6= K + k
336

Symmetric Subgame-Perfect Equilibria in Resource Allocation

The elements in row K + k corresponding to EY,k are defined as:

AK+k,l

Ô£±
Pr(0 winners|Y, pnk ,ck ) ¬∑
Ô£¥
Ô£¥
Ô£≤
0
:=
Pr(0
winners|Y, pnk ,ck ) ¬∑
Ô£¥
Ô£¥
Ô£≥
Pr(0 winners|Y, pnk ,ck ) ¬∑

Œ¥
K
Œ¥
K
Œ¥
K

for
for
for
¬∑ pnl ,cl
¬∑ (1 ‚àí pnl ,cl ) for

l =K +k
l=k
l ‚â§ K, l 6= k
K < l ‚â§ 2K, l 6= K + k

This system of equations has a unique solution if the matrix A is non-singular. This is
equivalent to saying that det(A) 6= 0.
PK
The matrix A is diagonally dominant, that is aii >
j=1,j6=i |aij |. This is because
P2K
0 < Œ¥ < 1, and the rows of the matrix A sum to l=1 Ak,l = Œ¥ ¬∑ Pr(0 winners|A, pnk ,ck ) for
P
1 < k ‚â§ K, and 2K
l=1 AK+k,l = Œ¥ ¬∑ Pr(0 winners|Y, pnk ,ck ) for K < K + k ‚â§ 2K.
It is known that diagonally dominant matrices are non-singular (Taussky, 1949). Therefore, a unique solution E of the system exists and the functions EA , EY are well-defined for
a fixed p, Œ¥ and Œ≥.
Lemma 3. There exists p‚àó such that for any k ‚àà K, one of the following is true:
1. p‚àók = 0 and EY (p‚àó , k) > EA (p‚àó , k);
2. p‚àók = 1 and EA (p‚àó , k) > EY (p‚àó , k);
3. 0 < p‚àók < 1 and EA (p‚àó , k) = EY (p‚àó , k).
Such p‚àó defines a symmetric best-response strategy for the losers.
Proof. Fix Œ≥ and Œ¥. We will show that for an arbitrary p and every signal k ‚àà K, there
exists p0k which satisfies one of the three conditions of the Lemma 3 above.
For contradiction, assume that for p0k = 0, EY (p0 , k) ‚â§ EA (p0 , k) and for p0k = 1,
EA (p0 , k) ‚â§ EY (p0 , k). Then from the fact that both functions EA and EY are well-defined
and continuous for 0 ‚â§ pk ‚â§ 1, they must intersect for some 0 < p0k < 1.
From this, there must exist a vector p‚àó where for all k ‚àà K, the conditions of the
Lemma 3 are satisfied.
Corollary 1. Let GN,C be an infinitely repeated resource allocation game. For any uniform
equilibrium convention Œæ of the game GN,C , there exists an equilibrium implementation œÄŒæ .
To illustrate the different equilibrium payoffs agents can get when they adopt different
conventions, consider the resource allocation game with N = 4 agents and C = 1 (to
simplify the presentation, assume that K = 1). Assume that before round t, the resource
has been claimed yet, so there are n = 4 losers and c = 1 unclaimed resource. If some
agent becomes a winner in round t, the agents adopt an extended uniform convention that
prescribes their strategies from then on.
For comparison, assume that the agents can adopt either a convention ŒæÀÜ1 , or a convention
ÀÜ
Œæ2 . If they adopt convention ŒæÀÜ1 , the winners have an expected payoff wŒæÃÇ1 = 4, and the losers
an expected payoff l = 0. On the other hand, if they adopt convention ŒæÀÜ2 , the winners
ŒæÃÇ1

have an expected payoff wŒæÃÇ2 = 2, and the losers an expected payoff lŒæÃÇ2 = 1.
337

Cigler & Faltings

(a) Convention ŒæÀÜ1

(b) Convention ŒæÀÜ2

Figure 2: Example of expected payoff functions for resource allocation game with N = 4
agents, C = 1 resources, cost of collision Œ≥ = 2 and discount factor Œ¥ = 0.8,
1 and E 1 are expected payoff
given the access probability p. The function EA
Y
functions of accessing and yielding, when the agents use an extended convention
2 and E 2 are expected payoff functions when the agents use an
ŒæÀÜ1 . Similarly, EA
Y
extended convention ŒæÀÜ2 . Convention ŒæÀÜ1 has an expected winner payoff wŒæÃÇ1 = 4,
and expected loser payoff l = 0. Convention ŒæÀÜ2 has an expected winner payoff
ŒæÃÇ1

wŒæÃÇ2 = 2 and expected loser payoff lŒæÃÇ2 = 1.
In the equilibrium implementation œÄ1 of the convention ŒæÀÜ1 , the agents access
the resource with probability p‚àó1 , and their expected payoff is E1‚àó = 0. In the
equilibrium implementation œÄ2 of the convention ŒæÀÜ2 , the agents access the resource
with probability p‚àó2 < p‚àó1 , and their expected payoff is E2‚àó > E1‚àó = 0.

1 and E 1 for the convention ŒæÀÜ , and E 2
Figure 2 shows the expected payoff functions (EA
1
Y
A
2
ÀÜ
and EY for the convention Œæ2 ), depending on the access probability p. We can see that the
equilibrium implementation payoff E2‚àó of the convention ŒæÀÜ2 is higher than the equilibrium
payoff E1‚àó of the convention ŒæÀÜ1 , even though the sum of the winner and loser payoffs is higher
for convention ŒæÀÜ1 . This is because the loser receives a positive payoff when the agents adopt
a convention ŒæÀÜ2 ; the agents are less likely to ‚Äúfight‚Äù to become a winner, and they access
the resource with a lower probability p‚àó2 < p‚àó1 . This way, there will be less collisions, and
the agents will receive a higher expected social payoff when they adopt the convention ŒæÀÜ2 .

3.3 Calculating the Equilibrium
While the symmetric subgame-perfect equilibrium is guaranteed to exist, in order to actually
play it, the agents need to be able to calculate it. It is not always possible to obtain the
338

Symmetric Subgame-Perfect Equilibria in Resource Allocation

closed form of the probability of accessing a resource. Therefore, we will show how to
calculate the equilibrium strategy numerically.
Let p be a probability vector and k a signal. Let p0 := (p1 , p2 , . . . , pk = 0, . . . , pK ), i.e.
vector p with pk set to 0. Let p1 := (p1 , p2 , . . . , pk = 1, . . . , pK ). From Lemma 3 we know
that either EY (p0 , k) > EA (p0 , k), or EA (p1 , k) > EY (p1 , k) or the two functions intersect
for some 0 ‚â§ pk ‚â§ 1. Furthermore, we know that EA (p0 , k) = wŒæÃÇ (c) since the probability
of successfully claiming a resource is 1 when everyone else yields, and also EY (p0 , k) = 0.
Therefore, EY (p0 , k) > EA (p0 , k) iff wŒæÃÇ (c) > 0.
W.l.o.g, we will assume that wŒæÃÇ (c) > 0. Algorithm 1 shows then how to calculate the
probability vector.
Algorithm 1 Calculating the equilibrium probabilities
for Each subset S ‚äÜ {1, 2, . . . , K} do
Let Œ£ be a system of equations
‚àÄi ‚àà
/ S, Œ£ contains two equations for E(p, i). One corresponding to EA (p, i), one to
EY (p, i) (see Equations 6 and 7).
‚àÄj ‚àà S, we set pj := 1. Œ£ contains only one equation for E(p, j), corresponding to
EA (p, j).
So Œ£ is a system of 2K ‚àí |S| equations with 2K ‚àí |S| variables.
Solve numerically the system of equations Œ£.
if there exists a solution to Œ£ for which ‚àÄi ‚àà
/ S, 0 ‚â§ pi ‚â§ 1 then
We have found a solution
break;
end if
end for
The numerical algorithm has a complexity exponential in K, and is therefore only suitable for small K. In Section 4.2.3, we will show conditions under which the access probabilities are easy to compute and define a Œµ-equilibrium of the repeated game. That is, no
agent can gain more than Œµ factor more by deviating from the prescribed strategy.

4. Actual Conventions
In the previous section, we have shown that we can find a symmetric way to reach any
convention, provided the agents access the resources with a certain probability. We have
also shown how to calculate the resource access probability in every stage of the game. In
this section, we would like to show specific examples of the conventions that agents can
adopt, and discuss their properties.
4.1 Bourgeois Convention
The bourgeois convention is the simplest one. Once an agent has accessed a resource
successfully for the first time, he will keep accessing it forever. We say that the agent has
claimed the resource. We don‚Äôt need any coordination signal to implement it, so we can set
K := 1.
339

Cigler & Faltings

We will describe the decision problem from the point of view of agent Œ±. Assume that
there are N agents and C resources. At round t, let ct be the number of resources which
have not been claimed yet, and nt := N ‚àíC +ct the number of players who have not claimed
a resource yet. Assume that other players besides Œ± use the following strategy:
‚Ä¢ If a player has claimed a resource previously, she will keep accessing it;
‚Ä¢ If a player hasn‚Äôt claimed any resource yet (she is a ‚Äúloser‚Äù), she will choose to access
with probability pct and then choose the actual resource to access uniformly at random.
Definition 28. (Expected payoff function of the Bourgeois convention) Let p :=
(p1 , p2 , . . . , pC ) be a probability vector, such that pc is the probability with which any of
the losers will access when there are c unclaimed resources. We define the expected payoff
function to player Œ± should she choose to access (play A, that is choose to access and then
choose the resource uniformly at random) or yield (play Y ), respectively:



p n‚àí1
1
p n‚àí1
EA (p, c) := 1 ‚àí
¬∑
+ 1‚àí 1‚àí
¬∑ (‚àíŒ≥)
c
1‚àíŒ¥
c
c
X
Pr(Œ± loses and nw = l|A) ¬∑ E(p, c ‚àí l);
+Œ¥¬∑


l=0

EY (p, c) := Œ¥ ¬∑

c
X

Pr(nw = l|Y ) ¬∑ E(p, c ‚àí l);

l=0

In both equations, E(p, c) = max {EA (p, c), EY (p, c)}.
Lemma 4. For any p and 1 ‚â§ c ‚â§ C, E(p, c) ‚â• 0.
Proof. No matter what is the strategy of the opponents, if agent Œ± chooses to always yield,
its payoff will be 0.
Lemma 5. Let p be a probability vector which defines the strategies of the other losers, and
let ct be the number of unclaimed resources in round t. If ‚àÄc ‚â§ ct , EA (p, c) = EY (p, c),
then ‚àÄc ‚â§ ct , E(p, c) = 0.
Proof. When there are ct unclaimed resources in round t, in every following round t0 ‚â• t
there will be c ‚â§ ct unclaimed resources (in bourgeois convention, agents never release
claimed resources). If the agent Œ± is indifferent between actions Y and A in every round
following round t, that means that it is indifferent between a strategy of the subgame that
prescribes Y in every round and any other strategy. The (expected) payoff of the strategy
that prescribes always Y is 0. Therefore, the expected payoff of any other subgame strategy
must be 0 as well.
For the purpose of our problem, all the unclaimed resources are identical. Therefore the
only parameter of the losers‚Äô strategy is the probability with which the agents decide to
access ‚Äì the resource itself is then chosen uniformly at random. Lemma 5 shows a necessary
condition on p for agent Œ± to be indifferent. The following lemma shows that such p exists
and is unique.
340

Symmetric Subgame-Perfect Equilibria in Resource Allocation

Lemma 6. Assumeat timer
t there are
 ct unclaimed resources. Let for all c ‚â§ ct unclaimed
|Œ≥|
resources be p‚àóc = c 1 ‚àí n‚àí1 |Œ≥|+ 1
the probability with which the losers play A. Then
1‚àíŒ¥

for all c ‚â§ ct unclaimed resources, agent Œ± is indifferent between yielding and accessing.
For a given c, such probability is unique on the interval [0, c].
Proof. From Lemma 5 we know that when agent Œ± is indifferent (i.e. EA (p, c) = EY (p, c)),
it must be that E(p, c) = 0 for all 1 ‚â§ c ‚â§ ct .
From Definition 28, the expected profit to agent Œ± from playing A and then following
best-response strategy (with zero payoff) is




pc n‚àí1
1
pc n‚àí1
EA (p, c) = 1 ‚àí
¬∑
+ 1‚àí 1‚àí
¬∑ (‚àíŒ≥)
c
1‚àíŒ¥
c
(8)
+ Œ¥ ¬∑ Pr(Œ± loses and nw = 0|A) ¬∑ E(p, c).
Here pc is the probability with which the other losers access. We want EA (p, c) =
EY (p, c) = 0. This holds if p‚àóc is defined as in the lemma above.
Function EA is decreasing in pc on the interval [0, c], while function EY is constantly 0.
Therefore, their intersection is unique on an interval [0, c].
Lemma 7. Assume that all the opponents who haven‚Äôt claimed any resource access a resource with probability p < p‚àóc . Then it is best-response for agent Œ± to access.
Proof. The probability that agent Œ± claims successfully a resource after playing A is

p n‚àí1
Pr(claim some resource|A) := 1 ‚àí
(9)
c
This probability increases as p decreases. Therefore the expected profit of playing A is
increasing as p decreases, whereas the profit of playing Y stays 0.
Theorem 8. Define an agent‚Äôs strategy œÑ as follows: If there are c unclaimed resources,
play A with probability pc := min (1, p‚àóc ) (where p‚àóc is defined in Lemma 6). Then a joint
strategy profile œÑ = (œÑ1 , œÑ2 , . . . , œÑN ) where ‚àÄc, œÑc = œÑ is a subgame-perfect equilibrium of the
infinitely repeated resource allocation game.
Proof. From Lemma 6, if p‚àóc < 1, any agent is indifferent between playing Y and playing A,
therefore will happily follow strategy œÑ . From Lemma 7, if pc = 1 < p‚àóc , it is best response
for any agent to play A, just as the strategy œÑ prescribes.
Theorem 9. For all c ‚àà N, if pc = p‚àóc , E(p, c) = 0.
Proof. We will proceed by induction.
For c = 0, the expected payoff is trivially E(p, 0) = 0, because there are no free resources.
Let ‚àÄj < c, E(p, j) = 0 and pc = p‚àóc . If agent Œ± plays Y , the expected payoff is clearly 0
(it will be 0 now and 0 in the future by the induction hypothesis). If agent Œ± plays A, the
expected payoff is (by Definition 28):

pc n‚àí1
1
¬∑
EA (p, c) := 1 ‚àí
c
1‚àíŒ¥


c
(10)


X
pc n‚àí1
Pr(Œ± loses and nw = l|A) ¬∑ E(p, c ‚àí l)
+ 1‚àí 1‚àí
¬∑ (‚àíŒ≥) + Œ¥ ¬∑
c
l=0

341

Cigler & Faltings

Because of the way the p‚àóc is defined, and from the induction hypothesis E(p, j) = 0 for
j < c, we get
EA (p, c) := Pr(Œ± loses and nw = 0|A) ¬∑ E(c, œÑ‚àíŒ± )
= Œ¥ Pr(Œ± loses and nw = 0|A) ¬∑ max{EA (p, c), EY (p, c)}

(11)

Since Œ¥ ¬∑ Pr(Œ± loses and nw = 0|A) < 1, it must be that EA (p, c) = 0.
Theorem 10. If pc < p‚àóc , E(p, c) > 0.
Proof. From Lemma 7 we know that when pc < p‚àóc , it is a best response to access, so
E(p, c) = EA (p, c). From Lemma 4 we know that for all j, E(p, j) ‚â• 0. If pc < p‚àóc , by
Definition 28 we see that E(p, c) > 0.
Theorem 10 shows that if we have enough resources so that p‚àóc ‚â• 1, the expected payoff
for the agents, even when they access all the time, will be positive.
Given the number of agents N , discount factor Œ¥ and collision cost Œ≥, the necessary
number of resources c‚àó for the expected payoff to be positive is:
c‚àó :=
1‚àí

1
r
n‚àí1

|Œ≥|
1
|Œ≥|+ 1‚àíŒ¥

(12)

Figure 3 illustrates the value of c‚àó depending on N , Œ¥, and Œ≥ respectively. Figure 3a shows
how the number of resources c‚àó increases as N increases ‚Äì naturally, more agents need more
resources.
Figure 3b shows on the other hand that with an increasing discount factor Œ¥, the necessary number of resources drops. This is because for high Œ¥, the agents are almost indifferent
between winning now and winning later. In Section 4.2.3, we will explore this idea in more
detail ‚Äì we will show that for high enough delta, a strategy which prescribes the agents to
access with a constant probability until they reach the asynchrony is an Œµ-equilibrium of
the resource allocation game.
Finally, Figure 3c shows an increasing number of resources which are necessary for the
bourgeois convention to have positive payoff, as the collision cost Œ≥ increases. The increase
is almost linear in Œ≥. This is because the higher the cost of collision, the lower the expected
payoff of accessing EA . For the bourgeois convention to have positive expected payoff, we
need EA > 0 for all 0 ‚â§ p ‚â§ 1.
Let us now look at the price of anonymity for the bourgeois convention (as defined in
Definition 16).
Theorem 11. The price of anonymity of the bourgeois convention is infinite.
Proof. The highest social payoff any strategy profile œÑ can achieve in an N -agent, C-resource
allocation game (N ‚â• C) is
C
max E(œÑ ) :=
.
(13)
1‚àíŒ¥
This is achieved when in every round, every resource is accessed by exactly one agent. Such
strategy profile is obviously asymmetric.
342

Symmetric Subgame-Perfect Equilibria in Resource Allocation

25

20

200

20
15

150

15
C*

C*
10

C*
100

10
5
0
0

50

5
5

10
N

15

20

0
0

0.5
Œ¥

(a) N

1

0
0

50
Œ≥

(b) Œ¥

100

(c) Œ≥

Figure 3: Minimum number of resources c‚àó needed for the expected payoff of bourgeois
convention to be positive, depending on N , Œ¥, and Œ≥. One parameter is varying,
the other parameters are set to N = 10, Œ¥ = 0.8, Œ≥ = 2. For varying N , the
dashed line shows when c = N .

3.5
N=3
N=4
3

2.5
R
2

1.5

1
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Figure 4: Market convention: Price of anonymity for C = 1, K = N , Œ≥ = 0.5 and varying Œ¥.

If each agent knew which part of the bourgeois convention to play at the beginning of the
game, this convention would be socially efficient. However, when the agents are anonymous,
they have to learn which part of the convention they should play through randomization.
For the bourgeois convention for small C relative to N , this randomization is such that
the agents are indifferent between accessing some resource and yielding, and their expected
payoff of both is zero. Therefore, its price of anonymity is infinite.
4.2 Market Convention
We saw that the bourgeois convention leads to zero expected social payoff for a small
number of resources. We would like to improve the expected payoff here. In the bourgeois
convention, the agents receive zero expected payoff because the demand for resources is
too high compared to the supply. We need to decrease the demand, while increasing the
343

Cigler & Faltings

2.5
N=3
N=4

2
R

1.5

1
0

1

2

3

4

5

Figure 5: Market convention: Price of anonymity for C = 1, K = N , Œ¥ = 0.9 and varying Œ≥.

supply. This is often achieved through markets. Shneidman et al. (2005) present some of
the reasons why markets might be appropriate for resource allocation.
We assume the following:
‚Ä¢ Agents can observe K ‚â• 1 coordination signals.
‚Ä¢ Agents have a decreasing marginal utility when they access a resource more often.
‚Ä¢ They pay a fixed price per each successful access, to the point that each agent prefers
to access a resource only for one signal out of K. In practice, this could be implemented by a central authority that observes the convergence rate of the agents, and
dynamically increases or decreases the price to achieve convergence.
Such assumptions define what we call ‚Äúmarket‚Äù convention, where the winners only access their claimed resource for the signals they observed when they first claimed it. The
price the agents have to pay serves to decrease the demand. The coordination signal effectively increases the supply of resources K-times, because the resource allocation may be
different for each of the signal values.
We know that we can implement this convention for C ‚â• 1 resources using symmetric
play (see Section 3). For small K, we can also use Algorithm 1 to calculate the access
probabilities. For the ease of exposition, we will first describe the market convention for
C = 1 resource. Then we will generalize the description to C > 1 resources.
4.2.1 One Resource
When each agent only accesses the resource for one signal, we need K = N signals to make
sure everyone gets to access once.
In the N -agent, 1-resource case, imagine there are still n agents playing and (N ‚àí n)
agents who have already claimed the resource for some signal. Imagine that the n agents
observe one of the n signals for which no resource has been claimed.
344

Symmetric Subgame-Perfect Equilibria in Resource Allocation

Assume that all agents access the resource with probability pn . The expected payoff of
accessing a resource for agent Œ± is


Œ¥
1
EA (pn , n) := (1 ‚àí pn )
¬∑ 1+
¬∑
N 1‚àíŒ¥




Œ¥n
+ 1 ‚àí (1 ‚àí pn )n‚àí1 ¬∑ ‚àíŒ≥ +
EA (pn , n)
N ‚àí Œ¥(N ‚àí n)
n‚àí1



(14)

The expected payoff of yielding for agent Œ± is

EY (pn , n) := (n ‚àí 1)pn (1 ‚àí pn )n‚àí2 E(n ‚àí 1)


Œ¥n
EY (pn , n)
+ 1 ‚àí (n ‚àí 1)pn (1 ‚àí pn )n‚àí2
N ‚àí Œ¥(N ‚àí n)

(15)

When pn = 1, accessing a resource will always lead to a collision, so the payoff of
accessing will be negative. When pn = 0, accessing a resource will always claim it, so the
payoff of accessing will be positive. In the equilibrium, the agents should be indifferent
between accessing and yielding. Therefore, we want to find p‚àón such that EA (p‚àón , n) =
EY (p‚àón , n) = E(n).
Finding a closed form expression for p‚àón is difficult, but we can use Algorithm 1 to
calculate this probability, as well as the expected payoff E(n), numerically (albeit in practice
only for small K).
Figures 4 and 5 show the price of anonymity of the market convention (as defined in
Definition 16) for varying discount factor Œ¥, and varying cost of collision Œ≥, respectively.
From Section 4.1, the price of anonymity for C = 1 is ‚àû. In contrast, for the market
convention this price is in both cases finite and relatively small.
4.2.2 Multiple Resources
Assume now that C ‚â• 1. In any given round, we will denote c := (c1 , c2 , . . . , cK ) the
vector of resources which have not been claimed yet for each value of the coordination
signal k ‚àà {1, 2, . . . , K}. We will denote n the number of players who have not claimed any
resource yet for any signal value. Finally, let p := (pn,c1 , pn,c2 , . . . , pn,cK ) be the vector of
probabilities, where pn,ck denotes the probability that a loser will access some resource for
signal k ‚àà K, given that ck resources are available.
From Corollary 1, we know that for the market convention, there exists an equilibrium
implementation. But what does it look like exactly? In order to be able to express it (albeit
only numerically), we need to define the expected payoff functions the players receive for
each action.
345

Cigler & Faltings

For number of losers n, observed coordination signal k and vectors p and c, we can
define expected payoff functions when a player Œ± takes action A and Y , respectively:
EA (p, c, n, k) := Pr(Œ± wins|A) ¬∑ w + (1 ‚àí Pr(Œ± wins|A)) ¬∑ (‚àíŒ≥)
min(ck ,n)

+

X

Pr(Œ± loses, nw winners|A) ¬∑ E(p, (ck ‚àí nw , c‚àík ), n ‚àí nw )

nw =1

+ Pr(nw = 0|A) ¬∑

Œ¥
¬∑ EA (p, c, n, k)
K
Ô£Æ

Ô£ØŒ¥
+ Pr(nw = 0|A) ¬∑ Ô£Ø
Ô£∞K ¬∑

K
X
l=1
l6=k

(16)
Ô£π

Ô£∫
(pn,cl EA (p, c, n, l) + (1 ‚àí pn,cl )EY (p, c, n, l))Ô£∫
Ô£ª

min(n,c)

EY (p, c, n, k) :=

X

Pr(nw winners|Y ) ¬∑ E(p, (ck ‚àí nw , c‚àík ), n ‚àí nw )

nw =1

Ô£´
+ Pr(nw = 0|Y ) ¬∑

Ô£∂

K
X
Ô£∑
Œ¥ Ô£¨
Ô£¨EY (p, c, n, k) +
(pn,cl EA (p, c, n, l) + (1 ‚àí pn,cl )EY (p, c, n, l))Ô£∑
Ô£≠
Ô£∏
K
l=1
l6=k

(17)
In the equations above, E(p, c, n) is the expected payoff before the players observe the
coordination signal. It is defined as
E(p, c, n) :=

K
1 X
E(p, c, n, k).
K
k=1

Œ¥
The winner payoff w is defined as w := 1 + K¬∑(1‚àíŒ¥)
. This is because the winner will
access for only one signal: once in the current round, and than in any future round with
probability K1 .
What are the probabilities that there will be nw winners in each of the cases? We will
start with the simplest case, Pr(nw winners|Y ), given that there are n agents (including
agent Œ±), ck resources and all agents except Œ± play action A with probability pk .
The problem of calculating this probability is very similar to the well-known balls-andbins problem (Raab & Steger, 1998). In the balls-and-bins problem we assume that we
have n balls who are each randomly assigned into one of the c bins. The goal is to find a
probability that i bins will have exactly one ball in them. We will express this probability
as œÜ(n, c, i).
There are Ni ways to pick some i balls, place them into some i bins so that each bin
has one ball, and place the remaining n ‚àí i balls into the remaining c ‚àí i bins randomly,

  
c n
Ni :=
¬∑ i! ¬∑ (c ‚àí i)n‚àíi
i
i
346

(18)

Symmetric Subgame-Perfect Equilibria in Resource Allocation

There are a total of cn ways to arrange n balls into c bins. Therefore, the probability
œÜ(n, c, i) is The total number of ways to place n balls in c bins so that exactly i have one
ball can be then obtained from the generalized inclusion-exclusion principle:
1
œÜ(n, c, i) := n
c
1
= n
c
=

min(c,n)

X

j‚àíi

(‚àí1)

j=i
min(c,n)

X

j‚àíi

(‚àí1)

j=i

 
j
Nj
i

   
j
c n
¬∑ j! ¬∑ (c ‚àí j)n‚àíj
i
j
j

(19)



  min(c,n)
n‚àíj
X
n! c
j‚àíi c ‚àí i (c ‚àí j)
(‚àí1)
.
j ‚àí i (n ‚àí j)!
cn i
j=i

 
 c‚àíi
In the simplification above, we use the absorption identity ji jc = ci j‚àíi
.
How can we use the function œÜ to calculate Pr(nw winners|Y )? The n ‚àí 1 agents (other
than Œ±) decide to play action A with probability pk , and then choose the resource to access
randomly. The agents who choose to access a resource correspond to the balls-and-bins
problem. Therefore,
Pr(nw winners|Y ) :=

n‚àí1
X


n‚àí1 i
pk ¬∑ (1 ‚àí pk )n‚àí1‚àíi ¬∑ œÜ(i, c, nw ).
i

i=0

(20)

To calculate the probability Pr(Œ± wins|A), we can proceed as follows. We assume w.l.o.g
that Œ± accesses resource 1. There will be some i agents (out of n ‚àí 1) who will choose action
A. We then need all of them to choose other resource than 1. Therefore,
Pr(Œ± wins|A) :=

n‚àí1
X
i=0




n‚àí1
1 i
i
n‚àí1‚àíi
¬∑ pk ¬∑ (1 ‚àí pk )
1‚àí
c
i

(21)

Finally, to calculate the probability Pr(Œ± loses, nw winners|A), we can use again the
balls-and-bins problem. Given that there are 0 ‚â§ i ‚â§ n ‚àí 1 agents who choose action A,
there will be 0 ‚â§ j ‚â§ i agents who choose the same resource as agent Œ±. The remaining
(i ‚àí j) agents face the same balls-and-bins problem for c ‚àí 1 bins (1 bin is already occupied
by agent Œ±). Therefore,
Pr(Œ± loses, nw winners|A) :=

n‚àí1
i    j 
X n ‚àí 1
X
i
1
1 i‚àíj
1‚àí
¬∑ œÜ(i ‚àí j, c ‚àí 1, nw )
pik (1 ‚àí pk )n‚àí1‚àíi
i
j
c
c
i=1

j=1

(22)
Now that we have expressed the expected payoff functions EA and EY explicitly, we can
use Algorithm 1 to calculate the equilibrium access probabilities and expected payoffs.
Figures 6 and 7 show the price of anonymity of the market convention for C = 3, K = 2
and N = C ¬∑ K = 6. When the discount factor Œ¥ grows, the price of anonymity decreases
347

Cigler & Faltings

600

Price of anonymity

500

400

300

200

100

0
0.1

0.2

0.3

0.4

0.5
Œ¥

0.6

0.7

0.8

0.9

Figure 6: Market convention: Price of anonymity for N = 6, C = 3, K = 2, Œ≥ = 0.5 and
varying Œ¥.

2.6
2.5

Price of anonymity

2.4
2.3
2.2
2.1
2
1.9
1.8
1.7
0

1

2

3

4

5

Œ≥

Figure 7: Market convention: Price of anonymity for N = 6, C = 3, K = 2, Œ¥ = 0.9 and
varying Œ≥.

(note that in Figure 6 the y-axis is logarithmic). This is because for small Œ¥, the benefit
of winning the resource right away is much higher than the payoff of winning later. On
the other hand, as Œ¥ gets closer to 1, the agents don‚Äôt care whether they win now or later.
Since the market convention guarantees that everyone will be able to access some resource
for some signal value, when Œ¥ ‚Üí 1, the expected payoff of winner and losers will be the
same. Also, as Œ¥ ‚Üí 1, the cost the agents have to pay for learning the convention decreases
compared to the payoff they obtain after they have learnt it.
When Œ≥ increases, the price of anonymity increases. The cost of collision has a direct
effect on the expected payoff functions EA and EY . Therefore, the expected equilibrium
348

Symmetric Subgame-Perfect Equilibria in Resource Allocation

payoff will be higher if the cost is lower. Changing the Œ≥ has no effect on the optimal
asymmetric outcome though, since the agents don‚Äôt have to pay any cost because there are
no collisions.
4.2.3 Œµ-equilibrium
Calculating the equilibrium access probabilities for the market convention is difficult ‚Äì we
need to use a numerical algorithm, and as the number of signals K grows, the number of
equations grows exponentially. Therefore, we would like to find access probabilities which
are easy to compute and for which the agents‚Äô incentive to deviate is too small. Indeed,
game theory is often interested in Œµ-equilibria, in which no agent can improve her payoff by
more than Œµ > 0.
The market pricing ensures that each agent only wants to access a resource for one signal
value. It also doesn‚Äôt depend on the access probabilities of the agents, only on their utility
functions. Once the agents converge to the asynchrony round (i.e. a pure-strategy NE of
the resource allocation for every signal value), their future expected payoff will be
K ‚àí1
,
1‚àíŒ¥

(23)

and no agent can improve her payoff by deviating since the players are playing a PSNE of
the stage game in each round.
If the agents who haven‚Äôt claimed their resource yet play action A with a constant
probability 0 < pconst < 1, the expected time before the reach the asynchrony is finite.
(from the properties of the balls-and-bins problem, see Section 4.3, or Raab & Steger,
1998). We can prove the following theorem:
Theorem 12. Suppose that in the N -agent, C-resource allocation game, the agents adopt
the market convention with the following implementation: The agents who haven‚Äôt claimed
any resource yet play action A with a constant probability pconst (we call this the constantprobability implementation). Let E(Œ¥) be the expected payoff for each agent in this case for
a given discount factor Œ¥. Let E 0 (Œ¥) be the expected payoff of the best-response strategy to
this convention and implementation.
Then for any Œµ > 0, there exists 0 < Œ¥0 < 1 such that for all Œ¥, Œ¥0 ‚â§ Œ¥ < 1,
E(Œ¥)
> 1 ‚àí Œµ.
E 0 (Œ¥)

(24)

Proof. Because of the market pricing, each agent only wants to access one resource for one
value of the coordination signal. So the best-response payoff E 0 is
E 0 (Œ¥) ‚â§

K ‚àí1
,
1‚àíŒ¥

(25)

no matter what strategy do the other agents play.
When the agents adopt the market convention with the constant-probability implementation, then in every round until they converge to a PSNE, they receive a payoff between
Œ≥ < 0 (the collision cost) and 1. After they reach the PSNE, their expected payoff is
K ‚àí1
1‚àíŒ¥
349

(26)

Cigler & Faltings

as stated above. We can therefore say that
E(Œ¥) ‚â•

‚àû
X
i=0

Œ¥i
1 ‚àí Œ¥i
+ K ‚àí1 ¬∑
Pr(agents reach PSNE in i steps) ¬∑ Œ≥ ¬∑
1‚àíŒ¥
1‚àíŒ¥



(27)

We can define a random variable X such that X = i if the agents reach a PSNE after
exactly i steps. From the properties of the expected value, we can ee that
 
 
Œ≥ ¬∑ (1 ‚àí E Œ¥ X ) + K ‚àí1 ¬∑ E Œ¥ X
E(Œ¥) ‚â•
.
(28)
1‚àíŒ¥
The function œÜ(x) := Œ¥ x is a convex function. From Jensen‚Äôs inequality (1906), we know
that
 
E Œ¥ X ‚â• Œ¥ E[X] .
(29)
Therefore,

Œ≥ ¬∑ 1 ‚àí Œ¥ E[X] + K ‚àí1 ¬∑ Œ¥ E[X]
E(Œ¥)
‚â•
.
E 0 (Œ¥)
K ‚àí1

(30)

The expected time E[X] to reach the PSNE is finite and doesn‚Äôt depend on Œ¥, so we can
treat is as a constant. Because Œ¥ E[X] is continuous in Œ¥, monotonous and limŒ¥‚Üí1‚àí Œ¥ E[X] = 1,
we can see that for a given Œµ > 0, there exists 0 < Œ¥0 < 1 such that for all Œ¥, Œ¥0 ‚â§ Œ¥ < 1,
E(Œ¥)
> 1 ‚àí Œµ.
E 0 (Œ¥)

(31)

By ensuring that each agent only wants to access some resource for one signal value,
the market convention makes the cooperative strategy from our previous work (Cigler &
Faltings, 2011) almost rational.
4.3 Expected Convergence Time
In this section, we will analyze what is the expected number of rounds the agents need to
converge to a perfect allocation of resources (one where all resources are used by exactly one,
and there are no collisions). We will first prove an upper bound on the expected number
of steps to the convergence for the bourgeois convention, and then present experiments for
the market convention.
4.3.1 Bourgeois Convention
In order to prove the convergence of the bourgeois convention, we will describe its execution
as a Markov chain. A Markov chain describing the execution of the bourgeois convention in
N -agent, C-resource allocation game is a chain whose state at round t is Xt ‚àà {0, 1, . . . , C},
where Xt = c means that there are c unclaimed resources at round t.
We are interested in the expected number of rounds it will take to this Markov chain to
reach state 0 if it started in state C. This is called the expected hitting time:
350

Symmetric Subgame-Perfect Equilibria in Resource Allocation

Definition 29. (Norris, 1998) (Hitting time) Let (Xt )t‚â•0 be a Markov chain with state
space I. Given a probability space (‚Ñ¶, Œ£, Pr), the hitting time of a subset A ‚äÇ I is a random
variable H A : ‚Ñ¶ ‚Üí {0, 1, . . .} ‚à™ {‚àû} given by
H A (œâ) = inf{t ‚â• 0 : Xt (œâ) ‚àà A}
In general, the expected hitting time of a set of states A can be found by solving a
system of linear equations:
Theorem 13. The vector of mean hitting times k A = E(H A ) = (kiA : i ‚àà I) is the minimal
non-negative solution to the system of linear equations
 A
ki = 0 P
for i ‚àà A
(32)
A for i ‚àà
kiA = 1 + j ‚ààA
p
k
/A
ij j
/
Solving them analytically for our Markov chain is however difficult. Fortunately, when
the Markov chain has only one absorbing state i = 0, and it can only move from state i to
j if i ‚â• j, we can use the following theorem to derive an upper bound on the hitting time
(proved by Rego, 1992):
Theorem 14. Let A = {0}. If
‚àÄi ‚â• 1 : E(Xt+1 |Xt = i) <
for some Œ≤ > 1, then


kiA < logŒ≤ i +

i
Œ≤

Œ≤
Œ≤‚àí1

In order to use the Theorem 14, we need to calculate the expected state E(Xt+1 |Xt = c).
Lemma 15. Let Xt = c, and let there be n := N ‚àí C + c agents who have not claimed a
n‚àí1
that a resource i will be claimed in
resource yet. Let us denote q(n, c) = pc ¬∑ n ¬∑ 1 ‚àí pc
round t if the agents play the subgame-perfect equilibrium strategy vector described above.
Then the next expected state is
E(Xt+1 |Xt = c) := (1 ‚àí q(n, c)) ¬∑ c
Proof. For a resource i, we can denote Wi the random variable, where Wi = 1 if the resource
i has been claimed in round t, and Wi = 0 otherwise. The random variable Wi is Bernoullidistributed with probability q(n, c).
The next expected state is then
" c
#
c
X
X
E(Xt+1 |Xt = c) = c ‚àí E
Wi = c ‚àí
E[Wi ] = (1 ‚àí q(n, c)) ¬∑ c,
(33)
i=1

i=1

because E[Wi ] = q(n, c).
In the following lemmas, we will denote
Œª :=

|Œ≥|
1
|Œ≥| + 1‚àíŒ¥
351

(34)

Cigler & Faltings

Lemma 16. For a given collision cost Œ≥ and discount factor Œ¥, there exists a constant
0 < ¬µ < 1 such that for c ‚â§ ¬µ ¬∑ n, p‚àó < 1.
Proof. According
to the definition of the subgame-perfect equilibrium strategy, p‚àó := c ¬∑

‚àö 
n‚àí1
1‚àí
Œª .
We want p‚àó < 1, which is equivalent to

‚àö 
n‚àí1
c¬∑ 1‚àí
Œª <1
(35)


1 n‚àí1
1‚àí
<Œª
(36)
c
(37)
We know that c ‚â§ ¬µ ¬∑ n, so



n‚àí1
1 n‚àí1
1
1‚àí
‚â§ 1‚àí
‚â§ e‚àí¬µ .
c
¬µ¬∑n

(38)

If we therefore set ¬µ such that e‚àí¬µ < Œª, the access probability p‚àó < 1.
Lemma 17. For given Œ≥ and Œ¥, there exists 0 < Œ∑ < 1 such that for any c,
E(Xt+1 |Xt = c) ‚â§ (1 ‚àí Œ∑) ¬∑ c
‚àó
Proof. We will prove the lemma for two cases: when p‚àó <
 1 and‚àöwhen
 p = 1.
First, let us prove the case p‚àó < 1, that is p‚àó = c ¬∑ 1 ‚àí n‚àí1 Œª . Therefore, q(n, c) =

‚àö 
1 ‚àí n‚àí1 Œª ¬∑ n ¬∑ Œª. It can be shown that for any n,


q(n, c) = 1 ‚àí

‚àö 
Œª ¬∑ n ¬∑ Œª ‚â• ‚àíŒª log Œª.

n‚àí1

Now let p‚àó = 1. From Lemma 16 it must be that c > ¬µ ¬∑ n. Then



n‚àí1
c
1 n‚àí1
1
q(n, c) := ¬∑ 1 ‚àí
‚â•¬µ¬∑ 1‚àí
,
n
c
¬µ¬∑n
because q(n, c) is increasing with c.
Now

¬µ¬∑ 1‚àí

1
¬µ¬∑n

n‚àí1

‚â• ¬µ ¬∑ e‚àí¬µ .

(39)

(40)

(41)

For fixed Œ≥, Œ¥, the ¬µ and Œª are constants, so we can set Œ∑ as
Œ∑ := min(¬µ ¬∑ e‚àí¬µ , ‚àíŒª log Œª).

(42)

From above, this proves the lemma.
Theorem 18. The expected time for the agents to converge to a resource allocation where
all the resources are claimed is O(log C).
352

Symmetric Subgame-Perfect Equilibria in Resource Allocation

Proof. We have shown how we can express the expected convergence time as expected
hitting time of a certain Markov chain.
From Lemma 17 we saw that there exists Œ∑ such that for any c,
E(Xt+1 |Xt = c) ‚â§ (1 ‚àí Œ∑) ¬∑ c.
We can now combine this result with Theorem 14 to show that the expected hitting
time from the state C to state 0 is


1
1
0
kN < dlog 1 Ce + ‚âà O
¬∑ log C = O(log C),
(43)
1‚àíŒ∑
Œ∑
Œ∑
because Œ∑ is a constant.

4.3.2 Market Convention
For the market convention, it is unfortunately very difficult to express the expected number
of convergence steps in a closed-form expression. However, we can use linear programming
to calculate the expected number of convergence steps for a given parameters N , C, K, Œ≥
and Œ¥.
The Markov chain for the market convention for K signals and C resources looks as
follows: Its state at time t is Vt ‚àà {0, 1, . . . , C}K , where Vtk denotes how many resources
have not been claimed for signal k. The initial state V0 is such that V0k = C for all
k ‚àà {1, . . . , K}. If N ‚â• C ¬∑ K,Pthe final state is when Vtk = 0 for all k. When N < C ¬∑ K,
the final states are such that k‚àà{1,...,K} Vtk = C ¬∑ K ‚àí N .
The transition probabilities between two states Vi and Vj , Vi 6= Vj , are the following:
Suppose ‚àÉk : Vjk < Vik and ‚àÄl 6= k : Vjl 6= Vil . Let us denote c := Vik , i.e. the number of
unclaimed resources in state Vi for signal k, and n := N ‚àí (C ‚àí Vik ) the number of agents
who have not claimed any resource for signal k in state Vi .
Pr(Vt+1

n  
1 X n m
= Vj |Vt = Vi ) :=
p (1 ‚àí pk )n‚àím ¬∑ œÜ(m, c, Vik ‚àí Vjk )
K
m k

(44)

m=0

Otherwise if Vj 6= Vi , Pr(Vt+1 = Vj |Vt = Vi ) := 0.
Figure 8 shows the expected number of rounds to converge for varying discount factor
Œ¥. Generally, we would expect the access probability to increase with increasing Œ¥, since
the profit from winning the resource increases. This should then increase the convergence
time. However, in our experiments the influence of Œ¥ on the convergence time is negligible,
although we can observe a slight increase as Œ¥ increases. Figure 9 shows the convergence for
varying collision cost Œ≥. For Œ≥ close to 0, the convergence time remains stable. However, for
very high cost Œ≥, the convergence time increases linearly with Œ≥. In this case, the high cost
of collision drives the resource access probability low, because agents try to avoid collisions
‚Äúat all costs‚Äù.
Figure 10 shows the expected convergence when we increase number of resources C and
number of agents N proportionally. The increase in convergence time is still sub-linear to
the increase in C.
353

Cigler & Faltings

7.8

Convergence steps

7.78
7.76
7.74
7.72
7.7
7.68
7.66
0

0.2

0.4

0.6

0.8

1

Œ¥

Figure 8: Market convention: Expected number of convergence steps given N = 6, C = 3,
K = 2, Œ≥ = 1.0 and varying Œ¥.

50
45

Convergence steps

40
35
30
25
20
15
10
5 ‚àí4
10

‚àí2

10

0

10
Œ≥

2

10

4

10

Figure 9: Market convention: Expected number of convergence steps given N = 6, C = 3,
K = 2, Œ¥ = 0.9 and varying Œ≥.

354

Symmetric Subgame-Perfect Equilibria in Resource Allocation

13
12

Convergence steps

11
10
9
8
7
6
5
4
1

1.5

2

2.5

3
C

3.5

4

4.5

5

Figure 10: Market convention: Expected number of convergence steps given K = 2, Œ¥ = 0.9,
Œ≥ = 1.0 and varying number of resources C and agents N = 2 ¬∑ C.

C&F‚Äô11
Bourgeois
Egalitarian2
Market

ex-post fair
(X)1
no
X
X

efficient
X
no
X
?

equilibrium
no
X
X
X

Table 1: Properties of conventions
4.4 Convention Properties
We compare the properties of the following conventions: C&F‚Äô11, a channel allocation
algorithm presented in our previous work (Cigler & Faltings, 2011); bourgeois and egalitarian
conventions, presented by Bhaskar (2000); and market convention, presented in above.
We compare the conventions according to the following properties:
Ex-post fairness Is the expected payoff to all agents the same even after asynchrony?
Efficiency Does the convention maximize social welfare among all possible conventions?
Equilibrium Does the convention have an equilibrium implementation?
Table 1 summarizes the properties of the conventions. The C&F‚Äô11 convention is only
approximately ex-post fair. The fairness is improving as the number of coordination signals
increases, but some agents might have a worse payoff than others. On the other hand,
it is efficient, at least with no discounting (Œ¥ = 1). However, it is not an equilibrium
1. Fair asymptotically, as N ‚Üí ‚àû.
2. Only for 2-agents, 1-resource games.

355

Cigler & Faltings

of the repeated game. The bourgeois convention is neither fair nor efficient, in fact the
expected payoff to the agents is 0 (for a small number of resources). It is has an equilibrium
implementation though, since the agents are indifferent between being a winner and a loser.
The egalitarian convention is fair, efficient and has an equilibrium implementation. However,
it is only defined for games of 2 agents and 1 resource. Finally, the market convention is fair
and also has an equilibrium implementation. It is clearly more efficient than the bourgeois
convention. Nevertheless, finding the most efficient convention remains an open problem.

5. Folk Theorems and Symmetric Equilibria
In the previous sections, we have analyzed a special kind of symmetric equilibria of the
resource allocation game. The agents first followed a Markovian implementation, and as
soon as they play a pure-strategy NE, they adopted a convention. For an infinitely repeated
game with discounting, the set of Nash equilibria can be characterized using the so-called folk
theorem. While their name indicates that they have been known and used well before they
were first published, we will follow the version described by Fudenberg and Maskin (1986).
Informally, the folk theorem states that in the infinitely repeated game, for every feasible
and individually rational payoff vector of the stage game, there exists a Nash equilibrium
of the repeated game where the average payoffs per round correspond to the stage game
payoff vector.
A payoff vector is individually rational if it Pareto-dominates the minimax payoff of the
stage game. For player i, the minimax payoff is
vi‚àó := min max ui (œái , œá‚àíi ).
œá‚àíi

œái

(45)

To simplify the notation, Fudenberg and Maskin (1986) normalize the payoffs so that
‚àó ) = (0, 0, . . . , 0). Let
for the minimax payoffs, (v1‚àó , v2‚àó , . . . , vN
U := {(v1 , . . . , vN )|‚àÉ(a1 , . . . , aN ) ‚àà A‚àû √ó . . . √ó AN s.t. u(a1 , . . . , aN ) = (v1 , . . . , vN )},
V := convex hull of U,
V ‚àó := {(v1 , . . . , vN ) ‚àà V |vi > 0 for all i}.
The set V is the set of feasible payoffs in the stage games (that is, payoffs which can be
achieved by playing some mixed or correlated strategy). The set V ‚àó is the subset of feasible
payoffs which are also individually rational.
Theorem 19. (Fudenberg & Maskin, 1986) For any (v1 , . . . , vN ) ‚àà V ‚àó , if the discount
factor Œ¥ is close enough to 1, there exists a Nash equilibrium of the infinitely repeated game
with discounting where, for all i, the average payoff to player i is vi .
The idea of the proof is as follows: The agents cycle through a prescribed sequence of
game outcomes so that they achieve the desired payoffs. If one player deviates, the others
punish him by playing the minimax strategy forever after.
Our focus so far has been on finding symmetric equilibrium strategies. The folk theorem doesn‚Äôt say anything about whether the equilibrium strategy will be symmetric, even
if the payoff vector is symmetric. Nevertheless, we can define another class of symmetric
356

Symmetric Subgame-Perfect Equilibria in Resource Allocation

strategies of the infinitely repeated game, than the one based on conventions and their
implementations. The strategies have the following form: The players follow a symmetric
(mixed) strategy of the stage game. If one player deviates from this strategy, other players
punish her by following the minimax strategy. For the resource allocation game, the minimax payoff is (0, 0, . . . , 0) and is achieved in the mixed strategy Nash equilibrium. From
the Folk theorem, such strategy can be sustained as the Nash equilibrium of the repeated
game (though not necessarily a subgame-perfect equilibrium).
A symmetric strategy of the stage resource allocation game is a vector of access probabilities q = (q1 , q2 , . . . , qC ) where qc is the probability that each agent will access resource c.
We are interested in finding access probability vector q‚àó which achieves the highest expected
payoff.
For a given access probability vector q, the expected payoff that an agent receives is as
follows:
C
X



E(q) :=
qj ¬∑ (1 ‚àí qj )N ‚àí1 ¬∑ 1 ‚àí 1 ‚àí (1 ‚àí qj )N ‚àí1 ¬∑ |Œ≥|
(46)
j=1

Theorem 20. For a resource allocation game with N = 2 agents and C = 1 resource, the
resource access probability which maximizes the expected payoff of the stage game is
q‚àó =

1
1
¬∑
.
2 1 + |Œ≥|

(47)

Proof. We calculate the derivative of expected payoff function from Equation 46 for N = 2
and C = 1:
‚àÇE(q)
= 1 ‚àí 2q ¬∑ (1 + |Œ≥|)
‚àÇq
Setting the first derivative equal to 0, we get
q‚àó =

1
1
¬∑
.
2 1 + |Œ≥|

Since the second derivative is
‚àÇ 2 E(q)
= ‚àí1 ‚àí |Œ≥| < 0,
‚àÇ2q
the probability q ‚àó is a local maximum of the expected payoff function E(q).
Corollary 2. For a resource allocation game with N = 2 agents and C = 1 resource, the
highest expected payoff of a symmetric strategy in the stage game is
E‚àó =

1
1
¬∑
.
4 1 + |Œ≥|

(48)

Corollary 3. For a resource allocation game with N = 2 agents and C = 1 resource, the
price of anonymity of the strategy which accesses with the optimal access probability q ‚àó is
4 ¬∑ (1 + |Œ≥|).
357

Cigler & Faltings

2

10

Folk theorem
Market convention

R 1
10

0

10

0

1

2

3

4

5

Œ≥

Figure 11: Price of anonymity of the symmetric strategy following from the folk theorem,
compared to the price of anonymity of the market convention for N = 3, C = 1
and varying cost of collision Œ≥.

For the general case of resource allocation game with N agents and C resources, we
can
46 (given the constraint
PC find the probability vector which maximizes the Equation
2
j=1 qj ‚â§ 1) using the method of Lagrangian multipliers .
P
2. Our goal is to maximize E(q1 , q2 , . . . , qC ) such that q0 + C
i=1 qi = 1 and qi ‚â• 0, where q0 is the
probability that an agent yields. The Lagrangian function is then
Œõ(q0 , q1 , . . . , qC , Œª) := E(q1 , q2 , . . . , qC ) + Œª ¬∑

q0 +

C
X

!
qi ‚àí 1 .

i=1

The first partial derivatives are
‚àÇŒõ
:= Œª
‚àÇq0
‚àÇŒõ
‚àÇE
:=
+Œª
‚àÇqi
‚àÇqi

(49)
(50)

= (1 ‚àí |Œ≥|) ¬∑ (1 ‚àí qi )N ‚àí1 ‚àí qi ¬∑ (1 + |Œ≥|) ¬∑ (N ‚àí 1) ¬∑ (1 ‚àí qi )N ‚àí2 ‚àí |Œ≥| + Œª for 1 ‚â§ i ‚â§ C
‚àÇŒõ
:= q0 +
‚àÇŒª

C
X

‚àí1.

(51)
(52)

i=1

A necessary condition for a solution to be a maximum
is that the partial derivatives of the Lagrangian
P
function are 0. Therefore, Œª := 0 and q0 := 1 ‚àí C
i=1 qi . For qi , 1 ‚â§ i ‚â§ C we can find the solution to
‚àÇŒõ
= 0 using a numerical root-finding algorithm. If there is no point where the partial derivatives are
‚àÇqi
zero, we have to compare E(q) for a (finite) number of extreme points.

358

Symmetric Subgame-Perfect Equilibria in Resource Allocation

Figure 11 compares the price of anonymity of the folk-theorem-based symmetric strategy
with the price of anonymity of the market convention, for N = 3 agents and C = 1 resource.
Since the price of anonymity of the folk theorem strategy doesn‚Äôt depend on the discount
factor Œ¥ (it only needs to be high enough for the strategy to be an equilibrium), we only show
the graph for varying collision cost Œ≥. The price of anonymity of the folk-theorem strategy
is an order of magnitude higher than the price of anonymity of the market convention.

6. Conclusions
In this paper, we considered symmetric protocols to rationally coordinate on an asymmetric,
efficient allocation infinitely repeated resource allocation game with discounting of N agents
and C resources. We assumed that the agents are identical, and that the resources are
homogeneous. We based our work on the idea of Bhaskar (2000): we let the agents choose
their actions randomly, after which they adopt a certain convention. We generalize the work
of Bhaskar to an arbitrary resource allocation problem with N agents and C resources. We
show that for any convention, there exists a symmetric subgame-perfect equilibrium that
implements it. We presented two such conventions for the repeated resource allocation
game: bourgeois and market convention. We defined the price of anonymity as the ratio
between the expected social payoff of the best asymmetric strategy profile and the expected
social payoff of a given symmetric Nash equilibrium. We showed that while the price of
anonymity for the bourgeois convention is infinite (at least for small number of resources),
the price of anonymity of the market convention is finite and relatively small. This is because
the market convention reduces the demand by imposing a price on successful access, while
at the same time increasing the supply by having the agents condition their strategy on
a global coordination signal k ‚àà {1, . . . , K}. This way, the conflict between the agents is
reduced. We also showed analytically that when the agents adopt the bourgeois convention,
they will converge to a perfect resource allocation in polynomial time.
For the market convention, calculating the equilibrium access probabilities is difficult.
We need to use a numerical algorithm, whose complexity is exponential in the number of
coordination signals K. However, the market mechanism already makes sure that no agent
wants to access some resource for more than one coordination signal. Therefore, we showed
that the cooperative solution where agents access the resources with constant probability
is an Œµ-equilibrium, given that the discount factor Œ¥ is high enough.
In the future work, we would like to investigate whether there exist more efficient conventions than the market convention (i.e. conventions with smaller price of anonymity). In
general, finding an optimal convention is an NP-hard problem (Balan, Richards, & Luke,
2011), but for a more restricted set of infinitely repeated resource allocation games, we might
be able to find the optimal convention, similar to the Thue-Morse sequence (Richman, 2001)
used by Kuzmics et al. (2010) in the Nash demand game.

Acknowledgements
We are particularly thankful to David Parkes for giving the first author the unique opportunity to spend a few weeks in his lab at Harvard. David‚Äôs openness and unparalleled
knowledge is what really helped originate this work. We would also like to thank Kate
359

Cigler & Faltings

Larson for reading the draft of this paper and helping make the theoretical analysis much
more readable.

References
Aumann, R. (1974). Subjectivity and correlation in randomized strategies. Journal of
Mathematical Economics, 1 (1), 67‚Äì96.
Balan, G., Richards, D., & Luke, S. (2011). Long-term fairness with bounded worst-case
losses. Autonomous Agents and Multi-Agent Systems, 22 (1), 43‚Äì63.
Bhaskar, V. (2000). Egalitarianism and efficiency in repeated symmetric games. Games
and Economic Behavior, 32 (2), 247‚Äì262.
Bonnet, F., & Raynal, M. (2011). The price of anonymity: Optimal consensus despite
asynchrony, crash, and anonymity. ACM Trans. Auton. Adapt. Syst., 6 (4), 23:1‚Äì
23:28.
Chothia, T., & Chatzikokolakis, K. (2005). A survey of anonymous peer-to-peer filesharing. In Embedded and Ubiquitous Computing‚ÄìEUC 2005 Workshops, pp. 744‚Äì755.
Springer.
Cigler, L., & Faltings, B. (2011). Reaching correlated equilibria through multi-agent learning. In The 10th International Conference on Autonomous Agents and Multiagent
Systems-Volume 2, pp. 509‚Äì516. International Foundation for Autonomous Agents
and Multiagent Systems.
Durresi, A., Paruchuri, V., Durresi, M., & Barolli, L. (2005). A hierarchical anonymous
communication protocol for sensor networks. In Embedded and Ubiquitous Computing‚Äì
EUC 2005, pp. 1123‚Äì1132. Springer.
Fudenberg, D., & Maskin, E. (1986). The folk theorem in repeated games with discounting
or with incomplete information. Econometrica, 54 (3), 533‚Äì554.
Jensen, J. L. (1906). Sur les fonctions convexes et les ineÃÅgaliteÃÅs entre les valeurs moyennes.
Acta Mathematica, 30 (1), 175‚Äì193.
Koutsoupias, E., & Papadimitriou, C. (1999). Worst-case equilibria. In Proceedings of the
16th annual conference on Theoretical aspects of computer science, STACS‚Äô99, pp.
404‚Äì413, Berlin, Heidelberg. Springer-Verlag.
Kuzmics, C., Palfrey, T., & Rogers, B. (2010). Symmetric players in repeated games: Theory
and evidence..
Leyton-Brown, K., & Shoham, Y. (2008). Essentials of Game Theory: A Concise, Multidisciplinary Introduction. Morgan & Claypool, San Rafael, CA.
Monderer, D., & Shapley, L. S. (1996). Potential games. Games and economic behavior,
14 (1), 124‚Äì143.
Norris, J. R. (1998). Markov Chains (Cambridge Series in Statistical and Probabilistic
Mathematics). Cambridge University Press.
360

Symmetric Subgame-Perfect Equilibria in Resource Allocation

Raab, M., & Steger, A. (1998). Balls into Bins A Simple and Tight Analysis, Vol. 1518 of
Lecture Notes in Computer Science, chap. 13, pp. 159‚Äì170. Springer Berlin Heidelberg,
Berlin, Heidelberg.
Rego, V. (1992). Naive asymptotics for hitting time bounds in markov chains. Acta Informatica, 29 (6), 579‚Äì594.
Richman, R. M. (2001). Recursive binary sequences of differences. Complex Systems, 13 (4),
381‚Äì392.
Rosenthal, R. W. (1973). A class of games possessing pure-strategy nash equilibria. International Journal of Game Theory, 2 (1), 65‚Äì67.
Shneidman, J., Ng, C., Parkes, D. C., Auyoung, A., Snoeren, A. C., Vahdat, A., & Chun, B.
(2005). Why markets could (but don‚Äôt currently) solve resource allocation problems
in systems. In In USENIX 05: Proceedings of the 10th USENIX Workshop on Hot
Topics in Operating Systems, p. 7.
Taussky, O. (1949). A recurring theorem on determinants. The American Mathematical
Monthly, 56 (10), 672‚Äì676.
Wang, L., Wu, K., Hamdi, M., & Ni, L. M. (2011). Attachment learning for multi-channel
allocation in distributed OFDMA networks. Parallel and Distributed Systems, International Conference on, 0, 520‚Äì527.

361

Journal of Artificial Intelligence Research 49 (2014) 207‚Äì240

Submitted 08/13; published 02/14

Selfishness Level of Strategic Games
Krzysztof R. Apt
Guido SchaÃàfer

k.r.apt@cwi.nl
g.schaefer@cwi.nl

Centre for Mathematics and Computer Science (CWI)
Networks and Optimization Group
Science Park 123
1098 XG Amsterdam
The Netherlands

Abstract
We introduce a new measure of the discrepancy in strategic games between the social
welfare in a Nash equilibrium and in a social optimum, that we call selfishness level.
It is the smallest fraction of the social welfare that needs to be offered to each player to
achieve that a social optimum is realized in a pure Nash equilibrium. The selfishness level
is unrelated to the price of stability and the price of anarchy and is invariant under positive
linear transformations of the payoff functions. Also, it naturally applies to other solution
concepts and other forms of games.
We study the selfishness level of several well-known strategic games. This allows us to
quantify the implicit tension within a game between players‚Äô individual interests and the
impact of their decisions on the society as a whole. Our analyses reveal that the selfishness
level often provides a deeper understanding of the characteristics of the underlying game
that influence the players‚Äô willingness to cooperate.
In particular, the selfishness level of finite ordinal potential games is finite, while that
of weakly acyclic games can be infinite. We derive explicit bounds on the selfishness level of
fair cost sharing games and linear congestion games, which depend on specific parameters
of the underlying game but are independent of the number of players. Further, we show
that the selfishness level of the n-players Prisoner‚Äôs Dilemma is c/(b(n‚àí1)‚àíc), where b and
c are the benefit and cost for cooperation, respectively, that of the n-players public goods
game is (1 ‚àí nc )/(c ‚àí 1), where c is the public good multiplier, and that of the Traveler‚Äôs
Dilemma game is 12 (b ‚àí 1), where b is the bonus. Finally, the selfishness level of Cournot
competition (an example of an infinite ordinal potential game), Tragedy of the Commons,
and Bertrand competition is infinite.

The intelligent way to be selfish is
to work for the welfare of others
Dalai-Lama1

1. Introduction
The discrepancy in strategic games between the social welfare in a Nash equilibrium and in
a social optimum has been long recognized by the economists. One of the flagship examples
is Cournot competition, a strategic game involving firms that simultaneously choose the
1. (Bowles, 2004, p. 109).
c
2014
AI Access Foundation. All rights reserved.

Apt & SchaÃàfer

production levels of a homogeneous product. The payoff functions in this game describe the
firms‚Äô profit in the presence of some production costs, under the assumption that the price
of the product depends negatively on the total output. It is well-known (see, e.g., Jehle &
Reny, 2011, pp. 174‚Äì175) that the price in the social optimum is strictly higher than in the
Nash equilibrium, which shows that the competition between the producers of a product
drives its price down.
In computer science the above discrepancy led to the introduction of the notions of the
price of anarchy (Koutsoupias & Papadimitriou, 2009) and the price of stability (Schulz
& Moses, 2003; Anshelevich, Dasgupta, Kleinberg, Tardos, Wexler, & Roughgarden, 2008)
that measure the ratio between the social welfare in a worst and, respectively, a best Nash
equilibrium and a social optimum. This originated a huge research effort aiming at determining both ratios for specific strategic games that possess (pure) Nash equilibria.
These two notions are descriptive in the sense that they assess the existing situation.
Said differently, these notions quantify the discrepancy between the social welfare in a Nash
equilibrium and a social optimum given the initial payoff functions. In contrast, we propose
a notion that is normative in the sense that it explains how to change these payoff functions
to resolve such a discrepancy. Intuitively, we are asking the question how much of the social
welfare needs to be added to the players‚Äô payoff functions so that their individual preferences
can bring them to an optimal outcome for the society. On an abstract level, the approach
that we propose here is related to one proposed by Axelrod (1984, p. 134), in chapter ‚ÄúHow
to Promote Cooperation‚Äù, from where we cite: ‚ÄúAn excellent way to promote cooperation
in a society is to teach people to care about the welfare of others.‚Äù
Our approach draws on the concept of altruistic games (see, e.g., Ledyard, 1995, and
more recently Marco & Morgan, 2007). In these games each player‚Äôs payoff is modified
by adding a positive fraction Œ± of the social welfare in the considered joint strategy to
the original payoff. The selfishness level of a game is defined as the infimum over all
Œ± ‚â• 0 for which such a modification yields that a social optimum is realized in a pure Nash
equilibrium. The underlying property is monotonic in the sense that if for some Œ± ‚â• 0 a
social optimum is a pure Nash equilibrium, then it is also the case for every Œ≤ ‚â• Œ±.
Intuitively, the selfishness level of a game can be viewed as a measure of the players‚Äô
willingness to cooperate. A low selfishness level indicates that the players are open to
align their interests in the sense that a small share of the social welfare is sufficient to
motivate them to choose a social optimum. In contrast, a high selfishness level suggests
that the players are reluctant to cooperate and a large share of the social welfare is needed
to stimulate cooperation among them. An infinite selfishness level means that cooperation
cannot be achieved through such means.
Notions like the price of stability and the price of anarchy were developed to measure
the quality of equilibria. In contrast, our notion of the selfishness level can be regarded as a
measure of willingness to cooperate. In general, these notions are incomparable (as we will
also argue formally) and provide different insights into the underlying game.
Our main motivation for analyzing the selfishness level of strategic games is to gain a
deeper understanding of the characteristics that influence the players‚Äô willingness to cooperate. As it turns out, for several games studied in this paper the selfishness level provides
such insights. To illustrate this point, we briefly elaborate on our findings for the public
goods game and the fair cost sharing game.
208

Selfishness Level of Strategic Games

In the public goods game there are n players who want to contribute to a public good.
Every player i chooses an amount si ‚àà [0, b] that he contributes. A central authority collects
all individual contributions, multiplies their sum by c > 1 (for simplicity we assume here
that n ‚â• c) and distributes the resulting
amount evenly among all players. The payoff of
c P
player i is thus pi (s) := b ‚àí si + n j sj . In the (unique) Nash equilibrium, every player
attempts to ‚Äúfree ride‚Äù by contributing 0 to the public good (which is a dominant strategy),
while in the social optimum every player contributes the full amount of b. As we will show,
the selfishness level of this game is (1 ‚àí nc )/(c ‚àí 1). This bound suggests that the temptation
to free ride (i) increases as the number of players grows and (ii) decreases as the parameter
c increases. Both phenomena were observed by experimental economists, (see, e.g., the
discussion in Ledyard, 1995, Section III.C.2). In comparison, the price of stability (which
coincides with the price of anarchy) for this game is c.
In a fair cost sharing game every player i chooses a facility from a set of facilities Si ‚äÜ E
available to him (for simplicity we discuss here only the case where players choose a single
facility). The cost ce of every used facility e ‚àà E is shared evenly among the players using
it. As we will prove, the selfishness level of this game is max{0, 12 cmax /cmin ‚àí 1}, where
cmax and cmin refer to the largest and smallest cost of a facility, respectively. Our analysis
therefore reveals a threshold phenomenon which also makes sense intuitively: In order to
motivate cooperation among the players it is crucial to convince the players having access to
a facility with cost cmin to adhere to a social optimum. If cmax ‚â§ 2cmin this is easy because
in a social optimum each such player either shares the cost of a facility e with ce ‚â• cmin
with at least one other player or uses a facility of cost cmin exclusively by himself. Thus, it
is in the self-interest of each player to cooperate and choose a social optimum in this case. If
cmax > 2cmin these players are reluctant to cooperate and the fraction of the social welfare
that needs to be offered to them to incite cooperation grows proportionally to cmax /cmin .
Anshelevich et al. (2008) showed that the price of stability and the price of anarchy of
this game are Hn and n, respectively, where n denotes the number of players.2 So these
measures depend on the number of players. In contrast, our notion reveals a dependency
on the discrepancy between the costs of the facilities.
A large body of literature in experimental economics indicates that players have a tendency to cooperate in social dilemmas like the Prisoner‚Äôs dilemma, the Traveler‚Äôs dilemma or
the public goods game, even though such behavior is ruled out by standard game-theoretic
analysis. Several studies suggest that the willingness to cooperate depends on certain parameters of the underlying game (like group-size, magnitude of payoffs, etc.); see, e.g., Isaac
and Walker (1988), Cooper, DeJong, Forsythe, and Ross (1996), Goeree and Holt (2001),
Becker, Carter, and Naeve (2005), and Dreber, Rand, Fudenberg, and Nowak (2008). For
example, Dreber et al. observe that in the Prisoner‚Äôs dilemma the willingness to cooperate
increases with the ratio of cost over benefit for cooperation. We therefore study the selfishness level of parameterized versions of these games. Our findings show that the selfishness
level also exhibits a dependency on certain parameters of the game.
In this paper, we define the selfishness level by taking pure Nash equilibrium as the
solution concept. This is in line with how the price of anarchy and price of stability were
defined originally (Koutsoupias & Papadimitriou, 2009; Schulz & Moses, 2003; Anshelevich
2. Hn denotes the nth Harmonic number.

209

Apt & SchaÃàfer

et al., 2008). However, the definition applies equally well to other solution concepts and
other forms of games. We discuss these matters in the final section.
1.1 Our Contributions
The main contributions presented in this paper are as follows:
1. We introduce (in Section 2) the notion of selfishness level of a game, derive some basic
properties and elaborate on some connections to other efficiency measures and models
of altruism.
In particular, we show that the selfishness level of a game is unrelated to the price
of stability and the price of anarchy. Moreover, the selfishness level is invariant under positive linear transformations of the payoff functions. We also show that our
model is equivalent to other models of altruism that have been studied before. As a
consequence, our bounds on the selfishness level directly transfer to these alternative
models.
2. We derive (in Section 3) a characterization result that allows us to determine the
selfishness level of a strategic game.
Our characterization shows that the selfishness level is determined by the maximum
appeal factor of unilateral profitable deviations from specific social optima, which we
call stable. As a result, we can focus on deviations from these stable social optima
only. Intuitively, the appeal factor of a single player deviation refers to the ratio of
the gain in his payoff over the resulting loss in social welfare.
3. We use (in Section 4) our characterization result to analyze the selfishness level of
several classical strategic games.
The games that we study are fundamental and often used to illustrate the consequences
of selfish behavior and the effects of competition. A summary of our results is given
in Table 1. In particular, we derive explicit bounds on the selfishness level of fair cost
sharing games and congestion games with linear delay functions. The obtained bounds
depend on specific parameters of the underlying game, which we find informative. We
also show that these bounds are tight for certain instances.
4. We also show (in Section 5) that our selfishness level notion naturally extends to other
solution concepts and other types of games, for instance mixed Nash equilibria and
extensive games.

1.2 Related Work
There are only few articles in the algorithmic game theory literature that study the influence
of altruism in strategic games (Caragiannis, Kaklamanis, Kanellopoulos, Kyropoulou, &
Papaioannou, 2010; Chen, de Keijzer, Kempe, & SchaÃàfer, 2011; Chen & Kempe, 2008;
Elias, Martignon, Avrachenkov, & Neglia, 2010; Hoefer & Skopalik, 2009). In these works,
altruistic player behavior is modeled by altering each player‚Äôs perceived payoff in order to
account also for the welfare of others. The models differ in the way they combine the player‚Äôs
210

Selfishness Level of Strategic Games

Game

Selfishness level

Ordinal potential games
Weakly acyclic games
Fair cost sharing games (singleton)

finite
‚àû
‚àí 1}‚Ä†
max{0, 12 ccmax
min

Fair cost sharing games (integer costs)
Linear congestion games (singleton)
Linear congestion games (integer coefficients)
Prisoner‚Äôs Dilemma for n players
Public goods game
Traveler‚Äôs dilemma
Cournout competition
Tragedy of the commons
Bertrand competition

max{0, 12 Lcmax ‚àí 1}‚Ä†
‚àÜmax ‚àí‚àÜmin
max{0, 12 (1‚àíŒ¥
‚àí 12 }‚Ä†
max )amin

max{0, 21 (L‚àÜmax ‚àí‚àÜmin ‚àí1)}‚Ä†

c
‚Ä†
b(n‚àí1)‚àíc
c
1‚àí n
max{0, c‚àí1 }‚Ä†
1
‚Ä†
2 (b ‚àí 1)

‚àû
‚àû
‚àû

Table 1: Selfishness level of the games studied in this paper.
‚Ä† see Section 4 for the definitions of the respective parameters of the games.

individual payoff with the payoffs of the other players. All these studies are descriptive in the
sense that they aim at understanding the impact of altruistic behavior on specific strategic
games.
Closest to our work are the articles by Elias et al. (2010) and by Chen et al. (2011).
Elias et al. study the inefficiency of equilibria in network design games (which constitute
a special case of the cost sharing games considered here) with altruistic (or, as they call
it, socially-aware) players. As we do here, they define each player‚Äôs cost function as his
individual cost plus Œ± times the social cost. They derive lower and upper bounds on the
price of anarchy and the price of stability, respectively, of the modified game. In particular,
they show that the price of stability is at most (Hn + Œ±)/(1 + Œ±), where n is the number of
players.
Chen et al. (2011) introduce a framework to study the robust price of anarchy, which
refers to the worst-case inefficiency of other solution concepts such as coarse correlated
equilibria (see Roughgarden, 2009) of altruistic extensions of strategic games. In their
model, player i‚Äôs perceived cost is a convex combination of (1 ‚àí Œ≥i ) times his individual cost
plus Œ≥i times the social cost, where Œ≥i ‚àà [0, 1] is the altruism level of player i. If all players
have a uniform altruism level Œ≥i = Œ≥, this model relates to the one we consider here by
setting Œ± = Œ≥/(1 ‚àí Œ≥) (see Section 2.3 for details). Although not being the main focus of
the paper, the authors also provide upper bounds of 2/(1 + Œ≥) and (1 ‚àí Œ≥)Hn + Œ≥ on the
price of stability for linear congestion games and fair cost sharing games, respectively.
Note that in all three cases mentioned above the price of stability approaches 1 as Œ±
goes to ‚àû. This seems to suggest that the selfishness level of these games is ‚àû. However,
this is not the case as our analyses reveal.
211

Apt & SchaÃàfer

Two other models of altruism were proposed in the literature. Chen and Kempe (2008)
define the perceived cost of a player as (1 ‚àí Œ≤) times his individual cost plus Œ≤/n times the
social cost, where Œ≤ ‚àà [0, 1]. Caragiannis et al. (2010) define the perceived cost of player i
as (1 ‚àí Œ¥) times his individual cost plus Œ¥ times the sum of the costs of all other players (i.e.,
excluding player i), where Œ¥ ‚àà [0, 1]. Also these two models can be shown to be equivalent
to our model using simple transformations (see Section 2.3 for details).
Subsequently, we mention a few related approaches that are normative. Conceptually,
our selfishness level notion is related to the Stackelberg threshold introduced by Sharma
and Williamson (2009) (see also Kaporis & Spirakis, 2009). The authors consider network
routing games in which a fraction of Œ≤ ‚àà [0, 1] of the flow is first routed centrally and the
remaining flow is then routed selfishly. The Stackelberg threshold refers to the smallest
value of Œ≤ that is needed to improve upon the social cost of a Nash equilibrium flow.
In a related paper, Hoefer and Skopalik (2009) study the minimum number, termed the
optimal stability threshold, of (pure) altruists that are needed in a congestion game to induce
a Nash equilibrium as a social optimum. They show that this number can be computed in
polynomial time for singleton congestion games.
In network congestion games, researchers studied the effect of imposing tolls on the edges
of the network in order to reduce the inefficiency of Nash equilibria (see, e.g., Beckmann,
McGuire, & Winsten, 1956). From a high-level perspective, these approaches can also be
regarded as normative.
Recently, Capraro (2013) proposed a new normative approach to measure incentive for
cooperation in symmetric games in which there is a tension between selfish and altruistic
behavior. The solution concept is a pure Nash equilibrium of a transformed game in which
the strategies are certain mixed strategic of the original game. These strategies depend
on the incentive and risk of deviating from cooperation in the original game. Strikingly,
Capraro‚Äôs conclusions about the influence of the parameters in the Prisoner‚Äôs Dilemma,
Traveler‚Äôs Dilemma and the public goods game are consistent with ours.
There are several other papers that propose notions allowing to assess the stability of
Nash equilibria. We mention a few of them below. Christodoulou, Koutsoupias, and Spirakis
(2011) study the inefficiency of approximate Nash equilibria in congestion games. In a
(1+Œµ)-approximate Nash equilibrium the cost of each player is at most (1+Œµ) times the cost
he experiences in every unilateral deviation. The authors derive (almost) tight bounds on
the price of stability and the price of anarchy for linear (non-atomic and atomic)
‚àö
‚àö congestion
games as a function of Œµ. In particular, they obtain a bound of min{1, (1 + 3)/(Œµ + 3)}
on the price of stability for atomic linear congestion games. In this context, an alternative
notion to assess the stability of Nash equilibria that comes to one‚Äôs mind is to consider the
smallest Œµ ‚â• 0 for which a social optimum is realized as a (1 + Œµ)-Nash equilibrium. Note
that the above bound implies that such an Œµ is at most 1 for linear congestion games. We
comment on this idea in more detail in Section 5.2.
Anshelevich, Das, and Naamad (2009) consider the problem of incentivizing players to
participate in socially desirable matchings by adding switching costs to player deviations.
In their model, the additional cost that a player incurs by changing his strategy accounts
for an Œµ fraction of his individual cost. Adopting this viewpoint, the authors study the
inefficiency of (1 + Œµ)-approximate stable matchings. They derive bounds on the price of
stability and the price of anarchy of (1 + Œµ)-approximate stable matchings as a function of
212

Selfishness Level of Strategic Games

Œµ ‚â• 0. Related to this work is the article of BiroÃÅ, Manlove, and Mittal (2010) who study the
problem of computing an optimal matching having a minimum number of blocking pairs.
Furthermore, Balcan, Blum, and Mansour (2009) study the impact of advertising strategies to players in order to induce them to select more efficient equilibria. More precisely, in
their model an authority first proposes a strategy to each player which is then accepted by
each player with probability Œ±. Each accepting player adheres to the proposed strategy and
all remaining players play a best response (assuming that the strategies of the accepting
players are fixed). In a final step all players follow a best response dynamics until a Nash
equilibrium is reached. The authors analyze the inefficiency of the resulting equilibria for
fair cost sharing games, machine scheduling games and party affiliation games. In particular, for fair cost sharing games they show that the expected cost of the resulting equilibrium
is at most a factor O(log n/Œ±) away from a social optimum.

2. Selfishness Level
In this section, we formally introduce our notion of selfishness level, establish some properties and relate it to other notions of altruism.
2.1 Definition
A strategic game (in short, a game) G = (N, {Si }i‚ààN , {pi }i‚ààN ) is given by a set N =
{1, . . . , n} of n > 1 players, a non-empty set of strategies Si for every player i ‚àà N , and a
payoff function pi for every player i ‚àà N with pi : S1 √ó ¬∑ ¬∑ ¬∑ √ó Sn ‚Üí R. The players choose
their strategies simultaneously and every player i ‚àà N aims at choosing a strategy si ‚àà Si
so as to maximize his individual payoff pi (s), where s = (s1 , . . . , sn ).
We call s ‚àà S1 √ó ¬∑ ¬∑ ¬∑ √ó Sn a joint strategy and denote its ith element by si . We denote
(s1 , . . . , si‚àí1 , si+1 , . . . , sn ) by s‚àíi and similarly with S‚àíi . Further, we write (s‚Ä≤i , s‚àíi ) for
(s1 , . . . , si‚àí1 , s‚Ä≤i , si+1 , . . . , sn ), where we assume that s‚Ä≤i ‚àà Si . Sometimes, when focusing on
player i we write (si , s‚àíi ) instead of s.
A joint strategy s is a Nash equilibrium if for all i ‚àà {1, . . . , n} and s‚Ä≤i ‚àà Si ,
pi (si , s‚àíi ) ‚â• pi (s‚Ä≤i , s‚àíi ). Further, given a joint strategy s we call the sum SW (s) :=
P
n
i=1 pi (s) the social welfare of s. When the social welfare of s is maximal we call s
a social optimum.
We shall also consider a ‚Äòcost‚Äô variant of the games in which we use the cost functions,
written as ci , instead of the payoff functions pi . In such a setup the objective of each player
is to minimize his costs, so a joint strategy s is a Nash equilibrium if for all i ‚àà {1, . . . , n}
and s‚Ä≤i ‚àà Si , ci (si , s‚àíi ) ‚â§ ci (s‚Ä≤i , s‚àíi ). Further,
instead of the social welfare one considers the
P
social cost of s, defined as SC(s) := ni=1 ci (s).
Given a strategic game G := (N, {Si }i‚ààN , {pi }i‚ààN ) and Œ± ‚â• 0 we define the game
G(Œ±) := (N, {Si }i‚ààN , {ri }i‚ààN ) by putting ri (s) := pi (s) + Œ±SW (s). So when Œ± > 0 the
payoff of each player in the G(Œ±) game depends on the social welfare of the players. G(Œ±)
is then an altruistic version of the game G.
Suppose now that for some Œ± ‚â• 0 a pure Nash equilibrium of G(Œ±) is a social optimum
of G(Œ±). Then we say that G is Œ±-selfish. We define the selfishness level of G as
inf{Œ± ‚àà R+ | G is Œ±-selfish}.
213

(1)

Apt & SchaÃàfer

Here we adopt the convention that the infimum of an empty set is ‚àû. Further, we stipulate
that the selfishness level of G is denoted by Œ±+ iff the selfishness level of G is Œ± ‚àà R+ but
G is not Œ±-selfish (equivalently, the infimum does not belong to the set). We show below
(Theorem 2) that pathological infinite games exist for which the selfishness level is of this
kind; none of the other studied games is of this type.
We give some remarks before we proceed.
1. The above definitions refer to strategic games in which each player i maximizes his
payoff function pi and the social welfare of a joint strategy s is given by SW (s). These
definitions obviously apply to the case when we use cost functions and the social cost.
2. Other definitions of an altruistic version of a game are conceivable and, depending
on the underlying application, might seem more natural than the one we use here.
However, we show in Section 2.3 that our definition is equivalent to several other
models of altruism that have been proposed in the literature.
3. The selfishness level refers to the smallest Œ± such that some Nash equilibrium in G(Œ±)
is also a social optimum. Alternatively, one might be interested in the smallest Œ± such
that every Nash equilibrium in G(Œ±) corresponds to a social optimum. However, as
explained in Section 5.2, this alternative notion is not always very meaningful.
4. The definition extends in the obvious way to other solution concepts (e.g., mixed or
correlated equilibria) and other forms of games (e.g., subgame perfect equilibria in
extensive games). We briefly comment on these extensions in Section 5.
Note that the social welfare of a joint strategy s in G(Œ±) equals (1 + Œ±n)SW (s), so the
social optima of G and G(Œ±) coincide. Hence we can replace in the definition of an Œ±-selfish
game the reference to a social optimum of G(Œ±) by one to a social optimum of G. This is
what we shall do in the proofs below.
Intuitively, a low selfishness level means that the share of the social welfare needed to
induce the players to choose a social optimum is small. This share can be viewed as an
‚Äòincentive‚Äô needed to realize a social optimum. Let us illustrate this definition on various
simple examples.
Example 1. Prisoner‚Äôs Dilemma
C
D

C
1, 1
2, ‚àí1

D
‚àí1, 2
0, 0

C
D

C
3, 3
3, 0

D
0, 3
0, 0

Consider the Prisoner‚Äôs Dilemma game G (on the left) and the resulting game G(Œ±)
for Œ± = 1 (on the right). In the latter game the social optimum, (C, C), is also a Nash
equilibrium. One can easily check that for Œ± < 1, (C, C) is also a social optimum of G(Œ±)
but not a Nash equilibrium. So the selfishness level of this game is 1.
Example 2. Battle of the Sexes
F
B

F
2, 1
0, 0
214

B
0, 0
1, 2

Selfishness Level of Strategic Games

Here each Nash equilibrium is also a social optimum, so the selfishness level of this game
is 0.
Example 3. Matching Pennies

H
T

H
1, ‚àí1
‚àí1, 1

T
‚àí1, 1
1, ‚àí1

Since the social welfare of each joint strategy is 0, for each Œ± the game G(Œ±) is identical
to the original game in which no Nash equilibrium exists. So the selfishness level of this
game is ‚àû. More generally, the selfishness level of a constant sum game is 0 if it has a Nash
equilibrium and otherwise it is ‚àû.
Example 4. Game with a bad Nash equilibrium
The following game results from equipping each player in the Matching Pennies game with
a third strategy E (for edge):
H
1, ‚àí1
‚àí1, 1
‚àí1, ‚àí1

H
T
E

T
‚àí1, 1
1, ‚àí1
‚àí1, ‚àí1

E
‚àí1, ‚àí1
‚àí1, ‚àí1
‚àí1, ‚àí1

Its unique Nash equilibrium is (E, E). It is easy to check that the selfishness level of
this game is ‚àû. (This is also an immediate consequence of Theorem 4 (iii) below.)
Example 5. Game with no Nash equilibrium
Consider a game G on the left and the resulting game G(Œ±) for Œ± = 1 on the right.

C
D

C
2, 2
3, 0

D
2, 0
1, 1

C
D

C
6, 6
6, 3

D
4, 2
3, 3

The game G has no Nash equilibrium, while in the game G(1) the social optimum,
(C, C), is also a Nash equilibrium. As in the Prisoner‚Äôs Dilemma game one can easily check
that for Œ± < 1, (C, C) is also a social optimum of G(Œ±) but not a Nash equilibrium. So the
selfishness level of the game G is 1.
2.2 Properties
Recall that, given a finite game G that has a Nash equilibrium, its price of stability is
the ratio SW (s)/SW (s‚Ä≤ ) where s is a social optimum and s‚Ä≤ is a Nash equilibrium with the
highest social welfare in G. The price of anarchy is defined as the ratio SW (s)/SW (s‚Ä≤ )
where s is a social optimum and s‚Ä≤ is a Nash equilibrium with the lowest social welfare in
G.
So the price of stability of G is 1 iff its selfishness level is 0. However, in general there
is no relation between these two notions. The following observation also shows that the
selfishness level of a finite game can be an arbitrary real number.
215

Apt & SchaÃàfer

Theorem 1. For every finite Œ± > 0 and Œ≤ > 1 there is a finite game whose selfishness level
is Œ± and whose price of stability is Œ≤.
Proof. Consider the following generalized form, which we denote by P D(Œ±, Œ≤), of the PrisŒ±
oner‚Äôs Dilemma game G with x = Œ±+1
:
C
D

C
1, 1
x + 1, 0

D
0, x + 1
1 1
Œ≤, Œ≤

In this game and in each game G(Œ≥) with Œ≥ ‚â• 0, (C, C) is the unique social optimum.
To compute the selfishness level we need to consider a game G(Œ≥) and stipulate that (C, C)
is its Nash equilibrium. This leads to the inequality 1 + 2Œ≥ ‚â• (Œ≥ + 1)(x + 1), from which it
x
, i.e., Œ≥ ‚â• Œ±. So the selfishness level of G is Œ±. Moreover, its price of
follows that Œ≥ ‚â• 1‚àíx
stability is Œ≤, since (D, D) is the only Nash equilibrium.
The notion of the selfishness level is invariant under simple payoff transformations. It
is a direct consequence of the following observation, where given a game G and a value a
we denote by G + a (respectively, aG) the game obtained from G by adding to each payoff
function the value a (respectively, by multiplying each payoff function by a).
Proposition 1. Consider a game G and Œ± ‚â• 0.
(i) For every a, G is Œ±-selfish iff G + a is Œ±-selfish.
(ii) For every a > 0, G is Œ±-selfish iff aG is Œ±-selfish.
Proof. (i) It suffices to note that r[a]i (s) = ri (s) + Œ±an + a, where ri and r[a]i are the payoff
functions of player i in the games G(Œ±) and (G + a)(Œ±). So for every joint strategy s
‚Ä¢ s is a Nash equilibrium of G(Œ±) iff it is a Nash equilibrium of (G + a)(Œ±),
‚Ä¢ s is social optimum of G(Œ±) iff it is a social optimum of (G + a)(Œ±).

(ii) It suffices to note that for every a > 0, r[a]i (s) = ari (s), where this time r[a]i is the
payoff function of player i in the game (aG)(Œ±), and argue as above.
Proposition 1 implies that the selfishness level is invariant under the game transformations of the form t(G) := aG + b, where a > 0. This is in contrast to the notions of the price
of anarchy and the price of stability that are invariant only under the game transformations
of the form t(G) := aG, where a > 0.
Note that the selfishness level is not invariant under a multiplication of the payoff functions by a value a ‚â§ 0. Indeed, for a = 0 each game aG has the selfishness level 0. For
a < 0 take the game G from Example 4 whose selfishness level is ‚àû. In the game aG the
joint strategy (E, E) is both a Nash equilibrium and a social optimum, so the selfishness
level of aG is 0.
The above proposition also allows us to frame the notion of selfishness level in the
following way. Suppose the original n-player game G is given to a game designer who has a
fixed budget of SW (s) for each joint strategy s and that the selfishness level of G is Œ± < ‚àû.
How should the game designer then distribute the budget of SW (s) for each joint strategy s
216

Selfishness Level of Strategic Games

among the players such that the resulting game has a Nash equilibrium that coincides with
a social optimum? By scaling G(Œ±) by the factor a := 1/(1 + Œ±n) we ensure that for each
joint strategy s its social welfare in the original game G and in aG(Œ±) is the same. Using
Proposition 1, we conclude that Œ± is the smallest non-negative real such that aG(Œ±) has
a Nash equilibrium that is a social optimum. The game aG(Œ±) can then be viewed as the
intended transformation of G. That is, each payoff function pi of the game G is transformed
into the payoff function
ri (s) :=

Œ±
1
pi (s) +
SW (s).
1 + Œ±n
1 + Œ±n

Let us return now to the ‚Äòborderline case‚Äô of the selfishness level that we denoted by
We have the following result.

Œ±+ .

Theorem 2. For every Œ± ‚â• 0 there exists a game whose selfishness level is Œ±+ .
Proof. We first prove the result for Œ± = 0. That is, we show that there exists a game that
is Œ±-selfish for every Œ± > 0, but is not 0-selfish. To this end we use the games P D(Œ±, Œ≤)
defined in the proof of Theorem 1.
We construct a strategic game G = (N, {Si }i‚ààN , {pi }i‚ààN ) with two players N = {1, 2}
by combining, for an arbitrary but fixed Œ≤ > 1, infinitely many P D(Œ±, Œ≤) games with Œ± > 0
as follows: For each Œ± > 0 we rename the strategies of the P D(Œ±, Œ≤) game to, respectively,
C(Œ±) and D(Œ±) and denote the corresponding payoff functions by pŒ±i . The set of strategies
of each player i ‚àà N is Si = {C(Œ±) | Œ± > 0} ‚à™ {D(Œ±) | Œ± > 0} and the payoff of i is defined
as
(
pŒ±i (si , s‚àíi ) if {si , s‚àíi } ‚äÜ {C(Œ±), D(Œ±)} for some Œ± > 0
pi (si , s‚àíi ) :=
0
otherwise.
Every social optimum of G is of the form (C(Œ±), C(Œ±)), where Œ± > 0. (Note that we
exploit that Œ≤ > 1 here.) By the argument given in the proof of Theorem 1, (C(Œ±), C(Œ±))
with Œ± > 0 is a Nash equilibrium in the game G(Œ±) because the deviations from C(Œ±) to a
strategy C(Œ≥) or D(Œ≥) with Œ≥ 6= Œ± yield a payoff of 0. Thus, G is Œ±-selfish for every Œ± > 0.
Finally, observe that G is not 0-selfish because every Nash equilibrium of G is of the form
(D(Œ±), D(Œ±)), where Œ± > 0.
To deal with the general case we prove two claims that are of independent interest.
Claim 1. For every game G and Œ± ‚â• 0 there is a game G‚Ä≤ such that G‚Ä≤ (Œ±) = G.
Proof. We define the payoff of player i in the game G‚Ä≤ by
p‚Ä≤i (s) := pi (s) ‚àí

Œ±
SW (s),
1 + nŒ±

217

Apt & SchaÃàfer

where pi is his payoff in the game G. Denote by SW ‚Ä≤ (s) the social welfare of a joint strategy
s in the game G‚Ä≤ and by ri‚Ä≤ the payoff function of player i in the game G‚Ä≤ (Œ±). Then
ri‚Ä≤ (s) = p‚Ä≤i (s) + Œ±SW ‚Ä≤ (s)


nŒ±
Œ±
SW (s) + Œ± SW (s) ‚àí
SW (s)
= pi (s) ‚àí
1 + nŒ±
1 + nŒ±


Œ±
nŒ±2
= pi (s) + Œ± ‚àí
‚àí
SW (s)
1 + nŒ± 1 + nŒ±
= pi (s).

Claim 2. For every game G and Œ±, Œ≤ ‚â• 0
G(Œ± + Œ≤) = G(Œ±)



Œ≤
1 + nŒ±



.

Proof. Denote by SW ‚Ä≤ (s) the social welfare of a joint strategy s in the game G(Œ±), by pi , ri
Œ≤
). Then
and r ‚Ä≤ the payoff functions of player i in the games G, G(Œ±), and G(Œ±)( 1+nŒ±
ri (s) := pi (s) + Œ±SW (s),
so
Œ≤
SW ‚Ä≤ (s)
1 + nŒ±
Œ≤
= pi (s) + Œ±SW (s) +
(SW (s) + nŒ±SW (s))
1 + nŒ±


Œ≤
Œ≤nŒ±
= pi (s) + Œ± +
+
SW (s)
1 + nŒ± 1 + nŒ±
= pi (s) + (Œ± + Œ≤)SW (s),

ri‚Ä≤ (s) = ri (s) +

which proves the claim.
To prove the general case fix Œ± ‚â• 0 and Œ≤ > 0 and take a game G whose selfishness level
is 0+ . By Claim 1 there is a game G‚Ä≤ such that G‚Ä≤ (Œ±) = G. Then G‚Ä≤ is not Œ±-selfish, since
G is not 0-selfish.
Further, by Claim 2
G‚Ä≤ (Œ± + Œ≤) = G‚Ä≤ (Œ±)
But by its choice the game G is
proof.



Œ≤
1 + nŒ±

Œ≤
1+nŒ± -selfish,



=G



Œ≤
1 + nŒ±



.

so G‚Ä≤ is (Œ± + Œ≤)-selfish, which concludes the

218

Selfishness Level of Strategic Games

2.3 Alternative Definitions
Our definition of the selfishness level depends on the way the altruistic versions of the
original game are defined. Three other models of altruism were proposed in the literature.
As before, let G := (N, {Si }i‚ààN , {pi }i‚ààN ) be a strategic game. Consider the following four
definitions of altruistic versions of G:
Model A (Elias et al., 2010): For every Œ± ‚â• 0, G(Œ±) := (N, {Si }i‚ààN , {riŒ± }i‚ààN ) with
riŒ± (s) = pi (s) + Œ±SW (s)

‚àÄi ‚àà N.

(2)

Model B (Chen & Kempe, 2008): For every Œ≤ ‚àà [0, 1], G(Œ≤) := (N, {Si }i‚ààN , {riŒ≤ }i‚ààN )
with
Œ≤
(3)
riŒ≤ (s) = (1 ‚àí Œ≤)pi (s) + SW (s) ‚àÄi ‚àà N.
n
Model C (Chen et al., 2011): For every Œ≥ ‚àà [0, 1], G(Œ≥) := (N, {Si }i‚ààN , {riŒ≥ }i‚ààN ) with
riŒ≥ (s) = (1 ‚àí Œ≥)pi (s) + Œ≥SW (s) ‚àÄi ‚àà N.

(4)

Model D (Caragiannis et al., 2010): For every Œ¥ ‚àà [0, 1], G(Œ¥) := (N, {Si }i‚ààN , {riŒ¥ }i‚ààN )
with
riŒ≥ (s) = (1 ‚àí Œ¥)pi (s) + Œ¥(SW (s) ‚àí pi (s)) ‚àÄi ‚àà N.
(5)
Our selfishness level notion for Model A extends to Models B, C and D in the obvious
way: We say that G is Œ≤-selfish for some Œ≤ ‚àà [0, 1] iff a pure Nash equilibrium of the
altruistic version G(Œ≤) is also a social optimum. The selfishness level of G with respect
to Model B is then defined as the infimum over all Œ≤ ‚àà [0, 1] such that G is Œ≤-selfish. The
respective notions for Models C and D are defined analogously.
The following theorem shows that the selfishness level of a game with respect to Models
A, B, C and D relate to each other via simple transformations. (Note that for Model D this
transformation only applies for Œ¥ ‚àà [0, 12 ].)
Theorem 3. Consider a strategic game G := (N, {Si }i‚ààN , {pi }i‚ààN ) and its altruistic versions defined according to Models A, B, C and D above.
(i) G is Œ±-selfish with Œ± ‚àà R+ iff G is Œ≤-selfish with Œ≤ =

Œ±n
1+Œ±n

‚àà [0, 1].

(ii) G is Œ±-selfish with Œ± ‚àà R+ iff G is Œ≥-selfish with Œ≥ =

Œ±
1+Œ±

‚àà [0, 1].

(iii) G is Œ±-selfish with Œ± ‚àà R+ iff G is Œ¥-selfish with Œ¥ =

Œ±
1+2Œ±

‚àà [0, 21 ].

Proof. We prove the following more general claim. Fix x, y > 0. For every Œª ‚àà [0, x1 ], define
G(Œª) := (N, {Si }i‚ààN , {riŒª }i‚ààN ) with
riŒª (s) = (1 ‚àí xŒª)pi (s) +

Œª
SW (s).
y

We show that G is Œ±-selfish for Œ± ‚â• 0 iff G is Œª-selfish for Œª =
219

(6)
Œ±y
1+Œ±xy

‚àà [0, x1 ].

Apt & SchaÃàfer

By substituting Œª =
riŒª (s) =

Œ±y
1+Œ±xy

in (6), we obtain

1
Œ±
1
pi (s) +
SW (s) =
r Œ± (s).
1 + Œ±xy
1 + Œ±xy
1 + Œ±xy i

1
> 0 for every Œ± ‚â• 0 the pure Nash equilibria and social
As a consequence, since 1+Œ±xy
1
1
optima, respectively, of G(Œª) and 1+Œ±xy
G(Œ±) coincide. Thus, G is Œª-selfish iff 1+Œ±xy
G is
1
Œ±-selfish. Also, it follows from Proposition 1 that 1+Œ±xy G is Œ±-selfish iff G is Œ±-selfish.
Further, note that


Œ±y
1
1
1
lim
=
1 ‚àí lim
= .
Œ±‚Üí‚àû 1 + Œ±xy
Œ±‚Üí‚àû 1 + Œ±xy
x
x

That is, the selfishness level of G with respect to Model A is ‚àû iff the selfishness level of G
with respect to G(Œª) is x1 .
Now, (i) follows from the above with x = 1 and y = n, (ii) follows with x = y = 1 and
(iii) follows with x = 2 and y = 1.

3. A Characterization Result
We now characterize the games with a finite selfishness level. To this end we shall need
the following notion. We call a social optimum s stable if for all i ‚àà N and s‚Ä≤i ‚àà Si the
following holds:
if (s‚Ä≤i , s‚àíi ) is a social optimum, then pi (si , s‚àíi ) ‚â• pi (s‚Ä≤i , s‚àíi ).
In other words, a social optimum is stable if no player is better off by unilaterally deviating
to another social optimum.
It will turn out that in order to determine the selfishness level of a game we need to
consider deviations from its stable social optima. Consider a deviation s‚Ä≤i of player i from
a stable social optimum s. If player i is better off by deviating to s‚Ä≤i , then by definition the
social welfare decreases, i.e., SW (si , s‚àíi ) ‚àí SW (s‚Ä≤i , s‚àíi ) > 0. If in the original game this
decrease is small, while the gain for player i is large, then strategy s‚Ä≤i is an attractive and
socially acceptable option for player i. We define player i‚Äôs appeal factor of strategy s‚Ä≤i
given the social optimum s as
AFi (s‚Ä≤i , s) :=

pi (s‚Ä≤i , s‚àíi ) ‚àí pi (si , s‚àíi )
.
SW (si , s‚àíi ) ‚àí SW (s‚Ä≤i , s‚àíi )

In what follows we shall characterize the selfishness level in terms of bounds on the
appeal factors of profitable deviations from a stable social optimum. First, note the following
properties of social optima.
Lemma 1. Consider a strategic game G := (N, {Si }i‚ààN , {pi }i‚ààN ) and Œ± ‚â• 0.
(i) If s is both a Nash equilibrium of G(Œ±) and a social optimum of G, then s is a stable
social optimum of G.
220

Selfishness Level of Strategic Games

(ii) If s is a stable social optimum of G, then s is a Nash equilibrium of G(Œ±) iff for all
i ‚àà N and s‚Ä≤i ‚àà Ui (s), Œ± ‚â• AFi (s‚Ä≤i , s), where
Ui (s) := {s‚Ä≤i ‚àà Si | pi (s‚Ä≤i , s‚àíi ) > pi (si , s‚àíi )}.

(7)

The set Ui (s), with the ‚Äú>‚Äù sign replaced by ‚Äú‚â•‚Äù, is called an upper contour set (see,
e.g., Ritzberger, 2002, p. 193). Note that if s is a stable social optimum, then s‚Ä≤i ‚àà Ui (s)
implies that SW (si , s‚àíi ) > SW (s‚Ä≤i , s‚àíi ).
Proof. (i) Suppose that s is both a Nash equilibrium of G(Œ±) and a social optimum of G.
Consider some joint strategy (s‚Ä≤i , s‚àíi ) that is a social optimum. By the definition of a Nash
equilibrium
pi (si , s‚àíi ) + Œ±SW (si , s‚àíi ) ‚â• pi (s‚Ä≤i , s‚àíi ) + Œ±SW (s‚Ä≤i , s‚àíi ),
so pi (si , s‚àíi ) ‚â• pi (s‚Ä≤i , s‚àíi ), as desired.
(ii) Suppose that s is a stable social optimum of G. Then s is a Nash equilibrium of
G(Œ±) iff for all i ‚àà N and s‚Ä≤i ‚àà Si
pi (si , s‚àíi ) + Œ±SW (si , s‚àíi ) ‚â• pi (s‚Ä≤i , s‚àíi ) + Œ±SW (s‚Ä≤i , s‚àíi ).

(8)

If pi (si , s‚àíi ) ‚â• pi (s‚Ä≤i , s‚àíi ), then (8) holds for all Œ± ‚â• 0 since s is a social optimum. If
pi (s‚Ä≤i , s‚àíi ) > pi (si , s‚àíi ), then, since s is a stable social optimum of G, we have SW (si , s‚àíi ) >
SW (s‚Ä≤i , s‚àíi ).
So (8) holds for all i ‚àà N and s‚Ä≤i ‚àà Si iff
Œ±‚â•

pi (s‚Ä≤i , s‚àíi ) ‚àí pi (si , s‚àíi )
= AFi (s‚Ä≤i , s)
SW (si , s‚àíi ) ‚àí SW (s‚Ä≤i , s‚àíi )

holds for all i ‚àà N and s‚Ä≤i ‚àà Ui (s).
This leads us to the following result.
Theorem 4. Consider a strategic game G := (N, {Si }i‚ààN , {pi }i‚ààN ).
(i) The selfishness level of G is finite iff a stable social optimum s exists for which Œ±(s) :=
supi‚ààN, s‚Ä≤i ‚ààUi (s) AFi (s‚Ä≤i , s) is finite.
(ii) If the selfishness level of G is finite, then it equals mins‚ààSSO Œ±(s), where SSO is the
set of stable social optima.
(iii) If G is finite, then its selfishness level is finite iff it has a stable social optimum. In
particular, if G has a unique social optimum, then its selfishness level is finite.
(iv) If Œ≤ > Œ± ‚â• 0 and G is Œ±-selfish, then G is Œ≤-selfish.
Proof. (i) and (iv) follow by Lemma 1, (ii) by (i) and Lemma 1, and (iii) by (i).
Using the above theorem we now exhibit a class of games for n players for which the
selfishness level is unbounded. In fact, the following more general result holds.
221

Apt & SchaÃàfer

Theorem 5. For each function f : N ‚Üí R+ there exists a class of games for n players,
where n > 1, such that the selfishness level of a game for n players equals f (n).
Proof. Assume n > 1 players and that each player has two strategies, 1 and 0. Denote by
1 the joint strategy in which each strategy equals 1 and by 1‚àíi the joint strategy of the
opponents of player i in which each entry equals 1. The payoff for each player i is defined
as follows:
Ô£±
Ô£¥
if s = 1
Ô£≤0
pi (s) := f (n)
if si = 0 and ‚àÄj < i, sj = 1
Ô£¥
Ô£≥ f (n)+1
‚àí n‚àí1
otherwise.
So when s 6= 1, pi (s) = f (n) if i is the smallest index of a player with si = 0 and otherwise
pi (s) = ‚àí f (n)+1
n‚àí1 . Note that SW (1) = 0 and SW (s) = ‚àí1 if s 6= 1. So 1 is a unique social
optimum.
We have pi (0, 1‚àíi ) ‚àí pi (1) = f (n) and SW (1) ‚àí SW (0, 1‚àíi ) = 1. So by Theorem 4 (ii)
the selfishness level equals f (n).

4. Examples
We now use the above characterization result to determine or compute an upper bound on
the selfishness level of some selected games. First, we exhibit a well-known class of games
(see Monderer & Shapley, 1996) for which the selfishness level is finite.
4.1 Ordinal Potential Games
Given a game G := (N, {Si }i‚ààN , {pi }i‚ààN ), a function P : S1 √ó ¬∑ ¬∑ ¬∑ √ó Sn ‚Üí R is called an
ordinal potential function for G if for all i ‚àà N , s‚àíi ‚àà S‚àíi and si , s‚Ä≤i ‚àà Si , pi (si , s‚àíi ) >
pi (s‚Ä≤i , s‚àíi ) iff P (si , s‚àíi ) > P (s‚Ä≤i , s‚àíi ). A game that possesses an ordinal potential function is
called an ordinal potential game.
Theorem 6. Every finite ordinal potential game has a finite selfishness level.
Proof. Each social optimum with the largest potential is a stable social optimum. So the
claim follows by Theorem 4 (ii).
In particular, every finite congestion game (see Rosenthal, 1973) has a finite selfishness
level. We shall derive explicit bounds for two special cases of these games in Sections 4.3
and 4.4.
4.2 Weakly Acyclic Games
Given a game G := (N, {Si }i‚ààN , {pi }i‚ààN ), a path in S1 √ó ¬∑ ¬∑ ¬∑ √ó Sn is a sequence (s1 , s2 , . . . )
k‚àí1
of joint strategies such that for every k > 1 there is a player i such that sk = (s‚Ä≤i , s‚àíi
) for
k‚àí1
‚Ä≤
some si 6= si
(see, e.g., Monderer & Shapley, 1996). A path is called an improvement
path if it is maximal and for all k > 1, pi (sk ) > pi (sk‚àí1 ), where i is the player who deviated
from sk‚àí1 . A game G has the finite improvement property (FIP ) if every improvement
path is finite. A game G is called weakly acyclic if for every joint strategy there exists a
finite improvement path that starts at it (see, e.g., Milchtaich, 1996; Young, 1993).
222

Selfishness Level of Strategic Games

Finite games that have the FIP coincide with the ordinal potential games. So by Theorem 6 these games have a finite selfishness level. In contrast, the selfishness level of a weakly
acyclic game can be infinite. Indeed, the following game is easily seen to be weakly acyclic:

H
T
E

H
1, ‚àí1
‚àí 1, 1
‚àí0.5, ‚àí1

T
‚àí 1, 1
1, ‚àí1
‚àí0.5, ‚àí1

E
‚àí 1, ‚àí0.5
‚àí 1, ‚àí0.5
‚àí0.5, ‚àí0.5

Yet, on the account of Theorem 4 (iii), its selfishness level is infinite.
4.3 Fair Cost Sharing Games
In this and the next subsection we consider cost-minimization instead of payoff-maximization
games. Recall that in these games each player i wants P
to minimize his individual cost function ci and that the social cost is defined as SC(s) = i ci (s).
In a fair cost sharing game (see, e.g., Anshelevich et al., 2008) players allocate facilities
and share the cost of the used facilities in a fair manner. Formally, a fair cost sharing game
is given by G = (N, E, {Si }i‚ààN , {ce }e‚ààE ), where N = {1, . . . , n} is the set of players, E is
the set of facilities, Si ‚äÜ 2E is the set of facility subsets available to player i, and ce ‚àà R+
is the cost of facility e ‚àà E. It is called a singleton cost sharing game if for every i ‚àà N
and for every si ‚àà Si : |si | = 1. For a joint strategy s ‚àà S1 √ó ¬∑ ¬∑ ¬∑ √ó Sn let xe (s) be the
number of players using facility e ‚àà E, i.e., xe (s) = |{i ‚àà N | e ‚àà si }|. The cost of a facility
e ‚àà E is evenly
P shared among the players using it. That is, the cost of player i is defined
as ci (s) = e‚ààsi ce /xe (s).
We first consider singleton cost sharing games. Let cmax = maxe‚ààE ce and cmin =
mine‚ààE ce refer to the maximum and minimum costs of the facilities, respectively.
Proposition 2. The selfishness level of a singleton cost sharing game is at most
‚àí 1}. Moreover, this bound is tight.
max{0, 21 ccmax
min
This result should be contrasted with the price of stability of Hn and the price of anarchy
of n for cost sharing games (Anshelevich et al., 2008). Cost sharing games admit an exact
potential function and thus by Theorem 6 their selfishness level is finite. However, as the
tight example given in the proof of Proposition 2 below shows, the selfishness level can be
arbitrarily large (as cmax /cmin ‚Üí ‚àû) even for n = 2 players and two facilities.
In order to prove Proposition 2, we first derive an expression of the appeal factor for
arbitrary fair cost sharing games, which we then specialize to singleton cost sharing games
to prove the claim.
Let s be a stable social optimum. Note that s exists by Theorem 4 (iii) and Theorem 6.
Because we consider a cost minimization game here the appeal factor of player i is defined
as
ci (si , s‚àíi ) ‚àí ci (s‚Ä≤i , s‚àíi )
(9)
AFi (s‚Ä≤i , s) :=
SC(s‚Ä≤i , s‚àíi ) ‚àí SC(si , s‚àíi )
and the condition in Theorem 4 (i) reads Œ±(s) := maxi‚ààN, s‚Ä≤i ‚ààUi (s) AFi (s‚Ä≤i , s), where Ui (s) :=
{s‚Ä≤i ‚àà Si | ci (s‚Ä≤i , s‚àíi ) < ci (si , s‚àíi )}.
223

Apt & SchaÃàfer

Fix some player i and let s‚Ä≤ = (s‚Ä≤i , s‚àíi ) for some s‚Ä≤i ‚àà Ui (s). We use xe and x‚Ä≤e to refer
to xe (s) and xe (s‚Ä≤ ), respectively. Note that
Ô£±
‚Ä≤
Ô£¥
Ô£≤xe + 1 if e ‚àà si \ si ,
x‚Ä≤e = xe ‚àí 1 if e ‚àà si \ s‚Ä≤i ,
Ô£¥
Ô£≥
xe
otherwise.

We have

ci (s) ‚àí ci (s‚Ä≤i , s‚àíi ) =

X ce
X
ce
‚àí
.
xe
xe + 1
‚Ä≤
‚Ä≤

e‚ààsi \si

(10)

e‚ààsi \si

Further, it is not difficult to verify that
SC(s‚Ä≤i , s‚àíi ) ‚àí SC(s) =

X

e‚ààs‚Ä≤i \si : xe =0

ce ‚àí

X

ce .

(11)

‚àí 1.

(12)

e‚ààsi \s‚Ä≤i : xe =1

Thus,
AFi (s‚Ä≤i , s)

=

P

ce
e‚ààsi \s‚Ä≤i : xe ‚â•2 xe

P

‚àí

e‚ààs‚Ä≤i \si : xe =0 ce

P

‚àí

We use the above to prove Proposition 2.

ce
e‚ààs‚Ä≤i \si : xe ‚â•1 xe +1

P

e‚ààsi \s‚Ä≤i : xe =1 ce

Proof of Proposition 2. Let s be a stable social optimum (which exists by Theorem 4 (iii)
and Theorem 6). If Ui (s) = ‚àÖ for every i ‚àà N then the selfishness level is 0 by Theorem 4 (ii).
Otherwise, there is some player i ‚àà N with Ui (s) 6= ‚àÖ. Recall that in a singleton cost
sharing game, each player‚Äôs strategy set consists of singleton facility sets. Let si = {e} and
s‚Ä≤i = {e‚Ä≤ } be the singleton sets of the facilities chosen by player i in s and in s‚Ä≤ = (s‚Ä≤i , s‚àíi )
with s‚Ä≤i ‚àà Ui (s). Clearly, e 6= e‚Ä≤ .
Note that SC(s‚Ä≤i , s‚àíi ) ‚àí SC(s) must be positive because s‚Ä≤i ‚àà Ui (s) and thus (11) implies
that xe‚Ä≤ = 0. Therefore, (10) reduces to ci (s) ‚àí ci (s‚Ä≤i , s‚àíi ) = ce /xe ‚àí ce‚Ä≤ . If xe = 1
then ce > ce‚Ä≤ because s‚Ä≤i ‚àà Ui (s). But this is a contradiction to the assumption that
SC(s‚Ä≤i , s‚àíi ) ‚àí SC(s) = ce‚Ä≤ ‚àí ce > 0. Thus xe ‚â• 2. Note that this also implies that ce > 2ce‚Ä≤
and thus cmax > 2cmin .
Using (12), we obtain
AFi (s‚Ä≤i , s)

=

ce
xe

ce‚Ä≤

‚àí1‚â§

1 cmax
‚àí 1.
2 cmin

The claim now follows by Theorem 4 (ii).
The following example shows that this bound is tight. Suppose N = {1, 2}, E =
{e1 , e2 }, S1 = {{e1 }}, S2 = {{e1 }, {e2 }}, ce1 = cmax and ce2 = cmin with cmax > 2cmin .
The joint strategy s = ({e1 }, {e1 }) is the unique social optimum with SC(s) = cmax and
c2 (s) = cmax /2. Suppose player 2 deviates to s‚Ä≤2 = {e2 }. Then SC(s‚Ä≤2 , s1 ) = cmax + cmin and
c2 (s‚Ä≤2 , s1 ) = cmin . Thus AFi (s‚Ä≤2 , s) = ( 21 cmax ‚àí cmin )/cmin = 12 cmax /cmin ‚àí 1.
224

Selfishness Level of Strategic Games

The following example shows that a bound similar to the one above, i.e., bounding
the selfishness level in terms of the ratio cmax /cmin , does not hold for arbitrary fair cost
sharing games. In particular, it shows that the minimum difference between any two costs
of facilities (here Œµ) must enter a bound of the selfishness level for arbitrary fair cost sharing
games.
Example 6. Let N = {1, 2}, E = {e1 , e2 , e3 }, S1 = {{e1 }}, S2 = {{e1 , e3 }, {e2 }}, ce1 =
cmax , ce2 = cmin + Œµ for some Œµ > 0 and ce3 = cmin . The joint strategy s = ({e1 }, {e1 , e3 }) is
the unique social optimum with SC(s) = cmax + cmin and c2 (s) = cmax /2 + cmin . Suppose
player 2 deviates to s‚Ä≤2 = {e2 }. Then SC(s‚Ä≤2 , s1 ) = cmax + cmin + Œµ and c2 (s‚Ä≤2 , s1 ) = cmin + Œµ.
Thus AFi (s‚Ä≤2 , s) = ( 12 cmax ‚àí Œµ)/Œµ = 21 cmax /Œµ ‚àí 1, which approaches ‚àû as Œµ ‚Üí 0.
We next derive a bound for arbitrary fair cost sharing games with non-negative integer
costs. Let L be the maximum number of facilities that any player can choose, i.e., L :=
maxi‚ààN,si ‚ààSi |si |.
Proposition 3. The selfishness level of a fair cost sharing game with non-negative integer
costs is at most max{0, 21 Lcmax ‚àí 1}. Moreover, this bound is tight.
Proof. Let s be a stable social optimum. As in the proof of Proposition 2, if Ui (s) = ‚àÖ for
every i ‚àà N then the selfishness level is 0 by Theorem 4 (ii). Otherwise, there is some player
i ‚àà N with Ui (s) 6= ‚àÖ. Let s‚Ä≤ = (s‚Ä≤i , s‚àíi ) for some s‚Ä≤i ‚àà Ui (s). Note that the denominator
of the appeal factor in (12) is at least 1 because s is stable, s‚Ä≤i ‚àà Ui (s) and ce ‚àà N for each
e ‚àà E. Thus
P
P
ce
e
‚àí e‚ààs‚Ä≤ \si : xe ‚â•1 xec+1
‚Ä≤ : x ‚â•2
e‚ààs
\s
x
e
i
e
‚Ä≤
i
i
P
‚àí1
AFi (si , s) = P
e‚ààs‚Ä≤i \si : xe =0 ce ‚àí
e‚ààsi \s‚Ä≤i : xe =1 ce
X
1
ce
‚àí 1 ‚â§ Lcmax ‚àí 1.
‚â§
xe
2
‚Ä≤
e‚ààsi \si : xe ‚â•2

The claim follows by Theorem 4 (ii).
The following example shows that the bound is tight. Suppose we are given L and
cmax . Let N = {1, . . . , n} and E = {e1 , . . . , en } where n = L + 1. Define Si = {{ei }} for
every i ‚àà N \ {n} and Sn = {{e1 , . . . , en‚àí1 }, {en }}. Let cei = cmax for every i ‚àà N \ {n}
and cen = 1. The joint strategy s = ({e1 }, . . . , {en‚àí1 }, {e1 , . . . , en‚àí1 }) is the unique social
optimum with SC(s) = (n ‚àí 1)cmax and cn (s) = (n ‚àí 1)cmax /2. Suppose player n deviates
to s‚Ä≤n = {en }. Then SC(s‚Ä≤n , s‚àín ) = (n ‚àí 1)cmax + 1 and cn (s‚Ä≤n , s‚àín ) = 1. Thus AFi (s‚Ä≤n , s) =
1
1
2 (n ‚àí 1)cmax ‚àí 1 = 2 Lcmax ‚àí 1.
Remark 1. We can bound the selfishness level of a fair cost sharing game with non-negative
rational costs ce ‚àà Q+ for every facility e ‚àà E by using Proposition 3 and the following
scaling argument: Simply scale all costs to integers, e.g., by multiplying them with the
least common multiplier q ‚àà N of the denominators. Note that this scaling does not change
the selfishness level of the game by Proposition 1. However, it does change the maximum
facility cost and thus q enters the bound. Also note that this scaling implicitly takes care
of the effect observed in Example 6: Suppose that cmax and cmin are integers and «´ = 1/q
for some q ‚àà N. Then all costs are multiplied by q and Proposition 3 yields a (non-tight)
bound of qcmax ‚àí 1 = cmax /«´ ‚àí 1 on the selfishness level, which approaches ‚àû as q ‚Üí ‚àû.
225

Apt & SchaÃàfer

4.4 Linear Congestion Games
In a congestion game G := (N, E, {Si }i‚ààN , {de }e‚ààE ) we are given a set of players N =
{1, . . . , n}, a set of facilities E with a delay function de : N ‚Üí R+ for every facility e ‚àà E,
and a strategy set Si ‚äÜ 2E for every player i ‚àà N . For a joint strategy s ‚àà S1 √ó ¬∑ ¬∑ ¬∑ √ó Sn ,
define xe (s) as the number of players using facility e ‚àà E, i.e.,Pxe (s) = |{i ‚àà N | e ‚àà si }|.
The goal of a player is to minimize his individual cost ci (s) = e‚ààsi de (xe (s)).
Here we call a congestion game symmetric if there is some common strategy set S ‚äÜ 2E
such that Si = S for all i. It is singleton if every strategy si ‚àà Si is a singleton set, i.e., for
every i ‚àà N and for every si ‚àà Si , |si | = 1. In a linear congestion game, the delay function
of every facility e ‚àà E is of the form de (x) = ae x + be , where ae , be ‚àà R+ are non-negative
real numbers.
We first derive a bound on the selfishness level for symmetric singleton linear congestion
games. As it turns out, a bound similar to the one for singleton cost sharing games does
not extend to symmetric singleton linear congestion games. Instead, the crucial insight here
is that the selfishness level depends on the discrepancy between facilities in a stable social
optimum. We make this notion more precise.
Let s be a stable social optimum and let xe refer to xe (s). Define the discrepancy
between two facilities e and e‚Ä≤ with ae + ae‚Ä≤ > 0 under s as
Œ¥(xe , xe‚Ä≤ ) =

2ae xe + be 2ae‚Ä≤ xe‚Ä≤ + be‚Ä≤
‚àí
.
ae + ae ‚Ä≤
ae + ae‚Ä≤

(13)

We show below that Œ¥(xe , xe‚Ä≤ ) ‚àà [‚àí1, 1]. Define Œ¥max (s) as the maximum discrepancy
between any two facilities e and e‚Ä≤ under s with ae + ae‚Ä≤ > 0 and Œ¥(xe , xe‚Ä≤ ) < 1; more
formally, let
Œ¥max (s) = max
{Œ¥(xe , xe‚Ä≤ ) | ae + ae‚Ä≤ > 0 and Œ¥(xe , xe‚Ä≤ ) < 1}.
‚Ä≤
e,e ‚ààE

Let Œ¥max be the maximum discrepancy over all stable social optima, i.e., Œ¥max = maxs‚ààSSO Œ¥max (s).
Further, let ‚àÜmax := maxe‚ààE (ae + be ) and ‚àÜmin := mine‚ààE (ae + be ). Moreover, let amin be
the minimum non-zero coefficient of a latency function, i.e., amin = mine‚ààE:ae >0 ae .
Proposition 4. The selfishness level of a symmetric singleton linear congestion game is at
most


1
1 ‚àÜmax ‚àí ‚àÜmin
‚àí
.
max 0,
2 (1 ‚àí Œ¥max )amin 2
Moreover, this bound is tight.
We first prove that the discrepancy between two facilities is bounded:
Claim 3. Let s be a social optimum and e, e‚Ä≤ ‚àà E be two facilities with ae + ae‚Ä≤ > 0. Then
the discrepancy between e and e‚Ä≤ under s satisfies Œ¥(xe , xe‚Ä≤ ) ‚àà [‚àí1, 1].

Proof. Let t = xe + xe‚Ä≤ be the total number of players on facilities e and e‚Ä≤ under s. Note
that since s is a social optimum and strategy sets are symmetric, t is distributed among xe
and xe‚Ä≤ such that the social cost of these two facilities is minimized. Said differently, xe = x
minimizes the function
f (x, t) := ae x2 + be x + ae‚Ä≤ (t ‚àí x)2 + be‚Ä≤ (t ‚àí x).
226

Selfishness Level of Strategic Games

It is not hard to verify that the minimum of f (x, t) (for fixed t) is attained at the (not
necessarily integral) point
2ae‚Ä≤ t ‚àí be + be‚Ä≤
.
xÃÑ0 :=
2(ae + ae‚Ä≤ )
Because f (x, t) is a parabola with its minimum at xÃÑ0 , the integral point xe that minimizes
f (x, t) is given by the point obtained by rounding xÃÑ0 to the nearest integer. Let xe := xÃÑ0 + 21 Œ¥
be this point, where Œ¥ = Œ¥(xe , xe‚Ä≤ ) ‚àà [‚àí1, 1], and xe‚Ä≤ = t ‚àí xe . Note that the choice of Œ¥ is
unique, unless xÃÑ0 is half-integral in which case Œ¥ ‚àà {‚àí1, 1}. Solving these equations for Œ¥
yields the definition in (13).
Proof of Proposition 4. Let s be a stable social optimum. Note that s exists by Theorem 4 (iii) and Theorem 6. If Ui (s) = ‚àÖ for every i ‚àà N then the selfishness level is 0 by
Theorem 4 (ii). Otherwise, there is some player i ‚àà N with Ui (s) 6= ‚àÖ. Let s‚Ä≤ = (s‚Ä≤i , s‚àíi )
for some s‚Ä≤i ‚àà Ui (s). We use xe and x‚Ä≤e to refer to xe (s) and xe (s‚Ä≤ ) for every facility e ‚àà E,
respectively. Note that for every e ‚àà E we have
Ô£±
‚Ä≤
Ô£¥
Ô£≤xe + 1 if e ‚àà si \ si ,
(14)
x‚Ä≤e = xe ‚àí 1 if e ‚àà si \ s‚Ä≤i ,
Ô£¥
Ô£≥
xe
otherwise.
Let si = {e} and s‚Ä≤i = {e‚Ä≤ } be the sets of facilities chosen by player i in s and s‚Ä≤ , respectively.
Exploiting (14), we obtain
ci (si , s‚àíi ) ‚àí ci (s‚Ä≤i , s‚àíi ) = ae xe + be ‚àí ae‚Ä≤ (xe‚Ä≤ + 1) ‚àí be‚Ä≤ .

(15)

SC(s‚Ä≤i , s‚àíi ) ‚àí SC(si , s‚àíi ) = ae‚Ä≤ (2xe‚Ä≤ + 1) + be‚Ä≤ ‚àí ae (2xe ‚àí 1) ‚àí be .

(16)

Moreover,

Note that we have ci (si , s‚àíi ) ‚àí ci (s‚Ä≤i , s‚àíi ) > 0 because s‚Ä≤i ‚àà Ui (s) and by the definition of
Ui (s) in (7). Further, SC(s‚Ä≤i , s‚àíi ) ‚àí SC(si , s‚àíi ) > 0 because s is a stable social optimum
and s‚Ä≤i ‚àà Ui (s). Thus, it must hold that ae + ae‚Ä≤ > 0; otherwise ae = ae‚Ä≤ = 0 and (15) and
(16) yield a contradiction.
Let Œ¥ = Œ¥(xe , xe‚Ä≤ ) be the discrepancy between e and e‚Ä≤ under s. Note that Œ¥ ‚àà [‚àí1, 1]
by Claim 3. Using the definition of Œ¥ in (13), we can rewrite (15) and (16) as
ci (si , s‚àíi ) ‚àí ci (s‚Ä≤i , s‚àíi ) = 12 (ae + ae‚Ä≤ )Œ¥ + 21 be ‚àí 21 be‚Ä≤ ‚àí ae‚Ä≤
and
SC(s‚Ä≤i , s‚àíi ) ‚àí SC(si , s‚àíi ) = (1 ‚àí Œ¥)(ae + ae‚Ä≤ ).
We conclude that Œ¥ 6= 1.
Thus,
1
2
1
‚â§
2

AFi (s‚Ä≤i , s) =

1 (ae + be ) ‚àí (ae‚Ä≤ + be‚Ä≤ ) 1
(ae + ae‚Ä≤ )Œ¥ + be ‚àí be‚Ä≤ ‚àí 2ae‚Ä≤
= ¬∑
‚àí
(1 ‚àí Œ¥)(ae + ae‚Ä≤ )
2
(1 ‚àí Œ¥)(ae + ae‚Ä≤ )
2
‚àÜmax ‚àí ‚àÜmin
1
¬∑
‚àí .
(1 ‚àí Œ¥max )amin 2
¬∑

227

Apt & SchaÃàfer

The claim now follows by Theorem 4 (ii).
The following example shows that this bound is tight even for n = 2 players and two
facilities. Let N = {1, 2}, E = {e, e‚Ä≤ } and S1 = S2 = {{e}, {e‚Ä≤ }}. Suppose we are given
Œ¥ ‚àà [0, 1) and ae‚Ä≤ ‚àà R+ . Define de (x) = (2 + Œ¥)ae‚Ä≤ and de‚Ä≤ (x) = ae‚Ä≤ x. The joint strategy s =
({e}, {e‚Ä≤ }) is the unique social optimum with SC(s) = (3 + Œ¥)ae‚Ä≤ . Further c1 (s) = (2 + Œ¥)ae‚Ä≤
and c2 (s) = ae‚Ä≤ . Suppose player 1 deviates to s‚Ä≤1 = {e‚Ä≤ }. Then SC(s‚Ä≤1 , s2 ) = 4ae‚Ä≤ and
c1 (s‚Ä≤1 , s2 ) = 2ae‚Ä≤ . Thus AFi (s‚Ä≤1 , s) = Œ¥/(1 ‚àí Œ¥), which matches precisely the upper bound
given above. The case Œ¥ ‚àà [‚àí1, 0] is proven analogously.
Observe that the selfishness level depends on the ratio (‚àÜmax ‚àí ‚àÜmin )/amin and 1/(1 ‚àí
Œ¥max ). In particular, the selfishness level becomes arbitrarily large as Œ¥max approaches 1.
We next derive a bound for the selfishness level of arbitrary congestion games with linear
delay functions and non-negative integer coefficients, i.e., de (x) = ae x + be with ae , be ‚àà N
for every e ‚àà E. Let L be the maximum number of facilities that any player can choose,
i.e., L := maxi‚ààN,si ‚ààSi |si |.
Proposition 5. The selfishness level of a linear congestion game with non-negative integer
coefficients is at most max{0, 12 (L‚àÜmax ‚àí ‚àÜmin ‚àí 1)}. Moreover, this bound is tight.
For linear congestion games, the price of anarchy is known to be 25 (see Christodoulou &
Koutsoupias, 2005; Awerbuch, Azar, & Epstein, 2013). Our bound shows that the selfishness
level depends on the maximum number of facilities in a strategy set and the magnitude of
the coefficients of the delay functions.
Proof of Proposition 5. Let s be a stable social optimum. Note that s exists by Theorem
4 (iii) and Theorem 6. If Ui (s) = ‚àÖ for every i ‚àà N then the selfishness level is 0 by
Theorem 4 (ii). Otherwise, there is some player i ‚àà N with Ui (s) 6= ‚àÖ. Let s‚Ä≤ = (s‚Ä≤i , s‚àíi ) for
some s‚Ä≤i ‚àà Ui (s). We use xe and x‚Ä≤e to refer to xe (s) and xe (s‚Ä≤ ), respectively.
Exploiting (14), we obtain
ci (si , s‚àíi ) ‚àí ci (s‚Ä≤i , s‚àíi ) =

X

e‚ààsi \s‚Ä≤i

(ae xe + be ) ‚àí

X

(ae (xe + 1) + be ).

e‚ààs‚Ä≤i \si

Similarly,
SC(s‚Ä≤i , s‚àíi ) ‚àí SC(si , s‚àíi ) =
+

X

(xe + 1)(ae (xe + 1) + be ) ‚àí xe (ae xe + be )

X

(xe ‚àí 1)(ae (xe ‚àí 1) + be ) ‚àí xe (ae xe + be )

X

(ae (2xe + 1) + be ) ‚àí

e‚ààs‚Ä≤i \si

e‚ààsi \s‚Ä≤i

=

e‚ààs‚Ä≤i \si

X

e‚ààsi \s‚Ä≤i

(ae (2xe ‚àí 1) + be ).

P
Given a congestion vector x = (xe )e‚ààE , define P (x) := e‚ààsi \s‚Ä≤ (ae xe + be ) and Q(x) :=
i
P
e‚ààs‚Ä≤ \si (ae (xe + 1) + be ). Note that P (x) and Q(x) are integers because ae , be ‚àà N for
i

228

Selfishness Level of Strategic Games

every facility e ‚àà E. Note that with these definitions, P (1) =
P
Q(0) = e‚ààs‚Ä≤ \si (ae + be ). We have
i

AFi (s‚Ä≤i , s) =

P

e‚ààsi \s‚Ä≤i (ae

+ be ) and

P (x) ‚àí Q(x)
.
2Q(x) ‚àí Q(0) ‚àí 2P (x) + P (1)

Because s‚Ä≤i ‚àà Ui (s), we know that P (x) > Q(x) and 2Q(x) ‚àí Q(0) > 2P (x) ‚àí P (1). So we
obtain
Q(x) + 1 ‚â§ P (x) ‚â§ Q(x) + 12 (P (1) ‚àí Q(0) ‚àí 1).
Exploiting these inequalities, we obtain
AFi (s‚Ä≤i , s)

1
1
‚â§ (P (1) ‚àí Q(0) ‚àí 1) =
2
2

 X

e‚ààsi \s‚Ä≤i

(ae + be ) ‚àí

1
‚â§ (|si \ s‚Ä≤i | ¬∑ ‚àÜmax ‚àí |s‚Ä≤i \ si | ¬∑ ‚àÜmin ‚àí 1).
2

X

e‚ààs‚Ä≤i \si


(ae + be ) ‚àí 1

Note that |s‚Ä≤i \ si | ‚â• 1; otherwise, s‚Ä≤i ‚äÜ si and thus SC(s‚Ä≤i , s‚àíi ) ‚â§ SC(s) which contradicts
s‚Ä≤i ‚àà Ui (s). The above expression is thus at most 12 (L‚àÜmax ‚àí ‚àÜmin ‚àí 1). The claim now
follows by Theorem 4 (ii).
The following example shows that this bound is tight. Fix L, ‚àÜmax and ‚àÜmin such
that (2n ‚àí 1)‚àÜmin = L‚àÜmax + 1 for some integer n. Consider a congestion game with
N = {1, . . . , n} and E = {e1 , . . . , eL+1 }. Define Si = {{eL+1 }} for every i ‚àà N \ {n}
and Sn = {{e1 , . . . , eL }, {eL+1 }}. Let deL+1 (x) = ‚àÜmin x and dei (x) = ‚àÜmax for every i ‚àà
{1, . . . , L}. For the joint strategy s = ({eL+1 }, . . . , {eL+1 }, {e1 , . . . , eL }) we have SC(s) =
‚àÜmin (n ‚àí 1)2 + L‚àÜmax and cn (s) = L‚àÜmax . If player n deviates to s‚Ä≤n = {eL+1 } we
have SC(s‚Ä≤n , s‚àín ) = ‚àÜmin n2 = ‚àÜmin (n ‚àí 1)2 + ‚àÜmin (2n ‚àí 1) and cn (s‚Ä≤n , s‚àín ) = ‚àÜmin n.
Exploiting that (2n ‚àí 1)‚àÜmin = L‚àÜmax + 1, we conclude that SC(s) < SC(s‚Ä≤n , s‚àín ) and
cn (s) > cn (s‚Ä≤n , s‚àín ) (for n ‚â• 3). Thus, s is a social optimum and s‚Ä≤n ‚àà Ui (s). We obtain
1
L‚àÜmax ‚àí ‚àÜmin n
= L‚àÜmax ‚àí (L‚àÜmax + ‚àÜmin + 1)
‚àÜmin (2n ‚àí 1) ‚àí L‚àÜmax
2
1
= (L‚àÜmax ‚àí ‚àÜmin ‚àí 1).
2

AFn (s‚Ä≤n , s) =

Remark 2. We can use Proposition 5 and the scaling argument outlined in Remark 1 to
derive bounds on the selfishness level of congestion games with linear delay functions and
non-negative rational coefficients.
4.5 Prisoner‚Äôs Dilemma for n Players
We assume that each player i ‚àà N
P = {1, . . . , n} has two strategies, 1 (cooperate) and 0
(defect). We put pi (s) := ‚àícsi + b j6=i sj , where b > c. Intuitively, b stands for the benefit
of cooperation and c for the cost of cooperation.
Proposition 6. The selfishness level of the n-players Prisoner‚Äôs Dilemma game is
229

c
b(n‚àí1)‚àíc .

Apt & SchaÃàfer

Intuitively, this means that when the number of players in the Prisoner‚Äôs Dilemma game
increases, a smaller share of the social welfare is needed to resolve the underlying conflict.
The same observation holds for the value of the benefit. That is, the ‚Äòacuteness‚Äô of the
dilemma diminishes with the number of players and also diminishes when the value of the
benefit grows. The formal reason is that the appeal factor of each unilateral deviation
from the social optimum is inversely proportional to the number of players and inversely
proportional to the benefit.
Proof. In this game s = 1 is the unique social optimum, with for each i ‚àà N , pi (s) =
bn ‚àí (b + c) and SW (s) = bn2 ‚àí (b + c)n. Consider now the joint strategy (s‚Ä≤i , s‚àíi ) in
which player i deviates to the strategy s‚Ä≤i = 0. We have then pi (s‚Ä≤i , s‚àíi ) = bn ‚àí b and
c
. The claim now
SW (s‚Ä≤i , s‚àíi ) = bn2 ‚àí (b + c)n + c ‚àí b(n ‚àí 1). Hence AFi (s‚Ä≤i , s) = b(n‚àí1)‚àíc
follows by Theorem 4 (ii).
In particular, for n = b = 2 and c = 1 we get the original Prisoner‚Äôs Dilemma game
considered in Example 1 and as already argued there the selfishness level is then 1.
4.6 Public Goods
We consider the public goods game with n players. Every player i ‚àà N = {1, . . . , n} chooses
an amount si ‚àà [0, b] that he contributes to a public good, where b ‚àà R+ is the budget. The
game designer collects the individual contributions of all players, multiplies their sum by
c > 1 and distributes the resulting
amount evenly among all players. The payoff of player
c P
i is thus pi (s) := b ‚àí si + n j‚ààN sj .
 1‚àí c 	
Proposition 7. The selfishness level of the n-players public goods game is max 0, c‚àí1n .

In this game, every player has an incentive to ‚Äúfree ride‚Äù by contributing 0 to the
public good (which is a dominant strategy if c ‚â§ n). This is exactly as in the n-players
Prisoner‚Äôs Dilemma game (where defect is a dominant strategy if c > 0). However, the
above proposition reveals that for fixed c, in contrast to the Prisoner‚Äôs Dilemma game, this
temptation becomes stronger as the number of players increases. Also, for a fixed number
of players this temptation becomes weaker as c increases.
P
Proof of Proposition 7. Note that SW (s) = bn+(c‚àí1) i‚ààN si . The unique social optimum
of this game is therefore s = b with pi (s) = cb for every i ‚àà N and SW (s) = cbn. Suppose
player i deviates from s by choosing s‚Ä≤i ‚àà [0, b). Then pi (s‚Ä≤i , s‚àíi ) = cb + (1 ‚àí nc )(b ‚àí s‚Ä≤i ). Thus,
pi (s‚Ä≤i , s‚àíi ) ‚àí pi (s) = (1 ‚àí nc )(b ‚àí s‚Ä≤i ) and

SW (s) ‚àí SW (s‚Ä≤i , s‚àíi ) = (c ‚àí 1)(b ‚àí s‚Ä≤i ).

If 1 ‚àí nc ‚â§ 0 then Ui (s) = ‚àÖ and the selfishness level is zero. Otherwise, 1 ‚àí nc > 0 and
Ui (s) = [0, b). We conclude that in this case AFi (s‚Ä≤i , s) = (1‚àí nc )/(c‚àí1) for every s‚Ä≤i ‚àà Ui (s).
The claim now follows by Theorem 4 (ii).
230

Selfishness Level of Strategic Games

4.7 Traveler‚Äôs Dilemma
This is a strategic game discussed by Basu (1994) with two players N = {1, 2}, strategy set
Si = {2, . . . , 100} for every player i, and payoff function pi for every i defined as
Ô£±
Ô£¥
if si = s‚àíi
Ô£≤s i
pi (s) := si + b
if si < s‚àíi
Ô£¥
Ô£≥
s‚àíi ‚àí b otherwise,

where b > 1 is the bonus.

Proposition 8. The selfishness level of the Traveler‚Äôs Dilemma game is

b‚àí1
2 .

Proof. The unique social optimum of this game is s = (100, 100), while (2, 2) is its unique
Nash equilibrium. If player i deviates from s to a strategy s‚Ä≤i ‚â§ 99, while the other player
remains at 100, the respective payoffs become s‚Ä≤i + b and s‚Ä≤i ‚àí b, so the social welfare becomes
‚Ä≤
2s‚Ä≤i . So AFi (s‚Ä≤i , s) = (s‚Ä≤i + b ‚àí 100)/(200 ‚àí 2s‚Ä≤i ). The maximum, b‚àí1
2 , is reached when si = 99.
So the claim follows by Theorem 4 (ii).
Intuitively, this means that as the bonus b increases a larger share of the social welfare
needs to be used to ensure cooperation.
4.8 Tragedy of the Commons
Assume that each player i ‚àà N = {1, . . . , n} has the real interval [0, 1] as its set of strategies.
Each player‚Äôs strategy is his chosen fraction of a common resource. Let (see Osborne, 2005,
Exercise 63.1 and Tardos & Vazirani, 2007, pp. 6‚Äì7):




pi (s) := max 0, si 1 ‚àí

n
X
j=1

sj



.

This payoff function reflects the fact that player‚Äôs enjoyment of the common resource depends positively from his chosen fraction of the resource and negatively from the total
fraction of the common resource used by all players. Additionally, if the total fraction of
the common resource by all players exceeds a feasible level, here 1, then player‚Äôs enjoyment
of the resource becomes zero.
Proposition 9. The selfishness level of the n-players Tragedy of the Commons game is ‚àû.
Intuitively, this result means that in this game no matter how much we ‚Äòinvolve‚Äô the
players in sharing the social welfare we cannot achieve that they will select a social optimum.
Proof. We
Pfirst determine the stable social optima of this game. Fix a joint strategy s and
let t :=
j‚ààN sj . If t > 1, then the social welfare is 0. So assume that t ‚â§ 1. Then
SW (s) = t(1 ‚àí t). This expression becomes maximal precisely when t = 12 and then it
social optima and each of them is stable.
equals 14 . So this game has infinitely many P
Take now a stable social optimum s. So j‚ààN sj = 21 . Fix i ‚àà {1, . . . , n}. Denote si by a
P
and consider a strategy x of player i such that pi (x, s‚àíi ) > pi (a, s‚àíi ). Then j6=i sj +x 6= 21 ,
so SW (a, s‚àíi ) > SW (x, s‚àíi ).
231

Apt & SchaÃàfer

a
P We have pi (a, s‚àíi ) = 2 and SW (a, s‚àíi ) =
j6=i sj + x < 1 and hence

pi (x, s‚àíi ) = x(a +

1
2

‚àí x)

1
4.

Further, pi (x, s‚àíi ) > pi (a, s‚àíi ) implies

and SW (x, s‚àíi ) = ( 12 ‚àí a + x)(1 ‚àí

1
2

+ a ‚àí x) =

1
4

‚àí (a ‚àí x)2 .

Also x 6= a. Hence
AFi (x, s) =

(a ‚àí x)(x ‚àí 12 )
a ‚àí 12
x ‚àí 12
pi (x, s‚àíi ) ‚àí pi (a, s‚àíi )
=
=
‚àí1
+
=
SW (a, s‚àíi ) ‚àí SW (x, s‚àíi )
(a ‚àí x)2
a‚àíx
a‚àíx

1
1
Since pi (x, s‚àíi ) ‚àí pi (a, s‚àíi ) =
P(a ‚àí x)(x ‚àí 21) we have pi (x, s‚àíi ) > pi (a, s‚àíi ) iff a < x < 2
1
1
or a > x > 2 . But a ‚â§ 2 , since j6=i sj +a = 2 . So the conjunction of pi (x, s‚àíi ) > pi (a, s‚àíi )
and SW (x, s‚àíi ) < SW (a, s‚àíi ) holds iff a < x < 12 . Now maxa<x< 1 AFi (x, s) = ‚àû. But s
2
was an arbitrary stable social optimum, so the claim follows by Theorem 4 (i).

4.9 Cournot Competition
We consider Cournot competition for n firms with a linear inverse demand function and
constant returns to scale (see, e.g., Jehle & Reny, 2011, pp. 174‚Äì175). So we assume
that each playerPi ‚àà N = {1, . . . , n} has a strategy set Si = R+ and payoff function
pi (s) := si (a ‚àí b j‚ààN sj ) ‚àí csi for some given a, b, c, where a > c ‚â• P
0 and b > 0.
The price of the product is represented by the expression a ‚àí b j‚ààN sj and the production cost corresponding to the production
level si by csi . In what follows we rewrite the
P
payoff function as pi (s) := si (d ‚àí b j‚ààN sj ), where d := a ‚àí c. Note that the payoffs can
be negative, which was not the case in the tragedy of the commons game. Still the proofs
are very similar for both games.
Proposition 10. The selfishness level of the n-players Cournot competition game is ‚àû.
Proof. We
P first determine the stable social optima of this game. Fix a joint strategy s and
let t := j‚ààN sj . Then SW (s) = t(d ‚àí bt). This expression becomes maximal precisely
d
. So this game has infinitely many social optima and each of them is stable.
when t = 2b
P
P
d
Take now a stable social optimum s. So j‚ààN sj = 2b
. Fix i ‚àà N . Let u := j6=i sj .
For every strategy z of player i
pi (z, s‚àíi ) = ‚àíbz 2 + (d ‚àí bu)z

and SW (z, s‚àíi ) = ‚àíbz 2 + (d ‚àí 2bu)z + u(d ‚àí bu).

Denote now si by y and consider a strategy x of player i such that pi (x, s‚àíi ) > pi (y, s‚àíi ).
d
, so SW (y, s‚àíi ) > SW (x, s‚àíi ).
Then u + x 6= 2b
We have
pi (x, s‚àíi ) ‚àí pi (y, s‚àíi ) = ‚àíb(x2 ‚àí y 2 ) + (d ‚àí bu)(x ‚àí y)

= ‚àíb(x ‚àí y)(x + y + u ‚àí db ) = ‚àíb(x ‚àí y)(x ‚àí

d
2b ),

d
) on the account of the equality u+y =
where the last equality holds since u‚àí db = ‚àí(y+ 2b
Further,

SW (y, s‚àíi ) ‚àí SW (x, s‚àíi ) = b(x2 ‚àí y 2 ) ‚àí (d ‚àí 2bu)(x ‚àí y)

= b(x ‚àí y)(x + y + 2u ‚àí db ) = b(x ‚àí y)2 ,
232

d
2b .

Selfishness Level of Strategic Games

where the last equality holds since 2u ‚àí db = ‚àí2y on the account of the equality u + y =
We have x 6= y. Hence
AFi (x, s) =

d
2b .

d
d
x ‚àí 2b
y ‚àí 2b
pi (x, s‚àíi ) ‚àí pi (y, s‚àíi )
=‚àí
= ‚àí1 +
.
SW (y, s‚àíi ) ‚àí SW (x, s‚àíi )
x‚àíy
y‚àíx

d
Since pi (x, s‚àíi )‚àípi (y, s‚àíi ) = b(y‚àíx)(x‚àí 2b
) we have pi (x, s‚àíi )‚àípi (y, s‚àíi ) > 0 iff y < x <
d
d
d
or y > x > 2b . But y ‚â§ 2b , since u + y = 2b . So the conjunction of pi (x, s‚àíi ) > pi (y, s‚àíi )
d
and SW (x, s‚àíi ) > SW (y, s‚àíi ) holds iff y < x < 2b
. Now supy<x< d AFi (x, s) = ‚àû. But s
2b
was an arbitrary stable social optimum, so the claim follows by Theorem 4 (i).
d
2b

This proof shows that for every stable social optimum s, for every player there exist
deviating strategies with an arbitrary high appeal factor. In fact, limx‚Üíy+ AFi (x, s) = ‚àû,
i.e., the appeal factor of the deviating strategy x converges to ‚àû when it converges from
the right to the original strategy y in s.
4.10 Bertrand Competition
Next, we consider Bertrand competition, a game concerned with a simultaneous selection of
prices for the same product by two firms (see, e.g., Jehle & Reny, 2011, pp. 175‚Äì177). The
product is then sold by the firm that chose the lower price. In the case of a tie the product
is sold by both firms and the profits are split. We assume that each firm has identical
marginal costs c > 0 and no fixed cost, and that each strategy set Si equals [c, ab ), where
c < ab . The payoff function for player i ‚àà {1, 2} is given by
Ô£±
if c < si < s3‚àíi
Ô£¥
Ô£≤(si ‚àí c)(a ‚àí bsi )
1
pi (si , s3‚àíi ) := 2 (si ‚àí c)(a ‚àí bsi ) if c < si = s3‚àíi
Ô£¥
Ô£≥
0
otherwise.

Proposition 11. The selfishness level of the Bertrand competition game is ‚àû.
Proof. Let d := a+bc
2b . If SW (s) > 0, then SW (s) = (s0 ‚àíc)(a‚àíbs0 ), where s0 := min(s1 , s2 ).
Note that d ‚àà (c, ab ), since by the assumption bc < a. Hence s is a social optimum iff
min(s1 , s2 ) = d.
If s is a social optimum with s1 6= s2 , then player i with the larger si can profitably
deviate to s3‚àíi (that equals d), while (s3‚àíi , s3‚àíi ) remains a social optimum. So the only
stable social optimum is (d, d).
Fix i ‚àà {1, 2}. Note that if si is slightly lower than d, then pi (si , d) > pi (d, d). Further,
lim (pi (si , d) ‚àí pi (d, d)) = 12 (d ‚àí c)(a ‚àí bd),

si ‚Üíd‚àí

while

lim (SW (d, d) ‚àí SW (si , d)) = 0

si ‚Üíd‚àí

and SW (d, d) ‚àí SW (si , d) 6= 0 for si 6= d. Hence
pi (si , d) ‚àí pi (d, d)
= ‚àû.
c<si <d SW (d, d) ‚àí SW (si , d)
sup

The claim now follows by Theorem 4 (i).
233

Apt & SchaÃàfer

5. Extensions and Future Research Directions
We introduced the selfishness level of a game as a new measure of discrepancy between
the social welfare in a Nash equilibrium and in a social optimum. Our studies reveal that
the selfishness level often provides deeper insights into the characteristics that influence the
players‚Äô willingness to cooperate. We conclude by mentioning some natural extensions and
future research directions.
5.1 Extensions
The definition of the selfishness level naturally extends to other solution concepts and other
forms of games.
5.1.1 Mixed Nash Equilibria
For mixed Nash equilibria we can simply adapt our definitions by stipulating that a strategic
game G is Œ±-selfish if a mixed Nash equilibrium of G(Œ±) is a social optimum, where now we
also allow social optima in mixed strategies. The selfishness level of G is then defined as
before in (1).
For example, with this notion the selfishness level of the Matching Pennies game (Example 3) is 0 since its unique mixed Nash equilibrium, ( 21 H + 21 T, 12 H + 12 T ), is also a social
optimum. The Matching Pennies game has no pure Nash equilibrium. In contrast, the game
from Example 4 does have a pure Nash equilibrium. When we use mixed Nash equilibria
its selfishness level also becomes 0. So in both games the selfishness level changed from ‚àû,
when pure Nash equilibria are used, to 0, when mixed Nash equilibria are used.
Further, a finite selfishness level of a finite game can decrease when we use mixed Nash
equilibria. As an example consider the following ‚Äòamalgamation‚Äô of the Matching Pennies
(with payoffs increased by 2) and Prisoner‚Äôs Dilemma (with payoffs increased by 1) games:

H
T
C
D

H
3, 1
1, 3
0, 0
0, 0

T
1, 3
3, 1
0, 0
0, 0

C
0, 0
0, 0
2, 2
3, 0

D
0, 0
0, 0
0, 3
1, 1

This game has a unique stable social optimum, (C, C), and a unique pure Nash equilibrium, (D, D). It is easy to check using Theorem 4 (ii) that its selfishness level is 1. On
the other hand, when we use mixed Nash equilibria then the selfishness level becomes 0.
Indeed, ( 12 H + 21 T, 12 H + 12 T ) is both a mixed Nash equilibrium and a social optimum in
mixed strategies.
5.1.2 Extensive Games
We can also consider extensive games and subgame perfect equilibria. As an example
consider the six-period version of the centipede game (see, e.g., Osborne, 2005):
234

Selfishness Level of Strategic Games

1

C

2

C

1

C

2

C

1

C

2

C

S

S

S

S

S

S

(1, 0)

(0, 2)

(3, 1)

(2, 4)

(5, 3)

(4, 6)

(6, 5)

In its unique subgame perfect equilibrium each player chooses S in every period and the
resulting payoffs are (1, 0). In contrast, the social optimum is obtained when each player
chooses C in every period and the resulting payoffs are (6, 5). We seek Œ± such that in the
resulting game G(Œ±) the latter pair of strategies forms a subgame perfect equilibrium. In
particular, player 2 should choose in the last round of G(Œ±) the action C. This happens
when 5 + (6 + 5)Œ± ‚â• 6 + (4 + 6)Œ± which holds iff Œ± ‚â• 1. Now, for Œ± = 1 the game G(Œ±) has
the following payoffs:
1

C

2

C

1

C

2

C

S

S

S

S

(2, 1)

(2, 4)

(7, 5)

(8, 10)

1

C
S

2

C

(17, 16)

S

(13, 11) (14, 16)

So in this game the pair of strategies in which each player chooses C in every period is
both a subgame perfect equilibrium and a social optimum and yields the payoffs (17, 16).
We conclude that the (appropriately adapted) selfishness level for this game is 1.
We leave for future work the study of such alternatives.
5.2 Future Research Directions
There are several intriguing questions that we left open. We discuss a few future research
directions below.
5.2.1 Abstract Games
It would be interesting to define the notion of a selfishness level for abstract games.
These are games in which the payoffs are replaced by preference relations (see Osborne &
Rubinstein, 1994). By a preference relation on a set A we mean here a linear ordering
on A. More precisely, an abstract game is defined as (N, {Si }i‚ààN , {i }i‚ààN ) where each
i is player‚Äôs i preference relation defined on the set S1 √ó ¬∑ ¬∑ ¬∑ √ó Sn of joint strategies.
By a realization of an abstract game (N, {Si }i‚ààN , {i }i‚ààN ) we mean any strategic game
(N, {Si }i‚ààN , {pi }i‚ààN ) such that for all i ‚àà N and s, s‚Ä≤ ‚àà S1 √ó ¬∑ ¬∑ ¬∑ √ó Sn we have s i s‚Ä≤ iff
pi (s) i pi (s‚Ä≤ ).
Unfortunately, it is not clear how to do this. First, note that the notion of a Nash
equilibrium is well defined for abstract games. However, there is no counterpart of the
notion of a social optimum, since there is no ‚Äòglobal‚Äô preference relation on the set of joint
strategies.
It is tempting to circumvent this difficulty by defining the notion of a selfishness level of
an abstract game G using its realizations G‚Ä≤ and the corresponding games G‚Ä≤ (Œ±(G‚Ä≤ )), where
235

Apt & SchaÃàfer

Œ±(G‚Ä≤ ) is the selfishness level of G‚Ä≤ . Unfortunately the resulting strategic games G‚Ä≤ (Œ±(G‚Ä≤ )),
where G‚Ä≤ is a realization of G are not realizations of a single abstract game, so this ‚Äòdetour‚Äô
does not allow us to associate with the initial abstract game another one.
As an example take two realizations of the abstract Prisoner‚Äôs Dilemma game and the
corresponding games G‚Ä≤ (Œ±(G‚Ä≤ )):
C
D
C
D

C
2, 2
3, 0
C
2, 2
2.5, 0

D
0, 3
1, 1
D
0, 3
1, 1

C
D

C
6, 6
6, 3

C
D

C
6, 6
5, 2.5

D
3, 6
3, 3
D
3, 6
3, 3

So both realizations have the selfishness level 1 but the transformed games do not correspond to the same abstract game, since in the first transformed game we have p2 (D, C) ‚â•
p2 (D, D), while in the second one p2 (D, D) > p2 (D, C).
5.2.2 Selfishness Function
In our approach we assigned to each game a positive real number, its selfishness level.
A natural generalization of this idea would be to assign to each game G the function
fG : R+ ‚Üí R+ , where f (Œ±) equals the price of stability of the game G(Œ±). Then the
selfishness level of G is inf{Œ± ‚àà R+ | fG (Œ±) = 1}.
The function fG has been studied for altruistic extensions of linear congestion games
and fair cost sharing games (Chen et al., 2011; Elias et al., 2010). However, in these papers
only upper bounds on fG are derived, which in light of the results obtained here cannot be
tight. It would be interesting to determine fG exactly for these games. This would probably
require a generalization of the characterization result presented in this paper.
5.2.3 Alternative Approach Based on the Price of Anarchy
We defined the selfishness level of a game as the smallest Œ± such that the price of stability
of G(Œ±) is 1. Alternatively, one might define the selfishness level as the smallest Œ± such that
the price of anarchy of G(Œ±) is 1. This alternative approach often yields the value ‚àû. Take
for instance the following coordination game G:
A
1, 1
0, 0

A
B

B
0, 0
0, 0

Then for every Œ± ‚â• 0 (A, A) is a social optimum in G(Œ±) with the social welfare 2 + 4Œ±,
while (B, B) is a Nash equilibrium in G(Œ±) with the social welfare 0. So this alternative
selfishness level of the game G is ‚àû, while the original selfishness level is of course 0.
As another example consider the game G below left and the corresponding game G(Œ±)
below right:
A
B

A
1, 1
0, 3

B
3, 0
0, 0

A
B

A
1 + 2Œ±, 1 + 2Œ±
3Œ±, 3 + 3Œ±
236

B
3 + 3Œ±, 3 + 3Œ±
0, 0

Selfishness Level of Strategic Games

Its selfishness level is 1, since this is the smallest value Œ± for which (A, B) is a Nash equilibrium in G(Œ±). On the other hand, if we focus on the price of anarchy, then we need to
choose the smallest Œ± such that (A, A) is not a Nash equilibrium in G(Œ±) while (A, B) is.
This is the case iff 3Œ± > 1 + 2Œ±, i.e., when Œ± > 1. So this alternative selfishness level of the
game G is 1+ .
In view of these examples we find this alternative approach not very promising. Still, it
might be interesting to clarify for which games it yields finite values.
5.2.4 Alternative Approach Based on Approximate Nash Equilibria
As mentioned in the related work section, an alternative approach to measure the stability of equilibria of a game is the following. Given a payoff-maximization game G =
(N, {Si }i‚ààN , {pi }i‚ààN ), we call G Œµ-stable for some Œµ ‚â• 0 if there exists a social optimum s
that is also a (1 + Œµ)-approximate Nash equilibrium, i.e., for every player i ‚àà N and every
s‚Ä≤i ‚àà Si , (1 + Œµ)pi (s) ‚â• pi (s‚Ä≤i , s‚àíi ).3 We define the stability level of G as the infimum over
all Œµ ‚â• 0 such that G is Œµ-stable. Intuitively, a stability level of Œµ means that if we alter the
players‚Äô incentives by scaling their payoffs by a factor of (1 + Œµ) then a social optimum is
realized as a Nash equilibrium.
It would be interesting to study how the stability level of a game relates to its selfishness
level. Using the above definitions, it is easy to see that when a game G admits a social
optimum s such that for every player i ‚àà N and every s‚Ä≤i ‚àà Si , pi (s) ‚â• SW (s)‚àíSW (s‚Ä≤i , s‚àíi ),
then G is Œ±-stable if G is Œ±-selfish.4 Said differently, the stability level of G is at most its
selfishness level. Similarly, when the reverse inequality holds then G is Œµ-selfish if G is
Œµ-stable.
In particular, the above observation can be applied to fair cost sharing games, where
for every joint strategy s it holds that for every i ‚àà N and every s‚Ä≤i ‚àà Si , ci (s‚Ä≤i , s‚àíi ) ‚â•
SC(s‚Ä≤i , s‚àíi ) ‚àí SC(s) (see also (11) in Section 4.3). We conclude that the stability level of
a fair cost sharing game G is at most its selfishness level. As a consequence, our bounds
on the selfishness level derived in Section 4.3 extend to the stability level in this case.
Further, it is not hard to verify that the stability level for singleton cost sharing games is
at least max{0, 21 cmax /cmin ‚àí 1} and for cost sharing games with integer costs is at least
max{0, 21 Lcmax ‚àí 1} by considering the examples given in the proofs of Proposition 2 and
Proposition 3, respectively. Thus, for these games the stability level coincides with the
selfishness level.
However, it can be seen that these two notions do not always coincide. The public
goods game is another example where it holds that there exists a social optimum s such
that for every player i ‚àà N and every s‚Ä≤i ‚àà Si , pi (s) ‚â• SW (s) ‚àí SW (s‚Ä≤i , s‚àíi ) (see proof of
Proposition 7). Thus, the stability level of this game is at most the selfishness level. In
fact, simple calculations show that the stability level is max{0, (1 ‚àí nc )/c} ‚â§ max{0, (1 ‚àí
c
n )/(c ‚àí 1)}, where the latter is the selfishness level of the game.
We leave it for future work to further investigate the stability level and its relation to
the selfishness level.
3. For cost-minimization games, we require that ci (s) ‚â§ (1 + Œµ)ci (s‚Ä≤i , s‚àíi ).
4. For cost-minimization games, this inequality reads ci (s‚Ä≤i , s‚àíi ) ‚â• SC(s‚Ä≤i , s‚àíi ) ‚àí SC(s).

237

Apt & SchaÃàfer

5.2.5 Other Social Welfare Functions
In this paper we exclusively concentrated on social welfare functions which are defined as
the sum of the individual payoffs of the players. We leave it for future research to study the
selfishness level of games for other social welfare functions, e.g., maximizing the minimum
payoff of all players.

Acknowledgments
A preliminary version of this paper appeared in the Proceedings of the 5th International
Symposium on Algorithmic Game Theory (Apt & SchaÃàfer, 2012).
We acknowledge initial discussions with Po-An Chen and final discussions with Valerio
Capraro. We also thank anonymous reviewers of the preliminary version for their valuable
comments. We are particularly grateful to all three reviewers of JAIR for the most helpful
remarks.
Krzysztof R. Apt is also affiliated with the University of Amsterdam, Institute for Logic,
Language and Computation, Science Park 107, 1098 XG Amsterdam, The Netherlands.
Guido SchaÃàfer is also affiliated with the VU University Amsterdam, Department of
Econometrics and Operations Research, De Boelelaan 1105, 1081 HV Amsterdam, The
Netherlands.

References
Anshelevich, E., Dasgupta, A., Kleinberg, J., Tardos, E., Wexler, T., & Roughgarden, T.
(2008). The price of stability for network design with fair cost allocation. SIAM
Journal on Computing, 38 (4), 1602‚Äì1623.
Anshelevich, E., Das, S., & Naamad, Y. (2009). Anarchy, stability, and utopia: Creating
better matchings. In Proc. 2nd International Symposium on Algorithmic Game Theory
(SAGT09), Lecture Notes in Computer Science 5814, pp. 159‚Äì170. Springer.
Apt, K. R., & SchaÃàfer, G. (2012). Selfishness level of strategic games. In Proc. 5th International Symposium on Algorithmic Game Theory (SAGT12), pp. 13‚Äì24. Springer.
Ashlagi, I., Monderer, D., & Tennenholtz, M. (2008). On the value of correlation. Journal
of Artificial Intelligence Research, 33, 575‚Äì613.
Awerbuch, B., Azar, Y., & Epstein, A. (2013). The price of routing unsplittable flow. SIAM
Journal on Computing, 42 (1), 160‚Äì177.
Axelrod, R. (1984). The Evolution of Cooperation. Basic Books.
Balcan, M.-F., Blum, A., & Mansour, Y. (2009). Improved equilibria via public service
advertising. In Proceedings of the 20th Annual ACM-SIAM Symposium on Discrete
Algorithms, pp. 728‚Äì737. Society for Industrial and Applied Mathematics.
Basu, K. (1994). The traveler‚Äôs dilemma: paradoxes of rationality in game theory. American
Economic Review, 84 (2), 391‚Äì395.
Becker, T. C., Carter, M., & Naeve, J. (2005). Experts playing the traveler‚Äôs dilemma.
Tech. rep. 252/2005, Department of Economics, University of Hohenheim, Germany.
238

Selfishness Level of Strategic Games

Beckmann, M., McGuire, B., & Winsten, C. (1956). Studies in the Economics of Transportation. Yale University Press, New Haven.
BiroÃÅ, P., Manlove, D. F., & Mittal, S. (2010). Size versus stability in the marriage problem.
Theoretical Computer Science, 411 (16‚Äì18), 1828‚Äì1841.
Bowles, S. (2004). Microeconomics: Behavior, Institutions, and Evolution. Princeton University Press, Princeton.
Capraro, V. (2013). A model of human cooperation in social dilemmas. PLoS ONE, 8 (8),
1‚Äì6. e72427.
Caragiannis, I., Kaklamanis, C., Kanellopoulos, P., Kyropoulou, M., & Papaioannou, E.
(2010). The impact of altruism on the efficiency of atomic congestion games. In
Proc. 5th Symposium on Trustworthy Global Computing, pp. 172‚Äì188.
Chen, P.-A., & Kempe, D. (2008). Altruism, selfishness, and spite in traffic routing. In
Proc. 10th ACM Conference on Electronic Commerce, pp. 140‚Äì149.
Chen, P.-A., de Keijzer, B., Kempe, D., & SchaÃàfer, G. (2011). The robust price of anarchy
of altruistic games. In Proc. 7th Workshop on Internet and Network Economics, pp.
383‚Äì390.
Christodoulou, G., & Koutsoupias, E. (2005). The price of anarchy of finite congestion
games. In Proc. 37th Annual ACM Symposium on Theory of Computing, pp. 67‚Äì73.
Christodoulou, G., Koutsoupias, E., & Spirakis, P. G. (2011). On the performance of
approximate equilibria incongestion games. Algorithmica, 61 (1), 116‚Äì140.
Cooper, R., DeJong, D. V., Forsythe, R., & Ross, T. W. (1996). Cooperation without reputation: Experimental evidence from prisoner‚Äôs dilemma games. Games and Economic
Behavior, 12 (2), 187‚Äì218.
Dreber, A., Rand, D., Fudenberg, D., & Nowak, M. (2008). Winners don‚Äôt punish. Nature,
452, 348‚Äì351.
Elias, J., Martignon, F., Avrachenkov, K., & Neglia, G. (2010). Socially-aware network
design games. In Proc. INFOCOM 2010, pp. 41‚Äì45.
Feldman, M., & Tamir, T. (2009). Approximate strong equilibrium in job scheduling games.
Journal of Artificial Intelligence Research, 36, 387‚Äì414.
Fiat, A., Karlin, A., Koutsoupias, E., & Vidali, A. (2013). Approaching utopia: strong truthfulness and externality-resistant mechanisms. In Proceedings of the 4th conference on
Innovations in Theoretical Computer Science, pp. 221‚Äì230. ACM.
Goeree, J. K., & Holt, C. A. (2001). Ten little treasures of game theory and ten intuitive
contradictions. American Economic Review, 91, 1402‚Äì1422.
Hoefer, M., & Skopalik, A. (2009). Altruism in atomic congestion games. In Proc. 17th
European Symposium on Algorithms, pp. 179‚Äì189.
Isaac, R. M., & Walker, J. M. (1988). Group size effects in public goods provision: The
voluntary contributions mechanism. The Quarterly Journal of Economics, 103 (1),
179‚Äì199.
239

Apt & SchaÃàfer

Jehle, G., & Reny, P. (2011). Advanced Microeconomic Theory (Third edition). Addison
Wesley, New York, NY.
Jurca, R., & Faltings, B. (2009). Mechanisms for making crowds truthful. Journal of
Artificial Intelligence Research, 34, 209‚Äì253.
Kaporis, A. C., & Spirakis, P. G. (2009). The price of optimum in stackelberg games on
arbitrary single commodity networks and latency functions. Theoretical Computer
Science, 410 (8-10), 745‚Äì755.
Koutsoupias, E., & Papadimitriou, C. H. (2009). Worst-case equilibria. Computer Science
Review, 3 (2), 65‚Äì69.
Ledyard, J. O. (1995). Public Goods: A Survey of Experimental Research, chap. 2, pp.
111‚Äì194. The Handbook of Experimental Economics. Princeton University Press.
Marco, G. D., & Morgan, J. (2007). Slightly altruistic equilibria in normal form games.
Working paper No 185, Center for Studies in Economics and Finance, University of
Salerno, Italy. Available from http://www.csef.it/WP/wp185.pdf.
Milchtaich, I. (1996). Congestion games with player-specific payoff functions. Games and
Economic Behaviour, 13, 111‚Äì124.
Monderer, D., & Shapley, L. S. (1996). Potential games. Games and Economic Behaviour,
14, 124‚Äì143.
Osborne, M. J. (2005). An Introduction to Game Theory. Oxford University Press, Oxford.
Osborne, M. J., & Rubinstein, A. (1994). A Course in Game Theory. The MIT Press,
Cambridge, Massachusetts.
Ritzberger, K. (2002). Foundations of Non-cooperative Game Theory. Oxford University
Press, Oxford.
Rosenthal, R. W. (1973). A class of games possessing pure-strategy Nash equilibria. International Journal of Game Theory, pp. 65‚Äì67.
Roughgarden, T. (2009). Intrinsic robustness of the price of anarchy. In Proc. 41st Annual
ACM Symposium on Theory of Computing, pp. 513‚Äì522.
Schulz, A. S., & Moses, N. E. S. (2003). On the performance of user equilibria in traffic
networks. In SODA, pp. 86‚Äì87.
Sharma, Y., & Williamson, D. P. (2009). Stackelberg thresholds in network routing games
or the value of altruism. Games and Economic Behavior, 67 (1), 174‚Äì190.
Tardos, EÃÅ., & Vazirani, V. J. (2007). Basic solution concepts and computational issues. In
Nisan, N., Roughgarden, T., Tardos, EÃÅ., & Vazirani, V. J. (Eds.), Algorithmic Game
Theory, chap. 1, pp. 3‚Äì28. Cambridge University Press.
Young, H. P. (1993). The evolution of conventions. Econometrica, 61 (1), 57‚Äì84.

240

Journal of Artificial Intelligence Research 49 (2014) 451-500

Submitted 10/13; published 03/14

Text-Based Twitter User Geolocation Prediction
Bo Han

HANB @ STUDENT. UNIMELB . EDU . AU

The University of Melbourne, VIC 3010, Australia
NICTA Victoria Research Laboratory

Paul Cook

PAULCOOK @ UNIMELB . EDU . AU

The University of Melbourne, VIC 3010, Australia

Timothy Baldwin

TB @ LDWIN . NET

The University of Melbourne, VIC 3010, Australia
NICTA Victoria Research Laboratory

Abstract
Geographical location is vital to geospatial applications like local search and event detection. In
this paper, we investigate and improve on the task of text-based geolocation prediction of Twitter
users. Previous studies on this topic have typically assumed that geographical references (e.g.,
gazetteer terms, dialectal words) in a text are indicative of its author‚Äôs location. However, these
references are often buried in informal, ungrammatical, and multilingual data, and are therefore
non-trivial to identify and exploit. We present an integrated geolocation prediction framework
and investigate what factors impact on prediction accuracy. First, we evaluate a range of feature
selection methods to obtain ‚Äúlocation indicative words‚Äù. We then evaluate the impact of nongeotagged tweets, language, and user-declared metadata on geolocation prediction. In addition, we
evaluate the impact of temporal variance on model generalisation, and discuss how users differ in
terms of their geolocatability.
We achieve state-of-the-art results for the text-based Twitter user geolocation task, and also
provide the most extensive exploration of the task to date. Our findings provide valuable insights
into the design of robust, practical text-based geolocation prediction systems.

1. Introduction
The growing volume of user-generated text posted to social media services such as Twitter, Facebook, and Tumblr can be leveraged for many purposes ranging from natural disaster response to targeted advertising (Tuten, 2008; NuÃÅnez-RedoÃÅ, Dƒ±ÃÅaz, Gil, GonzaÃÅlez, & Huerta, 2011; Yin, Lampert,
Cameron, Robinson, & Power, 2012). In many circumstances it is important to know a user‚Äôs location in order to accomplish these tasks effectively. For example, disaster response managers must
know where to direct resources in order to effectively coordinate aid, and advertisers could benefit
from tailoring advertisements to a user‚Äôs location. Similarly, search results localisation hinges on
knowledge of a user‚Äôs location. Although many social media services allow a user to declare their
location, such metadata is known to be unstructured and ad hoc (Hecht, Hong, Suh, & Chi, 2011)
(e.g., melbo denoting Melbourne, AU 1 ), as well as oftentimes non-geographical (e.g., in my own
1. Throughout the paper, we present city names with ISO 3166-1 alpha-2 country-level designators such as AU =
Australia and CA = Canada. Where US-based city names are mentioned in the context of the North American
regional dataset used in experimentation (NA), we use an ISO 3166-2:US designator such as US-CA = California or
US-PA = Pennsylvania.
c
‚Éù2014
AI Access Foundation. All rights reserved.

H AN , C OOK & BALDWIN

little bubble). Text-based geolocation ‚Äî automatically predicting a user‚Äôs location based on the
content of their messages ‚Äî is therefore becoming of increasing interest (e.g., Cheng, Caverlee, &
Lee, 2010, and others). In this paper we investigate and improve text-based geolocation prediction
for Twitter users. Specifically, we exploit the tweets and profile information of a given user to infer
their primary city-level location, which we claim is sufficiently fine-grained to support the sorts of
applications mentioned above.
As is well established in previous work (e.g., Wing & Baldridge, 2011, and others), it is reasonable to assume that user posts in social media reflect their geospatial locum, because lexical priors
differ from region to region. For example, a user in London is much more likely to talk about Piccadilly and tube than a user in New York or Beijing. That is not to say that those words are uniquely
associated with London, of course: tube could certainly be mentioned by a user outside of the UK.
However, the use of a range of such words with high relative frequency is strongly indicative of the
fact that a user is located in London. Most work in this area utilises geotagged data as ground truth
for evaluation (e.g., Eisenstein, O‚ÄôConnor, Smith, & Xing, 2010, and others). The geotagged data
contains GPS coordinates inserted with the user‚Äôs consent by a GPS-enabled device such as a smartphone, and offers accurate information about a user‚Äôs position at the time of tweeting. Although
approaches to text-based geolocation are offering increasingly promising results, the studies to date
on this topic have been limited in a number of important ways. We raise some key issues in Section
3 and investigate them in turn, focusing on the following issues:
1.1 Location Indicative Words
Text-based geolocation prediction models for social media are predominantly based on the full text
data of tweets, including common words with no geospatial dimension (e.g., today), potentially
hampering prediction, and because of the large number of words observed in tweets, leading to
slower, more memory-intensive models. We tackle this by automatically finding location indicative
words (LIWs) via feature selection, and demonstrating that the reduced feature set boosts geolocation accuracy. In Section 5, we carry out extensive evaluation over a wide range of feature selection
methods proposed in the literature, and show that an information gain ratio-based approach outperforms benchmark geolocation prediction methods by 10.6 percentage points in terms of accuracy,
and reduces the median prediction error distance by 209km on a publicly-available regional (North
America) dataset. We similarly demonstrate the effectiveness of LIW selection on a global dataset
in Section 6.
1.2 Non-geotagged Tweets
In addition to experimenting with geotagged data, we further extend our analysis to incorporate nongeotagged tweets. Some recent work (e.g., Roller, Speriosu, Rallapalli, Wing, & Baldridge, 2012)
has incorporated non-geotagged training data, although little work has analysed the contribution of
non-geotagged data, i.e., the extent to which incorporating non-geotagged data improves geolocation accuracy. Furthermore, the evaluation of previous models has been restricted to geotagged data
(in order to have access to a ground truth) although the goal of this line of research is to be able to
infer locations for users whose locations are not known. However, it is unclear how well models
evaluated only on geotagged data will generalise to non-geotagged data. For example, because geotagged tweets are sent from GPS-enabled devices such as smartphones, while non-geotagged tweets
452

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

are sent from a range of devices (including desktop computers), these two types of data could have
different characteristics (Gouws, Metzler, Cai, & Hovy, 2011).
In Section 7, we address these issues by training and testing on geotagged tweets, non-geotagged
tweets, and the combination of the two. We show that by exploiting a user‚Äôs non-geotagged tweets,
the city-level accuracy is improved from 12.6% to 28.0% on a benchmark dataset, underlining
the potential contribution of non-geotagged data. Furthermore, the numbers also suggest that a
model trained on geotagged data indeed generalises to non-geotagged data, although sub-domain
differences between geotagged data and non-geotagged data are observed.
1.3 Language Influence
With some exceptions (e.g., Kinsella, Murdock, & O‚ÄôHare, 2011), most text-based geolocation
studies have been carried out in an English-only setting, or a primarily English setting. Because
high-accuracy language identification tools (Lui & Baldwin, 2012; Nakatani, 2010) are now readily
available, this is not a problem: messages in the target language can be identified, and text-based
geolocation methods can be applied to only those messages. However, it remains to be seen whether
text-based geolocation approaches that have been shown to work well for English perform as well
on other languages, or perform well in a multilingual setting. English is tweeted throughout the
world, whereas languages such as Indonesian are primarily tweeted in localised areas. As such, the
performance of methods developed and tested over English data could be very different when applied to other languages. We investigate the language influence on a multilingual dataset in Section
8. The results suggest that our model indeed generalises from a monolingual English to a multilingual setting. Furthermore, the experiments reveal that geolocation prediction is much easier for
languages with more geographically-restricted use (e.g., Indonesian) than languages that are more
diverse in usage (e.g., English). We then go on to show that a composite model consisting of a
number of monolingual geolocation models based on language identification outperforms a model
trained on multilingual data.
1.4 Metadata and Ensemble Learning
Although tweet-based geolocation is worthy of study in its own right, tweets are accompanied by
rich metadata in public user profiles. This metadata is included in the payload of JSON objects containing tweets, and offers complementary information that may be exploited to improve accuracy,
e.g., timezone data and the user-declared location. While there has been some work on utilising
timezone (Mahmud, Nichols, & Drews, 2012) and user-declared location (Hecht et al., 2011) information for user geolocation, the metadata remains largely untouched in the literature. In Section
9, we investigate the performance of metadata-based geolocation models and compare them with
benchmark methods. We show that by incorporating information from metadata and the tweet message in a stacking-based approach, a city-level accuracy of 49.1%, and a median prediction error
distance of just 9km, can be achieved over our global dataset, which is a substantial improvement
over any of the base classifiers.
1.5 Temporal Influence
Because Twitter is a growing and evolving medium, the data in Twitter streams tends to be locally temporal to the time of posting. In addition to evaluating the geolocation model on ‚Äúold‚Äù
453

H AN , C OOK & BALDWIN

time-homogeneous data (sampled from the same time period as the training data), in Section 10
we evaluate the trained model on a ‚Äúnew‚Äù time-heterogeneous dataset, which was collected approximately one year after the training and test data used in our earlier experiments. The observed
moderate decline in results indicates that the stacked geolocation model is indeed influenced by
temporal changes. Error analysis reveals that this is primarily caused by the unreliability of the
base model trained on user-declared locations. In contrast, we find that models trained on tweet
text and timezone information are relatively insensitive to temporal changes. This finding on the
one hand justifies the efforts to-date in pursuing better text-based geolocation prediction, and on the
other hand suggests that if user-declared location data is to be used, the model has to be periodically
updated to remain current to temporal changes.
1.6 User Geolocatability and Prediction Confidence
We further discuss the geolocatability of users with regard to tweeting behaviour in Section 11.
For instance, does mentioning many local place names have a strong influence on the prediction
accuracy? Experiments suggest the number of LIWs (in particular, gazetted location names) and
user-declared metadata are key to geolocating a user. Because of different tweeting behaviours
among users, not all users are equally geolocatable, with only predictions for a proportion of them
being reliable. We further conduct a pilot study on approximating the prediction confidence through
a range of variables in Section 12.
This paper advances the state-of-the-art of text-based geolocation prediction in a number of
directions, and provides practical guidelines for the design of a text-based geolocation application.
This paper builds off our own previously-published work (Han, Cook, & Baldwin, 2012b, 2013)
with much more extensive evaluation, and new work in the following areas:
‚Ä¢ A large-scale comparative evaluation of twelve feature selection methods for user geolocation
‚Äî nine of which were not considered in our earlier work ‚Äî in Sections 4‚Äì6.
‚Ä¢ The analysis of the impact of training on non-geotagged data in Section 7.
‚Ä¢ A new set of experiments, and subsequent analysis, examining the influence of language in
Section 8.
‚Ä¢ Further analysis of the utility of user-supplied metadata and ensemble learning in Section 9.
‚Ä¢ More-detailed analysis of model generalisation on temporal change in Section 10 including
city-level meta-analysis.
‚Ä¢ A new pilot study on user geolocatablility and privacy in Section 11.
The proposed text-based method primarily uses words for geolocation prediction, and intentionally excludes Twitter specific entities, such as hashtags and user mentions. The prediction accuracy
therefore largely depends on whether the text contains sufficient geospatial information for geolocation prediction. Therefore, although this paper focuses exclusively on Twitter, the proposed method
could equally be applied to other forms of social media text, such as Facebook status updates or
user-submitted comments (to services such as YouTube).
454

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

2. Related Work
While acknowledging potential privacy concerns (Mao, Shuai, & Kapadia, 2011; Pontes, Vasconcelos, Almeida, Kumaraguru, & Almeida, 2012), accurate geolocation prediction is a key driver
for location-specific services such as localised search, and has been the target of research across
different disciplines. For example, the tagging of both user queries (Wang, Wang, Xie, Forman,
Lu, Ma, & Li, 2005; Backstrom, Kleinberg, Kumar, & Novak, 2008; Yi, Raghavan, & Leggetter,
2009) and web pages (Ding, Gravano, & Shivakumar, 2000; Amitay, Har‚ÄôEl, Sivan, & Soffer, 2004;
Zong, Wu, Sun, Lim, & Goh, 2005; Silva, Martins, Chaves, Afonso, & Cardoso, 2006; Bennett,
Radlinski, White, & Yilmaz, 2011) has been considered in information retrieval. In geographical
information science, the primary focus has been on recognising location mentions in text (Leidner
& Lieberman, 2011), with named entity recognition tools typically employed to detect and extract
such mentions (Quercini, Samet, Sankaranarayanan, & Lieberman, 2010; Gelernter & Mushegian,
2011). Within the social media realm, geolocation methods have been applied to images on Flickr
(Crandall, Backstrom, Huttenlocher, & Kleinberg, 2009; Serdyukov, Murdock, & van Zwol, 2009;
Hauff & Houben, 2012; O‚ÄôHare & Murdock, 2013; Laere, Schockaert, & Dhoedt, 2013), Wikipedia
articles (Lieberman & Lin, 2009), individual tweets (Kinsella et al., 2011), Twitter users (Eisenstein
et al., 2010; Cheng et al., 2010; Kinsella et al., 2011; Wing & Baldridge, 2011; Roller et al., 2012;
Han et al., 2012b), and for identifying words and topics on Twitter that are salient in particular
regions (Eisenstein et al., 2010; Yin, Cao, Han, Zhai, & Huang, 2011; Hong, Ahmed, Gurumurthy,
Smola, & Tsioutsiouliklis, 2012; Dalvi, Kumar, & Pang, 2012).
Identifying Twitter users‚Äô locations is non-trivial, mainly due to the unavailability of reliable
geographic information. Although Twitter allows users to declare their location in their user profile,
the location descriptions are unstructured and ad hoc (Cheng et al., 2010; Hecht et al., 2011), e.g.,
people use vernacular expressions such as philly, or non-standard spellings such as Filladephia, to
refer to Philadelphia; non-geographical descriptions like in your heart are also commonly found.
Without appropriate processing, the value of these location fields is greatly limited. Hecht et al.
(2011) demonstrate that trivially using these location fields in off-the-shelf geolocation tools is ineffective. Alternatively, some tweets sent from mobile devices are geotagged with accurate GPS
coordinates, however, the proportion of geotagged tweets is estimated to be a mere 1% (Cheng
et al., 2010), and the location of the vast majority of users are not geotagged. Methods based on
IP addresses (Buyukokkten, Cho, Garcia-Molina, Gravano, & Shivakumar, 1999) can be applied to
the task, and in general web contexts have been shown to achieve around 90% accuracy at mapping
Internet hosts to their locations (Padmanabhan & Subramanian, 2001). Such methods are not applicable to Twitter and many other social media services, however, as the IP address of the device
the message was sent from cannot be accessed via any of the public APIs. Doubtless Twitter itself
has access to this information and can use it for user geolocation, although even here, geographical divisions of IP addresses are not always credible. For instance, departments in an international
corporation might use the same IP address range, but their true locations could be spread across the
world. VPNs are also a complication for such approaches. Any third-party service provider making
use of Twitter data, however, has to look to other sources of geospatially-identifying information,
including the text content of the user‚Äôs posts and metadata information, as targeted in this research.
In the spatial data mining community, geographical references (e.g., gazetteer terms) in text have
also been exploited to infer geolocation. Intuitively, if a place is frequently mentioned by a user in
their tweets, they are likely tweeting from that region. Methods building on this intuition range from
455

H AN , C OOK & BALDWIN

naive gazetteer matching and rule-based approaches (Bilhaut, Charnois, Enjalbert, & Mathet, 2003),
to machine learning-based methods (primarily based on named entity recognition: Quercini et al.,
2010; Gelernter & Mushegian, 2011). Despite the encouraging results of this approach on longer
and more homogeneous documents sets (Quercini et al., 2010), its performance is impeded by the
nature of tweets: they are short and informal, and the chances of a user not mentioning gazetted
places in their tweets is high. Moreover, the handling of vernacular place names, e.g., melbo for
Melbourne, in this approach is limited. The reliance on named entity recognition is thwarted by the
unedited nature of social media data, where spelling and capitalisation are much more ad hoc than
in edited document collections (Ritter, Clark, Mausam, & Etzioni, 2011; Han, Cook, & Baldwin,
2012a).
Moving beyond off-the-shelf solutions, recently, many robust machine learning methods have
been applied to geolocation, with the primary approach being to estimate locations based on the
textual content of tweets. For instance, Cheng et al. (2010) exploit words known to be primarily used in particular regions, along with smoothing techniques, to improve a simple generative
geolocation model when applied to data from the continental United States. Wing and Baldridge
(2011) divide the world‚Äôs surface into a uniform-size grid, and compare the distribution of words in
a given user‚Äôs tweets to those in each grid cell using Kullback-Leibler (KL) divergence to identify
that user‚Äôs most likely location. One limitation of this approach is that grid cells in rural areas tend
to contain very few tweets, while there are many tweets from more urban grid cells. Roller et al.
(2012) therefore extend this method to use an adaptive grid representation in which cells contain
approximately the same amount of data, based on a k-d tree (Bentley, 1975). Kinsella et al. (2011)
examine geolocation prediction at different granularities (e.g., zip codes, city, state and country).
Chang, Lee, M., and Lee (2012) prune noisy data based on geometrically-local words (i.e., words
that occur geographically close to each other, and are only found in a limited number of cities) and
non-stop words that are dis-similar to stop words, and they experiment with the reduced feature set
using both a Gaussian mixture model and Maximum Likelihood Estimation for location prediction.
Beyond purely text-based methods (language model-based methods), other sources of information
have also been integrated. Li, Serdyukov, de Vries, Eickhoff, and Larson (2011) investigate geolocation prediction based on a linear rank combination of text and temporal factors. Mahmud et al.
(2012) combine timezone information and content-based classifiers in a hierarchical model for geolocation. In particular, nouns, hashtags, and place names are considered as content in the method.
Schulz, Hadjakos, Paulheim, Nachtwey, and MuÃàhlhaÃàuser (2013) combine scores from various geographical sources (e.g., tweet text, user profile data). The sum of scores for a location is represented
by the ‚Äúaggregated height‚Äù on a polygon-partitioned map, and the highest polygon is the predicted
location.
Topics discussed on Twitter vary across geographical regions. Intuitively, for instance, Americans are more likely to talk about NBA and baseball than Australians (who probably mention AFL
and rugby more often). To capture these regional topic variations in Twitter, topic modelling-based
approaches have also been used to incorporate geographical regions in the generative process. For
instance, Eisenstein et al. (2010) introduce a geographical variable (r); instead of generating an observed word w from a per-word topic distribution œïz as in the standard Latent Dirichlet Allocation
(LDA) model (Blei, Ng, & Jordan, 2003), their proposed approach refines this step by additionally
modeling the topic distributions across different geographical regions, i.e., w is generated from a
per-word region-topic distribution œïrz . Therefore, the observed user locations are generated from
geographical regions and the region variable in topic modeling is linked with user geographical
456

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

locations. Generally, a user‚Äôs location is predicted at the regional level by adopting the location
centroid for geotagged tweets from that region. Hong et al. (2012) further improve the approach by
considering more fine-grained factors in an additive generative model. In addition to introducing
per-region topic variance, they incorporate per-user topic variance, a regional language model, and
global background topics. To compensate for the computational complexity associated with these
extra hidden variables, they adopt sparse modeling in inference. On top of these geolocation prediction tasks, many other research problems also involve the modelling of geographical locations.
Dalvi et al. (2012) exploit the impact of geographical locations on users‚Äô discussions of pre-defined
objects (e.g., restaurants) in tweets. Yin et al. (2011) propose ways to discover and compare topics
for geographical regions by jointly modelling locations and text. Despite the benefits of incorporating per-region topic variance in these models, a few concerns prevent us from using topic modeling
approaches in this study. First, the temporal currency of geographical topics can be limited, e.g.,
Olympics or playoffs. These temporally-specific topics are less indicative of location for future inference, e.g., geolocating users after the model has been trained. Furthermore, topic modelling is
generally computationally expensive, and suffers efficiency problems when applied to large volumes of data, such as that available through social media. Therefore we experiment with language
model-based methods that are better suited to large-scale data.
Social network information, including both explicit friendship relations (Backstrom, Sun, &
Marlow, 2010; Sadilek, Kautz, & Bigham, 2012; Rout, Bontcheva, Preotiuc-Pietro, & Cohn, 2013)
and implicit social interactions (Chandra, Khan, & Muhaya, 2011; Jurgens, 2013), has been shown
to be effective in predicting locations. City-level prediction results range from approximately 50‚Äì
80% (Rout et al., 2013) depending on a wide range of factors including the user density in the social
network and the precise scope of the geolocation prediction task. However, social networks are
dynamic, and this information is often more difficult to obtain than text data on a large scale. For
instance, obtaining social network information requires multiple requests to the rate-limited Twitter
API to reconstruct the full social graph. We therefore only focus on approaches based on text,
and metadata that accompanies each individual tweet, and leave the possibility of integrating social
network information to future work.

3. Key Questions and Geolocation Prediction Framework
Though various geolocation prediction approaches have been proposed and adapted for social media
data, some fundamental questions remain. In the rest of the paper, we address each of the these
questions in turn.
‚Ä¢ Given that text-based methods rely on salient words local to particular regions to disambiguate
geolocations, do ‚Äúlocation indicative words‚Äù improve the accuracy over using the full word
set?
‚Ä¢ Does a model trained on geotagged data generalise to non-geotagged data? What is the impact
of adding non-geotagged texts to the training and test data? Is there an inherent sub-domain
difference between geotagged and non-geotagged tweets given that geotagged tweets are primarily sent from mobile devices?
‚Ä¢ Does geolocation prediction accuracy vary by language? For example, is a user who primarily
tweets in Japanese more geolocatable than a user who tweets mostly in English? If language
457

H AN , C OOK & BALDWIN

does influence accuracy, how can we exploit this to improve multilingual geolocation prediction?
‚Ä¢ Does the user-declared text metadata provide geographical information complementary to
that in the tweets themselves? How can we make use of these multiple sources of textual data
to produce a more accurate geolocation predictor?
‚Ä¢ As Twitter is rapidly growing and evolving, how do temporal factors influence the model
generalisation? Will a model trained on ‚Äúold‚Äù data perform comparably on ‚Äúnew‚Äù test data?
‚Ä¢ From the perspective of privacy protection, how does a user‚Äôs tweeting behaviour affect their
geolocatability, i.e., the ability of the model to predict their location? Are there steps a user
can take to reduce the risk of inadvertently leaking geographical information while sharing
tweets with the public?
‚Ä¢ Can measures of prediction confidence be formulated to estimate the accuracy of the geolocation prediction?
In this paper, we focus on predicting Twitter users‚Äô primary (referred to as their ‚Äúhome‚Äù) geolocation, and following Cheng et al. (2010) and others, assume that a given user will be based in a
single city-based location throughout the time period of study. We approach geolocation prediction
as a text classification task. Tweets from each city are taken to represent a class. All tweets from
a given user are aggregated and assigned to that user‚Äôs primary location. We characterise geolocation prediction by four key components, which we discuss in turn below: (1) the representation of
different geolocations, (2) the model, (3) the feature set, and (4) the data.
3.1 Representation: Earth Grid vs. City
Geolocations can be captured as points, or clustered based on a grid (Wing & Baldridge, 2011;
Roller et al., 2012), city centres (Cheng et al., 2010; Kinsella et al., 2011) or topic regions (Eisenstein et al., 2010; Hong et al., 2012). A point-based representation presents computational challenges, and is too fine-grained for standard classification methods. As for dynamic location partitioning, the granularity of regions is hard to control and will potentially vary across time, and the
number of regions is a variable which will depend on the dataset and potentially also vary across
time. Fixed grid-based representations are hindered because there is considerable variability in the
shape and size of geographical regions: a coarse-grained grid cell is perhaps appropriate in central
Siberia, but for densely-populated and linguistically/culturally diverse regions such as Luxembourg,
doesn‚Äôt lead to a natural representation of the administrative, population-based or language boundaries in the region. We therefore opt for a city-based representation, which is able to capture these
boundaries more intuitively. The downside to this representation is that it is inappropriate for classifying users in rural areas. As we will see in Figure 1, however, the bulk of Twitter users are,
unsurprisingly, based in cities.
Following Han et al. (2012b), we use the publicly-available Geonames dataset as the basis for
our city-level classes.2 This dataset contains city-level metadata, including the full city name, population, latitude and longitude. Each city is associated with hierarchical regional information, such
as the state and country it is based in, so that London, GB, e.g., is distinguished from London, CA.
2. http://www.geonames.org, accessed on October 25th, 2012.

458

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

We hence use a city-region-country format to represent each city (e.g., Toronto, CA is represented
as toronto-08-ca, where 08 signifies the province of Ontario and ca signifies Canada).3 Because
region coding schemes vary across countries, we only employ the first- and second-level region
fields in Geonames as the region. Furthermore, if the second-level field is too specific (i.e., longer
than 4 letters in our setting), we only incorporate the first-level region field (e.g., instead of using
melbourne-07-24600-au, we use melbourne-07-au). Moreover, because cities are sometimes
complex in structure (e.g., Boston, US colloquially refers to the metropolitan area rather than the
city, which is made up of cities including Boston, Revere and Chelsea), we collapse together cities
which are adjacent to one another within a single administrative region, as follows:
1. Identify all cities which share the same region code (i.e., are located in the same state,
province, county, etc.) in the Geonames dataset.
2. For each region, find the city c with the highest population.
3. Collapse all cities within 50km of c into c.4
4. Select the next-largest city c, and repeat.
5. Remove all cities with a population of less than 100K. The remaining cities form our citybased representation of geolocations.
As a result of this methodology, Boston, US ends up as a single city (incorporating Revere and
Chelsea), but neighbouring Manchester, US is a discrete city (incorporating Bedford) because it is
in New Hampshire. This algorithm identifies a total of 3,709 collapsed cities throughout the world.
3.2 Geolocation Prediction Models
Various machine learning algorithms can be applied to the task of multi-class text categorisation.
However, many state-of-the-art learning algorithms are not appropriate for this particular task for
reasons of scalability. For example, support vector machines (Vapnik, 1995) are not well suited
to massively multi-class problems (i.e., 3,709 cities in our case). Finally, we would ideally like to
have a learning algorithm which can be easily retrained, e.g., to incorporate new training data from
the Twitter data stream. As such, we primarily experiment with simple learning algorithms and
ensemble learning for geolocation prediction.
3.2.1 G ENERATIVE VS . D ISCRIMINATIVE M ODELS
Generative models (e.g., naive Bayes) are based on estimation of joint probability of observing a
word vector and a class (i.e., P (w1 , w2 , . . . , wn , ci ), where w1 , w2 , . . . are words and ci ‚àà C is a
city from a combined set of cities C). In contrast, discriminative models are based on estimation of
a class given a word vector (i.e., P (c|w1 , w2 , . . . , wn )). The objective of both models is to find a
3. Country code information can be found in http://download.geonames.org/export/dump/countryInfo.txt
4. We use the great-circle distance (Vincenty, 1975) for all distance calculations in our experiments, as opposed to
Euclidean distance, to properly capture the three-dimensional surface of the earth. The proximity of cities varies
across the world, e.g., cities on the east coast of the United States are much closer to each other than major cities in
Australia. There is therefore scope to explore the impact of this 50km setting on the city label set, which we leave to
future work.

459

H AN , C OOK & BALDWIN

city cmax ‚àà C such that the relevant probability is maximised. In our experiments, we experiment
with both models. For instance, we choose a state-of-the-art discriminative geolocation model based
on KL divergence over k-d tree partitioned unigrams (KL) (Roller et al., 2012). We also adopt a
generative multinomial naive Bayes (NB) model (Hecht et al., 2011) as our default benchmark, for
two reasons: (1) it incorporates a class prior, allowing it to classify an instance in the absence of any
features shared with the training data; and (2) generative models outperform discriminative models
when training data is relatively scarce (Ng & Jordan, 2002).5
3.2.2 S INGLE VS . E NSEMBLE M ODELS
In addition to single model comparisons (e.g., discriminative KL vs. generative NB in Sections 5
and 6), we further combine multiple base classifiers ‚Äî e.g., heterogeneous NB models trained
on each of Twitter text and user metadata ‚Äî to improve the accuracy. First, we investigate the
accuracies of base classifiers and correlations between them. Then, we apply different ensemble
learning strategies in Section 9.
3.3 Feature Set
Predominantly, geolocations are inferred based on geographical references in the text, e.g., place
names, local topics or dialectal words. However, these references are often buried in noisy tweet
text, in which lexical variants (e.g., tmrw for ‚Äútomorrow‚Äù) and common words without any geospatial dimension (e.g., weather, twitter) are prevalent. These noisy words have the potential to mislead
the model and also slow down the processing speed. To tackle this issue, we perform feature selection to identify ‚Äúlocation indicative words‚Äù. Rather than engineering new features or attempting
to capture named entities (e.g., the White House) or higher-order n-grams, we focus on feature selection over simple word unigrams (see Section 4). This is partly a pragmatic consideration, in that
unigram tokenisation is simpler.6 Partly, however, it is for comparability with past work, in determining whether a strategically-selected subset of words can lead to significant gains in prediction
accuracy (see Sections 5 and 6).
In addition to feature selection, the feature set can be further refined and extended in various
ways. For instance, feature selection can be enhanced by incorporating non-geotagged tweet data.
Furthermore, languages can be used to shape the feature set, as words from different languages
carry varying amounts of geospatial information, e.g., because Dutch is primarily used only in the
Netherlands, Dutch words are usually more location indicative than English words. Moreover, userprovided metadata (e.g., location and timezone) is readily accessible in the tweet JSON objects.
This metadata can be appended as extra text features, in addition to features derived from tweet text.
We investigate the impact of these factors in later sections.

5. There is certainly an abundance of Twitter data to train models over, but the number of Twitter users with sufficient amounts of geotagged tweets to be able to perform geolocation prediction is small, relative to the number of
parameters in the model (the product of the number of features and classes).
6. Also, preliminary results with both named entities and higher order n-grams were disappointing.

460

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

Proportion of tweets
(relative to preceding step)

Filtering criterion
Geotagged
Near a city
Non-duplicate and non-Foursquare
English

0.008
0.921
0.888
0.513

Table 1: Proportion of tweets remaining after filtering the data based on a series of cascaded criteria.
These numbers are based on a Twitter corpus collected over two months.

3.4 Data
Geolocation prediction models have primarily been trained and tested on geotagged data.7 We use
both regional datasets (i.e., geotagged tweets collected from the continental US: Eisenstein et al.,
2010; Mahmud et al., 2012) and global datasets (Kinsella et al., 2011; Han et al., 2012b) in this
research. Because of accessibility issues (e.g., many tweets in older datasets have been deleted and
are thus not accessible now) and data sparseness (e.g., there were only 10K users in the study of
Eisenstein et al., 2010), we are only able to experiment over a small number of public datasets. In
this paper, we employ three geotagged datasets:
1. A regional North American geolocation dataset from Roller et al. (2012) (NA hereafter), for
benchmarking purposes. NA contains 500K users (38M tweets) from a total of 378 of our
pre-defined cities. NA is used as-is to ensure comparability with previous work in Section 5.
2. A dataset with global coverage constructed by us in earlier work (Han et al., 2012b) (WORLD
hereafter), collected via the Twitter public Streaming API8 from 21 Sep, 2011 to 29 Feb,
2012. The tweet collection is further shaped for different evaluation tasks, e.g., geotagged
English data WORLD in Section 6, incorporating non-geotagged English data WORLD+NG
in Section 7, multilingual geotagged data WORLD+ML in Section 8 and with rich metadata
WORLD+META in Section 9.
3. A second dataset with global coverage novel to this research (LIVE), which contains tweets
collected more than 1 year after WORLD (from 3 Mar, 2013 to 3 May, 2013), to analyse the
influence of temporal recency on geolocation prediction. Unlike the other two datasets, LIVE
is used only as a test dataset, in Section 10.
WORLD was restricted to English tweets in order to create a dataset similar to NA (in which
English is the predominant language), but covering the entire world. It was pre-processed by filtering the data as follows. First, all non-geotagged tweets were removed. Next, we eliminated all
tweets that aren‚Äôt close to a city by dividing the earth into 0.5‚ó¶ √ó 0.5‚ó¶ grid cells, and discarding
any tweet for which no city in our Geonames class set is found in any of the 8 neighbouring grid
cells. We then assign each user to the single city in which the majority of their tweets occur. We
7. One exception to this is Cheng et al. (2010), who train on users whose user-declared metadata location fields correspond to canonical locations (e.g., Boston, MA), and test on users whose locations are indicated with GPS coordinates
in their metadata.
8. https://dev.twitter.com/docs/streaming-apis

461

Cumulative distribution of tweets

H AN , C OOK & BALDWIN

0.9

0.8

0.7

5%

15% 25% 35% 45% 55% 65% 75% 85% 95%
Top N% of cities

1

1

The number of users
10 100 1000

The number of users
100
10000

Figure 1: Cumulative coverage of tweets for increasing numbers of cities based on 26 million geotagged tweets.

1

10
100
1000
10000
Number of geo‚àítagged tweets

1
5
50
500
5000
Mean distance from city centre (kilometres)

Figure 2: The number of users with different numbers of tweets, and different mean distances from
the city center, for WORLD.

further remove cities with fewer than 50 feature types (i.e., word types) to reduce data sparsity. This
results in 3135 cities in WORLD (as opposed to 3709 cities in the full Geonames class set). We
eliminated exact duplicate tweets and Foursquare check-ins (which encode the user location in the
form of I‚Äôm at . . . ). After that, non-English tweets were further removed using langid.py, an
open-source language identification tool (Lui & Baldwin, 2012). This filtering is summarised in
Table 1 which also shows the proportion of tweets remaining after each step. The total number of
users and tweets in WORLD is 1.4M and 12M, respectively. Similar to NA, the development and
test datasets both contain 10K users, and the remainder of the users are used in training. The development and test data was sampled such that each user has at least 10 geotagged tweets to alleviate
data sparsity.9 We tokenised the tweets with a Twitter-specific tokeniser (adapted from O‚ÄôConnor,
Krieger, & Ahn, 2010).
Although there are certainly instances of social media users with high mobility (Li, Wang, &
Chang, 2012), recent studies have shown that most users tend to tweet from within a limited region
(Cho, Myers, & Leskovec, 2011; Hecht et al., 2011). We also analyse the spread of WORLD in
9. This restriction was not applied to the training data.

462

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

Figure 2, in terms of: (1) the number of users with at least 10 geotagged tweets; and (2) the number
of users with differing levels of geographical spread in their tweets, measured as the average distance
between each of a user‚Äôs tweets and the centre of the city to which that user is allocated.10 This
preliminary analysis shows that most users have a relatively small number of geotagged tweets, and
most users stay near a single city (e.g., 83% users have a geographical spread of 50 kilometres or
less). The high proportion of users with an average distance of 1km to the city centre is an artefact
of their geotagged tweets being mapped to a city centre before performing this analysis. In order to
investigate the coverage of the proposed city-based partition, we examine the recall in our original
sample of 26 million geotagged tweets (prior to filtering, as described above). The analysis reveals
that 92.1% of tweets are ‚Äúclose‚Äù to (in a neighbouring 0.5‚ó¶ √ó0.5‚ó¶ grid cell) to one of our pre-defined
cities, and that the top 40% of cities contain 90% of the geotagged tweets after filtering, as shown
in Figure 1. This supports our assumption that most (geotagged) Twitter users are based in cities.
3.5 Evaluation Measures
Having formulated the geolocation prediction task into a discrete class space through the use of our
city class set, it is possible to use simple classification accuracy to evaluate our models. However,
given that all of our class labels have a location (in the form of latitude‚Äìlongitude coordinates),
we can also sensitise the evaluation to distance-based predictive error. For instance, if the correct
location for a user is Seattle, US, a prediction of Portland, US is arguably better than a prediction
of Los Angeles, US, on the basis of geospatial proximity. We use a number of evaluation measures
which capture spatial proximity, in line with previous work (Wing & Baldridge, 2011; Roller et al.,
2012):11
1. Acc: city-level accuracy, i.e., the proportion of predictions that correspond to the correct city;
2. Acc@161: the proportion of predictions that are within a distance of 161 kilometres (100
miles) from the correct city-level location. This empirical measure (Cheng et al., 2010) is a
relaxed version of Acc, capturing near-miss predictions.
3. Acc@C: country-level accuracy, i.e., the proportion of predicted locations that are in the same
country as their corresponding true locations. This measure is useful for applications relying
on country-specific Twitter data, e.g., sentiment analysis in specific countries.
4. Median: median prediction error, measured in kilometres between the predicted city centres
and the true geolocations. We prefer to use the median, as opposed to mean, distance because
the median is less sensitive to wildly incorrect predictions ‚Äî e.g., a user from London, GB
classified as being based in Sydney, AU. In contrast, the mean distance can increase substantially due to a small number of extreme misclassifications, although this effect is limited for
inherently-bounded regional datasets such as NA.
10. The geographical spread is calculated over a random sub-sample of 10 tweets for a given user, for efficiency reasons.
11. In very recent work, Priedhorsky, Culotta, and Valle (2014) additionally proposed a set of probabilistic metrics
to evaluate tweet-based geolocation prediction, including using the expected distance between a tweet‚Äôs true point
location to a random point location drawn from the probability distribution of the geolocation model. While we
strongly support this new direction for geolocation modelling and evaluation, depending on the application context,
we argue that point- or region-based representations and related discrete evaluation measures are equally important
in user geolocation research.

463

H AN , C OOK & BALDWIN

4. Finding Location Indicative Words
Precise user locations for individual messages are embedded in geotagged tweets in the form of
latitude‚Äìlongitude coordinates. By mapping these coordinates to cities and representing each tweet
as a bag of words, we are able to make connections between words (i.e., features) and cities (i.e.,
classes). In this section, we present a range of methods for ranking these words by their location
indicativeness, i.e., the degree to which a word is associated with particular cities. Words that either
explicitly (e.g., place names) or implicitly (e.g., dialectal words, slang or local references) encode
geographical information are collectively referred to as ‚Äúlocation indicative words‚Äù (LIWs); it is
these words that we aim to automatically identify. Examples of LIWs are:
1. local words (1-local) that are used primarily in a single city, namely yinz (used in Pittsburgh to
refer to the second-person plural pronoun), dippy (used in Pittsburgh to refer to a style of fried
egg, or something that can be dipped in coffee) and hoagie (used primarily in Philadelphia,
to refer to a kind of sandwich);12
2. semi-local words (n-local) that refer to some feature of a relatively limited subset of cities,
namely ferry (found, e.g., in Seattle, New York and Sydney), Chinatown (common in many
of the largest cities in the US, Canada and Australia, but much less common in European and
Asian cities), and tram (found, e.g., in Vienna, Melbourne and Prague)
In addition to LIWs there are common words (common) which aren‚Äôt expected to have substantial
regional frequency variation, namely twitter, iphone and today.
In the remainder of this section, we present various feature selection methods for identifying
LIWs, drawn from the work of Han et al. (2012b), Chang et al. (2012) and Laere et al. (2013). The
feature selection methods can be broadly categorised into three types: (1) statistical; (2) informationtheoretic; and (3) heuristic. To reduce low-utility words and noise, for all feature selection methods,
we remove all words which include non-alphabetic letters, are less than 3 letters long, or have a
word frequency < 10.
4.1 Statistical-Based Methods
Statistical hypothesis testing is often used to determine whether an event occurs by chance (i.e., the
null hypothesis) or not (i.e., the alternative hypothesis) at a particular confidence level (e.g., 95%
‚â° p < 0.05). In our case, an event is defined to be a co-occurrence between a word and a city, and
the null hypothesis assumes the co-occurrence is by chance, i.e., the word and city are independent.
The goal of feature selection is then to find word‚Äìcity pairs where the null hypothesis is rejected.
4.1.1 œá2

AND

L OG -L IKELIHOOD

The œá2 statistic is commonly used to examine the degree of independence between random variables. A contingency table representing the observations of the variables is formed, as in Table 2.
The general form of the statistic is:
n
‚àë
(Oi ‚àí Ei )2
i

Ei

12. These words were identified with the aid of datasets of regional words such as DARE: http://dare.wisc.edu/.

464

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

in c
Ow,c
OwÃÑ,c

w
non-w word

not in c
Ow,cÃÑ
OwÃÑ,cÃÑ

Table 2: Contingency table for word and city co-occurrence
where Oi represents an observation (i.e., co-occurrence of a city (c) and word (w)), and n is the
number of cells in the table. Ow,c and OwÃÑ,cÃÑ denote the occurrence of word w in city c and non-w
words in cities other than c, respectively. Ew,c denotes the expected frequency of w in c, calculated
from the marginal probabilities and total counts N :
Ow,c + Ow,cÃÑ Ow,c + OwÃÑ,c
√ó
√óN
N
N
= Ow,c + OwÃÑ,c + Ow,cÃÑ + OwÃÑ,cÃÑ

Ew,c = P (w) √ó P (c) √ó N =
N

If the œá2 statistic is larger than the number in the œá2 distribution, with respect to the degrees of
freedom (in this case, 1), then the null hypothesis that city c and word w are independent is rejected.
As with many statistical tests, œá2 can be ineffective when counts are low. We address this through
our word frequency thresholding and use of massive amounts of training data.
Conventionally, œá2 is used to identify the set of features which satisfies a pre-defined confidence
level (e.g., p < 0.05). However, in the case of LIW selection, we instead use the œá2 statistic to rank
all word‚Äìcity pairs. The selection of LIWs is deferred to the parameter tuning state, in which the
boundary between LIWs and common words is optimised using development data.
At this point, a different ranking of LIWs is produced per city, where what we desire is a global
ranking of LIWs capturing their ability to discriminate between cities in the combined label set.
There are various ways to do this aggregation. As suggested by Laere et al. (2013), one approach to
selecting n features based on œá2 is to iteratively aggregate the top-m features from each city until
n features are obtained. Alternatively, they can be ranked based on the highest-scoring occurrence
of a given word for any city, by first sorting all city‚Äìword œá2 test pairs, then selecting the first
occurrence of a word type for the aggregated ranking. These two aggregation approaches produce
different feature selection rankings, and are distinguished using Chi and MaxChi , respectively.13
Similar to the œá2 test, the log-likelihood ratio (‚ÄúLoglike‚Äù: Dunning, 1993) has also been applied
to LIW selection (Laere et al., 2013). The Loglike test determines whether h0 (the null hypothesis,
i.e., the word is independent of the city) is more likely than h1 (the alternative hypothesis, i.e., the
word is dependent on the city). Following Dunning, the likelihood of a hypothesis, L(¬∑), is estimated
using binomial distributions.
( )
( )
k1
k2
n1 ‚àík1 n1
n2 ‚àík2 n2
L(h1 ) = p1 (1 ‚àí p1 )
p (1 ‚àí p2 )
k1 2
k2
p1 = P (w|c) =

Ow,c
k1
=
n1
Ow,c + OwÃÑ,c

13. One possible alternative to computing œá2 for each word and city, and then aggregating these values into a final
ranking of words, would be to compute a single œá2 value for each word from a contingency table with 2 rows as in
Table 2, but with one column per city. Nevertheless, this is not the standard use of œá2 in feature selection, and we
leave this possibility to future work.

465

H AN , C OOK & BALDWIN

Ow,cÃÑ
k2
=
n2
Ow,cÃÑ + OwÃÑ,cÃÑ
k1 (k2 ) represents the occurrences of word w in city c (not in city c), and n1 (n2 ) represents all word
occurrences in city c (not in city c). L(h0 ) is a special case of L(h1 ) for which p1 and p2 are equal,
as below:
p2 = P (w|cÃÑ) =

Ow,c + Ow,cÃÑ
N
The Loglike test statistic is then expanded using observations:
p1 = p2 = p =

Loglike(w) = 2[Ow,c log Ow,c + OwÃÑ,c log OwÃÑ,c + Ow,cÃÑ log Ow,cÃÑ + OwÃÑ,cÃÑ log OwÃÑ,cÃÑ + N log N
‚àí (Ow,c + OwÃÑ,c ) log(Ow,c + OwÃÑ,c ) ‚àí (Ow,cÃÑ + OwÃÑ,cÃÑ ) log(Ow,cÃÑ + OwÃÑ,cÃÑ )
‚àí (OwÃÑ,c + OwÃÑ,cÃÑ ) log(OwÃÑ,c + OwÃÑ,cÃÑ ) ‚àí (Ow,c + Ow,cÃÑ ) log(Ow,c + Ow,cÃÑ )]
Having calculated the Loglike for each word‚Äìcity pair, we then aggregate across cities similarly to
Chi (by selecting the top-m features per city until n features are obtained), following Laere et al.
(2013).14
4.1.2 R IPLEY ‚Äô S K S TATISTIC
Spatial information can also be incorporated into the hypothesis testing. For example, the Ripley K
function (Ripley: O‚ÄôSullivan & Unwin, 2010) measures whether a given set of points is generated
from a homogeneous Poisson distribution. The test statistic calculates the number of point pairs
within a given distance Œª over the square of the total number of points. With regards to LIW
selection, the set of points (Qw ) is the subset of geotagged users using a particular word w. The test
statistic is formulated as follows (Laere, Quinn, Schockaert, & Dhoedt, 2014):
K(Œª) = A √ó

|{p, q ‚àà Qw : distance(p, q) ‚â§ Œª}|
|Qw |2

where A represents the total area under consideration (e.g., the whole of North America, or the
whole globe); this is dropped when generating a ranking.
A larger value of K(Œª) indicates greater geographical compactness of the set Qw (i.e., p and q
are spatially close). However, |Qw | (i.e., the number of users who use word w) varies considerably
across words, and can dominate the overall statistic. A number of variations have been proposed
to alleviate this effect, including replacing the denominator with a factor based on L1, and taking
the logarithm of the overall value (Laere et al., 2014). The quadratic computational complexity of
Ripley becomes an issue when |Qw | is large (i.e., for common words). Randomised methods are
usually adopted to tackle this issue, e.g., subsampling points from training data for Ripley calculation relative to different distances Œª. For our experiments, we adopt the optimised implementation
of Laere et al. using Œª = 100km with 5K samples.
4.2 Information Theory-Based Methods
In addition to statistical methods, we also experiment with information-theoretic feature selection
methods based on measures which have been shown to be effective in text classification tasks, e.g.,
Information Gain (IG) (Yang & Pedersen, 1997).
14. Note also that, as we will see later in our experiments, there is almost no empirical difference between the two
aggregation methods for œá2 , so the choice of aggregation method here is largely arbitrary.

466

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

4.2.1 I NFORMATION G AIN AND G AIN R ATIO
Information Gain (IG) measures the decrease in class entropy a word brings about, where higher
values indicate greater predictability on the basis of that feature. Given a set of words w, the IG of
a word w ‚àà w across all cities (c) is calculated as follows:
IG(w) = H(c) ‚àí H(c|w)
‚àù ‚àíH(c|w)
‚àë
‚àë
‚àù P (w)
P (c|w)logP (c|w) + P (wÃÑ)
P (c|wÃÑ)logP (c|wÃÑ)
c‚ààc

c‚ààc

where P (w) and P (wÃÑ) represent the probabilities of the presence and absence of word w, respectively. Because H(c) is the same for all words, only H(c|w) ‚Äî the conditional entropy given w ‚Äî
needs to be calculated to rank the features.
Words carry varying amounts of ‚Äúintrinsic entropy‚Äù, which is defined as:
IV (w) = ‚àíP (w)logP (w) ‚àí P (wÃÑ)logP (wÃÑ)
Local words occurring in a small number of cities often have a low intrinsic entropy, where nonlocal common words have a high intrinsic entropy (akin to inverse city frequency; see Section 4.3.1).
For words with comparable IG values, words with smaller intrinsic entropies should be preferred.
Therefore, following Quinlan (1993) we further normalise IG(w) using the intrinsic entropy of
word w, IV (w), culminating in information gain ratio (IGR):
IGR(w) =

IG(w)
IV (w)

4.2.2 L OGISTIC R EGRESSION -BASED F EATURE W EIGHTS
The previous two information-theoretic feature selection methods (IG and IGR) optimise across all
classes simultaneously. Given that some LIWs may be strongly associated with certain locations,
but are less tied to other locations, we also conduct per-class feature selection based on logistic
regression (LR) modelling.15 We consider this method to be information theoretic because of its
maximisation of entropy in cases where there is uncertainty in the training data.
Given a collection of cities c, the LR model calculates the probability of a user (e.g., represented
by word sequence: w1 , w2 , . . . , wn ) assigned to a city c ‚àà c by linearly combining eligible LR
feature weights:
P (c|w1 , w2 , . . . , wn ) =

m
‚àë
1
exp(
Œªk fk )
Z
k=1

where Z is the normalisation factor, m is the total number of features, and fk and Œªk are the features
and feature weights, respectively. As with other discriminative models, it is possible to incorporate
arbitrary features into LR, however, a feature (function) in our task is canonically defined as a word
wi and a city c: when w occurs in the set of messages for users in class c, a feature fk (wi , c) is
15. For the logistic regression modeller, we use the toolkit of Zhang Le (https://github.com/lzhang10/maxent),
with 30 iterations of L-BFGS (Nocedal, 1980) over the training data.

467

H AN , C OOK & BALDWIN

denoted as [class = c ‚àß wi ‚àà c]. Each fk maps to a feature weight denoted as Œªk ‚àà R. The method
results in a per-city word ranking with words ranked in decreasing order of Œªk , from which we
derive a combined feature ranking in the same manner as MaxChi , following Han et al. (2012b).16
Notably, incorporating a regularisation factor balances model fitness and complexity, and could
potentially achieve better results. We don‚Äôt explicitly perform regularisation in the modelling stage.
Instead, we first obtain the feature list ranked by LR as other feature selection methods and then
evaluate the subset of top-n ranked features on the development data. This is in fact equivalent to
‚Äúfilter-based‚Äù regularisation (cf. filter-based feature selection: Guyon & Elisseeff, 2003), and we
leave experimentation with regularisation integrated into the models to future work.
4.2.3 D ISTRIBUTION D IFFERENCE
LIW selection can be likened to finding words that are maximally dissimilar to stop words (Chang
et al., 2012). Stop words like the and today are widely used across many cities, and thus exhibit
a relatively flat distribution. In contrast, LIWs are predominantly used in particular areas, and
are more skewed in distribution. To capture this intuition, LIW selection is then based on the
‚Äúdistribution difference‚Äù across cities between stop words and potential LIW candidates (i.e., all
non-stop words). Given a pre-defined set of stop words S, the distribution difference is calculated
as:
DistDiÔ¨Ä (wns ) =

‚àë

DiÔ¨Ä (wns , ws )

ws ‚ààS

Count(ws )
Count(S)

where Count(ws ) and Count(S) denote the number of occurrences of a stop word ws and the total
number of occurrences of all stop words, respectively. The difference (i.e., DiÔ¨Ä (wns , ws )) between
a stop word ws and non-stop word wns can be evaluated in various ways, e.g., symmetric KLdivergence (DistDiÔ¨Äskl ), or the total variance (DistDiÔ¨Ätv ) of absolute probability difference across
all cities c (Chang et al., 2012):
DiÔ¨Äskl (wns , ws ) =

‚àë

P (c|wns ) log

c‚ààc

DiÔ¨Ätv (wns , ws ) =

‚àë

P (c|ws )
P (c|wns )
+ P (c|ws ) log
P (c|ws )
P (c|wns )

|P (c|wns ) ‚àí P (c|ws )|

c‚ààc

where P (c|wns ) and P (c|ws ) denote the probability of a word occurring in a city in the per-word
city distribution for wns and ws , respectively. The non-stop words are then sorted by distribution
difference in decreasing order. In our experiments, we use the implementation of Chang et al..
4.3 Heuristic-Based Methods
Other than commonly-used feature selection methods, a number of heuristics can be used to select
LIWs.
4.3.1 D ECOUPLING C ITY F REQUENCY AND W ORD F REQUENCY
High-utility LIWs should have both of the following properties:
16. As with LogLike, the choice of aggregation method here is largely arbitrary, based on our empirical results for œá2 .

468

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

1. High Term Frequency (TF ): there should be a reasonable expectation of observing it from
the users‚Äô tweets in a city.
2. High Inverse City Frequency (ICF ): the word should occur in tweets associated with a relatively small number of cities.
We calculate the ICF of a word w simply as:
icf w =

|c|
cf w

where c is the set of cities and cf w is the number of cities with users who use w in the training data.
Combining the two together, we are seeking words with high TF -ICF , analogous to seeking words
with high TF -IDF values in information retrieval. In standard TF -IDF formulations, we multiply
TF and IDF . A simple product of TF and ICF tends to be dominated by the TF component,
however: for example, twitter scores as highly as Jakarta, because twitter has a very high TF . We
resolve this by decoupling the two factors and applying a radix sort ranking: we first rank features
by ICF then by TF , in decreasing order. As this approach is largely based on the inverse city
frequency, we denote it as ICF below.
4.3.2 G EOGRAPHICAL S PREAD AND D ENSITY
LIWs have ‚Äúpeaky‚Äù geographical distributions (Cheng et al., 2010). In this section, we discuss two
heuristic measures for LIW selection which are based on the geographical distribution of the word.
Geographical spread (GeoSpread : Laere et al., 2013) estimates the flatness of a word‚Äôs distribution over cities. First, the earth is divided into 1‚ó¶ latitude by 1‚ó¶ longitude cells. For each word
w, the cells in which w occurs are stored. Then, all neighbouring cells containing w are merged
by multi-pass scanning until no more cells can be merged. The number of cells containing w after
merging is further stored. Finally, the GeoSpread score for the word w is calculated as follows:
GeoSpread (w) =

# of cells containing w after merging
Max (w)

where Max (w) represents the maximum frequency of w in any of the original unmerged cells.
Smaller values indicate greater location indicativeness. This measure was originally used to rank
Flickr tags by locality, e.g., London is more location-indicative than beautiful. It ignores the influence of stop words, as they are not common in Flickr tags. However, stop words like the are
frequent in Twitter, and occur in many locations, making the numerator small and denominator
large. Furthermore, stop word frequencies in cells are usually high. Consequently, the has a similarly small GeoSpread to London, which is undesirable. In other words, GeoSpread is flawed in
not being able to distinguish stop words from local words, although it can be effective at ranking
less common words (e.g., London vs. beautiful).
Geographical density (GeoDen: Chang et al., 2012) strategically selects peaky words occurring
in dense areas. Given a subset of cities c‚Ä≤ ‚äÜ c where word w ‚àà w is used, the GeoDen is calculated
469

H AN , C OOK & BALDWIN

as:

‚àë

‚Ä≤
‚àë c‚ààc

GeoDen(w) =
|c‚Ä≤ |2
|c‚Ä≤ |

cj ,ck ‚ààc‚Ä≤ jÃ∏=k dist(cj ,ck )
|c‚Ä≤ |(|c‚Ä≤ |‚àí1)

‚àë

‚àë

=

P (c|w)

c‚ààc‚Ä≤

P (c|w)

cj ,ck ‚ààc‚Ä≤ jÃ∏=k dist(cj ,ck )
|c‚Ä≤ |‚àí1

where dist(cj , ck ) is the great-circle distance between cities cj and ck . Similarly, P (c|w) denotes
the distribution of word w across each city c ‚àà c‚Ä≤ . The denominator is made up of the square
of the number of cities |c‚Ä≤ | that w occurs in (which has a similar effect to ICF above), and the
average distance between all cities where w is used. LIWs generally have a skewed geographical
distribution in a small number of locations, meaning that the denominator is small and the numerator
is large. The issue with this measure is the computational complexity for common words that occur
in many cities. Furthermore, cities containing a small number of occurrences of w should not be
incorporated, to avoid systematic noise, e.g., from travellers posting during a trip. One approach to
counter these issues is to set a minimum P (c|w) threshold for cities, and further perform randomised
sampling from c‚Ä≤ . In this paper, we follow Chang et al. in constructing the final c‚Ä≤ : first, all cities
containing w are ranked by P (c|w) in decreasing order, then c‚Ä≤ is formed by adding cities according
to rank, stopping when the sum of P (c|w) exceeds a pre-defined threshold r. We choose r = 0.1 in
our experiments, based on the findings of Chang et al..

5. Benchmarking Experiments on NA
In this section, we compare and discuss the proposed feature selection methods. In particular,
we investigate whether using only LIWs for geolocation prediction is better than using the full
set of features, under various configurations of models and location partitions in Section 5.2. The
subsequent experiments in this section are exclusively based on the public NA dataset. We adopt the
same user partitions for training, dev and test as was used in the original paper (Roller et al., 2012).
We primarily use the city-based class representation in our experiments over NA, but additionally
present results using the original k-d tree partitions learned by Roller et al. in Section 5.2, for direct
comparability with their published results. For the distance-based evaluation measures (Acc@161
and Median), we calculate the user‚Äôs location based on the centroid of their tweets, and, depending
on the class representation used, represent the predicted location as either: (a) a city centre; or (b)
the user-centroid for a given k-d tree cell. In the case of Acc for the city-based class representation,
we map the centroid for each user to the nearest city centre ‚â§ 50km away, and use this as the basis
of the Acc calculation. In the case that there is no city centre that satisfies this constraint,17 we map
the user to the NULL class, and will always misclassify the user.18
5.1 Comparison of Feature Selection Methods
First, we compare the effectiveness of the various feature selection methods on NA using the citybased class representation. In total, 214K features were extracted from the training section of NA.
17. This occurs for 1139 (‚âà 11.4%) of test users.
18. As such, the upper bound Acc for the city-based representation is 0.886. Note also that the Acc for the k-d tree vs.
city-based representation is not comparable, because of the different class structure and granularity.

470

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

0.4

Acc161

0.3

0.2
ICF
GeoDen
Ripley
IGR
Loglike
Chi

0.1

0.0
2% 8%

16%

24%

32%

40%

48%

56%

64%

72%

80%

88%

96%

Top N% of Features

Figure 3: Acc@161 for varying levels of feature selection on the NA dataset, based on the citybased class representation.

We select the top-n% of features, with a step size of 2%, and then use the selected features within
a multinomial naive Bayes learner (we return to explore the choice of learner in Section 5.2). The
tuning of n for all methods is based on Acc@161 over the 10K held-out users in the development
data. We present results for a sample of feature selection methods in Figure 3, omitting methods
which are largely identical in behaviour to other methods presented in the graph, namely:
‚Ä¢ {DistDiÔ¨Ätv , DistDiÔ¨Äskl } ‚â° ICF
‚Ä¢ MaxChi ‚â° Chi
‚Ä¢ {LR, IG, GeoSpread } ‚â° LogLike
For all methods, the best result is achieved with a proper subset of features based on feature
selection, although the proportion of the features that gives the best results for a given method
varies greatly (e.g., the optima for Ripley, IGR and GeoDen are 10%, 88% and 66%, respectively).
This observation agrees with the expectations that: (1) when only a small number of features is
used, the trained model generally underfits the data; and (2) if the model is trained using the full
feature set, noisy words (e.g., the) cause overfitting. For instance, when using just the top 2% of
features in IGR, the most likely class for users with features ‚Äî noting that users with no feature
representation will default to the majority class, namely Los Angeles, US-CA ‚Äî is Monterrey, MX,
because Spanish words are highly location-indicative of the small number of Mexican cities in the
NA dataset. The features which are selected last are generally high-frequency function words (e.g.,
the) and common words (e.g., facebook), which give little indication as to geolocation, and lead to
prediction errors.
Two patterns can be observed in the results: (1) Chi , MaxChi , IG, LogLike, GeoSpread , LR
and Ripley (i.e., ‚Äúlocal‚Äù methods, which initially select features for each class, with the exception
471

H AN , C OOK & BALDWIN

of IG and Ripley) achieve their highest Acc@161 at an early stage, then the numbers drop gradually; and (2) ICF , IGR, DistDiÔ¨Äskl , DistDiÔ¨Ätv and GeoDen (i.e., the ‚Äúcollective‚Äù group, which
select features for all classes at once) gradually increase in accuracy as more features are added,
reach a peak when the majority of features are selected, then drop off in accuracy sharply. This
difference in behaviour can be attributed to the types of word that are preferred by the methods.
The ‚Äúlocal‚Äù methods tend to prefer 1-local words ‚Äî taking LR, for example, city names (e.g.,
philadelphia) and names of upper-level administrative regions (e.g., georgia) frequently occur in
the upper reaches of the ranking. In addition to these gazetted words, many local/regional words are
also found in the upper reaches of the feature ranking, including informal place names (e.g., philly,
an informal name for Philadelphia, US-PA), local transport references (e.g., skytrain, a public transport system in Vancouver, CA) and local greetings (e.g., aloha in Honolulu, US-HI). However, it
is reasonable to believe that 1-local words ‚Äî words that are predominantly used in one city and
are rarely mentioned in other cities ‚Äî are not common. As a result, the accuracy is bounded by
the limited number of true 1-local words. This could be the reason for the early, yet remarkably
high, peak in accuracy, and subsequent sharp decline, for Ripley; because of its reliance on pairwise
distances between users using a given word, Ripley tends to rank 1-local words highly. In contrast,
the ‚Äúcollective‚Äù methods assume words carry varying amounts of geospatial information. By leveraging combinations of LIWs, the true location of a user can be collectively inferred. For instance,
brunswick is a common suburb/street name in many cities, e.g., Melbourne, AU and London, GB.
This word alone is insufficient to make reliable predictions. However, if other LIWs (e.g., tram
and Flinders, which are again not uniquely disambiguating in themselves) are also observed, then
the chance of the location being Melbourne, AU becomes high, since it is unlikely that users from
cities other than Melbourne, AU would use that combination of words. This strategy can also be
explained in information-theoretic terms: by knowing more words, extra information is obtained,
and consequently the entropy is continuously reduced and the prediction of geolocation becomes
more certain.
Among all the feature selection methods, IGR, GeoDen and Ripley are the stand-out methods
in terms of Acc@161. We further compare the accuracy of classifiers trained using the optimised
set of LIWs (based on the development data) to that of the full model. The performance is measured
on the 10K held-out test users, using the city-based class representation. The results are displayed
in Table 3 (for the same subset of feature selection methods as were displayed in Figure 3), and
show that using LIWs offers an improvement over the full feature set for all evaluation measures
and all feature selection methods, except for slight dips in Acc@C for IGR and GeoDen. Nevertheless, these numbers clearly demonstrate that feature selection can improve text-based geolocation
prediction accuracy. IGR performs best in terms of accuracy, achieving 8.9% and 14.2% absolute
improvements in Acc and Acc@161, respectively, over the full feature set.
5.2 Comparison with Benchmarks
We further compare the best-performing method from Section 5.1 with a number of benchmarks
and baselines. We experiment with two class representations: (1) the city-based class representation based on Geonames; and (2) the k-d tree based partitioning of Roller et al. (2012), which
creates grid cells containing roughly even amounts of data of differing geographical sizes, such that
higher-population areas are represented with finer-grained grids.19 For both class representations,
19. Recent work (Schulz et al., 2013) also considers irregular-sized polygons, based on administrative regions like cities.

472

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

Dataset

Features

Acc

Acc@161

Acc@C

Median

NA

Full
ICF
Chi
IGR
LogLike
GeoDen
Ripley

0.171
0.209
0.233
0.260
0.191
0.258
0.236

0.308
0.359
0.402
0.450
0.343
0.445
0.432

0.831
0.840
0.850
0.811
0.836
0.791
0.849

571
533
385
260
489
282
306

Table 3: Results on the full feature set compared to that for each of a representative sample of
feature selection methodologies on NA with the city-based class representation. The best
numbers are shown in boldface.

we compare learners with and without feature selection. As observed previously, Acc is not comparable across the two class representations. Results based on the distance-based measures (Acc@161
and Median), on the other hand, are directly comparable. Acc@C results are not presented for the kd tree based class representation because the k-d tree cells do not map cleanly onto national borders;
although we could certainly take the country in which the centroid of a given k-d tree cell lies as the
country label for the entire cell, such an approach would ignore known geo-political boundaries.
We consider the following methods:
Baseline: Because the geographical distribution of tweets is skewed towards higher-population
areas (as indicated in Figure 1), we consider a most-frequent class baseline. We assign all
users to the coordinates of the most-common city centre (or k-d tree grid centroid) in the
training data.
Placemaker: Following Kinsella et al. (2011), we obtain results from Yahoo! Placemaker,20 a
publicly-available geolocation service. The first 50K bytes (the maximum query length allowed by Placemaker) from the tweets for each user are passed to Placemaker as queries.
The returned city centre predictions are mapped to our collapsed city representations. For
queries without results, or with a predicted location outside North America, we back off to
the most-frequent class baseline.21
Multinomial naive Bayes: This is the same model as was used in Section 5.1.
KL divergence: The previous best results over NA were achieved using KL divergence and a k-d
tree grid (Roller et al., 2012). Using a k-d tree, the earth‚Äôs surface is partitioned into nearrectangular polygons which vary in size, but contain approximately the same number of users.
Locations are represented as cells in this grid. KL divergence is then utilised to measure the
similarity between the distribution of words in a user‚Äôs aggregated tweets and that in each grid
cell, with the predicted location being the centroid of the most-similar grid cell.22
20. http://developer.yahoo.com/geo/placemaker/, accessed in August 2012.
21. An alternative would be to query Placemaker with each tweet, and then aggregate these predictions (e.g., by selecting
the majority location) to get a final user-level prediction. However, Kinsella et al. (2011) found the accuracy of such
an approach to be largely similar to that of the approach we use.
22. We use the same settings as Roller et al. (2012): a median-based k-d tree partition, with each partition containing
approximately 1050 users.

473

H AN , C OOK & BALDWIN

Partition

Method

City

Baseline
Placemaker
NB
NB+IGR
LR
LR+IGR

Acc

Acc@161

Acc@C

Median

0.003
0.049
0.171
0.260
0.129
0.229

0.062
0.150
0.308
0.450
0.232
0.406

0.947
0.525
0.831
0.811
0.756
0.842

3089
1857
571
260
878
369

Table 4: Geolocation performance using city-based partition on NA. Results using the optimised
feature set (+IGR) are also shown. The best-performing method for each evaluation measure and class representation is shown in boldface.

Partition

Method

k-d tree

Baseline
NB
NB+IGR
KL
KL+IGR

Acc

Acc@161

Acc@C

Median

0.003
0.122
0.153
0.117
0.161

0.118
0.367
0.432
0.344
0.437

‚Äì
‚Äì
‚Äì
‚Äì
‚Äì

1189
404
280
469
273

Table 5: Geolocation performance using k-d tree-based partition on NA. Results using the optimised feature set (+IGR) are also shown. The best-performing method for each evaluation
measure and class representation is shown in boldface.

Logistic regression: We also apply logistic regression from Section 4.2.2 as a learner. Instead of
modelling all the data, we use only the IGR-selected features from Section 5.1. While regularisation is commonly employed in logistic regression learners, we made a conscious choice
not to use it in our experiments as the implementation of the regulariser would differ across
learners and complicate the direct comparison of feature selection methods (i.e. it would be
difficult to tease apart the impact of the specific regulariser from the feature selection). Having said that, if the objective were to maximise the raw classifier accuracy ‚Äî as distinct from
exploring the impact of different features and feature selection methods on classification accuracy ‚Äî we would advocate the incorporation of a regulariser.
Instead of evaluating every possible combination of model, partition and feature set, we choose
representative combinations to test the extent to which LIWs improve accuracy. The results on the
city-based partition are shown in Table 4. We begin by considering the baseline results. The mostfrequent class for the city-based representation is Los Angeles, US-CA.23 Both the majority class
baseline and Placemaker perform well below multinomial naive Bayes (NB) and logistic regression
(LR), and have very high Median distances. Furthermore, when using the features selected in Section 5.1 (i.e., NB+IGR and LR+IGR), the performance is further improved by a large margin for
both models, demonstrating that identification of LIWs can improve text-based geolocation prediction. Finally, although LR performs poorly compared to NB, LR+IGR still improves substantially
23. New York is further divided into suburbs, such as manhattan-ny061-us, brooklyn-ny047-us, in Geonames. As
an artefact of this, these suburbs are not merged into a single city.

474

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

over LR. We plan to further explore the reasons for LR‚Äôs poor performance in future work. Overall,
NB+IGR performs best for the city-based representation in terms of Acc, Acc@161, and Median
distance.
Turning to the k-d tree-based partition in Table 5, we again observe the low performance of the
most-frequent class baseline (i.e., a grid cell near New York state). NB and KL ‚Äî representative
generative and discriminative models, respectively ‚Äî are evaluated using software provided by
Roller et al. (2012).24 Both approaches clearly outperform the baseline over the k-d tree class
representation. Furthermore, performance increases again when using the resultant feature set of
LIWs,25 demonstrating that for a variety of approaches, identification of LIWs can improve textbased geolocation.
Overall, compared to the previously-published results for the k-d tree based representation (KL),
IGR-based feature selection on the city-based partition achieves a 10.6% absolute improvement in
terms of Acc@161, and reduces the Median prediction error by 209km.
From the results on the k-d tree based representation, it is not clear which of KL or NB is better
for our task: in terms of Acc@161, NB outperforms KL, but KL+IGR outperforms NB+IGR. All
differences are small, however, suggesting that the two methods are largely indistinguishable for the
user geolocation task. As to the question of which class representation should be used for user geolocation, empirically, there seems to be little to separate the two, although further experimentation
may shed more light on this issue. The city-based approach is intuitive, and enables a convenient
country-level mapping for coarser-grained geolocation tasks. Furthermore, our observation from
Figure 1 suggests most Twitter users are from cities. We therefore use the city-based partition for
the remainder of this paper for consistency and ease of interpretation.
A spin-off benefit of feature selection is that it leads to more compact models, which are more
efficient in terms of computational processing and memory. Comparing the model based on LIWs
selected using IGR with the full model, we find that the prediction time is faster by a factor of
roughly five.

6. Experiments on WORLD
In addition to establishing comparisons on NA, we further evaluate the feature selection methods
on WORLD. This extends the evaluation from regional benchmarks to global geolocation performance. Similar to NA, for WORLD we reserve 10K random users for each of dev and test, and the
remainder of the users are used for training (preprocessed as described in Section 3.4). Here and in
all experiments over WORLD and related datasets, we base our evaluation on the city label set.
We apply the same tuning procedure as was used over NA to obtain the optimal feature set for
each feature selection method. We present results for a representative sample of the best-performing
methods in Figure 4. Once again, we omit methods that are largely identical in behaviour to other
methods, namely:
‚Ä¢ {DistDiÔ¨Ätv , DistDiÔ¨Äskl } ‚â° ICF
‚Ä¢ {MaxChi , Chi , LogLike, IG, GeoSpread } ‚â° LR
24. https://github.com/utcompling/textgrounder/wiki/RollerEtAl_EMNLP2012
25. Note that after LIWs are selected, a small proportion of users end up with no features. These users are not geolocatable
in the case of KL, a discriminative model. We turn off feature selection for such users, and backoff to the full feature
set, so that the number of test instances is consistent in all rows.

475

H AN , C OOK & BALDWIN

0.25

Acc161

0.20
0.15
0.10
ICF
GeoDen
Ripley
IGR
LR

0.05
0.00
2% 8%

16%

24%

32%

40%

48%

56%

64%

72%

80%

88%

96%

Top N% of Features

Figure 4: Acc@161 for varying percentages of features selected using representative feature selection methods on the WORLD dataset.

The biggest differences over Figure 3 are: (1) the œá2 -based methods converge in behaviour with
LR, LogLike and related methods; and (2) LR performs marginally better than LogLike, and is
thus the method we present in the graph.
Despite the difference in scope and data size, the overall trend over WORLD mirrors that for
NA. In particular, GeoDen, IGR and Ripley achieve the best Acc@161 numbers on the dev data,
although the numbers are lower than those achieved for NA in Figure 3. This is because WORLD
has fewer tweets per user than NA (as we only utilise geo-tagged data), and disambiguation at the
global level also makes it a more challenging task.
The results for multinomial naive Bayes with the chosen feature selection methods on WORLD
are shown in Table 6. Again GeoDen (62%), IGR (86%) and Ripley (20%) achieve the best
accuracy, although there is no clear winner: IGR achieves the best Acc and Ripley achieves the
best Acc@161. Nevertheless, the improved city-based Acc and Acc@161 numbers confirm the
general effectiveness of feature selection. On the basis of these similar results and the earlier NA
results (in which IGR delivers better results), we adopt IGR as our default LIW feature selection
method for the remainder of the paper.
In summary, the findings on the utility of feature selection in Table 3 (NA) and Table 6 (WORLD)
tell a similar story, namely that feature selection improves user geolocation accuracy. The impact
of feature selection on NA is much greater than WORLD, because WORLD has a larger number
of classes and smaller average number of tweets per user and also per class, making it a more
challenging dataset.
476

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

Dataset

Features

Acc

Acc@161

Acc@C

Median

WORLD

Full
ICF
IGR
LR
GeoDen
Ripley

0.081
0.110
0.126
0.104
0.123
0.121

0.200
0.241
0.262
0.233
0.266
0.268

0.807
0.788
0.684
0.792
0.691
0.582

886
837
913
640
842
1128

Table 6: Results on the full feature set compared to that of each of a representative sample of feature
selection methodologies on WORLD using NB. The best numbers are shown in boldface.

Train
G
G+NG
G
G+NG

Test
G
G
G+NG
G+NG

Acc
0.126
0.170
0.187
0.280

Acc@161
0.262
0.323
0.366
0.492

Acc@C
0.684
0.733
0.835
0.878

Median
913
615
398
170

G
G+NG

NG
NG

0.161
0.241

0.331
0.440

0.790
0.826

516
272

G
G

G-small
NG-small

0.121
0.114

0.258
0.248

0.675
0.666

960
1057

Table 7: The results of geolocation models trained and tested on geotagged (G) and non-geotagged
(NG) tweets, and their combination.

7. Exploiting Non-geotagged Tweets
Most Twitter-based geolocation research carried out to date (Eisenstein et al., 2010; Wing & Baldridge, 2011) has been trained only on geotagged tweets, that is tweets with known geographical
coordinates. Some work (Roller et al., 2012) has also incorporated non-geotagged tweets from
users whose location can be inferred from geotagged tweets. Clearly, if it is possible to effectively
utilise non-geotagged tweets, data sparsity can be ameliorated (as we aren‚Äôt restricting ourselves to
training on only the approximately 1% of tweets with known location), but there is a clear tradeoff
in the confidence we can place in the labels associated with those tweets/users. In this section, we
investigate the utility of non-geotagged tweets in geolocation prediction.
For experiments in this section, and the rest of the paper, we use WORLD+NG to denote
the dataset which incorporates both the geotagged and non-geotagged tweets from the users in
WORLD. We refer to the subparts of this dataset consisting of geotagged and non-geotagged tweets
as G and NG, respectively. Of the 194M tweets in WORLD+NG, 12M are geotagged and the remaining 182M are non-geotagged. We use the same partitioning of users into training, development,
and testing sets for WORLD+NG as for WORLD. We compare the relative impact of NG in which
we train and test the geolocation method on G, NG, or their combination. Results are presented in
Table 7.
The first row of Table 7 shows the results using only geotagged data (our best result from Table
6). In rows two and three, we show results when the data for each user in the training and test
477

H AN , C OOK & BALDWIN

datasets, respectively, is expanded to incorporate non-geotagged data (without changing the set
of users or the label for any user in either case). In both cases, for all evaluation measures, the
performance is substantially better than the benchmark (i.e., the first row). This finding is in line
with Cheng et al.‚Äôs (2010) results that data spareness is a big issue for text-based geolocation. It also
validates our hypothesis that non-geotagged tweets are indicative of location. The best results are
achieved when non-geotagged tweets are incorporated in both the training and testing data (shown
in row four). In this case we achieve an accuracy of 28.0%, a 15.4 percentage point increase over
the benchmark using only geotagged tweets to represent a given user. Moreover, our prediction
is within 161km of the correct location for almost one in every two users, and the country-level
accuracy reaches almost 88%.26
Although research on text-based geolocation has used geotagged data for evaluation, the ultimate goal of this line of research is to be able to reliably predict the locations of users for whom the
location is not known, i.e., where there is only non-geotagged data. Because geotagged tweets are
typically sent via GPS-enabled devices such as smartphones, while non-geotagged tweets are sent
from a wider range of devices, there could be systematic differences in the content of geotagged
and non-geotagged tweets. We examine this issue in rows five and six of Table 7, where we test
our model on only non-geotagged data. In this case we know a test user‚Äôs gold-standard location
based on their geotagged tweets. However these geotagged tweets are not used to represent the user
in the test instance; instead, the user is represented only by their non-geotagged tweets. The results
here are actually better than for experiments with the same training data but tested on geotagged
tweets (i.e., rows one and two of the table).27 This confirms that a model trained on G or G+NG
indeed generalises to NG data. However, it is not clear whether this finding is due to there being
much more non-geotagged than geotagged data for a given user, or whether some property of the
non-geotagged data makes it easier to classify. To explore this question, we carry out the following additional experiment. First, we construct a new dataset NG-small by down-sampling NG to
contain the same number of features per user as G (in terms of the feature token count). To make
the comparison fairer, we constructed a second new dataset ‚Äî G-small ‚Äî in which we exclude
test users with more G tweets than NG tweets. This guarantees that users in NG-small will contain
the same number of LIWs as in G-small. We average over five iterations of random subsampling,
and list the result in the final row of Table 7.28 Here we see that the results for NG-small are not
as good as G-small (i.e., row seven), suggesting that there might be minor sub-domain differences
between geotagged and non-geotagged tweets, though a strong conclusion cannot be drawn without
further in-depth analysis. One possible explanation is that there could be differences (e.g., demographic variations) between users who only have non-geotagged tweets and users who have both
non-geotagged tweets and geotagged tweets; however, comparing these two sources is beyond the
scope of this paper. Nonetheless, the results suggest the difference between NG and G is largely due
to the abundant data in NG. This explanation is also supported by the recent work of Priedhorsky
et al. (2014).

26. Note that this evaluation is over exactly the same set of users in all four cases; all that changes is whether we
incorporate extra tweets for the pre-existing set of users, in the training or test data.
27. We remove users who only have geotagged tweets in the test data, reducing the number of users marginally from
10,000 to 9,767.
28. Note that we calculated the variance over the five iterations of random subsampling, and found it to be negligible for
all evaluation measures.

478

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

In summary, we have quantitatively demonstrated the impact of non-geotagged tweets on geolocation prediction, and verified that models trained on geotagged data are indeed applicable to
non-geotagged data, even though minor sub-domain differences appear to exist. We also established that representing a user by the combination of their geotagged and non-geotagged tweets
produces the best results.

8. Language Influence on Geolocation Prediction
Previous research on text-based geolocation has primarily focused on English data. Most studies
have either explicitly excluded non-English data, or have been based on datasets consisting of primarily English messages, e.g., through selection of tweets from predominantly English-speaking
regions (Eisenstein et al., 2010; Cheng et al., 2010; Wing & Baldridge, 2011; Roller et al., 2012).
However, Twitter is a multilingual medium and some languages might be powerful indicators of
location: for example, if a user posts mostly Japanese tweets, this could be a strong indication that
the user is based in Japan, which could be used to bias the class priors for the user. In this section,
we explore the influence of language on geolocation prediction. The predominant language in a
given tweet was identified using langid.py,29 which has been trained to recognise 97 languages
(Lui & Baldwin, 2012).
To create a dataset consisting of multilingual geotagged tweets, we extract all geotagged data
‚Äî regardless of language ‚Äî from the same Twitter crawl that WORLD was based on. This multilingual dataset consists of 23M tweets from 2.1M users. 12M tweets are in English as in WORLD,
while the remaining 11M tweets are in other languages. Figure 5 shows the proportion of tweets
in the fifteen most common languages in the dataset.30 An immediate observation is the large difference in language distribution we observe for geo-tagged tweets as compared to what has been
observed over all tweets (irrespective of geotag: Hong, Convertino, & Chi, 2011; Baldwin, Cook,
Lui, MacKinlay, & Wang, 2013): among the higher-density languages on Twitter, there appears to
be a weak positive bias towards English users geotagging their tweets, and a strong negative bias
against Japanese, Korean and German users geotagging their tweets. We can only speculate that the
negative bias is caused by stronger concerns/awareness of privacy issues in countries such as Japan,
South Korea, Germany and Austria. We explored the question of whether this bias was influenced
by the choice of Twitter client by looking at the distribution of Twitter clients used to post messages
in each of English, German, Japanese and Korean: (a) overall (irrespective of whether the message
is geotagged or not), based on a 1M sample of tweets from 28 Sep, 2011; and (b) for geotagged
tweets, based on WORLD. Overall, we found there to be huge variety in the choice of client used
within a given language (with the top-10 clients accounting for only 65‚Äì78% of posts, depending on
the language), and significant differences in popular clients between languages (e.g. ‚ÄúKeitai Web‚Äù
is the most popular client for Japanese, ‚Äúweb‚Äù for English and German, and ‚ÄúTwitter for Android‚Äù
for Korean). For geotagged tweets, on the other hand, there is much greater consistency, with the
three most popular clients for all languages being ‚ÄúTwitter for iOS‚Äù, ‚ÄúTwitter for Android‚Äù and
‚Äúfoursquare‚Äù, accounting for a relatively constant two-thirds of posts for each language. This is
suggestive of the fact that the choice of client is one factor in biasing the relative proportion of
29. Based on the simplifying assumptions that: (a) every tweet contains linguistic content; and (b) all tweets are monolingual, or at least are predominantly in a single language.
30. We represent languages in Figure 5 using two-letter ISO 639-1 codes.

479

H AN , C OOK & BALDWIN

0.3
0.2
0.1

Percentage

0.4

0.5

0.53

0.09

0.0

0.04 0.03 0.03

en

es

pt

ja

nl

0.02 0.02 0.02 0.02 0.01 0.01 0.01 0.01 0.01 0.01
id

it

ru

de

tr

fr

ms

th

ko

ar

Languages

Figure 5: The percentage of tweets in WORLD+ML written in each of the fifteen most frequent
languages in the collected Twitter data. These fifteen languages account for 88% of the
tweets in the full dataset.

geotagged tweets in the different languages, although more research is required to fully understand
this effect.
The training, development and test data is re-partitioned for the multilingual setting to stratify
on language, and the resultant dataset is referred to as WORLD+ML. Again, the development and
testing sets consist of 10K users each, with the remaining users in the training set as in WORLD.
Although in Section 7 we showed that adding non-geotagged data improves geolocation accuracy,
the experiments in this section are based only on geotagged data, because of the prohibitive computational cost of experimenting with a much larger dataset. Note that this doesn‚Äôt limit the generalisability of our results, it simply means that we have to be careful to compare them to the monolingual
results from Table 7 based on only geotagged tweets (the first row).
We first compare geolocation performance in a multilingual setting with that in an English-only
setting, a comparison that past work on geolocation has not considered. The data in WORLD+ML
is further partitioned into two subsets ‚Äî E and NE ‚Äî according to whether the majority of a given
user‚Äôs tweets are in English or non-English, respectively. Of the 10K test users in WORLD+ML,
5,916 are English and 4,084 are non-English. One challenge with the multilingual setting of these
experiments is tokenisation. Although rudimentary tokenisation of many languages such as English
and French can be accomplished using whitespace and punctuation, tokenisation is much more
challenging for languages such as Japanese and Chinese which do not represent word boundaries
480

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

Train

Test

E+NE
E+NE
E+NE
E

E+NE
E
NE
E

Acc

Acc@161

Acc@C

0.196
0.134
0.287
0.169

0.343
0.256
0.468
0.317

0.772
0.715
0.855
0.746

Median
466
1067
200
632

Table 8: Results for multilingual geolocation, training and testing on English (E) and non-English
(NE) users, and their combination.

with whitespace. However, amongst the most-common languages on Twitter (as shown in Figure
5), Japanese is the only language which accounts for a substantial portion of the data (> 1%)
and requires a specialised tokenisation strategy (compared to English). For Japanese tweets we
apply the Japanese morphological segmenter MeCab (with the IPA dictionary),31 and post-correct
tokenisation errors relating to Twitter-specific tokens such as mentions, hashtags, and URLs (e.g.,
in instances where MeCab over-segments a mention into multiple morphemes). For non-Japanese
tweets, we apply the same tokeniser based on regular expressions used in our previous English-only
experiments.
After resolving the tokenisation issue, we apply the same IGR method from Section 4.2.1 to
select the optimised feature selection cut-off, based on Acc over the development data. We observe
that a much larger proportion of tokens are selected in the multilingual setting compared to the
English-only experiments. For example, of the 400K token types in the multilingual experiment,
384K (the top 96%) are selected as location-indicative, while for the English-only case 83K (the top
86%) location-indicative words are selected from the total of 96K token types.
The experimental results are shown in Table 8.32 The first row gives results for training and
testing on the full dataset of both English and non-English tweets. The next two rows show the
results when testing on English (E) and non-English (NE) subsets of the data. The much lower
accuracy for E compared to NE indicates that English tweets are much more difficult to geolocate
than non-English tweets. One reason for this is that for many non-English languages, there is a
strong bias towards a small number of cities. We verify this by calculating the class entropy with
respective to a language on the training data. The class probabilities are smoothed using a simple
add-Œ± method, with Œ± = 1/3709 (where 3709 is the size of the class set). As shown in Table 9, the
class entropy on English (en) data is the largest, indicating that English is prevalent across a large
number of locations. In contrast, Thai (th) and Turkish (tr) have much smaller entropies, suggesting
the location distributions are heavily skewed, and user geolocation over these languages will be
easier than for English.
To explore the extent to which the geolocatability of a user varies with respect to the predominant language of their tweets, we further break down the results by language in Table 10, which
shows results for the top-10 most frequent languages (by number of tweets) with at least 100 users
in our test data. This cut-off on users ensures we do not consider under-represented languages.
31. http://sourceforge.net/projects/mecab/
32. The English-only results reported here are not the same as for the comparable experiment in Table 7 using only
geotagged data, because the test sets consist of different users in these two cases.

481

H AN , C OOK & BALDWIN

Language

Entropy

Language

Entropy

Language

Entropy

en
es
pt
ja
nl

6.279
5.069
4.144
3.523
3.820

id
it
ru
de
tr

3.868
5.244
3.772
6.207
2.888

fr
ms
th
ko
ar

5.538
3.970
2.697
2.781
3.281

Table 9: Geolocation class entropy for top-15 languages

Lang.
en
es
pt
id
nl
ja
ru
tr
ar
th
All

No.
5916
945
673
398
342
298
217
186
164
154

Per-language Majority Class

Unified Multilingual

Monolingual Partitioning

Acc Acc@161 Acc@C Med.

Acc Acc@161 Acc@C Med.

Acc Acc@161 Acc@C Med.

0.019
0.116
0.223
0.264
0.175
0.326
0.336
0.538
0.335
0.325

0.039
0.159
0.296
0.472
0.789
0.530
0.378
0.656
0.470
0.766

0.655 3671
0.324 4267
0.952 490
0.899 197
0.889
87
0.960
96
0.857 633
0.930
0
0.463 379
0.981
20

0.134
0.267
0.232
0.324
0.173
0.336
0.346
0.538
0.354
0.279

0.256
0.346
0.305
0.565
0.789
0.544
0.387
0.656
0.488
0.623

0.715 1067
0.734 391
0.952 490
0.965 115
0.889
87
0.956
95
0.862 633
0.930
0
0.500 301
0.792
41

0.169
0.362
0.400
0.440
0.298
0.463
0.341
0.522
0.457
0.325

0.317
0.478
0.489
0.736
0.871
0.695
0.378
0.645
0.591
0.766

0.746
0.802
0.961
0.960
0.845
0.950
0.862
0.930
0.750
0.974

632
185
200
16
58
27
633
0
21
30

10000 0.107

0.189

0.693 2805

0.196

0.343

0.772

0.255

0.425

0.802

302

466

Table 10: Geolocation performance and comparison for the top-10 most frequent languages in the
multilingual test data, using (1) language prior (i.e., the city where a language is mostly
used); (2) a unified multilingual model (i.e., training and testing on multilingual data
regardless of languages); and (3) language-partitioned monolingual models (i.e., first
identify the primary language of users, train one model per language, and classify test
users with the model corresponding to the language of their tweets)

We observe that the results vary remarkably by language in the multilingual section of Table
10. The results are overall lowest for English (en), although the lowest country-level accuracy is for
Arabic (ar); we speculate that this is caused by the large number of countries that Arabic is spoken
in, and the relatively small number of Arabic speakers in our training data. Furthermore, the citylevel accuracy is better than 30% for Indonesian (id), Japanese (ja), Russian (ru), Turkish (tr) and
Arabic (ar); the regions in which these languages are commonly-spoken are more geographicallyrestricted than for English, suggesting that geolocation accuracy on languages with smaller geographic footprints will tend to be higher than for languages which are widely-used throughout a
larger geographical area. This finding agrees with the recent work of Priedhorsky et al. (2014), and
further underlines the power of language information in predicting locations. The best city-level
accuracy of 53.8% is observed for Turkish (one of the languages with the lowest city-level entropy).
Manually inspecting the outputs, we find that this is because our model predicts the city Istanbul for
all Turkish users, and a large proportion of Turkish tweets come from this city.
482

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

Based on this finding, we further consider a language-based benchmark which predicts the most
frequent city given the predominant language of a user‚Äôs tweets (denoted as Per-language Majority
Class). We also observe the performance gap between the multilingual model on English (the second
row of Table 8) and an English-only model (the bottom row in Table 8). These results show that if
the target data is known to be written in a single language then a monolingual model outperforms
a multilingual one. It also suggests an alternative approach for multilingual geolocation prediction:
rather than training and predicting on multilingual data (E+NE), we can train and evaluate models
on language-specific data. Motivated by this observation, we also apply a monolingual partitioned
model for users of a particular language based on langid.py (i.e., language partitions), e.g.,
selecting all Japanese users in the training data, and only applying the Japanese-specific model to
Japanese users in the test data. This is denoted as Monolingual Partitioning in Table 10, and is
contrasted with the simple approach of a combined model for all languages and users (‚ÄúUnified
Multilingual‚Äù).
By comparing the Per-language Majority Class with the Unified Multilingual model, we find
that the unified model performs better overall, with the exception of Thai (th) and Dutch (nl), both
of which are associated with a very small number of cities, and one city which is much larger than
the others (Bangkok, TH and Amsterdam, NL, respectively). Because of the relatively poor results
for this benchmark method on languages such as English (en) and Spanish (es) which are frequent
on Twitter, and its relatively poor overall performance, the Per-language Majority Class is not an
appropriate method for this task. Nevertheless, when using a Monolingual Partitioning model, the
results are far superior, and the partitioning effect of language can be seen. This suggests that
modelling each language independently can improve geolocation performance.
In summary, this series of experiments has shown the influence of language on geolocation prediction. Among the top-10 languages found on Twitter, English is the most difficult to perform user
geolocation over, as English is the most global language. Despite language variance, multilingual
geolocation prediction is certainly feasible, although the best way to leverage language for geolocation prediction is by training language-partitioned monolingual models and geolocating users based
on their primary language.

9. Incorporating User Meta Data

The metadata accompanying tweets is a valuable source of geographical information beyond that
available in tweets. In this section, we explore incorporating metadata information into our textbased geolocation system. We begin by selecting four metadata fields that could potentially provide
insights into the location of a user, and first evaluate models trained on each of these sources of
information. We then consider a number of ways to incorporate information from this metadata
with our best text-based method developed in Section 7. As discussed in Section 8, language has a
strong influence on geolocation prediction, and English-posting users are the hardest to geolocate.
As such, we experiment only on English data (i.e., WORLD+NG) for the remainder of this paper.
483

H AN , C OOK & BALDWIN

Data
Training
Test

LOC

TZ

DESC

0.813
0.813

0.752
0.753

0.760
0.761

Table 11: The proportion of users with non-empty metadata fields in WORLD+NG
9.1 Unlock the Potential of User-Declared Metadata
We choose the following four user-supplied metadata fields for our study: location (LOC), timezone
(TZ), description (DESC), and the user‚Äôs real name (RNAME).33 In contrast to rich social network
information which is much more expensive to extract, these metadata fields are included in the
JSON object that is provided by the Twitter Streaming API, i.e., we can extract this metadata at no
extra crawling cost. This information, however, is dynamic, i.e., users can change their profiles,
including the metadata of interest to us. By aggregating the extracted tweet-level metadata for each
user, we can calculate the ratio of users that change each metadata field. 18% of users changed their
DESC field during the approximately five months over which our dataset was collected. During this
same time period, for each of the other fields considered, less than 8% of users updated their data.
Given the relatively small number of profile updates, we ignore the influence of these changes, and
use the most frequent value for each metadata field for each user in our experiments.
All of this user-supplied metadata can be imprecise or inaccurate, because the user is free to
enter whatever textual information they choose. For example, some LOC fields are not accurate
descriptions of geographical locations (e.g., The best place in the universe). Moreover, although
some LOC fields are canonical renderings of a user‚Äôs true location (e.g., Boston, MA, USA), a large
number of abbreviations and non-standard forms are also observed (e.g., MEL for Melbourne, AU).
Cheng et al. (2010) find that only a small proportion of location fields in their US-based dataset are
canonical locations (i.e., of the form city, state). Nevertheless, these non-standard and inaccurate
location fields might still carry information about location (Kinsella et al., 2011), similarly to how
the text of tweets can indicate location without explicitly mentioning place names.
These metadata fields also differ with respect to the explicitness of the location information they
encode. For instance, while LOC and TZ can give direct location information, DESC might contain
references to location, e.g., A geek and a Lisp developer in Bangalore. Although RNAME does not
directly encode location there are regional preferences for names (Bergsma, Dredze, Van Durme,
Wilson, & Yarowsky, 2013), e.g., Petrov might be more common in Russia, and the name Hasegawa
might be more common in Japan. Finally, for all of the tweets that we consider, the text field (i.e., the
content of the tweet itself) and RNAME are always present, but LOC, TZ, and DESC can be missing
if a user has chosen to not supply this information. The proportion of non-empty metadata fields for
LOC , TZ and DESC for users in WORLD+NG are listed in Table 11.
9.2 Results of Metadata-Based Classifiers
Because of the variable reliability and explicitness of the selected metadata, we incorporate these
fields into our statistical geolocation model in a similar manner to the message text. In prelimi33. The user-supplied real name could be any name ‚Äî i.e., it is not necessarily the user‚Äôs actual name ‚Äî but is a different
field from the user‚Äôs screen name.

484

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

Classifier
LOC
TZ
DESC
RNAME
BASELINE
TEXT

Acc

Acc@161

Acc@C

Median

0.405
0.064
0.048
0.045

0.525
0.171
0.117
0.109

0.834
0.565
0.526
0.550

92
1330
2907
2611

0.008
0.280

0.019
0.492

0.600
0.878

3719
170

Table 12: The performance of NB classifiers based on individual metadata fields, as well as a baseline, and the text-only classifier with IGR feature selection.

nary experiments, we considered bag-of-words features for the metadata fields, as well as bag-ofcharacter n-gram features for n ‚àà {1, ..., 4}.34 We found character 4-grams to perform best, and
report results using these features here. (A bag-of-character 4-grams represents the frequency of
each four-character sequence including a start and end symbol.) The geolocation performance of a
classifier trained on features from each metadata field in isolation, as well as the performance of a
most frequent city baseline (BASELINE) and our best purely text-based classifier (TEXT, replicated
from Table 7), is shown in Table 12.
The classifier based on each metadata field outperforms the baseline in terms of Acc, Acc@161,
and Median error distance. This suggests these metadata fields do indeed encode geographicallyidentifying information, though some classifiers are less competitive than TEXT. Notably, despite
the potential for noise in the user-supplied location fields, this classifier (LOC) achieves even better
performance than the purely text-based method, reaching a city-level accuracy of over 40%, predicting a location within 161km of the true location for over half of the users. This suggests LOC
contains valuable information, even though LOC fields are noisy (Cheng et al., 2010), and are not
easily captured by off-the-shelf geolocation tools (Hecht et al., 2011). Manual analysis suggests
many vernacular place names are captured in the statistical modelling, such as Kiladelphia and
Philly used to represent Philadelphia. The utility of metadata fields is also confirmed by the recent
work of Priedhorsky et al. (2014).
9.3 Ensemble Learning on Text-Based Classifiers
To further analyse the behaviour of the four metadata classifiers, we consider the pairwise city-level
prediction agreement between them. Cohen‚Äôs Kappa (Carletta, 1996) is a conventional metric to
evaluate inter-annotator agreement for categorical items (such as the predicted cities in our case);
larger Kappa values indicate higher pairwise agreement. The double fault measure (Giacinto &
Roli, 2001) incorporates gold-standard information, and is equal to the proportion of test cases for
which both classifiers make a false prediction. This measure offers the empirical lowest error bound
for the pairwise ensemble classifier performance.
34. Although we could certainly also consider character n-grams for the text-based classifier, we opted for a bag-ofwords representation because it explicitly captures the LIWs that we believe are especially important for geolocation.
There could also be location-indicative character n-grams, the exploration of which we leave for future work.

485

H AN , C OOK & BALDWIN

TEXT

0.461

0.689

0.702

0.704

0.181

LOC

0.577

0.578

0.581

0.066

0.063

TZ

0.903

0.907

0.067

0.041

0.085

DESC

0.923

0.065

0.049

0.080

0.088

RNAME

Table 13: Pairwise correlation of the base classifiers using Cohen‚Äôs Kappa (bottom left, in light
grey; higher numbers indicate greater prediction similarity) and the double fault measure
(top right, in white; lower numbers indicate greater prediction similarity).

Pairwise scores for Cohen‚Äôs Kappa and the double fault measure are shown in Table 13. The
Kappa scores (bottom-left of Table 13) are very low, indicating that there is little agreement between
the classifiers. Because the classifiers achieve better than baseline performance, but also give quite
different outputs, it might be possible to combine the classifiers to achieve better performance.
The double fault results (top-right) further suggest that improved accuracy could be obtained by
combining classifiers.
We combine the individual classifiers using meta-classification. We first adopt a feature concatenation strategy that incrementally combines the feature vectors of TEXT, LOC, TZ, DESC and
RNAME . We also consider stacked generalisation (Wolpert, 1992), referred to simply as stacking,
in which the outputs from the base classifiers, and the true city-level locations, are used to train a
second classifier which produces the final output. The base classifiers, and the second classifier,
are referred to as the L0 and L1 classifiers, respectively. In conventional applications of stacking,
homogeneous training data is used to train heterogeneous L0 classifiers; in our case, however, we
train homogeneous L0 multinomial Bayes models on heterogeneous data (i.e., different types of
data such as TEXT, LOC, and TZ). We consider logistic regression (Fan, Chang, Hsieh, Wang, &
Lin, 2008) and multinomial Bayes as the L1 classifier.
We carry out 10-fold cross validation on the training users to obtain the L1 (final) classifier
results, a standard procedure for stacking experiments. We use stratified sampling when partitioning
the data because the number of users in different cities varies remarkably, and a simple random
sample could have a bias towards bigger cities. The ensemble learning results are tabulated in Table
14.
The combination of TEXT and LOC is an improvement over LOC (i.e., our best results so far).
However, using feature concatenation and multinomial naive Bayes stacking, accuracy generally
drops as metadata feature sets that perform relatively poorly in isolation (i.e., TZ, DESC, RNAME)
are incorporated. On the other hand, using logistic regression stacking, we see small increases in
accuracy as features that perform less well in isolation are incorporated. Though DESC and RNAME
are moderately useful (as shown in Table 12), these fields contribute little to the strong ensembles
(i.e., TEXT, LOC and TZ). The best model (using logistic regression stacking and all features)
assigns users to the correct city in almost 50% of the test cases, and has a Median error of just
9km. Moreover, with this approach the country-level accuracy reaches almost 92%, indicating the
effectiveness of our method for this coarse-grained geolocation task.
486

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

Feature concatenation
Acc
Acc@161
0.444
0.646
0.429
0.639
0.319
0.529
0.294
0.503

1.
2.
3.
4.

Features
TEXT + LOC
1. + TZ
2. + DESC
3. + RNAME

Acc@C
0.923
0.929
0.912
0.912

Median
27
32
127
156

1.
2.
3.
4.

Multinomial Bayes stacking
Features
Acc
Acc@161 Acc@C
TEXT + LOC
0.470
0.660
0.933
1. + TZ
0.460
0.653
0.930
2. + DESC
0.451
0.645
0.931
3. + RNAME 0.451
0.645
0.931

Median
19
23
27
27

1.
2.
3.
4.

Logistic regression stacking
Features
Acc
Acc@161 Acc@C
TEXT + LOC
0.483
0.653
0.903
1. + TZ
0.490
0.665
0.917
2. + DESC
0.490
0.666
0.919
3. + RNAME 0.491
0.667
0.919

Median
14
9
9
9

Table 14: The performance of classifiers combining information from text and metadata using feature concatenation (top), multinomial Bayes stacking (middle), and logistic regression
stacking (bottom). Features such as ‚Äú1. + TZ‚Äù refer to the features used in row ‚Äú1.‚Äù in
combination with TZ.

It is interesting to observe that, while we found NB to outperform LR as a standalone classifier
in Section 5.2, as an L1 classifier, LR clearly outperforms NB. The reason for this is almost certainly the fact that we use a much smaller feature set relative to the number of training instances
in our stacking experiments, under which circumstances, discriminative models tend to outperform
generative models (Ng & Jordan, 2002).

10. Temporal Influence
In addition to the held-out English test data in WORLD+NG, we also developed a new geotagged
test dataset to measure the impact of time on model generalisation. The training and test data in
WORLD+NG are time-homogeneous as they are randomly partitioned based on data collected in
the same period. In contrast, the new test dataset (LIVE) is much newer, collected more than 1
year later than WORLD+NG. Given that Twitter users and topics change rapidly, a key question is
whether the statistical model learned from the ‚Äúold‚Äù training data is still effective over the ‚Äúnew‚Äù
test data? This question has implications for the maintenance and retraining of geolocation models
over time. In the experiments in this section we train on WORLD+NG and test on our new dataset.
The LIVE data was collected over 48 hours from 3 Mar, 2013 to 5 Mar, 2013, based on geotagged tweets from users whose declared language was English. Recent status updates (up to 200)
were crawled for each user, and langid.py was applied to the data to remove any remnant nonEnglish messages. In addition to filtering users with less than 10 geotagged tweets for the test data
as in WORLD+NG, we further exclude users with less than 50% of geotagged tweets from one
487

H AN , C OOK & BALDWIN

Features
1. TEXT
2. LOC
3. TZ
1. + 2. + 3.

Acc
0.280
0.405
0.064
0.490

Features
1. TEXT
2. LOC
3. TZ
1. + 2. + 3.

Acc
0.268
0.326
0.065
0.406

WORLD+NG
Acc@161 Acc@C
0.492
0.878
0.525
0.834
0.171
0.565
0.665
0.917
LIVE
Acc@161
0.510
0.465
0.160
0.614

Acc@C
0.901
0.813
0.525
0.901

Median
170
92
1330
9

Median
151
306
1529
40

Table 15: Generalisation comparison between the time-homogeneous WORLD+NG and timeheterogeneous LIVE (1. + 2. + 3. denotes stacking over TEXT, LOC and TZ).

city. This is because if a user‚Äôs geotagged tweets are spread across different locations, it is less
credible to adopt the user‚Äôs most frequent location as their true primary location in evaluation. A
post-check on the WORLD+NG test data shows that 9,977 out of 10K users satisfy this requirement
on geographical coherence, and that we aren‚Äôt unnecessarily biasing the data in LIVE in applying
this criterion. Finally, all status updates are aggregated at the user-level, as in WORLD+NG. After
filtering, 32K users were obtained, forming the final LIVE dataset.
We use only TEXT, LOC and TZ in this section, as they require less computation and achieve
accuracy comparable to our best results, as shown in Table 14. The temporal factor impact on
geolocation prediction model generalisation is revealed in the accuracy for WORLD+NG and LIVE
shown in Table 15. Acc and Acc@161 numbers in the stacked model (1. + 2. + 3.) drop by
approximately 8 and 5 percentage points, respectively, on LIVE as compared to WORLD+NG. The
Median prediction error distance also increases moderately from 9km to 40km. By decomposing
the stacked models and evaluating against the base classifiers, we find the accuracy declines are
primarily caused by accuracy drops in the LOC classifier on the new LIVE data, of approximately
9% in Acc and 6% in Acc@161. This could be viewed as a type of over-fitting, in that the stacked
classifier is relying too heavily on the predictions from the LOC base classifier. The TZ classifier
performs relatively constantly in terms of accuracy, although the Median error increases slightly.
The TEXT classifier is remarkably robust, with all numbers except for Acc improving marginally.
We further investigate the poor LOC classifier generalisation on LIVE. First, we down-sample
LIVE to 10K users, the same size as WORLD+NG, and then compare the per-city prediction numbers on the two datasets using only the LOC classifier. We find two factors jointly cause the accuracy
decrease on LIVE: (1) the composition of test users, and (2) the decline in per-city recall. For instance, 80 test users are from London, GB in WORLD+NG. This number sharply increases to 155 in
LIVE, meaning that the influence of London, GB test users on the overall accuracy in LIVE is almost
doubled. Furthermore, the recall ‚Äî the proportion of users from a given location who are correctly
predicted as being from that location ‚Äî for London, GB drops from 0.676 in WORLD+NG to
0.568 in LIVE. We observe that the proportion of empty LOC fields among London, GB test users
jumps from 13% (WORLD+NG) to 26% (LIVE). This reduces the utility of the LOC data in LIVE
488

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

Rank

cities in LIVE

1
2
3
4
5
6
7
8
9
10

Los Angeles, US
Kuala Lumpur, MY
London, GB
Jakarta, ID
Anaheim, US
Singapore, SG
Fort Worth, US
Chicago, US
Pittsburgh, US
San Antonio, US

LIVE users

LIVE recall

WORLD+NG users

WORLD+NG recall

201
168
155
129
85
76
76
72
72
66

0.766
0.482
0.568
0.550
0.447
0.474
0.289
0.569
0.431
0.455

81
50
80
86
26
160
35
123
39
82

0.691
0.560
0.675
0.686
0.346
0.556
0.371
0.577
0.487
0.585

Table 16: The number of test users, and recall using
LIVE, compared with WORLD+NG.

LOC ,

by city, for the top-10 largest cities in

and explains why the per-city recall drops: all test users with an empty LOC field are assigned to
the city with highest class prior in the model (i.e., Los Angeles, US). Overall, the ratios of empty
LOC fields in WORLD+NG test data and LIVE are 0.176 and 0.305, respectively, suggesting that
user-declared locations in LIVE carry much less geospatial information than in WORLD+NG. We
show other comparisons for the top-10 cities in terms of test users in Table 16,35 as the accuracy
of more highly-represented cities has a greater impact on overall results than that of smaller cities.
Like London, GB, most cities shown in Table 16 experience lower recall scores for LIVE, and many
of them have more test users in LIVE than in WORLD+NG. Nevertheless, some cities have higher
recall and more test users in LIVE, e.g., Los Angeles, US and Anaheim, US in Table 16. The overall
numbers are, of course, determined by aggregated performance over all cities. To provide some insight, 35.6% of cities in WORLD+NG have more than 40% in recall, but the number is only 28.5%
in LIVE.
While an important base classifier in the stacked model, the LOC accuracy numbers are most
influenced by temporal changes, whether it is because of an increased reluctance to supply a userdeclared location (although admittedly for users who geotag their tweets), or primarily due to variance in user proportions from different cities in the sampled stream. Either way, a periodically retrained LOC classifier would, no doubt, go some way towards remedying the temporal gap. Overall,
the numbers suggest that time-homogeneous data (WORLD+NG) is easier to classify than timeheterogeneous data (LIVE). However, training on ‚Äúold‚Äù data and testing on ‚Äúnew‚Äù data has been
shown to be empirically viable for the TEXT and TZ base classifiers in particular. This result also
validates efforts to optimise text-based user geolocation classification accuracy. Recently, similar
results on tweet-level geolocation prediction were observed by Priedhorsky et al. (2014), supporting
the claim that the accuracy of geolocation prediction suffers from diachronic mismatches between
the training and test data.

35. We observe that the city proportions changed drastically between WORLD+NG and LIVE. The reasons for this are
unclear, and we can only speculate that it is due to significant shifts in microblogging usage in different locations
around the world.

489

H AN , C OOK & BALDWIN

11. User Tweeting Behaviour
Having improved and extended text-based geolocation prediction, we now shift our focus to user
geolocatability. If a user wishes to keep their geolocation private, they can simply disable public
access of their tweets and metadata. However, if users choose to share their (non-geotagged) tweets,
are there different tweeting behaviours which will make them more susceptible to geolocation privacy attacks? To investigate this question, in this section, we discuss the impact of user behaviour
on geolocation accuracy relative to predictions over LIVE based on the stacking model from Section
10.36
As an obvious first rule of thumb, geotagged tweets should be avoided, because they provide
immediate access to a user‚Äôs geographical footprint, e.g., favourite bars, or their office address.
Second, as an immediate implication of our finding that location metadata is a strong predictor of
geolocation (Section 9.2), if a user wants to avoid privacy attacks, they should avoid presenting
location metadata, in effect disabling the LOC base classifier in our stacked classifier. Third, the text
of a user‚Äôs posts can be used to geolocate the user (at approximately 27% Acc, from Table 15). To
investigate the impact of the volume of tweets on user ‚Äúgeolocatability‚Äù, we perform a breakdown
of results over LIVE across two dimensions: (1) the number of LIWs, to investigate whether the
sheer volume of tweets from a user makes them more geolocatable; and (2) the source of geospatial
information which we exploit in the geolocation model. We evaluate these questions in Figure 6
in four feature combination settings, relative to the: (1) tweet text-based classifier; (2) tweet textbased classifier with gazetteer names removed;37 (3) metadata stacking using LOC and TZ (invariant
to tweet number changes); and (4) the stacking of TEXT, LOC and TZ for all users. In each case,
we partition the data into 20 partitions of 5% of users each, ranked by the total number of LIWs
contained in the combined posts from that user. In addition to the Acc for each user partition, we
also indicate the average number of LIWs per user in each partition (as shown in the second y-axis,
on the right side of the graph).
Overall, the more LIWs are contained in a user‚Äôs tweets, the higher the Acc for text-based methods. When gazetted terms are removed from the tweets, Acc drops by a large margin. This suggests
gazetted terms play a crucial role in user geolocation. Metadata also contributes substantially to accuracy, improving the text-based accuracy consistently. Moreover, if a user tweets a lot, the Acc of
the tweet text-based approach is comparable to our best model, even without access to the metadata
(as shown in the top right corner of the graph). As an overall recommendation, users who wish to
obfuscate their location should leave the metadata fields blank and avoid mentioning LIWs (e.g.,
gazetted terms and dialectal words) in their tweets. This will make it very difficult for our best geolocation models to infer their location correctly (as demonstrated to the bottom left of the graph).
A similar conclusion on user geolocatability was recently obtained by Priedhorsky et al. (2014). To
help privacy-conscious Twitter users to avoid being geolocated by their tweets, we have made the
list of LIWs publicly available.38

36. Our analysis is limited to behaviours that could easily be adopted by many users. Given that our system predicts the
most likely city from a fixed set for a given user, one simple way to avoid being geolocated is to move far away from
any of these cities. However, it seems unlikely that this strategy would be widely adopted.
37. Our gazetteer is based on the ASCII city names in the Geonames data.
38. http://www.csse.unimelb.edu.au/Àútim/etc/liw-jair.tgz

490

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

100
200
300
400
Number of Location Indicative Words

0.5

Text
Text without gazetteers
Metadata stacking (LOC, TZ)
Stacking (TEXT, LOC, TZ)

Accuracy

0.4
0.3
0.2
0.1
Number of Location Indicative Words (LIWs)

0.0
20%

40%
60%
Location Indicative Word partitions

80%

0

0.6

100%

Figure 6: The impact of the use of LIWs on geolocation accuracy. Users are sorted by the number
of LIWs in their tweets, and are partitioned into 20 bins. Metadata includes LOC and TZ.

12. Prediction Confidence
In the task setup to date, we have forced our models to geolocate all users. In practice, however,
many users don‚Äôt explicitly mention any geolocating words in their posts, making the task nigh on
impossible even for a human oracle. An alternative approach would be to predict a user geolocation
only when the model is confident of its prediction. Here, we consider a range of variables that
potentially indicate the prediction confidence.
Absolute probability (AP): Only consider predictions with probability above a specified threshold.
Prediction coherence (PC): We hypothesise that for reliable predictions, the top-ranked locations
will tend to be geographically close. In this preliminary exploration of coherence, we formulate PC as the sum of the reciprocal ranks of the predictions corresponding to the second-level
administrative region in our class representation (i.e., state or province) of the top-ranking
prediction, calculated over the top-10 predictions.39 For example, suppose the top-10 secondlevel predictions were in the following states in the US: US-TX, US-FL, US-TX, US-TX,
US-CA, US-TX, US-TX, US-FL, US-CA, US-NY. The top-ranking state-level prediction is
therefore US-TX, which also occurs at ranks 3, 4, 6 and 7 (for different cities in Texas). In
this case, PC would be 11 + 13 + 14 + 16 + 17 .
Probability ratio (PR): If the model is confident in its prediction, the first prediction will tend to
be much more probable than other predictions. We formulate this intuition as PR, the ratio of
the probability of the first and second most-probable predictions.
39. It could be measured by the average distance between top predictions as well.

491

H AN , C OOK & BALDWIN

0.8

Acc@161

0.6

0.4
Absolute Probability (AP)
Prediction Coherence (PC)
Probability Ratio (PR)
Feature Number (FN)
Feature Weight (FW)

0.2

0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
Recall

Figure 7: Acc@161 for classification of the top-n% most-confident predictions for each measure
of text-based prediction confidence on NA

Feature number (FN): We take the number of features found in a user‚Äôs posts as the prediction
accuracy. The intuition here is that a geolocation prediction based on more features is more
reliable than a prediction based on fewer features.
Feature weight (FW): Similar to FN, but in this case we use the sum of IGR of all features, rather
than just the number of features.
We investigate these variables on both NA and LIVE results. In particular, we only evaluate them using the text-based model, as we experiment only with text-based user geolocation in
this section. Nevertheless, exploration of other metadata classifiers is also possible. We sort the
predictions by confidence (independently for each measure of prediction confidence) and measure
Acc@161 among the top-n% of predictions for the following values of n: {0.0, 0.05, ..., 1.0}, akin
to a precision‚Äìrecall curve, as shown in Figures 7 and 8. Results on Acc show a very similar trend,
and are omitted from the paper.
The naive AP method is least reliable with, surprisingly, accuracy increasing as AP decreases
in both figures. It appears that the raw probabilities are not an accurate reflection of prediction
confidence. We find this is because a larger AP usually indicates a user has few LIW features, and
the model often geolocates the user to the city with the highest class prior. In comparison, PR ‚Äî
which focuses on relative, as opposed to raw, probabilities ‚Äî performs much better, with higher
PR generally corresponding to higher accuracy. In addition, PC shows different trends on the two
figures. It achieves comparable performance with PR on NA, however it is incapable of estimating
the global prediction confidence. This is largely because world-level PC numbers are often very
small and less discriminating than the regional PC numbers, reducing the utility of the geographic
proximity of the top predictions. Furthermore, FN and FW display similar overall trends to PR, but
don‚Äôt outperform PR.
492

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

Acc@161

0.8

0.6

0.4
Absolute Probability (AP)
Prediction Coherence (PC)
Probability Ratio (PR)
Feature Number (FN)
Feature Weight (FW)

0.2

0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
Recall

Figure 8: Acc@161 for classification of the top-n% most-confident predictions for each measure
of text-based prediction confidence on LIVE

These experiments suggest that there is indeed a trade-off between coverage and accuracy, which
could be further exploited to obtain higher-accuracy predictions in applications that do not require
all the data to be classified. PR, as well as FN and FW, are fairly effective indicators of predictive accuracy. A further extension on this line of research would be to investigate the prediction
confidence per city, e.g., are users from New York, US more predictable than users from Boston,
US?

13. Future Work
This research could be expanded in a number of directions. First, hierarchical classification models (Mahmud et al., 2012; Ahmed, Hong, & Smola, 2013) are becoming increasingly popular, and
could be combined with our stacked model. Although explicit social network data (e.g., followers)
can be non-trivial to retrieve, user interactions can be reconstructed from the content of tweets (e.g.,
replies, retweets and user mentions: Jurgens, 2013). This implicit network information could be
combined with our current text-based geolocation methods to further improve geolocation accuracy. Additionally, we hypothesise that text-based geolocation prediction is a challenging task for
humans, and that our method is achieving or surpassing the accuracy levels of a human. It would be
interesting to test this hypothesis, e.g., using crowdsourcing methods.
Recently, Priedhorsky et al. (2014) proposed evaluating message-level geolocation. They use
Gaussian mixture models to characterise n-gram probability distributions and evaluate the geolocation prediction accuracy using probabilistic metrics. Their conclusions strongly agree with our
findings, although our task setting is at the user-level and the evaluation metrics are different. In the
future, we plan to adapt our methods to tweet-level geolocation and carry out a systematic evaluation
with their probabilistic analysis of geolocation.
493

H AN , C OOK & BALDWIN

14. Summary
In this paper, we have investigated a series of key issues relating to text-based geolocation prediction
for Twitter users. We applied a number of feature selection methods to identify location indicative
words (LIWs), and demonstrated the effectiveness of feature selection on both regional (NA) and
global (WORLD) datasets. We then extended our study to analyse the impact of non-geotagged data,
the influence of language and the complementary geographical information in the user metadata.
We further evaluated our model on a time-heterogeneous dataset to assess the model‚Äôs sensitivity
to temporal change. Moreover, we discussed how users‚Äô tweeting behaviour affects geolocation
prediction, and drew conclusions on how users make themselves less easily geolocatable. Finally,
we explored various indicators to estimate prediction confidence, in terms of the balance between
prediction coverage and accuracy.
A number of conclusions can be drawn from this study, corresponding to the different sections of
the paper. We believe these findings contribute to a deeper understanding of text-based geolocation
prediction, and further shape the design of practical solutions to the problem:
‚Ä¢ We demonstrate that explicit selection of location indicative words improves geolocation prediction accuracy, as compared to using the full feature set.
‚Ä¢ Non-geotagged tweets (from users whose location is known) boost the prediction accuracy
substantially in both training and testing. We also demonstrate that modeling on geotagged
data and inferencing on non-geotagged data is indeed feasible. This is largely because of the
similarity between geotagged data and non-geotagged data, although minor differences are
observed between geotagged and non-geotagged tweets.
‚Ä¢ Modelling and inference on multilingual data is viable and easier than on monolingual English data. This is because tweet language strongly affects the prediction accuracy. Due to the
uneven geographical distribution of languages in tweets, users of geographically-diverse languages (e.g., English and Spanish) are much harder to geolocate than users of geographicallyfocused languages (e.g., Japanese or Dutch). Although trivially determining locations based
on the language in tweets is fine for geographically-focused languages, it is insufficient for
the majority of users who post tweets using geographically-diverse languages. By integrating
language information in different ways, we found training a range of monolingual models
based on language identification, and predicting location using a model based on the user‚Äôs
primary language, achieves better results than a monolithic multilingual model.
‚Ä¢ User-declared metadata, though noisy and unstructured, offers complementary location-indicative information to what is contained in tweets. By combining tweet and metadata information through stacking, the best global geolocation results are attained: over 49% of English
users can be correctly predicted at the city level, with a Median error distance of just 9km.
‚Ä¢ Results on time-heterogeneous evaluation suggest applying a model trained on ‚Äúold‚Äù data to
predict ‚Äúnew‚Äù data is generally feasible. Although the user-declared location field (LOC) is
sensitive to temporal change, classifiers based on the tweet content (TEXT) and user timezone
(TZ) generalise reasonably well across time.
‚Ä¢ Our pilot study on user geolocatability led to the following recommendations to preserve
geolocation privacy: (1) reduce the usage of location indicative words, particularly gazetted
494

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

terms; and (2) delete location-sensitive metadata (e.g., user-declared location and timezone
metadata).
‚Ä¢ Probability ratio, which measures the ratio of the probability of the top prediction with that
of the second prediction, can be used to estimate prediction confidence, and select only users
where the system prediction is more accurate, e.g., for downstream applications that require
more-reliable geolocation predictions and where exhaustive user geolocation is not required.

Acknowledgments
The authors wish to thank Stephen Roller and Jason Baldridge making their data and tools available
to replicate their NA experiments.
NICTA is funded by the Australian government as represented by Department of Broadband,
Communication and Digital Economy, and the Australian Research Council through the ICT Centre
of Excellence programme.

References
Ahmed, A., Hong, L., & Smola, A. J. (2013). Hierarchical geographical modeling of user locations
from social media posts. In Proceedings of the 22nd international conference on World Wide
Web, WWW ‚Äô13, pp. 25‚Äì36, Rio de Janeiro, Brazil.
Amitay, E., Har‚ÄôEl, N., Sivan, R., & Soffer, A. (2004). Web-a-where: geotagging web content.
In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR 2004), pp. 273‚Äì280, Sheffield, UK.
Backstrom, L., Kleinberg, J., Kumar, R., & Novak, J. (2008). Spatial variation in search engine
queries. In Proceeding of the 17th international conference on World Wide Web, WWW ‚Äô08,
pp. 357‚Äì366, Beijing, China.
Backstrom, L., Sun, E., & Marlow, C. (2010). Find me if you can: improving geographical prediction with social and spatial proximity. In Proceedings of the 19th International Conference
on World Wide Web, pp. 61‚Äì70, Raleigh, USA.
Baldwin, T., Cook, P., Lui, M., MacKinlay, A., & Wang, L. (2013). How noisy social media text,
how diffrnt social media sources?. In Proceedings of the 6th International Joint Conference
on Natural Language Processing (IJCNLP 2013), pp. 356‚Äì364, Nagoya, Japan.
Bennett, P. N., Radlinski, F., White, R. W., & Yilmaz, E. (2011). Inferring and using location
metadata to personalize web search. In Proceedings of the 34th International ACM SIGIR
Conference on Research and Development in Information Retrieval, SIGIR ‚Äô11, pp. 135‚Äì144,
Beijing, China.
Bentley, J. L. (1975). Multidimensional binary search trees used for associative searching. Communication of the ACM, 18(9), 509‚Äì517.
Bergsma, S., Dredze, M., Van Durme, B., Wilson, T., & Yarowsky, D. (2013). Broadly improving
user classification via communication-based name and location clustering on Twitter. In
Proceedings of the 2013 Conference of the North American Chapter of the Association for
495

H AN , C OOK & BALDWIN

Computational Linguistics: Human Language Technologies (NAACL-HLT 2013), pp. 1010‚Äì
1019, Atlanta, USA.
Bilhaut, F., Charnois, T., Enjalbert, P., & Mathet, Y. (2003). Geographic reference analysis for geographic document querying. In Proceedings of the HLT-NAACL 2003 workshop on Analysis
of geographic references - Volume 1, pp. 55‚Äì62, Edmonton, Canada.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal of Machine
Learning Research, 3, 993‚Äì1022.
Buyukokkten, O., Cho, J., Garcia-Molina, H., Gravano, L., & Shivakumar, N. (1999). Exploiting
geographical location information of web pages. In ACM SIGMOD Workshop on The Web
and Databases (WebDB‚Äô99), pp. 91‚Äì96, Philadelphia, USA.
Carletta, J. (1996). Assessing agreement on classification tasks: the kappa statistic. Computational
Linguistics, 22(2), 249‚Äì254.
Chandra, S., Khan, L., & Muhaya, F. (2011). Estimating Twitter user location using social
interactions‚Äìa content based approach. In 2011 IEEE Third International Conference on
Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third International Conference
on Social Computing (SocialCom), pp. 838‚Äì843, Boston, USA.
Chang, H.-w., Lee, D., M., E., & Lee, J. (2012). @Phillies tweeting from Philly? predicting Twitter
user locations with spatial word usage. In IEEE/ACM International Conference on Advances
in Social Networks Analysis and Mining (ASONAM), pp. 111‚Äì118, Istanbul, Turkey.
Cheng, Z., Caverlee, J., & Lee, K. (2010). You are where you tweet: a content-based approach
to geo-locating twitter users. In Proceedings of the 19th ACM International Conference on
Information and Knowledge Management, pp. 759‚Äì768, Toronto, Canada.
Cho, E., Myers, S. A., & Leskovec, J. (2011). Friendship and mobility: user movement in locationbased social networks. In Proceedings of the 17th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pp. 1082‚Äì1090, San Diego, USA.
Crandall, D. J., Backstrom, L., Huttenlocher, D., & Kleinberg, J. (2009). Mapping the world‚Äôs
photos. In Proceedings of the 18th international conference on World wide web, WWW ‚Äô09,
pp. 761‚Äì770, Madrid, Spain.
Dalvi, N., Kumar, R., & Pang, B. (2012). Object matching in tweets with spatial models. In
Proceedings of the Fifth ACM International Conference on Web Search and Data Mining
(WSDM 2012), pp. 43‚Äì52, Seattle, USA.
Ding, J., Gravano, L., & Shivakumar, N. (2000). Computing geographical scopes of web resources.
In Proceedings of the 26th International Conference on Very Large Data Bases, VLDB ‚Äô00,
pp. 545‚Äì556, Cairo, Egypt.
Dunning, T. (1993). Accurate methods for the statistics of surprise and coincidence. Computational
Linguistics, 19(1), 61‚Äì74.
Eisenstein, J., O‚ÄôConnor, B., Smith, N. A., & Xing, E. P. (2010). A latent variable model for
geographic lexical variation. In Proceedings of the 2010 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2010), pp. 1277‚Äì1287, Cambridge, USA.
Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., & Lin, C.-J. (2008). LIBLINEAR: A library
for large linear classification. Journal of Machine Learning Research, 9, 1871‚Äì1874.
496

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

Gelernter, J., & Mushegian, N. (2011). Geo-parsing messages from microtext. Transactions in GIS,
15(6), 753‚Äì773.
Giacinto, G., & Roli, F. (2001). Design of effective neural network ensembles for image classification purposes. Image and Vision Computing, 19(9‚Äì10), 699‚Äì707.
Gouws, S., Metzler, D., Cai, C., & Hovy, E. (2011). Contextual bearing on linguistic variation in
social media. In Proceedings of the Workshop on Languages in Social Media, LSM ‚Äô11, pp.
20‚Äì29, Portland, USA.
Guyon, I., & Elisseeff, A. (2003). An introduction to variable and feature selection. Journal of
Machine Learning Research, 3, 1157‚Äì1182.
Han, B., Cook, P., & Baldwin, T. (2012a). Automatically constructing a normalisation dictionary
for microblogs. In Proceedings of the Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning 2012 (EMNLP-CoNLL
2012), pp. 421‚Äì432, Jeju, Korea.
Han, B., Cook, P., & Baldwin, T. (2012b). Geolocation prediction in social media data by finding
location indicative words. In Proceedings of the 24th International Conference on Computational Linguistics, pp. 1045‚Äì1062, Mumbai, India.
Han, B., Cook, P., & Baldwin, T. (2013). A stacking-based approach to Twitter user geolocation
prediction. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics: System Demonstrations, pp. 7‚Äì12, Sofia, Bulgaria.
Hauff, C., & Houben, G.-J. (2012). Geo-location estimation of Flickr images: social web based
enrichment. In Proceedings of the 34th European Conference on Advances in Information
Retrieval, pp. 85‚Äì96, Barcelona, Spain.
Hecht, B., Hong, L., Suh, B., & Chi, E. H. (2011). Tweets from Justin Bieber‚Äôs heart: the dynamics
of the location field in user profiles. In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, pp. 237‚Äì246, Vancouver, Canada.
Hong, L., Ahmed, A., Gurumurthy, S., Smola, A. J., & Tsioutsiouliklis, K. (2012). Discovering
geographical topics in the Twitter stream. In Proceedings of the 21st International Conference
on World Wide Web (WWW 2012), pp. 769‚Äì778, Lyon, France.
Hong, L., Convertino, G., & Chi, E. H. (2011). Language matters in Twitter: A large scale study.
In Proceedings of the 5th International Conference on Weblogs and Social Media (ICWSM
2011), pp. 518‚Äì521, Barcelona, Spain.
Jurgens, D. (2013). That‚Äôs what friends are for: Inferring location in online social media platforms
based on social relationships. In Proceedings of the 7th International Conference on Weblogs
and Social Media (ICWSM 2013), pp. 273‚Äì282, Boston, USA.
Kinsella, S., Murdock, V., & O‚ÄôHare, N. (2011). ‚ÄúI‚Äôm eating a sandwich in Glasgow‚Äù: modeling
locations with tweets. In Proceedings of the 3rd international workshop on Search and mining
user-generated contents, pp. 61‚Äì68, Glasgow, UK.
Laere, O. V., Quinn, J., Schockaert, S., & Dhoedt, B. (2014). Spatially-aware term selection for
geotagging. IEEE Transactions on Knowledge and Data Engineering, 26(1), 221‚Äì234.
Laere, O. V., Schockaert, S., & Dhoedt, B. (2013). Georeferencing Flickr resources based on textual
meta-data. Information Sciences, 238, 52‚Äì74.
497

H AN , C OOK & BALDWIN

Leidner, J. L., & Lieberman, M. D. (2011). Detecting geographical references in the form of place
names and associated spatial natural language. SIGSPATIAL Special, 3(2), 5‚Äì11.
Li, R., Wang, S., & Chang, K. C.-C. (2012). Multiple location profiling for users and relationships
from social network and content. VLDB, 5(11), 1603‚Äì1614.
Li, W., Serdyukov, P., de Vries, A. P., Eickhoff, C., & Larson, M. (2011). The where in the tweet.
In Proceedings of the 20th ACM International Conference on Information and Knowledge
Management (CIKM 2011), pp. 2473‚Äì2476, Glasgow, UK.
Lieberman, M. D., & Lin, J. (2009). You are where you edit: Locating Wikipedia contributors
through edit histories. In Proceedings of the 3rd International Conference on Weblogs and
Social Media (ICWSM 2009), pp. 106‚Äì113, San Jose, USA.
Lui, M., & Baldwin, T. (2012). langid.py: An off-the-shelf language identification tool. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL
2012) Demo Session, pp. 25‚Äì30, Jeju, Korea.
Mahmud, J., Nichols, J., & Drews, C. (2012). Where is this tweet from? Inferring home locations
of Twitter users. In Proceedings of the 6th International Conference on Weblogs and Social
Media (ICWSM 2012), pp. 511‚Äì514, Dublin, Ireland.
Mao, H., Shuai, X., & Kapadia, A. (2011). Loose tweets: an analysis of privacy leaks on Twitter.
In Proceedings of the 10th Annual ACM Workshop on Privacy in the Electronic Society, pp.
1‚Äì12, Chicago, USA.
Nakatani, S. (2010). Language detection library for Java. http://code.google.com/p/
language-detection/.
Ng, A. Y., & Jordan, M. I. (2002). On discriminative vs. generative classifiers: A comparison of
logistic regression and naive Bayes. In Advances in Neural Information Processing Systems
14 (NIPS-02), pp. 841‚Äì848, Whistler, Canada.
Nocedal, J. (1980). Updating quasi-Newton matrices with limited storage. Mathematics of Computation, 35(151), 773‚Äì782.
NuÃÅnez-RedoÃÅ, M., Dƒ±ÃÅaz, L., Gil, J., GonzaÃÅlez, D., & Huerta, J. (2011). Discovery and integration of
Web 2.0 content into geospatial information structures: a use case in wild fire monitoring. In
Proceedings of the 6th International Conference on Availability, Reliability and Security, pp.
50‚Äì68, Vienna, Austria.
O‚ÄôConnor, B., Krieger, M., & Ahn, D. (2010). TweetMotif: Exploratory search and topic summarization for Twitter. In Proceedings of Fourth International AAAI Conference on Weblogs and
Social Media, pp. 384‚Äì385, Washington, D.C., USA.
O‚ÄôHare, N., & Murdock, V. (2013). Modeling locations with social media. Information Retrieval,
16(1), 30‚Äì62.
O‚ÄôSullivan, D., & Unwin, D. J. (2010). Point Pattern Analysis, pp. 121‚Äì155. John Wiley & Sons,
Inc.
Padmanabhan, V. N., & Subramanian, L. (2001). An investigation of geographic mapping techniques for internet hosts. In Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications, SIGCOMM ‚Äô01, pp.
173‚Äì185, San Diego, USA.
498

T EXT-BASED T WITTER U SER G EOLOCATION P REDICTION

Pontes, T., Vasconcelos, M., Almeida, J., Kumaraguru, P., & Almeida, V. (2012). We know where
you live: Privacy characterization of Foursquare behavior. In 4th International Workshop on
Location-Based Social Networks (LBSN 2012), Pittsburgh, USA.
Priedhorsky, R., Culotta, A., & Valle, S. Y. D. (2014). Inferring the origin locations of tweets with
quantitative confidence. In Proceedings of the 17th ACM Conference on Computer Supported
Cooperative Work and Social Computing, Baltimore, USA. To appear.
Quercini, G., Samet, H., Sankaranarayanan, J., & Lieberman, M. D. (2010). Determining the spatial
reader scopes of news sources using local lexicons. In Proceedings of the 18th SIGSPATIAL
International Conference on Advances in Geographic Information Systems, GIS ‚Äô10, pp. 43‚Äì
52, San Jose, USA.
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo,
USA.
Ritter, A., Clark, S., Mausam, & Etzioni, O. (2011). Named entity recognition in tweets: An experimental study. In Proceedings of the 2011 Conference on Empirical Methods in Natural
Language Processing, pp. 1524‚Äì1534, Edinburgh, UK.
Roller, S., Speriosu, M., Rallapalli, S., Wing, B., & Baldridge, J. (2012). Supervised text-based
geolocation using language models on an adaptive grid. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 1500‚Äì1510, Jeju Island, Korea.
Rout, D. P., Bontcheva, K., Preotiuc-Pietro, D., & Cohn, T. (2013). Where‚Äôs @wally?: A classification approach to geolocating users based on their social ties. In Proceedings of the 24th ACM
Conference on Hypertext and Social Media, pp. 11‚Äì20, Paris, France.
Sadilek, A., Kautz, H., & Bigham, J. P. (2012). Finding your friends and following them to where
you are. In Proceedings of the Fifth ACM International Conference on Web Search and Data
Mining, pp. 723‚Äì732, Seattle, USA.
Schulz, A., Hadjakos, A., Paulheim, H., Nachtwey, J., & MuÃàhlhaÃàuser, M. (2013). A multi-indicator
approach for geolocalization of tweets. In Proceedings of the 7th International Conference
on Weblogs and Social Media (ICWSM 2013), pp. 573‚Äì582, Boston, USA.
Serdyukov, P., Murdock, V., & van Zwol, R. (2009). Placing Flickr photos on a map. In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR 2009), pp. 484‚Äì491, Boston, USA.
Silva, M. J., Martins, B., Chaves, M. S., Afonso, A. P., & Cardoso, N. (2006). Adding geographic
scopes to web resources. Computers, Environment and Urban Systems, 30, 378‚Äì399.
Tuten, T. L. (2008). Advertising 2.0: Social media marketing in a Web 2.0 world. Praeger Publishers,
Westport, USA.
Vapnik, V. N. (1995). The nature of Statistical Learning Theory. Springer-Verlag, New York, USA.
Vincenty, T. (1975). Direct and inverse solutions of geodesics on the ellipsoid with application of
nested equations. Survey Review, 22(176), 88‚Äì93.
Wang, L., Wang, C., Xie, X., Forman, J., Lu, Y., Ma, W.-Y., & Li, Y. (2005). Detecting dominant
locations from search queries. In Proceedings of the 28th Annual International ACM SIGIR
499

H AN , C OOK & BALDWIN

Conference on Research and Development in Information Retrieval (SIGIR 2005), pp. 424‚Äì
431, Salvador, Brazil.
Wing, B. P., & Baldridge, J. (2011). Simple supervised document geolocation with geodesic grids.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, pp. 955‚Äì964, Portland, USA.
Wolpert, D. H. (1992). Stacked generalization. Neural Networks, 5(2), 241‚Äì259.
Yang, Y., & Pedersen, J. O. (1997). A comparative study on feature selection in text categorization.
In Proceedings of the Fourteenth International Conference on Machine Learning, ICML ‚Äô97,
pp. 412‚Äì420, San Francisco, USA.
Yi, X., Raghavan, H., & Leggetter, C. (2009). Discovering users‚Äô specific geo intention in web
search. In Proceedings of the 18th International Conference on World Wide Web, WWW ‚Äô09,
pp. 481‚Äì490, Madrid, Spain.
Yin, J., Lampert, A., Cameron, M., Robinson, B., & Power, R. (2012). Using social media to
enhance emergency situation awareness. Intelligent Systems, 27(6), 52‚Äì59.
Yin, Z., Cao, L., Han, J., Zhai, C., & Huang, T. (2011). Geographical topic discovery and comparison. In Proceedings of the 20th International Conference on World Wide Web, pp. 247‚Äì256,
Hyderabad, India.
Zong, W., Wu, D., Sun, A., Lim, E.-P., & Goh, D. H.-L. (2005). On assigning place names to
geography related web pages. In ACM/IEEE Joint Conference on Digital Libraries, pp. 354‚Äì
362, Denver, USA.

500

Journal of Artificial Intelligence Research 49 (2014) 1-47

Submitted 7/13; published 1/14

Multimodal Distributional Semantics
Elia Bruni

elia.bruni@unitn.it

Center for Mind/Brain Sciences,
University of Trento, Italy

Nam Khanh Tran

ntran@l3s.de

L3S Research Center,
Hannover, Germany

Marco Baroni

marco.baroni@unitn.it

Center for Mind/Brain Sciences,
University of Trento, Italy
Department of Information Engineering and Computer Science,
University of Trento, Italy

Abstract
Distributional semantic models derive computational representations of word meaning
from the patterns of co-occurrence of words in text. Such models have been a success
story of computational linguistics, being able to provide reliable estimates of semantic
relatedness for the many semantic tasks requiring them. However, distributional models
extract meaning information exclusively from text, which is an extremely impoverished
basis compared to the rich perceptual sources that ground human semantic knowledge. We
address the lack of perceptual grounding of distributional models by exploiting computer
vision techniques that automatically identify discrete ‚Äúvisual words‚Äù in images, so that the
distributional representation of a word can be extended to also encompass its co-occurrence
with the visual words of images it is associated with. We propose a flexible architecture
to integrate text- and image-based distributional information, and we show in a set of
empirical tests that our integrated model is superior to the purely text-based approach,
and it provides somewhat complementary semantic information with respect to the latter.

1. Introduction
The distributional hypothesis states that words that occur in similar contexts are semantically similar. The claim has multiple theoretical roots in psychology, structuralist linguistics, lexicography and possibly even in the later writings of Wittgenstein (Firth, 1957;
Harris, 1954; Miller & Charles, 1991; Wittgenstein, 1953). However, the distributional hypothesis has had a huge impact on computational linguistics in the last two decades mainly
for empirical reasons, that is, because it suggests a simple and practical method to harvest
word meaning representations on a large scale: Just record the contexts in which words
occur in easy-to-assemble large collections of texts (corpora) and use their contextual profiles as surrogates of their meaning. Nearly all contemporary corpus-based approaches to
semantics rely on contextual evidence in one way or another, but the most systematic and
extensive application of distributional methods is found in what we call distributional
semantic models (DSMs), also known in the literature as vector space or semantic space
¬©2014 AI Access Foundation. All rights reserved.

Bruni, Tran & Baroni

models of meaning (Landauer & Dumais, 1997; Sahlgren, 2006; Sch√ºtze, 1997; Turney &
Pantel, 2010).
In DSMs, the meaning of a word is approximated with a vector that keeps track of
the patterns of co-occurrence of the word in text corpora, so that the degree of semantic
similarity or, more generally, relatedness (Budanitsky & Hirst, 2006) of two or more words
can be precisely quantified in terms of geometric distance between the vectors representing
them. For example, both car and automobile might occur with terms such as street, gas
and driver, and thus their distributional vectors are likely to be very close, cuing the fact
that these words are synonyms. Extended empirical evidence has shown that distributional
semantics is very good at harvesting effective meaning representations on a large scale,
confirming the validity of the distributional hypothesis (see some references in Section 2.1
below).
Still, for all its successes, distributional semantics suffers of the obvious limitation that
it represents the meaning of a word entirely in terms of connections to other words. A long
tradition of studies in cognitive science and philosophy has stressed how models where the
meaning of symbols (e.g., words) are entirely accounted for in terms of other symbols (e.g.,
other words) without links to the outside world (e.g., via perception) are deeply problematic, an issue that is often referred to as the symbol grounding problem (Harnad, 1990).
DSMs have also come under attack for their lack of grounding (Glenberg & Robertson,
2000).1 Although the specific criticisms vented at them might not be entirely well-founded
(Burgess, 2000), there can be little doubt that the limitation to textual contexts makes
DSMs very dissimilar from humans, who, thanks to their senses, have access to rich sources
of perceptual knowledge when learning the meaning of words ‚Äì so much so that some cognitive scientists have argued that meaning is directly embodied in sensory-motor processing
(see the work in de Vega, Glenberg, & Graesser, 2008, for different views on embodiment in
cognitive science). Indeed, in the last decades a large amount of behavioural and neuroscientific evidence has been amassed indicating that our knowledge of words and concepts is
inextricably linked with our perceptual and motor systems. For example, perceiving actiondenoting verbs such as kick or lick involves the activation of areas of the brain controlling
foot and tongue movements, respectively (Pulvermueller, 2005). Hansen, Olkkonen, Walter, and Gegenfurtner (2006) asked subjects to adjust the color of fruit images objects until
they appeared achromatic. The objects were generally adjusted until their color was shifted
away from the subjects‚Äô gray point in a direction opposite to the typical color of the fruit,
e.g., bananas were shifted towards blue because subjects‚Äô overcorrected for their typical
yellow color. Typical color also influences lexical access: For example, subjects are faster
at naming a pumpkin in a picture in which it is presented in orange than in a grayscale
representation, slowest if it is in another color (Therriault, Yaxley, & Zwaan, 2009). As
a final example, Kaschak, Madden, Therriault, Yaxley, Aveyard, Blanchard, and Zwaan
(2005) found that subjects are slower at processing a sentence describing an action if the
sentence is presented concurrently to a visual stimulus depicting motion in the opposite
1. Harnard, in the original paper, is discussing formal symbols, such as those postulated in Fodor‚Äôs ‚Äúlanguage
of thought‚Äù (Fodor, 1975), rather than the words of a natural language. However, when the latter are
represented in terms of connections to other words, as is the case in DSMs, the same grounding problem
arises, and we follow the recent literature on the issue in referring to it as ‚Äúsymbol grounding‚Äù, where
our symbols are natural language words.

2

Multimodal Distributional Semantics

direction of that described (e.g., The car approached you is harder to process concurrently
to the perception of motion away from you). See the review in Barsalou (2008) for a review
of more evidence that conceptual and linguistic competence is strongly embodied.
One might argue that the concerns about DSMs not being grounded or embodied are
exaggerated, because they overlook the fact that the patterns of linguistic co-occurrence
exploited by DSMs reflect semantic knowledge we acquired through perception, so that
linguistic and perceptual information are strongly correlated (Louwerse, 2011). Because
dogs are more often brown than pink, we are more likely to talk about brown dogs than
pink dogs. Consequently, a child can learn useful facts about the meaning of the concept
denoted by dog both by direct perception and through linguistic input (this explains, among
other things, why congenitally blind subjects can have an excellent knowledge of color terms;
see, e.g., Connolly, Gleitman, & Thompson-Schill, 2007). One could then hypothesize that
the meaning representations extracted from text corpora are indistinguishable from those
derived from perception, making grounding redundant. However, there is by now a fairly
extensive literature showing that this is not the case. Many studies (Andrews, Vigliocco,
& Vinson, 2009; Baroni, Barbu, Murphy, & Poesio, 2010; Baroni & Lenci, 2008; Riordan
& Jones, 2011) have underlined how text-derived DSMs capture encyclopedic, functional
and discourse-related properties of word meanings, but tend to miss their concrete aspects.
Intuitively, we might harvest from text the information that bananas are tropical and eatable,
but not that they are yellow (because few authors will write down obvious statements such
as ‚Äúbananas are yellow‚Äù). On the other hand, the same studies show how, when humans are
asked to describe concepts, the features they produce (equivalent in a sense to the contextual
features exploited by DSMs) are preponderantly of a perceptual nature: Bananas are yellow,
tigers have stripes, and so on.2
This discrepancy between DSMs and humans is not, per se, a proof that DSMs will
face empirical difficulties as computational semantic models. However, if we are interested
in the potential implications of DSMs as models of how humans acquire and use language
‚Äìas is the case for many DSM developers (e.g., Griffiths, Steyvers, & Tenenbaum, 2007;
Landauer & Dumais, 1997; Lund & Burgess, 1996, and many others)‚Äì then their complete
lack of grounding in perception is a serious blow to their psychological plausibility, and
exposes them to all the criticism that classic ungrounded symbolic models have received.
Even at the empirical level, it is reasonable to expect that DSMs enriched with perceptual
information would outperform their purely textual counterparts: Useful computational semantic models must capture human semantic knowledge, and human semantic knowledge
is strongly informed by perception.
If we accept that grounding DSMs into perception is a desirable avenue of research, we
must ask where we can find a practical source of perceptual information to embed into DSMs.
Several interesting recent experiments use features produced by human subjects in concept
description tasks (so-called ‚Äúsemantic norms‚Äù) as a surrogate of true perceptual features
(Andrews et al., 2009; Johns & Jones, 2012; Silberer & Lapata, 2012; Steyvers, 2010).
While this is a reasonable first step, and the integration methods proposed in these studies
2. To be perfectly fair, this tendency might in part be triggered by the fact that, when subjects are asked to
describe concepts, they might be encouraged to focus on their perceptual aspects by the experimenters‚Äô
instructions. For example McRae, Cree, Seidenberg, and McNorgan (2005) asked subjects to list first
‚Äúphysical properties, such as internal and external parts, and how [the object] looks.‚Äù

3

Bruni, Tran & Baroni

are quite sophisticated, using subject-produced features is unsatisfactory both practically
and theoretically (see however the work reported by Kievit-Kylar & Jones, 2011, for a
crowdsourcing project that is addressing both kinds of concerns). Practically, using subjectgenerated properties limits experiments to those words that denote concepts described in
semantic norms, and even large norms contain features for just a few hundred concepts.
Theoretically, the features produced by subjects in concept description tasks are far removed
from the sort of implicit perceptual features they are supposed to stand for. For example,
since they are expressed in words, they are limited to what can be conveyed verbally.
Moreover, subjects tend to produce only salient and distinctive properties. They do not
state that dogs have a head, since that‚Äôs hardly a distinctive feature for an animal!
In this article, we explore a more direct route to integrate perceptual information into
DSMs. We exploit recent advances in computer vision (Grauman & Leibe, 2011) and the
availability of documents that combine text and images to automatically extract visual
features that are naturally co-occurring with words in multimodal corpora. These imagebased features are then combined with standard text-based features to obtain perceptuallyenhanced distributional vectors. In doing this, we rely on a natural extension of the distributional hypothesis, that encompasses not only similarity of linguistic context, but also
similarity of visual context. Interestingly, Landauer and Dumais, in one of the classic papers
that laid the groundwork for distributional semantics, already touched on the grounding
issue and proposed, speculatively, a solution along the lines of the one we are implementing
here: ‚Äú[I]f one judiciously added numerous pictures of scenes with and without rabbits to
the context columns in the [. . . ] corpus matrix, and filled in a handful of appropriate cells
in the rabbit and hare word rows, [a DSM] could easily learn that the words rabbit and hare
go with pictures containing rabbits and not to ones without, and so forth.‚Äù (Landauer &
Dumais, 1997, p. 227).3
Although vision is just one source of perceptual data, it is a reasonable starting point,
both for convenience (availability of suitable data to train the models) and because it is
probably the dominating modality in determining word meaning. As just one piece of
evidence for this claim, the widely used subject-generated semantic norms of McRae et al.
(2005) contain 3,594 distinct perceptual features in total, and, of these, 3,099 (86%) are
visual in nature!
Do the relatively low-level and noisy features that we extract from images in multimodal corpora contribute meaningful information to the distributional representation of
word meaning? We report the results of a systematic comparison of the network of semantic relations entertained by a set of concrete nouns in the traditional text-based and
novel image-based distributional spaces confirming that image-based features are, indeed,
semantically meaningful. Moreover, as expected, they provide somewhat complementary
information with respect to text-based features. Having thus found a practical and effective way to extract perceptual information, we must consider next how to combine textand image-derived features to build a multimodal distributional semantic model. We
propose a general parametrized architecture for multimodal fusion that, given appropriate
sample data, automatically determines the optimal mixture of text- and image-based features to be used for the target semantic task. Finally, we evaluate our multimodal DSMs in
3. We thank Mike Jones for pointing out this interesting historical connection to us.

4

Multimodal Distributional Semantics

two separate semantic tasks, namely predicting the degree of semantic relatedness assigned
to word pairs by humans, and categorizing nominal concepts into classes. We show that
in both tasks multimodal DSMs consistently outperform purely textual models, confirming
our supposition that, just like for humans, the performance of computational models of
meaning improves once meaning is grounded in perception.
The article is structured as follows. Section 2 provides the relevant background from
computational linguistics and image analysis, and discusses related work. We lay out a
general architecture for multimodal fusion in distributional semantics in Section 3. The
necessary implementation details are provided in Section 4. Section 5 presents the experiments in which we tested our approach. Section 6 concludes summarizing our current
results as well as sketching what should come next.

2. Background and Related Work
In this section we first give a brief introduction to traditional distributional semantic models
(i.e., those based solely on textual information). Then, we describe the image analysis
techniques we adopt to extract and manipulate visual information. Next, we discuss earlier
attempts to construct a multimodal distributional representation of meaning. Finally, we
describe the most relevant strategies to combine information coming from text and images
proposed inside the computer vision community.
2.1 Distributional Semantics
In the last few decades, a number of different distributional semantic models (DSMs) of word
meaning have been proposed in computational linguistics, all relying on the assumption that
word meaning can be learned directly from the linguistic environment.
Semantic space models are one of the most common types of DSM. They approximate
the meaning of words with vectors that record their distributional history in a corpus
(Turney & Pantel, 2010). A distributional semantic model is encoded in a matrix whose m
rows are semantic vectors representing the meanings of a set of m target words. Each
component of a semantic vector is a function of the occurrence counts of the corresponding
target word in a certain context (see Lowe, 2001, for a formal treatment). Definitions of
context range from simple ones (such as documents or the occurrence of another word
inside a fixed window from the target word) to more linguistically sophisticated ones (such
as the occurrence of certain words connected to the target by special syntactic relations)
(Pad√≥ & Lapata, 2007; Sahlgren, 2006; Turney & Pantel, 2010). After the raw targetcontext counts are collected, they are transformed into association scores that typically
discount the weights of components whose corresponding word-context pairs have a high
probability of chance co-occurrence (Evert, 2005). The rank of the matrix containing the
semantic vectors as rows can optionally be decreased by dimensionality reduction, that
might provide beneficial smoothing by getting rid of noise components and/or allow more
efficient storage and computation (Landauer & Dumais, 1997; Sahlgren, 2005; Sch√ºtze,
1997). Finally, the distributional semantic similarity of a pair of target words is estimated
by a similarity function that takes their semantic vectors as input and returns a scalar
similarity score as output.
5

Bruni, Tran & Baroni

There are many different semantic space models in the literature. Probably the best
known is Latent Semantic Analysis (LSA, Landauer & Dumais, 1997), where a highdimensional semantic space for words is derived by the use of co-occurrence information
between words and the passages where they occur. Another well-known example is the
Hyperspace Analog to Language model (HAL, Lund & Burgess, 1996), where each word is
represented by a vector containing weighted co-occurrence values of that word with the other
words in a fixed window. Other semantic space models rely on syntactic relations instead
of windows (Grefenstette, 1994; Curran & Moens, 2002; Pad√≥ & Lapata, 2007). General
overviews of semantic space models are provided by Clark (2013), Erk (2012), Manning and
Sch√ºtze (1999), Sahlgren (2006) and Turney and Pantel (2010).
More recently, probabilistic topic models have been receiving increasing attention as
an alternative implementation of DSMs (Blei, Ng, & Jordan, 2003; Griffiths et al., 2007).
Probabilistic topic models also rely on co-occurrence information from large corpora to derive meaning but, differently from semantic space models, they are based on the assumption
that words in a corpus exhibit some probabilistic structure connected to topics. Words are
not represented as points in a high-dimensional space but as a probability distribution over
a set of topics. Conversely, each topic can be defined as a probability distribution over
different words. Probabilistic topic models tackle the problem of meaning representation
by means of statistical inference: use the word corpus to infer the hidden topic structure.
Distributional semantic models, whether of the geometric or the probabilistic kind,
ultimately are mainly used to provide a similarity score for arbitrary pairs of words, and
that is how we will also employ them. Indeed, such models have shown to be very effective
in modeling a wide range of semantic tasks including judgments of semantic relatedness and
word categorization.
There are several data sets to assess how well a DSM captures human intuitions about
semantic relatedness, such as the Rubenstein and Goodenough set (Rubenstein & Goodenough, 1965) and WordSim353 (Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman,
& Ruppin, 2002). Usually they are constructed by asking subjects to rate a set of word
pairs according to a similarity scale. Then, the average rating for each pair is taken as an
estimate of the perceived relatedness between the words (e.g., dollar-buck: 9.22, cord-smile:
0.31). To measure how well a distributional model approximates human semantic intuitions,
usually a correlation measure between the similarity scores generated by the model and the
human ratings is computed. The highest correlation we are aware of on the WordSim353
set we will also employ below is of 0.80 and it was obtained by a model called Temporal
Semantic Analysis, which captures patterns of word usage over time and where concepts
are represented as time series over a corpus of temporally-ordered documents (Radinsky,
Agichtein, Gabrilovich, & Markovitch, 2011). This temporal knowledge could be integrated
with the perceptual knowledge we encode in our model. As a more direct comparison point,
Agirre, Alfonseca, Hall, Kravalova, Pas√ßa, and Soroa (2009) presented an extensive evaluation of distributional and WordNet-based semantic models on WordSim, both achieving a
maximum correlation of 0.66 across various parameters.4
4. WordNet, available at http://wordnet.princeton.edu/, is a large computational lexicon of English
where nouns, verbs, adjectives and adverbs are grouped into sets of synonyms (synsets), each expressing
a distinct concept.

6

Multimodal Distributional Semantics

Humans are very good at grouping together words (or the concepts they denote) into
classes based on their semantic relatedness (Murphy, 2002), therefore a cognitive-aware
representation of meaning must show its proficiency also in categorization (e.g., Poesio
& Almuhareb, 2005; Baroni et al., 2010). Concept categorization is moreover useful for
applications such as automated ontology construction and recognizing textual entailment.
Unlike similarity ratings, categorization requires a discrete decision to group coordinates/cohyponyms into the same class and it is performed by applying standard clustering techniques
to the model-generated vectors representing the words to be categorized. As an example,
the Almuhareb-Poesio data set (Almuhareb & Poesio, 2005), that we also employ below,
includes 402 concepts from WordNet, balanced in terms of frequency and degree of ambiguity. The distributional model of Rothenh√§usler and Sch√ºtze (2009) exploits syntactic
information to reach state-of-the-art performance on the Almuhareb-Poesio data set (maximum clustering purity across various parameter: 0.79). The window-based distributional
approach of Baroni and Lenci (2010), more directly comparable to our text-based models,
achieves 0.65 purity.
Other semantic tasks DSMs have been applied to include semantic priming, generation
of salient properties of concepts and intuitions about the thematic fit of verb arguments (see,
e.g., Baroni & Lenci, 2010; Baroni et al., 2010; McDonald & Brew, 2004; Pad√≥ & Lapata,
2007; Pad√≥, Pad√≥, & Erk, 2007). Distributional semantic vectors can be used in a wide
range of applications that require a representation of word meaning, and in particular an
objective measure of meaning relatedness, including document classification, clustering and
retrieval, question answering, automatic thesaurus generation, word sense disambiguation,
query expansion, textual advertising and some areas of machine translation (Dumais, 2003;
Turney & Pantel, 2010).
2.2 Visual Words
Ideally, to build a multimodal DSM, we would like to extract visual information from
images in a way that is similar to how we do it for text. Thanks to a well-known image
analysis technique, namely bag-of-visual-words (BoVW), it is indeed possible to discretize
the image content and produce visual units somehow comparable to words in text, known
as visual words (Bosch, Zisserman, & Munoz, 2007; Csurka, Dance, Fan, Willamowski, &
Bray, 2004; Nister & Stewenius, 2006; Sivic & Zisserman, 2003; Yang, Jiang, Hauptmann,
& Ngo, 2007). Therefore, semantic vectors can be extracted from a corpus of images
associated with the target (textual) words using a similar pipeline to what is commonly
used to construct text-based vectors: Collect co-occurrence counts of target words and
discrete image-based contexts (visual words), and approximate the semantic relatedness of
two words by a similarity function over the visual words representing them.
The BoVW technique to extract visual word representations of documents was inspired
by the traditional bag-of-words (BoW) method in Information Retrieval. BoW in turn is
a dictionary-based method to represent a (textual) document as a ‚Äúbag‚Äù (i.e., order is not
considered), which contains words from the dictionary. BoVW extends this idea to visual
documents (namely images), describing them as a collection of discrete regions, capturing
their appearance and ignoring their spatial structure (the visual equivalent of ignoring word
order in text). A bag-of-visual-word representation of an image is convenient from an image7

Bruni, Tran & Baroni

	




	

	






Figure 1: Representing images by BoVW: (i) Salient image patches or keypoints that contain rich local information are detected and represented as vectors of low-level
features called descriptors; (ii) Descriptors are mapped to visual words on the
basis of their distance from centers of clusters corresponding to the visual words


(the preliminary clustering step is not shown
in the figure); (iii) Images are finally
represented as a bag-of-visual-words feature vector according to the distribution
of visual words they contain. Images depicting the same things with rotations,
occlusions, small differences in the low-level descriptors might still have a similar
distribution of visual words, hence the same object can be traced very robustly
across images while these conditions change.

analysis point of view because it translates a usually large set of high-dimensional local
descriptors into a single sparse vector representation across images. Importantly, the size
of the original set varies from image to image, while the bag-of-visual-word representation
is of fixed dimensionality. Therefore, machine learning algorithms which by default expect
fixed-dimensionality vectors as input (e.g., for supervised classification or unsupervised
clustering) can be used to tackle typical image analysis tasks such as object recognition,
image segmentation, video tracking, motion detection, etc.
More specifically, similarly to terms in a text document, an image has local interest
points or keypoints defined as salient image patches that contain rich local information
about the image. However ‚Äúkeypoint types‚Äù in images do not come off-the-shelf like word
8

Multimodal Distributional Semantics

types in text documents. Local interest points have to be grouped into types (i.e., visual
words) within and across images, so that an image can be represented by the number of
occurrences of each type in it, analogously to BoW. The following pipeline is typically
followed. From every image of a data set, keypoints are automatically detected (note
that in most recent approaches a dense, pixelwise sampling of the keypoints is preferred
to detecting the most salient ones only, and this is the solution that we also adopt, as
explained in Section 4.2.2) and represented as vectors of low-level features called descriptors.
Keypoint vectors are then grouped across images into a number of clusters based on their
similarity in descriptor space. Each cluster is treated as a discrete visual word. With its
keypoints mapped onto visual words, each image can then be represented as a BoVW feature
vector recording how many times each visual word occurs in it. In this way, we move from
representing the image by a varying number of high-dimensional keypoint descriptor vectors
to a representation in terms of a single visual word count vector of fixed dimensionality
across all images, with the advantages we discussed above. Visual word assignment and
its use to represent the image content is exemplified in Figure 1, where two images with a
similar content are described in terms of bag-of-visual-word vectors.
What kind of image content a visual word captures exactly depends on a number of
factors, including the descriptors used to identify and represent keypoints, the clustering
algorithm and the number of target visual words selected. In general, local interest points
assigned to the same visual word tend to be patches with similar low-level appearance; but
these local patterns need not be correlated with object-level parts present in the images
(Grauman & Leibe, 2011).
2.3 Multimodal Distributional Semantics
The availability of large amounts of mixed media on the Web, on the one hand, and the
discrete representation of images as visual words, on the other, has not escaped the attention
of computational linguists interested in enriching distributional representations of word
meaning with visual features.
Feng and Lapata (2010) propose the first multimodal distributional semantic model.
Their generative probabilistic setting requires the extraction of textual and visual features
from the same mixed-media corpus, because latent dimensions are here estimated through a
probabilistic process which assumes that a document is generated by sampling both textual
and visual words. Words are then represented by their distribution over a set of latent
multimodal dimensions or ‚Äútopics‚Äù (Griffiths et al., 2007) derived from the surface textual
and visual features. Feng and Lapata experiment with a collection of documents downloaded
from the BBC News website as corpus. They test their semantic representations on the
free association norms of Nelson, McEvoy, and Schreiber (1998) and on a subset of 253
pairs from WordSim, obtaining gains in performance when visual information is taken into
account (correlations with human judgments of 0.12 and 0.32 respectively), compared to
the textual modality standalone (0.08 and 0.25 respectively), even if performance is still
well below state-of-the-art for WordSim (see Section 2.1 above).
The main drawbacks of this approach are that the textual and visual data must be
extracted from the same corpus, thus limiting the choice of the corpora to be used, and
that the generative probabilistic approach, while elegant, does not allow much flexibility
9

Bruni, Tran & Baroni

in how the two information channels are combined. Below, we re-implement the Feng and
Lapata method (MixLDA) training it on the ESP-Game data set, the same source of labeled
images we adopt for our model. This is possible because the data set contains both images
and the textual labels describing them. More in general, we recapture Feng and Lapata‚Äôs
idea of a common latent semantic space in the latent multimodal mixing step of our pipeline
(see Section 3.2.1 below).
Leong and Mihalcea (2011) also exploit textual and visual information to obtain a multimodal distributional semantic model. While Feng and Lapata merge the two sources of
information by learning a joint semantic model, Leong and Mihalcea propose a strategy
akin to what we will call Scoring Level fusion below: Come up with separate text- and
image-based similarity estimates, and combine them to obtain the multimodal score. In
particular, they use two combination methods: summing the scores and computing their
harmonic mean. Differently from Feng and Lapata, Leong and Mihalcea extract visual information not from a corpus but from a manually coded resource, namely the ImageNet
database (Deng, Dong, Socher, Li, & Fei-Fei, 2009), a large-scale ontology of images.5 Using
a handcoded annotated visual resource such as ImageNet faces the same sort of problems
that using a manually developed lexical database such as WordNet faces with respect to
textual information, that is, applications will be severely limited by ImageNet coverage (for
example, ImageNet is currently restricted to nominal concepts), and the interest of the
model as a computational simulation of word meaning acquisition from naturally occurring language and visual data is somewhat reduced (humans do not learn the meaning of
‚Äúmountain‚Äù from a set of carefully annotated images of mountains with little else crowding
or occluding the scene). In the evaluation, Leong and Mihalcea experiment with small subsets of WordSim, obtaining some improvements, although not at the same level we report
(the highest reported correlation is 0.59 on just 56 word pairs). Furthermore they use the
same data set to tune and test their models.
In Bruni, Tran, and Baroni (2011) we propose instead to directly concatenate the textand image-based vectors to produce a single multimodal vector to represent words, as in
what we call Feature Level fusion below. The text-based distributional vector representing a word, taken there from a state-of-the-art distributional semantic model (Baroni &
Lenci, 2010), is concatenated with a vector representing the same word with visual features, extracted from all the images in the ESP-Game collection we also use here. We
obtain promising performance on WordSim and other test sets, although appreciably lower
than the results we report here (we obtain a maximum correlation of 0.52 when text- and
image-based features are used together; compare to Table 2 below).
Attempts to use multimodal models derived from text and images to perform more
specific semantic tasks have also been reported. Bergsma and Goebel (2011) use textual
and image-based cues to model selectional preferences of verbs (which nouns are likely
arguments of verbs). Their experiment shows that in several cases visual information is
more useful than text in this task. For example, by looking in textual corpora for words
such as carillon, migas or mamey, not much useful information is obtained to guess which
of the three is a plausible argument for the verb to eat. On the other hand, they also show
5. http://image-net.org/

10

Multimodal Distributional Semantics

that, by exploiting Google image search functionality,6 enough images for these words are
found that a vision-based model of edible things can classify them correctly.
Finally, we evaluate our multimodal models in the task of discovering the color of concrete objects, showing that the relation between words denoting concrete things and their
typical color is better captured when visual information is also taken into account (Bruni,
Boleda, Baroni, & Tran, 2012). Moreover, we show that multimodality helps in distinguishing literal and nonliteral uses of color terms.
2.4 Multimodal Fusion
When textual information is used for image analysis, this is mostly done with different
aims than ours: Text is used to improve image-related tasks, and typically there is an
attempt to model the relation between specific images and specific words or textual passages
(e.g., Barnard, Duygulu, Forsyth, de Freitas, Blei, & Jordan, 2003; Berg, Berg, & Shih,
2010; Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, & Forsyth, 2010; Griffin,
Wahab, & Newell, 2013; Kulkarni, Premraj, Dhar, Li, Choi, Berg, & Berg, 2011). In
contrast, (i) we want to use image-derived features to improve the representation of word
meaning and (ii) we are interested in capturing the meaning of word types on the basis of
sets of images connected to a word, and not to model specific word-image relations.
Despite these differences, some of the challenges addressed in the image analysis literature that deals with exploiting textual cues are similar to the ones we face. In particular,
the problem of merging, or ‚Äúfusing‚Äù, textual and visual cues into a common representational
space is exactly the same we have to face when we construct a multimodal semantic space.
Traditionally, the image analysis community distinguishes between two classes of fusion
schemes, namely early fusion and late fusion. The former fuses modalities in feature space,
the latter fuses modalities in semantic similarity space, analogously to what we will call
Feature Level and Scoring Level fusion, respectively. For example, Escalante, H√©rnadez,
Sucar, and Montes (2008) propose an image retrieval system for multimodal documents.
Both early and late fusion strategies for the combination of the image and the textual
channels are considered. Early fusion settings include a weighted linear combination of the
two channels and a global strategy where different retrieval systems are used contemporarily
on the entire, joint data set. Late fusion strategies include a per-modality strategy, where
documents are retrieved by using only one or the other channel and a hierarchical setting
where first text, image and their combination are used independently to query the database
and then results are aggregated with four weighted combinations. Vreeswijk, Huurnink,
and Smeulders (2011) train a visual concept classifier for abstract subject categories such as
biology and history by using a late fusion approach where image and text information are
combined at the output level, that is, first obtaining classification scores from the image- and
text-based models separately and then joining them. Similarly to our multimodal mixing
step, Pham, Maillot, Lim, and Chevallet (2007) and Caicedo, Ben-Abdallah, Gonz√°lez, and
Nasraoui (2012) propose an early fusion in which the two inputs are mapped onto the same
latent space using dimensionality reduction techniques (e.g., Singular Value Decomposition).
The multimodal representation obtained in this way is then directly used to retrieve image
documents.
6. http://images.google.com/

11

Bruni, Tran & Baroni

3. A Framework for Multimodal Distributional Semantics
In this section, a general and flexible architecture for multimodal semantics is presented.
The architecture makes use of distributional semantic models based on textual and visual
information to build a multimodal representation of meaning. To merge the two sources, it
uses a parameter-based pipeline which is able to capture previously proposed combination
strategies, with the advantage of having all of them explored within a single system.
3.1 Input of the Multimodal Architecture
To construct a multimodal representation of meaning, a semantic model for each single
modality has to be implemented. Independently of the actual parameters that are chosen for
its creation (that, from our point of view, can be in a black box), there are some requirements
that each model has to satisfy in order to guarantee a good functioning of the framework.
In the first place, each modality must provide a separate representation, to leave room for
the various fusion strategies afterwards. Then, each modality must encode the semantic
information pertaining to each word of interest into a fixed-size vectorial representation.
Moreover, we assume that both text- and image-based vectors are normalized and arranged
in matrices where words are rows and co-occurring elements are columns.
In what follows, we assume that we harvested a matrix of text-based semantic vectors,
and one of image-based semantic vectors for the same set of target words, representing,
respectively, verbal and visual information about the words. In Section 4 below we give the
details of how in our specific implementation we construct these matrices.
3.2 Multimodal Fusion
The pipeline is based on two main steps:
(1) Latent Multimodal Mixing: The text and vision matrices are concatenated, obtaining a single matrix whose row vectors are projected onto a single, common space
to make them interact.
(2) Multimodal Similarity Estimation: Information in the text- and image-based
matrices is combined in two ways to obtain similarity estimates for pairs of target
words: at the Feature Level and at the Scoring Level.
Figure 2 describes the infrastructure we propose for fusion. First, we introduce a mixing
phase to promote the interaction between modalities that we call Latent Multimodal Mixing.
While this step is part of what other approaches would consider Feature Level fusion (see
below), we keep it separated as it might benefit the Scoring Level fusion as well.
Once the mixing is performed, we proceed to integrate the textual and visual features.
As reviewed in Section 2.4 above, in the literature fusion is performed at two main levels,
the Feature Level and the Scoring Level. In the first case features are first combined and
considered as a single input for operations, in the second case a task is performed separately
with different sets of features and the separate results are then combined. Each approach
has its own advantages and limitations and this is why both of them are incorporated into
the multimodal infrastructure and together constitute what we call Multimodal Similarity
12

Multimodal Distributional Semantics



	



	

	











	





	





	




Figure 2: Multimodal fusion for combining textual and visual information in a semantic
model.



Estimation. A Feature Level approach requires only one learning step (i.e., determining the
parameters of the feature vector combination) and offers a richer vector-based representation
of the combined information, that can also be used for other purposes (e.g., image and text
features could be used together to train a classifier). Benefits of a Scoring Level approach
include the possibility to have different representations (in principle, not even vectorial) and
different similarity scores for different modalities and the ease of increasing (or decreasing)
the number of different modalities used in the representation.
3.2.1 Latent Multimodal Mixing
This is a preparatory step in which the textual and the visual components are projected
onto a common representation of lower dimensionality to discover correlated latent factors.
The result is that new connections are made in each source matrix taking into account
information and connections present in the other matrix, originating from patterns of covariance that overlap. Importantly, we assume that mixing is done via a dimensionality
reduction technique that has the following characteristics: a parameter k that determines
13

Bruni, Tran & Baroni

the dimensionality of the reduced space and the fact that when k equals the rank of the
original matrix the reduced matrix is identical or can be considered a good approximation
of the original one. The commonly used Singular Value Decomposition reduction method
that we adopt here for the mixing step satisfies these constraints.
As a toy example of why mixing might be beneficial, consider the concepts pizza and
coin, that we could use as features in our text-based semantic vectors (i.e., record the cooccurrences of target words with these concepts as part of the vector dimensions). While
these words are not likely to occur in similar contexts in text, they are obviously visually similar. So, the original text features pizza and coin might not be highly correlated.
However, after mixing in multimodal space, they might both be associated with (have high
weights on) the same reduced space component, if they both have similar distributions to
visual features that cue roundness. Consequently, two textual features that were originally
uncorrelated might be drawn closer to each other by multimodal mixing, if the corresponding concepts are visually similar, resulting in mixed textual features that are, in a sense,
visually enriched, and vice versa for mixed visual features (interestingly, psychologists have
shown that, under certain conditions, words such as pizza and coin, that are not strongly
associated but perceptually similar, can prime each other; e.g., Pecher, Zeelenberg, & Raaijmakers, 1998).
Note that the matrices obtained by splitting the reduced-rank matrix back into the
original textual and visual blocks have the same number of feature columns as the original
textual and visual blocks, but the values in them have been smoothed by dimensionality
reduction (we explain the details of how this is achieved in our specific implementation in
the next paragraph). These matrices are then used to calculate a similarity score for a word
pair by (re-)merging information at the feature and scoring levels.
Mixing with SVD In our implementation, we perform mixing across text- and imagebased features by applying the Singular Value Decomposition (SVD)7 to the matrix obtained by concatenating the two feature types row-wise (so that each row of the concatenated
matrix describes a target word in textual and visual space). SVD is a widely used technique
to find the best approximation of the original data points in a space of lower underlying
dimensionality whose basis vectors (‚Äúprincipal components‚Äù or ‚Äúlatent dimensions‚Äù) are
selected to capture as much of the variance in the original space as possible (Manning,
Raghavan, & Sch√ºtze, 2008, Ch. 18). By performing SVD on the concatenated textual and
visual matrices, we project the two types of information into the same space, where they
are described as linear combinations of principal components. Following the description by
Pham et al. (2007), the SVD of a matrix M of rank r is a factorization of the form
M = U Œ£V t
where

Ô£±
Ô£¥
U : matrix of eigenvectors derived from M M t
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤Œ£ : r √ó r diagonal matrix of singular values œÉ
Ô£¥
œÉ : square roots of the eigenvalues of M M t
Ô£¥
Ô£¥
Ô£¥
Ô£¥
t
Ô£≥ t

V : matrix of eigenvectors derived from M M

7. Computed with SVDLIBC: http://tedlab.mit.edu/~dr/SVDLIBC/

14

Multimodal Distributional Semantics

In our context, the matrix M is given by normalizing two feature matrices separately and
then concatenating. By selecting the k largest values from matrix Œ£ and keeping the
corresponding columns in matrices U and V , the reduced matrix Mk is given by
Mk = Uk Œ£k Vkt
where k < r is the dimensionality of the latent space. While Mk keeps the same number
of columns/dimensions as M , its rank is now k. k is a free parameter that we tune on the
development sets. Note that when k equals the rank of the original matrix, then trivially
Mk = M . Thus we can consider not performing any SVD reduction as a special case of
SVD, which helps when searching for the optimal parameters.
Note also that, if M has n columns, then Vkt is a k √ó n matrix, so that Mk has the same
number of columns of M . If the first j columns of M contain textual features, and columns
from j + 1 to n contain visual features, the same will hold for Mk , although in the latter
the values of the features will have been affected by global SVD smoothing. Thus, in the
current implementation of the pipeline in Figure 2, block splitting is attained simply by
dividing Mk into a textual mixed matrix containing its first j columns, and a visual mixed
matrix containing the remaining columns.
3.2.2 Multimodal Similarity Estimation
Similarity Function Following the distributional hypothesis, DSMs describe a word in
terms of the contexts in which it occurs. Therefore, to measure the similarity of two words
DSMs need a function capable of determining the similarity of two such descriptions (i.e.,
of two semantic vectors). In the literature, there are many different similarity functions
used to compare two semantic vectors, including cosine similarity, Euclidean distance, L1
norm, Jaccard‚Äôs coefficient, Jensen-Shannon divergence, Lin‚Äôs similarity. For an extensive
evaluation of different similarity measures, see the work by Weeds (2003).
Here we focus on cosine similarity since it has been shown to be a very effective measure
on many semantic benchmarks (Bullinaria & Levy, 2007; Pad√≥ & Lapata, 2007). Also,
given that our system is based on geometric principles, the cosine, together with Euclidean
distance, is the most principled choice to measure similarity. For example, some of the
measures listed above, having been developed from probabilistic considerations, will only be
applicable to vectors that encode well-formed probability distributions, which is typically
not the case (for example, after multimodal mixing, our vectors might contain negative
values).
The cosine of two semantic vectors a and b is their dot product divided by the product
of their lengths:
Pi=n

i=1 ai √ó bi
qP
i=n 2
i=n 2
a
√ó
i=1 i
i=1 bi

cos(a, b) = qP

The cosine ranges from 0 (orthogonal vectors) to |1| (parallel vectors pointing in the
same or opposite directions have cosine values of 1 and -1, respectively).
15

Bruni, Tran & Baroni

Feature Level Fusion In Feature Level fusion (FL), we use the linear weighted fusion
method to combine text- and image-based feature vectors of words into a single representation and then we use the latter to estimate the similarity of pairs. The linear weighted
combination function is defined as
F = Œ± √ó Ft ‚äï (1 ‚àí Œ±) √ó Fv
where ‚äï is the vector-concatenate operator.
Scoring Level Fusion In Scoring Level fusion (SL), text- and image-based matrices are
used to estimate similarity of pairs independently. The scores are then combined to obtain
the final estimate by using a linear weighted scoring function:
S = Œ≤ √ó St + (1 ‚àí Œ≤) √ó Sv
General Form and Special Cases Given fixed and normalized text- and image-based
matrices, our multimodal approach is parametrized by k (dimensionality of latent space),
FL vs. SL, Œ± (weight of text component in FL similarity estimation) and Œ≤ (weight of text
component in SL).
Note that when k=r, with r the rank of the original combined matrix, Latent Multimodal
Mixing returns the original combined matrix (no actual mixing). Picking SL with Œ≤=1 or
Œ≤=0 corresponds to using the textual or visual matrix only, respectively. We thus derive as
special cases the models in which only text (k=r, SL, Œ≤=1) or only images (k=r, SL, Œ≤=0) are
used (called Text and Image models in the Results section below). The simple approach
of Bruni et al. (2011), in which the two matrices are concatenated without mixing, is the
parametrization k=r, FL, Œ±=0.5 (called NaiveFL model, below). The summing approach
of Leong and Mihalcea (2011) corresponds to k=r, SL, Œ≤=0.5 (NaiveSL, below). Picking
k<r, SL, Œ≤ =1 amounts to performing latent multimodal mixing, but then using textual
features only; and the reverse with mixed image features only for Œ≤ = 0 (Textmixed and
Imagemixed , respectively). Reducing these and other models to the same parametrized
approach means that, given a development set for a specific task that requires similarity
measurements, we can discover in a data-driven way which of the various models is best
for the task at hand (for example, for a certain task we might discover that we are better
off using text only, for another mixed text features, for yet another both text and image
features, and so on).
Formally, given the set k1 , ..., kn ‚àà R of n dimensionalities of the latent space (with kn
equal to the original dimensionality, and arbitrary steps between the chosen values), the
sets Œ±1 , ..., Œ±m ‚àà R of m potential weights of the text block in FL (with Œ±1 = 0 and Œ±m = 1)
and Œ≤1 , ..., Œ≤l ‚àà R of l weights of the text block in SL (with Œ≤1 = 0 and Œ≤l = 1), we can
calculate the number of possible configurations to explore by totc = n(m + l). Unless n, m
and l are very large (i.e., we consider very small intervals between the values to be tested),
it is completely feasible to perform a full search for the best parameters for a certain task
without approximate optimization methods. In our experiments, n = 9, m = l = 11, and
consequently totc = 198.
16

Multimodal Distributional Semantics

4. Implementation Details
Both our implementation of the multimodal framework and of the visual feature extraction
procedure are publicly available and open source.8 Moreover the visual feature extraction
procedure is presented by Bruni, Bordignon, Liska, Uijlings, and Sergienya (2013).
4.1 Construction of the Text-Based Semantic Matrix
As reviewed in Section 2.1 above, a text-based distributional model is encoded in a matrix whose rows are ‚Äúsemantic vectors‚Äù representing the meaning of a set of target words.
Important parameters of the model are the choice of target and contextual elements,
the source corpora used to extract co-occurrence information, the context delimiting the
scope of co-occurrence, and the function to transform raw counts into statistical association scores downplaying the impact of very frequent elements.
Source Corpora We collect co-occurrence counts from the concatenation of two corpora,
ukWaC and Wackypedia (size: 1.9B and 820M running words, or tokens, respectively).
ukWaC is a collection of Web pages based on a linguistically-controlled crawl of the .uk
domain conducted in the mid 2000s. Wackypedia was built from a mid-2009 dump of the
English Wikipedia. Both corpora have been automatically annotated with lemma (dictionary form) and part-of-speech (POS) category information using the TreeTagger,9 they are
freely and publicly available,10 and they are widely used in linguistic research.
Target and Context Elements Since our source corpora are annotated with lemma and
part-of-speech information, we take both into account when extracting target and context
words (e.g., the string sang is treated as an instance of the verb lemma sing). We collect
semantic vectors for a set of 30K target words (lemmas), namely the top 20K most frequent
nouns, 5K most frequent adjectives and 5K most frequent verbs in the combined corpora.
The same 30K lemmas are also employed as contextual elements (consequently, our textbased semantic models are encoded in a 30K√ó30K matrix). Note that when we combine
the text matrices with the image-based ones, we preserve only those rows (target words)
for which we also have an image-based vector, trimming the matrix to size 20,525√ó30K.
Context We define context in terms of words that co-occur within a window of fixed
width, in the tradition of the popular HAL model (Lund & Burgess, 1996). Window-based
models are attractive for their simplicity and the fact that they do not require resourceintensive advanced linguistic annotation. They have moreover been reported to be at the
state of the art in various semantic tasks (Rapp, 2003; Sahlgren, 2008), and in Bruni,
Uijlings, Baroni, and Sebe (2012) we show that the window-based methods we use here
outperform both a document-as-context model and a sophisticated syntax- and lexicalpattern-based model on the MEN and WordSim test sets introduced in Section 5.2 below
(see also the post-hoc analysis using the document-based model discussed at the end of
Section 5.2.2 below). We consider two variants, Window2 and Window20 (we chose these
particular variants arbitrarily, as representatives of narrow and wide windows, respectively).
8. See https://github.com/s2m/FUSE/ and https://github.com/vsem/, respectively.
9. http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
10. http://wacky.sslmit.unibo.it/

17

Bruni, Tran & Baroni

Window2 records sentence-internal co-occurrence with the nearest 2 content words to the left
and right of each target word (function words such as articles and prepositions are ignored).
Window20 considers a larger window of 20 content words to the left and right of the target.
A narrower window is expected to capture a narrower kind of semantic similarity, such as the
one that exists between terms that are closely taxonomically related, for example coordinate
concepts (dog and cat) or pairs of superordinate and subordinate concepts (animal and
dog). The rationale behind this expectation is that terms will share many narrow-window
collocates only if they are very similar, both semantically and syntactically. On the other
hand, a broader window will capture a broader kind of ‚Äútopical‚Äù similarity, such as one
would expect of words that tend to occur in the same paragraphs (for example, war and oil,
that are rather distant concepts in a taxonomic sense, but might easily occur in the same
discourse). See the work by Sahlgren (2006) for further discussion of the effects of context
width on distributional semantic models.
Association Score We transform raw co-occurrence counts into nonnegative Local Mutual Information (LMI) association scores. LMI scores are obtained by multiplying raw
counts by Pointwise Mutual Information, and in the nonnegative case they are a close approximation to Log-Likelihood Ratio scores, that are one of the most widely used weighting
schemes in computational linguistics (Evert, 2005). The nonnegative LMI of target element
t and context element c is defined as:


P(t, c)
,0
P(t)P(c)



LM I(t, c) = max Count(t, c) √ó log

It is worth observing that, in an extensive study of how parameters affect the quality of
semantic vectors, Bullinaria and Levy (2007) and Bullinaria and Levy (2012) found that
a model similar to our Window2 (co-occurrence statistics from ukWaC, narrow window,
lemmatized content word collocates, nonnegative pointwise mutual information instead of
LMI) performs at or near the top in a variety of semantic tasks. Thus, we have independent
grounds to claim that we are using a state-of-the-art text-based model.
4.2 Construction of the Image-Based Semantic Matrix
Given that image-based semantic vectors are a novelty with respect to text-based ones,
in the next subsections we dedicate more space to how we constructed them, including
full details about the source corpus we utilize as input of our pipeline (Section 4.2.1), the
particular image analysis technique we choose to extract visual collocates and how we finally
arrange them into semantic vectors that constitute the visual block of our distributional
semantic matrix (Section 4.2.2).
4.2.1 Image Source Corpus
We adopt as our source corpus the ESP-Game data set11 that contains 100K images, labeled
through the famous ‚Äúgame with a purpose‚Äù developed by Louis von Ahn, in which two
11. http://www.cs.cmu.edu/~biglou/resources/

18

Multimodal Distributional Semantics

	

	


  
		

  
 
   











   



Figure
3: Samples of images and their tags from the ESP-Game data set

people partnered online must independently and rapidly agree on an appropriate word to
label random selected images. Once a word is entered by both partners in a certain number
of game rounds, that word is added as a tag for that image, and it becomes a taboo term for
next rounds of the game involving the same image, to encourage players to produce more
terms describing the image (Von Ahn, 2006). The tags of images in the data set form a
vocabulary of 20,515 distinct word types. Images have 14 tags on average (4.56 standard
deviation), while a word is a tag for 70 images on average (737.71 standard deviation).
To have the words in the same format as in our text-based models, the tags are lemmatized and POS-tagged. To annotate the words with their parts of speech, we could not
run a POS-tagger, since here words are out of context (i.e., each tag appears alphabetically
within the small list of words labeling the same image and not within the ordinary sentence
required by a POS-tagger). Thus we used a heuristic method, which assigned to the words
in the ESP-Game vocabulary their most frequent tag in our textual corpora.
The ESP-Game corpus is an interesting data set from our point of view since, on the
one hand, it is rather large and we know that the tags it contains are related to the images.
On the other hand, it is not the product of experts labelling representative images, but of a
noisy annotation process of often poor-quality or uninteresting images (e.g., logos) randomly
downloaded from the Web. Thus, analogously to the characteristics of a textual corpus, our
algorithms must be able to exploit large-scale statistical information, while being robust
to noise. While cleaner and more illustrative examples of each concept are available in
carefully constructed databases such as ImageNet (see Section 2.3), noisy tag annotations
19

Bruni, Tran & Baroni

are available on a massive scale on sites such as Flickr12 and Facebook,13 so if we want to
eventually exploit such data it is important that our methods can work on noisy input. A
further advantage of ESP-Game with respect to ImageNet is that its images are associated
not only with concrete noun categories but also with adjectives, verbs and nouns related
to events (e.g., vacation, party, travel, etc). From a more practical point of view, ‚Äúclean‚Äù
data sets such as ImageNet are still relatively small, making experimentation with standard
benchmarks difficult. In concrete, looking at the benchmarks we experiment with, as of mid
2013, ImageNet covers only just about half the pairs in the WordSim353 test set, and less
than 40% of the Almuhareb-Poesio words. While in the future we want to explore to what
extent higher-quality data sources can improve image-based models, this will require larger
databases, or benchmarks relying on a very restricted vocabulary.
The image samples in Figure 6 exemplify different kinds of noise that characterize the
ESP-Game data set. Both on top and bottom left and top right there are images where
the scene is cluttered or partially occluded. The top center image is hardly a good representative of accompanying words such as building, tower(s) or square. Similarly, the center
bottom image is only partially a good illustration of a coin, and certainly not a very good
example of a man! Finally, the bottom right image is useless from a visual feature extraction
perspective.
4.2.2 Image-Based Semantic Vector Construction
We collect co-occurrence counts of target words and image-based contexts by adopting the
BoVW pipeline that, as we already explained in 2.2, is particularly convenient in order to
discretize visual information into ‚Äúvisual collocates‚Äù. We are adopting what is currently
considered a standard implementation of BoVW. In the future, we could explore more
cutting-edge ways to build image-based semantic vectors, such as local linear encoding
(Wang, Yang, Yu, Lv, Huang, & Gong, 2010) or Fisher encoding (Perronnin, Sanchez, &
Mensink, 2010). Chatfield, Lempitsky, Vedaldi, and Zisserman (2011) present a systematic
evaluation of several recent methods.
Our current implementation is composed of the following steps: (i) Extraction of the
local descriptors, that is, vectors of low-level features that encode geometric or other
information about the area around each keypoint, i.e., pixel of interest (here, SIFT descriptors); (ii) Constructing a vector representation of an image by assigning the
local descriptors to clusters corresponding to visual words, and recording their distribution
across these clusters in the vector (this presupposes a preliminary step in which a clustering
algorithm has been applied to the whole image collection or a sample, to determine the visual word vocabulary) (iii) Including some spatial information into the representation with
spatial binning; (iv) Summing visual word occurrences across the list of images associated
with a word label to obtain the co-occurrence counts associated with each word label
and transforming these counts into association scores, analogously to what is done in text
analysis. The process (without spatial binning) is schematically illustrated in Figure 4, for
a hypothetical example in which there are three images in the collection labeled with the
word monkey. More details follow.
12. http://www.flickr.com
13. http://www.facebook.com

20

Multimodal Distributional Semantics

Dense sampling
of pixels of
interest

Mapping SIFT
descriptors to visual
word clusters

Extracting
local
descriptors

SIFT 4x4

monkey:

0

4

monkey

3

4

0

3

+

Labeled
images

monkey:

2

11

Instance
counts

+

monkey
monkey:

0

3

9

0

monkey:

2

18

12

7

monkey
Total
counts

Figure 4: The procedure to build an image-based semantic vector for a target word. First,
a bag-of-visual-word representation for each image labeled with the target word
is computed (in this case, three images are labeled with the target word monkey).
Then, the visual word occurrences across instance counts are summed to obtain
the co-occurrence counts associated with the target word.

Local Descriptors To construct the local descriptors of pixels of interest we use ScaleInvariant Feature Transform (SIFT) (Lowe, 1999, 2004). We chose SIFT for its invariance
to image scale, orientation, noise, distortion and partial invariance to illumination changes.
A SIFT vector is formed by measuring the local image gradients in the region around each
location and orientation of the feature at multiple scales. In particular, the contents of
4 √ó 4 sampling subregions are explored around each keypoint. For each of the resulting
16 samples, the magnitude of the gradients at 8 orientations are calculated, which would
already result in a SIFT feature vector of 128 components. However, we extract color
SIFT descriptors in HSV (Hue, Saturation and Value) space (Bosch, Zisserman, & Munoz,
2008). We use HSV because it encodes color information in a similar way to how humans
21

Bruni, Tran & Baroni

do. We compute SIFT descriptors for each HSV component. This gives 3√ó128 dimensions
per descriptor, 128 per channel. Color channels are then averaged to obtain the final
128-dimensional descriptors. We experimented also with different color scales, such as
LUV, LAB and RGB, obtaining significantly worse performance compared to HSV on our
development set introduced in 5.2.1, therefore we do not conduct further experiments with
them. Van de Sande, Gevers, and Snoek (2010) present a systematic evaluation of color
features.
Instead of searching for interesting keypoints with a salient patch detection algorithm,
we use a more computationally intensive but also more thorough dense keypoint sampling
approach, with patches of fixed size and localized on a regular grid covering the whole image
and repeated over multiple scales. SIFT descriptors are computed on a regular grid every
five pixels, at four scales (10, 15, 20, 25 pixel radii) and zeroing the low contrast descriptors.
For their extraction we use the vl_phow command included in the VLFeat toolbox (Vedaldi
& Fulkerson, 2010). This implementation has been shown to be very close to Lowe‚Äôs original
but it is much faster for dense feature extraction. Nowak, Jurie, and Triggs (2006) report
a systematic evaluation of different patch sampling strategies.
Importantly, SIFT feature vectors are extracted from a large corpus of representative
images to populate a feature space, which subsequently is quantized into a discrete number of
visual words by clustering. Once this step is performed, every SIFT vector (local descriptor)
from the original or new images can be translated into a visual word by determining which
cluster it is nearest to in the quantized space.
Visual Vocabulary To map SIFT descriptors to visual words, we first cluster all local
descriptors extracted from all images in a training image corpus in their 3√ó128-dimensional
space using the k-means clustering algorithm, and encode each descriptor by the index of the
cluster (visual word) to which it belongs. k-means is the most common way of constructing
visual vocabularies (Grauman & Leibe, 2011). Given a set x1 , ..., xn ‚àà RD of n training
descriptors, k-means aims to partition the n descriptors into k sets (k ‚â§ n) so as to minimize
P
the cumulative approximation error ni=1 ||xi ‚àí ¬µqi ||2 , with K centroids ¬µ1 , ..., ¬µK ‚àà RD
and data-to-means assignments q1 , ..., qN ‚àà {1, ..., K}. We use an approximated version of
k-means called Lloyd‚Äôs algorithm (1982) as implemented in the VLFeat toolbox.
To construct our visual vocabulary we extracted SIFT descriptors from all the 100K
images of the ESP-Game data set. To tune the parameter k we used the MEN development
set (see Section 5.2.1). By varying k between 500 and 5000 in steps of 500, we found the
optimal k being 5000. It is most likely that the performance has not peaked even at 5000
visual words and enhancements could be attained by adopting larger visual vocabularies
via more efficient implementations of the BoVW pipeline, as for example by Chatfield et al.
(2011).
Image Representation Given a set of descriptors x1 , ..., xn sampled from an image, let
qi be the assignment of each descriptor xi to its corresponding visual word. The bag-ofvisual-words representation of an image is a nonnegative vector v ‚àà Rk such that vk = |{i :
qi = k}|, with q ranging from 1 to the number of visual words in the vocabulary (in our
case, 5000). This representation is a vector of visual words obtained via hard quantization
(i.e., assignment of each local descriptor vector to the single nearest codeword).
22

Multimodal Distributional Semantics

Spatial Binning A consolidated way of introducing weak geometry in BoVW is the use
of spatial histograms (Grauman & Darrell, 2005; Lazebnik, Schmid, & Ponce, 2006). The
main idea is to divide the image in several (spatial) regions and to perform the entire visual
word extraction and counting pipeline for each region and then concatenate the vectors. In
our experiments the spatial regions are obtained by dividing the image in 4 √ó 4, for a total
of 16 regions. Therefore, crossing the values for k with the spatial region, we increase the
feature dimensions 16 times, for a total of 80,000 components in our vectors.
Co-occurrence Counts and Weighting Once the BoVW representations are built,
each target (textual) word is associated to the list of images which are labeled with it; the
visual word occurrences across the list of images is summed to obtain the co-occurrence
counts associated with the target (textual) word. In total, 20,515 target words (those that
constitute ESP-Game tags) have an image-based semantic vector associated.
Also in the image-based semantic matrix, like in the text-based one, raw counts are
transformed into nonnegative LMI. The difference is that here LMI is computed between a
target element t that is a textual word and a context element c that is a visual word instead.
Note that, just like in the standard textual approach, we are accumulating visual words
from all images that contain a word without taking into account the fact that words might
denote concepts with multiple appearances, can be polysemous or even hide homonyms
(our bank vector will include visual words extracted from river as well as building pictures).
An interesting direction for further research would be to cluster the images associated to a
word in order to distinguish the ‚Äúvisual senses‚Äù of the word, e.g., along the lines of what
was done for textual models by Reisinger and Mooney (2010).
4.3 Multimodal Fusion Tuning
We performed two separate parameter optimizations, one specifically for the semantic relatedness task (using MEN development, see Section 5.2.1) and the other specifically for
the clustering task (using Battig, see Section 5.3.1). We determined the best model by
performing an exhaustive search across SVD k (from 24 to 212 in powers of 2), FL and SL
with Œ± varying from 0 to 1 (inclusive) in steps of 0.1 and similarly for Œ≤. In total, 198 models were explored and the one with the highest performance on the development data was
chosen. Note that tuning was performed separately for the Window2 and Window20 models.

4.4 MixLDA
To reimplement Feng and Lapata‚Äôs approach (discussed in Section 2.3) in a comparable
setting to ours, we treat the ESP-Game data set as a mixed-media corpus where each
image together with the associated tags constitutes a document. For each image, we extract
the image-based features with the procedure described above in 4.2.2 and use the words
labeling that image to obtain the text-based features. These features are then stored in a
term-by-document matrix, in which each image is treated as a document and a term can be
either a textual tag or a visual word extracted from that image. We obtain a matrix of size
90K√ó100K, with 10K textual words (the word list resulting from the intersection of all the
words used in our experimental data sets), 80K visual words and 100K documents (images).
23

Bruni, Tran & Baroni

The Latent Dirichlet Allocation (MixLDA) model is trained on this matrix and tuned on
the MEN development set by varying the number of topics Kt .14 The optimal value we find
is Kt = 128. Under MixLDA, each target word in an evaluation set is represented by the
vector giving its distribution over the 128 latent topics.

5. Experiments
We test our semantic representation in three different tasks, that is, evaluating the distribution of different kinds of semantic relations among a word‚Äôs neighbours (5.1), modeling
word relatedness judgments (5.2) and clustering words into superordinate concepts (5.3).
Together, these tasks should give us a clear idea of the general quality of our models and
of the relative contribution of visual information to meaning representation.
5.1 Differentiation Between Semantic Relations
To acquire a qualitative insight into how well our text- and image-based models are capturing word meaning, we test them on BLESS (Baroni-Lenci Evaluation of Semantic Similarity), a benchmark recently introduced by Baroni and Lenci (2011) to analyze specific
aspects of lexico-semantic knowledge. Rather than focusing on a point estimate of quality
of a model on a specific semantic task, BLESS allows us to assess the overall pattern of
semantic relations that the model tends to capture. We run the BLESS evaluation before
combining the textual and the visual channels together as a sanity check on the semantic
meaningfulness of the image-based vectors, looking for potential complementary information with respect to text which can further motivate fusion. Note that since we are not
combining the textual and visual sources, there are no tuning parameters to report.
5.1.1 Benchmark and Method
BLESS contains a set of 200 pivot words denoting concrete concepts (we use 184 pivots,
since for the remaining 16 we do not have a sufficiently large set of related words covered
by our models). For each of the pivots, the data set contains a number of related words, or
relata, instantiating the following 8 common semantic relations with the pivots: coord:
the relatum is a noun that is a co-hyponym (coordinate) of the pivot (alligator-lizard);
hyper: the relatum is a noun that is a hypernym (superordinate) of the pivot (alligatorreptile); mero: the relatum is a noun referring to a meronym, that is, a part or material
of the pivot (alligator-teeth); attri: the relatum is an adjective expressing an attribute
of the pivot (alligator-ferocious); event: the relatum is a verb referring to an action or
event involving the concept (alligator-swim); ran.n, ran.j and ran.v, finally, are control
cases where the pivot is matched to a set of random nouns (alligator-trombone), adjectives
(alligator-electronic) and verbs (alligator-conclude), respectively.
For each pivot, BLESS contains a set of relata of each category (ranging from 7 hypernyms to 33 random nouns per pivot on average). In this way, BLESS can highlight the
broader semantic properties of a model independently of its more specific preferences. For
example, both a model that assigns a high score to alligator-ferocious and a model that
assigns a high score to alligator-green will be correctly treated as models that have picked
14. LDA was computed with Gensim: http://radimrehurek.com/gensim/

24

Multimodal Distributional Semantics

a relevant attribute of alligators. At the same time, the comparison of the specific relata
selected by the models allows a more granular qualitative analysis of their differences.
Following the guidelines of Baroni and Lenci (2011), we analyze a semantic model as
follows. We compute the cosine between the model vectors representing each of the 184
pivots and each of its relata, picking the relatum with the highest cosine for each of the
8 relations (the nearest hypernym, the nearest random noun, etc.). We then transform
the 8 similarity scores collected in this way for each pivot onto standardized z scores (to
get rid of pivot-specific effects), and produce a boxplot summarizing the distribution of
scores per relation across the 184 pivots (for example, the leftmost box in the first panel
of Figure 5 reports the distribution of 184 standardized cosines of nearest coordinate relata
with the respective pivots). Besides analyzing the distributions qualitatively, we also discuss
significant differences between the cosines of different relation types that were obtained via
Tukey‚Äôs Honestly Significance tests, thus correcting for multiple pairwise comparisons (Abdi
& Williams, 2010).
5.1.2 Results
In Fig. 5, we report BLESS nearest relata distributions for the purely textual model Window20 (the Window2 distribution shows an even stronger skew in favour of coordinate
neighbours) and the purely visual model we call Image in the next sections. The patterns
produced by the text-based model (left panel) illustrate how a sensible word meaning profile
should look like: coordinates are the most similar terms (an alligator is maximally similar
to a crocodile), followed by superordinates (reptile) and parts (teeth). Semantically related
adjectives (attri: ferocious) and verbs (event: swim) are less close to the pivots, but still
more so than any random item.
The right panel shows the distribution of relata in the image-based semantic vectors.
The overall pattern is quite similar to the one observed with the text-based vectors: there
is a clear preference for coordinates, followed by hypernyms and parts, then attributes
and events, with all random relata further away from the pivots than the semantically
meaningful categories. For both models, coordinates are significantly closer to the relata
than hypernyms and meronyms, that are significantly closer than attributes and events,
that are in turn significantly closer than any random category. Although the difference
between hypernyms and parts is not significant with either representation, intriguingly the
image-based vectors show a slight preference for the more imageable parts (teeth) than
the more abstract hypernyms (reptile). The only difference of statistical import is the one
between events and attributes, where the text-based model shows a significant preference
for events, whereas the two categories are statistically indistinguishable in the image-based
model (as we will see shortly, the relative preference of the latter for attributes is probably
due to its tendency to pick perceptual adjectives denoting color and size).
Looking more closely at the specific relata picked by the text- and image-based models,
the most striking differences pertain, again, to attributes. The text- and image-based
models picked the same attribute for a pivot in just 20% of the cases (compare to 40%
overlap across all non-random relation types). Table 1 reports the attributes picked by the
text- vs. image-based models for 20 random cases where the two mismatch.
25

Bruni, Tran & Baroni

Image-based semantic vectors

-2

-2

-1

-1

0

0

1

1

2

2

Text-based semantic vectors

COORD

HYPER

MERO

ATTRI

EVENT

RAN.N

RAN.J

RAN.V

COORD

HYPER

MERO

ATTRI

EVENT

RAN.N

RAN.J

RAN.V

Figure 5: Distribution of z-normalized cosines of words instantiating various relations across
BLESS pivots. Text-based vectors from the Window20 model.

pivot
cabbage
carrot
cherry
deer
dishwasher
elephant
glider
gorilla
hat
hatchet

text
leafy
fresh
ripe
wild
electric
wild
heavy
wild
white
sharp

image
white
orange
red
brown
white
white
white
black
old
short

pivot
helicopter
onion
oven
plum
sofa
sparrow
stove
tanker
toaster
trout

text
heavy
fresh
electric
juicy
comfortable
wild
electric
heavy
electric
fresh

image
old
white
new
red
old
little
hot
grey
new
old

Table 1: Attributes preferred by text- (Window20) vs. image-based models.

26

Multimodal Distributional Semantics

It is immediately clear from the table that, despite the fact that the pivots are nouns
denoting concrete concepts, the text-based model almost never picks adjectives denoting
salient perceptual properties (and in particular visual properties: just white for hat and leafy
for cabbage). The text-based model focuses instead on encyclopedic properties such as fresh,
ripe, wild, electric and comfortable. This is in line with earlier analyses of the ‚Äúungrounded‚Äù
semantics provided by text-based models (Andrews et al., 2009; Baroni et al., 2010; Baroni
& Lenci, 2008; Riordan & Jones, 2011), and differs greatly from the trend found in the
image-based model. In 12/20 cases, the closest attribute for the latter model is a color. In
the remaining cases, we have size (short, little), one instance of hot and, surprisingly, four
of old.
To conclude, the analysis we presented confirms, on the one hand, our hypothesis that
image-based distributional vectors contain sufficient information to capture a network of
sensible word meaning relations. On the other, there are intriguing differences in the relations picked by the text- and image-based models, pointing to their complementarity.
5.2 Word Relatedness
As is standard in the distributional semantics literature (Budanitsky & Hirst, 2006; Sahlgren,
2006), we assess the performance of our models on the task of predicting the degree of semantic relatedness between two words as rated by human judges. We test the models on
the WS and MEN benchmarks.
5.2.1 Benchmarks and Method
WS, that is, WordSim35315 (see also Section 2.1) is a widely used benchmark constructed by
asking 13 subjects to rate a set of 353 word pairs on an 11-point meaning similarity scale and
averaging their ratings (e.g., dollar/buck gets a very high average rating, professor/cucumber
a very low one). Our target words cover 252 WS pairs (thus, the correlations reported below
are not directly comparable to those reported in other studies that used WS). However, our
text-based models have much higher WS coverage (96%). When evaluated on the larger WS
set they cover, Window2 and Window20 achieve 0.64 and 0.68 correlations, respectively.
We are thus comparing the multimodal approach with purely textual models that are at
the state of the art for WS (see results reported in Section 2.1 above).
The second benchmark we use, MEN (for Marco, Elia and Nam, the resource creators)
was developed by us, specifically for the purpose of testing multimodal models. We created
a large data set that, while comparable to WS and other benchmarks commonly used
by the computational semantics community, contains only words that appear as image
labels in the ESP-Game and MIRFLICKR-1M16 collections, thus ensuring full coverage to
researchers that train visual models from these resources. MEN consists of 3,000 word pairs
with [0, 1]-normalized semantic relatedness ratings provided by Amazon Mechanical Turk
workers (via the CrowdFlower17 interface). For example, beach/sand has a MEN score of
0.96, bakery/zebra received a 0 score.
15. http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/
16. http://press.liacs.nl/mirflickr/
17. http://crowdflower.com/

27

Bruni, Tran & Baroni

Compared to WS, MEN is sufficiently large to allow us to separate development and
test data, avoiding issues of overfitting. We use indeed 2,000 MEN pairs (development set)
for model tuning and 1,000 pairs for evaluation (test set). Importantly, the development
set has been used to find the best configuration once for both the MEN test set and WS.
Thus, the WS evaluation illustrates how well the parameters learned on training data from
a specific data set generalize when applied to the same semantic task but on a different
data set.
Models are evaluated as follows. For each pair in a data set, we compute the cosine
of the model vectors representing the words in the pair, and then calculate the Spearman
correlation of these cosines with the (pooled) human ratings of the same pairs, the idea
being that the higher the correlation the better the model can simulate the relatedness
scores.
MEN Construction An earlier version of MEN has been used for the first time by the
authors by Bruni et al. (2012) but since the current article is the first major publication
in which we focus specifically on it, and we have recently improved the benchmark by
extending the ratings, we provide here further details on how it was constructed.
The word pairs that constitute MEN were randomly selected from words that occur at
least 700 times in the concatenated ukWaC and Wackypedia text corpora and at least 50
times as tags in the ESP-Game and MIRFLICKR-1M tagged image collections. In order
to avoid picking only pairs that were weakly related, as would happen if we were to sample
random word pairs from a list, we ranked all possible pairs by their cosines according to our
text-based model Window20. To gather the 3000 word pairs needed for the construction
of MEN, we subsequently picked the first 1000 word pairs, another 1000 was sampled from
pairs placed between 1001 and 3000 in the cosine-ranked list and the last block of 1000 pairs
from the remaining items.
To acquire human semantic relatedness judgments, we decided to ask for comparative
judgments on two pair exemplars at a time rather than absolute scores for single pairs, as
was done by the creators of WS. This should constitute a more natural way to evaluate the
target pairs, since human judgments are comparative in nature. When a person evaluates
a given target, she does not do so in a vacuum, but in relation with a certain context.
Moreover, binary choices were preferred because they make the construction of ‚Äúright‚Äù and
‚Äúwrong‚Äù control items straightforward (see Footnote 18). Operationally, each word pair
was randomly matched with a comparison pair coming from the same set of 3000 items
and rated by a single Turker as either more or less related than the comparison item. The
validity of this approach is confirmed by the high annotation accuracy we observe in the
control set,18 and by the high correlation of the MEN scores with ratings collected on a
Likert scale we report below.
18. The control items are correct annotations created prior to running the job on Amazon Mechanical Turk,
which act as hidden tests that are randomly shown to Turkers as they complete the job. In this way,
we can calculate the quality of a contributor‚Äôs performance and reject their annotations if the accuracy
drops below a certain percentage (we set a required minimum precision equal to 70%, but we obtained
almost 100% average accuracy overall). Control items are also of great help to train quickly new workers
to perform the required task. To create our control items we harvested two equally-sized sets of word
pairs from WS, one containing only pairs with a high relatedness score, one containing only pairs with
a low relatedness score. Each control item was then obtained by juxtaposing a high score pair with a
low score pair and by treating the pair with the higher score as the one that should be selected by the

28

Multimodal Distributional Semantics

In the instructions, annotators were warned that sometimes both candidate pairs could
contain words related in meaning and in such cases we asked them to pick the pair with the
more strongly related words (e.g., both wheels-car and dog-race are somewhat related pairs,
but the first one should be preferred as every car has wheels but not every dog is involved
in a race). In other cases, annotators could find that neither pair contains closely related
words, and in such cases they were instructed to pick the pair that contained slightly more
related words (e.g., neither auction-car nor cup-asphalt are closely related words, but the
first pair should be picked because fancy vintage cars are sold at auctions). We requested
participants to be native speakers and only accepted those connecting from an English
speaking country. We cannot guarantee that non-natives did not take part in the study,
but our subject filtering techniques based on control pairs (see Footnote 18) ensures that
only the data of speakers with a good command of English were retained.
To transform binary preference data to relatedness scores about the retrieved pairs, each
of them was evaluated against 50 randomly picked comparison pairs, thus it received a score
on a 50-point scale (given by the number of times out of 50 the pair was picked as the most
related of the two). The score was subsequently normalized between 0 and 1 by dividing
the number of times the pair was picked as the most related by 50. For example, fun-night
was chosen as more related than the comparison pair 20 times, thus its normalized score
is given by 20 √∑ 50 = 0.4. Note that, in each comparison, we only recorded the preference
assigned to one of the two pairs, to avoid dependencies between the final scores assigned
to different pairs (that is, the times a pair was selected as a random comparison item for
another pair were not counted as ratings of that pair).
Because raters saw the MEN pairs matched to different random items, with the number
of pairs also varying from rater to rater, it is not possible to compute annotator agreement
scores for MEN. However, to get a sense of human agreement, the first and third author
rated all 3,000 pairs (presented in different random orders) on a standard 1-7 Likert scale.
The Spearman correlation of the two authors is at 0.68, the correlation of their average
ratings with the MEN scores is at 0.84. On the one hand, this high correlation suggests
that MEN contains meaningful semantic ratings. On the other, it can also be taken as an
upper bound on what computational models can realistically achieve when simulating the
human MEN judgments.
The high-score MEN pairs include not only pairs of terms that are strictly taxonomically close (cathedral-church: 0.94) but also terms that are connected by broader semantic
relations, such as whole-part (flower-petal: 0.92), item and related event (boat-fishing: 0.9),
etc. For this reason, we prefer to refer to MEN as a semantic relatedness rather than
similarity score data set. Note that WS is also capturing a broader notion of relatedness (Agirre et al., 2009). MEN is publicly available and it can be downloaded from:
http://clic.cimec.unitn.it/~elia.bruni/MEN.
5.2.2 Results
Table 2 reports the correlations on the MEN testing and WS data sets when using either
Window2 or Window20 as textual model. Our automated tuning method selected k = 29
annotators as the most related. All control items were manually checked. Examples of control items are
hotel-word vs. psychology-depression, telephone-communication vs. face-locomotive.

29

Bruni, Tran & Baroni

Model
Text
Image
NaiveFL
NaiveSL
MixLDA
Textmixed
Imagemixed
TunedFL
TunedSL

Window2
MEN WS
0.73
0.70
0.43
0.36
0.75
0.67
0.76
0.69
0.30
0.23
0.77 0.73
0.55
0.52
0.78 0.72
0.78 0.71

Window20
MEN WS
0.68
0.70
0.43
0.36
0.73
0.67
0.74
0.64
0.30
0.23
0.74 0.75
0.57
0.51
0.76 0.75
0.77 0.72

Table 2: Spearman correlation of the models on MEN and WordSim (all coefficients significant with p < 0.001). TunedFL is the model selected automatically on the MEN
development data; TunedSL is automatically tuned after fixing SL similarity estimation.

(when textual information comes from Window2) and k = 210 (with Window20) as optimal,
and Feature Level (FL) similarity estimation with Œ± = 0.5 in both cases (since the input
matrices are row-normalized, the latter setting assigns equal weights to the textual and
visual components). These are the models called TunedFL in the table. The Scoring Level
(SL) strategy (again with similar weights assigned to the two channels, and same k values
as TunedFL) performed only slightly worse than TunedFL, and we report the results for the
best SL-based models as tuned on the development MEN data as well (TunedSL). In all other
models reported in the table (NaiveFL, NaiveSL, MixLDA, Textmixed and Imagemixed ),
some parameters were tuned manually in order to gain insights on combination strategies
representing ideas from the earlier literature.19
The first two rows of the table show results of the text- and image-based models, before
any mixing. Text shows comparable performances on both data sets. Image correlates
significantly better with MEN than WS but the correlations are lower than those of Text,
in accordance with what was found in earlier studies. In the next three rows we find the
results of the earlier multimodal approaches we took into consideration (Bruni et al., 2011;
Feng & Lapata, 2010; Leong & Mihalcea, 2011). While the NaiveFL approach (analogous
to Bruni et al.‚Äôs method), in which textual and visual matrices are concatenated without
mixing, performs slightly better than Text on MEN, it attains lower performance on WS.
Also NaiveSL (equivalent to Leong and Mihalcea‚Äôs summing approach), where text and
image sources are combined at the scoring level, obtains improvements only on MEN, loosing
several correlation points on WS compared to Text.
Our implementation of MixLDA achieves very poor results both on MEN and WS. One
might attribute this to the fact that Feng and Lapata‚Äôs approach is constrained to using the
same source for the textual and the visual model and our image data set is a poor source
19. For Textmixed and Imagemixed , the best k values were found on the development data. They were both
set to 210 with both textual sources.

30

Multimodal Distributional Semantics

Textmixed
TunedFL
TunedSL

Window2
0.47
0.46
0.46

Window20
0.49
0.49
0.47

Table 3: Pearson correlation of some of our best multimodal combinations on the WordSim
subset covered by Feng and Lapata (2010) (all coefficients significant with p <
0.001; Pearson used instead of Spearman for full comparability with Feng and
Lapata). The models assigned 0 similarity to the 71/253 pairs for which they were
missing a vector. Feng and Lapata (2010) report 0.32 correlation for MixLDA.

of textual data. Our approach is however also outperforming the original MixLDA by a
large margin on the latter WS test set, where we are strongly disfavoured. In particular,
Feng and Lapata (2010) report a correlation of 0.32 for the subset of 253 WS pairs covered
by their model. We tested our system on the same subset, despite the fact that we are
missing one or both vectors for 71 of the pairs (almost one third), so that our models are
forced to assign 0 cosines to all these cases. Despite this huge handicap, our models are
still attaining much higher correlations than the original MixLDA on the Feng and Lapata
pairs, as illustrated for the most interesting fusion strategies in Table 3.
Analyzing now the effects of our fusion strategies, we can first see a uniform enhancement on both MEN and WS for Textmixed and Imagemixed (the models obtained by first
performing latent multimodal mixing on the combined matrix, but then using textual features only for Textmixed and visual features only for Imagemixed ). Textmixed reaches the best
performance overall on WS with both source textual models, and it is significantly better
than Text on MEN according to a two-tailed paired permutation test (Moore & McCabe,
2005). Looking then at the automatically selected TunedFL model, it reaches the best performance overall. Not only it significantly outperforms Text models on both data sets, but
it is significantly better than Textmixed on MEN with Window20 (the difference is approaching significance with Window2 as well: p = 0.06). TunedSL is also very competitive. It is
also significantly better than Text with both window sizes and Textmixed for Window20. It
is noticeably worse than TunedFL on WS with Window20 only, and it is actually having a
slight advantage on MEN with Window20 (the difference between TunedFL and TunedSL
is never significant).
It is worth remarking that while Textmixed is a bit worse than the full fusion models,
it still achieves high correlations with the human judgments and it has an extremely high
correlation with the TunedFL best model (œÅ = 0.98). This suggests that most of the
benefits of multimodality are already captured by latent mixing. Textmixed is an attractive
model because it has less parameters than the whole pipeline and it is more compact than
TunedFL, since it discards the visual features after using them for mixing.
Validating the Results While we have shown significant improvements when visual features are added to distributional models, one could object that improvements are due to the
fact that we are using more information: a larger number of features (higher-dimensional
31

Bruni, Tran & Baroni

vectors) for Feature Level fusion, and a more complex model (two similarity scores as
independent variables to predict human judgments) for Scoring Level fusion. Further experiments provide evidence to respond to this objection.
First, we built purely textual models with the same number of features as our multimodal
models ‚Äì that is, instead of collecting co-occurrence of the target terms with the 30K most
frequent content lemmas in our corpus (see Section 4.1 above), we extended the list of
context items to the 110K most frequent content lemmas. The results with this larger
textual models were virtually identical to those with 30K-dimensional vectors reported in
Table 2 (correlation for the Window20 model on MEN was 0.69 instead of 0.68). Thus, at
least when using our large corpus and a window-based approach, with 30K features we have
pretty much exhausted the useful textual information, and it‚Äôs the nature, not simply the
quantity of the extra visual features we add that matters.
To answer the objection that the Scoring Level approach is using a more complex model,
with two independent variables (text- and image-base similarities) instead of one, we casted
the problem in standard inferential statistical terms (see, e.g, Baayen, 2008, ch. 6). Specifically, we fitted ordinary linear regression models to predict the MEN and WS ratings with
only text-based similarities vs. text- and image-based similarities (for comparability with
the Spearman correlation results reported above, the analyses were also replicated after
transforming ratings and similarities into ranks). Both variables were highly significant in
all experiments, and, more importantly, sequential F-tests over the nested models revealed
that in all cases adding image-based similarities explains significantly more variance than
what would be expected by chance given the extra parameter (p < 0.01).
Qualitative Analysis To acquire qualitative insights into how multimodality is contributing to meaning representation, we first picked the top 200 most related pairs from the
combined MEN and WS norms, so that we would be confident that they are indeed highly
related pairs for humans, and then we looked, within this subset, at those pairs with the
most pronounced difference in cosines between Text and TunedFL, using Window20 as our
textual source. That is, the first column of Table 4 presents pairs that are considered very
related by humans and where relatedness was better captured by Text, the second column
pairs where relatedness was better captured by TunedFL.
Notice that 7/10 of the relations better captured by TunedFL are between coordinates
or synonyms pertaining to concrete objects (candy/chocolate, bicycle/bike, apple/cherry,
military/soldier, paws/whiskers, stream/waterfall and cheetah/lion), that should indeed be
maximally visually similar (either the objects themselves or, in a case such as paws/whiskers,
their surrounds). The purely text-based model, on the other hand, captures relations
between times of the day, that, while imageable, are not well-delimited concrete objects
(dawn/dusk, sunrise/sunset). It captures properties of concepts expressed by adjectives
(dog/canine, skyscraper/tall, cat/feline, pregnancy/pregnant, rain/misty), and at least one
case where spotting the relation requires encyclopedic knowledge (grape/wine). We thus
hypothesize that the added value of the multimodally-enhanced model derives from the
power of vision in finding relations between concrete objects at the same taxonomic level,
that results in detecting particularly ‚Äútight‚Äù forms of relatedness, such as synonymy and
coordination.
32

Multimodal Distributional Semantics

Text
dawn/dusk
sunrise/sunset
canine/dog
grape/wine
foliage/plant
foliage/petal
skyscraper/tall
cat/feline
pregnancy/pregnant
misty/rain

TunedFL
pet/puppy
candy/chocolate
paw/pet
bicycle/bike
apple/cherry
copper/metal
military/soldier
paws/whiskers
stream/waterfall
cheetah/lion

Table 4: Top 10 pairs whose relatedness is better captured by Text (Window20)
vs. TunedFL.

As observed by one reviewer, given the taxonomic nature of the information captured by
the multimodal approach, it will be interesting to compare it in future work with features
directly extracted from a linguistic taxonomy, such as WordNet. We observe in passing that
such a manually-constructed resource, unlike those extracted from textual corpora, is likely
to reflect both the linguistic and the perceptual knowledge of the lexicographers who built
it.
Going in the opposite direction, another reviewer observed that we might get more
mileage by combining visual features with textual models that are less taxonomic in nature.
This hypothesis is partially confirmed by the fact that we obtain a larger relative improvement by mixing vision with Window20 than with Window2 (look back at Table 2, and see
Section 4.1 above on why we think that the narrower window mainly captures taxonomic
relations, the larger one broader topical themes). To further explore this conjecture, we
re-ran the MEN and WS experiments combining the visual vectors with a document-based
textual model (i.e., a semantic space whose dimensions record the number of occurrences
of words in documents). Such a space is expected to capture mostly topical information,
as it estimates relatedness on the basis of the tendency of words to occur in the same
documents (Sahlgren, 2006). The document-based model alone was not as good as the
window-based models (it obtained a Spearman correlation of 0.68 on MEN and of 0.63 on
WS), and combining it with image-based models led to relative improvements comparable
or inferior to those attained with Window20 (the best combined-model correlations were
0.73 on MEN and 0.70 on WS). We conclude that, while looking for textual models that
are more complementary with respect to visual information seems a reasonable direction
to develop multimodal systems that cover a broader range of semantic phenomena, simply
emphasizing the topical side of textual models evidently does not suffice.
33

Bruni, Tran & Baroni

5.2.3 The Concreteness Factor in Modeling Relatedness Ratings: A Pilot
Study
In both previous experiments, we have observed a trend towards a ‚Äúdivision of labour‚Äù between text- and image-based models, where the latter are more apt at capturing similarity
among concrete concepts and properties. One of the strongest limitations of the current
version of our framework is the fact that every target word is assumed to be equally perceptually salient and consequently uniformly enriched with visual information. Intuitively, we
might want to distinguish instead between concrete words, such as chair or cat, that require
an integration of perceptual information for their representation, and abstract words, such
as consequence or absurd, that can be represented on a purely symbolic/linguistic basis.
Indeed, Recchia and Jones (2012) recently presented evidence that, in lexical decision and
naming tasks, rich physical contexts favour the activation of concrete concepts, whereas rich
linguistic contexts facilitate the activation of abstract concepts. With the follow-up pilot
experiment presented in this section we want to pave the way for a systematic introduction of the concreteness factor in multimodal meaning representation. Operationally, we
separate the abstract from the concrete word pairs in our semantic relatedness benchmark
MEN, assessing the contribution of textual and visual information in approximating word
meaning in the two domains independently. Importantly, we use an automated method
to determine if a word is concrete or abstract, with an eye to a future integration of an
automatically-determined abstractness score into our fusion algorithm.
In particular, we use abstractness scores automatically assigned by the algorithm recently introduced by Turney, Neuman, Assaf, and Cohen (2011). Scores are calculated by
computing the difference between the sum of text-based semantic similarities of a target
word with a set of concrete paradigm words and the sum of its semantic similarities with
a set of abstract paradigm words. All words (i.e., both the paradigm words and the words
for which an abstractness score was computed) were represented in a co-occurrence based
matrix gathered from a large corpus of university websites. Co-occurrence counts were then
transformed into Positive Pointwise Mutual Information scores (Church & Hanks, 1990) and
the resulting matrix was smoothed with SVD. Pairwise semantic similarity was measured
by cosines. The paradigm words were in turn selected with a supervised learning method
trained on subject-rated words from the MRC Psycholinguistic Database Machine Usable
Dictionary (Coltheart, 1981). Examples of highly abstract words in the automatically rated
list are purvey: 1.00, sense: 0.96 and improbable: 0.92, while examples of highly concrete
words (i.e., words with a very low abstractness score) are donut: 0.00, bullet: 0.07 and shoe:
0.10.
Once the abstractness score was assigned to all the MEN testing words, we divided
the data set into two subsets, one containing only concrete word pairs (MEN-conc, 837
pairs), the other containing both abstract pairs and mixed pairs, that is pairs formed by one
concrete and one abstract word (MEN-abst, 163 pairs). A word was considered concrete
if its abstract score was ‚â§ 0.5, abstract otherwise. For example, the word pair arm-bicycle
was considered concrete (with scores of 0.33 and 0.35 respectively), fun-relax was considered
abstract (with scores of 0.6 and 0.59 respectively) and design-orange was considered mixed
(with scores of 0.55 and 0.20 respectively). We experimented with Window20 as our purely
34

Multimodal Distributional Semantics

Model
Window20
Image
TunedFL

MEN-conc
0.70
0.47
0.78

MEN-abst
0.51
0.37
0.52

MEN-full
0.68
0.43
0.76

Table 5: Spearman correlation of the models on MEN divided into concrete and abstract
subsets. Results on the full data set are also repeated. All coefficients significant
with p < 0.001.

textual model, Image is our usual visual model and TunedFL trained on MEN development
is our multimodal model.
In Table 5 we show the correlation scores for the three models on the two MEN subsets
(as well as repeating the correlations they attain on the full set). First of all, it is worth
noticing that all models have higher correlations with MEN-conc than MEN-abst, suggesting
that approximating similarity judgments for pairs of concrete pairs is in general an easier
task for distributional semantics (and, we suspect, for humans as well!). Besides this broad
effect, we also observe a clear interaction for the added value of the visual component
between MEN-abst and MEN-conc. In fact, TunedFL gains more than 11% in performance
on MEN-conc compared to Window20, while its performance is essentially the same as that
of the text-only model in the case of MEN-abst. This indicates that visual information is
mostly beneficial in the concrete domain, while it maintains a neutral (timidly positive)
impact on the abstract domain (recall that, in any case, MEN-abst also contains mixed
pairs).
To conclude, in this section we followed up on the qualitative analysis of the main
relatedness results with a pilot experiment focusing on the concreteness factor. We showed
that when we divide the MEN benchmark into concrete and abstract subsets, the visual
information enhances the text-based model only in the concrete domain, where its impact
is very strong. We exploited an automatic scoring function to divide the data set into the
concrete and abstract subsets. We can thus see the results we are reporting here also as
a validation of Turney et al.‚Äôs algorithm, and, more importantly for our purposes, as an
encouragement to incorporate the automated abstractness/concreteness scoring in the way
in which our model mixes textual and visual information on a word-by-word basis.
5.3 Concept Categorization
To verify if the conclusions reached on WS and MEN extend to different semantic tasks and,
in particular, to assess whether our multimodal approach is able to capture and organize
meaning as humans do, we use two existing concept categorization benchmarks that we
call Battig and Almuhareb-Poesio (AP), respectively, where the goal is to cluster a set of
(nominal) concepts into broader categories, as already discussed in Section 2.1.
In particular, we use Battig exclusively for tuning (in the same way we used the MEN
development set in the previous section) and AP for testing. Only results on AP are
reported. While in the word relatedness task the tuning and testing sets were quite similar
35

Bruni, Tran & Baroni

(MEN development and MEN testing are two subsets of the same data set and the words
in WS are similar to those in MEN), here the task is more challenging since Battig and AP
are two independent data sets which were built following different strategies and populated
with different kinds of concepts, namely very concrete and unambiguous concepts for Battig,
vs. a mixture of concrete and abstract, possibly ambiguous concepts in AP. We adopted the
present challenging training and testing regime because we felt that neither data set was of
sufficient size to allow a split between development and testing data. More details follow.
5.3.1 Benchmarks and Method
The Battig benchmark was introduced by Baroni et al. (2010) and it is based on the Battig
and Montague norms of Van Overschelde, Rawson, and Dunlosky (2004). It consists of
83 highly prototypical concepts from 10 common concrete categories (up to 10 concepts
per class). Battig contains basic-level concepts belonging to categories such as bird (eagle,
owl. . . ), kitchenware (bowl, spoon. . . ) or vegetable (broccoli, potato. . . ). In the version we
cover there are 77 concepts from 10 different classes.
AP was introduced by Almuhareb and Poesio (2005) and it is made of 402 nouns from
21 different WordNet classes. In the version we cover, AP contains 231 concepts to be
clustered into 21 classes such as vehicle (airplane, car. . . ), time (aeon, future. . . ) or social
unit (brigade, nation). The data set contains many difficult cases of unusual or ambiguous
instances of a class, such as casuarina and samba as trees.
For both sets, following the original proponents and others, we cluster the words based on
their pairwise cosines in the semantic space defined by a model using the CLUTO toolkit
(Karypis, 2003). We use CLUTO‚Äôs built-in repeated bisections with global optimization
method, accepting all of CLUTO‚Äôs default values. Cluster quality is often evaluated by
percentage purity (Zhao & Karypis, 2003). If nir is the number of items from the i-th true
(gold standard) class that were assigned to the r-th cluster, n the total number of items,
and k the number of clusters, then
purity =

X
1 i=n
max (nri )
n i=1

In words, the number of items belonging to the majority true class (i.e., the most represented
class in the cluster) are summed up across clusters and divided by the total number of items.
In the best scenario purity will be 1 and it will approach 0 as cluster quality deteriorates.
Since we lack full AP coverage, the results we report below are not directly comparable
with other studies that used it. However, our text-based models do have perfect coverage,
and when evaluated on the full set achieve purities of 0.67 (Window2) and 0.61 (Window2),
that are at state-of-the-art levels for comparable models, as reported in Section 2.1 above.
So, again, we can confidently claim that the improvements achieved with multimodality are
obtained by comparing our approach to competitive purely textual models.
5.3.2 Results
Table 6 reports percentage purities in the AP clustering task. Also here the best automatically selected model (TunedFL) uses FL similarity estimation as in the previous task, and
has similar SVD k (27 for Window2 and 29 for Window20) and Œ± (0.5) parameters to the
36

Multimodal Distributional Semantics

Model
Text
Image
NaiveFL
NaiveSL
MixLDA
Textmixed
Imagemixed
TunedFL
TunedSL

Window2
AP
0.73
0.26
0.74
0.65
0.14
0.74
0.35
0.74
0.75

Window20
AP
0.65
0.26
0.64
0.66
0.14
0.67
0.29
0.69
0.69

Table 6: Percentage purities of the models on AP. TunedFL is the model automatically
selected on the Battig data; TunedSL is automatically tuned after fixing SL similarity estimation.

ones found for relatedness, suggesting that this particular parameter choice is robust and
could be used out-of-the-box in other tasks as well. TunedSL is the best SL-based method
on the tuning Battig set (same ks as TunedFL, Œ± = 0.5 for Window20 but Œ± = 0.9 on
Window2).
Analogously to the previous semantic task, we see that the Image model alone is not
at the level of the text models, although its AP purities are significantly above chance
(p < 0.05 based on simulated distributions for random cluster assignment). Thus, we have
a further confirmation of the fact that image-based vectors do capture important aspects
of meaning. As in the previous task, MixLDA achieves very poor results.
Looking at the text-based models enhanced with visual information, we can see a general
improvement in performance in almost all the multimodal combination strategies, except
for NaiveFL with Window20 and NaiveSL with Window2. Even if Textmixed benefits from
visual smoothing in both cases, it is again outperformed by TunedFL, whose performance is
here very similar to that of TunedSL, that actually is slightly better on Window2. Interestingly, TunedSL outperforms Text on Window2 despite the fact this is the single combination
strongly unbalanced towards textual similarity (Œ± = 0.9), indicating that visual information can be beneficial even when textual information accounts for the lion‚Äôs share of the
composed estimate.
Like in the relatedness task, adding an equal amount of further textual features instead
of image-based ones does not help with Window20 (0.66 purity with 110K textual features)
and even lowers performance with Window2 (0.69 purity). Thus, the improvement brought
about by visual features must be attributed to their quality, not just quantity.
According to a two-tailed permutation test, even the largest difference between TunedFL
and Text on Window20 is not significant. This might be due to the brittleness of the purity
statistics leading to high variance in the permutations, and possibly to suboptimal tuning.
Recall, in this respect, that the tuning phase was performed on a rather different data
set (Battig) compared to the data set on which we eventually evaluated the models (AP).
37

Bruni, Tran & Baroni

However, the overall trends are very encouraging, and in line with what we found in the
relatedness study.

6. Conclusion
In this paper we have provided an extensive introduction to a new approach to distributional semantics that we named Multimodal Distributional Semantics. A multimodal
distributional semantic model integrates a traditional text-based representation of meaning
with information coming from vision. In this way, it tries to answer to the critique that distributional models lack grounding, since they base their representation of meaning entirely
on the linguistic input, neglecting statistical information inherent in perceptual experience,
that we humans instead exploit. Of course, a truly multimodal representation of meaning
should account for the entire spectrum of human senses. On the other hand, this line of
research is still in its embryonic stage and there is still a shortage of both perceptual data
available and techniques to automatize their processing. This is why, in this article, we
focused our analysis on the visual perceptual channel, for which we have at our disposal
both large data sets and effective methods to analyze them.
In particular, we exploited the ESP-Game data set, where the image documents are
tagged with words describing their content. To harvest visual information we adopted the
bag-of-visual-words technique, which discretizes image content in ways that are analogous to
standard text-based distributional representations. We introduced a multimodal framework
that optimizes text-image fusion in a data-driven fashion on development data.
We conducted a number of experiments to assess the quality of the obtained models.
We first investigated the general semantic properties of a purely image-based model, to
assess its overall quality as well as to look for information complementary to that present in
text. We found systematic differences between the two modalities, such as the preference for
encyclopedic properties of a text-based model and for perceptual properties in the case of the
image-based model. We proceeded to test a selection of models obtained by the combination
of the text- and image-based representations via our multimodal framework. We used two
benchmarks for word relatedness and one benchmark for word categorization and in both
cases we obtained a systematic improvement in performance with the multimodal models
compared to models based on standalone channels.
Still, by looking at the numerical results, we cannot deny that the improvement in performance attained when including visual information is not dramatic. Indeed, a pessimistic
interpretation of the experiments could be that they confirm the hypothesis by Louwerse
and others (e.g., Louwerse, 2011; Louwerse & Connell, 2011; Tillman, Datla, Hutchinson, &
Louwerse, 2012) that perceptual information is already encoded, to a sufficient degree, into
linguistic data, so direct visual features don‚Äôt bring much to the table. However, we showed
through various statistical and validation tests that our most important result, namely that
adding visual information improves over using text alone, is robust and reliable. We think
a more realistic take-home message is that the experiments we reported, while establishing
the basic result we just mentioned, had some drawbacks we should overcome in further
work.
First of all, we deliberately used general semantic benchmarks and state-of-the-art text
models, so that the performance of computational methods might be getting close to the
38

Multimodal Distributional Semantics

ceiling. At 0.78 correlation, our best models still have a few percentage points to go on
MEN (estimated upper bound based on raters‚Äô agreement: 0.84, see Section 5.2.1), but
the improvements are bound to be quite small. Concerning the AP benchmark, consider
how difficult it would be even for humans to categorize casuarina and samba among the
trees. Indeed, an error analysis of the TunedFL clustering results suggests that factors
that might lead to better performance have little to do with vision. For example, the
model ‚Äúwrongly‚Äù clusters branch (a social unit according to AP) with the trees, and merges
concepts such as melon and peach (fruit in AP) with mandarin and lime (trees). In lack of
further contextual information, it‚Äôs hard to dispute the model choices. Similarly, TunedFL
splits the AP animal class into a cluster of small domestic mammals (cats, dogs, kittens,
mice, puppies and rats) and a cluster containing everything else (mostly larger mammals
such as cows and elephants). Again, the clustering procedure had no information about the
classes we were searching for (e.g., animals in general, and not small animals), and so it is
hard to see how performance could have improved thanks to better semantic features, visual
or of other kinds. Moreover, all data sets include abstract terms, and are not specifically
designed to test the more grounded aspects of meaning, where visual features might help
most. We think it made sense to start our investigation with these general benchmarks of
semantics, as opposed to ad hoc test sets, to show the viability of the multimodal approach.
However, in the future we want to focus on experimental challenges where the strengths of
visually-enhanced models might emerge more clearly. We took a first step in this direction
by Bruni et al. (2012), where we focused specifically on how visual features can help in
processing both literal and metaphorical colours.
Another factor to take into account is that both large-scale image data sets and the
techniques to extract features from them are in their infancy, and we might be able to
improve performance further by developing better image-based models. Regarding the data
sets, we explained in Section 4.2.1 above why we chose ESP-Game, but obviously it is
sub-optimal in many respects, as we also discuss there. Regarding the features, as we
mentioned at the beginning of Section 4.2.2, recent advances in image processing, such as
Fisher encoding, might lead to better ways to extract the information contained in images.
In the experiments, we also compared our automatically tuned multimodal model to
other settings, showing its overall stability and superiority, with two important caveats.
First, in both experiments good results are already obtained by using visual information
to smooth text features, without using the visual features directly (what we called the
Textmixed approach). Note that this is already a multimodal approach, in that visual
information is crucially used to improve the quality of the textual dimensions, and indeed
we‚Äôve seen that it consistently outperforms using non-multimodally-smoothed text features.
While Textmixed is not as good as our full tuned model, its simplicity makes it a very
attractive approach.
Second, although automated tuning led us to prefer Feature Level over Scoring Level
fusion on the development sets, TunedSL was clearly worse than TunedFL in just one case
(with Window20 on WS), suggesting that, at least for the evaluation settings we considered,
the difference between the two fusion strategies is not crucial. However, when comparing
the ‚Äúnaive‚Äù versions of both strategies to the tuned ones across the results, it is clear that
tuning is important to obtain consistently good performance, confirming the usefulness of
our general fusion architecture.
39

Bruni, Tran & Baroni

We also conducted a pilot experiment on the concreteness/abstractness factor, to assess
its impact on meaning representation and to check if it is a good candidate for a new
weighted-fusion strategy we plan to investigate in the future. In fact, in the current version
of the multimodal framework, the parametrization of the combination strategy works at a
global level (i.e, it is the same for all words). It could be more productive to combine textual
and visual information on a word-by-word basis, and tune the two modality contributions
in meaning representation depending on the particular nature of each single word. Concrete
vs. abstract does not constitute a neat binary distinction for all words, but it has to be rather
thought as an ideal distinction to be offset with a less abrupt, real-world formulation, which
takes into account the degree according to which a certain word can be considered concrete
or abstract. There is no doubt that words such as backdrop, squalor or sharp evoke some
perceptual cues gathered from our experience about them, but at the same time there
is an unequivocal ‚Äúamount of abstractness‚Äù accompanying them. We plan also to refine
the concreteness scoring method in order to make it focus specifically on the imageable
components of concreteness, as we expect them to be more relevant to our visual channel.
Further developments will focus on the techniques to extract the image-based semantic
models. For example, in a pilot study (Bruni et al., 2012), we exploit new methods developed
in computer vision to improve object recognition by capturing object location (Felzenszwalb,
Girshick, McAllester, & Deva Ramanan, 2010; de Sande, Uijlings, Gevers, & Smeulders,
2011). We show that it is possible to extract better image-based semantic vectors by first
localizing the objects denoted by words and then extracting visual information from the
object location and from its surround independently. Interestingly, we discovered that
image-based semantic vectors extracted from the object surround are more effective than
those based on the object location when tested on our word relatedness task. For example,
the fact that pictures containing deers and wolves depict similar surrounds tells us that
such creatures live in similar environments, and it is thus likely that they are somewhat
related. This can be seen as the distributional hypothesis transposed to images: objects
that are semantically similar occur in similar visual contexts. Nevertheless, the work has
to be considered a proof of concept, since we experimented with 20 words only. In future
studies we will test a larger number of words.
While there is obviously much room for improvement, and many exciting routes to
explore, we hope that the framework and empirical results we presented in this study
convinced the reader that multimodal distributional semantics is a very promising avenue
to pursue in the development of human-like models of meaning.

Acknowledgments
We thank Jasper Uijlings for his valuable suggestions about the image analysis pipeline. A
lot of code and many ideas came from Giang Binh Tran, and we owe Gemma Boleda many
further ideas and useful comments. Peter Turney kindly shared the abstractness score list
we used in Section 5.2.3 and Yair Neuman generously helped with a preliminary analysis
of the impact of abstractness on our multimodal models. Mirella Lapata kindly made the
WordSim353 set used in the experiments of Feng and Lapata (2010) available to us. We
thank the JAIR associated editor and reviewers for helpful suggestions and constructive
40

Multimodal Distributional Semantics

criticism. Google partially funded this project with a Google Research Award to the third
author. The BLESS study of Section 5.1.2 was first presented by Bruni et al. (2012).

References
Abdi, H., & Williams, L. (2010). Newman-Keuls and Tukey test. In Salkind, N., Frey, B., &
Dougherty, D. (Eds.), Encyclopedia of Research Design, pp. 897‚Äì904. Sage, Thousand
Oaks, CA.
Agirre, E., Alfonseca, E., Hall, K., Kravalova, J., Pas√ßa, M., & Soroa, A. (2009). A study
on similarity and relatedness using distributional and WordNet-based approaches. In
Proceedings of HLT-NAACL, pp. 19‚Äì27, Boulder, CO.
Almuhareb, A., & Poesio, M. (2005). Concept learning and categorization from the web.
In Proceedings of CogSci, pp. 103‚Äì108, Stresa, Italy.
Andrews, M., Vigliocco, G., & Vinson, D. (2009). Integrating experiential and distributional
data to learn semantic representations. Psychological Review, 116 (3), 463‚Äì498.
Baayen, H. (2008). Analyzing Linguistic Data: A Practical Introduction to Statistics using
R. Cambridge University Press, Cambridge, UK.
Barnard, K., Duygulu, P., Forsyth, D., de Freitas, N., Blei, D., & Jordan, M. (2003). Matching words and pictures. Journal of Machine Learning Research, 3, 1107‚Äì1135.
Baroni, M., Barbu, E., Murphy, B., & Poesio, M. (2010). Strudel: A distributional semantic
model based on properties and types. Cognitive Science, 34 (2), 222‚Äì254.
Baroni, M., & Lenci, A. (2008). Concepts and properties in word spaces. Italian Journal
of Linguistics, 20 (1), 55‚Äì88.
Baroni, M., & Lenci, A. (2010). Distributional Memory: A general framework for corpusbased semantics. Computational Linguistics, 36 (4), 673‚Äì721.
Baroni, M., & Lenci, A. (2011). How we BLESSed distributional semantic evaluation. In
Proceedings of the EMNLP GEMS Workshop, pp. 1‚Äì10, Edinburgh, UK.
Barsalou, L. (2008). Grounded cognition. Annual Review of Psychology, 59, 617‚Äì645.
Berg, T., Berg, A., & Shih, J. (2010). Automatic attribute discovery and characterization
from noisy Web data. In ECCV, pp. 663‚Äì676, Crete, Greece.
Bergsma, S., & Goebel, R. (2011). Using visual information to predict lexical preference.
In Proceedings of RANLP, pp. 399‚Äì405, Hissar, Bulgaria.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal of
Machine Learning Research, 3, 993‚Äì1022.
Bosch, A., Zisserman, A., & Munoz, X. (2007). Image classification using random forests
and ferns. In Proceedings of ICCV, pp. 1‚Äì8, Rio de Janeiro, Brazil.
Bosch, A., Zisserman, A., & Munoz, X. (2008). Scene classification using a hybrid generative/discriminative approach. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 30 (4).
Bruni, E., Boleda, G., Baroni, M., & Tran, N. K. (2012). Distributional semantics in
Technicolor. In Proceedings of ACL, pp. 136‚Äì145, Jeju Island, Korea.
41

Bruni, Tran & Baroni

Bruni, E., Bordignon, U., Liska, A., Uijlings, J., & Sergienya, I. (2013). Vsem: An open
library for visual semantics representation. In Proceedings of ACL, Sofia, Bulgaria.
Bruni, E., Tran, G. B., & Baroni, M. (2011). Distributional semantics from text and images.
In Proceedings of the EMNLP GEMS Workshop, pp. 22‚Äì32, Edinburgh, UK.
Bruni, E., Uijlings, J., Baroni, M., & Sebe, N. (2012). Distributional semantics with eyes:
Using image analysis to improve computational representations of word meaning. In
Proceedings of ACM Multimedia, pp. 1219‚Äì1228, Nara, Japan.
Budanitsky, A., & Hirst, G. (2006). Evaluating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32 (1), 13‚Äì47.
Bullinaria, J., & Levy, J. (2007). Extracting semantic representations from word cooccurrence statistics: A computational study. Behavior Research Methods, 39, 510‚Äì
526.
Bullinaria, J., & Levy, J. (2012). Extracting semantic representations from word cooccurrence statistics: Stop-lists, stemming and SVD. Behavior Research Methods,
44, 890‚Äì907.
Burgess, C. (2000). Theory and operational definitions in computational memory models:
A response to Glenberg and Robertson. Journal of Memory and Language, 43 (3),
402‚Äì408.
Caicedo, J., Ben-Abdallah, J., Gonz√°lez, F., & Nasraoui, O. (2012). Multimodal representation, indexing, automated annotation and retrieval of image collections via nonnegative matrix factorization. Neurocomputing, 76 (1), 50‚Äì60.
Chatfield, K., Lempitsky, V., Vedaldi, A., & Zisserman, A. (2011). The devil is in the
details: an evaluation of recent feature encoding methods. In Proceedings of BMVC,
Dundee, UK.
Church, K., & Hanks, P. (1990). Word association norms, mutual information, and lexicography. Computational Linguistics, 16 (1), 22‚Äì29.
Clark, S. (2013). Vector space models of lexical meaning. In Lappin, S., & Fox, C. (Eds.),
Handbook of Contemporary Semantics, 2nd ed. Blackwell, Malden, MA. In press.
Coltheart, M. (1981). The MRC psycholinguistic database. Quarterly Journal of Experimental Psychology, 33.
Connolly, A., Gleitman, L., & Thompson-Schill, S. (2007). Effect of congenital blindness on
the semantic representation of some everyday concepts. Proceedings of the National
Academy of Sciences, 104 (20), 8241‚Äì8246.
Csurka, G., Dance, C., Fan, L., Willamowski, J., & Bray, C. (2004). Visual categorization
with bags of keypoints. In In Workshop on Statistical Learning in Computer Vision,
ECCV, pp. 1‚Äì22, Prague, Czech Republic.
Curran, J., & Moens, M. (2002). Improvements in automatic thesaurus extraction. In
Proceedings of the ACL Workshop on Unsupervised Lexical Acquisition, pp. 59‚Äì66,
Philadelphia, PA.
42

Multimodal Distributional Semantics

de Sande, K. V., Uijlings, J., Gevers, T., & Smeulders, A. (2011). Segmentation as selective
search for object recognition. In Proceedings of ICCV, pp. 1879‚Äì1886, Barcelona,
Spain.
de Vega, M., Glenberg, A., & Graesser, A. (Eds.). (2008). Symbols and Embodiment: Debates
on Meaning and Cognition. Oxford University Press, Oxford, UK.
Deng, J., Dong, W., Socher, R., Li, L.-J., & Fei-Fei, L. (2009). Imagenet: A large-scale
hierarchical image database. In Proceedings of CVPR, pp. 248‚Äì255, Miami Beach,
FL.
Dumais, S. (2003). Data-driven approaches to information access. Cognitive Science, 27,
491‚Äì524.
Erk, K. (2012). Vector space models of word meaning and phrase meaning: A survey..
Language and Linguistics Compass, 6 (10), 635‚Äì653.
Escalante, H. J., H√©rnadez, C. A., Sucar, L. E., & Montes, M. (2008). Late fusion of heterogeneous methods for multimedia image retrieval. In Proceedings of ICMR, Vancouver,
Canada.
Evert, S. (2005). The Statistics of Word Cooccurrences. Dissertation, Stuttgart University.
Farhadi, A., Hejrati, M., Sadeghi, M. A., Young, P., Rashtchian, C., Hockenmaier, J., &
Forsyth, D. (2010). Every picture tells a story: Generating sentences from images. In
Proceedings of ECCV, Crete, Greece.
Felzenszwalb, P., Girshick, R., McAllester, D., & Deva Ramanan, D. (2010). Object detection with discriminatively trained part based models. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 32, 1627‚Äì1645.
Feng, Y., & Lapata, M. (2010). Visual information in semantic representation. In Proceedings of HLT-NAACL, pp. 91‚Äì99, Los Angeles, CA.
Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G., & Ruppin,
E. (2002). Placing search in context: The concept revisited. ACM Transactions on
Information Systems, 20 (1), 116‚Äì131.
Firth, J. R. (1957). Papers in Linguistics, 1934-1951. Oxford University Press, Oxford,
UK.
Fodor, J. (1975). The Language of Thought. Crowell Press, New York.
Glenberg, A., & Robertson, D. (2000). Symbol grounding and meaning: A comparison of
high-dimensional and embodied theories of meaning. Journal of Memory and Language, 3 (43), 379‚Äì401.
Grauman, K., & Darrell, T. (2005). The pyramid match kernel: Discriminative classification
with sets of image features. In Proceedings of ICCV, pp. 1458‚Äì1465, Beijing, China.
Grauman, K., & Leibe, B. (2011). Visual Object Recognition. Morgan & Claypool, San
Francisco.
Grefenstette, G. (1994). Explorations in Automatic Thesaurus Discovery. Kluwer, Boston,
MA.
43

Bruni, Tran & Baroni

Griffin, L., Wahab, H., & Newell, A. (2013). Distributional learning of appearance. PLoS
ONE, 8 (2). Published online: http://www.plosone.org/article/info:doi/10.
1371/journal.pone.0058074.
Griffiths, T., Steyvers, M., & Tenenbaum, J. (2007). Topics in semantic representation.
Psychological Review, 114, 211‚Äì244.
Hansen, T., Olkkonen, M., Walter, S., & Gegenfurtner, K. (2006). Memory modulates color
appearance. Nature Neuroscience, 9, 1367‚Äì1368.
Harnad, S. (1990). The symbol grounding problem. Physica D: Nonlinear Phenomena,
42 (1-3), 335‚Äì346.
Harris, Z. (1954). Distributional structure. Word, 10 (2-3), 1456‚Äì1162.
Johns, B., & Jones, M. (2012). Perceptual inference through global lexical similarity. Topics
in Cognitive Science, 4 (1), 103‚Äì120.
Karypis, G. (2003). CLUTO: A clustering toolkit. Tech. rep. 02-017, University of Minnesota
Department of Computer Science.
Kaschak, M., Madden, C., Therriault, D., Yaxley, R., Aveyard, M., Blanchard, A., & Zwaan,
R. (2005). Perception of motion affects language processing. Cognition, 94, B79‚ÄìB89.
Kievit-Kylar, B., & Jones, M. (2011). The Semantic Pictionary project. In Proceedings of
CogSci, pp. 2229‚Äì2234, Austin, TX.
Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A. C., & Berg, T. L. (2011).
Baby talk: Understanding and generating simple image descriptions. In Proceedings
of CVPR, Colorado Springs, MSA.
Landauer, T., & Dumais, S. (1997). A solution to Plato‚Äôs problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological
Review, 104 (2), 211‚Äì240.
Lazebnik, S., Schmid, C., & Ponce, J. (2006). Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories. In Proceedings of CVPR, pp. 2169‚Äì
2178, Washington, DC.
Leong, C. W., & Mihalcea, R. (2011). Going beyond text: A hybrid image-text approach
for measuring word relatedness. In Proceedings of IJCNLP, pp. 1403‚Äì1407.
Lloyd, S. (1982). Least squares quantization in PCM. IEEE Transactions on Information
Theory, 28, 129‚Äì137.
Louwerse, M. (2011). Symbol interdependency in symbolic and embodied cognition. Topics
in Cognitive Science, 3, 273‚Äì302.
Louwerse, M., & Connell, L. (2011). A taste of words: Linguistic context and perceptual
simulation predict the modality of words. Cognitive Science, 35, 381‚Äì398.
Lowe, D. (1999). Object recognition from local scale-invariant features. In Proceedings of
ICCV, pp. 1150‚Äì1157.
Lowe, D. (2004). Distinctive image features from scale-invariant keypoints. International
Journal of Computer Vision, 60 (2).
44

Multimodal Distributional Semantics

Lowe, W. (2001). Towards a theory of semantic space. In Proceedings of CogSci, pp. 576‚Äì581,
Edinburgh, UK.
Lund, K., & Burgess, C. (1996). Producing high-dimensional semantic spaces from lexical
co-occurrence. Behavior Research Methods, 28, 203‚Äì208.
Manning, C., Raghavan, P., & Sch√ºtze, H. (2008). Introduction to Information Retrieval.
Cambridge University Press, Cambridge, UK.
Manning, C., & Sch√ºtze, H. (1999). Foundations of Statistical Natural Language Processing.
MIT Press, Cambridge, MA.
McDonald, S., & Brew, C. (2004). A distributional model of semantic context effects in
lexical processing. In Proceedings of ACL, pp. 17‚Äì24, Barcelona, Spain.
McRae, K., Cree, G., Seidenberg, M., & McNorgan, C. (2005). Semantic feature production
norms for a large set of living and nonliving things. Behavior Research Methods, 37 (4),
547‚Äì559.
Miller, G., & Charles, W. (1991). Contextual correlates of semantic similarity. Language
and Cognitive Processes, 6 (1), 1‚Äì28.
Moore, D., & McCabe, G. (2005). Introduction to the Practice of Statistics (5 edition).
Freeman, New York.
Murphy, G. (2002). The Big Book of Concepts. MIT Press, Cambridge, MA.
Nelson, D., McEvoy, C., & Schreiber, T. (1998). The University of South Florida word association, rhyme, and word fragment norms. http://www.usf.edu/FreeAssociation/.
Nister, D., & Stewenius, H. (2006). Scalable recognition with a vocabulary tree. In Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition - Volume 2, CVPR ‚Äô06, pp. 2161‚Äì2168.
Nowak, E., Jurie, F., & Triggs, B. (2006). Sampling strategies for bag-of-features image
classification. In Proceedings of ECCV, pp. 490‚Äì503, Graz, Austria.
Pad√≥, S., & Lapata, M. (2007). Dependency-based construction of semantic space models.
Computational Linguistics, 33 (2), 161‚Äì199.
Pad√≥, U., Pad√≥, S., & Erk, K. (2007). Flexible, corpus-based modelling of human plausibility
judgements. In Proceedings of EMNLP, pp. 400‚Äì409, Prague, Czech Republic.
Pecher, D., Zeelenberg, R., & Raaijmakers, J. (1998). Does pizza prime coin? Perceptual
priming in lexical decision and pronunciation. Journal of Memory and Language, 38,
401‚Äì418.
Perronnin, F., Sanchez, J., & Mensink, T. (2010). Improving the fisher kernel for large-scale
image classification. In Proceedings of ECCV, pp. 143‚Äì156, Berlin, Heidelberg.
Pham, T.-T., Maillot, N., Lim, J.-H., & Chevallet, J.-P. (2007). Latent semantic fusion
model for image retrieval and annotation. In Proceedings of CIKM, pp. 439‚Äì443,
Lisboa, Portugal.
Poesio, M., & Almuhareb, A. (2005). Identifying concept attributes using a classifier. In
Proceedings of the ACL Workshop on Deep Lexical Semantics, pp. 18‚Äì27, Ann Arbor,
MI.
45

Bruni, Tran & Baroni

Pulvermueller, F. (2005). Brain mechanisms linking language and action. Nature Reviews
Neuroscience, 6, 576‚Äì582.
Radinsky, K., Agichtein, E., Gabrilovich, E., & Markovitch, S. (2011). A word at a time:
computing word relatedness using temporal semantic analysis. In Proceedings of
WWW, pp. 337‚Äì346, Hyderabad, India.
Rapp, R. (2003). Word sense discovery based on sense descriptor dissimilarity. In Proceedings of the 9th MT Summit, pp. 315‚Äì322, New Orleans, LA.
Recchia, G., & Jones, M. (2012). The semantic richness of abstract concepts. Frontiers in
Human Neuroscience, 6 (315).
Reisinger, J., & Mooney, R. J. (2010). Multi-prototype vector-space models of word meaning. In Proceedings of NAACL, pp. 109‚Äì117, Los Angeles, CA.
Riordan, B., & Jones, M. (2011). Redundancy in perceptual and linguistic experience:
Comparing feature-based and distributional models of semantic representation. Topics
in Cognitive Science, 3 (2), 1‚Äì43.
Rothenh√§usler, K., & Sch√ºtze, H. (2009). Unsupervised classification with dependency
based word spaces. In Proceedings of the EACL GEMS Workshop, pp. 17‚Äì24, Athens,
Greece.
Rubenstein, H., & Goodenough, J. (1965). Contextual correlates of synonymy. Communications of the ACM, 8 (10), 627‚Äì633.
Sahlgren, M. (2005). An introduction to random indexing. http://www.sics.se/~mange/
papers/RI_intro.pdf.
Sahlgren, M. (2006). The Word-Space Model. Dissertation, Stockholm University.
Sahlgren, M. (2008). The distributional hypothesis. Italian Journal of Linguistics, 20 (1),
33‚Äì53.
Sch√ºtze, H. (1997). Ambiguity Resolution in Natural Language Learning. CSLI, Stanford,
CA.
Silberer, C., & Lapata, M. (2012). Grounded models of semantic representation. In Proceedings of EMNLP-CoNLL, pp. 1423‚Äì1433, Jeju, Korea.
Sivic, J., & Zisserman, A. (2003). Video Google: A text retrieval approach to object matching in videos. In Proceedings of ICCV, pp. 1470‚Äì1477, Nice, France.
Steyvers, M. (2010). Combining feature norms and text data with topic models. Acta
Psychologica, 133 (3), 234‚Äì243.
Therriault, D., Yaxley, R., & Zwaan, R. (2009). The role of color diagnosticity in object
recognition and representation. Cognitive Processing, 10 (4), 335‚Äì342.
Tillman, R., Datla, V., Hutchinson, S., & Louwerse, M. (2012). From head to toe: Embodiment through statistical linguistic frequencies. In Proceedings of CogSci, pp.
2434‚Äì2439, Austin, TX.
Turney, P., Neuman, Y., Assaf, D., & Cohen, Y. (2011). Literal and metaphorical sense
identification through concrete and abstract context. In Proceedings of EMNLP, pp.
680‚Äì690, Edinburgh, UK.
46

Multimodal Distributional Semantics

Turney, P., & Pantel, P. (2010). From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37, 141‚Äì188.
Van de Sande, K., Gevers, T., & Snoek, C. (2010). Evaluating color descriptors for object and
scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence,
32 (9), 1582‚Äì1596.
Van Overschelde, J., Rawson, K., & Dunlosky, J. (2004). Category norms: An updated and
expanded version of the Battig and Montague (1969) norms. Journal of Memory and
Language, 50, 289‚Äì335.
Vedaldi, A., & Fulkerson, B. (2010). Vlfeat ‚Äì an open and portable library of computer
vision algorithms. In Proceedings of ACM Multimedia, pp. 1469‚Äì1472, Firenze, Italy.
Von Ahn, L. (2006). Games with a purpose. Computer, 29 (6), 92‚Äì94.
Vreeswijk, D. T., Huurnink, B., & Smeulders, A. W. (2011). Text and image subject
classifiers: dense works better. In Proceedings of ACM Multimedia, pp. 1449‚Äì1452,
Scottsdale, AZ.
Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., & Gong, Y. (2010). Locality-constrained
linear coding for image classification. In Proceedings of CVPR, pp. 3360‚Äì3367, San
Francisco, CA.
Weeds, J. (2003). Measures and Applications of Lexical Distributional Similarity. Ph.D.
thesis, Department of Informatics, University of Sussex.
Wittgenstein, L. (1953). Philosophical Investigations. Blackwell, Oxford, UK. Translated
by G.E.M. Anscombe.
Yang, J., Jiang, Y.-G., Hauptmann, A., & Ngo, C.-W. (2007). Evaluating bag-of-visualwords representations in scene classification. In Wang, J. Z., Boujemaa, N., Bimbo,
A. D., & Li, J. (Eds.), Multimedia Information Retrieval, pp. 197‚Äì206. ACM.
Zhao, Y., & Karypis, G. (2003). Criterion functions for document clustering: Experiments
and analysis. Tech. rep. 01-40, University of Minnesota Department of Computer
Science.

47

Journal of ArtiÔ¨Åcial Intelligence Research 49 (2014) 669-703

Submitted 11/13; published 4/14

Improved Separations of Regular Resolution from Clause
Learning Proof Systems
Maria Luisa Bonet

bonet@lsi.upc.edu

Lenguajes y Sistemas InformaÃÅticos,
Universidad PoliteÃÅcnica de CatalunÃÉa,
Barcelona, Spain

Sam Buss

sbuss@math.ucsd.edu

Department of Mathematics,
University of California, San Diego,
La Jolla, CA 92093-0112, USA

Jan Johannsen

jan.johannsen@ifi.lmu.de

Institut fuÃàr Informatik,
Ludwig-Maximilians UniversitaÃàt MuÃànchen,
D-80538 MuÃànchen, Germany

Abstract
This paper studies the relationship between resolution and conÔ¨Çict driven clause learning (CDCL) without restarts, and refutes some conjectured possible separations. We prove
that the guarded, xor-iÔ¨Åed pebbling tautology clauses, which Urquhart proved are hard for
regular resolution, as well as the guarded graph tautology clauses of Alekhnovich, Johannsen, Pitassi, and Urquhart have polynomial size pool resolution refutations that use only
input lemmas as learned clauses. For the latter set of clauses, we extend this to prove that
a CDCL search without restarts can refute these clauses in polynomial time, provided it
makes the right choices for decision literals and clause learning. This holds even if the
CDCL search is required to greedily process conÔ¨Çicts arising from unit propagation. This
refutes the conjecture that the guarded graph tautology clauses or the guarded xor-iÔ¨Åed
pebbling tautology clauses can be used to separate CDCL without restarts from general
resolution. Together with subsequent results by Buss and Kolodziejczyk, this means we lack
any good conjectures about how to establish the exact logical strength of conÔ¨Çict-driven
clause learning without restarts.

1. Introduction
The problem SAT of deciding the satisÔ¨Åability of propositional CNF formulas is of great theoretical and practical interest. Even though SAT is NP-complete, industrial instances with
hundreds of thousands variables are routinely solved by state-of-the-art SAT solvers. Most
of these solvers use conÔ¨Çict-driven clause learning (CDCL) based on the work of MarquesSilva and Sakallah (1999). These CDCL solvers use the DPLL (Davis-Putnam-LogemannLoveland) search procedure with clause learning, extended with additional techniques such
as fast backtracking, restarts, and variable selection heuristics.
c
2014
AI Access Foundation. All rights reserved.

Bonet, Buss, & Johannsen

Without clause learning, the DPLL procedure is equivalent to tree-like resolution. With
the addition of clause learning,1 CDCL becomes considerably more powerful. In fact, CDCL
together with unlimited restarts is capable of polynomially simulating general resolution
proofs (Pipatsrisawat & Darwiche, 2011). Without restarts, CDCL is known to polynomially
simulate regular resolution (Buss, HoÔ¨Ämann, & Johannsen, 2008). Furthermore, general
resolution is known to be strictly stronger than regular resolution (Alekhnovich, Johannsen,
Pitassi, & Urquhart, 2007). However, the exact power of CDCL without restarts is unknown.
This question is interesting not just because CDCL without restarts is a core search method
for most SAT aolvers, but also because a better understanding of the power of CDCL may
lead to a better understanding of the practical performance of SAT solvers.
Alekhnovich et al. (2007) and Urquhart (2011) gave three examples of unsatisÔ¨Åable sets
of clauses that require exponentially longer regular resolution refutations than (general)
resolution refutations. In view of the fact that CDCL without restarts lies between regular resolution and resolution, these three examples were conjecturally good candidates for
showing that CDCL without restarts cannot polynomially simulate general resolution. The
present paper refutes these conjectures for two of these examples; namely, we prove that
CDCL without restarts can give polynomial size refutations of the guarded graph tautologies clauses of Alekhnovich et al. and the guarded xor-iÔ¨Åed pebbling tautologies clauses of
Urquhart, provided the CDCL search makes optimal choices for decision literals and for
learning clauses and forgetting learned clauses. For the former tautology, we further show
that the CDCL search can be required to be greedy and never ignore contradictions that
can be found by unit propagation. It follows that those two tautologies do not give a superpolynomial separation of resolution refutations from CDCL refutations without restarts.
Buss and Kolodziejczyk (2012) subsequently proved a similar result for the Stone tautologies, which Alekhnovich et al. proved give an exponential separation of regular resolution
and resolution. Thus, there are presently no conjectured examples of tautologies that would
provide an exponential separation between the power of CDCL without restarts and the
power of resolution. On the other hand, it looks very diÔ¨Écult to prove that CDCL without
restarts can polynomially simulate resolution. We consequently lack good conjectures about
how to characterize the exact strength of CDCL without restarts.
Beame, Kautz, and Sabharwal (2004) gave the Ô¨Årst theoretical analysis of CDCL. Among
other things, they noted that CDCL with restarts simulates general resolution. Their construction, however, was rather unnatural as it requires the CDCL algorithm to ignore some
contradictions. This situation was rectiÔ¨Åed by Pipatsrisawat and Darwiche (2011), who
showed that CDCL solvers with restarts which use unit propagation and never ignore contradictions can also simulate resolution. Their proof was based on the technique of absorption
that was Ô¨Årst deÔ¨Åned by Atserias, Fichte, and Thurley (2011).
Beame et al. (2004) also studied CDCL without restarts. Using ‚Äúproof trace extensions‚Äù,
they showed that CDCL without restarts is strictly stronger than any ‚Äúnatural‚Äù proof
system strictly weaker than resolution. A natural proof system is one in which proofs do
not increase in length superpolynomially when variables are restricted to constants; natural
1. In this paper, we use ‚ÄúCDCL‚Äù as a synonym for ‚ÄúDPLL with clause learning‚Äù. When we discuss whether
CDCL without restarts can polynomially simulate resolution, we mean whether the simulation is possible
with the correct choices for decision literals and learned clauses.

670

Separations of Regular Resolution

proof systems include systems such as tree-like and regular resolution. The proof trace
method changes the formulas by introducing extraneous variables and clauses, which have
the eÔ¨Äect of giving CDCL more freedom in choosing decision variables for branching.
Buss et al. (2008) and Hertel, Bacchus, Pitassi, and Van Gelder (2008) gave improved
versions of the proof trace extension method so that the extraneous variables depend only
on the set of clauses being refuted and not on the resolution refutation of the clauses. The
drawback remains, however, that the proof trace extension method gives contrived sets of
clauses and contrived resolution refutations, and consequently does not give much insight
into the power of CDCL.
There have been two approaches to formalizing CDCL without restarts as a static proof
system rather than as a proof search algorithm. The Ô¨Årst is pool resolution with a degenerate
resolution inference, due to Van Gelder (2005) and studied further by Hertel et al. (2008).
Pool resolution requires proofs to have a depth-Ô¨Årst regular traversal similarly to the search
space of a DPLL algorithm. Degenerate resolution allows resolution inferences in which one
or both of the hypotheses may be lacking occurrences of the resolution literal. (Detailed
deÔ¨Ånitions are given in Section 2.) Van Gelder argued that pool resolution with degenerate
resolution inferences simulates a wide range of CDCL algorithms without restarts. He
also gave a proof, using techniques of Alekhnovich et al. (2007), that pool resolution with
degenerate inferences is stronger than regular resolution, using extraneous variables similar
to proof trace extensions.
The second approach is due to Buss et al. (2008) who introduced a diÔ¨Äerent degenerate resolution rule called w-resolution, and a proof system, called regWRTI, based on
w-resolution and clause learning of ‚Äúinput lemmas‚Äù. They proved that regWRTI exactly
captures non-greedy CDCL without restarts. As discussed below, ‚Äúnon-greedy‚Äù means that
contradictions may need to be ignored by the CDCL search.
It remains open whether any of CDCL without restarts, pool resolution (with or without
degenerate inferences), or the regWRTI proof system can polynomially simulate general resolution. One approach to answering these questions is to try to separate pool resolution or
regWRTI from general resolution. However, the best so-far obtained separations from resolution apply only to the weaker system of regular resolution, based on work of Alekhnovich
et al. (2007) and Urquhart (2011) giving exponential separations between regular resolution
and general resolution. Alekhnovich et al. proved their exponential separation of regular
resolution and resolution for two families of tautologies, variants of the graph tautology
clauses GT and the ‚ÄúStone‚Äù pebbling tautology clauses. Urquhart subsequently gave a
related separation using a diÔ¨Äerent set of pebbling tautology clauses which he denoted denoted Œ†i .2 The present paper calls the GT clauses the guarded graph tautology clauses, and
denotes them GGT instead of GT ; their deÔ¨Ånition is given in Section 4. Section 3 deÔ¨Ånes
the guarded xor-ified pebbling tautology clauses GPebk‚äï (G), which are essentially the same
as the clauses Œ†i .
An obvious question is whether pool resolution or regWRTI has polynomial size refutations of the GGT, GPebk‚äï , or Stone clauses. The present paper resolves the Ô¨Årst two
questions by showing that both pool resolution and regRTI do indeed have polynomial size
2. Huang and Yu (1987) also gave a separation of regular resolution and general resolution, but only for a
single set of clauses. Goerdt (1993) gave a quasipolynomial separation of regular resolution and general
resolution.

671

Bonet, Buss, & Johannsen

refutations of the GGT and GPebk‚äï clauses. The refutations avoid the use of extraneous
variables in the style of proof trace extensions; furthermore, they use only the traditional
resolution rule and do not require degenerate resolution inferences or w-resolution inferences.
In addition, we use only learning of input clauses; thus, our refutations are also regWRTI
refutations (and in fact regRTI refutations) in the terminology of Buss et al. (2008). As
a corollary of the characterization of regWRTI by Buss et al., GGT and GPebk‚äï have
polynomial size refutations that can be found by CDCL without restarts.
The Stone clauses have recently been shown to also have regRTI refutations by Buss and
Kolodziejczyk (2012); although they use a rather diÔ¨Äerent method than the present paper.
Thus, none of the three principles separate CDCL without restarts from general resolution.
It is natural to speculate that perhaps pool resolution, regWRTI, or CDCL without restarts
can simulate general resolution. However, it is hard to be optimistic that such simulations
exist, as we have been unable to extend our methods or those of Buss and Kolodziejczyk to
give a polynomial simulation of general resolution by CDCL without restarts.
Our results are proved by giving regRTI refutations and then invoking a result of Buss
et al. (2008, Thm. 5.6) which states that a regWRTI refutation of size n of a set Œì of clauses
can be translated into a CDCL refutation that does not use restarts and has runtime polynomially bounded by n. The mentioned theorem of Buss et al. applies to CDCL algorithms
that can learn clauses using the framework of Marques-Silva and Sakallah (1999) (see also
Beame et al., 2004), but does make some important assumptions. The Ô¨Årst assumption is
that the CDCL algorithm makes optimal choices for decision literals and for learned clauses;
the second assumption is that the CDCL algorithm may be non-greedy.
Informally, the fact that the CDCL search must make optimal choices for decision literals
and learned clauses means that Buss et al. (2008) prove the equivalence of regWRTI with
nondeterministic CDCL search without restarts. This assumption of nondeterminism is
probably unavoidable in light of the conditional non-automatizability of resolution proved
by Alekhnovich and Razborov (2001). However, it is a very reasonable assumption for
characterizing CDCL search in terms of a formal proof system. Furthermore, lower bounds
on the sizes of regWRTI refutations will imply lower bounds on the runtimes of CDCL
without restarts.
A CDCL search is called non-greedy if it is allowed to ignore contradictions while continuing to assign further decision literals. Implemented CDCL algorithms are always greedy;
namely, they learn conÔ¨Çicts and backtrack whenever possible; and it is probably a rare
event that non-greedy CDCL algorithms would consistently outperform greedy algorithms,
at least in practical applications. (However, this is an open question.) The upper bounds
for CDCL without restarts obtained via upper bounds on regWRTI refutations potentially
give only non-greedy CDCL searches. For the guarded graph tautologies, however, we prove
more, namely that greedy and unit propagating CDCL without restarts can give polynomial
size refutations of the guarded graph tautology clauses provided it makes optimal choices
for decision literals and for learning and forgetting clauses.
We conjecture that the guarded xor-iÔ¨Åed pebbling tautologies GPebk‚äï can also be refuted by polynomial size greedy and unit propagating CDCL without restarts. Preliminary
investigations reveal no obstacle; however, the technical details are quite involved, and due
to the length of the present paper, we have not carried out the complete construction. Each
tautology seems to require a separate proof. Indeed, it is open whether greedy and unit
672

Separations of Regular Resolution

propagating CDCL without restarts can simulate arbitrary regRTI proofs, or even arbitrary
(dag-like) regular resolution refutations.
The outline of the paper is as follows. Section 2 deÔ¨Ånes resolution, degenerate resolution, and w-resolution, and then regular, tree, and pool resolution. It concludes with the
deÔ¨Ånition of greedy and unit propagating. Section 3 deÔ¨Ånes the GPeb tautologies, including ‚Äúxor-iÔ¨Åcation‚Äù and ‚Äúguarded‚Äù initial clauses. It then proves the existence of polynomial
size pool resolution and regRTI refutations of the guarded xor-iÔ¨Åed GPebk‚äï clauses. The
Ô¨Årst idea of the proof is to try to follow the regular refutations of the unguarded Pebk‚äï
clauses. These refutations cannot be used directly however, since the initial clauses of
Pebk‚äï are ‚Äúguarded‚Äù in the GPebk‚äï clauses and this yields refutations which violate the
regularity/pool property. So, the second idea is that the proof search branches as needed
to learn the initial unguarded Pebk‚äï clauses. This generates additional clauses that must
be proved, and the tricky part is to be sure that exactly the right set of additional clauses
is generated.
Section 4 turns to the graph tautology clauses GTn and their guarded versions, GGTn . It
Ô¨Årst deÔ¨Ånes these clauses and states the main theorems about polynomial size refutations of
the GGTn clauses in pool resolution and regRTI. Section 4.1 deÔ¨Ånes the notion of bipartite
partial order, and discusses the regular refutations of the graph tautology clauses GTn
as given by StaÃälmarck (1996) and Bonet and Galesi (2001). Section 4.2 constructs the
pool/regRTI refutations of the GGTn clauses. The intuition for this construction is similar
to the constructions for the pebbling tautologies, but the technical details are much more
involved. Section 5 concludes with an explicit description of a polynomial time greedy and
unit propagating CDCL search without restarts which refutes the GGTn clauses.
This paper is a reworking and an expansion of an extended abstract (Bonet & Buss,
2012a) and an unpublished preprint (Bonet & Buss, 2012b) by the Ô¨Årst two authors. These
earlier versions included only the results for the GGT tautologies and did not consider the
GPeb principles.

2. Preliminaries
Propositional formulas are deÔ¨Åned over a set of variables and the connectives ‚àß, ‚à® and ¬¨.
We use the notation x to express the negation ¬¨x of x. A literal is either a variable x
or a negated variable x. A clause C is a set of literals, interpreted as the disjunction of
its members. The empty clause, 2, has truth value False. We shall only use formulas in
conjunctive normal form, CNF; namely, a formula will be a set (conjunction) of clauses.
We often use disjunction (‚à®), union (‚à™), and comma (,) interchangeably.
Definition 1. The three forms of resolution deÔ¨Åned below take two clauses A and B called
the premises and a literal x called the resolution variable, and produce a new clause C called
the resolvent.
A

B
C
/ A and x ‚àà
/ B. The diÔ¨Äerent forms of resolution are:
In all cases, it is required that x ‚àà
Resolution rule. The hypotheses have the forms A := A ‚à® x and B := B  ‚à® x. The
resolvent C is A ‚à® B  .
673

Bonet, Buss, & Johannsen

Degenerate resolution rule. (Van Gelder, 2005; Hertel et al., 2008) If x ‚àà A and x ‚àà B, we
apply the resolution rule to obtain C. If A contains x, and B doesn‚Äôt contain x, then
the resolvent C is B. If A doesn‚Äôt contain x, and B contains x, then the resolvent C
is A. If neither A nor B contains the literal x or x, then C is the lesser of A or B
according to some tiebreaking ordering of clauses.
w-resolution rule. (Buss et al., 2008) From A and B as above, we infer the clause C :=
/ A (resp., x ‚àà
/ B), then it is called a phantom
(A \ {x}) ‚à® (B \ {x}). If the literal x ‚àà
literal of A (resp., B).
Degenerate and w-resolution combine weakening with resolution. Of course, it is wellknown that adding weakening to resolution does not increase the refutational strength of
resolution; however, the point of allowing degenerate or w-resolution is that the derivation
may ‚Äúlearn‚Äù some clauses in the parts of the derivation that would have otherwise been
pruned away if weakening were not allowed. For this reason, degenerate and w-resolution
actually correspond better to DPLL search than resolution does.
Definition 2. A resolution derivation, or proof, of a clause C from a CNF formula F is
a sequence of clauses C1 , . . . , Cs such that C = Cs and such that each clause from the
sequence is either a clause from F or is the resolution resolvent of two previous clauses.
If the derived clause, Cs , is the empty clause, this is called a resolution refutation of F .
The more general concepts of degenerate and w-resolution derivations and refutations are
deÔ¨Åned similarly. The size of a proof is the number of clauses in the proof.
We use the terms ‚Äúproof‚Äù and ‚Äúderivation‚Äù interchangeably. A derivation is represented
as a directed acyclic graph (dag) on the vertices C1 , . . . , Cs , where each clause from F has
out-degree 0, and all the other vertices from C1 , . . . , Cs have edges pointing to the two
clauses from which they were derived. The empty clause has in-degree 0.
Resolution is sound and complete in the refutational sense: a CNF formula F has a
refutation if and only if F is unsatisÔ¨Åable. Furthermore, if there is a derivation of a clause C
from F , then C is a consequence of F ; that is, for every truth assignment œÉ, if œÉ satisÔ¨Åes
F then it satisÔ¨Åes C. Conversely, if C is a consequence of F then there is a derivation of
some C  ‚äÜ C from F .
A resolution refutation is regular provided that, along any path in the directed acyclic
graph, each variable is resolved on at most once. A resolution derivation of a clause C
is regular provided that, in addition, no variable appearing in C is used as a resolution
variable in the derivation. A refutation is tree-like if the underlying graph is a tree, so each
occurrence of a clause in the refutation is used at most once as a premise of an inference.
We next deÔ¨Åne a version of pool resolution, using the conventions of Buss et al. (2008)
who called this ‚Äútree-like regular resolution with lemmas‚Äù or ‚ÄúregRTL‚Äù. The idea is that
clauses obtained previously in the proof can be used freely as learned lemmas. To be able
to talk about clauses previously obtained, we need to deÔ¨Åne an ordering of clauses.
Definition 3. Given a tree T , the post-order ordering <T of the nodes is deÔ¨Åned as follows:
if u is a node of T , v is a node in the subtree rooted at the left child of u, and w is a node
in the subtree rooted at the right child of u, then v <T w <T u.
674

Separations of Regular Resolution

Definition 4. A pool resolution proof (also called a regRTL proof) from a set of initial
clauses F is a resolution proof tree T that fulÔ¨Ålls the following conditions: (a) each leaf is
labeled with either a clause of F or a clause (called a ‚Äúlemma‚Äù) that appears earlier in the
tree in the <T ordering; (b) each internal node is labeled with a clause and a literal, and the
clause is obtained by resolution from the clauses labeling the node‚Äôs children by resolving
on the given literal; (c) the proof tree is regular; (d) the root is labeled with the conclusion
clause. If the labeling of the root is the empty clause 2, the pool resolution proof is a pool
refutation.
The notions of degenerate pool resolution proof and pool w-resolution proof are deÔ¨Åned similarly, but allowing degenerate resolution or w-resolution inferences, respectively.
Van Gelder (2005) and Hertel et al. (2008) deÔ¨Åned pool resolution to be the degenerate
pool resolution system, so our notion of pool resolution is more restrictive than theirs. Our
deÔ¨Ånition is equivalent to the one by Buss (2009), however. It is also equivalent to the
system regRTL deÔ¨Åned by Buss et al. (2008). Pool w-resolution is the same as the system
regWRTL of Buss et al. It is open whether the systems of pool resolution, degenerate
pool resolution, and pool w-resolution are distinct. The present paper give examples of
superpolynomial separations between these three systems and regular resolution.
A ‚Äúlemma‚Äù in clause (a) of the above deÔ¨Ånition is called an input lemma if it is derived
by input subderivation, namely by a subderivation in which each inference has at least one
hypothesis which is a member of F or is a lemma. Input resolution is the same as the
‚Äútrivial resolution‚Äù of Beame et al. (2004), who used it to characterize clauses that can be
learned from conÔ¨Çicts found by unit propagation (see also Chang, 1970). The use of input
subderivations for learning clauses in pool resolution proofs is due to Buss et al. (2008). In
their terminology, a pool resolution proof which uses only input lemmas is called a regRTI
proof. Likewise a regWRTL proof that uses only input lemmas is called a regWRTI proof.
To understand the nomenclature; ‚Äúreg‚Äù stands for ‚Äúregular‚Äù, ‚ÄúW‚Äù for ‚Äúw-resolution‚Äù, ‚ÄúRT‚Äù
for ‚Äúresolution tree‚Äù, ‚ÄúL‚Äù for lemma, and ‚ÄúI‚Äù for ‚Äúinput lemma‚Äù.
Based on the deÔ¨Ånition of Van Gelder (2005), C pool is deÔ¨Åned as the ‚Äúpool‚Äù of falsiÔ¨Åed
literals at clause C. The deÔ¨Ånition of the ‚Äúpool‚Äù includes the phantom literals used in
w-resolution inferences:
Definition 5. Let R be a tree-like, regular refutation with lemmas using degenerate resolution, w-resolution, or resolution. Let C be a clause in R. Then, the clause C pool is deÔ¨Åned
to equal
C pool := {x : the literal x occurs, either explicitly or as a phantom literal,
in some clause of R that lies on the branch
from the root node of R up to and including C},
Note that C ‚äÜ C pool , and the regularity of R ensures that C pool contains no contradictory
literals.

3. Guarded, Xor-ified, Pebbling Principles
This section gives polynomial size regRTI refutations for the guarded pebbling tautology
clauses which Urquhart (2011) proved require exponential size regular resolution proofs.
675

Bonet, Buss, & Johannsen

Definition 6. A pointed dag G = (V, E) is a directed acyclic graph with a single sink t
such that every vertex in G has indegree either 0 or 2. The pebbling tautology clauses
Peb(G) for a pointed dag G are the following unsatisÔ¨Åable set of clauses in the variables xv
for v ‚àà V :
(Œ±) xs , for every source s ‚àà V ,
(Œ≤) xu ‚à® xv ‚à® xw , for every vertex w with two (immediate) predecessors u and v,
(Œ≥) xt , for t the sink vertex.
The clauses Peb(G) are Horn clauses and hence have a short tree-like resolution refutation of linear size. However, these clauses can be made diÔ¨Écult to refute by using ‚ÄúoriÔ¨Åcation‚Äù or ‚Äúxor-iÔ¨Åcation‚Äù (see Ben-Sasson, Impagliazzo, & Wigderson, 2004; Ben-Sasson,
2009, and Urquhart, 2011). Here we deÔ¨Åne Urquhart‚Äôs (2011) ‚Äúxor-iÔ¨Åcation‚Äù of a pebbling
tautology clause. Xor-iÔ¨Åcation, for two variables, is due to Alekhnovich and Razborov as
discussed by Ben-Sasson (2009), and is similar to the ‚Äúor-iÔ¨Åcation‚Äù used by Ben-Sasson
et al. (2004) which replaces each variable by the disjunction of two variables. The intuition
for xor-iÔ¨Åcation is that each variable xu is replaced by a set of clauses which expresses the
exclusive or xu,1 ‚äï ¬∑ ¬∑ ¬∑ ‚äï xu,k of k new variables.
Definition 7. Let k > 0, and xu be a variable of Peb(G). Let xu,1 , . . . , xu,k be new
k‚äï
variables, and let x1u,j be xu,j , and x‚àí1
u,j be its complement xu,j . DeÔ¨Åne xu to be the set of
clauses of the form
1
2
k
‚à® xiu,2
‚à® ¬∑ ¬∑ ¬∑ ‚à® xiu,k
(1)
xiu,1
where an even number of the values ij equal ‚àí1 (and the rest equal 1). Dually, deÔ¨Åne xk‚äï
u
to be the set of clauses of the form (1) with an odd number of the ij ‚Äôs equal to ‚àí1. Note
k‚äï
there are 2k‚àí1 clauses in each of xk‚äï
u and xu . If C is a clause C = z1 ‚à® ¬∑ ¬∑ ¬∑ ‚à® z , each zi a
literal xu or xu , then C k‚äï is the set of clauses of the form
C1 ‚à® C2 ‚à® ¬∑ ¬∑ ¬∑ ‚à® C ,
where each Ci ‚àà zik‚äï . There are 2(k‚àí1) many clauses in C k‚äï .
Definition 8. The xor-ified pebbling tautology clauses Pebk‚äï (G) is the set of clauses C k‚äï
for C ‚àà Peb(G). If G has n vertices, Pebk‚äï (G) has O(23k n) clauses.
Definition 9. Let G be a pointed graph with n vertices and k = k(n) > 0. Let œÅ be a
function with domain the set of clauses of Pebk‚äï (G) and range the set of variables xu,i of
Pebk‚äï (G), such that, for all C, the variable œÅ(C) is not used in C. The guarded xor-ified
pebbling tautology clauses, GPebk‚äï (G), are the clauses of the form
C ‚à® œÅ(C)

and

C ‚à® œÅ(C)

for C ‚àà Pebk‚äï (G).
The GPebk‚äï (G) clauses depend on the choice of œÅ; however, this is suppressed in the
notation. GPebk‚äï (G) consists of O(23k n) clauses.
Our deÔ¨Ånitions of Pebk‚äï (G) and GPebk‚äï (G) diÔ¨Äer somewhat from Urquhart‚Äôs, but these
diÔ¨Äerences are inessential and make no diÔ¨Äerence to asymptotic proof sizes.
676

Separations of Regular Resolution

Of course, the Pebk‚äï (G) clauses are readily derivable from the GPebk‚äï (G) clauses by
resolving on the guard literals as given by œÅ. There are simple polynomial size regular
resolution refutations of the Pebk‚äï (G) clauses; hence there are polynomial size, but not
regular, resolution refutations of the GPebk‚äï (G) clauses. Indeed, Urquhart (2011) proved
that there are pointed graphs G with n vertices and values k = k(n) = O(log log n), and
functions œÅ, such that regular resolution refutations of the GPebk‚äï (G) clauses require size
2
2Œ©(n/((log n) log log n)) .
Theorem 10. The guarded xor-ified pebbling tautology clauses GPebk‚äï (G) have polynomial
size regRTI refutations, and thus polynomial size pool refutations.
We make some simple observations about working with xor-iÔ¨Åed clauses before proving
Theorem 10.
Lemma 11. Let u be a vertex in G. There is a tree-like regular refutation of the clauses in
k‚äï
k
k
xk‚äï
u and xu with 2 ‚àí 1 resolution inferences, height k, and 2 leaf clauses. Its resolution
variables are the variables xu,i .
Proof. This is immediate by inspection: the refutation consists of resolving on the literals xu,i successively for i = 1, 2, . . . , k, giving a refutation of height k. The refutation
corresponds to a complete binary decision tree over the k many variables xu,i ; the leaf
k‚äï
clauses of the refutation are the members of xk‚äï
u and xu .
The refutation of Lemma 11 can be viewed as being the ‚Äúk‚äï-translation‚Äù of the proof
xu

‚ä•

xu

The next lemma describes a similar ‚Äúk‚äï-translation‚Äù of a proof
C, xu

D, xu
C, D

Lemma 12. Let u be a vertex in G, and let C and D be clauses which do not contain
either xu and xu . Then each clause of (C ‚à® D)k‚äï has a tree-like regular derivation from the
clauses in (C ‚à® xu )k‚äï and (D ‚à® xu )k‚äï in which the variables used as resolution variables are
exactly the variables xu,i . This derivation has 2k ‚àí 1 resolution inferences, height k, and
2k leaf clauses.
Proof. Fix a clause E from (C ‚à® D)k‚äï ; we must describe its derivation from clauses in
(C ‚à® xu )k‚äï and (D ‚à® xu )k‚äï . Let EC be the subclause of E which is from C k‚äï , and let
ED the subclause of E which is from D k‚äï . If C and D have non-empty intersection, EC
and ED are not disjoint; however, in any event, E = EC ‚à™ ED .
Form the refutation from Lemma 11. Then add EC to every leaf clause from x‚äï
u , add
k‚äï
ED to every leaf clause from xu , and add E to every non-leaf clause. This gives the desired
derivation of E.
Lemma 12 lets us further generalize the construction of k‚äï-translations of proofs. As a
typical example, the next lemma gives the k‚äï-translation of the following derivation:
677

Bonet, Buss, & Johannsen

xu , x v , xw
xv , xw

xu
xw

xv

Lemma 13. Let w be a vertex of G, and u and v its predecessors. Then, each clause
k‚äï
k‚äï
in xk‚äï
w has a dag-like regular resolution derivation P from the clauses in xu , xv , and
k‚äï
2k
(xu ‚à® xv ‚à® xw ) . This derivation contains < 2 resolution inferences and resolves on the
resolve on
literals xu,i and xv,i . In addition, the paths in P that lead to clauses in xk‚äï
v
exactly the literals xv,i .
Lemma 13 follows by applying Lemma 12 twice.
It is important to note that the left-to-right order of the leaves of the derivation of
Lemma 13 can be altered by changing the left-to-right order of hypotheses of resolution
inferences. In particular, given any leaf clause D of a refutation P , we can order the
hypotheses of the resolution inferences so that D is the leftmost leaf clause. This will be
useful when D needs to be learned.
Definition 14. Gw is the induced pointed subgraph of G with sink w and containing
those vertices from which the vertex w is reachable. G[w] is the subgraph of G obtained
by making the vertex w a leaf by removing its incoming edges, and then removing those
vertices from which the sink vertex of G is no longer reachable. Note that G[w] is a pointed
dag and has the same sink as G.
The vertex u is an ancestor of w if u 	= w and u ‚àà Gw, i.e., if there is a path from
u to w. We call u and v independent ancestors of w provided u, v, and w are distinct
and u ‚àà (Gw)[v] and v ‚àà (Gw)[u]. This means there is a path from u to w that does
not contain v, and a path from v to w to that does not contain u. We write G[u, v] for
G[u][v] = G[v][u].
More generally, let  ‚â• 0 and u1 , . . . , u , w be distinct vertices. We say u1 , . . . , u are
independent ancestors of w, if for every i ‚â§ , there is a path from ui to w that does not
contain any uj for j 	= i. We write G[u1 , . . . , u ] for G[u1 ] ¬∑ ¬∑ ¬∑ [u ]. Note that the deÔ¨Ånition
of G[u1 , . . . , u ] is independent of the order of the ui ‚Äôs.
Since G is a dag, it is possible for u and v to be independent ancestors of w, and also
have u an ancestor of v or vice-versa.
The next lemma states that the polynomial size regular resolution refutations of the
Pebk‚äï (G) clauses also apply to subgraphs such as Gw and (Gw)[u1 , . . . , u ]. We write
Pebk‚äï
Œ±Œ≤ (G) to denote the k‚äï-translations of Peb(G) clauses of type (Œ±) and (Œ≤), omitting
u
the clauses of type (Œ≥), and similarly for GPebk‚äï
Œ±Œ≤ (G). To save space, we often write 
instead of u1 , . . . , u . For instance, in the next lemma, ((Gw)[u]) \ {u, w} is shorthand for
((Gw)[u1 , . . . , u ]) \ {u1 , . . . , u , w}.
Lemma 15. Let w be a vertex of G. Let  ‚â• 0 and u1 , . . . u be independent ancestors of w.
Then each clause of (xu1 ‚à® ¬∑ ¬∑ ¬∑ ‚à® xu ‚à® xw )k‚äï has a regular resolution derivation from the
u]). The derivation uses only resolution variables of the form xv,i for
clauses Pebk‚äï
Œ±Œ≤ ((Gw)[
v ‚àà ((Gw)[u]) \ {u, w}. The derivation is dag-like and has size O(23k n) and height O(kn).
Proof. There is a simple regular dag-like derivation P of xu1 ‚à® ¬∑ ¬∑ ¬∑ ‚à® xu ‚à® xw from the clauses
(Œ±) and (Œ≤) of (non-xoriÔ¨Åed) Peb(G), of size O(n) where n is the size of G. P proceeds by
678

Separations of Regular Resolution

visiting vertices v in a depth-Ô¨Årst traversal of (Gw)[u] and deriving some subclause Cv of
xu1 ‚à® ¬∑ ¬∑ ¬∑ ‚à® xu ‚à® xv : the subclause Cv contains xv and those xui ‚Äôs such that there is a path
from ui to v that contains no other uj . For v a leaf distinct from the uj ‚Äôs, Cv is just xv
and also a Peb(G) clause of type (Œ±). If v has immediate predecessors v1 and v2 , then Cv
is formed by resolving Cv1 and Cv2 against the Peb(G) clause xv1 ‚à® xv2 ‚à® xv of type (Œ≤).
Now let U be some clause in (xu1 ‚à® ¬∑ ¬∑ ¬∑ ‚à® xu )k‚äï and W some clause in xk‚äï
w . We need
to give a derivation of U ‚à® W . We claim that a k‚äï-translation P k‚äï of P forms the desired
derivation. For this, each clause Cv of P is translated into 2k‚àí1 many clauses CX where
X ‚àà xk‚äï
v . Each CX is a subclause of U ‚à® X; namely the subclause which omits the literals
xui,j and xui,j of U when xui ‚àà
/ Cv . For v1 and v2 the two immediate ancestors of v as
k‚äï
before, and X ‚àà xv , the clause CX is derived in P k‚äï from the 22k‚àí2 many clauses CX1
and CX2 where Xi ‚àà (xvi )k‚äï for i = 1, 2. By Lemma 13, each such subderivation in P k‚äï
has height ‚â§ 2k and size ‚â§ 22k , and resolves on exactly the variables xv1 ,j and xv2 ,j .
The result is that a variable xv,j is resolved on in P k‚äï precisely when xv is resolved on
in P . And, since P has size O(n), P k‚äï has size O(23k n) and height O(kn).
Proof. (of Theorem 10.) We will construct a series of ‚ÄúLR partial refutations‚Äù, denoted
R0 , R1 , R2 , . . .; this process eventually terminates with a pool resolution (regRTL) refutation of GPebk‚äï (G). The terminology ‚ÄúLR partial‚Äù indicates that the refutation is being
constructed in left-to-right order, with the left part of the refutation properly formed, but
with many of the remaining leaves labeled with ‚ÄúunÔ¨Ånished clauses‚Äù instead of with valid
learned clauses or initial clauses from GPebk‚äï (G).
An LR partial refutation R is a tree with nodes labeled with clauses that form a correct
regRTI resolution refutation (and thus a correct pool resolution refutation), except at the
unÔ¨Ånished clauses at leaves. Furthermore, it must satisfy the following conditions:
a. Rt is a tree of nodes labeled with clauses. The root is labeled with the empty clause.
Each non-leaf node in Rt has a left child and a right child, and the clauses labeling
these nodes form a valid resolution inference.
b. Each leaf of Rt is either ‚ÄúÔ¨Ånished‚Äù or ‚ÄúunÔ¨Ånished‚Äù. Each Ô¨Ånished node leaf L is labeled
with either a clause from GPebk‚äï (G) or with a clause that was derived by an input
subderivation of Rt to the left of L in the post-order. The input subderivation may
not contain any unÔ¨Ånished leaves.
c. Each unÔ¨Ånished leaf is labeled with a clause C ‚àà E k‚äï where E is a clause of the form
xu1 ‚à® ¬∑ ¬∑ ¬∑ ‚à® xu ‚à® xw with  ‚â• 0 and u1 , . . . , u independent ancestors of w. Furthermore
C pool contains no literal xv,i with v ‚àà (Gw)[u] \ {u, w}.
We introduce a new notational convention to describe (sub)clauses in Rt . For w a vertex

k‚äï
in G, the notation W or W  denotes a clause in xk‚äï
w , and W or W denotes a clause in xw .


The notation W or W in no way denotes the negation of W or W ; instead, they are names
of clauses, with the overline meant only to serve as a reminder of the semantic meaning.
The initial LR-partial refutation R0 is formed as follows. Let Q be the refutation
obtained as the k‚äï-translation of the inference
xt

‚ä•
679

xt

Bonet, Buss, & Johannsen

as given by Lemma 11, where t is the sink of G. There are 2k leaf clauses of Q: half of
k‚äï
them are labeled with clauses T ‚àà xk‚äï
t and the other half are labeled with clauses T ‚àà xt .
with a derivation
Form R0 from Q by replacing each leaf clause T ‚àà xk‚äï
t
T , œÅ(T )

T , œÅ(T )
T

resolving on the guard literal œÅ(T ). These inferences are regular, since œÅ(T ) is not an xt,i .
The clauses T , œÅ(T ) and T , œÅ(T ) are in GPebk‚äï (G) and hence are Ô¨Ånished clauses. The
k‚äï
and  = 0;
other leaf clauses, of the form T ‚àà xk‚äï
t , satisfy condition c. with T = C ‚àà xt
these T ‚Äôs are unÔ¨Ånished clauses in R0 .
For the inductive step t ‚â• 0, the LR partial refutation Rt will be transformed into Rt+1 .
The goal is to replace one unÔ¨Ånished leaf in Rt , either by a derivation containing only
Ô¨Ånished leaves, or by a derivation which learns one more Pebk‚äï (G) clause while adding
only polynomially many more unÔ¨Ånished leaves.
Consider the leftmost unÔ¨Ånished leaf of Rt . By condition c., its clause C will have the
k‚äï
form U 1 , . . . , U  , W where  ‚â• 0, U i ‚àà xk‚äï
ui for i ‚â§ , and W ‚àà xw . By Lemma 15,
k‚äï
there is a dag-like regular refutation P of C from the clauses of PebŒ±Œ≤ ((Gw)[u]). We wish
u])
to convert P (if possible) into a derivation of C from the clauses of GPebk‚äï
Œ±Œ≤ ((Gw)[
and the already learned clauses of Rt . Consider a particular leaf clause D of P , so D ‚àà
u]): each such D needs to be handled in some way that makes P a valid
Pebk‚äï
Œ±Œ≤ ((Gw)[
derivation. There are four cases to consider:
(i) If the clause D is already learned as an input lemma in Rt to the left of C, then D may
be used in P as is.
For the remaining cases, assume D has not been learned as an input lemma.
(ii) Let y = œÅ(D). If either y or y is a member of C pool , then add that literal to D and to
every clause below D until reaching the Ô¨Årst clause where it appears. This replaces D
u]) clauses D‚à®y or D‚à®y. By construction, it preserves
with one of the GPebk‚äï
Œ±Œ≤ ((Gw)[
the validity of the resolution inferences of Rt as well as the regularity property.
(iii) Suppose cases (i) and (ii) do not apply and that y is not used in P as a resolution
variable below D. In this case, replace D by a resolution inference deriving D from
D ‚à® y and D ‚à® y. This preserves the regularity of the derivation. It also makes D a
learned clause.
It is possible that C itself is a Pebk‚äï
Œ±Œ≤ (G) clause. If so, then C = D and P is the trivial
derivation containing only C, and one of cases (i)-(iii) holds.
If all leaf clauses D of P can be treated by cases (i)-(iii), then we have successfully
transformed P into a (still dag-like) derivation P  which satisÔ¨Åes regularity and in which
leaf clauses are from GPebk‚äï (G) or already learned as input lemmas in Rt . By a result of
Buss et al. (2008, Thm. 3.3), P  can be converted in a regRTI proof P  of the same conclusion
as P , preserving the regularity conditions, and with the size of P  bounded by twice the
product of the size of P and the height of P . Therefore, the size of P  is O((23k n)(kn)) =
680

Separations of Regular Resolution

O(k23k n2 ). Form Rt+1 by replacing the clause C in Rt with the derivation P  . Rt+1 satisÔ¨Åes
conditions a.-c., and has one fewer unÔ¨Ånished clauses than Rt .
However, if even one leaf clause D of P fails cases (i)-(iii), then a completely diÔ¨Äerent
construction is used to form Rt+1 . Fix some leaf clause D which of P does not fall into cases
(i)-(iii). The unÔ¨Ånished clause C of Rt will be replaced by a small derivation of C which
learns D (in its leftmost inference), and which adds up to O(23k ) new unÔ¨Ånished clauses
in Rt+1 .
The leaf clause D is the k‚äï-translation of an (Œ±) or (Œ≤) clause of Pebk‚äï (G) and thus
k‚äï
either has the form E ‚àà x‚äï
e for some source e in G or has the form A, B, E where A ‚àà xa ,
k‚äï
B ‚àà xk‚äï
b , and E ‚àà xe for a, b, and e vertices in Gw with a and b the two predecessors
of e in G. Without loss of generality, b is not an ancestor of a in G; otherwise interchange
a and b. There are two cases depending on whether D is E or is A, B, E.
for e a source node in G. We claim that e, u1 , . . . , u
First suppose D is E ‚àà xk‚äï
e
are independent ancestors of w. As already remarked, P is not the trivial derivation since
otherwise the cases (i)-(iii) would hold; therefore e is not equal to w. Consequently, E is not
a subclause of the Ô¨Ånal clause C = U 1 , . . . U  , W of P . Hence some xe,i is resolved on in P .
Since P was formed using Lemma 15, e therefore cannot equal any ui . Since e ‚àà (Gw)[u],
there must exist a path from e to w that avoids all the nodes ui . Thus, since e is a source
node, the vertices e, u1 , . . . , u are independent ancestors of w.
To form Rt+1 , Ô¨Årst replace the derivation P by the k‚äï-translation of the following:
xe

xe , U 1 , . . . , U  , W
U 1, . . . , U , W

(2)

Note that (2) contains a blend of variables from Peb(G) (non-xoriÔ¨Åed) and from Pebk‚äï (G)
(xor-iÔ¨Åed). However, we can still form its k‚äï-translation Q: the leaf clauses of Q are the


k‚äï
2k clauses of the form E  ‚àà xk‚äï
e and of the form E , U 1 , . . . , U  , W for E ‚àà xe . By choosing
the appropriate left-to-right order for the hypotheses in Q, we arrange for D = E to be the
leftmost leaf clause of Q. Let y = œÅ(D). Then y is not one of the variables xe,i , nor is y
or y in C pool since condition (ii) does not hold for D. Therefore, the regularity condition is
preserved when we modify Q by replacing D with
D, œÅ(D)

D, œÅ(D)
D

(3)

Form Rt+1 from Rt by replacing C with the modiÔ¨Åed Q. This causes D to become learned
as an input lemma in Rt+1 . The other leaf clauses of Q all satisfy condition c. in Rt+1 and
thus become unÔ¨Ånished clauses of Rt+1 : they are all to the right of D. This adds < 2k
new unÔ¨Ånished clauses to Rt+1 . The number of inferences in the derivation structure is less
than 2k .
Second suppose D is A, B, E, where a and b are the predecessors of e in G. Suppose for
the moment that e 	= w, and that the nodes u1 , . . . , u are distinct from a and b. By the
same reasoning as in the previous case, e is not equal to any ui . We begin by replacing the
681

Bonet, Buss, & Johannsen

derivation P with the k‚äï-translation Q of:
xa , xb , xe
U a , xa
U a , xb , xe
U b , xb
U a , U b , xe
Uw , x e , W
U 1, . . . , U , W

(4)

where Ua , Ub and Uw are (possibly empty) sets of literals that form a partition of the set
{U 1 , . . . , U  }. We must deÔ¨Åne Ua , Ub , and Uw so that (the k‚äï-translation of) the clause
Ua , xa , the clause Ub , xb and the clause Uw , xe , W each fulÔ¨Åll the condition c. in order to
be legitimate ‚ÄúunÔ¨Ånished leaves‚Äù.
Since u1 , . . . , u are independent ancestors of w, we can Ô¨Åx paths œÄ1 , . . . , œÄ in G such
that œÄi is a path from ui to w and such that œÄi does not contain uj for any j 	= i. Call œÄi an
‚Äúa-path‚Äù if it contains the node a. Call œÄi a ‚Äúb-path if it contains b but does not contain a.
Finally, call œÄi a ‚Äúw-path‚Äù if it contains neither a nor b. Then, deÔ¨Åne Ua to be the set of
Ui ‚Äôs such that œÄi is an ‚Äúa-path. Likewise, let Ub (respectively, Uw ) be the set of Ui ‚Äôs such
that œÄi is a b-path (respectively a w-path). Clearly, Ua , Ub and Uw form a partition of
{U1 , . . . , U }. Also, the ui ‚Äôs for Ui in Ua form a set of independent ancestors of a, so Ua , xa
satisÔ¨Åes condition c. Likewise, the clauses Ub , xb and Uw , xe , W also satisfy condition c. To
prove the latter, note that any path from ui to w that contains e must contain at least one
of a or b.
The k‚äï-translation Q of (4) has D as its leftmost leaf clause. Let y = œÅ(D). Then y
is not one of the variables xa,i , xb,i , xe,i , nor is y or y in C pool since condition (ii) does not
hold for D. Therefore, the regularity condition is preserved when we modify Q by replacing
D with (3). Form Rt+1 from Rt by replacing C with the modiÔ¨Åed Q. This causes D to
become learned as an input lemma in Rt+1 . As previously argued, the other leaf clauses
of Q have become valid unÔ¨Ånished clauses that satisfy condition c. Rt+1 gains < 23k new
inferences, and less than 3 ¬∑ 2k new unÔ¨Ånished clauses.
We still have to consider the cases where u1 , . . . , u , a and b are not distinct. There
are 2 (very similar) cases where only one of the ui ‚Äôs is in {a, b}. For instance, suppose
that u1 = a and no ui is equal to b. We claim that A is the same clause as U1 . To prove
this, note that if A is diÔ¨Äerent from U1 , then there is a literal xu1 ,j or xu1 ,j appearing in A
which appears in negated form xu1 ,j or xu1 ,j (respectively) in U1 . This implies that the
derivation P uses xu1 ,j as a resolution variable, contradicting the fact that P was formed
via Lemma 15. With A = U1 , we form Q as the k‚äï-translation of
U 1 , xb , xe
U b , xb
U b , U 1 , xe
U w , xe , W
U 1, . . . , U , W
Here Ub and Uw are deÔ¨Åned as above (and Ua equals {U1 }). Order Q so that D is its
leftmost leaf clause, and then form Rt+1 as in the previous paragraph.
The case where there are i, j ‚â§  and i 	= j, such that ui = a and uj = b, the proof
structure is even simpler: Suppose u1 = a and u2 = b. Similarly to the previous case, A
and B must be the same as U1 and U2 , respectively. Then let Q be the k‚äï-translation of
U 1 , U 2 , xe
xe , U 3 , . . . , U  , W
U 1, . . . , U , W
682

Separations of Regular Resolution

(so Uw = {U3 , . . . , U }) and proceed as before.
Finally, consider the case w = e. Clearly, each Ui is an ancestor of either a or b.
Therefore, each Ui is in Ua or Ub . The refutation Rt+1 is formed from the proof structure
as in (4), but omitting the last inference.
This concludes the construction of Rt+1 from Rt . The process of constructing Rt ‚Äôs halts
once there are no remaining unÔ¨Ånished clauses, and yields a Ô¨Ånal refutation R which is a
valid regRTI refutation of the GPebk‚äï (G) clauses.
We need to bound the size of the refutation R. First consider how Rt+1 is formed
from Rt . In cases (i)-(iii), an unÔ¨Ånished leaf is completely handled without adding any
new unÔ¨Ånished leaves. In these cases, at most O(k23k n2 ) many new clauses are introduced
in Rt+1 . In case (iv), a new Pebk‚äï (G) clause is learned as an input lemma while adding
only O(3 ¬∑ 2k ) many new unÔ¨Ånished leaves in Rt+1 (and only O(23k ) new clauses).
Even though there are exponentially many potential unÔ¨Ånished clauses, the case (iv) construction can occur at most polynomially many times because there are only polynomially
many Pebk‚äï (G) clauses to be learned. Indeed, there are only < n23(k‚àí1) many Pebk‚äï (G)
clauses. Therefore, at most O(n23(k‚àí1) ¬∑ 3 ¬∑ 2k ) many distinct unÔ¨Ånished leaf clauses can appear during the construction of R. Consequently, cases (i)-(iii) occur only this many times.
Therefore, the total number of clauses in R is bounded by O(n23(k‚àí1) ¬∑ 3 ¬∑ 2k ¬∑ k23k n2 ) =
O(27k n3 ). Thus the size of R is polynomially bounded by the size of the GPebk‚äï (G) clauses;
in fact, it is bounded by a degree three polynomial.
This completes the proof of Theorem 10.

4. Guarded Graph Tautologies
We deÔ¨Åne various graph tautologies, sometimes also called ‚Äúordering principles‚Äù. They
use a size parameter n > 1, and variables xi,j with i, j ‚àà [n] and i 	= j, where [n] =
{0, 1, 2, . . . , n‚àí1}. A variable xi,j will intuitively represent the condition that i ‚â∫ j with ‚â∫
intended to be a total, linear order. We will thus always adopt the simplifying convention
that xi,j and xj,i are the identical literal, i.e., only the variables xi,j for i < j actually exist,
and xj,i for j < i is just a notation for xi,j , and xj,i stands for xi,j . This identiÔ¨Åcation
makes no essential diÔ¨Äerence to the complexity of proofs of the tautologies, but it reduces
the number of literals and clauses, and simpliÔ¨Åes the deÔ¨Ånitions. In particular, it means
there are no axioms for the antisymmetry or totality of ‚â∫.
The following GTn clauses are based on the tautologies deÔ¨Åned by Krishnamurthy
(1985). These tautologies, or similar ones, have also been studied by StaÃälmarck (1996),
Bonet and Galesi (2001), Segerlind, Buss, and Impagliazzo (2004), Beckmann and Buss
(2005), Van Gelder (2006), Alekhnovich et al. (2007) and Johannsen (2009).
Definition 16. Let n > 1. Then GTn is the following set of clauses:

(Œ±‚àÖ ) The clauses j=i xj,i , for each value i < n.
(Œ≤‚àÖ ) The transitivity clauses Ti,j,k := xi,j ‚à® xj,k ‚à® xk,i for all distinct i, j, k in [n].
Note that the clauses Ti,j,k , Tj,k,i and Tk,i,j are identical. For this reason Van Gelder
(2005) uses the name ‚Äúno triangles‚Äù (NT) for a similar principle.
The next deÔ¨Ånition is due to Alekhnovich et al. (2007), who used the notation GTn .
They used particular functions r and s for their lower bound proof, but since our upper
683

Bonet, Buss, & Johannsen

bound proof does not depend on the details of r and s we leave them unspeciÔ¨Åed. We
require that r(i, j, k) 	= s(i, j, k) and that the set {r(i, j, k), s(i, j, k)} 	‚äÇ {i, j, k}. In addition,
w.l.o.g., r(i, j, k) = r(j, k, i) = r(k, i, j), and similarly for s.
Definition 17. Let n ‚â• 1, and let r(i, j, k) and s(i, j, k) be functions mapping [n]3 to [n]
as above. The guarded graph tautology clauses, GGTn , consist of:

(Œ±‚àÖ ) The clauses j=i xj,i , for each value i < n.
(Œ≤‚àÖ ) The guarded transitivity clauses Ti,j,k ‚à® xr,s and Ti,j,k ‚à® xr,s , for all distinct i, j, k in
[n], where r = r(i, j, k) and s = s(i, j, k).
Note that the GGTn clauses depend on the functions r and s; this is suppressed in the
notation. Our Ô¨Årst main result for the guarded graph tautologies is:
Theorem 18. The guarded graph tautology clauses GGTn have polynomial size pool resolution (regRTL) refutations.
The proof of Theorem 18 will construct pool refutations in the form of regular tree-like
refutations with lemmas. A key part of this is learning transitive closure clauses that are
derived using resolution on the guarded transitivity clauses of GGTn . A slightly modiÔ¨Åed
construction, that uses a result of Buss et al. (2008), gives instead tree-like regular resolution
refutations with input lemmas. This will establish the following:
Theorem 19. The guarded graph tautology clauses GGTn have polynomial size, tree-like
regular resolution refutations with input lemmas (regRTI refutations).
As discussed in the introduction, Theorem 19 and a result of Buss et al. (2008) together
imply the GGTn clauses can be shown unsatisÔ¨Åable by non-greedy polynomial size CDCL.
This follows via the mentioned theorem of Buss et al. (2008, Thm. 5.6), since the refutations
of GGTn are regRTI, and hence regWRTI, proofs in the sense of Buss et al.. Theorem 31
of Section 5 will improve on this by giving greedy CDCL refutations.
Theorem 19 is strictly stronger than Theorem 18, but we Ô¨Ånd it convenient to prove
Theorem 18 Ô¨Årst.
4.1 Resolution Refutations for Guarded Graph Tautologies
The following theorem is an important ingredient of our upper bound proof.
Theorem 20. (StaÃälmarck, 1996; Bonet & Galesi, 2001; Van Gelder, 2006) The sets GTn
have regular resolution refutations Pn of polynomial size O(n3 ).
The proofs of Theorems 18 and 19 use the refutation Pn as a ‚Äúblack box‚Äù: the only
property needed is that the Pn ‚Äôs are regular and polynomial size. Section 5 will need to use
the details of Pn however.
Proof. (Proof sketch for Theorem 20.) For , k ‚àà [n], deÔ¨Åne the clause Totk, to be

xj, ,
Totk, :=
j‚àà[k+1]\{}

684

Separations of Regular Resolution

Tot3,3

Tot3,2

Tot3,1

Tot3,0

Tot2,2

Tot2,1

Tot2,0

Tot1,1

Tot1,0
Tot0,0 = 2

Figure 1: Main structure of the regular refutation Pn of the GTn clauses, with n = 4.
Inferences that resolve against transitivity clauses Ti,j,k are not shown: the slanted
line from Totk+1,k+1 to Totk, represents k many resolution inferences against the
clauses T,i,k+1 for i ‚àà [k + 1] \ {}. Each subderivation with hypotheses Totk,k
and Totk, and conclusion Totk‚àí1, is an input derivation.

which expresses the condition that  has a predecessor j ‚â§ k. Of course, the clauses Totn‚àí1, ,
for  = 0, 1, . . . , n ‚àí 1, are the initial clauses (Œ±‚àÖ ) of GTn . Note that Tot0,0 is the empty
clause.
As pictured in Figure 1, Pn proceeds by deriving Totk, from Totk+1,k+1 and Totk+1,
for k = n‚àí2, . . . , 0 and  ‚â§ k, until deriving the empty clause Tot0,0 . The Ô¨Årst part of the
derivation of Totk, resolves Totk+1,k+1 successively against the k many transitivity clauses
T,i,k+1 for i ‚àà [k + 1] \ {}. By the convention that xi, and x,i are the same literal, T,i,k+1
is the clause xi, , xi,k+1 , xk+1, . The resolution with T,i,k+1 uses xi,k+1 as the resolution
literal and has the eÔ¨Äect of adding xk+1, to the clause, and replacing xi,k+1 with xi, . Thus,
the conclusion of these k resolution steps is

xk+1, ‚à®
xi, .
i‚àà[k+1]\{}

One Ô¨Ånal resolution against Totk+1, on the literal xk+1, yields Totk, as desired. The
regularity of Pn is evident by inspection.
The refutations Pn can be modiÔ¨Åed to give refutations of GGTn by Ô¨Årst deriving each
transitive clause Ti,j,k from the two guarded transitivity clauses of (Œ≤‚àÖ ). This however
destroys the regularity property, and as already discussed, no polynomial size regular refutations exist for GGTn (Alekhnovich et al., 2007).
As usual, a partial order ‚â∫ on [n] is an antisymmetric, transitive binary relation on [n].
We will be primarily interested in ‚Äúbipartite‚Äù partial orders, which are partial orders that
do not have any chain of inequalities x ‚â∫ y ‚â∫ z.
Definition 21. A bipartite partial order is a binary relation œÄ on [n] such that the domain
and range of œÄ do not intersect. We write x ‚â∫œÄ y for (x, y) ‚àà œÄ. The set of œÄ-minimal
elements is denoted MœÄ .
The righthand side of Figure 2 shows an example. The bipartiteness of œÄ arises from the
/ MœÄ .
fact that MœÄ and [n] \ MœÄ partition [n] into two sets. If i ‚â∫œÄ j, then i ‚àà MœÄ and j ‚àà
685

Bonet, Buss, & Johannsen

10
6

11
7

8

9

‚áí

MœÄ :
1

2

3

4

6

[n] ‚àí MœÄ :
1

2

10 7 8 9 11
3

4

5

5

Figure 2: Example of a dag (left) and its associated bipartite partial order (right).
In addition, MœÄ contains the isolated points of œÄ. It is permitted for ‚â∫ to be the empty
relation, and then MœÄ = [n].
A (bipartite) partial order on [n] can also be viewed as a directed acyclic graph (dag)
G = ([n], œÄ); note that [n] is the set of vertices, and œÄ is the set of edges of G. Conversely,
we deÔ¨Åne a mapping from an arbitrary dag G = ([n], œÑ ) to an associated bipartite partial
order œÄ:
Definition 22. Let G = ([n], œÑ ) be a dag. We write x ‚â∫+
œÑ y to denote that G contains a
path of length ‚â• 1 from x to y. The bipartite partial order œÄ associated with G is deÔ¨Åned
by letting x ‚â∫œÄ y hold precisely when x is œÑ -minimal in G (that is, x has indegree 0) and
x ‚â∫+
œÑ y.
Note that the deÔ¨Ånition does not require œÑ to be transitive. It is easy to check that the
œÄ associated with œÑ is in fact a bipartite partial order. The intuition is that œÄ retains only
the information about whether i ‚â∫+
œÑ j for minimal elements i, and forgets the ordering that
œÑ imposes on non-minimal elements.
We deÔ¨Åne the graph tautology clauses GTœÄ,n relative to œÄ as follows.
Definition 23. Let œÄ be a bipartite partial order on [n]. Then GTœÄ,n contains:

(Œ±) The clauses j=i xj,i , for each value i ‚àà MœÄ .
(Œ≤) The transitivity clauses Ti,j,k := xi,j ‚à® xj,k ‚à® xk,i for all distinct i, j, k in MœÄ . (Vertices
i, j, k  in Figure 3 show an example.)
(Œ≥) The transitivity clauses Ti,j,k for all distinct i, j, k such that i, j ‚àà MœÄ and i 	‚â∫œÄ k and
j ‚â∫œÄ k. (As shown in Figure 3.)
The set GTœÄ,n is satisÔ¨Åable if œÄ is nonempty. As an example, there is the assignment
/ MœÄ and every i ‚àà MœÄ , and sets all other variables
that sets xj,i true for some Ô¨Åxed j ‚àà
false. However, if œÄ is applied as a restriction, then GTœÄ,n becomes unsatisÔ¨Åable. That is
to say, there is no assignment which satisÔ¨Åes GTœÄ,n and is consistent with œÄ. This fact is
proved by the regular derivation PœÄ described in the next lemma.

Definition 24. For œÄ a bipartite partial order, the clause ( œÄ) is
 
œÄ := {xi,j : i ‚â∫œÄ j}.
Lemma
25. Let œÄ be a bipartite partial order on [n]. Then there is a regular derivation PœÄ

of ( œÄ) from the set GTœÄ,n .
The only variables resolved on in PœÄ are the following: the variables xi,j such that
/ MœÄ , i ‚àà MœÄ , and i 	‚â∫œÄ k.
i, j ‚àà MœÄ , and the variables xi,k such that k ‚àà
686

Separations of Regular Resolution

1

[n] \ MœÄ :

2

MœÄ :

i

3

k
j

k

Figure 3: A bipartite partial order œÄ is pictured, with the ordered pairs of œÄ shown as
directed edges. (For instance, j ‚â∫œÄ k holds.) The set MœÄ is the set of minimal
vertices. The nodes i, j, k shown are an example of nodes used for a transitivity
axiom xi,j ‚à® xj,k ‚à® xk,i of type (Œ≥). The nodes i, j, k are an example of the nodes
for a transitivity axiom of type (Œ≤).

Lemma 25 implies that if œÄ is the bipartite partial order associated with a dag œÑ , then
PœÄ does not resolve on any literal whose value is set by œÑ . This is proved by noting that if
/ MœÄ .
i ‚â∫œÑ j, then j ‚àà
If œÄ is empty, MœÄ = [n] and there are no clauses of type (Œ≥). In this case, GTœÄ,n
is identical to GTn , and the PœÄ of Lemma 25 is the same as the refutation of GTn of
Theorem 20.
Proof. By renumbering the vertices, we can assume w.l.o.g. that MœÄ = {0, . . . , m‚àí1}. For
each k ‚â• m, there is at least one value of j such that j ‚â∫œÄ k: let Jk be an arbitrary such
value j. Note Jk < m.
 Fix i ‚àà MœÄ ; that is, i < m. Recall that the clause of type (Œ±) in GTœÄ,n for i is
j=i xj,i . We resolve this clause successively, for each k ‚â• m such that i 	‚â∫œÄ k, against the
clauses Ti,Jk ,k of type (Œ≥)
xi,Jk ‚à® xJk ,k ‚à® xk,i
using resolution variables xk,i . (Note that Jk 	= i since i 	‚â∫œÄ k.) This yields a clause TotœÄm‚àí1,i :




xi,Jk ‚à®
xJk ,k ‚à®
xk,i ‚à®
xk,i .
k‚â•m
i‚â∫œÄ k

k‚â•m
i‚â∫œÄ k

k‚â•m
i‚â∫œÄ k

k<m
k=i

The Ô¨Årst two disjuncts shown above come from
 the side literals of the clauses Ti,Jk ,k ; the
last two disjuncts come from the literals in j=i xj,i which were not resolved on. Since a
literal xi,Jk is the same literal as xJk ,i and since Jk < m, the literals in the Ô¨Årst disjunct are
also contained in the fourth disjunct. Thus, eliminating duplicate literals, TotœÄm‚àí1,i is equal
to the clause



xJk ,k ‚à®
xk,i ‚à®
xk,i .
(5)
SiœÄ ‚à® Totm‚àí1,i :=
k‚â•m
i‚â∫œÄ k

k‚â•m
i‚â∫œÄ k

k<m
k=i

where SiœÄ is deÔ¨Åned to equal the Ô¨Årst two big disjuncts in the lefthand side of the equation,
and Totm‚àí1,i is the same as before, namely the last big disjunct on the righthand side.
Repeating this process, we obtain derivations of the clauses TotœÄm‚àí1,i for all i < m. Their
subclauses Totm‚àí1,i are the same as the (Œ±‚àÖ ) clauses in GTm . Thus, the clauses TotœÄm‚àí1,i
give all (Œ±‚àÖ ) clauses of GTm , but with SiœÄ ‚Äôs added in as side literals. Moreover, the clauses
of type (Œ≤) in GTœÄ,n are exactly the transitivity clauses of GTm . All these clauses can be
687

Bonet, Buss, & Johannsen

Tot3,3

Tot3,2

Tot3,1

Tot3,0

TotœÄ3,3

TotœÄ3,2

TotœÄ3,1

TotœÄ3,0

TotœÄ2,2

TotœÄ2,1

TotœÄ2,0

TotœÄ1,1

TotœÄ1,0

TotœÄ0,0 = ( œÄ)

Figure 4: Main structure of the regular derivation PœÄ , with m = 4. The initial clauses
are GTm clauses. Inferences that resolve against transitivity clauses Ti,j,k are not
shown: slanted lines and the top row of vertical lines represent multiple resolution
inferences against transitivity clauses.

combined exactly as in the refutation of GTm described in Theorem 20, but carrying along
extra side literals SiœÄ ; namely carrying along literals xJk ,k for Jk ‚â∫œÄ k, and xi,k for i ‚â∫œÄ k.
Since the refutation of GTm uses all of its transitivity clauses and since each xJk ,k literal is
the same as an xi,k with i ‚â∫œÄ k, this yields a resolution derivation PœÄ of the clause
{xi,k : i ‚â∫œÄ k}.

This is the clause ( œÄ) as desired.

The just-constructed derivation PœÄ of ( œÄ) has the structure shown in Figure 4: the
clauses TotœÄk,i are equal to
TotœÄk,i

‚éß œÄ
‚é® Si ‚à® Totk,i
S œÄ ‚à® S œÄ ‚à® Totk,i
=
‚é© im‚àí1 m‚àí1
œÄ
j=i Sj ‚à® Totk,i

if k = m ‚àí 1
if i < k < m ‚àí 1
if i = k

(6)

To show that PœÄ is regular, note that the Ô¨Årst parts of PœÄ deriving the clauses TotœÄi,m are
regular by construction, and they use resolution only on variables xk,i with i < m ‚â§ k, and
i 	‚â∫œÄ k. The remaining part of PœÄ is also regular by Theorem 20, and uses resolution only
on variables xi,j with i, j ‚â§ m.
4.2 Pool and w-Resolution Refutations for GGTn
We now prove Theorem 18, and then indicate the minor changes needed to prove Theorem 19.
Proof. (Theorem 18) We again construct a Ô¨Ånite sequence of ‚ÄúLR partial refutations‚Äù,
denoted R0 , R1 , R2 , . . .. This terminates after Ô¨Ånitely many steps with the desired pool
(regRTL) refutation R of GGTn . Each LR partial refutation Rt will be a correct pool
688

Separations of Regular Resolution

resolution refutation, except possibly for the presence of ‚ÄúunÔ¨Ånished clauses‚Äù at leaves.
UnÔ¨Ånished clauses will correspond to bipartite partial orders. Each Rt will satisfy the
following conditions:
a. R is a tree. The root is labeled with the empty clause. Each non-leaf node in R has a
left child and right child; the clause labeling the node is derived by resolution from
the clauses on its two children.
b. For each clause C occurring in R, the set of ordered pairs œÑ (C) is deÔ¨Åned as œÑ (C) =
{i, j : xi,j ‚àà C pool }. In many cases, œÑ (C) will be a dag, but this is not always true.
For instance, if C is a transitivity axiom, then œÑ (C) has a 3-cycle and is not a dag.
d. Leaves are either ‚ÄúÔ¨Ånished‚Äù or ‚ÄúunÔ¨Ånished‚Äù. Each Ô¨Ånished leaf L is labeled with either a
clause from GGTn or a clause that occurs to the left of L in the post-order traversal
of R.
e. For an unÔ¨Ånished leaf labeled with clause C, the set œÑ (C) is a dag. Furthermore, letting

œÄ be the bipartite partial order associated with œÑ (C), the clause C is equal to ( œÄ).
Property e. is particularly crucial and is novel to our construction. As shown below,
each unÔ¨Ånished leaf, labeled with a clause C = ( œÄ), will be replaced by a derivation S.
The derivation S often will be based on PœÄ , and thus might be expected to end with exactly
the clause C; however, some of the resolution inferences needed for PœÄ might be disallowed
by the regularity property of pool resolution proofs. This can mean that S will instead be
a derivation of a clause C  such that C ‚äÜ C  ‚äÜ C pool . The condition C  ‚äÜ C pool is required
because any literal x ‚àà C  \ C will be handled by modifying the refutation R by propagating
x downward in R until reaching a clause that already contains x. Since C  ‚äÜ C pool , such
a clause exists. The fact that C  ‚äá C implies that enough literals are present for the
derivation to use only (non-degenerate) resolution inferences ‚Äî by virtue of the fact that
our constructions will pick C so that it contains the literals that must be present for use as
resolution literals.
The construction begins by letting R0 be the ‚Äúempty‚Äù refutation, containing just the
empty clause. Of course, this clause is an unÔ¨Ånished leaf, and œÑ (‚àÖ) = ‚àÖ. Thus R0 is a valid
LR partial refutation.
For the induction step, Rt has been constructed already. Let C be the leftmost unÔ¨Ånished
clause in Rt . Rt+1 will be formed by replacing C with an (LR-partial) derivation S of some
clause C  such that C ‚äÜ C  ‚äÜ C pool .
We need to describe S. Let œÄ be the bipartite partial order
 associated with œÑ (C), and
consider the derivation PœÄ from Lemma 25. Since C is ( œÄ) by condition e., the Ô¨Ånal
line of PœÄ is the clause C. The intuition is that we would like to let S be PœÄ . The Ô¨Årst
diÔ¨Éculty with this is that PœÄ is dag-like, and the LR-partial refutation is intended to be
tree-like, This diÔ¨Éculty, however, can be circumvented by just expanding PœÄ , which is
regular, into a tree-like regular derivation with lemmas by the simple expedient of using a
depth-Ô¨Årst traversal of PœÄ . The second, and more serious, diÔ¨Éculty is that PœÄ is a derivation
from GTn , not GGTn . Namely, the derivation PœÄ uses the transitivity clauses of GTn as
initial clauses instead of the guarded transitivity clauses of GGTn . The transitivity clauses
Ti,j,k := xi,j ‚à® xj,k ‚à® xk,i in PœÄ are handled one at a time as described below. There are four
689

Bonet, Buss, & Johannsen

separate constructions: case (i) requires no change to PœÄ ; cases (ii) and (iii) require small
changes; but in the fourth case, the subproof PœÄ is abandoned in favor of ‚Äúlearning‚Äù the
transitivity clause.
By the remark after Lemma 25, no literal in C pool is used as a resolution literal in PœÄ .
(i) Suppose a transitivity clause Ti,j,k of PœÄ already appears earlier in Rt , that is, to the left
of C in the post-order. Then Ti,j,k is already learned, and can be used freely in PœÄ .
(Since we are building a pool resolution refutation, not a w-resolution refutation, there
is no need for Ti,j,k to be an input lemma.)
In the remaining cases (ii)-(iv), the transitivity clause Ti,j,k is not yet learned. Let the guard
variable for Ti,j,k be xr,s , so r = r(i, j, k) and s = s(i, j, k).
(ii) Suppose case (i) does not apply and that the guard variable xr,s or its negation xr,s is a
member of C pool . The guard variable thus is used as a resolution variable somewhere
along the branch from the root to clause C. Then, as mentioned above, Lemma 25
implies that xr,s is not resolved on in PœÄ . Therefore, we can add the literal xr,s or xr,s
(respectively) to the clause Ti,j,k and to every clause on any path below Ti,j,k until
reaching a clause that already contains that literal. This replaces Ti,j,k with one of
the initial clauses Ti,j,k ‚à® xr,s or Ti,j,k ‚à® xr,s of GGTn . Note this adds the literal xr,s
or xr,s to the Ô¨Ånal clause C  of the modiÔ¨Åed PœÄ . This maintains the property that
C ‚äÜ C  ‚äÜ C pool .
(iii) Suppose case (i) does not apply and that xr,s is not used as a resolution variable
below Ti,j,k in PœÄ and neither xr,s nor xr,s is a member of C pool . In this case, PœÄ is
modiÔ¨Åed so as to derive the clause Ti,j,k from the two GGTn clauses Ti,j,k ‚à® xr,s and
Ti,j,k ‚à® xr,s by resolving on xr,s . This maintains the regularity of the derivation. It
also means that henceforth Ti,j,k will be learned.
If all of the transitivity clauses in PœÄ can be handled by cases (i)-(iii), then we use PœÄ to
deÔ¨Åne Rt+1 . Namely, let PœÄ be the derivation PœÄ as modiÔ¨Åed by the applications of cases
(ii) and (iii). The derivation PœÄ is regular and dag-like, so we can recast it as a tree-like
derivation S with lemmas, by using a depth-Ô¨Årst traversal of PœÄ . The size of S is linear in
the size of PœÄ , since only lemmas need to be repeated. The Ô¨Ånal line of S is the clause C  ,
namely C plus the literals introduced by case (ii). The derivation Rt+1 is formed from Rt
by replacing the clause C with the derivation S of C  , and then propagating each new literal
x ‚àà C  \ C down towards the root of Rt , adding x to each clause below S until reaching
a clause that already contains x. The derivation S contains no unÔ¨Ånished leaf, so Rt+1
contains one fewer unÔ¨Ånished leaves than Rt .
On the other hand, if even one transitivity axiom Ti,j,k in PœÄ is not covered by the above
three cases, then case (iv) must be used instead. This introduces a completely diÔ¨Äerent
construction to form S:
(iv) Let Ti,j,k be any transitivity axiom in PœÄ that is not covered by cases (i)-(iii). In
this case, the guard variable xr,s is used as a resolution variable in PœÄ somewhere
below Ti,j,k ; in general, this means we cannot use resolution on xr,s to derive Ti,j,k
while maintaining the desired pool property. Hence, PœÄ is no longer used, and we
690

Separations of Regular Resolution

instead will form S with a short left-branching path that ‚Äúlearns‚Äù Ti,j,k . This will
generate two or three new unÔ¨Ånished leaf nodes. Since unÔ¨Ånished leaf nodes in a
LR partial derivation must be labeled with clauses from bipartite partial orders, it is
also necessary to attach short derivations to these unÔ¨Ånished leaf nodes to make the
unÔ¨Ånished leaf clauses of S correspond correctly to bipartite partial orders. These
unÔ¨Ånished leaf nodes are then kept in Rt+1 to be handled at later stages.
The construction of S is described in detail next, and depends on whether Ti,j,k is type
(Œ≤) or (Œ≥). The base case of R0 is type (Œ≤), but we describe the type (Œ≥) construction
Ô¨Årst as it is somewhat simpler.
Suppose Ti,j,k is type (Œ≥), and thus xj,k appears in C. (Refer to Figure 3.) Let xr,s be
the guard variable for the transitivity axiom Ti,j,k . The derivation S will have the form
S1 . . .. . .
xi,j , xj,k , xk,i , xr,s
xi,j , xj,k , xk,i , xr,s
...
xi,j , xj,k , xk,i
xi,j , xi,k , œÄ ‚àí[jk;jR(i)]
xi,j , xj,k , œÄ ‚àí[jk;jR(i)]
xj,k , œÄ ‚àí[jk]

S2 . . .. . .
...
xj,i , xj,k , œÄ ‚àí[jk;iR(j)]

The notation œÄ ‚àí[jk] denotes the disjunction of the negations of the literals in œÄ omitting the
literal xj,k . We write ‚ÄúiR(j)‚Äù to indicate literals xi, such that j ‚â∫œÄ . (The ‚ÄúR(j)‚Äù means
‚Äúrange of j‚Äù.) Thus œÄ ‚àí[jk;iR(j)] denotes the clause containing the negations of the literals
in œÄ, omitting xj,k and any literals xi, such that j ‚â∫œÄ . The clause œÄ ‚àí[jk;jR(i)] is deÔ¨Åned
similarly.
The upper leftmost inference of S is a resolution inference on the variable xr,s . Since
Ti,j,k is not covered by either case (i) or (ii), the variable xr,s is not in C pool . Thus, this
use of xr,s as a resolution variable does not violate regularity. Furthermore, since Ti,j,k is
of type (Œ≥), we have i	‚â∫œÑ (C) j, j	‚â∫œÑ (C) i, i	‚â∫œÑ (C) k, and k	‚â∫œÑ (C) i. Thus the literals xi,j and xi,k
are not in C pool , so they also can be resolved on without violating regularity.
Let C1 and C2 be the Ô¨Ånal clauses of S1 and S2 , and let C1‚àí be the clause below C1 and
above C. The set œÑ (C2 ) is obtained by adding j, i to œÑ (C), and similarly œÑ (C1‚àí ) is œÑ (C)
plus i, j. Since Ti,j,k is type (Œ≥), we have i, j ‚àà MœÄ . Therefore, since œÑ (C) is a dag, œÑ (C2 )
these dags
and œÑ (C1‚àí ) are also dags. Let œÄ2 and œÄ1 be the bipartite orders associated with 
(respectively). We will form the subderivation S1 so that it contains the clause ( œÄ 1 ) as its
only unÔ¨Ånished clause. This will require adding inferences in S1 which add and remove the
appropriate literals. The Ô¨Årst step of this type already occurs in going up from C1‚àí to C1
since this has removed xj,k and added xi,k , reÔ¨Çecting the fact that j is not œÄ1 -minimal and
/ œÄ1 . Similarly, we will form S2 so that its only unÔ¨Ånished clause is
thus
 xi,k ‚àà œÄ1 but xj,k ‚àà
( œÄ 2 ).
We Ô¨Årst describe the subderivation S2 . The situation is pictured in Figure 5, which
shows an extract from Figure 3: the edges shown in part (a) of the Ô¨Ågure correspond to
the literals present in the Ô¨Ånal line C2 of S2 . In particular, recall that the literals xi, such
that j ‚â∫œÄ  are omitted from the last line of S2 . (Correspondingly, the edge from i to 1 is
omitted from Figure 5.) The last line C2 of S2 may not correspond to a bipartite partial
order as it may not partition [n] into minimal and non-minimal elements; thus, C2 may
not qualify to be an unÔ¨Ånished node of Rt+1 . (An example of this in Figure 5(a) is that
691

Bonet, Buss, & Johannsen

1

2

k

1

3

i
j
(a) xj,k , xi,2 , xj,i , œÄ ‚àó

2

k

3

i
j
(b) xj,k , xi,2 , xj,i , œÄ ‚àó

Figure 5: The partial orders for the fragment of S2 shown in (7).

j ‚â∫œÑ (C2 ) i ‚â∫œÑ (C2 ) 2 , corresponding to xj,i and xi,2 being in C2 .) The bipartite partial
order œÄ2 associated with œÑ (C2 ) is equal to the bipartite partial order that agrees with œÄ
except that each i ‚â∫œÄ  condition is replaced with the condition j ‚â∫œÄ2 . (This is represented
in Figure 5(b) by the fact that the edge from i to 2 has been replaced by the edge from
/ MœÄ2 .)
j to 2 . Note that the vertex i is no longer a minimal element of œÄ2 ; that is, i ‚àà
We
 wish to form S2 to be a regular derivation of the clause xj,i , œÄ ‚àí[jk;iR(j)] from the clause
( œÄ 2 ).
The subderivation of S2 for replacing xi,2 in œÄ with xj,2 in œÄ 2 is as follows, letting œÄ ‚àó
be œÄ ‚àí[jk;iR(j);i2] .
. . .. . . rest of S2
S2 . . .. . .
...
...
xj,i , xi,2 , x2 ,j
xj,k , xj,2 , xj,i , œÄ ‚àó
xj,k , xi,2 , xj,i , œÄ ‚àó

(7)

The part labeled ‚Äúrest of S2 ‚Äù will handle similarly the other literals  such that i ‚â∫œÄ  and
j 	‚â∫œÄ . The Ô¨Ånal line of S2 is the transitivity axiom Tj,i,2 . This is a GTn axiom, not
a GGTn axiom; however, it can be handled by the methods of cases (i)-(iii). Namely, if
Tj,i,2 has already been learned by appearing somewhere to the left in Rt , then S2 is just
this single clause. Otherwise, let the guard variable for Tj,i,2 be xr ,s . If xr ,s is used as a
resolution variable below Tj,i,2 , then replace Tj,i,2 with Tj,i,2 ‚à® xr ,s or Tj,i2 ‚à® xr ,s , and
propagate the xr ,s or xr ,s to clauses down the branch leading to Tj,i,2 until reaching a
clause that already contains that literal. Finally, if xr ,s has not been used as a resolution
variable in Rt below C, then let S2 consist of a resolution inference deriving (and learning)
Tj,i,2 from the clauses Tj,i,2 , xr ,s and Tj,i,2 , xr ,s .
To complete the construction of S2 , the inference (7) is repeated for each value of  such
	‚â∫œÄ . The result is that S2 has one unÔ¨Ånished leaf clause, and it is labeled
that i ‚â∫œÄ  and j
with the clause ( œÄ 2 ).
We next describe the subderivation S1 . The situation is shown in Figure 6. As in the
formation of S2 , the Ô¨Ånal clause C1 in S1 may need to be modiÔ¨Åed in order to correspond
to the bipartite partial order œÄ1 which is associated with œÑ (C1 ). First, note that the literal
xj,k is already replaced by xi,k in the Ô¨Ånal clause of S1 . The other change that is needed is
that, for every  such that j ‚â∫œÄ  and i 	‚â∫œÄ , we must replace xj, with xi, since we have
j 	‚â∫œÄ1  and i ‚â∫œÄ1 . Vertex 3 in Figure 6 is an example of a such a value . The ordering
in the Ô¨Ånal clause of S1 is shown in part (a), and the desired ordered pairs of œÄ1 are shown
in part (b). Note that j is no longer a minimal element in œÄ1 .
692

Separations of Regular Resolution

1

2

k

1

3

i
j
(a) xi,k , xj,3 , xi,j , œÄ ‚àó

2

3

k

i
j
(b) xi,k , xi,3 , xi,j , œÄ ‚àó

Figure 6: The partial orders for the fragment of S1 shown in (8).
The replacement of xj,3 with xi,3 is eÔ¨Äected by the following inference, letting œÄ ‚àó now
be œÄ ‚àí[jk;jR(i);j3] .
. . .. . . rest of S1
S1 . . .. . .
...
...
(8)
xi,j , xj,3 , x3 ,i
xi,k , xi,3 , xi,j , œÄ ‚àó
‚àó
xi,k , xj,3 , xi,j , œÄ
The ‚Äúrest of S1 ‚Äù will handle similarly the other literals  such that j ‚â∫œÄ  and i 	‚â∫œÄ .
Note that the Ô¨Ånal clause of S1 is the transitivity axiom Ti,j,3 . The subderivation S1 is
formed in exactly the same way that S2 was formed above. Namely, depending on the
status of the guard variable xr ,s for Ti,j,3 , one of the following is done: (i) the clause Ti,j,3
is already learned and can be used as is, or (ii) one of xr ,s or xr ,s is added to the clause
and propagated down the proof, or (iii) the clause Ti,j,3 is inferred using resolution on xr ,s
and becomes learned.
To complete the construction of S1 , the inference (8) is repeated for each value of 
such that j ‚â∫œÄ  and i 	‚â∫œÄ . The result is that S1 has one unÔ¨Ånished leaf clause, and it
corresponds to the bipartite partial order œÄ1 .
That completes the construction of S for the subcase of (iv) where Ti,j,k is of type (Œ≥).
Now suppose Ti,j,k is of type (Œ≤). (For instance, the values i, j, k of Figure 3.) In this case,
S will have the form
S3 . . .. . .
Ti,j,k , xr,s Ti,j,k , xr,s
...
S4 . . .. . .
Ti,j,k
xi,j , xi,k , œÄ ‚àí[jR(i),kR(i‚à™j)]
...
xi,j , xj,k , œÄ ‚àí[jR(i),kR(i‚à™j)]
xi,j , xk,j , œÄ ‚àí[jR(i‚à©k)]
xi,j , œÄ ‚àí[jR(i‚à©k)]
œÄ

S5 . . .. . .
...
xj,i , œÄ ‚àí[iR(j)]

where xr,s is the guard variable for Ti,j,k . We write [œÄ ‚àí[jR(i‚à©k)] ] to mean the negations
of literals in œÄ omitting any literal xj, such that both i ‚â∫œÄ  and k ‚â∫œÄ . Similarly,
œÄ ‚àí[jR(i),kR(i‚à™j)] indicates the negations of literals in œÄ, omitting the literals xj, such that
i ‚â∫œÄ  and the literals xk, such that either i ‚â∫œÄ  or j ‚â∫œÄ .
Note that the resolution on xr,s used to derive Ti,j,k does not violate regularity, since
otherwise Ti,j,k would have been covered by case (ii). Likewise, the resolutions on xi,j , xi,k
and xj,k do not violate regularity since Ti,j,k is of type (Œ≤).
The subderivation S5 is formed exactly like S2 above, with the exception that now the
literal xj,k is not present. Thus we omit the description of S5 .
We next describe the construction of S4 . Let C4 be the Ô¨Ånal clause ofS4 ; it is easy to
check that œÑ (C4 ) is a dag. As before, we must derive C4 from the clause ( œÄ 4 ) where œÄ4 is
the bipartite partial order associated with œÑ (C4 ). A typical situation is shown in Figure 7.
693

Bonet, Buss, & Johannsen

1

2

1

3

j
i
k
(a) xi,j , xk,j , xj,2 , œÄ ‚àó

2

3

j
k
(b) xi,j , xi,2 , xk,j , xk,2 , œÄ ‚àó
i

Figure 7: The partial orders as changed by S4 .
As pictured there, it is necessary to add the literals xi, such that j ‚â∫œÄ  and i 	‚â∫œÄ , while
removing xj, ; examples of this are  equal to 2 and 3 in Figure 7. At the same time, we
must add the literals xk, such that j ‚â∫œÄ  and k 	‚â∫œÄ , while removing xj, ; examples of
this are  equal to 1 and, again, 2 in the same Ô¨Ågure.
For a vertex 3 such that j ‚â∫œÄ 3 and k ‚â∫œÄ 3 but i 	‚â∫œÄ 3 , this is done similarly to the
inferences (7) and (8) but without the side literal xj,k :
. . .. . . rest of S4
S4 . . .. . .
...
...
xi,j , xj,3 , x3 ,i
xi,3 , xk,j , xi,j , œÄ ‚àó
xj,3 , xk,j , xi,j , œÄ ‚àó

(9)

Here œÄ ‚àó is œÄ ‚àí[jR(i‚à©k);j3] . The transitivity axiom Ti,j,3 shown as the last line of S4 is handled
exactly as before. This construction is repeated for all such 3 ‚Äôs.
The vertices 1 such that j ‚â∫œÄ 1 and i ‚â∫œÄ 1 but k 	‚â∫œÄ 1 are handled in exactly the
same way. (The side literals œÄ ‚àó change each time to reÔ¨Çect the literals that have already
been replaced.)
Finally, consider a vertex 2 such that i 	‚â∫œÄ 2 and j ‚â∫œÄ 2 and k 	‚â∫œÄ 2 . This is handled
by the derivation
S4 . . .. . .
...
xi,j , xj,2 , x2 ,i

. . .. . . rest of S4
S4 . . .. . .
...
...
xk,j , xj,2 , x2 ,k
xi,j , xi,2 , xk,j , xk,2 , œÄ ‚àó
xi,j , xi,2 , xk,j , xj,2 , œÄ ‚àó
xi,j , xk,j , xj,2 , œÄ ‚àó

As before, the set œÄ ‚àó of side literals is changed to reÔ¨Çect the literals that have already
been added and removed as S4 is being created. The subderivations S4 and S4 of the
transitivity axioms Ti,j,2 and Tk,j,2 are handled exactly as before, depending on the status
of their guard variables.
Finally, we describe how to form S3 . For this, we must form the bipartite partial order œÄ3
which is associated with œÑ (C3 ), where C3 is the Ô¨Ånal clause of S3 . To obtain œÄ 3 , we need
to add the literals xi, such that i 	‚â∫œÄ  and such that either j ‚â∫œÄ  or k ‚â∫œÄ , while
removing any literals xj, and xk, . This is done by exactly the same construction used
above in (9). The literals in œÄ ‚àí[jR(i);kR(i‚à™j)] are exactly the literals needed to carry this
out. The construction is quite similar to the above constructions, and we omit any further
description.
That completes the description of how to construct the LR partial refutations Rt . The
process stops once some Rt has no unÔ¨Ånished clauses. We claim that the process stops after
polynomially many stages.
694

Separations of Regular Resolution

To prove this, recall that Rt+1 is formed by handling the leftmost unÔ¨Ånished clause
using one of cases (i)-(iv). In the Ô¨Årst three cases, the unÔ¨Ånished clause is replaced by a
derivation based on PœÄ for some bipartite order œÄ. Since PœÄ has size O(n3 ), this means
that the number of clauses in Rt+1 is at most the number of clauses in Rt plus O(n3 ).
Also, by construction, Rt+1 has one fewer unÔ¨Ånished clauses than Rt . In case (iv) however,
Rt+1 is formed by adding up to O(n) many clauses to Rt plus adding either two or three
new unÔ¨Ånished leaf clauses. In addition, case (iv) always causes at least
	n
 one transitivity
axiom Ti,j,k to be learned.	 
Therefore, case (iv) can occur at most 2 3 = O(n3 ) times.
Consequently at most 3 ¬∑ 2 n3 = O(n3 ) many unÔ¨Ånished clauses are added throughout the
entire process.
that the process stops with Rt having no unÔ¨Ånished clauses for
	n
 It follows
3
some i ‚â§ 6 3 = O(n ). Therefore there is a pool refutation of GGTn with O(n6 ) lines.
Since the GGTn principle has O(n3 ) many clauses, the number of inferences in the refutation
is bounded by a quadratic polynomial of the number of the clauses being refuted.
By inspection, each clause in the refutation contains O(n2 ) literals. This is because
the largest clauses are those corresponding to (small modiÔ¨Åcations of) bipartite partial
orders, and because bipartite partial orders can contain at most O(n2 ) many ordered pairs.
Furthermore, the refutations Pn for the GTn contain only clauses of size O(n2 ).
Q.E.D. Theorem 18
Theorem 19 is proved with nearly the same construction. In fact, the only change
needed is the construction of S from PœÄ . Recall that in the proof of Theorem 18, the pool
derivation S was formed by using a depth-Ô¨Årst traversal of PœÄ . This is not suÔ¨Écient for
Theorem 19, since now the derivation S must use only input lemmas. Instead, we again
use the same result as before of Buss et al. (2008, Thm. 3.3), which states that a (regular)
dag-like resolution derivation can be transformed into a (regular) tree-like derivation with
input lemmas. Forming S in this way from PœÄ suÔ¨Éces for the proof of Theorem 19: the
lemmas of S are either transitive closure axioms derived earlier in Rt or are derived by
input subderivations earlier in the post-order of S. Since the transitive closure axioms that
appeared earlier in Rt were derived by resolving two GGTn axioms, the lemmas used in S
are all input lemmas.
The transformation of the theorem of Buss et al. (2008, Thm. 3.3) may multiply the size
of the derivation by the depth of the original derivation. The proofs PœÄ have depth O(n),
so the regRTI refutation has overall size O(n7 ). This completes the proof of Theorem 19.

5. Greedy and Unit Propagating CDCL
This section proves that the guarded graph tautology clauses GGTn can be refuted by a
polynomial-time greedy, unit propagating CDCL search without restarts. One diÔ¨Éculty is
that we do not even know whether regular dag-like resolution refutations can be polynomially simulated by greedy, unit propagating CDCL without restarts. Therefore, we must
Ô¨Årst establish Theorems 26 and 30 showing that there are greedy, unit propagating CDCL
refutations without restarts for the graph tautologies GTn and GTœÄ,n . Theorem 31 then
gives the polynomial-time greedy, unit propagating CDCL algorithm (without restarts) for
GGTn . The intuition is that the CDCL search traverses the regRTI refutations for GGTn
695

Bonet, Buss, & Johannsen

from Theorem 19, learning transitivity clauses Ti,j,k whenever possible. We will also need
to use the notion of ‚Äúabsorption‚Äù used by Atserias et al. (2011) and Pipatsrisawat and
Darwiche (2011).
We give a quick overview of CDCL search algorithms and clause learning algorithms; for
greater detail, see the works of Marques-Silva and Sakallah (1999) and Beame et al. (2004).
Given a set of clauses Œì as input, the CDCL search maintains a stack of literals assigned
the value True and a collection Œî of learned clauses. A simpliÔ¨Åed, but general, form of the
procedure for CDCL without restarts is as follows:
Input: A set Œì of clauses.
Algorithm:
Set Œî := ‚àÖ.
Loop:
If unit propagation from Œì ‚à™ Œî yields a contradiction,
Halt. Œì is unsatisÔ¨Åable.
Else if unit propagation from the assigned literals and Œì ‚à™ Œî
yields a contradiction,
Infer zero or more clauses by input resolution based on the
conÔ¨Çict, and add them to Œî. (Learning)
Select Œî‚àí ‚äÜ Œî, and set Œî = Œî \ Œî‚àí . (Garbage collection)
Unassign the last assigned literal. (Backtracking)
Else if all literals are assigned a value,
Halt. Œì is satisÔ¨Åable.
Else
Choose an unassigned literal, and assign it the value True.
Endif
The above-described CDCL algorithm lacks many important features of the usual implementations of CDCL algorithms. However, this simpliÔ¨Åed CDCL algorithm can polynomially simulate more sophisticated implementations of CDCL by learning and unlearning
the appropriate clauses. Conversely, more sophisticated CDCL algorithms can polynomially simulate the simpliÔ¨Åed CDCL algorithm, and since we are proving a polynomial time
runtime for the simpliÔ¨Åed algoithm, this certainly implies a polynomial time runtime for
more sophisticated CDCL algorithms (subject as usual to specifying which decision literals
are set and which clauses are learned and garbage collected).
The clause learning used in Theorems 26 and 30 is not the usual Ô¨Årst-UIP (unique
implication point) clause learning, but does learn only clauses that are obtained by picking
a cut in the conÔ¨Çict graph, as is common for CDCL algorithms.
Theorem 26. Greedy, unit propagating CDCL search without restarts can refute the GTn
clauses in polynomial time.
Proof. Recall the construction of the refutations Pn of the GTn clauses as pictured in
Figure 1. The key property needed is that the derivation of Totk,j from Totk+1,k+1 and
Totk+1,j is an input derivation, and this will permit learning Totk,j once Totk+1,k+1 and
Totk+1,j have been learned.
696

Separations of Regular Resolution

We will give a direct description of the CDCL search. The search initially chooses to
set the literals x1,0 , x2,1 . . ., xn‚àí2,n‚àí3 true as decision literals, in that order, so xi+1,i is
set true at (decision) level i + 1. A literal xi,j is interpreted as meaning that i ‚â∫ j in the
partial order ‚â∫; thus the decision literals express that n ‚àí 2 ‚â∫ n ‚àí 3 ‚â∫ ¬∑ ¬∑ ¬∑ ‚â∫ 1 ‚â∫ 0 holds.
Unit propagation with the transitive closure clauses now implies the literals xj,i , or in other
words that j ‚â∫ i, for 0 ‚â§ i < j ‚â§ n ‚àí 2. The literal xj,i is derived at level j.
Until reaching the literal xn‚àí2,n‚àí3 , no other unit propagations are possible. But, upon
assigning xn‚àí2,n‚àí3 true, the clause Totn‚àí1,n‚àí2 becomes a unit clause, and xn‚àí1,n‚àí2 is inferred. In other words, n ‚àí 1 ‚â∫ n ‚àí 2 is inferred. Now, from the literals xn‚àí2,j , further unit
propagations with transitive closure clauses gives xn‚àí1,j , that is n ‚â∫ j, for all j < n‚àí2. This
falsiÔ¨Åes Totn‚àí1,n‚àí1 , and yields a conÔ¨Çict, so the CDCL search backtracks to the previous
level.
As it backtracks, the CDCL algorithm learns the two clauses Totn‚àí3,n‚àí3 ‚à® xn‚àí3,n‚àí2 and
Totn‚àí2,n‚àí2 , and sets the decision literal xn‚àí2,n‚àí3 false at level n ‚àí 3. The Ô¨Årst clause is
learned by taking this decision literal xn‚àí2,n‚àí3 as the UIP, since the literals xn‚àí3,j for j <
n‚àí3 were the literals of level < n‚àí3 that were used to infer level n‚àí2 literals in the conÔ¨Çict
graph. To see it is possible to learn the second clause Totn‚àí2,n‚àí2, note that the conÔ¨Çict was
obtained via unit propagation from the decision literal xn‚àí2,n‚àí3 and the literals xn‚àí2,j , and
that Totn‚àí2,n‚àí2 contains exactly the negations of these literals. The literals xn‚àí2,j were all
set at level n ‚àí 2, so Totn‚àí2,n‚àí2 was not learned from a UIP; nonetheless, it falls within
what is commonly permitted for CDCL learning. Once Totn‚àí3,n‚àí3 ‚à® xn‚àí3,n‚àí2 is learned,
the literal xn‚àí3,n‚àí2 follows by a single unit propagation.
After this Ô¨Årst backtrack, the decision literals x1,0 , x2,1 , . . ., xn‚àí3,n‚àí4 have been set true
as decision literals, Totn‚àí2,n‚àí2 and Totn‚àí3,n‚àí3 ‚à® xn‚àí3,n‚àí2 are learned, and xn‚àí3,n‚àí2 is true
at level n ‚àí 3 by unit propagation. It is possible to use unit propagation to get yet another
conÔ¨Çict. However, instead of describing this, we go to the general cases.
In the general case, there are parameters , r, s so that 0 ‚â§  < r < s < n with either
s = r + 1 or s = n ‚àí 1. In this general case, the literals x1,0 , x2,1 , . . ., x,‚àí1 are set true
as the Ô¨Årst  decision literals and the literals x,+2, x,+3 , . . . , x,r are set as the next
decision literals, and the three clauses Tots, , Tot, ‚à® x,+1 , and Totk,k for all k >  have
been learned. (Refer to Figure 1. The situation in the previous paragraph has  = n ‚àí 3,
r = n ‚àí 2, and s = n ‚àí 1.) Using Tot, ‚à® x,+1 and unit propagation, the literal x,+1 is
set at level . In terms of ‚â∫, the literals which are set true express the conditions
 ‚â∫  ‚àí 1 ‚â∫ ¬∑¬∑¬∑ ‚â∫ 1 ‚â∫ 0

and  ‚â∫ j for j =  + 1, . . . , r.

(10)

If r < s ‚àí 1 = n ‚àí 2, unit propagation does not yield a contradiction, and the CDCL search
proceeds by setting x,r+1 true as the next decision literal. The CDCL search is now in the
general case with parameters , r+1, n‚àí1.
When r = s ‚àí 1, unit propagation with transitivity clauses yields x,j , namely  ‚â∫ j, for
all j ‚àà [s] \ {}. This makes Tots, a unit clause and thus xs, , namely s ‚â∫ , is set true by
unit propagation. Using the literals x,j again, unit propagation with transitivity clauses
falsiÔ¨Åes the learned clause Tots,s . The CDCL algorithm now backtracks one level. When
r >  + 1, it learns Tots‚àí1, . This can be learned since the conÔ¨Çict was obtained from the
literals x,j for j ‚àà [s] \ {}. The learned clause Tots, is forgotten by garbage collection.
This puts the CDCL search in the new general situation with parameters , r‚àí1, s‚àí1.
697

Bonet, Buss, & Johannsen

For r =  + 1 = s ‚àí 1, the two clauses Tot, and Tot‚àí1,‚àí1 ‚à® x‚àí1, are learned instead.
The second clause is the clause obtained by the usual learning algorithm using the decision
literal x,‚àí1 as the UIP. The clause Tot, can be learned since the conÔ¨Çict graph used the
literals x,j for j <  to obtain the conÔ¨Çict: this is not obtained by a UIP, but it is obtained
by by using a cut in the conÔ¨Çict graph. Garbage collection is used to forget the clauses
Tot, ‚à® x,+1 and Tots, . This puts the CDCL search in the general case, with the new
, r, s parameters equal to  ‚àí 1, , and n ‚àí 1, respectively.
The CDCL search ends after dealing with the case  = 0, r = 1 and s = 2. In this case,
the empty clause Tot0,0 is learned, and the CDCL search halts, establishing that GTn is
unsatisÔ¨Åable.
The next theorem discusses how a greedy, unit propagating CDCL search can simulate
the derivations PœÄ of the graph tautologies GTœÄ,n obtained by restricting GTn with a
bipartite partial order œÄ. Of course, this does not fully make sense, since CDCL search
only simulates refutations, not derivations. What we really want is to simulate PœÄ as a
subderivation of the regRTI refutations of GGTn as constructed in Theorem 19. For this,
let C be a clause in the regRTI refutation of GGTn . Let œÑ be the dag such that i ‚â∫œÑj iÔ¨Ä
xi,j ‚àà C pool , and let œÄ be the associated bipartite partial order, so C is the clause ( œÄ).
We claim there is a greedy, unit propagating CDCL refutation of the (non-guarded) GTœÄ,n
clauses from the literals xi,j such that i ‚â∫œÑ j. In other words, greedy, unit propagating
CDCL search can refute the GTn clauses once it has set the literals in C pool false.
To state Theorem 30 in full generality, we need the following deÔ¨Ånitions. These are
modiÔ¨Åcations of deÔ¨Ånitions by Atserias et al. (2011) and Pipatsrisawat and Darwiche (2011).
Definition 27. Let C be a clause and x a literal in C. We say C is u.p.-absorbed at x by
a set Œì of clauses provided that when every literal in C \ {x} is set false, then x is implied
by unit propagation using clauses from Œì. We say C is u.p.-absorbed by Œì provided it is
u.p.-absorbed by Œì at every x ‚àà C. We say C is absorbed by Œì provided that when all
literals in C are set false, then unit propagation using Œì yields a contradiction.
Once a clause C becomes u.p.-absorbed, it becomes redundant in terms of unit propagation. In particular, adding C as an additional clause would not yield any additional
conÔ¨Çicts and would not make it possible to learn any additional clauses from conÔ¨Çicts.
We need a slightly more general deÔ¨Ånition, however.
Definition 28. Let C and Œì be as above, and let L be a set of literals. We write L to denote
the set of unit clauses {x} for x ‚àà L, i.e. the clauses asserting every x ‚àà L is false. Then C
is L-absorbed or L-u.p.-absorbed (at x) by Œì provided it is absorbed or u.p.-absorbed (at x)
by Œì ‚à™ L, respectively.
Suppose C is L-u.p.-absorbed. The intuition is that Œì is the current set of initial and
learned clauses for a CDCL search S, and that the literals in L have all been set false (either
as decision literals or by unit propagation). Then, unit propagation from Œì ‚à™ L yields a
contradiction if and only if unit propagation from Œì ‚à™ L ‚à™ {C} does. The learned clause for
the former may need to include additional literals from L however. To state this formally:
Lemma 29. Let Œì be a set of clauses, L be a set of literals set false as above, and C be
L-u.p.-absorbed by Œì. Suppose unit propagation from Œì ‚à™ L ‚à™ {C} yields a contradiction and
698

Separations of Regular Resolution

allows a clause D to be learned. Then unit propagation from Œì‚à™L also yields a contradiction,
and allows a clause D  to be learned such that D  ‚äÜ D ‚à™ L.
The proof of Lemma 29 is almost immediate from the deÔ¨Ånitions, and is left to the reader.
Note that the additional literals from L which appear in D  are precisely the negations of
the literals of L which are needed to simulate the invocation of C for unit propagation.
Theorem 30. Let œÑ define a dag on [n], and let œÄ ‚äÜ œÑ be the bipartite partial order
associated with œÑ . Then there is a greedy, unit propagating CDCL search that finds a
refutation of the GTœÄ,n clauses and the unit clauses xi,j such that i ‚â∫œÑ j.
Proof. As in the proof of Lemma 25, let MœÄ = [m] and Jk be such that Jk ‚â∫œÄ k for k ‚â• m.
Let L be the set of literals xi,j such that i ‚â∫œÄ j. Let S be the greedy, unit propagating
CDCL search refuting the GTm clauses of size polynomial in m given by Theorem 26. We
wish to prove that S can be transformed into a greedy, unit propagating CDCL search S 
that refutes the GTœÄ,n clauses and the clauses L. The search S  will mimic S very closely,
setting decision literals in the same order as S, but will learn the clauses TotœÄi,j in place of
the clauses Toti,j . (Compare Figures 1 and 4.)
S  must use the GTœÄ,n clauses as initial clauses instead of the GTm clauses used by S.
First consider a GTm transitivity clause used by S; this is of type (Œ≤‚àÖ ) from DeÔ¨Ånition 17
and is equal to Ti,j,k for distinct i, j, k < m. As such, it is also a GTœÄ,n clause of type (Œ≤)
and thus is already available for S  to use.

Second, consider a totality clause Totm‚àí1,i = k‚àà[m]\{i} xk,i of type (Œ±‚àÖ ) that is used for
unit propagation by the search S. This clause may not be L-u.p.-absorbed by the GTœÄ,n
clauses. But, if not, then it is L-absorbed by GTœÄ,n and this will suÔ¨Éce for the search S  to
succeed. To see that it is L-absorbed, suppose that S is at a point where unit propagation
with Totm‚àí1,i is used to obtain a conÔ¨Çict. In this case, all but one literal of Totm‚àí1,i
have been set false, namely there is a j0 ‚àà [m] \ {i} so that S has set the literals xj,i for
j ‚àà [m] \ {i, j0 } false and now infers xj0 ,i by unit propagation. The new search S  also has
set the literals xj,i for j ‚àà [m] \ {i, j0 } false and needs to infer xj0 ,i ; but S  must use the
GTœÄ,n clause Totn‚àí1,i instead of the GTm clause Totm‚àí1,i . Consider any xk,i in Totn‚àí1,i
with k ‚â• m. If there is a j ‚àà [m] \ {i, j0 } such that j ‚â∫œÄ k (we let Jk denote any such j),
then, since xj,k ‚àà L, unit propagation with the transitivity clause Ti,j,k sets xi,k true, i.e.,
falsiÔ¨Åes xk,i in Totn‚àí1,i . If this holds for all k ‚â• m, then Totn‚àí1,i is reduced to the unit
xj0 ,i , so Totm‚àí1,i is L-u.p.-absorbed at xj0 ,i by GTœÄ,n . Otherwise Totm‚àí1,i is not L-u.p..absorbed. In this case, let k1 , . . . , kR be the values ‚â• m such that j0 ‚â∫œÄ kr for 1 ‚â§ r ‚â§ R,
so the just-described unit propagation is able to reduce Totn‚àí1,i to the (non-unit) clause
xj0 ,i ‚à® xk1 ,i ‚à® ¬∑ ¬∑ ¬∑ ‚à® xkR ,i . The search S  now branches to set xj0 ,i false as a decision literal.
Unit propagation with the transitivity clauses Ti,j0 ,kr , for r ‚â§ R falsiÔ¨Åes the literals xkr ,i and
thereby falsiÔ¨Åes Totn‚àí1,i . Then S  backtracks and learns the clause SiœÄ ‚à® Totm‚àí1,i (see (5)).
This of course is just the clause TotœÄm‚àí1,i . With TotœÄm‚àí1,i learned, and since the literals
in SiœÄ are in L and are set false, S  can now obtain a conÔ¨Çict in the same as way as S.
In general, the literals in TotœÄi,j \ Toti,j are all members of L, and so are set false by S  .
It is thus straightforward to check that the remaining steps of S can be simulated directly
by S  using exactly the same decision literals, and obtaining conÔ¨Çict clauses in the same
way, but now learning the clauses TotœÄi,j instead of the clauses Toti,j .
699

Bonet, Buss, & Johannsen

We can now describe the greedy and unit propagating CDCL without restart procedures
that refute the GGTn clauses.
Theorem 31. There are CDCL without restart search procedures which are greedy and unit
propagating, and refute the GGTn clauses in polynomial time.
Proof. The basic idea for the greedy, unit propagating CDCL search S is that it follows the
structure of the refutation described in the proofs of Theorems 18 and 19; that is, S follows
the stages of the construction of LR-partial refutations.
At a stage of S that corresponds to an unÔ¨Ånished clause C of a LR-partial refutation,
S has some set of transitivity clauses Ti,j,k as its only learned clauses, and has set zero or
more literals xi,j true. These literals describe a dag œÑ , namelyi ‚â∫œÑ j iÔ¨Ä xi,j has been set
true. Let œÄ be the associated bipartite partial order, so C = ( œÄ). Then S has set every
xi,j true for which i ‚â∫œÄ j. If possible, S will now use the refutation of Theorem 30 to
backtrack from the current stage; otherwise, S will branch to learn a another transitivity
clause. The Ô¨Årst possibility holds provided that every transitivity clause Ti,j,k , of type (Œ≤)
or (Œ≥) needed for the refutation of GTœÄ,n is either learned or C pool -u.p.-absorbed by S.
Otherwise, suppose some transitivity clause Ti,j,k of type (Œ≥) is neither learned nor
pool
-u.p.-absorbed. Following the pattern of the argument for this case in the proof of
C
Theorem 18, S sets xi,j true as a decision literal. Since Ti,j,k was of type (Œ≥), xi,j has not
already been set. We claim that setting xi,j true does not yield a contradiction by unit
propagation. To prove this claim, note that the only way unit propagation can occur is
if some (guarded or unguarded) transitivity clause becomes a unit clause. Referring back
to Figure 3, the only (possibly guarded) transitivity clauses which can become unit upon
setting xi,j true are clauses Ti,j, for  ‚â• m such that S has set xj, true. Therefore unit
propagation does not yielda contradiction from xi,j . S now sets xk,i true. Since Ti,j,k
is of type (Œ≥) and C = ( œÄ), S has set xj,k true; therefore, unit propagation yields a
contradiction from Ti,j,k ‚à® œÅ and Ti,j,k ‚à® œÅ. From this, S backtracks, setting xi,k true and
learning Ti,j,k .
After backtracking, S is at a stage corresponding to the clause C1 of the subrefutation S1
in the proof of Theorem 18. Let L1 be the literals that are set false, œÑ1 be the dag deÔ¨Åned by
the literals of L1 , and let œÄ1 be the associated bipartite order. S needs to infer the literals
xi, for i ‚â∫œÄ1 . This is done using transitivity clauses. Any L1 -u.p.-absorbed transitivity
clause can be used directly. For other transitivity clauses Ti,j, , once xi,j and xj, are set true,
then S branches to set x,i false as a decision literal, obtains an immediate contradiction by
unit propagation from the two GGTn clauses for Ti,j, and backtracks, learning Ti,j, and
inferring xi, by unit propagation. Once all the literals xi, where i ‚â∫œÄ1  are set true, S is
again at a stage corresponding to an unÔ¨Ånished clause.
Once S backtracks out of the stage corresponding to the clause C1 , S enters the stage
corresponding to C2 . This is handled similarly.
The argument for the case where a transitivity clause Ti,j,k of type (Œ≤) has not been
learned is very similar. Again we follow the construction in the proof of Theorem 18, and
now use stages that correspond to clauses C3 , C4 , and C5 . Since the construction is quite
similar, we omit its description.
Theorem 31 shows a very close correspondence between the regRTI proofs of the GGTn
clauses and greedy, unit propagating CDCL refutations without restarts. It is open, how700

Separations of Regular Resolution

ever, whether an arbitrary regRTI or regWRTI refutation can be converted into a polynomial
size greedy, unit propagating CDCL refutation without restarts. It is even open whether every (dag-like) regular refutation corresponds to a greedy, unit propagating CDCL refutation
without restarts.

Acknowledgements
We are grateful to J. HoÔ¨Ämann for assisting with a correction to an earlier version of the
proof of Theorem 19. We also thank A. Van Gelder, L. Kolodziejczyk, A. Beckmann,
T. Pitassi, and three anonymous referees for encouragement, suggestions, and useful substantial comments.
M. L. Bonet was supported in part by grant TIN2010-20967-C04-02, and by a Research
Abroad Fellowship (Generalitat de Catalunya BE, 2012).
S. Buss was supported in part by National Science Foundation grants DMS-0700533,
DMS-1101228 and CCF-1213151, and by a grant from the Simons Foundation (#208717 to
Sam Buss). He also thanks the John Templeton Foundation for supporting his participation
in the CRM InÔ¨Ånity Project at the Centre de Recerca MatemaÃÄtica, Barcelona, Catalonia,
Spain during which some of these results were obtained.
S. Buss and J. Johannsen thank the BanÔ¨Ä International Research Station for the workshop on Proof Complexity (11w5103) held in October 2011 during which part of these
results were obtained.

References
Alekhnovich, M., Johannsen, J., Pitassi, T., & Urquhart, A. (2007). An exponential separation between regular and general resolution. Theory of Computing, 3 (5), 81‚Äì102.
Alekhnovich, M., & Razborov, A. A. (2001). Resolution is not automatizable unless W [P ]
is tractable. In Proc. 42nd IEEE Conf. on Foundations of Computer Science (FOCS),
pp. 210‚Äì219.
Atserias, A., Fichte, J. K., & Thurley, M. (2011). Clause-learning algorithms with many
restarts and bounded-width resolution. Journal of Artificial Intelligence Research, 40,
353‚Äì373.
Beame, P., Kautz, H. A., & Sabharwal, A. (2004). Towards understanding and harnessing
the potential of clause learning. J. Artificial Intelligence Research, 22, 319‚Äì351.
Beckmann, A., & Buss, S. R. (2005). Separation results for the size of constant-depth
propositional proofs. Annals of Pure and Applied Logic, 136, 30‚Äì55.
Ben-Sasson, E. (2009). Size space tradeoÔ¨Äs for resolution. SIAM Journal on Computing,
38 (6), 2511‚Äì2525.
Ben-Sasson, E., Impagliazzo, R., & Wigderson, A. (2004). Near optimal separation of treelike and general resolution. Combinatorica, 24 (4), 585‚Äì603.
Bonet, M. L., & Buss, S. R. (2012a). An improved separation of regular resolution from pool
resolution and clause learning. In Proc. 15th International Conference on Theory and
701

Bonet, Buss, & Johannsen

Applications of Satisfiability Testing ‚Äì SAT 2012, Lecture Notes in Computer Science
#7317, pp. 45‚Äì57.
Bonet, M. L., & Buss, S. R. (2012b). An improved separation of regular resolution from pool
resolution and clause learning. Full version, arxiv.org, arXiv:1202.2296v2 [cs.LO].
Bonet, M. L., & Galesi, N. (2001). Optimality of size-width tradeoÔ¨Äs for resolution. Computational Complexity, 10 (4), 461‚Äì474.
Buss, S., & Kolodziejczyk, L. (2012). Small stone in pool. Submitted for publication.
Buss, S. R. (2009). Pool resolution is NP-hard to recognise. Archive for Mathematical Logic,
48 (8), 793‚Äì798.
Buss, S. R., HoÔ¨Ämann, J., & Johannsen, J. (2008). Resolution trees with lemmas: Resolution
reÔ¨Ånements that characterize DLL-algorithms with clause learning. Logical Methods
in Computer Science, 4, 4:13 (4:13), 1‚Äì18.
Chang, C. L. (1970). The unit proof and the input proof in theorem proving. J. ACM,
17 (4), 698‚Äì707.
Goerdt, A. (1993). Regular resolution versus unrestricted resolution. SIAM Journal on
Computing, 22 (4), 661‚Äì683.
Hertel, P., Bacchus, F., Pitassi, T., & Van Gelder, A. (2008). Clause learning can eÔ¨Äectively
p-simulate general propositional resolution. In Proc. 23rd AAAI Conf. on Artificial
Intelligence (AAAI 2008), pp. 283‚Äì290. AAAI Press.
Huang, W., & Yu, X. (1987). A DNF without regular shortest consensus path. SIAM
Journal on Computing, 16 (5), 836‚Äì840.
Johannsen, J. (2009). An exponential lower bound for width-restricted clause learning.
In Proc. 12th International Conference on Theory and Applications of Satisfiability
Testing ‚Äì SAT 2009, Lecture Notes in Computer Science #5584, pp. 128‚Äì140.
Krishnamurthy, B. (1985). Short proofs for tricky formulas. Acta Informatica, 22 (3), 253‚Äì
275.
Marques-Silva, J. P., & Sakallah, K. A. (1999). GRASP ‚Äî A new search algorithm for
satisÔ¨Åability. IEEE Transactions on Computers, 48 (5), 506‚Äì521.
Pipatsrisawat, K., & Darwiche, A. (2011). On the power of clause-learning SAT solvers as
resolution engines. Artificial Intelligence, 172 (2), 512‚Äì525.
Segerlind, N., Buss, S. R., & Impagliazzo, R. (2004). A switching lemma for small restrictions
and lower bounds for k-DNF resolution. SIAM Journal on Computing, 33 (5), 1171‚Äì
1200.
StaÃälmarck, G. (1996). Short resolution proofs for a sequence of tricky formulas. Acta
Informatica, 33 (3), 277‚Äì280.
Urquhart, A. (2011). A near-optimal separation of regular and general resolution. SIAM
Journal on Computing, 40 (1), 107‚Äì121.
Van Gelder, A. (2005). Pool resolution and its relation to regular resolution and DPLL
with clause learning. In Logic for Programming, Artificial Intelligence, and Reasoning
(LPAR 2005), Lecture Notes in Computer Science 3835, pp. 580‚Äì594. Springer-Verlag.
702

Separations of Regular Resolution

Van Gelder, A. (2006). Preliminary report on input cover number as a metric for propositional resolution proofs. In Theory and Applications of Satisfiability Testing - SAT
2006, Lecture Notes in Computer Science 4121, pp. 48‚Äì53. Springer Verlag.

703

Journal of Artificial Intelligence Research 49 (2014) 569-600

Submitted 04/2013; published 04/2014

Inapproximability of Treewidth,
One-Shot Pebbling, and Related Layout Problems
Yu (Ledell) Wu
Per Austrin
Toniann Pitassi
David Liu

wuyu@cs.toronto.edu
austrin@cs.toronto.edu
toni@cs.toronto.edu
liudavid@cs.toronto.edu

Department of Computer Science
University of Toronto, Ontario, Canada

Abstract
Graphical models, such as Bayesian Networks and Markov networks play an important
role in artificial intelligence and machine learning. Inference is a central problem to be
solved on these networks. This, and other problems on these graph models are often
known to be hard to solve in general, but tractable on graphs with bounded Treewidth.
Therefore, finding or approximating the Treewidth of a graph is a fundamental problem
related to inference in graphical models. In this paper, we study the approximability of a
number of graph problems: Treewidth and Pathwidth of graphs, Minimum Fill-In, OneShot Black (and Black-White) pebbling costs of directed acyclic graphs, and a variety of
different graph layout problems such as Minimum Cut Linear Arrangement and Interval
Graph Completion. We show that, assuming the recently introduced Small Set Expansion
Conjecture, all of these problems are NP-hard to approximate to within any constant factor
in polynomial time.

1. Introduction
Graphical models provide a computational framework for efficiently manipulating probability distributions over high dimensional spaces, often involving hundreds of thousands of
variables. This framework has found applications in an enormous range of domains including: medical and fault diagnosis, image understanding, speech recognition, web search,
coding theory, and statistical physics (Koller & Friedman, 2009). A graphical model is an
efficient representation of a joint distribution over some set of n random variables. Even
if the random variables are binary, it is well known that an arbitrary joint distribution
requires the specification of 2n probabilities. Luckily, in the real world, there is often structure in the distribution that allows one to express it more succinctly. A graphical model
represents such a joint probability distribution by a graph where the vertices represent the
random variables, and the dependences are modeled by the graph structure. Associated
with each vertex of the graph is a conditional probability table, which specifies the conditional probabilities of this random variable, conditioned on its neighboring vertices. The two
most common types of graphical models are Bayesian networks (also called belief networks),
where the underlying graph is directed, and Markov networks (also called Markov random
fields), where the underlying graph is undirected. The most basic problem in graphical
models is the inference problem, which is the problem of computing the posterior marginal
c
2014
AI Access Foundation. All rights reserved.

Wu, Austrin, Pitassi, & Liu

distribution of a variable at some vertex. Unfortunately, inference in general is well-known
to be NP-hard to compute exactly as well as to approximate (Roth, 1996).
Despite this intractability, an important class of bounded Treewidth instances of probabilistic inference has been identified and shown to be exactly computable in polynomial
time. The Treewidth of a graph (Robertson & Seymour, 1984, 1986) is a fundamental parameter of a graph that measures how close the graph is to being a tree. Treewidth is very
closely related to the other notions in machine learning such as Branch-width, Clique-width
and Elimination-width (for an overview of Treewidth and related notions, see Bodlaender,
Gilbert, Hafsteinsson, & Kloks, 1995). On graphs with small Treewidth and where the
tree decomposition is known, a dynamic programming algorithm yields a polynomial-time
algorithm. Particular algorithms for probabilistic inference on bounded Treewidth graphs
are the junction-tree method, variable elimination and clique trees (e.g. see Koller & Friedman, 2009, ch. 9, 10). These algorithms runs in time exponential in the Treewidth of the
tree decomposition and polynomial in the size of the graph. Thus for graphs where a tree
decomposition of bounded Treewidth is given, inference is polynomial-time computable.
The same ideas also yield polynomial-time algorithms and often even linear time algorithms for small Treewidth instances for an astonishing variety of other NP-hard problems, including: satisfiability, counting satisfying assignments, constraint satisfaction, vertex cover, maximum independent set, Hamiltonian circuit, query optimization, matrix decomposition, and more generally all problems definable in monadic second-order logic. (See
the excellent survey Bodlaender, 2005 for motivation, including theoretical as well as practical applications of Treewidth.) One catch is that for all of these problems, the algorithm
must begin by finding a tree decomposition, and then use the decomposition to solve the
problem. Given the tree decomposition, the algorithm is typically exponential in the width
of the underlying tree decomposition. Thus there is a need for efficient algorithms to actually compute the Treewidth of a given graph, and to find tree decompositions with optimal
or close to optimal width.
Unfortunately, while there are many good heuristics for finding a good tree decomposition, it is NP-hard in general to determine the Treewidth of a graph (Arnborg, Corneil,
& Proskurowski, 1987). However, Bodlaender et al. (1995) obtained an O(log n) factor
approximation algorithm for Treewidth. In fact, they actually show that if there is a factor
c approximation algorithm for vertex separator, then there is an O(c) approximation algorithm for Treewidth. And if there is a factor b approximation algorithm for Treewidth then
there is an O(b log n) approximation algorithm for the related Pathwidth
problem. The
‚àö
best currently known approximation factor for vertex separator is O( log n) (Feige, Hajiaghayi, & Lee, 2005)p
and thus the best algorithm for Treewidth finds‚àö
a tree decomposition
that is within an O( log n) factor of the optimal width, and an O(( log n)(log n)) factor
approximation algorithm for Pathwidth.
It is a longstanding open question whether or not there is a constant factor approximation algorithm for Treewidth. Such an algorithm would lead to faster algorithms to
find good tree-decompositions for all of the problems mentioned above. The current best
known algorithm that achieves a constant factor approximation for Treewidth runs in time
2O(w) O(n), where w is the Treewidth of the underlying graph, and achieves a factor 5 approximation (Bodlaender, 2007) (Earlier approximation algorithms with similar runtimes
are Reed, 1992; Amir, 2001). In a book devoted to Treewidth Kloks (1994, p. 62) states:
570

Inapproximability of Treewidth and Related Problems

‚ÄúWe feel this is one of the biggest open problems in the research dealing with
Treewidth and Pathwidth at the moment. If the fast algorithms for solving NPhard problems for graphs with bounded Treewidth are ever to become of practical importance, it is undoubtedly of importance to find good tree-decompositions
for these graphs. Since the (current best) approximations do not make many of
these algorithms practical, it is of great interest to know whether approximations
with a small constant exist.‚Äù
Nearly twenty years later, it is still an open problem whether or not there is a polynomialtime algorithm to approximate Treewidth to within a constant factor. Similarly, the approximability of many related graph layout problems is also unresolved, including Minimum Cut
Linear Arrangement and Interval Graph Completion. In this paper, we make an important
step to resolve this problem by showing that Treewidth, Pathwidth, and a host of related
graph layout problems are hard to approximate to within any constant factor, under the
Small Set Expansion (SSE) conjecture (Raghavendra & Steurer, 2010).
The SSE conjecture is a strengthened version of the conjecture that P is different from
NP and warrants some explanation. In the next subsection (Section 1.1), we explain the
SSE conjecture, and how it relates to the P versus NP question and to related conjectures.
We then state our main hardness results for Treewidth, pebbling problems and graph layout
problems (Sections 1.2, 1.4, and 1.5), and discuss related results in Section 1.6.
1.1 The Small Set Expansion Conjecture
The P versus NP problem is the most important and intriguing open problem in the field of
computational complexity theory. Many decision problems in theory and practice have been
proven to be NP-hard, which indicates that they are impossible to compute in polynomial
time, under the widely believed conjecture that P 6= NP. The discovery of the PCP theorem
in the late 80‚Äôs (Arora, Lund, Motwani, Sudan, & Szegedy, 1998; Arora & Safra, 1998) made
it possible to prove that for many optimization problems, approximating the optimal value
to within a certain factor is as hard as computing the exact optimal value. In other words,
under the conjecture that P 6= NP, it is not possible to approximate certain optimization
problems within some factor that depends on the problem. Celebrated results show that
it is NP-hard to approximate MAX-3SAT within a ratio of 78 +  for any  > 0 (HaÃästad,
2001), which gives the optimal lower bound, since there is a simple algorithm that achieves
an approximation ratio of 78 . Also, it is NP-hard to approximate clique to within a n1‚àí
factor for any  > 0 (HaÃästad, 1999). Despite this success, for many important problems, the
hardness of approximation results obtained through the PCP theorem have not matched
the best approximation algorithms known. For example, there are still significant gaps in
our understanding of the optimal approximability factor for important problems such as:
Vertex Cover, Max-Cut, Bipartite Clique, and Kernel Clustering.
The formulation of the Unique Games Conjecture (UGC) due to Khot (2002) was intended to clarify the approximability of many optimization problems. The conjecture postulates that the problem of determining the value of a certain type of game, known as a
unique game, is NP-hard. The conjecture has inspired a remarkable body of work since its
formulation. Under UGC, many of the known algorithms in approximation are proven to be
tight (for an excellent survey on this topic, see Khot & Vishnoi, 2005). For instance, under
571

Wu, Austrin, Pitassi, & Liu

the UGC, the Vertex Cover problem is NP-hard to approximate within a factor of 2 ‚àí ,
for any  > 0 (Khot & Regev, 2008). Perhaps most strikingly, Raghavendra (2008) proved
that under the UGC, the semi-definite programming (SDP) approximation algorithm for a
large class of constraint satisfaction problems (CSP) are essentially the best one can hope
for. More specifically, he showed that every maximum constraint satisfaction problem (Max
CSP) has an associated sharp approximation threshold œÑ : for every  > 0, one can achieve
a œÑ ‚àí  approximation in polynomial time using SDP, but obtaining a œÑ +  approximation
is NP-hard. Thus, the UGC has become the central open problem in inapproximability and
encapsulates the barrier of designing better polynomial time approximation algorithms for
a large class of problems.
Despite this tremendous progress, still there remain important yet stubborn problems
such as Treewidth, Balanced Separator, Minimum Linear Arrangement (MLA), and many
other graph layout problems whose approximation status remains unresolved even assuming
the UGC. Intuitively this is because the hard instances for these problems seem to require
a certain global structure such as expansion. (Expansion is a graph property that is akin to
high connectivity, and requires that every subset of vertices that is not too large has a large
boundary.) Typical reductions for these problems are gadget reductions which preserve
global properties of the unique games instance, such as the lack of expansion. Therefore,
barring radically new types of reductions that do not preserve global properties, proving
hardness for these problems seems to require a stronger version of UGC, where the instance
is guaranteed to have certain expansion properties.
In the work of Raghavendra and Steurer (2010), the Small Set Expansion (SSE) Conjecture was introduced, and it was shown that it implies the UGC, and that the SSE Conjecture
follows if one assumes that the UGC is true for somewhat expanding graphs. In follow-up
work by Raghavendra et al. (2012), it was shown that the SSE Conjecture is in fact equivalent to the UGC on somewhat expanding graphs, and that the SSE Conjecture implies
NP-hardness of approximation for balanced separator and MLA. In this light, the Small
Set Expansion conjecture serves as a natural unified conjecture that yields all of the implications of UGC and also hardness for expansion-like problems that could not be resolved
with the UGC.
Our main contribution in this paper is to prove that a wide range of other graph layout
problems are SSE-hard to approximate to within any constant factor. For these problems,
no evidence of hardness of approximation was known prior to our results. Moreover, we
show that Treewidth, Pathwidth, and Minimum Fill-In are SSE-hard to approximate within
any constant factor. This is the first result giving hardness of (relative) approximation for
these problems, and gives evidence that no constant factor approximation algorithm exists
for them.
It should be noted that the status of the SSE conjecture is very open at this point. In
particular, recent results (Arora, Barak, & Steurer, 2010; Barak, Raghavendra, & Steurer,
2011; Guruswami & Sinop, 2011) give subexponential-time algorithms for small set expansion. Still despite this recent progress providing evidence against the SSE conjecture, it
remains open. Our SSE-hardness results for Treewidth and related problems may therefore
be viewed as establishing a new connection between a fundamental conjecture in complexity
theory, and the approximability of a ubiquitous problem in artificial intelligence.
572

Inapproximability of Treewidth and Related Problems

1.2 Width Parameters of Graphs
As mentioned earlier, determining the exact Treewidth of a graph and producing an associated optimal tree decomposition (see Definition 2.1) is known to be NP-hard (Arnborg
et al., 1987), and a central open problem is to determine whether or not there exists a
polynomial time constant factor approximation algorithm for Treewidth (see e.g., Bodlaender et al., 1995; Feige et al., 2005; Bodlaender, 2005). The current best polynomial time
approximation algortihm
for Treewidth (Feige et al., 2005), computes the Treewidth tw(G)
p
within a factor O( log tw(G)). On the other hand, the only hardness result to date for
Treewidth shows that it is NP-hard to compute Treewidth within an additive error of n
for some  > 0 (Bodlaender et al., 1995). No hardness of approximation is known and not
even the possibility of a polynomial-time approximation scheme for Treewidth has been
ruled out. In many important special classes of graphs, such as planar graphs (Seymour &
Thomas, 1994), asteroidal triple-free graphs (BouchitteÃÅ & Todinca, 2003), and H-minor-free
graphs (Feige et al., 2005), constant factor approximations are known, but the general case
has remained elusive.
On the positive side, there is a large body of literature developing fixed-parameter algorithms for Treewidth. Exactly determining the Treewidth is fixed-parameter tractable:
there is a linear time algorithm for computing the (exact) Treewidth for graphs of constant treewidth (Bodlaender, 1996). More specifically this exact algorithm runs in time
2poly(k) poly(n). Constant factor approximation algorithms achieve better dependence on
the treewidth, k, and n, with the best such algorithm running in time 2O(k) O(n) (Bodlaender, 2007).
A related graph parameter is the so-called Pathwidth, which can be viewed as measuring
how close G is to a path. The Pathwidth pw(G) is always at least tw(G), but can be much
larger. The current state of affairs here is similar as for Treewidth;pthough the current best
approximation algorithm only has an approximation ratio of O( log pw(G) log n) (Feige
et al., 2005), the best hardness result is NP-hardness of additive n error approximation.
Using the recently proposed Small Set Expansion (SSE) Conjecture (Raghavendra &
Steurer, 2010) discussed earlier, we show that both tw(G) and pw(G) are hard to approximate within any constant factor. In fact, we show something stronger: it is hard to
distinguish graphs with small Pathwidth from graphs with large Treewidth. Specifically:
Theorem 1.1. For every Œ± > 1 there is a c > 0 such that given a graph G = (V, E)
it is SSE-hard to distinguish between the case when pw(G) ‚â§ c ¬∑ |V | and the case when
tw(G) ‚â• Œ± ¬∑ c ¬∑ |V |.
In particular, both Treewidth and Pathwidth are SSE-hard to approximate within any
constant factor.
This is the first result giving hardness of (relative) approximation for these problems,
and gives evidence that no constant factor approximation algorithm exists for either of
them.
1.3 Minimum Fill-In
A closely related graph theoretic property is the Minimum Fill-In of a graph, the minimum number of edges required to add to a graph to triangulate it (i.e., make it chordal).
573

Wu, Austrin, Pitassi, & Liu

This property has important applications with sparse matrix computations (and in particular Gaussian elimination) and artificial intelligence (see the excellent survey in Heggernes,
2006).
Minimum Fill-in has been known to be fixed parameter tractable since 1994, when
Kaplan et al. (1994) gave an O(|E|16k ) algorithm, where k is the number of edges required.
From there, several improvements to the running time have been given, with the most recent
in 2012 by Fomin and Villanger (2012),
who gave the first subexponential parameterized
‚àö
algorithm, running in time O(2O( k log k) + k 2 |V | ¬∑ |E|). In the work of Natanzon et al.
(1998), a polynomial time approximation algorithm was presented, which computed a value
at most 8k 2 , where k is the optimal solution. For graphs with degree bounded by d, their
algorithm achieves an approximation ratio of O(d2.5 log4 (kd)).
This remains the best polynomial time approximation algorithm known to date. In
particular, it has remained an open question whether a polynomial time constant factor
approximation algorithm exists. In this paper, we show that this is not possible, assuming
the SSE Conjecture.
Theorem 1.2. It is SSE-hard to approximate the Minimum Fill-In of a graph to within a
constant factor.
1.4 Pebbling Problems
Graph pebbling is a rich and relatively mature topic in theoretical computer science. Pebbling is a game defined on a directed acyclic graph (DAG), where the goal is to pebble the
sink nodes of the DAG according to certain rules, using the minimum number of pebbles.
The rules for pebbling are as follows. A black pebble can be placed on a node if all of the
node‚Äôs immediate predecessors contain pebbles, and can always be removed. A white pebble
can always be placed on a node, but can only be removed if all of the node‚Äôs immediate
predecessors contain pebbles. A pebbling strategy is a process of pebbling the sink nodes
in a graph according to the above rules. The pebbling cost of a pebbling strategy is the
maximum number of pebbles used in the strategy. The Black-White pebbling cost of a
DAG is the minimum pebbling cost of all possible pebbling strategies. The black pebbling
cost is the minimum pebbling cost over all pebbling strategies that only use black pebbles.
Pebbling games were originally devised for studying programming languages and compiler construction, but have later found a broad range of applications in computational
complexity theory. Pebbling is a tool for studying the relationship between computation
time and space by means of a game played on directed acyclic graphs. It was employed to
model register allocation, and to analyze the relative power of time and space as Turing
machine resources. For a comprehensive recent survey on graph pebbling, see the work of
NordstroÃàm (2010).
Apart from the cost of a pebbling, another important measure is the pebbling time, which
is the number of steps (pebble placements/removals) performed. In the context of measuring
memory used by computations, this corresponds to computation time, and hence keeping
the pebbling time small is a natural priority. The extreme case of this is what we refer to
as One-Shot Pebbling, also known as progressive pebbling, considered in the literature (e.g.,
Sethi, 1973; Lengauer, 1981; Kirousis & Papadimitriou, 1986). In One-Shot Pebbling, we
574

Inapproximability of Treewidth and Related Problems

have the restriction that each node can receive a pebble only once. Note that this restriction
can cause a huge increase in the pebbling cost of the graph (Lengauer & Tarjan, 1982).
The One-Shot Pebbling problem is easier to analyze for the following reasons. In the
original pebbling problem, in order to achieve the minimum pebbling number, the pebbling
time might be required to be exponentially long, which becomes impractical when n is
large. On the other hand, the One-Shot Pebbling problem is more amenable to complexity
theoretic analysis as it minimizes the space used in a computation subject to the execution
time being minimum. In particular, the decision problem for One-Shot Pebbling is in NP
(whereas the unrestricted pebbling problems are PSPACE-complete).
The One-Shot
Black Pebbling problem and One-Shot Black-White Pebbling problems
‚àö
admit an O( log n log n) approximation ratio. We show that they are SSE-hard to approximate to within any constant factor. For black pebbling we show that this holds for single
sink DAGs with in-degree 2, which is the canonical setting for pebbling games (it seems
plausible that the black-white hardness can be shown to hold for this case as well, though
we have not attempted to prove this).
Theorem 1.3. It is SSE-hard to approximate the One-Shot Black Pebbling problem within
any constant factor, even in DAGs with a single sink and maximum in-degree 2.
Theorem 1.4. It is SSE-hard to approximate the One-Shot Black-White Pebbling problem
within any constant factor.
No hardness of approximation result of any form was known for One-Shot Pebbling
problems. We believe that these results can be extended to obtain hardness for more
relaxed versions of bounded time pebbling costs as well. We are currently working on this,
and have some preliminary results.
1.5 The Connection: Layout Problems
The graph width and One-Shot Pebbling problems discussed in the previous sections may
at first glance appear to be unrelated. However, both sets of problems are instances of a
general family of problems, known as graph layout problems. In a graph layout problem
(also known as an arrangement problem, or a vertex ordering problem), the goal is to find
an ordering of the vertices, optimizing some condition on the edges, such as adjacent pairs
being close. Layout problems are an important class of problems that have applications in
many areas such as VLSI circuit design.
A classic example is the Minimum Cut Linear Arrangement problem (MCLA). In this
problem, the objective is to find a permutation œÄ of the vertices V of an undirected graph
G = (V, E), such that the largest number of edges crossing any point,
max |{(u, v) ‚àà E|œÄ(u) ‚â§ i < œÄ(v)}|,
i

(1)

is minimized. MCLA is closely related to the Minimum Linear Arrangement problem
(MLA), in which the max in (1) is replaced by a sum.
‚àö
The MCLA problem can be approximated to within a factor O(log n log n). To the
best of our knowledge, there is no hardness of approximation for MCLA in the literature.
Its cousin MLA was recently proved SSE-hard to approximate within any constant factor
575

Wu, Austrin, Pitassi, & Liu

(Raghavendra et al., 2012), and we observe that the same hardness applies to the MCLA
problem.
Theorem 1.5. It is SSE-hard to approximate the Minimum Cut Linear Arrangement problem within any constant factor.
Another example of graph layout is the Interval Graph Completion Problem (IGC). In
this problem, the objective is to find a supergraph G0 = (V, E 0 ) of G with the same vertex set
V , such that G0 is an interval graph (i.e., the intersection graph of a set of intervals on the
real line) and having minimum number of edges. While not immediately appearing to be a
layout problem, using a simple structural characterization of interval graphs (Ramalingam
& Rangan, 1988) one can show that IGC can be reformulated as finding a permutation of
the vertices that minimizes the sum over the longest edges going out from each vertex, i.e.,
minimizing
X
max max{œÄ(v) ‚àí œÄ(u), 0}.
(2)
u‚ààV

(u,v)‚ààE

See, for example, the work of Charikar ‚àö
et al. (2010). The current best approximation
algorithm for IGC achieves a ratio of O( log n log log n) (Charikar et al., 2010). It turns
out that the SSE Conjecture can be used to prove super-constant hardness for this problem
as well.
Theorem 1.6. It is SSE-hard to approximate the Interval Graph Completion problem within
any constant factor.
There is a distinction in IGC of whether one counts the number of edges in the final
interval graph ‚Äì this is the most common definition ‚Äì or whether one only counts the
number of edges added to make G an interval graph (which makes the problem harder from
an approximability viewpoint). Our result holds for the common definition and therefore
applies also to the harder version. Note that Interval Graph Completion is well connected
to Pathwidth: the pathwidth of a graph G is one less than the smallest clique number of
an interval graph that contains G as a subgraph.
Theorems 1.5 and 1.6 are just two examples of layout problems that we prove hardness of
approximation for. By varying the precise objective function and also considering directed
acyclic graphs, in which case the permutation œÄ must be a topological ordering of the
graph, one can obtain a wide variety of graph layout problems. We consider a set of eight
such problems, generated by three natural variations (see Section 2.3 for precise details),
and show super-constant SSE-based hardness for all of them in a unified way. This set of
problems includes MLA, MCLA, and IGC, but not problems such as Bandwidth (but on
the other hand, strong NP-hardness inapproximability results for Bandwidth are already
known Dubey, Feige, & Unger, 2011). See Table 1 in Section 2.3 for a complete list of
problems covered.
Theorem 1.7. Assuming the SSE Conjecture, all problems listed in Table 1 (see page 581)
are NP-hard to approximate to within any constant factor.
Let us now return to the problems discussed in the previous sections. It should not
be surprising that the One-Shot Black Pebbling problem is equivalent to a graph layout
576

Inapproximability of Treewidth and Related Problems

problem: the one-shot constraint reduces the problem to determining in which order to
pebble the vertices; such an ordering induces a pebbling strategy in an obvious way. (Given
a graph layout ordering, the vertices can be black pebbled in that order, with each black
pebble removed as soon as that vertex is no longer be needed. Conversely, any black
pebbling sequence induces the corresponding ordering on the vertices.) For the black-white
case, it is known that the One-Shot Black-White Pebbling cost of D is interreducible with
a layout problem on an undirected graph G. Both of these layout problems are included
in the set of problems we show hardness for, so Theorems 1.3 and 1.4 follow immediately
from Theorem 1.7.
Turning to the width parameters, Treewidth is equivalent to a graph layout problem
called elimination width. Here the objective function is somewhat more intricate than in
the set of basic layout problems we consider in Theorem 1.7, but we are able to extend
those results to hold also for elimination width. Pathwidth is also known to be equivalent
to a certain graph layout problem, and in fact is equivalent to the layout problem which
One-Shot Black-White Pebbling reduces to. We use these connections to prove the hardness
of approximation for both Treewidth and Pathwidth, thereby obtaining Theorem 1.1.
1.6 Previous Work
As the reader may have noticed, for all the problems mentioned, the best current algorithms
achieve similar poly-logarithmic approximation ratios. Given their close relation, this is of
course not surprising. Most of the algorithms are obtained by recursively applying some
algorithm for the c-balanced separator problem, in which the objective is to find a bipartition
of the vertices of a graph such that both sides contain at least a c fraction of vertices, and
the number of edges crossing the partition is minimized.
In the pioneering work on separators by Leighton and Rao (1999), an O(log n) approximation algorithm for c-balanced separator was given, which was used to design O(log2 n)
approximation algorithm for a number of graph layout problems such as MLA, MCLA,
and Register Sufficiency. Later, Rao and Richa (1998) improved the approximation algorithm for MLA to a ratio O(log n log log n), using a spreading metric method. In the
groundbreaking work of Arora et al. (2009),
semidefinite programming was used to give
‚àö
an improved approximation ratio of O( log n) for c-balanced separator. Using
‚àö their ideas,
improved algorithms for ordering problems have been found, such as the O(‚àö log n log log n)
approximation algorithm for IGC and MLA (Charikar et al., 2010),
‚àö the O( log n) approximation algorithm for Treewidth (Feige et al., 2005) and the O( log n log n) approximation
algorithm for Pathwidth (Feige et al., 2005).
It is known that the Register Sufficiency problem (also known as One-Shot Black Pebbling) admits a O(log2 n) approximation algorithm (Ravi, Agrawal, & Klein, 1991). We
observe that by plugging in the improved approximation algorithm for direct vertex separator (Agarwal, Charikar, Makarychev, &‚àöMakarychev, 2005) into the algorithm by Ravi et
al. (1991), one can improve this to an O( log n log n) approximation algorithm.
Again, in these algorithms, the approximation algorithm for c-balanced separator plays
a key role. An improved algorithm for c-balanced separator will also improve the approximation algorithms for the other problems. On the other hand, hardness of approximating
577

Wu, Austrin, Pitassi, & Liu

c-balanced separator (Raghavendra et al., 2012) does not necessarily imply hardness of
approximating layout problems.
On the hardness side, our work builds upon the work by Raghavendra et al. (2012),
which showed that the SSE Conjecture implies superconstant hardness of approximation
for MLA (and for c-balanced separator). The only other hardness of relative approximation
that we are aware of for these problems is a result of the work of AmbuÃàhl et al. (2007),
showing that MLA does not have a PTAS unless NP has randomized subexponential time
algorithms.
1.7 Organization
The outline for the rest of the paper is as follows. In Section 2, we formally define the
layout problems studied as well as Treewidth, Pathwidth, and Minimum Fill-In. After
giving an overview of the reductions used in Section 3 we give the full proof of Theorem 1.7
in Section 4. Then, in Section 5 we give the lower bound on Treewidth which combined
with the results from Section 4 gives Theorem 1.1. In Section 6 we give the lower bound on
Minimum Fill-In. Finally in Section 7 we give some additional reductions for our pebbling
instances in order to achieve indegree 2 and single sinks, as promised in Theorem 1.3. We
end with some concluding remarks in Section 8.

2. Definitions and Preliminaries
For an undirected graph G = (V, E), and subsets S, S 0 ‚äÜ V , E(S, S 0 ) denotes the set of
edges that go between S and S 0 . In other words, E(S, S 0 ) is the set of edges (u, v) ‚àà E such
that u ‚àà S and v ‚àà S 0 .
2.1 Treewidth, Elimination Width, and Pathwidth
Definition 2.1 (Tree decomposition, Treewidth). Let G = (V, E) be a graph, T a tree,
and let V = (Vt )t‚ààT be a family of vertex sets Vt ‚äÜ V indexed by the vertices t of T . The
pair (T, V) is called a tree decomposition of G if it satisfies the following three conditions:
(T1) V = ‚à™t‚ààT Vt ;
(T2) for every edge e ‚àà E, there exists a t ‚àà T such that both endpoints of e lie in Vt ;
(T3) for every vertex v ‚àà V , {t ‚àà T | v ‚àà Vt } is a subtree of T ‚Äô.
The width of (T, V) is the number max{|Vt |‚àí1 | t ‚àà T }, and the Treewidth of G, denoted
tw(G), is the minimum width of any tree decomposition of G.
Definition 2.2. Let G = (V, E) be a graph, and let v1 , . . . , vn be some ordering of its
vertices. Consider the following process: for each vertex vi in order, add edges to turn the
neighborhood of vi into a clique, and then remove vi from G. This is an elimination ordering
of G. The width of an elimination ordering is the maximum over all vi of the degree of vi
when vi is eliminated. The elimination width of G is the minimum width of any elimination
order.
578

Inapproximability of Treewidth and Related Problems

Theorem 2.3 (See e.g., Bodlaender, 2007). For every graph G, the elimination width of G
equals tw(G).
Thus Treewidth is another example of a layout problem. In principle this layout problem
can be formulated in the framework of Section 2.3, but the choice of cost function is now
more involved than the vertex- and edge-counting considered there.
Definition 2.4 (Path decomposition, Pathwidth). Given a graph G, we say that (T, V) is
a path decomposition of G if it is a tree decomposition of G and T is a path. The Pathwidth
of G, denoted pw(G), is the minimum width of any path decomposition of G.
As claimed earlier, Pathwidth is in fact equivalent with a graph layout problem. (See
the next section for the formal definition of layout.)
Theorem 2.5 (Kinnersley, 1992). For every graph G, we have pw(G) = Layout(G; V, max).
2.2 Minimum Fill-In
Definition 2.6 (Chordal, Triangulation). A graph G is chordal if and only if every cycle
of length at least 4 has a chord. For any (possibly non-chordal) graph G, a triangulation of
G is a supergraph of G which is chordal.
Definition 2.7 (Minimum Fill-In). The Minimum Fill-In of a graph G is the minimum
number of edges required to add to G to triangulate it; i.e., so that the resulting supergraph
is chordal.
The problem of determining the Minimum Fill-In of a graph is sometimes called the
Chordal Graph Completion problem.
A perfect elimination ordering of G is an elimination ordering such that no edges are
ever added to G. Put another way, for each vertex vi , its neighbours appearing after it in
the ordering form a clique.
Theorem 2.8 (Fulkerson & Gross, 1965). A graph G is chordal if and only if it has a
perfect elimination ordering.
Treewidth and Minimum Fill-In are related through the following theorem.
Theorem 2.9 (Folklore). Suppose G is a graph with Treewidth k. Then every triangulation
of G has a clique of size k + 1.
2.3 Graph Layout Problems
In this subsection, we describe the set of graph layout problems that we consider. A problem
from the set is described by three parameters, giving rise to several different problems. These
three parameters are by no means the only interesting graph layout problems (and some
of the settings give rise to more or less uninteresting layout problems). However, they are
sufficient to capture the problems we are interested in except Treewidth, which in principle
could be incorporated as well though we refrain from doing so in order to keep the definitions
simple (see Section 2.1 for more details).
579

Wu, Austrin, Pitassi, & Liu

First a word on notation. Throughout the paper, G = (V, E) denotes an undirected
graph, and D = (V, E) denotes a directed (acyclic) graph. Letting n denote the number of
vertices of the graph, we are interested in bijective mappings œÄ : V ‚Üí [n]. We say that an
edge (u, v) ‚àà E crosses point i ‚àà [n] (with respect to the permutation œÄ, which will always
be clear from context), if œÄ(u) ‚â§ i < œÄ(v).
We consider the following variations:
1. Undirected or directed acyclic: In the case of an undirected graph G, any ordering
œÄ of the vertices is a feasible solution. In the case of a DAG D, only the topological
orderings of D are feasible solutions.
2. Counting edges or vertices: for a point i ‚àà [n] of the ordering, we are interested in
the set Ei (œÄ) of edges crossing this point. When counting edges, we use the cardinality
of Ei as our basic measure. When counting vertices, we only count the set of vertices
Vi to the left of i that are incident upon some edge crossing i. In other words, Vi is
the projection of Ei (œÄ) to the left-hand side vertices. Formally:
Ei (œÄ) = {e ‚àà E | œÄ(u) ‚â§ i < œÄ(v) where e = (u, v)}
Vi (œÄ) = {u ‚àà V | œÄ(u) ‚â§ i < œÄ(v) for some (u, v) ‚àà E}
We refer to |Ei (œÄ)| or |Vi (œÄ)| (depending on whether we are counting edges or vertices)
as the cost of œÄ at i.
3. Aggregation by sum or max: given an ordering œÄ, we aggregate the costs of each
point i ‚àà [n], by either summation or by taking the maximum cost.
Given these choices, the objective is to find a feasible ordering œÄ that minimizes the
aggregated cost.
Definition 2.10. (Layout value) For a graph H (either an undirected graph G or a DAG
D), a cost function C (either E or V ), and an aggregation function agg : R‚àó ‚Üí R (either
Œ£ or max), we define Layout(H; C, agg) as the minimum aggregated cost over all feasible
orderings of H. Formally:
Layout(H; C, agg) =

min

agg |Ci (œÄ)|.

feasible œÄ i‚àà[n]

Example 2.11.
Layout(G; E, max) = min max |Ei (œÄ)|,
œÄ

i‚àà[n]

where œÄ ranges over all orderings of V (G). This we recognize from Section 1.5 as the
Minimum Cut Linear Arrangement value of G.
Example 2.12.
Layout(D; V, max) = min max |Vi (œÄ)|,
œÄ

i‚àà[n]

where œÄ ranges over all topological orderings of the DAG D. As we shall see in Section 2.4,
this is precisely the One-Shot Black Pebbling cost of D.
580

Inapproximability of Treewidth and Related Problems

Problem
undir. edge
sum
undir. edge max
undir.

vertex

sum

undir.

vertex

max

DAG

edge

sum

DAG
DAG
DAG

edge
vertex
vertex

max
sum
max

Also known as / Equivalent with
Minimum/Optimal Linear Arrangement
Minimum Cut Linear Arrangement
CutWidth
Interval Graph Completion
SumCut
Pathwidth
One-Shot Black-White Pebbling
Minimum Storage-Time Sequencing
Directed MLA/OLA

One-Shot Black Pebbling
Register Sufficiency

Table 1: Taxonomy of Layout Problems
Combining the different choices gives rise to a total of eight layout problems (some more
natural than others). Several of these appear in the literature under one or more names,
and some turn out to be equivalent1 to problems that at first sight appear to be different.
We summarize some of these names in Table 1. In some cases the standard definitions of
these problems look somewhat different than the definition given here (e.g., for Pathwidth,
One-Shot Pebbling, and Interval Graph Completion). For the Pebbling and Pathwidth
problems, we discuss these equivalences of definitions in the following two sections.
For Interval Graph Completion, recall from Section 1.5 that the objective is to minimize
X
max max{œÄ(v) ‚àí œÄ(u), 0}.
u‚ààV

(u,v)‚ààE

In other words, we are counting the longest edge going to the right from each point i. If
the length of this edge is l then the edge contributes 1 to Vi (œÄ), . . . , Vi+l‚àí1 (œÄ) and hence
the objective can be rewritten as
X
|Vi (œÄ)|,
u‚ààV

so that Interval Graph Completion is precisely Layout(G; V, Œ£).
2.4 Pebbling Problems
In this section we define pebbling problems and their one-shot versions.
Definition 2.13. (Pebbling Configurations) Let D = (V, E) be a directed acyclic graph
(DAG). A pebbling configuration of D is a pair (B, W ) of (disjoint) subsets of vertices
(representing the set B of vertices that have black pebbles, and the set W of vertices that
have white pebbles on them).
1. Here, we consider two optimization problems equivalent if there are reductions between them that change
the objective values by at most an additive constant.

581

Wu, Austrin, Pitassi, & Liu

Definition 2.14. (Black and Black-White Pebbling Strategies) Let D = (V, E) be a directed acyclic graph. A Black-White Pebbling strategy for D is a sequence of pebble configurations P = {P0 , . . . , PœÑ } such that:
(i) the first and last configurations contain no pebbles; that is P0 = PœÑ = (‚àÖ, ‚àÖ).
(ii) each sink vertex u of D is pebbled at least once, i.e., there is some Pt = (Bt , Wt ) such
that u ‚àà Bt ‚à™ Wt .
(iii) each configuration follows from the previous configuration by one of the following
rules:
(a) A black pebble can be removed from a vertex.
(b) A black pebble can be placed on a pebble-free vertex v if all of the immediate
predecessors of v are pebbled.
(c) A white pebble can be placed on a pebble-free vertex.
(d) A white pebble can be removed from a vertex v if all of the immediate predecessors of v are pebbled.
A Black Pebbling Strategy for G is a Black-White Pebbling strategy in which no white
pebbles are used.
The cost of a pebbling strategy is cost(P) = max0‚â§t‚â§œÑ {|Bt ‚à™ Wt |}. The Black-White
Pebbling cost of D is the minimum cost of any Black-White Pebbling strategy of D, and
similarly the Black Pebbling cost of D is the minimum cost of any Black Pebbling Strategy
of D.
Definition 2.15. (One-Shot Black and One-Shot Black-White Pebbling) A One-Shot Black
(resp. Black-White) pebbling strategy is a Black (resp. Black-White) Pebbling strategy in
which each node is only pebbled once. The One-Shot Black (resp. Black-White) pebbling
cost of D, denoted BP1s (D) (resp. BWP1s (D)) is the minimum cost of any One-Shot Black
(resp. Black-White) Pebbling strategy of D.
As mentioned in Table 1, One-Shot Pebbling problems can be formulated as Layout
problems.
Lemma 2.16. For every DAG D = (V, E), we have BP1s (D) = Layout(D, V, max).
Proof. Suppose œÄ is the optimal ordering of Layout(D, V, max), we pebble the vertices according to œÄ. We remove a pebble from vertex u if and only if all of the successors of
u are pebbled. Since œÄ is a topological order of D, this is a valid pebbling strategy. It
is easy to verify that after pebbling œÄ(i), the number of pebbles on the graph is |Vi (œÄ)|.
Therefore the number of pebbles used in the above strategy is Layout(D, V, max). On the
other hand, suppose Œì is the optimal pebbling strategy, let œÉ be the ordering of vertices
to receive a pebble in Œì. We consider the number of pebbles on the graph after pebbling
the i-th vertex in œÉ. For any vertex u that has a pebble, if the vertex has a successor
that has not yet been pebbled, then the pebble on u cannot be removed, since u cannot
be pebbled again. Therefore the number of pebbles on the graph is at least |Vi (œÉ)|. Thus
BP1s (D) ‚â• maxi‚àà[n] |Vi (œÉ)| ‚â• Layout(D, V, max).
582

Inapproximability of Treewidth and Related Problems

For One-Shot Black-White Pebbling, we have the following reductions by Lengauer
(1981), showing that One-Shot Black-White Pebbling is equivalent to the undirected MaxVertex Layout Problem.
Lemma 2.17 (Lengauer, 1981). For a given DAG D = (V, E), let GD = (V, ED ) be an
undirected graph with ED = {(v, w) | (v, w) ‚àà E} ‚à™ {(v, w) | ‚àÉu, (v, u), (w, u) ‚àà E}. Then
BWP1s (D) = Layout(GD , V, max) ‚àí 1.
Lemma 2.18 (Lengauer, 1981). For an undirected graph G = (V, E), let DG = (V ‚à™E, EG )
be a DAG with EG = {(v, e) | e ‚àà E, v ‚àà V, v ‚àà e}. Then
Layout(G, V, max) = BWP1s (DG ) + 2.
2.5 Small Set Expansion Conjecture
In this section we define the SSE Conjecture. Let G = (V, E) be an undirected d-regular
graph. For a set S ‚äÜ V of vertices, we write Œ¶G (S) for the (normalized) edge expansion of
S,
|E(S, V \ S)|
Œ¶G (S) =
d|S|
The Small Set Expansion Problem with parameters Œ∑ and Œ¥, denoted SSE(Œ∑, Œ¥), asks if G
has a small set S which does not expand or whether all small sets are highly expanding.
Definition 2.19 (SSE(Œ∑, Œ¥)). Given a d-regular graph G = (V, E) 2 , SSE(Œ∑, Œ¥) is the problem
of distinguishing between the following two cases:
Yes There is an S ‚äÜ V with |S| = Œ¥|V | and Œ¶G (S) ‚â§ Œ∑.
No For every S ‚äÜ V with |S| = Œ¥|V | it holds that Œ¶G (S) ‚â• 1 ‚àí Œ∑.
This problem was introduced by Raghavendra and Steurer (Raghavendra & Steurer,
2010), who conjectured that the problem is hard.
Conjecture 2.20 (Small Set Expansion Conjecture). For every Œ∑ > 0, there is a Œ¥ > 0
such that SSE(Œ∑, Œ¥) is NP-hard.
As has become common for a conjecture like this (such as the Unique Games Conjecture),
we say that a problem is SSE-hard if it is as hard to solve as the SSE problem. Formally, a
decision problem P (e.g., a gap version of some optimization problem) is SSE-hard if there
is some Œ∑ > 0 such that for every Œ¥ > 0, SSE(Œ∑, Œ¥) polynomially reduces to P.
Subsequently, Raghavendra et al. (2012) showed that the SSE Problem can in turn be
reduced to a quantitatively stronger form of itself. To state this stronger version, we need
to first define Gaussian noise stability.
Definition 2.21. Let œÅ ‚àà [‚àí1, 1]. We define ŒìœÅ : [0, 1] ‚Üí [0, 1] by


ŒìœÅ (¬µ) = Pr X ‚â§ Œ¶‚àí1 (¬µ) ‚àß Y ‚â§ Œ¶‚àí1 (¬µ)
where Œ¶‚àí1 is inverse function of normal distribution, and X and Y are jointly normal
1 œÅ
random variables with mean 0 and covariance matrix
.
œÅ 1
2. d is a constant

583

Wu, Austrin, Pitassi, & Liu

The only property we shall need of ŒìœÅ is the following well-known fact on the asymptotic
behaviour for œÅ close to 1 and ¬µ bounded away from 0.
Fact 2.22. (Raghavendra et al., 2012) There is a constant c > 0 such that for all sufficiently
small  and all ¬µ ‚àà [1/10, 1/2],
‚àö
Œì1‚àí (¬µ) ‚â§ ¬µ(1 ‚àí c ).
We can now state the strong form of the SSE Conjecture.
Conjecture 2.23 (SSE Conjecture, Equivalent Formulation). For every integer q > 0 and
, Œ≥ > 0, it is NP-hard to distinguish between the following two cases for a given d-regular
graph G = (V, E)
Yes There is a partition of V into q equi-sized sets S1 , . . . , Sq such that Œ¶G (Si ) ‚â§ 2 for
every 1 ‚â§ i ‚â§ q.
No For every S ‚äÜ V , letting ¬µ = |S|/|V |, it holds that Œ¶G (S) ‚â• 1 ‚àí (Œì1‚àí/2 (¬µ) + Œ≥)/¬µ.
For future reference, let us make two remarks about the strong form of the conjecture.
Remark 2.24. In the Yes case of Conjecture 2.23, the number of edges leaving Si is at
most
|E(Si , V \ Si )|3 = Œ¶G (Si )d|S| ‚â§ 4|E|/q.
In particular, the total number of edges that are not contained in one of the Si ‚Äôs is at most
1X
|E(Si , V \ Si )| ‚â§ 2|E|.
2
i

Remark 2.25. Using Fact 2.22 we see that, in the No case of Conjecture 2.23, we have
‚àö
Œ¶G (S) ‚â• c0 ,
‚àö
for some constant c0 > 0, provided ¬µ ‚àà [1/10, 1/2] and setting Œ≥ ‚â§ . In particular, for
‚àö
every |V |/10 ‚â§ |S| ‚â§ 9|V |/10, we have |E(S, V \ S)| ‚â• c |E| (switching roles of S and
V \ S for |S| > |V |/2), for some constant c > 0 (not the same constant as in Fact 2.22).

3. Brief Overview of Reductions
In this section, we give a very brief overview of the reductions used to prove that the layout
problems of Table 1 are SSE-hard to approximate within any constant factor. The full
details of these reductions can be found in Section 4.
For the two undirected edge problems (i.e., MLA and MCLA), the hardness follows
immediately from the strong form of the SSE Conjecture (Conjecture 2.23) ‚Äì for the case of
MLA this was proved in the work of Raghavendra et al. (2012) and the proof for MCLA is
similar. This is our starting point for the remaining problems. Unfortunately, the results do
3. E(S1 , S2 ) indicates the number of edges between vertex set S1 and S2

584

Inapproximability of Treewidth and Related Problems

SSE
Problem

Undirected Edge
Problems (MLA/MCLA)
with expansion

Directed Problems

Nice Pebbling
Instances

Undirected Vertex
Problems

Treewidth

Figure 1: Overview of Reductions. Dashed arrows indicate that the reduction is obtained by
the identity mapping, whereas solid arrows indicate a nontrivial transformation
from one problem to the other.

not follow from hardness for MLA/MCLA in a black-box way; for the soundness analyses
we end up having to use the expansion properties of the original SSE instance.
We then give a reduction from MLA/MCLA to the four directed problems. This reduction simply creates the bipartite graph where the vertex set is the union of the edges and
vertices of the original graph G, with directed arcs from an edge e to the vertices incident
upon e in G. The use of direction here is crucial: it essentially ensures that both the vertex
and edge counts of any feasible ordering corresponds very closely to the number of edges
crossing the point in the induced ordering of G.
To obtain hardness for the remaining two undirected problems, we perform a similar
reduction as for the directed case, creating the bipartite graph of edge-vertex incidences.
However, since we are now creating an undirected graph, we can no longer force the edges to
be chosen before the vertices upon which they are incident, which was a key property in the
reduction for the directed case. In order to overcome this, we duplicate each original vertex
a large number of times. This gives huge penalties to orderings which do not ‚Äúessentially‚Äù
obey the desired direction of the edges, and makes the reduction work out.
The results for Treewidth, which are presented in Section 5, follows from an additional
analysis of the instances produced by the reduction for undirected vertex problems.
Finally, the reduction for directed problems, implying hardness for One-Shot Black
Pebbling, does not produce the kind of ‚Äúnice‚Äù instances promised by Theorem 1.3. In
Section 7, we give some additional transformation to achieve these properties.
Figure 1 gives a high-level overview of these reductions.

4. Hardness For Layout Problems
In this section, we show that all of the layout problems defined in Section 2.3 are SSEhard to approximate within any constant factor. This also shows that Pathwidth and the
One-Shot Pebbling problems are hard to approximate within any constant.
4.1 Hardness for MCLA and MLA
In this section, we recall the proof in the work of Raghavendra et al. (2012) for MLA, and
observe that it applies to MCLA as well. For an undirected graph G, let us write MCLA(G)
585

Wu, Austrin, Pitassi, & Liu

(resp., MLA(G)) for the MCLA value (resp., MLA value) of G, i.e.,
MLA(G) = Layout(G; E, Œ£) = min
œÄ

X

|Ei (œÄ)|

i‚àà[n]

MCLA(G) = Layout(G; E, max) = min max |Ei (œÄ)|.
œÄ

i‚àà[n]

Theorem 4.1. For every  > 0, given a graph G = (V, E), it is SSE-hard to distinguish
between:
Yes MLA(G) ‚â§ O( ¬∑ |V | ¬∑ |E|) and MCLA(G) ‚â§ O(|E|)
‚àö
No For every S ‚äÜ V with |V |/10 ‚â§ |S| ‚â§ 9|V |/10, it holds that |E(S, V \ S)| ‚â• ‚Ñ¶( |E|).
‚àö
‚àö
In particular, MLA(G) ‚â• ‚Ñ¶(  ¬∑ |V | ¬∑ |E|) and MCLA(G) ‚â• ‚Ñ¶( |E|).
Proof. We use the instances for Conjecture 2.23 with q = 1/. Let G = (V, E) be an
instance for Conjecture 2.23.
In the Yes case, we have disjoint sets S1 , . . . , Sq and for each set Sj , |Sj | = n/q = n,
Œ¶G (Sj ) ‚â§ 2. We give an ordering œÄ : V ‚Üí [1, .., n] of the vertices such that maxi‚àà[n] |Ei (œÄ)| ‚â§
3|E| as follows. Order the vertices as S1 , . . . , Sq (with the order within each Sj chosen arbitrary) and let this order be œÄ. For any i ‚àà [n], we show that |Ei (œÄ)| ‚â§ 3|E|. Suppose
œÄ ‚àí1 (i) is a vertex in Sj . Each edge in Ei (œÄ) either has both end-points inside Sj , or its endpoints in two different Sk ‚Äôs. The total number of edges inside Sj is at most dn/2 = |E|.
Moreover, by Remark 2.24, the total number of edges with end-points in two different Sk ‚Äôs
is at most 2|E|. Therefore, MCLA(G) ‚â§ maxi |Ei (œÄ)| ‚â§ 3|E|. The MLA value can be
bounded similarly.
The property of the No instance is the same as in Conjecture 2.23 (via Remark 2.25),
and the implications for the MLA and MCLA values are immediate.
4.2 Reduction To Directed Graphs
Given an undirected graph G = (V, E), we construct a directed graph D = (V 0 , E 0 ) as
follows. In order to distinguish the elements of V and E from the elements of V 0 and E 0 ,
we refer to elements of V as vertices, elements of E as edges, elements of V 0 as nodes, and
elements of E 0 as arcs.
There is a node in D for each vertex and for each edge of G, i.e., V 0 = V ‚à™ E. The
graph D is bipartite with bipartition V, E, and there is an arc in D from e ‚àà E to v ‚àà V if
e is incident upon v. Formally,
V0 = V ‚à™E
E 0 = {(e, v) | e ‚àà E, v ‚àà V, v ‚àà e}.
See also Figure 2.
The remainder of this section is devoted to analyzing the reduction. First, it is easy to
give an upper bound on the four Layout values of D in terms of the MLA and MCLA values
of G.
586

Inapproximability of Treewidth and Related Problems

u

v

v

u

w

(u, v)

G

w

(u, w)

D
Figure 2: The reduction from G to D.

Lemma 4.2. The DAG D constructed from G as above satisfies the following:
Layout(D; E, Œ£) ‚â§ (MLA(G) + O(|E|)) ¬∑ (d + 1)
Layout(D; E, max) ‚â§ MCLA(G) + d.
Note that, for the purposes of applying this to the graphs of Theorem 4.1 the error term
of |E| (resp. d) is insignifcant compared to the MLA (resp. MCLA) value of G.
Proof. Consider an ordering œÄ of V . For a set of vertices S of V , let uœÄ (S) ‚àà S denote the
vertex of S that comes first in the ordering œÄ.
We extend œÄ to an ordering œÄ 0 of V 0 by inserting each edge e = (u, v) immediately before
the vertex uœÄ (e). It is easy to see that for each node z ‚àà V 0 ,
|Ez (œÄ 0 )| ‚â§ |EuœÄ (z) (œÄ)| + d
where Ev (œÄ) for a vertex v is an abbreviation for EœÄ(v) (œÄ). This immediately implies
Layout(D; E, max) ‚â§ max0 |Ez (œÄ 0 )| ‚â§ max |Eu (œÄ)| + d,
u‚ààV

z‚ààV

Setting œÄ to be an optimal MCLA ordering of G, we obtain the second claim of the Lemma.
Similarly, using that |u‚àí1
œÄ (v)| ‚â§ d + 1 for every v ‚àà V , we get
X X
X
X
Layout(D; E, Œ£) ‚â§
|Ez (œÄ 0 )| =
|Ez (œÄ 0 )| ‚â§ (d + 1)
|Ev (œÄ)| + d|V 0 |,
z‚ààV 0

v‚ààV

z‚ààV 0
uœÄ (z)=v

v‚ààV

Setting œÄ to be an optimal MLA ordering of G and using |V 0 | = O(|E|), we obtain the first
claim of the Lemma.
Next we use Theorem 4.1 to argue the converse direction.
Lemma 4.3. Let 0 ‚â§  < 1. Suppose G has the property that for every |V |/10 ‚â§ |S| ‚â§
‚àö
9|V |/10 we have |E(S, V \ S)| ‚â• ‚Ñ¶( |E|). Then,
‚àö
Layout(D; V, Œ£) ‚â• ‚Ñ¶( |E|2 )
‚àö
Layout(D; V, max) ‚â• ‚Ñ¶( |E|)
587

Wu, Austrin, Pitassi, & Liu

Proof. Let œÄ 0 be any ordering of V 0 . Using the expansion property of Theorem 4.1, we‚Äôll
show that this ordering must have high cost. For a point i ‚àà [N ], let Si be the set of vertices
of V that appear after i in œÄ 0 .
The bound on Layout(D; V, max) is immediate: consider a point i ‚àà [N ] such that
‚àö
|Si | = |V |/2. By the expansion property |E(Si , V \ Si )| ‚â• ‚Ñ¶( |E|), and since each such
edge e has one of its endpoints before or at point i, the node e itself must appear before
‚àö
point i and thus |Vi (œÄ 0 )| ‚â• |E(Si , V \ Si )| ‚â• ‚Ñ¶( |E|).
Let us then turn to Layout(D; V, Œ£). Write ci for the fraction of edges e that appear
before (or at) point i in œÄ 0 . We shall show that whenever 1/5 ‚â§ ci ‚â§ 4/5, we have |Vi (œÄ 0 )| ‚â•
‚àö
‚àö
‚Ñ¶( |E|), giving a total of Layout(D; V, Œ£) ‚â• ‚Ñ¶( |E|2 ).
By a simple counting argument, we have
d|Si | ‚â• 2(1 ‚àí ci )|E|,
implying |Si | ‚â• (1 ‚àí ci )|V | which for ci ‚â§ 4/5 is at least a 1/5 fraction of vertices. If in
‚àö
addition |Si | ‚â§ 9|V |/10, the argument above gives |Vi (œÄ 0 )| ‚â• ‚Ñ¶( |E|). The remaining case
is that |Si | ‚â• 9|V |/10. But then Si is incident upon at least a 9/10 fraction of edges. This
implies that the number of edges incident upon Si , appearing before i in œÄ 0 , are at least
‚àö
|E|(ci ‚àí 1/10) which for ci ‚â• 1/5 is ‚Ñ¶( |E|).
Combining Lemma 4.2 and Lemma 4.3, with Theorem 4.1, and using the fact that edge
costs are always larger than the corresponding vertex costs, we immediately obtain the
following theorem.
Theorem 4.4. Given a DAG D, Layout(D; E, max), Layout(D; E, Œ£), Layout(D; V, max),
and Layout(D; V, Œ£) are all SSE-hard to approximate within any constant factor, even in
DAG‚Äôs with maximum path length 1 (i.e., every vertex is a source or a sink).
Proof. Given a graph G, Theorem 4.1 says that it is SSE-hard to distinguish between
Yes MLA(G) ‚â§ O( ¬∑ |V | ¬∑ |E|) and MCLA(G) ‚â§ O(|E|)
‚àö
No For every S ‚äÜ V with |V |/10 ‚â§ |S| ‚â§ 9|V |/10, it holds that |E(S, V \ S)| ‚â• ‚Ñ¶( |E|).
‚àö
‚àö
In particular, MLA(G) ‚â• ‚Ñ¶(  ¬∑ |V | ¬∑ |E|) and MCLA(G) ‚â• ‚Ñ¶( |E|).
Applying the reduction to directed graphs, Lemma 4.2 tells us that the Yes case becomes
Layout(D; E, Œ£) ‚â§ (d + 1)O(|V ||E| + |E|) = O(|E|2 )
Layout(D; E, max) ‚â§ O(|E|),
and Lemma 4.3 tells us that the No case becomes
‚àö
Layout(D; V, Œ£) ‚â• ‚Ñ¶( |E|2 )
‚àö
Layout(D; V, max) ‚â• ‚Ñ¶( |E|).
‚àö
thereby establishing a hardness factor of ‚Ñ¶(1/ ) for each of the four problems (using that
Layout(D; V, ¬∑) ‚â§ Layout(D; E, ¬∑)).
588

Inapproximability of Treewidth and Related Problems

u1 , . . . , ur

u

(u, v)

v

(u, w)

(w, v)

w
v1 , . . . , vr

w1 , . . . , wr

G0

G

Figure 3: The reduction from G to G0 , illustrated for r = 3.

Remark 4.5. In fact we see that, as in Theorem 4.1, the four hardness results apply to the
same instance, so that it is SSE-hard to distinguish all of the four Layout values being high
from all of them being low.
As the One-Shot Black Pebbling problem is precisely Layout(D; V, max), we obtain hardness for One-Shot Black Pebbling as an immediate corollary. However, the instances are
not single-sink DAGs with maximum indegree 2, as promised in Theorem 1.3. In Section 7
we show how to transform the instances further to obtain such DAGs.
4.3 Undirected Vertex Problems
The reduction for undirected vertex problems is very similar to the reduction for directed
problems given in the previous section. As before, we introduce nodes for every edge of
G. As in the directed case, we are interested in orderings where an edge appears before its
two endpoints, but we cannot use direction to force this anymore. Instead, we ensure that
orderings that are not like this incur a high cost by replicating each node corresponding to
a vertex of G many times.
Given an undirected graph G = (V, E), we construct a new graph G0 = (V 0 , E 0 ) as
follows.
There are r nodes in G0 for each vertex and one node for each edge of G, i.e., V 0 =
V √ó [r] ‚à™ E. For a vertex u ‚àà V we write u1 , . . . , ur to denote the r copies of u and refer
to each such set of r nodes as a vertex group. The graph G0 is bipartite with bipartition
V √ó [r], E, and there is an edge in G0 between e ‚àà E and v i ‚àà V √ó [r] if e is incident upon
v. Formally,
V 0 = {v i | v ‚àà V, i ‚àà [r]} ‚à™ {e | e ‚àà E}
E 0 = {(e, v i ) | e ‚àà E, v ‚àà V, v ‚àà e, i ‚àà [r]}.
See also Figure 3.

589

Wu, Austrin, Pitassi, & Liu

Lemma 4.6. The graph G0 constructed from G as above satisfies the following:
Layout(G0 ; V, Œ£) ‚â§ (d + r) MLA(G)
Layout(G0 ; V, max) ‚â§ MCLA(G).
Proof. We proceed as in the proof of Lemma 4.2. An ordering œÄ of V naturally induces an
ordering œÄ 0 of V 0 : put all r copies of u ‚àà V consecutively, with vertices of V appearing in the
same order as in œÄ, and insert each edge e ‚àà E immediately before its first vertex. Again,
for an edge e ‚àà E, let uœÄ (e) denote the endpoint of e that appears first in œÄ. Similarly, for
a copy v i ‚àà V 0 of v ‚àà V , let uœÄ (v i ) = v. It is easy to see that the constructed ordering œÄ 0
satisfies
|Vz (œÄ 0 )| ‚â§ |EuœÄ (z) (œÄ)|
for every z ‚àà V 0 . This immediately implies
Layout(G0 ; V, max) ‚â§ max0 |Vz (œÄ 0 )| ‚â§ max |Eu (œÄ)|,
z‚ààV

u‚ààV

Similarly, as in Lemma 4.2 we use that |u‚àí1
œÄ (œÄ)| ‚â§ d + r and get
X
X
Layout(G0 ; V, Œ£) ‚â§
|Vz (œÄ 0 )| ‚â§ (d + r)
|Ev (œÄ)|.
z‚ààV 0

v‚ààV

Lemma 4.7. Let 0 ‚â§  < 1. Suppose G has the property that for every |V |/10 ‚â§ |S| ‚â§
‚àö
9|V |/10 we have |E(S, V \ S)| ‚â• ‚Ñ¶( |E|). Then, if r ‚â• |V | ¬∑ |E|, we have
‚àö
Layout(G0 ; V, Œ£) ‚â• ‚Ñ¶(  ¬∑ r ¬∑ |V | ¬∑ |E|)
‚àö
Layout(G0 ; V, max) ‚â• ‚Ñ¶( |E|)
Proof. Let œÄ 0 be an ordering of V 0 . First we have the following simple claim, establishing
that for good orderings, most vertices appear after their edges.
Claim 4.8. Suppose that for some vertex u ‚àà V , at least r/2 of the copies of u in G0 appear
before some edge e = (u, v) ‚àà E adjacent upon u. Then
‚àö
max |Vi (œÄ 0 )| ‚â• r/4  ‚Ñ¶(  ¬∑ |E|)
i‚àà[N ]
X
‚àö
|Vi (œÄ 0 )| ‚â• (r/4)2  ‚Ñ¶(  ¬∑ r ¬∑ |V | ¬∑ |E|).
i‚àà[N ]

Proof. Let I1 be the first half of the positions where copies of u appear before e, and I2 the
second half. Thus, |I1 |, |I2 | ‚â• r/4. Then each element of I1 contributes to Vi (œÄ 0 ) for each
i ‚àà I2 , giving the claimed bounds.
Thus we may without loss of generality assume that for each vertex u of V , at least r/2
of its r copies in G0 appear after all edges adjacent upon u. From now on, let us discard all
the ‚â§ r/2 ‚Äúbad‚Äù copies of each vertex of V that appear before some of its edges. This only
decreases the cost of œÄ 0 , and there are still ‚â• r|V |/2 vertex nodes left.
Let i1 be the (first) point of œÄ 0 such that r|V |/10 vertex nodes are to the left of i1 , and
i2 the (last) point of œÄ 0 such that r|V |/10 vertex nodes are to the right of i2 .
590

Inapproximability of Treewidth and Related Problems

‚àö
Claim 4.9. For any point i between i1 and i2 , we have |Vi (œÄ 0 )| ‚â• ‚Ñ¶( |E|).
Proof. Let S ‚äÜ V (resp. T ‚äÜ V ) be the set of vertices u such that some copy of u appears
|/10
before i (resp. after i). We then have |S|, |T | ‚â• r|Vr/2
‚â• |V |/5, and S ‚à™ T = V . Thus
we can partition V into S 0 ‚äÜ S, T 0 ‚äÜ T such that |S 0 |, |T 0 | ‚â§ 4|V |/5. By the expansion
‚àö
property of G we have |E(S 0 , T 0 )| ‚â• ‚Ñ¶( |E|). Further, we also have |Vi (œÄ 0 )| ‚â• |E(S 0 , T 0 )|
as each e ‚àà E(S 0 , T 0 ) must appear before i in œÄ 0 (because one of their endpoints is in S)
but have an edge crossing i (because the other of their endpoints is in T ).
From Claim 4.9, the proof of the lemma follows immediately.
As in the previous section, we can now combine Lemma 4.6 and Lemma 4.7, with
Theorem 4.1, to obtain:
Theorem 4.10. Given a graph G, Layout(G; V, max), Layout(G; V, Œ£) are both SSE-hard
to approximate within any constant factor, even in bipartite graphs.
As the Pathwidth problem is precisely Layout(G; V, max), we obtain hardness for Pathwidth as an immediate corollary. In the next section, we‚Äôll show the stronger soundness
required for Theorem 1.1.

5. Hardness For Treewidth
In this section we shall complete our proof of Theorem 1.1 by showing that the hard instances
for Pathwidth from Theorem 4.10 also have large Treewidth.
Lemma 5.1. Let 0 ‚â§  < 1. Let G = (V, E) be an undirected graph with the property that
‚àö
for every |V |/10 ‚â§ |S| ‚â§ 9|V |/10 we have |E(S, V \ S)| ‚â• ‚Ñ¶( |E|), and let G0 be the graph
obtained by applying the reduction of Section 4.3 to G. Then, if r ‚â• |V | ¬∑ |E|, we have
‚àö
tw(G0 ) ‚â• ‚Ñ¶( |E|)
To prove Lemma 5.1, we shall use the fact that the Treewidth of a graph is closely
related to an expansion-like property called the 1/2-separator number, defined in the work
of Bodlaender et al. (1995).
Definition 5.2 (1/2-vertex separator, 1/2-separator number). Let G = (V, E) be an undirected graph. For W ‚äÜ V , a 1/2-vertex separator of W in G is a set S ‚äÜ V of vertices such
that every connected component of the graph G[V ‚àí S] contains at most |W |/2 vertices of
W . Let œàG (1/2, W ) denote the minimum size of a 1/2-vertex separator of W in G. We
define the 1/2-separator number K1/2 (G) to be
K1/2 (G) = max œàG (1/2, W ).
W ‚äÜV

Lemma 5.3 (Bodlaender et al., 1995). For every graph G = (V, E), it holds that tw(G) ‚â•
K1/2 (G) ‚àí 1.
Using this, it is now straightforward to prove the lower bound on the Treewidth.
591

Wu, Austrin, Pitassi, & Liu

‚àö
Proof of Lemma 5.1. We‚Äôll show that œàG0 (1/2, V 0 ) ‚â• ‚Ñ¶( |E|) (i.e. we choose W = V 0 ).
Suppose C is an optimal 1/2-vertex separator of V 0 and it separates V 0 \ C into l sets
V10 , . . . , Vl0 , each of size at most |V 0 |/2. We can merge two sets in V10 , . . . , Vl0 by combining
the vertex set of them. By merging different Vi0 we may assume that we only have two sets
V10 and V20 , both of size at least |V 0 |/5 (this can be achieved by always merging the two sets
with smallest number of vertices, whever there are more than two sets left).
Now, similarly to the proof of Claim 4.9, let S ‚äÜ V (resp. T ‚äÜ V ) be the set of vertices
v such that some copy of V appears in V10 (resp. V20 ). As |V10 |, |V20 | ‚â• r|V |/5, this implies
that both |S|, |T | are at least |V |/5, and furthermore S ‚à™ T = V (since otherwise all r copies
‚àö
of some vertex are in C, implying |C| ‚â• r  ‚Ñ¶( |E|)). We can thus choose a balanced
‚àö
partition S 0 , T 0 such that S 0 ‚äÜ S, T 0 ‚äÜ T , and we have |E(S 0 , T 0 )| ‚â• ‚Ñ¶( |E|). But every
edge e = (u, v) such that u ‚àà S 0 and v ‚àà T 0 must belong to C, since it is connected (in G0 )
to every copy of u and v.

6. Hardness for Minimum Fill-In
In this section, we use the previous construction and results for Treewidth to prove the inapproximability of Minimum Fill-In. Specifically, we will show that applying the construction
to the SSE No instances produces graphs with high Minimum Fill-In, while Yes instances
yields graphs with low Minimum Fill-In.
Lemma 6.1. Let 0 ‚â§  < 1. Let G = (V, E) be an undirected graph with the property
‚àö
that for every |V |/10 ‚â§ |S| ‚â§ 9|V |/10 we have |E(S, V \ S)| ‚â• ‚Ñ¶( |E|), and let G0 be the
graph obtained by applying the reduction of Section 4.3 to G. Then, if r ‚â• |V | ¬∑ |E|, at least
‚Ñ¶(|E|2 ) edges must be added to triangulate G0 .
Proof. We use the observation that G0 is bipartite. Consider a minimum triangulation of
G, which must have a clique of size tw(G0 ) + 1. By Lemma 5.1, one set of of the bipartition
‚àö
of G must have ‚Ñ¶( |E|) vertices in this clique, and since these vertices are independent
in G0 , ‚Ñ¶(|E|2 ) edges must be added.
Note that this lemma holds independently of the choice of q. Now we prove a good
upper bound on Yes instances.
Lemma 6.2. Let  > 0 and q = 1/2 . Let G = (V, E) be a d-regular graph, and suppose
there is a partition of V into q equi-sized sets S1 , . . . , Sq such that Œ¶G (Si ) ‚â§ 2 for every
1 ‚â§ i ‚â§ q.
Let G0 = (V 0 , E 0 ) be the graph obtained by applying the reduction of Section 4.3 to G.
Then G0 has a triangulation with |E 0 | + O(2 |E|2 ) edges.
Suppose G = (V, E) is a Yes instance of Conjecture 2.23 with parameters q, , with
q = 1/2 , and apply the previous construction with r ‚â• |V | ¬∑ |E| to get a graph G0 = (V 0 , E 0 ).
Then G0 has a triangulation with |E 0 | + O(2 |E|2 ) edges.
Proof. Note that it suffices to find a ‚Äúgood‚Äù ordering of the vertices of G0 which is close to
being a perfect elimination ordering, i.e., that requires few additional edges to turn it into
a perfect elimination ordering. For each i, we define the set
Ei = {(u, v) ‚àà E | u, v ‚àà Si }.
592

Inapproximability of Treewidth and Related Problems

Also define the set of ‚Äúcut‚Äù edges Ec = {(u, v) ‚àà E | u ‚àà Si , v ‚àà Sj , i 6= j}. Clearly, the Ei
together with Ec form a partition of E. Further define subsets of Ec by Eci = E(Si , V \Si ),
the set of cut edges of Si . By the definition of the Si , Œ¶G (Si ) ‚â§ 2, and hence |Eci | ‚â§
4|E|/q = 43 |E|. We‚Äôll identify the Ei and Ec as subsets of Y in the constructed graph.
Now let œÄ be an ordering of V 0 such that all r|V | of the vertex copies appear first (in any
order), followed by the sets E1 , . . . , Eq , Ec in that order, where within each set the vertex
order is arbitrary. Now consider the following supergraph H of G0 obtained by:
‚Ä¢ making each set Ei ‚à™ Eci a clique
‚Ä¢ making Ec a clique/
Claim 1: This only adds O(2 |E|2 ) edges.
|
i
Note that |Ei ‚à™ Eci | ‚â§ d|Si | = d|V
= 2 |E|
q
2 . Thus making Ei ‚à™ Ec a clique requires
1
4
2
O( |E| ) edges. Since there are q = 2 of these cliques in total, this requires O(2 |E|2 )
edges total. Finally, by Remark 2.24, |Ec | ‚â§ 2|E|, so making this a clique takes O(2 |E|2 )
edges as well.
Claim 2: Adding these edges makes œÄ a perfect elimination ordering for H.
Consider a vertex copy v k ‚àà X. Its neighbours in H are the edges which are incident
with v; if v ‚àà Si , then these edges must all be in Ei ‚à™ Eci . So every vertex in X satisfies the
perfect elimination property. Now consider (u, v) ‚àà Ei ‚äÜ Y . Its ‚Äúedge‚Äù neighbours are all
in Ei or Eci . Finally, for every ‚Äúedge‚Äù vertex (u, v) ‚àà Ec , its only neighbours that appear
after it with respect to œÄ are also in Ec .
Putting these two claims together yields the desired result.

Combining these two lemmas prove Theorem 1.2.

7. Nicer Pebbling Instances
In this section we show how to transform our hard instances for One-Shot Black Pebbling
so as to have in-degree bounded by 2 and single sinks.
Lemma 7.1. Given a DAG D = (V, E) we can in polynomial time construct a DAG
D0 = (V 0 , E 0 ) such that D0 has a single sink and
BP1s (D) ‚â§ BP1s (D0 ) ‚â§ BP1s (D) + s + 1,
where s is the number of sinks in D. Furthermore, the maximum in-degree in D0 is no
larger than the maximum in-degree in D, provided this quantity is at least 2.
Proof. Construct D0 by adding a binary tree with s leaves to D, and identifying the leaves
of the tree with the sinks of D. That is, D0 consists of a binary tree with s leaves on top
of a copy of D where the leaves of the binary tree are identified with the sinks of D. The
number of vertices of D0 is equal to |D| + |s| ‚àí 1. Note that any binary tree with s leaves
will suffice. The properties of D0 are easily verified. Since D0 is a super-DAG of D, its
pebbling cost must be at least BP1s (D). Conversely, a valid pebbling of D0 can be obtained
by using a One-Shot Pebbling of D but without removing pebbles from the sinks, and then
pebbling the tree.
593

Wu, Austrin, Pitassi, & Liu

Figure 4: Pyramid of size 4
For the in-degree, we prove the following.
Lemma 7.2. Given a DAG D = (V, E) we can in polynomial time construct a DAG
D0 = (V 0 , E 0 ) such that every node of D0 has in-degree at most 2 and
BP1s (D) ‚â§ BP1s (D0 ) ‚â§ BP1s (D) + d,
where d is the maximum in-degree of D. Furthermore, if D has a single sink then so does
D0 .
Before proving Lemma 7.2, let us see how to use the two Lemmas to derive Theorem 1.3.
Proof of Theorem 1.3. By (the proof of) Theorem 4.4, there is a reduction which takes an
instance (G = (V, E) for Conjecture 2.23 and produces a dag D such that BP1s (D) =
‚àö
Layout(D; V, max) = O(|E|) if G is a Yes instance, and BP1s (D) = ‚Ñ¶( |E|) if G is a No
instance. The number of sinks in D is |V | = 2|E|/d, which is much smaller than O(|E|)
provided d  1/ (which may be assumed without loss of generality). The maximum indegree in D is the maximum degree of G which is d = O(1). Applying the reduction of
Lemma 7.1 and then the reduction of Lemma 7.2 we obtain a dag D0 with a single sink and
in-degree 2 such that BP1s (D) ‚â§ BP1s (D0 ) ‚â§ BP1s (D) + d + 2|E|/d. Since d + 2|E|/d 
O(|E|) ‚â§ BP1s (D), we conclude that it is SSE-hard to distinguish between BP1s (D0 ) ‚â§
‚àö
O(|E|) and BP1s (D0 ) ‚â• ‚Ñ¶( |E|).
7.1 Lemma 7.2
In this section we prove Lemma 7.2. First, recall the definition of a pyramid graph.
Definition 7.3. A pyramid graph of size d is a layered graph of indegree two, with d layers,
labelled 0, 1, . . . , d‚àí1. Layer zero (the input layer) consists of d vertices, and layer i contains
d ‚àí i vertices. Call the vertices at layer i vi1 , . . . vid‚àíi . For all i, 1 ‚â§ i ‚â§ d ‚àí 1, 1 ‚â§ j ‚â§ d ‚àí i,
j
j+1
Vertex vij has incoming edges from vertices vi‚àí1
and vi‚àí1
. See Figure 4.
The reduction of Lemma 7.2 to produce DAGs of indegree 2 is as follows. Construct D0
by replacing each vertex u by a pyramid Pu of size d(u) (here, d(u) denotes the indegree of
u), where the d(u) vertices at layer 0 of Pu are identified with the predecessors of u, and u
is identified with the vertex at layer d(u) ‚àí 1 of Pu . See Figure 5.
594

Inapproximability of Treewidth and Related Problems

u1

v1

v2

v3

u2

v4

v5

u1

v6

v7

v1

v2

u2

v3

v4

v5

v6

v7

Figure 5: Reduction to DAGs of indegree 2
To prove the lemma we need to show that D0 constructed this way satisfies
BP1s (D) ‚â§ BP1s (D0 ) ‚â§ BP1s (D) + d,
where d is the maximum indegree of any vertex u of D.
In what follows whenever we say ‚Äúpebbling strategy‚Äù of D or D0 we always refer to a
One-Shot Black Pebbling strategy of D or D0 .
The upper bound on BP1s (D0 ) is trivial: if S is a valid pebbling strategy for D, then
clearly we can create a corresponding pebbling strategy for D0 by pebbling through the
pyramid whenever D pebbles the sink of the pyramid. This takes at most d additional
pebbles.
In the other direction, we want to show that a pebbling strategy for D0 can be converted
into a pebbling strategy for D. We first show that D0 can be assumed to be in a particular
normal form, and then using this normal form, we show how to simulate the pebbling.
Definition 7.4. Let S 0 be a pebbling strategy of D0 . That is, S 0 is a sequence of configurations, where each configuration is a set of black pebble placements, and such that
the sequence of configurations follows the black pebbling rules. We say that configuration
c ‚àà S 0 is saturated with respect to a pyramid Pu if c is the first time in S 0 that there is a
black pebble path cutting the sink of Pu from all of the sources of Pu . (The cut does not
include any sources or the sink of Pu .) Note that this cut has size d ‚àí 1.
Claim 7.5. Let S 0 be a pebbling strategy for D0 . We can assume without loss of generality
that S 0 has the following normal form. For each configuration c0 ‚àà S 0 , if c0 is saturated with
respect to pyramid Pu , then the subsequent moves of S 0 pebble the sink of Pu (in the obvious
way), removing all other black pebbles on the internal nodes of Pu .
Proof Sketch. We show that any pebbling strategy can be converted into a normal form
strategy of the same pebbling cost. At a saturated configuration c0 , there must be d ‚àí 1
pebbles on internal nodes of Pu . If we subsequently pebble the sink of p, we never use
more than d ‚àí 1 pebbles on internal nodes of p, and all other pebbles on the graph stay
as they were. Thus the normal form does not use more pebbles than the original strategy.
Furthermore, since the internal nodes of a pyramid are only used to pebble the sink of this
pyramid, we have not lost anything by pebbling through to the sink and removing the other
internal black pebbles.
595

Wu, Austrin, Pitassi, & Liu

From now on we assume that the pebbling S 0 of D0 has the above normal form. That is,
if a configuration is saturated (with respect to a pyramid Pu ), the next thing that happens
in S 0 is to pebble the sink of Pu . (After pebbling the sink, we will have not touched whatever
pebbles were on the source nodes of Pu , and we will have a pebble on the sink node of Pu ,
and no other internal pebbles on Pu .)
Our strategy for constructing a pebbling, S, of D, given a normal form pebbling, S 0 of
0
D is as follows. For each node v of D, pebble v whenever it is first pebbled in S 0 , and
remove the pebble from v as soon as all successors of v (in the original graph D) are pebbled.
We want to argue that the cost of the pebbling strategy S is not greater than that of S 0 .
To see this, we use the following Lemma.
Lemma 7.6. In any minimal-length One-Shot Black Pebbling of a size d pyramid, the
number of pebbles on the pyramid at any point in time, up until all sources are pebbled,
must be at least the number of sources in that pyramid that have been pebbled so far.
Assuming the above Lemma it is clear that if S 0 is a normal form pebbling of D0 , then
for any pyramid Pu in D0 , and any configuration c0 , if there are k pebbles on Pu at c0 , then
in the corresponding configuration c of D, there are at most k pebbles on source nodes of
Pu . To see this, first notice that by the above Lemma, any time a pyramid is being pebbled
in D0 up until the time when all source nodes of the pyramid are pebbled for the first time,
the number of pebbles on the pyramid will be at least as large as the number of source
nodes in D that contain pebbles. Then by the normal form property of D0 , as soon as all
source nodes of D0 are pebbled for the first time, the strategy pebbles the sink of D0 , and
thus again the number of corresponding pebbles on D is never greater than the number of
pebbles on D0 .
Proof of Lemma 7.6. Let P be a size d pyramid graph, and let S be a One-Shot Black
Pebbling of P . Let c be a configuration occurring in S such that the set of source nodes
that have been pebbled up to c are the source nodes of P 0 , where P 0 is a size d0 subpyramid of P . We want to argue that c must contain at least d0 pebbles. Assume without
loss of generality that P 0 is the leftmost sub-pyramid of P , of size d0 < d (with source
0
vertices v01 , . . . , v0d .) Consider the outer rightmost vertices of P 0 ‚Äì those vertices with labels
0
0
vd10 ‚àí1 , vd20 ‚àí2 , . . . , v1d ‚àí1 , v0d . Relabel these outer rightmost vertices of P 0 by vd‚àí1 , ..., v0 , where
vd‚àí1 is the sink vertex of P 0 , and for all i < d ‚àí 1, vi is the rightmost vertex in P 0 at level i.
Corresponding to each named vertex vi is a diagonal set of vertices, diag(vi ), beginning at
vi and travelling southwest to a source vertex of P 0 . Note that the sets diag(vi ) are pairwise
disjoint. We will argue that for each i, 0 ‚â§ i ‚â§ d ‚àí 1, at least one vertex from diag(vi ) must
appear in c. To see this, first notice that for each vi , there is a vertex vi0 that is an immediate
successor of vi and that lies outside of P 0 . This vertex vi0 must be pebbled at some point,
and furthermore it must be pebbled at some time after configuration c, since pebbling vi0
requires pebbling a source vertex of P that is not in P 0 , and the only source vertices that
have been pebbled so far are the source vertices of P 0 . But in order to pebble vi0 in the
future, if S is a minimal-length pebbling, then there must be a black pebble on some vertex
in diag(vi ) in c. (In a minimal-length pebbling of graph G, if any set of configurations is
removed from the pebbling, what results is no longer a black pebbling of G.) Thus, we have
shown that if c is any configuration in S such that d0 < d source vertices are pebbled thus
far, then there must be d0 vertices pebbled in c.
596

Inapproximability of Treewidth and Related Problems

8. Conclusion and Open Problems
We proved SSE-hardness of approximation for a variety of graph problems. Most importantly we obtained the first inapproximability result for the Treewidth problem and
Minimum Fill-In.
Some remarks are in order. The status of the SSE conjecture is, at this point in time,
very uncertain, and our results should therefore not be taken as absolute evidence that
there is no polynomial time approximation algorithm for (e.g.) Treewidth. However, at the
very least, our results do give an indication of the difficulty involved in obtaining such an
algorithm for Treewidth, and builds a connection between these two important problems.
We also find it remarkable how simple our reductions and proofs are. We leave the choice
of whether to view this as a healthy sign of strength of the SSE Conjecture, or whether to
view it as an indication that the conjecture is too strong, to the reader.
There are many important open questions and natural avenues for further work, including:
1. It seems plausible that these results can be extended to a wider range of graph layout
problems. For instance, our two choices of aggregators max and Œ£ can be viewed as
taking `‚àû and `1 norms, and it seems likely that the results would apply for any `p
norm (though we are not aware of any previous literature studying such variants).
2. It would be nice to obtain hardness of approximation result for our problems based
on a weaker hardness assumption such as UGC. It is conjectured in the work of
Raghavendra et al. (2012) that the SSE conjecture is equivalent to UGC. Alternatively,
it would be nice to show that hardness of some of our problems imply hardness for
the SSE Problem.
3. For pebbling, it would be very interesting to obtain results for the unrestricted pebbling problems (for which finding the exact pebbling cost is even PSPACE-hard). As
far as we are aware, nothing is known for these problems, not even, say, whether one
can obtain a non-trivial approximation in NP. As mentioned in the introduction, we
are currently working on extending our One-Shot Pebbling results to bounded time
pebblings. We have some preliminary progress there and are hopeful that we can relax
the pebbling results to a much larger class of pebblings.

Acknowledgments
Research supported by NSERC.

References
‚àö
Agarwal, A., Charikar, M., Makarychev, K., & Makarychev, Y. (2005). O( logn) approximation algorithms for min UnCut, min 2CNF deletion, and directed cut problems. In
Proceedings of the thirty-seventh annual ACM Symposium on Theory of computing,
STOC ‚Äô05, pp. 573‚Äì581, New York, NY, USA. ACM.
597

Wu, Austrin, Pitassi, & Liu

Ambuhl, C., Mastrolilli, M., & Svensson, O. (2007). Inapproximability Results for Sparsest
Cut, Optimal Linear Arrangement, and Precedence Constrained Scheduling. In Proceedings of the 48th Annual IEEE Symposium on Foundations of Computer Science,
FOCS ‚Äô07, pp. 329‚Äì337, Washington, DC, USA. IEEE Computer Society.
Amir, E. (2001). Efficient approximation for triangulation of minimum treewidth. In Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence, pp. 7‚Äì15.
Morgan Kaufmann Publishers.
Arnborg, S., Corneil, D. G., & Proskurowski, A. (1987). Complexity of finding embeddings
in a k-tree. SIAM J. Algebraic Discrete Methods, 8, 277‚Äì284.
Arora, S., Barak, B., & Steurer, D. (2010). Subexponential algorithms for unique games
and related problems. In Annual Symposium on Foundations of Computer Science,
FOCS ‚Äô10, pp. 563‚Äì572.
Arora, S., Lund, C., Motwani, R., Sudan, M., & Szegedy, M. (1998). Proof verification and
the hardness of approximation problems. J. ACM, 45 (3), 501‚Äì555.
Arora, S., Rao, S., & Vazirani, U. (2009). Expander flows, geometric embeddings and graph
partitioning. J. ACM, 56, 5:1‚Äì5:37.
Arora, S., & Safra, S. (1998). Probabilistic checking of proofs: a new characterization of np.
J. ACM, 45 (1), 70‚Äì122.
Barak, B., Raghavendra, P., & Steurer, D. (2011). Rounding semidefinite programming
hierarchies via global correlation. ECCC Report TR11-065.
Bodlaender, H. L., Gilbert, J. R., Hafsteinsson, H., & Kloks, T. (1995). Approximating
treewidth, pathwidth, frontsize, and shortest elimination tree. Journal of Algorithms,
18 (2), 238‚Äì255.
Bodlaender, H. (2007). Treewidth: Structure and algorithms. In Prencipe, G., & Zaks, S.
(Eds.), Structural Information and Communication Complexity, Vol. 4474 of Lecture
Notes in Computer Science, pp. 11‚Äì25. Springer Berlin / Heidelberg. 10.1007/978-3540-72951-8 3.
Bodlaender, H. L. (1996). A linear-time algorithm for finding tree-decompositions of Small
Treewidth. SIAM Journal on Computing, 25 (6), 1305‚Äì1317.
Bodlaender, H. L. (2005). Discovering treewidth. In SOFSEM, pp. 1‚Äì16.
BouchitteÃÅ, V., & Todinca, I. (2003). Approximating the treewidth of at-free graphs. Discrete
Applied Mathematics, 131 (1), 11‚Äì37.
Charikar, M., Hajiaghayi, M., Karloff, H., & Rao, S. (2010). l22 spreading metrics for vertex
ordering problems. Algorithmica, 56, 577‚Äì604.
Dubey, C. K., Feige, U., & Unger, W. (2011). Hardness results for approximating the
bandwidth. J. Comput. Syst. Sci., 77 (1), 62‚Äì90.
Feige, U., Hajiaghayi, M., & Lee, J. R. (2005). Improved approximation algorithms for
minimum-weight vertex separators. In Proceedings of the thirty-seventh annual ACM
Symposium on Theory of computing, STOC ‚Äô05, pp. 563‚Äì572, New York, NY, USA.
ACM.
598

Inapproximability of Treewidth and Related Problems

Fomin, F. V., & Villanger, Y. (2012). Subexponential parameterized algorithm for minimum
fill-in. In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA ‚Äô12, pp. 1737‚Äì1746. SIAM.
Fulkerson, D. R., & Gross, O. A. (1965). Incidence matrices and interval graphs. Pacific
Journal of Mathematics, 15, 835‚Äì855.
Guruswami, V., & Sinop, A. K. (2011). Lasserre hierarchy, higher eigenvalues, and approximation schemes for quadratic integer programming with psd objectives..
HaÃästad, J. (1999). Clique is hard to approximate withinn 1- Œµ. Acta Mathematica, 182 (1),
105‚Äì142.
HaÃästad, J. (2001). Some optimal inapproximability results. J. ACM, 48 (4), 798‚Äì859.
Heggernes, P. (2006). Minimal triangulations of graphs: A survey. Discrete Mathematics.
Kaplan, H., Shamir, R., & Tarjan, R. E. (1994). Tractability of parameterized completion problems on chordal and interval graphs: Minimum fill-in and physical mapping
(extended abstract). SIAM J. Comput, 28, 780‚Äì791.
Khot, S., & Regev, O. (2008). Vertex cover might be hard to approximate to within 2- Œµ.
Journal of Computer and System Sciences, 74 (3), 335‚Äì349.
Khot, S., & Vishnoi, N. (2005). On the unique games conjecture. In Annual Symposium
on Foundations of Computer Science, Vol. 46 of FOCS ‚Äô05, p. 3. IEEE COMPUTER
SOCIETY PRESS.
Khot, S. (2002). On the power of unique 2-prover 1-round games. In Proceedings of the
thiry-fourth annual ACM Symposium on Theory of Computing, STOC ‚Äô02, pp. 767‚Äì
775, New York, NY, USA. ACM.
Kinnersley, N. G. (1992). The vertex separation number of a graph equals its path-width.
Information Processing Letters, 42 (6), 345‚Äì350.
Kirousis, L. M., & Papadimitriou, C. H. (1986). Searching and pebbling. Theor. Comput.
Sci., 47, 205‚Äì218.
Kloks, T. (1994). Treewidth: computations and approximations, Vol. 842. Springer.
Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques. MIT press.
Leighton, T., & Rao, S. (1999). Multicommodity max-flow min-cut theorems and their use
in designing approximation algorithms. J. ACM, 46, 787‚Äì832.
Lengauer, T. (1981). Black-white pebbles and graph separation. Acta Informatica, 16,
465‚Äì475. 10.1007/BF00264496.
Lengauer, T., & Tarjan, R. E. (1982). Asymptotically tight bounds on time-space trade-offs
in a pebble game. J. ACM, 29, 1087‚Äì1130.
Natanzon, A., Shamir, R., & Sharan, R. (1998). A polynomial approximation algorithm for
the minimum fill-in problem. In Proceedings of the thirtieth annual ACM Symposium
on Theory of Computing, STOC ‚Äô98, pp. 41‚Äì47, New York, NY, USA. ACM.
NordstroÃàm, J. (2010). New wine into old wineskins: A survey of some pebbling classics with
supplemental results. Draft manuscript.
599

Wu, Austrin, Pitassi, & Liu

Raghavendra, P. (2008). Optimal algorithms and inapproximability results for every csp?.
In Proceedings of the 40th annual ACM Symposium on Theory of computing, pp. 245‚Äì
254. ACM.
Raghavendra, P., & Steurer, D. (2010). Graph expansion and the unique games conjecture.
In Proceedings of the 42nd ACM Symposium on Theory of computing, STOC ‚Äô10, pp.
755‚Äì764, New York, NY, USA. ACM.
Raghavendra, P., Steurer, D., & Tulsiani, M. (2012). Reductions Between Expansion Problems. IEEE Conference on Computational Complexity.
Ramalingam, G., & Rangan, C. P. (1988). A unified approach to domination problems on
interval graphs. Inf. Process. Lett., 27, 271‚Äì274.
Rao, S., & Richa, A. W. (1998). New approximation techniques for some ordering problems.
In Proceedings of the ninth annual ACM-SIAM Symposium on Discrete Algorithms,
SODA ‚Äô98, pp. 211‚Äì218, Philadelphia, PA, USA. Society for Industrial and Applied
Mathematics.
Ravi, R., Agrawal, A., & Klein, P. (1991). Ordering problems approximated: single-processor
scheduling and interval graph completion. In Albert, J., Monien, B., & Artalejo, M.
(Eds.), Automata, Languages and Programming, Vol. 510 of Lecture Notes in Computer Science, pp. 751‚Äì762. Springer Berlin / Heidelberg. 10.1007/3-540-54233-7180.
Reed, B. A. (1992). Finding approximate separators and computing tree width quickly. In
Proceedings of the Twenty-fourth Annual ACM Symposium on Theory of Computing,
STOC ‚Äô92, pp. 221‚Äì228, New York, NY, USA. ACM.
Robertson, N., & Seymour, P. D. (1984). Graph minors. III. Planar tree-width. J. Comb.
Theory, Ser. B, 36 (1), 49‚Äì64.
Robertson, N., & Seymour, P. D. (1986). Graph minors. II. Algorithmic aspects of treewidth. Journal of Algorithms, 7 (3), 309‚Äì322.
Roth, D. (1996). On the hardness of approximate reasoning. Artificial Intelligence, 82 (1),
273‚Äì302.
Sethi, R. (1973). Complete register allocation problems. In Proceedings of the fifth annual
ACM Symposium on Theory of computing, STOC ‚Äô73, pp. 182‚Äì195, New York, NY,
USA. ACM.
Seymour, P. D., & Thomas, R. (1994). Call routing and the ratcatcher. Combinatorica,
14 (2), 217‚Äì241.

600

Journal of Artificial Intelligence Research 49 (2014) 705-731

Submitted 12/13; published 4/14

Convergence of a Q-learning Variant for
Continuous States and Actions
Stephen Carden

carden@clemson.edu

Department of Mathematical Sciences,
Clemson University

Abstract
This paper presents a reinforcement learning algorithm for solving infinite horizon
Markov Decision Processes under the expected total discounted reward criterion when both
the state and action spaces are continuous. This algorithm is based on Watkins‚Äô Q-learning,
but uses Nadaraya-Watson kernel smoothing to generalize knowledge to unvisited states.
As expected, continuity conditions must be imposed on the mean rewards and transition
probabilities. Using results from kernel regression theory, this algorithm is proven capable
of producing a Q-value function estimate that is uniformly within an arbitrary tolerance
of the true Q-value function with probability one. The algorithm is then applied to an
example problem to empirically show convergence as well.

1. Introduction
A Markov Decision Process (MDP) is a stochastic control problem, usually considered in
discrete time. At each time step, the system is in a state and a controller chooses an action.
The system then transitions to a new state with a probability distribution that depends
both on the previous state and the action utilized. A reward (or cost, for some authors)
is also received. The rewards are allowed to be random variables with a distribution that
depends on the state and the action utilized. The goal is to find a policy (a function that
maps states to actions) that maximizes rewards (or minimizes costs) according to some
measure of goodness. We will consider MDPs for which there are no terminal, absorbing
states. In this case, a common optimality criterion is the expected total discounted reward.
MDPs under this criterion on finite state-action spaces have been well studied. If the
state transition probabilities and reward means are known, then dynamic programming
methods can obtain an optimal policy in a finite number of steps (Ross, 1992). However, if
probabilities and rewards are initially unknown but transitions and rewards can be simulated
or observed, then the problem becomes much more difficult, one amenable to the methods
of reinforcement learning.
Watkins (1989) developed Q-learning, a novel and popular algorithm for learning the
value of a state-action pair under an optimal policy without explicitly calculating transition
probabilities and mean rewards. The values can then be used to construct an optimal
policy. If there are a finite number of states and actions, then the learned values are proven
to converge to their true values. However, problems with a very large number of states or
actions may cause the values to converge unreasonably slowly. Furthermore, if the number
of state-action pairs is infinite, then the convergence guarantee no longer holds. A natural
question is whether the knowledge gained from an observation can be generalized using
function approximators to similar states and actions, and whether the state-action values
c
2014
AI Access Foundation. All rights reserved.

Carden

still converge, either theoretically or empirically. The immediacy of this question is apparent
even in Watkins‚Äô thesis (1989), where a form of function approximation, the CMAC (Albus,
1975), is used in the solution to two example problems with a large number of state-action
pairs.
Function approximators in reinforcement learning may cause value estimates to converge
to suboptimal values (Bertsekas, 1995), oscillate (Gordon, 1996), or even diverge (Fairbank
& Alonso, 2012; Tsitsiklis & Van Roy, 1996). However, when used carefully, they have also
been impressively effective. A famous example of success with a large yet finite state-action
space is the TD-Gammon program (Tesauro, 1995), which used temporal difference learning
(Sutton, 1988) and a neural network to generalize knowledge. It eventually learned to be
competitive with tournament-level human backgammon players. Scenarios with continuous
states and finite actions have been well studied. Empirical convergence has been observed
with CMACs (Sutton, 1996), neural networks (Rummery & Niranjan, 1994; ten Hagen,
1991), and regression trees (Ernst et al., 2005). Theoretically, there have been positive
results as well. A parametric function approximator can yield convergent results if it satisfies
certain interpolation properties (SzepesvaÃÅri & Smart, 2004). A non-parametric technique,
kernel regression, has also been proven to converge to optimal values (Ormoneit & Sen,
1999).
Problems where both the states and the actions are allowed to be continuous have
been less well-studied. One major reason is that constructing a policy requires taking a
supremum over all possible actions, which is straightforward when actions are finite but
generally difficult when continuous. This difficulty, though present, has not prevented
researchers from developing techniques to make the process manageable. One method is
to calculate values for a discrete set of actions, then use a weighted average to produce a
continuous valued action (MillaÃÅn et al., 2002). Baird and Klopf (1993) specifically designed
a function approximator, termed ‚Äúwire-fitting,‚Äù such that the supremum over the action
space can be immediately obtained. Wire-fitting has been combined with neural networks
to learn thruster-control in order to drive a submersible to a target with success (Gaskett
et al., 1999). Algorithms allowing for stochastic policies can rapidly choose actions from a
probability distribution, as in Sequential Monte Carlo Learning (Lazaric et al., 2008).
Despite the practical success of these algorithms, there has been very little theoretical
investigation into the convergence properties when actions are continuous. The purpose of
this paper is to present what is, to my knowledge, the first off-policy Q-learning variant
allowing for continuous state and actions which is proven to converge to optimal values with
probability one. Specifically, if the state and action spaces are both allowed to be infinite
but compact subsets of Euclidean space, then with a sufficiently small kernel regression
bandwidth and suitable continuity conditions on the random rewards and transition probabilities, one may obtain a Q-value function estimate that is uniformly within an arbitrary
tolerance of the true Q-value function. Though the intention of the algorithm is to fill a
theoretical gap, the results of a proof-of-concept example implementation will be provided.
The particular method of knowledge generalization is Nadaraya-Watson kernel regression, which is a memory-based non-parametric smoothing technique with well-studied properties. It is similar to the function approximators used by Ormoneit and Sen (1999) and
Santamarƒ±ÃÅa et al. (1996). A distinction should be made between Nadaraya-Watson kernel
regression and the use of kernel functions through the so-called ‚Äúkernel trick‚Äù in the context
706

A Continuous Action Q-learning Variant

of reinforcement learning. The methods are similar in that they use a kernel function and
deliver non-linear regression results as kernel based weighted sums, but the intermediate
methodologies are quite different. See the work of Taylor and Parr (2009) for a discussion
of kernelized value function methods, and the work of Xu et al. (2007) for an application
of kernelized regression to policy iteration.
In Section 2, Markov Decision Processes will be defined. Section 3 introduces NadarayaWatson kernel regression and describes an algorithm that uses kernel smoothing to obtain
a function that estimates the Q-value of each state-action pair. The main result of this
paper, theoretical convergence of a continuous state and action algorithm, is stated in
Section 4 along with sufficient conditions for convergence. Section 5 describes the strategy
of the proof, proves a set of lemmas, and finally proves the main result. An example
implementation along with a discussion of practical difficulties is in Section 6. Finally,
Section 7 concludes with a few ideas for extensions.

2. Description of a Continuous Markov Decision Process
It is assumed the reader is familiar with the basic theory of discrete Markov Decision
Processes. For an introduction, we recommend the texts by Puterman (1994) and Ross
(1992). This paper will consider Markov Decision Processes for which the state and action
spaces are not discrete, but are compact subsets of Euclidean space. Let S, the state
space, be a compact subset of Euclidean space of dimension dS and B(S) be the Borel œÉalgebra on S. Let A, the action space, be a compact subset of Euclidean space of dimension
dA . Note that S √ó A is a subset of Rd where d = dS + dA . Elements of this space will
be written (s, a) to make the state and action clear, but for all computational purposes
they are best regarded as numerical vectors. Transitions between states are governed by a
function P : S √ó B(S) √ó A ‚Üí R+ satisfying:
‚Ä¢ For each a ‚àà A and B ‚àà B(S), P (¬∑, B, a) : S ‚Üí R+ is a measurable function.
‚Ä¢ For each s ‚àà S and a ‚àà A, P (s, ¬∑, a) : B(S) ‚Üí R+ is a probability measure on S.
‚Ä¢ For each s ‚àà S and B ‚àà B(S), P (s, B, ¬∑) : A ‚Üí R+ is a measurable function.
For each state and action (s, a) there is a random reward R(s, a) bounded by a constant
C0 .
The Markov Decision Process proceeds as follows: the process begins in some state s ‚àà S
and the controller chooses an action a ‚àà A. A random reward R(s, a) is received and the
system transitions to a new state in accordance with the probability measure P (s, ¬∑, a). The
system progresses to the next time step, and this scenario repeats from the new state. The
reward distributions and transition probabilities are assumed to have the Markov property:
they only depend on the current state and action; they do not depend on the history of
previous states and actions.
A policy œÄ is a measurable function œÄ : S ‚Üí A. In some settings, policies are allowed to
be stochastic or non-stationary, but we will restrict our attention to deterministic policies.
Define PœÄ (s, B) := P (s, B, œÄ(s)). Then PœÄ is a transition kernel for a Markov chain on the
space S.
707

Carden

Let Œ≥, 0 < Œ≥ < 1, be a discount factor. Let (st , at ) be the (random) state and action
at time t. Under a policy œÄ, the value of a state s is defined under the expected total
discounted reward criterion:
"‚àû
#
X
V œÄ (s) = E
Œ≥ t R(st , œÄ(st ))|s0 = s .
t=0
‚àó

A policy œÄ ‚àó is optimal if it satisfies V œÄ (s) = sup V œÄ (s), ‚àÄs ‚àà S.
œÄ

Our goal is to find an algorithm which will, under suitable conditions, converge to an
optimal policy. To that end, we will define Q-values as
Z
QœÄ (s, a) := E[R(s, a)] + Œ≥
V œÄ (t)P (s, dt, a).
S

This is a continuous version of the discrete definition made by Watkins (1989). Intuitively
QœÄ (s, a) is the value obtained if we start in state s, utilize action a, and then follow policy
œÄ thereafter. Consider the Q-values associated with an optimal policy œÄ ‚àó : Q‚àó (s, a) :=
‚àó
QœÄ (s, a). If we can learn the values Q‚àó (s, a) for all (s, a), then an optimal policy can easily
be recovered by setting
œÄ ‚àó (s) = argsup Q‚àó (s, a).
a‚ààA

3. Description of Algorithm
This section will introduce the form of the function approximator to be used, and then
describe an algorithm which will generate a sequence of estimates of the optimal Q-values.
3.1 Nadaraya-Watson Kernel Regression
Nadaraya-Watson kernel regression (Watson, 1964; Nadaraya, 1964) is a smoothing technique which estimates the value at a point as a weighted average of nearby observations,
using a non-negative kernel function to assign weights to observations based on their distance. Kernels typically are symmetric, peak at zero, and decrease away from zero so that
the weight of an observation is inversely related to the distance. Formally, we will require
K : Rd ‚Üí R+ to be a multivariate function satisfying:
R
KI. K is integrable: Rd K(u)du < ‚àû.
KII. K is bounded: ||K||‚àû < CK < ‚àû.
KIII. K has compact support: There exists L < ‚àû such that
K(u) = 0 when ||u||‚àû > L.
KIV. K is Lipschitz: There exists M < ‚àû such that for all u, u0 ‚àà Rd ,
|K(u) ‚àí K(u0 )| ‚â§ M ||u ‚àí u0 ||‚àû .
708

A Continuous Action Q-learning Variant

The compact support condition is not required in all applications (indeed, the Gaussian
function is a common kernel choice) but will be used here to reduce computation load by
assigning zero weight to sufficiently distant observations.
A bandwidth h > 0 is a parameter used to fine-tune the level of smoothing. Define
Kh (u) :=

1
K(u/h).
h

If u is non-scalar, the division may be understood to be component-wise. Values of h which
are too large or small relative to the sample size tend to oversmooth or overfit the data.
Typically when using kernel regression the bandwidth decreases toward zero as the number
of observations increases. A discussion of the particulars of bandwidth selection is beyond
the scope of this paper, but a rule of thumb obtained from assuming Gaussian density and
1
unit variance is h = n‚àí 4+d , where n is the number of observations and d is the dimension
(HaÃàrdle, 2004). This rule of thumb should be regarded as a starting value, not a definitive
answer. For example, cross-validation is a common method for selecting bandwidths. For
more information, the interested reader is referred to the texts by Silverman (1986) or
Simonoff (1996). For the purpose of the algorithm to follow, we need only a constant
bandwidth which is sufficiently small.
Given a set of n ordered pairs (xi , yi ) and a bandwidth h, the Nadaraya-Watson estimator is
Pn
Kh (x ‚àí xi )yi
m
ch (x) = Pi=1
.
n
i=1 Kh (x ‚àí xi )
3.2 The Algorithm
We will use the following notation. For n > 0,
‚Ä¢ h is a fixed bandwidth.
‚Ä¢ Œ≥ is a fixed discount factor.
‚Ä¢ (sn , an ) is the state-action pair at the beginning of iteration n.
‚Ä¢ rn is the sample reward observed at iteration n.
‚Ä¢ un is the state that is transitioned to during iteration n.
b h,n‚àí1 (un , a), where Q
b h,n‚àí1 (un , a) is defined below in (2).
‚Ä¢ yh,n := rn + Œ≥ supa‚ààA Q
Here is a description of the algorithm.
b h,0 (s, a) = 0 for all
1. Set the initial Q-value estimate function to be zero everywhere. Q
(s, a) ‚àà S √ó A. Choose some initial state s1 .
2. For n > 0, pick an action an -greedily (or according to some other exploration
method). Define a function Œ±h,n : S √ó A ‚Üí [0, 1] by
Ô£±
P
Ô£¥
Ô£≤ P Kh ((s, a) ‚àí (sn , an ))
if nj=1 Kh ((s, a) ‚àí (sj , aj )) 6= 0
n
Œ±h,n (s, a) =
(1)
j=1 Kh ((s, a) ‚àí (sj , aj ))
P
Ô£¥
Ô£≥0
if nj=1 Kh ((s, a) ‚àí (sj , aj )) = 0
709

Carden

and use this to update the Q-value estimate function by setting
b h,n (s, a) := (1 ‚àí Œ±h,n (s, a))Q
b h,n‚àí1 (s, a) + Œ±h,n (s, a)yh,n .
Q

(2)

Set sn+1 = un .
Note that as a result of Equation (2), one may show by induction that
Ô£± Pn
P
Ô£¥
j=1 Kh ((s, a) ‚àí (sj , aj ))yh,j
Ô£¥
P
if nj=1 Kh ((s, a) ‚àí (sj , aj )) 6= 0
Ô£¥
n
Ô£≤
j=1 Kh ((s, a) ‚àí (sj , aj ))
b h,n (s, a) =
P
Q
b
Ô£¥
Qh,0 (s, a)
if nj=1 Kh ((s, a) ‚àí (sj , aj )) = 0.
Ô£¥
Ô£¥
Ô£≥

(3)

Equation (2) is how the updates are conceptually performed and has a form familiar to
classic Q-learning, but Equation (3) is how all calculations are made. The estimates
produced by this algorithm are essentially kernel-smoothed averages of the terms yh,n :=
b h,n‚àí1 (un , a).
rn + Œ≥ supa‚ààA Q
Algorithm 1 Pseudocode for theoretical algorithm
Initialize h = bandwidth value, m = maximum iterations,
Œ≥ = discount factor,  = exploration parameter
b h,0 (s, a) = 0 ‚àÄ(s, a)
Initialize Q
Set initial state s1
for i=1:m do
r = Uniform(0,1) random value
if r <  then ai = random action
else
b h,n‚àí1 (si , a)
ai = supa‚ààA Q
end if
ui = next state, ri = reward
b h,i‚àí1 (ui , a)
yh,i := ri + Œ≥ supa‚ààA Q
Pi
j=1 Kh ((s,a)‚àí(sj ,aj ))yh,j
b h,i (s, a) = P
Q
i
si+1 = ui
end for

j=1

Kh ((s,a)‚àí(sj ,aj ))

4. Statement of Theorem
Assume the following conditions hold:
AI. The state-action pair to be utilized at the beginning of an iteration (regarded as a
random variable due to exploration and random transitions) has a density f : S √óA ‚Üí
R which is positive everywhere and has uniformly continuous and bounded second
derivatives.
AII. Rewards are bounded. There exists a C0 < ‚àû such that for any (s, a) ‚àà S √ó A,
R(s, a) < C0 .
710

A Continuous Action Q-learning Variant

AIII. The expected values of the rewards are Lipschitz continuous across the state-action
space. There exists a Cr such that for all (s1 , a1 ), (s2 , a2 ) ‚àà S √ó A,
|E[R(s1 , a1 )] ‚àí E[R(s2 , a2 )]| ‚â§ Cr ||(s1 , a1 ) ‚àí (s2 , a2 )||.
Also, E[R(s, a)]f (s, a) has uniformly continuous and bounded second derivatives.
AIV. Transition probabilities converge weakly and Lipschitz continuously. Let g : S ‚Üí R be
continuous and bounded. There exists a Ct such that for any (s1 , a1 ), (s2 , a2 ) ‚àà S √ó A,
Z

Z


 g(u)P (s1 , du, a1 ) ‚àí g(u)P (s2 , du, a2 ) ‚â§ Ct ||g(u)||‚àû ||(s1 , a1 ) ‚àí (s2 , a2 )||


Also,
tives.

R

g(u)P (s, du, a)f (s, a) has uniformly continuous and bounded second deriva-

Theorem 1. Let assumptions AI. - AIV. and kernel conditions KI. - KIV. hold and
b h,n (s, a) be defined by (2). With probability one, for any  > 0, there exists h = h()
Q
and N = N (h, ) such that for n > N
sup

b h,n (s, a) ‚àí Q‚àó (s, a)| < .
|Q

(4)

(s,a)‚ààS√óA

5. Lemmas and Proof of Theorem
The proof strategy is inspired by Watkins‚Äô original proof over finite spaces. For the interested
reader, we suggest the technical report (Watkins & Dayan, 1992) that followed Watkins‚Äô
thesis. This method is different from the later approaches to proving convergence of Qlearning, such as those of Tsitsiklis (1994) and Jaakkola et al. (1993), which are variations
and extensions of stochastic approximation theory. Although Watkins‚Äô method does rely
on a stochastic approximation result in a minor way, the key intuition is how Q-learning
imitates model estimation.
One of the strengths of Q-learning is that it does not require a model of the system to be
estimated or maintained. However, the learned Q-values are the optimal values for a model
that can be constructed by appropriately assigning weights to the observed rewards and
transitions. Specifically, if the learning parameters used to update value estimates are used
to weight reward and transition observations to carefully define an artificial process, then
the learned Q-value estimates will be optimal for this artificial process. As more rewards
and transitions are observed, the artificial process becomes more similar to the real process,
so the optimal Q-values for the two processes become more similar, thus the learned values
converge to the optimal values for the real process. Watkins called such an artificial process
the Action Replay Process.
The proof strategy can be summarized in the following five steps:
1. For a fixed bandwidth value, define an auxiliary Action Replay Process (ARP). This
artificial process is purely a proof device.
b h,n (s, a) is the optimal Q-value function for the ARP with
2. Show that the function Q
corresponding bandwidth.
711

Carden

3. Show that for sufficiently small bandwidths, the rewards and transition probabilities
from the ARP become arbitrarily close to those of the original process.
4. Show that two MDPs with similar rewards and transition probabilities have similar
optimal Q-values.
5. The optimal values of the ARP converge to the optimal values of the original process.
Hence by step 2, the Q-values learned by the algorithm converge to the optimal Qvalues.
5.1 Construction of the ARP (Action Replay Process)
Now the ARP will be defined. It is a finite length, terminating MDP. Pick a bandwidth
value h, and keep it fixed throughout the discussion that follows. We suggest thinking of
the ARP as a card game. Suppose that one is preparing to run the algorithm from Section
3. Also suppose that we have a deck of index cards. For each iteration of the algorithm,
b h,k (¬∑), Œ±h,k (¬∑) >. These values
we will write the following values on a card: < sk , ak , uk , rk , Q
are regarded as random variables depending on the random states, actions, and rewards
from the original process rather than fixed values from a particular sample trajectory. We
b h,0 (¬∑) is written. With this card on bottom, stack all
also have a card on which the initial Q
cards in ascending order by iteration as shown in Figure 1.
b h,n (¬∑), Œ±h,n (¬∑) >
< sn , an , un , rn , Q

Level n

b h,n‚àí1 (¬∑), Œ±h,n‚àí1 (¬∑) >
< sn‚àí1 , an‚àí1 , un‚àí1 , rn‚àí1 , Q

Level n ‚àí 1

..
..
..
b h,2 (¬∑), Œ±h,2 (¬∑) >
< s2 , a2 , u2 , r2 , Q

Level 2

b h,1 (¬∑), Œ±h,1 (¬∑) >
< s1 , a1 , u1 , r1 , Q

Level 1

b h,0 (¬∑) >
<Q

Level 0

Figure 1: The ARP envisioned as a stack of cards.

States A state of the ARP is a 2-tuple < s, n > consisting of a state from the original state
space s ‚àà S and a non-negative level n which tells us which card is to be inspected.
All cards above level n are ignored, leaving us with a finite stack of cards. Any state
with level 0 is a terminal, absorbing state.
Actions The actions for the ARP are the same as the actions from the real process, a ‚àà A.
712

A Continuous Action Q-learning Variant

Transitions and Rewards Suppose the ARP is in state < s, n > and action a is chosen.
Inspect the card corresponding to level n. With probability Œ±h,n (s, a), we accept this
card and receive reward rn and transition to state < un , n ‚àí 1 >. Otherwise, ignore
that card and inspect the card at level n ‚àí 1. With probability Œ±h,n‚àí1 (s, a), accept
this card, receive reward rn‚àí1 and transition to state < un‚àí1 , n ‚àí 2 >. Otherwise,
continue inspecting cards until one is accepted, where the probability of accepting the
card at level k is Œ±h,k (s, a). If the bottom card corresponding to level 0 is reached
(that is, the state is < s, 0 > and action a is being utilized) then a reward Qh,0 (s, a)
is received and the ARP terminates.
As the ARP is an MDP, it has optimal Q-values. Let Q‚àóh (< s, n >, a) be the function
giving the optimal Q-values for state < s, n > and action a for the ARP with bandwidth h.
5.2 Lemmas
The optimal Q-values at level n for the ARP corresponding to bandwidth h are the learned
Q-value estimates for the original process at iteration n as defined by (2).
Lemma 1.
b h,n (s, a), ‚àÄ(s, a) ‚àà S √ó A, ‚àÄn ‚â• 0.
Q‚àóh (< s, n >, a) = Q
Proof. Proceed by induction on the level n of the ARP. Recall that if the ARP is in state
b h,0 (s, a) and the process terminates.
< s, 0 > and action a is used, then the reward is Q
Thus
b h,0 (s, a)
Q‚àóh (< s, 0 >, a) = Q
which proves the n = 0 case.
b h,n‚àí1 (s, a) for all states
Now suppose by way of induction that Q‚àóh (< s, n ‚àí 1 >, a) = Q
and actions. Let the ARP be in state < s, n > with action a utilized. Recall that by
construction of the ARP,
‚Ä¢ with probability Œ±h,n (s, a) we receive reward rn and transition to < un , n ‚àí 1 >.
Otherwise,
‚Ä¢ with probability 1 ‚àí Œ±h,n (s, a) that card is thrown away and the situation is identical
to utilizing a in state < s, n ‚àí 1 >.
Then by conditioning on whether the level n card is accepted or not, we have by the
induction hypothesis and (2):
Q‚àóh (< s, n >, a) = (1 ‚àí Œ±h,n (s, a))Q‚àóh (< s, n ‚àí 1 >, a)
+ Œ±h,n (s, a)(rn + Œ≥ sup Q‚àóh (< un , n ‚àí 1 >, b))
b‚ààA

b h,n‚àí1 (s, a)
= (1 ‚àí Œ±h,n (s, a))Q
b h,n‚àí1 (un , b))
+ Œ±h,n (s, a)(rn + Œ≥ sup Q
b‚ààA

b h,n‚àí1 (s, a) + Œ±h,n (s, a)yh,n
= (1 ‚àí Œ±h,n (s, a))Q
b h,n (s, a).
=Q

713

Carden

The following lemma, which is needed in the proof of Lemma 3, shows that
grows uniformly across the state-action space.

Pn

k=1 Œ±h,k (s, a)

Lemma 2. If assumption AI. holds, then with probability one,
lim

inf

n
X

n‚Üí‚àû (s,a)‚ààS√óA

Œ±h,k (s, a) ‚Üí ‚àû.

(5)

k=1

Proof. S √ó A is compact, so it can be covered by a finite number of disjoint sets of fixed
positive diameter with each set having positive Lebesgue measure. Choose the diameter
such that u, u0 in the same set implies Kh (u ‚àí u0 ) > Œ¥ > 0. Suppose that J sets are needed,
and denote them by B1 , B2 , ..., BJ . Set T0 = 0 and for k > 0, let Tk denote the first time at
which each set has been visited at least k times. Note that by AI. and the Borel-Cantelli
Lemma, P (Tk < ‚àû) = 1. Fix (s, a) ‚àà S √ó A, and letting CK denote the bound on the
kernel values,
Tn
X
k=1

Œ±h,k (s, a) =

n
X

Tk
X

Œ±h,j (s, a) ‚â•

k=1 j=Tk‚àí1 +1

n
X
k=1

n
Œ¥ X ‚àí1
=
Tk .
CK Tk
CK

Œ¥

k=1

The terms Tk , k > 0, are not independent but can be bounded by random variables which
are. Set W1 = T1 . Ignoring all previous visits to sets prior to and at time T1 , set W2 to be
the number of steps from T1 until each Bj is visited again. It is obvious that W2 ‚â• T2 ‚àí T1 .
Likewise, for k > 1, set Wk to be the number of steps from Tk‚àí1 until each set is visited
again, so that Wk ‚â• Tk ‚àíTk‚àí1 . Notice that W1 , W2 , ... are i.i.d. random variables. Applying
these inequalities, one may write
T1‚àí1 = W1‚àí1
T2‚àí1 ‚â• (W1 + W2 )‚àí1
..
.
Ô£´
Ô£∂‚àí1
k
X
T ‚àí1 ‚â• Ô£≠
Wj Ô£∏ .
k

j=1

We now have
Tn
X
k=1

Ô£´
Ô£∂‚àí1
n
k
Œ¥ X Ô£≠X
Œ±h,k (s, a) ‚â•
Wj Ô£∏ .
CK
k=1

j=1

‚àí1
Pn Pk
It suffices to show that
goes to infinity with probability one. Set
k=1
j=1 Wj
Pn
Pn
Pn 1 k
P
1
1
Sn = k=1 Wk . Then k=1 Sk = k=1 k Sk . Since Skk ‚Üí E[W
> 0, nk=1 k1 Skk ‚Üí ‚àû with
1]
probability one. (s, a) was arbitrary, which proves the result.
The next lemma shows that if one starts the ARP at a sufficiently high level, the
probability of ending below a certain level after a fixed number of actions can be made
arbitrarily small.
714

A Continuous Action Q-learning Variant

Lemma 3. Let  > 0, l0 be any level of the ARP, and positive integer n represent the length
of a sequence of actions to be followed. Then there exists a level l > l0 such that if the ARP
starts at a level above l and follows the sequence of n actions, the probability of ending below
level l0 is less than .
Proof. Proceed by induction. Consider first the case where n = 1. For m > l0 , suppose the
ARP is in state < s, m > and action a utilized. Transitioning to a state with level below l0
means no card at or above level l0 is accepted. The probability of not accepting the card
at level k is 1 ‚àí Œ±h,k (s, a), so the probability of accepting none is Œ†m
k=l0 (1 ‚àí Œ±h,k (s, a)). A
well-known inequality from real analysis yields
‚àí

Œ†m
k=l0 (1 ‚àí Œ±h,k (s, a)) < e

Pm

k=l0

Œ±h,k (s,a)

.

It follows from Lemma 2 that it is possible to choose l1 such that m > l1 implies
m
X

inf
(s,a)‚ààS√óA

Œ±h,k (s, a) > ‚àí log .

k=l0

Then

‚àí

P (ending below l0 ) = Œ†m
k=l0 (1 ‚àí Œ±h,k (s, a)) < e

Pm

k=l0

Œ±h,k (s,a)

< .

For a sequence of n actions, there exists l1 such that the probability of ending below
l0 from l1 in one transition is less than 2 and by induction there is an l2 such that the
probability of ending below l1 in n ‚àí 1 transitions is less than 2 . Then if the ARP starts
above level l2 ,
P (below l0 after n transitions)
=P (below l0 after n transitions|above l1 after n ‚àí 1 transitions)
‚àó P (above l1 after n ‚àí 1 transitions)
+P (below l0 after n transitions|below l1 after n ‚àí 1 transitions)
‚àó P (below l1 after n ‚àí 1 transitions)


‚â§ ¬∑ 1 + 1 ¬∑ = .
2
2
The next lemma allows one to work with finite sequences of actions rather than infinite
ones with an arbitrarily small error.
Lemma 4. Consider the value of a state s when a specific sequence of n actions (a0 , a1 , ..., an‚àí1 )
is to be followed and the process is then terminated:
"n‚àí1
#
X
t
E
Œ≥ R(st , at )|s0 = s .
t=0

Also consider the value of the same state under the same n actions but then followed by an
arbitrary policy œÄ:
"n‚àí1
#
‚àû
X
X
t
t
E
Œ≥ R(st , at ) +
Œ≥ R(st , œÄ(st ))|s0 = s .
t=0

t=n

The difference of these two values goes to zero as n increases towards infinity.
715

Carden

Proof. We are interested in the difference
"n‚àí1
#
"n‚àí1
#
‚àû
X
X
X
E
Œ≥ t R(st , at ) +
Œ≥ t R(st , œÄ(st ))|s0 = s ‚àí E
Œ≥ t R(st , at )|s0 = s .
t=n

t=0

t=0

The difference is clearly the expectation of the second term in the first expectation. Because
the rewards are bounded by C0 , we can use a change of variables v = t ‚àí n and write
"‚àû
#
"‚àû
#
X
X
t
v+n
E
Œ≥ R(st , œÄ(st ))|s0 = s = E
Œ≥
R(sv , œÄ(sv ))|s0 = s
t=n

v=0

"
‚â§ Œ≥nE

‚àû
X

#
Œ≥ v |R(sv , œÄ(sv ))||s0 = s

v=0

‚â§ Œ≥n

‚àû
X

Œ≥ v C0 = Œ≥ n

v=0

C0
1‚àíŒ≥

which goes to zero as n goes to infinity.
The next lemma will consider the reward received from the ARP when in state < s, n >
and action a is utilized. This random quantity depends on the state-actions chosen and
rewards received in the original process, but it also depends on which levels of the ARP
are accepted during state transitions. For the ARP constructed with bandwidth h and
b h,n (s, a)] denote an
rewards received when in state < s, n > and utilizing action a, let E[R
expectation taken over ARP state transitions, but not over the sequence of state-actions
b h,n (s, a)] is not a constant, but a
and rewards from the original process. Note that E[R
random quantity defined on the same sample space as the original process.
Similarly, the probability of the ARP transitioning into a state in set T ‚àà B(S) is also
a random variable depending on which state-actions are chosen in the original process. For
the ARP constructed with bandwidth h and starting in state < s, n > and utilizing action
a, let Pbh,n (s, T, a) denote the random probability of the ARP transitioning into a state in
set T .
The next lemma shows that for high enough levels, the rewards and transition probabilities of the ARP are close to the rewards and transition probabilities of the original
process.
Lemma 5. If assumptions AI.-AIV. and kernel conditions KI.-KIV. are met, then
a) With probability one, for any  > 0 there exists an h = h(, T ) and N = N (, h) such
that if n > N ,


b

sup E[R
(s,
a)]
‚àí
E[R(s,
a)]
 < .
h,n
(s,a)‚ààS√óA

b) With probability one, for any  > 0 and g : S ‚Üí R that is integrable, continuous, and
bounded, there exists a h() and N (, h) such that if n > N ,
Z

Z


b

sup  g(u)Ph,n (s, du, a) ‚àí
g(u)P (s, du, a) < .
(s,a)‚ààS√óA

S

S

716

A Continuous Action Q-learning Variant

Proof. Consider the expected reward associated with a state-action (s, a) at level n of the
ARP. Condition on whether the card at level n is accepted. With probability 1 ‚àí Œ±h,n (s, a)
we do not accept the card, and the expected reward is that of level n ‚àí 1. With probability
Œ±h,n (s, a) the level n card is accepted and we receive reward rn . For the special case n = 0,
b h,0 (s, a). So the expected rewards
we are at the lowest level of the ARP and receive reward Q
satisfy the recursive relationship
b h,0 (s, a)] = Q
b h,0 (s, a),
E[R
b h,n (s, a)] = (1 ‚àí Œ±h,n (s, a))E[Rh,n‚àí1 (s, a)] + Œ±h,n (s, a)rn , for n > 0.
E[R

Now we show by induction that
Pn
j=1 Kh ((s, a) ‚àí (sj , aj ))rj
b
E[Rh,n (s, a)] = Pn
j=1 Kh ((s, a) ‚àí (sj , aj ))
when

Pn

j=1 Kh ((s, a)

‚àí (sj , aj )) 6= 0. For the n = 1 case,

b h,1 (s, a)] = (1 ‚àí Œ±h,1 (s, a))Q
b h,0 (s, a) + Œ±h,1 r1 .
E[R
Replace the Œ±h,1 (s, a) terms as per the definition in (1).
P1

b h,1 (s, a)] =
E[R

1‚àí

j=1 Kh ((s, a)
P1
j=1 Kh ((s, a)

‚àí (sj , aj ))
‚àí (sj , aj ))

!

P1

b h,0 (s, a) + Pj=1
Q
1

Kh ((s, a) ‚àí (sj , aj ))

j=1 Kh ((s, a)

‚àí (sj , aj ))

r1

= r1 .
Now assume the induction hypothesis holds for n ‚àí 1.
b h,n (s, a)] = (1 ‚àí Œ±h,n (s, a))E[R
b h,n‚àí1 (s, a)] + Œ±h,n (s, a)rn
E[R
! Pn‚àí1
!
Pn‚àí1
j=1 Kh ((s, a) ‚àí (sj , aj ))
j=1 Kh ((s, a) ‚àí (sj , aj ))rj
= Pn
Pn‚àí1
j=1 Kh ((s, a) ‚àí (sj , aj ))
j=1 Kh ((s, a) ‚àí (sj , aj ))
Kh ((s, a) ‚àí (sj , aj ))
+ Pn
rn
j=1 Kh ((s, a) ‚àí (sj , aj ))
Pn
j=1 Kh ((s, a) ‚àí (sj , aj ))rj
= Pn
.
j=1 Kh ((s, a) ‚àí (sj , aj ))
The expected rewards for the ARP are equivalent to kernel regression estimates. Assumptions AI., AII., AIII., and kernel conditions KI.-KIV. meet the technical conditions of
Hansen (2008). See Theorem 9 on page 735. This yields
sup

b h,n (s, a)] ‚àí E[R(s, a)]| < 
|E[R

(s,a)‚ààS√óA

for sufficiently large n, which proves part a).
717

Carden

For part b), recall that un denotes the state that is transitioned to at iteration n.
Consider the random variable indicator function for the event that the n-th transition from
the original process is into T ‚àà B(S).
(
1 if un ‚àà T
1T (un ) =
0 if un ‚àà
/T .
Similar to rewards, the transition probabilities for the ARP at level n can be conditioned
on whether the level n card is accepted or not. Assume we are in state-action (s, a). With
probability 1 ‚àí Œ±h,n (s, a), we do not accept the card and the probability of transitioning
into T is that of level n ‚àí 1. With probability Œ±h,n (s, a) the level n card is accepted and
the value of 1T (un ) tells us whether we transition into T or not. Thus we have the relation
Pbh,1 (s, T, a) =Œ±h,1 (s, a)1T (u1 ),
Pbh,n (s, T, a) =(1 ‚àí Œ±h,n (s, a))Pbh,n‚àí1 (s, T, a) + Œ±h,n (s, a)1T (un ).
P
Let g : S ‚Üí R be a simple function, g(u) = m
i=1 ai 1Ti (u). We can apply this relation to
the integral of g to find
Z
m
m
X
X
g(u)Pbh,1 (s, du, a) =
ai Pbh,1 (s, Ti , a) =
ai 1Ti (u1 )
i=1

i=1

= g(u1 )
and
Z
m
X
g(u)Pbh,n (s, du, a) =
ai Pbh,1 (s, Ti , a)
=

i=1
m
X

h
i
ai (1 ‚àí Œ±h,n (s, a))Pbh,n‚àí1 (s, Ti , a) + Œ±h,n‚àí1 (s, a)1Ti (un )

j=1

Z
= (1 ‚àí Œ±h,n‚àí1 (s, a))

g(u)Pbh,n‚àí1 (s, du, a) + Œ±h,n (s, a)g(un ).

Using a common argument from measure theory, this relation also holds for any integrable
function by approximating with simple functions. The rest of the proof proceeds as in part
a), with assumptions AI., AIV., and kernel conditions KI.-KIV. fulfilling the requirements
of Hansen‚Äôs Theorem 9.
Definition. For a given MDP, let Q(s1 , < a1 , a2 , ..., an , t >) denote the expected discounted reward received when the process starts in state s1 and actions a1 , a2 , ..., an are
utilized consecutively and the process then terminates. Formally
Ô£Æ
Ô£π
n
X
Q(s1 , < a1 , a2 , ..., an , t >) = E Ô£∞
Œ≥ j‚àí1 R(sj , aj )Ô£ª
j=1

718

A Continuous Action Q-learning Variant

where the chance of transitioning to states s2 , ..., sn is understood to be governed by
P (sj , ¬∑, aj ).
The following lemma shows that state-action values possess a sort of continuity. This
property will be needed to apply Lemma 7 to state-action values.
Lemma 6. If assumptions AII., AIII., and AIV. hold, then for any n, and for any fixed
but arbitrary sequence of actions < a1 , a2 , ..., an >, the function f : S ‚Üí R defined by
f (s) = Q(s, < a1 , a2 , ..., an , t >)
is Lipschitz continuous.
Proof. Consider first the case n = 1. Let s1 , s2 ‚àà S. Then by AIII.
|Q(s1 , < a1 , t >) ‚àí Q(s2 , < a1 , t >)|
=|E[R(s1 , a1 )] ‚àí E[R(s2 , a1 )]| ‚â§ Cr ||(s1 , a1 ) ‚àí (s2 , a1 )||.
Before considering the case n > 1, notice that by assumption AII.
Ô£Æ
Ô£π
n
X
|Q(s, < a1 , a2 , ..., an , t >)| ‚â§ E Ô£∞
Œ≥ j‚àí1 |R(sj , aj )|Ô£ª
j=1

‚â§

‚àû
X

Œ≥ j‚àí1 C0

j=1

=

C0
.
1‚àíŒ≥

Applying AIII. to the difference of means and AIV. to the difference of integrals, we have
for n > 1 and s1 , s2 ‚àà S
|f (s1 ) ‚àí f (s2 )| =|Q(s1 , < a1 , ..., an , t) ‚àí Q(s2 , < a1 , ..., an , t >)|
‚â§|E[R(s1 , a1 )] ‚àí E[R(s2 , a1 )]|
 Z

+ Œ≥
Q(u, < a2 , ...an , t >)P (s1 , du, a1 )

ZS

‚àíŒ≥
Q(u, < a2 , ...an , t >)P (s2 , du, a1 )
S

‚â§Cr ||((s1 , a1 ) ‚àí (s2 , a1 ))|| + Ct ||Q(u, < a2 , ...an , t >)||‚àû ||(s1 , a1 ) ‚àí (s2 , a1 )||
C0
‚â§Cr ||((s1 , a1 ) ‚àí (s2 , a1 ))|| + Ct
||(s1 , a1 ) ‚àí (s2 , a1 )||.
1‚àíŒ≥
t C0
} and inserting into the last inequality, we have
By taking C1 = 2 max{Cr , C1‚àíŒ≥

|f (s1 ) ‚àí f (s2 )| ‚â§ C1 ||(s1 , a1 ) ‚àí (s2 , a1 )||.
Finally, notice that ||(s1 , a1 )‚àí(s2 , a1 )|| = ||s1 ‚àís2 || from properties of the Euclidean metric.
We end with
|f (s1 ) ‚àí f (s2 )| ‚â§ C1 ||s1 ‚àí s2 ||,
which proves the result.
719

Carden

For the next lemma, suppose one has two MDPs defined on the same state and action
space. For (s, a) ‚àà S √ó A, let R(i) (s, a) and P (i) (s, ¬∑, a) denote the rewards and transition
probabilities of process i for i = 1, 2. The lemma will show that if the expected rewards and
transition probabilities of the processes are sufficiently close, then the expected discounted
reward of a series of actions is also close.
Lemma 7. Suppose the assumptions required for Lemma 6 hold. For any  > 0 and sequence
of actions of length n, there exists Œ¥r (n, ) and Œ¥t (n, ) such that if g : S ‚Üí R is continuous
C0
and ||g||‚àû ‚â§ 1‚àíŒ≥
and the following two conditions hold:
1.
sup
(s,a)‚ààS√óA





E[R(1) (s, a)] ‚àí E[R(2) (s, a)] < Œ¥r (n, )

2.
sup
(s,a)‚ààS√óA


Z
Z


(2)
 < Œ¥t (n, ),
 g(u)P (1) (s, du, a) ‚àí
g(u)P
(s,
du,
a)


S

S

then for any sequence of actions of length n,
|Q(1) (s1 , < a1 , a2 , ..., an , t >) ‚àí Q(2) (s1 , < a1 , a2 , ..., an , t >)| < .
Proof. We proceed by induction on n, the number of actions to be executed. For n = 1,
take Œ¥r (1, ) = .
|Q(1) (s1 , < a1 , t >) ‚àí Q(2) (s1 , < a1 , t >)| = |E[R(1) (s1 , a1 )] ‚àí E[R(2) (s1 , a1 )]| < Œ¥r = .
Now assume the induction hypothesis
holds for a sequence
of actions
	

	 of length n‚àí1. Take



Œ¥r (n, ) = min 3 , Œ¥r (n ‚àí 1, 3 ) , and Œ¥t (n, ) = min 3 , Œ¥t (n ‚àí 1, 3 ) . Then for a sequence
of length n, we can condition on state s2 and write
|Q(1) (s1 , < a1 , ..., an , t >) ‚àí Q(2) (s1 , < a1 , ..., an , t >)|
‚â§ |E[R(1) (s1 , a1 )] ‚àí E[R(2) (s1 , a1 )]|
Z

+ Œ≥  Q(1) (s2 , < a2 , ..., an , t >)P (1) (s1 , ds2 , a1 )

Z S

(2)
(2)
‚àí
Q (s2 , < a2 , ..., an , t >)P (s1 , ds2 , a1 ) .
S

720

(6)
(7)
(8)

A Continuous Action Q-learning Variant

Add and subtract

R
S

Q(1) (s2 , < a2 , ..., an , t >)P (2) (s1 , ds2 , a1 ):

|Q(1) (s1 , < a1 , ..., an , t >) ‚àí Q(2) (s1 , < a1 , ..., an , t >)|
‚â§|E[R(1) (s1 , a1 )] ‚àí E[R(2) (s1 , a1 )]|
Z

+  Q(1) (s2 , < a2 , ..., an , t >)P (1) (s1 , ds2 , a1 )

ZS

(1)
(2)
Q (s2 , < a2 , ..., an , t >)P (s1 , ds2 , a1 )
‚àí
ZS

+  Q(1) (s2 , < a2 , ..., an , t >)P (2) (s1 , ds2 , a1 )

ZS

(2)
(2)
Q (s2 , < a2 , ..., an , t >)P (s1 , ds2 , a1 )
‚àí
S

From Lemma 6 the value function is continuous, so by assumption the first difference
can be made smaller than /3 by choice of Œ¥r , the second difference made small by choice
of Œ¥t , and the third difference made small by the induction hypothesis. Each difference is
less than 3 , and the result is proved.
5.3 Proof of Theorem
We are now prepared to prove Theorem 1.
Proof. The terms with subscripts will denote the values of the ARP with bandwidth h. The
terms with no subscripts will refer to the original process. For any policy œÄ, consider the
difference
|QœÄh (< s, n >, a) ‚àí QœÄ (s, a)|
‚â§ |QœÄh (<

(9)

s, n >, a) ‚àí Qh (< s, n >, < a, œÄ(s2 ), ..., œÄ(sm ), t >)|

+ |Qh (< s, n >, < a, œÄ(s2 ), ..., œÄ(sm ), t >) ‚àí Q(s, < a, œÄ(s2 ), ..., œÄ(sm ), t >)|
œÄ

+ |Q(s, < a, œÄ(s2 ), ..., œÄ(sm ), t >) ‚àí Q (s, a)| .

(10)
(11)
(12)

By Lemma 4, m can be chosen large enough such that lines (10) and (12) are less than 3 .
By Lemma 5, there exists an h and N such that if more than N iterations are run, the
rewards and transition probabilities of the ARP become arbitrarily close to those of the
real process. Furthermore, by Lemma 3 there is a higher level N1 such that if one starts
at a level above N1 , the probability of straying below N after performing m actions is less
than (1‚àíŒ≥)
12C0 . When above level N , the requirements to apply Lemma 7 are met, and one
0
can make line (11) less than 12C02C
‚àí(1‚àíŒ≥) . If the ARP does stray below level N , then the
2C0
difference is bounded by 1‚àíŒ≥
as mentioned in Lemma 6. Then by conditioning on whether
the ARP strays below level N ,

|Qh (< s, n >, < a, œÄ(s2 ), ..., œÄ(sm ), t >) ‚àí Q(s, < a, œÄ(s2 ), ..., œÄ(sm ), t >)|

 


2C0 
12C0 ‚àí (1 ‚àí Œ≥)

2C0 (1 ‚àí Œ≥)
‚â§
+
= .
1‚àíŒ≥
12C0
12C0 ‚àí (1 ‚àí Œ≥)
12C0
3
721

Carden

It follows that
|QœÄh (< s, n >, a) ‚àí QœÄ (s, a)| < .
Because the policy œÄ was arbitrary, we can replace it with a policy optimal for the real
process, œÄ ‚àó . Then we know
 ‚àó

 œÄ

Qh (< s, n >, a) ‚àí Q‚àó (s, a) < .
Now we claim that œÄ ‚àó also gives optimal values for the ARP. If it did not, and some other
policy œÄ0 gave higher values, then consider that

 œÄ
Q 0 (< s, n >, a) ‚àí QœÄ0 (s, a)
h
‚àó

can be made arbitrarily small, but this would imply QœÄ0 (s, a) > QœÄ (s, a) which contradicts
the choice of œÄ ‚àó as an optimal policy. We now have
|Q‚àóh (< s, n >, a) ‚àí Q‚àó (s, a)| < .
b h (s, a). Thus
But by Lemma 1, Q‚àóh (< s, n >, a) = Q


b

Qh,n (s, a) ‚àí Q‚àó (s, a) < .
(s, a) ‚àà S √ó A was arbitrary, so the convergence is uniform.

6. Experimental Results
The primary intent of this paper and therefore also this algorithm is to serve as an addition
to the theory of continuous domain reinforcement learning by providing asymptotic convergence results. In this section we explore the practical application of the algorithm and show
that with minor modification it is capable of obtaining a satisfactory solution to a standard
benchmark problem in a reasonable amount of time.
In this section we detail an application to the Mountain Car problem (Moore, 1991).
The state space consists of two variables: the position p of the car, constrained to the
interval [‚àí1, 1] and the velocity v, constrained to [‚àí3, 3]. The action space consists of a
single variable, a force applied to the vehicle along a line tangent to the road with a value in
[‚àí4, 4]. The car starts at the bottom of the hill at a standstill, corresponding to an initial
state of (‚àí.5, 0). The goal is to drive the car to the top of the hill in the positive direction,
i.e., p > 1 while keeping the velocity in bounds. If the goal is reached, a reward of 1 is
experienced and the trial terminates. If the car goes out of bounds (that is, p < ‚àí1 or
|v| > 3) then a reward of ‚àí1 is received and the trial terminates. In all other states the
reward received is zero. The hill is defined by the expression
Ô£±
Ô£≤p2 + p
if p < 0
H(p) = ‚àö p
if p ‚â• 0 .
Ô£≥
2
1+5p

Figure 2 shows the shape of the hill, the initial position, and the location of the goal.
This problem is non-trivial because the maximum force in the positive direction is not large
enough to move directly to the goal. Instead, the agent must learn to build momentum by
alternating acceleration before trying to move to the goal.
722

A Continuous Action Q-learning Variant

1
0.8
0.6
0.4
0.2
0
‚àí0.2
‚àí0.4
‚àí0.6
‚àí0.8
‚àí1
‚àí1

‚àí0.8 ‚àí0.6 ‚àí0.4 ‚àí0.2

0
0.2
Position

0.4

0.6

0.8

1

Figure 2: The hill for the mountain car to climb. The circle represents the initial position,
and the star represents the goal.

6.1 Examination of Assumptions
Assumptions AI.-AIV., while sufficient to ensure convergence, are quite strong. In fact,
there are many standard problems in Reinforcement Learning which do not meet them.
For example, rewards are often of a discontinuous nature, where most states emit zero
reward and crossing a boundary suddenly yields a reward or penalty. The purpose of this
section is to show that the proposed algorithm is somewhat robust. That is, even in the
presence of violated conditions, it can obtain policies adequate for solving the problem at
hand. Specifically, we will see how the Mountain Car problem violates two out of four
assumptions, yet the algorithm still produces a policy which can reach the goal state.
The first assumption is the continuous analogue of classic Q-learning‚Äôs requirement that
every state-action pair be visited an infinite number of times. We require the distribution
of (sn , an ) to possess a density which is positive and sufficiently smooth. However, given
the dynamics of the Mountain Car, there are areas of the state space which are inaccessible
with the available actions. Consider the state consisting of position p = .9 and velocity
v = ‚àí2.9. The position is at the top of the hill near the goal. The maximum acceleration in
the negative direction is not strong enough to produce a negative velocity as large as ‚àí2.9
in the space given.
Figure 3 suggests that the density over the state space is adequately smooth, but it
is clear that there are states which have not been visited. However, if the state space is
restricted to the set of accessible states, it seems reasonable that the density is everywhere
positive.
The second assumption requires bounded rewards. The Mountain Car problem clearly
satisfies this.
723

Carden

Figure 3: A histogram showing frequency of visits across the state space.

The third assumption requires rewards to be Lipschitz continuous. It is common in
Reinforcement Learning for rewards to be zero for most of the state-action space, with a
goal state emitting positive rewards and penalties issued for leaving boundaries or entering
undesirable states. This is exactly the case for the Mountain Car. One could circumvent
this problem by replacing impulse rewards with a low-variance Gaussian density function
centered at the goal state, or use a similar method for smoothing rewards without disrupting
the overall reward structure.
The fourth assumption requires weak convergence of transition probability measures as
states and actions become more similar. Furthermore, that convergence must be uniform
across the state-action space. For a problem with smooth, deterministic transitions such
as Mountain Car, it is easy to verify convergence in distribution for un (the state being
transitioned to) as state-actions (sn , an ) converge, which is equivalent to weak convergence
(see Jacod & Protter, 2003, ch. 18).
It should be noted that the strength of the assumptions are directly linked to the desire
for the value function estimate to converge uniformly. Kernel methods can achieve weaker
forms of convergence with much weaker assumptions. For example, Devroye and GyoÃàrfi
(1985) show that with proper bandwith selection, kernel density estimates can converge
in L1 with no conditions on the density being estimated. This suggests that kernel-based
algorithms may be able to return good policies despite discontinuities in the reward function,
as is the case for the Mountain Car problem. However, this has not been thoroughly
investigated for other problems yet.
6.2 Computational Considerations
An advantage of non-parametric approximators is that they have the potential to represent
any function with arbitrary precision, but at a cost of increased computational load as
724

A Continuous Action Q-learning Variant

more observations are used. The following paragraphs will discuss a few suggestions for
alleviating this problem.
One idea is to be selective about which observations are kept to use for future calculations
while discarding the rest. For example, consider that before a boundary is reached for the
first time, all rewards received are equal to zero, which is the initialized function value.
Recording these observations will slow down future value function computations but does
not add to the knowledge of the system. Suppose that when in state-action (s, a), reward
rn is received and the next state is un . One may choose to discard this experience if
b h,n‚àí1 (un , b) ‚àí Q
b h,n‚àí1 (s, a)| < Œ¥
|rn + Œ≥ sup Q
b‚ààA

for some tolerance Œ¥. Only keeping experiences with the potential to improve the value
function estimate ensures the algorithm learns efficiently.
Another idea is to, at some point, begin discarding old observations as new ones are
b h,n‚àí1 (un , b)
recorded to keep a fixed number in memory. Consider that for terms rn +Œ≥ supb‚ààA Q
b h,n‚àí1 (un , b) is not yet calculated with enough data to give a
calculated early, the value Q
good estimate. Dropping old values in favor of new ones serves a double purpose of capping the increasing computational load and speeding convergence of value estimates. Care
should be taken so that no region of the state-action space is left without observations for
performing kernel regression.
A third option, and the approach taken in this example, is to ‚Äúseed‚Äù the algorithm
with a fixed number of randomly-generated episodes which have reached the goal before
beginning the standard -greedy exploration strategy. Exploration episodes are generated
using random action selection. Episodes which reach a penalty state or fail to find the goal
within 500 iterations are discarded, while successful episodes are passed to the algorithm.
After 20 such episodes, the algorithm chooses random actions with probability  and optimal
(based on current knowledge) actions otherwise. At this point all iterations are passed to
the learning algorithm.
As mentioned in the introduction, taking a supremum over all possible actions for a
given state is, in general, a difficult problem. For the Mountain Car problem, which has
only one action variable, a suitable choice for a kernel function allows for exact solutions.
The Epanechnikov kernel is defined by
(
3
(1 ‚àí x2 ) if |x| < 1
e
K (x) = 4
(13)
0
otherwise
and is a common choice for kernel regression as it minimizes an approximation of mean
integrated square error (see Silverman, 1986, section 3.3.2 for more details). For the three
dimensional Mountain Car problem, we define the multivariate kernel as the product of
Epanechnikov kernels for each variable. That is, for state-action pairs represented by
(pk , vk , ak ), we define
K((p1 , v1 , a1 ) ‚àí (p2 , v2 , a2 )) = K e (p1 ‚àí p2 )K e (v1 ‚àí v2 )K e (a1 ‚àí a2 )
An advantage of formulating the kernel in this way is that the numerator of the derivative
with respect to the action variable is quadratic in the action variable. Because the Epanechnikov kernel is non-zero on a finite interval, the domain of the action variable can be broken
725

Carden

into a finite number of intervals on which the quadratic coefficients are constant. Thus all
points which are candidate maxima are either a root of one of a finite number of quadratics
or breakpoints between intervals. Using this method, the optimal action can be calculated
relatively efficiently. If the routine finds multiple actions with the same optimal value, ties
are broken randomly. Currently, this strategy works only on problems with one action
variable, though the possibility of extending into higher dimensions is being investigated.
6.3 Results

3

2

Velocity

1

0

‚àí1

‚àí2

‚àí3

‚àí1

‚àí0.8 ‚àí0.6 ‚àí0.4 ‚àí0.2

0
0.2
Position

0.4

0.6

0.8

1

Figure 4: The trajectory through the state space as obtained by the final policy.

4

3

2

Action

1

0

‚àí1

‚àí2

‚àí3

‚àí4
0

2

4

6

8

10

12

14

Iteration

Figure 5: Actions chosen from the final policy.
726

A Continuous Action Q-learning Variant

Implementation was in MATLAB 2012b in Ubuntu 12.04 on hardware with an Intel
Xeon 3.47 gigahertz processor and 24 gigabytes of RAM. 7,500 iterations were recorded
in 858 seconds. Parameter values were bandwidth h = .2, exploration parameter  = .9,
discount factor Œ≥ = .9, and k = 20 successful episodes to initialize with 1 .
The policy obtained is near-optimal. Figure 4 shows the trajectory of the car through
the state space. The car begins by accelerating in the negative direction, travels partway
up the hill to the left, then changes to positive acceleration until it reaches the goal. Figure
5 shows action selection over time. The optimal policy will choose actions on the extreme
ends of the action interval, and the learned policy sometimes chooses actions near but not
at the boundary. Figure 6 shows iterations against time. There is an initially surprising
phenomena here. One would expect the number of iterations per second to increase as more
data is collected, but after around 400 iterations the calculations actually speed up. The
reason is that when only a small amount of data is available, the action-selection subroutine
will find many actions with the same maximum value. Each candidate best action is kept
in memory until the tie is randomly broken at the end of the subroutine. As more data
is collected, ties occur less commonly, and the subroutine finishes in less time. For a brief
time, the increase in speed from faster action-selection outpaces the decrease in speed from
accumulating data. Over time, this phenomena vanishes, and the algorithm slows down
once again. Figure 7 shows iterations against time for 25,000 iterations. From this plot it
is clear that the described phenomena is temporary.
Another unexpected occurence involved the optimal bandwidth. Repeating the experiment multiple times with different bandwidths revealed that the best results were obtained
when the algorithm learned with a bandwidth of .2 but used a smaller bandwidth, .07, when
building the final policy. Early trials resulted in a policy that could find the goal but spent
more time than necessary building momentum. Upon investigation it was found that the
value of state-actions corresponding to the initial state were calculated using values in the
initial position but with positive velocity. This caused the policy to use positive acceleration
(the best action when velocity is already positive) rather than negative acceleration (the
best action when velocity is zero) from the initial state. A smaller bandwidth, specifically
.07, avoided this issue. Note that using .07 when the algorithm was in the learning phase
did not yield good results because it did not sufficiently generalize when a small amount
of data was available. This suggests that even though the convergence proof uses a fixed
bandwidth, in practice it is best to decrease the bandwidth as the amount of data increases.

7. Conclusion
This paper has presented a reinforcement learning algorithm for continuous states and actions which is proven to converge to an optimal solution. Knowledge generalization is based
on Nadaraya-Watson kernel regression. The most similar previous result is that of Ormoneit
and Sen (1999), which uses kernel-smoothing for problems with a continuous state space
and a finite number of actions. The algorithm presented here is different in a few important
ways. First, actions are allowed to be continuous. However, this introduces the burden of
ensuring small changes in actions result in small changes to transition dynamics. Second,
the assumption on the distribution of states is weaker. Ormoneit and Sen‚Äôs Assumption
1. For the full source code, see the online appendix associated with this publication.

727

Carden

8000
7000
6000

Iterations

5000
4000
3000
2000
1000
0

0

100

200

300

400
500
600
Time (seconds)

700

800

900

1000

Figure 6: Graph of 7,500 iterations completed over time.
4

2.5

x 10

2

Iterations

1.5

1

0.5

0

0

1000

2000

3000
Time (seconds)

4000

5000

6000

Figure 7: Graph of 25,000 iterations completed over time.
3 requires states to be sampled uniformly from the state space. This has been weakened
in the present paper to requiring the distribution of states to have a positive and smooth
density.
The presented algorithm is sequential in that it updates after each single observation.
The primary reason for considering a sequential rather than batch-mode algorithm is so
Watkins‚Äô proof strategy can be applied as directly as possible. However, for applications, a
batch-mode version possesses many advantages. Informal experiments with the Mountain
Car and Inverted Pendulum problems with a batch-mode implementation have produced
better results in typically a quarter of the amount of time the sequential algorithm requires.
728

A Continuous Action Q-learning Variant

One of the practical difficulties in implementing the algorithm is that the amount of
computation required increases with each recorded observation. In addition to the ideas
in Section 6, the author has experimented with sparsifying by selecting a grid of M points
{(sj , aj )|j = 1, ..., M } in the state-action space and assigning to each point a value such
that performing kernel regression on those values will yield predictions similar to predictions
obtained using kernel regression on the entire set of observations. Essentially, the idea is to
use the kernel as a radial basis function at fixed locations in the state-action space and learn
the weights for each. Specifically, suppose N transitions have been observed, N ‚â• M , with
b h,i‚àí1 (ui , a)|i = 1, ..., N } recorded. Define a
{(si , ai )|i = 1, ...N } and {yh,i := ri + Œ≥ supa‚ààA Q
matrix XN √óM entry-wise by
Kh ((si , ai ) ‚àí (sj , aj ))
Xi,j = PM
j=1 Kh ((si , ai ) ‚àí (sj , aj )
~N √ó1 by Yi = yh,i . Solve for Œ≤~ that minimizes ||X Œ≤~ ‚àí Y
~ ||2 . For a state-action (s, a), deand Y
~ M √ó1 (s, a) by Kj (s, a) = K((s, a) ‚àí (sj , aj )). The value of (s, a) can then be estimated
fine K
~ For future observations, update Œ≤~ as described by Chambers (1971). This
~ T (s, a)Œ≤.
by K
method requires computation that is linear in the number of observations and constant in
storage requirements.
The fourth assumption concerns weak convergence of state transition probability measures. If the state-action space is separable, it is possible to define a metric on the set of all
transition measures such that convergence in the metric is equivalent to weak convergence.
See, for example, the Prokhorov metric (Billingsley, 1999). Therefore it is possible to restate
assumption four in a more succint manner which is sometimes easier to verify. However,
extra work is then required to derive the properties needed in the convergence proof. The
author has chosen to state assumption four in the manner that simplifies the proof.

Acknowledgements
The author would like to thank Peter Kiessler for technical advice, three anonymous referees
for many helpful comments, and Tanya Carden and Kris Kelly for proofreading.

References
Albus, J. S. (1975). A new approach to manipulator control: the cerebellar model articulation controller (CMAC). Journal of Dynamic Systems, Measurement, and Control,
97, 220‚Äì227.
Baird, L. C., & Klopf, A. H. (1993). Reinforcement learning with high-dimensional, continuous actions. Tech. rep. WL‚ÄìTR-93-1147, Wright-Patterson Air Force Base Ohio:
Wright Laboratory.
Bertsekas, D. P. (1995). A counterexample to temporal differences learning. Neural Computation, 7 (2), 270‚Äì279.
Billingsley, P. (1999). Convergence of probability measures (Second edition). Wiley Series
in Probability and Statistics: Probability and Statistics. John Wiley & Sons Inc., New
York. A Wiley-Interscience Publication.
729

Carden

Devroye, L., & GyoÃàrfi, L. (1985). Nonparametric density estimation: the L1 view. Wiley
series in probability and mathematical statistics. Wiley.
Ernst, D., Geurts, P., & Wehenkel, L. (2005). Tree-based batch mode reinforcement learning.
Journal of Machine Learning Research, 6, 503‚Äì556.
Fairbank, M., & Alonso, E. (2012). The divergence of reinforcement learning algorithms with
value-iteration and function approximation. In Proceedings of the IEEE International
Joint Conference on Neural Networks.
Gaskett, C., Wettergreen, D., & Zelinsky, A. (1999). Q-learning in continuous state and
action spaces. In Australian Joint Conference on Artificial Intelligence, pp. 417‚Äì428.
Springer-Verlag.
Gordon, G. J. (1996). Chattering in SARSA(lambda) - a CMU learning lab internal report.
Tech. rep., Carnegie Mellon University.
Hansen, B. E. (2008). Uniform convergence rates for kernel estimation with dependent data.
Econometric Theory.
HaÃàrdle, W. (2004). Nonparametric and Semiparametric Models. Springer Series in Statistics.
Springer Berlin Heidelberg.
Jaakkola, T., Jordan, M. I., & Singh, S. P. (1994). Convergence of stochastic iterative
dynamic programming algorithms. Neural Computation, 6, 1185‚Äì1201.
Jacod, J., & Protter, P. (2003). Probability Essentials. Universitext (1979). Springer.
Lazaric, A., Restelli, M., & Bonarini, A. (2007). Reinforcement learning in continuous action
spaces through sequential Monte Carlo methods. In Adv. Neural Information Proc.
Systems.
MillaÃÅn, J. R., Posenato, D., & Dedieu, E. (2002). Continuous-action Q-learning. Machine
Learning.
Moore, A. (1991). Efficient Memory-based Learning for Robot Control.
Robotics Institute, Carnegie Mellon University.

Ph.D. thesis,

Nadaraya, E. (1964). On estimating regression. Theory of Probability and its Applications,
9, 141‚Äì142.
Ormoneit, D., & Sen, S. (1999). Kernel-based reinforcement learning. Machine Learning.
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley-Interscience.
Ross, S. (1992). Applied Probability Models with Optimization Applications. Dover Books
on Mathematics. Dover Publications.
Rummery, G. A., & Niranjan, M. (1994). On-line Q-learning using connectionist systems.
Tech. rep., Cambridge University Engineering Department.
Santamarƒ±ÃÅa, J. C., Sutton, R. S., & Ram, A. (1996). Experiments with reinforcement
learning in problems with continuous state and action spaces. Adaptive Behavor.
Silverman, B. W. (1986). Density estimation for statistics and data analysis. Chapman and
Hall, London.
730

A Continuous Action Q-learning Variant

Simonoff, J. (1996). Smoothing methods in statistics. Springer, New York.
Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine
Learning, 3, 9‚Äì44.
Sutton, R. S. (1996). Generalization in reinforcement learning: Successful examples using
sparse coarse coding. In Advances in Neural Information Processing Systems 8, pp.
1038‚Äì1044. MIT Press.
SzepesvaÃÅri, C., & Smart, W. D. (2004). Interpolation-based Q-learning. In Proceedings of
the International Conference on Machine Learning, pp. 791‚Äì798. ACM Press.
Taylor, G., & Parr, R. (2009). Kernelized Value Function Approximation for Reinforcement
Learning. In Proceedings of the 26th International Conference on Machine Learning,
pp. 1017‚Äì1024.
ten Hagen, S. H. (2001). Continuous State Space Q-Learning for Control of Nonlinear
Systems. Ph.D. thesis, University of Amsterdam.
Tesauro, G. (1995). Temporal difference learning and TD-Gammon. Commun. ACM, 38 (3),
58‚Äì68.
Tsitsiklis, J. N. (1994). Asynchronous stochastic approximation and Q-learning. In Machine
Learning, pp. 185‚Äì202.
Tsitsiklis, J. N., & Van Roy, B. (1996). Feature-based methods for large scale dynamic
programming. Machine Learning, 22 (1‚Äì3), 59‚Äì94.
Watkins, C. (1989). Learning from Delayed Rewards. Ph.D. thesis, University of Cambridge.
Watkins, C. J. C. H., & Dayan, P. (1992). Technical note: Q-learning. Machine Learning,
8, 279‚Äì292.
Watson, G. S. (1964). Smooth regression analysis. Sankhya: The Indian Journal of Statistics, 26, 359‚Äì372.
Xu, X., Hu, D., & Lu, X. (2007). Kernel-based least squares policy iteration for reinforcement learning. IEEE Transactions on Neural Networks, 18 (4), 973‚Äì992.

731

Journal of Artificial Intelligence Research 49 (2014) 635-668

Submitted 01/14; published 04/14

Algorithms for Argumentation Semantics: Labeling Attacks as a
Generalization of Labeling Arguments
Samer Nofal

S AMER .N OFAL @ GJU . EDU . JO

Dept. of Computer Science, German-Jordanian University
P.O. Box 35247, Amman 11180, Jordan

Katie Atkinson
Paul E. Dunne

K.M.ATKINSON @ LIVERPOOL . AC . UK
P.E.D UNNE @ LIVERPOOL . AC . UK

Dept. of Computer Science, University of Liverpool
Ashton Street, Liverpool L69 3BX, United Kingdom

Abstract
A Dung argumentation framework (AF) is a pair (A, R): A is a set of abstract arguments and
R ‚äÜ A √ó A is a binary relation, so-called the attack relation, for capturing the conflicting arguments. ‚ÄúLabeling‚Äù based algorithms for enumerating extensions (i.e. sets of acceptable arguments)
have been set out such that arguments (i.e. elements of A) are the only subject for labeling. In
this paper we present implemented algorithms for listing extensions by labeling attacks (i.e. elements of R) along with arguments. Specifically, these algorithms are concerned with enumerating
all extensions of an AF under a number of argumentation semantics: preferred, stable, complete,
semi stable, stage, ideal and grounded. Our algorithms have impact, in particular, on enumerating
extensions of AF-extended models that allow attacks on attacks. To demonstrate this impact, we
instantiate our algorithms for an example of such models: namely argumentation frameworks with
recursive attacks (AFRA), thereby we end up with unified algorithms that enumerate extensions of
any AF / AFRA.

1. Introduction
Computational argumentation, covering its theory and applications, has attracted major attention in
the AI research community, notably in the last twenty years (e.g. Bench-Capon & Dunne, 2007;
Besnard & Hunter, 2008; Rahwan & Simari, 2009; Modgil, Toni, Bex, Bratko, ChesnÃÉevar, DvorÃåaÃÅk,
Falappa, Fan, Gaggl, Garcƒ±ÃÅa, GonzaÃÅlez, Gordon, Leite, MozÃåina, Reed, Simari, Szeider, Torroni, &
Woltran, 2013). Dung‚Äôs abstract argumentation frameworks (AFs) (Dung, 1995) are a widely studied
model in which an AF is described by a pair (A, R): A is a set of abstract arguments and R ‚äÜ A√óA is a
binary relation, so-called the attack relation, to represent the conflicting arguments. A central notion
in AFs is an argumentation semantics: a set of criteria that characterise the acceptable arguments;
we define these criteria rigorously in section 2. For different reasons a number of argumentation
semantics have been proposed in the literature. Explaining these reasons in detail is out of the scope
of this paper; however, see the work of Baroni, Caminada, and Giacomin (2011a) for an excellent
introduction to argumentation semantics.
Under various argumentation semantics, one might find multiple distinct extensions (defined in
section 2). Labeling based algorithms (e.g. Dimopoulos, Magirou, & Papadimitriou, 1997; Doutre
& Mengin, 2001; Modgil & Caminada, 2009) for listing all extensions have been developed such
that arguments (i.e. elements of A) are the only target to be labeled. In this paper we illustrate how to
enumerate extensions under several argumentation semantics by labeling attacks (i.e. elements of R)
c
‚Éù2014
AI Access Foundation. All rights reserved.

N OFAL , ATKINSON , & D UNNE

along with arguments, instead of labeling arguments solely. This is particularly of interest in listing
extensions of AF-extended formalisms that allow attacks on attacks (e.g. Modgil, 2009b; Gabbay,
2009; Baroni, Cerutti, Giacomin, & Guida, 2011b). As we show throughout the paper, the term
‚Äúlabeling based algorithms‚Äù for argumentation semantics is distinguished from the common term
‚Äúlabeling based semantics‚Äù, although both concepts involve a labeling mapping. The former term
(i.e. ‚Äúlabeling based algorithms‚Äù) refers to the course of actions by which an extension enumeration
process classifies arguments: those which might be in an extension from those which are excluded
from the respective extension. This classification is essential in order to construct all concrete
extensions of a given AF. The later term (i.e. ‚Äúlabeling based semantics‚Äù) refers to an approach to
describing (i.e. not constructing) extensions using a labeling mapping.
In section 2 we provide necessary background materials. In section 3 we review explicit algorithms for a selection of dominant argumentation semantics: preferred, stable, complete, semi
stable, stage, ideal and grounded. These algorithms list extensions by labeling arguments only.
Then in section 4 we develop, under the respective argumentation semantics, definite algorithms
for enumerating extensions of an argumentation framework with recursive attacks (AFRA): an AFextended model that allows attacks on attacks (Baroni et al., 2011b). These algorithms construct
extensions by labeling attacks together with arguments. Since an AF is a special case of AFRA (Baroni et al., 2011b), the developed algorithms for AFRA also list extensions of an AF. In section 5 we
report on experiments concerning the practical efficiency of the algorithms. Section 6 concludes the
paper with a summary and a review of related work.

2. Preliminaries
We start with the definition of Dung‚Äôs argumentation frameworks (Dung, 1995).
Definition 1. (Dung‚Äôs Argumentation Frameworks)
An argumentation framework (or AF) is a pair (A, R) where A is a set of arguments and R ‚äÜ A √ó A
is a binary relation.
We refer to (x, y) ‚àà R as x attacks y (or y is attacked by x). We denote by {x}‚àí respectively
the subset of A containing those arguments that attack (resp. are attacked by) the argument x,
extending this notation in the natural way to sets of arguments, so that for S ‚äÜ A,
{x}+

S‚àí
S+

=
=

{ y ‚àà A : ‚àÉ x ‚àà S s.t. y ‚àà {x}‚àí }
{ y ‚àà A : ‚àÉ x ‚àà S s.t. y ‚àà {x}+ }

Given a subset S ‚äÜ A, then
‚Ä¢ x ‚àà A is acceptable w.r.t. S if and only if for every (y, x) ‚àà R, there is some z ‚àà S for which
(z, y) ‚àà R.
‚Ä¢ S is conflict free if and only if for each (x, y) ‚àà S √ó S, (x, y) ‚àà
/ R.
‚Ä¢ S is admissible if and only if it is conflict free and every x ‚àà S is acceptable w.r.t. S.
‚Ä¢ S is a preferred extension if and only if it is a maximal (w.r.t. ‚äÜ) admissible set.
‚Ä¢ S is a stable extension if and only if it is conflict free and S+ = A \ S.
636

A LGORITHMS FOR A RGUMENTATION S EMANTICS

Figure 1: An argumentation framework.
‚Ä¢ S is a complete extension if and only if it is an admissible set such that for each x acceptable
w.r.t. S, x ‚àà S.
‚Ä¢ S is a stage extension if and only if it is conflict free and S ‚à™ S+ is maximal (w.r.t. ‚äÜ).
‚Ä¢ S is a semi stable extension if and only if it is admissible and S ‚à™ S+ is maximal (w.r.t. ‚äÜ).
‚Ä¢ S is the ideal extension if and only if it is the maximal (w.r.t. ‚äÜ) admissible set that is contained in every preferred extension.
‚Ä¢ S is the grounded extension if and only if it is the least fixed point of F(T ) = {x ‚àà A |
x is acceptable w.r.t. T }.
Preferred, complete, stable and grounded semantics are introduced in the work of Dung (1995),
whereas stage semantics, ideal semantics and semi stable semantics are presented in the papers
of Verheij (1996), Dung, Mancarella, and Toni (2007) and Caminada, Carnielli, and Dunne (2012)
respectively. To give an example, consider the framework depicted in figure 1 where nodes represent
arguments and edges correspond to attacks (i.e. elements of R). For this example {b, d} is the
preferred, grounded, stable, ideal, complete, semi stable and stage extension. Note that we do not
intend by this example to show differences between semantics.
Offering an explicit means to weaken attacks, the formalisms of Modgil (2009b), Gabbay
(2009) and Baroni et al. (2011b) extend AFs such that attacks (i.e. elements of R) are subject
to attacks themselves. We present extension enumeration algorithms for an instance of such formalisms: namely argumentation frameworks with recursive attacks (AFRA) introduced by Baroni et
al. (2011b).
Definition 2. An argumentation framework with recursive attacks (AFRA) is a pair (A, R) where A
is a set of arguments and R is a set of pairs (x, y) such that x ‚àà A and (y ‚àà A or y ‚àà R).
Let x = (y, z) ‚àà R then we say that y is the source of x, denoted as src(x) = y, and z is the target
of x, denoted as trg(x) = z.
Let x ‚àà A ‚à™ R and y ‚àà R then we say that y directly de f eats x if and only if x = trg(y).
Let x, y ‚àà R then we say y indirectly de f eats x if and only if src(x) = trg(y).
Let x ‚àà A ‚à™ R and y ‚àà R, we say y de f eats x if and only if y directly or indirectly defeats x.
Given a subset S ‚äÜ A ‚à™ R, then
‚Ä¢ S is conflict free if and only if there does not exist x, y ‚àà S s.t. x defeats y.
‚Ä¢ An element x ‚àà A ‚à™ R is acceptable w.r.t. S if and only if for each y ‚àà R : y defeats x, there is
some z ‚àà S such that z defeats y.
‚Ä¢ S is admissible if and only if S is conflict free and for each x ‚àà S, x is acceptable w.r.t. S.
637

N OFAL , ATKINSON , & D UNNE

Figure 2: An argumentation framework with recursive attacks.
‚Ä¢ S is a preferred extension if it is a maximal (w.r.t. ‚äÜ) admissible set.
/ S, there
‚Ä¢ S is a stable extension if and only if it is conflict free and for each x ‚àà A ‚à™ R : x ‚àà
exists y ‚àà S such that y de f eats x.
‚Ä¢ S is a complete extension if and only if it is admissible and every element of A ‚à™ R, which is
acceptable w.r.t. S, belongs to S.
‚Ä¢ S is a stage (resp. semi stable) extension if and only if S is conflict free (resp. admissible) and
S ‚à™ {x | ‚àÉy ‚àà S s.t. y defeats x} is maximal (w.r.t. ‚äÜ).
‚Ä¢ S is the ideal extension if and only if it is the maximal (w.r.t. ‚äÜ) admissible set that is contained in every preferred extension.
‚Ä¢ S is the grounded extension if and only if it is the least fixed point of F(T ) = {x ‚àà A ‚à™ R |
x is acceptable w.r.t. T }.
Referring to figure 2, {b, d, h, e} is the grounded, stable, preferred, ideal, complete, stage and
semi stable extension.
We consider now the issue of expressing an AFRA as an AF. Let H = (A, R) be an AFRA,
then the corresponding AF H ‚Ä≤ = (A‚Ä≤ , R‚Ä≤ ) is defined such that A‚Ä≤ = A ‚à™ R and R‚Ä≤ = {(x, y) | x, y ‚àà
A ‚à™ R and x de f eats y}. For example, the corresponding AF of the AFRA depicted in figure 2 is
described by A‚Ä≤ = {b, c, d, e, f , g, h} and R‚Ä≤ = {(e, g), ( f , e), (g, e), (g, d), (h, c), (h, f ), (h, g)}.

3. Algorithms for a Selection of Argumentation Semantics
In this section we review explicit algorithms that list, under a number of argumentation semantics,
all extensions of an AF by labeling arguments solely. Particularly, in subsection 3.1 we recall the
algorithm of Nofal, Atkinson and Dunne (2014) for preferred semantics. In section 3.2 we present
a new implementation of the algorithm of Dimopoulos et al. (1997) for stable semantics. Then
we modify the algorithm of Nofal, Atkinson and Dunne (2014) to produce specific algorithms for
complete, stage, semi stable and ideal semantics in subsections 3.3, 3.4, 3.5 and 3.6 respectively.
In subsection 3.7 we present an implementation for building the grounded extension.
3.1 Enumerating Preferred Extensions of any AF
Algorithm 1 lists all preferred extensions of an AF. Algorithm 1 is taken from the work of Nofal,
Atkinson and Dunne (2014) where it has been shown that the algorithm is likely to be more efficient
than the algorithms of Doutre and Mengin (2001), and Modgil and Caminada (2009). We recall
638

A LGORITHMS FOR A RGUMENTATION S EMANTICS

algorithm 1 because other implemented algorithms of the present paper can be seen as an extension
of this algorithm. The algorithm is a backtracking procedure that traverses an abstract binary search
tree. A core notion of the algorithm is related to the use of five labels: IN, OUT, MUST OUT,
BLANK and UNDEC. Informally, the IN label identifies arguments that might be in a preferred
extension. The OUT label identifies an argument that is attacked by an IN argument. The BLANK
label is for any unprocessed argument whose final label is not decided yet. The MUST OUT label
identifies arguments that attack IN arguments. The UNDEC label designates arguments which might
not be included in a preferred extension because they might not be defended by any IN argument.
To enumerate all preferred extensions algorithm 1 starts with BLANK as the default label for all
arguments. This initial state represents the root node of the search tree. Then the algorithm forks to
a left (resp. right) child (i.e. state) by picking an argument, that is BLANK, to be labeled IN (resp.
UNDEC). Every time an argument, say x, is labeled IN some of the neighbour arguments‚Äô labels
might change such that for every y ‚àà {x}+ the label of y becomes OUT and for every z ‚àà {x}‚àí \{x}+
the label of z becomes MUST OUT. This process, i.e. forking to new children, continues until for
every x ‚àà A the label of x is not BLANK. At this point, the algorithm captures a preferred extension
if and only if for every x ‚àà A the label of x belongs to {IN,OUT,UNDEC} and {x | the label of x is
IN} is not a subset of a previously found preferred extension (if such exists). Then the algorithm
backtracks to try to find all preferred extensions. It is important in these kinds of algorithms to
exploit properties whereby we might bypass expanding a child of the search tree, thus considerable
time might be saved. Algorithm 1 uses two pruning properties:
1. Algorithm 1 (lines 12-17) skips labeling an argument y IN (i.e. skips expanding a left child)
if and only if there is z ‚àà {y}‚àí such that the label of z is not OUT while there is no w ‚àà {z}‚àí
with the BLANK label. In other words, such z can not be labeled OUT later while for each
w ‚àà {z}‚àí the label of w is OUT, MUST OUT or UNDEC. Thus, it is more efficient to skip
trying to include any argument that is attacked by such z in a preferred extension.
2. Algorithm 1 (lines 19-22) skips labeling an argument y UNDEC (i.e. skips expanding a right
child) if and only if for every z ‚àà {y}‚àí the current label of z is OUT or MUST OUT. This
is because if an admissible set, say S, is constructed while such y is UNDEC then S ‚à™ {y} is
admissible also. Recall that preferred extensions are the maximal admissible sets, hence no
need to label such y UNDEC.
Another fundamental issue to take into account is the selection of BLANK arguments that are
to be labeled IN. The point behind adopting a selection strategy is to try to achieve a preferred extension more efficiently. This is critical when the problem is about constructing only one extension.
Therefore, algorithm 1 (line 8) applies the following selection options:
1. Algorithm 1 tries to select first a BLANK argument, say y, that is not attacked at all or is
attacked by OUT/MUST OUT arguments only. The justification of this selection is related to
the second pruning property used by the algorithm. Note that the earlier we pick such y to be
labeled IN, the bigger part of the search tree to be avoided. Recall that such y will not lead to
expanding a right child according to the second pruning property.
2. Otherwise the algorithm picks up a BLANK argument, say y, such that |{z : z ‚àà {y}+ and
the label of z is not OUT}| is maximal. The intuition is that maximising the number of
OUT arguments will minimise the number of BLANK/MUST OUT arguments. Thus, the
639

N OFAL , ATKINSON , & D UNNE

Figure 3: Enumerating preferred extensions of an AF using algorithm 1.
new generated state (i.e. child), due to selecting such y, is much closer to the state where
a preferred extension is captured. Recall that a preferred extension is achieved if and only if
for each x ‚àà A the label of x is IN, OUT or UNDEC.
Algorithm 1, like all algorithms in this paper, is self-contained and self-explanatory. Figure 3,
however, illustrates algorithm 1 running on an AF.
3.2 Enumerating Stable Extensions of any AF
Algorithm 2 lists all stable extensions. Algorithm 2 can be seen as a new implementation of the
algorithm of Dimopoulos et al. (1997). Algorithm 2 differs from algorithm 1 in two ways:
640

A LGORITHMS FOR A RGUMENTATION S EMANTICS

Algorithm 1: Enumerating all preferred extensions of an AF (A, R).
1
2
3
4
5

6
7
8

9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26

Lab : A ‚Üí {IN, OUT, MUST OUT,UNDEC, BLANK}; Lab ‚Üê 0;
/
foreach x ‚àà A do Lab ‚Üê Lab ‚à™ {(x, BLANK)};
E pre f erred ‚äÜ 2A ; E pre f erred ‚Üê 0;
/
call find-preferred-extensions(Lab);
report E pre f erred is the set of all preferred extensions;
procedure find-preferred-extensions(Lab) begin
while ‚àÉy ‚àà A : Lab(y) = BLANK do
select y with Lab(y) = BLANK and ‚àÄz ‚àà {y}‚àí Lab(z) ‚àà {OUT, MUST OUT },
otherwise select y with Lab(y) = BLANK s.t. ‚àÄz ‚àà A : Lab(z) = BLANK, |{x : x ‚àà
{y}+ ‚àß Lab(x) Ã∏= OUT }| ‚â• |{x : x ‚àà {z}+ ‚àß Lab(x) Ã∏= OUT }|;
Lab‚Ä≤ ‚Üê Lab;
Lab‚Ä≤ (y) ‚Üê IN;
foreach z ‚àà {y}+ do Lab‚Ä≤ (z) ‚Üê OUT ;
foreach z ‚àà {y}‚àí do
if Lab‚Ä≤ (z) ‚àà {UNDEC, BLANK} then
Lab‚Ä≤ (z) ‚Üê MUST OUT ;
if Ã∏ ‚àÉw ‚àà {z}‚àí : Lab‚Ä≤ (w) = BLANK then
Lab(y) ‚Üê UNDEC;
goto line 7;
call find-preferred-extensions(Lab‚Ä≤ );
if ‚àÉz ‚àà {y}‚àí : Lab(z) ‚àà {BLANK,UNDEC} then
Lab(y) ‚Üê UNDEC;
else
Lab ‚Üê Lab‚Ä≤ ;
if Ã∏ ‚àÉx : Lab(x) = MUST OUT then
S ‚Üê {x | Lab(x) = IN};
if Ã∏ ‚àÉT ‚àà E pre f erred : S ‚äÜ T then E pre f erred ‚Üê E pre f erred ‚à™ {S};
end procedure

641

N OFAL , ATKINSON , & D UNNE

1. Algorithm 2 uses four labels: IN, OUT, BLANK and MUST OUT. The usage of these labels
is as outlined in algorithm 1 with one distinction: the role of the UNDEC label used in algorithm 1 is now overloaded to the MUST OUT label. Meaning, in algorithm 2 the MUST OUT
label is used also for labeling an argument, say x, trying to build a stable extension without x.
This is because any argument, say x, outside a candidate stable extension should be attacked
by an argument in the extension, hence x should be labeled MUST OUT (not UNDEC as it is
the case in algorithm 1.)
2. In algorithm 1, P = {w | the label of w is IN} is a preferred extension if and only if for each
x ‚àà A, the label of x is not BLANK nor MUST OUT and P is not a subset of a previously
found preferred extension. In algorithm 2 (line 24) the set {w | the label of w is IN} is a stable
extension if and only if for every x ‚àà A, the label of x is not BLANK nor MUST OUT.
Algorithm 2: Enumerating all stable extensions of an AF (A, R).
1
2
3
4
5

6
7
8

9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26

Lab : A ‚Üí {IN, OUT, MUST OUT, BLANK}; Lab ‚Üê 0;
/
foreach x ‚àà A do Lab ‚Üê Lab ‚à™ {(x, BLANK)};
Estable ‚äÜ 2A ; Estable ‚Üê 0;
/
call find-stable-extensions(Lab);
report Estable is the set of all stable extensions;
procedure find-stable-extensions(Lab) begin
while ‚àÉy ‚àà A : Lab(y) = BLANK do
select y with Lab(y) = BLANK and ‚àÄz ‚àà {y}‚àí Lab(z) ‚àà {OUT, MUST OUT },
otherwise select y with Lab(y) = BLANK s.t. ‚àÄz : Lab(z) = BLANK, |{x : x ‚àà
{y}+ ‚àß Lab(x) Ã∏= OUT }| ‚â• |{x : x ‚àà {z}+ ‚àß Lab(x) Ã∏= OUT }|;
Lab‚Ä≤ ‚Üê Lab;
Lab‚Ä≤ (y) ‚Üê IN;
foreach z ‚àà {y}+ do Lab‚Ä≤ (z) ‚Üê OUT ;
foreach z ‚àà {y}‚àí do
if Lab‚Ä≤ (z) = BLANK then
Lab‚Ä≤ (z) ‚Üê MUST OUT ;
if ‚àÄw ‚àà {z}‚àí Lab‚Ä≤ (w) Ã∏= BLANK then
Lab(y) ‚Üê MUST OUT ;
goto line 7;
call find-stable-extensions(Lab‚Ä≤ );
if ‚àÉz ‚àà {y}‚àí : Lab(z) = BLANK then
Lab(y) ‚Üê MUST OUT ;
else
Lab ‚Üê Lab‚Ä≤ ;
if ‚àÄx : Lab(x) Ã∏= MUST OUT then
S ‚Üê {x | Lab(x) = IN};
Estable ‚Üê Estable ‚à™ {S};
end procedure

642

A LGORITHMS FOR A RGUMENTATION S EMANTICS

3.3 Enumerating Complete Extensions of any AF
Algorithm 3 lists all complete extensions. Algorithm 3 is a modification of algorithm 1 that enumerates preferred extensions. In algorithm 1, P = {w | the label of w is IN} is a preferred extension
if and only if for each x ‚àà A, the label of x is not BLANK nor MUST OUT and P is not a subset of
a previously found preferred extension. In algorithm 3 (line 10) the set {w | the label of w is IN} is
a complete extension if and only if
C1. for every x ‚àà A, the label of x is not MUST OUT and
C2. there is no z with UNDEC (or BLANK) label such that for every y ‚àà {z}‚àí the label of y is
OUT.
Recall that a complete extension is an admissible set S such that for every x acceptable with respect
to S, x belongs to S. Thus, the condition C1 ensures admissibility while C2 guarantees completeness.
3.4 Enumerating Stage Extensions of any AF
Algorithm 4 lists all stage extensions. Algorithm 4 is an alteration of algorithm 1 that enumerates
preferred extensions. Algorithm 4 uses four labels: IN, OUT, UNDEC and BLANK. The usage of
these labels is as outlined in algorithm 1 with one distinction: the role of the MUST OUT label used
in algorithm 1 is now overloaded to the UNDEC label. Meaning, in algorithm 4 the UNDEC label is
used also for identifying arguments that attack an IN argument. This is because any argument attacks
an argument of a stage extension is not necessarily attacked by an argument of the extension.
Algorithm 4 constructs conflict free subsets of A. In particular, algorithm 4 (line 26) keeps a
record of the conflict free set {w | the label of w is IN} if and only if for each x ‚àà A, the label of
x is not BLANK. After constructing such conflict free subsets, algorithm 4 decides that a conflict
free subset, say S, is a stage extension if and only if S ‚à™ S+ is maximal, see lines 5-9. As might
be expected, argument selection and pruning strategies used in admissibility based semantics will
not be applicable to stage semantics, which are based on conflict free sets. Therefore, as a pruning
strategy we skip labeling an argument, say y, UNDEC if and only if for each z ‚àà {y}+ ‚à™ {y}‚àí , the
label of z is OUT or UNDEC. This is based on the following property: if a conflict free set, say S,
will be captured while such y is UNDEC then S ‚à™ {y} is also conflict free, and hence, there is no
need to label y with UNDEC since S ‚à™ {y} ‚äÉ S; recall that algorithm 4 labels an argument UNDEC
trying to build a stage extension excluding the argument. On selecting the next BLANK argument
to be labeled IN, we consider the rule:
R1. select a BLANK argument y s.t. for each z ‚àà {y}+ ‚à™ {y}‚àí , the label of z is OUT or UNDEC.
R2. otherwise select a BLANK argument y such that |{x : x ‚àà {y}+ ‚à™ {y}‚àí and the label of x is
BLANK}| is maximal.
Note the correlation between R1 and the applied pruning strategy: the earlier we label the
argument selected by R1 with IN, the bigger the part of the search tree that will be bypassed.
Regarding the benefit of R2, recall that the aim of argument selection is to accelerate achieving a
goal state, which is a conflict free subset S such that S ‚à™ S+ is maximal and there is no x ‚àà A with the
BLANK label. Indeed, R2 minimises the number of BLANK arguments by maximising the number
of OUT/UNDEC arguments.
643

N OFAL , ATKINSON , & D UNNE

Algorithm 3: Enumerating all complete extensions of an AF (A, R).
1
2
3
4
5

6
7
8
9
10
11
12

13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

Lab : A ‚Üí {IN, OUT, MUST OUT,UNDEC, BLANK}; Lab ‚Üê 0;
/
foreach x ‚àà A do Lab ‚Üê Lab ‚à™ {(x, BLANK)};
Ecomplete ‚äÜ 2A ; Ecomplete ‚Üê 0;
/
call find-complete-extensions(Lab);
report Ecomplete is the set of all complete extensions;
procedure find-complete-extensions(Lab) begin
if Ã∏ ‚àÉy ‚àà A : Lab(y) = MUST OUT then
if Ã∏ ‚àÉx : Lab(x) ‚àà {UNDEC, BLANK} ‚àß ‚àÄz ‚àà {x}‚àí Lab(z) = OUT then
S ‚Üê {w ‚àà A | Lab(w) = IN};
Ecomplete ‚Üê Ecomplete ‚à™ {S};
while ‚àÉy ‚àà A : Lab(y) = BLANK do
select y with Lab(y) = BLANK and ‚àÄz ‚àà {y}‚àí Lab(z) ‚àà {OUT, MUST OUT },
otherwise select y with Lab(y) = BLANK s.t. ‚àÄz ‚àà A : Lab(z) = BLANK, |{x : x ‚àà
{y}+ ‚àß Lab(x) Ã∏= OUT }| ‚â• |{x : x ‚àà {z}+ ‚àß Lab(x) Ã∏= OUT }|;
Lab‚Ä≤ ‚Üê Lab;
Lab‚Ä≤ (y) ‚Üê IN;
foreach z ‚àà {y}+ do Lab‚Ä≤ (z) ‚Üê OUT ;
foreach z ‚àà {y}‚àí do
if Lab‚Ä≤ (z) ‚àà {UNDEC, BLANK} then
Lab‚Ä≤ (z) ‚Üê MUST OUT ;
if Ã∏ ‚àÉw ‚àà {z}‚àí : Lab‚Ä≤ (w) = BLANK then
Lab(y) ‚Üê UNDEC;
goto line 11;
call find-complete-extensions(Lab‚Ä≤ );
if ‚àÉz ‚àà {y}‚àí : Lab(z) ‚àà {BLANK,UNDEC} then
Lab(y) ‚Üê UNDEC;
else
Lab ‚Üê Lab‚Ä≤ ;
end procedure

644

A LGORITHMS FOR A RGUMENTATION S EMANTICS

Algorithm 4: Enumerating all stage extensions of an AF (A, R).
1
2
3
4
5
6
7
8
9
10
11

12
13
14

15
16
17
18
19
20
21
22
23
24
25
26
27

Lab : A ‚Üí {IN, OUT,UNDEC, BLANK}; Lab ‚Üê 0;
/
foreach x ‚àà A do Lab ‚Üê Lab ‚à™ {(x, BLANK)};
Estage ‚äÜ {Lab1 | Lab1 : A ‚Üí {IN, OUT,UNDEC, BLANK}}; Estage ‚Üê 0;
/
call find-conflict-free-sets(Lab);
foreach Lab1 ‚àà Estage do
foreach Lab2 ‚àà Estage do
if {x : Lab1 (x) ‚àà {IN, OUT }} ( {z : Lab2 (z) ‚àà {IN, OUT }} then
Estage ‚Üê Estage \ {Lab1 };
continue to next iteration from line 5;
foreach Lab1 ‚àà Estage do
report {x : Lab1 (x) = IN} is a stage extension ;
procedure find-conflict-free-sets(Lab) begin
while ‚àÉy ‚àà A : Lab(y) = BLANK do
select y with Lab(y) = BLANK such that ‚àÄz ‚àà {y}+ ‚à™ {y}‚àí Lab(z) ‚àà {OUT,UNDEC},
otherwise select y with Lab(y) = BLANK such that ‚àÄz : Lab(z) = BLANK, |{x : x ‚àà
{y}+ ‚à™ {y}‚àí ‚àß Lab(x) = BLANK}| ‚â• |{x : x ‚àà {z}+ ‚à™ {z}‚àí ‚àß Lab(x) = BLANK}|;
Lab‚Ä≤ ‚Üê Lab;
Lab‚Ä≤ (y) ‚Üê IN;
foreach z ‚àà {y}+ do Lab‚Ä≤ (z) ‚Üê OUT ;
foreach z ‚àà {y}‚àí do
if Lab‚Ä≤ (z) ‚àà {BLANK} then
Lab‚Ä≤ (z) ‚Üê UNDEC;
call find-conflict-free-sets(Lab‚Ä≤ );
if ‚àÉz ‚àà {y}+ ‚à™ {y}‚àí with Lab(z) = BLANK then
Lab(y) ‚Üê UNDEC;
else
Lab ‚Üê Lab‚Ä≤ ;
Estage ‚Üê Estage ‚à™ {Lab};
end procedure

645

N OFAL , ATKINSON , & D UNNE

3.5 Enumerating Semi Stable Extensions of any AF
Algorithm 5 enumerates all semi stable extensions. Again, algorithm 5 is a reproduction of algorithm 1. Actually, algorithm 5 firstly builds admissible sets. Then, the algorithm decides that an
admissible set, say S, is a semi stable extension if and only if S ‚à™ S+ is maximal; see lines 6-10.

Algorithm 5: Enumerating all semi stable extensions of an AF (A, R).
1
2
3
4
5
6
7
8
9
10
11
12

13
14
15

16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32

Lab : A ‚Üí {IN, OUT, MUST OUT,UNDEC, BLANK}; Lab ‚Üê 0;
/
foreach x ‚àà A do Lab ‚Üê Lab ‚à™ {(x, BLANK)};
Esemi‚àístable ‚äÜ {Lab1 | Lab1 : A ‚Üí {IN, OUT, MUST OUT,UNDEC, BLANK}};
Esemi‚àístable ‚Üê 0;
/
call find-admissible-sets(Lab);
foreach Lab1 ‚àà Esemi‚àístable do
foreach Lab2 ‚àà Esemi‚àístable do
if {x : Lab1 (x) ‚àà {IN, OUT }} ( {z : Lab2 (z) ‚àà {IN, OUT }} then
Esemi‚àístable ‚Üê Esemi‚àístable \ {Lab1 };
continue to next iteration from line 6;
foreach Lab1 ‚àà Esemi‚àístable do
report {x : Lab1 (x) = IN} is a semi stable extension ;
procedure find-admissible-sets(Lab) begin
while ‚àÉy ‚àà A : Lab(y) = BLANK do
select y with Lab(y) = BLANK and ‚àÄz ‚àà {y}‚àí Lab(z) ‚àà {OUT, MUST OUT },
otherwise select y with Lab(y) = BLANK s.t. ‚àÄz ‚àà A : Lab(z) = BLANK, |{x : x ‚àà
{y}+ ‚àß Lab(x) Ã∏= OUT }| ‚â• |{x : x ‚àà {z}+ ‚àß Lab(x) Ã∏= OUT }|;
Lab‚Ä≤ ‚Üê Lab;
Lab‚Ä≤ (y) ‚Üê IN;
foreach z ‚àà {y}+ do Lab‚Ä≤ (z) ‚Üê OUT ;
foreach z ‚àà {y}‚àí do
if Lab‚Ä≤ (z) ‚àà {UNDEC, BLANK} then
Lab‚Ä≤ (z) ‚Üê MUST OUT ;
if Ã∏ ‚àÉw ‚àà {z}‚àí : Lab‚Ä≤ (w) = BLANK then
Lab(y) ‚Üê UNDEC;
goto line 14;
call find-admissible-sets(Lab‚Ä≤ );
if ‚àÉz ‚àà {y}‚àí : Lab(z) ‚àà {BLANK,UNDEC} then
Lab(y) ‚Üê UNDEC;
else
Lab ‚Üê Lab‚Ä≤ ;
if Ã∏ ‚àÉx ‚àà A : Lab(x) = MUST OUT then
Esemi‚àístable ‚Üê Esemi‚àístable ‚à™ {Lab};
end procedure

646

A LGORITHMS FOR A RGUMENTATION S EMANTICS

3.6 Constructing the Ideal Extension of any AF
Algorithm 6 builds the ideal extension. The algorithm is a modification of algorithm 1. Algorithm 6
(line 28) records {w | the label of w is IN} as an admissible set if and only if for each x ‚àà A, the label
of x is not BLANK nor MUST OUT. However, the algorithm also constructs (line 27) S = {x ‚àà A |
there exists an admissible set T such that x ‚àà T + }. After building a set of admissible sets and
having S constructed, algorithm 6 considers an admissible set I as the ideal extension if and only
if I ‚à© S = 0;
/ see lines 6-8. Recall that the ideal extension is the maximal (w.r.t. ‚äÜ) admissible
set that is contained in every preferred extension. Satisfying the condition I ‚à© S = 0/ implies that
the arguments of I are not attacked by any admissible set (see the definition of S above), which
means I is contained in every preferred extension. To ensure such I is maximal, algorithm 6 collects
admissible sets in descending order: from larger sets to smaller ones. In consequence, the algorithm
checks the collected admissible sets with the condition I ‚à© S = 0/ starting from larger admissible sets
to smaller ones.
3.7 Constructing the Grounded Extension of any AF
Algorithm 7 can be viewed as another implementation of the algorithm described by Modgil and
Caminada (2009) for building the grounded extension.

4. Labeling Attacks as a Generalization of Labeling Arguments
In this section we illustrate how to enumerate extensions, under a number of argumentation semantics, by labeling attacks together with arguments instead of labeling arguments solely. To this end,
we develop algorithms for listing extensions of an AFRA (Baroni et al., 2011b) under preferred,
stable, complete, stage, semi stable, ideal and grounded semantics in subsections 4.1, 4.2, 4.3, 4.4,
4.5, 4.6 and 4.7 respectively. All these algorithms are basically a generalization of the algorithms
presented in the previous section, hence these algorithms list extensions of any AF / AFRA.
4.1 Enumerating Preferred Extensions of any AF / AFRA
Algorithm 8 enumerates all preferred extensions of an AFRA. Algorithm 8 is a generalization of
algorithm 1. The idea is based on using five labels: IN, OUT, MUST OUT, BLANK and UNDEC.
The BLANK label is the initial label for all arguments and attacks. A BLANK attack y ‚àà R is labeled
IN to indicate that y might be in a preferred extension. An argument x is labeled OUT if and only if
there is y ‚àà R with the label IN such that trg(y) = x. An attack z ‚àà R is labeled OUT if and only if
there is y ‚àà R with the label IN such that trg(y) ‚àà {z, src(z)}. A BLANK argument x is labeled IN,
implying that x might be in a preferred extension, if and only if there is y ‚àà R with the label IN such
that src(y) = x or for each z ‚àà R : trg(z) = x the label of z is OUT. An attack y is labeled UNDEC to
try to find a preferred extension excluding y. An attack z with the label BLANK/UNDEC is labeled
MUST OUT if and only if there is y ‚àà R with the label IN such that trg(z) ‚àà {y, src(y)}. Every
time an attack is labeled IN the labels of some attacks and arguments might change accordingly, see
lines 10-20 of algorithm 8. As a selection rule, line 8 represents the strategy by which the algorithm
selects the next attack, that is BLANK, to be labeled IN. The rule and its grounds is in parallel to
the selection rule applied in algorithm 1 for enumerating preferred extensions of an AF. Likewise,
algorithm 8 applies two pruning tactics:
647

N OFAL , ATKINSON , & D UNNE

Algorithm 6: Constructing the ideal extension of an AF (A, R).
1
2
3
4
5
6
7
8

9
10
11

12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29

Lab : A ‚Üí {IN, OUT, MUST OUT,UNDEC, BLANK}; Lab ‚Üê 0;
/
foreach x ‚àà A do Lab ‚Üê Lab ‚à™ {(x, BLANK)};
Eideal : Z ‚Üí 2A ; Eideal ‚Üê 0;
/
S ‚Üê 0;
/
call find-admissible-sets(Lab);
foreach i = 1 .. |Eideal | do
if Eideal (i) ‚à© S = 0/ then
report Eideal (i) is the ideal extension; exit;
procedure find-admissible-sets(Lab) begin
while ‚àÉy ‚àà A : Lab(y) = BLANK do
select y with Lab(y) = BLANK and ‚àÄz ‚àà {y}‚àí Lab(z) ‚àà {OUT, MUST OUT },
otherwise select y with Lab(y) = BLANK s.t. ‚àÄz ‚àà A : Lab(z) = BLANK, |{x : x ‚àà
{y}+ ‚àß Lab(x) Ã∏= OUT }| ‚â• |{x : x ‚àà {z}+ ‚àß Lab(x) Ã∏= OUT }|;
Lab‚Ä≤ ‚Üê Lab;
Lab‚Ä≤ (y) ‚Üê IN;
foreach z ‚àà {y}+ do Lab‚Ä≤ (z) ‚Üê OUT ;
foreach z ‚àà {y}‚àí do
if Lab‚Ä≤ (z) ‚àà {UNDEC, BLANK} then
Lab‚Ä≤ (z) ‚Üê MUST OUT ;
if Ã∏ ‚àÉw ‚àà {z}‚àí : Lab‚Ä≤ (w) = BLANK then
Lab(y) ‚Üê UNDEC;
goto line 10;
call find-admissible-sets(Lab‚Ä≤ );
if ‚àÉz ‚àà {y}‚àí : Lab(z) ‚àà {BLANK,UNDEC} then
Lab(y) ‚Üê UNDEC;
else
Lab ‚Üê Lab‚Ä≤ ;
if Ã∏ ‚àÉw ‚àà A : Lab(w) = MUST OUT then
S ‚Üê S ‚à™ {x | Lab(x) = OUT };
Eideal ‚Üê Eideal ‚à™ {(|Eideal | + 1, {z | Lab(z) = IN})};
end procedure

Algorithm 7: Constructing the grounded extension of an AF (A, R).
1
2
3
4
5
6
7

Lab : A ‚Üí {IN, OUT,UNDEC}; Lab ‚Üê 0;
/
foreach w ‚àà A do Lab ‚Üê Lab ‚à™ {(w,UNDEC)};
while ‚àÉx with Lab(x) = UNDEC : ‚àÄy ‚àà {x}‚àí Lab(y) = OUT do
foreach x with Lab(x) = UNDEC : ‚àÄy ‚àà {x}‚àí Lab(y) = OUT do
Lab(x) ‚Üê IN;
foreach z ‚àà {x}+ do Lab(z) ‚Üê OUT ;
report the grounded extension is {w | Lab(w) = IN};

648

A LGORITHMS FOR A RGUMENTATION S EMANTICS

Figure 4: How algorithm 8 works on an AFRA.
1. Algorithm 8 (lines 17- 20) skips labeling an attack y IN (i.e. skips expanding a left child) if
and only if
‚àÉz : trg(z) ‚àà {y, src(y)} and the label o f z is not OUT and
Ã∏ ‚àÉ w with the label BLANK : trg(w) ‚àà {z, src(z)}.
2. Algorithm 8 (lines 22- 25) skips labeling an attack y UNDEC (i.e. skips expanding a right
child) if and only if for each z ‚àà R : trg(z) ‚àà {y, src(y)}, the label of z is OUT or MUST OUT.
To get the general idea of algorithm 8 see figure 4 that shows how the algorithm works on the
AFRA depicted in figure 2.
4.2 Enumerating Stable Extensions of any AF / AFRA
Algorithm 9 enumerates all stable extensions. Actually, algorithm 9 is a modification of algorithm 8
that lists preferred extensions. However there are two differences:
1. Algorithm 9 uses four labels: IN, OUT, BLANK and MUST OUT. The usage of these labels
is as outlined in algorithm 8 with one difference: the role of the UNDEC label used in algorithm 8 is now overloaded to the MUST OUT label. That is, in algorithm 9 the MUST OUT
label is used also for labeling an attack, say x, trying to build a stable extension without x.
This is because any attack, say x, outside a candidate stable extension should be defeated by
an attack in the extension, hence x should be labeled MUST OUT.
2. In algorithm 8 we find a preferred extension, say P, if and only if for each x ‚àà A ‚à™ R, x is not
BLANK nor MUST OUT and P is not a subset of a previously found preferred extension. In
algorithm 9 we encounter a stable extension if and only if for each x ‚àà A ‚à™ R, x is not BLANK
nor MUST OUT.
649

N OFAL , ATKINSON , & D UNNE

Algorithm 8: Enumerating all preferred extensions of an AFRA (A, R).
1
2
3
4
5

6
7
8

9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

28
29
30
31

Lab : (A ‚à™ R) ‚Üí {IN, OUT, MUST OUT,UNDEC, BLANK}; Lab ‚Üê 0;
/
foreach x ‚àà A ‚à™ R do Lab ‚Üê Lab ‚à™ {(x, BLANK)};
E pre f erred ‚äÜ 2A‚à™R ; E pre f erred ‚Üê 0;
/
call find-preferred-extensions(Lab);
report E pre f erred is the set of all preferred extensions;
procedure find-preferred-extensions(Lab) begin
while ‚àÉy ‚àà R : Lab(y) = BLANK do
select y ‚àà R with Lab(y) = BLANK such that
‚àÄz ‚àà R : trg(z) ‚àà {y, src(y)} Lab(z) ‚àà {OUT, MUST OUT }, otherwise select y ‚àà R with
Lab(y) = BLANK such that ‚àÄz ‚àà R : Lab(z) = BLANK
|{x : src(x) = trg(y) ‚àß Lab(x) Ã∏= OUT }| ‚â• |{x : src(x) = trg(z) ‚àß Lab(x) Ã∏= OUT }|;
Lab‚Ä≤ ‚Üê Lab;
Lab‚Ä≤ (y) ‚Üê IN;
Lab‚Ä≤ (src(y)) ‚Üê IN;
Lab‚Ä≤ (trg(y)) ‚Üê OUT ;
if trg(y) ‚àà A then
foreach z ‚àà R : src(z) = trg(y) do
Lab‚Ä≤ (z) ‚Üê OUT ;
foreach z ‚àà R : Lab‚Ä≤ (z) ‚àà {BLANK,UNDEC} ‚àß trg(z) ‚àà {y, src(y)} do
Lab‚Ä≤ (z) ‚Üê MUST OUT ;
if Ã∏ ‚àÉ w ‚àà R : Lab‚Ä≤ (w) = BLANK ‚àß trg(w) ‚àà {z, src(z)} then
Lab(y) ‚Üê UNDEC;
goto line 7;
call find-preferred-extensions(Lab‚Ä≤ );
if ‚àÉz ‚àà R : Lab(z) ‚àà {BLANK,UNDEC} ‚àß trg(z) ‚àà {y, src(y)} then
Lab(y) ‚Üê UNDEC;
else
Lab ‚Üê Lab‚Ä≤ ;
if Ã∏ ‚àÉw ‚àà R : Lab(w) = MUST OUT then
foreach x ‚àà A with Lab(x) = BLANK s.t. ‚àÄz ‚àà R : trg(z) = x (Lab(z) = OUT ) do
Lab(x) ‚Üê IN;
S ‚Üê {x ‚àà A ‚à™ R | Lab(x) = IN};
if ‚àÄT ‚àà E pre f erred (S Ã∏‚äÜ T ) then
E pre f erred ‚Üê E pre f erred ‚à™ {S};
end procedure

650

A LGORITHMS FOR A RGUMENTATION S EMANTICS

Algorithm 9: Enumerating all stable extensions of an AFRA (A, R).
1
2
3
4
5

6
7
8

9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

28
29

Lab : (A ‚à™ R) ‚Üí {IN, OUT, MUST OUT, BLANK}; Lab ‚Üê 0;
/
foreach x ‚àà A ‚à™ R do Lab ‚Üê Lab ‚à™ {(x, BLANK)};
E stable ‚äÜ 2A‚à™R ; E stable ‚Üê 0;
/
call find-stable-extensions(Lab);
report E stable is the set of all stable extensions;
procedure find-stable-extensions(Lab) begin
while ‚àÉy ‚àà R : Lab(y) = BLANK do
select y ‚àà R with Lab(y) = BLANK such that
‚àÄz ‚àà R : trg(z) ‚àà {y, src(y)} Lab(z) ‚àà {OUT, MUST OUT }, otherwise select y ‚àà R with
Lab(y) = BLANK such that ‚àÄz ‚àà R : Lab(z) = BLANK
|{x : src(x) = trg(y) ‚àß Lab(x) Ã∏= OUT }| ‚â• |{x : src(x) = trg(z) ‚àß Lab(x) Ã∏= OUT }|;
Lab‚Ä≤ ‚Üê Lab;
Lab‚Ä≤ (y) ‚Üê IN;
Lab‚Ä≤ (src(y)) ‚Üê IN;
Lab‚Ä≤ (trg(y)) ‚Üê OUT ;
if trg(y) ‚àà A then
foreach z ‚àà R : src(z) = trg(y) do
Lab‚Ä≤ (z) ‚Üê OUT ;
foreach z ‚àà R : Lab‚Ä≤ (z) = BLANK ‚àß trg(z) ‚àà {y, src(y)} do
Lab‚Ä≤ (z) ‚Üê MUST OUT ;
if Ã∏ ‚àÉ w ‚àà R : Lab‚Ä≤ (w) = BLANK ‚àß trg(w) ‚àà {z, src(z)} then
Lab(y) ‚Üê MUST OUT ;
goto line 7;
call find-stable-extensions(Lab‚Ä≤ );
if ‚àÉz ‚àà R : Lab(z) = BLANK ‚àß trg(z) ‚àà {y, src(y)} then
Lab(y) ‚Üê MUST OUT ;
else
Lab ‚Üê Lab‚Ä≤ ;
if Ã∏ ‚àÉw ‚àà R : Lab(w) = MUST OUT then
foreach x ‚àà A with Lab(x) = BLANK s.t. ‚àÄz ‚àà R : trg(z) = x (Lab(z) = OUT ) do
Lab(x) ‚Üê IN;
E stable ‚Üê E stable ‚à™ {{x ‚àà A ‚à™ R | Lab(x) = IN}};
end procedure

651

N OFAL , ATKINSON , & D UNNE

4.3 Enumerating Complete Extensions of any AF / AFRA
Algorithm 10 enumerates all complete extensions. Again, algorithm 10 is a modification of algorithm 8 that lists preferred extensions. In algorithm 8 we achieve a preferred extension, say P, if and
only if for each x ‚àà A ‚à™ R, the label of x is not BLANK nor MUST OUT and P is not a subset of a
previously found preferred extension. However, in algorithm 10 (line 7) we encounter a complete
extension if and only if
C1. there is no z ‚àà R with the MUST OUT label and
C2. there does not exist w ‚àà R such that
(a) the label of w is UNDEC or BLANK and
(b) for each y ‚àà R : trg(y) ‚àà {w, src(w)}, the label of y is OUT.
Thus, C1 ensures admissibility while C2 guarantees completeness.
4.4 Enumerating Stage Extensions of any AF / AFRA
Algorithm 11 lists all stage extensions. The algorithm is a rewrite of algorithm 8. However, algorithm 11 uses four labels: IN, OUT, BLANK and UNDEC. The usage of these labels is as outlined
in algorithm 8 with one difference: the role of the MUST OUT label used in algorithm 8 is now
overloaded to the UNDEC label. That is, in algorithm 11 the UNDEC label is used also for identifying attacks that attack an IN argument/attack. This is because any attack defeats an argument/attack
of a stage extension is not necessarily defeated by an attack of the extension.
Algorithm 11 (lines 14-29) finds a set of conflict free subsets of A ‚à™ R rather than constructing
admissible subsets as done by algorithm 8. In algorithm 8 the set {w ‚àà A ‚à™ R | the label of w is IN}
is reported as an admissible set if and only if for each x ‚àà A ‚à™ R, the label of x is not BLANK nor
MUST OUT. In algorithm 11 the set {w ‚àà A ‚à™ R | the label of w is IN} is recorded as a conflict free
set (i.e. a stage extension candidate) if and only if for each x ‚àà A ‚à™ R, the label of x is not BLANK,
see lines 14 & 31. After building a set of conflict free subsets, algorithm 11 decides that a conflict
free subset S ‚äÜ A ‚à™ R is a stage extension if and only if S ‚à™ {x | ‚àÉy ‚àà S : y defeats x} is maximal, see
lines 6-10. As we stated earlier, argument selection and pruning strategies used in semantics that are
based on admissible sets will not be applicable to stage semantics, which are based on conflict free
sets. Therefore, as a pruning strategy (line 29 of algorithm 11) we skip labeling an attack y UNDEC
(i.e. skip expanding a right child) if and only if
‚àÄz ‚àà R : trg(z) ‚àà {y, src(y)} ‚à® trg(y) ‚àà {z, src(z)},
the label o f z is OUT or UNDEC.
This is based on the property that if a conflict free set, say S, is formed while such y is UNDEC
then S ‚à™ {y} is also conflict free, and hence, there is no need to label y UNDEC since S ‚à™ {y} ‚äÉ S.
On selecting the next BLANK attack to be labeled IN, we apply the following rule (see line 15):
R1. select a BLANK attack y s.t. for each z ‚àà R : trg(z) ‚àà {y, src(y)} ‚à® trg(y) ‚àà {z, src(z)}, the
label of z is OUT or UNDEC.
R2. otherwise select a BLANK attack y such that |{x : the label of x is BLANK and (src(x) =
trg(y) ‚à® trg(x) ‚àà {y, src(y)})}| is maximal.
652

A LGORITHMS FOR A RGUMENTATION S EMANTICS

Algorithm 10: Enumerating all complete extensions of an AFRA (A, R).
1
2
3
4
5

6
7

8

9
10
11

12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29

Lab : (A ‚à™ R) ‚Üí {IN, OUT, MUST OUT,UNDEC, BLANK}; Lab ‚Üê 0;
/
foreach x ‚àà A ‚à™ R do Lab ‚Üê Lab ‚à™ {(x, BLANK)};
E complete ‚äÜ 2A‚à™R ; E complete ‚Üê 0;
/
call find-complete-extensions(Lab);
report E complete is the set of all complete extensions;
procedure find-complete-extensions(Lab) begin
if Ã∏ ‚àÉv ‚àà R with Lab(v) = MUST OUT and Ã∏ ‚àÉw ‚àà R : Lab(w) ‚àà {UNDEC, BLANK} with
‚àÄy ‚àà R : trg(y) ‚àà {w, src(w)} Lab(y) = OUT then
foreach x ‚àà A with Lab(x) = BLANK s.t. ‚àÄz ‚àà R : trg(z) = x (Lab(z) = OUT ) do
Lab(x) ‚Üê IN;
E complete ‚Üê E complete ‚à™ {{x ‚àà A ‚à™ R | Lab(x) = IN)}};
while ‚àÉy ‚àà R : Lab(y) = BLANK do
select y ‚àà R with Lab(y) = BLANK such that
‚àÄz ‚àà R : trg(z) ‚àà {y, src(y)} Lab(z) ‚àà {OUT, MUST OUT }, otherwise select y ‚àà R with
Lab(y) = BLANK such that ‚àÄz ‚àà R : Lab(z) = BLANK
|{x : src(x) = trg(y) ‚àß Lab(x) Ã∏= OUT }| ‚â• |{x : src(x) = trg(z) ‚àß Lab(x) Ã∏= OUT }|;
Lab‚Ä≤ ‚Üê Lab;
Lab‚Ä≤ (y) ‚Üê IN;
Lab‚Ä≤ (src(y)) ‚Üê IN;
Lab‚Ä≤ (trg(y)) ‚Üê OUT ;
if trg(y) ‚àà A then
foreach z ‚àà R : src(z) = trg(y) do
Lab‚Ä≤ (z) ‚Üê OUT ;
foreach z ‚àà R : Lab‚Ä≤ (z) ‚àà {BLANK,UNDEC} ‚àß trg(z) ‚àà {y, src(y)} do
Lab‚Ä≤ (z) ‚Üê MUST OUT ;
if Ã∏ ‚àÉ w ‚àà R : Lab‚Ä≤ (w) = BLANK ‚àß trg(w) ‚àà {z, src(z)} then
Lab(y) ‚Üê UNDEC;
goto line 10;
call find-complete-extensions(Lab‚Ä≤ );
if ‚àÉz ‚àà R : Lab(z) ‚àà {BLANK,UNDEC} ‚àß trg(z) ‚àà {y, src(y)} then
Lab(y) ‚Üê UNDEC;
else
Lab ‚Üê Lab‚Ä≤ ;
end procedure

653

N OFAL , ATKINSON , & D UNNE

The aim of R1 is to maximise the gain of the applied pruning strategy. Meaning, the earlier
we label the selected argument by R1 with IN, the greater the saving will be in terms of the part
of the search tree pruned. Regarding R2, note that a goal state (i.e. conflict free set) is reached
if and only if for each x ‚àà A ‚à™ R, the label of x is not BLANK. Thus, R2 tries to maximise the
number of OUT/UNDEC attacks/arguments, which implies minimising the number of BLANK
attacks/arguments.
4.5 Enumerating Semi Stable Extensions of any AF / AFRA
Algorithm 12 lists all semi stable extensions. Algorithm 12 is a variation of algorithm 8 such that it
basically constructs admissible sets. Algorithm 12 (line 31) records the set {w | the label of w is IN}
as an admissible set (that is a semi stable extension candidate) if and only if for each x ‚àà A ‚à™ R, the
label of x is not BLANK nor MUST OUT. After constructing a set of admissible sets, algorithm 12
decides that an admissible set S is a semi stable extension if and only if S ‚à™ {x | ‚àÉy ‚àà S : y defeats x}
is maximal, see lines 6-10.
4.6 Constructing the Ideal Extension of any AF / AFRA
Algorithm 13 builds the ideal extension. In particular, algorithm 13 finds admissible sets (lines 1028) in the same way algorithm 8 does. However, in enumerating admissible sets algorithm 13
(line 31) also builds the set
S = {x ‚àà A ‚à™ R | there is y in an admissible set such that trg(y) ‚àà {x, src(x)}}
After building a set of admissible sets and having such S constructed, algorithm 13 decides
that an admissible set I is the ideal extension if and only if I ‚à© S = 0,
/ see lines 6-8. Recall that
the ideal extension is the maximal (w.r.t. ‚äÜ) admissible set that is contained in every preferred
extension. Satisfying the condition I ‚à© S = 0/ implies that the arguments/attacks of I are not defeated
by any admissible set (see the definition of S above), which means I is contained in every preferred
extension. To ensure such I is maximal, algorithm 13 collects admissible sets in descending order:
from larger sets to smaller ones. In consequence, the algorithm checks the collected admissible sets
with the condition I ‚à© S = 0/ starting from larger admissible sets to smaller ones.
4.7 Constructing the Grounded Extension of any AF / AFRA
Algorithm 14 builds the grounded extension. Algorithm 14 is actually a generalization of algorithm 7.

5. Practical Efficiency
All algorithms presented in this paper were implemented in C++ on a Fedora (release 13) based
machine with 4 processors (Intel core i5-750 2.67GHz) and 16GB of memory. As an evaluation criterion, we considered the average elapsed time measured in seconds; the elapsed time was obtained
by using the time command of Linux. We present experimental results for two purposes. First,
we explore the efficiency of the algorithms of section 3. For the second purpose, we confirm that
the generalized algorithms of section 4, which enumerate extensions by labeling attacks together
with arguments, perform as efficiently as the algorithms of section 3, which enumerate extensions
by labeling arguments alone.
654

A LGORITHMS FOR A RGUMENTATION S EMANTICS

Algorithm 11: Enumerating all stage extensions of an AFRA (A, R).
1
2
3
4
5
6
7
8
9
10
11
12

13
14
15

16
17
18
19
20
21
22
23
24
25
26
27
28
29
30

31
32

Lab : (A ‚à™ R) ‚Üí {IN, OUT,UNDEC, BLANK}; Lab ‚Üê 0;
/
foreach x ‚àà A ‚à™ R do Lab ‚Üê Lab ‚à™ {(x, BLANK)};
E stage ‚äÜ {Lab1 | Lab1 : (A ‚à™ R) ‚Üí {IN, OUT,UNDEC, BLANK}};
E stage ‚Üê 0;
/
call find-conflict-free-sets(Lab);
foreach Lab1 ‚àà E stage do
foreach Lab2 ‚àà E stage do
if {x : Lab1 (x) ‚àà {IN, OUT }} ( {z : Lab2 (z) ‚àà {IN, OUT }} then
E stage ‚Üê E stage \ {Lab1 };
continue to next iteration from line 6;
foreach Lab1 ‚àà E stage do
report {x : Lab1 (x) = IN} is a stage extension ;
procedure find-conflict-free-sets(Lab) begin
while ‚àÉy ‚àà R : Lab(y) = BLANK do
select y ‚àà R with Lab(y) = BLANK s.t.
‚àÄz ‚àà R : trg(z) ‚àà {y, src(y)} ‚à® trg(y) ‚àà {z, src(z)} (Lab(z) ‚àà {OUT,UNDEC}),
otherwise select y ‚àà R with Lab(y) = BLANK s.t.
‚àÄz ‚àà R : Lab(z) = BLANK |{x : Lab(x) = BLANK ‚àß (src(x) = trg(y) ‚à® trg(x) ‚àà
{y, src(y))}}| ‚â• |{x : Lab(x) = BLANK ‚àß (src(x) = trg(z) ‚à® trg(x) ‚àà {z, src(z))}}|;
Lab‚Ä≤ ‚Üê Lab;
Lab‚Ä≤ (y) ‚Üê IN;
Lab‚Ä≤ (src(y)) ‚Üê IN;
Lab‚Ä≤ (trg(y)) ‚Üê OUT ;
if trg(y) ‚àà A then
foreach z ‚àà R : src(z) = trg(y) do
Lab‚Ä≤ (z) ‚Üê OUT ;
foreach z ‚àà R : Lab‚Ä≤ (z) = BLANK ‚àß trg(z) ‚àà {y, src(y)} do
Lab‚Ä≤ (z) ‚Üê UNDEC;
call find-conflict-free-sets(Lab‚Ä≤ );
if ‚àÉz ‚àà R : Lab(z) = BLANK ‚àß (trg(z) ‚àà {y, src(y)} ‚à® trg(y) ‚àà {z, src(z)}) then
Lab(y) ‚Üê UNDEC;
else
Lab ‚Üê Lab‚Ä≤ ;
foreach x ‚àà A with Lab(x) = BLANK s.t. ‚àÄz ‚àà R : trg(z) = x (Lab(z) = OUT ) do
Lab(x) ‚Üê IN;
E stage ‚Üê E stage ‚à™ {Lab};
end procedure

655

N OFAL , ATKINSON , & D UNNE

Algorithm 12: Enumerating all semi stable extensions of an AFRA (A, R).
1
2
3
4
5
6
7
8
9
10
11
12

13
14
15

16
17
18
19
20
21
22
23
24
25
26
27
28
29
30

31
32

Lab : (A ‚à™ R) ‚Üí {IN, OUT, MUST OUT,UNDEC, BLANK}; Lab ‚Üê 0;
/
foreach x ‚àà A ‚à™ R do Lab ‚Üê Lab ‚à™ {(x, BLANK)};
E semi‚àístable ‚äÜ {Lab1 | Lab1 : (A ‚à™ R) ‚Üí {IN, OUT, MUST OUT,UNDEC, BLANK}};
E semi‚àístable ‚Üê 0;
/
call find-admissible-sets(Lab);
foreach Lab1 ‚àà E semi‚àístable do
foreach Lab2 ‚àà E semi‚àístable do
if {x : Lab1 (x) ‚àà {IN, OUT }} ( {z : Lab2 (z) ‚àà {IN, OUT }} then
E semi‚àístable ‚Üê E semi‚àístable \ {Lab1 };
continue to next iteration from line 6;
foreach Lab1 ‚àà E semi‚àístable do
report {x : Lab1 (x) = IN} is a semi stable extension ;
procedure find-admissible-sets(Lab) begin
while ‚àÉy ‚àà R : Lab(y) = BLANK do
select y ‚àà R with Lab(y) = BLANK s.t.
‚àÄz ‚àà R : trg(z) ‚àà {y, src(y)} Lab(z) ‚àà {OUT, MUST OUT }, otherwise select y ‚àà R with
Lab(y) = BLANK s.t. ‚àÄz ‚àà R : Lab(z) = BLANK |{x : src(x) = trg(y) ‚àß Lab(x) Ã∏=
OUT }| ‚â• |{x : src(x) = trg(z) ‚àß Lab(x) Ã∏= OUT }|;
Lab‚Ä≤ ‚Üê Lab; Lab‚Ä≤ (y) ‚Üê IN; Lab‚Ä≤ (src(y)) ‚Üê IN;
Lab‚Ä≤ (trg(y)) ‚Üê OUT ;
if trg(y) ‚àà A then
foreach z ‚àà R : src(z) = trg(y) do Lab‚Ä≤ (z) ‚Üê OUT ;
foreach z ‚àà R : Lab(z) ‚àà {BLANK,UNDEC} ‚àß trg(z) ‚àà {y, src(y)} do
Lab‚Ä≤ (z) ‚Üê MUST OUT ;
if Ã∏ ‚àÉ w ‚àà R : Lab‚Ä≤ (w) = BLANK ‚àß trg(w) ‚àà {z, src(z)} then
Lab(y) ‚Üê UNDEC; goto line 14;
call find-admissible-sets(Lab‚Ä≤ );
if ‚àÉz ‚àà R : Lab(z) ‚àà {BLANK,UNDEC} ‚àß trg(z) ‚àà {y, src(y)} then
Lab(y) ‚Üê UNDEC;
else
Lab ‚Üê Lab‚Ä≤ ;
if Ã∏ ‚àÉy ‚àà R : Lab(y) = MUST OUT then
foreach x ‚àà A with Lab(x) = BLANK s.t. ‚àÄz ‚àà R : trg(z) = x (Lab(z) = OUT ) do
Lab(x) ‚Üê IN;
E semi‚àístable ‚Üê E semi‚àístable ‚à™ {Lab};
end procedure

656

A LGORITHMS FOR A RGUMENTATION S EMANTICS

Algorithm 13: Constructing the ideal extension of an AFRA (A, R).
1
2
3
4
5
6
7
8

9
10
11

12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30

31
32
33

Lab : (A ‚à™ R) ‚Üí {IN, OUT, MUST OUT,UNDEC, BLANK}; Lab ‚Üê 0;
/
foreach x ‚àà A ‚à™ R do Lab ‚Üê Lab ‚à™ {(x, BLANK)};
E ideal : Z ‚Üí 2A‚à™R ; E ideal ‚Üê 0;
/
S ‚Üê 0;
/
call find-admissible-sets(Lab);
foreach i : 1 to |E ideal | do
/ S) then
if ‚àÄx ‚àà E ideal (i) (x ‚àà
report E ideal (i) is the ideal extension; exit;
procedure find-admissible-sets(Lab) begin
while ‚àÉy ‚àà R : Lab(y) = BLANK do
select y ‚àà R with Lab(y) = BLANK s.t.
‚àÄz ‚àà R : trg(z) ‚àà {y, src(y)} Lab(z) ‚àà {OUT, MUST OUT }, otherwise select y ‚àà R with
Lab(y) = BLANK such that ‚àÄz ‚àà R : Lab(z) = BLANK
|{x : src(x) = trg(y) ‚àß Lab(x) Ã∏= OUT }| ‚â• |{x : src(x) = trg(z) ‚àß Lab(x) Ã∏= OUT }|;
Lab‚Ä≤ ‚Üê Lab;
Lab‚Ä≤ (y) ‚Üê IN;
Lab‚Ä≤ (src(y)) ‚Üê IN;
Lab‚Ä≤ (trg(y)) ‚Üê OUT ;
if trg(y) ‚àà A then
foreach z ‚àà R : src(z) = trg(y) do
Lab‚Ä≤ (z) ‚Üê OUT ;
foreach z ‚àà R : Lab‚Ä≤ (z) ‚àà {BLANK,UNDEC} ‚àß trg(z) ‚àà {y, src(y)} do
Lab‚Ä≤ (z) ‚Üê MUST OUT ;
if Ã∏ ‚àÉ w ‚àà R : Lab‚Ä≤ (w) = BLANK ‚àß trg(w) ‚àà {z, src(z)} then
Lab(y) ‚Üê UNDEC;
goto line 10;
call find-admissible-sets(Lab‚Ä≤ );
if ‚àÉz ‚àà R : Lab(z) ‚àà {BLANK,UNDEC} and trg(z) ‚àà {y, src(y)} then
Lab(y) ‚Üê UNDEC;
else
Lab ‚Üê Lab‚Ä≤ ;
if Ã∏ ‚àÉw ‚àà R : Lab(w) = MUST OUT then
foreach x ‚àà A with Lab(x) = BLANK s.t. ‚àÄz ‚àà R : trg(z) = x (Lab(z) = OUT ) do
Lab(x) ‚Üê IN;
S ‚Üê S ‚à™ {x ‚àà A ‚à™ R | Lab(x) = OUT };
E ideal ‚Üê E ideal ‚à™ {(|E ideal | + 1, {z | Lab(z) = IN})};
end procedure

657

N OFAL , ATKINSON , & D UNNE

Algorithm 14: Constructing the grounded extension of an AFRA (A, R).
1
2
3
4

5
6
7
8
9
10
11

12

Lab : (A ‚à™ R) ‚Üí {IN, OUT,UNDEC}; Lab ‚Üê 0;
/
foreach w ‚àà A ‚à™ R do Lab ‚Üê Lab ‚à™ {(w,UNDEC)};
while ‚àÉx ‚àà R with Lab(x) = UNDEC s.t. ‚àÄy ‚àà R : trg(y) ‚àà {x, src(x)} (Lab(y) = OUT ) do
foreach x ‚àà R with Lab(x) = UNDEC s.t. ‚àÄy ‚àà R : trg(y) ‚àà {x, src(x)} (Lab(y) = OUT )
do
Lab(x) ‚Üê IN;
Lab(src(x)) ‚Üê IN;
Lab(trg(x)) ‚Üê OUT ;
if trg(x) ‚àà A then
foreach z ‚àà R : trg(x) = src(z) do
Lab(z) ‚Üê OUT ;
foreach x ‚àà A with Lab(x) = UNDEC s.t. ‚àÄz ‚àà R : trg(z) = x (Lab(z) = OUT ) do
Lab(x) ‚Üê IN;
report the grounded extension is {w ‚àà A ‚à™ R | Lab(w) = IN};

We compared the algorithms with dynPARTIX, which is an implemented system based on the
dynamic programming algorithm of DvorÃåaÃÅk, Pichler, and Woltran (2012b). Given an AF, dynPARTIX basically computes a tree decomposition of the AF then the extensions are enumerated based
on the tree decomposition. The algorithm used in dynPARTIX is fixed-parameter tractable such that
its time complexity depends on the tree width of the given AF while it is linear in the size of the
AF (DvorÃåaÃÅk et al., 2012b). Since dynPARTIX computes only extensions under preferred, stable and
complete semantics, figures 5, 6 & 7 depict respectively the efficiency of algorithms 1, 2 & 3 versus dynPARTIX. In summary, the figures show that these algorithms are likely to be more efficient
than dynPARTIX. In running the experiments that are represented by these figures, we set a time
limit of 120 seconds for every execution. Out of 1000 runs, dynPARTIX encountered 316 timeouts
in enumerating preferred extensions and 827 timeouts in enumerating complete extensions. These
timeouts are plotted within the figures as 120 seconds. This explains the steady behavior of dynPARTIX that can be noted, particularly, in figure 7. To see the performance of algorithms 1-7 in
contrast to the behavior of algorithms 8-14 we present figures 8-14 respectively. In profiling algorithms 1-7, we reported running times including the time needed to get the corresponding AF of
an AFRA. Note that such process (i.e. expressing an AFRA as an AF) runs in polynomial time: at
worst quadratic time. The figures 8-14 plot the running times for 1000 instances of AFRA randomly
generated with |A| = 5 and R = R1 ‚à™ R2 s.t. R1 ‚äÜ A √ó A and R2 ‚äÜ A √ó R1 . For these instances |R|
grows from 0 to 100 as the probability, which was used for setting attacks in the random generation, goes over {0.01,0.02,0.03,...,1}. Note that instances with |A| = 5 should not be considered
quite small. For example, a randomly generated AFRA with |A| = 5 and |R| = 49 has a corresponding AF with |A| = 54 and |R| = 190. Again, we emphasize that the aim of the experiments is to
compare the performance of algorithms 1-7 with the performance of algorithms 8-14. We do not
mean by the experiments to check the scalability of these algorithms, although an important issue
to be examined. Said that, it is not crucial in such evaluation to consider very large frameworks or
higher level of recursive attacks. Back to the results of the experiments, the only case that involved
exceeding the 120-second time limit occurred in enumerating semi stable extensions. Referring
658

A LGORITHMS FOR A RGUMENTATION S EMANTICS

Figure 5: Enumerating all preferred extensions of 1000 instances of AF with |A|=40, for each p ‚àà
{0.01, 0.02, 0.03, ..., 1} we tracked the average elapsed time for 10 instances generated
randomly with a probability p (i.e. the probability that x attacks y for any x, y ‚àà A).

to figure 12 we note that algorithm 5 (resp. 12) encountered 68 (resp. 66) timeouts. The bottom
line conclusion of these figures is: enumerating extensions of an AFRA by labeling attacks together
with arguments seems to be as efficient as enumerating the extensions of the corresponding AF via
labeling arguments alone.

6. Discussion and Conclusion
We started the paper by refining implemented algorithms1 for enumerating all extensions of Dung‚Äôs
argumentation frameworks (AFs) under a number of argumentation semantics: preferred, stable,
complete, stage, semi stable, ideal and grounded. Algorithms for all these semantics, except stage
and grounded semantics, share a similar core structure: they all basically build admissible sets in
order to construct extensions. In the case of stage semantics, the algorithm actually constructs
conflict free sets for the purpose of listing stage extensions, and hence, the algorithm applies a
slightly different approach to expanding the search tree as we elaborated earlier. Concerning the
grounded semantics, the presented algorithm builds the grounded extension in polynomial time.
Furthermore, we explored the practical efficiency of these algorithms by profiling their performance
running on a wide spectrum of AF instances: from sparse instances to dense ones. In essence
these algorithms construct extensions by using a total function that maps arguments solely to a
set of labels reflecting different states as we illustrated in the paper. Then, we generalized these
algorithms by using a total mapping that labels attacks together with arguments. We implemented
the generalized algorithms to enumerate extensions of an AFRA, which is an AF-extended model that
1. C++ implementations can be found at http://sourceforge.net/projects/argtools/files/

659

N OFAL , ATKINSON , & D UNNE

Figure 6: Enumerating all stable extensions of 1000 instances of AF with |A|=60, for each p ‚àà
{0.01, 0.02, 0.03, ..., 1} we tracked the average elapsed time for 10 instances generated
randomly with a probability p.

Figure 7: Enumerating all complete extensions of 1000 instances of AF with |A|=30, for each p ‚àà
{0.01, 0.02, 0.03, ..., 1} we tracked the average elapsed time for 10 instances generated
randomly with a probability p.

660

A LGORITHMS FOR A RGUMENTATION S EMANTICS

Figure 8: Enumerating all preferred extensions of 1000 instances of AFRA, for each p ‚àà
{0.01, 0.02, 0.03, ..., 1} we tracked the average elapsed time for 10 instances generated
randomly with a probability p.

Figure 9: Enumerating all stable extensions of 1000 instances of AFRA, for each p ‚àà
{0.01, 0.02, 0.03, ..., 1} we tracked the average elapsed time for 10 instances generated
randomly with a probability p.

661

N OFAL , ATKINSON , & D UNNE

Figure 10: Enumerating all complete extensions of 1000 instances of AFRA, for each p ‚àà
{0.01, 0.02, 0.03, ..., 1} we tracked the average elapsed time for 10 instances generated
randomly with a probability p.

Figure 11: Listing all stage extensions of 1000 instances of AFRA, for each p ‚àà
{0.01, 0.02, 0.03, ..., 1} we tracked the average elapsed time for 10 instances generated randomly with a probability p.

662

A LGORITHMS FOR A RGUMENTATION S EMANTICS

Figure 12: Listing all semi stable extensions of 1000 instances of AFRA, for each p ‚àà
{0.01, 0.02, 0.03, ..., 1} we tracked the average elapsed time for 10 instances generated
randomly with a probability p.

Figure 13: Constructing the ideal extension of 1000 instances of AFRA, for each p ‚àà
{0.01, 0.02, 0.03, ..., 1} we tracked the average elapsed time for 10 instances generated
randomly with a probability p.

663

N OFAL , ATKINSON , & D UNNE

Figure 14: Constructing the grounded extension of 1000 instances of AFRA, for each p ‚àà
{0.01, 0.02, 0.03, ..., 1} we tracked the average elapsed time for 10 instances generated
randomly with a probability p.

allows attacks on attacks. In other words, we offered a unified approach to enumerating extensions
of any AF/AFRA, given the fact that an AF is a special case of AFRA (Baroni et al., 2011b). On
the other hand, we showed how labeling attacks alongside arguments can be potentially used as a
basis for enumerating extensions of related formalisms that allow attacks on attacks (e.g. Modgil,
2009b; Gabbay, 2009); nonetheless this is to be confirmed by further research. In fact, extensions
of an instance of such formalisms can be listed by working on the corresponding AF. However, we
demonstrated how to enumerate extensions of an AFRA by applying labeling directly on its native
form without compromising the running time efficiency. We omitted the soundness/completeness
proof of the presented algorithms since it follows immediately from the proof of the algorithm of
Nofal, Atkinson and Dunne (2014) for preferred semantics. The algorithms presented in this paper
do not handle frameworks with self-attacking arguments perfectly. However, the algorithms can
be easily modified such that the initial label for any self-attacking argument is UNDEC instead of
BLANK. For instance, the only change necessary to be made in algorithm 1 is to modify line 2 as
follows
foreach x ‚àà A do
if (x, x) ‚àà R then Lab ‚Üê Lab ‚à™ {(x,UNDEC)};
else Lab ‚Üê Lab ‚à™ {(x, BLANK)};
In general, the UNDEC label (instead of the BLANK label) should be the default label for any
argument/attack that can not be in any extension, like self-attacking arguments and their outgoing
attacks because simply such arguments present conflict by themselves. Recall that only BLANK
arguments/attacks are to be tried with the IN label.
For future work, we plan to study additional options for argument selection. Also, we intend to
evaluate further strategies for pruning the search space.
664

A LGORITHMS FOR A RGUMENTATION S EMANTICS

We discuss now related work. The existing algorithms of Doutre and Mengin (2001) and Modgil
and Caminada (2009) for listing preferred extensions can also be re-engineered towards enumerating
extensions under other argumentation semantics. For example, the papers of Caminada (2007, 2010)
presented algorithms for enumerating semi stable, respectively stage, extensions building up on the
algorithm of Modgil and Caminada (2009). However, the algorithms of the present paper are based
on the algorithm of Nofal, Atkinson and Dunne (2014) for enumerating preferred extensions, which
is likely to be more efficient than existing algorithms (Nofal et al., 2014). We give now some
examples of related work on labeling-based semantics. The theory of Caminada and Gabbay (2009)
defined argumentation semantics by using a total mapping Œª : A ‚Üí {IN, OUT,UNDEC} such that,
broadly speaking, the IN labeled arguments correspond to an extension, say S, while the OUT
labeled arguments correspond to S+ and the UNDEC labeled arguments correspond to A \ (S ‚à™ S+ ).
It is not hard to see the connection between our algorithms and the theory of Caminada and Gabbay
(2009). For example, in algorithm 1 we capture a preferred extension when all arguments are
mapped to one of those labels: IN, OUT and UNDEC. Listing other works that present labelingbased semantics, the paper of Modgil (2009a) defined labeling-based semantics for the extended
AF s of Modgil (2009b) while Villata, Boella, and van der Torre (2011) described argumentation
semantics in terms of attacks and arguments. Also, the work of Gabbay (2009) set argumentation
semantics for the AF-extended model of Barringer, Gabbay, and Woods (2005) that among other
features allow attacks on attacks. On the topic of extension computation in general, the study of
Li, Oren, and Norman (2012) examined approximation versus exact computations, whereas the
experiments of Baumann, Brewka, and Wong (2012), Liao, Lei, and Dai (2013) evaluated the effect
of splitting an AF on the computation of preferred extensions. The work of Dondio (2013) studied,
under the grounded semantics, how the acceptance status of an argument varies in all the subgraphs
of the given AF. Computational complexity of argumentation semantics are widely studied (see e.g.
Dimopoulos, Nebel, & Toni, 2000; Dunne, 2007, 2009; Ordyniak & Szeider, 2011). Another line of
research concerns encoding computational problems of AFs into other formalisms and then solving
them by using a respective solver (e.g. Besnard & Doutre, 2004; Nieves, Cortes, & Osorio, 2008;
Egly, Gaggl, & Woltran, 2010; Amgoud & Devred, 2011; DvorÃåaÃÅk, JaÃàrvisalo, Wallner, & Woltran,
2012a; Cerutti, Dunne, Giacomin, & Vallati, 2013; Charwat, DvorÃåaÃÅk, Gaggl, Wallner, & Woltran,
2013), such approaches are called reduction based methods. We stress that the focus of this paper
was on algorithmic based implementations of argumentation semantics.

Acknowledgments
We thank the anonymous reviewers for the comments that improved the presentation of this work.

References
Amgoud, L., & Devred, C. (2011). Argumentation frameworks as constraint satisfaction problems.
In Benferhat, S., & Grant, J. (Eds.), SUM, Vol. 6929 of Lecture Notes in Computer Science,
pp. 110‚Äì122. Springer.
Baroni, P., Caminada, M., & Giacomin, M. (2011a). An introduction to argumentation semantics.
The Knowledge Engineering Review, 26(4), 365‚Äì410.
665

N OFAL , ATKINSON , & D UNNE

Baroni, P., Cerutti, F., Giacomin, M., & Guida, G. (2011b). Argumentation framework with recursive attacks. International Journal of Approximate Reasoning, 52(1), 19‚Äì37.
Barringer, H., Gabbay, D., & Woods, J. (2005). Temporal dynamics of support and attack networks:
From argumentation to zoology. In Hutter, D., & Stephan, W. (Eds.), Mechanizing Mathematical Reasoning, Vol. 2605 of Lecture Notes in Computer Science, pp. 59‚Äì98. Springer.
Baumann, R., Brewka, G., & Wong, R. (2012). Splitting argumentation frameworks: An empirical
evaluation. In Modgil, S., Oren, N., & Toni, F. (Eds.), First International Workshop on Theory
and Applications of Formal Argumentation 2011, Vol. 7132 of Lecture Notes in Computer
Science, pp. 17‚Äì31. Springer.
Bench-Capon, T., & Dunne, P. (2007). Argumentation in artificial intelligence. Artificial Intelligence, 171, 619‚Äì641.
Besnard, P., & Doutre, S. (2004). Checking the acceptability of a set of arguments. In Delgrande,
J., & Schaub, T. (Eds.), NMR, pp. 59‚Äì64.
Besnard, P., & Hunter, A. (2008). Elements of Argumentation. MIT press.
Caminada, M. (2007). An algorithm for computing semi-stable semantics. In Mellouli, K. (Ed.),
ECSQARU, Vol. 4724 of Lecture Notes in Computer Science, pp. 222‚Äì234. Springer.
Caminada, M. (2010). An algorithm for stage semantics. In Baroni, P., Cerutti, F., Giacomin, M., &
Simari, G. (Eds.), COMMA, Vol. 216 of Frontiers in Artificial Intelligence and Applications,
pp. 147‚Äì158. IOS Press.
Caminada, M., Carnielli, W., & Dunne, P. (2012). Semi-stable semantics. J. Log. Comput., 22(5),
1207‚Äì1254.
Caminada, M., & Gabbay, D. (2009). A logical account of formal argumentation. Studia Logica,
93(2-3), 109‚Äì145.
Cerutti, F., Dunne, P., Giacomin, M., & Vallati, M. (2013). A sat-based approach for computing
extensions in abstract argumentation. In TAFA, Second International Workshop on Theory
and Applications of Formal Argumentation.
Charwat, G., DvorÃåaÃÅk, W., Gaggl, S., Wallner, J., & Woltran, S. (2013). Implementing abstract argumentation - a survey. Tech. rep. DBAI-TR-2013-82, Technische UniversitaÃàt Wien, Database
and Artificial Intelligence Group.
Dimopoulos, Y., Magirou, V., & Papadimitriou, C. (1997). On kernels, defaults and even graphs.
Annals of Mathematics and Artificial Intelligence, 20, 1‚Äì12.
Dimopoulos, Y., Nebel, B., & Toni, F. (2000). Finding admissible and preferred arguments can
be very hard. In Cohn, A., Giunchiglia, F., & Selman, B. (Eds.), KR, pp. 53‚Äì61. Morgan
Kaufmann.
Dondio, P. (2013). Computing the grounded semantics in all the subgraphs of an argumentation
framework: an empirical evaluation. In CLIMA, XIV Workshop on Computational Logic in
Multi-Agent Systems.
Doutre, S., & Mengin, J. (2001). Preferred extensions of argumentation frameworks: Query answering and computation. In GoreÃÅ, R., Leitsch, A., & Nipkow, T. (Eds.), IJCAR, Vol. 2083 of
Lecture Notes in Computer Science, pp. 272‚Äì288. Springer.
666

A LGORITHMS FOR A RGUMENTATION S EMANTICS

Dung, P. (1995). On the acceptability of arguments and its fundamental role in non monotonic
reasoning, logic programming and n-person games. Artificial Intelligence, 77(2), 321‚Äì357.
Dung, P., Mancarella, P., & Toni, F. (2007). Computing ideal skeptical argumentation. Artificial
Intelligence, 171(10-15), 642‚Äì674.
Dunne, P. (2007). Computational properties of argument systems satisfying graph-theoretic constraints. Artificial Intelligence, 171, 701‚Äì729.
Dunne, P. (2009). The computational complexity of ideal semantics. Artificial Intelligence, 173(18),
1559 ‚Äì 1591.
DvorÃåaÃÅk, W., JaÃàrvisalo, M., Wallner, J. P., & Woltran, S. (2012a). Complexity-sensitive decision
procedures for abstract argumentation. In Brewka, G., Eiter, T., & McIlraith, S. (Eds.), KR.
AAAI Press.
DvorÃåaÃÅk, W., Pichler, R., & Woltran, S. (2012b). Towards fixed-parameter tractable algorithms for
abstract argumentation. Artificial Intelligence, 186, 1‚Äì37.
Egly, U., Gaggl, S., & Woltran, S. (2010). Answer-set programming encodings for argumentation
frameworks. Argument and Computation, 1(2), 147‚Äì177.
Gabbay, D. (2009). Semantics for higher level attacks in extended argumentation frames part 1:
Overview. Studia Logica, 93(2-3), 357‚Äì381.
Li, H., Oren, N., & Norman, T. (2012). Probabilistic argumentation frameworks. In Modgil, S.,
Oren, N., & Toni, F. (Eds.), First International Workshop on Theory and Applications of
Formal Argumentation 2011, Vol. 7132 of Lecture Notes in Computer Science, pp. 1‚Äì16.
Springer.
Liao, B., Lei, L., & Dai, J. (2013). Computing preferred labellings by exploiting sccs and most
sceptically rejected arguments. In TAFA, Second International Workshop on Theory and Applications of Formal Argumentation.
Modgil, S. (2009a). Labellings and games for extended argumentation frameworks. In Boutilier, C.
(Ed.), IJCAI, pp. 873‚Äì878.
Modgil, S. (2009b). Reasoning about preferences in argumentation frameworks. Artificial Intelligence, 173, 901‚Äì934.
Modgil, S., & Caminada, M. (2009). Proof theories and algorithms for abstract argumentation
frameworks. In Rahwan, I., & Simari, G. (Eds.), Argumentation in Artificial Intelligence, pp.
105‚Äì129. Springer.
Modgil, S., Toni, F., Bex, F., Bratko, I., ChesnÃÉevar, C., DvorÃåaÃÅk, W., Falappa, M., Fan, X., Gaggl, S.,
Garcƒ±ÃÅa, A., GonzaÃÅlez, M., Gordon, T., Leite, J., MozÃåina, M., Reed, C., Simari, G., Szeider, S.,
Torroni, P., & Woltran, S. (2013). The added value of argumentation. In Ossowski, S. (Ed.),
Agreement Technologies, Vol. 8 of Law, Governance and Technology Series, pp. 357‚Äì403.
Springer Netherlands.
Nieves, J., Cortes, U., & Osorio, M. (2008). Preferred extensions as stable models. Theory and
Practice of Logic Programming, 8(4), 527‚Äì543.
Nofal, S., Atkinson, K., & Dunne, P. (2014). Algorithms for decision problems in argument systems
under preferred semantics. Artif. Intell., 207, 23‚Äì51.
667

N OFAL , ATKINSON , & D UNNE

Ordyniak, S., & Szeider, S. (2011). Augmenting tractable fragments of abstract argumentation. In
Walsh, T. (Ed.), Proceedings of the 22nd International Joint Conference on Artificial Intelligence IJCAI 2011, pp. 1033‚Äì1038.
Rahwan, I., & Simari, G. (2009). Argumentation in Artificial Intelligence. Springer.
Verheij, B. (1996). Two approaches to dialectical argumentation: admissible sets and argumentation
stages. In Proceedings of the Eighth Dutch Conference on AI, pp. 357‚Äì368.
Villata, S., Boella, G., & van der Torre, L. (2011). Attack semantics for abstract argumentation. In
Walsh, T. (Ed.), Proceedings of the 22nd International Joint Conference on Artificial Intelligence IJCAI 2011, pp. 406‚Äì413.

668

Journal of Artificial Intelligence Research 49 (2014) 733-773

Submitted 07/13; published 04/14

Comparative Evaluation of Link-Based Approaches for
Candidate Ranking in Link-to-Wikipedia Systems
Norberto FernaÃÅndez Garcƒ±ÃÅa
JesuÃÅs Arias Fisteus
Luis SaÃÅnchez FernaÃÅndez

berto@it.uc3m.es
jaf@it.uc3m.es
luiss@it.uc3m.es

Telematics Engineering Department
Universidad Carlos III de Madrid
Avda. Universidad, 30, E-28911
LeganeÃÅs, Madrid, Spain.

Abstract
In recent years, the task of automatically linking pieces of text (anchors) mentioned in
a document to Wikipedia articles that represent the meaning of these anchors has received
extensive research attention. Typically, link-to-Wikipedia systems try to find a set of
Wikipedia articles that are candidates to represent the meaning of the anchor and, later,
rank these candidates to select the most appropriate one. In this ranking process the
systems rely on context information obtained from the document where the anchor is
mentioned and/or from Wikipedia. In this paper we center our attention in the use of
Wikipedia links as context information. In particular, we offer a review of several candidate
ranking approaches in the state-of-the-art that rely on Wikipedia link information. In
addition, we provide a comparative empirical evaluation of the different approaches on
five different corpora: the TAC 2010 corpus and four corpora built from actual Wikipedia
articles and news items.

1. Introduction
Due to the important volume of information contained in Wikipedia, but also to the open
nature of this content, the on-line encyclopedia has been adopted in recent times as a useful
resource for computational linguistics tasks like name translation (Lin, Snover, & Ji, 2011),
named entity recognition (Nothman, Murphy, & Curran, 2009), etc.
The development of automatic link discovery systems (Erbs, Zesch, & Gurevych, 2011)
is another area of research where Wikipedia has had an important impact. The task of
discovering links to Wikipedia articles has been addressed, with slight variants and under
different names, by different communities. For instance, Hachey et al. (2013) distinguish
between named entity linking, addressed in the context of the Knowledge Base Population
(KBP) track (National Institute of Standards and Technology, 2014b) at the Text Analysis
Conference (TAC) (National Institute of Standards and Technology, 2014a), and wikification, addressed in the Link-the-Wiki track at the Initiative for the Evaluation of XML
retrieval (INEX) (INEX, 2014). In both cases the goal is to automatically find Wikipedia
articles that represent the meaning of a certain piece of text in the document and define
a link to Wikipedia using as anchor that piece of text. However, there are differences in
aspects like the anchors considered (only named entities in named entity linking, named
c
2014
AI Access Foundation. All rights reserved.

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

entities and common terms in wikification) or in whether the Wikipedia is considered as a
complete source of knowledge (wikification) or not (named entity linking).
Henceforth, we will simply refer as link-to-Wikipedia to the general task of discovering
links to Wikipedia, which includes both wikification and named entity linking as particular
cases.
According to Erbs et al. (2011), the task of discovering links can be divided into a series
of steps. They include: identifying the anchors to be linked, searching for candidate link
targets for each anchor, and selecting the best candidate from the results of the searching
step. It is common that link-to-Wikipedia approaches address these steps independently
and sequentially (though there are also examples where the steps are not independent,
like Cucerzan, 2012 or Sil, 2013).
Due to its important role (Ji, Grishman, & Dang, 2011), in the context of this paper
we will center our attention in the last of the aforementioned processes. It is referred
to as disambiguation by Hachey et al. (2011). However, as selecting the best link target
usually involves creating a ranking of all the candidates to choose the one with the highest
rank, other authors refer to the process as target ranking (Erbs et al., 2011) or candidate
ranking (Guo, Tang, Che, Liu, & Li, 2011; Ploch, Hennig, de Luca, & Albayrak, 2011; Ji
et al., 2011). In this article, we will also adopt the term candidate ranking.
In order to select the best Wikipedia article to link from a given anchor, the candidate
ranking process relies on the context information provided by a set of features. These
features are extracted from the document where the anchor is placed (the context document)
and/or the different Wikipedia articles considered as candidates to become the link target.
According to Erbs et al. (2011) the features can be classified into three groups: (1) those
extracted from the text of the document/articles, (2) those extracted from their titles; and
(3) those based on existing links. The latter are the subject of study in this paper.
Traditional research in the area of computational linguistics has shown the effectiveness of using WordNet (Miller, 1995) graph links for tasks like computing semantic relatedness (Budanitsky & Hirst, 2006) or performing word sense disambiguation (Navigli &
Lapata, 2010). In the case of link discovery, Erbs et al. (2011) indicate that, when enough
training information is available, link-based approaches can outperform text-based ones.
Taking this into account, it is not surprising to find many link-to-Wikipedia approaches
that use features for candidate ranking based only on information about links. Some examples are the work of Milne and Witten (2008b), Pilz (2010), Radford et al. (2010), FernaÃÅndez
et al. (2010), Guo et al. (2011), Ploch et al. (2011) and Ratinov et al. (2011).
Given the ample variety of link-based features for candidate ranking described in the
state of the art, a comparative analysis of the different alternatives can be useful to decide
which approach (or approaches) should be considered when designing link-to-Wikipedia
systems. However, using only the results published in the state of the art it is difficult
to compare across systems the ranking performance of the different link-based approaches.
First, link-to-Wikipedia systems are usually evaluated in an end-to-end setup, that is, the
evaluation involves not only the ranking stage, but also the candidate searching and the
candidate selection processes. Thus, the impact in performance of the different system
components is mixed. Second, in general, link-to-Wikipedia systems do not only rely on
link-based features to rank the candidates, but also combine them with features of the other
types. Thus, the effects of the different contributions to the ranking process are also mixed.
734

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

Taking this into account, the main goals of this paper are twofold: (1) offer an overview of
link-based approaches for candidate ranking in link-to-Wikipedia systems; and, (2) perform
an empirical evaluation to compare these approaches. In order to address the aforementioned difficulties, we: (1) focus our analysis on the candidate ranking stage, isolating it
as much as possible from the candidate search/selection stages; and (2) consider only linkbased features, not combined with those based on text or titles. A similar comparison is,
to the knowledge of the authors, not available at the time of writing.
The rest of this paper is organized as follows: section 2 presents some definitions and
a formal description of the problem to be addressed. Section 3 outlines the different linkbased approaches to be compared. Section 4 describes the setup of the empirical evaluation
we have carried out, as well as its results. Section 5 offers an overview of related work.
Finally, section 6 closes this paper with concluding remarks and future lines of work.

2. Definitions and Problem Formalization
In this section we introduce some definitions and nomenclature that will be helpful in the
rest of the article.
The textual document that mentions the anchor a that is going to be linked to Wikipedia
is named context document and will be represented by dc .
The set of all the Wikipedia articles will be denoted by W , whereas particular Wikipedia
articles will be represented by wi , i = 1, . . . , |W |. In our case we will consider in the set W
only the Wikipedia pages that belong to the Main namespace (Wikipedia, 2014b) and that
represent non-ambiguous concepts (that is, disambiguation pages will be filtered out).
We will denote as C(dc , a) the set of Wikipedia articles {c1 , c2 , . . . , c|C(dc ,a)| }, ck ‚àà W
that are selected as candidates to fit the meaning of a in dc .
A link l can be defined as a duple l = (src(l), dest(l)), where src(l) represents the
document that is the source of the link and dest(l) is the document that is pointed by the
link, that is, its destination.
We will denote as F (d) the set of documents that are destination of the forward links
from d, that is: F (d) = {f | ‚àÉl, src(l) = d ‚àß dest(l) = f }. Similarly, we will represent as
B(d) the set of documents that are the source of the backward links of d, that is: B(d) =
{b | ‚àÉl, src(l) = b ‚àß dest(l) = d}. In this paper, we use only the information provided
by Wikipedia links. Thus, we will only consider Wikipedia articles as members of F (d)
and B(d). Note also that both F (d) and B(d) are sets and, thus, they do not consider
duplicates. However, it might happen that a document has several links pointing to the
same destination. In order to represent this information, we will denote the number of links
that have as source the document s and as destination the document d as n(s, d) .
Taking into account the aforementioned definitions, the candidate ranking process to be
addressed in the context of this paper may be formalized as follows:
Definition 1. Given a context document dc , which mentions an anchor a, and a set of
candidate Wikipedia articles C(dc , a), the candidate ranking task consists on ordering the
members of C(dc , a) according to a rating. This rating measures the suitability of each
candidate to represent the meaning of the anchor. The candidate ci ‚àà C(dc , a) with the
highest rank, which fits best with the meaning of the anchor a in the context of the document
dc , is then selected to define a new link (dc , ci ).
735

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

A few aspects should be stressed from this definition:
‚Ä¢ We do not introduce any restriction on the nature of the anchors to be linked. In
particular, they may represent either named entities (persons, organizations, etc.) or
common terms.
‚Ä¢ As in some previous related work (Cucerzan, 2007; Mihalcea & Csomai, 2007; Han,
Sun, & Zhao, 2011), we do not address in this paper the scenario where an adequate
Wikipedia article to be linked to a does not exist.

3. Overview of Approaches
We describe here the different candidate ranking approaches that are evaluated in this
paper. They have in common that their only source of context information are links. In
particular, the links considered are those available in the context document, dc , as well
as those that constitute the link structure of Wikipedia, including the links to/from the
candidate Wikipedia articles, ci ‚àà C(dc , a).
However, not all the approaches that are considered use the link information in the
same manner. In particular, we classify them in two families, which we name (1) bag-oflinks approaches, and (2) graph approaches. The main difference between them is that in
the second group the links are used to build a graph structure, which is later analyzed to
select the best candidate. This is not the case of the approaches in the first group.
3.1 Bag-of-Links Approaches
In this section we present a set of approaches with a characteristic in common: they do not
rely on building a graph with the link context information to rank the candidates.
Nevertheless, the approaches in this family have also differences in the way they address
the task. In particular, we can distinguish at least three alternative groups:
‚Ä¢ Some approaches rely on similarity metrics that compute a similarity score between
the context document and each candidate, to later select the candidate with the
highest score (the most similar one).
‚Ä¢ Another alternative is to rely on popularity metrics, which simply try to compute a
popularity score for each candidate. The most popular candidate is selected. These
approaches do not rely on the information provided by the context document.
‚Ä¢ The ranking process can also be modeled as a statistical problem. Statistical methods
are used to select the most likely candidate, given the context information.
In accordance with this classification, the following sections describe each group of approaches.
3.1.1 Similarity Metrics
The first similarity metrics we consider is the relatedness, computed on the basis of the
Wikipedia Link-based Measure (Milne & Witten, 2008a). The relatedness is used as feature
736

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

in link-to-Wikipedia approaches such as that of Milne and Witten (2008b), Han and Zhao
(2009), Kulkarni et al. (2009), Pilz (2010), Fahrni et al. (2011), Han et al. (2011) and Ratinov
et al. (2011).
Basically, the relatedness allows to compute the similarity between two Wikipedia documents wi , wj from the links they have in common. In the original definition by Milne and
Witten (2008a), it can be computed as:
RB (wi , wj ) =

log(max{|B(wi )|, |B(wj )|}) ‚àí log(|B(wi ) ‚à© B(wj )|)
log(|W |) ‚àí log(min{|B(wi )|, |B(wj )|})

(1)

According to Milne and Witten (2008a), the relatedness metrics is based on the Normalized Google Distance (NGD), defined by Cilibrasi and Vitanyi (2007). The NGD is based
on the intuition that terms that have a similar or related meaning co-occur frequently in
documents. Thus, given a pair of terms, the Google search engine can be used to obtain
pages which mention these terms. Pages that mention both of them indicate relatedness,
while pages with only one of them suggest unrelatedness. As indicated by Milne and Witten
(2008a), the relatedness metrics, as defined in equation 1, shares the same inspiring principle, but uses Wikipedia links instead of Google search results to account for mentions.
Being a distance metrics, relatedness values are expected to be smaller the more similar
the Wikipedia articles are. However, it is easy to transform the distance metrics into a
similarity metrics following the approach of Gracia and Mena (2008), which requires the
computation of:
simRB (wi , wj ) = e‚àí2RB (wi ,wj )

(2)

A second similarity metrics between Wikipedia articles to be considered is based on
computing the Pointwise Mutual Information (PMI) between the sets of links in the articles
to be compared. For instance, it is used by Ratinov et al. (2011) for the the link-to-Wikipedia
task. It is defined in that work as:
PB (wi , wj ) =

|B(wi ) ‚à© B(wj )|/|W |
(|B(wi )|/|W |)(|B(wj )|/|W |)

(3)

Note that the definitions of equations (1) and (3) rely on backlinks (B(x)) for computation. However, as indicated by Ratinov et al. (2011), both the relatedness and the PMI
can also be computed using the outgoing links from a document. In this paper we explore
and compare both alternatives and denote the relatedness similarity and the PMI computed
with forward links as simRF and PF respectively.
Taking the aforementioned definitions into account, for each Wikipedia article {c1 , . . . , ck }
‚àà C(dc , a) we can compute its relatedness and PMI with each of the Wikipedia articles linked
from dc , that is, with the fj ‚àà F (dc ). Combining these different values we obtain the final
relatedness and PMI between ci and dc . According to Ratinov et al. (2011) several ways
to combine the values may be followed, such as taking their average or the maximum. We
explore these different possibilities in the paper. In particular, for the relatedness:
737

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

RelFA (ci , dc ) =
A
RelB
(ci , dc ) =

1
|F (dc )|
1
|F (dc )|

RelFM (ci , dc )

=

M
RelB
(ci , dc ) =

X

simRF (ci , fj )

(4)

X

simRB (ci , fj )

(5)

max

simRF (ci , fj )

(6)

max

simRB (ci , fj )

(7)

‚àÄfj ‚ààF (dc )

‚àÄfj ‚ààF (dc )
‚àÄfj ‚ààF (dc )
‚àÄfj ‚ààF (dc )

Whereas the Pointwise Mutual Information can be computed as:
P M IFA (ci , dc ) =

1
|F (dc )|

1
A
P M IB
(ci , dc ) =
|F (dc )|
P M IFM (ci , dc )

=

M
P M IB
(ci , dc ) =

X

PF (ci , fj )

(8)

X

PB (ci , fj )

(9)

max

PF (ci , fj )

(10)

max

PB (ci , fj )

(11)

‚àÄfj ‚ààF (dc )

‚àÄfj ‚ààF (dc )
‚àÄfj ‚ààF (dc )
‚àÄfj ‚ààF (dc )

Another well-known approach to compute document similarity within the natural language processing and information retrieval communities is the cosine similarity. Basically, a
vector is built to represent the context document and each candidate article. Then, the similarity between the context document and each candidate is computed as the cosine of the
angle between the respective vectors. Several approaches in the state of the art (Bunescu
& Pasca, 2006; Fader, Soderland, & Etzioni, 2009; Nguyen & Cao, 2010; Fahrni et al.,
2011; Ploch et al., 2011; Ratinov et al., 2011) use the cosine similarity. However, there are
differences between them in the features used to build the vector representations for the
documents.
In our case, we have the requirement of considering solely links as context information.
Thus, each document will be represented using only the links that are mentioned in that
document. A similar approach to model documents and compute their cosine similarity is
used, for instance, by Fahrni et al. (2011) and Ploch et al. (2011).
In particular, a document d is represented as a vector vd ‚àà R|W | . Each component
vd,i , i = 1, . . . , |W | of the vector vd can be computed with the traditional term frequency
(TF), inverse document frequency (IDF) product (Manning, Raghavan, & Schtze, 2008) as
follows:
vd,i = T F (d, wi ) ¬∑ IDF (wi ) = P

|W |
n(d, wi )
¬∑ log
|B(wi )|
‚àÄwj ‚ààF (d) n(d, wj )

(12)

Note that if F (d) does not contain a certain Wikipedia article wi , then n(d, wi ) = 0,
T F (d, wi ) = 0 and, thus, vd,i = 0. Due to this, the vector vd is expected to be sparse.
738

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

Given two documents to be compared (for instance, dc and a Wikipedia article ci ‚àà
C(dc , a)), the cosine similarity metrics is computed as the cosine of the angle between the
vectors of the two documents, as follows:
simcos (vci , vdc ) =

vdc ¬∑ vci
||vdc ||2 ||vci ||2

(13)

Finally, Radford et al. (2010) suggest a metrics based on the Wikipedia link structure,
which can also be interpreted as a similarity metrics. In order to compute this metrics, the
following equation is computed for each candidate ci ‚àà C(dc , a):
simR (ci , dc ) = log(|B(ci ) ‚à© Ldc | + 1) + 1

(14)

where Ldc is the set built by the union of all the backlinks of all the Wikipedia articles
linked from dc :
Ldc =

[

B(fi )

(15)

‚àÄfi ‚ààF (dc )

All the aforementioned similarity metrics can be trivially used to address the candidate
ranking process. The candidate ci ‚àà C(dc , a) to be selected as link destination has a
maximal similarity with the context document dc :
arg max{simf (ci , dc )}
ci

(16)

A , RelM , P M I A , P M I M ,
where simf represents one the functions: RelFA , RelFM , RelB
B
F
F
A , P M I M , sim
P M IB
cos , simR .
B

3.1.2 Popularity Metrics
Algorithms based on popularity metrics constitute the second group of the bag-of-links
family.
A first approach that could be used to compute the popularity of a certain candidate,
ci ‚àà C(dc , a), is simply counting the number of Wikipedia articles that link to it, that is,
its indegree, |B(ci )| or, alternatively, the number of Wikipedia articles linked from it, its
outdegree, |F (ci )|. These metrics are considered, for instance, in the work of Dredze et al.
(2010), Guo et al. (2011) and Cao et al. (2011).
Fader et al. (2009) describe a popularity score that is also based on the incoming links
from Wikipedia to a candidate ci :
|B(ci )|
))
(17)
Œ±
where Œ± is a parameter that is set to Œ± = 15 (Fader et al., 2009).
Finally, the degree centrality of a certain Wikipedia candidate article ci can also be
considered as a bag-of-links popularity metrics. Hachey et al. (2011) define the degree
centrality as:
popF (ci ) = (1 + log(1 +

D(ci ) =

|B(ci )|
|W | ‚àí 1

739

(18)

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

Note that, as indicated at the beginning of this section, the aforementioned popularity
metrics do not take into account the information provided by the context document. All of
them depend only on information obtained from the candidates.
All the aforementioned metrics can be used to rank the candidates by popularity. Then,
the most popular candidate ci ‚àà C(dc , a) is selected as the link destination. Taking into
account that all the functions of |B(ci )| involved in the aforementioned approaches (linear,
logarithm) are monotonically increasing functions, the order (ranking) provided in all the
cases should be the same. Due to this, in the context of this paper, we consider for evaluation
the indegree and outdegree only:

arg max{indegree(ci )}

(19)

arg max{outdegree(ci )}

(20)

ci

ci

3.1.3 Statistical Techniques
The candidate ranking process that we are addressing in the context of this paper can also
be mathematically modeled using statistical techniques, as has been suggested in the work
of Fader et al. (2009) and Han and Sun (2011).
In our particular case, considering the set of Wikipedia articles linked from dc , F (dc ) =
{f1 , . . . , f|F (dc )| } as input features, the destination for a can be computed by selecting the
Wikipedia article ci ‚àà C(dc , a) that maximizes the conditional probability:
P (ci /f1 , . . . , f|F (dc )| ) where fi ‚àà F (dc )

(21)

If the number of features |F (dc )| to be considered is relatively large, estimating the values
of the conditional probability in equation (21) for each ci would be a complex problem. Due
to this, in practice, the problem is reformulated to make it more treatable. In particular:
(1) the Bayes rule is used to reverse the conditional probability in equation (21); and, (2)
it is assumed that the features (links in F (dc ) in our case) are conditionally independent
(Naive Bayes assumption).
The result of this problem reformulation is known in the state of the art as the Naive
Bayes classifier (Manning et al., 2008). In our specific scenario, this classifier should be
able to distinguish which of the classes (the different ci ‚àà C(dc , a)) is the most likely for
the anchor a.
Mathematically, the expression that we use to select the best ci using the maximum a
posteriori decision rule (Manning et al., 2008) is:
|F (dc )|

arg max{N B(ci , dc )} = arg max{log P (ci ) +
ci

ci

X

n(dc , fj ) log P (fj /ci )}

(22)

j=1

where the logarithm function is used to avoid underflows (as suggested in Manning et al.,
2008).
In order to compute the values in equation (22) for each ci , we need to know the value
of two probabilities: (1) the prior probability of class ci , P (ci ); and, (2) the conditional
740

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

probabilities P (fj /ci ). To estimate these two probabilities we follow the approach described
by Manning et al. (2008):
P
‚àÄbj ‚ààB(ci ) n(bj , ci )
P
(23)
P (ci ) = P
‚àÄwi ‚ààW
‚àÄfj ‚ààF (wi ) n(wi , fj )

That is, P (ci ) represents the Maximum Likelihood Estimate (MLE) of the probability
that a certain document contains a link to ci , computed by dividing the number of actual
links to ci by the total number of links in Wikipedia:
P
1 + ‚àÄbi ‚ààB(ci ) n(bi , fj )
P
P (fj /ci ) = P
(24)
‚àÄwj ‚ààW (1 +
‚àÄbi ‚ààB(ci ) n(bi , wj ))

In this case, P (fj /ci ) represents the probability of having an anchor linking to fj when
the document already contains a link to ci . Again, the MLE is also used for the conditional
probabilities and, thus, the probabilities are computed by dividing the number of links to
fj in documents that contain a link to ci by the total number of links in documents that
contain a link to ci . It can be seen that the MLE is smoothed using the Laplace smoothing
to avoid zeros.
3.2 Graph Approaches
The second family of link-based approaches for candidate ranking we consider are the graph
approaches, which rely on building a graph and processing it to select the best candidate.
3.2.1 PageRank and Personalized PageRank
The first algorithm that we consider within the graph family is PageRank, first defined
by Page et al. (1999), and widely known due to its use as part of the Google search engine.
Examples of application of PageRank as a method for candidate ranking can be found for
instance in the work of FernaÃÅndez et al. (2010), Dredze et al. (2010) and Hachey et al.
(2011).
Basically, PageRank is an algorithm that can be used to compute the popularity of a
certain page, taking into account the popularity and number of pages that link to it. Using
the mathematical formulation described by Brin and Page (1998) in the particular scenario
that we are addressing in this paper, to compute the popularity P R(wi ) of a Wikipedia
article wi , we need to solve the following equation:
P R(wi ) =

(1 ‚àí d)
+d¬∑[
|W |

X

‚àÄwj ‚ààB(wi )

1
P R(wj ) ]
|F (wj )|

(25)

Where d is a damping factor which can be set between 0 and 1, but is typically set to
0.85 according to Brin and Page (1998) and Hachey et al. (2011).
Note that, according to equation (25), we are only taking into account links to/from
Wikipedia, because computing PageRank in the general scenario requires complete information of the link structure of the Web, which is a computationally expensive problem. The
same simplification is also assumed by FernaÃÅndez et al. (2010) and Hachey et al. (2011).
741

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

Note also that, as it happens with the popularity metrics described in section 3.1.2, the
PageRank metrics does not depend on the context information in dc , but only on the graph
built from the link structure of Wikipedia. However, the context information in dc can be
included in the process by using a variant of the algorithm known as Personalized PageRank
or Topic-Sensitive PageRank (Haveliwala, 2003). This algorithm is used for instance by Yeh
et al. (2009) to define a semantic relatedness metrics.
The main difference between classical PageRank and Personalized PageRank is that,
instead of relying on a uniform damping vector, it is biased to give more relevance to a
given set of resources (Haveliwala, 2003). In our particular case, these resources are the
articles linked from dc , that is, the members of F (dc ). In practice, equation (25) is adapted
as follows to compute the Personalized PageRank:

P P R(wi , dc ) =

Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤

d¬∑[

X

1
P P R(wj , dc )] if wi ‚àà
/ F (dc )
|F (wj )|

X

1
P P R(wj , dc )] if wi ‚àà F (dc )
|F (wj )|

‚àÄwj ‚ààB(wi )

Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥ (1 ‚àí d) ¬∑ T F (dc , wi ) + d ¬∑ [

‚àÄwj ‚ààB(wi )

where T F (dc , wi ) represents the term frequency of the link to wi in the context of the
document dc , computed as indicated in equation (12).
Once the PageRank and Personalized PageRank values are computed, they can be used
to rank the candidates. The article ci ‚àà C(dc , a) with the highest P R(ci ) or P P R(ci , dc )
value is then selected as the link destination:

arg max{P R(ci )}

(26)

arg max{P P R(ci , dc )}

(27)

ci

ci

3.2.2 Random Walk
Several works in the state of the art (Gentile et al., 2009; FernaÃÅndez et al., 2010; Han et al.,
2011; Ploch et al., 2011; JimeÃÅnez et al., 2013) define techniques to link several anchors in the
same context document at the same time. Usually, these approaches address the candidate
ranking process by building a graph and computing a random walk (Spitzer, 1976) over that
graph to rank its nodes.
Though these approaches share the same underlying principle, there are differences
between them mainly in two aspects: the nature of the nodes to be considered as part of
the graph and the nature of the edges. For instance, Gentile et al. (2009) indicate that
the nodes represent either concepts (candidates) or features (like words in the title of a
certain candidate), and the edges link the candidates with their specific features. Han et al.
(2011) define nodes for each anchor to be linked and for its candidates. The edges link each
anchor with its candidates but also the candidates among themselves on the basis of their
relatedness (see section 3.1.1). In the work of FernaÃÅndez et al. (2010) the nodes include only
the candidates, and the edges are defined on the basis of information about co-occurrence
742

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

of candidates in Wikipedia articles. A similar approach is used by Ploch et al. (2011),
including nodes for the candidates and edges defined on the basis of Wikipedia links.
Note that the PageRank metrics can also be interpreted as a random walk (Page et al.,
1999). However, while PageRank, as described in section 3.2.1, operates on the graph of the
whole link structure of Wikipedia, the approaches in this section build their own graphs,
typically much smaller and tailored to the concrete scenario to be addressed.
The approaches of Gentile et al. (2009) and Han et al. (2011) rely on text-based features
to build their graphs: in the work of Gentile et al. (2009) these features are used as nodes
in the graph, whereas Han et al. (2011) use a text-based similarity metrics to compute the
weights of the edges connecting each anchor with its candidates. Due to this, in the context
of this paper we evaluate the approaches of FernaÃÅndez et al. (2010) and Ploch et al. (2011),
which rely only on link information.
As indicated above, FernaÃÅndez et al. (2010) and Ploch et al. (2011) designed their
approaches to link at the same time several anchors in the same context document. Thus,
we need to adapt these approaches to the scenario addressed in this paper, where only
an anchor a is considered. To do so, each element in F (dc ) is treated as a single-element
pseudo-candidate set for an anchor ai in dc with i = 1, . . . , |F (dc )|.
To compute the score for each candidate ci ‚àà C(dc , a) according to Ploch et al. (2011)
(that we name RWP (ci , dc ))) we build a graph having as nodes all the candidates and
pseudo-candidates (that is, the elements in C(dc , a) plus the Wikipedia articles linked in
F (dc )). An edge between two nodes appears when there is a link in Wikipedia between
the articles represented by the nodes. Once the graph is built, the PageRank algorithm is
applied to this graph. The score assigned to each node is its PageRank value.
A similar approach is used in the case of FernaÃÅndez et al. (2010). Again, the nodes
include all the candidates and pseudo-candidates, but in this case the edges represent cooccurrences. In particular, there is an edge from node wi to node wj when:
1. There is at least a third Wikipedia article wk that links both to wi and wj , that is,
wi , wj ‚àà F (wk ). These edges are assigned weights according to:
weightC (wi ‚Üí wj ) =

|B(wi ) ‚à© B(wj )|
|B(wi )|

(28)

2. A direct link exists between wi and wj , that is, wj ‚àà F (wi ). These edges are assigned
weights as follows:
weightL (wi ‚Üí wj ) = T Fij IDFj = P

n(wi , wj )
|W |
log
|B(wj )|
‚àÄwk ‚ààF (wi ) n(wi , wk )

(29)

When two nodes wi and wj match both the conditions above, that is, they are directly
linked and they co-occur in a third article wk , a single edge is created that combines the
contributions as follows:

weight(wi ‚Üí wj ) =

kL
kC
weightC (wi ‚Üí wj ) +
weightL (wi ‚Üí wj )
kC + kL
kC + kL
743

(30)

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

where kL and kC are configuration parameters. In this case we will use the values
kL = 0.55 and kC = 0.25 as suggested by FernaÃÅndez et al. (2010).
Once the weighted, directed graph is built, the PageRank is computed for this graph.
The score for each candidate ci ‚àà C(dc , a), named RWF (ci , dc ), is the PageRank value of
the candidate node in the graph. When the scores of all the candidates are computed, the
candidate with highest score is selected as the best one:
arg max{RWP (ci , dc )}

(31)

arg max{RWF (ci , dc )}

(32)

ci

ci

Note that, in all the approaches listed in section 3, it might happen that different
members of the candidates set obtain the same weight and, thus, there would be a tie in the
ranking. We have used the most frequently linked (MFL) algorithm to break these potential
ties. This algorithm simply assigns a weight to each candidate according to its total number
of incoming links, that is:
M F L(ci , dc ) =

X

n(bj , ci )

(33)

‚àÄbj ‚ààB(ci )

4. Comparative Evaluation
This section reports the results of the evaluation of the different approaches described in
section 3. It is organized as follows: the experimental setup (Wikipedia dataset, corpora,
etc.) used for the evaluation is outlined in section 4.1, whereas section 4.2 reports the
quantitative results as well as some analysis and interpretation of these results.
4.1 Experimental Setup
In order to evaluate the approaches described in section 3, we need a set of elements: (1)
corpora of queries to evaluate the approaches; (2) information about the Wikipedia link
structure that will be used as input by the different approaches; and (3) adequate metrics
to measure and compare the performance of each approach. The next sections describe
these three elements briefly:
4.1.1 Corpora of Queries
In order to evaluate the different approaches, we need corpora containing link-to-Wikipedia
queries. According to the definition of the problem (see section 2) each of the queries in
these corpora should provide:
‚Ä¢ The anchor a that is going to be linked.
‚Ä¢ The context document dc in which the anchor a appears. The links in this document
provide the context information used by some algorithms.
‚Ä¢ A set of candidates, C(dc , a), with Wikipedia articles that are potential targets for
the anchor.
744

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

‚Ä¢ A golden standard that indicates the correct answer (candidate in C(dc , a) to be ranked
at the top) for each query. This golden standard is used to compute the performance
of the algorithms evaluated.
In the state of the art, we distinguish different approaches regarding the corpora they
use for empirical evaluation. A first approach is to build specific corpora. It is followed by
early work (Bunescu & Pasca, 2006; Cucerzan, 2007; Mihalcea & Csomai, 2007), as well
as by more recent work (Milne & Witten, 2008b; Nguyen & Cao, 2010; Pilz, 2010). A
common approach within this first group is to use as corpus a subset of Wikipedia articles
and compare the links suggested by automatic algorithms with those provided by Wikipedia
editors (see for instance Bunescu & Pasca, 2006; Cucerzan, 2007; Milne & Witten, 2008b;
Nguyen & Cao, 2010 and Pilz, 2010). This methodology has also been used in the context of
the INEX Link-the-wiki track (Huang, Xu, Trotman, & Geva, 2008). A second alternative
is to use already available corpora, like the TAC/KBP corpus (used for instance in Han &
Sun, 2011 and Hachey et al., 2011), or the corpora defined by Cucerzan (2007) (used for
instance in Gentile et al., 2009 and Ratinov et al., 2011).
In the context of this paper, we adopt both approaches. In particular, we use the
following corpora in our comparative evaluation:
‚Ä¢ Cucerzan In the work of Cucerzan (2007) the authors use two different corpora, one
built from Wikipedia articles and the other from manually annotated MSNBC (MSNBC,
2014) news items. We have used these corpora to build our own. In order to do so,
we proceeded as follows:
1. The documents in the Cucerzan corpora contain a set of pairs {anchor, Wikipedia
article}, each one representing a potential link-to-Wikipedia query. We select
randomly 250 pairs from each of the Cucerzan‚Äôs corpora. These pairs provide us
with the anchor a to be linked and the golden standard (correct answer to the
query).
2. A typical approach among the systems in TAC/KBP to generate the candidate
set, C(dc , a), is to rely on information retrieval techniques (Ji et al., 2011). In
this paper we adopt this approach. However, as a difference with the TAC/KBP
scenario, where the evaluation involves all the stages of entity linking, we center
our evaluation only on the candidate ranking stage. Due to this, we are interested
in isolating as much as possible this stage from the potential bad performance
of a particular candidate search implementation. That is, we are interested in
analyzing the performance of different candidate ranking approaches assuming
that the candidate search stage is ideal, in the sense that it always returns the
correct candidate among the candidate set. Obviously, there does not exist
an ideal candidate searcher. Thus, in practice, we rely on a state of the art
search engine (Google) and append the correct answer to the candidate set in
case it is not found by the search engine. In particular, we query the Google
search engine with the text of the anchor and a site:en.wikipedia.org restriction,
filtering out from the top-10 Google results the Wikipedia pages not included in
the Main namespace. In case the correct Wikipedia article to be linked is not
745

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

included within the Google result set, it is appended at the end, though this
only happens in a very limited number of queries: in the Curcerzan Wikipedia
corpus the correct candidate was added in 9 out of 250 queries (3.6%), while in
the Curcerzan news corpus it was added in 14 out of 250 queries (5.6%).
3. Finally, from the rest of the pairs {anchor, Wikipedia article} included in the
document where the query has been selected, we obtain the Wikipedia article
component to be used as context information (links in F (dc )), filtering out the
links to articles that are included in the candidate set in order to avoid bias.
‚Ä¢ Ad-hoc corpora Two ad-hoc corpora have been used in the evaluation. One corpus
(Wikipedia random corpus) was built by following the methodology suggested by
previous works in the state of the art, that is, selecting a set of 500 Wikipedia articles
using the Random article page (Wikipedia, 2014a).
The second ad-hoc corpus (Wikinews corpus) was built using documents from the
English Wikinews site (Wikinews, 2014b). These documents represent news items.
They are usually annotated by human editors with Wikipedia links. In this case 500
news items were selected with the Random article functionality of Wikinews (Wikinews,
2014a).
For each document in the total set of 1000 documents in the two ad-hoc corpora, we
built a link-to-Wikipedia query by using the following procedure:
1. A Wikipedia link is randomly selected from the document.
2. From the selected link we obtain the anchor a and the golden standard (link
destination in Wikipedia).
3. We use the anchor and Google search engine to build the candidate set, as it
was indicated in the case of the Cucerzan corpora. Again, we append the correct
candidate when it is not included in the Google result set. In particular, the
correct candidate was added in 14 out of 500 queries (2.8%) in the case of the
Wikinews corpus and in 36 out of 500 queries (7.2%) in the Wikipedia random
corpus.
4. The query context information is obtained from the rest of the links in the
document, filtering out those linking to members of the candidate set in order
to avoid bias.
‚Ä¢ TAC2010 The TAC 2010 dataset includes a total of 2250 entity linking queries and,
for each one, provides the anchor a to be linked, the context document dc and the
golden standard. We have used this dataset as basis to build the last corpus involved
in our evaluation. In order to do so, we proceeded as follows:
1. From the total set of 2250 queries, 1230 have as golden standard the NIL answer,
that is, there is no Wikipedia article to link in these cases. Thus, no correct
candidate instance exists and, due to this, it is difficult to take advantage of these
queries to evaluate the candidate ranking process. Taking this into account, we
discard these queries and keep the remaining 1020.
746

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

2. The documents in the TAC 2010 corpus do not contain links. Thus, we use
the following procedure in order to obtain the context links needed by some
algorithms:
‚Äì For each query, we analyze its context document dc by using natural language
processing techniques. In particular, we extract named entities (persons,
locations and organizations) from text using the Stanford NER tool (Finkel,
Grenager, & Manning, 2005).
‚Äì Then, we link the detected entities to Wikipedia using Google. In particular,
we query the Google search engine with the text of the named entity and a
site:en.wikipedia.org restriction, filtering out from the top-10 Google results
the Wikipedia pages not included in the Main namespace, and assigning as
link the top result in the filtered list. We discard the named entities where no
Google results are found. The links from named entities to Wikipedia defined
with this procedure are used as context information for candidate ranking,
filtering out from the context those links to members of the candidate set in
order to avoid bias.
We discard those queries where no context information is available, that is, where
the NER tool does not find named entities in dc , or when they are filtered in
the process of linking them to Wikipedia. This results in a total of 1012 valid
queries.
3. The candidate set C(dc , a) for each query is obtained by using the same procedure
as in previous corpora: using Google to search the anchor a and appending the
correct candidate in case it is not found (which happens in 70 out of 1012 queries
(6.9%)).
To summarize, we carry out our evaluation in five different corpora (Cucerzan news,
Cucerzan Wikipedia, Wikipedia random, Wikinews and TAC 2010)1 , which add up to a
total of 2512 link-to-Wikipedia queries. Boxplot diagrams representing the distributions in
each corpus of the number of candidates (|C(dc , a)|) per query, and the number of links
(|F (dc )|) per query, are shown in Figures 1 and 2 respectively.
Note that appending the correct candidate to the candidates set was needed in only 143
of the 2513 queries. This indicates that Google performs quite well as a candidate searcher
in our case, with a candidate recall near to 95% (considering only the first 10 results).
To put this result in context, we can indicate that Hachey et al. (2013) compare several
candidate search approaches in the TAC 2009 dataset and report that their candidate recall
is below the 75% when limited to a maximum of 10 results. However our results are similar
to those by Lehmann et al. (2010), where the authors use Google search combined with a
set of additional techniques and report a 97% candidate recall in the TAC 2009 dataset.
4.1.2 Wikipedia Link Structure
All the approaches described in section 3 require information from the Wikipedia link structure to carry out the candidate ranking process. In our case, that information has been
1. These corpora are available to download at: http://www.it.uc3m.es/berto/link-to-wikipedia/survey/

747

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

2

4

6

8

10

Distribution of the number of candidates per query in each corpus

Cucerzan News

Cucerzan Wiki.

Wiki. Random

Wikinews

TAC2010

Figure 1: Boxplot diagram of the number of candidates (|C(dc , a)|) per query for each
corpus.

0

50

100

150

200

250

300

350

Distribution of the number of context links per query for each corpus

Cucerzan News

Cucerzan Wiki.

Wiki. Random

Wikinews

TAC2010

Figure 2: Boxplot diagram of the number of links in the context (|F (dc )|) per query for
each corpus.

748

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

obtained from a dump of Wikipedia page links provided by DBpedia (Bizer et al., 2009)
version 3.82 , which was generated from a full Wikipedia dump dated in June 2012.
The links dump was preprocessed as follows:
‚Ä¢ The redirections were resolved, using the redirections mapping table from DBpedia
3.83 .
‚Ä¢ As indicated in section 2, we consider only the Wikipedia pages that belong to the
Main namespace. Thus, the links from/to pages in other namespaces (like Talk pages,
User pages, etc.) were removed.
‚Ä¢ Using the information provided by the disambiguation map from DBpedia 3.84 , the
links from/to disambiguation pages were also removed.
‚Ä¢ The inner links (from an article to itself) were also filtered out.
‚Ä¢ The evaluation corpora described in section 4.1.1 include in some cases Wikipedia
articles. To separate the input data from the evaluation data, all the links with source
or destination in one of the Wikipedia articles included in the evaluation corpora were
filtered out.
4.1.3 Performance Metrics
To measure and compare the performance of each of the considered approaches, we need
adequate metrics. A well-known evaluation metrics for link-to-Wikipedia approaches is the
accuracy, used for instance by Bunescu and Pasca (2006), Cucerzan (2007), Hachey et al.
(2011), Ratinov et al. (2011) and Hachey et al. (2013). The accuracy can be computed as
the percentage of queries where the candidate selected by the algorithm is the correct one,
or, more formally:
Accuracy =

1 X
S(q)
|Q|

(34)

‚àÄq‚ààQ

Where Q represents a set of evaluation queries, q a particular query in the set, and S(q)
a function so that S(q) = 1 if the candidate article ranked at the top for query q is the
correct answer or S(q) = 0 otherwise.
However, as we center the evaluation in the candidate ranking stage of the link-toWikipedia task, the accuracy presents a limitation: it does not take into account the actual
position of the correct answer within the ranking produced by each algorithm. For example,
if one algorithm ranks the correct answer for a query in the 2nd position, whereas another
algorithm ranks it in the 8th position, the contribution from this query to the accuracy is
zero in both cases, despite the first algorithm having ranked the correct answer higher.
In scenarios where link-to-Wikipedia approaches work under human-supervision (for instance, if these systems are used within the production process of a news agency (FernaÃÅndez
2. http://downloads.dbpedia.org/3.8/en/page links en.nt.bz2 (April, 2014)
3. http://downloads.dbpedia.org/3.8/en/redirects transitive en.nt.bz2 (April, 2014)
4. http://downloads.dbpedia.org/3.8/en/disambiguations en.nt.bz2 (April, 2014)

749

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

et al., 2006) to add metadata to news items) the particular order of the candidates is relevant, because in case the top-ranked candidate is not the correct one, the human supervisor
can continue reading the ranked list of candidates and select another option. Obviously,
the nearer to the top the correct candidate is in the list of suggestions, the better.
Taking this into account, we decided to report performance using not only accuracy, but
also two position-based discounting schemes to measure the overall quality of the ranked
list of results:
‚Ä¢ The Mean Reciprocal Rank (MRR) is used for instance in the evaluation of question
answering systems (Voorhees, 1999). The MRR of a set of evaluation queries Q can
be computed as:
M RR(Q) =

1 X 1
|Q|
r(q)

(35)

‚àÄq‚ààQ

Where r(q) represents the position of the correct candidate in the rank for a query
q ‚àà Q.
‚Ä¢ As shown in equation 35, the MRR penalizes the differences in position severely.
Taking this into account, we also report the results of the Discounted Cumulative
Gain at a certain level K (DCG@K), which introduces a smoother penalization with
position. The DCG@K can be computed as:
k
1 X X 2R(q,i) ‚àí 1
DCG@K(Q) =
|Q|
log2 (1 + i)

(36)

‚àÄq‚ààQ i=1

Where Q represents a set of evaluation queries, q a particular query in the set, and
R(q, i) the relevance score given to the candidate article in position i for query q. We
adopt a binary relevance model and, thus, R(q, i) = 1 if the candidate in position
i is the correct answer and R(q, i) = 0 otherwise. Furthermore, we only consider a
single candidate as relevant for each query. Taking this into account, the DCG@K is
equivalent to its normalized version, the Normalized Discounted Cumulative Gain at
K (NDCG@K) (Manning et al., 2008), and equation 36 can be simplified into:

1 X
DCG@K(Q) =
f (q, k) with f (q, k) =
|Q|
‚àÄq‚ààQ

(

1
log2 (1+r(q))

if r(q) <= k

0

if r(q) > k

(37)

Where r(q) represents the position of the correct candidate in the rank for query q.
As it can be seen in equation 37, the bigger the value of r(q) (that is, the farther away
the correct candidate from the top of the rank) the lesser the value of the term added
to DCG@K. Note also that DCG@1 would be equivalent to the accuracy as defined
in equation 34.
750

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

Figure 3: Taxonomy of the different approaches considered for evaluation.
4.2 Evaluation
This section reports the results of the empirical evaluation of the approaches. We have
structured the presentation of results in three parts: section 4.2.1 compares the individual
algorithms described in section 3, section 4.2.2 analyzes the combination of the different approaches through the use of machine learning techniques and, finally, section 4.2.3 evaluates
the impact of changing the search stage on the performance of the algorithms.
4.2.1 Comparison of Individual Approaches
All the approaches described in section 3 (summarized in the taxonomy shown in Figure 3)
were evaluated in the corpora described in section 4.1.1. Table 1 reports the accuracy
obtained by each approach in the different evaluation corpora. We have highlighted in
boldface the best accuracy among the link-based evaluated approaches for each particular
corpus.
Table 1 includes a column (Overall) that reports the results obtained in the corpus
generated by aggregating all the queries. The last column, Confidence Interval (Overall),
reports the 95% confidence interval for the accuracy in the Overall case, computed using
bootstrap methods as suggested by Adibi, Cohen, and Morrison (2004).
As it can be seen in the Approach column in Table 1, apart from the approaches considered in section 3, we include the results of two naive algorithms as reference baselines:
(1) the random algorithm, which simply ranks all the candidates randomly; and, (2) the
most frequently linked (MFL) algorithm, described in section 3 (see equation (33)). We also
report (see row Google) the accuracy obtained by using a trivial ranker that simply returns
751

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

the candidates in the same order as they are defined in the corpus (that is, in the same
order as returned by Google, with the correct candidate at the end if it was not found by
Google)5 .
Cucerzan
Approach
Random
MFL
Google
RelFA
RelFM
A
RelB
M
RelB
P M IFA
P M IFM
A
P M IB
M
P M IB
simcos
simR
indegree
outdegree
NB
PR
PPR
RWP
RWF

Wiki.

News

Wiki

Random

0.133
0.700
0.844
0.568
0.416
0.716
0.576
0.204
0.140
0.272
0.192
0.440
0.760
0.700
0.532
0.772
0.684
0.692
0.776
0.760

0.149
0.630
0.883
0.458
0.406
0.651
0.542
0.233
0.229
0.321
0.285
0.486
0.687
0.647
0.462
0.719
0.623
0.687
0.663
0.715

0.153
0.571
0.795
0.489
0.365
0.717
0.597
0.289
0.252
0.421
0.383
0.483
0.687
0.581
0.327
0.729
0.565
0.697
0.647
0.643

TAC
Wikinews
0.155
0.676
0.914
0.436
0.338
0.694
0.526
0.206
0.174
0.318
0.274
0.516
0.734
0.670
0.472
0.752
0.668
0.702
0.752
0.744

2010
0.118
0.668
0.748
0.495
0.227
0.738
0.418
0.081
0.055
0.237
0.167
0.421
0.725
0.671
0.579
0.766
0.635
0.553
0.732
0.740

Overall
0.137
0.650
0.813
0.486
0.313
0.715
0.503
0.174
0.144
0.301
0.245
0.461
0.719
0.653
0.491
0.752
0.631
0.639
0.717
0.721

Confidence
Interval
(Overall)
(0.124, 0.151)
(0.631, 0.668)
(0.798, 0.828)
(0.466, 0.505)
(0.295, 0.331)
(0.696, 0.732)
(0.483, 0.522)
(0.160, 0.189)
(0.130, 0.157)
(0.283, 0.319)
(0.228, 0.262)
(0.441, 0.480)
(0.701, 0.736)
(0.634, 0.671)
(0.471, 0.510)
(0.734, 0.768)
(0.612, 0.650)
(0.619, 0.657)
(0.698, 0.734)
(0.703, 0.738)

Table 1: Accuracy obtained by the different approaches in each of the evaluation corpora.
Figure 4 shows the DCG@K for different values of K in the Overall aggregated corpus.
MRR values for the different evaluation corpora are reported in Table 2, where, again,
we have highlighted in boldface the best MRR among the link-based evaluated approaches
for each particular corpus. Furthermore, in order to provide a more detailed idea of the
differences among methods, we show in Figure 5 the percentage of queries in which the
correct candidate is ranked at position K (with K from 1 to 10) for each algorithm.
We can also provide some empirical results about the run-time of the different algorithms. In particular, the average run-time per query (in seconds) measured on a Linux
2.6.32, Intel Core i7 2.80GHz PC with 16GB RAM was under one second for all the approaches except RWF and P P R, which run closer to 4 and 571 seconds per query respectively. The relatively large response time of P P R is due to the fact that this algorithm uses
context information to personalize the PageRank damping vector. Taking into account that,
in general, each query has a different context, this means that we need to run a PageRank
5. Note that in the Google case, the queries where the correct candidate is appended to the result set are
accounted as errors when computing accuracy.

752

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

Approach
Random
MFL
Google
RelFA
RelFM
A
RelB
M
RelB
P M IFA
P M IFM
A
P M IB
M
P M IB
simcos
simR
indegree
outdegree
NB
PR
PPR
RWP
RWF

Cucerzan
News
Wiki
0.347 0.370
0.806 0.763
0.894 0.922
0.727 0.651
0.607 0.600
0.830 0.787
0.727 0.710
0.428 0.458
0.356 0.440
0.500 0.543
0.415 0.502
0.650 0.671
0.850 0.813
0.806 0.773
0.700 0.648
0.859 0.834
0.799
0.76
0.812 0.801
0.864 0.796
0.854 0.821

Wiki.
Random
0.374
0.726
0.858
0.667
0.573
0.830
0.746
0.501
0.463
0.622
0.570
0.666
0.805
0.729
0.550
0.832
0.719
0.810
0.778
0.774

Wikinews
0.379
0.794
0.939
0.635
0.548
0.820
0.705
0.435
0.403
0.547
0.503
0.699
0.837
0.793
0.655
0.848
0.786
0.810
0.847
0.837

TAC
2010
0.326
0.791
0.828
0.688
0.473
0.847
0.643
0.318
0.271
0.490
0.414
0.620
0.838
0.794
0.730
0.861
0.774
0.734
0.837
0.836

Overall
0.353
0.778
0.872
0.673
0.534
0.831
0.691
0.403
0.361
0.534
0.472
0.653
0.830
0.780
0.668
0.850
0.767
0.778
0.826
0.824

Table 2: MRR obtained by the different approaches in each of the evaluation corpora.
computation on the whole Wikipedia graph for each query in the corpus, a process that is
time consuming6 . Note, however, that we have used a Python implementation which was
not optimized and, thus, these results are provided only as a reference.
To contextualize the results reported in Table 1, we can indicate that the TAC 2010
corpus that we are using in our evaluation is practically equivalent (apart from 8 queries
removed due to the lack of context information, as indicated in section 4.1.1) to the Non-NIL
queries in the TAC 2010 dataset. Due to this, the results reported in the column TAC 2010
of Table 1 can be roughly compared (less than 1% of error) with the TAC 2010 Non-NIL
accuracy reported by some papers in the state of the art. For instance, the best performing
approach in TAC 2010 (Lehmann et al., 2010) reported an accuracy on the Non-NIL queries
of 80.6%. Note, however, that we have to be cautious with these comparisons, as the results
we are reporting would be equivalent to those obtained with an end-to-end system using
an ideal candidate search stage (we always append the correct candidate) and without a
candidate selection process (we report the results of the candidate ranking stage).
Analyzing the results reported in Table 1, a first conclusion that may be drawn is that
the overall accuracy achieved by using the Google ranking is better than that obtained
by any of the evaluated approaches. However, if we observe the results obtained for each
6. According to Bianchini, Gori, and Scarselli (2005), this computation depends linearly on the number of
edges on the Wikipedia graph.

753

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

Algorithms
RelAF

0.75

RelM
F
RelAB
RelM
B
PMIAF

DCG@K

PMIM
F
PMIAB
PMIM
B
simCOS

0.50

simR
indegree
outdegree
NB
PR
PPR
RWP
RWF

0.25

1

2

3

4

5

6

7

8

9

10

K

Figure 4: DCG@K values for the different algorithms considered when evaluated on the
Overall corpus generated by aggregating all the queries.

individual corpus, we can also note that using Google is not always the best approach. In
particular, the accuracy of N B in the TAC 2010 corpus is slightly better than that achieved
by Google.
To interpret these findings, we have to take into account that previous work in the area
(notably that of Chang et al., 2010) had already pointed out that using Google produces
relatively good results for the entity linking task (accuracy near 78% in TAC 2009 experimental setup). In that sense, the overall performance obtained by Google is not completely
unexpected. The degradation in performance in the TAC 2010 case is partially explained
754

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

100%

90%

80%
Position (K)

Percentage of queries

70%

10
9

60%

8
7
6

50%

5
4

40%

3
2

30%

1

20%

10%

RWF

RWP

PPR

PR

NB

outdegree

indegree

simR

simCOS

PMIM
B

PMIAB

PMIM
F

PMIAF

RelM
B

RelAB

RelM
F

RelAF

0%

Algorithms

Figure 5: Percentage of queries where the correct candidate is ranked at position K (with
K from 1 to 10) for each of the different algorithms compared.

755

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

by the fact that this corpus is specifically built for the entity linking task by using a careful
targeted process7 . Due to this, the queries in the TAC 2010 corpus are expected to be
challenging. For instance, a common case8 within this corpus is to have groups of queries
sharing the same anchor to be linked, but with different correct answers depending on the
particular query. In this case, as the queries share the anchor, they also share the Google
ranking and, thus, the top ranked candidate but, as the correct answer changes between
queries, using always the Google top ranked candidate as answer introduces some errors.
Note also that by using Google we are not taking advantage of the context information,
which is expected to be valuable to decide the best link for an anchor, especially when the
queries are challenging.
A second conclusion is that naive popularity metrics like the indegree (or the M F L,
whose performance is very similar to the indegree) produce reasonably accurate results.
This aspect is consistent with previous work in the state of the art (Ji & Grishman, 2011)
where the authors indicate that naive candidate ranking approaches based on web popularity
can achieve accuracies around 71% on the TAC 2010 corpus.
Another aspect to be highlighted is that, consistently across evaluation corpora, the
indegree metrics produces better accuracy than the outdegree, which indicates that the
number of backlinks offers a better representation of the popularity of a Wikipedia article
than the number of its outgoing links. One aspect that may explain, at least partially, this
difference in performance is the fact that the number of outgoing links may be high due
to several reasons. When a Wikipedia article is long (which indicates that it has received
extensive attention by Wikipedia contributors and is, in that sense, popular) we can expect
it to have more links than shorter articles. However, there are other cases in which a
Wikipedia article can contain many outgoing links. It is the case, for instance, of articles
that represent a hub of links, such as List articles.
To test the hypothesis that the outdegree metrics introduces bias to favor hubs like
List articles, we compared the number of queries where the candidate that is ranked at the
top by indegree and outdegree is a list (its title starts with List ) in the different corpora.
These results are shown in Table 3, where x, is the proportion of queries where the candidate
ranked at the top is a list, whereas y is the proportion of the queries where the candidate
ranked at the top is a list and this is not the correct answer. That is:
Queries where the top ranked candidate is a list
Total number of queries in the corpus

(38)

Queries where the top ranked candidate is a list and is not correct
Queries where the top ranked candidate is a list

(39)

x=

y=

As it can be seen in Table 3, the outdegree metrics ranks list pages more frequently
at the top than the indegree metrics. It can also be seen that, in most cases, when the
candidate ranked at the top is a list, it is not the correct answer. This particularity explains
a significant part of the difference between the overall results of indegree and outdegree.
7. As indicated in the TAC KBP 2010 task definition document, available at:
http://www.it.uc3m.es/berto/link-to-wikipedia/survey/KBP2010 TaskDefinition.pdf (April, 2014)
8. We have identified a total of 144 queries (approximately a 14%) following this pattern.

756

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

Approach
Name
Param
x
indegree
y
x
outdegree
y

Cucerzan
News Wiki
0.004 0.004
1.0
1.0
0.06 0.088
1.0
1.0

Wiki.
Random
0.022
0.818
0.138
0.956

Wikinews
0.006
0.667
0.078
0.974

TAC
2010
0.014
1.0
0.049
1.0

Overall
0.012
0.903
0.078
0.979

Table 3: Comparison between indegree and outdegree regarding the tendency to rank a
Wikipedia list at the top.

The difference in performance between using backlinks and forward links can also be
noticed in the similarity metrics, where those approaches relying on backlink information
A , RelM , P M I A , P M I M ) produce better results than the corresponding metrics work(RelB
B
B
B
ing on forward links (RelFA , RelFM , P M IFA , P M IFM ).
According to the results in Table 1, it can also be pointed out that, among the linkbased approaches being evaluated, taking advantage of context information is, in general,
beneficial. To support this conclusion we can compare the results of M F L and N B. Note
that N B uses the prior P (ci ) (see equations (22) and (23)), which is basically a normalized
version of M F L. However, N B combines this prior with the probabilities P (fj /ci ), which
capture context information. As it can be seen in Tables 1 and 2, the result of this combination is that N B produces better results than M F L. Note also that none of the alternatives
which use only popularity information are included among the top-5 link-based evaluated
A ). However, using context
approaches with higher accuracy (N B, RWF , simR , RWP , RelB
information is not a sufficient condition to ensure a good performance, as reflected by the
results of the PMI variants.
Another conclusion that can be reached is that, in the cases of relatedness and PMI
metrics, averaging the pairwise similarities between the candidate and the articles in F (dc )
A , P M I A , RelA and P M I A ) produces, consistently across corpora, better
(as is done in RelB
B
F
F
M , P M I M , RelM and P M I M ).
accuracy than relying on the maximum (as is done in RelB
B
F
F
A possible explanation to this result is that by relying on the maximum similarity we
just take into account one of the elements in F (dc ) (the one that maximizes similarity)
to represent the semantics of the document dc , whereas, when averaging, all the elements
in F (dc ) contribute to the final similarity value. It is reasonable to think that the set of
forward links in dc provides a more accurate representation of the semantics of the context
document than a single link in the document.
M
With the objective of testing this intuition, we implemented two new variants of RelB
M , that we name RelM (P ) and P M I M (P ). In order to obtain the RelM (P )(c , d )
and P M IB
i c
B
B
B
scores we compute the simRB (ci , fj ) ‚àÄfj ‚àà F (dc ) as in equation (7). However, instead of just
selecting the maximum value, as it is done in equation (7), we select a certain percentage
P of the top values and average them. For instance, if |F (dc )| = 10 and P = 50%, we
select the top 5 simRB (ci , fj ) values and average them. Note that, with this approach,
A . To obtain
when P = 100% the scores would be equivalent to those obtained with RelB
M (P )(c , d ) scores we proceed in a similar way, but using the P (c , f ) values
the P M IB
i c
B i j
M (P ) and
(see equation (11)) instead of the simRB (ci , fj ) ones. We evaluated the RelB
757

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

0.7

RelAB

Accuracy

0.6

RelM
B
0.5

Algorithms
PMIM
B ( P)
RelM
B ( P)

0.4

PMIAB
0.3

PMIM
B

10%

30%

50%

70%

90%

Percentage
M (P ) and P M I M (P ) approaches for different values
Figure 6: Accuracy values for the RelB
B
of percentage P when evaluated on the Overall corpus generated by aggregating
all the queries.

M (P ) variants on the overall aggregated corpus for different values of percentage P .
P M IB
Figure 6 reports the accuracy obtained by these new variants. We have also included as
M , P M I M , RelA and
references horizontal lines representing the overall accuracy of RelB
B
B
A
P M IB . As it can be seen, increasing the context information improves results.
Also related with the relatedness and the PMI metrics is the fact that the results obtained
by PMI variants are quite poor when compared with the equivalent relatedness variants. As
A (0.715) and P M I A (0.301).
an example, see the difference in overall accuracy between RelB
B

758

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

An inspection of the results of P M I revealed that, because of using absolute values instead
of logarithmic values (as in relatedness), the PMI is more sensitive to outliers. In order to
verify and quantify this observation, we decided to compare the results of PMI with two
other alternatives:
‚Ä¢ We implemented a logarithmically smoothed version of the averaging PMI variants
by adapting equations (8) and (9) as follows:

logP M IFA (ci , dc ) =
A
logP M IB
(ci , dc ) =

1
|F (dc )|
1
|F (dc )|

X

log[PF (ci , fj )]

(40)

X

log[PB (ci , fj )]

(41)

‚àÄfj ‚ààF (dc )

‚àÄfj ‚ààF (dc )

‚Ä¢ We used the symmetric conditional probability (SCP), introduced by da Silva and
Lopes (1999). The SCP of two Wikipedia documents wi , wj can be computed as:
SB (wi , wj ) =

|B(wi ) ‚à© B(wj )|2
|B(wi )||B(wj )|

(42)

That can also be adapted to use forward links as:
SF (wi , wj ) =

|F (wi ) ‚à© F (wj )|2
|F (wi )||F (wj )|

(43)

Using equations (42) and (43) the following two metrics were implemented:
SCPFA (ci , dc ) =
SCPBA (ci , dc ) =

1
|F (dc )|
1
|F (dc )|

X

[SF (ci , fj )]

X

[SB (ci , fj )]

(44)

‚àÄfj ‚ààF (dc )

(45)

‚àÄfj ‚ààF (dc )

We run these approaches on all the corpora, and report the results in Table 4 (accuracies)
and Table 5 (MRR). As it can be seen comparing the results in Table 4 with those for
A in Table 1, a significant increase in performance is achieved by using the
P M IFA and P M IB
logarithmically smoothed version of P M I. It can also be seen that the accuracies reported
A and more similar to the
for SCPFA and SCPBA are better than those for P M IFA and P M IB
A , respectively.
results of the relatedness variants RelFA and RelB
4.2.2 Combining Individual Approaches
We want also to explore the possibility of combining the results of the different link-based
approaches to test whether better results can be obtained or not. The approach that we
follow to combine the alternatives described in section 3 is based on supervised machine
learning techniques. In particular, we use a learning to rank (Joachims, 2002) method.
759

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

Cucerzan
Approach
logP M IFA
A
logP M IB
SCPFA
SCPBA

Wiki.

News

Wiki

Random

0.392
0.552
0.500
0.728

0.357
0.550
0.454
0.634

0.423
0.669
0.441
0.693

TAC
Wikinews
0.370
0.590
0.404
0.658

2010
0.396
0.674
0.286
0.550

Overall
0.392
0.632
0.378
0.626

Confidence
Interval
(Overall)
(0.372, 0.411)
(0.612, 0.650)
(0.359, 0.397)
(0.607, 0.645)

Table 4: Accuracy obtained by the logarithmically smoothed P M I variants and the SCP based metrics in each of the evaluation corpora.

Approach
logP M IFA
A
logP M IB
A
SCPF
SCPBA

Cucerzan
News Wiki
0.604 0.574
0.735 0.714
0.681 0.648
0.835 0.788

Wiki.
Random
0.623
0.800
0.637
0.815

Wikinews
0.580
0.752
0.610
0.797

TAC
2010
0.607
0.805
0.545
0.741

Overall
0.601
0.778
0.600
0.781

Table 5: MRR obtained by the logarithmically smoothed P M I variants and the SCP -based
metrics in each of the evaluation corpora.

Though several learning to rank algorithms are available in the state of the art (Liu, 2009),
we decided to rely on the ListN et method described by Cao et al. (2007). Our decision
is backed on the results reported by Chen and Ji (2011), where several alternatives are
evaluated and compared in the context of the entity linking problem. In particular, we
took advantage of the open source implementation of ListN et provided by the University
of Massachusetts‚Äô RankLib package (Van B. Dang, 2014).
Basically, we use the scores returned by the individual approaches in section 3 as features
to be taken into account by the ListN et algorithm. The values of the features are normalized
in the range [0, 1] to avoid any bias that might favor some of them.
We have tested three different combinations of approaches. The first variant (that
we name ListN etAll ) combines all the link-based approaches under evaluation (that is,
all the algorithms included in Table 1 except Google and the naive references M F L and
Random). The second variant (ListN etT op ) combines just the top-5 best performing linkbased algorithms under evaluation (according to Overall accuracy in Table 1, that is, N B,
A ). Finally, the third case (ListN et
RWF , simR , RWP , RelB
T op+Google ) combines the top5 best performing link-based algorithms with the Google baseline. In all the cases, we
have used the configuration parameters for ListN et that are suggested in the RankLib
implementation (1500 epochs and a learning rate of 10‚àí5 ).
In order to report the accuracy, MRR and DCG@K of the ListN et variants, we use
the results obtained by averaging 10 repetitions of 10-fold cross validation on the particular
corpus being analyzed. Table 6 reports the accuracy in the different corpora for the combinations that have been considered, while Table 7 reports the MRR results for the same
760

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

Approach
ListN etAll
ListN etT op
ListN etT op+Google

Cucerzan
News Wiki
0.780 0.716
0.824 0.786
0.854 0.864

Wiki.
Random
0.742
0.769
0.850

Wikinews
0.738
0.803
0.877

TAC
2010
0.678
0.793
0.816

Overall
0.705
0.797
0.846

Table 6: Accuracy obtained when combining approaches in section 3 with ListN et in each
of the evaluation corpora.

Approach
ListN etAll
ListN etT op
ListN etT op+Google

Cucerzan
News Wiki
0.867 0.834
0.888 0.879
0.916 0.930

Wiki.
Random
0.847
0.865
0.916

Wikinews
0.848
0.885
0.929

TAC
2010
0.812
0.882
0.894

Overall
0.828
0.882
0.913

Table 7: MRR obtained when combining approaches in section 3 with ListN et in each of
the evaluation corpora.

combinations. Figure 7 compares the DCG@K achieved by ListN et variants in the Overall
case with those of the top-5 link-based evaluated approaches.
We can compare the accuracy values reported in Table 6 with those in Table 1. In the
overall case, the best result is obtained by ListN etT op+Google . The ListN etT op combination
shows a lower performance than the Google reference, but outperforms N B (the best of the
individual algorithms under comparison). Regarding the ListN etAll variant, its accuracy is
lower than that obtained by both Google and N B. Similar conclusions can also be reached
from Figure 7 for the DCG@K metrics in the overall case. These conclusions suggest that
some particular combinations of features can have a positive impact on results.
However, we have to be cautious with these results, because, as indicated by Vanwinckelen (2012), repeated cross validation should not be assumed to provide perfectly precise
estimates of a model‚Äôs predictive accuracy. In fact, Vanwinckelen (2012) does not recommend reporting confidence intervals or making significance claims from repeated cross validation. They report that, though popular among researchers, this practice can contribute
to misleading interpretations.
4.2.3 Effect of Changes in the Search Stage
As indicated in section 4.1.1, in order to isolate the results of the candidate ranking algorithms being evaluated from the potential bad performance of a particular candidate search
implementation, we would need to rely on an ideal candidate search stage, in the sense that
it always returns the correct answer among the candidate set. Obviously, there does not
exist an ideal candidate searcher. Thus, in practice, to try to mimic this behavior, we have
relied on a state of the art search engine (Google) and we have appended the correct answer
to the candidate set in case it is not found by the search engine.
761

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

0.90

Algorithms
RelAB

0.85

simR

DCG@K

NB
RWP
RWF
ListNetAll

0.80

ListNetTop
ListNetTop+Google

0.75

0.70
1

2

3

4

5

6

7

8

9

10

K

Figure 7: DCG@K values for the top-5 link-based evaluated approaches and ListN et variants when evaluated on the Overall corpus generated by aggregating all the
queries.

762

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

However, we are also interested in evaluating the impact on the results achieved by
the different algorithms when the aforementioned conditions change into a more restrictive
setup. In order to do so, we proceeded as follows:
‚Ä¢ We used the information retrieval library Apache Lucene (Apache Software Foundation, 2014) to build an index with the titles of the DBpedia 3.8 pages. Each title was
processed by the StandardAnalyzer of Lucene.
‚Ä¢ For each of the 1012 queries in the TAC 2010 corpus we carried out the following
process:
‚Äì The anchor a of the query is used to search into the Lucene index for the candidates, C(dc , a). The result set was limited to the top-10 entries, like in previous experiments. However, on the contrary to our previous experiments, when
Lucene does not return the correct answer within its result set, we do not append
it.
‚Äì As the documents in the TAC 2010 corpus do not contain context links, these
are automatically generated using a similar approach to the one described in
section 4.1.1: the named entities obtained from the context documents using
Stanford NER are resolved into links by querying Lucene with the text of the
named entity and assigning as link destination the top result from the search
engine (as usual, filtering out the links to articles that are included within the
candidate set).
Using the aforementioned procedure we built a new version of the TAC 2010 corpus
annotated with Lucene. Thus, we have now two variants of TAC 2010:
‚Ä¢ TAC 2010 Google, where Google has been used as candidate searcher and the correct
candidate is appended to the Google results set in case it is not found. This is the
version used in the experiments of previous sections.
‚Ä¢ TAC 2010 Lucene, which is the version built following the procedure described in this
section.
We run all the evaluated approaches, as well as the references Random and M F L, in
the TAC 2010 Lucene corpus. The accuracies achieved by the different algorithms and their
95% confidence intervals are shown in Figure 8. To ease comparison, we have depicted in
the same figure the accuracies for the TAC 2010 Google corpus. We have also included
(with the name Search) the accuracy achieved when the candidate ranking provided by the
search engine (either Google or Lucene) is directly used.
A first aspect to be noted from the results reported in Figure 8 is that, not surprisingly,
the accuracies obtained by the different approaches when using Lucene search are, in general,
lower. Note that in the Lucene case we are not including the correct candidate in the
candidates set. Thus, there are many queries (363 cases, almost a 36% of the total queries)
where it is impossible for the candidate rankers to rank the correct candidate at the top.
Another issue to be highlighted is that, if we look at the top-5 best performing link-based
A , all of them have greatly reduced their
approaches in Table 1: N B, RWF , simR , RWP , RelB
763

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

0.8

Accuracy

0.6

Case
Google

0.4

Lucene

0.2

RWF

RWP

PPR

PR

NB

indegree

outdegree

simR

simCOS

PMIM
B

PMIAB

PMIM
F

PMIAF

RelM
B

RelAB

RelM
F

RelAF

Search

MFL

Random

0.0

Algorithms

Figure 8: Accuracy obtained by the different approaches on the TAC 2010 Google and TAC
2010 Lucene corpora.

performance in the TAC 2010 Lucene corpus. In fact, though N B is the top performing in
that corpus, there is no statistically significant difference with popularity approaches like
indegree or PageRank (P R). A possible explanation for this result is that in the TAC 2010
corpus the context information is automatically generated from the search engine and not
supervised. Thus, we can expect it to be noisy. This noise affects all N B, RWF , simR ,
A , which rely on context information to take their decisions, but does not impact
RWP , RelB
indegree and P R, which do not rely on context information. Note that, though the noise in
the context information affects both the cases where Google and Lucene are used as search
764

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

engines, the better performance of Google (note results for Search) makes it much worse in
the Lucene case.

5. Related Work
The foundations of the link-to-Wikipedia task can be found in two different research communities. First, this area is related with traditional computational linguistics tasks like
cross-document co-reference resolution (Bagga & Baldwin, 1998), or word sense disambiguation (Navigli, 2009). The main difference with respect to these traditional tasks is that
Wikipedia is used as a source of knowledge instead of lexicons such as WordNet (Miller,
1995) typically used in former work (see for instance Li, Szpakowicz, & Matwin, 1995).
Second, the link-to-Wikipedia task is also related to the link prediction task in link mining (Getoor & Diehl, 2005), though in this case the goal is mainly to decide whether two
objects (for instance, two actors in a social network, or an actor and an event) are linked
or not, instead of finding the best link destination in Wikipedia for a particular anchor in
a text document.
Traditionally, the works of Bunescu and Pasca (2006) and Cucerzan (2007) have been
considered as seminal in this area. Since the publication of these papers the problem of
linking anchors in a text document to Wikipedia articles has been addressed by several
other works, like those referenced in section 3.
Two initiatives are also especially relevant in this sense: the Knowledge Base Population (KBP) track at the Text Analysis Conference (TAC), and Link-the-Wiki track of the
Initiative for the Evaluation of XML retrieval (INEX). Both initiatives share the same goal:
offer a common environment (corpora, performance metrics, etc) to allow a fair comparative evaluation of different techniques and, thus, foster this area of research. However, as
indicated in the introductory section, they approach the link-to-Wikipedia task with slight
differences. In the case of KBP, the final aim is to automatically populate a Knowledge
Base (KB) built from Wikipedia with information about named entities. Thus, the linkto-Wikipedia variant (named entity linking) is focused on these entities and covers the case
where no good Wikipedia target exists for the link, as this case indicates the need to add
a new entry to the KB. In the case of the Link-the-Wiki INEX track, the focus is set in
keeping the links up to date in a rich and dynamic hypermedia document collection (such as
a wiki). Therefore, the link-to-Wikipedia variant (wikification) covers both common terms
and named entities as anchors to be linked, and does not pay special attention to the case
where no good Wikipedia target exists, as in this case no link needs to be created.
In both cases, the overview papers published by the organizers of these events (Huang
et al., 2008, 2009, 2010; Ji et al., 2010, 2011; Ji & Grishman, 2011) offer a good source
of references in this area. However, the comparisons provided by these works refer only to
systems taking part in TAC/KBP or INEX, and not to other external work. Furthermore,
as indicated in the introductory section, link-to-Wikipedia systems combine, in general, a
variety of techniques and features of different types (based on text, on links, etc.) to address
the task. Because the results reported in the overviews refer usually to full systems, it is
difficult to analyze and compare the performance of the individual techniques that are part
of these systems. The goal of our work is doing this analysis and comparison for link-based
techniques.
765

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

Other surveys related with the task of link-to-Wikipedia and, thus, relevant for the purposes of this paper are that of Navigli and Lapata (2010), Chen and Ji (2011), and Hachey
et al. (2013).
Chen and Ji (2011) evaluate several supervised candidate rankers for named entity
linking, and compare them with reference unsupervised approaches: a naive algorithm and
three different similarity metrics based on textual features. The main goal of this comparison
was to assess which machine learning mechanism (maximum entropy, SVM, SV M rank and
ListNet) was the top performing. Thus, the results reported by Chen and Ji (2011) and
those in our paper are complementary, because, as we indicated in section 4.2.2, the different
approaches analyzed here can be used as features in supervised systems. Obviously, this
requires to know which supervised techniques work better (Chen & Ji, 2011), but also to
know which link-based techniques are better, that is the goal of our paper.
Hachey et al. (2013) re-implement and compare three different named entity linking
systems in the state of the art. However, the main goal of their work was different to ours,
as the aim of Hachey et al. (2013) was to analyze the impact of the candidate searching and
candidate ranking stages in the final performance of the entity linking system.
Navigli and Lapata (2010) compare several metrics based on graph connectivity, including some that we have also considered in our paper, like PageRank and indegree. However,
their work has a different scope to ours: it is centered on a different task (word sense
disambiguation), and uses a different data source (WordNet).
To the knowledge of the authors, a previous overview and comparison of different linkbased approaches for candidate ranking in link-to-Wikipedia systems, as proposed in this
paper, is not available at the time of writing.

6. Conclusions and Future Lines
In this paper we have presented an overview of link-based approaches for candidate ranking
in link-to-Wikipedia systems. Apart from this overview, a comparative analysis of the
different approaches is also carried out. We have structured this analysis into three parts:
‚Ä¢ The first part was devoted to compare the performance of the individual approaches
according to three metrics (accuracy, DCG@K and MRR) in five different corpora
(Cucerzan news, Cucerzan Wikipedia, random Wikipedia articles, random Wikinews
articles and TAC 2010). The results in this part of the analysis indicate that, though
naive approaches based only on the popularity of the candidates perform reasonably
well, taking advantage of the context information is, in general, beneficial in linkbased approaches. We have also found that by using information from backlinks we
can obtain better results than by using forward links with the same techniques.
‚Ä¢ In the second part of the analysis we have combined different approaches by using
ListNet. The main conclusion of this part is that, according to the results obtained,
combining algorithms can produce positive effects in performance.
‚Ä¢ Finally, the third part of the analysis was devoted to evaluate the impact of the
candidate search stage in the candidate ranking results, an impact that was found to
be very significant.
766

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

Regarding potential future lines of development of the work described in this paper, a
first aspect to consider is to evaluate the impact of the quality of the context links in the
performance of the algorithms. We also want to analyze the effect of ignoring links that
might be introducing some noise into the ranking process, like Lists. On the opposite case,
we are interested in measuring the impact of including links to pages in other namespaces,
like Categories, that have not been considered in this paper. In this sense, taking Categories
into account will open the door to the use of semantic relatedness measures based on this
information, like those described by Ponzetto and Strube (2007).
According to the results in the paper, using ListNet to combine algorithms can produce
positive effects in performance in some cases. However, an exhaustive analysis of different
combinations has not been carried out. Thus, another potential line of development could be
exploring further combinations of algorithms, either by taking advantage of some proposals
of mechanisms for feature selection in learning to rank (Geng, Liu, Qin, & Li, 2007) or
empirically.
We have analyzed the different algorithms from the perspective of their performance on
the link-to-Wikipedia task. The computational complexity aspects have not been addressed.
An exhaustive analysis of the different algorithms along this line is left for future work.
As suggested in section 4.1.3, link-to-Wikipedia systems can be integrated into content
production workflows, where they have to interact with human supervisors. Assessing the
impact of this human factor on the final performance of the systems can also constitute an
area for future research.
Finally, though in this paper we have centered our attention on the candidate ranking
stage, link-to-Wikipedia systems usually include other processing stages: identifying the
anchors to be linked, searching the candidate links for these anchors, and deciding whether
a link is to be suggested or not (detect NIL answers). An end-to-end evaluation including
these additional processing stages is also an interesting line to continue the work reported
in this paper.

Acknowledgements
In memoriam of ConcepcioÃÅn Garcƒ±ÃÅa Alonso (1943-2012) and all the passengers that passed
away in the Angrois railway accident (24/Jul/2013).

References
Adibi, J., Cohen, P. R., & Morrison, C. T. (2004). Measuring confidence intervals in link
discovery: a bootstrap approach. In Proceedings of the ACM Special Interest Group
on Knowledge Discovery and Data Mining (ACM-SIGKDD-04.
Apache Software Foundation (2014). Apache Lucene - Welcome to Apache Lucene. Available
at: http://lucene.apache.org/.
Bagga, A., & Baldwin, B. (1998). Entity-based cross-document coreferencing using the
Vector Space Model. In Proceedings of the 17th international conference on Computational linguistics - Volume 1, COLING ‚Äô98, pp. 79‚Äì85, Stroudsburg, PA, USA.
Association for Computational Linguistics.
767

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

Bianchini, M., Gori, M., & Scarselli, F. (2005). Inside PageRank. ACM Trans. Internet
Technol., 5 (1), 92‚Äì128.
Bizer, C., Lehmann, J., Kobilarov, G., Auer, S., Becker, C., Cyganiak, R., & Hellmann, S.
(2009). DBpedia - A crystallization point for the Web of Data. Web Semant., 7 (3),
154‚Äì165.
Brin, S., & Page, L. (1998). The anatomy of a large-scale hypertextual Web search engine.
Comput. Netw. ISDN Syst., 30 (1-7), 107‚Äì117.
Budanitsky, A., & Hirst, G. (2006). Evaluating WordNet-based Measures of Lexical Semantic Relatedness. Comput. Linguist., 32 (1), 13‚Äì47.
Bunescu, R. C., & Pasca, M. (2006). Using Encyclopedic Knowledge for Named entity
Disambiguation. In Proceedings of the 11st Conference of the European Chapter of
the Association for Computational Linguistics, EACL.
Cao, Y., Lin, C., & Zheng, G. (2011). MSRA at TAC 2011: Entity Linking. In Proceedings
of the Knowledge Base Population (KBP) track of the 4th Text Analysis Conference
(TAC). National Institute of Standards and Technololgy (NIST).
Cao, Z., Qin, T., Liu, T.-Y., Tsai, M.-F., & Li, H. (2007). Learning to rank: from pairwise
approach to listwise approach. In ICML ‚Äô07: Proceedings of the 24th international
conference on Machine learning, pp. 129‚Äì136, New York, NY, USA. ACM.
Chang, A., Spitkovsky, V., Yeh, E., Aguirre, E., & Manning, C. (2010). Stanford-UBC Entity
Linking at TAC-KBP. In Proceedings of the Knowledge Base Population (KBP) track
of the 3rd Text Analysis Conference (TAC).
Chen, Z., & Ji, H. (2011). Collaborative ranking: a case study on entity linking. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP
‚Äô11, pp. 771‚Äì781, Stroudsburg, PA, USA. Association for Computational Linguistics.
Cilibrasi, R. L., & Vitanyi, P. M. B. (2007). The Google Similarity Distance. IEEE Trans.
on Knowl. and Data Eng., 19 (3), 370‚Äì383.
Cucerzan, S. (2007). Large-Scale Named Entity Disambiguation Based on Wikipedia Data.
In Proceedings of EMNLP-CoNLL 2007, pp. 708‚Äì716.
Cucerzan, S. (2012). The MSR System for Entity Linking at TAC 2012. In Proceedings
of the Knowledge Base Population (KBP) track of the 5th Text Analysis Conference
(TAC).
da Silva, J. F., & Lopes, G. P. (1999). A local maxima method and a fair dispersion normalization for extracting multi-word units from corpora. In Sixth Meeting of Mathematics
of Language.
Dredze, M., McNamee, P., Rao, D., Gerber, A., & Finin, T. (2010). Entity disambiguation
for knowledge base population. In Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ‚Äô10, pp. 277‚Äì285, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Erbs, N., Zesch, T., & Gurevych, I. (2011). Link Discovery: A Comprehensive Analysis. In
Proceedings of the 2011 IEEE Fifth International Conference on Semantic Computing,
ICSC ‚Äô11, pp. 83‚Äì86, Washington, DC, USA. IEEE Computer Society.
768

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

Fader, A., Soderland, S., & Etzioni, O. (2009). Scaling Wikipedia-based Named Entity
Disambiguation to Arbitrary Web Text. In Proceedings of the WikiAI 09 - IJCAI
Workshop: User Contributed Knowledge and Artificial Intelligence: An Evolving Synergy, Pasadena, CA, USA.
Fahrni, A., Nastase, V., & Strube, M. (2011). HITS‚Äô Cross-lingual Entity Linking System
at TAC 2011: One Model for All Languages. In Proceedings of the Knowledge Base
Population (KBP) track of the 4th Text Analysis Conference (TAC). National Institute
of Standards and Technololgy (NIST).
FernaÃÅndez, N., Fisteus, J., SaÃÅnchez, L., & Martin, E. (2010). WebTLab: A Cooccurence‚Äì
based Approach to KBP 2010 Entity-Linking Task. In Proceedings of the Knowledge
Base Population (KBP) track of the 3rd Text Analysis Conference (TAC). National
Institute of Standards and Technololgy (NIST).
FernaÃÅndez, N., BlaÃÅzquez, J. M., Fisteus, J. A., SaÃÅnchez, L., Sintek, M., Bernardi, A., Fuentes,
M., Marrara, A., & Ben-Asher, Z. (2006). NEWS: Bringing Semantic Web Technologies into News Agencies. In The Semantic Web - ISWC 2006, Vol. 4273 of Lecture
Notes in Computer Science, pp. 778‚Äì791. Springer Berlin Heidelberg.
Finkel, J. R., Grenager, T., & Manning, C. (2005). Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43nd
Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp.
363‚Äì370.
Geng, X., Liu, T.-Y., Qin, T., & Li, H. (2007). Feature selection for ranking. In Proceedings
of the 30th annual international ACM SIGIR conference on Research and development
in information retrieval, SIGIR ‚Äô07, pp. 407‚Äì414, New York, NY, USA. ACM.
Gentile, A., Zhang, Z., Xia, L., & Iria, J. (2009). Graph-based Semantic Relatedness for
Named Entity Disambiguation. In Proceeding of the 1st International Conference on
Software, Services and Semantic Technologies (S3T).
Getoor, L., & Diehl, C. P. (2005). Link mining: a survey. SIGKDD Explor. Newsl., 7 (2),
3‚Äì12.
Gracia, J., & Mena, E. (2008). Web-Based Measure of Semantic Relatedness. In Proceedings
of the 9th international conference on Web Information Systems Engineering, WISE
‚Äô08, pp. 136‚Äì150.
Guo, Y., Tang, G., Che, W., Liu, T., & Li, S. (2011). HIT Approaches to Entity Linking at
TAC 2011. In Proceedings of the Knowledge Base Population (KBP) track of the 4th
Text Analysis Conference (TAC). National Institute of Standards and Technololgy
(NIST).
Hachey, B., Radford, W., & Curran, J. R. (2011). Graph-based named entity linking with
Wikipedia. In Proceedings of the 12th international conference on Web information
system engineering, WISE‚Äô11, pp. 213‚Äì226, Berlin, Heidelberg. Springer-Verlag.
Hachey, B., Radford, W., Nothman, J., Honnibal, M., & Curran, J. R. (2013). Evaluating
Entity Linking with Wikipedia. Artificial Intelligence, 194 (0), 130 ‚Äì 150.
Han, X., & Sun, L. (2011). A generative entity-mention model for linking entities with
knowledge base. In Proceedings of the 49th Annual Meeting of the Association for
769

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

Computational Linguistics: Human Language Technologies - Volume 1, HLT ‚Äô11, pp.
945‚Äì954, Stroudsburg, PA, USA. Association for Computational Linguistics.
Han, X., Sun, L., & Zhao, J. (2011). Collective entity linking in web text: a graph-based
method. In Proceedings of the 34th international ACM SIGIR conference on Research
and development in Information Retrieval, SIGIR ‚Äô11, pp. 765‚Äì774, New York, NY,
USA. ACM.
Han, X., & Zhao, J. (2009). Named entity disambiguation by leveraging Wikipedia semantic
knowledge. In Proceedings of the 18th ACM conference on Information and knowledge
management, CIKM ‚Äô09, pp. 215‚Äì224, New York, NY, USA. ACM.
Haveliwala, T. H. (2003). Topic-Sensitive PageRank: A Context-Sensitive Ranking Algorithm for Web Search. IEEE Trans. on Knowl. and Data Eng., 15 (4), 784‚Äì796.
Huang, D. W., Xu, Y., Trotman, A., & Geva, S. (2008). Overview of INEX 2007 Link
the Wiki Track. In Fuhr, N., Kamps, J., Lalmas, M., & Trotman, A. (Eds.), Focused
Access to XML Documents, pp. 373‚Äì387. Springer-Verlag, Berlin, Heidelberg.
Huang, D. W. C., Geva, S., & Trotman, A. (2009). Overview of the INEX 2008 Link the
Wiki Track. In Geva, S., Kamps, J., & Trotman, A. (Eds.), Advances in Focused
Retrieval, Vol. 5631 of Lecture Notes in Computer Science, pp. 314‚Äì325. Springer
Berlin Heidelberg.
Huang, W., Geva, S., & Trotman, A. (2010). Overview of the INEX 2009 Link the Wiki
Track. In Geva, S., Kamps, J., & Trotman, A. (Eds.), Focused Retrieval and Evaluation, Vol. 6203 of Lecture Notes in Computer Science, pp. 312‚Äì323. Springer Berlin
Heidelberg.
INEX (2014). INEX 2014 main page. Available at:
https://inex.mmci.uni-saarland.de/.
Ji, H., & Grishman, R. (2011). Knowledge base population: Successful approaches and
challenges. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1148‚Äì1158.
Ji, H., Grishman, R., & Dang, H. T. (2011). Overview of the TAC2011 Knowledge Base
Population Track. In Proceedings of the Knowledge Base Population (KBP) track of
the 4th Text Analysis Conference (TAC).
Ji, H., Grishman, R., Dang, H. T., Griffitt, K., & Ellis, J. (2010). Overview of the TAC2010
Knowledge Base Population Track. In Proceedings of the Knowledge Base Population
(KBP) track of the 3rd Text Analysis Conference (TAC).
JimeÃÅnez, M., FernaÃÅndez, N., Fisteus, J., & SaÃÅnchez, L. (2013). WikiIdRank++: extensions
and improvements of the WikiIdRank system for entity linking. International Journal
on Artificial Intelligence Tools, 22 (3).
Joachims, T. (2002). Optimizing search engines using clickthrough data. In Proceedings of
the eighth ACM SIGKDD international conference on Knowledge discovery and data
mining, KDD ‚Äô02, pp. 133‚Äì142, New York, NY, USA. ACM.
Kulkarni, S., Singh, A., Ramakrishnan, G., & Chakrabarti, S. (2009). Collective annotation
of Wikipedia entities in web text. In Proceedings of the 15th ACM SIGKDD inter770

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

national conference on Knowledge discovery and data mining, KDD ‚Äô09, pp. 457‚Äì466,
New York, NY, USA. ACM.
Lehmann, J., Monahan, S., Nezda, L., Jung, A., & Shi, Y. (2010). LCC Approaches to
Knowledge Base Population at TAC 2010. In Proceedings of the Knowledge Base
Population (KBP) track of the 3rd Text Analysis Conference (TAC).
Li, X., Szpakowicz, S., & Matwin, S. (1995). A WordNet-based algorithm for word sense
disambiguation. In Proceedings of the 14th international joint conference on Artificial
intelligence - Volume 2, IJCAI‚Äô95, pp. 1368‚Äì1374, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Lin, W.-P., Snover, M., & Ji, H. (2011). Unsupervised language-independent name translation mining from Wikipedia infoboxes. In Proceedings of the First Workshop on
Unsupervised Learning in NLP, EMNLP ‚Äô11, pp. 43‚Äì52, Stroudsburg, PA, USA. Association for Computational Linguistics.
Liu, T.-Y. (2009). Learning to Rank for Information Retrieval. Found. Trends Inf. Retr.,
3 (3), 225‚Äì331.
Manning, C. D., Raghavan, P., & Schtze, H. (2008). Introduction to Information Retrieval.
Cambridge University Press, New York, NY, USA.
Mihalcea, R., & Csomai, A. (2007). Wikify!: linking documents to encyclopedic knowledge.
In Proceedings of the sixteenth ACM conference on Conference on information and
knowledge management, CIKM ‚Äô07, pp. 233‚Äì242, New York, NY, USA. ACM.
Miller, G. A. (1995). WordNet: a lexical database for English. Commun. ACM, 38 (11),
39‚Äì41.
Milne, D., & Witten, I. H. (2008a). An effective, low-cost measure of semantic relatedness obtained from Wikipedia links. In Proceedings of the first AAAI Workshop on
Wikipedia and Artificial Intelligence (WIKIAI‚Äô08).
Milne, D., & Witten, I. H. (2008b). Learning to link with Wikipedia. In Proceedings of
the 17th ACM conference on Information and knowledge management, CIKM ‚Äô08, pp.
509‚Äì518, New York, NY, USA. ACM.
MSNBC (2014). MSNBC: news, video and progressive community. Available at:
http://www.msnbc.msn.com.
National Institute of Standards and Technology (2014a). Text Analysis Conference (TAC).
Available at: http://www.nist.gov/tac/.
National Institute of Standards and Technology (2014b). Text Analysis Conference (TAC)
KBP 2014 Tracks. Available at: http://www.nist.gov/tac/2014/KBP/.
Navigli, R. (2009). Word sense disambiguation: A survey. ACM Comput. Surv., 41 (2),
10:1‚Äì10:69.
Navigli, R., & Lapata, M. (2010). An Experimental Study of Graph Connectivity for
Unsupervised Word Sense Disambiguation. IEEE Trans. Pattern Anal. Mach. Intell.,
32 (4), 678‚Äì692.
771

FernaÃÅndez Garcƒ±ÃÅa, Arias Fisteus & SaÃÅnchez FernaÃÅndez

Nguyen, H., & Cao, T. (2010). Exploring Wikipedia and Text Features for Named Entity
Disambiguation. In Nguyen, N., Le, M., & Swiatek, J. (Eds.), Intelligent Information
and Database Systems, Vol. 5991 of Lecture Notes in Computer Science, pp. 11‚Äì20.
Springer Berlin / Heidelberg.
Nothman, J., Murphy, T., & Curran, J. R. (2009). Analysing Wikipedia and gold-standard
corpora for NER training. In Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguistics, EACL ‚Äô09, pp. 612‚Äì620,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Page, L., Brin, S., Motwani, R., & Winograd, T. (1999). The PageRank Citation Ranking:
Bringing Order to the Web. Technical report 1999-66, Stanford InfoLab.
Pilz, A. (2010). Entity Disambiguation using Link based Relations extracted from
Wikipedia. In First Workshop on Automated Knowledge Base Construction (AKBC
2010), Grenoble, France.
Ploch, D., Hennig, L., de Luca, E. W., & Albayrak, S. (2011). DAI Approaches to the
TAC-KBP 2011 Entity Linking Task. In Proceedings of the Knowledge Base Population (KBP) track of the 4th Text Analysis Conference (TAC). National Institute of
Standards and Technololgy (NIST).
Ponzetto, S. P., & Strube, M. (2007). Knowledge derived from Wikipedia for computing
semantic relatedness. J. Artif. Int. Res., 30 (1), 181‚Äì212.
Radford, W., Hachey, B., Nothma, J., Honnibal, M., & Curran, J. (2010). CMCRC at
TAC 2010: Document-level Entity Linking with graph-based re-ranking. In Proceedings of the 3rd Text Analysis Conference (TAC), National Institute of Standards and
Technology, NIST, Maryland, USA.
Ratinov, L., Roth, D., Downey, D., & Anderson, M. (2011). Local and global algorithms
for disambiguation to Wikipedia. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies - Volume
1, HLT ‚Äô11, pp. 1375‚Äì1384, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Sil, A. (2013). Exploring Re-ranking Approaches for Joint Named-entityrecognition and
Linking. In Proceedings of the Sixth Workshop on Ph.D. Students in Information and
Knowledge Management, PIKM ‚Äô13, pp. 11‚Äì18.
Spitzer, F. (1976). Principles of Random Walk (2nd Edition). Springer.
Van B. Dang (2014). RankLib (software package). Available at:
http://people.cs.umass.edu/Àúvdang/ranklib.html.
Vanwinckelen, Gitte; Blockeel, H. (2012). On estimating model accuracy with repeated
cross-validation. In Proceedings of the 21st Belgian-Dutch Conference on Machine
Learning, pp. 39‚Äì44.
Voorhees, E. (1999). TREC-8 Question Answering Track Report. In Proceedings of the 8th
Text Retrieval Conference, pp. 77‚Äì82.
Wikinews (2014a). Wikinews Random Page Generator. Available at:
http://en.wikinews.org/wiki/Special:Random.
772

Evaluation of Link-Based Approaches for Candidate Ranking in Link-to-Wikipedia

Wikinews (2014b). Wikinews, the free news source. Available at:
http://en.wikinews.org/.
Wikipedia (2014a). Wikipedia Random Page Generator. Available at:
http://en.wikipedia.org/wiki/Special:Random.
Wikipedia (2014b). Wikipedia:Namespace - Wikipedia, the free encyclopedia. Available at:
http://en.wikipedia.org/wiki/Wikipedia:Namespace.
Yeh, E., Ramage, D., Manning, C. D., Aguirre, E., & Soroa, A. (2009). WikiWalk: random
walks on Wikipedia for semantic relatedness. In Proceedings of the 2009 Workshop
on Graph-based Methods for Natural Language Processing, TextGraphs-4, pp. 41‚Äì49,
Stroudsburg, PA, USA. Association for Computational Linguistics.

773

Journal of Artificial Intelligence Research 49 (2014) 269‚Äì321

Submitted 09/13; published 02/14

Efficient HEX-Program Evaluation Based on Unfounded Sets
EITER @ KR . TUWIEN . AC . AT
FINK @ KR . TUWIEN . AC . AT
TKREN @ KR . TUWIEN . AC . AT

Thomas Eiter
Michael Fink
Thomas Krennwallner
Christoph Redl

REDL @ KR . TUWIEN . AC . AT

Institut fuÃàr Informationssysteme
Technische UniversitaÃàt Wien
Favoritenstra√üe 9-11, A-1040 Vienna, Austria
PETERSCHUELLER @ SABANCIUNIV. EDU

Peter SchuÃàller
Faculty of Engineering and Natural Sciences
Sabanci University
Orhanli, Tuzla, 34956 Istanbul, Turkey

Abstract
HEX-programs extend logic programs under the answer set semantics with external computations through external atoms. As reasoning from ground Horn programs with nonmonotonic external atoms of polynomial complexity is already on the second level of the polynomial hierarchy,
minimality checking of answer set candidates needs special attention. To this end, we present an
approach based on unfounded sets as a generalization of related techniques for ASP programs. The
unfounded set detection is expressed as a propositional SAT problem, for which we provide two
different encodings and optimizations to them. We then integrate our approach into a previously
developed evaluation framework for HEX-programs, which is enriched by additional learning techniques that aim at avoiding the reconstruction of the same or related unfounded sets. Furthermore,
we provide a syntactic criterion that allows one to skip the minimality check in many cases. An
experimental evaluation shows that the new approach significantly decreases runtime.

1. Introduction
Answer Set Programming (ASP) is a declarative problem solving approach. Due to expressive
extensions and efficient systems like SMODELS (Simons, NiemelaÃà, & Soininen, 2002), DLV (Leone,
Pfeifer, Faber, Eiter, Gottlob, Perri, & Scarcello, 2006) and CLASP (Gebser, Kaufmann, & Schaub,
2012), it has been gaining popularity for many applications (Brewka, Eiter, & TruszczynÃÅski, 2011).
However, current trends in computing, such as context awareness or distributed systems, raised the
need for access to external sources in a program. For instance, external sources on the Web range
from light-weight data access (e.g., XML, RDF, or data bases) to knowledge-intensive formalisms
(e.g., OWL ontologies).
To cater for this need, Eiter, Ianni, Schindlauer, and Tompits (2005) defined HEX-programs as
an extension of ASP with so-called external atoms, through which the user can couple any external
information source with a logic program. Roughly, such atoms pass information, given by predicate extensions, from the program to an external source which returns output values of an (abstract)
function that it computes. For example, a rule nb(X, Y ) ‚Üê &neighbor [0 map 0 , X](Y ) may informally import for a point X on a map that is stored in the file map (in a particular data format), each
point Y in the neighborhood of X into the predicate nb. Such convenient external access has been
c
2014
AI Access Foundation. All rights reserved.

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

exploited for many applications, including querying data and ontologies (Eiter et al., 2008b; Hoehndorf et al., 2007; Marano et al., 2010), e-government (ZirtilogÃålu & Yolum, 2008), fuzzy answer set
programming (Nieuwenborgh, Cock, & Vermeir, 2007a), multi-context reasoning (Brewka & Eiter,
2007; Eiter et al., 2012b), (Nieuwenborgh et al., 2007b; Basol et al., 2010). The formalism is highly
expressive as recursive data exchange between the rules and external sources is possible.
The semantics of HEX-programs is model-based and given by answer sets following the approach of Faber, Leone, and Pfeifer (2011), which extends the answer set semantics of logic programs (Gelfond & Lifschitz, 1991) to logic programs with aggregates; the Faber et al. approach
(known as the FLP semantics) preserves the property that answer sets have, in the spirit of the
closed world assumption, smallest positive information content, which is formally captured by a
minimality condition on models.
The current approach for the evaluation of HEX-programs (Eiter et al., 2006a, 2011) is to rewrite
a given HEX-program to an answer set program by (i) eliminating external atoms in favor of auxiliary
atoms using so called replacement atoms, and (ii) introducing auxiliary rules such that the answer
sets of the HEX-program Œ† correspond to a subset of the answers sets of the resulting program Œ†ÃÇ
in which the auxiliary atoms faithfully represent the values of the external atoms; this compatibility
condition of an answer set of Œ†ÃÇ is tested in a postcheck.
For computing the answer sets of a (disjunctive) logic program P like Œ†ÃÇ, different methods have
been proposed. An immediate one is to implement the definition of an answer set and test whether
a given interpretation is a minimal model of the so called reduct of the program P wrt. the interpretation; to this end, a suitable candidate answer set might be guessed or generated by heuristics.
This approach was essentially adopted for the solvers G N T (Janhunen et al., 2006) and CMODELS
(Lierler, 2005), which use for this test a logic program, respectively a SAT encoding. A different
approach was presented by Leone et al. (1997) based on the notion of unfounded set (Van Gelder,
Ross, & Schlipf, 1991), which they extended from normal (non-disjunctive) to disjunctive logic
programs. Intuitively, a set U of atoms is unfounded wrt. to a model of a program P , if switching
all atoms in U to false does not lead to violated rules; the answer sets of P are then its models
that are unfounded-free, i.e., the models disjoint from all respective unfounded sets. For checking
(un)foundedness of a given candidate answer set, Koch et al. (2003) presented a SAT encoding.
Drescher et al. (2008) later exploited findings of them and Leone et al. to extend the technique of
conflict-driven clause learning used by the CLASP solver to disjunctive logic programs.
In all the quoted works, however, access to external sources was not an issue, and thus they
cannot be deployed to HEX-programs. In fact, in addition to the compatibility check of an answer set
of the replacement program Œ†ÃÇ, the current HEX evaluation must in a second step test the minimality
of the interpretation induced for the HEX-program Œ† wrt. the program reduct. This method, which
we refer to as the explicit FLP check, turns out to be less efficient in practice, and it often dominates
the total runtime; thus a more efficient method is desirable.
Motivated by this and the seminal approach of Leone et al., we consider in this paper the use of
unfounded sets as an alternative to the explicit FLP check for HEX-programs, which we refer to as
the unfounded set check. To this end, we extend the notion of unfounded sets for disjunctive logic
programs to HEX-programs, following the lines of Faber (2005), where unfounded sets for logic
programs with aggregates were defined, and consider their use in combination with clause learning
techniques. Our main contributions are summarized as follows:
‚Ä¢ We present a basic encoding of unfounded set existence to a set of nogoods, i.e., constraints
that have to be satisfied, and we show that its solutions are in 1-1 correspondence with the
270

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

unfounded sets. The latter can thus be computed running a SAT solver, followed by a postprocessing step which checks that the values of replacement atoms are compatible with the
external call results. Benchmarks show that this strategy is already more efficient than the
explicit FLP check.
‚Ä¢ We then present an advanced encoding of unfounded set existence that is reusable for any
interpretation. Compared to our first encoding, it is conceptually more involved and has
(slightly) higher initialization cost, but it has the advantage that it can be reused for all unfounded set checks and needs no separate initialization for each check. Our benchmarks show
that the advanced encoding is superior to the first one for many practical problems.
‚Ä¢ Next, we consider optimizations which hinge on dependencies between external and ordinary
atoms that are determined in careful analysis. These optimizations can be integrated into our
encodings by adding further nogoods which restrict the search space to relevant parts.
‚Ä¢ We consider how to exploit information gained in the unfounded set check of a candidate answer set in answer set candidate generation, i.e., in the evaluation of the program Œ†ÃÇ. Adopting a Conflict Driven Clause Learning approach (Drescher et al., 2008), this step has been
recently enhanced by external behavior learning (Eiter et al., 2012a), in which nogoods describing the external source behavior are learned during the search to guide model generation
towards proper guesses. We show how to learn in the candidate generation step additional
nogoods from unfounded sets that avoid the reconstruction of the same or related unfounded
sets, yielding further gain.
‚Ä¢ We present a syntactic decision criterion that can be used to decide whether a program possibly has unfounded sets. If the result of this check is negative, then the computationally
expensive search for unfounded sets can be skipped entirely. The criterion is based on atom
dependency and, loosely speaking, says that there are no cyclic dependencies of ground atoms
through external atoms. This property can be efficiently checked for a given ground HEXprogram using standard methods. In fact it applies to a range of applications, in particular for
input-stratified programs, where external sources are accessed in a workflow to produce input for the next stage of computation. However, advanced applications of HEX-programs can
have cycles through external atoms, e.g., in natural encodings of problems on multi-context
systems (Brewka & Eiter, 2007) or abstract argumentation systems (Dung, 1995), for which
the FLP check can not be simply skipped.
‚Ä¢ In further elaboration, we then consider a program decomposition based on the dependency
graph that is induced by a program Œ† (note that exploiting syntactic modularization of unfounded sets can be traced back to Leone et al., 1997; Koch et al., 2003). We show that Œ†
has some unfounded set wrt. a candidate answer set A exactly if some of the components Œ†C
in its decomposition has some unfounded set wrt. A; as computing the decomposition can be
realized efficiently and does not incur large overhead, we can apply the decision criterion for
skipping the FLP check efficiently on a finer-grained level, and the search for unfounded sets
can be guided to relevant program parts.
‚Ä¢ An experimental evaluation on advanced reasoning applications shows that unfounded sets
checking combined with learning methods of Eiter et al. (2012a) improves HEX-program
evaluation considerably, sometimes drastically. More specifically, the benchmark applications
271

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

include reasoning tasks in Multi-Context Systems (Brewka & Eiter, 2007; Eiter et al., 2012b),
abstract argumentation (Dung, 1995), terminological default reasoning over description logic
knowledge bases (Baader & Hollunder, 1995), and conformant planning (Goldman & Boddy,
1996; Smith & Weld, 1998). The experiments have been carried out with DLVHEX version
2.3.0, a prototype solver for HEX-programs, which was extended to support the techniques
developed in this paper. The decomposition approach can yield a considerable gain, as it
appears e.g. for the HEX-encoding of a Dung-style argumentation semantics (Dung, 1995)
and DL-programs (Eiter et al., 2008a). On the other hand, for the terminological default
reasoning benchmark, our syntactic criterion lets us conclude that the FLP check is obsolete.
In conclusion, the new approach enables significant speedup and thus enlarges the scope of
HEX applicability.
1.1 Organization
The rest of this paper is organized as follows. The next section provides preliminaries on HEXprograms and their evaluation via answer sets of a transformed ASP program without external
atoms. In Section 3, we define unfounded sets and present a basic and a uniform encoding of
unfounded set search using nogoods. Section 4 considers refinements and optimizations of the
encodings, as well as external behavior learning to prevent reconstruction of unfounded sets. In
Section 5, we give a syntactic decision criterion to avoid the FLP check and a program decomposition method exploiting it. Experimental results of a prototype implementation are reported in
Section 6. In Section 7, we consider related work and extensions of our approach. In Section 8, we
conclude and point out issues for further research.

2. Preliminaries
In this section, we start with some basic definitions, and then introduce HEX-programs.
In accordance with Gebser et al. (2012) and Eiter et al. (2012a), a (signed) literal is a positive or a
negative formula Ta resp. Fa, where a is a ground atom of form p(c1 , . . . , c` ), with predicate p and
constants c1 , . . . , c` , abbreviated p(c). For a literal œÉ = Ta or œÉ = Fa, let œÉ denote its opposite, i.e.,
Ta = Fa and Fa = Ta. An assignment A over a (finite) set of atoms A is a consistent set of signed
literals Ta or Fa, where Ta expresses that a ‚àà A is true and Fa that it is false; A is complete,
also called an interpretation, if no assignment A0 ‚äÉ A exists. We denote by AT = {a | Ta ‚àà A}
and AF = {a | Fa ‚àà A} the set of atoms that are true resp. false in A, and by ext(q, A) = {c |
Tq(c) ‚àà A} the extension of a predicate q in A. Furthermore, A|q is the set of all literals over
atoms with predicate q in A. ForSa list q = q1 , . . . , qk of predicates, we write p ‚àà q iff qi = p for
some 1 ‚â§ i ‚â§ k, and let A|q = j A|qj .
A nogood is a set {L1 , . . . , Ln } of signed literals Li , 1 ‚â§ i ‚â§ n. An interpretation A is a
solution to a nogood Œ¥ (resp. to a set ‚àÜ of nogoods), iff Œ¥ 6‚äÜ A (resp. Œ¥ 6‚äÜ A for all Œ¥ ‚àà ‚àÜ).
Example 1 The interpretation A = {Ta, Fb, Tc} is a solution to the nogood {Ta, Tb, Tc} but
not to {Ta, Fb, Tc}.
2.1 HEX-Programs
Next, we recall syntax and semantics of HEX-programs.
272

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

2.1.1 HEX -P ROGRAM S YNTAX
As introduced by Eiter et al. (2005), HEX-programs are a generalization of (disjunctive) extended
logic programs under the answer set semantics (Gelfond & Lifschitz, 1991). HEX-programs extend
ASP programs by external atoms, which enable a bidirectional interaction between a program and
external sources of computation. External atoms have a list of input parameters (constants or predicate names) and a list of output parameters. Informally, to evaluate an external atom, the reasoner
passes the constants and extensions of the predicates in the input tuple to the external source associated with the external atom. The external source computes output tuples which are matched with
the output list. Syntactically, a ground external atom is of the form
&g[p](c),

(1)

where &g is an external predicate, p = p1 , . . . , pk are the input list consisting of predicate names
or object constants, and c = c1 , . . . , cl are the output list consisting of constant terms. Predicates in
the input list are sometimes called input predicates.
A default literal is a formula b or not b, where b is a ground ordinary atom of form p(c1 , . . . , c` )
with constants ci , 1 ‚â§ i ‚â§ `, or and external atom. For every set S of ordinary and external atoms,
we let not S = {not b | b ‚àà S}.
Ground HEX-programs are then defined similar to ground ASP programs.
Definition 1 (Ground HEX-programs) A ground HEX-program consists of rules
a1 ‚à® ¬∑ ¬∑ ¬∑ ‚à® ak ‚Üê b1 , . . . , bm , not bm+1 , . . . , not bn ,

(2)

where each ai is an ordinary ground atom, each bj is either an ordinary ground atom or a ground
external atom, and k + n > 0.1
For a rule r, the head is H(r) = {a1 , . . . , ak } and the body is B(r) = B + (r) ‚à™ not B ‚àí (r),
where B + (r) = {b1 , . . . , bm } is the positive body, B ‚àí (r) = {bm+1 , . . . , bn } is the negative body.
If B(r) = ‚àÖ, then r is a fact, and we omit ‚Üê. For a program Œ†, let A(Œ†) be the set of all ordinary
atoms and EA(Œ†) be the set of all external atoms occurring in Œ†. For a default literal b, let tb = Ta
if b = a for an atom a, and tb = Fa if b = not a. Furthermore, f b = Fa if b = a and f b = Ta if
b = not a.
We call a rule r a constraint, if B(r) = ‚àÖ.
Example 2 For rule r = a‚à®b ‚Üê c, not d we have H(r) = {a, b}, B + (r) = {c} and B ‚àí (r) = {d}.
We further have tc = Tc, f c = Fc, t not d = Fd and f not d = Td.
We will also consider non-ground programs (i.e., with variables allowed in place of object constants) in our examples. In particular, external atoms &g[X](Y) may contain variables in their
input list X and output list Y. For such programs, suitable safety conditions allow for using a
grounding procedure which transforms the program to a variable-free program with the same answer sets (Eiter, Ianni, Schindlauer, & Tompits, 2006). However, we limit our formal investigation
to ground programs.
1. For simplicity, we do not formally introduce strong negation but view, as customary, classical literals ¬¨a as new
atoms together with a constraint which disallows that a and ¬¨a are simultaneously true.

273

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

Example 3 The program
domain(a); domain(b)
sel (X) ‚Üê domain(X), not nsel (X)

nsel (X) ‚Üê domain(X), not sel (X)

encodes the problem of partitioning two domain elements a and b into two sets sel and nsel .
2.1.2 HEX -P ROGRAM S EMANTICS
An ordinary ground atom a is true relative to assignment A, denoted A |= a, if Ta ‚àà A and
false otherwise. A default-negated ground atom not a is true relative to assignment A, denoted
A |= not a, if Fa ‚àà A and false otherwise.
The semantics of a ground external atom of form (1) wrt. an interpretation A is given by the
value of a 1+k+l-ary Boolean oracle function f&g that is defined for all possible values of A, p and
c, such that &g[p](c) is true relative to A, denoted A |= &g[p](c), if and only if f&g (A, p, c) = 1.
Example 4 (Set Partitioning) Consider the program Œ†

sel (a) ‚Üê domain(a), &diff [domain, nsel ](a)

nsel (a) ‚Üê domain(a), &diff [domain, sel ](a)

domain(a)

where for predicates p and q, &diff [p, q](X) computes the set of all elements X which are in
the extension of p but not in the extension of q. Informally, this program implements a choice
from sel (a) and nsel (a).
Satisfaction of ordinary rules and ASP programs (Gelfond & Lifschitz, 1991) is then extended to
and programs in the obvious way: a rule r is satisfied by assignment A, denoted A |= r,
iff A |= h for some h ‚àà H(r), or A 6|= b for some b ‚àà B + (r), or A |= b for some b ‚àà B ‚àí (r). A
program Œ† is satisfied by assignment A iff A |= r for all r ‚àà Œ†. An interpretation A is a model of
a program Œ†, denoted A |= Œ†, iff A |= r for all r ‚àà Œ†.
The notion of extension ext(¬∑, A) for external predicates &g with input lists p is naturally
defined by ext(&g[p], A) = {c | f&g (A, p, c) = 1}.
HEX -rules

Definition 2 (FLP-Reduct (Faber et al., 2011)) For an interpretation A over a program Œ†, the
FLP-reduct of Œ† wrt. A is the set f Œ†A = {r ‚àà Œ† | A |= b, for all b ‚àà B(r)} of all rules whose
body is satisfied by A.
An assignment A1 is smaller or equal to another assignment A2 wrt. a program Œ†, denoted
A1 ‚â§Œ† A2 , iff {Ta ‚àà A1 | a ‚àà A(Œ†)} ‚äÜ {Ta ‚àà A2 | a ‚àà A(Œ†)}.
Definition 3 (Answer Set) An answer set of Œ† is a ‚â§Œ† -minimal model A of f Œ†A .
Example 5 Consider the program Œ†:

p ‚Üê &id [q]()
q‚Üêp

where f&id (A, p) = 1 iff Tp ‚àà A is true. Then Œ† has the answer set A1 = ‚àÖ; indeed it is a
‚â§Œ† -minimal model of f Œ†A1 = ‚àÖ. Note that A2 = {Tp, Tq} is not an answer set of Œ†, as it is not
a minimal model of f Œ†A2 = Œ†.
274

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

2.2 Evaluation of HEX-Programs
A possible way to determine the answer sets of a HEX-program Œ† is to use a transformation to an
ASP program without external atoms whose answer sets encompass all answer sets of Œ†. We now
describe such a transformation. Each external atom a = &g[p](c) in a rule r ‚àà Œ† is replaced
by an ordinary ground replacement atom aÃÇ = e&g[p] (c) (resulting in a rule rÃÇ), and an additional
rule e&g[p] (c) ‚à® ne &g[p] (c) ‚Üê is added to the program. The answer sets of the resulting guessing
program Œ†ÃÇ are determined by an ASP solver and projected to non-replacement atoms. However,
the resulting assignments might be spurious answer sets of Œ†, as the values of &g[p] and e&g[p] (c)
relative to an interpretation may not coincide. Each answer set of Œ†ÃÇ is thus merely a candidate which
must be checked against the external sources. If no discrepancy is found, the model candidate is a
compatible set of Œ†. More precisely,
Definition 4 (Compatible Set) A compatible set of a program Œ† is an assignment AÃÇ such that
(i) AÃÇ is an answer set (Gelfond & Lifschitz, 1991) of the guessing program Œ†ÃÇ, and
(ii) f&g (AÃÇ, p, c) = 1 iff Te&g[p] (c) ‚àà AÃÇ for all external atoms &g[p](c) in Œ†, i.e., the guessed
values coincide with the values of the oracle functions.
A subset of the compatible sets of Œ† represents the answer sets of Œ†, where each answer set A
of Œ† is given by the restriction of a unique compatible set AÃÇ to the non-replacement atoms. More
formally, an answer set A of a program Œ† corresponds to the compatible set
Œ∫(Œ†, A) = A ‚à™ {Tea , Fne a | a is an external atom in Œ†, A |= e}

‚à™ {Fea , Tne a | a is an external atom in Œ†, A 6|= e} .

To filter out those compatible sets that do not yield answer sets, each compatible set AÃÇ has to be
checked against models of the FLP reduct. To be more specific, a procedure called explicit FLP
check constructs the reduct f Œ†A and checks whether it has a model A0 smaller than A; if such
an A0 is found, it rejects A, otherwise outputs A as an answer set.
The explicit FLP check rewrites the HEX-program to an ASP program without external atoms
and amounts to the search for answer sets of the following program, in which the truth values of all
replacement atoms coincide with the according oracle function values:
Check (Œ†, A) = f Œ†ÃÇAÃÇ ‚à™ {‚Üê a | a ‚àà A(Œ†), Ta 6‚àà AÃÇ} ‚à™ {a ‚à® a0 ‚Üê| Ta ‚àà AÃÇ}
‚à™ {‚Üê not smaller } ‚à™ {smaller ‚Üê not a | a ‚àà A(Œ†), Ta ‚àà AÃÇ} .
It consists of the reduct f Œ†ÃÇAÃÇ and rules that restrict the search to proper subinterpretations of AÃÇ,
where smaller is a new atom. Moreover, as we actually need to search for models and not just
compatible sets, rules of the form a ‚à® a0 ‚Üê (where a0 is a new atom for each Ta ‚àà AÃÇ) make sure
that atoms can be arbitrarily true without having a justifying rule in Œ†.
Proposition 1 Let A be an interpretation extracted from a compatible set AÃÇ of program Œ†. Then
the program Check (Œ†, A) has an answer set A0 such that f&g (A0 , p, c) = 1 iff Te&g[p] (c) ‚àà A0
for all external atoms &g[p](c) in Œ†, if and only if A is not an answer set of Œ†.
Because of the guessing rules, we can rewrite all rules in f Œ†ÃÇAÃÇ except the guesses on replacement
atoms to constraints as follows:
CheckOptimized (Œ†, A) = f¬ØŒ†ÃÇAÃÇ ‚à™ {‚Üê a | a ‚àà A(Œ†), Ta 6‚àà AÃÇ} ‚à™ {a ‚à® a0 ‚Üê| Ta ‚àà AÃÇ}
‚à™ {‚Üê not smaller } ‚à™ {smaller ‚Üê not a | a ‚àà A(Œ†), Ta ‚àà AÃÇ}.
275

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

add external replacement
atoms + guessing rules

Œ†

Œ†ÃÇ
Step S1

External Atom
Evaluation

Main Search (CDNL)

Plugin
1

Model Candidates
check unverified
external atom guesses

..
.

Compatible Sets

Step S2

FLP Check
(Unfounded Set Check)

Answer
Sets

Plugin
k

Figure 1: Overview of the framework for evaluating HEX-programs.
where f¬ØŒ†ÃÇAÃÇ denotes the FLP reduct of Œ†ÃÇ wrt. interpretation AÃÇ with each rule of form (2) except
guessing rules for replacement atoms being rewritten to
‚Üê not a1 , . . . , not ak , b1 , . . . , bm , not bm+1 , . . . , not bn .
Proposition 2 Let A be an interpretation extracted from a compatible set AÃÇ of program Œ†. Then
the program CheckOptimized (Œ†, A) has an answer set A0 such that f&g (A0 , p, c) = 1 if and only
if Te&g[p] (c) ‚àà A0 for all external atoms &g[p](c) in Œ†, if and only if A is not an answer set of Œ†.
This program is more efficient for evaluation. Our comparison in Section 6 uses this optimized
version of the explicit check, but still demonstrates a significant performance gain by our novel
approach.
Example 6 (cont‚Äôd) Reconsider the program Œ† = { p ‚Üê &id [q](); q ‚Üê p } from above. Then the
corresponding guessing program is Œ†ÃÇ = {p ‚Üê e&id[q] (); q ‚Üê p; e&id[q] ()‚à®ne &id[q] () ‚Üê} and yields
the compatible sets AÃÇ1 = ‚àÖ and AÃÇ2 = {Tp, Tq, Te&id[p] }. While A1 = ‚àÖ is also a ‚â§Œ† -minimal
model of f Œ†A1 = ‚àÖ, A2 = {Tp, Tq} is not a ‚â§Œ† -minimal model of f Œ†A2 = Œ†. Indeed, the
program
Check (Œ†, A2 ) = Œ†ÃÇ ‚à™ {p ‚à® p0 ‚Üê; q ‚à® q 0 ‚Üê; e&id[q] () ‚à® e0&id[q] () ‚Üê} ‚à™ {‚Üê not smaller }

‚à™ {smaller ‚Üê not p} ‚à™ {smaller ‚Üê not q; smaller ‚Üê not e&id[q] ()}

has the answer set A0 =

n
o
Fp, Tp0 , Fq, Tq 0 , Fe&id[q] (), Tne &id[q] (), Te0&id[q] (), Tsmaller and

f&id (A0 , q, ) = 0 and Fe&id[q] () ‚àà A0 .
276

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

Œ†
compatible
set AÃÇ

Basic
Encoding

Uniform
Encoding
‚Ñ¶Œ†

ŒìA
Œ†

Assumptions

AA

search for UFS
candidates
FLP Checks
verify
candidate

Main Search
(CDNL)

External
Atom
Evaluation

UFS
unfounded free

Figure 2: FLP Check based on Unfounded Sets

The complete prodedure of computing answer sets of HEX-programs has been described by Eiter
et al. (2012a) and is shown as a block diagram in Figure 1; the first version introducing this approach
was DLVHEX version 2.1.0. Step S1 enumerates compatible sets; we reuse this part of the evaluation
process from prior work and do not modify it. Step S2 checks whether a compatible set is indeed a
HEX answer set. The improvement of the efficiency of S2 is the focus of this work. S1 transforms
the input program Œ† into Œ†ÃÇ by introducing replacement atoms and guesses for replacement atoms.
The main search enumerates model candidates, i.e., answer sets of Œ†ÃÇ. (Depending on heuristics, this
search can evaluate external atoms for guiding the search.) Model candidates are verified against
the semantics of external atoms. If this check fails, the main search continues to enumerate model
candidates; if the check succeeds, then the model candidate is a compatible set. S2 checks whether
a compatible set is an answer set of Œ†. Previous work realized this step using an explicit FLP check.
In this work we propose two alternatives to carry out this FLP check based on unfounded sets.
Figure 2 depicts a block diagram of the FLP checks proposed in this work, called basic and
uniform encoding. Both encodings are SAT theories. The basic encoding builds on the program
Œ† and the compatible set AÃÇ. The uniform encoding is based only on Œ† (hence we can reuse it for
all compatible sets), however it requires us to set solver assumptions based on AÃÇ. The encoding
(and assumptions) are used to search for unfounded set (UFS) candidates (by a SAT solver). A
UFS candidate has to be verified against the values of external atoms (these are guessed in the UFS
encoding). If all UFS candidates fail the external atom check, or if there are no UFS candidates,
then AÃÇ is an unfounded-free compatible set and hence an answer set of Œ†. Otherwise the FLP check
found an unfounded set wrt. AÃÇ and the main search continues looking for a new model candidate.
We next describe our encodings for UFS-checking.
277

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

3. Unfounded Set Detection
As described in Section 2.2, the minimality check, also called the explicit FLP check, is computationally costly and involves much overhead: all models of Check (Œ†, A) must be enumerated,
and calls to the external sources to test compatibility must be made. Even worse, as we need to
search for a smaller model and not just for a smaller compatible set, Check (Œ†, A) usually (to our
experience) has even more models than the original program. Moreover, it appears that in many
current application scenarios there is no smaller model of the reduct f Œ†A , i.e., most assignments
extracted from compatible sets AÃÇ pass the FLP check. There are two possible reliefs: developing a
cheaper minimality check or to avoid the minimality check if possible. While this section targets at
the former idea, the latter one is addressed in Section 5.
To this end, we present a novel FLP check algorithm based on unfounded sets (UFS). Instead of
explicitly searching for smaller models of the reduct, we check whether the candidate answer set is
unfounded-free. To this end, we use unfounded sets for HEX-programs akin to those by Faber (2005)
for programs with arbitrary aggregates.
Definition 5 (Unfounded Set) Given a program Œ† and an assignment A, let X be any set of ordinary ground atoms appearing in Œ†. Then, X is an unfounded set for Œ† wrt. A if, for each rule
r having
some atoms from X in the head, at least one of the following conditions holds, where
.
A ‚à™ ¬¨.X = (A \ {Ta | a ‚àà X}) ‚à™ {Fa | a ‚àà X}:
(i) some literal of B(r) is false wrt. A,
.

(ii) some literal of B(r) is false wrt. A ‚à™ ¬¨.X, or
(iii) some atom of H(r) \ X is true wrt. A.
Intuitively, an unfounded set is a set of atoms which only circularly support each other; by
assigning all of them false, no violation of any rule will be introduced. As for answer sets, their
minimality enforces now that no subset of the atoms that are true in an answer set can form an unfounded set; in fact, answer sets can be characterized in terms of unfounded sets, using the following
notion.
Definition 6 (Unfounded-free Interpretations) An interpretation A of a program Œ† is unfoundedfree, iff AT ‚à© X = ‚àÖ, for every unfounded set X of Œ† wrt. A.
The following result is a generalization of a respective result for ordinary (disjunctive) logic
programs (Leone et al., 1997) and logic programs with aggregates (Faber, 2005).
Theorem 3 (Characterization of Answer Sets) A model A of a HEX-program Œ† is an answer set
of Œ† iff it is unfounded-free.
Example 7 Consider the program Œ† and A1 from Example 6. Trivially, A1 is unfounded-free, and
thus A1 is an answer set of Œ†. On the other hand, the set X. = {p, q} is an unfounded set w.r.t. A2 ,
since X intersects with the head of p ‚Üê &id [q]() and A ‚à™ ¬¨.X 6|= &id [q](). Therefore A2 is not
unfounded-free and not an answer set of Œ†.
278

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

3.1 Basic Encoding of the Unfounded Set Search
We realize the search for unfounded sets using nogoods, i.e., for a given Œ† and an assignment A we
construct a set of nogoods, such that solutions to this set correspond to (potential) unfounded sets;
we then use a SAT solver to search for such unfounded sets.
More specifically, our encoding of unfounded set detection uses a set
A
A
ŒìA
Œ† = NŒì,Œ† ‚à™ OŒì,Œ† ,
A contains all necessary constraints and the set O A are optional optimization
of nogoods where NŒì,Œ†
Œì,Œ†
nogoods that prune irrelevant parts of the search space; the latter is related to the one of Drescher
et al. (2008) but respects external atoms. The idea is that the set of ordinary atoms of a solution to
ŒìA
Œ† represents a (potential) unfounded set U of Œ† wrt. A, .while the replacement atoms encode the
truth values of the corresponding external atoms under A ‚à™ ¬¨.U .
For a rule r, let Bo+ (r) ‚äÜ B + (r) consist of all ordinary atoms, and let Be (r) ‚äÜ B(r) consist of
A
all external replacement atoms. Then, ŒìA
Œ† is built over atoms A(ŒìŒ† ) = A(Œ†ÃÇ) ‚à™ {hr , lr | r ‚àà Œ†},
where hr , and lr are new atoms for every rule r in Œ†. The necessary part

S
A = {{Fa | Ta ‚àà A}} ‚à™
NŒì,Œ†
r‚ààŒ† Rr,A

of ŒìA
Œ† consists of a nogood {Fa | Ta ‚àà A}, which eliminates unfounded sets that do not intersect
with true atoms in A, and of nogoods Rr,A = Hr,A ‚à™ Cr,A for every r ‚àà Œ† where
‚Ä¢ Hr,A = {{Thr } ‚à™ {Fh | h ‚àà H(r)}} ‚à™ {{Fhr , Th} | h ‚àà H(r)}, called the head criterion,
encodes Ô£±
that hr is true for a rule r iff some atom of H(r) is in the unfounded set; and
Ô£¥
{{Thr } ‚à™
Ô£¥
Ô£¥
Ô£¥
Ô£≤ {Fa | a ‚àà B + (r), A |= a} ‚à™ {ta | a ‚àà B (rÃÇ)} ‚à™
e
o
‚Ä¢ Cr,A =
Ô£¥
{Th
|
h
‚àà
H(r),
A
|=
h}}
if A |= B(r),
Ô£¥
Ô£¥
Ô£¥
Ô£≥{}
otherwise,
called the conditional part Cr,A , encodes that Condition (i), (ii) or (iii) of Definition 5 must
hold if hr is true.
More specifically, for an unfounded set U and a rule r with H(r) ‚à© U 6= ‚àÖ (hr is true) it must
not happen that A |= B(r) (Condition
(i) fails), no a ‚àà Bo+ (r) with A |= a is in the unfounded set
.
and all a ‚àà Be (rÃÇ) are true under A ‚à™ ¬¨.U (Condition (ii) fails), and all h ‚àà H(r) with A |= h are
Œì,A
are defined in Section 4.
in the unfounded set (Condition (iii) fails). Concrete instances for OŒ†
Example 8 Consider Œ† = {r1 : p ‚Üê &id [p]()} and the compatible set AÃÇ = {Tp, Te&id[p] }. The
A2
nogood set NŒì,Œ†
is {{Thr1 , Fp}, {Fhr1 , Tp}, {Thr1 , Te&id[p] (), Tp}}.

Towards computing unfounded sets, observe that every unfounded set can be extended to a solution
A
A
to the nogood set ŒìA
Œ† over A(ŒìŒ† ). Conversely, the solutions to ŒìŒ† include specific extensions of
the unfounded sets, given for each unfounded set U by assigning true to all atoms in U , to all hr
such that H(r) intersects with U , and to all replacement atoms e&g[p] (c) such that &g[p](c) is true
.
under A ‚à™ ¬¨.U , and assigning false to all other atoms in A(ŒìA
Œ† ). More formally,
Definition 7 (Induced Assignment of an Unfounded Set wrt. ŒìA
Œ† ) Let U be an unfounded set of
A
a program Œ† wrt. assignment A. The assignment induced by U wrt. ŒìA
Œ† , denoted IŒì (U, ŒìŒ† , Œ†, A),
is
0
A
0
IŒì (U, ŒìA
Œ† , Œ†, A) = IŒì (U, Œ†, A) ‚à™ {Fa | a ‚àà A(ŒìŒ† ), Ta 6‚àà IŒì (U, Œ†, A)} ,
279

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

where
IŒì0 (U, Œ†, A) = {Ta | a ‚àà U } ‚à™ {Thr | r ‚àà Œ†, H(r) ‚à© U 6= ‚àÖ} ‚à™
.
{Te&g[p] (c) | &g[p](c) ‚àà EA(Œ†), A ‚à™ ¬¨.U |= &g[p](c)} .
We call a set N of nogoods conservative, if it holds for every unfounded set U of Œ† wrt. A that
A
IŒì (U, ŒìA
Œ† , Œ†, A) is a solution to N . We then show that the solutions to ŒìŒ† include all assignments
Œì,A
induced by unfounded sets of Œ† wrt. A, assuming that OŒ† is conservative.
Proposition 4 Let U be an unfounded set of a program Œ† wrt. assignment A such that AT ‚à©U 6= ‚àÖ.
A
Then IŒì (U, ŒìA
Œ† , Œ†, A) is a solution to ŒìŒ† .
Note that the converse does not hold, i.e., not every solution corresponds to some induced assignment; intuitively this is because it does not reflect the semantics of external sources. Regardless
of this we immediately obtain from Proposition 4 a useful test for unfounded-freeness.
T
Corollary 5 If ŒìA
Œ† has no solution, then U ‚à© A = ‚àÖ for every unfounded set U of Œ†.

Using the following result, we can find the unfounded sets of Œ† wrt. A among all solutions
to ŒìA
Œ† by using a postcheck on the external atoms.
Theorem 6 Let S be a solution to ŒìA
Œ† such that
.

(a) Te&g[p] (c) ‚àà S and A 6|= &g[p](c) implies A ‚à™ ¬¨.U |= &g[p](c); and
.

(b) Fe&g[p] (c) ‚àà S and A |= &g[p](c) implies A ‚à™ ¬¨.U 6|= &g[p](c)
where U = {a | a ‚àà A(Œ†), Ta ‚àà S}. Then U is an unfounded set of Œ† wrt. A.
Informally, the proposition states that the non-replacement atoms in S that are true and also
appear in Œ† form an unfounded set, provided that truth of the replacement atoms e&g[p] (c) in S co.
incides with the truth of the corresponding &g[p](c) under A ‚à™ ¬¨.U (as in Definition 7). However,
this check is just required if the truth values of e&g[p] (c) in S and of &g[p](c) under A differ. This
gives rise to an important optimization for the implementation: external atoms, whose (known) truth
value of &g[p](c) under A matches the truth value of e&g[p] (c) in S, do not need to be postchecked.
It follows immediately from Definition 7 that this postcheck does not eliminate unfounded sets,
as formalized by the following proposition.
Proposition 7 Let U be an unfounded set of a program Œ† wrt. assignment A such that AT ‚à©U 6= ‚àÖ.
Then IŒì (U, ŒìA
Œ† , Œ†, A) fulfills Conditions (a) and (b) of Theorem 6.
Example 9 Reconsider program Œ† = {r1 : p ‚Üê &id [p]()} from Example 8 and the compatible set
A2
AÃÇ2 = {Tp, Te&id[p] }. The nogood set NŒì,Œ†
= {{Thr1 , Fp}, {Fhr1 , Tp}, {Thr1 , Te&id[p] (), Tp}}
has solutions S ‚äá {Thr1 , Tp, Fe&id[p] ()}, which correspond to the unfounded set U = {p}. Here,
.
Fe&id[p] () represents that A2 ‚à™ ¬¨.U 6|= &id [p]().
Note that due to the premises in Conditions (a) and (b) of Theorem 6, the postcheck is faster
if Te&g[p] (c) ‚àà S whenever A |= &g[p](c) holds for many external atoms in Œ†. This can be
exploited during the construction of S as follows: if it is not absolutely necessary to set the truth
value of e&g[p] (c) differently, then carry over the value from &g[p](c) under A. Specifically, this is
successful if e&g[p] (c) does not occur in ŒìA
Œ†.
280

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

3.2 Uniform Encoding of the Unfounded Set Search
The encoding ŒìA
Œ† presented in the previous subsection has the disadvantage that it depends on
the current assignment A. Therefore it needs to be generated separately for every unfounded set
check if the assignment changed (which is very likely). As this causes significant overhead, we
present now an advanced encoding which can be reused for any assignment. We introduce some
additional variables which represent the truth values of the atoms in the current assignment. Prior
to an unfounded set check, the current assignment is injected by setting the values of these variables
to fixed values, which can be done using assumptions as supported by modern SAT solvers such
as CLASP. Changing assumptions is much easier than changing the encoding, which leads to an
additional speedup in some cases, especially for programs which need many unfounded set checks.
Our advanced encoding uses a set ‚Ñ¶Œ† of nogoods. As before, the idea is that the set of ordinary
atoms of a solution to ‚Ñ¶Œ† represents a (potential) unfounded set U of Œ† wrt. some assignment A,.
while the replacement atoms encode the truth values of the corresponding external atoms under A ‚à™
¬¨.U . The encoding ‚Ñ¶Œ† is conceptually more complex than ŒìA
Œ† ; the initialization is computationally
(slightly) more costly, hence the advantages of our new encoding become visible for instances with
many compatible sets (thus many unfounded set checks), while it might be counterproductive for
small instances.
The nogood set ‚Ñ¶Œ† is built over the atoms of Œ†ÃÇ and and further fresh atoms not occurring in Œ†ÃÇ:
hr and lr , for every rule r in Œ†, aA for every ordinary atom a ‚àà A(Œ†ÃÇ) (i.e. ordinary atoms in Œ† and
.
replacement atom auxiliaries), and aA‚à™¬¨.U
, aA‚àßU , aA‚à®U for every ordinary atom a ‚àà A(Œ†). The
.
auxiliary atoms aA , aA‚à™¬¨.U
, aA‚àßU , aA‚à®U are used to make the encoding usable for any assignment
A. Only during the unfounded set check with respect to a certain assignment, we will temporarily
add assumptions to the solver which force certain truth values of the atoms aA for all a ‚àà A(Œ†ÃÇ)
depending on the current
assignment A. Intuitively, aA represents the truth value of a in A and
.
.
aA‚à™¬¨.U of a in A ‚à™ ¬¨.U (where U is the current unfounded set), aA‚àßU represents that a is true in
A and is contained in U , and aA‚à®U represents that a is false in A or it is contained in U .
To this end, a set of assumptions is a consistent set A of signed literals. A solution A to a
nogood Œ¥ resp. a set ‚àÜ of nogoods satisfies A, if A ‚äÜ A. That is, assumptions fix the truth value
of some atoms. Modern ASP and SAT solvers support assumptions natively, and they can be easily
undone without a complete reset of the reasoner and recreating the whole problem instance. This is
an essential feature for efficiently implementing our improved encoding.
Our encoding ‚Ñ¶Œ† is then
‚Ñ¶Œ† = N‚Ñ¶,Œ† ‚à™ O‚Ñ¶,Œ† ,
S
S
where N‚Ñ¶,Œ† = {{Fa | a ‚àà A(Œ†)}} ‚à™ a‚ààA(Œ†) Da ‚à™ r‚ààŒ† (Hr ‚à™ Cr ) is the necessary part and
‚Ä¢ {Fa | a ‚àà A(Œ†)} encodes that we search for a nonempty unfounded set;
Ô£±
	
Ô£¥
Ô£≤{FaA‚àßU , TaA , Ta}, {TaA‚àßU , FaA }, {TaA‚àßU , Fa}	 ‚à™
‚Ä¢ Da =
{FaA‚à®U , FaA }, {FaA‚à®U , Ta}, {TaA‚à®U , TaA , Fa} ‚à™
Ô£¥
	
Ô£≥
.
.
.
{TaA‚à™¬¨.U
, FaA }, {TaA‚à™¬¨.U
, Ta}, {FaA‚à™¬¨.U
, TaA , Fa}

encodes that aA‚àßU is true iff aA and a are both true, aA‚à®U is true iff aA is false or a is true,
.
and aA‚à™¬¨.U
is true iff aA is true and a is false;

‚Ä¢ Hr = {{Thr } ‚à™ {Fh | h ‚àà H(r)}} ‚à™ {{Fhr , Th} | h ‚àà H(r)}
encodes that hr is true for a rule r iff some atom of H(r) is in the unfounded set; and
281

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

Ô£±
Ô£¥
{{Thr } ‚à™
Ô£¥
Ô£¥
Ô£¥
Ô£≤ {Ta | a ‚àà B + (rÃÇ)} ‚à™ {Fa | a ‚àà B ‚àí (rÃÇ)} ‚à™
A
A
‚Ä¢ Cr =
+ (r)} ‚à™ {ta | a ‚àà B (rÃÇ)} ‚à™
Ô£¥
{Fa
|
a
‚àà
B
e
Ô£¥
A‚àßU
o
Ô£¥
Ô£¥
Ô£≥ {Th
|
h
‚àà
H(r)}}
A‚à®U

(i)
(ii)
(iii)

encodes that if hr is true, then one of (i), (ii) or (iii) in Definition 5 must hold.
More specifically, for an unfounded set U and a rule r with H(r) ‚à© U 6= ‚àÖ (hr is true) it
must not happen that A |= B(r) (Condition (i) fails),. no a ‚àà Bo+ (r) with A |= a is in
the unfounded set and all a ‚àà Be (rÃÇ) are true under A ‚à™ ¬¨.U (Condition (ii) fails), and all
h ‚àà H(r) with A |= h are in the unfounded set (Condition (iii) fails).

Example 10 For Œ† = {r1 : p ‚Üê &id [p]() } in Example 6, the constructed nogood set is
‚Ñ¶Œ† = {{Fp}, {FpA‚àßU , TpA , Tp}, {TpA‚àßU , FpA }, {TpA‚àßU , Fp},

.
, FpA },
{FpAÃÑ‚à®U , Fp}, {FpAÃÑ‚à®U , Tp}, {TpAÃÑ‚à®U , TpA , Fp}, {TpA‚à™¬¨.U

.
.
{TpA‚à™¬¨.U
, Tp}, {FpA‚à™¬¨.U
, TpA , Fp},

{Thr1 , Fp}, {Fhr1 , Tp}, {Thr1 , Te&id[p] ()A , Te&id[p] (), TpAÃÑ‚à®U }} .

Towards computing unfounded sets, observe that every unfounded set can be extended to a solution
to the set of nogoods ‚Ñ¶Œ† over A(‚Ñ¶Œ† ). Conversely, the solutions to ‚Ñ¶Œ† include specific extensions of
all unfounded sets, which are again characterized by induced assignments; that is, by assigning true
to all atoms in U , to all hr such that H(r) intersects with U , and to all replacement atoms e&g[p] (c)
.
such that &g[p](c) is true under A ‚à™ ¬¨.U , appropriate truth values to the auxiliary atoms according
to their intuitive meaning, and assigning false to all other atoms in A(‚Ñ¶Œ† ). More formally, this leads
us to the following assignment:
Definition 8 (Induced Assignment of an Unfounded Set wrt. ‚Ñ¶Œ† ) Let U be an unfounded set of
a program Œ† wrt. assignment A. The assignment induced by U wrt. ‚Ñ¶Œ† , denoted I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A),
is
I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) = I‚Ñ¶0 (U, Œ†, A) ‚à™ {Fa | a ‚àà A(‚Ñ¶Œ† ), Ta 6‚àà I‚Ñ¶0 (U, Œ†, A)} ,
where
I‚Ñ¶0 (U, Œ†, A) = {Ta | a ‚àà U } ‚à™ {Thr | r ‚àà Œ†, H(r) ‚à© U 6= ‚àÖ} ‚à™
.
{Te&g[p] (c) | &g[p](c) ‚àà EA(Œ†), A ‚à™ ¬¨.U |= &g[p](c)} ‚à™
{TaA | a ‚àà A(Œ†), Ta ‚àà A} ‚à™ {TaÃÇA | a ‚àà EA(Œ†), A |= a} ‚à™
{TaA‚àßU | a ‚àà A(Œ†), Ta ‚àà A, a ‚àà U } ‚à™
.
{TaA‚à™¬¨.U
| a ‚àà A(Œ†), Ta ‚àà A, a 6‚àà U } ‚à™
{TaA‚à®U | a ‚àà A(Œ†), Fa ‚àà A or a ‚àà U } .
If we adopt for an assignment A the assumption set

AA = {TaA | a ‚àà A(Œ†), Ta ‚àà A} ‚à™ {FaA | a ‚àà A(Œ†), Fa ‚àà A} ‚à™

{TaÃÇA | a ‚àà EA(Œ†), A |= a} ‚à™ {FaÃÇA | a ‚àà EA(Œ†), A 6|= a} ,

then all assignments induced by unfounded sets of Œ† wrt. A are solutions to ‚Ñ¶Œ† wrt. AA (but not
conversely, because intuitively the latter do not reflect the semantics of external sources).
As before, we call a set of nogoods N conservative, if I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) is a solution to N for
every unfounded set U of Œ† wrt. A. Under this property, those interpretations are solutions of the
whole nogood set which comply with the assumptions from A.
282

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

Proposition 8 Let U be an unfounded set of a program Œ† wrt. assignment A such that AT ‚à©U 6= ‚àÖ.
If O‚Ñ¶,Œ† is conservative, then I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) is a solution to ‚Ñ¶Œ† that satisfies AA .
Corollary 9 If ‚Ñ¶Œ† has no solution which satisfies AA , then U ‚à© AT = ‚àÖ for every unfounded set
U of Œ† (assuming O‚Ñ¶,Œ† is conservative).

The next property allows us to find the unfounded sets of Œ† wrt. A among all solutions to ‚Ñ¶A that
satisfy AA by using a postcheck on the external atoms.
Theorem 10 Let S be a solution to ‚Ñ¶Œ† (with conservative O‚Ñ¶,Œ† ) that satisfies AA such that
.

(a) Te&g[p] (c) ‚àà S and A 6|= &g[p](c) implies A ‚à™ ¬¨.U |= &g[p](c); and
.

(b) Fe&g[p] (c) ‚àà S and A |= &g[p](c) implies A ‚à™ ¬¨.U 6|= &g[p](c),

where U = {a ‚àà A(Œ†) | Ta ‚àà S}. Then U is an unfounded set of Œ† wrt. A.

As for ŒìA
Œ† , the proposition states that the non-replacement atoms in S that are true and appear
in Œ† form an unfounded set, provided that each replacement atom e&g[p] (c) in S has the same truth
.
value as &g[p](c) under A ‚à™ ¬¨.U (as in Definition 8). Again, this check is just required if the truth
value of e&g[p] (c) in S is different from the one of &g[p](c) under A.
Similarly as for the encoding Œì, it follows immediately from Definition 8 that this postcheck
does not eliminate unfounded sets, as formalized by the following proposition.
Proposition 11 Let U be an unfounded set of a program Œ† wrt. assignment A such that AT ‚à© U 6=
‚àÖ. Then I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) fulfills Conditions (a) and (b) of Theorem 10.
Example 11 Reconsider program Œ† = {r1 : p ‚Üê &id [p]()} from Example 6 and the compatible
set A2 = {Tp, Te&id[p] }. The nogood set
‚Ñ¶Œ† = {{Fp}, {FpA‚àßU , TpA , Tp}, {TpA‚àßU , FpA }, {TpA‚àßU , Fp},

.
{FpAÃÑ‚à®U , Fp}, {FpAÃÑ‚à®U , Tp}, {TpAÃÑ‚à®U , TpA , Fp}, {TpA‚à™¬¨.U
, FpA },

.
.
{TpA‚à™¬¨.U
, Tp}, {FpA‚à™¬¨.U
, TpA , Fp},

{Thr1 , Fp}, {Fhr1 , Tp}, {Thr1 , Te&id[p] ()A , Te&id[p] (), TpAÃÑ‚à®U }}
with assumptions AA2 = {TpA } has solutions S ‚äá {Thr1 , Tp, TpA , Fe&id[p] , TpA‚àßU , TpAÃÑ‚à®U ,
.
FpA‚à™¬¨.U
}, which correspond to the unfounded set U = {p}. Here, Fe&id[p] () represents that
.
A2 ‚à™ ¬¨.U 6|= &id [p]().
We will see in Section 6 that the encoding ‚Ñ¶Œ† is superior to ŒìA
Œ† for many practically relevant
programs. The effect becomes especially visible if they need many unfounded set checks, which intuitively is the case when many answer sets exist; here reusability of the encoding is very beneficial,
while for small programs with few answer sets, the incurred overhead does not lead to savings.

4. Optimization and Learning
In this section we first discuss some refinements and optimizations of our nogood encodings for UFS
search. In particular, we present nogoods which prune irrelevant parts of the search space; they can
be integrated into both encodings ŒìA
Œ† and ‚Ñ¶Œ† under suitable adjustments. After that, we propose a
strategy for learning nogoods from detected unfounded sets, avoiding that the same unfounded set
is generated later again.
283

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

4.1 Optimization
We present now three optimizations which turned out to be effective in improving UFS search,
where the second and the third exclude each other, i.e., they can not be used simultaneously.
4.1.1 R ESTRICTING THE UFS S EARCH TO ATOMS IN THE C OMPATIBLE S ET
First, not all atoms in a program are relevant for the unfounded set search: atoms that are false under
A can be ignored.
Proposition 12 Suppose U is an unfounded set of Œ† wrt. an interpretation A such that A 6|= a for
some a ‚àà U . Then U \ {a} is an unfounded set of Œ† wrt. A.
The nogoods for this optimization are simple. In the encoding ŒìA
Œ† , we add the conservative
A and in the encoding
nogood {Ta} for each a ‚àà A(Œ†) with A 6|= a to the optimization part OŒì,Œ†
‚Ñ¶Œ† the conservative nogood {FaA , Ta} for each a ‚àà A(Œ†) to the optimization part O‚Ñ¶,Œ† .
4.1.2 AVOIDING G UESSES OF R EPLACEMENT ATOMS
In some situations, the truth value of a replacement atom b in a solution S to ŒìA
Œ† resp. ‚Ñ¶Œ† with
assumptions AA is irrelevant. That is, both STb = (S \ {Tb, Fb}) ‚à™ {Tb} and SFb = (S \
{Tb, Fb}) ‚à™ {Fb} are solutions to ŒìA
Œ† resp. ‚Ñ¶Œ† that satisfy AA , and they represent the same
unfounded set. We then can set the truth value of b to an (arbitrary) fixed value instead of inspecting
both alternatives. The next proposition states a sufficient criterion for this irrelevance.
Proposition 13 Let b be a replacement atom, and let S be a solution to ŒìA
Œ† resp. ‚Ñ¶Œ† satisfying
AA . If for every rule r ‚àà Œ† such that b ‚àà B + (rÃÇ) ‚à™ B ‚àí (rÃÇ) and A |= B(r), either
(a) for some a ‚àà Bo+ (r) such that A |= a, it holds that Ta ‚àà S, or
(b) for some a ‚àà H(r) such that A |= a, it holds that Fa ‚àà S,

then both STb and SFb are solutions to ŒìA
Œ† resp. ‚Ñ¶Œ† that satisfy AA .
This property can be utilized by adding conservative nogoods. Recall that A(ŒìA
Œ† ) and A(‚Ñ¶Œ† )
contain atoms lr for every r ‚àà Œ†. They intuitively serve to encode for a solution S to ŒìA
Œ† resp. ‚Ñ¶Œ†
with assumptions AA whether the truth values of the replacement atoms in B(r) are relevant or can
be set arbitrarily. The following nogoods label relevant rules r, forcing lr to be false iff some of the
A
conditions in Proposition 13 holds. For the encoding ŒìA
Œ† , we add to OŒì,Œ† for each rule r:
+
LA
Œì,r ={{Tlr , Ta} | a ‚àà Bo (r), A |= a} ‚à™ {{Tlr , Fa} | a ‚àà H(r), A |= a} ‚à™
{{Flr } ‚à™ {Fa | a ‚àà Bo+ (r), A |= a} ‚à™ {Ta | a ‚àà H(r), A |= a}} .

For the encoding ‚Ñ¶Œ† , we add to O‚Ñ¶,Œ† for each rule r:
L‚Ñ¶,r ={{Tlr , Ta, TaA } | a ‚àà Bo+ (r)} ‚à™ {{Tlr , Fa, TaA } | a ‚àà H(r)} ‚à™
{{Flr } ‚à™ {FaA‚àßU | a ‚àà Bo+ (r)} ‚à™ {TaAÃÑ‚à®U | a ‚àà H(r)}} .
These constraints exclusively enforce either Tlr or Flr . Hence, the truth value of lr deterministically depends on the other atoms, i.e., the nogoods do not cause additional guessing.
By Proposition 13 we can set the truth value of a replacement atom b arbitrarily, if lr is false
for all r such that b ‚àà B + (rÃÇ) or b ‚àà B ‚àí (rÃÇ). However, it must be ensured that changing the truth
284

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

value of replacement atoms does not harm the satisfaction of the conditions in Theorem 6 (resp.
Theorem 10).
As mentioned after Theorem 6, it is beneficial to set the truth value of e&g[p] (c) to the one of
&g[p](c) under A, because this can reduce the number of external atoms that must be checked.
Importantly, this also relaxes the antecedence of the conditions in Theorem 6 (resp. Theorem 10),
and guarantees that they are not harmed. The following nogoods enforce a coherent interpretation
of the replacement atoms.
A
For the encoding ŒìA
Œ† we add to OŒì,Œ† for each rule r:

	
A
FŒì,r
= {Flr | b ‚àà B + (rÃÇ) ‚à™ B ‚àí (rÃÇ)} ‚à™ {Fb} | b ‚àà Be (rÃÇ), A |= b ‚à™

	
{Flr | b ‚àà B + (rÃÇ) ‚à™ B ‚àí (rÃÇ)} ‚à™ {Tb} | b ‚àà Be (rÃÇ), A 6|= b ,
while for the encoding ‚Ñ¶Œ† we add to OŒì,Œ† for each rule r:

	
F‚Ñ¶,r = {Flr | b ‚àà B + (rÃÇ) ‚à™ B ‚àí (rÃÇ)} ‚à™ {TbA , Fb} | b ‚àà Be (rÃÇ) ‚à™

	
{Flr | b ‚àà B + (rÃÇ) ‚à™ B ‚àí (rÃÇ)} ‚à™ {FbA , Tb} | b ‚àà Be (rÃÇ) .
S
A
A
In summary, the encoding ŒìA
optimization part OŒì,Œ†
= r‚ààŒ† LA
Œ† has the S
Œì,r ‚à™ FŒì,r and the
encoding ‚Ñ¶Œ† the optimization part O‚Ñ¶,Œ† = r‚ààŒ† L‚Ñ¶,r ‚à™ F‚Ñ¶,r .
We give now an example for this optimization using our encoding Œì.
Example 12 Consider the program Œ† = {r1 : p ‚Üê &id [p](); r2 : q ‚Üê &id [q]()}, and the compatible set AÃÇ = {Tp, Tq, Te&id[p] (), Te&id[q] ()}. Then the necessary part of encoding ŒìA
Œ† has
solutions S1 ‚äá {Thr1 , Tp, Fe&id[p] (), Fhr2 , Fq, Fe&id[q] ()} and S2 ‚äá {Thr1 , Tp, Fe&id[p] (),
Fhr2 , Fq, Te&id[q] ()} (which represent the same unfounded set U = {p}). Here, the optimizaA
tion part for r2 , LA
r2 ‚à™ Fr2 = {{Tlr2 , Fq}, {Flr2 , Tq}, {Flr2 , Te&id[q] ()}}, eliminates solutions S2
for ŒìA
Œ† . This is beneficial as for solutions S1 the postcheck is easier (e&id[q] () in S1 and &id [q]()
have the same truth value under A).
Note that if this optimization is not used, then for all rules r the atom lr is in fact not needed
and thus unconstrained. To avoid an exponential increase of the number of UFS candidates, these
atoms should then be set to a fixed value.
4.1.3 E XCHANGING N OGOODS B ETWEEN UFS AND M AIN S EARCH
The third optimization allows for the exchange of learned knowledge about external atoms between
the UFS check and the main search for compatible sets. For this purpose, we first define nogoods
which correctly describe the input-output relationship of external atoms.
Definition 9 A nogood of the form N = {Tt1 , . . . , Ttn , Ff1 , . . . , Ffm , œÉe&g[p] (c)}, where œÉ is T
or F, is a valid input-output-relationship, if for every assignment A such that N \{œÉe&g[p] (c)} ‚äÜ A
it holds that A |= &g[p](c) if œÉ = F, and A 6|= &g[p](c) if œÉ = T.

Here, the signed literals with atoms ti , 1 ‚â§ i ‚â§ n, resp. fj , 1 ‚â§ j ‚â§ m, reflect the relevant true
resp. false atoms in the interpretation A, built over predicates which occur in the input list p. Techniques for learning such nogoods have been described by Eiter et al. (2012a) and exploit properties
of external sources (such as monotonicity and functionality) to restrict the size of N .
Let N be a nogood which is a valid input-output-relationship learned during the main search,
i.e., for compatible sets of Œ†ÃÇ, and let FÃÑ = T and TÃÑ = F.
285

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

Definition 10 (Nogood Transformation TŒì ) For a valid input-output relationship N = {Tt1 ,
. . . , Ttn , Ff1 , . . . , Ffm , œÉe&g[p] (c)} and an assignment A, the nogood transformation TŒì is defined as
Ô£±
Ô£¥‚àÖ
if Fti ‚àà A for some 1 ‚â§ i ‚â§ n,
Ô£≤
TŒì (N, A) = { {Ft1 , . . . , Ftn } ‚à™ {œÉe&g[p] (c)} ‚à™
Ô£¥
Ô£≥
{Tfi | 1 ‚â§ i ‚â§ m, A |= fi } }
otherwise.
The next result states that TŒì (N, A) can be considered, for all valid input-output relationships N
under all assignments A, without losing unfounded sets.
Proposition 14 Let N be a valid input-output relationship, and let U be an unfounded set wrt. Œ†
A contains only conservative nogoods, then I (U, ŒìA , Œ†, A) is a solution to T (N, A)
and A. If OŒì,Œ†
Œì
Œì
Œ†
(i.e., also nogoods TŒì (N, A) are conservative).
Hence, all valid input-output relationships for external atoms that are learned during the search
for compatible sets can be reused (applying the above transformation) for the UFS check. Moreover,
during the evaluation of external atoms in the postcheck for candidate unfounded sets (i.e., solutions
to ŒìA
Œ† ), further valid input-output relationships might be learned. They can in turn be used by
(further) unfounded set checks (in transformed form) but also in the search for compatible sets.
Example 13 (Set Partitioning) For the program Œ† from Example 4, consider the compatible set
AÃÇ = {Tdomain(a), Tsel (a), Te&diff [nsel] (a)}. Suppose the main search has learned the inputoutput relationship N = {Tdomain(a), Fnsel (a), Fe&diff [nsel] (a)}. Then the transformed nogood
is aTŒì (N,A)={{Fdomain(a), Fe&diff [nsel] (a)}}; it intuitively encodes that if domain(a) is not in
.
the unfounded set U , then e&diff [nsel] (a) is true under A ‚à™ ¬¨.U . This holds because e&diff [nsel] (a)
is true under A and can only change its truth value if domain(a) becomes false.
This learning technique can be adopted for our encoding ‚Ñ¶Œ† as follows.
Definition 11 (Nogood Transformation T‚Ñ¶ ) For a valid input-output relationship N , the nogood
transformation T‚Ñ¶ is defined as
.
.
T‚Ñ¶ (N ) = {{œÉe&g[p] (c)} ‚à™ {Tt1A , Ft1 , . . . , TtnA , Ftn , Ff1 A‚à™¬¨.U
, . . . , Ffm A‚à™¬¨.U
}} .

Compared to TŒì (N, A), the main difference is that T‚Ñ¶ (N ) is reusable for every assignment, similar
to the definition of our unfounded set detection problem ‚Ñ¶Œ† .
The next result states that T‚Ñ¶ (N ) can be considered, for all valid input-output relationships N
under all assignments A, without losing unfounded sets.
Proposition 15 Let N be a valid input-output relationship, and let U be an unfounded set wrt. Œ†
and A. If O‚Ñ¶,Œ† contains only conservative nogoods, then I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) is a solution to T‚Ñ¶ (N )
(i.e., also nogoods T‚Ñ¶ (N ) are conservative).
Hence, also with encoding ‚Ñ¶Œ† all valid input-output relationships for external atoms that are
learned during the search for compatible sets can be reused and vice versa.
286

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

Example 14 (cont‚Äôd) Reconsider the program Œ† from Example 4 and the compatible set AÃÇ =
{Tdomain(a), Tsel (a), Te&diff [domain,nsel] (a)}. Suppose the main search has learned the inputoutput relationship N = {Tdomain(a), Fnsel (a), Fe&diff [domain,nsel] (a)}. The transformed nogood is
.
.
T‚Ñ¶ (N )={{Tdomain(a)A , Fdomain(a), Fnsel (a)A‚à™¬¨.U
, Fe&diff [domain,nsel] (a)A‚à™¬¨.U
}} ;

it intuitively encodes that if domain(a)
is true in the assignment A but not in the
unfounded set U ,
.
.
and if nsel (a) is false in A ‚à™ ¬¨.U , then e&diff [domain,nsel] (a) is true under A ‚à™ ¬¨.U . This holds as
e&diff [domain,nsel] (a) is true under A and can only change its truth value if domain(a) gets false.
The nogood exchange also benefits from our advanced encoding. With our previous encoding ŒìA
Œ†,
we needed to build the SAT instance from scratch for every unfounded set check. Thus, nogoods
learned in the main search for compatible sets need to be transformed and added to the UFS detection problem for every check (otherwise they are lost). With our new encoding ‚Ñ¶Œ† , this is done
only once because learned nogoods are kept for multiple unfounded set checks. This also allows us
to make use of advanced forgetting heuristics in SAT solvers more effective.
Finally, an important note is that the optimizations presented in Section 4.1.2 and 4.1.3 can not
be used simultaneously (differently from the optimizations in Section 4.1.1 and 4.1.2 resp. 4.1.1 and
4.1.3), as this can result in contradictions due to (transformed) learned nogoods. We thus disabled
the optimization for avoiding guesses of replacement atoms (Section 4.1.2) in our experiments.
4.2 Learning Nogoods from Unfounded Sets
Until now we have considered merely detecting unfounded sets. A strategy to learn from detected
unfounded sets for the main search for compatible sets is missing. We next develop such a strategy
which we call unfounded set learning (UFL).
Example 15 Consider the program Œ† = { p ‚Üê &id [p](); x1 ‚à®x2 ‚à®¬∑ ¬∑ ¬∑‚à®xk ‚Üê }. As we know from
Example 7, U = {p} is a UFS of the subprogram Œ†0 = { p ‚Üê &id [p]() } wrt. A = {Tp, Te&id ()}.
The same is true for Œ† and moreover for every A0 ‚äÉ A; i.e., p must never be true.
The program in Example 15 has many compatible sets, and half of them (all where p is true) will fail
the UFS check for the same reason. We thus develop a strategy for generating additional nogoods to
guide the search for compatible sets in a way such that the same unfounded sets are not reconsidered.
We present two such strategies, but will focus on the first one because our experiments have shown
that the first one is superior for all our instances.
4.2.1 UFS-BASED L EARNING
For an unfounded set U of Œ† wrt. A we define a set L1 (U, Œ†, A) of learned nogoods as follows.
Suppose that r1 , . . . , rj are all rules r in Œ† such that H(r) ‚à© U 6= ‚àÖ and U ‚à© Bo+ (r) = ‚àÖ, i.e., the
set of all ‚Äúexternal‚Äù rules of Œ† wrt. U (rules which do not directly depend positively on U ). Then
L1 (U, Œ†, A) = {{œÉ0 , œÉ1 , . . . , œÉj } | œÉ0 ‚àà {Ta | a ‚àà U }, œÉi ‚àà Hi for all 1 ‚â§ i ‚â§ j)} ,
where Hi = {Th | h ‚àà H(ri ) \ U, A |= h} ‚à™ {Fb | b ‚àà Bo+ (ri ), A 6|= b}. Formally we can show
that adding this set of nogoods is correct, i.e., does not prune answer sets:
287

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

Proposition 16 If U is an unfounded set of Œ† wrt. A, then every answer set of Œ† is a solution to
the nogoods in L1 (U, Œ†, A).
Example 16 Consider the program Œ† from Example 15 and suppose we have found the unfounded
set U = {p} wrt. the interpretation A = {Tp, Tx1 } ‚à™ {Fxi | 1 < i ‚â§ k}. Then the learned nogood
L1 (U, A, Œ†) = {Tp} immediately guides the search to the part of the search tree where p is false,
i.e., roughly half of the guesses are avoided.
4.2.2 R EDUCT-BASED L EARNING
A different learning strategy is based on the models of f Œ†A rather
than the unfounded set U itself,
.
hinging on the observation that for every unfounded set U , A ‚à™ ¬¨.U is a model of f Œ†A ; hence
U 6= ‚àÖ refutes A as a minimal model of f Œ†A . This was noted by Faber (2005) for aggregates.
We exploit this
to construct nogoods from a nonempty U wrt. a model A as follows. The
.
interpretation A ‚à™ ¬¨.U is not only a model of f Œ†A , but of all programs Œ†0 ‚äÜ f.Œ†A . Hence, if an
assignment A0 falsifies at least the rules of Œ† which A falsifies, and A0 T ‚äÉ (A ‚à™ ¬¨.U )T , then A0
is not an answer set of Œ†. This yields the following nogood set L2 (U, Œ†, A). Suppose r1 , . . . , rn
are all rules r of Œ† which are not in its FLP-reduct wrt. A (i.e., A 6|= B(ri ). Then
.

L2 (U, Œ†, A) = {{Ta | a ‚àà (A ‚à™ ¬¨.U )T } ‚à™ {œÉ0 , œÉ1 , . . . , œÉn }

| œÉ0 ‚àà {Ta | a ‚àà U }, œÉi ‚àà Hi for all 1 ‚â§ i ‚â§ n} ,

where Hi = {ta | a ‚àà B(rÃÇ),
AÃÇ 6|= a}, 1 ‚â§ i ‚â§ n. That is, each nogood consists of the true-part
.
of the
smaller
model
A
‚à™
¬¨.U
of the reduct f Œ†A , an unfounded atom œÉ0 (i.e. true in A but not in
.
A ‚à™ ¬¨.U ), and a false body literal œÉi (1 ‚â§ i ‚â§ n) for each rule of Œ† with unsatisfied body wrt. A.
Example 17 Let Œ† = {p ‚Üê &id [p](); q ‚Üê &id [q]()}, where &id [a]() evaluates to true iff a is
true. Suppose A = {Tp, Tq}.
Then U = {Tp, Tq} is an unfounded set wrt. A. In the above
.
construction rule we have A ‚à™ ¬¨.U = {}, œÉ0 ‚àà {Tp, Tq} and n = 0 (because both rule bodies are
satisfied wrt. A). The learned nogoods are L2 (U, Œ†, A) = {{Tp}, {Tq}}.
In Example 17, the learned nogoods will immediately guide the search to the interpretation
{Fp, Fq}, which is the only one which becomes an answer set. Formally, we can show:
Proposition 17 If U is an unfounded set of Œ† wrt. A and A |= Œ†, then each answer set of Œ† is a
solution to all nogoods in L2 (U, Œ†, A).
However, L2 (U, Œ†, A) appeared to be clearly inferior to L1 (U, Œ†, A) from Section 4.2.1. Informally, its nogoods overfit the detected unfounded set and do not generalize well to other ones.

5. Deciding the Necessity of the Minimality Check
Although the minimality check based on unfounded sets is more efficient than the explicit minimality check, the computational costs are still high. Moreover, during the evaluation of Œ†ÃÇ for computing
the compatible set AÃÇ, the ASP solver has already made an unfounded set check, and we can safely
assume that it is founded from the perspective of the ASP solver. Hence, all remaining unfounded
sets which were not discovered by the ASP solver must involve external sources, as their behavior
is not fully captured by the ASP solver.
288

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

In this section we pursue these ideas and give a decision criterion for deciding whether a further UFS check is necessary. We eventually define a class of programs which needs no additional
UFS check. Intuitively, we show that every unfounded set that is not already detected during the
construction of AÃÇ contains input atoms of external atoms involved in cycles. If the program has no
such input atom, then the UFS check is superfluous. Afterwards, we show how to apply this criterion, which holds in practically relevant cases, to program components; this often yields additional
speedup. However, there are also cases where the UFS check can not be skipped; e.g., recursive
URL retrieval from a web resource (which requires cyclic use of an external atom).
5.1 Basic Decision Criterion
We start with a definition of atom dependency.
Definition 12 (Atom Dependency) For a ground program Œ†, and ground atoms p(c) and q(d), we
say that
(i) p(c) depends on q(d), denoted p(c) ‚Üí q(d), if for some rule r ‚àà Œ† we have p(c) ‚àà H(r) and
q(d) ‚àà B(r);

(ii) p(c) depends externally on q(d), denoted p(c) ‚Üíe q(d), if some rule r ‚àà Œ† and external atom
&g[q1 , . . . , qn ](e) ‚àà B + (r)‚à™B ‚àí (r) exist such that p(c) ‚àà H(r) and q ‚àà {q1 , . . . , qn }.
In the following, we consider dependency graphs GR
Œ† = (V, E) for a ground program Œ†, whose
vertices V are the ground atoms and whose edges E are given by a binary relation R over ground
atoms (E = R). We call p(c) ‚Üí q(d) also an ordinary edge and p(c) ‚Üíe q(d) an e-edge.
We establish a lemma that allows us to restrict our attention to the ‚Äúcore‚Äù of an unfounded set,
i.e., its most essential part; we can disregard atoms in a cut of GR
Œ† , which is defined as follows.
Definition 13 (Cut) Let U be an unfounded set of Œ† wrt. A. A set of atoms C ‚äÜ U is a cut of GR
Œ†,
if
(i) b ‚Üíe a ‚àà
/ GR
Œ† , for all a ‚àà C and b ‚àà U (C has no incoming e-edge from U ),
R
(ii) b ‚Üí a 6‚àà GR
Œ† and a ‚Üí b 6‚àà GŒ† , for all a ‚àà C and b ‚àà U \ C (there are no ordinary edges
between C and U \ C).

We first prove that cuts can be removed from unfounded sets and the resulting set is still an
unfounded set.
Lemma 18 (Unfounded Set Reduction Lemma) Let U be an unfounded set of Œ† wrt. a complete
assignment A, and let C be a cut of GR
Œ† . Then, Y = U \ C is an unfounded set of Œ† wrt. A.
Example 18 Consider the following program:
Œ† = {r ‚Üê &id [r]();

p ‚Üê &id [r]();

p ‚Üê q;

q ‚Üê p} .

Then we have p ‚Üí q, q ‚Üí p, r ‚Üíe r and p ‚Üíe r. Œ† has the unfounded set U = {p, q, r}
wrt. A = {Tp, Tq, Tr}. Observe that C = {p, q} is a cut of GR
Œ† , and therefore U \ C = {r} is an
unfounded set of Œ† wrt. A.
289

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

Next we prove that intuitively, for each unfounded set U of Œ†, either the input to some external
atom is unfounded itself, or U is already detected when Œ†ÃÇ is evaluated.
Lemma 19 (EA-Input Unfoundedness) Let U be an unfounded set of Œ† wrt. an assignment A. If
GR
Œ† has no edge x ‚Üíe y such that x, y ‚àà U , then U is an unfounded set of Œ†ÃÇ wrt. AÃÇ.
Example 19 Reconsider the program Œ† from Example 18. Then the unfounded set U 0 = {p, q}
wrt. A0 = {Tp, Tq, Fr} is already detected when
Œ†ÃÇ = { e&id[r] () ‚à® ne &id[r] () ‚Üê ;

r ‚Üê e&id[r] ();

p ‚Üê e&id[r] ();

p ‚Üê q;

q ‚Üê p}

is evaluated by the ASP solver because no edges p ‚Üíe q and q ‚Üíe p exist. In contrast, the
unfounded set U 00 = {p, q, r} wrt. A00 = {Tp, Tq, Tr} is not detected by the ASP solver because
p, r ‚àà U 00 and p ‚Üíe r.
Thus, the unfounded sets of Œ† wrt. A that are not recognized during the evaluation of Œ†ÃÇ have
cyclic dependencies over input atoms of some external atom. Programs with acyclic dependencies
do not need additional UFS checks.
Recall that a cycle wrt. a binary relation R is a sequence C = c0 , c1 , . . . , cn , cn+1 of elements,
n ‚â• 0, such that (ci , ci+1 ) ‚àà R for all 0 ‚â§ i ‚â§ n and c0 = cn+1 . A set S contains a cycle wrt. R,
if there is a cycle C = c0 , c1 , . . . , cn , cn+1 wrt. R such that ci ‚àà S for all 0 ‚â§ i ‚â§ n + 1.
Informally, the next proposition states that each unfounded set of Œ† wrt. A which contains
no cycle through the input atoms to some external atom corresponds to some unfounded set of Œ†ÃÇ
wrt. AÃÇ, i.e., the unfoundedness is already detected when Œ†ÃÇ is evaluated.
Let ‚Üíd = ‚Üí ‚à™ ‚Üê ‚à™ ‚Üíe , where ‚Üê is the inverse of ‚Üí (i.e. ‚Üê = {(x, y) | (y, x) ‚àà ‚Üí}). A
cycle c0 , c1 , . . . , cn , cn+1 under ‚Üíd is called an e-cycle, if it contains an e-edge, i.e., ci ‚Üíe ci+1 for
some 0 ‚â§ i ‚â§ n.
Theorem 20 (Relevance of e-cycles) Let U 6= ‚àÖ be an unfounded set of Œ† wrt. an interpretation
A which does not contain any e-cycle under ‚Üíd . Then Œ†ÃÇ has a nonempty unfounded set wrt. AÃÇ.
Corollary 21 If a program Œ† has no e-cycle under ‚Üíd and Œ†ÃÇ has no unfounded set wrt. an interpretation AÃÇ, then A is unfounded-free for Œ†.
This corollary can be used to increase the performance of an evaluation algorithm as follows:
if there is no cycle under ‚Üíd containing e-edges, then an explicit unfounded set check is not necessary because the unfounded set check during the evaluation of Œ†ÃÇ is sufficient. Note that this test
can be done efficiently (in fact in linear time, similar to deciding stratifiability of an ordinary logic
program). Moreover, in practice one can abstract from ‚Üíd by using analogous relations on the
level of predicate symbols instead of atoms. Clearly, if there is no e-cycle in the predicate dependency graph, then there can also be no e-cycle in the atom dependency graph. Hence, the predicate
dependency graph can be safely used to decide whether the unfounded set check can be skipped.
Example 20 All example programs so far need an UFS check, but the program Œ† = {out(X) ‚Üê
&diff [set 1 , set 2 ](X)}‚à™F , where diff computes the set difference of unary predicates set 1 and set 2
and F is any set of facts, needs no UFS check as there is no e-cycle under ‚Üíd . Also the program
Œ† = {str (Z) ‚Üê dom(Z), str (X), str (Y ), not &concat[X, Y ](Z)} (where &concat takes two
constants and computes their string concatenation) needs no UFS check; there is a cycle over an
external atom, but no e-cycle under ‚Üíd .
290

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

Unfortunately, the converse of Theorem 20 does not hold, that is, Œ†ÃÇ may fail to be unfoundedfree wrt. AÃÇ but no unfounded set of Œ† wrt. A contains an e-cycle; thus, the condition in Corollary 21
is not necessary for unfounded-freeness of Œ† wrt. A. However, the following generalization of
Theorem 20 allows us to conclude that if Œ†ÃÇ is unfounded-free wrt. AÃÇ, then every unfounded set U
of Œ† wrt. A must contain an atom that provides input to an external atom on a cycle under ‚Üíd .
Definition 14 (Cyclic Input Atoms) For a program Œ†, an atom a is a cyclic input atom, if some
edge b ‚Üíe a with a path from a to b under ‚Üíd exists.
Let CA(Œ†) denote the set of all cyclic input atoms of program Œ†.
Theorem 22 (Unfoundedness of Cyclic Input Atom) Let U 6= ‚àÖ be an unfounded set of Œ† wrt. A
such that U ‚à© CA(Œ†) = ‚àÖ. Then, Œ†ÃÇ has a nonempty unfounded set wrt. AÃÇ.
As a consequence of Theorem 22, we can add the nogood {Fa | a ‚àà CA(Œ†)} to ŒìA
Œ† . Again
using predicate symbols instead of atoms reduces the overhead of the dependency graph.
Example 21 Reconsider Œ† in Example 18. Then U = {p, q} is an unfounded set wrt. A =
{Tp, Tq, Fr}; as U is disjoint from CA(Œ†) = {r}, it is detected during the evaluation of Œ†ÃÇ.
5.2 Program Decomposition
The usefulness of the decision criterion can be increased by decomposing the program into components, such that the criterion can be applied componentwise. This allows us to restrict the UFS
check to components with e-cycles, while e-cycle-free components can be ignored.
Let C be a partitioning of the ordinary atoms A(Œ†) of Œ† into subset-maximal strongly connected
components under ‚Üí ‚à™ ‚Üíe . We define for each partition C ‚àà C the subprogram Œ†C associated
with C as Œ†C = {r ‚àà Œ† | H(r) ‚à© C 6= ‚àÖ}.
We next show that if a program has an unfounded set U wrt. A, then U ‚à© C is an unfounded set
wrt. A for the subprogram of some strongly connected component C.
Theorem 23 Let U 6= ‚àÖ be an unfounded set of Œ† wrt. A. Then, for some C ‚àà C it holds that U ‚à© C
is a nonempty unfounded set of Œ†C wrt. A.
Note that constraints (i.e., rules with empty head) do not harm this proposition. Each constraint
r of kind ‚Üê B(r) can be rewritten to p ‚Üê B(r), not p for a new atom p, and C = {p} is a strongly
connected component with Œ†C = {r}, which does not contain an e-cycle. Thus, for the rewritten
constraints the according subprograms Œ†C can be ignored anyways.
This proposition states that a search for unfounded sets can be done independently for the subprograms Œ†C for all C ‚àà C. If there is an unfounded set of Œ† wrt. an assignment, then there is also
an unfounded set of at least one program component wrt. this assignment. We know by Corollary 21
that programs Œ† without e-cycles can only contain unfounded sets that are already detected when
Œ†ÃÇ is solved. If we apply Theorem 23 to the subprograms Œ†C , we can safely ignore e-cycle-free
program components.
Example 22 Reconsider the program Œ† from Example 18. Then C contains the components C1 =
{p, q} and C2 = {r} and we have Œ†C1 = {p ‚Üê &id [r ](); p ‚Üê q; q ‚Üê p} and Œ†C2 = {r ‚Üê
&id [r ]()}. By Theorem 23, each unfounded set of Œ† wrt. some assignment A gives rise to an
291

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

unfounded set of either Œ†C1 or Œ†C2 . E.g., consider U = {p, q, r} and A = {Tp, Tq, Tr}; then
U ‚à© {r} = {r} is an unfounded set of Œ†C2 wrt. A. As Œ†C1 has no e-cycles, we conclude from
Corollary 21 that all unfounded sets of Œ†C1 are already detected when Œ†ÃÇ (resp., Œ†ÃÇC1 ) is evaluated.
Hence, only Œ†C2 needs an additional UFS check. Indeed, the only unfounded set of Œ†ÃÇ that is not
detected when Œ†ÃÇ is evaluated is {r}, which is unfounded wrt. each interpretation A ‚äá {Tr} for
Œ†C2 and Œ†.
Finally, we show that splitting, i.e., the component-wise check for foundedness, does not lead
to spurious unfounded sets.
Proposition 24 If U is an unfounded set of Œ†C wrt. A such that U ‚äÜ C, then U is an unfounded
set of Œ† wrt. A.
The results can be generalized to subprograms that are larger than strongly connected components; however, we leave a detailed study of this for future work.

6. Implementation and Evaluation
For implementing our technique, we integrated CLASP into our prototype system DLVHEX; we use
CLASP as an ASP solver for computing compatible sets and as a SAT solver for solving the nogood
set of the UFS check.
In our experiments, we will also use external behavior learning (EBL) as developed by Eiter
et al. (2012a). The basic idea is to learn additional nogoods from evaluations of external atoms,
which capture (parts of) the behavior of external sources. Thus, these nogoods eliminate model
candidates which violate the known semantics of external atoms.
Regarding a concrete setting, there is a large number of combinations of EBL and the techniques
presented in this paper. Indeed, we may either activate or deactivate external behavior learning and
use either the explicit or the UFS-based minimality check. In the latter case, we can further use
unfounded set learning (UFL), the decision criterion for skipping the unfounded set check can be
exploited or ignored, and program decomposition might be used or not. Moreover, we can choose
between the encodings Œì and ‚Ñ¶. In total, these are 34 different settings.
However, we will restrict our discussion to some interesting configurations. In general, we will
activate the developed features stepwise such that in our tables the efficiency increases from left
to right. We will start with the traditional algorithm based on an explicit minimality check without
any learning techniques of Eiter et al. (2012a) and from this paper (i.e., only conflict-driven learning
inside CLASP is used). In the next step we will add external behavior learning, while UFL is not possible with the explicit check. Then we switch from the explicit minimality check to the UFS-based
check without learning and without exploiting the decision criterion and program decomposition.
Nevertheless, this naive kind of UFS-based minimality checking is often more efficient than the
explicit minimality check with EBL. In the next step, we add the decision criterion and program
decomposition. In the following, monolithic (mol) means that both the decision criterion and the
program decomposition are off, and modular (mod) that they are on. Next we add EBL and UFL to
the UFS-based minimality check, and finally we switch the encoding from Œì to ‚Ñ¶ (including EBL,
UFL and modular decomposition). However, we might skip some of the steps for specific benchmarks and argue why they are uninteresting in the respective cases. Detailed instance information
and results with all combinations of parameters are available.2
2. http://www.kr.tuwien.ac.at/research/projects/hexhex/ufs

292

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

Briefly, our results show a clear improvement, for both synthetic and application instances,
by the UFS check and EBL. Moreover, a closer analysis shows that the UFS check decreases in
some cases not only the runtime but also the number of enumerated candidates (UFS resp. model
candidates of the FLP reduct) and the number of external atom evaluations.
We evaluated the implementation on a Linux server with two 12-core AMD 6176 SE CPUs
with 128GB RAM running DLVHEX version 2.3.0. The evaluated techniques were configured using
commandline arguments. To the best of our knowledge, DLVHEX is the only implementation of the
HEX semantics. In each test run the CPU usage was limited to two CPU cores, running a Condor
load distribution system which ensures robust runtimes (i.e., multiple runs of the same instance have
negligible deviations). The timeout was uniformly set to 300 seconds for each instance; for each
parameter value, the average runtime over all instances is printed where timeouts, whose number is
shown in parentheses, are fully taken into account.
6.1 Detailed Benchmark Description and Experimental Results
We give now a detailed description of the benchmarks used in our experiments, and present the
results of our experimental evaluation.
6.1.1 S ET PARTITIONING
This benchmark extends the program from Example 4 by the additional constraint
‚Üê sel (X), sel (Y ), sel (Z), X 6= Y, X 6= Z, Y 6= Z
and varies the size of domain. The results are shown in Table 1. Here we see a big advantage of
the UFS check over the explicit check, for both computing all answer sets and finding the first one.
A closer investigation shows that the improvement is mainly due to the optimizations in Section 4,
which make the UFS check investigate significantly fewer candidates than the explicit FLP check.
Furthermore the UFS check requires fewer external computations.
Both the explicit and the UFS-based minimality check benefit from EBL if we compute all
answer sets, but the results show that the UFS-based check benefits more. In contrast, UFL (not
shown in the table) does not lead to a further speedup as no unfounded sets are found in this program.
The decision criterion and program decomposition improve the runtime slightly for small instances. However, for large instances the decision criterion cannot avoid the UFS check in most
components of the program because of its cyclic structure. Thus a single UFS check over the whole
program is replaced by multiple UFS checks over individual program components, which involves
more overhead that becomes visible when computing all answer sets.
If we compute only one answer set, then EBL turns out to be counter-productive. This is because learning is involved with additional overhead, while we cannot profit much from the learned
knowledge if we abort after the first answer set, hence the costs exceed the benefit.
Using the encoding ‚Ñ¶ instead of Œì increases the efficiency in this case, because there is not only
a large number of answer sets but also a large number of answer set candidates. Thus, a reusable
encoding is very beneficial, even if we compute only one answer set.
Since in the evaluation of this program no unfounded sets are encountered, it is obvious that
additional unfounded set checks over partial interpretations increase the overhead at no benefit;
hence we do not discuss respective results.
293

domain

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

explicit

5 (1)
6 (1)
7 (1)
8 (1)
9 (1)
10 (1)
15 (1)
20 (1)
25 (1)

300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)

+EBL

all answer sets
UFS Œì
UFS Œì
mol
mod

+EBL

‚Ñ¶

explicit

300.00 (1)
0.33 (0)
0.32 (0) 0.09 (0)
300.00 (1)
0.77 (0)
0.81 (0) 0.12 (0)
300.00 (1)
1.73 (0)
1.78 (0) 0.20 (0)
300.00 (1)
4.35 (0)
4.17 (0) 0.31 (0)
300.00 (1) 10.42 (0) 10.21 (0) 0.47 (0)
300.00 (1) 26.31 (0) 25.13 (0) 0.53 (0)
300.00 (1) 300.00 (1) 300.00 (1) 2.83 (0)
300.00 (1) 300.00 (1) 300.00 (1) 12.98 (0)
300.00 (1) 300.00 (1) 300.00 (1) 45.18 (0)

0.07 (0)
0.10 (0)
0.13 (0)
0.16 (0)
0.23 (0)
0.29 (0)
0.79 (0)
1.95 (0)
4.11 (0)

54.02 (0)
300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)

first answer set
+EBL UFS Œì UFS Œì +EBL
mol
mod
53.80 (0)
300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)
300.00 (1)

0.05 (0)
0.04 (0)
0.06 (0)
0.07 (0)
0.08 (0)
0.09 (0)
0.19 (0)
0.38 (0)
0.70 (0)

0.05 (0)
0.05 (0)
0.06 (0)
0.06 (0)
0.07 (0)
0.09 (0)
0.15 (0)
0.29 (0)
0.47 (0)

0.05 (0)
0.06 (0)
0.06 (0)
0.07 (0)
0.08 (0)
0.11 (0)
0.27 (0)
0.57 (0)
1.09 (0)

‚Ñ¶
0.05 (0)
0.06 (0)
0.07 (0)
0.07 (0)
0.09 (0)
0.12 (0)
0.26 (0)
0.57 (0)
1.08 (0)

Table 1: Set Partitioning Benchmark Results
Note that the results are not comparable to those by Eiter et al. (2012a), because previous work
focused on the computation of subset-minimal compatible sets and did not perform a minimality
check wrt. the reduct, i.e., the semantics was different.
6.1.2 N ONMONOTONIC M ULTI -C ONTEXT S YSTEMS
Nonmonotonic Multi-Context-Systems (MCSs) (Brewka & Eiter, 2007) are a generic formalism
for aligning knowledge bases called contexts, which emerged from an approach by Ghidini and
Giunchiglia (2001). The contexts are interlinked via bridge rules which enable belief exchange
across contexts; the MCS semantics requires that local belief sets are compliant with the bridge
rules. Such compliance can be impossible to achieve; that is, the MCS is inconsistent. To understand
the reasons for the latter, Eiter et al. (2012b) defined inconsistency explanations (IEs) for a MCS,
which can be computed with a HEX-program encoding. This encoding is based on Saturation,
which is a general technique for solving Œ£p2 problems in disjunctive answer set programming (cf.,
Leone et al., 2006). Intuitively, a quantified Boolean formula (QBF) of the form ‚àÉX‚àÄYŒ¶(X, Y)
is evaluated using this technique as follows. Disjunctions are used to guess whether a variable v is
true or false. A ‚Äòspoil‚Äô atom is made true whenever Œ¶ evaluates to true given truth assignments to
X and Y. Finally whenever ‚Äòspoil‚Äô is true, all literals over Y are set to true; this creates a unique
assignment of the respective atoms. Given a guess on X, its unique ‚Äòspoil‚Äô extension is an answer
set if and only if all guesses of truth assignments Y make the spoil atom true and saturate the guess
to become the unique extension‚Äîthis holds due to the minimality condition on answer sets of the
reduct (see Definition 3).
We use the HEX-encoding for computing IEs as a benchmark, as the saturation is rich in cycles through external atoms and disjunctive rule heads. External atoms in this benchmark evaluate
semantics of contexts in the MCS (i.e., the local belief sets or models).
We use random instances of different MCS topologies, i.e., connection graphs of contexts, created with our MCS benchmark generator.3 Note that the topologies are by their structure bound
to certain system sizes (number of contexts), and that the difficulty of the instances varies among
topologies; thus larger instances may have shorter runtimes. Our instances have up to 10 contexts,
each consisting of a randomly generated consistent normal answer set program.
3. Described at http://www.kr.tuwien.ac.at/research/systems/dmcs/experiments.html, online available at https://dmcs.
svn.sourceforge.net/svnroot/dmcs/dmcs/trunk

294

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

#ctx
3 (6)
4 (10)
5 (8)
6 (6)
7 (12)
8 (5)
9 (8)
10 (11)

explicit

+EBL

UFS Œì
mol

UFS Œì
mod

+EBL

+UFL

‚Ñ¶

4.78 (0)
51.90 (1)
149.53 (3)
159.41 (3)
231.23 (9)
244.39 (4)
300.00 (8)
300.00 (11)

3.97 (0)
45.91 (1)
137.95 (3)
154.69 (3)
227.45 (9)
204.92 (3)
278.44 (7)
268.78 (9)

2.96 (0)
48.71 (1)
150.80 (3)
157.62 (3)
234.74 (9)
246.42 (4)
300.00 (8)
300.00 (11)

2.97 (0)
48.59 (1)
150.64 (3)
157.72 (3)
234.63 (9)
246.34 (4)
300.00 (8)
300.00 (11)

1.65 (0)
23.48 (0)
94.45 (1)
151.89 (3)
216.75 (8)
190.60 (3)
264.65 (6)
247.16 (8)

0.08 (0)
0.10 (0)
0.10 (0)
0.12 (0)
0.17 (0)
0.17 (0)
0.22 (0)
0.25 (0)

0.08 (0)
0.11 (0)
0.12 (0)
0.15 (0)
0.20 (0)
0.21 (0)
0.24 (0)
0.31 (0)

Table 2: Consistent MCSs Benchmark Results

#ctx

explicit

+EBL

3.29 (0)
41.57 (1)
154.55 (5)
130.90 (7)
166.14 (5)
261.96 (5)
286.74 (13)
300.00 (12)

2.70 (0)
17.94 (0)
148.11 (5)
102.57 (6)
118.04 (5)
143.75 (2)
206.10 (9)
300.00 (12)

#ctx

explicit

+EBL

3 (9)
4 (14)
5 (11)
6 (18)
7 (13)
8 (6)
9 (14)
10 (12)

0.09 (0)
0.13 (0)
0.16 (0)
0.18 (0)
0.19 (0)
0.23 (0)
0.32 (0)
0.44 (0)

0.09 (0)
0.14 (0)
0.17 (0)
0.19 (0)
0.17 (0)
0.20 (0)
0.27 (0)
0.33 (0)

3 (9)
4 (14)
5 (11)
6 (18)
7 (13)
8 (6)
9 (14)
10 (12)

all answer sets
UFS Œì
UFS Œì
mol
mod

+EBL

+UFL

‚Ñ¶

2.34 (0)
37.03 (1)
153.94 (5)
128.12 (7)
157.06 (5)
263.00 (5)
287.32 (12)
300.00 (12)

1.09 (0)
6.05 (0)
108.87 (2)
87.75 (4)
107.50 (4)
118.36 (2)
189.48 (8)
290.18 (11)

0.14 (0)
2.71 (0)
3.65 (0)
10.61 (0)
84.08 (2)
55.86 (1)
124.34 (5)
290.69 (11)

0.14 (0)
0.61 (0)
1.28 (0)
1.55 (0)
29.47 (0)
51.13 (1)
130.56 (6)
277.05 (11)

first answer set
UFS Œì
UFS Œì
mol
mod

+EBL

+UFL

‚Ñ¶

0.08 (0)
0.12 (0)
0.14 (0)
0.15 (0)
0.15 (0)
0.17 (0)
0.22 (0)
0.29 (0)

0.08 (0)
0.11 (0)
0.14 (0)
0.15 (0)
0.15 (0)
0.17 (0)
0.23 (0)
0.29 (0)

0.09 (0)
0.13 (0)
0.16 (0)
0.18 (0)
0.17 (0)
0.19 (0)
0.28 (0)
0.34 (0)

2.44 (0)
37.04 (1)
154.17 (5)
128.26 (7)
157.67 (5)
262.95 (5)
287.10 (12)
300.00 (12)

0.08 (0)
0.11 (0)
0.14 (0)
0.16 (0)
0.17 (0)
0.21 (0)
0.28 (0)
0.39 (0)

0.08 (0)
0.12 (0)
0.14 (0)
0.16 (0)
0.17 (0)
0.20 (0)
0.28 (0)
0.39 (0)

Table 3: Inconsistent MCSs Benchmark Results

The number of candidates for smaller models of the FLP reduct equals the number of unfounded
set candidates as each unfounded set corresponds to a smaller model. However, as we stop the
enumeration as soon as a smaller model respectively an unfounded set is found, the explicit and the
UFS check may consider depending on the specific program and solver heuristics different numbers
of interpretations. This explains why the UFS check is sometimes slightly slower than the explicit
check. However, the delay between different UFS candidates was always smaller, which sometimes
makes it faster even if it visits more candidates.
The results for consistent and inconsistent MCSs are shown in Table 2 and 3, respectively, where
the number of instances of of each system size is given in parentheses. Intuitively, consistent and
inconsistent MCSs are dual, as for each candidate the explicit resp. UFS check fails (i.e., stops
early), vs. for some (or many) candidates the check succeeds (stops late). However, the mixed
results do not permit us to draw solid conclusions on the computational relationship of the evaluation
295

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

methods. Nonetheless, we can see that the UFS check based on ‚Ñ¶ was often much faster than the
explicit check (up to three orders of magnitude).
As consistent MCSs have no IEs and hence no answer sets, we need not distinguish for them
between computing one or all answer sets. The effects of external behavior learning (Eiter et al.,
2012a) and of unfounded set learning are clearly evident in the MCS benchmarks, for both computing the first and all answer sets. The UFS check profits more from EBL than the explicit check,
further adding to its advantage. By activating UFL (which is not possible in the explicit check) we
gain another significant speedup.
We now discuss the effects of the syntactic decision criterion and program decomposition. Due
to saturation, the encoding contains cycles where nearly all cycles in the HEX-program contain at
least one external atom. Therefore, the decision criterion can reduce the set of atoms, for which
the UFS check needs to be performed, only by the atoms that are defined in the EDB. This does
not yield significant efficiency improvements. However, the benchmark results for MCS instances
confirm that the syntactic check is cheap and does not hurt performance. Over all 186 instances, the
total runtime with decision criterion and program decomposition was 11,695 seconds compared to
11,702 seconds without, and the number of instance timeouts was the same.
If we use encoding ‚Ñ¶ instead of Œì, we can observe another significant speedup for computing all
IEs of inconsistent MCSs. This is because there usually exist many answer sets (often thousands),
and thus a reusable encoding is very beneficial. In contrast, if we compute only the first answer set
or the MCS is consistent (no answer set exists), then the check with the more involved encoding ‚Ñ¶
is slightly slower; its reusability does not pay off if we abort after the first answer set.
In summary, we can observe that the encoding ‚Ñ¶ leads to a significant performance gain over
encoding Œì, while the decision criterion and decomposition do not help. In our next benchmark we
will observe opposite effects.
6.1.3 A BSTRACT A RGUMENTATION
In this benchmark we compute ideal set extensions (Dung, Mancarella, & Toni, 2007) for randomly
generated instances of abstract argumentation frameworks (AFs) (Dung, 1995) of different sizes.
The problem of checking whether a given set of arguments is an ideal set of an AF is co-NPcomplete (Dunne, 2009). In this benchmark we use a HEX encoding that mirrors this complexity: it
guesses such a set and checks its ideality using the Saturation technique involving an external atom
(see Appendix A.1).
Table 4 shows the results for different numbers of arguments, where each entry is the average of
30 benchmark instances. We compare the following configurations for both computing all and the
first answer set.
In the first column we do an explicit minimality check without learning techniques. The second
column shows that learning (EBL) leads to almost the same runtime results. This can be explained
by the structure of the encoding, which does not allow for effectively reusing learned nogoods.
In the third column, we perform an UFS-based minimality check using our encoding Œì, but
without applying the decision criterion and decomposition. We can observe that this is already a
significant improvements compared to the explicit minimality check, illustrating the effectiveness
of our new approach. Similar as in the MCS benchmark, the number of reduct model candidates
is equal to the number of UFS candidates in most cases, but the UFS check again enumerates its
candidates faster; this explains the observed speedup.
296

#args

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

#args

1 (30)
2 (30)
3 (30)
4 (30)
5 (30)
6 (30)
7 (30)
8 (30)
9 (30)
10 (30)
11 (30)
12 (30)
13 (30)
14 (30)
15 (30)
16 (30)
17 (30)
18 (30)
19 (30)
20 (30)

1 (30)
2 (30)
3 (30)
4 (30)
5 (30)
6 (30)
7 (30)
8 (30)
9 (30)
10 (30)
11 (30)
12 (30)
13 (30)
14 (30)
15 (30)
16 (30)
17 (30)
18 (30)
19 (30)
20 (30)

explicit

+EBL

0.06 (0)
0.08 (0)
0.11 (0)
0.19 (0)
0.32 (0)
0.71 (0)
1.58 (0)
4.75 (0)
14.02 (0)
41.10 (0)
129.35 (1)
250.16 (12)
294.91 (27)
290.01 (29)
290.01 (29)
300.00 (30)
300.00 (30)
300.00 (30)
300.00 (30)
300.00 (30)

0.06 (0)
0.07 (0)
0.10 (0)
0.19 (0)
0.32 (0)
0.72 (0)
1.66 (0)
5.04 (0)
14.97 (0)
44.38 (0)
139.80 (2)
258.82 (17)
296.67 (27)
290.01 (29)
290.01 (29)
300.00 (30)
300.00 (30)
300.00 (30)
300.00 (30)
300.00 (30)

all answer sets
UFS Œì
UFS Œì
mol
mod
0.05 (0)
0.06 (0)
0.08 (0)
0.14 (0)
0.26 (0)
0.55 (0)
1.16 (0)
3.06 (0)
8.65 (0)
24.53 (0)
51.39 (0)
119.44 (0)
274.65 (19)
290.00 (29)
290.00 (29)
300.00 (30)
300.00 (30)
300.00 (30)
280.39 (28)
278.20 (27)

explicit

+EBL

0.05 (0)
0.07 (0)
0.09 (0)
0.14 (0)
0.22 (0)
0.46 (0)
0.76 (0)
2.34 (0)
7.35 (0)
19.47 (0)
63.39 (1)
119.65 (4)
197.04 (14)
227.27 (22)
260.02 (26)
230.04 (23)
250.03 (25)
270.02 (27)
230.06 (23)
220.07 (22)

0.05 (0)
0.07 (0)
0.09 (0)
0.14 (0)
0.22 (0)
0.47 (0)
0.79 (0)
2.44 (0)
7.82 (0)
21.05 (0)
67.39 (1)
126.18 (4)
201.27 (15)
227.72 (22)
260.02 (26)
230.04 (23)
250.03 (25)
270.02 (27)
230.06 (23)
220.07 (22)

0.05 (0)
0.06 (0)
0.08 (0)
0.12 (0)
0.18 (0)
0.33 (0)
0.52 (0)
1.09 (0)
1.86 (0)
4.73 (0)
9.34 (0)
12.49 (0)
24.26 (0)
51.38 (3)
79.93 (3)
80.10 (4)
81.90 (5)
127.43 (8)
173.16 (13)
167.72 (12)

first answer set
UFS Œì
UFS Œì
mol
mod
0.05 (0)
0.06 (0)
0.08 (0)
0.12 (0)
0.21 (0)
0.42 (0)
0.68 (0)
1.98 (0)
5.76 (0)
15.37 (0)
26.30 (0)
60.88 (0)
149.25 (3)
218.00 (17)
260.01 (26)
230.02 (23)
250.01 (25)
270.01 (27)
211.12 (21)
200.29 (20)

0.05 (0)
0.06 (0)
0.08 (0)
0.10 (0)
0.15 (0)
0.27 (0)
0.37 (0)
0.89 (0)
1.36 (0)
3.54 (0)
4.61 (0)
6.11 (0)
16.34 (0)
41.28 (2)
40.92 (2)
40.63 (3)
35.24 (2)
74.89 (5)
66.58 (4)
81.81 (5)

+EBL
+UFL

‚Ñ¶

0.05 (0)
0.06 (0)
0.08 (0)
0.12 (0)
0.18 (0)
0.33 (0)
0.51 (0)
1.08 (0)
1.84 (0)
4.58 (0)
9.34 (0)
12.38 (0)
24.33 (0)
51.65 (3)
78.00 (3)
77.91 (4)
77.04 (5)
126.57 (8)
148.13 (10)
167.02 (12)

0.05 (0)
0.07 (0)
0.09 (0)
0.13 (0)
0.19 (0)
0.36 (0)
0.56 (0)
1.15 (0)
1.95 (0)
4.79 (0)
9.48 (0)
12.39 (0)
24.44 (0)
51.98 (3)
78.19 (3)
77.95 (4)
76.85 (5)
125.91 (8)
147.62 (10)
166.07 (12)

+EBL
+UFL

‚Ñ¶

0.05 (0)
0.06 (0)
0.07 (0)
0.10 (0)
0.15 (0)
0.27 (0)
0.37 (0)
0.90 (0)
1.28 (0)
3.53 (0)
4.66 (0)
6.11 (0)
16.49 (0)
41.68 (2)
41.38 (2)
40.69 (3)
35.60 (2)
75.47 (5)
67.03 (4)
82.33 (5)

0.05 (0)
0.06 (0)
0.08 (0)
0.12 (0)
0.17 (0)
0.29 (0)
0.40 (0)
0.94 (0)
1.34 (0)
3.68 (0)
4.69 (0)
6.13 (0)
16.50 (0)
41.76 (2)
41.62 (2)
40.84 (3)
35.57 (2)
75.10 (5)
67.04 (4)
82.45 (5)

Table 4: Argumentation Benchmark Results

When we enable the decision criterion and program decomposition, we can observe a further
speedup. This is because cycles in argumentation instances usually involve only small parts of
the overall program; thus the UFS search can be significantly simplified by excluding large program parts. We further have observed that program decomposition without the decision criterion is
counter-productive (not shown in the table), because a single UFS search over the whole program is
replaced by many UFS searches over program components (without the decision criterion, no such
check is excluded). This incurs more overhead.
297

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

In the fifth column we enable EBL and UFL, which leads to a small speedup in some cases.
However, as already mentioned above, no effective reuse of learned nogoods is possible.
Switching the encoding from Œì to ‚Ñ¶ leads to a small speedup in some cases, but is also counterproductive in others. This is because the programs in this benchmark have usually only very
few compatible sets, and only few unfounded set checks need to be performed. Hence the lower
initialization overhead of the encoding ‚Ñ¶ does not influence the runtime dramatically. On the other
hand, the higher complexity of the encoding ‚Ñ¶ increases the runtime of small instances.
6.1.4 D EFAULT R EASONING OVER D ESCRIPTION L OGICS
A prominent instance of HEX-programs are DL-programs, which combine description logic ontologies with rules; they result by using a special external atom that is available as DL-plugin in
DLVHEX . Via DL-programs, we obtain an encoding of terminological default reasoning over description logic ontologies in the approach of Baader and Hollunder (1995) into HEX-programs, in
which defaults require cyclic dependencies over external atoms. However, as all such dependencies
involve default negated atoms, we have no cycles according to Definition 12, which respects only
positive dependencies. Hence, the decision criterion comes to the conclusion that no UFS check is
required.
We used variants of the benchmarks presented by Eiter et al. (2012a), which query wines from
an ontology and classify them as red or white wines, where a wine is assumed to be white unless
the ontology explicitly entails the opposite. In this scenario, the decision criterion eliminates all
unfounded set checks. However, as there is only one compatible set per instance, there would be
only one unfounded set check anyway, hence the speedup due to the decision criterion is not significant. But the effect of the decision criterion can be increased by slightly modifying the scenario
such that there are multiple compatible sets. This can be done, for instance, by nondeterministic
default classifications, e.g., if a wine is not Italian, then it is either French or Spanish by default. Our
experiments have shown that with a small number of compatible sets, the performance enhancement
due to the decision criterion is marginal, but increases with larger numbers of compatible sets. For
instance, for 243 compatible sets (and thus 243 unfounded set checks) we could observe a speedup
from 13.59 to 12.19 seconds.
6.1.5 C ONFORMANT P LANNING
In classical AI planning, a planning domain contains a description of actions with their preconditions
and effects in the world. Finding a plan means to find a sequence of actions that reaches from a given
initial state a state fulfilling a given goal condition. Conformant planning (Goldman & Boddy, 1996;
Smith & Weld, 1998) is the same problem but where the initial state is only partially specified and/or
the domain is nondeterministic, such that by executing the plan we reach the goal regardless of the
action outcomes and the actual initial state.
We here experimented on a very simple conformant planning domain: two robots with a limited
sensor range patrol an area, in which is an object at an unknown initial location. The goal is to find
a sequence of movements of the two robots such that they detect the object in all cases. For experiments we used an encoding which realizes conformant planning using Saturation (see above) and
contains an external atom for computing whether the patrol robots detect object (cf. Appendix A.2).
In general, deciding the existence of a short (polynomial length bounded) conformant plan is Œ£p3 298

map size
3√ó4 (10)
4√ó4 (10)
5√ó4 (10)
6√ó4 (10)
7√ó4 (10)
8√ó4 (10)
9√ó4 (10)
10√ó4 (10)
11√ó4 (10)
12√ó4 (10)
13√ó4 (10)
14√ó4 (10)
15√ó4 (10)
16√ó4 (10)

plan length

3√ó4 (10)
4√ó4 (10)
5√ó4 (10)
6√ó4 (10)
7√ó4 (10)
8√ó4 (10)
9√ó4 (10)
10√ó4 (10)
11√ó4 (10)
12√ó4 (10)
13√ó4 (10)
14√ó4 (10)
15√ó4 (10)
16√ó4 (10)

all answer sets
explicit

UFS Œì
mol

UFS Œì
mod

+EBL

+UFL

‚Ñ¶
-EBL-UFL

‚Ñ¶
+EBL+UFL

1
1
1
2
2
3
3
4
4
5
5
6
6
7

7.10 (0)
10.66 (0)
10.69 (0)
206.45 (2)
258.82 (5)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)

0.12 (0)
0.16 (0)
0.15 (0)
1.98 (0)
2.85 (0)
36.80 (0)
43.20 (0)
300.00 (10)
299.76 (9)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)

0.11 (0)
0.15 (0)
0.15 (0)
1.38 (0)
1.79 (0)
16.41 (0)
19.53 (0)
274.53 (5)
239.61 (5)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)

0.11 (0)
0.15 (0)
0.14 (0)
1.67 (0)
2.44 (0)
40.94 (0)
78.11 (0)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)

0.12 (0)
0.15 (0)
0.14 (0)
1.69 (0)
2.43 (0)
40.99 (0)
77.10 (0)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)

0.12 (0)
0.15 (0)
0.13 (0)
1.09 (0)
1.50 (0)
10.42 (0)
13.91 (0)
203.70 (2)
174.86 (2)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)

0.14 (0)
0.18 (0)
0.15 (0)
1.35 (0)
1.84 (0)
13.88 (0)
19.62 (0)
252.31 (5)
209.41 (3)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)

first answer set

plan length

map size

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

explicit

UFS Œì
mol

UFS Œì
mod

+EBL

+UFL

‚Ñ¶
-EBL-UFL

‚Ñ¶
+EBL+UFL

1
1
1
2
2
3
3
4
4
5
5
6
6
7

0.89 (0)
1.36 (0)
2.23 (0)
7.21 (0)
17.39 (0)
139.26 (1)
150.50 (3)
255.89 (7)
300.00 (10)
287.76 (9)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)

0.05 (0)
0.06 (0)
0.06 (0)
0.22 (0)
0.34 (0)
6.07 (0)
3.24 (0)
92.19 (2)
97.11 (2)
198.75 (5)
287.07 (9)
300.00 (10)
300.00 (10)
300.00 (10)

0.05 (0)
0.05 (0)
0.07 (0)
0.15 (0)
0.22 (0)
2.73 (0)
1.47 (0)
47.58 (0)
39.99 (0)
143.52 (4)
211.97 (5)
244.33 (7)
300.00 (10)
300.00 (10)

0.05 (0)
0.05 (0)
0.06 (0)
0.14 (0)
0.21 (0)
2.73 (0)
1.69 (0)
82.84 (2)
84.08 (1)
184.81 (5)
277.79 (9)
300.00 (10)
300.00 (10)
300.00 (10)

0.05 (0)
0.06 (0)
0.06 (0)
0.14 (0)
0.20 (0)
2.69 (0)
1.70 (0)
82.52 (2)
83.85 (1)
184.78 (5)
277.71 (9)
300.00 (10)
300.00 (10)
300.00 (10)

0.06 (0)
0.06 (0)
0.07 (0)
0.12 (0)
0.17 (0)
1.45 (0)
0.89 (0)
24.23 (0)
19.53 (0)
131.46 (4)
165.64 (4)
213.89 (5)
285.36 (9)
300.00 (10)

0.06 (0)
0.06 (0)
0.07 (0)
0.13 (0)
0.18 (0)
1.78 (0)
1.16 (0)
31.36 (0)
25.85 (0)
136.64 (4)
185.84 (4)
232.85 (6)
296.10 (9)
300.00 (10)

Table 5: Conformant Planning Benchmark Results
complete, see Turner (2002), but if action executability is decidable in polynomial time, the problem
is in Œ£p2 ; our example domain enjoys the property.
Our results are displayed in Table 5, which shows averages over 10 instances per size. The
instances consist of n√ó4 grids with n ‚àà {3, . . . , 16}, the plan length required for finding a solution
increases with larger instance sizes. (This is because the number of robots does not increase while
the two robots must still cover the whole area.) Instances were generated by randomly placing
robots in opposite quarters of the map.
As expected we observe that the explicit FLP check performs worst, followed by the monolithic
UFS check with Œì encoding, and the modular UFS Œì encoding; the UFS ‚Ñ¶ encoding (without
external behavior nor unfounded set learning) performs best. External behavior learning (EBL) and
unfounded set learning (UFL) do not improve the performance, on the contrary it increases the run
times significantly for the modular UFS Œì check and slightly for the ‚Ñ¶ check. EBL does not change
times significantly for the explicit check, therefore we omit results for explicit +EBL.
By looking into profiling information and at the domain we found the reasons: (a) the external
atoms depend on a large part of the interpretation (locations of all robots) so EBL cannot cut away
299

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

significant portions of the search space; (b) evaluating the external atom takes a negligible amount
of time, so beneficial effects of EBL will be outweighed by is computational overhead. For UFL we
observed that the benchmark instances contain only few unfounded sets (pruning less than half of
the answer set candidates) and thus UFL cannot improve performance given the overhead it incurs.
We conclude that in some scenarios, using EBL and UFL can reduce efficiency.
As in the ‚Ñ¶ check the UFS check encoding is constructed only once, the overhead for EBL
and UFL observed with the Œì encoding no longer has such a big impact. Nevertheless ‚Ñ¶ without
learning slightly outperforms ‚Ñ¶ with UFL and EBL. An analysis of the number of UFS checks and
the number of external atom evaluations (not shown in table) revealed that UFL and EBL decreases
(i) the number of unfounded set checks needed and (ii) the number of external atoms evaluated (in
one 9√ó4 instance, from 5132 to 3341). Thus if external computations would be more costly, the
positive effects of UFL and EBL on ‚Ñ¶ would outweigh their computational overhead and it would
be beneficial to activate UFL and EBL in this domain.
Interestingly, for small map sizes we see that with the ‚Ñ¶ encoding, the 3√ó4 instances actually
seem to be harder to solve than the larger 4√ó4 and 5√ó4 instances. This is because all these instances
require plan length 1 to find a solution; so the larger instances are more constrained than the smaller
instances because there robots have less freedom to move around while still detecting all objects.
Hence, for 5√ó4 maps the solver finds solutions faster than for 4√ó4 maps.
The conclusion from this benchmark is that depending on the computational task in external
atoms, UFL and EBL can be beneficial or harmful for efficiency of reasoning.
6.2 Summary
Our experiments have shown that the learning technique EBL developed by Eiter et al. (2012a) and
the techniques introduced in this paper lead to significant performance improvements in most cases,
with few exceptions for specific benchmarks. The effects of external behavior learning (EBL) are
clearly evident both for the explicit minimality check and for the unfounded set-based check, but are
even more prominent with the latter. Independently of whether EBL is used or not, unfounded set
checking pushes the efficiency of HEX-program evaluation compared to explicit minimality checking. Moreover, it allows for learning of additional nogoods, which is also advantageous in most of
our benchmarks. Regarding the two problem encodings, the benchmarks show that the UFS check
is usually faster with the ‚Ñ¶ encoding than with the Œì encoding, however the former UFS check
involves more initialization overhead, which might be counterproductive for small programs.
The decision criterion may lead to an additional speedup and does not introduce notable overhead, thus it can always be activated. Finally, program decomposition often leads to an additional
performance gain, but should only be used in combination with the decision criterion because otherwise a single UFS check is replaced by multiple UFS checks, which involves more overhead.

7. Discussion
We now discuss related work and outline some possible starting points for extensions.
7.1 Related Work
Constraint answer set solving (Gebser, Ostrowski, & Schaub, 2009) can be seen as a special case
of HEX-programs. It extends ASP by dedicated constraint atoms in rule bodies (comparisons of
300

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

numeric constraint variables) that allow for a bidirectional exchange of information between the
logic program and the constraint solver. While the constraint solver is a concrete instance of an
external source, HEX-programs support arbitrary external sources. The idea of external behavior
learning (EBL), introduced by Eiter et al. (2012a) is further related to the work of Drescher and
Walsh (2012) since both approaches generate nogoods on-the-fly.
Besides grounding, one of the main differences between ASP and SAT solving is foundedness,
i.e., the truth of each atom in an answer set is justified by a non-circular derivation from rules and
facts. To account for circularity, the notion of unfounded set has been introduced by van Gelder et
al. (1991) for defining the well-founded semantics of logic programs with negation, by constructing
the least fixpoint of a monotone operator on partial assignments; the total fixpoints of this operator
correspond to the stable models of the logic program. This actually allows to give a characterization
of the stable models in terms of unfounded sets. In fact, unfounded set checking turned out to be a
fruitful model-based approach in ASP solving, which has an equally successful syntactic counterpart
known as loop formulas (Lin & Zhao, 2004; Lee & Lifschitz, 2003; Lee, 2005). Different kinds of
unfounded set checks with different complexities have been developed for various program classes.
The computation of answer sets by a model generate and test approach, which is pursued by
many ASP solvers, requires that a form of minimality check or unfounded set check is carried
out already for ordinary logic programs (i.e., in absence of external atoms). However, for normal
program this test is tractable and it is frequently realized using source pointers (Simons et al., 2002).
Intuitively, the reasoner stores for each atom a pointer to a rule which possibly supports this atom.
The list of source pointers is updated during propagation. If at some point there is no supporting
rule for an atom, then it can conclude that this atom must be false.
In the context of ASP, the notion of unfounded set has been explicitly formulated and extended to disjunctive logic programs by Leone et al. (1997), who proved that the stable models
of a disjunctive logic program are its models that are unfounded-free. This results was the basis
for the architecture of the DLV solver, which generates answer set candidates that are checked for
unfounded-freeness. This test, which like answer set checking for disjunctive answer set programs,
is co-NP-complete (Faber, 2005), has been reduced by Koch et al. (2003) to unsatisfiability testing
of a SAT instance. This approach has been later extended to conflict-driven learning and unfounded
set checking by Drescher et al. (2008), where two instances for the CLASP solver, an ASP instance
and a SAT instance, are used to generate and check answer set candidates, respectively. In parallel
to our work, this technique was recently refined by exploiting assumptions such that the encoding
of the unfounded set search does not need to be adopted to the current assignment (Gebser, Kaufmann, & Schaub, 2013). This is related to our uniform encoding of the unfounded set search, but
still restricted to disjunctive ASP without external sources. For HEX-programs, the unfounded set
search needs to respect the semantics of external atoms and is thus a more general problem. Alviano, Calimeri, Faber, Leone, and Perri (2011) consider normal logic programs with monotone
and antimonotone aggregate atoms, and defined unfounded sets for such programs. Based on this,
they extended the well-founded semantics of Van Gelder et al. (1991), which‚Äîalthough closely
related‚Äîis weaker than the general FLP semantics (Faber et al., 2011).
We now considered unfounded set checking in the presence of external sources and for FLP
answer sets, for which the results by Drescher et al. (2008) do not immediately carry over. Indeed,
already for ground Horn programs, the presence of nonmonotonic external atoms that are decidable
in polynomial time makes unfoundedness checking intractable (more precisely, co-NP-complete),
such that deciding the existence of an FLP answer set is a Œ£p2 -complete problem in the ground case.
301

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

For computationally harder external atoms, the complexity may increase relative to an oracle for
the external function (see Faber, 2005). However, the results from this paper do still apply in such
cases.
Drescher et al. (2008) employed also a splitting technique, which goes back to the work of Leone
et al. (1997); it is related to our program decomposition, but works for ASP programs without external sources only. Note that our notion of splitting is different from the well-known splitting sets by
Lifschitz and Turner (1994), as we consider only positive dependencies for ordinary atoms. While
we consider e-cycles, which are specific for HEX-programs, the interest of Drescher et al. (2008)
was with head-cycles, which may arise with disjunctive rule heads. In fact, our approach may be regarded as an extension of the one of Drescher et al., since the evaluation of Œ†ÃÇ follows their principles
of performing UFS checks in case of head-cycles.
For the evaluation of the FLP semantics, an unfounded set check or explicit FLP check is instrumental. we just mention that other semantics of HEX-programs may not involve such a step which
is intractable in general (as follows from Leone et al., 2006, already for ground Horn programs with
nonmonotonic external atoms that are decidable in polynomial time). For instance, Shen (2011)
and Shen and Wang (2011) present a well-justified semantics where unfounded set checking is essentially replaced by a fixpoint iteration which, intuitively, tests if a model candidate reproduces
itself but excludes circular justifications. However, the complexity of answer set computation does
not decrease by this approach in general, and in particular deciding well-justified answer set existence for ground Horn programs with nonmonotonic external atoms that are decidable in polynomial
time is Œ£p2 -complete, and thus as hard as deciding the existence of answer sets as in Definition 3.
7.2 Extensions
We have designed our unfounded set check as a postcondition test; that is like the explicit FLP check,
it is carried out only after a complete assignment has been generated as an answer set candidate.
However, in certain cases it might be obvious that a partial interpretation (in which some truth values
are open) can be extended to an answer set, because the existence of an unfounded set is guaranteed
for any extension to a complete assignment. One can then backtrack earlier, which intuitively leads
to a saving for certain classes of instances.
Exploring this idea, we have generalized our framework with a control component which decides, based on a heuristics, when an unfounded set check is carried out; in the standard setting,
this is whenever an assignment in the model generation is complete (i.e., a complete assignment is
given).
A UFS check on partial assignments, that is sound with respect to any extension to a complete
assignment, is possible if the ASP solver has finished unit propagation over a maximal subset of
the program such that the interpretation is already complete on it, and all guessed values of external
atom replacements are correct. We thus used this criterion, which is easy to test, for a greedy
heuristics to issue UFS checks in our prototype system.
However, in contrast to our initial expectation, we found that for all our benchmarks the UFS
check wrt. partial assignments was not productive. A closer look reveals that this is essentially because nogood learning from unfounded sets (UFL) effectively avoids the reconstruction of the same
unfounded set anyway. It therefore rarely happens that UFS checking wrt. a partial interpretation
identifies an unfounded set earlier than UFS checking wrt. complete interpretations. Therefore, we
believe that UFS checking wrt. a partial interpretation rarely identifies an unfounded set earlier than
UFS checking wrt. complete assignments. As UFS checking for HEX-programs involves the evalu302

#args

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

5 (1)
6 (1)
7 (1)
8 (1)
9 (1)
10 (1)
15 (1)
20 (1)
25 (1)

‚Ñ¶
+EBL+UFL

all answer sets
‚Ñ¶ partial (periodic) ‚Ñ¶ partial (max)
+EBL+UFL
+EBL+UFL

0.07 (0)
0.10 (0)
0.13 (0)
0.18 (0)
0.24 (0)
0.29 (0)
0.80 (0)
1.96 (0)
4.15 (0)

0.09 (0)
0.13 (0)
0.15 (0)
0.20 (0)
0.26 (0)
0.33 (0)
0.96 (0)
2.46 (0)
5.52 (0)

0.11 (0)
0.15 (0)
0.19 (0)
0.26 (0)
0.35 (0)
0.47 (0)
1.61 (0)
4.92 (0)
11.25 (0)

‚Ñ¶
+EBL+UFL

first answer set
‚Ñ¶ partial (periodic)
+EBL+UFL

‚Ñ¶ partial (max)
+EBL+UFL

0.05 (0)
0.06 (0)
0.07 (0)
0.08 (0)
0.09 (0)
0.11 (0)
0.24 (0)
0.51 (0)
0.97 (0)

0.06 (0)
0.07 (0)
0.08 (0)
0.10 (0)
0.12 (0)
0.14 (0)
0.38 (0)
0.97 (0)
1.98 (0)

0.07 (0)
0.09 (0)
0.11 (0)
0.14 (0)
0.17 (0)
0.21 (0)
0.73 (0)
2.30 (0)
4.50 (0)

#ctx

Table 6: Set Partitioning with UFS Checking over Partial Assignments

3 (6)
4 (10)
5 (8)
6 (6)
7 (12)
8 (5)
9 (8)
10 (11)

‚Ñ¶
+EBL+UFL

‚Ñ¶ partial (periodically)
+EBL+UFL

‚Ñ¶ partial (max)
+EBL+UFL

0.08 (0)
0.11 (0)
0.12 (0)
0.15 (0)
0.20 (0)
0.21 (0)
0.24 (0)
0.31 (0)

0.09 (0)
0.11 (0)
0.12 (0)
0.15 (0)
0.20 (0)
0.21 (0)
0.24 (0)
0.31 (0)

0.10 (0)
0.12 (0)
0.13 (0)
0.16 (0)
0.21 (0)
0.22 (0)
0.27 (0)
0.32 (0)

#ctx

Table 7: Consistent MCSs Benchmark Results with UFS Checking over Partial Assignments

3 (9)
4 (14)
5 (11)
6 (18)
7 (13)
8 (6)
9 (14)
10 (12)

‚Ñ¶
+EBL+UFL
0.14 (0)
0.61 (0)
1.28 (0)
1.55 (0)
29.47 (0)
51.13 (1)
130.56 (6)
277.05 (11)

all answer sets
‚Ñ¶ partial (periodic) ‚Ñ¶ partial (max)
+EBL+UFL
+EBL+UFL
0.13 (0)
0.64 (0)
1.36 (0)
1.67 (0)
31.54 (0)
51.22 (1)
130.99 (6)
277.20 (11)

0.16 (0)
0.88 (0)
1.81 (0)
2.49 (0)
44.90 (1)
51.66 (1)
133.84 (6)
278.21 (11)

‚Ñ¶
+EBL+UFL

first answer set
‚Ñ¶ partial (periodic)
+EBL+UFL

‚Ñ¶ partial (max)
+EBL+UFL

0.09 (0)
0.13 (0)
0.16 (0)
0.18 (0)
0.17 (0)
0.19 (0)
0.28 (0)
0.34 (0)

0.09 (0)
0.13 (0)
0.16 (0)
0.18 (0)
0.17 (0)
0.20 (0)
0.27 (0)
0.35 (0)

0.10 (0)
0.14 (0)
0.17 (0)
0.18 (0)
0.18 (0)
0.21 (0)
0.28 (0)
0.36 (0)

Table 8: Inconsistent MCSs Benchmark Results with UFS Checking over Partial Assignments
ation of external sources and compatibility testing, this easily leads to costs that are higher than the
potential savings. A more detailed analysis requires further studies; since the results do not seem to
be promising, we leave this for possible future work.
Table 6‚Äì10 show the benchmark results if UFS checking wrt. partial assignments is enabled
when computing all or the first answer set only.
The first column shows the runtime with UFS checking wrt. complete interpretations only, using
encoding ‚Ñ¶, EBL and UFL (equivalent to the last column in the tables in Section 6). The second
column shows the results with UFS checking wrt. partial assignments, using a heuristics which
performs the UFS check periodically (periodic). The third column shows the runtimes if the UFS
check is always performed, if no other propagation technique can derive further truth values (max).
303

#args

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

1 (30)
2 (30)
3 (30)
4 (30)
5 (30)
6 (30)
7 (30)
8 (30)
9 (30)
10 (30)
11 (30)
12 (30)
13 (30)
14 (30)
15 (30)
16 (30)
17 (30)
18 (30)
19 (30)
20 (30)

‚Ñ¶
+EBL+UFL
0.05 (0)
0.07 (0)
0.09 (0)
0.13 (0)
0.19 (0)
0.36 (0)
0.56 (0)
1.15 (0)
1.95 (0)
4.79 (0)
9.48 (0)
12.39 (0)
24.44 (0)
51.98 (3)
78.19 (3)
77.95 (4)
76.85 (5)
125.91 (8)
147.62 (10)
166.07 (12)

all answer sets
‚Ñ¶ partial (periodic) ‚Ñ¶ partial (max)
+EBL+UFL
+EBL+UFL
0.05 (0)
0.06 (0)
0.09 (0)
0.14 (0)
0.20 (0)
0.36 (0)
0.56 (0)
1.15 (0)
1.94 (0)
4.80 (0)
9.49 (0)
12.42 (0)
24.45 (0)
52.03 (3)
78.14 (3)
77.99 (4)
76.86 (5)
126.17 (8)
147.51 (10)
165.96 (12)

0.05 (0)
0.07 (0)
0.10 (0)
0.16 (0)
0.22 (0)
0.39 (0)
0.59 (0)
1.19 (0)
2.01 (0)
4.96 (0)
9.71 (0)
12.79 (0)
25.32 (0)
52.57 (3)
79.81 (3)
79.52 (4)
77.82 (5)
128.83 (8)
149.62 (10)
168.53 (12)

‚Ñ¶
+EBL+UFL

first answer set
‚Ñ¶ partial (periodic)
+EBL+UFL

‚Ñ¶ partial (max)
+EBL+UFL

0.05 (0)
0.06 (0)
0.08 (0)
0.12 (0)
0.17 (0)
0.29 (0)
0.40 (0)
0.94 (0)
1.34 (0)
3.68 (0)
4.69 (0)
6.13 (0)
16.50 (0)
41.76 (2)
41.62 (2)
40.84 (3)
35.57 (2)
75.10 (5)
67.04 (4)
82.45 (5)

0.05 (0)
0.07 (0)
0.08 (0)
0.12 (0)
0.16 (0)
0.29 (0)
0.40 (0)
0.94 (0)
1.35 (0)
3.67 (0)
4.71 (0)
6.11 (0)
16.46 (0)
41.80 (3)
41.53 (2)
40.79 (3)
35.53 (2)
75.32 (5)
66.88 (4)
82.27 (5)

0.05 (0)
0.07 (0)
0.09 (0)
0.14 (0)
0.18 (0)
0.31 (0)
0.42 (0)
0.96 (0)
1.39 (0)
3.75 (0)
4.74 (0)
6.23 (0)
16.80 (0)
41.98 (3)
42.02 (2)
41.04 (3)
35.58 (2)
75.37 (5)
67.59 (4)
82.90 (5)

Table 9: Argumentation with UFS Checking over Partial Assignments
It can be observed that UFS checking wrt. partial assignments does not lead to a further speedup
in any case. Quite the contrary, some instances have significantly higher runtimes with more frequent unfounded set checks. This is best visible in the set partitioning benchmark (Table 6), when
computing all explanations for inconsistent MCSs with 5, 6 or 7 contexts (Table 9), and when computing all answer sets in the conformant planning benchmark (Table 10). In the set partitioning
benchmark the effects are especially significant, which is as expected because every compatible set
is unfounded-free. Thus, additional UFS checks are always counterproductive. In the consistent
multi-context systems, reasoning is fast anyway, thus the frequency of UFS checking has no significant impact (Table 7). In the argumentation benchmark we can also observe a slight slowdown by
more frequent UFS checking, although it is less dramatic than in the other benchmarks because the
other propagation methods are applicable more frequently and thus fewer UFS checks are performed
even with setting max (Table 9).
On the other hand, for ASP solving (where no such extra costs incur), UFS checks over partial
interpretations may still be beneficial, as reported by Gebser et al. (2013). In conclusion, UFS
checks on partial assignments of HEX-programs will require tailored heuristics that not only take
the structure of the program, but also domain-specific knowledge into account, which remains for
future work.

8. Conclusion
HEX -programs are an expressive extension of non-monotonic logic programs with access to external
information via external atoms; supported by a plugin architecture they can be fruitfully deployed
for a range of applications. External atoms however make the efficient evaluation of HEX-programs
a challenging task, and in particular to compute answer sets of a HEX-program Œ†, which are the
models A of Œ† that are subset-minimal models of its FLP-reduct f Œ†A (which keeps all rules whose

304

#ctx

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

3√ó4 (10)
4√ó4 (10)
5√ó4 (10)
6√ó4 (10)
7√ó4 (10)
8√ó4 (10)
9√ó4 (10)
10√ó4 (10)
11√ó4 (10)
12√ó4 (10)
13√ó4 (10)
14√ó4 (10)
15√ó4 (10)
16√ó4 (10)

1
1
1
2
2
3
3
4
4
5
5
6
6
7

all answer sets
‚Ñ¶ ‚Ñ¶ partial (periodic) ‚Ñ¶ partial (max)
+EBL+UFL
+EBL+UFL
+EBL+UFL

first answer set
‚Ñ¶ ‚Ñ¶ partial (periodic) ‚Ñ¶ partial (max)
+EBL+UFL
+EBL+UFL
+EBL+UFL

0.14 (0)
0.18 (0)
0.15 (0)
1.35 (0)
1.84 (0)
13.88 (0)
19.62 (0)
252.31 (5)
209.41 (3)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)

0.06 (0)
0.06 (0)
0.07 (0)
0.13 (0)
0.18 (0)
1.78 (0)
1.16 (0)
31.36 (0)
25.85 (0)
136.64 (4)
185.84 (4)
232.85 (6)
296.10 (9)
300.00 (10)

0.14 (0)
0.17 (0)
0.15 (0)
1.35 (0)
1.83 (0)
14.23 (0)
19.96 (0)
257.18 (5)
214.72 (3)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)

0.16 (0)
0.20 (0)
0.18 (0)
1.48 (0)
2.03 (0)
17.27 (0)
23.75 (0)
289.20 (7)
244.84 (5)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)
300.00 (10)

0.06 (0)
0.06 (0)
0.07 (0)
0.13 (0)
0.18 (0)
1.86 (0)
1.18 (0)
33.40 (0)
27.15 (0)
137.22 (4)
188.73 (4)
235.06 (7)
297.42 (9)
300.00 (10)

0.08 (0)
0.08 (0)
0.09 (0)
0.15 (0)
0.21 (0)
2.36 (0)
1.42 (0)
49.36 (0)
37.64 (0)
142.74 (4)
209.44 (4)
243.73 (7)
300.00 (10)
300.00 (10)

Table 10: Conformant Planning with UFS Checking over Partial Assignments

bodies are satisfied). To improve on this expensive test (which was customary in implementations
so far), we have presented an alternative test based on unfounded sets that we obtain by adapting the
notion of unfounded set for aggregates by Faber (2005) to external atoms. Also Alviano et al. (2011)
use a related notion of unfounded sets for programs with aggregates, but restrict the discussion to
monotonic and antimonotonic aggregates. We have realized unfounded set (UFS) checking by a
transformation to SAT solving, where the satisfying assignments of a constructed CNF generate
candidate unfounded sets, which in turn are subject to a (rather simple) postcheck that takes external
atom evaluation into account.
In particular, we have provided two SAT encodings for UFS checking: the conceptually simple
encoding ŒìA
Œ† , which needs initialization for every UFS check, and the advanced encoding ‚Ñ¶Œ† ,
which can be reused for all UFS checks. To further boost performance, we have shown how to learn
from unfounded sets by deriving nogoods, i.e., constraints (possibly involving also external atoms)
which guide future search in model generation and help to avoid that unfounded sets are regenerated.
In further elaboration, we have refined the basic approach by suitable program splitting, such that the
UFS check can be carried out independently on program components, cutting down the complexity.
Furthermore, we have presented a syntactic criterion that allows us to decide efficiently whether the
UFS check can be safely skipped for a component or the whole program, exploiting that the answer
set candidates from the model search have only special unfounded sets that involve cyclic input to
external atoms; for HEX-programs in simple applications, this is usually not the case.
The experimental evaluation of our new approach, which considered different combinations
of the techniques and comprised problems from various domains including multi-context systems,
abstract argumentation, default reasoning over ontologies, and conformant planning, where HEXprograms serve for easy-cut declarative problem solving, has shown that it is more efficient than
the traditional minimal model check; it can lead to exponential gains and yields often drastic performance improvements, while it is not slower (except in very few cases by a marginal amount).
Furthermore, the reusable encoding ‚Ñ¶Œ† turned out to be beneficial for programs that require many
unfounded set checks, which includes all programs that have many answer sets.
305

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

8.1 Future work
An issue for further improvement of the approach in this paper are other heuristics for UFS checking over partial assignments. While the natural heuristics that we considered are counterproductive,
others might lead to additional speedup in some cases. However, this may require to incorporate
domain-specific knowledge about external atoms; currently this is only done in the learning algorithms but not in the heuristics. In the same line, one might consider developing a heuristics for
dynamically choosing between the UFS search encodings that we presented, and to study heuristics
for guiding the unfounded set search, i.e., for variable selection by the SAT solver. Currently, our
implementation applies the same heuristics for the unfounded set search as in the model generation process. However, our experimental comparison with the explicit minimality check in terms
of the considered candidate sets suggests that there is room for improvement by employing suitable
choices. Developing appropriate heuristics and validating their effectiveness on candidate set enumeration remains to be explored. Finally, an obvious issue are other criteria that allow to skip the
UFS check or simplify it while they can be efficiently tested.

Acknowledgments
Preliminary results of this work have been presented at the 13th European Conference on Logics in
Artificial Intelligence (JELIA 2012), September 26-28, 2012, Toulouse, France (Eiter, Fink, Krennwallner, Redl, & SchuÃàller, 2012c), and the 5th Workshop on Answer Set Programming and Other
Computing Paradigms (ASPOCP 2012), September 4, 2012, Budapest, Hungary (Eiter, Fink, Krennwallner, Redl, & SchuÃàller, 2012b). We are grateful to the anonymous reviewers for their helpful
and constructive comments. This work was supported by the Austrian Science Fund (FWF) via
the projects P20840, P20841, P24090, and by the Vienna Science and Technology Fund (WWTF)
project ICT08-020. Peter SchuÃàller was supported by TUBITAK Fellowship 2216.

Appendix A. Benchmark Encodings
In this appendix, we give details to some of the benchmark encodings (those which are not described in the references). We note that these encodings have not been developed and tuned for
good performance and serve merely for an experimental comparison of the various FLP check realizations. Benchmark encodings and HEX-plugins are publicly available at https://github.com/
hexhex/benchmarks.
A.1 Abstract Argumentation
The Abstract Argumentation benchmark results in Section 6.1 were obtained using the following
encoding, which is derived from encodings for admissible and preferred set extensions of an argumentation framework (A, att) described by Egly, Gaggl, and Woltran (2010).
Input instances of this benchmark are defined over a set A of arguments encoded as facts arg(a)
for each a ‚àà A and a set att of attacks between arguments, encoded as facts att(a, b) for some
(a, b) ‚àà A √ó A. The encoding consists of the following rules where x, y, z ‚àà A; very similar
encodings are explained in detail by Egly et al. (2010) (but without the use of external atoms).
We define defeat from attacks.
defeat(x, y) ‚Üê att(x, y).
306

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

We guess a set S ‚äÜ A using predicates inS and outS .
inS (x) ‚Üê not outS (x), arg(x).

outS (x) ‚Üê not inS (x), arg(x).

We require that all arguments in S are conflict-free and defended from S.

‚Üê inS (x), inS (y), defeat(x, y).

defeated (x) ‚Üê inS (y), defeat(y, x).

notDefended (x) ‚Üê defeat(y, x), not defeated (y).
‚Üê inS (x), notDefended (x).

For saturation we define a linear order on arguments, including infimum and supremum.
lt(x, y) ‚Üê arg(x), arg(y).

(x < y)

nsucc(x, z) ‚Üê lt(x, y), lt(y, z).

succ(x, y) ‚Üê lt(x, y), not nsucc(x, y).
ninf (x) ‚Üê lt(y, x).

nsup(x) ‚Üê lt(x, y).

inf (x) ‚Üê not ninf (x), arg(x).

sup(x) ‚Üê not nsup(x), arg(x).

We perform a guess over a set T ‚äÜ A using a disjunction.

inT (x) ‚à® outT (x) ‚Üê arg(x).

We check each argument of T whether it is in S and spoil the answer set if S ‚äÜ T .
sInT upto (y) ‚Üê inf (y), inS (y), inT (y).
sInT upto (y) ‚Üê inf (y), outS (y).

sInT upto (y) ‚Üê succ(z, y), inS (y), inT (y), sInT upto (z).
sInT upto (y) ‚Üê succ(z, y), outS (y), sInT upto (z).
sInT ‚Üê sup(y), sInT upto (y).
spoil ‚Üê sInT .

We also spoil the answer set if T is not a preferred extension, determined by an external atom with
the semantic function f&argSemExt such that f&argSemExt (A, pref , arg, att, inT , unused , spoil ) =
1 iff Fspoil ‚àà A or the extension of predicate inT is a preferred set extension of the argumentation
framework specified by the extension of predicates arg and att. Internally, the external atom uses
another ASP program to compute the semantics. This check is performed using an ASP encoding
for preferred extensions from Egly et al. (2010).
tIsNotPref ‚Üê &argSemExt[pref , arg, att, inT , unused , spoil ]().
spoil ‚Üê tIsNotPref .

Note that the parameters pref and unused support more general functionalities of f&argSemExt
which are not relevant for this benchmark. We create a unique answer set whenever spoil is true
and require that only spoiled answer sets are returned.
inT (x) ‚Üê spoil , arg(x).
sInT ‚Üê spoil .

outT (x) ‚Üê spoil , arg(x).

tIsNotPref ‚Üê spoil .

‚Üê not spoil .

Given an instance encoded as above, an answer set to the above program exists iff there exists
an ideal set extension of the given argumentation framework.
307

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

A.2 Conformant Planning
The Conformant Planning benchmark results in Section 6.1 were obtained using the following encoding.
Input instances of this benchmark are defined over a set R of robots, sets X and Y of valid x and
y coordinates of the environment, and a maximum plan length l; an instance contains for each robot
r ‚àà R the initial position (x, y) as facts robo X (r, x, 0) and robo Y (r, y, 0). The encoding consists
of the following rules where, unless stated otherwise, 0 ‚â§ t < l, r ‚àà R, x ‚àà X, y ‚àà Y .
For each robot we generate four possible moves in the environment.
move(r, x, y + 1, t) ‚à® move(r, x, y + 1, t) ‚Üê robo X (r, x, t), robo Y (r, y, t).

(y + 1 ‚àà Y )

move(r, x + 1, y, t) ‚à® move(r, x + 1, y, t) ‚Üê robo X (r, x, t), robo Y (r, y, t).

(x + 1 ‚àà X)

move(r, x, y ‚àí 1, t) ‚à® move(r, x, y ‚àí 1, t) ‚Üê robo X (r, x, t), robo Y (r, y, t).

(y ‚àí 1 ‚àà Y )

move(r, x ‚àí 1, y, t) ‚à® move(r, x ‚àí 1, y, t) ‚Üê robo X (r, x, t), robo Y (r, y, t).

(x ‚àí 1 ‚àà X)

We disallow moving to multiple locations and standing still (the latter is not strictly necessary but
we obtained experimental results that way).
‚Üê move(r, x1 , y1 , t), move(r, x1 , y2 , t).

(x1 , x2 ‚àà X, x1 < x2 , y1 , y2 ‚àà Y )

‚Üê move(r, x, y1 , t), move(r, x, y2 , t).

(y1 , y2 ‚àà Y, y1 < y2 )

move‚àÉ (r, t) ‚Üê move(r, x, y, t).

‚Üê not move‚àÉ (r, t).

The effect of moving is a deterministic change of location.
robo X (r, x, t + 1) ‚Üê move(r, x, y, t).

robo Y (r, y, t + 1) ‚Üê move(r, x, y, t).

For saturation we guess the position of the object.
obj X (x) ‚à® obj X (x) ‚Üê .

obj X (y) ‚à® obj Y (y) ‚Üê .

We spoil the answer set if the object is at multiple locations.
spoil ‚Üê obj X (x1 ), obj X (x2 ).

(x1 , x2 ‚àà X, x1 < x2 )

spoil ‚Üê obj Y (y1 ), obj Y (y2 ).

(y1 , y2 ‚àà Y, y1 < y2 )

We spoil the answer set if the object is at no location.
objectHasNoXUpTo(1) ‚Üê obj X (1).

objectHasNoXUpTo(x) ‚Üê objectHasNoXUpTo(x ‚àí 1), obj X (x).
spoil ‚Üê objectHasNoXUpTo(xmax ).

objectHasNoYUpTo(1) ‚Üê obj Y (1).

objectHasNoYUpTo(y) ‚Üê objectHasNoYUpTo(y ‚àí 1), obj Y (y).
spoil ‚Üê objectHasNoYUpTo(ymax ).

(x ‚àí 1 ‚àà X)

(xmax = max(X))
(y ‚àí 1 ‚àà Y )

(ymax = max(Y ))

We spoil the answer set if the object is sensed, which is determined by an external atom with the
semantic function f&sense such that f&sense (A, robo X , robo Y , obj X , obj Y , range, spoil ) = 1 iff
308

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

Tspoil ‚àà A or the predicates robo X , robo Y , obj X , obj Y represent in A a state where the robot has
a distance less than range to the object, i.e., the robot can detect the object. Theq
implementation
of this external atom was realized in C++ and consists of verifying range ‚â§
‚àÜ2x + ‚àÜ2y and
bookkeeping code to extract ‚àÜx and ‚àÜy from A.
spoil ‚Üê &sense[robo X , robo Y , obj X , obj Y , range, spoil ]().
We create a unique answer set whenever spoil is true and require that only spoiled answer sets are
returned.
obj X (x) ‚Üê spoil .

obj X (x) ‚Üê spoil .

objectHasNoXUpTo(x) ‚Üê spoil .

objectHasNoYUpTo(y) ‚Üê spoil .

obj Y (x) ‚Üê spoil .

obj Y (x) ‚Üê spoil .

‚Üê not spoil .

Given an instance encoded as above, an answer set to the above program exists iff there exists a
sequence of movements that ensures to detect the object no matter where it is located. Furthermore
the movements required to detect the object, i.e., the conformant plan, is encoded in the answer set
in the extension of the move predicate.

Appendix B. Proofs
Proof of Proposition 1. (‚áí) Let A0 be an answer set of Check (Œ†, A) such that f&g (A0 , p, c) = 1
iff Te&g[p] (c) ‚àà A0 for all external atoms &g[p](c) in Œ†.
Since AÃÇ is a compatible set of Œ†, f&g (A, p, c) = 1 iff Te&g[p] (c) ‚àà AÃÇ for all external atoms

&g[p](c) in Œ†. Thus, f Œ†ÃÇAÃÇ is the same as f Œ†A with replacement atoms in place of external atoms,
and with additional guessing rules for replacement atoms. Since A0 is a model of Check (Œ†, A) it
is also a model of f Œ†ÃÇAÃÇ . Let A00 = {Ta ‚àà A0 | a ‚àà A(Œ†)} ‚à™ {Fa ‚àà A0 | a ‚àà A(Œ†)}. Since
f&g (A0 , p, c) = 1 iff Te&g[p] (c) ‚àà A0 for all external atoms &g[p](c) in Œ† by assumption, A00 is
a model of f Œ†A .
Since A0 is an answer set of Check (Œ†, A), and ‚Üê a ‚àà Check (Œ†, A) for all a ‚àà A(Œ†) with
Ta 6‚àà AÃÇ (and thus Ta 6‚àà A), {Ta ‚àà A00 | a ‚àà A(Œ†)} ‚äÜ A. Finally, due to {‚Üê not smaller } ‚à™
{smaller ‚Üê not a | a ‚àà A(Œ†), Ta ‚àà AÃÇ} ‚àà Check (Œ†, A), there is at least one a ‚àà A(Œ†)
s.t. Ta ‚àà AÃÇ (and thus also Ta ‚àà A), but Fa ‚àà A0 (and thus also Fa ‚àà A00 ). Therefore
{Ta ‚àà A00 | a ‚àà A(Œ†)} ( A is a model of Œ†, and thus A is not an answer set of Œ†.
(‚áê) If A is not an answer set of Œ†, then there is a model A00 of f Œ†A which is smaller in the
positive part, i.e., {Ta ‚àà A00 } ( {Ta ‚àà A}. Let
A0 = Œ∫(Œ†, A00 ) ‚à™ {Ta0 | Ta ‚àà AÃÇ, Fa ‚àà A00 } ‚à™ {Fa0 | Ta ‚àà AÃÇ, Ta ‚àà A00 } ‚à™ {Tsmaller }.
We show that A0 is an answer set of Check (Œ†, A) such that f&g (A0 , p, c) = 1 iff Te&g[p] (c) ‚àà A0
for all external atoms &g[p](c) in Œ†.
Since A has been extracted from a compatible set AÃÇ of Œ†, f Œ†ÃÇAÃÇ is the same as f Œ†A with
replacement atoms in place of external atoms, and with additional guessing rules for replacement
atoms. Since A00 is a model of f Œ†A , and the truth values of all replacement atoms in A0 coincide
309

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

with the oracle functions by definition of Œ∫(Œ†, A00 ), and set exactly one of ea or ne a for each
external atom a in Œ† to true (and thus satisfy the guessing rules for replacement atoms), A0 is a
model of f Œ†ÃÇAÃÇ . Since {Ta ‚àà A00 } ( {Ta ‚àà A} and thus also {Ta ‚àà A0 | a ‚àà A(Œ†)} ( {Ta ‚àà
A | a ‚àà A(Œ†)}, the corresponding constraint ‚Üê a in Check (Œ†, A) is not violated. Moreover, for
each a with Ta ‚àà AÃÇ we have either Ta ‚àà A0 or Ta0 ‚àà A0 , thus the corresponding rule a ‚à® a0 ‚Üê
in Check (Œ†, A) is satisfied. Finally, since Tsmaller ‚àà A0 , the rules {smaller ‚Üê not a | a ‚àà
A(Œ†), Ta ‚àà AÃÇ} are satisfied and the constraint ‚Üê not smaller does not fire. Thus A0 is a model
of Check (Œ†, A).
0
We show now that A0 is also a subset-minimal model of f Check (Œ†, A)A . Observe that
0

f Check (Œ†, A)A = f Œ†ÃÇAÃÇ ‚à™ {a ‚à® a0 ‚Üê| Ta ‚àà AÃÇ}

‚à™ {smaller ‚Üê not a | a ‚àà A(Œ†), Ta ‚àà AÃÇ}.
0

However, if any atom a ‚àà A(f Check (Œ†, A)A ) with Ta ‚àà A0 is changed to false, then the interpretation is not a model anymore because the corresponding rule a ‚à® a0 ‚Üê is violated, as only one
of a and a0 is true in A0 by definition; thus no interpretation with smaller positive part than A0 can
0
be a model of f Check (Œ†, A)A . Hence A0 is an answer set of Check (Œ†, A).
Finally, f&g (A0 , p, c) = 1 iff Te&g[p] (c) ‚àà A0 for all external atoms &g[p](c) in Œ† by definition of Œ∫(Œ†, A00 ).
2
Proof of Proposition 2. The result follows from Proposition 1 and the fact that the programs
Check (Œ†, A) and CheckOptimized (Œ†, A) have the same answer sets. Indeed, by construction the
programs have the same models; consequently, every answer set A0 of Check (Œ†, A) (which is a
0
model of Check (Œ†, A)) is a model of f CheckOptimized (Œ†, A)A . As A0 is a minimal model
0
of f Check (Œ†, A)A , due to the guessing rules a ‚à® a0 ‚Üê, for Ta ‚àà AÃÇ, and ea ‚à® ne a ‚Üê, for all
external atoms a in Check (Œ†, A), we have either {Ta, Fa0 } ‚äÜ A0 or {Fa, Ta0 } ‚äÜ A0 (but not
both) and either {Tea , Fne a } ‚äÜ A0 or {Fea , Tne a } ‚äÜ A0 (but not both); furthermore, due to
the constraints ‚Üê a for every a ‚àà A(Œ†) such that Ta ‚àà
/ AÃÇ, we have Fa ‚àà A0 for each such a.
As the same guessing rules and constraints are also in CheckOptimized (Œ†, A), in every model A00
0
of CheckOptimized (Œ†, A)A such that A00 ‚â§CheckOptimized(Œ†,A) A0 all atoms a, a0 , ea , and ne a
must thus have the same value as in A0 ; consequently, also smaller must have the same value as in
0
A0 . It follows that A0 is a minimal model of f CheckOptimized (Œ†, A)A , i.e., A0 is an answer set
of CheckOptimized (Œ†, A). The argument that every answer set of CheckOptimized (Œ†, A) is an
answer set of Check (Œ†, A) is analogous.
2
Proof of Theorem 3. The argument that proves Corollary 3 by Faber (2005) can be used mutatis
mutandi to prove this statement, with external atoms in place of aggregates.
2
Proof of Proposition 4. We proceed by contraposition and show that if IŒì (U, ŒìA
Œ† , Œ†, A) is not a
A
T
solution to ŒìŒ† , then U cannot be an unfounded set such that A ‚à© U 6= ‚àÖ.
First observe that the nogoods in Hr,A demand Thr to be true for a rule r ‚àà Œ† in a solution
to ŒìA
Œ† if and only if some head atom h ‚àà H(r) of this rule is in U . As the truth values of hr
and all h ‚àà H(r) are defined in IŒì (U, ŒìA
Œ† , Œ†, A) exactly to this criterion, no nogood from Hr,A
A is violated
can be involved in a contradiction. Furthermore, the nogood {Fa | Ta ‚àà A} ‚àà NŒì,Œ†
T
A
A
by IŒì (U, ŒìA
Œ† , Œ†, A) only if A ‚à© U = ‚àÖ; hence, if IŒì (U, ŒìŒ† , Œ†, A) is not a solution to ŒìŒ† and
310

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

A is conservative, then for some rule r ‚àà Œ† the nogood in C
AT ‚à© U 6= ‚àÖ, as OŒì,Œ†
r,A must be
violated. That is, we know the following: Thr ‚àà IŒì (U, ŒìA
,
Œ†,
A)
(and
therefore
H(r)
‚à© U 6= ‚àÖ),
Œ†
A
+
A
Fa ‚àà IŒì (U, ŒìŒ† , Œ†, A) for all a ‚àà Bo (r), ta ‚àà IŒì (U, ŒìŒ† , Œ†, A) for all a ‚àà Be (rÃÇ), and Th ‚àà
IŒì (U, ŒìA
Œ† , Œ†, A) for all h ‚àà H(r) with A |= h. Moreover, we have Cr,A 6= ‚àÖ. We now show that
this implies that none of the conditions (i)‚Äì(iii) of Definition 5 holds for r wrt. U and A, which
means that U is not an unfounded set.
Condition (i) does not hold for r because A |= B(r) (otherwise Cr,A = ‚àÖ).
Condition (ii) does not
hold for r. Suppose to the contrary that it holds. Then there must be
.
some b ‚àà B(r) s.t. A ‚à™ ¬¨.U 6|= b. Because Cr,A 6= ‚àÖ, we know that A |= b. We make a case
distinction on the type of b:
‚Ä¢ If b is a positive default
literal from A(Œ†), then Fb ‚àà IŒì (U, ŒìA
Œ† , Œ†, A) and therefore b 6‚àà U .
.
Consequently A ‚à™ ¬¨.U |= b. Contradiction.
.
‚Ä¢ If b is a negative default literal over A(Œ†) , then A |= b implies A ‚à™ ¬¨.U |= b. Contradiction.
‚Ä¢ If b is a positive or default-negated replacement atom,
then tb ‚àà IŒì (U, ŒìA
Œ† , Œ†, A). But this
.
A
implies, by definition of IŒì (U, ŒìŒ† , Œ†, A), that A ‚à™ ¬¨.U |= b. Contradiction.
Condition (iii) does not hold for r because Th ‚àà IŒì (U, ŒìA
Œ† , Œ†, A) and thus, by definition of
IŒì (U, ŒìA
,
Œ†,
A),
h
‚àà
U
for
all
h
‚àà
H(r)
with
A
|=
h.
Thus
A
6|= a for all a ‚àà H(r) \ U .
2
Œ†

Proof of Theorem 6. Suppose U is not an unfounded set. Then there is an r ‚àà Œ† s.t. H(r)‚à©U 6= ‚àÖ
and none of the conditions (i)‚Äì(iii) in Definition 5 is satisfied. We show now that S cannot be a
solution to ŒìA
Œ† that satisfies conditions (a) and (b), which proves the result.
Because Condition (i) does not hold, there is a nogood of form
{{Thr } ‚à™ {Fa | a ‚àà Bo+ (r), A |= a} ‚à™ {ta | a ‚àà Be (rÃÇ)} ‚à™ {Th | h ‚àà H(r), A |= h}}

in ŒìA
Œ†.
We now show that S contains all signed literals of this nogood, i.e., the nogood is violated by
the assignment S.
Since H(r) ‚à© U 6= ‚àÖ, Thr ‚àà S (otherwise a nogood in HrA is violated). As U is not an
unfounded set, Condition (ii) .in Definition 5 does not hold. Consider any a ‚àà Bo+ (r) s.t. A |= a.
Then a 6‚àà U , otherwise A ‚à™ ¬¨.U 6|= a and we have a contradiction with the assumption that
Condition (ii) is unsatisfied. But then Fa ‚àà S (because S is complete and would imply a ‚àà U
otherwise).
.
Now consider any &g[p](c) ‚àà EA(r). Then A ‚à™ ¬¨.U |= &g[p](c) (as (ii) is violated). If
A 6|= &g[p](c), then Condition (i) would be satisfied, hence A |= &g[p](c). But then Te&g[p] (c) ‚àà
.
S, otherwise A ‚à™ ¬¨.U 6|= &g[p](c)
by precondition (b) of this proposition. Next consider all
.
not &g[p](c) ‚àà Be (r). Then A ‚à™ ¬¨.U 6|= &g[p](c) (as (ii) is violated). If A |= &g[p](c),
then Condition (i) would be satisfied, hence A 6|= &g[p](c). But then Fe&g[p] (c) ‚àà S, otherwise
.
A ‚à™ ¬¨.U |= &g[p](c) by precondition (a) of this proposition. Therefore, we have ta ‚àà S for all
a ‚àà Be (rÃÇ).
Finally, because Condition (iii) in Definition 5 does not hold, h ‚àà U and therefore also Th ‚àà S
for all h ‚àà H(r) with A |= a.
This concludes the proof that S cannot be a solution to ŒìA
Œ† satisfying (a) and (b), if U is not an
unfounded set.
2
Proof of Proposition 7. Let S = IŒì (U, ŒìA
&g[y](x) in Œ† we
Œ† , Œ†, A). If for an external atom
.
have Te&g[y] (x) ‚àà S, then by definition of IŒì (U, ŒìA
,
Œ†,
A)
we
have
A
‚à™
¬¨.U |= &g[y](x)
Œ†
311

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

(satisfying (a)). If for an external atom &g[y](x) in Œ† we have Fe&g[y] (x) ‚àà S, then by definition
.
of IŒì (U, ŒìA
2
Œ† , Œ†, A) we have A ‚à™ ¬¨.U 6|= &g[y](x) (satisfying (b)).
Proof of Proposition 8. We proceed by contraposition and show that if I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) is not a
solution to ‚Ñ¶Œ† with assumptions AA , then U cannot be an unfounded set such that AT ‚à© U 6= ‚àÖ.
First observe that the nogoods in Hr demand Thr to be true for a rule r ‚àà Œ† if and only if some
head atom h ‚àà H(r) of this rule is in U . Moreover, the nogoods in Da for each a ‚àà
A(Œ†) force
.
.
aA‚à™¬¨.U to true if and only if Ta ‚àà A and a ‚àà
/ U , which is equivalent to Ta ‚àà A ‚à™ ¬¨.U ; aA‚àßU
to true if and only if Ta ‚àà A and a ‚àà U ; and aA‚à®U to true if and only if Fa ‚àà A or a ‚àà U .
.
As the truth values of hr for each r ‚àà Œ†, and aA‚à™¬¨.U
and aA‚àßU and aA‚à®U for each a ‚àà A(Œ†)
in I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) are defined exactly to these conditions, a contradiction must involve Cr for
some r ‚àà Œ†. Furthermore, the nogood {Fa | a ‚àà A(Œ†)} is violated by I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) only if
A(Œ†) ‚à© U = ‚àÖ, and thus AT ‚à© U 6= ‚àÖ; hence, if I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) is not a solution to ‚Ñ¶Œ† such that
AT ‚à© U 6= ‚àÖ, since O‚Ñ¶,Œ† is conservative, for some rule r ‚àà Œ† the nogood in Cr must be violated.
That is, we know the following:
(I) Thr ‚àà I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) (and therefore H(r) ‚à© U 6= ‚àÖ),
(II) TaA ‚àà I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) for all a ‚àà B + (rÃÇ) and FaA ‚àà I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) for all a ‚àà B ‚àí (rÃÇ),
(III) FaA‚àßU ‚àà I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) for all a ‚àà Bo+ (r), ta ‚àà I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) for all a ‚àà Be (rÃÇ), and
(IV) ThA‚à®U ‚àà I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) for all h ‚àà H(r).

We now show that this implies that none of the conditions of Definition 5 holds for r wrt. U
and A, which means that U is not an unfounded set (hr is true in I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A), which implies
H(r) ‚à© U 6= ‚àÖx).
Condition (i) does not hold for r because of (II), which by our assumptions AA implies A |=
B(r). Condition (ii) does. not hold for r. Suppose to the contrary that it holds. Then there must be
some b ‚àà B(r) s.t. A ‚à™ ¬¨.U 6|= b. Since Condition (i) is already known to be violated, we can
assume that A |= b. We make a case distinction on the type of b:
.
‚Ä¢ If b is a positive default literal from A(Œ†) , then b ‚àà U (otherwise A ‚à™ ¬¨.U |= b). But then
we have by definition of I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) that TbA‚àßU ‚àà I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A), which contradicts
(III).
.
‚Ä¢ If b is a negative default literal over A(Œ†) , then A |= b implies A ‚à™ ¬¨.U |= b. Contradiction.
‚Ä¢ If b is a positive or default-negated replacement atom,
then tb ‚àà I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A). But this
.
implies, by definition of I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A), that A ‚à™ ¬¨.U |= b. Contradiction.
Condition (iii) does not hold for r because ThA‚à®U ‚àà I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) and thus, by definition
of I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A), h ‚àà U for all h ‚àà H(r) with A |= h. Thus A 6|= a for all a ‚àà H(r) \ U . 2
Proof of Theorem 10. Suppose U is not an unfounded set. Then some r ‚àà Œ† exists such that
H(r) ‚à© U 6= ‚àÖ and none of the conditions (i)‚Äì(iii) in Definition 5 is satisfied. We show that then S,
assuming that AA is satisfied and (a) and (b) hold, cannot be a solution to ‚Ñ¶Œ† .
Due to rule r, ‚Ñ¶Œ† (more specifically, N‚Ñ¶,Œ† ) contains a nogood N of form
N = { {Thr }‚à™
{TaA | a ‚àà B + (rÃÇ)} ‚à™ {FaA | a ‚àà B ‚àí (rÃÇ)} ‚à™
{FaA‚àßU | a ‚àà Bo+ (r)} ‚à™ {ta | a ‚àà Be (rÃÇ)} ‚à™
{ThA‚à®U | h ‚àà H(r)} }.
312

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

We show that S contains all signed literals of N , i.e., N is violated by S. As H(r) ‚à© U 6= ‚àÖ,
Thr ‚àà S (otherwise a nogood in Hr is violated). Furthermore, as U is not an unfounded set,
Condition (i) in Definition 5 does not hold for U wrt. A, and hence A |= B(r). The assumptions
AA thus enforce that TaA ‚àà S for all a ‚àà B + (rÃÇ) and FaA ‚àà S for all a ‚àà B ‚àí (rÃÇ).
Consider next an arbitrary a ‚àà Bo+ (r). As Condition (ii) in Definition 5 does not hold for U
wrt. A, we have A |= a and a 6‚àà U ; the latter implies by definition of U that Fa ‚àà S. From the
nogood {TaA‚àßU , Fa} we conclude FaA‚àßU ‚àà S.
We next
show that for every a ‚àà Be (rÃÇ), it holds that ta ‚àà S. Indeed, let &g[p](c) ‚àà EA(r). We
.
have A ‚à™ ¬¨.U |= &g[p](c) (as Condition (ii) is violated). Furthermore, A |= &g[p](c), as A 6|=
&g[p](c) would imply that Condition (i) is satisfied. Thus from Condition (b)
of the hypothesis, it
.
follows that Te&g[p] (c) ‚àà S. Similarly, let not &g[p](c) ‚àà Be (r). Then A ‚à™ ¬¨.U 6|= &g[p](c) (as
Condition (ii) is violated) and A 6|= &g[p](c) as A |= &g[p](c) would satisfy Condition (i). Thus
from Condition (a) of the hypothesis, it follows that Fe&g[p] (c) ‚àà S. Thus we have ta ‚àà S for all
a ‚àà Be (rÃÇ).
Finally, because Condition (iii) in Definition 5 does not hold for U wrt. A, we have h ‚àà U and
therefore also Th ‚àà S for all h ‚àà H(r) with A |= a. That is, for each h ‚àà H(r), either FhA ‚àà S
or Th ‚àà S. But by the nogoods {FhA‚à®U , FhA }, {FhA‚à®U , Th} ‚àà Dh , in both cases we have
ThA‚à®U ‚àà S.
Hence, N ‚äÜ S holds, i.e., N is violated by S, and thus S is not a solution of ‚Ñ¶Œ† . This completes
the proof of the result.
2
Proof of Proposition 11. Let S = I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A). If for an external atom
&g[y](x) in Œ†
.
we have Te&g[y] (x) ‚àà S, then by definition of I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) we have A ‚à™ ¬¨.U |= &g[y](x)
(satisfying (a)). If for an external atom &g[y](x) in Œ† we have Fe&g[y] (x) ‚àà S, then by definition
.
2
of I‚Ñ¶ (U, ‚Ñ¶A
Œ† , Œ†, A) we have A ‚à™ ¬¨.U 6|= &g[y](x) (satisfying (b)).
Proof of Proposition 12. To show that U \ {a} is an unfounded set wrt. A, consider r ‚àà Œ† such
that H(r) ‚à© (U \ {a}) 6= ‚àÖ. We have to show that one of the conditions (i)‚Äì(iii) of Definition 5
holds for r. By hypothesis, U is an unfounded set of Œ† wrt. A, and H(r) ‚à© (U \ {a}) 6= ‚àÖ implies
H(r)‚à©U 6= ‚àÖ. If Condition (i) holds for U , it also holds wrt. U \{a} because this condition depends
.
only on r and A. Also
if Condition (ii) holds for U , it also holds wrt. U \ {a} because A ‚à™ ¬¨.U
.
is equivalent to A ‚à™ ¬¨.(U \ {a}) since A 6|= a. Finally, if Condition (iii) holds for U , then some
atom b ‚àà H(r) \ U exists such that A |= b. As A 6|= a, it follows a 6= b and hence condition (iii)
holds for U \ {a}. This proves the result.
2
Proof of Proposition 13. Suppose that changing the truth value of b in S turns the solution to
A
a counterexample Sb of ŒìA
Œ† (resp. ‚Ñ¶Œ† ). That is, Sb must violate some nogood N ‚àà ŒìŒ† resp.
(N ‚àà ‚Ñ¶Œ† ) containing b, i.e., either Tb ‚àà N or Fb ‚àà N .
+
‚àí
For the encoding ŒìA
Œ† , the nogood N corresponds to a rule with b ‚àà B (rÃÇ) or b ‚àà B (rÃÇ) and
+
A |= B(r), and N contains also the signed literals (1) Fa for all a ‚àà Bo with A |= a and (2) Ta
for all a ‚àà H(r) with A |= a. By hypothesis, we have either (a) Ta ‚àà S for some a ‚àà Bo+ (r)
with A |= a, or (b) Fa for some a ‚àà H(r) with A |= a. But then the nogood cannot be violated,
because (a) contradicts one of (1) and (b) contradicts one of (2).
For the encoding ‚Ñ¶Œ† , the nogood N corresponds to a rule r with b ‚àà B + (rÃÇ) or b ‚àà B ‚àí (rÃÇ). The
nogood contains also the signed literals (1) TaA for all a ‚àà B + (rÃÇ) and FaA for all a ‚àà B ‚àí (rÃÇ), (2)
FaA‚àßU for all a ‚àà Bo+ , and (3) ThAÃÑ‚à®U for all h ‚àà H(r). Because of (1) and since A is a solution
313

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

to AA , we have A |= B(r). By hypothesis, we have either (a) Ta ‚àà S for some a ‚àà Bo+ (r)
with A |= a, or (b) Fa for some a ‚àà H(r) with A |= a. But then the nogood N cannot be
violated, because (a) contradicts part (2) by definition of AA and aA‚àßU , and (b) contradicts part (3)
by definition of AA and hAÃÑ‚à®U .
2
Proof of Proposition 14.
If TŒì (N, A) = ‚àÖ then the proposition trivially holds. Otherwise
TŒì (N, A) = {C} and we know that Tti ‚àà A for all 1 ‚â§ i ‚â§ n. Suppose C is violated by
A
IŒì (U, ŒìA
Œ† , Œ†, A). Then Fti ‚àà IŒì (U, ŒìŒ† , Œ†, A) and therefore ti 6‚àà U for all 1 ‚â§ i ‚â§ n, and
A
Tfi ‚àà IŒì (U, ŒìŒ† , Œ†, A) for all 1 ‚â§ i ‚â§ m with A |= fi , and œÉe&g[p] (c) ‚àà IŒì (U, ŒìA
Œ† , Œ†, A).
.
.
But then A ‚à™ ¬¨.U |= ti for all 1 ‚â§ i ‚â§ n and A ‚à™ ¬¨.U 6|= fi for all 1 ‚â§. i ‚â§ m. Because the
nogood N is a valid input-output-relationship, this implies œÉÃÑ&g[p](c) ‚àà A ‚à™ ¬¨.U . By definition
A
of IŒì (U, ŒìA
Œ† , Œ†, A), we then have œÉÃÑe&g[p] (c) ‚àà IŒì (U, ŒìŒ† , Œ†, A), which contradicts the assumption
that TŒì (N, A) is violated.
2
Proof of Proposition 15. We know T‚Ñ¶ (N ) = {C}. Suppose I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) violates C. Then
TtiA ‚àà I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) and therefore Tti ‚àà A and Fti ‚àà I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) for all 1 ‚â§ i ‚â§ n;
.
Ffi A‚à™¬¨.U
‚àà I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A) and therefore either Ffi ‚àà A or fi ‚àà U , for all 1 ‚â§ i ‚â§ m; and
œÉe&g[p] (c) ‚àà I(U, ‚Ñ¶Œ† , A).
But
then, by definition of I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A), Tti ‚àà
A and ti 6‚àà U for all 1 ‚â§ i ‚â§ n, hence
.
.
A ‚à™ ¬¨.U |= ti for all 1 ‚â§ i ‚â§ n. Moreover, A ‚à™ ¬¨.U 6|= fi for all 1 ‚â§
i ‚â§ m. Because
.
nogood N is a valid input-output-relationship, this implies œÉÃÑ&g[p](c) ‚àà A ‚à™ ¬¨.U . On the other
hand, by definition of I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A), we have œÉÃÑe&g[p] (c) ‚àà I‚Ñ¶ (U, ‚Ñ¶Œ† , Œ†, A); this contradicts
the assumption that the nogood C is violated.
2
Proof of Proposition 16. Suppose some answer set A0 of Œ† exists which is not a solution to
L1 (U, Œ†, A), i.e., it violates some nogood N = {œÉ0 , œÉ1 , . . . , œÉn } ‚àà L1 (U, Œ†, A). We show that
in this case U is an unfounded set of Œ† wrt. A0 such that U ‚à© A0 6= ‚àÖ; this means that A0 is
not-unfounded-free, which contradicts that A0 is an answer set of Œ†.
Let r ‚àà Œ† be a rule such that H(r) ‚à© U 6= ‚àÖ. We have to show that one of the conditions (i)‚Äì(iii)
of Definition 5 holds.
If Bo+ (r) ‚à© U 6= ‚àÖ, then Condition (ii) holds. Hence we assume in the following that Bo+ (r) ‚à©
U = ‚àÖ, which means that r is an external rule of Œ† wrt. U . But then for some œÉi ‚àà N with
1 ‚â§ i ‚â§ n we have either (1) œÉi = Th for some h ‚àà H(r) with h 6‚àà U and A |= h, or (2) œÉi = Fb
for some b ‚àà Bo+ (r) with A 6|= b. Since A0 violates N by assumption, we have œÉi ‚àà A0 . In Case (1)
the Condition (iii) is satisfied, while in Case (2) then Condition (i) is satisfied.
Moreover, by definition of L1 some a ‚àà U exists such that Ta ‚àà A0 , i.e., A0 intersects with U .
This proves the result.
2
Proof of Proposition 17. Towards a contradiction,
suppose some answer set A0 of Œ† is not a
.
solution to L2 (U, Œ†, A). Let N = {Ta | a ‚àà A ‚à™ ¬¨.U } ‚à™ {œÉ0 , œÉ1 , . . . , œÉn } ‚àà L2 (U, Œ†, A) be a
violated nogood. Because œÉi ‚àà A0 for all 1 ‚â§ i ‚â§ n, we know that A0 falsifies (at least) the bodies
0
of rules in Œ† that are falsified by A; consequently,
f Œ†A ‚äÜ f Œ†A . From A |= Œ† and
the hypothesis
.
.
A ; hence, also A ‚à™ ¬¨.U |= f Œ†A0 .
that U is an unfounded set it follows
that
A
‚à™
¬¨.U
|=
f
Œ†
.
.
Moreover, Ta ‚àà A0 for all. a ‚àà A ‚à™ ¬¨.U , and therefore A0T ‚äá (A ‚à™ ¬¨.U )T . Because œÉ0 ‚àà A0 ,
0
we conclude A0T ) (A ‚à™ ¬¨.U )T , i.e., A0 is not a subset-minimal model of Œ†A . Consequently,
A0 is not an answer set of Œ†, which is a contradiction.
2
314

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

Proof of Lemma 18. If Y = ‚àÖ, then the result holds trivially. Otherwise, let r ‚àà Œ† with
H(r) ‚à© Y 6= ‚àÖ. We show that one of the conditions (i)‚Äì(iii) in Definition 5 holds. Observe that
H(r) ‚à© U 6= ‚àÖ because U ‚äá Y . Since U is an unfounded set of Œ† wrt. A, either
(i) A 6|= b for some b ‚àà B(r); or
.

(ii) A ‚à™ ¬¨.U 6|= b for some b ‚àà B(r); or
(iii) A |= h for some h ‚àà H(r) \ U .
In Case (i), the. condition also holds wrt. Y . In case (ii), let a ‚àà H(r) such that a ‚àà Y , and b ‚àà B(r)
such that A ‚à™ ¬¨.U 6|= b. We make a case distinction: either b is an ordinary literal or an external
one.
.
If b is an ordinary default-negated
atom
not
c,
then
A
‚à™
¬¨.U 6|= b implies Tc ‚àà A and c 6‚àà U ,
.
and therefore also A ‚à™ ¬¨.Y 6|= b. So assume b is an ordinary atom. If b 6‚àà U then A 6|= b and
Case (i) applies, so assume b ‚àà U . Because a ‚àà H(r) and b ‚àà B(r), we have a ‚Üí b and therefore
either a, b ‚àà C or a, b ‚àà Y (because there are no
ordinary edges between C and Y ). But by
.
assumption a ‚àà Y , and therefore b ‚àà Y , hence A ‚à™ ¬¨.Y 6|= b.
If b is an external literal, then there is no q ‚àà U with a ‚Üíe q and q 6‚àà Y . Otherwise, this would
imply q ‚àà C and C would have an incoming e-edge, which contradicts the assumption that C is a
cut .of GR
q ‚àà Y , and therefore the truth value of b under
Œ† . Hence,. for all q ‚àà U with a ‚Üíe q, also
.
A ‚à™ ¬¨.U and A ‚à™ ¬¨.Y is the same. Hence A ‚à™ ¬¨.Y 6|= b.
In Case (iii), also A |= h for some h ‚àà H(r) \ Y because Y ‚äÜ U and therefore H(r) \ Y ‚äá
H(r) \ U .
2
Proof of Lemma 19. If U = ‚àÖ, then the result holds trivially. Otherwise, suppose rÃÇ ‚àà Œ†ÃÇ and
a ‚àà H(rÃÇ) ‚à© U . Observe that rÃÇ cannot be an external atom guessing rule because U contains only
ordinary atoms. We show that one of the conditions in Definition 5 holds for rÃÇ wrt. AÃÇ.
Because rÃÇ is no external atom guessing rule, there is a corresponding rule r ‚àà Œ† containing
external atoms in place of replacement atoms. Because U is an unfounded set of Œ† and H(r) =
H(rÃÇ), either:
(i) A 6|= b for some b ‚àà B(r); or
.

(ii) A ‚à™ ¬¨.U 6|= b for some b ‚àà B(r); or
(iii) A |= h for some h ‚àà H(r) \ U
In Case (i), let b ‚àà B(r) such that A 6|= b and bÃÇ the corresponding literal in B(bÃÇ) (which is the same
if b is ordinary and the corresponding replacement literal if b is external). Then also AÃÇ 6|= bÃÇ because
AÃÇ is compatible.
In Case (ii), we make a case distinction:. either b is ordinary or external.
If b is ordinary, then b ‚àà B(rÃÇ) and AÃÇ ‚à™ ¬¨.U 6|= b holds because A and AÃÇ are equivalent for
ordinary atoms.
If b is an external atom or default-negated external atom, then no atom p(c) ‚àà U is input to
it, i.e., p is not a predicate input parameter of b; otherwise
we had a ‚Üíe p(c), contradicting our
.
assumption that U has
no internal e-edges. But then A ‚à™ ¬¨.U implies A 6|= b because the truth
.
value of b under A ‚à™ ¬¨.U and A is the same. Therefore we can apply Case (i).
315

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

In Case (iii), also AÃÇ |= h for some h ‚àà H(rÃÇ) \ U because H(r) = H(rÃÇ) contains only ordinary
atoms and A is equivalent to AÃÇ for ordinary atoms.
2
Proof of Theorem 20. We define the reachable set R(a) from some atom a as
R(a) = {b | (a, b) ‚àà {‚Üí ‚à™ ‚Üê}‚àó },
i.e., the set of atoms b ‚àà U reachable from a using edges from ‚Üí ‚à™ ‚Üê only but no e-edges.
We first assume that U contains at least one e-edge, i.e., there are x, y ‚àà U such that x ‚Üíe y.
Now we show that there is a u ‚àà U with outgoing e-edge (i.e., u ‚Üíe v for some v ‚àà U ), but
such that R(u) has no incoming e-edges from U (i.e., for all v ‚àà R(u) and b ‚àà U , b 6‚Üíe v holds).
Suppose to the contrary that for all a with outgoing e-edges, the reachable set R(a) has an incoming
e-edge from U . We now construct an e-cycle under ‚Üíd , which contradicts our assumption. Start
with an arbitrary node c0 ‚àà U that has an outgoing e-edge, let p0 be the (possibly empty) path
(under ‚Üí ‚à™ ‚Üê) from c0 to the node d0 ‚àà R(c0 ) such that d0 has an incoming e-edge, i.e., there is
a c1 such that c1 ‚Üíe d0 ; note that c1 6‚àà R(c0 ).4 By assumption, also some node d1 in R(c1 ) has an
incoming e-edge (from some node c2 6‚àà R(c1 )). Let p1 be the path from c1 to d1 , etc. By iteration
we can construct the concatenation of the paths p0 , (d0 , c1 ), p1 , (d1 , c2 ), p2 , . . . , pi , (di , ci+1 ), . . .,
where the pi from ci to di are the paths within reachable sets, and the (di , ci+1 ) are the e-edges
between reachable sets. However, as U is finite some nodes on this path must be equal, i.e., a
subsequence of the constructed sequence represents an e-cycle (in reverse order).
This proves that u is a node with outgoing e-edge but such that R(u) has no incoming e-edges.
We next show that R(u) is a cut of GR
Œ† . Condition (i) is immediately satisfied by definition of u.
Condition (ii) is shown as follows. Let u0 ‚àà R(u) and v 0 ‚àà U \ R(u). We have to show that u0 6‚Üí v 0
and v 0 6‚Üí u0 . Suppose, towards a contradiction, that u0 ‚Üí v 0 . Because u0 ‚àà R(u), there is a path
from u to u0 under ‚Üí ‚à™ ‚Üê. But if u0 ‚Üí v 0 , then there would also be a path from u to v 0 under
‚Üí ‚à™ ‚Üê and v 0 would be in R(u), a contradiction. Analogously, v 0 ‚Üí u0 would also imply that
there is a path from u to v 0 because there is a path from u to u0 , again a contradiction.
Therefore, R(u) ‚äÜ U is a cut of GR
Œ† , and by Lemma 18, it follows that U \R(u) is an unfounded
set. Observe that U \ R(u) contains one e-edge less than U because u has an outgoing e-edge and
that U \ R(u) 6= ‚àÖ; indeed, by assumption some w ‚àà U exists such that u ‚Üíe w, and clearly
w 6‚àà R(u). By iterating this argument, the number of e-edges in the unfounded set can be reduced
to zero in a nonempty core. Eventually Lemma 19 applies, proving that the remaining set is an
unfounded set of Œ†ÃÇ.
2
Proof of Corollary 21. Towards a contradiction, suppose an unfounded set U of Œ† wrt. A exists.
Then U contains no e-cycle because there is no e-cycle under ‚Üíd . By Theorem 20 there is an
unfounded set of Œ†ÃÇ wrt. AÃÇ, which contradicts our assumption that Œ†ÃÇ has no unfounded set wrt. A.
2
Proof of Theorem 22. If U contains no cyclic input atoms, then all cycles under ‚Üíd containing
e-edges in the atom dependency graph of Œ† are broken, i.e., U does not contain an e-cycle under
‚Üíd . Then by Theorem 20 there exists a nonempty unfounded set of Œ†ÃÇ wrt. AÃÇ.
2
4. Whenever x ‚Üíe y for x, y ‚àà U , then there is no path from x to y under ‚Üí ‚à™ ‚Üê, because otherwise we would have
an e-cycle under ‚Üíd .

316

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

Proof of Theorem 23. Let U be a nonempty unfounded set of Œ† wrt. A. Because C is a decomposition of A(Œ†) into strongly connected components, the component dependency graph
hC, {(C1 , C2 ) | C1 , C2 ‚àà C, ‚àÉa1 ‚àà C1 , a2 ‚àà C2 : (a1 , a2 ) ‚àà‚Üí ‚à™ ‚Üíe }i
is acyclic. Following the hierarchical component dependency graph from the nodes without predecessor components downwards, we can find a ‚Äúfirst‚Äù component which has a nonempty intersection
with U , i.e., there exists a component C ‚àà C such that C ‚à© U 6= ‚àÖ but C 0 ‚à© U = ‚àÖ for all transitive
predecessor components C 0 of C.
We show that U ‚à© C is an unfounded set of Œ†C wrt. A. Let r ‚àà Œ†C be a rule such that
H(r) ‚à© (U ‚à© C) 6= ‚àÖ. We have to show that one of the conditions (i)-(iii) of Definition 5 holds for
r wrt. A and U ‚à© C.
Because U is an unfounded set of Œ† wrt. A we know that one of the conditions (i)‚Äì(iii) holds
for r wrt. A and U . In case of Condition (i), then it trivially holds also wrt. A and U ‚à© C because this condition depends only on the assignment A, but not on the unfounded set U ; in case of
Condition (iii), it clearly holds because H(r) \ U 6= ‚àÖ is included in H(r) \ (U ‚à© C).
.
In case of Condition (ii), we have that A ‚à™ ¬¨.U 6|= b for some (ordinary or external) body
literal
.
b ‚àà. B(r). We show next that the truth value of all literals in B(r) is the same under A ‚à™ ¬¨.U and
A ‚à™ ¬¨.(U ‚à© C), which proves that Condition (ii) holds also wrt. A and U ‚à© C.
If b =. not a for some ordinary atom a, then Ta ‚àà A and a 6‚àà U and consequently a 6‚àà U ‚à© C,
hence A. ‚à™ ¬¨.(U ‚à©C) 6|= b. If b is an ordinary atom, then either Fb ‚àà A, which implies immediately
that A ‚à™ ¬¨.(U ‚à© C) 6|= b, or b ‚àà U . But in the latter case b is either in a predecessor component
C 0 of C or in C itself (since h ‚Üí b for all h ‚àà H(r)). But since U ‚à© C 0 = ‚àÖ for
all predecessor
.
components of C, we know b ‚àà C and therefore b ‚àà (U ‚à© C), which implies A ‚à™ ¬¨.(U ‚à© C) 6|= b.
If b is a positive or default-negated external atom, then all input atoms a to b are either in a
predecessor component C 0 of C or in C itself (since h ‚Üíe a for all h ‚àà H(r)). We show .with a
similar argument
as before that
the truth value of each input atom a is the same under A ‚à™ ¬¨.U
.
.
and A ‚à™ ¬¨.(U
‚à©
C):
if
A
‚à™
¬¨.U |=
a, then Ta ‚àà A and a 6‚àà U , hence a 6‚àà (U ‚à© C) and
.
.
therefore
A ‚à™ ¬¨.(U ‚à© C) |= a. If A ‚à™ ¬¨.U 6|= a, then either Fa ‚àà A, which immediately implies
.
A ‚à™ ¬¨.(U ‚à© C) 6|= a, or a ‚àà U . But in the latter case a must be in C because U. ‚à© C 0 = ‚àÖ for all
predecessor components C 0 of C. Therefore a ‚àà (U ‚à© C) and. consequently. A ‚à™ ¬¨.(U ‚à© C) 6|= a.
Because all input atoms a have the same truth value under A ‚à™ ¬¨.U and A ‚à™ ¬¨.(U ‚à© C), the same
holds also for the positive or default-negated external atom b itself.
2
Proof of Proposition 24. If U = ‚àÖ, then the result holds trivially. By definition of Œ†C , we have
H(r) ‚à© C = ‚àÖ for all r ‚àà Œ† \ Œ†C . By hypothesis we have U ‚äÜ C. But then H(r) ‚à© U = ‚àÖ for all
r ‚àà Œ† \ Œ†C and U is an unfounded set of Œ† wrt. A.
2

References
Alviano, M., Calimeri, F., Faber, W., Leone, N., & Perri, S. (2011). Unfounded Sets and WellFounded Semantics of Answer Set Programs with Aggregates. Journal of Artificial Intelligence Research, 42, 487‚Äì527.
Baader, F., & Hollunder, B. (1995). Embedding Defaults into Terminological Knowledge Representation Formalisms. Journal of Automated Reasoning, 14(1), 149‚Äì180.
317

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

Basol, S., Erdem, O., Fink, M., & Ianni, G. (2010). HEX Programs with Action Atoms. In
Hermenegildo, M., & Schaub, T. (Eds.), Technical Communications of the 26th International
Conference on Logic Programming (ICLP‚Äô10), Vol. 7 of Leibniz International Proceedings
in Informatics (LIPIcs), pp. 24‚Äì33, Dagstuhl, Germany. Schloss Dagstuhl‚ÄìLeibniz-Zentrum
fuer Informatik.
Brewka, G., & Eiter, T. (2007). Equilibria in Heterogeneous Nonmonotonic Multi-Context Systems. In Holte, R. C., & Howe, A. (Eds.), 22nd AAAI Conference on Artificial Intelligence
(AAAI‚Äô07), pp. 385‚Äì390. AAAI Press.
Brewka, G., Eiter, T., & TruszczynÃÅski, M. (2011). Answer set programming at a glance. Communications of the ACM, 54(12), 92‚Äì103.
Drescher, C., Gebser, M., Grote, T., Kaufmann, B., KoÃànig, A., Ostrowski, M., & Schaub, T. (2008).
Conflict-driven disjunctive answer set solving. In Brewka, G., & Lang, J. (Eds.), 11th International Conference Principles of Knowledge Representation and Reasoning (KR 2008),
Sydney, Australia, September 16-19, 2008, pp. 422‚Äì432. AAAI Press.
Drescher, C., & Walsh, T. (2012). Answer set solving with lazy nogood generation. In Dovier,
A., & Costa, V. S. (Eds.), Technical Communications of the 28th International Conference on
Logic Programming (ICLP 2012), Vol. 17 of Leibniz International Proceedings in Informatics
(LIPIcs), pp. 188‚Äì200. Schloss Dagstuhl‚ÄìLeibniz-Zentrum fuer Informatik.
Dung, P. M. (1995). On the acceptability of arguments and its fundamental role in nonmonotonic
reasoning, logic programming and n-person games. Artificial Intelligence, 77(2), 321‚Äì357.
Dung, P., Mancarella, P., & Toni, F. (2007). Computing ideal sceptical argumentation. Artificial
Intelligence, 171, 642‚Äì674.
Dunne, P. E. (2009). The computational complexity of ideal semantics. Artificial Intelligence,
173(18), 1559‚Äì1591.
Egly, U., Gaggl, S. A., & Woltran, S. (2010). Answer-set programming encodings for argumentation
frameworks. Argument and Computation, 1(2), 147‚Äì177.
Eiter, T., Fink, M., Ianni, G., Krennwallner, T., & SchuÃàller, P. (2011). Pushing Efficient Evaluation
of HEX Programs by Modular Decomposition. In Delgrande, J., & Faber, W. (Eds.), 11th International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR‚Äô11),
Vol. 6645 of LNAI, pp. 93‚Äì106. Springer.
Eiter, T., Fink, M., Krennwallner, T., & Redl, C. (2012a). Conflict-driven ASP Solving with External
Sources. Theory and Practice of Logic Programming, 12(4-5), 659‚Äì679.
Eiter, T., Fink, M., Krennwallner, T., Redl, C., & SchuÃàller, P. (2012b). Eliminating Unfounded Set
Checking for HEX-Programs. In Fink, M., & Lierler, Y. (Eds.), 5th Workshop on Answer
Set Programming and Other Computing Paradigms (ASPOCP 2012), September 4, 2012,
Budapest, Hungary, pp. 83‚Äì97.
Eiter, T., Fink, M., Krennwallner, T., Redl, C., & SchuÃàller, P. (2012c). Exploiting Unfounded Sets
for HEX-Program Evaluation. In del Cerro, L. F., Herzig, A., & Mengin, J. (Eds.), 13th
European Conference on Logics in Artificial Intelligence (JELIA 2012), September 26-28,
2012, Toulouse, France, Vol. 7519 of LNCS, pp. 160‚Äì175. Springer.
318

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

Eiter, T., Fink, M., SchuÃàller, P., & Weinzierl, A. (2012b). Finding explanations of inconsistency
in nonmonotonic multi-context systems. Tech. rep. INFSYS RR-1843-12-09, INFSYS RR1843-03-08, Inst. fuÃàr Informationssysteme, TU Wien. Preliminary version in Proc. 12th International Conference on Knowledge Representation and Reasoning (KR 2010), pp. 329‚Äì339,
AAAI Press, 2010.
Eiter, T., Ianni, G., Krennwallner, T., & Schindlauer, R. (2008a). Exploiting conjunctive queries
in description logic programs. Annals of Mathematics and Artificial Intelligence, 53(1‚Äì4),
115‚Äì152.
Eiter, T., Ianni, G., Lukasiewicz, T., Schindlauer, R., & Tompits, H. (2008b). Combining answer set
programming with description logics for the semantic web. Artificial Intelligence, 172(1213), 1495‚Äì1539.
Eiter, T., Ianni, G., Schindlauer, R., & Tompits, H. (2005). A Uniform Integration of Higher-Order
Reasoning and External Evaluations in Answer-Set Programming. In Kaelbling, L. P., &
Saffiotti, A. (Eds.), 19th International Joint Conference on Artificial Intelligence (IJCAI‚Äô05),
pp. 90‚Äì96. Professional Book Center.
Eiter, T., Ianni, G., Schindlauer, R., & Tompits, H. (2006a). dlvhex: A Prover for Semantic-Web
Reasoning under the Answer-Set Semantics. In Proceedings of the ICLP‚Äô06 Workshop on
Applications of Logic Programming in the Semantic Web and Semantic Web Services (ALPSWS2006), pp. 33‚Äì39. CEUR WS.
Eiter, T., Ianni, G., Schindlauer, R., & Tompits, H. (2006). Effective Integration of Declarative Rules
with External Evaluations for Semantic-Web Reasoning. In Sure, Y., & Domingue, J. (Eds.),
3rd European Conference on Semantic Web (ESWC‚Äô06), Vol. 4011 of LNCS, pp. 273‚Äì287.
Springer.
Faber, W. (2005). Unfounded sets for disjunctive logic programs with arbitrary aggregates. In
Baral, C., Greco, G., Leone, N., & Terracina, G. (Eds.), 8th International Conference on Logic
Programming and Nonmonotonic Reasoning (LPNMR‚Äô05), Vol. 3662, pp. 40‚Äì52. Springer.
Faber, W., Leone, N., & Pfeifer, G. (2011). Semantics and complexity of recursive aggregates in
answer set programming. Artificial Intelligence, 175(1), 278‚Äì298.
Gebser, M., Ostrowski, M., & Schaub, T. (2009). Constraint answer set solving. In Hill, P., & Warren, D. (Eds.), Proceedings of the Twenty-fifth International Conference on Logic Programming (ICLP‚Äô09), Vol. 5649 of Lecture Notes in Computer Science, pp. 235‚Äì249. SpringerVerlag.
Gebser, M., Kaufmann, B., & Schaub, T. (2012). Conflict-driven answer set solving: From theory
to practice. Artificial Intelligence, 187‚Äì188, 52‚Äì89.
Gebser, M., Kaufmann, B., & Schaub, T. (2013). Advanced conflict-driven disjunctive answer
set solving. In Proceedings of the Twenty-Third International Joint Conference on Artificial
Intelligence, IJCAI‚Äô13, pp. 912‚Äì918. AAAI Press.
Gelfond, M., & Lifschitz, V. (1991). Classical Negation in Logic Programs and Disjunctive
Databases. New Generation Computing, 9(3‚Äì4), 365‚Äì386.
Ghidini, C., & Giunchiglia, F. (2001).
Local models semantics, or contextual reasoning=locality+compatibility. Artificial Intelligence, 127(2), 221‚Äì259.
319

E ITER , F INK , K RENNWALLNER , R EDL , & S CH UÃàLLER

Goldman, R., & Boddy, M. (1996). Expressive Planning and Explicit Knowledge. In Drabble, B.
(Ed.), 3rd International Conference on Artificial Intelligence Planning Systems (AIPS‚Äô96),
pp. 110‚Äì117. AAAI Press.
Hoehndorf, R., Loebe, F., Kelso, J., & Herre, H. (2007). Representing default knowledge in biomedical ontologies: application to the integration of anatomy and phenotype ontologies. BMC
Bioinformatics, 8, 377.
Janhunen, T., NiemelaÃà, I., Seipel, D., Simons, P., & You, J.-H. (2006). Unfolding partiality and
disjunctions in stable model semantics. ACM Trans. Comput. Log., 7(1), 1‚Äì37.
Koch, C., Leone, N., & Pfeifer, G. (2003). Enhancing disjunctive logic programming systems by
SAT checkers. Artificial Intelligence, 151(1‚Äì2), 177‚Äì212.
Lee, J. (2005). A model-theoretic counterpart of loop formulas. In Kaelbling, L. P., & Saffiotti,
A. (Eds.), 19th International Joint Conference on Artificial Intelligence (IJCAI‚Äô05), pp. 503‚Äì
508. Professional Book Center.
Lee, J., & Lifschitz, V. (2003). Loop Formulas for Disjunctive Logic Programs. In Palamidessi, C.
(Ed.), 19th International Conference on Logic Programming (ICLP‚Äô03), Vol. 2916 of LNCS,
pp. 451‚Äì465. Springer.
Leone, N., Pfeifer, G., Faber, W., Eiter, T., Gottlob, G., Perri, S., & Scarcello, F. (2006). The DLV
System for Knowledge Representation and Reasoning. ACM Transactions on Computational
Logic, 7(3), 499‚Äì562.
Leone, N., Rullo, P., & Scarcello, F. (1997). Disjunctive Stable Models: Unfounded Sets, Fixpoint
Semantics, and Computation. Information and Computation, 135(2), 69‚Äì112.
Lierler, Y. (2005). cmodels - SAT-Based Disjunctive Answer Set Solver. In Baral, C., Greco, G.,
Leone, N., & Terracina, G. (Eds.), 8th International Conference on Logic Programming and
Nonmonotonic Reasoning (LPNMR 2005), Vol. 3662 of Lecture Notes in Computer Science,
pp. 447‚Äì451. Springer.
Lifschitz, V., & Turner, H. (1994). Splitting a logic program. In Hentenryck, P. V. (Ed.), 11th
International Conference on Logic Programming (ICLP‚Äô94), pp. 23‚Äì37. MIT Press.
Lin, F., & Zhao, Y. (2004). ASSAT: computing answer sets of a logic program by SAT solvers.
Artificial Intelligence, 157(1‚Äì2), 115‚Äì137.
Marano, M., Obermeier, P., & Polleres, A. (2010). Processing RIF and OWL2RL within DLVHEX.
In Hitzler, P., & Lukasiewicz, T. (Eds.), 4th International Conference on Web Reasoning and
Rule Systems (RR‚Äô10), Vol. 6333 of LNCS, pp. 244‚Äì250. Springer.
Nieuwenborgh, D. V., Cock, M. D., & Vermeir, D. (2007a). Computing fuzzy answer sets using
dlvhex. In Dahl, V., & NiemelaÃà, I. (Eds.), 23rd International Conference on Logic Programming (ICLP‚Äô07), Vol. 4670 of LNCS, pp. 449‚Äì450. Springer.
Nieuwenborgh, D. V., Eiter, T., & Vermeir, D. (2007b). Conditional planning with external functions. In Baral, C., Brewka, G., & Schlipf, J. S. (Eds.), 9th International Conference on Logic
Programming and Nonmonotonic Reasoning (LPNMR‚Äô07), Vol. 4483 of LNCS, pp. 214‚Äì227.
Springer.
Shen, Y.-D. (2011). Well-supported semantics for description logic programs. In Walsh, T. (Ed.),
22nd International Joint Conference on Artificial Intelligence (IJCAI‚Äô11), pp. 1081‚Äì1086.
AAAI Press.
320

E FFICIENT HEX-P ROGRAM E VALUATION BASED ON U NFOUNDED S ETS

Shen, Y.-D., & Wang, K. (2011). Extending logic programs with description logic expressions for
the semantic web. In Aroyo, L., Welty, C., Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy,
N. F., & Blomqvist, E. (Eds.), 10th International Semantic Web Conference (ISWC‚Äô11), Vol.
7031 of LNCS, pp. 633‚Äì648. Springer.
Simons, P., NiemelaÃà, I., & Soininen, T. (2002). Extending and implementing the stable model
semantics. Artificial Intelligence, 138(1-2), 181‚Äì234.
Smith, D. E., & Weld, D. S. (1998). Conformant Graphplan. In Mostow, J., Rich, C., & Buchanan,
B. (Eds.), 15th National Conference on Artificial Intelligence (AAAI‚Äô98), pp. 889‚Äì896. AAAI
Press / The MIT Press.
Turner, H. (2002). Polynomial-length planning spans the polynomial hierarchy. In Flesca, S., Greco,
S., Leone, N., & Ianni, G. (Eds.), European Conference on Logics in Artificial Intelligence
(JELIA‚Äô02), Vol. 2424 of LNCS, pp. 111‚Äì124. Springer.
Van Gelder, A., Ross, K. A., & Schlipf, J. S. (1991). The Well-Founded Semantics for General
Logic Programs. Journal of the ACM, 38(3), 619‚Äì649.
Zakraoui, J., & Zagler, W. L. (2011). A logical approach to web user interface adaptation. In
Holzinger, A., & Simonic, K.-M. (Eds.), 7th Conference of the Workgroup Human-Computer
Interaction and Usability Engineering of the Austrian Computer Society (USAB‚Äô11), Vol.
7058 of LNCS, pp. 645‚Äì656. Springer.
ZirtilogÃålu, H., & Yolum, P. (2008). Ranking semantic information for e-government: complaints
management. In 1st International Workshop on Ontology-supported Business Intelligence
(OBI‚Äô08), No. 5 in OBI‚Äô08, p. 7. ACM.

321

Journal of Artificial Intelligence Research 49 (2014) 79-109

Submitted 07/13; published 01/14

Closure and Consistency In Logic-Associated Argumentation
Phan Minh Dung
Phan Minh Thang

dung.phanminh@gmail.com
thangfm@gmail.com

Computer Science and Information Management Program
Asian Institute of Technology
GPO Box 4, Klong Luang, Pathumthani 12120, Thailand

Abstract
Properties like logical closure and consistency are important properties in any logical
reasoning system. Caminada and Amgoud showed that not every logic-based argument
system satisfies these relevant properties. But under conditions like closure under contraposition or transposition of the monotonic part of the underlying logic, ASPIC-like systems
satisfy these properties. In contrast, the logical closure and consistency properties are not
well-understood for other well-known and widely applied systems like logic programming or
assumption based argumentation. Though conditions like closure under contraposition or
transposition seem intuitive in ASPIC-like systems, they rule out many sensible ASPIC-like
systems that satisfy both properties of closure and consistency.
We present a new condition referred to as the self-contradiction axiom that guarantees
the consistency property in both ASPIC-like and assumption-based systems and is implied
by both properties of closure under contraposition or transposition. We develop a logicassociated abstract argumentation framework, by associating abstract argumentation with
abstract logics to represent the conclusions of arguments. We show that logic-associated
abstract argumentation frameworks capture ASPIC-like systems (without preferences) and
assumption-based argumentation. We present two simple and natural properties of compactness and cohesion in logic-associated abstract argumentation frameworks and show that
they capture the logical closure and consistency properties. We demonstrate that in both
assumption-based argumentation and ASPIC-like systems, cohesion follows naturally from
the self-contradiction axiom. We further give a translation from ASPIC-like systems (without preferences) into equivalent assumption-based systems that keeps the self-contradiction
axiom invariant.

1. Introduction
Properties like logical closure and consistency are important properties in any logical reasoning system. Caminada and Amgoud (2007) showed that not every logic-based argument
system satisfies these relevant properties. But under conditions like closure under contraposition or transposition of the monotonic part of the underlying logic,1 these properties
are fulfilled for ASPIC systems. Prakken (2010) and later Modgil and Prakken (2013) have
developed this idea further for ASPIC+, a rich and complex logic-based argumentation
system.
The following example illustrates that there are many sensible ASPIC or assumptionbased systems that satisfy both properties of closure and consistency but are neither closed
under contraposition nor under transposition.
1. A precise definition of closure under contraposition and transposition is given in section 4.
c
2014
AI Access Foundation. All rights reserved.

Dung & Thang

Example 1 Consider an ASPIC-like system AS = (RS, RD) where
1. RD = RD0 ‚à™ RD1 is a set of defeasible rules and
(a) RD0 consists of two defeasible rules
d1 : p ‚áí ¬¨f

d2 : b ‚áí f

representing defaults ‚Äùbirds fly, penguins don‚Äôt‚Äù and
(b) RD1 consists of a single defeasible rule
th ‚áí bh
representing the default ‚ÄùThais normally have black hair‚Äù
2. RS = RS0 ‚à™ RS1 is a set of strict rules and
(a) RS0 consists of three strict rules
‚Üíp

p‚Üíb

p ‚Üí ¬¨Oj(d2 )

where ¬¨Oj(d2 ) means that rule d2 is not applicable,
(b) RS1 consists of a single strict rule
‚Üí th
It is not difficult to see that AS satisfies both properties of closure and consistency.
Even though AS is neither closed under contraposition nor under transposition, there
is no reason to rule out systems like AS from consideration as long as they capture our
intuition in the concerned applications. In fact, systems like AS often offer a more natural representation of the concerned applications than those closed under contraposition or
transposition. To illuminate this point, imagine another ASPIC-like system AS ‚Ä≤ containing
the strict and defeasible rules in AS but is closed under contraposition.2
Suppose we are interested in the colour of the hair of the concerned Thai individual.
Consider the argument A : ‚Üí th ‚áí bh. Let B1 : ‚Üí p ‚áí ¬¨f and B2 : ‚Üí p ‚Üí b ‚áí f . Let
CN be the consequence operator defined by the strict rules3 in AS ‚Ä≤ . From f ‚àà CN ({f, bh}),
it follows from the closure under contraposition property of CN, ¬¨bh ‚àà CN ({f, ¬¨f }). Therefore there is an argument B with conclusion ¬¨bh that contains B1 , B2 as subarguments. B
hence attacks A. As B is attacked (by undercut) by argument C : ‚Üí p ‚Üí ¬¨Oj(d2 ) at B2
and there is no attack against C, A is accepted in the grounded extension. But the set {A}
itself is not admissible. In other words, to draw a conclusion about the hair colour of Thais,
the system needs to resolve a completely unrelated controversy about the flying capabilities
of penguins and birds.
2. For example, by adding to AS ‚Äùabsurdity rules‚Äù representing the proposition ‚Äùinconsistency implies
every thing‚Äù, of the form a, ¬¨a ‚Üí l where a is an atom appearing in RS ‚à™ RD and l is a literal over this
set of atoms. It is not difficult to see that AS ‚Ä≤ is closed under contraposition (see appendix of section 1).
3. A precise definition is given in definition 1.

80

Closure and Consistency In Logic-Associated Argumentation

In general, the condition of closure under contraposition creates an attack against any
defeasible argument from any inconsistency in ASPIC systems. In other words, if the systems happen to contain knowledge about several causally independent domains like about
the flying penguins and the hair colour of Thais, closure under contraposition will interlink
them and making it necessary to resolve all possible inconsistencies in all parts of the systems before being able to answer any query, independent of whether these inconsistencies
are causally related to the query or not.4
In contrast, in argument systems like AS, there is no attack against A and {A} is
admissible. Controversy about the flying capabilities of penguins do not have any effect on
the acceptance of A.
Though closure under transposition avoids the problem of conflict propagation, it is not
adopted in many well-known and practical systems as an indiscriminate application of it
could lead to counter-intuitive result in many cases. For example, consider a simplified
version of example ‚Äùbirds fly penguins don‚Äôt‚Äù in aspic+ whose set of strict rules is the
closure under transposition of the following strict rules:
app, b ‚Üí f

p ‚Üí ¬¨f

p‚Üíb

p ‚Üí ¬¨app

where app is an ordinary premise with app = ¬¨app.
Therefore the rule ‚Äùapp ‚Üí ¬¨p‚Äù is included in the set of strict rules.
Suppose the knowledge base only consists of the ordinary premise app stating that the
default ‚Äùbirds fly‚Äù is applicable.
Without any further information, the only two arguments are: A ‚â° app and B ‚â°
A ‚Üí ¬¨p.
Accepting ordinary premise app simply says that there is no information at all why the
default ‚Äùbirds fly‚Äù should not be applied. It does not say anything about penguin. But closure
under transposition implies that there is definitely no penguin around (a rather slippery way
to make a conclusion).
In the context of logic programming, the rule ‚Äùapp ‚Üí ¬¨p‚Äù is also rather unnatural as app
is viewed as an assumption and ¬¨ is the explicit negation operator whose intuitive reading
is that an explicit negation of an assertion should be based on some ‚Äùhard evidence‚Äù, not
on another assumption (Gelfond & Lifschitz, 1990). 
In contrast to ASPIC systems, not much research has been done to study the logical
closure and consistency of the semantics of other well-known and widely applied systems
like logic programming or assumption based argumentation (Gelfond & Lifschitz, 1988;
Lifschitz, 1999; Bondarenko, Dung, Kowalski, & Toni, 1997). Properties like closure under
transposition or contraposition are not embraced in logic programming or assumption based
argumentation. Prakken (2012) has observed that assumption-based argumentation yields
unintuitive argument with unwanted conclusion if the inference rules satisfy the transposition property. It is hence natural to ask
whether there are conditions that are implied by both conditions of closure under contraposition and transposition but still guaranteeing the important properties of logical closure
and consistency, and
4. Pollock (1995) and Wu (2012) have also pointed out that classical propositional proof systems propagate
conflicts throughout the knowledge bases to unrelated parts.

81

Dung & Thang

whether the logical closure and consistency properties could be stated and studied in a
general framework generalizing both assumption-based and ASPIC-like systems and possibly
also other logic-based argument systems ?
We will present in this paper a new condition referred to as the self-contradiction axiom5
that guarantees the consistency of complete extension semantics and is implied by both
conditions of closure under contraposition or transposition while at the same time, also
includes systems that could avoid the problem of conflict-propagation. In example 1 both
AS, AS ‚Ä≤ satisfy the self-contradiction axiom.
It turns out that the consistency and closure properties in logic-based argument systems could be studied within a general logic-associated abstract argumentation framework,
obtained by associating abstract argumentation with abstract logics to represent the conclusions of arguments. We demonstrate that logic-associated abstract argumentation frameworks capture ASPIC-like systems (without preferences) and assumption-based argumentation. We present two simple and natural properties of compactness and cohesion in
logic-associated abstract argumentation frameworks and show that they capture the logical
closure and consistency of the complete extension semantics. We demonstrate that in both
assumption-based argumentation and ASPIC-like systems cohesion follows naturally from
the self-contradiction axiom.
We further give a translation from ASPIC-like systems (without preferences) into equivalent assumption-based systems that keeps the self-contradiction property invariant.
The paper is structured as follows. In section 2 we recall abstract argumentation and
Tarski abstract logics. We then introduce in section 3 a framework in which abstract argumentation is associated to abstract logics where sentences in abstract logics represent
argument conclusions. We present in this section two simple and natural conditions of compactness and cohesion and show that they ensure the satisfaction of the properties of logical
closure and consistency of complete extensions. In sections 4, 5 we show how compactness
and cohesions could be captured naturally in ASPIC-like systems or assumption-based argumentation. We introduce in section 4 the fundamental axiom of self-contradiction and show
the connections between it and the properties of closure under contraposition and transposition. We provide in section 6 a transformation from ASPIC-like systems into equivalent
assumption-based framework. In section 7, we discuss the more recent systems based on
Tarski‚Äôs abstract logics. We then conclude.6

2. Preliminaries
An abstract argumentation framework (Dung, 1995) is defined simply by a pair (AR, att)
of a set of arguments AR and att ‚äÜ AR √ó AR where (A, B) ‚àà att represents an attack
from argument A against argument B. A set of argument S attacks an argument A if
some argument in S attacks A. S attacks another set S ‚Ä≤ if S attacks an argument in
S ‚Ä≤ . S is conflict-free if it does not attack itself. S is conflicting if it attacks itself. An
argument A is acceptable wrt set of arguments S if S attacks each attack against A. S is
admissible if S is conflict-free and it counter-attacks each attack against it. The semantics
5. The intuitive reading of ‚Äùself-contradiction‚Äù is that if ‚ÄùX causes contradiction‚Äù then ‚ÄùX contradicts
itself‚Äù.
6. A very preliminary extended abstract of this paper is published by Dung and Thang (2011).

82

Closure and Consistency In Logic-Associated Argumentation

of abstract argumentation is determined by the acceptability of arguments and various
associated notions of extensions. A complete extension is an admissible set containing every
argument acceptable wrt it. Complete extensions could also be viewed as conflict-free fixed
points of the characteristic function F : 2AR ‚Üí 2AR defined by F (S) = set of acceptable
arguments wrt S. Preferred extensions are maximal conflict-free fixed points of F while the
least fixed point of F is called the grounded extension. While there could be many preferred
extensions, there exists an unique grounded extension. A stable extension is a conflict-free
set of arguments that attacks every argument not belonging to it.
Amgoud and Besnard (2009) have proposed the use of Tarski‚Äôs abstract logic in argumentation that is characterized simply by a consequence operator.
Given a language L of well-formed formulas, a Tarski abstract logic (Amgoud & Besnard,
2009) is defined by a consequence operator CN: 2L ‚Üí 2L such that following axioms are
satisfied:
1. (Expansion)

X ‚äÜ CN (X)

2. (Idempotence)
3. (Finiteness)

CN (CN (X)) = CN (X)
S
CN (X) = {CN (Y ) | Y ‚äÜ X and Y is finite }

4. (Absurdity)

CN ({x}) = L for some x ‚àà L

5. (Coherence)

CN (‚àÖ) 6= L

We introduce below the consequence operator over a set of strict or inference rules.
A strict rule7 is of the form
Œ±1 , . . . , Œ±n ‚Üí Œ±
where Œ±1 , . . . , Œ±n , Œ± are from L.
Definition 1 Let RS be a set of strict rules. Define the consequence operator CNRS :
2L ‚Üí 2L as follows: For a set X ‚äÜ L, CNRS (X) is the smallest set such that
1. X ‚äÜ CNRS (X), and
2. for each rule œÉ1 , . . . œÉn ‚Üí œÉ in RS, if {œÉ1 , . . . œÉn } ‚äÜ CNRS (X) then œÉ ‚àà CNRS (X).


3. Associating Abstract Argumentation With Abstract Logics
Intuitively, an argument is a ‚Äùproof‚Äù of some conclusion. In many cases, such proofs are constructed following some proof theory of some formal logics. Such logics could be nonmonotonic. The notions of closure and consistency are then defined according to the monotonic
parts of the underlying logics.
Many logics underlying argumentation systems like assumption-based argumentation
or ASPIC-systems do not always impose the absurdity axiom. This motivates our slight
generalization of Tarski abstract logics in the following definition.
7. often also referred to as inference rules in assumption-based argumentation.

83

Dung & Thang

Definition 2 Given a language L, an abstract logic is defined as a pair (CN, CON T RA)
where CN: 2L ‚Üí 2L represents a consequence operator and CON T RA ‚äÜ 2L is a collection
of contradictory sets such that following axioms hold:
1. CN satisfies the expansion, idempotence and finiteness axioms.
2. (Weak Absurdity) If S ‚àà CON T RA then each superset of S also belongs to CON T RA.8
3. (Weak Coherence) CN (‚àÖ) 6‚àà CON T RA. 
Given an abstract logic (CN, CON T RA), X is closed iff X = CN (X). A set X ‚àà
CON T RA is said to be contradictory.
A set X ‚äÜ L is said to be inconsistent iff its closure CN (X) is contradictory.9 X is said
to be consistent iff it is not inconsistent. X is minimal inconsistent iff X is inconsistent
and each proper subset of X is consistent.10
Definition 3 We say that an abstract logic satisfies the
strong absurdity axiom if CON T RA 6= ‚àÖ and for each X ‚àà CON T RA, CN (X) = L.

It follows immediately that an abstract logic is a Tarski abstract logic if it satisfies the
strong absurdity axiom and there is x ‚àà L such that {x} is inconsistent.11
Example 2 Let RS0 = {‚Üí wr; ‚Üí go; b ‚Üí ¬¨hw; m ‚Üí hw} be a set of strict rules,12
and L be a language consisting of literals whose atoms occur in the rules in RS0 . Define
AL0 = (CN0 , CON T RA) as follows:
‚Ä¢ X ‚àà CON T RA iff X contains a pair of literals {l, ¬¨l}.
‚Ä¢ CN0 is the consequence operator defined by RS0 .
For illustration, CN0 (‚àÖ) = {wr, go}, CN0 ({m}) = {wr, go, m, hw} and CN0 ({m, b}) =
{wr, go, m, hw, b, ¬¨hw}. Hence the set {m, b} is inconsistent but not contradictory.
It is not difficult to see that AL0 is an abstract logic. From CN0 ({hw, ¬¨hw}) =
{hw, ¬¨hw}, it follows that AL0 does not satisfy the strong absurdity axiom. 
8. Note that CON T RA could be empty like in the case of definite logic programs where L consists only of
positive literals. Nonetheless, if CON T RA 6= ‚àÖ then L ‚àà CON T RA.
9. In other words, a set of sentences X is inconsistent if a contradiction could be derived from it though
contradiction may not be present directly in X (i.e. an inconsistent set may not be contradictory).
10. Y is a proper subset of X if Y is a subset of X and Y 6= X.
11. To be a Tarskian one, an abstract logic needs to satisfy absurdity and coherence. Because {x} is
inconsistent, CN ({x}) is contradictory. Since strong absurdity is satisfied, CN (CN ({x})) = L. Because
of idempotence, CN (CN ({x})) = CN ({x}). Hence CN ({x}) = L. So coherence holds.
From the weak coherence axiom of abstract logic, CN (‚àÖ) 6‚àà CON T RA. Due to the strong absurdity,
CON T RA 6= ‚àÖ. Due to the weak absurdity, and the fact that CON T RA 6= ‚àÖ, it follows that L ‚àà
CON T RA. Hence CN (‚àÖ) 6= L.
12. The rules are taken from an example by Caminada and Amgoud (2007) where wr = ‚ÄùJohn wears
something that looks like a a wedding ring‚Äù, m = ‚ÄùJohn is married‚Äù, hw = ‚ÄùJohn has a wife‚Äù, go =
‚ÄùJohn often goes out until late‚Äù, b = ‚ÄùJohn is a bachelor‚Äù.

84

Closure and Consistency In Logic-Associated Argumentation

Inspired by Amgoud and Besnard‚Äôs idea (2009), we use abstract logics to represent the
conclusions of arguments but in difference to them, we do not specify in detail the structure
of individual arguments.
Definition 4 A logic-associated argumentation framework over a language L is a
quadruple (AF, ‚äë, AL, Cnl) where
1. AF = (AR, att) is an abstract argumentation framework, and
2. AL = (CN, CON T RA) is an abstract logic over L, and
3. Cnl : AR ‚Üí L assigns to each argument A, its conclusion Cnl(A) in L, and
4. ‚äë is a partial order13 over AR where A ‚äë B means that A is a subargument of B such
that for all arguments C ‚àà AR, if C attacks A then C attacks B. 
Remark 1
‚Ä¢ For a set S of arguments, Cnl(S) denotes the set of the conclusions of
the arguments in S.
‚Ä¢ The set of all subarguments of A is denoted by Sub(A). For a set of arguments S,
Sub(S) contains all subarguments of arguments in S.

We next give an example of a logic-associated argumentation framework.
Example 3 Let LAF0 = (AF0 , ‚äë0 , AL0 , Cnl) where AL0 is defined in example 2 and
AF0 = (AR0 , att0 ) such that
1. The arguments in AR0 are constructed with rules from the set of strict rules RS0 in
example 2, and a set of defeasible rules RD = {wr ‚áí m; go ‚áí b}. There are 6
arguments14 :
A1 : ‚Üí wr, A3 : ‚Üí wr ‚áí m, A5 : ‚Üí wr ‚áí m ‚Üí hw.
A2 : ‚Üí go, A4 : ‚Üí go ‚áí b,

A6 : ‚Üí go ‚áí b ‚Üí ¬¨hw.

Attack relation: A5 attacks A6 and vice versa. There are no other attacks. Let
att0 = {(A5 , A6 ), (A6 , A5 )}.
2. The subargument relation ‚äë0 is the reflexive and transitive closure of A1 ‚äë A3 ‚äë A5
and A2 ‚äë A4 ‚äë A6 . 
Definition 5 Let LAF = (AF, ‚äë, AL, Cnl), AL = (CN, CON T RA), be a logic-associated
argumentation framework.
1. LAF is said to satisfy the closure-property if for each complete extension E of AF,
Cnl(E) is closed wrt AL.
13. A partial order is a reflexive, asymmetric and transitive relation.
14. For a precise definition see definition 12.

85

Dung & Thang

2. LAF is said to satisfy the consistency-property if for each complete extension E
of AF, Cnl(E) is consistent wrt AL. 
Example 4 (Continuation of example 3)
The grounded extension of AF0 is GE = {A1 , A2 , A3 , A4 }. There are two preferred
extensions E1 = {A1 , A2 , A3 , A4 , A5 } and E2 = {A1 , A2 , A3 , A4 , A6 }.
Cnl(GE) = {wr, go, m, b} and CN0 (Cnl(GE)) = Cnl(GE) ‚à™ {hw, ¬¨hw}. Hence the
set of conclusions of arguments in the grounded extension are neither closed nor consistent wrt the abstract logic AL0 . Hence LAF0 satisfies neither the closure-property nor the
consistency-property.
It is also easy to see that the sets of conclusions of the arguments of the two preferred
extensions are neither closed nor consistent either. 
Example 5 (Continuation of example 4)
Let RS1 = RS0 ‚à™ {¬¨hw ‚Üí ¬¨m, hw ‚Üí ¬¨b}. The consequence operator wrt RS1 is
denoted by CN1 . Let AL1 = (CN1 , CON T RA) and AF1 = (AR1 , att1 ) where
‚Ä¢ AR1 = AR0 ‚à™ {A7 , A8 } with A7 : A5 ‚Üí ¬¨b and A8 : A6 ‚Üí ¬¨m,15
‚Ä¢ att1 = {(A7 , A4 ), (A7 , A6 ), (A7 , A8 ), (A8 , A3 ), (A8 , A5 ), (A8 , A7 )},
‚Ä¢ ‚äë1 is the reflexive and transitive closure of ‚äë0 ‚à™{A5 ‚äë A7 , A6 ‚äë A8 }.
The grounded extension of AF1 is GE ‚Ä≤ = {A1 , A2 }. Two preferred extensions of AF1
are E1‚Ä≤ = {A1 , A2 , A3 , A5 , A7 } and E2‚Ä≤ = {A1 , A2 , A4 , A6 , A8 }.
It is not difficult to see that the sets Cnl(GE ‚Ä≤ ) = {wr, go}, Cnl(E1‚Ä≤ ) = {wr, go, m, hw, ¬¨b},
Cnl(E2‚Ä≤ ) = {wr, go, b, ¬¨hw, ¬¨m} are closed and consistent.
Let LAF1 = (AF1 , ‚äë1 , AL1 , Cnl). Both closure and consistency properties are satisfied
in LAF1 . 
From now on until the end of this section, we assume an arbitrary but fixed logicassociated framework LAF = (AF, ‚äë, AL, Cnl).
It turns out that both the closure and consistency properties are based on an intuitive
idea of a base of an argument.
Definition 6 Let A be an argument and BA be a finite set of subarguments of A. BA is
said to be a base of A if following conditions are satisfied:
1. Cnl(A) ‚àà CN (Cnl(BA))
2. For each argument C, C attacks A iff C attacks BA. 
It is easy to see that for each argument A, {A} is a base of A.
In example 3, though Cnl(A5 ) ‚àà CN0 (Cnl(A3 )), {A3 } is not a base of A5 since A6
attacks A5 but A6 does not attack A3 .
Note that the empty set is a base of both arguments A1 and A2 .
In contrast, in example 5, {A3 } is a base of A5 and A7 and {A4 } is a base of A6 and A8 .
15. i.e. A7 ‚â° ‚Üí wr ‚áí m ‚Üí hw ‚Üí ¬¨b, and A8 ‚â° ‚Üí go ‚áí b ‚Üí ¬¨hw ‚Üí ¬¨m.

86

Closure and Consistency In Logic-Associated Argumentation

Definition 7
1. An argument A is said to be generated by a set of arguments S if
there is a base BA of A such that BA ‚äÜ Sub(S).
2. The set of all arguments generated by S is denoted by GN(S). 
It follows immediately
Lemma 1 Let S be a set of arguments. The following assertions hold:
1. For each argument A, A is generated by {A}.
2. Sub(S) ‚äÜ GN (S).
3. Cnl(GN (S)) ‚äÜ CN (Cnl(Sub(S))).
4. CN (Cnl(GN (S))) = CN (Cnl(Sub(S))).
5. For each argument C, C attacks GN (S) iff C attacks S.
Proof See appendix of section 3. 
Theorem 1 Let E be a complete extension. Then GN (E) = E
Proof See appendix of section 3. 
Theorem 1 motivates the following definitions 8, 9.
Definition 8 We say that a logic-associated argumentation framework LAF is compact
if for each set of arguments S, Cnl(GN (S)) is closed. 
The argumentation framework in example 3 is not compact since for S = {A3 }, GN (S) =
{A1 , A2 , A3 } and Cnl(GN (S)) = {wr, go, m} is not closed since CN ({wr, go, m}) = {wr, go, m, hw}.
In contrast, the argumentation framework in example 5 is compact. For example,
GN (S) = {A1 , A2 , A3 , A5 , A7 } (wrt LAF1 ) and Cnl(GN (S)) = {wr, go, m, hw, ¬¨b} is both
closed and consistent.
Theorem 2 Each compact logic-associated argumentation framework satisfies the closureproperty.
Proof. Let E be a complete extension. From the compactness, it follows that Cnl(GN (E))
is closed. From theorem 1, Cnl(E) is closed. 
From lemma 1, it follows that LAF is compact iff Cnl(GN (S)) = CN (Cnl(Sub(S))) iff
CN (Cnl(Sub(S))) ‚äÜ Cnl(GN (S)). We have proved
Lemma 2 LAF is compact iff for each set S of arguments, CN (Cnl(Sub(S))) ‚äÜ Cnl(GN (S))
Notation 1 Abusing the notations for simplicity, we often refer to a set of arguments
S as inconsistent (resp. consistent) if Cnl(Sub(S)) is inconsistent (resp. consistent).
87

Dung & Thang

Intuitively, inconsistency among a set of arguments indicates a possible conflict among
its generated arguments. This insight motivates the following definitions.
Definition 9 A logic-associated argumentation framework LAF is said to be cohesive if
for each inconsistent set of argument S, GN (S) is conflicting. 
For illustration, consider the logic-associated argumentation framework in example 3.
Let S = {A3 , A4 }. It is clear that S is inconsistent. It is not difficult to see that each
base of each argument A ‚àà {A5 , A6 } contains argument A itself. Therefore GN (S) =
{A1 , A2 , A3 , A4 }. GN (S) is thus conflict-free. The framework LAF0 is hence not cohesive.
Note that the argumentation framework LAF1 in example 5 is both compact and cohesive.
Theorem 3 Let LAF be a cohesive logic-associated argumentation framework. Then LAF
satisfies the consistency-property.
Proof Let E be a complete extension. Suppose Cnl(E) is inconsistent. From the cohesion of
LAF, it follows that GN (E) is conflicting. From theorem 1, E is conflicting. Contradiction.
Hence Cnl(E) is consistent. 
It follows immediately from theorems 2 and 3:
Corollary 1 Let LAF be a compact and cohesive logic-associated argumentation framework. Then LAF satisfies both the closure- and consistency-properties. 
In the next sections, we show that ASPIC-like systems (without preferences) and assumptionbased argumentation are instances of logic-associated argumentation frameworks. We will
also introduce the axiom of self-contradiction to guarantee the cohesion (and consistency)
of these systems.

4. Argumentation with Strict and Defeasible Rules
We assume a language L of literals where a literal is an atom a or the explicit negation ¬¨a
of atom a. A set of literals is said to be contradictory if it contains a pair a, ¬¨a.
It is important to note that we identify ¬¨¬¨a with a. For X ‚äÜ L, denote ¬¨X = {¬¨l | l ‚àà
X}.
A defeasible rule is of the form
Œ±1 , . . . , Œ±n ‚áí Œ±
where Œ±1 , . . . , Œ±n , Œ± are from L.
Definition 10 A rule-based argumentation system is a pair AS = (RS, RD) of a set RS
of strict rules and a set RD of defeasible rules such that CNRS (‚àÖ) is not contradictory. 
The following definition 11 identifies the abstract logic underlying a rule-based argumentation system AS = (RS, RD).
88

Closure and Consistency In Logic-Associated Argumentation

Definition 11 Let AS = (RS, RD) be a rule-based argumentation system. Define
ALAS = (CNAS , CON T RAAS )
where CON T RAAS is the collection of all contradictory sets and CNAS = CNRS . 
It follows immediately
Lemma 3 ALAS is an abstract logic. 
We recall below the arguments and attack relations of rule-based argumentation systems
introduced by Caminada and Amgoud (2007), Prakken (2010), Pollock (1987), and Modgil
and Prakken (2013).
Definition 12

1. Rules of the form ‚Üí / ‚áí Œ± , are arguments with conclusion Œ±.

2. Let r be a strict/defeasible rule of the form Œ±1 , . . . , Œ±n ‚Üí / ‚áí Œ±, n ‚â• 0. Further suppose that A1 , . . . , An , n ‚â• 0, are arguments with conclusions Œ±1 , . . . , Œ±n respectively.
Then A1 , . . . , An ‚Üí / ‚áí Œ± is an argument with conclusion Œ± and last rule r.
3. Every argument is constructed by applying finitely many times the above two steps. 
We next introduce key notations.
Notation 2
1. A strict argument is an argument containing no defeasible rule. Nonstrict arguments are called defeasible arguments.
2. A basic defeasible argument is an argument whose last rule is a defeasible one,
i.e. of the form A1 , . . . , An ‚áí Œ±.
For a basic defeasible argument B, the last rule of B is denoted by Lr(B).
3. B is a subargument of an argument A of the form A1 , . . . , An ‚Üí / ‚áí Œ±, denoted by
B ‚äë A, if B = A or B is a subargument of some Ai . 
Remark 2 The conclusion of an argument A is denoted by Cnl(A).
Remark 3 Arguments of the form A1 , . . . , An ‚Üí / ‚áí Œ± are also often viewed as proof
trees with the root labelled by Œ± and the children of the root are the roots of subtrees
A1 , . . . , An . Note that if n = 0, the proof tree consists of just the root.
Illustrations for argumentation systems based on strict and defeasible rules are given in
examples 3, 5.
The following notion of attack is adopted from articles of Caminada and Amgoud (2007),
Prakken (2010), Pollock (1987), and Modgil and Prakken (2013).
Definition 13 An argument A attacks an argument B (on B‚Äô) if B‚Äô is basic defeasible
subargument of B and one of the following conditions is satisfied:
89

Dung & Thang

1. (Undercutting) Cnl(A) = ¬¨Oj(Lr(B ‚Ä≤ )) where for a defeasible rule r, Oj(r) is an atom
denoting that rule r is applicable.
2. (Rebutting) Cnl(A) = ¬¨Cnl(B ‚Ä≤ ). 
Remark 4 For simplicity, we identify a rule-based argumentation system AS with the logicassociated argumentation framework (AFAS , ‚äë, ALAS , Cnl) where AFAS is the argumentation framework obtained from AS according to definitions 12,13.
Theorem 4 Rule-based argumentation systems are compact.
Proof See appendix of section 4. 
The following lemma reveals an simple but important relation between an arguments
and its basic defeasible subarguments.
Lemma 4 Let A be an argument and BD be the set of basic defeasible subarguments of A.
Then Cnl(A) ‚àà CNAS (Cnl(BD)).
Proof See appendix of section 4. 
We introduce now a fundamental condition underlying the cohesion of rule-based argumentation.
Definition 14 The abstract logic ALAS is said to satisfy the
self-contradiction axiom if for each minimal inconsistent set X ‚äÜ L : ¬¨X ‚äÜ CNAS (X).16

The example below illustrates the intuition of the self-contradiction axiom using again
the famous ‚Äùbirds fly penguins don‚Äôt‚Äù-example.
Example 6 Let AS = (RS, RD) where RD consists of two defeasible rules:
d1 : p ‚áí ¬¨f

d2 : b ‚áí f

and RS consists of three strict rules
r0 : ‚Üí p

r1 : p ‚Üí b

r2 : p ‚Üí ¬¨ Oj(d2 )

where Oj(d2 ) is an atom stating that rule d2 is applicable.
It is obvious that the set of strict rules RS is not closed under transposition. It is also
straightforward to see that CNAS ({f, ¬¨f }) = {f, ¬¨f } =
6 L. Hence the underlying abstract
logic ALAS satisfies neither the strong absurdity axiom nor the closure under transposition
property.17
16. The intuitive reading of ‚Äùself-contradiction‚Äù is that if ‚ÄùX causes contradiction‚Äù then ‚ÄùX contradicts
itself‚Äù.
17. See section 4.1 for precise definitions of closure under transposition or contraposition and their relationships with strong absurdity and self-contradiction

90

Closure and Consistency In Logic-Associated Argumentation

It is not difficult to see that ALAS satisfies the self-contradiction axiom.18
It is not difficult to see that AS satisfies both properties of closure and consistency. 
The following theorem shows that self-contradiction is sufficient for cohesion.
Theorem 5 Suppose ALAS satisfies the self-contradiction axiom. Then AS is cohesive.
Proof See appendix of section 4. 
It follows immediately from corollary 1 that
Corollary 2 Suppose ALAS satisfies the self-contradiction axiom. Then AS satisfies both
closure- and consistency-properties.
We next relate theorem 5 and corollary 2 to the the results by Caminada and Amgoud
(2007), Prakken (2010), and Modgil and Prakken (2013).
4.1 Sufficient Conditions for Self-Contradiction in Abstract Logics ALAS
For simplicity, if there are no possibilities for misunderstanding, we often write respectively
CN , CON T RA and AL for CNAS , CON T RAAS and ALAS in this section.
We first recall the definitions of closure under contraposition and transposition from
articles of Caminada and Amgoud (2007), Prakken (2010), and Modgil and Prakken (2013).
Definition 15

1. AL is said to be

closed under contraposition if for each set X ‚äÜ L, for each Œ± ‚àà X if œÉ ‚àà CN (X)
then ¬¨Œ± ‚àà CN (X \ {Œ±} ‚à™ {¬¨œÉ}).
2. A set of strict rules RS is said to be
closed under transposition if for each rule Œ±1 , . . . , Œ±n ‚Üí œÉ in RS, all the rules
of the form Œ±1 , . . . , Œ±i‚àí1 , ¬¨œÉ, Œ±i+1 , Œ±n ‚Üí ¬¨Œ±i also belong to RS.
The relations between the closure under contraposition and the axioms of self-contradiction
and strong absurdity are illuminated in the following lemma.
Lemma 5
1. If AL is closed under contraposition, then AL satisfies the strong absurdity
axiom.
2. If AL satisfies the strong absurdity axiom then AL satisfies the self-contradiction axiom.
18. Let S be a minimal inconsistent set. We show ¬¨S ‚äÜ CNAS (S). If S is contradictory then the minimality
of S implies that S = {a, ¬¨a} for some a ‚àà L. The self-contradiction axioms holds obviously. Suppose
S is not contradictory. Hence there is a pair {a, ¬¨a} ‚äÜ CNAS (S) for some atom a. It is not difficult to
see that CNAS (S) = CNAS (‚àÖ) ‚à™ S. Therefore CNAS (S) \ S ‚äÜ CNAS (‚àÖ) = {p, b, ¬¨Oj(d2 )}. It follows
{a, ¬¨a} ‚à© CNAS (‚àÖ) 6= ‚àÖ and {a, ¬¨a} ‚à© S 6= ‚àÖ. From the minimality of S, S consists of exactly one element.
Therefore ‚àÄœÉ ‚àà S : ¬¨œÉ ‚àà CNAS (S).

91

Dung & Thang

Proof See appendix of section 4. 
The relations between the closure under transposition and the self-contradiction axiom
is illuminated in the following lemma.
Lemma 6 Let AS = (RS, RD) such that the set of strict rules RS is closed under transposition. Then ALAS satisfies the self-contradiction axiom.
Proof See appendix of section 4. 
The following example shows that the reverses of assertions in lemmas 5, 6 do not hold
in general.
Example 7 Let L = {a, ¬¨a, b, ¬¨b}. Let CONTRA be the set of all contradictory sets over
L.
1. For each X ‚äÜ L, define CN (X) = X. It is obvious that the abstract logic AL =
(CN, CON T RA) satisfies the self-contradiction axiom but not the strong absurdity
axiom.
2. Consider a set of strict rules RS consisting of a ‚Äùnormal‚Äù rule a ‚Üí b together with
‚Äùabsurdity rules‚Äù of the form x, ¬¨x ‚Üí y where x ‚àà {a, b}, y ‚àà L. Let CN be the
consequence operator wrt RS.
It is obvious that AL = (CN, CON T RA) satisfies the strong absurdity axiom (and
hence also the self-contradiction axiom). From b ‚àà CN ({a}), but ¬¨a 6‚àà CN ({¬¨b}) =
{¬¨b}, it follows that AL is not closed under contraposition.
It is clear that the set of strict rules is not closed under transposition.
3. Consider a set of strict rules RS consisting of just two strict rules a ‚Üí b and ¬¨b ‚Üí
¬¨a. It is clear that the rule set is closed under transposition but the corresponding
consequence operator does not satisfy the strong absurdity axiom. 
The following picture illustrates the relationships between key properties of rule-based
argumentation.

Figure 1:

92

Closure and Consistency In Logic-Associated Argumentation

5. Assumption-Based Argumentation
Given a logical language L, an assumption-based argumentation (ABA) framework (Bondarenko et al., 1997) is a triple F = (R, A, ) where R is a set of inference rules of the form
is a (total) one-one
œÉ1 , . . . œÉn ‚Üí œÉ (for n ‚â• 0), and A ‚äÜ L is a set of assumptions, and
mapping from A into L, where x is referred to as the contrary of x such that following
properties are satisfied:
‚Ä¢ assumptions in A do not appear in the heads of rules in R, and
‚Ä¢ contraries of assumptions are not assumptions, and
‚Ä¢ if L contains an explicit negation operator ¬¨ then CNR (‚àÖ) is not contradictory wrt
¬¨, i.e. for all œÉ ‚àà L, {œÉ, ¬¨œÉ} 6‚äÜ CNR (‚àÖ).
The following edition of the ‚Äùbirds fly penguins don‚Äôt‚Äù example provides an illustration.
Example 8 F = (R, A,

) where R consists of rules

not ab1 , p ‚Üí ¬¨f

not ab2 , b ‚Üí f

‚Üíp

p‚Üíb

p ‚Üí ab2

and A = {not ab1 , not ab2 } and not ab1 = ab1 , not ab2 = ab2 
We identify the structure of abstract logics underlying ABA frameworks below.
5.1 Assumption-based Abstract Logics
A set X ‚äÜ L is said to be contradictory iff
‚Ä¢ X is contradictory wrt
or

, i.e. there exists an assumption Œ± ‚àà A such that {Œ±, Œ±} ‚äÜ X,

‚Ä¢ X is contradictory wrt ¬¨,19 i.e. there exists œÉ ‚àà L such that {œÉ, ¬¨œÉ} ‚äÜ X.
Definition 16 Let F = (R, A,

) be an ABA framework. Define

ALF = (CNF , CON T RAF )
where
1. CNF = CNR .
2. CON T RAF is the set of all contradictory sets. 
It follows immediately
Lemma 7 ALF is an abstract logic.
19. if L contains the explicit negation operator ¬¨.

93

Dung & Thang

Remark 5 For simplicity, if there are no possibilities for misunderstanding, we often write
in this section CN (S) or CON T RA for CNF (S) or CON T RAF respectively.
We adapt the self-contradiction axiom for assumption-based argumentation below.
Definition 17 Let F be an ABA framework. We say that the abstract logic ALF satisfies
the
ab-self-contradiction axiom20 if for each inconsistent set of assumptions X, there is
Œ± ‚àà X such that Œ± ‚àà CNF (X). 
5.2 Closure and Consistency in Assumption-Based Argumentation
We first recall definitions of arguments and attack relation associated to an ABA framework.
Definition 18
1. Any assumption Œ± is an argument whose support and conclusion are
{Œ±}, Œ± respectively.
2. Let œÉ1 , . . . œÉn ‚Üí œÉ be a rule. Further suppose that A1 , . . . , An are arguments with
conclusions œÉ1 , . . . , œÉn respectively. Then A1 , . . . , An ‚Üí œÉ is an argument whose
conclusion is œÉ and whose support is the union of the supports of A1 , . . . , An
3. Every argument is constructed by applying finitely many times the above two steps. 
Remark 6 Arguments are often viewed as proof trees. Arguments of the form A1 , . . . , An ‚Üí
œÉ are proof trees with the root labelled by œÉ and the children of the root are the roots of
subtrees A1 , . . . , An . Note that if n = 0, the proof tree consists of just the root. If A is an
assumption Œ± then the proof tree consists of just the root labelled by Œ±
Notation 3

1. The support of an argument A is denoted by supp(A).

The support of a set of arguments S is the union of the supports of each individual
argument in it and denoted by supp(S).
2. The conclusion of an argument A is denoted by Cnl(A). 
Definition 19
supp(B).

1. An argument A attacks an argument B if Cnl(A) = Œ± for some Œ± ‚àà

2. We say B is a subargument of an argument A of the form A1 , . . . , Ak ‚Üí Œ±, denoted
by B ‚äë A, if B = A or B is a subargument of some Ai . 
Remark 7 For simplicity, we identify an assumption-based framework F with the logicassociated argumentation framework (AFF , ‚äë, ALF , Cnl) where AFF is the argumentation
framework generated from F (according to definitions 18, 19).
It is not difficult to see
Theorem 6 ABA frameworks are compact.
20. ab stands for assumption-based.

94

Closure and Consistency In Logic-Associated Argumentation

Proof See appendix of section 5. 

Theorem 7 Let F be an ABA framework. If ALF satisfies the ab-self-contradiction axiom,
then F is cohesive.
Proof See appendix of section 5. 
It follows immediately from theorems 6,7, corollary 1
Corollary 3 Let F be an ABA framework. If ALF satisfies the ab-self-contradiction axiom,
then F satisfies both the closure- and consistency-properties. 
5.3 Logic Programming
Logic programming could be classified into three different classes of definite programs,
normal programs and extended programs with increasing complexity. Bondarenko, Dung,
Kowalski and Toni(1997) showed that logic programs are instances of assumption-based
argumentation. We discuss below the underlying abstract logics of all classes and the selfcontradiction axiom.
5.3.1 Definite Logic Programs
A definite logic program is simply an assumption-based argumentation framework F =
(R, A, ) based on a language L where
1. L consists only of ground atoms and the set of assumptions is empty.
2. Rules in R are of the form a1 , . . . , an ‚Üí h where h, a1 , . . . , an are atoms from L.
As there is no contradiction in L, CON T RAF = ‚àÖ. The ab-self-contradiction axiom
hold trivially. Since there is no attack between arguments, the only extension is the set of
all arguments. The closure and consistency properties hold obviously.

5.3.2 Normal Logic Programs
A normal logic program is an assumption-based argumentation framework F = (R, A,
based on a language L where

)

1. L consists of atoms of the form a, b, . . . together with the negation-as-failure literals
of the form not a where a is an atom.
2. Assumptions are negation-as-failure literals not a whose contraries are a.
3. Rules in R are of the form l1 , . . . , ln ‚Üí h where h is an atom and l1 , . . . , ln are literals
from L.
95

Dung & Thang

CON T RAF consists of all subsets of L that contain a pair a, not a for some atom a.
The ab-self-contradiction axiom holds obviously.21 The closure and consistency properties hence hold for all extensions of normal programs.

5.3.3 Extended Logic Programs
An extended logic program (Gelfond & Lifschitz, 1990; Lifschitz, 1999) is an assumptionbased argumentation framework F = (R, A, ) based on a language L where
1. L consists of atoms of the form a, b, . . . and their explicit negations ¬¨a, ¬¨b, . . . together
with the negation-as-failure literals of the form not l where l is a classical literal (i.e.
an atom or the explicit negation of an atom).
2. Assumptions are negation-as-failure literals not l whose contraries are l.
3. R consists of rules of the form l1 , . . . , ln ‚Üí h where h is a classical literal and
l1 , . . . , ln are literals from L
CON T RAF consists of all subsets of L that contain a pair a, ¬¨a for some atom a or a
pair l, not l for some classical literal l.
From theorems 6,7 and corollary 3, it follows immediately
Corollary 4 Let F be a extended logic program. If CNF (‚àÖ) is not contradictory wrt ¬¨ and
ALF satisfies the ab-self-contradiction axiom then F is compact and cohesive and hence
satisfies both the closure and consistency properties.

6. Translating Rule-Based Argumentation into Assumption-Based
Argumentation
We have showed in the previous two sections that the self-contradiction axioms are sufficient
and natural conditions for ensuring closure and consistency properties in both assumptionbased and rule-based argumentation.
In this section, we argue that the self-contradiction axiom in rule-based systems is
subsumed by the assumption-based self-contradiction axiom by giving a translation from
rule-based systems into equivalent assumption-based ones. More generally, the translation suggests that rule-based argument systems (without preferences) are subsumed by
assumption-based argumentation.
Let AS = (RS, RD) be an arbitrary but fixed rule-based argumentation system such
that for each r ‚àà RD, there is no rule in RS ‚à™ RD whose head coincides with Oj(r) or
whose body contains an occurrence of Oj(r). We translate AS into an assumption-based
system in the following definition.
Definition 20 T (AS) = (R, A,

) is defined as follows:

21. We give a short proof here. Let X be an inconsistent set of assumptions. Hence there is an assumption
Œ± s.t. {Œ±, Œ±} ‚äÜ CNF (X). Since assumptions do not appear in the heads of rules, Œ± ‚àà X.

96

Closure and Consistency In Logic-Associated Argumentation

1. A = {Oj(r) | r ‚àà RD } ‚à™ {not l | l is the head of some rule in RD}
where Oj(r) is viewed as an assumption indicating that rule r is applicable and not l
is a negation-as-failure assumption stating that there is ‚Äùno evidence-to-the-contrary‚Äù
of l.
2.

R = RS ‚à™ {T r(r) | r ‚àà RD }
where T r(r) is of the form
Oj(r), not ¬¨h, Œª1 , . . . , Œªn ‚Üí h
if r is of the form Œª1 , . . . , Œªn ‚áí h

3.

Oj(r) = ¬¨Oj(r) and not l = l 

Remark 8 Since there is no rule in RS ‚à™ RD whose head is of the form Oj(r), no assumption in A coincides with the head of any rule in R. Therefore CNT (AS) (‚àÖ) is not
contradictory wrt . It is not difficult to see that CNT (AS) (‚àÖ) = CNAS (‚àÖ). CNT (AS) (‚àÖ) is
hence not contradictory wrt ¬¨. Therefore CNT (AS) (‚àÖ) is not contradictory.
T (AS) is hence an assumption-based argumentation system.
As AS and T (AS) are distinct systems, an attentive reader may ask in what sense they
are equivalent?
Before giving a formal elaboration on this question, let us look at an example.
Example 9 Consider a simple rule-based system AS consisting of one strict rule and one
defeasible rule :
r0 : ‚Üí b
r1 : b ‚áí f
There are two arguments here:
A1 : ‚Üí b and A2 : A1 ‚áí f
The arguments do not attack each other. Hence the only complete extension E of AS
contains both arguments A1 , A2 .
The corresponding assumption-based system T (AS) consists of two rules:
‚Üíb

Oj(r1 ), not ¬¨f, b ‚Üí f

There are four arguments in the assumption-based system:
B1 : ‚Üí b

B2 : C 0 , C 1 , B1 ‚Üí f

C0 : Oj(r1 )

C1 : not ¬¨f

There are no attacks between these four arguments. The only complete extension E ‚Ä≤ of
T (AS) consists of all four arguments. In fact the information contained in E ‚Ä≤ is fully captured in the set S = {B1 , B2 } since C0 , C1 are subarguments of B2 , hence any attack against
97

Dung & Thang

them is also an attack against B2 . We could view the set S as an equivalent representative
of E ‚Ä≤ . S could be viewed as representing the core of E ‚Ä≤ .
The equivalence between E and E ‚Ä≤ is captured by the correspondence between arguments
A1 , A2 and arguments B1 , B2 respectively.
Note that arguments C0 , C1 in extension E ‚Ä≤ of T (AS) explicitly represent the implicit
meta-level information contained in extension E of AS, namely, defeasible rule r1 is applicable and there is no argument with conclusion ¬¨f . 
Let AF0 = (AR0 , att0 ), AF1 = (AR1 , att1 ) be the argumentation frameworks corresponding to AS, T (AS) respectively.
Definition 21 Let S be a set of arguments in AF1 . The core of S, denoted by Core(S),
is defined by
Core(S) = S \ A
i.e Core(S) contains arguments in S that are not assumptions. 
For illustration, in example 9, Core(E ‚Ä≤ ) = {B1 , B2 }.
Lemma 8
1. Let S be a set of arguments in AF1 and A be an argument in AF1 . It
holds: A is acceptable wrt S iff A is acceptable wrt Core(S).
2. Let S be a set of arguments in AF1 . S is admissible iff Core(S) is admissible.
Proof See appendix of section 6. 
The following lemma states that complete sets are identified uniquely by their cores.
Lemma 9 Let E, E ‚Ä≤ be complete extensions of AF1 . It holds:
E = E ‚Ä≤ iff Core(E) = Core(E ‚Ä≤ )
Proof See appendix of section 6. 
We present a bijection between complete extensions in AF0 and complete extensions in
AF1 by defining a natural one-one mapping from AR0 into AR1 :
Definition 22 Define
C : AR0 ‚àí‚Üí AR1
such that following properties are satisfied:
1. If A is of the form A1 , . . . , An ‚Üí h, n ‚â• 0, then C(A) is of the form
C(A1 ), . . . , C(An ) ‚Üí h 22
22. Note that if A is of the form ‚Üí h, C(A) = A.

98

Closure and Consistency In Logic-Associated Argumentation

2. If A is a basic defeasible argument of the form A1 , . . . , An ‚áí h then C(A) is of the
form
Oj(r), not ¬¨h, C(A1 ), . . . , C(An ) ‚Üí h 23
where r is the last rule of A. 
For a set of arguments S ‚äÜ AR0 , let C(S) = {C(A) | A ‚àà S }. It follows
Lemma 10 Let A, B ‚àà AR0 and S ‚äÜ AR0 . The following observations hold:
1. Cnl(A) = Cnl(C(A)).
2. C is an one-one mapping from AR0 onto AR1 \ A
3. (A, B) ‚àà att0 iff (C(A), C(B)) ‚àà att1 .
4. S is admissible in AF0 if and only if C(S) is admissible in AF1 .
5. A is acceptable wrt S iff C(A) is acceptable wrt C(S)
Proof See appendix of section 6. 
Let L be the language of AS and
L0 = L \ {Oj(r), ¬¨Oj(r) | r is a defeasible rule in AS}
The equivalence of AS and T (AS) is established in the following theorem.
Theorem 8 For each complete extension E of AF0 there is a complete extension E ‚Ä≤ of AF1
and vice versa such that the following properties hold:
1. C(E) = Core(E ‚Ä≤ )
2. For each literal l ‚àà L0 ,

l ‚àà Cnl(E) iff l ‚àà Cnl(E ‚Ä≤ )

Proof See appendix of section 6. 
The following theorem shows that the self-contradiction axiom in rule-based argumentation is subsumed by the ab-self-contradiction axiom in assumption-based argumentation.
Let ALi = (CNi , CON T RAi ), i = 0,1, be the abstract logics associated to AS, T (AS)
respectively.
Theorem 9 If AL0 satisfies the self-contradiction axiom then AL1 satisfies the ab-selfcontradiction axiom.
Proof See appendix of section 6. 
23. Note that if A is a defeasible rule r of the form ‚áí h, C(A) is of the form Oj(r), not ¬¨h, ‚Üí h.

99

Dung & Thang

7. Discussion
Amgoud and Besnard (2009) have introduced the use of Tarski‚Äôs abstract logic to study
the consistency property of logic-based argumentation. In the following, we discuss how
compactness and cohesion could be fulfilled in their systems.
As remarked earlier, a Tarski abstract logic is represented by an abstract logic where
CON T RA 6= ‚àÖ and for each X ‚àà CON T RA, CN (X) = L and there is x ‚àà L such that
CN ({x}) ‚àà CON T RA.
A Tarski abstract logic is said to be adjunctive if for all x, y ‚àà L, if CN ({x}) 6=
CN ({x, y}) 6= CN ({y}) then there exists z such that CN ({z}) = CN ({x, y}).
A knowledge base is defined as a set Œ£ ‚äÜ L such that for each x ‚àà Œ£, x is consistent.
An argument over Œ£ is a pair A = (X, œÉ) where X is finite consistent support of A
denoted by supp(A), and œÉ ‚àà CN (X) is the conclusion of A denoted by Cnl(A). The
support of a set of arguments is the union of the supports of the individual arguments.
AR denotes the set of all arguments over Œ£.
X ‚äÜ Œ£ is called a minimal conflict set if X is inconsistent and each proper subset of X
is consistent.24
For the rest of this discussion, we assume that all minimal conflict sets contain two or
more elements.
Let att ‚äÜ AR √ó AR be an attack relation.
1. att is said to be context-sensitive iff for all a, b ‚àà AR, if supp(a)‚à™supp(b) is inconsistent
then either (a, b) ‚àà att or (b, a) ‚àà att.
2. att is said to be conflict-dependent iff for all a, b ‚àà AR, if (a, b) ‚àà att then supp(a) ‚à™
supp(b) is inconsistent.
3. att is said to be symmetric iff for all a, b ‚àà AR, if (a, b) ‚àà att then (b, a) ‚àà att.
An argument B is said to be a subargument of an argument A, denoted B ‚äë A if B = A
or B = ({œÉ}, œÉ) for some œÉ ‚àà supp(A).
Lemma 11 If all minimal conflict sets are binary and the attack relation att is contextsensitive, conflict-dependent and symmetric then LAF = (AF, ‚äë, AL, Cnl) is a compact
and cohesive logic-associated argumentation framework.
Proof See appendix of section 7. 
It follows immediately from theorem 1:
Corollary 5 If all minimal conflict sets are binary and the attack relation att is contextsensitive, conflict-dependent and symmetric then LAF satisfies both properties of closure
and consistency.
24. It is not difficult to see that each minimal conflict set is finite as from CN (X) = L, and L = CN ({x})
for some x ‚àà L, it follows that x ‚àà CN (X). From the finiteness axiom, there is a finite subset Y of X
such that x ‚àà CN (Y ). Hence CN (Y ) = L. From the minimality of X, it follows X = Y . X is hence
finite.

100

Closure and Consistency In Logic-Associated Argumentation

Corollary 5 is rather limited due to restrictions imposed on the attack relations. This
is because the structure of arguments here is rather poor as abstract logics do not reveal
any structure of the consequence relation. Our approach of marrying abstract argumentation with abstract logics resulting in logic-associated abstract argumentation addresses this
problem by specifying the subargument structure and its relation to the attack relation.
Caminada and Amgoud (2007) have also studied unrestricted rebuts where two arguments with contrary conclusions are considered to attack each other. Defeasible argumentation with attacks based on unrestricted rebuts violates both the consistency and closure
properties except for the grounded semantics. Unrestricted rebuts did not gain much attention in the research in assumption-based argumentation or logic programming. This
suggests that there are relevant structural features underlying unrestricted attacks that are
still not understood. A sensible idea could be to study this kind of attacks within our
proposed framework of logic-associated abstract argumentation as it could shed lights on
its instances in both defeasible and assumption-based argumentation.
Non-interference, another key rationality postulate for structured argumentation has
been proposed by Caminada, Carnielli, and Dunne (2012) and studied extensively by Caminada et al. (2012), and Wu (2012). Non-interference is conceptually different to the consistency and closure properties as the later properties could be viewed as about the correctness
of the argument systems while the former is about their structural modularity. As we focus
on the correctness of argument systems, a study on their structural modularity is outside
the scope of this paper. Nonetheless, non-interference seems to be related to a property of
localizing conflicts in arguments systems where we say that an argument system is localized
if there is no argument attacking every argument. It is not difficult to see that aspic systems
that are closed under contraposition is not localized if there is a rebutting attack in it. In
contrast, the self-contradiction axiom allows us to develop localized aspic systems. It would
be interesting to see how the two concepts of localized conflicts and non-interference are
interrelated.
Toni (2008) has generalized assumption-based argumentation to represent reasoning
with both strict and defeasible rules satisfying both the rational properties of logical closure
and consistency. As we have showed in section 6, standard assumption-based argumentation
captures rule-based argumentation system by a simple and elegant transformation. Hence it
is not necessary to generalize assumption-based argumentation to capture defeasible reasoning with strict and defeasible rules. Nevertheless, the proposal by Toni (2008) is interesting.
Nielsen and Parson (2007) have also proposed a generalization of abstract argumentation
allowing sets of attacking arguments. Prakken (2010) and Modgil and Prakken (2013) had
also studied preferences between arguments. It would be interesting to see whether the
properties of compactness and cohesion are satisfied in these frameworks.
We believe that the compactness and cohesion and self-contradiction properties could
serve as guideline principles in the design of logic-based argumentation systems to ensure the
satisfaction of properties of logical closure and consistency. As pointed out by Caminada and
Amgoud (2007), there are several argument systems (Garcia & Simari, 2004; Governatori,
Maher, Antoniou, & Billington, 2004) not satisfying the consistency property. It would be
interesting to see how the results in this paper could be applied on them.
101

Dung & Thang

Appendix A. Section 1
Let CN be the consequence operator wrt AS ‚Ä≤ . It is clear that CN (‚àÖ) = {p, b, ¬¨Oj(d2 ), th}.
Let c ‚àà CN (X) for c ‚àà L and X ‚äÜ L. Let x ‚àà X. We show that ¬¨x ‚àà CN (Y ) where
Y = X \ {x} ‚à™ {¬¨c}. If c ‚àà CN (‚àÖ), from the ‚Äùabsurdity rules‚Äù, it follows immediately that
¬¨x ‚àà CN (Y ). Suppose c 6‚àà CN (‚àÖ). Hence c ‚àà CN (X) iff c ‚àà X or X contains a pair of
literals a, ¬¨a. If c ‚àà X and x 6= c, then c ‚àà Y . Hence {c, ¬¨c} ‚äÜ Y . From the ‚Äùabsurdity
rules‚Äù, L = CN (Y ). If c ‚àà X and x = c then ¬¨x ‚àà Y . Hence ¬¨x ‚àà CN (Y ). If X contains
a pair of literals a, ¬¨a and x 6‚àà {a, ¬¨a}, then L = CN (Y ). If x ‚àà {a, ¬¨a} then ¬¨x ‚àà Y .

Appendix B. Section 3
Lemma 1 Let S be a set of arguments. The following assertions hold:
1. For each argument A, A is generated by {A}.
2. Sub(S) ‚äÜ GN (S).
3. Cnl(GN (S)) ‚äÜ CN (Cnl(Sub(S))).
4. CN (Cnl(GN (S))) = CN (Cnl(Sub(S))).
5. For each argument C, C attacks GN (S) iff C attacks S.
Proof The first assertion is obvious from definitions 6 and 7. Since for each A ‚àà Sub(S),
{A} ‚äÜ Sub(S), it follows immediately from the first assertion that each argument in Sub(S)
is generated by S. The third assertion follow immediately from definitions 6 and 7. The
fourth assertion follows from the second and third ones.
From S ‚äÜ GN (S), it is clear that if C attacks S, C attacks GN (S). Suppose C attacks
GN (S). Let A ‚àà GN (S) s.t. C attacks A. Let BA be a base of A such that BA ‚äÜ Sub(S).
C hence attacks BA. Therefore C attacks Sub(S). Thus C attacks S. 
Theorem 1 Let E be a complete extension. Then GN (E) = E
Proof Since each attack against GN (E) is an attack against E (lemma 1, last assertion),
each attack against GN (E) is counterattacked by E as E is a complete extension. Therefore
GN (E) ‚äÜ E. From second assertion in lemma 1, it follows E ‚äÜ GN (E). Hence GN (E) =
E. 

Appendix C. Section 4
Remark 9 A strict argument over X ‚äÜ L is a strict argument over the set of rules RS ‚à™{‚Üí
Œ± | Œ± ‚àà X}.
Remark 10 For a strict argument A over X, the set of premises of A, denoted by P rem(A),
is the set of literals from X labelling the leaves of A (viewed as a proof tree).
Theorem 4 Rule-based argumentation systems are compact.

102

Closure and Consistency In Logic-Associated Argumentation

Proof Let AS be a rule-based system and let S be a set of arguments wrt AS and œÉ ‚àà
CNAS (Cnl(Sub(S))). From lemma 2, we only need to show that œÉ ‚àà Cnl(GN (S)).
Let X be a minimal subset of Cnl(Sub(S)) such that œÉ ‚àà CNAS (X). Hence there is a
strict argument A0 over X with conclusion œÉ. Further let SX be a minimal set of arguments
from Sub(S) s.t. Cnl(SX ) = X. Let A be the argument obtained by replacing each leaf in
A0 (viewed as a proof tree) labelled by a literal Œ± from X by an argument with conclusion
Œ± from SX . It is obvious that the conclusion of A is œÉ. We show that SX is a base of
A. Suppose B is an argument attacking A. Since A0 is a strict argument over X, B must
attack a basic defeasible subargument of some argument in SX . Hence B attacks SX . Thus
A ‚àà GN (S). Hence œÉ ‚àà Cnl(GN (S)). We have proved that that the rule-based argumentation system AS is compact. 
Lemma 4 Let A be an argument and BD be the set of basic defeasible subarguments of
A. Then Cnl(A) ‚àà CNAS (Cnl(BD)).
Proof By induction on the size of A.
Basic Step A is of the form ‚Üí / ‚áí œÉ.
Suppose A is of the form ‚Üí œÉ then œÉ ‚àà CNAS (‚àÖ). From BD = ‚àÖ, the lemma holds.
Suppose A is of the form ‚áí œÉ then BD = {A}. The lemma holds.
Inductive Step. Suppose A is of the form A1 , . . . , An ‚Üí / ‚áí œÉ
Suppose A is of the form A1 , . . . , An ‚áí œÉ then A ‚àà BD. The lemma holds obviously.
Suppose A is of the form A1 , . . . , An ‚Üí œÉ . Hence BD is the union of the sets
BD1 , . . . , BDn of basic defeasible subarguments of A1 , . . . , An respectively. From the induction hypothesis, Cnl(Ai ) ‚àà CNAS (Cnl(BDi )), 0 ‚â§ i ‚â§ n. Hence Cnl(A) ‚àà CNAS (Cnl(BD)).

Theorem 5 Suppose ALAS satisfies the self-contradiction axiom. Then AS is cohesive.
Proof Let S be an inconsistent set of arguments. Hence Cnl(Sub(S)) is inconsistent. Define
BD to be the set of all basic defeasible arguments in Sub(S). It is clear that BD 6= ‚àÖ. From
lemma 4, it follows that Cnl(Sub(S) ‚äÜ CNAS (Cnl(BD)). Hence CNAS (Cnl(Sub(S))) =
CNAS (Cnl(BD)). Cnl(BD) is therefore inconsistent. Since ALAS satisfies the self-contradiction
axiom, there is œÉ ‚àà Cnl(BD) such that ¬¨œÉ ‚àà CNAS (Cnl(BD)). Let B ‚àà BD with
Cnl(B) = œÉ. From CNAS (Cnl(Sub(S))) = CNAS (Cnl(BD)), it follows that ¬¨œÉ ‚àà CNAS (Cnl(Sub(S))).
From the compactness of AS and Sub(S) ‚äÜ GN (S), it follows that there is an argument
A ‚àà GN (S) such that Cnl(A) = ¬¨œÉ. Hence A attacks B. Since B ‚àà BD ‚äÜ Sub(S) ‚äÜ
GN (S), GN (S) is conflicting. 
Lemma 5
1. If AL is closed under contraposition, then AL satisfies the strong absurdity axiom.
2. If AL satisfies the strong absurdity axiom then AL satisfies the self-contradiction
axiom.
Proof
103

Dung & Thang

1. Suppose AL is closed under contraposition. Let X ‚àà CON T RA. Hence there is
an atom a s.t. {a, ¬¨a} ‚äÜ X. From a ‚àà CN ({a, Œ±}) for any literal Œ±, and from the
closure under contraposition property, it follows that ¬¨Œ± ‚àà CN ({a, ¬¨a}). Hence ¬¨Œ± ‚àà
CN (X) for any literal Œ±. We have proved that L = CN (X) for each X ‚àà CON T RA.
From definition 10, it follows that AL satisfies the strong absurdity axiom.
2. Suppose AL satisfies the strong absurdity axiom. Let X ‚äÜ L such that X is minimal
inconsistent. Therefore CN (X) ‚àà CON T RA. From the idempotence axiom and the
strong absurdity axiom, CN (X) = L. It holds obviously: ¬¨X ‚äÜ CN (X). 
Lemma 6 Let AS = (RS, RD) such that the set of strict rules RS is closed under transposition. Then ALAS satisfies the self-contradiction axiom.
Proof We first prove the following assertion.
Assertion: Let A be a strict argument over X with conclusion œÉ and ‚àÖ =
6 P rem(A) ‚äÜ X.
Then for each Œ± ‚àà P rem(A), there is an argument B with premises in P rem(A) ‚à™ {¬¨œÉ}
and conclusion ¬¨Œ± .
Proof We prove by induction on the height of A (as a proof tree).25
If the height of A is 0, the theorem is obvious.
Suppose A is of the form A1 , . . . , An ‚Üí œÉ where Cnl(Ai ) = Œ±i . Let Œ± ‚àà P rem(A).
Without loss of generality, let Œ± ‚àà P rem(An ). From the closure under transposition, the rule
Œ±1 , . . . , Œ±n‚àí1 , ¬¨œÉ ‚Üí ¬¨Œ±n also belongs to RS. Let B be the argument A1 , . . . , An‚àí1 , ¬¨œÉ ‚Üí
¬¨Œ±n .
From the induction hypothesis, there is a proof tree T r whose premises are in P rem(An )‚à™
{¬¨Œ±n } and whose conclusion is ¬¨Œ±.
Let T r‚Ä≤ be the tree obtained from T r by replacing each occurrence of premise ¬¨Œ±n by
the argument B. It is clear that P rem(T r‚Ä≤ ) ‚äÜ P rem(A) ‚à™ {¬¨œÉ} and Cnl(T r‚Ä≤ ) = ¬¨Œ±. 
Let X ‚äÜ L s.t. X is minimal inconsistent. Hence there are two arguments A0 , A1
with premises in X and conclusions œÉ, ¬¨œÉ respectively. From the minimality of X, it
holds: X = P rem(A0 ) ‚à™ P rem(A1 ). Let Œ± ‚àà X. Without loss of generality, suppose
Œ± ‚àà P rem(A0 ). From the above assertion, it follows that there exists an argument B with
conclusion ¬¨Œ± and P rem(B) ‚äÜ P rem(A0 ) ‚à™ {¬¨œÉ}. Let A be the argument obtained by
replacing each leaf labelled by ¬¨œÉ in B by tree A1 . It is clear that P rem(A) ‚äÜ X and the
conclusion of A is ¬¨Œ±. 

Appendix D. Section 5
Theorem 6 ABA frameworks are compact.
Proof Let S be a set of arguments. Let SU = Sub(S) and CSU = Cnl(SU ). We only
need to prove that CN (CSU ) ‚äÜ Cnl(GN (S)) (lemma 2). Let œÉ ‚àà CN (CSU ). It is easy
to see that there is an argument A (viewed as a proof tree) with conclusion œÉ whose leaves
25. The height of a proof tree is the length (the number of links) of the longest path from the root to a leaf
node.

104

Closure and Consistency In Logic-Associated Argumentation

are labelled by sentences in CSU . Expand this proof tree at each leaf labelled by Œ¥ ‚àà CSU
by a proof tree representing an argument in SU with conclusion Œ¥. The new proof tree
corresponds to an argument B with conclusion œÉ. The proof trees in SU that are used to
expand A obviously form a base of B. It is hence clear that B is generated by S. 
Theorem 7 Let F be an ABA framework. If ALF satisfies the ab-self-contradiction axiom,
then F is cohesive.
Proof Let S be an inconsistent set of arguments. Hence supp(S) is inconsistent. Since
CNF satisfies the assumption-based self-contradiction axiom, there is Œ± ‚àà supp(S) such
that Œ± ‚àà CNF (supp(S)). From lemma 2, there is an argument A ‚àà GN (S) such that
Cnl(A) = Œ±. It is obvious that A attacks any argument in S whose premises contain Œ±.
Since S ‚äÜ GN (S), GN (S) is hence conflicting. 

Appendix E. Section 6
Lemma 8
1. Let S be a set of arguments in AF1 and A be an argument in AF1 . It holds:
A is acceptable wrt S iff A is acceptable wrt Core(S).
2. Let S be a set of arguments in AF1 . S is admissible iff Core(S) is admissible.
Proof
1. Since Core(S) ‚äÜ S, if A is acceptable wrt Core(S), A is obviously acceptable wrt S.
Suppose now that A is acceptable wrt S. Let B attack A. Hence there is A‚Ä≤ ‚àà S s.t.
A‚Ä≤ attacks B. Therefore A‚Ä≤ is not an assumption. Hence A‚Ä≤ ‚àà Core(S). B is hence
attacked by Core(S), i.e. A is acceptable by Core(S).
2. Follows immediately from the previous assertion. 
Lemma 9. Let E, E ‚Ä≤ be complete extensions of AF1 . It holds:
E = E ‚Ä≤ iff Core(E) = Core(E ‚Ä≤ )
Proof We only need to show that Core(E) = Core(E ‚Ä≤ ) implies E = E ‚Ä≤ . The reverse
direction is obvious. Let Core(E) = Core(E ‚Ä≤ ) = S. Let A ‚àà E \ S. A is hence an
assumption acceptable wrt E. From lemma 8, A is acceptable wrt S. Thus A is acceptable
wrt E ‚Ä≤ (lemma 8). Hence A ‚àà E ‚Ä≤ . Similarly, we could show that each assumption in E ‚Ä≤ \ S
belongs to E. We thus proved E = E ‚Ä≤ . 
Lemma 10 Let A, B ‚àà AR0 and S ‚äÜ AR0 . The following observations hold:
1. Cnl(A) = Cnl(C(A)).
2. C is an one-one mapping from AR0 onto AR1 \ A
105

Dung & Thang

3. (A, B) ‚àà att0 iff (C(A), C(B)) ‚àà att1 .
4. S is admissible in AF0 if and only if C(S) is admissible in AF1 .
5. A is acceptable wrt S iff C(A) is acceptable wrt C(S)
Proof
1. The first assertion is obvious.
2. It is obvious that there is no argument A in AR0 such that C(A) ‚àà A.
Viewing an argument in AR0 as a proof tree, the height of a tree is defined to be the
length (i.e. number of links) of the longest path from the root to a leaf. Let AR0,k be
the set of all trees of height ‚â§ k in AR0 . We prove by induction that C is one-one on
AR0,k .
It is obvious that C is one-one on AR0,0 . Suppose C is one-one on AR0,k . Let A, B
be two different arguments in AR0,k+1 . If the last rules of A,B are different then it is
obvious that C(A), C(B) are different. Suppose that the last rules of A,B are identical.
Then A, B respectively have the forms A1 , . . . , An ‚Üí / ‚áí h, B1 , . . . , Bn ‚Üí / ‚áí h.
Without loss of generality, we can assume that A1 6= B1 . Hence from the induction
hypothesis, C(A1 ) 6= C(B1 ). Therefore C(A) 6= C(B).
It is also straightforward to prove by induction that for each B ‚àà AR1 \ A, there is
A ‚àà AR0 such that C(A) = B.
3. (a) Suppose (A, B) ‚àà att0 . Let B ‚Ä≤ be a basic defeasible subargument of B such that
A attacks B (on B ‚Ä≤ ). There are two cases:
i. Cnl(A) = ¬¨Oj(Lr(B ‚Ä≤ )) (undercut attack). From Cnl(A) = Cnl(C(A)),
it follows Cnl(C(A)) = ¬¨Oj(Lr(B)) and Oj(Lr(B)) ‚àà supp(C(B)). C(A)
hence attacks C(B) wrt att1 .
ii. Cnl(A) = ¬¨h for h = Cnl(B ‚Ä≤ ). Hence Cnl(C(A)) = ¬¨h and not ¬¨h ‚àà
supp(C(B)). C(A) hence attacks C(B) wrt att1 .
(b) Suppose (C(A), C(B)) ‚àà att1 . There are two cases:
i. Cnl(C(A)) = ¬¨Oj(r) for some defeasible rule r such that Oj(r) ‚àà supp(C(B)).
From Cnl(A) = Cnl(C(A)), it follows Cnl(A) = ¬¨Oj(r) and r is a defeasible
rule in B. Hence there is basic defeasible subargument B ‚Ä≤ of B such that
Lr(B ‚Ä≤ ) = r. Hence A attacks B on B ‚Ä≤ in AF0 .
ii. Cnl(C(A)) = ¬¨h such that not ¬¨h ‚àà supp(B). Hence there is basic defeasible
rule r in B such that hd(r) = h. Therefore there is a subargument B ‚Ä≤ of B
such that Lr(B ‚Ä≤ ) = r. Hence A attacks B on B ‚Ä≤ (by rebutting) in AF0 .
4. From assertion 3, it is clear that S is conflict-free iff C(S) is conflict-free.
Suppose S defends itself against all attacks. Let A attack C(S) in AF1 . Therefore A is
not an assumption. From the second assertion, there is B = C ‚àí1 (A). From assertion
3, it follows B attacks S. Therefore S attacks B. Hence C(S) attacks A.
106

Closure and Consistency In Logic-Associated Argumentation

Suppose C(S) defends itself against all attacks. Let A attack S in AF0 . Let B = C(A).
From assertion 3, it follows B attacks C(S). Therefore C(S) attacks B. Hence S attacks
A.
5. Follows immediately from assertion 3. 
Theorem 8 For each complete extensions E of AF0 there is a complete extension E ‚Ä≤ of
AF1 and vice versa such that following properties hold:
1. C(E) = Core(E ‚Ä≤ )
2. For each literal l ‚àà L0 , l ‚àà Cnl(E) iff l ‚àà Cnl(E ‚Ä≤ )
Proof Let E be a complete extension of AF0 . From assertion 4 in lemma 10, it follows
S = C(E) is admissible. Let AS be the set of assumptions acceptable wrt S. We show
that E ‚Ä≤ = S ‚à™ AS is complete. Let B in AF1 be acceptable wrt E ‚Ä≤ . Suppose B is not an
assumption. Let A = C ‚àí1 (B). From lemma 10, assertion 5, A is acceptable wrt E. Hence
A ‚àà E. Therefore B ‚àà S. If B is an assumption, B ‚àà AS . We have proved that E ‚Ä≤ is
complete and Core(E ‚Ä≤ ) = C(E). The uniqueness of E ‚Ä≤ follows directly from the lemma 9.
Let l ‚àà L0 such that l ‚àà Cnl(E ‚Ä≤ ). Since l 6‚àà A, it is clear that l ‚àà Cnl(Core(E ‚Ä≤ )). Hence
l ‚àà Cnl(C(E)). From the first assertion in lemma 10, it follows l ‚àà Cnl(E). 
Theorem 9 If AL0 satisfies the self-contradiction axiom then AL1 satisfies the ab-selfcontradiction axiom.
Proof Suppose AL0 satisfies the self-contradiction axiom. Let X ‚äÜ A be an inconsistent
set of assumptions in T (AS). We want to show that there exists Œ± ‚àà X such that Œ± ‚àà
CN1 (X). Suppose the contrary. It follows immediately that there is atom a such that
{a, ¬¨a} ‚äÜ CN1 (X). There are two cases.
Case 1: {a, ¬¨a} ‚à© A =
6 ‚àÖ. Since ¬¨a 6‚àà A, a is an assumption. As classical negation
does not apply to negation-as-failure literal, it follows a = Oj(d) for some d ‚àà RD. From
Oj(d) = ¬¨a ‚àà CN1 (X), this is a contradiction to the hypothesis that 6 ‚àÉŒ± ‚àà X such that
Œ± ‚àà CN1 (X). This case hence can not occur.
Case 2: {a, ¬¨a} ‚à© A = ‚àÖ. Therefore {a, ¬¨a} ‚à© X = ‚àÖ. Let S1 be the set of all arguments
in AR1 \ A whose support is a subset of X. From {a, ¬¨a} ‚äÜ CN1 (X) \ X, it follows S1 6= ‚àÖ.
It is not difficult to see that Cnl(S1 ) = CN1 (X) \ X. Let S0 be a set of arguments
from AR0 such that S0 = C ‚àí1 (S1 ). From Cnl(S0 ) = Cnl(S1 ), it follows that Cnl(S0 ) is
closed (wrt CN0 ). It is also easy to see that Sub(S0 ) = S0 . Let BS be the set of basic
defeasible arguments in S0 . From lemma 4, it is clear that CN0 (Cnl(BS)) = Cnl(S0 ). Since
{a, ¬¨a} ‚äÜ CN1 (X) \ X = Cnl(S0 ) = CN0 (Cnl(BS)), Cnl(BS) is hence also inconsistent
wrt AS. Because AL0 satisfies the self-contradiction axiom, there is a literal h ‚àà Cnl(BS)
such that ¬¨h ‚àà CN0 (Cnl(BS)). Let Œ± = not ¬¨h. From h ‚àà Cnl(BS), it follows Œ± ‚àà
supp(C(BS)) ‚äÜ supp(S1 ) = X. From ¬¨h ‚àà CN0 (Cnl(BS)) = Cnl(S1 ) = CN1 (X) \ X, it
follows Œ± ‚àà CN1 (X). Contradiction.
We have proved that AL1 satisfies the assumption-based self-contradiction axiom. 
107

Dung & Thang

Appendix F. Section 7
Lemma 11 If all minimal conflict sets are binary and the attack relation att is contextsensitive, conflict-dependent and symmetric then LAF = (AF, ‚äë, AL, Cnl) is a compact
and cohesive logic-associated argumentation framework.
Proof To show that LAF is a logic-associated argumentation framework, we need to show
that for A, B, C ‚àà AR, if C attacks B and B ‚äë A then C attacks A. If B = A, there is nothing to prove. Suppose B = ({œÉ}, œÉ) for some œÉ ‚àà supp(A). From the conflict-dependency,
it follows supp(C) ‚à™ {œÉ} is inconsistent. Hence supp(C) ‚à™ supp(A) is inconsistent. From the
context-sensitivity and symmetry of attack relation, it follows C attacks A.
Let S be a set of arguments and œÉ ‚àà CN (Cnl(Sub(S))). Hence there exists a finite
X ‚äÜ supp(S) such that œÉ ‚àà CN (X). Let A = (X, œÉ). Let X = {({Œ±}, Œ±) | Œ± ‚àà X}. It is
clear that X ‚äÜ Sub(S). We show that X is a base of A. Suppose B attacks A. Hence from the
conflict-dependency of att, there exists a minimal conflict set {Œ±, Œ≤} such that Œ± ‚àà supp(A),
Œ≤ ‚àà supp(B). Hence B attacks the argument ({Œ±}, Œ±) ‚àà X . Hence A ‚àà GN (S). We have
proved CN (Cnl(Sub(S))) ‚äÜ Cnl(GN (S). Hence from lemma 2, AF is compact.
Let S be an inconsistent set of arguments. Hence supp(S) is inconsistent. There exists a
binary minimal conflict set {Œ±, Œ≤} ‚äÜ supp(S). Hence arguments A = ({Œ±}, Œ±), B = ({Œ≤}, Œ≤)
attack each other. As A, B ‚àà GN (S), GN (S) is conflicting. 

References
Amgoud, L., & Besnard, P. (2009). Bridging the gap between abstract argumentation
systems and logic. In SUM, pp. 12‚Äì27.
Bondarenko, A., Dung, P., Kowalski, R., & Toni, F. (1997). An abstract, argumentationtheoretic approach to default reasoning. Artif. Intell., 93, 63‚Äì101.
Caminada, M., & Amgoud, L. (2007). On the evaluation of argumentation formalisms.
Artificial Intelligence, 171, 286‚Äì310.
Caminada, M., Carnielli, W., & Dunne, P. (2012). Semi-stable semantics. Journal of Logic
and Computation, 22 (5), 1207‚Äì1254.
Dung, P. M. (1995). On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person gamescceptability of arguments and
its fundamental role in nonmono- tonic reasoning, logic programming and n-person
games. Artif. Intell., 77 (2), 321‚Äì358.
Dung, P. M., & Thang, P. M. (2011). Closure and consistency rationalities in logic-based argumentation. In Balduccini, M., & Son, T. C. (Eds.), Logic Programming, Knowledge
Representation, and Nonmonotonic Reasoning, Vol. 6565, pp. 33‚Äì43. Springer.
Garcia, A., & Simari, G. (2004). Defeasible logic programming: An argumentative approach.
TPLP, 4 (1-2), 95‚Äì138.
Gelfond, M., & Lifschitz, V. (1988). The stable model semantics of logic programming. In
ICLP/SLP, pp. 579‚Äì597. MIT Press.
108

Closure and Consistency In Logic-Associated Argumentation

Gelfond, M., & Lifschitz, V. (1990). Logic programs with classical negation. In ICLP, pp.
579‚Äì597. MIT Press.
Governatori, G., Maher, M., Antoniou, G., & Billington, D. (2004). Argumentation semantics for defeasible logic. J. Log. Comput., 14 (5), 675‚Äì702.
Lifschitz, V. (1999). Answer set planning (abstract). In LPNMR, pp. 373‚Äì374. MIT Press.
Modgil, S., & Prakken, H. (2013). A general account of argumentation with preferences.
Artificial Intelligence, 195, 361‚Äì397.
Nielsen, S., & Parson, S. (2007). A generation of dung‚Äôs abstract framework for argumentation: Arguing with sets of attacking arguments. In LNCS, No. 4766, pp. 54‚Äì73.
Springer Verlag.
Pollock, J. (1995). Cognitive carpentry: A blueprint for how to build a person.. MIT Press,
Cambridge MA.
Pollock, J. (1987). Defeasible reasoning. Cognitive Science, 11 (4), 481‚Äì518.
Prakken, H. (2010). An abstract framework for argumentation with structured arguments.
J. Arguments and Computation, 1 (2), 93‚Äì124.
Prakken, H. (2012). Some reflection on two current trends in formal argumentation. In Artikis, A., Craven, R., Cicekli, N. K., Sadighi, B., & Stathis, K. (Eds.), Logic Programs,
Norms and Action, Vol. 7360 of Lecture Notes in Computer Science, pp. 249‚Äì272.
Springer Verlag.
Toni, F. (2008). Assumption-based argumentation for closed and consistent defeasible reasoning. JSAI, 390‚Äì402.
Wu, Y. (2012). Between Arguments and Conclusions. Ph.D. thesis, University of Luxemburg.

109

Journal of Artificial Intelligence Research 49 (2014) 171-206

Submitted 07/13; published 02/14

Representing and Reasoning About the Rules of
General Games With Imperfect Information
Stephan SchiÔ¨Äel

stephans@ru.is

School of Computer Science, Reykjavƒ±ÃÅk University,
Menntavegur 1, 101 Reykjavƒ±ÃÅk ICELAND

Michael Thielscher

mit@cse.unsw.edu.au

School of Computer Science and Engineering,
The University of New South Wales,
Sydney, NSW 2052 AUSTRALIA

Abstract
A general game player is a system that can play previously unknown games just by
being given their rules. For this purpose, the Game Description Language (GDL) has been
developed as a high-level knowledge representation formalism to communicate game rules to
players. In this paper, we address a fundamental limitation of state-of-the-art methods and
systems for General Game Playing, namely, their being confined to deterministic games with
complete information about the game state. We develop a simple yet expressive extension
of standard GDL that allows for formalising the rules of arbitrary finite, n-player games
with randomness and incomplete state knowledge. In the second part of the paper, we
address the intricate reasoning challenge for general game-playing systems that comes with
the new description language. We develop a full embedding of extended GDL into the
Situation Calculus augmented by Scherl and Levesque‚Äôs knowledge fluent . We formally
prove that this provides a sound and complete reasoning method for players‚Äô knowledge
about game states as well as about the knowledge of the other players.

1. Introduction
General Game Playing (GGP) is concerned with the development of systems that understand the rules of previously unknown games and learn to play these games well without
human intervention. The annual AAAI GGP Competition, which has been established in
2005 to foster research in this area, has led to a number of successful approaches and systems (Kuhlmann, Dresner, & Stone, 2006; Clune, 2007; SchiÔ¨Äel & Thielscher, 2007; Kaiser,
2008; Finnsson & BjoÃàrnsson, 2008; Kissmann & Edelkamp, 2011; MeÃÅhat & Cazenave, 2011;
Kirci, Sturtevant, & SchaeÔ¨Äer, 2011). General game-playing programs are a quintessential
example of systems that end users can customise for their own specific tasks. This makes
GGP an interesting and challenging problem for AI, involving many fundamental issues such
as reasoning, learning, planning and decision making (Pell, 1993). Consequently, General
Game Playing has broader significance to a variety of AI disciplines beyond conventional
computer game playing (Genesereth, Love, & Pell, 2005).
c
‚Éù2014
AI Access Foundation. All rights reserved.

Schiffel & Thielscher

1.1 Representing the Rules of General Games
The first AAAI Competition saw the introduction of the general Game Description Language (GDL) as the foundation for general game-playing systems (Genesereth et al., 2005).
Other machine-processable languages for the specification of games existed before, most notably Gala (for: ‚ÄúGame Language‚Äù). But the latter was never used for any purpose other
than as the front-end to a system for computing optimal strategies from game trees (Koller
& PfeÔ¨Äer, 1997). Presumably its tight coupling with a programming language (Prolog) and
its operational‚Äîrather than declarative‚Äîsemantics prevented Gala from being adopted
by others as a system-independent game specification language.
GDL allows for the description of any game with finitely many players and finitely
many legal moves in each state, where all moves are deterministic and fully observable, and
where the complete game rules are known to each player. Simultaneous moves are possible,
which provides for a restricted form of imperfect information. However, players are always
immediately informed about each other‚Äôs moves and hence always have perfect information
about the game state after every round.
With GDL the organisers of the AAAI GGP Competition sought a high-level game
specification language that admits a purely declarative reading. Thus, it enables general
game-playing systems to reason about the rules of a previously unknown game, for example, in order to extract game-specific knowledge or to automatically design evaluation
functions. This proved to be successful as these problems have been studied extensively in
the recent past (Kuhlmann et al., 2006; Kaiser, 2007; Clune, 2007; SchiÔ¨Äel & Thielscher,
2007; Edelkamp & Kissmann, 2008; SchiÔ¨Äel & Thielscher, 2009; Ruan, van der Hoek, &
Wooldridge, 2009; SchiÔ¨Äel, 2010; Thielscher & Voigt, 2010). GDL is a purely declarative
language in the tradition of AI Planning languages and in fact can be seen as a multi-agent
extension thereof since existing planning languages always describe a problem from the perspective of a single agent‚Äîeven in case of adversarial planning (Jensen & Veloso, 2000).
The presence of other agents that have their own actions and goals is crucial since reasoning
about their intentions is the basis for Opponent Modelling, a central aspect in which GGP
goes beyond AI Planning (Genesereth et al., 2005). What GDL inherits from existing planning languages is the compactness of specifications, which contrasts it with other encoding
techniques, for instance the use of propositionalised graphs (La Mura, 2000).
Despite steady progress, the current state of the art in General Game Playing is limited
to deterministic games with complete information about the game state, owing to the restricted expressiveness of original GDL. This covers a variety of classic games such as Chess,
Go, Chinese Checkers etc. but excludes games with elements of chance like Backgammon;
games with information asymmetry such as Bridge or Poker; and games that involve private communication among cooperating players like in Bughouse Chess, or negotiations
like in Diplomacy. Moreover, envisaged applications for General Game Playing systems,
such as automated trading agents (Thielscher & Zhang, 2010), are usually characterised by
imperfect information.
In this paper, we lay the foundations for truly general game-playing systems by developing and analysing an extension of the existing description language to GDL-II (for ‚ÄúGame
172

Representing and Reasoning About General Games

Description Language with Imperfect/Incomplete Information‚Äù).1 GDL-II will allow for
the description of any extensive-form game with finitely many players and finitely many
legal moves in each state but where moves can be nondeterministic and players can have
imperfect and asymmetric information. Although GDL-II will be based on the assumption that the game rules themselves are fully known to each player, incomplete-information
games‚Äîin the game-theoretic sense (Rasmusen, 2007), that is, where players do not know
exactly their own goal or what type the opponents are‚Äîcan be modelled in our extended
language via the standard Harsanyi (1967) transformation of adding an unobserved move
by nature at the beginning.
1.2 Reasoning With the Rules of General Games
Game descriptions in GDL-II will include a precise specification of the information that
players get throughout a game. But the rules of a game do not imply anything about
how players use this information, that is, which conclusions they draw from it, how they
combine it with what they already know, or whether they memorise it. These are separate
questions that concern the reasoning that game-playing agents perform on the basis of the
rules of a game. The clear separation of these two issues means that, for example, ‚Äúperfect
recall‚Äù (Rasmusen, 2007) is a property of individual players and depends on the reasoning
mechanism implemented on them, rather than being a property of a game itself.
In basic GDL, where players are always informed about each other‚Äôs moves, reasoning
with the rules of a game is rather straightforward: a simple, Prolog-like inference engine
suÔ¨Éces to compute one‚Äôs own and everyone else‚Äôs legal moves and to maintain a complete
state description throughout the match (Genesereth et al., 2005). In contrast, playing
arbitrary games with incomplete state knowledge poses an intricate reasoning challenge for
general game-playing systems: Imperfect information and information asymmetry require a
player to draw conclusions about his own percepts and what they entail about the current
position, about his and everybody else‚Äôs possible moves, as well as about what the other
players may know.
Action formalisms like the classic Situation Calculus (McCarthy, 1963) have been developed for precisely this purpose. They are readily available, but to deploy them in General
Game Playing presupposes a proper translation from GDL-II into an existing, suitably expressive action language. Therefore, in the second part of this paper we address the problem
of reasoning about GDL-II by presenting a full embedding of our extended general game
description language into a suitably extended variant of the Situation Calculus.
1.3 Overview of Results
Our specific contributions in this paper to the foundations for General Game Playing can
be summarised as follows:
1. We show that the addition of just two more keywords to GDL suÔ¨Éces to obtain the
desired generality of the game description language: The first, called sees, is used
1. In an unfortunate clash of terminology, an agent who does not know the full state of the environment
is said to have incomplete information in AI, whereas in Game Theory, if players do not know the full
state when it is their turn, then the game is said to be of imperfect information.

173

Schiffel & Thielscher

to control the information that each player gets. The second, called random, denotes
a special player who chooses moves randomly.
2. We develop a new execution model for GDL-II and demonstrate how‚Äîdespite the
conceptual simplicity of the representation language‚Äîthe operational semantics gives
rise to an intricate epistemic model, which provides players with suÔ¨Écient information
to enable them to reason about their own knowledge and the knowledge of their
opponents, to predict how their knowledge will evolve, and to reason about what
players know about other players‚Äô knowledge.
3. We develop a new communication protocol for GDL-II to address practical issues that
arise in General Game Playing with imperfect information.
4. We investigate the expressive power of our high-level knowledge representation language by relating it to the mathematical concept of extensive-form games (Rasmusen,
2007).
5. We extend the Situation Calculus that includes Scherl and Levesque‚Äôs knowledge fluent (Scherl & Levesque, 2003) with multi-agent knowledge, simultaneous moves, and
the new concept of derived action predicates.
6. We present a mapping by which GDL-II is fully embedded into this extended Situation
Calculus, and we formally prove that this provides for a correct axiomatic account of
the semantics of GDL-II, which enables players to draw conclusions about their own
and the other players‚Äô knowledge in past, present, and future game states.
The paper proceeds as follows. In the next section, we introduce GDL-II as an extension of GDL with randomness and imperfect information, and we use various examples to
demonstrate the range of phenomena and games that can be described in this new language.
We also develop a modified execution model for GDL-II. In Section 3, we show that the
language is suÔ¨Éciently expressive to give rise to an intricate multi-agent epistemic model,
and we relate our language to that of extensive-form games. In Section 4, we present a
formal embedding of this language into the Situation Calculus and prove the correctness of
this translation. We discuss related work in Section 5 and conclude in Section 6.

2. Describing General Imperfect-Information Games With Randomness
General Game Playing requires a formal language for describing the rules of arbitrary games.
Any complete game description needs to provide
‚Ä¢ the names of the players,
‚Ä¢ the initial position,
‚Ä¢ the legal moves and how they aÔ¨Äect the position, and
‚Ä¢ the terminating and winning criteria.
174

Representing and Reasoning About General Games

role(R)
init(F)
true(F)
legal(R,M)
does(R,M)
next(F)
terminal
goal(R,V)
sees(R,P)
random

R is a player
F holds in the initial position
F holds in the current position
R can do move M in the current position
player R does move M
F holds in the next position
the current position is terminal
R scores V points in the current position
R perceives P in the next position
the random player

Table 1: GDL-II keywords. Standard GDL comprises the top eight. The keywords are
accompanied by the auxiliary, pre-defined predicate distinct(X,Y), meaning
the syntactic inequality of the two arguments (Love et al., 2006).

The description language GDL has been developed for this purpose (Genesereth et al.,
2005; Love, Hinrichs, Haley, Schkufza, & Genesereth, 2006). The emphasis is on high-level,
declarative game rules that are easy to understand and maintain. At the same time, GDL
has a precise semantics and is fully machine-processable. Moreover, background knowledge
is not required‚Äîa set of rules is all a player needs to know in order to be able to play a
previously unknown game.
GDL is based on the standard syntax and semantics of Logic Programming. A few
special keywords are used for the diÔ¨Äerent elements of a game description mentioned above.
The original game description language GDL uses the first eight keywords shown in Table 1.
This version of GDL is suitable for describing any finite, synchronous, and deterministic
n-player game (Genesereth et al., 2005).2 The execution model entails complete state
information: The initial position is fully specified and the players are immediately informed
about each other‚Äôs moves, with all (joint) moves being deterministic.
Although GDL was developed for complete-information games only, a surprisingly simple
extension to its syntax suÔ¨Éces to generalise it to arbitrary (discrete and finite) games with
information asymmetry and random moves.
1. The new keyword random is introduced as a special role.
It is assumed that this ‚Äúplayer‚Äù always makes a purely random choice among its legal
moves in a position. This allows the game designer to describe elements of chance,
such as rolling dice or shuÔ¨Ñing cards.
2. The second new keyword sees(R,P) is introduced for specifying the conditions
under which player R receives information (i.e., ‚Äúperceives‚Äù) P.
To ensure greatest flexibility, arbitrary terms may serve as percepts P. Not only does
this allow a game designer to have players observe specific state features or actions,
but games may also feature rules according to which players are informed about logical
2. Synchronous means that all players move simultaneously. In this setting, turn-taking games are modelled
by allowing players only one legal move, without eÔ¨Äect, if it is not their turn.

175

Schiffel & Thielscher

combinations of conditions or receive partial information about a state feature or a
move by another player.
The language extension will be accompanied by a modified execution model, in which
players are no longer informed about each other‚Äôs moves by default; rather, they only
get to see what the game rules entail about their percepts.
We again refer to Table 1 with a list of all the keywords of the new language GDL-II.
2.1 Examples
Prior to giving the precise definition of syntax and semantics we illustrate the expressiveness
of this extended Game Description Language with several examples; the first and third will
recur later in the paper.
2.1.1 Example 1 (Krieg-Tictactoe)
The rules in Figure 1 describe standard Tic-Tac-Toe but with the twist that the players
cannot see their opponent‚Äôs moves, just like in the chess variant Kriegspiel (Pritchard, 1994),
hence the name.3
The two roles and the initial position are given in lines 1‚Äì2 and lines 4‚Äì7, respectively.
The moves are specified by the rules with head legal (lines 9‚Äì14): The player whose turn
it is can attempt to mark any cell that he does not already own or has tried earlier in the
game. The other player can only do noop , a move without eÔ¨Äect.
The position update is specified by the rules with head next (lines 18‚Äì29): If the
submitted move mark (M, N ) by player R is not valid (where ‚Äúvalid‚Äù means that the
targeted cell is still blank; cf. line 16) then every feature of the current position continues
to hold, and the only change in the overall state is that tried (R, M, N ) becomes true.4
Otherwise the cell with coordinates (M, N ) will be marked while none of the other cells
change. Moreover, control goes to the opponent.
The clauses with head sees (lines 31‚Äì33) say that the player whose turn it is will always
be informed about this. This suÔ¨Éces because when a player does not perceive yourmove ,
then it follows that it must be his opponent‚Äôs turn.
!
2.1.2 Example 2 (Coloured Trails)
This class of games is a popular research test-bed for decision-making and negotiation in a
competitive setting (Grosz, Kraus, Talman, Stossel, & Havlin, 2004). Each specific game
comes with one or more fixed protocols defining possible interactions among the players.
3. A word on the notation: In this paper we will be largely concerned with the semantics behind GDL
game descriptions, which are interpreted as logic programs. For this reason we will use the standard
Prolog syntax for GDL, where variables are indicated by uppercase letters. This is in contrast to the
more customary KIF-notation of GDL introduced by Genesereth et al. (2005). The KIF-syntax also
allows for disjunctions in clause bodies, but these can be easily transformed into normal logic program
clauses (Lloyd, 1987). Throughout the paper, we will use ‚Äúclause‚Äù and ‚Äú(game) rule‚Äù interchangeably.
4. It is important to note the diÔ¨Äerence between legal and valid moves in Krieg-Tictactoe: Each attempt to
mark a cell is considered legal, but only those moves are accepted as valid that are actually possible in
the current position. Feature tried (R, M, N ) is used to prevent a player from resubmitting a previously
rejected move.

176

Representing and Reasoning About General Games

1
2

role(xplayer).
role(oplayer).

3
4
5
6
7

init(control(xplayer)).
init(cell(1,1,b)).
init(cell(1,2,b)).
init(cell(2,1,b)).
init(cell(2,2,b)).
init(cell(3,1,b)).
init(cell(3,2,b)).

init(cell(1,3,b)).
init(cell(2,3,b)).
init(cell(3,3,b)).

8
9
10
11
12
13
14

legal(xplayer,mark(M,N)) :- true(control(xplayer)), true(cell(M,N,Z)),
distinct(Z,x), not true(tried(xplayer,M,N)).
legal(oplayer,mark(M,N)) :- true(control(oplayer)), true(cell(M,N,Z)),
distinct(Z,o), not true(tried(oplayer,M,N)).
legal(xplayer,noop)
:- true(control(oplayer)).
legal(oplayer,noop)
:- true(control(xplayer)).

15
16

validmove :- does(R,mark(M,N)), true(cell(M,N,b)).

17
18
19

next(F)
:- not validmove, true(F).
next(tried(R,M,N)) :- not validmove, does(R,mark(M,N)).

20
21
22
23
24
25
26
27
28
29

next(cell(M,N,x))
next(cell(M,N,o))
next(cell(M,N,Z))

:- validmove, does(xplayer,mark(M,N)).
:- validmove, does(oplayer,mark(M,N)).
:- validmove, true(cell(M,N,Z)),
does(R,mark(I,J)), distinct(M,I).
next(cell(M,N,Z))
:- validmove, true(cell(M,N,Z)),
does(R,mark(I,J)), distinct(N,J).
next(control(oplayer)) :- validmove, true(control(xplayer)).
next(control(xplayer)) :- validmove, true(control(oplayer)).
next(tried(R,M,N))
:- validmove, true(tried(R,M,N)).

30
31
32
33

sees(R,
yourmove) :- not validmove, true(control(R)).
sees(xplayer,yourmove) :validmove, true(control(oplayer)).
sees(oplayer,yourmove) :validmove, true(control(xplayer)).

Figure 1: A GDL-II description of ‚ÄúKrieg-Tictactoe.‚Äù The game positions are encoded
using the three features control (R), cell(M, N, Z), and tried (R, M, N ), where
R ‚àà {xplayer , oplayer }; M, N ‚àà {1, 2, 3}; and Z ‚àà {x, o, b}, with b meaning
‚Äúblank.‚Äù For the sake of simplicity, we have omitted the (straightforward) specification of both the terminating conditions and the goal values.

For example, a simple negotiation may consist of player R oÔ¨Äering player S to exchange
two of their chips, C and D. This can be formalised by these GDL-II clauses:
legal(R,propose(S,tradeChips(C,D))) :true(hasChip(R,C)), true(hasChip(S,D)), distinct(R,S).
sees(S,offer(R,C,D)) :- does(R,propose(S,tradeChips(C,D))).
Under these rules the communication is private: only the addressee gets to see the oÔ¨Äer. !
177

Schiffel & Thielscher

2.1.3 Example 3 (Monty Hall)
As our second running example, the rules in Figure 2 describe a simple but famous game
where a car prize is hidden behind one of three doors and where a candidate is given two
chances to pick a door. The host is modelled by the new pre-defined role random.5
Lines 1‚Äì7 introduce the players‚Äô names and the state features that hold initially. The
possible moves are specified in lines 9‚Äì17: First, the random player decides where to place
the car and, simultaneously, the candidate chooses a door. Next, the host (i.e., random)
opens a door that is not the one with the car behind it nor the one that the candidate has
chosen. Finally, the candidate can either stick to her earlier choice (noop ) or switch to the
other door that is still closed.
The candidate‚Äôs only percept throughout the game, viz. the door that gets opened, is
defined by the rule with head sees (line 19). The remaining clauses specify the position update (lines 21‚Äì31), the fact that the game ends after the candidate‚Äôs final decision
(line 33), and the payoÔ¨Ä for the candidate depending on whether she got the door right in
the end (lines 35‚Äì36).
!
2.1.4 Example 4 (Poker)
Together the two new keywords can be used to describe all kinds of card games, which are
typically characterised by both randomness (shuÔ¨Ñing) and information asymmetry (individual hands). A single card dealt face down to a player, say, can be axiomatised thus:
legal(random,deal(P,C)) :- role(P), in_deck(C),
distinct(P,random).
next(in_hand(P,C))
:- does(random,deal(P,C)).
sees(P,your_card(C)) :- does(random,deal(P,C)).
Here, only the player who is dealt the card can see it. Multiple cards can be handed out
in a single move which takes each card as a separate argument and of which players only
get to see the argument positions that correspond to their cards. In contrast, a card dealt
face-up, as in Texas Hold‚Äôem, would be axiomatised as follows.
legal(random,deal_river(C)) :- in_deck(C).
next(river(C))
:- does(random,deal_river(C)).
sees(P,river(C)) :- does(random,deal_river(C)), role(P).
Here, all players are informed about the river card.

!

The example game descriptions illustrate the two new features in GDL-II: The special
role random is used to model nature, who always moves randomly. The keyword sees
controls the information that players have about the game state. Although all players have
full knowledge of the game rules including the initial state, both imperfect and asymmetric
knowledge about later states are a natural consequence of players‚Äô individual and limited
percepts.
5. Although random is a pre-defined constant, it needs to be declared as a role (line 2) with his own goal
(line 37). In this way GDL-II supports upward compatibility with the original GDL.

178

Representing and Reasoning About General Games

1
2

role(candidate).
role(random).

3
4
5
6
7

init(closed(1)).
init(closed(2)).
init(closed(3)).
init(step(1)).

8
9
10
11
12

legal(random,hide_car(D)) :- true(step(1)), true(closed(D)).
legal(random,open_door(D)) :- true(step(2)), true(closed(D)),
not true(car(D)), not true(chosen(D)).
legal(random,noop)
:- true(step(3)).

13
14
15
16
17

legal(candidate,choose(D))
legal(candidate,noop)
legal(candidate,noop)
legal(candidate,switch)

::::-

true(step(1)), true(closed(D)).
true(step(2)).
true(step(3)).
true(step(3)).

18
19

sees(candidate,D) :- does(random,open_door(D)).

20
21
22
23
24
25
26

next(car(D))
next(car(D))
next(closed(D))
next(chosen(D))
next(chosen(D))
next(chosen(D))

::::::-

27

does(random,hide_car(D)).
true(car(D)).
true(closed(D)), not does(random,open_door(D)).
does(candidate,choose(D)).
true(chosen(D)), not does(candidate,switch).
does(candidate,switch),
true(closed(D)), not true(chosen(D)).

28
29
30
31

next(step(2)) :- true(step(1)).
next(step(3)) :- true(step(2)).
next(step(4)) :- true(step(3)).

32
33

terminal :- true(step(4)).

34
35
36
37

goal(candidate,100) :- true(chosen(D)), true(car(D)).
goal(candidate, 0) :- true(chosen(D)), not true(car(D)).
goal(random,
0).

Figure 2: A GDL-II description of the Monty Hall game (Rosenhouse, 2009).
2.2 Formal Syntax
GDL-II is based on the standard syntax of logic programs, including negation.
Definition 1
Consider a signature with function symbols (including constants) and variables.
‚Ä¢ A term is either a variable, or a function symbol with terms as arguments.
‚Ä¢ An atom is a predicate symbol with terms as arguments.
‚Ä¢ A literal is an atom or its negation.
179

Schiffel & Thielscher

‚Ä¢ A clause is of the form h :- b1 , . . . , bn ., where h (the head) is an atom and
b1 , . . . , bn (the body) are literals (n ‚â• 0), with the meaning that b1 , . . . , bn together
imply h.
GDL-II imposes the following restrictions on the use of the pre-defined keywords in Table 1.
Definition 2
The dependency graph for a set G of clauses is a directed, labeled graph whose nodes
+
q if G
are the predicate symbols that occur in G and where there is a positive edge p ‚Üí
‚àí
‚Éó
contains a clause p(‚Éós) :- . . . , q(t ), . . . ., and a negative edge p ‚Üí q if G contains a clause
p(‚Éós) :- . . . , ¬¨q(‚Éót ), . . . . We say that p depends on q in G if there is a path from p to q
in the dependency graph of G.
A GDL-II game description is a finite set of clauses where
‚Ä¢ role only appears in facts (i.e., clauses with empty body) or in the body of clauses;
‚Ä¢ init only appears in the head of clauses and does not depend on any of true,
legal, does, next, terminal, or goal;
‚Ä¢ true only appears in the body of clauses;
‚Ä¢ does only appears in the body of clauses, and none of legal, terminal, or goal
depends on does;
‚Ä¢ next and sees only appear in the head of clauses;
‚Ä¢ distinct only appears in the body of clauses. 6
2.3 Valid Game Descriptions
In order to admit an unambiguous interpretation as a game, GDL-II descriptions must obey
further general syntactic restrictions, all of which are inherited from its predecessor GDL.
Definition 3
To constitute a valid GDL-II description, a set of clauses G must satisfy the following.
1. G is stratified, that is, there are no cycles involving a negative edge in the dependency
graph for G (Apt, Blair, & Walker, 1987; Gelder, 1989).
2. G is allowed, that is, each variable in a clause occurs in at least one positive atom in
the body (Lloyd & Topor, 1986).
3. If p and q occur in a cycle in the dependency graph and G contains a clause
p(‚Éós) :- q1 (‚Éót1 ), . . . , q(v1 , . . . , vk ), . . . , qn (‚Éótn ).
then for every vi ‚àà {v1 , . . . , vk },
‚Ä¢ vi is ground, or
6. The formal meaning of this predicate is given by tacitly assuming the unary clause distinct(s, t)., for
every pair s, t of syntactically diÔ¨Äerent, ground (i.e., variable-free) terms.

180

Representing and Reasoning About General Games

‚Ä¢ vi is an element of ‚Éós, or

‚Ä¢ vi is an element of some ‚Éótj (1 ‚â§ j ‚â§ n) such that qj does not appear in a cycle
with p.
The last condition imposes a restriction on the combination of function symbols and recursion to ensure finiteness and decidability of all relevant derivations (Love et al., 2006).
The reader is invited to verify that our example game descriptions in Figure 1 and 2 satisfy
all requirements of a valid GDL-II description. The syntactic restrictions ensure that a set
of game rules can be eÔ¨Äectively and unambiguously interpreted by a state transition system
as a formal game model, which we will describe next.
2.4 Semantics
A unique game model is obtained from a valid GDL-II game description using the concept
of a stable model of a logic program with negation (Gelfond & Lifschitz, 1988).
Definition 4
For a set of clauses G and a set of ground atoms M, let GM be the set of negation-free
implications h ‚äÇ b1 ‚àß . . . ‚àß bk obtained by taking all ground instances of clauses in G and
‚Ä¢ deleting all clauses with a negative body literal ¬¨bi such that bi ‚àà M,
‚Ä¢ deleting all negative body literals from the remaining clauses.
Then M is a stable model for G if and only if M is the least model for GM .
A useful property of stable models is to provide a unique model whenever the underlying
set of clauses is stratified (Gelfond & Lifschitz, 1988), which is a requirement of valid GDL-II
specifications according to Definition 3. Moreover, the syntactic restrictions in GDL-II
ensure that this model is finite for all logic programs we consider‚Äîa property inherited
from original GDL (Love et al., 2006). Hence, for the following game model for GDL-II we
can assume a finite set of players, finite states, and finitely many legal moves in each state.
We shall denote by SM [G] the unique stable model for stratified set of clauses G.
Specifically, then, the derivable instances of keyword role(R) from a given game
description define the players. The initial state is composed of the derivable instances of
init(F). In order to determine the legal moves in any given state, this state has to be
encoded first, using the keyword true. Let, to this end, œÉ = {f1 , . . . , fn } be a state (i.e.,
a finite set of ground terms, a.k.a. fluents), then the game description G is augmented by
the n facts
def
œÉ true = { true(f1 ).
...
true(fn ).}
Those instances of legal(R,M) that are derivable from G ‚à™ œÉ true define all legal
moves M for player R in position œÉ. In the same way, the clauses for terminal and
goal(R,V) define termination and goal values relative to the encoding of a given
position.

Example 2 (continued)
The rules in Figure 1 entail that Krieg-Tictactoe features the two roles xplayer and oplayer .
181

Schiffel & Thielscher

The initial position is œÉ0 = {control (xplayer ), cell (1, 1, b), . . . , cell (3, 3, b)}. With œÉ0 true
added to the rules we can infer legal(xplayer , mark (M, N )) for each combination of
coordinates M, N ‚àà {1, 2, 3}, while oplayer only has the one move noop .
!
Determining a position update and the percepts of the players requires the encoding of
both the current position and a move by each player. Suppose joint move ¬µ is such that
players r1 , . . . , rk take moves m1 , . . . , mk , then let
def

¬µdoes = { does(r1 , m1 ).

...

does(rk , mk ). }

Taken together, all instances of next(F) that are derivable from G‚à™¬µ does ‚à™œÉ true compose
the updated position. Similarly, the derivable instances of keyword sees(R,P) describe
what a player perceives when joint move ¬µ is played in position œÉ.
Example 4 (continued)
According to the rules in Figure 2, the initial position in the Monty Hall game is given
by œÉ0 = {closed (1), closed (2), closed (3), step(1)}. There are three legal moves for each
player in this state, viz. hide car (D) for random and choose (D) for candidate , where
D ‚àà {1, 2, 3}. Consider, say, ¬µ1 = {random (‚Üí hide car (1), candidate (‚Üí choose(3)}, then
with œÉ0 true and ¬µdoes
added to the game rules, the clauses for keyword next determine
1
the updated state œÉ1 = {car (1), chosen (3), closed (1), closed (2), closed (3), step(2)}. According to the only clause for keyword sees the candidate perceives nothing at this stage and
hence cannot know that the car is hidden behind door 1.
!
All of the above is summarised in the following definition.
Definition 5
Let G be a valid GDL-II description. The game semantics of G is the state transition
system (R, œÉ0 , T, l, u, I, g) given by
‚Ä¢ roles R = {r : role(r) ‚àà SM[G]};
‚Ä¢ initial position œÉ0 = {f : init(f ) ‚àà SM[G]};
‚Ä¢ terminal positions T = {œÉ : terminal ‚àà SM[G ‚à™ œÉ true ]};
‚Ä¢ legal moves l = {(r, m, œÉ) : legal(r, m) ‚àà SM[G ‚à™ œÉ true ]};
‚Ä¢ state update function u(¬µ, œÉ) = {f : next(f ) ‚àà SM[G ‚à™ œÉ true ‚à™ ¬µdoes ]}, for all joint
moves ¬µ and states œÉ;
‚Ä¢ information relation I = {(r, ¬µ, œÉ, p) : sees(r, p) ‚àà SM[G ‚à™ œÉ true ‚à™ ¬µdoes ]};
‚Ä¢ goal relation g = {(r, v, œÉ) : goal(r, v) ‚àà SM[G ‚à™ œÉ true ]}.
182

Representing and Reasoning About General Games

2.5 A New Execution Model
The additional elements in GDL-II and the modified semantics require a new execution
model for games with incomplete state information and randomness: Starting in the initial
position, in each state œÉ each player chooses a legal move. The special random role is
assumed to choose randomly with uniform probability among its legal moves.7 Given a
joint move ¬µ the game state changes to u(¬µ, œÉ). In contrast to the execution model for
GDL (Genesereth et al., 2005; Love et al., 2006; SchiÔ¨Äel & Thielscher, 2010), the players
are not informed about the joint move; rather each role r ‚àà R \ {random} gets to see any
p that satisfies I(r, ¬µ, œÉ, p). The game ends as soon as a terminal state is reached, and
then the goal relation determines the result for the players. This modified execution model
is spelled out in Figure 3. It is straightforwardly implemented on a Game Master, which
runs a game by collecting all moves, which allows it to maintain the actual game state and
thus to compute all percepts and also to determine the end of a match and the resulting
goal values for the players.
1. Send each r ‚àà R \ {random} the GDL-II description and inform them about
their individual roles r (e.g., xplayer or oplayer ). Set œÉ := œÉ0 .
2. After the appropriate time, collect a move mr from each player r ‚àà R\{random}
and, in case random ‚àà R, choose with uniform probability an element m random
from the set {m : (random, m, œÉ) ‚àà l}. Set ¬µ := {r1 (‚Üí mr1 , . . . , rk (‚Üí mrk }.
3. Send each r ‚àà R \ {random} the set of percepts {p : (r, ¬µ, œÉ, p) ‚àà I}. Set œÉ :=
u(¬µ, œÉ).
4. Repeat 2. and 3. until œÉ ‚àà T . Determine the result v for r ‚àà R by (r, v, œÉ) ‚àà g.
Figure 3: Game play in GDL-II given a game (R, œÉ0 , T, l, u, I, g).
2.6 A New Communication Protocol
The changes in the execution model from GDL to GDL-II require some modifications to the
communication protocol between the Game Master program and the general game players.
First of all, the Game Master has to inform the players about their individual percepts
instead of the joint move from the previous step. For practical purposes, the Game Master
should also confirm back each individual move as well as the current step in the game.
This information is useful for players to become aware of communication problems (such as
dropped messages or timeouts) and to be able to recover from those problems at least in
some cases. In order to make the transition from GDL to GDL-II easier, we keep most of
the communication protocol as defined by Love et al. (2006) and only apply the necessary
changes.
7. Note that this does not necessarily mean that all resulting states have equal probability. For example,
tossing an unfair coin that shows head just with probability 13 may be axiomatised in GDL-II by three
legal moves for random, two of which have the same eÔ¨Äect (the coin showing tails). We stress that basic
GDL-II could easily be extended by syntactic means for specifying non-uniform transition probabilities.

183

Schiffel & Thielscher

The communication between the Game Master and the players happens through HTTP
messages, where the players take the role of HTTP servers that ‚Äúserve‚Äù moves to the
Game Master. The body of the HTTP messages are commands that are encoded using the
Knowledge Interchange Format (KIF) (Genesereth, 1991). We use the messages START,
PLAY and STOP as follows.
(START <MATCHID> <ROLE> <DESCRIPTION> <STARTCLOCK> <PLAYCLOCK>)
This is the first message sent to each player. It contains a unique identifier for the
match (<MATCHID>) and informs the player about his role in the game (<ROLE>), the
game rules (<DESCRIPTION>) as well as the time constraints for the start-up phase
(<STARTCLOCK>) and for submitting moves (<PLAYCLOCK>). Players are supposed
to reply to this message with the string READY within <STARTCLOCK> seconds.
This message is identical to the start message in the original GDL communication
protocol.
(PLAY <MATCHID> <TURN> <LASTMOVE> <PERCEPTS>)
This message is sent to a player at each step of the match. It informs the player about
the number of moves so far in the game (<TURN>), his last move (<LASTMOVE>), and
his percepts (<PERCEPTS>) according to the information relation I .

<TURN> is 0 for the first play message and increased by one for every subsequent step
of the game. For the first play message, when there is no previous move, <LASTMOVE>
is NIL.
Players are supposed to reply within <PLAYCLOCK> seconds with their next move.
If they do not submit a legal move on time, the Game Master will make a random
selection on their behalf. In this case the <LASTMOVE> argument of the next message,
which informs the player about that move, is vital knowledge to be able to continue.

(STOP <MATCHID> <TURN> <LASTMOVE> <PERCEPTS>)
This message is sent to each player for the last step of the match, that is, when a
terminal state has been reached. It has the same structure as the play message above.
Players should reply to this message with the string DONE.
Note that the parameter <LASTMOVE> in the play and stop messages is necessary if we
want to guarantee that players always know their own move in case of dropped messages
or timeouts. It is worth stressing that this information cannot be provided through a game
rule like sees(R,lastmove(M)) :- does(R,M). This clause is redundant in the light of the
execution model for GDL-II in Figure 3, which implies that players always choose, and
hence know, their own moves anyway.

3. The Expressive Power of GDL-II
Percepts in GDL-II can be arbitrary terms and are triggered by arbitrary conditions on the
current state and joint move. This provides greater flexibility than the standard definition
of a sensing action in planning domain description languages, by which an agent learns
about the truth values of one or more fluents (Golden & Weld, 1996; Petrick & Bacchus,
184

Representing and Reasoning About General Games

2004). Observations in GDL-II need not be related to one‚Äôs own move, which is especially
appropriate in a multi-agent setting, where often players see something as a side-eÔ¨Äect of
someone else‚Äôs actions. A point in case are the rules 32 and 33 in Figure 1. This follows
the spirit behind general eÔ¨Äect axioms in GDL, that is, rules for next, which also may be
independent of a specific move. A simple example are the rules 29‚Äì31 in Figure 2.
GDL-II descriptions are solely concerned with providing the players with an objective
description of the rules of a game. This puts it in contrast also to standard axiomatisations
in action formalisms (Lakemeyer & Levesque, 1998; Thielscher, 2000; Scherl & Levesque,
2003; Forth & Shanahan, 2004), where observations are described in terms of how they
aÔ¨Äect the knowledge of an agent. Our game description language is simpler in this regard
because it is agnostic about how players use a percept to draw inferences, combine it with
what they already know, and whether they remember it.
The conceptual simplicity of describing observations in GDL-II notwithstanding, our
language extension gives rise to an intricate epistemic model about what a player can,
in principle, know of a position and of the other players‚Äô knowledge. For example, the
GDL-II rules for Krieg-Tictactoe shown in Figure 1 imply that one cell will carry xplayer ‚Äôs
mark after the first round but oplayer will be unable to determine which one. Moreover,
provided that they have perfect recall (Rasmusen, 2007) and are capable of drawing the right
conclusions, both players should know about each other‚Äôs knowledge and lack of knowledge,
respectively. In the following, we will analyse formally what the semantics of a GDL-II
description and the execution model of Figure 3 entail about the knowledge that a player
with perfect reasoning capabilities can have at any point during game play.
3.1 Legal Play Sequences
We begin by defining the set of all possible ways in which a game can develop.
Definition 6
Consider a game (R, œÉ0 , T, l, u, I, g). A legal play sequence is
¬µ1

¬µ2

¬µn

œÉ0 ‚Üí œÉ1 ‚Üí . . . œÉn‚àí1 ‚Üí œÉn
where n ‚â• 0 and for all i ‚àà {1, . . . , n},
‚Ä¢ ¬µi is a joint move, i.e., a mapping from players r to their individual move, ¬µi (r);
‚Ä¢ (r, ¬µi (r), œÉi‚àí1 ) ‚àà l for all r ‚àà R (that is, players make legal moves); and
‚Ä¢ œÉi = u(¬µi , œÉi‚àí1 ) (position update).
Furthermore, {œÉ0 , . . . , œÉn‚àí1 } ‚à© T = ‚àÖ, that is, only the last state may be terminal.
The following definition characterises precisely what a player with perfect reasoning
capabilities can in principle know at a specific stage in the course of a game.
Definition 7
¬µ‚Ä≤
¬µ‚Ä≤
¬µ‚Ä≤n ‚Ä≤
¬µ1
¬µ2
¬µn
‚Ä≤
‚Üí
œÉn be two legal play
Let Œ¥ = œÉ0 ‚Üí œÉ1 ‚Üí . . . œÉn‚àí1 ‚Üí œÉn and Œ¥‚Ä≤ = œÉ0 ‚Üí1 œÉ1‚Ä≤ ‚Üí2 . . . œÉn‚àí1
sequences in a game with roles R, initial state œÉ0 , and information relation I . A player
r ‚àà R \ {random} cannot distinguish Œ¥ from Œ¥ ‚Ä≤ if, and only if, the following holds for all
i ‚àà {1, . . . , n}.
185

Schiffel & Thielscher

‚Ä≤ , p‚Ä≤ ) ‚àà I} and
1. {p : (r, ¬µi , œÉi‚àí1 , p) ‚àà I} = {p‚Ä≤ : (r, ¬µ‚Ä≤i , œÉi‚àí1

2. ¬µi (r) = ¬µ‚Ä≤i (r).
Example 2 (continued)
Let œÉ1 be the initial state in Krieg-Tictactoe, where all cells are empty, and consider these
two legal play sequences:
Œ¥ = œÉ0

{xplayer $‚Üímark (1,3),oplayer $‚Üínoop}

Œ¥ ‚Ä≤ = œÉ0

{xplayer $‚Üímark (2,2),oplayer $‚Üínoop}

‚Üí
‚Üí

œÉ1

{xplayer $‚Üínoop,oplayer $‚Üímark (2,2)}

‚Üí

{xplayer $‚Üínoop,oplayer $‚Üímark (2,2)}
œÉ1‚Ä≤
‚Üí

œÉ2

(1)

œÉ2‚Ä≤

(2)

Role xplayer can distinguish Œ¥ from Œ¥‚Ä≤ already after the first round because of his diÔ¨Äerent
moves. In contrast, oplayer finds the two sequences indistinguishable at this step since he
took the same move, noop , and got the same percept {yourmove} by rule 30 in Figure 1.
Ultimately, however, oplayer also is able to distinguish (1) from (2) because, according to
the game rules, after his second move he perceives { } in Œ¥ but {yourmove} in Œ¥‚Ä≤ (where
he attempted to mark a non-blank cell).
!
(In-)distinguishable play sequences can be used also to ensure that game descriptions
obey desirable properties such as the following two, which are a straightforward consequence
of Definition 7.
1. A game description with roles R and legality relation l entails that all players can
always know their legal moves iÔ¨Ä for all r ‚àà R \ {random} there are no two legal
play sequences Œ¥, Œ¥‚Ä≤ leading to states œÉn , œÉn‚Ä≤ such that
{m : (r, m, œÉn ) ‚àà l} Ã∏= {m‚Ä≤ : (r, m‚Ä≤ , œÉn‚Ä≤ ) ‚àà l}
and Œ¥, Œ¥‚Ä≤ are indistinguishable for r.
2. A game description with roles R, terminal states T , and goal relation g entails
that all players can know both the end of a game and their own result iÔ¨Ä for all
r ‚àà R \ {random} there are no two legal play sequences Œ¥, Œ¥ ‚Ä≤ leading to states œÉn , œÉn‚Ä≤
such that
œÉn ‚àà T, œÉn‚Ä≤ Ã∏‚àà T

or

{œÉn , œÉn‚Ä≤ } ‚äÜ T, {v : (r, v, œÉn ) ‚àà g} Ã∏= {v ‚Ä≤ : (r, v ‚Ä≤ , œÉn‚Ä≤ ) ‚àà g}

and Œ¥, Œ¥‚Ä≤ are indistinguishable for r.
While a ‚Äúfair‚Äù game should always satisfy both of these properties, proving them for a new
game may be diÔ¨Écult in practice since in general this requires checking all possible legal
play sequences. There is an easy way around this, namely, by adding sees-rules that
always inform players explicitly about their legal moves, termination, and goal values. On
the other hand, some games may be especially designed to test the ability of players to
logically infer their legal moves under highly incomplete knowledge.
186

Representing and Reasoning About General Games

Example 4 (continued)
The candidate in our axiomatisation of the Monty Hall game (cf. Figure 2) can always
derive her legal moves and when the game ends. This is easily seen from the fact that she
can determine the value of the step fluent at all times. On the other hand, the candidate
never receives enough information to be able to know her result. For example, she cannot
distinguish these two legal play sequences:
Œ¥ = œÉ0

{candidate$‚Üíchoose door (1),random $‚Üíhide car (1)}

Œ¥‚Ä≤

{candidate$‚Üíchoose door (1),random $‚Üíhide car (2)}

= œÉ0

‚Üí
‚Üí

œÉ1

{candidate $‚Üínoop,random $‚Üíopen door (3)}

‚Üí

{candidate $‚Üínoop,random $‚Üíopen door (3)}
œÉ1‚Ä≤
‚Üí

œÉ2
œÉ2‚Ä≤

But no matter which action the candidate chooses in the end, Œ¥ will result in a diÔ¨Äerent
goal value for her than Œ¥‚Ä≤ .
!
Knowledge at the object level, as we have considered thus far, can be lifted to higher
levels so as to determine what the players can know about each other‚Äôs knowledge. This is
possible because the GDL-II description of a game provides the complete rules to all players,
so that each of them is able to derive which information the other roles get to see under
any (hypothetical) legal play sequence. Formalising this requires constructing a suitable
epistemic structure based on possible play sequences. Consider, to this end, for every Œ¥ the
function KŒ¥ that maps every pair (r, i) onto the set of legal play sequences that player r
at state i cannot distinguish from Œ¥. Meta-level knowledge is then obtained as follows.
Definition 8
‚Ä≤
If the game develops according to Œ¥, then as far as player r knows, all functions KŒ¥ are
possible where Œ¥‚Ä≤ is indistinguishable from Œ¥ for r.
Put in words, meta-level knowledge is characterised by a set of possible sets of legal play
sequences. It follows that a player knows what holds in all K-sets he considers possible.
If, say, a Krieg-Tictactoe match unfolds according to the example sequence Œ¥‚Ä≤ from above
(cf. (2)) then xplayer knows that oplayer knows that cell (2, 2) is marked.
This process can be iterated inductively to determine arbitrary levels of meta-knowledge,
which shows that common knowledge of the game rules does not necessarily mean common
knowledge of other players. For example, it is possible to obtain a situation in a 3-player
game where player A knows about a property f and where player B knows that A knows
about f , while player C considers it possible that A knows nothing about f : Just let
A make a move by which he learns about f in a game where B (but not C ) is always
informed about A‚Äôs moves.
We conclude our analysis of (in-)distinguishable play sequences by proving that GDL-II
provides a true extension of GDL since any game in which agents can derive the complete
state after every round can be easily specified in GDL-II.
Proposition 1 Consider a game with roles R such that random Ã∏‚àà R and where the
only clause for keyword sees is
sees(R1,moves(R,M)) :- role(R1), does(R,M).
Then there are no two legal play sequences Œ¥, Œ¥‚Ä≤ of any length n ‚â• 0 such that Œ¥ and Œ¥‚Ä≤
are indistinguishable for any r ‚àà R.
187

Schiffel & Thielscher

Proof :
By induction on n. For n = 0 there is only one legal play sequence, viz. the
initial game state œÉ0 , which establishes the claim. For the induction step, suppose there
‚Ä≤
such that Œ¥ and Œ¥‚Ä≤ are
are two legal play sequences Œ¥, Œ¥‚Ä≤ leading to states œÉn+1 Ã∏= œÉn+1
indistinguishable for some role r ‚àà R. From Definition 7 it follows that Œ¥, Œ¥‚Ä≤ shortened
by one step are indistinguishable for this player. By the induction hypothesis, this is only
possible if Œ¥ and Œ¥‚Ä≤ are identical up to step n. Hence, the two legal play sequences are of
the form
¬µn+1
¬µn
¬µ1
Œ¥ = œÉ0 ‚Üí . . . ‚Üí œÉn ‚Üí œÉn+1
¬µ1

¬µn

Œ¥ ‚Ä≤ = œÉ0 ‚Üí . . . ‚Üí œÉn

¬µ‚Ä≤n+1

‚Ä≤
‚Üí œÉn+1

‚Ä≤
From Definition 5 it follows that state update is deterministic, hence œÉn+1 Ã∏= œÉn+1
implies
‚Ä≤
¬µn+1 Ã∏= ¬µn+1 . The latter in conjunction with the above clause for sees implies that the
percepts for all roles diÔ¨Äer when in state œÉn joint move ¬µ‚Ä≤n+1 is taken instead of ¬µn+1 .
This contradicts the assumption that Œ¥ and Œ¥‚Ä≤ are indistinguishable for role r, which
establishes the claim.
!

3.2 GDL-II vs. Extensive Form
By virtue of its state-transition semantics and the concept of indistinguishable play sequences, every terminating GDL-II game can be understood as a so-called extensive-form
game. For the following, we assume the reader to be familiar with the basic notion of
a mathematical game tree with imperfect information (Rasmusen, 2007; Leyton-Brown &
Shoham, 2008), which is based on the very general concept of information sets as a model
of partial observability and information asymmetry. The sees(R,P) predicate in GDL-II
can be used equally generally. The main reason is that percepts are not confined to specific
observables (e.g., state features or opponents‚Äô moves) but can be arbitrary symbols used as
identifiers for any desired information partition.
3.2.1 From GDL-II to Extensive-Form Games
For a given GDL-II game, a corresponding extensive-form game can be obtained in two
steps.
1. Using an arbitrary but fixed order of the roles, joint moves in GDL-II are serialised
in such a way that no player is aware of the simultaneous moves by the other players (Rasmusen, 2007).
2. The information sets for a player r are identified with the set of legal play sequences
that this player cannot distinguish.
As an example, Figure 4 depicts the extensive-form game thus obtained from the Monty
Hall game description in Figure 2.
It is important to realise that percepts in GDL-II may sometimes provide information
that is not contained in a state but that nonetheless needs to be taken into account when
partitioning nodes into information sets. The following example illustrates that the ability
to solve a game may depend on such information.
188

Representing and Reasoning About General Games
‚úò!‚ù≥
‚úò
‚ù≥
‚úò‚úò‚úò1 1 1‚ù≥‚ù≥‚ù≥‚ù≥
hide car(3)
‚ù≥‚ù≥‚ù≥
3
3 3
‚úò‚úò
‚ù≥‚ù≥
‚úò
‚ù≥‚ù≥‚ù≥
hide car(2)
‚úò‚úò‚úò
‚úò
‚ù≥‚ù≥‚ù≥!
!‚úò‚úò
!
‚úö‚ù©
‚úö‚ù©
‚úö
‚ù© choose(1)
‚úö
‚ù© choose(3)
..
choose(3)‚úö
choose(1)‚úö
‚ù©
‚ù©
.
‚úö
‚ù©
‚úö
‚ù©
‚úö
‚úö
choose(2)‚ù©
choose(2)‚ù©
!‚úö
!
‚ù©!
!‚úö
!
‚ù©!
‚ú°‚ùè
‚ú°‚ùè
‚ùè
‚ùè open(3)
open(2) open(3)
open(2) ‚ú°
open(3)
open(3)
open(1) ‚ú°
open(1)
1‚ùè
1‚ùè
‚ú°1
‚ú°1
2 ‚ùè!
2 ‚ùè!
!
!‚ú° 2
!
!‚ú° 2
!
!
..
..
..
..
..
..
‚ùè
‚ú°
.
.
.
. ‚ú°
.
.
‚ùè
switch
switch
‚ú°
‚ùè
noop
noop
!‚ú°
!
!
‚ùè!
hide car(1) ‚úò‚úò‚úò

100

0

0

0

1

0

1

100

Figure 4: The Monty Hall game of Figure 2 mapped onto extensive form (with some nodes
and branches omitted). The numbers to the very right indicate which player owns
the nodes of that height (0 = random, 1 = candidate ). Dotted lines connect
nodes in the same information set for the candidate.

3.2.2 Example 5 (Spy & Spy)
The GDL-II in Figure 5 describes a game in which one spy sees which of three coloured
wires is used to arm a bomb. He can signal the name of a colour to a second spy, who
then tries to disarm the bomb. Both win if the second player cuts the right wire and lose
otherwise. Hence, the first spy has every incentive to help the second spy, and to do so
there seems to be one obvious rational move, namely, to signal the colour of the wire.
The crux, however, is that in the game rules, the colour being signalled is logically
independent of the colour of the wire used to arm the bomb. This is illustrated in the
game tree in Figure 6: If the second player were to distinguish only the possible states
themselves after round 2, that is, {armed (red ), step(3)} , {armed (blue), step(3)} , and
{armed (green), step(3)}, then they all would belong to the same information set, no matter
which colour has been signalled. This would leave both players with no better a choice than
moving randomly.
!
3.2.3 From Extensive-Form Games to GDL-II
To describe faithfully in GDL-II (that is, move-by-move) a game given in extensive form,
two issues need to be addressed.
1. The given information sets need to be encoded by appropriate sees-rules for the
individual players. This can be achieved by indexing these sets and always letting
their owner see the index but nothing else.
2. Non-uniform probabilities for moves by nature need to be mapped onto uniform probability distributions for random‚Äôs moves. This can be achieved by introducing a
189

Schiffel & Thielscher

1
2
3

role(spy1).
role(spy2).
role(random).

4
5
6
7

colour(red).
colour(blue).
colour(green).

8
9

init(step(1)).

10
11
12
13

legal(random,arm_bomb(C)) :- colour(C), true(step(1)).
legal(spy1,signal(C))
:- colour(C), true(step(2)).
legal(spy2,cut_wire(C))
:- colour(C), true(step(3)).

14
15
16
17

legal(random,noop) :- not true(step(1))
legal(spy1,noop)
:- not true(step(2))
legal(spy2,noop)
:- not true(step(3))

18
19
20

sees(spy1,C) :- does(random,arm_bomb(C)).
sees(spy2,C) :- does(spy1,signal(C)).

21
22
23
24
25
26
27

next(armed(C))
next(armed(C))
next(explosion)
next(step(2))
next(step(3))
next(step(4))

::::::-

does(random,arm_bomb(C)).
true(armed(C)).
does(spy2,cut_wire(C)), not true(armed(C)).
true(step(1)).
true(step(2)).
true(step(3)).

28
29

terminal :- true(step(4)).

30
31
32
33
34
35

goal(spy1,100) :- not true(explosion).
goal(spy2,100) :- not true(explosion).
goal(spy1, 0) :true(explosion).
goal(spy2, 0) :true(explosion).
goal(random,0).

Figure 5: GDL-II description of a cooperative Spy & Spy game.

proportional number of moves that have diÔ¨Äerent names but lead to the same successor state.
A finite extensive-form game can then be described in GDL-II by using a single fluent
to indicate which node the game is at and by specifying each arc in the tree as a state
transition. Using the methods of Schulte and Delgrande (2004), it can be shown that the
resulting GDL-II description is correct for any finite n-player game in extensive form with
perfect recall. The size of a description resulting from this transformation is obviously of the
same order as the original game tree since moves are not parallelised and states are encoded
as individual objects rather than being factored into atomic features. Hence, unlike the
Monty Hall rules in Figure 2, say, this direct construction does not exploit the conciseness
of descriptions made possible by having a high-level knowledge representation language.
190

Representing and Reasoning About General Games
‚úò!‚ù≥
‚úò
‚ù≥
‚úò‚úò‚úò1 1 1‚ù≥‚ù≥‚ù≥‚ù≥
arm bomb(green)
‚ù≥‚ù≥‚ù≥
3
3 3
‚úò‚úò
‚ù≥‚ù≥
‚úò
‚ù≥‚ù≥‚ù≥
arm bomb(blue)
‚úò‚úò‚úò
‚úò
‚ù≥‚ù≥‚ù≥!
!‚úò‚úò
!
‚úö‚ù©
‚úö‚ù©
‚úö
‚ù© signal(green)
‚úö
‚ù© signal(red)
..
signal(red)
signal(green)
‚úö
‚ù©
‚úö
‚ù©
.
‚úö
‚ù©
‚úö
‚ù©
‚úö
‚úö
signal(blue)‚ù©
signal(blue)‚ù©
!‚úö
!
‚ù©!
!‚úö
!
‚ù©!
arm bomb(red) ‚úò‚úò‚úò

armed(red)
step(3)

armed(red)
step(3)

armed(red)
step(3)

armed(blue)
step(3)

armed(blue)
step(3)

armed(blue)
step(3)

Figure 6: The game of Figure 5 mapped onto extensive form (with some nodes and branches
omitted). Dotted lines connect nodes that spy2 cannot distinguish. If the nodes
at depth 3 with identical states were collapsed onto a single node, then they all
would be in the same information set of this player.

4. A Logic for Reasoning About GDL-II Games
Building a basic general game player, e.g. one that only knows how to move legally, is a
relatively simple task under the restriction to perfect-information games. In the general
case, however, even basic game play is a much more intricate problem. Recall, for example,
the description of the Krieg-Tictactoe game in Figure 1. The rules in lines 31‚Äì33 specify
the players‚Äô percepts, indicating that the player who has control next will be informed
about this fact. This suÔ¨Éces since both players should be able to always derive whether
or not it is their turn. But to do so they must be capable of inferring that when they
do not perceive yourmove , then it must be their opponent‚Äôs turn. Another example of a
(strategically useful) inference would be to conclude that a cell must carry your opponent‚Äôs
marker if you just tried to mark it yourself and you perceive yourmove again, implying
that your attempt must have been unsuccessful.
Drawing conclusions of this sort is the domain of action theories, and the Situation Calculus is the oldest technique for formalising and automating reasoning about actions (McCarthy, 1963). In this second part of the paper, we will lay the foundations for building
general game-playing systems that are capable of reasoning with imperfect information. Using existing techniques like the Situation Calculus for this purpose requires a full embedding
of the game description language GDL-II into these formalisms.
In the following, we will develop such a mapping based on the Situation Calculus variant
that uses a special fluent to represent the knowledge of agents (Scherl & Levesque, 2003).
We will have to slightly modify and further extend this formalism for our purposes. Generally speaking, the Situation Calculus is a predicate logic with a few pre-defined language
elements:
‚Ä¢ constant s0 , which denotes the initial situation, along with constructor Do(A, S) to
denote the situation resulting from doing action A in situation S;
‚Ä¢ predicate Holds(F, S), which denotes that fluent F (i.e., an atomic state feature)
is true in situation S;
191

Schiffel & Thielscher

‚Ä¢ predicate Poss(A, S), which denotes that action A is possible in situation S.
4.1 Compound Actions
In games with two or more players, positions are updated as a consequence of all players
moving simultaneously. For an adequate formalisation in the Situation Calculus we therefore
need to identify the concept of an action with a vector ‚ü®m1 , . . . , mk ‚ü© containing one move for
each player. In a given GDL-II description, the domain of moves is implicitly determined
by the (second) arguments of the keywords legal and does; e.g. mark (M, N ) and
noop in Krieg-Tictactoe. Assuming an arbitrary but fixed order of the players, say as in
(xplayer , oplayer ), we define a simple axiom that identifies the individual move of a player r
in an action vector:
(3)
Act(ri , ‚ü®M1 , . . . , Mi , . . . , Mk ‚ü©) = Mi .
For instance, Act(xplayer , ‚ü®mark (1, 3), noop ‚ü©) = mark (1, 3).
4.2 Derived Action Predicates
Given a GDL-II game description G, we identify as primitive fluents those terms that occur
in the scope of either of the keywords init(F), true(F), or next(F); in Figure 1 these
are control (R), cell (M, N, Z), and tried (R, M, N ). As derived fluents we take all domainspecific predicates that depend on true but not on does in G. Derived fluents (Davis,
1990) do not require their own successor state axioms because their truth-values are fully
determined by the game rules once the values of all primitive fluents are fixed in a (successor)
situation. The keywords terminal and goal(R,V) are treated as derived fluents too.
In addition, a mapping of GDL-II into the Situation Calculus requires the introduction
of the new concept of a derived action predicate. These are the domain-specific predicates
that depend on both true and does in G. An example is validmove in our description
of Krieg-Tictactoe (cf. line 16 in Figure 1). Similar to derived fluents, the truth-value of a
derived action predicate is fully determined by the game rules once we have fixed both the
values of all primitive fluents in a situation and the (compound) action that is being taken
in that situation.
4.3 The Mapping
We now show how any GDL-II description G can be mapped‚Äîin a modular way‚Äîinto a
Situation Calculus theory. First, some atoms that occur in G are rewritten as follows.
‚Éó are replaced by f (X,
‚Éó S) and all derived action predicates
1. All derived fluents f (X)
‚Éó by p(X,
‚Éó A, S), indicating the dependence on a situation S and an action A,
p(X)
8
respectively.
2. Each init(F ) is replaced by Holds(F, s0 ).
3. Each true(F ) is replaced by Holds(F, S).
4. Each next(F ) is replaced by Holds(F, Do(A, S)).
8. This mapping also applies to the derived fluents legal(R, M ) , goal(R, V ) , and terminal.

192

Representing and Reasoning About General Games

5. Each does(R, M ) is replaced by Act(R, A) = M .
As an example, the clause in line 16 of Figure 1 becomes
validmove(A, S) ‚äÇ Act(R, A) = mark (M, N ) ‚àß Holds(cell(M, N, b), S) .
GDL-II game descriptions are based on the negation-as-failure principle, that is, every
proposition that cannot be derived from the game rules is supposed to be false. To reflect
this in the Situation Calculus theory, we use the completion (Clark, 1978) of all clauses in
‚Éó replace the clauses in G with p in the head
the following way: For every predicate p(X),
by
!
‚Éó = ‚Éót ‚àß body .
‚Éó ‚â°
(‚àÉ)X
(4)
p(X)
p(‚Éót ):-body ‚àà G

‚Éó , where Y
‚Éó are all variables that occur
In this context we use (‚àÉ) as abbreviation for ‚àÉY
‚Éó
in either ‚Éót or body but not in X .
4.3.1 Initial Situation
The transformation defined above yields the following axiomatisation of the initial situation:
!
Holds(F, s0 ) ‚â°
(‚àÉ)F = t ‚àß body .
init(t):-body ‚àà G

4.3.2 Preconditions
Based on the completion of legal according to (4), i.e.,
!
(‚àÉ)R = r ‚àß M = m ‚àß body ,
Legal(R, M, S) ‚â°
legal(r,m):-body ‚àà G

we define the precondition axiom for compound actions A in situations S thus:
Poss(A, S) ‚â° ‚àÄR. Legal(R, Act(R, A), S) .

(5)

4.3.3 Effects
As a result of the transformation above, we obtain a general successor state axiom (Reiter,
1991) as follows:
!
(‚àÉ)F = t ‚àß body .
(6)
Holds(F, Do(A, S)) ‚â°
next(t):-body ‚àà G

4.3.4 Knowledge
Scherl and Levesque (2003) use the special fluent K(S ‚Ä≤ , S)‚Äîto be read as: situation S ‚Ä≤ is
accessible from situation S‚Äîin order to axiomatise knowledge states (of an agent) in the
Situation Calculus. We use a straightforward generalisation for the multi-agent case, where
K(R, S ‚Ä≤ , S) expresses that player R considers S ‚Ä≤ a possible situation in S. This allows
us to formalise subjective knowledge similar to Scherl and Levesque thus:
Knows(R, Œ¶, S) = ‚àÄS ‚Ä≤ . K(R, S ‚Ä≤ , S) ‚äÉ Œ¶[S ‚Ä≤ ] .
def

193

(7)

Schiffel & Thielscher

Here, Œ¶ is a reified formula where the situation argument in all fluents is suppressed; and
Œ¶[S ‚Ä≤ ] means that all situation arguments are reinstated to S ‚Ä≤ . For example,
Knows(xplayer , ‚àÄX, Y. cell (X, Y, b), s0 )
stands for ‚àÄS ‚Ä≤ . K(xplayer , S ‚Ä≤ , s0 )‚äÉ‚àÄX, Y. Holds(cell (X, Y, b), S ‚Ä≤ ). To express the knowledge
of a player about another player‚Äôs knowledge, macro definition (7) can be easily extended
to form nested expressions, as in
Knows(xplayer , Knows(oplayer , control (oplayer )), Do(‚ü®mark (1, 1), noop ‚ü©, s0 )) .
It is also possible to define common knowledge of a group of players as any property that
is shared by all situations belonging to the reflexive and transitive closure of the combined
accessibility relations.
In GDL-II all players have complete knowledge of the initial situation. In terms of the
Situation Calculus,
(8)
K(R, S, s0 ) ‚â° S = s0 .
The eÔ¨Äects of actions and percepts on the knowledge states of the players are defined by the
successor state axiom for the special fluent K, for which we adapt Scherl and Levesque‚Äôs
definition as follows:
K(R, S ‚Ä≤‚Ä≤ , Do(A, S)) ‚â° ‚àÉA‚Ä≤ , S ‚Ä≤ . S ‚Ä≤‚Ä≤ = Do(A‚Ä≤ , S ‚Ä≤ ) ‚àß K(R, S ‚Ä≤ , S) ‚àß
Poss(A‚Ä≤ , S ‚Ä≤ )‚àßAct(R, A) = Act(R, A‚Ä≤ ) ‚àß
‚àÄP. Sees(R, P, A, S) ‚â° Sees(R, P, A‚Ä≤ , S ‚Ä≤ ) .

(9)

Put in words, a player considers S ‚Ä≤‚Ä≤ a possible situation after joint move A in S if, and
only if, S ‚Ä≤‚Ä≤ is obtained by doing some A‚Ä≤ in a situation S ‚Ä≤ that was conceivable in S; A‚Ä≤
was executable in S ‚Ä≤ ; the player did the same move in A‚Ä≤ as in A; and the player‚Äôs sensing
result for A‚Ä≤ , S ‚Ä≤ is identical to his sensing result for the actual A, S (so that he cannot
distinguish the two).
It should be noted that axioms (8) and (9) postulate both perfect recall and common
knowledge of the game rules among all players: From any actual situation Do(. . . , S0 )
only situations that are consistent with all of a player‚Äôs previous information are accessible.
Moreover, all these accessible situations are of the form Do(. . . , S0 ) too, which implies that
every situation belonging to the reflexive and transitive closure of the combined players‚Äô
accessibility relations are governed by the same precondition and eÔ¨Äect rules described by
axioms (5)‚Äì(6).
4.4 Completion Semantics and Stable Models
The axiomatisation above applies Clark‚Äôs completion to a given set of GDL-II game rules. In
general, however, the first-order semantics of the completion of a (stratified) logic program
is too weak to fully characterise its (unique) stable model in the presence of redundant
rules like validmove :- validmove. The stable model remains the same when such
‚Äúsuperfluous‚Äù clauses are added, but Clark‚Äôs completion is weakened by them. For example,
for the mentioned rule (assuming no further clauses) the only stable model is the empty
194

Representing and Reasoning About General Games

set, while the first-order semantics of the completion admits to two models‚Äîone in which
validmode holds and one where it is false.
This issue can be resolved by a second-order axiom (Ferraris, Lee, & Lifschitz, 2011).
Denoted by SM [F ], the axiom provides a stable model operator for arbitrary first-order
formulas F . The operator SM [F ] is defined in the following way:
Definition 9
‚Éó = (U1 , . . . , Un ) be a list of distinct
Let P‚Éó = (P1 , . . . , Pn ) be a list of predicates and U
‚Éó
‚Éó = P‚Éó denote the conjunction
predicate variables of the same length as P . Furthermore, let U
‚Éó i (X)
‚Éó ‚â° Pi (X)
‚Éó for all i = 1, . . . , n . By U
‚Éó ‚â§ P‚Éó we denote the
of the formulas ‚àÄX.U
‚Éó
‚Éó
‚Éó
‚Éó < P‚Éó stands for
conjunction of the formulas ‚àÄX.Ui (X)‚äÉPi (X) for all i = 1, . . . , n , and U
‚Éó
‚Éó
‚Éó
‚Éó
(U ‚â§ P ) ‚àß ¬¨(U = P ) .
The expression SM[F ; P‚Éó ] stands for the second-order sentence
‚Éó .(U
‚Éó < P‚Éó ) ‚àß F ‚àó (U
‚Éó) ,
F ‚àß ¬¨‚àÉU
‚Éó ) is defined recursively:
where F ‚àó (U
‚Ä¢ Pi (‚Éót )‚àó = Ui (‚Éót ) for any list ‚Éót of terms;
‚Ä¢ F ‚àó = F for any atomic formula F that does not contain members of P‚Éó ;
‚Ä¢ (F ‚àß G)‚àó = F ‚àó ‚àß G‚àó ;
‚Ä¢ (F ‚à® G)‚àó = F ‚àó ‚à® G‚àó ;
‚Ä¢ (F ‚äÉG)‚àó = (F ‚àó ‚äÉ G‚àó ) ‚àß (F ‚äÉ G) ;
‚Ä¢ (¬¨F )‚àó = (F ‚äÉ‚ä•)‚àó ;
‚Ä¢ (F ‚â° G)‚àó = (F ‚äÉ G)‚àó ‚àß (G ‚äÉ F )‚àó ;
‚Éó )‚àó = ‚àÄX.F
‚Éó ‚àó;
‚Ä¢ (‚àÄX.F
‚Éó )‚àó = ‚àÉX.F
‚Éó ‚àó.
‚Ä¢ (‚àÉX.F
Expression SM[F ] is shorthand for SM[F ; P‚Éó ] where P‚Éó is the list of all predicates in F .
The models of SM [F ] are the stable models of F . Specifically, if F is the completion of
a stratified logic program, SM [F ] has a unique Herbrand model that corresponds to the
unique stable model of the logic program.
As the last step of our translation we therefore add the axiom SM [F ] , where F is the
conjunction of all rules in the transformed game description.
It should be noted that the Situation Calculus typically uses a second-order induction
axiom to limit the set of situations to the smallest set containing s0 that is closed under
the Do operator. However, diÔ¨Äerent from SM [F ], the induction axiom is not suÔ¨Écient to
enforce a unique model in the presence of redundant rules.
195

Schiffel & Thielscher

4.5 Soundness and Completeness
The theory obtained by the transformation developed in the previous section is indeed a
Situation Calculus theory, as we will show now.
Proposition 2 Let G be a valid GDL-II game description and D be the axiomatisation
obtained from it by the transformation defined above. Then D is a syntactically correct
Situation Calculus theory.
Proof :
As a Situation Calculus theory, D must include a precondition axiom for each
‚Éó
action a(X) of the form
‚Éó S) ‚â° œÄ(X,
‚Éó S) .
Poss(a(X),
‚Éó S) must not refer to any situation other than S. In our general preconThe formula œÄ(X,
dition axiom (5), the variable A can be instantiated with every compound action to obtain
an axiom of the form above. The right-hand side of (5) has the only free variables A and
S and contains no reference to any other situation besides S.
‚Éó of
Furthermore, D must contain successor state axioms for each primitive fluent f (X)
the following form:
‚Éó Do(A, S)) ‚â° Œ≥(X,
‚Éó A, S) .
Holds(f (X),
‚Éó A, S) must not refer to any situation other than S. In our general
Again, the formula Œ≥(X,
successor state axiom (6) the variable F can be instantiated with every primitive fluent
to obtain an axiom of the form above. The right-hand side of (6) refers to the bodies of
rules with head next(F ‚Ä≤ ). According to the syntactic restrictions of GDL-II these bodies
may not depend on init or next and therefore do indeed never refer to any situation
besides S.
!
We will now show that the transformation from the previous section is sound and complete, that is, a GDL-II game description and the resulting Situation Calculus theory are
equivalent in terms of the knowledge that can be inferred from them. For this, we first
recall from Definition 6 the notion of a legal play sequence:
¬µ1

¬µ2

¬µn

œÉ0 ‚Üí œÉ1 ‚Üí . . . œÉn‚àí1 ‚Üí œÉn .
Intuitively, any such sequence can be interpreted as the situation
Do(An , . . . , Do(A2 , Do(A1 , s0 )) . . .)
in the Situation Calculus, where each joint move ¬µi = {(r1 , m1 ), . . . , (rk , mk )} corresponds
to a compound action Ai = ‚ü®m1 , . . . , mk ‚ü©. The following theorem states that for a given
state and joint move, the GDL-II game rules and the Situation Calculus theory entail the
same propositions.
Theorem 3 Let G be a valid GDL-II game description and D be the Situation Calculus
theory that is obtained from G with the translation defined above. Furthermore, let
¬µ1
¬µn
Œ¥ = œÉ0 ‚Üí . . . ‚Üí œÉn be a legal play sequence of the game with corresponding situation
S = Do(An , . . . , Do(A1 , s0 ) . . .); ¬µ be a joint move legal in œÉn ; and A be the compound
action corresponding to ¬µ.
‚Éó in the GDL-II description, let its translation to the Situation
For every predicate p(X)
t
‚Éó A, S), as in the examples given below.
Calculus be denoted by p (X,
196

Representing and Reasoning About General Games

‚Ä¢ truet (F, A, S) = Holds(F, S),
‚Éó A, S) = p(X,
‚Éó S) for derived fluents,
‚Ä¢ pt (X,
‚Éó A, S) = p(X,
‚Éó A, S) for derived action predicates.
‚Ä¢ pt (X,
‚Éó of the GDL-II description, p(X)
‚Éó ‚àà SM [G ‚à™ œÉntrue ‚à™ ¬µdoes ]
Then for every predicate p(X)
t
‚Éó
iÔ¨Ä D |= p (X, A, S).
Proof :
The theorem follows from the construction of the Situation Calculus theory in
Section 4.3 and a result by Ferraris et al. (2011) according to which the unique stable model
of a stratified logic program is equivalent to the Herbrand model of SM [F ] if F is the
conjunction of the completion of the program.
!
Our main proposition states that indistinguishable legal play sequences for some player
correspond to situations that the player considers mutually possible, and vice versa. This
implies that players can use the Situation Calculus theory to reason about their knowledge
about past, present, and future positions as well as about the knowledge of other players.
Proposition 4 Let G be a valid GDL-II description with semantics (R, œÉ0 , T, l, u, I, g)
and D be the Situation Calculus theory obtained from G with the translation defined
above. Let there be two legal play sequences
¬µ1

¬µn

¬µ‚Ä≤

¬µ‚Ä≤

Œ¥ = œÉ0 ‚Üí . . . ‚Üí œÉn , and
n
Œ¥‚Ä≤ = œÉ0 ‚Üí1 . . . ‚Üí
œÉn‚Ä≤

of the game with the corresponding situations S = Do(An , . . . , Do(A1 , s0 ) . . .) and S ‚Ä≤ =
Do(A‚Ä≤n , . . . , Do(A‚Ä≤1 , s0 ) . . .). A role r ‚àà R cannot distinguish Œ¥ and Œ¥‚Ä≤ iÔ¨Ä D |= K(r, S ‚Ä≤ , S).
Proof :
We prove the proposition by induction on the length n of sequence Œ¥.
For the base case n = 0 there is only one legal play sequence‚Äîthe initial state œÉ0 with
the corresponding initial situation s0 . Accordingly, by (8), D |= K(r, S, s0 ) iÔ¨Ä S = s0 ,
which proves the base case.
For the induction step, consider the two legal play sequences
¬µn+1

Œ¥+ = Œ¥ ‚Üí œÉn+1

and

¬µ‚Ä≤n+1

‚Ä≤
‚Ä≤
Œ¥+
= Œ¥‚Ä≤ ‚Üí œÉn+1

with the corresponding situations Do(An+1 , S) and Do(A‚Ä≤n+1 , S ‚Ä≤ ), respectively.
‚Ä≤ are indistinguishable for r if, and only if,
We have to show that Œ¥+ , Œ¥+
D |= K(r, Do(A‚Ä≤n+1 , S ‚Ä≤ ), Do(An+1 , S)) .
From the successor state axiom for the special fluent K (cf. axiom (9)), we know that
K(r, Do(A‚Ä≤n+1 , S ‚Ä≤ ), Do(An+1 , S)) if, and only if, all of the following hold.
(a) K(r, S ‚Ä≤ , S);
(b) Poss(A‚Ä≤n+1 , S ‚Ä≤ );
(c) Act(r, An+1 ) = Act(r, A‚Ä≤n+1 ); and
197

Schiffel & Thielscher

(d) ‚àÄP. Sees(r, P, An+1 , S) ‚â° Sees(r, P, A‚Ä≤n+1 , S ‚Ä≤ ).
We now show that each one of these holds.
By the induction hypotheses, (a) holds.
From the definition of a legal play sequence (Definition 6) it follows that each player‚Äôs
move in ¬µ‚Ä≤n+1 is legal in the state œÉn‚Ä≤ , that is, for all players r,
legal(r, ¬µ‚Ä≤n+1 (r)) ‚àà SM [G ‚à™ œÉn‚Ä≤true ] .
By Theorem 3 we conclude that this holds exactly if D |= Legal(r, ¬µ‚Ä≤n+1 (r), S ‚Ä≤ ) for all
players r. Because of ¬µ‚Ä≤n+1 (r) = Act(r, A‚Ä≤n+1 ), this is equivalent to the right hand side of
our precondition axiom (5) and hence equivalent to Poss(A‚Ä≤n+1 , S ‚Ä≤ ). Thus (b) follows.
According to Definition 7, provided that Œ¥ and Œ¥‚Ä≤ are indistinguishable for role r, Œ¥+
‚Ä≤ are indistinguishable for r if, and only if,
and Œ¥+
{p : (r, ¬µn+1 , œÉn , p) ‚àà I} = {p‚Ä≤ : (r, ¬µ‚Ä≤n+1 , œÉn‚Ä≤ , p‚Ä≤ ) ‚àà I}
and ¬µn+1 (r) =

¬µ‚Ä≤n+1 (r)

(10)
(11)

By the definition of Act(r, A) (cf. (3)), Act(r, An+1 ) = ¬µn+1 (r) and Act(r, A‚Ä≤n+1 ) =
¬µ‚Ä≤n+1 (r) . Thus, equation (11) holds if and only if (c).
By Definition 5, I = {(r, ¬µ, œÉ, p) : sees(r, p) ‚àà SM [G ‚à™ œÉ true ‚à™ ¬µdoes ]}. Thus,
(r, ¬µ‚Ä≤n+1 , œÉn , p) ‚àà I if, and only if, sees(r, p) ‚àà SM [G ‚à™ œÉntrue ‚à™ ¬µdoes
n+1 ] . By Theorem 3,
this is equivalent to D |= Sees(r, p, An+1 , S) . The same reasoning holds for the right hand
side of (10). Thus, (10) holds just in case (d) holds.
This proves that (a) to (d) are consequences of the induction hypothesis. Hence,
‚Ä≤ ,
K(r, Do(A‚Ä≤n+1 , S ‚Ä≤ ), Do(An+1 , S)) holds if, and only if, r cannot distinguish Œ¥+ and Œ¥+
which concludes the induction step and the proof.
!
4.6 Practical Considerations
Our completeness result requires the second-order axiom SM [F ] in the translated game
description. In practice, this can be avoided if we can confine ourselves to finitely many
situations. The syntactic restrictions in GDL-II imply that every ground atom only depends
on finitely many other ground atoms; this is a consequence of the restricted recursion
according to Definition 3. Our mapping extends the GDL rules by situation arguments, but
also for these only finitely many ground instances are considered in Theorem 3 because the
legal play sequences are fixed and hence the situations are depth-restricted. In this case
it suÔ¨Éces to consider a finite subset R(P, F ) of the grounding of a program P to decide
whether a ground atom F is entailed by P (Bonatti, 2001). Thus, we can consider a finite
ground program and replace the second-order axiom by propositional loop formulas (Lin &
Zhao, 2004) to reconcile the semantics of GDL-II with the Situation Calculus theory. This
solution applies whenever we can confine ourselves to finitely many ground situations for
the reasoning problem at hand.9
9. We also remark that this issue is actually of little relevance to the practice of General Game Playing;
e.g. none of the numerous games played at the past AAAI Competitions featured logically redundant
clauses that players had to recognise.

198

Representing and Reasoning About General Games

Although with these points in mind decidability of reasoning is not an issue, tractability
still is. It is possible to construct game rules where computing all legal moves alone is too
expensive to do any kind of planning ahead in a reasonable amount of time. A practically
viable approach is the use of sound but incomplete proof methods (Haufe & Thielscher,
2012; Thielscher, 2013).

5. Related Work
The new language GDL-II is the first extension of the existing GDL for nondeterministic
games with imperfect information. The idea behind General Game Playing itself goes
back to early work by Pitrat (1968) and, later, Pell (1993). Both define formal languages
to describe whole classes of games, but these languages are even less general than basic
GDL for perfect-information games. Earlier work also includes the definition of Gala
for describing general games with imperfect information (Koller & PfeÔ¨Äer, 1997). The
main diÔ¨Äerence to GDL-II is that Gala is tightly coupled with a programming language
(Prolog) and therefore has an operational‚Äîrather than declarative‚Äîsemantics. It should
be mentioned also that GDL-II draws from concepts that have been used in Action and
Planning Languages (Lobo, Mendez, & Taylor, 2001) to represent eÔ¨Äects of actions in the
presence of incomplete knowledge. GDL-II can be seen as generalising this line of work to
multi-agent and competitive settings.
The keyword sees that we introduced into the Game Description Language allows
us to describe a player‚Äôs percepts as side-eÔ¨Äects of moves. This is diÔ¨Äerent from the traditional modelling in reasoning about actions of observations as direct eÔ¨Äects of sensing
actions (Scherl & Levesque, 2003; Thielscher, 2000; Lobo et al., 2001) and is especially
appropriate for multi-player games where agents may see something without having acted,
e.g., when they observe other players‚Äô moves or cards they are being dealt.
Our translation from GDL-II into the Situation Calculus is the first full embedding of
GDL-II into an action formalism; a recent mapping (Thielscher, 2011) into the Action Description Language C+ (Giunchiglia, Lee, Lifschitz, McCain, & Turner, 2004) is restricted
to basic GDL and hence covers neither imperfect information games nor knowledge and
sensing. But we expect our translation to provide the basis for mappings of GDL-II into
other existing action languages, for example, the one developed by Lobo et al. (2001). This
opens up the road to the full range of applicable methods and systems that exist in reasoning
about actions and AI planning.
The Situation Calculus has previously been shown to be a viable formalism for representing and reasoning about games by Schulte and Delgrande (2004), who also use the Situation
Calculus variant with knowledge (Scherl & Levesque, 2003) to axiomatise extensive-form
games. On the one hand, their approach is more general then ours in that it can deal with
infinite games, does not assume perfect recall and is not restricted to uniform probabilities
for nature‚Äôs moves. On the other hand, it is these restrictions that make GDL-II a compact
axiomatisation language which focuses on the rules of games rather than on how to reason
about them. In addition, GDL-II has a state update semantics, which is in general preferred
over the usual regression semantics of the Situation Calculus for performance reasons.
A limitation of Schulte and Delgrande‚Äôs axiomatisation is the restriction of knowledge
fluent K(S ‚Ä≤ , S) to a single agent (namely, the player whose move it is in situation S,
199

Schiffel & Thielscher

assuming that players do not move simultaneously). This does not allow for reasoning
about the knowledge of any other player in these situations, nor about what one agent can
conclude about what another agent would know. This is remedied in a recent definition of
a multi-agent epistemic variant of the Situation Calculus for axiomatising games (Belle &
Lakemeyer, 2010). Despite notable diÔ¨Äerences to GDL-II and our variant of the Situation
Calculus, e.g. the restriction to non-simultaneous moves and the augmentation of a domain
theory with an epistemic theory about subjective knowledge, we believe that a translation
similar to the one presented in this paper can be constructed by which full GDL-II is
embedded into a suitably adapted version of Belle and Lakemeyer‚Äôs approach.
There are other formalisms beside the Situation Calculus for reasoning about knowledge
and actions in multi-agent environments, for example, the epistemic logic developed by Belardinelli and Lomuscio (2009). A translation of GDL-II into this formalism seems possible
and might be interesting because model checkers for this logic are available (Lomuscio, Qu,
& Raimondi, 2009).
Several General Game Playing systems exist that are able to play games encoded in
GDL-II (Schofield, Cerexhe, & Thielscher, 2012; Edelkamp, Federholzner, & Kissmann,
2012). Although these systems have to reason about a game as well, the focus lies on
solving specific reasoning tasks (such as computing legal moves and possible continuations
of the game) under time constraints.

6. Conclusion
One of the reasons why General Game Playing has not yet found as many applications
outside the game-playing area as it could, is that the current state of the art is restricted to
deterministic games with complete state information. Aiming at overcoming this limitation,
we have defined a conceptually simple yet powerful extension of the Game Description
Language for representing the rules of general games with information asymmetry and
random moves. We have shown that GDL-II, notationally simple as it is, gives rise to an
intricate epistemic model and thus suÔ¨Éces to provide players with all information they need
to reason about their own knowledge as well as that of the other players up front and during
game play.
We have argued that our language extension suÔ¨Éces to describe any finite n-player
game in extensive form with perfect recall. This shows that for the purpose of General
Game Playing the language GDL-II can be considered complete; additional elements can
only serve to obtain more succinct descriptions (e.g., by allowing explicit specifications of
non-uniform probabilities for moves by random) or will be needed when the very concept
of General Game Playing itself is extended beyond the current setting, e.g. to open-world
games such as Scrabble (Sheppard, 2002) or systems that play real, physical games (Barbu,
Narayanaswamy, & Siskind, 2010).
In the second part of the paper, we have presented an embedding of our extended game
description language into a suitable variant of the Situation Calculus that features multiagent knowledge, simultaneous actions, and action-independent sensing. While a GDL-II
game description itself defines what observations players make throughout a game but not
how they use this information, the Situation Calculus axiomatisation tells players how to
200

Representing and Reasoning About General Games

reason about their own percepts and what they entail about the current position, about the
possible moves, and about what the other players may know.
A central aspect of our work is the distinction between objective information on the
one hand and how it aÔ¨Äects the epistemic state of an agent on the other hand. The diÔ¨Äerence manifests itself in the two key predicates of this paper: GDL-II keyword sees(R,P)
describes objective information while Knows(R, Œ¶, S) in the Situation Calculus axiomatises subjective knowledge. This distinction is also present in the two semantics given in
this paper: state transition systems to model the rules of a game environment and Situation Calculus axiomatisations to model the information processing of players. This draws
a connection of our work to the more general distinction often made in AI between the
functionality of agents and properties of their task environments (Russell & Norvig, 2010).
Seen from this perspective, GDL-II extends GDL in that it enables us to describe a larger
class of game environments in a rule-based language. Meanwhile, our Situation Calculus axiomatisation can be viewed as a model for game-playing agents whose functionality includes
perfect recall and reasoning abilities. An interesting direction for future work is to develop
alternative Situation Calculus axiomatisations of reasoning about GDL-II games and to use
these to study other types of agents, e.g. which are memoryless and purely reactive, as well
as aspects of bounded rationality in game-playing programs (Russell & Wefald, 1991).
Beyond General Game Playing we therefore envision applications of GDL-II in other AI
areas in which game models arise, such as automated trading agents (Thielscher & Zhang,
2010), Multiagent Planning (Larbi, Konieczny, & Marquis, 2007), or Multiagent Systems
in general. A general description language for games of any kind is potentially useful
wherever there is a need for compact and high-level yet machine-processable specifications
of problem domains and applications in the form of games. Also there is potential for using
the available General Game Playing infrastructure and systems for developing and testing
new games or for modelling, simulating, and reasoning about dynamic environments like, for
example, economic systems. However, some additions to the language, such as arithmetic,
might be necessary to allow for concise descriptions of some of these domains. In addition,
GDL-II and the General Game Playing infrastructure have been used as tools for teaching
AI and the development of agent programs for dynamic environments in several university
courses.10

Acknowledgments
We thank Joohyung Lee and the anonymous reviewers of an earlier version of this paper for their thoughtful and very constructive comments. This research was supported
under Australian Research Council‚Äôs (ARC) Discovery Projects funding scheme (project
number DP 120102023) and by the Icelandic Centre for Research (RANNIS, grant number 210019). The second author is the recipient of an ARC Future Fellowship (project
number FT 0991348). He is also aÔ¨Éliated with the University of Western Sydney.

10. Some material is available at www.general-game-playing.de/teaching/teaching.html

201

Schiffel & Thielscher

References
Apt, K., Blair, H., & Walker, A. (1987). Towards a theory of declarative knowledge. In
Minker, J. (Ed.), Foundations of Deductive Databases and Logic Programming, chap. 2,
pp. 89‚Äì148. Morgan Kaufmann.
Barbu, A., Narayanaswamy, S., & Siskind, J. (2010). Learning physically-instantiated game
play through visual observation. In Proceedings of the IEEE International Conference
on Robotics and Automation (ICRA), pp. 1879‚Äì1886, Anchorage. IEEE Press.
Belardinelli, F., & Lomuscio, A. (2009). Quantified epistemic logics for reasoning about
knowledge in multi-agent systems. Artificial Intelligence, 173 (9‚Äì10), 982‚Äì1013.
Belle, V., & Lakemeyer, G. (2010). Reasoning about imperfect information games in the
epistemic situation calculus. In Fox, M., & Poole, D. (Eds.), Proceedings of the AAAI
Conference on Artificial Intelligence, pp. 255‚Äì260, Atlanta. AAAI Press.
Bonatti, P. (2001). Reasoning with open logic programs. In Eiter, T., Faber, W., &
Trusczynski, M. (Eds.), Proceedings of the International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR), Vol. 2173 of LNCS, pp. 147‚Äì159,
Vienna. Springer.
Clark, K. (1978). Negation as failure. In Gallaire, H., & Minker, J. (Eds.), Logic and Data
Bases, pp. 293‚Äì322. Plenum Press.
Clune, J. (2007). Heuristic evaluation functions for general game playing. In Proceedings
of the AAAI Conference on Artificial Intelligence, pp. 1134‚Äì1139, Vancouver. AAAI
Press.
Davis, E. (1990). Representations of Commonsense Knowledge. Morgan Kaufmann.
Edelkamp, S., Federholzner, T., & Kissmann, P. (2012). Searching with partial belief states
in general games with incomplete information. In Glimm, B., & KruÃàger, A. (Eds.),
Proceedings of the German Annual Conference on Artificial Intelligence (KI), Vol.
7526 of LNCS, pp. 25‚Äì36, SaarbruÃàcken, Germany. Springer.
Edelkamp, S., & Kissmann, P. (2008). Symbolic classification of general two-player games.
In Dengel, A., Berns, K., Breuel, T., Bomarius, F., & Roth-Berghofer, T. (Eds.),
Proceedings of the German Annual Conference on Artificial Intelligence (KI), Vol.
5243 of LNCS, pp. 185‚Äì192, Kaiserslautern. Springer.
Ferraris, P., Lee, J., & Lifschitz, V. (2011). Stable models and circumscription. Artificial
Intelligence, 175 (1), 236‚Äì263.
Finnsson, H., & BjoÃàrnsson, Y. (2008). Simulation-based approach to general game playing. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 259‚Äì264,
Chicago. AAAI Press.
Forth, J., & Shanahan, M. (2004). Indirect and conditional sensing in the event calculus.
In de MaÃÅntras, R. L., & Saitta, L. (Eds.), Proceedings of the European Conference on
Artificial Intelligence (ECAI), pp. 900‚Äì904, Valencia, Spain. IOS Press.
Gelder, A. V. (1989). The alternating fixpoint of logic programs with negation. In Proceedings of the 8th Symposium on Principles of Database Systems, pp. 1‚Äì10. ACM
SIGACT-SIGMOD.
202

Representing and Reasoning About General Games

Gelfond, M., & Lifschitz, V. (1988). The stable model semantics for logic programming. In
Kowalski, R., & Bowen, K. (Eds.), Proceedings of the International Joint Conference
and Symposium on Logic Programming (IJCSLP), pp. 1070‚Äì1080, Seattle. MIT Press.
Genesereth, M. (1991). Knowledge interchange format. In Allen, J. F., Fikes, R., & Sandewall, E. (Eds.), Proceedings of the International Conference on Principles of Knowledge Representation and Reasoning (KR), pp. 599‚Äì600, Cambridge. Morgan Kaufmann.
Genesereth, M., Love, N., & Pell, B. (2005). General game playing: Overview of the AAAI
competition. AI Magazine, 26 (2), 62‚Äì72.
Giunchiglia, E., Lee, J., Lifschitz, V., McCain, N., & Turner, H. (2004). Nonmonotonic
causal theories. Artificial Intelligence, 153 (1‚Äì2), 49‚Äì104.
Golden, K., & Weld, D. (1996). Representing sensing actions: The middle ground revisited.
In Aiello, L. C., Doyle, J., & Shapiro, S. (Eds.), Proceedings of the International
Conference on Principles of Knowledge Representation and Reasoning (KR), pp. 174‚Äì
185, Cambridge, MA. Morgan Kaufmann.
Grosz, B., Kraus, S., Talman, S., Stossel, B., & Havlin, M. (2004). The influence of social
dependencies on decision-making: Initial investigations with a new game. In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems
(AAMAS), pp. 782‚Äì789, New York.
Harsanyi, J. (1967). Games with incomplete information played by ‚ÄúBayesian‚Äù players,
I‚ÄìIII: Part I. the basic model. Management Science, 14 (3), 159‚Äì182.
Haufe, S., & Thielscher, M. (2012). Automated verification of epistemic properties for general game playing. In Brewka, G., Eiter, T., & McIlraith, S. (Eds.), Proceedings of the
International Conference on Principles of Knowledge Representation and Reasoning
(KR), pp. 339‚Äì349, Rome. AAAI Press.
Jensen, R., & Veloso, M. (2000). OBDD-based universal planning for synchronized agents
in non-deterministic domains. Journal of Artificial Intelligence Research, 13, 189‚Äì226.
Kaiser, D. (2007). Automatic feature extraction for autonomous general game playing
agents. In Durfee, E., Yokoo, M., Huhns, M., & Shehory, O. (Eds.), Proceedings of the
International Conference on Autonomous Agents and Multiagent Systems (AAMAS),
Honolulu.
Kaiser, D. (2008). The design and implementation of a successful general game playing
agent. In Wilson, D., & SutcliÔ¨Äe, G. (Eds.), Proceedings of the International Florida
Artificial Intelligence Research Society Conference (FLAIRS), pp. 110‚Äì115, Key West.
AAAI Press.
Kirci, M., Sturtevant, N., & SchaeÔ¨Äer, J. (2011). A GGP feature learning algorithm. KI‚Äî
KuÃànstliche Intelligenz, 25, 35‚Äì42. Springer.
Kissmann, P., & Edelkamp, S. (2011). Gamer, a general game playing agent. KI‚ÄîKuÃànstliche
Intelligenz, 25, 49‚Äì52. Springer.
Koller, D., & PfeÔ¨Äer, A. (1997). Representations and solutions for game-theoretic problems.
Artificial Intelligence, 94 (1), 167‚Äì215.
203

Schiffel & Thielscher

Kuhlmann, G., Dresner, K., & Stone, P. (2006). Automatic heuristic construction in a
complete general game player. In Proceedings of the AAAI Conference on Artificial
Intelligence, pp. 1457‚Äì1462, Boston. AAAI Press.
La Mura, P. (2000). Game networks. In Proceedings of the Conference on Uncertainty in
Artificial Intelligence (UAI), pp. 335‚Äì342, Stanford. Morgan Kaufmann.
Lakemeyer, G., & Levesque, H. (1998). AOL : A logic of acting, sensing, knowing, and only
knowing. In Cohn, A. G., Schubert, L. K., & Shapiro, S. C. (Eds.), Proceedings of the
International Conference on Principles of Knowledge Representation and Reasoning
(KR), pp. 316‚Äì327, Trento. AAAI Press.
Larbi, R. B., Konieczny, S., & Marquis, P. (2007). Extending classical planning to the
multi-agent case: A game-theoretic approach. In Mellouli, K. (Ed.), Proceedings of
the European Conference on Symbolic and Quantitative Approaches to Reasoning with
Uncertainty (ECSQARU), Vol. 4724 of LNCS, pp. 63‚Äì75, Hammamet. Springer.
Leyton-Brown, K., & Shoham, Y. (2008). Essentials of Game Theory. Synthesis Lectures
on Artificial Intelligence and Machine Learning. Morgan & Claypool.
Lin, F., & Zhao, Y. (2004). ASSAT: Computing answer sets of a logic program by SAT
solvers. Artificial Intelligence, 157, 115‚Äì137.
Lloyd, J. (1987). Foundations of Logic Programming (second, extended edition). Series
Symbolic Computation. Springer.
Lloyd, J., & Topor, R. (1986). A basis for deductive database systems II. Journal of Logic
Programming, 3 (1), 55‚Äì67.
Lobo, J., Mendez, G., & Taylor, S. R. (2001). Knowledge and the action description language
A . Theory and Practice of Logic Programming, 1 (2), 129‚Äì184.

Lomuscio, A., Qu, H., & Raimondi, F. (2009). MCMAS: A model checker for the verification of multi-agent systems. In Proceedings of the International Conference on Computer Aided Verification (CAV), Vol. 5643 of LNCS, pp. 682‚Äì688, Grenoble, France.
Springer.
Love, N., Hinrichs, T., Haley, D., Schkufza, E., & Genesereth, M. (2006). General Game
Playing: Game Description Language Specification. Tech. rep. LG‚Äì2006‚Äì01, Stanford
Logic Group, Computer Science Department, Stanford University, 353 Serra Mall,
Stanford, CA 94305. Available at: games.stanford.edu.
McCarthy, J. (1963). Situations and Actions and Causal Laws. Stanford Artificial Intelligence Project, Memo 2, Stanford University, CA.
MeÃÅhat, J., & Cazenave, T. (2011). A parallel general game player. KI‚ÄîKuÃànstliche Intelligenz, 25, 43‚Äì47. Springer.
Pell, B. (1993). Strategy Generation and Evaluation for Meta-Game Playing. Ph.D. thesis,
Trinity College, University of Cambridge.
Petrick, R., & Bacchus, F. (2004). Extending the knowledge-based approach to planning
with incomplete information and sensing. In Dubois, D., Welty, C., & Williams, M.A. (Eds.), Proceedings of the International Conference on Principles of Knowledge
Representation and Reasoning (KR), pp. 613‚Äì622, Whistler. AAAI Press.
204

Representing and Reasoning About General Games

Pitrat, J. (1968). Realization of a general game playing program. In Morrell, A. (Ed.),
Proceedings of IFIP Congress, pp. 1570‚Äì1574, Edinburgh.
Pritchard, D. (1994). The Encycolpedia of Chess Variants. Godalming.
Rasmusen, E. (2007). Games and Information: an Introduction to Game Theory (4th
edition). Blackwell Publishing.
Reiter, R. (1991). The frame problem in the situation calculus: A simple solution (sometimes) and a completeness result for goal regression. In Lifschitz, V. (Ed.), Artificial
Intelligence and Mathematical Theory of Computation, pp. 359‚Äì380. Academic Press.
Rosenhouse, J. (2009). The Monty Hall Problem. Oxford University Press.
Ruan, J., van der Hoek, W., & Wooldridge, M. (2009). Verification of games in the game
description language. Journal of Logic and Computation, 19 (6), 1127‚Äì1156.
Russell, S., & Norvig, P. (2010). Artificial Intelligence: A Modern Approach (3rd edition).
Prentice Hall.
Russell, S., & Wefald, E. (1991). Do the Right Thing: Studies in Limited Rationality. MIT
Press.
Scherl, R., & Levesque, H. (2003). Knowledge, action, and the frame problem. Artificial
Intelligence, 144 (1), 1‚Äì39.
SchiÔ¨Äel, S. (2010). Symmetry detection in general game playing. In Proceedings of the
AAAI Conference on Artificial Intelligence, pp. 980‚Äì985, Atlanta. AAAI Press.
SchiÔ¨Äel, S., & Thielscher, M. (2007). Fluxplayer: A successful general game player. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 1191‚Äì1196, Vancouver.
AAAI Press.
SchiÔ¨Äel, S., & Thielscher, M. (2009). Automated theorem proving for general game playing.
In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),
pp. 911‚Äì916, Pasadena.
SchiÔ¨Äel, S., & Thielscher, M. (2010). A multiagent semantics for the Game Description
Language. In Filipe, J., Fred, A., & Sharp, B. (Eds.), Agents and Artificial Intelligence,
Vol. 67 of Communications in Computer and Information Science, pp. 44‚Äì55. Springer.
Schofield, M., Cerexhe, T., & Thielscher, M. (2012). HyperPlay: A solution to general
game playing with imperfect information. In Proceedings of the AAAI Conference on
Artificial Intelligence, pp. 1606‚Äì1612, Toronto. AAAI Press.
Schulte, O., & Delgrande, J. (2004). Representing von Neumann-Morgenstern games in
the situation calculus. Annals of Mathematics and Artificial Intelligence, 42 (1‚Äì3),
73‚Äì101.
Sheppard, B. (2002). World-championship-caliber Scrabble. Artificial Intelligence, 134 (1‚Äì
2), 241‚Äì275.
Thielscher, M. (2000). Representing the knowledge of a robot. In Cohn, A., Giunchiglia,
F., & Selman, B. (Eds.), Proceedings of the International Conference on Principles of
Knowledge Representation and Reasoning (KR), pp. 109‚Äì120, Breckenridge. Morgan
Kaufmann.
205

Schiffel & Thielscher

Thielscher, M. (2011). Translating general game descriptions into an action language. In
Balduccinin, M., & Son, T. (Eds.), Logic Programming, Knowledge Representation,
and Nonmonotonic Reasoning: Essays in Honor of Michael Gelfond, Vol. 6565 of
LNAI, pp. 300‚Äì314. Springer.
Thielscher, M. (2013). Filtering with logic programs and its application to general game
playing. In Proceedings of the AAAI Conference on Artificial Intelligence, Bellevue.
AAAI Press.
Thielscher, M., & Voigt, S. (2010). A temporal proof system for general game playing.
In Fox, M., & Poole, D. (Eds.), Proceedings of the AAAI Conference on Artificial
Intelligence, pp. 1000‚Äì1005, Atlanta. AAAI Press.
Thielscher, M., & Zhang, D. (2010). From general game descriptions to a market specification language for general trading agents. In David, E., Gerding, E., Sarne, D., &
Shehory, O. (Eds.), Agent-Mediated Electronic Commerce: Designing Trading Strategies and Mechanisms for Electronic Markets, Vol. 59 of LNBIP, pp. 259‚Äì274. Springer.

206

Journal of Artificial Intelligence Research 49 (2014) 403-449

Submitted 10/13; published 03/14

Mechanisms for Fair Allocation Problems:
No-Punishment Payment Rules in Verifiable Settings
Gianluigi Greco

ggreco@mat.unical.it

Dipartimento di Matematica e Informatica
UniversitaÃÄ della Calabria
I-87036 Rende, Italy

Francesco Scarcello

scarcello@dimes.unical.it

DIMES
UniversitaÃÄ della Calabria
I-87036 Rende, Italy

Abstract
Mechanism design is considered in the context of fair allocations of indivisible goods
with monetary compensation, by focusing on problems where agents‚Äô declarations on allocated goods can be verified before payments are performed. A setting is considered where
verification might be subject to errors, so that payments have to be awarded under the presumption of innocence, as incorrect declared values do not necessarily mean manipulation
attempts by the agents. Within this setting, a mechanism is designed that is shown to be
truthful, efficient, and budget-balanced. Moreover, agents‚Äô utilities are fairly determined by
the Shapley value of suitable coalitional games, and enjoy highly desirable properties such as
equal treatment of equals, envy-freeness, and a stronger one called individual-optimality.
In particular, the latter property guarantees that, for every agent, her/his utility is the
maximum possible one over any alternative optimal allocation.
The computational complexity of the proposed mechanism is also studied. It turns
out that it is #P-complete so that, to deal with applications with many agents involved,
two polynomial-time randomized variants are also proposed: one that is still truthful and
efficient, and which is approximately budget-balanced with high probability, and another
one that is truthful in expectation, while still budget-balanced and efficient.

1. Introduction
Whenever the outcome of some social choice process depends on the information collected
from a number of self-interested agents, strategic issues may come into play. Indeed, agents
may find it convenient to misreport their types, i.e., the relevant information they own
as private knowledge, so that the (global) best possible solution can be missed. In these
cases, mechanism design techniques can be used as solution approaches, which augment
combinatorial algorithms with appropriate monetary payments, aimed at motivating all
agents to truthfully report their private types (see, e.g., Nisan, Roughgarden, Tardos, &
Vazirani, 2007; Shoham & Leyton-Brown, 2009).
On the class of social choice utilitarian problems, agent types encode (monetary) valuations over the set of all solutions and the goal is to compute a solution maximizing the social
welfare, i.e., the sum of agents‚Äô true evaluations. A prominent role in mechanism design
for problems of this class is played by the Vickrey-Clarke-Grove (VCG) paradigm (Vick-

c
2014
AI Access Foundation. All rights reserved.

Greco & Scarcello

ery, 1961; Clarke, 1971; Groves, 1973), which is a general method for designing truthful
mechanisms, i.e., mechanisms where truth-telling is a dominant strategy for each agent. In
particular, VCG mechanisms are efficient. That is, they guarantee that a solution maximizing the social welfare is actually computed. However, they are not budget-balanced, i.e., the
algebraic sum of the monetary transfers is not always zero and mechanisms from this class
can run into deficit. In fact, this is a well-known drawback of VCG mechanisms (see, e.g.,
Archer & Tardos, 2007), but it is essentially the best one can hope to do, given classical
impossibility theorems (Green & Laffont, 1977; Hurwicz, 1975) stating that no truthful
mechanism can be designed to be always efficient and budget-balanced.
In many practical applications, however, payments to agents can be performed after
the final outcome is known, so that some kind of verification on reported types might be
possible. This additional power is not considered in the classical mechanism-design setting
and, in fact, whenever verification is allowed, some impossibility results might no longer
hold. Mechanisms with verification have been introduced by Nisan and Ronen (2001), who
considered verification for a task scheduling problem: We have some agents declaring the
amount of time they need to solve each task, and the goal is to have all tasks being solved,
by minimizing the completion time of the last-solved one (hence, the make-span). In this
context, payments are computed after the actual task release times have been observed, so
that we have, for instance, the ability to ‚Äúpunish‚Äù some agent whose declared ability has
been verified to be different than its actual performance in the process.
Compared to standard mechanisms (see, e.g., Nisan et al., 2007), those with verification
have received considerably less attention in the literature (see, e.g., Nisan & Ronen, 2001;
Auletta, De Prisco, Penna, & Persiano, 2009; Penna & Ventre, 2012a; Krysta & Ventre,
2010; Ferrante, Parlato, Sorrentino, & Ventre, 2009; Penna & Ventre, 2012b; Auletta,
De Prisco, Penna, Persiano, & Ventre, 2006; Auletta, Penna, Persiano, & Ventre, 2011). In
particular, such works consider a verification ability that is partial, in the sense that agents‚Äô
reporting is restricted to true types plus certain specific kinds of deviations (e.g., values
that are lower than the true ones) and verification is focused on detecting such lies only.
An extension of the above model has been recently proposed by Caragiannis, Elkind,
Szegedy, and Yu (2012), who assume no a-priori restrictions on the agents‚Äô reported types,
within a setting where an agent cheating on her/his type will be caught with some probability that may depend on her/his true type, the reported type, or both. In fact, despite the
different facets of the verification power, most of the mechanisms with verification proposed
in the literature share the idea of providing incentives to truthfully report private types by
exploiting the intimidation of punishing those agents that will be caught lying. Moreover,
while budget limits have been considered in some approaches (see, e.g., Nisan & Ronen,
2001), no mechanism with verification has been designed to be budget-balanced, with the
focus being on truthfulness and efficiency.
In this paper, we consider instead a budget-balanced mechanism based on a model
of verification where there is no restriction on the possible declarations (hence, arbitrary
deviations are possible), and nevertheless no punishment can be used after the verification
process. This design constraint has been guided by real-world applications where it clearly
emerges that a punishing approach would hardly be acceptable by agents, unless a clear
proof of a deliberate malevolent behavior can be exhibited. Moreover, even in this case
the punishment should be proportional to the amount of discrepancy between declared and
404

Mechanisms for Fair Allocation Problems

verified values that can be attributed to a malevolent behavior. The resulting setting shares
the spirit of the work by Feige and Tennenholtz (2011), where it is observed that possible
discrepancies between agents‚Äô declarations and third-party verified values are more often
due to different reasons, in particular to the fact that agents, while not being malevolent,
might still be unable to accurately collect and/or report information about their valuations.
In more detail, Feige and Tennenholtz (2011) considered a scheduling problem on a single
machine where each agent reports the length of her/his job and the scheduler needs to finish
as many jobs as possible by a given deadline. Differently from earlier literature, it is assumed
that agents are uncertain of their own job lengths, for instance, because of their limited
computational resources. Moving from the observation that mechanisms with verification
are often designed in way that performs well when agents have accurate information about
their private features, but might perform arbitrarily bad when agents are uncertain of this
information, Feige and Tennenholtz then proposed the use of the ‚Äúforgiving‚Äù mechanisms,
where punishments are not used to enforce truthfulness. These mechanisms are applied
over two models of uncertainty: One that is probabilistic in nature, and another (called
‚Äúqualitative private input‚Äù) where there is no quantitative model explaining to which extent
the agents can trust their estimate, and the preference of an agent over various lotteries
might be even inconsistent with any probability distribution.
In this paper we follow the work by Feige and Tennenholtz (2011) and in particular their
qualitative model of uncertainty. Moreover, in addition to their ‚Äúsubjective‚Äù perspective of
the problem, where uncertainty is inherent to private inputs, we also take into account the
dual (‚Äúobjective‚Äù) perspective, where discrepancies between declared and verified values
might due to errors that can occur in the verification process. Indeed, verification can
be practically implemented by sensing some parameters that become observable after the
mechanism is performed, and sensing is clearly affected by errors (it is unrealistic to assume
that it can be carried out with arbitrary precision).
In fact, no matter the perspective (objective vs subjective) from which the problem
discussed above is analyzed, an intrinsic limit of mechanisms with verification has clearly
emerged: Whenever an agent misreports her/his type and this is detected by the verifier,
‚Äúpunishing‚Äù this agent might be effective in mathematical studies, but very inappropriate
in real life situations in which uncertainty is inherent. Accordingly, we will therefore assume
that only a limited use of the verification power given at hand can be made. In particular,
the goal of the paper is to design mechanisms that are not based on punishments (while
nonetheless resulting to be truthful, efficient, and budget-balanced) and that are tolerant of
measurement errors and uncertain inputs, in the sense that ‚Äúsmall‚Äù errors should determine
small deviations from the outcome we would have obtained with no errors at all.
1.1 Mechanisms for Fair Division with Monetary Compensation
We consider mechanisms with verification in the context of fair allocation problems (see,
e.g., Moulin, 2003; Young, 1994; Thomson, 2011). We assume that we are given a set
of indivisible goods to be allocated to a set of agents. Each agent is equipped with a private preference relation, which is encoded as a real-valued function (basically, a monetary
valuation) over all possible goods‚Äîformal definitions are in Section 2. An agent can have
allocated more then one good, in which case her/his evaluation is additive over them. More-

405

Greco & Scarcello

over, goods are indivisible, i.e., each good can be allocated to one agent at most. However,
monetary transfers are allowed, in terms of both payments charged to agents and monetary
compensations provided to them. The goal is to find an efficient allocation, that is, an allocation maximizing the total value of the allocated goods, by designing rules guaranteeing
that certain desirable properties are achieved, such as truthfulness and individual rationality, i.e., no agent is ever worse off than she/he would have been without participating in the
mechanism. Moreover, we want to obtain outcomes that are ‚Äúpolitically‚Äù acceptable. That
is, agents should perceive the designed mechanism as a fair one (see, e.g., Brandt, Conitzer,
& Endriss, 2012), independently of the rules leading them to be honest. For instance, it
is desirable that no agent envies the allocation of any other agent, or that the selected
outcome is Pareto efficient, i.e., there must be no different allocation that is preferred by
all agents and strictly preferred by at least one of them.
The model and, in particular, properties of fair allocations with indivisible objects
and monetary transfers have been studied, e.g., by Svensson (1983), BeviaÃÅ (1998), Maskin
(1987), Tadenuma and Thomson (1993), Meertens, Potters, and Reijnierse (2002), Tadenuma and Thomson (1991), Alkan, Demange, and Gale (1991), Willson (2003), Su (1999),
Yang (2001), Quinzii (1984), and Sakai (2007). Moreover, procedures to compute fair allocations have been proposed by Aragones (1995), Klijn (2000), Haake, Raith, and Su (2002),
Brams and Kilgour (2001), Potthoff (2002), and AbdulkadirogÃÜlu, SoÃànmez, and UÃànver (2004).
None of the approaches listed above, however, can guarantee the elicitation of honest
preferences from the agents. In fact, the question of designing truthful and fair mechanisms
has been recently considered as well (Andersson & Svensson, 2008; Andersson, 2009; Svensson, 2009; Yengin, 2012; Ohseto, 2004; Porter, Shoham, & Tennenholtz, 2004; Shioura,
Sun, & Yang, 2006). In these approaches, while budget limits are sometimes enforced and
mechanisms are defined that cannot run into deficit, budget-balance is never guaranteed.
Indeed, this comes again with no surprise, given that no truthful mechanism can be simultaneously fair (e.g., envy-free or Pareto-efficient) and budget-balanced (see, e.g., Tadenuma
& Thomson, 1995; Alcalde & BarberaÃÄ, 1994; Andersson, Svensson, & Ehlers, 2010).
To circumvent this impossibility, approaches have been studied that focus on weaker
notions of truthfulness. For instance, Andersson et al. (2010) and Pathak (2013) consider
a notion of degree of manipulability which can be used to compare the ease of manipulation in allocation mechanisms, whereas the notion of weak strategy-proofness is considered
by Lindner (2010), i.e., cheating agents are always risking an actual loss, and are never
guaranteed to cheat successfully.
In this paper, we depart from the settings studied in all such earlier approaches, because
we are interested in applications where a form of verification is available to the mechanism
at the time of deciding monetary compensations among agents. In particular, we assume
that valuations as well as allocation scenarios are determined by objective properties of
goods and agents that can be observed and measured by a verifier, after an allocation is
performed and payments are to be computed. Note that only information on allocated
goods can be verified and hence used by the mechanism. In this framework, classical
impossibility results no longer hold. Indeed, we propose mechanisms for allocation problems
that enjoy a number of highly desirable properties, being in particular truthful, efficient,
budget-balanced, individually rational, and fair, even though agents with verified incorrect
declarations are not punished. Observe that having this kind of a-posteriori knowledge
406

Mechanisms for Fair Allocation Problems

at payment time is quite common to many applications. We also point out that in some
cases a thorough verification could also be performed in advance, in order to get the best
performances independently of agents‚Äô declarations. However, in practice this is not done
because of either money or time restrictions, so that it is more convenient to allocate goods
on the basis of agents‚Äô declarations (especially if a mechanism makes them honest enough).
Anyway, our results can be used even when the full information is known to the mechanism,
to provide a fair division enjoying a number of desirable properties listed below.
Appendix A reports a number of examples of possible applications of the proposed
framework, including the real-world case of the Italian research-assessment program, which
first motivated this work.
For completeness, we leave the section by recalling that our work, as well as the above
mentioned related literature, deals with a setting where monetary transfers are allowed. In
fact, fair division of indivisible goods without money transfers has also attracted attention
in the literature. For instance, this topic has been studied by Bouveret and Lang (2008)
from the points of view of compact representation (for expressing preference relations)
and computational complexity (of reasoning about efficiency and fairness concepts in the
resulting framework), and by Lipton, Markakis, Mossel, and Saberi (2004) from the point of
view of defining approximation schemes for envy-freeness. Finally, it is relevant to point out
that, in our paper and in the papers discussed above, allocations are assumed to be computed
in a centralized way. However, it might be relevant in some cases to adopt decentralized
approaches, based on successive negotiations of goods (and of money) between groups of
agents. The reader interested in distributed negotiation frameworks is referred to the work
by Sandholm (1998), Dunne, Wooldridge, and Laurence (2005), Dunne (2005), and Endriss,
Maudet, Sadri, and Toni (2006), and to the references therein.
1.2 Contributions
In this paper, we study allocation problems in a strategic setting where agents can misreport
their private types, and we study mechanisms with verification from both the algorithmic
and the computational complexity viewpoint.
1.2.1 Algorithmic Issues
We show that in the given setting none of the classical impossibility theorems discussed
above holds. In particular, we exhibit a payment rule pŒæ that turns any optimal allocation
algorithm, i.e., any algorithm computing an optimal allocation given the reported types,
into a mechanism with verification such that:
I The mechanism is truthful. This is shown by pointing out a number of properties of
allocation problems which are of interest on their own.
I The mechanism is efficient, budget-balanced, individually rational, envy-free, and Pareto
efficient.
I The payment rule is indifferent w.r.t. the values (possibly misreports) declared for goods
that do not occur in the allocation being selected (and hence that are not verified).

407

Greco & Scarcello

I For each agent, her/his utility (when truthtelling) is the maximum one over all possible
allocations. In particular, the utility is indifferent w.r.t. the specific choice of allocated
goods in optimal allocations. Note that this is a strong fairness property, which immediately entails envy-freeness and Pareto-efficiency.
I Verification is not used to force truthfulness by just punishing those agents whose reported values are found different from the verified ones, so that the mechanism is ‚Äúforgiving‚Äù in the sense recently discussed by Feige and Tennenholtz (2011). Moreover, the
mechanism is shown to be tolerant of discrepancies emerging between declared types and
verified/true ones. That is, all its properties hold at the equilibrium where all agents
report their true types, and are also preserved approximatively in case of discrepancies,
with a guarantee that is within a constant factor from the ‚Äúdistance‚Äù between declared
types and verified/true ones. Note that this is generally not possible in mechanisms
based on punishment approaches where, to enforce truthfulness, the punishment might
be disproportional to the harm done by misreporting (cf. Feige & Tennenholtz, 2011).
I Agents‚Äô utilities are distributed according to the Shapley value of two suitably-associated
coalitional games‚Äîsee, e.g., (Nisan et al., 2007), for a comprehensive introduction to
sharing problems and coalitional games. In fact, the Shapley value is a prototypical
solution concept for fair division with monetary compensations,1 and its desirable properties in (games associated with) allocation problems have been extensively studied in
the literature (e.g., Moulin, 1992; Maniquet, 2003; Mishra & Rangarajan, 2007).
Note that the Shapely value has been studied in mechanism-design contexts too, where
emphasis has been given to the pricing problem for a service provider (Moulin & Shenker,
2001; Moulin, 1999; Jain & Vazirani, 2001): The cost of providing a service is a function
of the sets of customers, and the goal is that of determining which customers (and at
what price) have to receive it. The model gives rise to a cross-monotonic cost-sharing
game, where Shapley-value based sharing mechanisms can be defined that are truthful and
budget-balanced, and which achieve the lowest worst-case loss of efficiency over all utility
profiles (Moulin & Shenker, 2001). With this respect, our pricing rule pŒæ can abstractly be
viewed as a witness that, whenever (partial) verification is possible, Shapley-valued based
mechanisms may also be implemented with no loss of efficiency at all.
1.2.2 Complexity Issues
Computing an optimal allocation on the basis of the reported types is an easy task, which
can be carried out via adaptations of classical matching algorithms. However, one might
suspect that computing payments is not computationally-efficient, as it is based on the
computation of a Shapley value. This is indeed a challenging task that involves iterating
over all possible subsets of agents. We analyze these issues, and we provide the following
contributions:
I We show that computing the Shapley value for allocation problems is inherently intractable, in fact, #P-complete. Note that #P-hardness results for problems involving
1. Depending on the application, solutions concepts different from the Shapley value might be more appropriate. For instance, in bankruptcy problems, the nucleolus is considered as the most appropriate
solution concept for fair distribution (Aumann & Maschler, 1985).

408

Mechanisms for Fair Allocation Problems

Shapley value computation have been proven in the literature, for instance, for weighted
voting games (Deng & Papadimitriou, 1994), minimum spanning-tree games (Nagamochi, Zeng, Kabutoya, & Ibaraki, 1997), and games associated with normative systems (AÃägotnes, van der Hoek, Tennenholtz, & Wooldridge, 2009). Moreover, #Phardness results have been established for the Banzhaf power index, which is a solution
concept closely related to the Shapley value (see, e.g., Bachrach & Rosenschein, 2009,
2008; Bachrach, Zuckerman, Wooldridge, & Rosenschein, 2013).
I Therefore, in order to deal also with scenarios involving a large number of agents, two
modified rules, pÃÇŒæ and pÃÑŒæ , are presented, which allow us to employ a fully polynomialtime randomized approximation scheme for the Shapley value computation. The resulting polynomial-time mechanisms retain most of the properties of pŒæ . In particular,
the mechanism based on pÃÇŒæ is universally truthful, efficient, and with high-probability
approximately budget-balanced. Instead, the mechanism based on pÃÑŒæ is truthful in expectation, but it is always efficient and budget-balanced.
1.2.3 Organization
The rest of the paper is organized as follows. Section 2 illustrates the formal framework and
the basic concepts to design mechanisms with verification, whose desirable properties are
illustrated in Section 3. The payment rule pŒæ is defined in Section 4, and its connections
with coalitional games are pointed out in Section 5. Rules pÃÇŒæ and pÃÑŒæ are defined in Section 6,
where computational issues are dealt with. A comparison with related works is reported in
Section 7, and a few concluding remarks are discussed in Section 8. Finally, Appendix A
illustrates a real-world case study and further application examples of the notions presented
in the paper.

2. Formal Framework
In this section, we define a formal framework for studying allocation problems based on
mechanism design tools. In particular, we focus on mechanisms equipped with a verification
ability that meets the ‚Äúno-punishment‚Äù perspective.
2.1 Allocation Scenarios
We focus on allocation problems where goods from a set G have to be allocated to a set of
agents A = {1, ..., n} in such a way that the overall value of the allocated goods is maximum
over all feasible allocations, that is, the social welfare is maximized. More precisely, an
allocation is a function œÄ : A ‚Üí 2G mapping each agent i ‚àà A into a non-empty set of
goods œÄ(i) ‚äÜ G such that œÄ(i) ‚à© œÄ(j) = ‚àÖ, for each j 6= i. Moreover, we are given a vector
of upper-bound constraints Œ∂ = (Œ∂1 , ..., Œ∂n ) that specifies the maximum number of goods Œ∂i
that can be assigned to any agent i ‚àà A. The tuple S = hA, G, Œ∂i is called an allocation
scenario. A mapping œÄ : A ‚Üí 2G is a feasible allocation for the scenario S if it is an
allocation of goods in G to agents in A such that, for each agent i ‚àà A, |œÄ(i)| ‚â§ Œ∂i holds.
Observe that it is not required that all goods from G are allocated to the agents.
Note that in most applications the maximum number of goods Œ∂i that can be assigned
to agent i represents some ability of i (e.g., the maximum number of tasks that she/he
409

Greco & Scarcello

can execute), hence we next represent it as a function Œ∂i = fu (œái ), where œái is an objective
property of the agent (e.g., her/his speed) and fu is a public-knowledge computable function.
Note that the setting is more general than earlier approaches in the literature for which
upper bounds are fixed and independent of agents‚Äô features.
Moreover, in this paper we assume that the value of each good g for agent i is determined
by some objective property Œªg of the good, as well as by the property œái of the agent.
Formally, we assume that good valuations are encoded by a valuation vector w = (w1 , ..., wn )
where, for each i ‚àà A, i‚Äôs valuation function assigns to any good g ‚àà G a real value
wi (g) = fv (Œªg , œái ), for some public-knowledge computable function fv .2
The idea is that a verifier, best described in the next section, should be able to measure,
after an allocation œÄ of goods to agents has been performed, the objective property Œªg
of every allocated good g and the objective property œái of every agent. Therefore, by
using the (public) function fv , it is then possible to compute the values of all those goods
that have been assigned to some agent. We believe that this assumption about valuations
as functions of objective properties of goods and agents holds for many applications of
allocation problems, in particular for those where the social welfare is to be maximized
(since this is typically a measurable value). We provide some examples in Appendix A,
including the real-world application about the evaluation of the research activities in Italy,
which originally motivated the present work.
Let us fix an allocation scenario S = hA,
PG, Œ∂i and a valuation vector w = (w1 , ..., wn ).
Let œÄ be an allocation. Define
wi (œÄ) =
g‚ààœÄ(i) wi (g), for each i ‚àà A, and denote by
P
val(œÄ, w) the overall value i‚ààA wi (œÄ). We say that œÄ is optimal (for S) w.r.t. w if it is a
feasible allocation for S and there is no feasible allocation œÄ 0 for S such that val(œÄ 0 , w) >
val(œÄ, w). The value of an optimal allocation for S w.r.t. w is denoted by opt(S, w).
An allocation algorithm is a function A mapping each allocation scenario S and each
vector w to a feasible allocation A(S, w) for S. The algorithm is optimal if A(S, w) is an
optimal allocation w.r.t. w, for any given pair (S, w).
2.2 Strategic Issues and Verification
We consider a classical setting for mechanism design where optimal allocations have to be
computed in a context where neither the agent-depending upper bounds Œ∂ nor the valuation
vector w is known to the allocation algorithm. Therefore, even having an optimal algorithm
A at hand, we do not have enough information to find an optimal allocation, in general.
In fact, we assume as usual that each agent i ‚àà A privately knows certain features,
called i‚Äôs type, determining both the maximum number of goods Œ∂i that can be allocated to
her/him, as well as the function wi encoding her/his preferences over the allocations. Note
that, in our setting, the type of agent i naturally consists of her/his characterizing property
œái plus the property Œªg , for any good g (s)he is interested in. In Section 3.2, we also consider
what happens when agent i may have a subjective, possibly incorrect, perception of such
properties (including her/his characterizing property œái ).
2. Note that, for the sake of simplicity, we use two functions fu and fv for all agents. However, nothing
changes in the paper if we consider a slightly more general version where different agents may have
different public functions.

410

Mechanisms for Fair Allocation Problems

Figure 1: Running example in Section 2.
We assume that the type of agent i is taken from a set Œòi of available types, and we
denote by Œò the cartesian product Œò1 √ó ¬∑ ¬∑ ¬∑ √ó Œòn of all possible agents‚Äô types. Then, we
consider direct revelation mechanisms where agents are asked to report such types to let the
mechanism compute an allocation. While doing so, agents are self-interested, and strategic
issues come into play.
For each agent i ‚àà A, we hereinafter assume that ti ‚àà Œòi always denotes the true type
of agent i, i.e., the type owned as private knowledge, and that di ‚àà Œòi is her/his declared
type. Then, t = (t1 , ..., tn ) and d = (d1 , ..., dn ) are the vectors of true and declared types,
respectively. In general, it can happen that d does not coincide with t, if agents find
convenient to misreport their types.
For any (true or declared) type vector Œ∏ = (Œ∏1 , ..., Œ∏n ) ‚àà Œò, we denote hereinafter
by SŒ∏ = hA, G, Œ∂ Œ∏ i the allocation scenario for Œ∏, that is, the scenario where the vector
Œ∂ Œ∏ = (Œ∂1 , ..., Œ∂n ) is such that, for each i ‚àà A, Œ∂i is the upper bound determined by i‚Äôs
type Œ∏i . Similarly, we denote by wŒ∏ = (w1 , ..., wn ) the valuation vector such that wi is the
valuation function determined by i‚Äôs type Œ∏i , for each i ‚àà A.
Example 2.1. Let us consider two agents, a1 and a2 , the type vector t = (t1 , t2 ), and
the allocation scenario St = hA, G, Œ∂ t i illustrated in Figure 1(I) by means of an intuitive
graphical notation, with A = {a1 , a2 }, G = {g1 , ..., g8 }, and Œ∂ t = (3, 3). Moreover, consider
the valuation vector w with wt = (wa1 , wa2 ) such that, if there is an edge connecting
a1 (resp., a2 ) with gj in Figure 1(I), then wa1 (gj ) (resp., wa2 (gj )) is the value associated
with it. Otherwise, i.e., if there is no edge connecting a1 (resp., a2 ) with gj , then wa1 (gj )
(resp., wa2 (gj )) is some negative number, say ‚àí1. Given this setting, it is easily seen that
an optimal allocation for St w.r.t. wt is the allocation œÄ ‚àó where œÄ ‚àó (a1 ) = {g1 , g2 , g4 } and
œÄ ‚àó (a2 ) = {g5 , g7 , g8 }‚Äîsee Figure 1(II). Note that wa1 (œÄ ‚àó ) = 25 and wa2 (œÄ ‚àó ) = 26.
Consider now the allocation œÄÃÇ ‚àó of Figure 1(III). Note that œÄÃÇ ‚àó is another optimal allocation. However, we have wa1 (œÄÃÇ ‚àó ) = 26 > wa1 (œÄ ‚àó ) and wa2 (œÄÃÇ ‚àó ) = 25 < wa2 (œÄ ‚àó ).


411

Greco & Scarcello

Figure 2: Strategic manipulation.
Example 2.2. Consider again the setting of Example 2.1, where the type vector t is private
knowledge of the agents. Moreover, assume that the vector d of declared types is such that
Sd = St (the allocation scenario is the correct one) and that the vector wd is the one
illustrated in Figure 2(I), where negative edges are omitted. Basically, agent a2 truthfully
reports her/his type, while agent a1 reports a type for which the values of the goods g2
and g3 are underestimated. Therefore, wd 6= wt holds. The only optimal allocation œÄÃÑ ‚àó for
Sd w.r.t. wd is shown in Figure 2(II). It emerges that, because of the declarations of agent
a1 , it is not convenient to include g2 and g3 in œÄÃÑ ‚àó . In fact, œÄÃÑ ‚àó coincides with the optimal
allocation œÄÃÇ ‚àó depicted in Figure 1(III). Hence, by misreporting the type, agent a1 has now
the guarantee that the overall value of the goods assigned to her/him is 26. Instead, by
truthfully reporting the type, a1 might risk that the allocation œÄÃÇ ‚àó is selected, where the
overall value of the goods assigned to her/him is only 25.
Finally, consider a slight variant of the problem instance where the actual value of good
g7 is 6 (instead of 8) for a2 . Then, the above egoistic behavior of agent a1 also leads to an
allocation that is no longer optimal. Indeed, due to the low declared values for g2 and g3 ,
the good g7 is selected and allocated to a2 in the unique (wrong) optimal allocation, whose
total value is now 49 (instead of 51).

In this paper, we focus on allocation problems where some kind of verification on reported types is possible, because objective properties of agents and goods can be observed
and measured (we say verified, hereinafter) by a third-party verifier after an allocation œÄ is
performed. Recall that
Sin general only a subset of goods is actually assigned to some agent
by œÄ. Denote the set i‚ààA œÄ(i) ‚äÜ G of allocated goods by img(œÄ). Thus, in our model, for
each good g ‚àà img(œÄ), its objective property Œªg can be verified, while nothing can be said
for non-allocated and hence non-observed goods. Moreover, for each agent i ‚àà A participating in the mechanism, i‚Äôs property œái that determines the upper-bound constraint and
i‚Äôs preferences over good allocations is verifiable, too. Thus, by using such a verifier, the
correct allocation scenario and a restriction over img(œÄ) of the valuation functions can be
determined, as formalized next.
Definition 2.3. Let t be the vector of true types. Then, the verifier v (for t) is the function
mapping any allocation œÄ to the pair Sv(œÄ) , wv(œÄ) such that:
(1) Sv(œÄ) is the actual allocation scenario St (in particular, the ‚Äútrue‚Äù upper bound fu (œái )
is computed by the verifier for each agent i); and
(2) wv(œÄ) = (wv1 , ..., wvn ) is the vector such that, for each agent i ‚àà A, wvi : img(œÄ) ‚Üí R
is the function assigning to each good g ‚àà img(œÄ) its actual value fv (Œªg , œái ) for agent
412

Mechanisms for Fair Allocation Problems

i. Observe that, by definition of the framework, wvi coincides with the restriction over
img(œÄ) of the valuation function wti : G ‚Üí R determined by i‚Äôs (true) type ti .
2
It is worthwhile noting that the verifier is in general unable to discover whether some
agent misreported her/his type, because goods that are not allocated do not undergo any
measurement process. We pinpoint that this is actually the case in practical applications,
where measurements over non-allocated goods may be technically unfeasible or simply too
expensive (in money or time). We refer the reader to Appendix A.1 (in particular, subsections A.1.1‚ÄìA.1.3) for an exemplification of these notions in the real-world case study of the
Italian research evaluation (VQR).
Example 2.4. Consider again the setting of Example 2.2, and recall that agent a1 finds it
convenient to underestimate the true values of g2 and g3 . However, since g2 and g3 are not
selected in œÄÃÑ ‚àó , as we can see in Figure 2(II), then there is no way to discover that a1 has
actually misreported her/his type.

In some cases, the constraints in the allocation scenario depend only on the specific
application and not on agents‚Äô types, so that item (1) above is immaterial. More generally,
however, the proposed setting allows us to model classes of problems where types play a
role even in the definitions of upper-bound constraints.
For completeness, we remark that all results in the paper can easily be shown to hold
even for allocation problems where every agent must get a minimum number of goods
greater than one (defined by the specific application). However, for the sake of presentation
we prefer to keep the standard setting where any non-empty set of goods can be assigned
to each agent, as long as her/his upper bound constraint is met.
2.3 Payment Rules and Mechanisms with Verification
In order to encourage agents to truthfully report their private types, we design mechanisms
where monetary transfers can be performed, after the verification process.
For the sake of presentation, let us assume that St = hA, G, Œ∂ t i is an allocation scenario
(recall that t denotes the vector of true types), that d is a vector of declared types with an
associated allocation scenario Sd = hA, G, Œ∂ d i, and that v is the verifier (for t).
A payment rule p is defined as a vector of functions (p1 , ..., pn ), with pi (œÄ, wd , v) being
some amount of money that is given to agent i, on the basis of a given allocation œÄ, the
vector of declared valuations wd , and the verifier v. Observe that, with this notation, any
negative value pi (œÄ, wd , v) means that some amount of money is charged to i. Let wt be
the vector (w1 , ..., wn ). Then, i‚Äôs (quasi-linear) utility under p, sometimes called individual
welfare, is defined as the value ui,p (œÄ, wd , v) = wi (œÄ) + pi (œÄ, wd , v). As the verifier v for t
is always understood, ui,p (œÄ, wd , v) and pi (œÄ, wd , v) are simply denoted by ui,p (œÄ, wd ) and
pi (œÄ, wd ), respectively. Moreover, whenever the payment rule is also understood from the
context, i‚Äôs utility is simply denoted as ui (œÄ, wd ).
As payments can be computed after the verification process, to define the amount of
money pi (œÄ, wd ) to be paid to agent i, we exploit the verifier v. Accordingly, it is desirable
that goods that are not allocated, and hence not verified, play no role in the definition of
the payments. This latter property is formalized below.

413

Greco & Scarcello

 verifiability: Let d0 and d00 be two type vectors, and let wd0 = (w10 , ..., wn0 ) and wd00 =
(w100 , ..., wn00 ) be their associated valuations. Moreover, let œÄ be any mapping that is a
feasible allocation for both scenarios Sd0 and Sd00 . Then, for each i ‚àà A, pi (œÄ, wd0 ) =
pi (œÄ, wd00 ) whenever wi0 (g) = wi00 (g) holds for every allocated good g ‚àà img(œÄ). Therefore,
d0 and d00 are undistinguishable as far as the computation of the payments is concerned,
even if they differ on some unallocated goods. That is, the payment rule depends only
on goods subject to the verifier evaluation.
A mechanism with verification is a pair (A, p), where A is an allocation algorithm and
p is a payment rule that can exploit the verifier v. The mechanism (A, p) can be viewed
as consisting of the following two-phases: First, agents report a declaration vector d, and
a feasible allocation œÄ = A(wd ) for Sd is computed. Second, v(œÄ) is made available, and
payments under a given rule p are calculated with respect to the allocation œÄ and the
valuations wd , by exploiting the knowledge of v(œÄ). Our goal is to design a payment rule p
guaranteeing that declared types in d lead to an allocation œÄ maximizing the social welfare,
i.e., such that œÄ is an optimal allocation for St w.r.t. wt . This might be problematic as, in
our setting, even the fact that œÄ is a feasible allocation for St is not guaranteed, because
the allocation scenario depends on the types of the agents and we might have St 6= Sd .
In order to accomplish the above goal, we use an optimal allocation algorithm A, and
we need that p encourages agents to truthfully report their private types. Formally, for any
type vector Œ∏ = (Œ∏1 , ..., Œ∏n ) ‚àà Œò and for any type Œ∏ÃÑi ‚àà Œòi , with i ‚àà A, let (Œ∏ÃÑi , Œ∏‚àíi ) be the
type vector (Œ∏1 , ..., Œ∏i‚àí1 , Œ∏ÃÑi , Œ∏i+1 , ..., Œ∏n ) ‚àà Œò. Then, we shall consider the following concept
of truthful mechanism.
Definition 2.5. Let (A, p) be a mechanism with verification, and let i be any agent in A.
We say that Œ∏ÃÑi is a dominant strategy of agent i w.r.t. (A, p) if, for each type vector Œ∏ ‚àà Œò,
ui (A(w(Œ∏ÃÑi ,Œ∏‚àíi ) ), w(Œ∏ÃÑi ,Œ∏‚àíi ) ) ‚â• ui (A(wŒ∏ ), wŒ∏ ) holds. The mechanism (A, p) is truthful if, for
each i ‚àà A, ti is a dominant strategy.
2
Example 2.6. Consider again the setting discussed in Example 2.2, and the trivial payment
rule p‚ó¶ where no payment is actually performed. Consider again the optimal allocation œÄ ‚àó
(for St w.r.t. wt ) depicted in Figure 1(II), and note that ua1 (œÄ ‚àó , wt ) = 25. Instead, for the
allocation œÄÃÑ ‚àó depicted in Figure 2(II), we have ua1 (œÄÃÑ ‚àó , wt ) = 26, with œÄÃÑ ‚àó being the unique
optimal allocation for Sd = St w.r.t. wd , where d = (da1 , t‚àía1 ).
Therefore, any mechanism (A, p‚ó¶ ), with A being an optimal allocation algorithm such
that A(wt ) = œÄ ‚àó , is not truthful. More generally, for each optimal allocation algorithm A,
an example witnessing that (A, p‚ó¶ ) is not truthful can be easily defined by suitably adapting
the above. Hence, non-trivial payment rules are necessary to encourage agents to truthfully
report their types.

A comparison of our approach to verification with existing ones is reported in Section 7.

3. Properties of (Truthful) Mechanisms
In this section, we discuss desiderata for mechanisms that lead to fair allocations and that
are tolerant of uncertain inputs. Note that the design of mechanisms that are able to
deal with these two issues is of interest even in settings that are not strategic, i.e., even
414

Mechanisms for Fair Allocation Problems

when we are granted that all agents truthfully report their types. Exemplifications of the
proposed notions are reported Appendix A.1.4, which completes the description of the case
study regarding the Italian research evaluation (VQR). Further examples are described in
Appendix A.2 and Appendix A.3.
3.1 Fairness Issues and Further Desirable Properties
Let (A, p) be any truthful mechanism with verification. In the paper, we focus on a number
of (ex-post) properties of such a mechanism, to be checked at the equilibrium t where agents
truthfully report their private types.
 (allocative) efficiency: A(wt ) is an optimal allocation for St w.r.t. wt . That is, the social
welfare is maximized.
 individual rationality: ui (A(w(ti ,Œ∏‚àíi ) ), w(ti ,Œ∏‚àíi ) ) ‚â• 0, for each agent i ‚àà A and for each
type vector Œ∏‚àíi for the agents in A \ {i}. Hence, voluntary participation of each agent to
take part in the allocation problem is encouraged (independently of whether the other
agents actually report their true types or not).
P
 (strong) budget-balance: i‚ààA pi (A(wt ), wt ) = 0. In other words, there is no transfer of
money out or into the scenario.
 envy-freeness: for each pair of agents i, j ‚àà A, and for each feasible allocation œÄ for St
such that œÄ(i) ‚à© A(wt )(j) 6= ‚àÖ, ui (A(wt ), wt ) ‚â• ui (œÄ, wt ).
 Pareto-efficiency: there is no feasible allocation œÄ for St such that: (1) ui (œÄ, wt ) ‚â•
ui (A(wt ), wt ), for each agent i ‚àà A, and (2) there is an agent j ‚àà A with uj (œÄ, wt ) >
uj (A(wt ), wt ). That is, A(wt ) is not Pareto-dominated by any other allocation.
 equal treatment of equals: for each pair of agents i, j ‚àà A such that Œ∂i = Œ∂j and wi = wj ,
with wt = (w1 , ..., wn ), it must be the case that ui (A(wt ), wt ) = uj (A(wt ), wt ).
 individual optimality: ui (A(wt ), wt ) ‚â• ui (œÄ, wt ), for each i ‚àà A and each feasible allocation œÄ for St .
Note that all the above properties make sense even in settings that are not strategic,
but where the goal is to compute a fair allocation. Indeed, in a setting that is not strategic,
agents report their true types, even without any monetary incentive. However, without such
payments, fairness cannot be achieved in general: just think, for instance, about a scenario
with two agents, a1 and a2 , and one good g having the same value for both agents. In this
case, no matter which optimal allocation is considered (where g is assigned to either a1 or
a2 ), one of the two agents would envy the other. This makes it clear that payment rules
play not only the role to encourage agents to reports their true types, but they are also
crucial to induce agents to perceive a given allocation as a fair one. In fact, all the above
properties, but the last, have been classically considered in the context of fair allocation
problems, also in absence of strategic issues.
Here, we have additionally considered individual optimality, which is readily seen to
imply both envy-freeness and Pareto-efficiency. It also entails that there is a unique possible
415

Greco & Scarcello

vector of utilities for agents. In particular, this means that agents‚Äô utilities are not sensible
to possible alternative allocations, and hence are independent of the specific set of allocated
goods selected by the optimal algorithm A.
Example 3.1. Consider the trivial payment rule p‚ó¶ discussed in Example 2.6. Consider
the optimal allocation œÄ ‚àó depicted in Figure 1(II), and compare it with the allocation œÄÃÇ ‚àó
of Figure 1(III). Recall that œÄÃÇ ‚àó is another optimal allocation (for St w.r.t. wt ). Moreover,
under p‚ó¶ , ua1 (œÄ ‚àó , wt ) = 25 and ua1 (œÄÃÇ ‚àó , wt ) = 26 hold. Therefore, while from the optimization perspective the choice between œÄ ‚àó and œÄÃÇ ‚àó is immaterial, a1 might have good arguments
to complain if œÄ ‚àó is selected in place of œÄÃÇ ‚àó . In fact, p‚ó¶ is not ‚Äúindividually optimal‚Äù.

Individual optimality is definitely a very desirable requirement, but we still miss something. Indeed, notice that this property is trivially satisfied by the fully ‚Äúuniform‚Äù payment
rule, which guarantees that each agent gets the same utility, no matter of her/his valuation
of goods. Of course, this is not desirable in general. Rather, meritocracy should be somehow
addressed, so that a true fair rule should reflect the actual ‚Äúcontribution‚Äù of each agent to
the overall value of the allocation.
Let œÄ be an allocation for St , not necessarily an optimal one w.r.t. wt , and define the
marginal contribution of a non-empty set C ‚äÜ A of agents to œÄ w.r.t. wt as the value:
margœÄ,wt (C) = opt(hA, img(œÄ), Œ∂ t i, wt ) ‚àí opt(hA \ C, img(œÄ), Œ∂ t i, wt ).

(1)

In words, the marginal contribution margœÄ,wt (C) of the agents in C assesses the loss of
the overall value of œÄ we would register if the agents in C were not part of the problem.
Example 3.2. Consider the setting of Example 2.1 and the optimal allocation œÄ ‚àó of Figure 1(II). Note that margœÄ‚àó ,wt ({a1 }) = margœÄ‚àó ,wt ({a2 }) = 51 ‚àí 26 = 25. Therefore, the two
agents (viewed as singletons) have the same marginal contribution, and defining a payment
rule that leads to utilities equally sharing the overall value of 51 is a natural option.
Consider now a different setting, where true types induce the same allocation scenario
St and the same vector wt , but for the valuation of good g8 for agent a2 , which is now
110 instead of 10. In this case, œÄ ‚àó would still remain an optimal allocation, with overall
value 151. Moreover, the marginal contribution of a1 is not affected by the modification,
while the marginal contribution of a2 would become 151 ‚àí 26 = 125. This witnesses that a2
contributes more than a1 , and a payment rule leading to equally sharing the overall value
can be no longer perceived as a fair one.

The intuition conveyed by the above example can be then formalized via the following
requirement.
P
 marginality: For each non-empty set C ‚äÜ A of agents,
i‚ààC ui (A(wt ), wt ) ‚â•
margA(wt ),wt (C). Hence, each group of agents gets at least its own marginal contribution
to the given allocation.
3.2 Sensing and Errors
We conclude the presentation of our (strategic) setting for fair allocation problems by illustrating some subtle issues arising with the process of verification. The starting point of our
416

Mechanisms for Fair Allocation Problems

discussion is the observation that verifiers can be practically implemented by sensing some
parameters (in our setting the parameters œái , for each agent i, and Œªg , for each allocated
good g) that become observable after the allocation is performed and that, in real-world
applications, sensing is subject to errors; for instance, because of the limited precision of
the measurement instruments. Therefore, it is unrealistic to assume that the verifier is
always able to exactly discover (i.e., with arbitrary precision) the actual upper bounds in
the scenario St and the valuation vector wt , and it might be problematic to decide whether
an observed discrepancy between verified values and declared ones is due to a strategic
behavior or to such sensing errors. In fact, sensing troubles arise even in settings where all
relevant information is available as public knowledge that can be acquired via sensing the
environment, i.e., even by getting rid of any strategic consideration.
As discussed in the Introduction, this issue has been pointed out in the recent work by
Feige and Tennenholtz (2011), though from a slightly different perspective. There, it is observed that mechanisms with verification are often designed in way that performs well when
agents have accurate information about their private inputs, but might perform arbitrarily
bad when agents are uncertain of their private features. Uncertainty might be again the
result of ‚Äúhardware‚Äù measurement errors, or due to the limited computational resources
employed by agents for identifying the declared valuations. For instance, in our setting, the
type of an agent i consists of her/his characterizing property œái plus the property Œªg , for
each good g (s)he is interested in. According to the perspective of Feige and Tennenholtz
(2011), agent i might only have an estimate of this type. For instance, the agent might
not have enough computational resources to precisely determine such properties Œªg for all
goods g, so that i‚Äôs type actually represent just a subjective perception of them.
In the light of the above observations, it clearly emerges that ‚Äúpunishing‚Äù agents might
be effective in mathematical studies, but very inappropriate in real life situations in which
uncertainty is inherent due to measurements errors or uncertain inputs. Therefore, in addition to the requirements discussed so far, another desirable property is for the mechanism
to use no punishment (or to be ‚Äúforgiving‚Äù, in the sense of Feige & Tennenholtz, 2011).
 no punishment: For each type vector Œ∏ ‚àà Œò, for each feasible allocation œÄ for SŒ∏ , and for
each agent i ‚àà A, it is the case that pi (œÄ, wŒ∏ ) = pi (œÄ, w(ti ,Œ∏‚àíi ) ). That is, discrepancies
between the given type (possibly declared) and the true/verified one do not have any
impact on the payment to agent i. In other words, we may think of payments being
always computed under the presumption of innocence, where incorrect declared values
do not mean manipulation attempts by the agents.
Moreover, if we admit that sensing errors (or uncertain inputs) might occur, then it
is relevant to quantitatively assess their impact, too. Ideally, we would like to deal with
mechanisms that can tolerate such errors, in the sense that ‚Äúsmall‚Äù errors should determine
small deviations from the outcome we would have obtained with no errors at all. Note
that this is generally not possible in mechanisms based on punishment approaches where,
to enforce truthfulness, the punishment might be disproportional to the harm done by
misreporting (cf. Feige & Tennenholtz, 2011). We next formalize our final desideratum.
For any type vector Œ∏, for any set C ‚äÜ A of agents, and for any set G0 ‚äÜ G of goods,
let us define hC, G0 , Œ∂ Œ∏ i as the restriction of the scenario hA, G, Œ∂ Œ∏ i where only agents in C
417

Greco & Scarcello

and goods in G0 are considered.3 Moreover, given two type vectors Œ∏ and Œ∏ÃÑ, we denote by
Œò[Œ∏, Œ∏ÃÑ] the set of all type vectors of the form (X1 , ..., Xn ), where Xi ‚àà {Œ∏i , Œ∏ÃÑi }, for each
i ‚àà A. Then, the ‚Äòdistance‚Äù dist w (Œ∏, Œ∏ÃÑ) between Œ∏ and Œ∏ÃÑ under the valuation vector w
(or just dist(Œ∏, Œ∏ÃÑ), if w is understood from the context) is defined by looking at the worst
possible impact that type vectors in Œò[Œ∏, Œ∏ÃÑ] may have on the optimal solutions computed
over all possible restrictions of the given setting:
dist w (Œ∏, Œ∏ÃÑ) =

max

C‚äÜA, G0 ‚äÜG, Œ∏ 0 ,Œ∏ 00 ‚ààŒò[Œ∏,Œ∏ÃÑ]

|opt(hC, G0 , Œ∂ Œ∏0 i, wŒ∏0 ) ‚àí opt(hC, G0 , Œ∂ Œ∏00 i, wŒ∏00 )|.

Now, recall that in a truthful mechanism it is always convenient for the agents to report
the true types. Assume however that agents declare a type vector tÃÑ different from their
true type vector t.4 As a consequence, we get a revealed setting StÃÑ and a valuation vector
wtÃÑ = (wÃÑ1 , ..., wÃÑn ), while the available verifier v discloses information about the scenario St
and the vector wt = (w1 , ..., wn ) (restricted to allocated goods). In standard mechanisms
design settings (and in particular under punishment approaches), no guarantee on any
property could be given in this case, as mechanisms are designed to be analyzed when
reports are truthful. Instead, we would like here that mechanisms are tolerant of sensing
errors, as formalized below.
 error tolerance: There is a constant c ‚â• 0 such that, for each type vector tÃÑ and for each
agent i ‚àà A, |ui (A(wt ), wt ) ‚àí ui (A(wtÃÑ ), wtÃÑ )| ‚â§ c √ó dist(t, tÃÑ).
Intuitively, under an error tolerant mechanism, the consequences of errors over good
allocation outcomes produce a linear ‚Äúdistorting effect‚Äù over agents‚Äô utilities (and, in turn
on the various properties of the mechanism). In particular, the above property is stated
without any assumption about how sensing errors come into play. Indeed, the notion of
dist(t, tÃÑ) formalizes these errors from a global perspective. For instance, we do not require
that errors affect uniformly the valuations of agents, and it might be well the case that
errors are biased towards some specific agent.

4. Mechanisms with Verification for Allocation Problems
In this section, we introduce a mechanism with verification for allocation problems and start
its analysis, by preliminary proving some properties that hold over optimal allocations.
4.1 General Properties of Allocation Problems
Let Œ∏ ‚àà Œò be any given type vector, and consider the allocation scenario SŒ∏ = hA, G, Œ∂ Œ∏ i,
where Œ∂ Œ∏ = (Œ∂1 , ..., Œ∂n ), together with the valuations given by wŒ∏ = (w1 , ..., wn ). We start
3. Note the little abuse of notation: the vector Œ∂ in hC, G0 , Œ∂i should be in fact its restriction over C. However,
to keep the notation simple, we just write Œ∂, as no confusion may arise. Similarly, any valuation vector
w for A will be transparently considered as a valuation vector for any subset of agents C ‚äÜ A‚Äîwe just
get rid of the unused components associated with agents in A \ C.
4. The property is discussed from the perspective of uncertain inputs. The adaptation to the case of
verification errors (or to the case when both types of errors occur) is an easy task, as it is mainly a
matter of different interpretation of concepts.

418

Mechanisms for Fair Allocation Problems

Figure 3: One-good version of the allocation problem in Example 4.1, with two allocations
and their associated update graph, as defined in the proof of Theorem 4.4. In the
graphical representation, crossing lines represent the edges of the bipartite cliques
connecting the two groups of virtual agents with the goods they are interested in.

by observing that the optimization problem used to allocate goods to agents can be equivalently reformulated in such a way that precisely one good can be allocated to each agent.
Intuitively, we may replace each agent i by Œ∂i fresh agents with the same valuations as i.
We remark that such an equivalence is just used for combinatorial optimization purposes,
i.e., without affecting any game theoretic issue.
Let us now formalize the above intuition. First, we denote by SŒ∏1 the one-good version
hA1 , G, 1i of the scenario SŒ∏ , where:
S
‚Ä¢ A1 is the set of agents i‚ààA clones(i) such that for each agent i ‚àà A, clones(i) is a
set of Œ∂i fresh agents;
‚Ä¢ 1 is the vector where all components are 1.
Moreover, we denote by wŒ∏1 the vector for agents in A1 where, for each agent c ‚àà A1 ,
the component wc1 associated with c is such that wc1 = wi , with i being the agent in A
for which c ‚àà clones(i) holds. Thus, in the allocation problem SŒ∏1 and by considering the
vector wŒ∏1 , each ‚Äúclone‚Äù c ‚àà clones(i) gets exactly one good, and has the same valuations
as agent i in wŒ∏ .
Example 4.1. Consider the scenario St = hA, {g1 , ..., g8 }, Œ∂ t i, with A = {a1 , a2 }, and the
vector wt = (wa1 , wa2 ) discussed in Example 2.1. The one-good version is the scenario
St1 = hA1 , {g1 , ..., g8 }, 1i shown Figure 3(I). Note that the set of agents in this scenario
is A1 = {(a1 )1 , (a1 )2 , (a1 )3 , (a2 )1 , (a2 )2 , (a2 )3 }, where clones(ai ) = {(ai )1 , (ai )2 , (ai )3 }, for
1
each i ‚àà {1, 2}. Indeed, recall that Œ∂ t = (3, 3). Finally, the vector wt1 is such that w(a
=
1 )h
1

wa1 (resp., w(a2 )h = wa2 ), for each h ‚àà {1, 2, 3}.
419

Greco & Scarcello

Now, consider the scenario hC, G0 , Œ∂ Œ∏ i, i.e., the restriction of hA, G, Œ∂ Œ∏ i where only agents
in C and goods in G0 are considered, and let œÄC1 be an allocation for S
its one-good version
0
0
1
G
hC, G , Œ∂ Œ∏ i . Consider the function œÄC : C ‚Üí 2 such that œÄC (i) = c‚ààclones(i) œÄC1 (c), for
each i ‚àà A. Note that |œÄC (i)| ‚â§ Œ∂i , for each i ‚àà A. Therefore, œÄC is a feasible allocation for
hC, G0 , Œ∂ Œ∏ i, denoted by Œ∂-good(œÄC1 ). Moreover, by construction, val(œÄC , wŒ∏ ) = val(œÄC1 , wŒ∏1 ).
Conversely, any feasible allocation œÄÃÑC for hC, G0 , Œ∂ Œ∏ i is associated with the non-empty set
one-good(œÄÃÑC ) of all those allocations œÄÃÑC1 for hC, G0 , Œ∂ Œ∏ i1 such that œÄÃÑC = Œ∂-good(œÄÃÑC1 ), also
called the one-good forms of œÄÃÑC . The following is immediate by definition of one-good
version and forms.
Fact 4.2. Let Œ∏ ‚àà Œò be any given type vector, let SŒ∏1 be the one-good version of SŒ∏ , and let
œÄC1 be an allocation for SŒ∏1 . Then, œÄC1 is an optimal allocation for SŒ∏1 w.r.t. wŒ∏1 if, and only
if, Œ∂-good(œÄC1 ) is an optimal allocation for SŒ∏ w.r.t. wŒ∏ .
Example 4.3. Consider again the setting of Example 4.1, and the allocation œÄ such that
œÄ(a1 ) = {g1 , g2 , g3 } and œÄ(a2 ) = {g5 , g7 , g8 }. Note that œÄ is not an optimal allocation
w.r.t. the valuation vector wt (reported in Figure 1(I)), because val(œÄ, wt ) = 50 while
opt(St , wt ) = 51 (see, e.g., the optimal allocation œÄ ‚àó in Figure 1(II)). Now, notice that the
allocation depicted in Figure 3(II) is indeed an associated one-good form allocation, which
is actually not an optimal allocation for St1 w.r.t. wt1 , by Fact 4.2.

We are now in the position of stating a property that holds on any optimal allocation œÄ.
The property is in fact of interest of its own, i.e., independently of its application to the
study of fair allocation problems. In words, it tells us that, whenever we are interested
in allocating goods to any subset of agents, we may safely consider only goods in img(œÄ),
rather than the whole set G. In our case, it is a basic technical ingredient for showing a
number of key properties because, intuitively, it allows us to get rid of alternative (optimal)
allocations, possibly based on non-allocated goods in G \ img(œÄ).
Theorem 4.4. Let Œ∏ ‚àà Œò be any given type vector, let œÄ be an optimal allocation for
hA, G, Œ∂ Œ∏ i w.r.t. wŒ∏ , and let C ‚äÜ A be a set of agents. Then, every optimal allocation for
hC, img(œÄ), Œ∂ Œ∏ i w.r.t. wŒ∏ is an optimal allocation for hC, G, Œ∂ Œ∏ i w.r.t. wŒ∏ .
Proof. Let C ‚äÜ A be any set of agents, and assume that Œ∑C is an optimal allocation for
hC, img(œÄ), Œ∂ Œ∏ i w.r.t. wŒ∏ . We shall show that Œ∑C is an optimal allocation for the unrestricted
problem hC, G, Œ∂ Œ∏ i w.r.t. wŒ∏ , too.
To this end, consider any optimal allocation ŒªC for the problem hC, G, Œ∂ Œ∏ i where all
goods in G are available to the agents in C. We next prove that val(Œ∑C , wŒ∏ ) = val(ŒªC , wŒ∏ ).
This clearly follows from the optimality of Œ∑C if img(ŒªC ) ‚äÜ img(œÄ) holds. Therefore, to
be strictly better than Œ∑C , ŒªC must allocate some good in G \ img(œÄ). Assume thus by
contradiction that val(Œ∑C , wŒ∏ ) < val(ŒªC , wŒ∏ ), and hence img(ŒªC ) 6‚äÜ img(œÄ), which entails
that img(œÄ) ‚äÇ G. Consider two allocations Œ∑C1 ‚àà one-good(Œ∑C ) and Œª1C ‚àà one-good(ŒªC ), and
observe first that: val(Œ∑C1 , wŒ∏1 ) = val(Œ∑C , wŒ∏ ) < val(ŒªC , wŒ∏ ) = val(Œª1C , wŒ∏1 ).
Let L be the set of agents whose good-assignment are the same according to these
allocations, i.e., L = {c ‚àà C 1 | Œª1C (c) = Œ∑C1 (c)}. Then, define ‚àÜ(Œ∑C1 , Œª1C ) = (C 1 \ L ‚à™ {s, t}, E)
to be the directed graph, called update graph for Œ∑C1 w.r.t. Œª1C , whose nodes are the agents
in C 1 that change their goods in the two allocations plus two distinguished nodes s and t,
and whose edges in E are defined as follows:
420

Mechanisms for Fair Allocation Problems

‚àí There is an edge from agent c to agent c0 if Œª1C (c0 ) = Œ∑C1 (c) 6= ‚àÖ;
‚àí There is an edge from s to agent c0 if there is no agent c such that Œª1C (c0 ) = Œ∑C1 (c) 6= ‚àÖ;
‚àí There is an edge from agent c to t if there is no agent c0 such that Œª1C (c0 ) = Œ∑C1 (c) 6= ‚àÖ;
‚àí No further edges are in E.
For an example construction, consider Figure 3(IV) showing the update graph for the allocation shown in Figure 3(II) w.r.t. the allocation shown in Figure 3(III).
As each agent gets one good in Œ∑C1 and Œª1C , each node in ‚àÜ(Œ∑C1 , Œª1C ) but s and t has exactly
one incoming edge and one outgoing edge. Moreover, by construction, s has no incoming
edge, and t has no outgoing edge. Thus, the update graph consists of a number of paths
from s to t and a number of cycles, all of them being disjoint from each other.
Let {œÑ1 , ..., œÑh } be the set of all possible paths from s to t or cycles in ‚àÜ(Œ∑C1 , Œª1C ), and
for a path or a cycle œÑi = Œ±1 , ..., Œ±m , let agents(œÑi ) be the set {Œ±1 , ..., Œ±m } \ {s, t}. In
addition, let us fix the following notation: For any function œÄ : X ‚Üí 2G over some given
domain X, let œÄ[X 0 ] denote the restriction of œÄ over the (sub-)domain X 0 ‚äÜ X. Moreover,
: X2 ‚Üí 2G2 over the two domains X1 and X2 ,
given two functions œÄ1 : X1 ‚Üí 2G1 and œÄ2 U
respectively,
= ‚àÖ, let œÄ1 œÄ2 : X1 ‚à™ X2 ‚Üí 2G1 ‚à™G2 be the function such
U such that X1 ‚à© X2 U
that (œÄ1 œÄ2 )[X1 ] = œÄ1 and (œÄ1 œÄ2 )[X2 ] = œÄ2 .
By the construction of the update graph, note that Œª1C can be expressed in terms of the
disjoint paths/cycles œÑ1 , ..., œÑh by the following expression:
Œ∑C1 [C 1 \

h
[

agents(œÑi )]

h
]

Œª1C [agents(œÑi )].

i=1

i=1

Observe now that, because val(Œ∑C1 , w1 ) < val(Œª1C , w1 ), there must exists a set of agents
agents(œÑk ), associated with some disjoint path/cycle œÑk , with 1 ‚â§ k ‚â§ h, such that the value
of the goods allocated to these agents according to Œª1C is greater than the corresponding
value for theUsame agents obtained with Œ∑C1 . Then, consider the function œÄœÑk = Œ∑C1 [C 1 \
agents(œÑk )] Œª1C [agents(œÑk )], and note first that œÄœÑk is an allocation for hC, G, Œ∂ Œ∏ i1 . In
particular, note that, for each agent c ‚àà C 1 , |œÄœÑk (c)| = 1 holds, because this
U constraint
actually holds on Œ∑C1 and Œª1C , and because of the definition of the operator ‚Äò ‚Äô. Moreover,
by the choice of œÑk , we also have val(œÄœÑk , wŒ∏1 ) > val(Œ∑C1 , wŒ∏1 ).
Note that if œÑk were either a cycle or a path of the form s, Œ±2 , . . . , Œ±m‚àí1 , t such that
Œª1C (Œ±2 ) ‚äÜ img(œÄ), then img(œÄœÑk ) ‚äÜ img(œÄ) would hold. Indeed, by definition of the edges
of the update graph, only the first node of a path (that is, Œ±2 in case œÑk is a path) may
be such that Œª1C (Œ±2 ) \ img(œÄ) 6= ‚àÖ. However, as observed above, this is impossible because
val(œÄœÑk , wŒ∏1 ) > val(Œ∑C1 , wŒ∏1 ) would contradict the optimality of Œ∑C1 , and hence the optimality
of Œ∑C , by Fact 4.2. Therefore, we can conclude that œÑk is a path of the form s, Œ±2 , . . . , Œ±m‚àí1 , t
with œÄœÑk (Œ±2 ) = Œª1C (Œ±2 ) = {g 0 } ‚àà G \ img(œÄ). That is, the allocation œÄœÑk (over the agents
in C 1 ) is such that img(œÄœÑk ) = {g 0 } ‚à™ img(Œ∑C1 ) \ Œ∑C1 (Œ±m‚àí1 ). In particular, observe that
Œ∑C1 (Œ±m‚àí1 ) ‚äÜ img(œÄ) \ img(œÄœÑk ) holds, by definition of the edges of the update graph and
since there is an edge from Œ±m‚àí1 to t.
Let us now come back to the optimal allocation œÄ for hA, G, Œ∂ Œ∏ i w.r.t. wŒ∏ , and let œÄ 1
be an (optimal) allocation in one-good(œÄ). Let A ‚äÜ C 1 be a set of agents with Œ±2 ‚àà A
421

Greco & Scarcello

Input:
Assumption:
Notation:
1.
2.
3.
4.
5.
6.
7.
8.

A type vector Œ∏ ‚àà Œò, and a feasible allocation œÄ for SŒ∏ ;
The verifier v (for t) is available, with v(œÄ) = (v1 , ..., vn );
SŒ∏ = hA, G, Œ∂ Œ∏ i, wŒ∏ = (w1 , ..., wn ), wv(œÄ) = (wv1 , ..., wvn );

Let C denote the set of all possible subsets of A;
For each i ‚àà A and C ‚àà C,
b Compute an optimal allocation œÄC,i for hC, img(œÄ), Œ∂ (vi ,Œ∏‚àíi ) i w.r.t. w(vi ,Œ∏‚àíi ) ;
For each agent i ‚àà A,
| For each set C ‚àà C such that i ‚àà P
C,
1
(=val(œÄC,i , w(vi ,Œ∏‚àíi ) ));
| | Let ‚àÜC,i (œÄ, Œ∏) := wvi (œÄC,i ) + j‚ààC\{i} wj (œÄC,i );
P
(=val(œÄC\{i},i , w(vi ,Œ∏‚àíi ) ));
| b Let ‚àÜ2C,i (œÄ, Œ∏) := j‚ààC\{i} wj (œÄC\{i},i );
P
(‚àÜ1C,i (œÄ, Œ∏) ‚àí ‚àÜ2C,i (œÄ, Œ∏));
| Let Œæi (œÄ, wŒ∏ ) := C‚ààC (|A|‚àí|C|)!(|C|‚àí1)!
|A|!

9. b

Define pŒæi (œÄ, wŒ∏ ) := Œæi (œÄ, wŒ∏ ) ‚àí wvi (œÄ);

Figure 4: Payment rule pŒæ .
S
such
of goods c‚ààA œÄ 1 (c) allocated to these agents according to œÄ 1 is equal to
S that the set
0
00
00
00
c‚ààA œÄœÑk (c)\{g }‚à™G , where G ‚äÜ img(œÄ)\img(œÄœÑk ) and |G | ‚â§ 1. Note that a set A having
this property in fact exists: just start with {Œ±2 } and then add agents from agents(œÑk ) until
some c is found with œÄ 1 (c) ‚äÜ img(œÄ)
U 1 \1img(œÄœÑk ).
Consider then œÄÃÑ = œÄœÑk [A] œÄ [A \ A] and note that œÄÃÑ is well-defined, because the
construction of the set A guarantees that no good allocated according to œÄœÑk [A] can be
allocated by œÄ 1 to agents in A1 \ A, and vice-versa. Moreover, œÄÃÑ is a feasible allocation for
hA, G, Œ∂ Œ∏ i1 . Indeed, for each agent c ‚àà A1 , |œÄÃÑ(c)| = 1 holds, because this constraint actually
holds on œÄœÑk and œÄ 1 . Eventually, since œÄ 1 is an optimal allocation for hA, G, Œ∂ Œ∏ i1 w.r.t. wŒ∏1 ,
val(œÄ 1 , wŒ∏1 ) ‚â• val(œÄÃÑ, wŒ∏1 ) holds. Thus, by construction of œÄÃÑ, we get val(œÄ 1 [A], wŒ∏1 ) ‚â•
val(œÄÃÑ[A], wŒ∏1 ) = val(œÄœÑk [A], wŒ∏1 ). U
Finally, let œÄÃÑC0 = œÄœÑk [C 1 \ A] œÄ 1 [A] and note that œÄÃÑC0 is a feasible allocation for
hC, G, Œ∂ Œ∏ i1 , because of the arguments used above to show that œÄÃÑ is a feasible allocation.
Moreover, observe that val(œÄÃÑC0 , wŒ∏1 ) ‚â• val(œÄœÑk , wŒ∏1 ) > val(Œ∑C1 , wŒ∏1 ) and img(œÄÃÑC0 ) ‚äÜ img(œÄ).
For this latter, just recall that Œ±2 ‚àà A is the only agent in C 1 such that œÄœÑk (Œ±2 )\img(œÄ) 6= ‚àÖ.
Again, this entails that Œ∑C1 is not optimal w.r.t. wŒ∏1 and hence, by Fact 4.2, Œ∑C is also not
optimal for hC, img(œÄ), Œ∂ Œ∏ i w.r.t. wŒ∏ . Contradiction.
The result immediately entails the following two corollaries.
Corollary 4.5. For each optimal allocation œÄ for hA, G, Œ∂ Œ∏ i w.r.t. wŒ∏ and for each set
C ‚äÜ A of agents, opt(hC, img(œÄ), Œ∂ Œ∏ i, wŒ∏ ) = opt(hC, G, Œ∂ Œ∏ i, wŒ∏ ).
Corollary 4.6. Let œÄ be an optimal allocation for hA, G, Œ∂ Œ∏ i w.r.t. wŒ∏ , and let œÄ 0 be any
feasible allocation for hA, G, Œ∂ Œ∏ i, hence with val(œÄ, wŒ∏ ) ‚â• val(œÄ 0 , wŒ∏ ). Then, for each set
C ‚äÜ A of agents, opt(hC, img(œÄ), Œ∂ Œ∏ i, wŒ∏ ) ‚â• opt(hC, img(œÄ 0 ), Œ∂ Œ∏ i, wŒ∏ ).

422

Mechanisms for Fair Allocation Problems

4.2 The Design of a Truthful Mechanism
Consider the payment rule pŒæ defined in Figure 4: We are given a type vector Œ∏ ‚àà Œò,
and a feasible allocation œÄ for SŒ∏ that selects some goods img(œÄ) ‚äÜ G for the agents in
A. Moreover, we use the verifier v (for the vector of true types t) that, given œÄ, is able
to compute the actual scenario Sv(œÄ) and the valuation vector wv(œÄ) = (wv1 , ..., wvn ) over
the allocated goods in img(œÄ). Note that, for the sake of presentation, in this section it is
convenient to look at the output of the verifier as a list of equivalent types v(œÄ) = (v1 , ..., vn )
where vi , for each agent i, is such that i‚Äôs upper bound constraint and i‚Äôs goods valuation
over img(œÄ) are those computed by the verifier v. Then, as usual, (vi , Œ∏‚àíi ) denotes the
type vector obtained from Œ∏ by replacing i‚Äôs type Œ∏i with the verified type vi . In particular,
w(vi ,Œ∏‚àíi ) denotes the valuation vector (defined over img(œÄ)) where the function wvi is used
in place of the valuation function declared in Œ∏i .
In the first three steps, the payment rule associates an optimal allocation œÄC,i for
hC, img(œÄ), Œ∂ (vi ,Œ∏‚àíi ) i w.r.t. w(vi ,Œ∏‚àíi ) with each set C ‚àà C of agents and each agent i ‚àà A,
where C is the powerset of A, i.e., the set of all possible subsets of agents. Then, for each
agent i ‚àà A, the rule computes the value Œæi (œÄ, wŒ∏ ) in step 8, by means of a formula that
depends on the valuations associated with the allocations œÄC,i and œÄC\{i},i , for each C ‚àà C.
In particular, it defines two terms (‚àÜ1C,i (œÄ, Œ∏) and ‚àÜ2C,i (œÄ, Œ∏)), which evaluate the allocations
œÄC,i and œÄC\{i},i , respectively, w.r.t. the valuation vector w(vi ,Œ∏‚àíi ) . Actually, for the term
‚àÜ2C,i (œÄ, Œ∏), i‚Äôs valuation is immaterial as œÄC\{i},i is an allocation over C \ {i}. Finally, the
payment pŒæi (œÄ, w) is defined in step 9 as the difference between Œæi (œÄ, wŒ∏ ) and wvi (œÄ).
Note that the payment rule depends only on the values of the goods in img(œÄ), so that
it is verifiable, according to the definition provided in Section 2.3. Moreover note that, as
far as paying agent i is concerned, the rule depends only on the values given by wvi over
allocated goods, i.e., by those values returned by the verifier, rather than by wi . Thus, i‚Äôs
declaration is immaterial as far as the computation of the payment is concerned, and the
next fact easily follows.
Fact 4.7 (no punishment). For each type vector Œ∏ ‚àà Œò, for each feasible allocation œÄ for
SŒ∏ , and for each agent i ‚àà A, it is the case that pŒæi (œÄ, wŒ∏ ) = pŒæi (œÄ, w(ti ,Œ∏‚àíi ) ).
Moreover, note that the idea underlying the definition of pŒæ is that, after verification
is performed, the utility function will precisely coincide with the ‚Äúbonus‚Äù Œæi (œÄ, wŒ∏ ), hence
sharing the spirit5 of the approach by Nisan and Ronen (2001).
Lemma 4.8. For each type vector Œ∏ ‚àà Œò and for each feasible allocation œÄ for SŒ∏ , it is the
case that ui (œÄ, wŒ∏ ) = Œæi (œÄ, wŒ∏ ).
By exploiting this characterization, we can now show the first crucial result on the payment rule pŒæ , i.e., that the mechanism (A, pŒæ ) is truthful, provided that A is any arbitrary
optimal allocation algorithm.
To get a high-level intuition of the proof below observe that Œæi (œÄ, wŒ∏ ) depends on two
groups of terms, with ‚àÜ2C,i (œÄ, Œ∏) being basically independent on the given agent i. Thus,
5. We say the ‚Äúspirit‚Äù, because the peculiar form of Œæi (œÄ, wŒ∏ ) does not formally fit the framework considered
by Nisan and Ronen (2001).

423

Greco & Scarcello

the goal of agent i is to maximize the terms of the form ‚àÜ1C,i (œÄ, Œ∏) defined in step 6 as
the valuations of optimal allocations computed (in step 3) by considering i‚Äôs verified type,
by focusing on goods in img(œÄ) (so that verified values coincide with true ones), and by
considering subsets of the whole set A of agents. The salient machinery is then provided
by Corollary 4.5, according to which it will be always convenient for agent i to report its
true type. Indeed, if œÄ is an optimal allocation computed via A based on the true type
of agent i, then Corollary 4.5 guarantees that ‚àÜ1C,i (œÄ, Œ∏) will get the maximum possible
value over all possible allocations for scenarios obtained by considering subsets of agents,
i.e., independently on the allocation œÄ being actually selected. This can be done without
strategically interacting with the other agents. Therefore, by designing the payment rule in
a way that depends only on the values returned by the verifier, not only we end up with a
verifiable rule using no punishment, but we also obtain a truthful mechanism based on it.
Theorem 4.9 (truthfulness). Let A be any optimal allocation algorithm. Then, the mechanism with verification (A, pŒæ ) is truthful.
Proof. We have to show that, for each agent i ‚àà A and each reported type vector d, the
following holds: ui (A(w(ti ,d‚àíi ) ), w(ti ,d‚àíi ) ) ‚â• ui (A(wd ), wd ); hence, by Lemma 4.8, that
Œæi (A(w(ti ,d‚àíi ) ), w(ti ,d‚àíi ) ) ‚â• Œæi (A(wd ), wd ).
Consider the construction reported in Figure 4 for the two cases of Œ∏ = d and Œ∏ =
(ti , d‚àíi ), and let œÄ = A(wd ) and œÄ 0 = A(w(ti ,d‚àíi ) ) be the corresponding optimal allocations
(for the scenarios Sd and S(ti ,d‚àíi ) , respectively) received as input by the payment rule in
0 ) be
Figure 4. For any set C ‚àà C of agents, and for any agent i ‚àà A, let œÄC,i (resp., œÄC,i
the allocation computed at step 3. Note that this step is well defined, because such an
optimal allocation always exists. Indeed, just note that there exist feasible allocations for
any scenario SŒ∏ , e.g., any allocation œÄÃÑ : C ‚Üí 2img(œÄ) such that œÄÃÑ(j) = {g} for some good
g ‚àà œÄ(j). That is, any allocation that trivially satisfies every upper-bound constraint, since
it assigns one good to each agent.
We now show that the following two properties hold, for each C ‚àà C and i ‚àà A:
(A) ‚àÜ1C,i (œÄ 0 , (ti , d‚àíi )) ‚â• ‚àÜ1C,i (œÄ, d), and
(B) ‚àÜ2C,i (œÄ 0 , (ti , d‚àíi )) = ‚àÜ2C,i (œÄ, d).
In order to prove (A), observe that by step 6, ‚àÜ1C,i (œÄ 0 , (ti , d‚àíi )) = val(œÄC,i , w(vi ,d‚àíi ) ) =
0
val(œÄC,i , w(ti ,d‚àíi ) ), where the last equality holds because œÄC,i
‚äÜ img(œÄ 0 ), so that the true
0
is an optimal allocation for
valuation is disclosed by the verifier. Then, observe that œÄC,i
0
0
hC, img(œÄ ), Œ∂ (vi ,d‚àíi ) i = hC, img(œÄ ), Œ∂ (ti ,d‚àíi ) i w.r.t. w(vi ,d‚àíi ) and, hence, w.r.t. w(ti ,d‚àíi ) , and
œÄ 0 = A(w(ti ,d‚àíi ) ) is an optimal allocation for S(ti ,d‚àíi ) = hA, G, Œ∂ (ti ,d‚àíi ) i w.r.t. w(ti ,d‚àíi ) .
Thus, by Corollary 4.5, we get the following expression:
0
‚àÜ1C,i (œÄ 0 , (ti , d‚àíi )) = val(œÄC,i
, w(ti ,d‚àíi ) ) = opt(hC, G, Œ∂ (ti ,d‚àíi ) i, w(ti ,d‚àíi ) ).

(2)

Similarly, ‚àÜ1C,i (œÄ, d) = val(œÄC,i , w(vi ,d‚àíi ) ) = val(œÄC,i , w(ti ,d‚àíi ) ) holds. Thus, we
can use Equation 2, in order to get ‚àÜ1C,i (œÄ 0 , (ti , d‚àíi )) = opt(hC, G, Œ∂ (ti ,d‚àíi ) i, w(ti ,d‚àíi ) ) ‚â•
val(œÄC,i , w(ti ,d‚àíi ) ). This shows that (A) holds.

424

Mechanisms for Fair Allocation Problems

0
Let us now focus on (B). By step 7, we have ‚àÜ2C,i (œÄ 0 , (ti , d‚àíi )) = val(œÄC\{i},i
, w(vi ,d‚àíi ) )
2
0
whereas ‚àÜC,i (œÄ, d) = val(œÄC\{i},i , w(vi ,d‚àíi ) ). Recall that œÄC\{i},i (resp., œÄC\{i},i ) is an optimal allocation for hC \{i}, img(œÄ), Œ∂ (vi ,d‚àíi ) i (resp., hC \{i}, img(œÄ 0 ), Œ∂ (vi ,d‚àíi ) i) w.r.t. w(vi ,d‚àíi ) .
Then, because of the fact that i‚Äôs evaluation is immaterial here, we have that œÄC\{i},i (resp.,
0
œÄC\{i},i
) is an optimal allocation for hC \ {i}, img(œÄ), Œ∂ d i (resp., hC \ {i}, img(œÄ 0 ), Œ∂ (ti ,d‚àíi ) i)
w.r.t. wd (resp., w(ti ,d‚àíi ) ). Then, we recall that œÄ (resp., œÄ 0 ) is an optimal allocation for
Sd (resp., S(ti ,d‚àíi ) ) w.r.t. wd (resp., w(ti ,d‚àíi ) ). Thus, by Corollary 4.5, ‚àÜ2C,i (œÄ 0 , (ti , d‚àíi )) =
opt(hC \ {i}, G, Œ∂ (ti ,d‚àíi ) i, w(ti ,d‚àíi ) ). Moreover, we get:

‚àÜ2C,i (œÄ, d) = opt(hC \ {i}, G, Œ∂ d i, wd ).

(3)

Eventually, ‚àÜ2C,i (œÄ 0 , (ti , d‚àíi )) = opt(hC \ {i}, G, Œ∂ (ti ,d‚àíi ) i, w(ti ,d‚àíi ) ) = opt(hC \
{i}, G, Œ∂ d i, wd ) holds, as i‚Äôs valuation is immaterial, and we get (B) by Equation 3.
4.3 Further Properties of Truthful Strategies
Let us now analyze some relevant properties that hold whenever agents choose the dominant strategy of truthfully reporting their private types. The first property is a useful
characterization of agents‚Äô utilities.
Theorem 4.10. For each optimal allocation œÄ for St = hA, G, Œ∂ t i w.r.t. wt , and for each
agent i ‚àà A, it holds that:
ui (œÄ, wt ) =


X (|A| ‚àí |C|)!(|C| ‚àí 1)! 
opt(hC, G, Œ∂ t i, wt ) ‚àí opt(hC \ {i}, G, Œ∂ t i, wt ) .
|A|!

C‚ààC

Proof. By Lemma 4.8, we know that ui (œÄ, wt ) = Œæi (œÄ, wt ). Then, for each set C ‚àà C of
agents, and for each agent i ‚àà A, consider the expressions ‚àÜ1C,i (œÄ, t) and ‚àÜ2C,i (œÄ, t) defined
in step 6 and step 7, respectively, of the mechanism in Figure 4. Note that, because of the
properties of the verifier v stated in Definition 2.3 and the fact that the payment rule consider only goods that are allocated via œÄ, ‚àÜ1C,i (œÄ, t) = val(œÄC,i , w(vi ,t‚àíi ) ) = val(œÄC,i , wt ) and
‚àÜ2C,i (œÄ, t) = val(œÄC,i , w(vi ,t‚àíi ) ) = val(œÄC\{i},i , wt ) hold, where œÄC,i and œÄC\{i},i are optimal
allocations for hC, img(œÄ), Œ∂ t i w.r.t. wt and for hC \ {i}, img(œÄ), Œ∂ t i w.r.t. wt , respectively.
Thus, ‚àÜ1C,i (œÄ, t) = opt(hC, img(œÄ), Œ∂ t i, wt ) and ‚àÜ2C,i (œÄ, t) = opt(hC \ {i}, img(œÄ), Œ∂ t i, wt ).
It follows that:
ui (œÄ, wt ) =

X (|A| ‚àí |C|)!(|C| ‚àí 1)!
(opt(hC, img(œÄ), Œ∂ t i, wt ) ‚àí opt(hC \ {i}, img(œÄ), Œ∂ t i, wt )) .
|A|!

C‚ààC

(4)

Recall now by Corollary 4.5 that, for each optimal allocation œÄ for hA, G, Œ∂ t i w.r.t. wt
and for each set C ‚àà C of agents, opt(hC, img(œÄ), Œ∂ t i, wt ) = opt(hC, G, Œ∂ t i, wt ). Therefore,
‚àÜ1C,i (œÄ, t) = opt(hC, G, Œ∂ t i, wt ) and ‚àÜ2C,i (œÄ, t) = opt(hC \ {i}, G, Œ∂ t i, wt ). By using these
equalities, the result follows from Equation 4.
As agents‚Äô utilities are completely independent of the particular optimal allocation œÄ,
every agent gets precisely the same utility in every optimal allocation.
425

Greco & Scarcello

Corollary 4.11. Let œÄ and œÄ 0 be two optimal allocations for St w.r.t. wt . Then, ui (œÄ, wt ) =
ui (œÄ 0 , wt ) holds, for each i ‚àà A.
Example 4.12. Consider the scenario S = hA, G, Œ∂ t i, with A = {a1 , a2 } and G =
{g1 , ..., g8 }, the valuation vector wt = (wa1 , wa2 ) discussed in Example 2.1, and the allocation œÄ ‚àó illustrated in Figure 1(I)). Then, we have:
ua1 (œÄ ‚àó , wt ) =

1
2 (opt(h{a1 , a2 }, G, Œ∂ t i, wt ) ‚àí opt(h{a2 }, G, Œ∂ t i, wt ))+
1
2 (opt(h{a1 }, G, Œ∂ t i, wt ) ‚àí opt(h{}, G, Œ∂ t i, wt ))+
1
2 (opt(h{a2 }, G, Œ∂ t i, wt ) ‚àí opt(h{a2 }, G, Œ∂ t i, wt ) =
1
1
1
51
2 (51 ‚àí 26) + 2 (26 ‚àí 0) + 2 (26 ‚àí 26) = 2 .

For instance, note that opt(h{a1 }, G, Œ∂ t i, wt )) = 26, as we can allocate g1 , g4 , and g5 to
a1 , if (s)he is the only agent in the scenario.
Similarly, we get ua2 (œÄ ‚àó , wt ) = 51
2 . That is, the two agents will share precisely half of
the total value, based on our payment scheme. In fact, by looking at the allocation œÄ ‚àó in
Figure 1(II), one might naƒ±Ãàvely suppose that a2 contributed more than a1 to the overall
value associated with œÄ ‚àó . However, this is only due to the specific allocation considered, and
not to the actual values of the goods of the two agents. Indeed, the fairness of the utility
values resulting from our payment rule suddenly appears when considering the existence
of the alternative allocation œÄÃÇ ‚àó in Figure 1(II), which is symmetric w.r.t. œÄ ‚àó and where it
seems that a1 contributed more than a2 to the overall value: As a matter of fact, the two
agents are completely interchangeable over optimal allocations, and this is correctly reflected
by our payment scheme (without the need of actually looking at œÄÃÇ ‚àó ). In particular, from
Corollary 4.11, the agents are indifferent w.r.t. the specific optimal allocation being selected,
and hence in this case they equally divide all the available value between themselves.

Further basic properties of the mechanism are pointed out next.
Theorem 4.13 (basic properties). Let A be any optimal allocation algorithm. Then, the
mechanism (A, pŒæ ) is efficient and guarantees an equal treatment of the equals. Moreover,
if all valuations are non-negative, then (A, pŒæ ) is individually rational.
Proof. It is clear that (A, pŒæ ) satisfies efficiency and equal treatment of equals.
Let now i ‚àà A be an agent and recall from Lemma 4.8 that, for each type vector
Œ∏ ‚àà Œò and allocation œÄ for SŒ∏ , ui (œÄ, wŒ∏ ) = Œæi (œÄ, wŒ∏ ) holds. Consider the payment rule
pŒæ defined in Figure 4, and observe that Œæi (œÄ, wŒ∏ ) is defined as a weighted summation,
over all coalitions C ‚àà C, of terms having the form ‚àÜ1C,i (œÄ, Œ∏) ‚àí ‚àÜ2C,i (œÄ, Œ∏). In particular,
all weights are positive and we claim that ‚àÜ1C,i (œÄ, Œ∏) ‚àí ‚àÜ2C,i (œÄ, Œ∏) ‚â• 0 holds. Indeed, just
check that, by definition, ‚àÜ1C,i (œÄ, Œ∏) = opt(hC, img(œÄ), Œ∂ Œ∏ i, wŒ∏ ) and ‚àÜ2C,i (œÄ, Œ∏) = opt(hC \
{i}, img(œÄ), Œ∂ Œ∏ i, wŒ∏ ). So, under the hypothesis that all valuations are non-negative, we have
that opt(hC, img(œÄ), Œ∂ Œ∏ i, wŒ∏ ) ‚àí opt(hC \ {i}, img(œÄ), Œ∂ Œ∏ i, wŒ∏ ) ‚â• 0 holds, for each C ‚äÜ A and
agent i ‚àà A. Thus, ui (œÄ, wŒ∏ ) ‚â• 0, hence the fact that (A, pŒæ ) is individually rational trivially
follows (even independently of the allocation œÄ, and of whether agent i is truthtelling).
Moreover, the mechanism is also tolerant of sensing errors (or uncertain inputs), in the
sense of Section 3.2.
426

Mechanisms for Fair Allocation Problems

Theorem 4.14 (error tolerance). Let A be any optimal allocation algorithm. Then, the
mechanism (A, pŒæ ) is such that for each type vector tÃÑ and each agent i ‚àà A, |ui (A(wt ), wt )‚àí
ui (A(wtÃÑ ), wtÃÑ )| ‚â§ 3 √ó dist(t, tÃÑ).
Proof. Consider the construction reported in Figure 4 for the two cases of Œ∏ = t and Œ∏ = tÃÑ,
and let œÄ = A(wt ) and œÄÃÑ = A(wtÃÑ ) be the corresponding allocations received as its input by
the payment rule (optimal w.r.t. wt and wtÃÑ , respectively). Moreover, for any set C ‚àà C of
agents and for any agent i ‚àà A, let œÄC,i and œÄÃÑC,i be the corresponding allocations computed
at step 3. Then, we get:
‚Ä¢ ‚àÜ1C,i (œÄ, t) = opt(hC, G, Œ∂ t i, wt ), by Equation 2 in the proof of Theorem 4.9;
‚Ä¢ ‚àÜ1C,i (œÄÃÑ, tÃÑ) = opt(hC, img(œÄÃÑ), Œ∂ (ti ,tÃÑ‚àíi )i, w(ti ,tÃÑ‚àíi ) ), by step 3 in Figure 4 and the fact the
true valuation of agent i is disclosed by the verifier because œÄÃÑC,i ‚äÜ img(œÄÃÑ).
‚Ä¢ ‚àÜ2C,i (œÄ, t) = opt(hC \ {i}, G, Œ∂ t i, wt ), by Equation 3 in the proof of Theorem 4.9;
‚Ä¢ ‚àÜ2C,i (œÄÃÑ, tÃÑ) = opt(hC \ {i}, G, Œ∂ tÃÑ i, wtÃÑ ), again by Equation 3;
Note now that |‚àÜ1C,i (œÄÃÑ, tÃÑ) ‚àí opt(hC, img(œÄÃÑ), Œ∂ tÃÑ i, wtÃÑ )| ‚â§ dist(t, tÃÑ). Moreover, since œÄÃÑ is an
optimal allocation w.r.t. wtÃÑ , by Corollary 4.5, opt(hC, img(œÄÃÑ), Œ∂ tÃÑ i, wtÃÑ ) = opt(hC, G, Œ∂ tÃÑ i, wtÃÑ )
and, hence, |‚àÜ1C,i (œÄÃÑ, tÃÑ) ‚àí opt(hC, G, Œ∂ tÃÑ i, wtÃÑ )| ‚â§ dist(t, tÃÑ) holds. Then, observe that
|opt(hC, G, Œ∂ tÃÑ i, wtÃÑ ) ‚àí ‚àÜ1C,i (œÄ, t)| ‚â§ dist(t, tÃÑ). Therefore, we conclude that |‚àÜ1C,i (œÄÃÑ, tÃÑ) ‚àí
‚àÜ1C,i (œÄ, t)| ‚â§ 2 √ó dist(t, tÃÑ).
Similarly, |‚àÜ2C,i (œÄÃÑ, tÃÑ) ‚àí ‚àÜ2C,i (œÄ, t)| ‚â§ dist(t, tÃÑ) holds. Hence, by putting it all together,
we derive |(‚àÜ1C,i (œÄÃÑ, tÃÑ) ‚àí ‚àÜ2C,i (œÄÃÑ, tÃÑ)) ‚àí (‚àÜ1C,i (œÄ, t) ‚àí ‚àÜ1C,i (œÄ, t))| ‚â§ 3 √ó dist(t, tÃÑ).
In the light of the above expression and by step 7 in Figure 4, we have that
|Œæi (œÄÃÑ, wtÃÑ ) ‚àí Œæi (œÄ, wt )| ‚â§

X (|A| ‚àí |C|)!(|C| ‚àí 1)!
3 √ó dist(t, tÃÑ).
|A|!

C‚ààC

P
To conclude the proof, observe that C‚ààC (|A|‚àí|C|)!(|C|‚àí1)!
= 1 holds, so that |Œæi (œÄÃÑ, wtÃÑ ) ‚àí
|A|!
Œæi (œÄ, wt )| ‚â§ 3 √ó dist(t, tÃÑ).
Therefore, by Lemma 4.8, we get |ui (A(wtÃÑ ), wtÃÑ ) ‚àí
ui (A(wt ), wt )| = |Œæi (œÄÃÑ, wtÃÑ ) ‚àí Œæi (œÄ, wt )| ‚â§ 3 √ó dist(t, tÃÑ).
In fact, we next show that (A, pŒæ ), with A being an optimal allocation algorithm, satisfies
all the remaining properties discussed in Section 3.1. To this end, we first discuss an
interpretation of pŒæ in the context of coalitional games.

5. A Coalitional Game Theory Viewpoint
A coalitional game can be modeled as a pair G = hN, œïi, where N = {1, ..., n} is a finite
set of agents, and œï is a function associating with each coalition R ‚äÜ N a real-value
œï(R) ‚àà R, with œï({}) = 0, which is meant to encode the worth that agents in C obtain
by collaborating with each other. The function œï is supermodular (resp., submodular ) if
œï(R ‚à™ T ) + œï(R ‚à© T ) ‚â• œï(R) + œï(T ) (resp., œï(R ‚à™ T ) + œï(R ‚à© T ) ‚â§ œï(R) + œï(T )) holds,
for each pair of coalitions R, T ‚äÜ N .
427

Greco & Scarcello

A fundamental problem for coalitional games is to single out the most desirable outcomes, usually called solution concepts, in terms of appropriate notions
of worth distribuP
n
tions, i.e., of vectors of payoffs x = (x1 , ..., xn ) ‚àà R such that i‚ààN xi = œï(N ). This
question was studied in economics and game theory with the aim of providing arguments
and counterarguments about why such proposals are reasonable mathematical renderings
of the intuitive concepts of fairness and stability. For further background on coalitional
games, the reader is referred to, e.g., the work of Osborne and Rubinstein (1994).
Here, we consider the Shapley value of G = hN, œïi, which is a well-known solution
concept such that:
X (|N | ‚àí |R|)!(|R| ‚àí 1)!
œÜi (G) =
(œï(R) ‚àí œï(R \ {i})), for each i ‚àà N.
|N |!
R‚äÜN

Indeed, we shall show that the mechanism defined in Section 4 has a nice interpretation in
terms of the Shapley value of some suitable-defined coalitional games. The correspondence
will be exploited to prove further properties of our mechanism, at the equilibrium t where
agents truthfully report their types.
5.1 The Shapley Value of Allocation Games
We consider two coalitional games defined on top of an allocation problem.
marg
best
Definition 5.1. Given the valuation vector w, we define Gw
= hA, margw i and Gw
=
hA, bestw i as the coalitional games such, that for each set C ‚äÜ A of agents,

‚Ä¢ margw (C) = opt(hA, G, Œ∂ t i, wt ) ‚àí opt(hA \ C, G, Œ∂ t i, wt ); and,
‚Ä¢ bestw (C) = opt(hC, G, Œ∂ t i, wt ).

2

Recall that in Section 3.1 we have defined the concept of marginal contribution
margœÄ,w (C) of a coalition C with respect to a given allocation œÄ (because it is sensible
to the set of goods allocated by œÄ). In the above definition, with a slight abuse of notation,
we have defined a similar concept margw (C), which does not depend on any goods allocation.
It turns out that these two concepts actually coincide over optimal allocations.
Theorem 5.2. Let œÄ be any optimal allocation for St w.r.t. wt , and let C ‚äÜ A be an
arbitrary set of agents. Then, margœÄ,w (C) = margw (C).
Proof. Let œÄ be an optimal allocation for St = hA, G, Œ∂ t i w.r.t. wt , hence optimal
for hA, img(œÄ), Œ∂ t i w.r.t. wt . That is, opt(St , wt ) = opt(hA, img(œÄ), Œ∂ t i, wt ). Moreover, for each set C ‚äÜ A of agents, by Corollary 4.5, opt(hA \ C, img(œÄ), Œ∂ t i, wt ) =
opt(hA\C, G, Œ∂ t i, wt ) holds. Therefore, margw (C) = opt(St , wt )‚àíopt(hA\C, G, Œ∂ t i, wt ) =
opt(hA, img(œÄ), Œ∂ t i, wt ) ‚àí opt(hA \ C, img(œÄ), Œ∂ t i, wt ), and the result follows as this value
is equivalent to the definition of margœÄ,w (C) in Equation 1 (on page 416).
Note also that bestw (C) is the best contribution of C, computed assuming that agents in
best
has already
C were the only agents in the allocation problem. In particular, the game Gw
been considered by Moulin (1992), precisely in the setting of fair division for allocation
best
is submodular.
problems. There, it is shown that the cost function associated with Gw
428

Mechanisms for Fair Allocation Problems

Proposition 5.3. The function bestw is submodular.
marg
Since opt(hC, G, Œ∂ t i, wt ) = bestw (C), it turns out that Gw
is what is called in the
best
literature the dual game of Gw , and the following result is known to hold. Nevertheless,
we give a direct proof, for completeness.

Corollary 5.4. The function margw is supermodular.
Proof. Let St = hA, G, Œ∂ t i be the given scenario. The result just follows by noticing that
margw (C) = opt(hA, G, Œ∂ t i, wt ) ‚àí opt(hA \ C, G, Œ∂ t i, wt ) = opt(hA, G, Œ∂ t i, wt ) ‚àí bestw (A \
C), for each set of agents C ‚äÜ A. Therefore, bestw (C) = opt(hA, G, Œ∂ t i, wt )‚àímargw (A\C).
Thus, if bestw (R ‚à™ T ) + bestw (R ‚à© T ) ‚â§ bestw (R) + bestw (T ) holds ‚àÄR, T ‚äÜ A, then
we have that margw (A \ (R ‚à™ T )) + margw (A \ (R ‚à© T )) ‚â• margw (A \ R) + margw (A \ T )
holds as well, ‚àÄR, T ‚äÜ A. Eventually, by letting R0 = A \ R and T 0 = A \ T , we get
margw (R0 ‚à© T 0 ) + margw (R0 ‚à™ T 0 )) ‚â• margw (R0 ) + margw (T 0 ), for each ‚àÄR0 , T 0 ‚äÜ A. That
is, margw is supermodular.
The second relevant property is that the payment rule in Section 4 coincides with the
best
Shapley value of the game Gw
associated with w. The result follows by comparing the utility function as in Theorem 4.10 with the expression for the Shapley value of the coalitional
best
game Gw
. Moreover, we show that the same result can be established for the ‚Äúdual‚Äù game
marg
Gt , so that the Shapley values of the two games are identical‚Äîfor similar correspondences
between Shapley values of different games, see also the works by Maniquet (2003) and Kalai
and Samet (1983).
Theorem 5.5. For each optimal allocation œÄ for St w.r.t. wt , and for each agent i ‚àà A,
marg
best
).
) = œÜi (Gw
it holds that ui (œÄ, wt ) = Œæi (œÄ, wt ) = œÜi (Gw
Proof. By comparing the utility function as in Theorem 4.10 with the expression for the
best
associating with each coalition C of agents the
Shapley value of the coalitional game Gw
worth opt(hC, G, Œ∂ t i, wt ), we immediately get that, for each optimal allocation œÄ for St
best
).
w.r.t. wt , and for each agent i ‚àà A, it holds that ui (œÄ, wt ) = Œæi (œÄ, wt ) = œÜi (Gw
marg
best
In order to conclude the proof, we show that for each agent i ‚àà A, œÜi (Gw ) = œÜi (Gw
)
holds. To this end, first note that these Shapley values can be written as follows:
P
marg
‚àí œÜi (Gw
) = C‚äÜA,i‚ààC (|A|‚àí|C|)!(|C|‚àí1)!
TC0 , and
|A|!
best
‚àí œÜi (Gw
)=

P

C‚äÜA,i‚ààC

(|A|‚àí|C|)!(|C|‚àí1)!
TC ,
|A|!

where TC0 = margw (C)‚àímargw (C\{i}) and TC = opt(hC, G, Œ∂ t i, wt )‚àíopt(hC\{i}, G, Œ∂ t i, wt ).
Then, we claim that:
(1) for each set C ‚äÜ A of agents with i ‚àà C, the set C¬Ø = (A \ C) ‚à™ {i} is such that TC0 = TC¬Ø,
and
¬Ø the set C = (A \ C)
¬Ø ‚à™ {i} is such that T 0 = T ¬Ø.
(2) for each set C¬Ø ‚äÜ A of agents with i ‚àà C,
C
C

429

Greco & Scarcello

(1) Let C ‚äÜ A such that i ‚àà C, and observe that TC0 = margw (C) ‚àí margw (C \ {i}) =
(opt(St , wt ) ‚àí opt(hA \ C, G, Œ∂ t i, wt )) ‚àí (opt(St , wt ) ‚àí opt(hA \ (C \ {i}), G, Œ∂ t i, wt )) =
opt(hA \ (C \ {i}), G, Œ∂ t i, wt ) ‚àí opt(hA \ C, G, Œ∂ t i, wt ) = opt(h(A \ C) ‚à™ {i}), G, Œ∂ t i, wt ) ‚àí
opt(hA \ C, G, Œ∂ t i, wt ). Thus, let C¬Ø = (A \ C) ‚à™ {i}, and note that TC0 = TC¬Ø.
¬Ø G, Œ∂ t i, wt ) ‚àí
¬Ø and observe that T ¬Ø = opt(hC,
(2) Let C¬Ø ‚äÜ A such that i ‚àà C,
C
¬Ø
¬Ø
opt(hC \ {i}, G, Œ∂ t i, wt ) = (opt(St , wt ) ‚àí opt(hC \ {i}, G, Œ∂ t i, wt )) ‚àí (opt(St , wt ) ‚àí
¬Ø G, Œ∂ t i, wt )) = marg ((A \ C)
¬Ø ‚à™ {i}) ‚àí marg (A \ C).
¬Ø Thus, let C = (A \ C)
¬Ø ‚à™ {i} and
opt(hC,
w
w
0
note that TC = TC¬Ø.
marg
best
As (1) and (2) hold, and given the two expressions for œÜi (Gw
) and œÜi (Gw
), we conclude
that the two values coincide.

5.2 Marginality, Budget-Balancedness, and Individual Optimality
Now that we have established a precise correspondence between our mechanism and the
Shapley value of its associated allocation games, we can show further desirable properties of pŒæ . In fact, we exploit the following well-known properties (see, e.g., Osborne &
Rubinstein, 1994; Young, 1985) of the Shapley value of any game G = hN, œïi:
(I)

P

i‚ààN

œÜi (G) = œï(N );

(II) P
If œï is supermodular (resp., submodular ), then
i‚ààR œÜi (G) ‚â§ œï(R)), for each coalition R ‚äÜ N .

P

i‚ààR œÜi (G)

‚â• œï(R) (resp.,

(III) If G 0 = hN, œï0 i is a game such that œï0 (R) ‚â• œï(R), for each R ‚äÜ N , then œÜi (G 0 ) ‚â• œÜi (G),
for each agent i ‚àà N .
In particular, Property
(II) entails that the Shapley value of the supermodular game
P
marg
) ‚â• margw (R). This means that the Shapley value of this
Gw is such that i‚ààR œÜi (Gw
game belongs to another important solution concept for coalitional games called core (Osborne & Rubinstein, 1994), which is highly desirable for its ‚Äústability‚Äù, as every coalition
marg
gets at least the worth it deserves according to the function margw .
of agents in Gw
In our context, this will easily entail that the utility of every group (coalition) of agents
C (i.e., the sum of the utilities of agents in C), is not less than its actual marginal contribution to the best possible allocations. Note that considering such a collective utility is
particularly useful whenever we reason in terms of fairness for groups of agents, rather than
just singletons. We first pinpoint that bestw (C) and margw (C) provide an upper bound
and a lower bound, respectively, to the utility of C.
marg

Theorem 5.6. Let œÄ be
Pan optimal allocation for St w.r.t. wt . Then, for each set C ‚äÜ A
of agents, bestw (C) ‚â• i‚ààC ui (œÄ, wt ) ‚â• margw (C).

marg
best
), for each agent
) = œÜi (Gw
Proof. By Theorem 5.5, we know that ui (œÄ, wt ) = œÜi (Gw
i ‚àà A and optimal allocation œÄ. Then, we can simply recall that the function margw (resp.,
marg
best
bestw ) associated with the game Gw
(resp., Gw
) is supermodular (resp.,
P submodular)
by
Corollary
5.4
(resp.,
Proposition
5.3).
Hence,
the
result
follows
as
i‚ààC ui (œÄ, wt ) =
P
P
marg
best
œÜ
(G
)
and
by
property
(II).
œÜ
(G
)
=
i‚ààC i w
i‚ààC i w

430

Mechanisms for Fair Allocation Problems

By combining the above result and Theorem 5.2, we immediately get the following.
Corollary 5.7 (marginality).
P Let A be any optimal allocation algorithm. Then, the mechanism (A, pŒæ ) is such that i‚ààC ui (A(wt ), wt ) ‚â• margA(wt ),w (C), for each set C ‚äÜ A.
Hence, under the payment rule pŒæ and optimal allocation algorithms, we get mechanisms
that accurately take care of the marginality property defined in Section 3.1. Our next result
pertains the budget-balance property of the mechanisms. Again, the correspondence with
the Shapley value is crucial to establish the result.
Theorem
5.8. Let œÄ be an optimal allocation for St w.r.t. wt .
P
Œæ
p
(œÄ,
w
t ) = 0.
i‚ààA i

Then, it holds that

best
), for each agent i ‚àà A and optiProof. By Theorem 5.5, we know that ui (œÄ, wt ) = œÜi (Gw
best
best
mal allocation œÄ, where
œÜ
(G
)
is
the
Shapley
value
of
G
w . By property (I)
Pof the Shapley
P
P i w best
best
)=
value, we know that i‚ààA œÜi (Gw ) = bestw (A). Thus, i‚ààA ui (œÄ, wt ) = i‚ààA œÜi (Gw
opt(hA, G, Œ∂ t i, wt ). By definition of the utility and letting wt = (w1 , ..., wn ), it then folP
P
P
Œæ
Œæ
lows that opt(hA, G, Œ∂ t i, wt ) =
i‚ààA pi (œÄ, w). Hence,
i‚ààA wi (œÄ) ‚àí
i‚ààA pi (œÄ, wt ) =
opt(hA, G, Œ∂ t i, wt ) ‚àí val(œÄ, wt ) = 0, as œÄ is indeed an optimal allocation w.r.t. wt .

Corollary 5.9 (budget-balancedness). Let A be any optimal allocation algorithm. Then,
the mechanism (A, pŒæ ) is budget-balanced.
Finally, we complete the picture of our analysis by proving a strong fairness property
of the proposed payment rule pŒæ : In words, the best outcome for every agent is always
determined by a (global) optimal allocation. Moreover, from Corollary 4.11, any agent
is indifferent about the specific optimal allocation being considered. That is, any chosen
optimal allocation leads to the best results for all agents.
Lemma 5.10. Let œÄ and œÄ 0 be two feasible allocations for St such that œÄ is optimal w.r.t. wt ,
and hence val(œÄ, wt ) ‚â• val(œÄ 0 , wt ). Then, ui (œÄ, wt ) ‚â• ui (œÄ 0 , wt ), for each i ‚àà A. Moreover, if œÄ 0 is not optimal, there exists some agent i ‚àà A such that ui (œÄ, wt ) > ui (œÄ 0 , wt ).
Proof. For any allocation œÄÃÑ, consider the coalitional game G œÄÃÑ = hA, v œÄÃÑ i such that v œÄÃÑ (C) =
opt(hC, img(œÄÃÑ), Œ∂ t i, wt ), for each C ‚äÜ A. By looking at the expression of the Shapley value
for G œÄÃÑ , it is easy to check that ui (œÄÃÑ, wt ) = œÜi (G œÄÃÑ ) (just use the same reasoning in the proof
of Theorem 4.10). Assume now that œÄ 0 is an allocation with val(œÄ, wt ) ‚â• val(œÄ 0 , wt ), and
0
consider the value v œÄ (C) = opt(hC, img(œÄ 0 ), Œ∂ t i, wt ), for each C ‚äÜ A. By Corollary 4.6,
0
we have that v œÄ (C) ‚â• v œÄ (C), for each C ‚äÜ A. Then, we derive that ui (œÄ, wt ) = œÜi (G œÄ ) ‚â•
0
œÜi (G œÄ ) = ui (œÄ 0 , w) for every i ‚àà A, because of property (III).
Now assume that œÄ 0 is not optimal, and thus val(œÄ, wt ) > val(œÄ 0 , wt ). Therefore, for
0
the grand-coalition A, we have v œÄ (A) > v œÄ (A). Because of property (I) of the Shapley
0
value, only (and all) the total value v œÄ (A) is distributed to agents. It follows that there
0
exists some agent i ‚àà A such that ui (œÄ, wt ) = œÜi (G œÄ ) > œÜi (G œÄ ) = ui (œÄ 0 , wt ).
By focusing on optimal allocation algorithms, the above lemma immediately entails the
following two fairness properties.

431

Greco & Scarcello

Theorem 5.11 (individual optimality). Let A be any optimal allocation algorithm.
Then, for any agent i ‚àà A and any feasible allocation œÄ for St , ui (A(wt ), wt ) ‚â• ui (œÄ, wt ).
Corollary 5.12 (Pareto-efficiency and envy-freeness). Let A be any optimal allocation
algorithm. Then, the mechanism (A, pŒæ ) is Pareto efficient and envy-free.
Note that individual optimality guarantees much more than classical Pareto efficiency
and envy-freeness, because it entails that the mechanism leads to a unique evaluation,
independently of the chosen optimal allocation. In particular, the Pareto set is a singleton.

6. Complexity Issues
In this section, we shall reconsider our mechanism with verification from a computational
perspective. Note first that computing an optimal allocation on the basis of the reported
types is an easy task, which can be carried out via adaptations of classical matching algorithms. Indeed, in the light of Fact 4.2, computing an optimal allocation for an arbitrary
scenario (with agents in A and goods in G) reduces to computing an optimal allocation for
a scenario where each agent (in A1 ) gets one good. This is equivalent to finding a matching
of maximum weight over a complete bipartite graph over the set of disjoint nodes A1 and
G, and where edge weights are encoded via the function w1 . Such a combinatorial problem
is known to be feasible in polynomial time (e.g., Schrijver, 2003).
6.1 Hardness Result
Despite the fact that optimal allocations can be computed in polynomial time, our mechanism is not computationally-efficient, since payments are unlikely to be computable in
polynomial time. Indeed, we next show that this computation problem is complete for the
complexity class #P (see Papadimitriou, 1993).
Let us recall that a counting Turing machine is a standard nondeterministic Turing
machine with an auxiliary output device that prints in binary notation the number of
accepting computations induced by the input. It has (worst-case) time complexity f (n) if
the longest accepting computation induced by the set of all inputs of size n takes f (n) steps.
Then, #P is the class of all functions that can be computed by counting Turing machines of
polynomial time complexity. A prototypical #P-complete problem is to count the number
of truth variable assignments that satisfy a Boolean formula. Of course, NP‚äÜ#P, and a
polynomial-time algorithm for solving a #P-complete problem would imply P = NP.
Theorem 6.1. Computing the Shapley value of coalitional games associated with allocation
problems (as in Definition 5.1) is #P-complete.
Proof. The problem belongs to #P, because computing the Shapley value is known to be feasible in #P for any class of coalitional games with polynomial-time value/cost functions (c.f.
Deng & Papadimitriou, 1994). To show that it is #P-hard, we exhibit a reduction from the
following problem: Let G = (A ‚à™ B, E) be a bipartite graph with |A| = |B| = n, E ‚äÜ A √ó B,
and |E| = m ‚â• n. Recall that a matching is a set E 0 ‚äÜ E of edges such that for each pair
of distinct edges (a, b) and (a0 , b0 ) in E 0 , a 6= a0 and b 6= b0 hold. The matching E 0 is perfect
if |E 0 | = n. The problem of counting the number of perfect matchings in such bipartite
graphs is #P-complete (Valiant, 1979a).
432

Mechanisms for Fair Allocation Problems

Given a graph G = (A ‚à™ B, E) as above and a constant k ‚â• 1 (which we shall fix below),
we build in polynomial-time a tuple S(G) = hA, G, Œ∂ t i and a vector wt such that:
S
(1) A = {Œ±} ‚à™ (a,b)‚ààE {(a, b)1 , ..., (a, b)k }, i.e., agents are one-to-one associated with k
distinct clones of each edge (a, b) ‚àà E, plus a distinguished agent Œ±. Note that |A| > n,
because in the considered bipartite graphs m ‚â• n holds.
(2) G = {gŒ± } ‚à™ A ‚à™ B, i.e., goods correspond to nodes, plus a distinguished good gŒ± .
(3) Œ∂Œ± = 1 and Œ∂(a,b)i = 2, for each (a, b) ‚àà A and i ‚àà {1, ..., k}, are the components of Œ∂ t
associated with agents Œ± and (a, b)i , respectively.
(4) For each agent c ‚àà A, the associated component wc in the vector wt is defined as
follows. For each (a, b)i ‚àà A, w(a,b)i (a) = 2, w(a,b)i (b) = 2, w(a,b)i (gŒ± ) = 1, w(a,b)i (x) = 0,
‚àÄx ‚àà (A ‚à™ B) \ {a, b}. Moreover, wŒ± (gŒ± ) = 1, wŒ± (x0 ) = 0, ‚àÄx0 ‚àà (A ‚à™ B).
Let us now fix some notations. For any set E 0 ‚äÜ E of edges, let match(E 0 ) denote the
size of the largest set E 00 ‚äÜ E 0 of edges that is a matching. For any set C ‚äÜ A \ {Œ±} of
agents, let A(C) = {a | (a, b)i ‚àà C} and B(C) = {b | (a, b)i ‚àà C}. Finally, we say that
C ‚äÜ A \ {Œ±} is tight if it does not contain two agents of the form (a, b)i and (a, b)j , with
i 6= j, i.e., associated with the same edge of G.
Observe that, for each set C ‚äÜ A \ {Œ±} of agents,
opt(hC ‚à™ {Œ±}, G, Œ∂ t i, wt ) ‚àí opt(hC, G, Œ∂ t i, wt ) =



1
0

if C is tight, and |C| = A(C) = B(C)
otherwise

(5)

Indeed, if œÄC0 is an optimal allocation for hC ‚à™ {Œ±}, G, Œ∂ t i w.r.t. wt , then we always have that
val(œÄC0 , t) = 2 √ó |A(C)| + 2 √ó |B(C)| + 1. Instead, if œÄC is an optimal allocation for hC, G, Œ∂ t i
w.r.t. wt , then we have

2 √ó |A(C)| + 2 √ó |B(C)|
if C is tight, and |C| = A(C) = B(C)
val(œÄC ) =
2 √ó |A(C)| + 2 √ó |B(C)| + 1 otherwise
best
for
By exploiting Equation 5, we can now express the Shapley value of the game GS(G),t
agent Œ± in a convenient way. Let Xh denote the number of sets C ‚äÜ A \ {Œ±} of agents which
are tight and such that |C| = |A(C)| = |B(C)| = h, and let X0 = 1. Then,

|A|‚àí1
best
œÜŒ± (Gw
)=

X (|A| ‚àí h ‚àí 1)!(h)!
Xh .
|A|!

(6)

h=0

In particular, let us now focus on the coefficient Xh . Denote by Yh the number of matchings
in G whose cardinality is h. By construction of S(G) it is immediate to check that for each
matching of cardinality h in G, there are precisely k h sets of agents C ‚äÜ A \ {Œ±} that are
tight and such that |C| = |A(C)| = |B(C)| = h. Thus, we can rewrite the above expression:
|A|‚àí1
best
œÜŒ± (Gw
)=

X

(Zh √ó Yh ) √ó k h , with Zh =

h=0

433

(|A|‚àíh‚àí1)!(h)!
.
|A|!

(7)

Greco & Scarcello

best
For an expression as the one above, given the value of œÜŒ± (Gw
), it is known that under
certain circumstances we can reconstruct in polynomial time the value of each single term of
the form Zh √ó Yh (see Fact 6 in the work by Valiant (1979b)): We need an integer constant
A > 2 such that, for each h ‚àà {0, ..., |A| ‚àí 1}, Zh √ó Yh ‚â§ A, and k ‚â• A2 . In our case, it can
be noticed that, for each h ‚àà {0, ..., |A|‚àí1}, Zh √óYh ‚â§ 1 holds, as Yh ‚â§ |A|!/((h)!(|A| ‚àí h)!).
best
), we can compute in polynomial
Thus, for k = 9, we have that, given the value of œÜŒ± (Gw
time all such terms. In particular, we can compute in polynomial time the term associated
to h = |A| = |B| = n, where recall that |A| > n. This term has the form Zn √ó Yn , with Yn
being the number of perfect matchings in G. Thus, by putting it all together and since Zn
can be computed in polynomial time (as the size of the numbers n and |A| are logarithmic
w.r.t. the size of G), the number of perfect matchings in bipartite graphs can be counted in
polynomial time too, which concludes the proof.

By Lemma 4.8 and Theorem 5.5, the following is immediate.
Corollary 6.2. Computing the payments as given by the rule pŒæ is #P-complete.
6.2 A Fully Polynomial-Time Randomized Approximation Scheme
An approach to circumvent the intractability of the Shapley value is based on approximation:
For a game G = hN, œïi, a vector œÜÃÇ is an Œµ-approximation of the Shapley value if |œÜÃÇi ‚àíœÜi (G)| ‚â§
Œµ √ó œÜi (G) holds, for each i ‚àà N .
Recently, a sampling method conceived by Bachrach, Markakis, Resnick, Procaccia,
Rosenschein, and Saberi (2010) for the special class of simple coalitional games has been
extended to deal with arbitrary games that are supermodular and monotone 6 (Liben-Nowell,
Sharp, Wexler, & Woods, 2012), under the assumption that the value œï(R) can be computed
by an oracle having unitary cost, for each R ‚äÜ N . The result is that, for any Œµ > 0 and
Œ¥ > 0, it is possible to compute in time poly(N, 1/Œµ, log(1/Œ¥)) a vector œÜÃÇ that is an Œµapproximation of the Shapley value with probability of failure at most Œ¥. A method with
these properties is called a fully polynomial-time randomized approximation scheme.
Next, we propose a payment rule pÃÇŒæ that is founded on the sampling strategy described
in the work by Liben-Nowell et al. (2012). The payment rule, reported in Figure 5, samples
m subsets of A storing them in CÃÇ (together with other subsets functionally determined by
the samples), and then computes the value Œæ(œÄ, wŒ∏ ) as in Figure 4, but with CÃÇ playing the
role of the power-set C. The process is repeated Œò(log(1/Œ¥)) times, and the componentwise median vector of all such payments is computed. The resulting payment is eventually
defined at step 9.
The new rule pÃÇŒæ gives rise to a randomized mechanism that is of course still verifiable
and uses no punishment. Moreover, the mechanism is truthful even when the realization
of the set CÃÇ, sampled in step 1, is known beforehand.7 Formally, the mechanism turns out
to be universally truthful, i.e., it is a probability distribution over deterministic truthful
mechanisms (see, e.g., Dobzinski & Dughmi, 2009).
6. Monotonicity of G = hN, œïi means that œï(R) ‚â• œï(T ), ‚àÄT ‚äÜ R ‚äÜ N .
7. This evidences that if truthfulness is our only desideratum, then there is no need to implement a payment
rule computing the Shapley value of the coalitional game associated with the allocation problem (as in
Definition 5.1), which in fact is a concept defined over all possible subsets of A and which we have shown
to be computationally hard in Theorem 6.1.

434

Mechanisms for Fair Allocation Problems

Input:
A type vector Œ∏ ‚àà Œò, a feasible allocation œÄ for SŒ∏ , and an integer m > 0;
Assumption: The verifier v (for t) is available, with v(œÄ) = (v1 , ..., vn );
Notation:
SŒ∏ = hA, G, Œ∂ Œ∏ i, wŒ∏ = (w1 , ..., wn ), wv(œÄ) = (wv1 , ..., wvn );
1.
2.
3.
4.
5.
6.
7.
8.
9.

Generate a set C of m subsets of A, and
let CÃÇ := C ‚à™ {(A \ C) ‚à™ {i} | C ‚àà C, i ‚àà C} ‚à™ {A};
For each i ‚àà A and C ‚àà CÃÇ,
| Compute an optimal allocation œÄC,i for hC, img(œÄ), Œ∂ (vi ,Œ∏‚àíi ) i w.r.t. w(vi ,Œ∏‚àíi ) ;
b Compute an optimal allocation for œÄC\{i},i for hC \ {i}, img(œÄ), Œ∂ (vi ,Œ∏‚àíi ) i w.r.t. w(vi ,Œ∏‚àíi ) ;
For each agent i ‚àà A,
b Compute Œæi (œÄ, wŒ∏ ) as in Figure 4 (steps 4‚Äî8), with C := CÃÇ;
Repeat Œò(log(1/Œ¥)) times steps 1, 2, and 5, and
ÀÜ wŒ∏ ) be the component-wise median vector of these vectors Œæ(œÄ, wŒ∏ );
Let Œæ(œÄ,
Define pÃÇŒæi (œÄ, wŒ∏ ) := ŒæÀÜi (œÄ, wŒ∏ ) ‚àí wvi (œÄ);
Figure 5: Payment rule pÃÇŒæ .

Theorem 6.3. Let A be any optimal allocation algorithm. Then, the mechanism (A, pÃÇŒæ ) is
universally truthful.
Proof. The result follows by inspecting the proof of Theorem 4.9 for rule pŒæ in Section 4.
Indeed, it can be immediately checked that the proof does not depend on the specific
subset of coalitions C, and thus it smoothly applies if any set of coalitions CÃÇ is used as
in Figure 5, instead of all possible subsets of A. Note in particular that, in the proof of
Theorem 4.9, properties (A) and (B) are precisely those guaranteeing truthfulness, and that
these properties hold for each given coalition C. Therefore, they still hold for any subset of
coalitions randomly chosen by the mechanism.
Similarly, by inspecting the proofs in Section 4, in this universally truthful mechanism,
the following properties are seen to hold for every realization of the random coin tosses.
Theorem 6.4 (basic properties). Let A be any optimal allocation algorithm. Then, the
mechanism (A, pÃÇŒæ ) is efficient and guarantees an equal treatment of the equals. Moreover,
if all valuations are non-negative, then (A, pÃÇŒæ ) is individually rational.
For a deeper analysis of the payment rule pÃÇŒæ , we next point out a relationship between
utility values and approximations of the Shapley value, at the equilibrium when all agents
report their true types.
Lemma 6.5. Let A = {1, ..., |A|}, and let m = Œò(|A|2 /Œµ2 ). For each optimal allocation
œÄ for St w.r.t. wt , the vector (u1,pÃÇŒæ (œÄ, wt ), ..., u|A|,pÃÇŒæ (œÄ, wt )) is an Œµ-approximation of the
marg
best
Shapley value of Gw
(and Gw
) with probability 1 ‚àí Œ¥, and coincides with it in expectation.
Proof. By exploiting the same line of reasoning as in the proofs in Section 4 for pŒæ , we can
see that Œæi (œÄ, wt ) (at step 6 of the algorithm in Figure 5) can be rewritten as follows:
Œæi (œÄ, wt ) =

X (|A| ‚àí |C|)!(|C| ‚àí 1)!
(bestw (C) ‚àí bestw (C \ {i})) .
|A|!

C‚ààCÃÇ

435

Greco & Scarcello

Then, we observe that by construction of CÃÇ in step 1, for each set C ‚àà CÃÇ and each agent
i ‚àà A, the set (A \ C) ‚à™ {i} is in CÃÇ, too. Thus, we can apply the same line of reasoning as
in the proof of Theorem 5.5, in order to conclude that:
Œæi (œÄ, wt ) =

X (|A| ‚àí |C|)!(|C| ‚àí 1)!
(margw (C) ‚àí margw (C \ {i})) .
|A|!

C‚ààCÃÇ

marg
= hA, margw i is supermodular by Corollary 5.4. MoreNow, recall that the game Gw
marg
over, Gw is clearly monotone. Thus, by Liben-Nowell et al. (2012), the above expression
marg
, and it approximates this value
coincides in expectation with the Shapley value of Gw
with constant probability. Steps 7 and 8 serve to amplify the probability (c.f. Liben-Nowell
et al., 2012), and to get a fully polynomial-time randomized approximation scheme. The
result follows as step 9 implements the usual bonus and compensation approach, so that
ui,pÃÇŒæ (œÄ, wt ) coincides with the above expression, for each i ‚àà A (cf. Lemma 4.8).

Note that in the approach by Liben-Nowell
P et al. (2012), a final normalization step
is carried out that would guarantee that
i‚ààA u1,pÃÇŒæ (œÄ, wt ) = margw (A) = bestw (A).
Unfortunately, this way truthfulness might be lost, hence we did not include such a normalization procedure in the above payment rule. As a consequence, the mechanism pÃÇŒæ does
not guarantee (for instance) budget-balancedness and Pareto-efficiency. However, because
of Lemma 6.5, since the expected utility profile coincides with the Shapley value, pÃÇŒæ enjoys
in expectation all the properties of pŒæ including the above two ones. In particular, we can
still have approximate counterparts for Theorem 5.6 and Theorem 5.8.
Theorem 6.6. Let œÄ be an optimal allocation for St w.r.t. wt . Let m = Œò(|A|2 /Œµ2 ). Then,
with probability 1 ‚àí Œ¥,
P
‚Ä¢ (1 + Œµ) √ó bestw (C) ‚â• i‚ààC ui,pÃÇŒæ (œÄ, wt ) ‚â• (1 ‚àí Œµ) √ó margw (C), for each C ‚äÜ A;
‚Ä¢ Œµ √ó val(œÄ, wt ) ‚â•

P

Œæ
i‚ààA pÃÇi (œÄ, wt )

‚â• ‚àíŒµ √ó val(œÄ, wt ).

Proof. Here, just observe
of Lemma 6.5 and TheoremP5.5, for each set
Pthat, in the light P
C ‚äÜ A, we have (1‚àíŒµ)√ó i‚ààC ui,pÃÇŒæ (œÄ, wt ) ‚â§ i‚ààC ui,pŒæ (œÄ, wt ) ‚â§ (1+Œµ)√ó i‚ààC ui,pÃÇŒæ (œÄ, wt ).
The result then follows by substituting such bounds in Theorem 5.6 and Theorem 5.8,
respectively, with simple algebraic manipulations.
Finally, we propose a further randomized mechanism that is able to guarantee both economic efficiency and budget-balancedness. The price to be paid is however that truthfulness
holds in expectation only. The mechanism is based on a payment rule that we call pÃÑŒæ .
Theorem 6.7. Let A be any optimal allocation algorithm. Then, there is a (randomized)
mechanism with verification (A, pÃÑŒæ ) that is truthful in expectation, and that (at the truthful
equilibrium) is efficient and budget-balanced. Moreover, if all valuations are non-negative,
then (A, pÃÑŒæ ) is individually rational.
Proof. The payment rule pÃÑŒæ follows the steps in Figure 5, with minor modifications in
step 8 and step 9: First, in step 8, whenever we compute the median value ŒæÀÜi (œÄ, wŒ∏ ) for
agent i, we also compute the corresponding value ŒæÀÜi (œÄ, wv(œÄ) ) (evaluated on the revealed
436

Mechanisms for Fair Allocation Problems

types rather than on the reported ones). Then, we define a normalization factor R =
P
opt(hA, img(œÄ), Œ∂ v(œÄ) i, wv(œÄ) )/( i‚ààA ŒæÀÜi (œÄ, wv(œÄ) )), so that, in step 9, pÃÑŒæi (œÄ, wŒ∏ ) is eventually
returned as wvi (œÄ) ‚àí ŒæÀÜi (œÄ, wŒ∏ ) √ó R.
Concerning truthfulness, we can just note that the expected value of R is 1. Indeed, by
Lemma 6.5, the expected value of ŒæÀÜi (œÄ, wv(œÄ) ) is Œæi (œÄ, wv(œÄ) ); hence, the sum of all these
values coincides with opt(hA, img(œÄ), Œ∂ v(œÄ) i, wv(œÄ) ) by the efficiency of the Shapley value
(as in the proof of Theorem 5.8). Thus, the expected utility of an agent i under the payment
rule pÃÑŒæ coincides with the (actual, i.e., not in expectation) utility of i under the rule pÃÇŒæ .
Hence, truthfulness in expectation follows by Theorem 6.3. Now, we can just check that,
at the truthful equilibrium, the maximum social welfare is achieved (equilibrium efficiency)
P
P
and i‚ààA pÃÑŒæi (œÄ, wt ) = opt(hA, img(œÄ), Œ∂ t i, wv(œÄ) ) ‚àí i‚ààA ŒæÀÜi (œÄ, wt ) √ó R = 0. That is, the
mechanism is budget-balanced, too. Finally, the mechanism pÃÑŒæ is seen to be individuallyrational, by exploiting the same line of reasoning as the one used for the mechanism based on
pŒæ , since the corresponding proof in Section 4 is not affected by the sampling strategy.
Again, by Lemma 6.5, all the remaining properties hold in expectation.

7. Related approaches to Mechanisms with Verification
We next review the main approaches in the literature for mechanisms with verification, and
point out the differences w.r.t. our proposal.
First we observe that, differently from our general setting where the types of the agents
can determine the vector of the upper bounds for the allocation problem, in earlier approaches it is assumed that agent types do not have any impact on the underlying combinatorial problem, so that the type vector precisely coincides with the valuation vector. Instead,
we deal explicitly with both allocation constraints and goods valuations. For instance, in
a scheduling problem formalized in our setting, the private type of an agent/machine can
be its speed, while its valuation function is fully determined by this speed and the size
of the job to be processed. Then, the verifier is just asked to measure the speed of such
agents/machines‚Äîsee also Appendix A. In fact, it is immediate to encode the valuations
as types as well, at the price however of hiding the true complexity (or simplicity) of the
setting. For instance, in the above example, if the type of an agent is the vector of its
valuations, then we would miss the information that the agent is ultimately characterized
by one (observable) parameter only.
More substantial differences between earlier approaches and our proposal come in the
very definition of the utility of an agent. In the works by Auletta et al. (2009), Penna
and Ventre (2012a, 2012b), Krysta and Ventre (2010), Auletta et al. (2006) and Ferrante
et al. (2009), the individual welfare of an agent i, given the outcome œÄ and the vector d of
reported types, is assumed to be of the following form:

0
if i is caught lying
ui,p (œÄ, d) = ti (œÄ) ‚àí
pi (œÄ, d) otherwise
where pi (¬∑, ¬∑) is a payment that does not depend on the vector t of the true types.
In these papers, the only information that is assumed to be available at payment time is
whether the reported type di of agent i differs or not from its actual true type ti . This is the

437

Greco & Scarcello

role played by the verifier, and in fact pi (œÄ, d) does not exploit the possibility of partially
revealing ti . Moreover, the payment scheme adopted punishes those agents that are caught
lying. Hence, while the verification process provides a smaller amount of information than
the verification process in our approach, the rules used to discourage strategic behaviors
are stronger than ours and based on punishing agents. Also, the above works assume
that agents‚Äô misreporting is restricted to certain kinds of lies (e.g., values lower than the
corresponding true ones), so that a form of ‚Äúone-sided‚Äù verification suffices.
Recently, the above model of (partial) verification has been extended by Caragiannis
et al. (2012) to a setting where an agent cheating on her/his type will be identified with
some probability that may depend on her/his true type, the reported type, or both. In fact,
Caragiannis et al. also showed that there are cases in which verification does not help if
it is not one-sided. The payment scheme is exactly the same as the one discussed above
and, hence, verification does not exploit any (possibly partial) knowledge of the actual
true type and a punishment approach is still used. The main novelty, in addition to the
probabilistic verification, is that there is no constraint on the type that an agent can report
while cheating.
Finally, a different kind of verification model goes back to the seminal paper by Nisan
and Ronen (2001), and it is actually closer to our ‚Äúno-punishment‚Äù perspective, because an
agent i can in principle be paid by the mechanism even if i has been caught lying. Given
n agents, Nisan and Ronen consider a vector e = (e1 , ..., en ) of ‚Äúobserved‚Äù agent types,
which are completely known after the verification process. Moreover, the individual utility
of any agent i has the form ui,p (œÄ, d) = ei (œÄ) ‚àí pi (œÄ, d), so that the vector e in such a
framework plays the same role as the verifier‚Äôs output in our approach. A first difference
between the work by Nisan and Ronen and our approach is that, in the above model, agents‚Äô
misreporting is again restricted only to certain kinds of lies. On the other hand, we consider
a restriction on the form of valuation functions, since our verification model assumes that
valuations are determined by some objective and observable properties of goods and agents
(encoded by functions of the form Œªg and œái , as defined in Section 2). As a consequence,
at payment time, the valuation of any agent i is known for every good in img(œÄ). Instead,
in the setting by Nisan and Ronen, the valuation of each agent i for goods in img(œÄ) \ œÄ(i)
remains unknown even after the verification is performed. From the results presented in this
paper, it turns out that this difference between the two framework is crucial to overcome
classical impossibility results, and to meet all desirable properties at once (without using
any punishing power).

8. Conclusion
In this paper, we have proposed and analyzed mechanisms for fair allocation problems in a
setting where agents‚Äô declarations regard objective properties of goods or agents, and thus
can be (partially) verified before payments are made. In particular, we have considered a
model of verification that is able to disclose the true values of allocated goods, in contrast
to previous approaches in the literature where partial and probabilistic verification have
been considered. However, the use of this verification power is in fact quite limited, because
payment rules have been designed so that no punishment is meted out to agents whose
declarations do not match the output of the verification process. This requirement is crucial
438

Mechanisms for Fair Allocation Problems

for practical applications of the framework, such as the Italian research evaluation described
in (Greco & Scarcello, 2013) (and summarized in the Appendix), because any discrepancy
between declared and verified values may be due to sensing errors or subjective issues that
cannot be interpreted as agents‚Äô lies to be punished.
The challenge was to show that, in this framework, truthfulness, efficiency, budgetbalancedness, and fairness (as well as further desirable properties discussed in Section 2)
can be achieved simultaneously, unlike the classical setting. It is worthwhile noting that, if
one is guaranteed that all agents truthfully report their true types (no strategic behaviors)
or, equivalently, if these types are given as public knowledge, then the problem is quite easy.
For instance, one can just use payments in a way that each agent gets as her/his utility
the Shapley value according to either coalitional game defined in Section 5. Moreover, if
fairness is not an issue, then the other properties can be obtained by even using the simple
‚Äúuniform‚Äù payment rule. However, whenever agents may behave strategically, reporting the
true type is no longer a dominant strategy, if such approaches are used. In fact, truthfulness
might be enforced by equipping the mechanism with suitable punishment rules. However,
we already argued that this would be not appropriate in our context, and typically the
resulting mechanism would not be error tolerant.
By looking at the proposed framework from an abstract perspective, one may notice that
it is based on two fundamental ingredients: a base combinatorial problem that determines
feasible and optimal allocations, and a game-theoretic notion that describes what is considered fair, with respect to agents‚Äô contributions and expectations. Namely, in the application
domain of allocation problems addressed in the paper, the weighted matching is the basic
combinatorial problem and the Shapley value is the most natural game-theoretic solution
concept. Under this perspective, a natural research direction is to study different instances
of such an abstract framework for mechanisms with verification, where other combinatorial
problems (colorings, coverings, etc.) and different solution concepts (Nucleolus, Banzhaf
index, etc.) may be more appropriate and best describe the problem at hands.
Another interesting avenue for further research would be to study a modification of
the framework where agents‚Äô preferences are not directly expressed in terms of real-valued
functions, but they are rather formalized in terms of orderings/ranks over available goods.
This is particularly interesting given that, in the strategic setting we have defined, agents‚Äô
declarations only contribute to the definition of which goods will be selected, but they do
not determine their values.

Acknowledgments
We thank the anonymous referees and the Associate Editor for their very useful comments.

Appendix A. Application Scenarios
Fair allocation with monetary compensation has been intensively studied in the literature,
very often in settings where all parameters of interest are public knowledge (or, simply,
by getting rid of strategic issues). One example application discussed in the literature is
parking space and benefit allocation at a workplace, where each employee gets a parking
space and a share from a fixed benefits package. House allocation problems are another

439

Greco & Scarcello

classical example, where agents collectively own a set of houses, and we look for a systematic
way of exclusively assigning a house to each agent, possibly with monetary compensations.
A third example is room assignment-rent division, where a group of agents rent a house,
with each of them getting a room and paying a share of the rent.
In the following, we illustrate further applications of fair allocation problems that fit
our general framework discussed in Section 2. In the applications we shall discuss, part of
the relevant information is private knowledge of the agents, and verification can be adopted
to measure observable properties before the payment phase. Nonetheless, we stress here
that, even if all the relevant information were available as public knowledge or could be
measured in advance, such applications would still remain of interest, as they show the
need of defining allocation policies that guarantee fair and/or error tolerant solutions.
A.1 The Italian Research Assessment Program (VQR) 2004‚Äì2010
We start our overview of possible application scenarios by focusing on a real-world case
study that first motivated our investigation on allocation problems. This application is
best described in a companion paper (Greco & Scarcello, 2013), which applies the results
presented here to the specific case of the Italian research assessment program.
A.1.1 The Setting
In 2012, the National Agency for the Evaluation of Universities and Research Institutes
(ANVUR) has promoted the ‚ÄòVQR‚Äô assessment program devoted to evaluate the quality of
the whole Italian research production. In its first application, the program focuses on the
period 2004-2010, while the evaluation will be repeated on a regular basis (the next one
should cover years 2011‚Äì2014).
In the first phase of the program, every structure R (a university or a research institute)
selects and submits to ANVUR a set PœÄ of its products, under the constraints that (i) each
product has to be univocally associated with an author (even if the product is co-authored)
and that (ii) at most three products can be associated with each author affiliated with R.8
In abstract terms, R computes in this phase an allocation œÄ, with img(œÄ) = PœÄ , for the
scenario where R is the set of the agents/researchers affiliated with R, where P is the set
of all the goods/products they have co-authored, and where for each r ‚àà R, Œ∂r = 3 is the
associated constraint on the number of goods/products that can be allocated to r. The way
this allocation is performed is described below.
In the second phase, ANVUR evaluates the products in PœÄ by equipping each of them
with a quality score, expressed as a number.9 Hence, this phase defines an official valuation
such that, for each product p ‚àà œÄ(r), w(p) is the quality score assigned by ANVUR to p.
The overall score of R will be the sum of the values of all products in PœÄ , and it will be
used to proportionally transfer to R the funds allocated by the Ministry to support research
activities in the next years, until data from a new evaluation for the subsequent period will
be available.
8. We simplify here. Actually, the number of publications may be less than three, for some authors.
9. The set of the possible scores is defined in the VQR guidelines. To our ends, this detail is immaterial
and scores are just viewed as (arbitrary) real numbers.

440

Mechanisms for Fair Allocation Problems

A.1.2 The Need of a Fair Division
The VQR program actually assigns a score not only to the structure R, but also to all its
substructures (e.g., to departments, if R is a university). Of course, this is expected to
have an impact on the funds redistribution inside every research structure, and therefore a
crucial problem is to define a fair rule for funds redistribution, that is, a rule that is capable
to assign funds by clearly reflecting the true contribution of each researcher/substructure
to the performances of the structure as a whole. Moreover, the recruitment policies of
universities are going to be evaluated as well, by looking at the (VQR) performances of
researchers that were hired recently. However, no redistribution rule has been specified in
the program, and most researchers believe that the evaluation
P of (the contribution of) each
r ‚àà R will just coincide with the overall value wr (œÄ) =
p‚ààœÄ(r) w(p) of those products
allocated to r in the submission phase. Of course this is not fair in general, because if a
product p ‚àà PœÄ is co-authored by two researchers of R, then they both (and, in turn, their
structures) might claim this contribution even if the product has been formally allocated
just to one of them.10 Therefore, more sophisticated approaches have to be defined for a
proper evaluation of individuals and substructures.
A.1.3 Strategic Issues and Verification in the VQR Program
Recall that the goal of structure R is to submit to ANVUR (in the first phase of the program) the products that will likely get the highest possible scores (in the second phase).
To this end, there are publicly available evaluation criteria that should allow one to perform a ranking of products, ideally by equipping each of them with the quality score that
will be assigned to it by ANVUR in the subsequent phase. Both for time and economic
reasons, this first evaluation phase was based only on self-evaluations performed by the
authors of the products, which are clearly assumed to be the best experts on their research
subjects. Moreover, in most structures, to decide the precise allocation of products, hence
to deal with the conflicts related to co-authored products, researchers performed choices in
a decentralized way; only a few structures set up an optimization framework to compute
an optimal allocation, based on agents‚Äô declarations. In any case, by abstracting from the
specific method adopted to end up with a feasible allocation to be submitted for the VQR,
strategic issues emerged in this phase. In particular, there are some researchers r ‚àà R that
‚Äúguided‚Äù the allocation of the products (e.g., cheating on their quality) for the supposed
personal interest of maximizing the value wr (œÄ). Clearly enough, this way optimal product
selections can hardly be achieved by the structure.
Note that the VQR case perfectly fits our framework of allocation problems: here the research products are the indivisible goods to be allocated to researchers/agents, who initially
declare their goods‚Äô valuations. Moreover, the social welfare is the total ‚ÄúANVUR score‚Äù of
the research structure, that should be distributed in a budget-balanced way to researchers,
and hence to their substructures, in a fair way. Note that the upper bound constraint for
each researcher (3 products, with some exceptions) does not depend on the type in this case,
10. This problem does not occur for products co-authored by researchers belonging to different structures.
Indeed, the same product can be submitted by several structures (assigned to a different co-author in
each of them). Thus, for a given research structure R, the co-authors of interest are only those affiliated
with R.

441

Greco & Scarcello

because it is a parameter fixed by ANVUR. Therefore, the private information of agents
is limited to good valuations. Moreover, observe that the specific valuation functions occurring in the VQR fit the verification model proposed in the paper. In particular, every
research product has an objective value, which can be ‚Äúmeasured‚Äù by a verifier for each
allocated and hence submitted product (while nothing can be said for products that are not
allocated to any researcher). Thus, ANVUR precisely acts as the verifier of the model.
A.1.4 Mechanisms for the VQR Program
The basic approach of assigning to each researcher r ‚àà R the value wr (œÄ) is precisely the
(trivial no-payment) rule p‚ó¶ in Example 2.6. There, we have observed that mechanisms
based on this rule are not truthful in general. Moreover, from Example 3.1 is also emerged
that the rule is not fair (in particular) when combined with an optimal allocation algorithm
(in order to to guarantee efficiency). Therefore, this rule is highly undesirable, and different
kinds of mechanisms have to be defined for the VQR setting.
In fact, we would like to end up with a mechanism that is able as able to collect the
correct self-evaluations (truthfulness) and to obtain the maximum possible performance
(efficiency) for the research structures. These two goals, however, have to be accomplished
together with fairness. Indeed, we would like to guarantee to every researcher or group of
researchers to get at least their marginal contributions, and that the resulting mechanism is
also individually optimal. The latter property is very important for this application, since
it guarantees that, for any researcher, her/his score is the maximum one over all possible
alternative allocations, including allocations using products that were not submitted by the
structure and hence not verified. Moreover, it is also relevant that these properties are
guaranteed via mechanisms enjoying the verifiability property, in that the payment rule
only uses ‚Äúcertified‚Äù valuations (it would be unacceptable to use product values that are
not verified by ANVUR).
Finally, it is worthwhile noting that ANVUR guidelines defines a range of possible product values determined, e.g., by the publishing venues and by citation indices, by pointing
out that peer-reviews can also be used to override such a basic classification (e.g., for papers published on a good journal but without many citations, or after an explicit author‚Äôs
request). Clearly enough, this entails that discrepancies between authors‚Äô declarations and
ANVUR evaluations might well emerge even in absence of any malicious behavior. Hence,
the no-punishment (and error-tolerant) approach seems to be unavoidable for a concrete
and politically acceptable application of any mechanism.
A.2 (Cooperative) Scheduling and Task Allocation
Assume that h jobs in the set J = {Ôöæ1 , ..., Ôöæh } have to be executed within a deadline d, and
that n agents/machines, m1 , ..., mn , with n ‚â§ h, are available to execute in parallel these
jobs. For each machine mi , with i ‚àà {1..., n}, let Œ≥i ‚äÜ J denote the set of jobs that mi can
in principle execute. These sets are determined, for instance, by physical, technological, or
accessibility constraints and are known to the scheduler, while the precise speed that the
machines guarantee for this process is not known to the scheduler, and should be declared by
agents. It is assumed that each machine mi can always work at such a speed independently
of the workload actually assigned to it (for the considered jobs Œ≥i at hand). For the sake of
442

Mechanisms for Fair Allocation Problems

simplicity, assume also that each job consists of the same workload wl , and that for each
executed job, a fixed profit pr is earned. Moreover, a further profit pr add will be earned
by the scheduler if all jobs are correctly executed, and (part of it) can be distributed to
agents. Every machine participating in the process will execute at least one job (i.e., we
are assuming we have many jobs that can be executed by any machine and machines have
comparable speeds so that it is convenient to use all of them). Furthermore, every machine
is dedicated to this process and it aims at executing as many jobs from J as possible,
because it cannot earn any profit from external jobs (not in J ).
Therefore, we have an allocation problem where the goods/jobs in the set {Ôöæ1 , ..., Ôöæh }
have to be allocated/scheduled to the agents/machines in the set {m1 , ..., mn }. Here, the
private type of agent mi is its speed si (devoted to this process), for each i ‚àà {1, ..., n}.
Moreover, the vector Œ∂ of the upper bounds to the number of goods that can be allocated
i
to the agents is defined as Œ∂i (si ) = b d√ós
wl c, for each i ‚àà {1, ..., n}. Note, in particular, that
the jobs have to be completed within the deadline, so that the upper bound constraints of
the allocation scenario are functions of the types/speeds of agents/machines. Finally, we
can define the valuation vector w = (w1 , ..., wh ) so that, for each i ‚àà {1, ..., n}, wi (Ôöæx ) = pr ,
if Ôöæx ‚àà Œ≥i ; and wi (Ôöæx ) = ‚àí1, if Ôöæx 6‚àà Œ≥i . Note that in this case valuations are independent on
the types of the agents (of course, we are assuming that any single job in Œ≥i can be executed
within the deadline d).
Given that the same task can be carried out by different agents, it is sensible that
allocations are perceived to be as fair ones (see, e.g., Porter et al., 2004). To this end, the
mechanism with monetary compensations described in this paper can be adopted. In fact,
note that strategic issues come into play, and the egoistic behavior of some agents may lead
to allocations that are not optimal and possibly can miss the extra-reward pr add . Observe
that the scheduler may act as the verifier in our model, at the end of the process. Indeed,
after the allocation is computed and jobs are executed based on it, it can immediately
verify the truthfulness of agents‚Äô declarations by looking at the amount of time used by
each machine to execute the jobs assigned to it.
Furthermore, in practice, speed values are given with a finite precision, as well as their
physical verification is subject to measurement errors. For this reason, even this mechanism
should be tolerant of errors, and not based on punishments.
As a further example, consider the following task allocation problem, which can be
viewed as a variation of the previous one: Assume that a company select some agents
e1 , ..., en to perform a given set of tasks {t1 , ..., tm } within a certain deadline, and assume
that the company does not know precisely whether or not such agents have the necessary
skills (experience, strength, speed, competence, etc.). For instance, this may happen if the
company is starting some new line of production with new tasks, or if such tasks should be
executed by means of crowdsourcing, where agents are selected through an internet call.
Therefore, in this case agents‚Äô types declarations comprise both their skills and the
number of tasks they are able to execute within the required deadline. Valuation functions
encode the profit to be earned by any agent executing a given task (where the value 0
means that the agent is not able to execute that task). Note that here both the upper
bound constraints of the allocation scenario and the valuations functions depend on agents‚Äô
types. Again, a mechanism with monetary compensations can be used, in order to provide a
fair distribution of the company reward for those tasks, and to encourage agents to truthfully
443

Greco & Scarcello

declare their skills. Note that the framework with verification proposed in this paper can
be used under the assumption that the agents‚Äô skills necessary for the proposed set of tasks
can be observed and evaluated by the verifier, (at least) at the end of the process, where
all agents performed their work (possibly with failures).
A.3 Protocols for Wireless Communication Networks
We conclude this overview of possible applications by considering a cost problem where,
moreover, there is no private information, and hence no strategic behavior. This is to remark
that the proposal described in the paper can be used even when mechanism design is not
necessary, but one still needs a policy for fair division that enjoys the properties described
in Section 3.1 (e.g., envy freeness, individual optimality, and so on). In fact, fairness
issues are currently attracting much attention in the design of scheduling protocols over
wireless communication networks, where the underlying problem is bandwidth allocation;
for instance, in the design of protocols for high-speed wide area wireless networks, where
the role of monetary compensation is played by adjustments in the priorities of users (see,
e.g., Jalali, Padovani, & Pankaj, 2000). Moreover, a number of network applications are
emerging where ‚Äúdirect‚Äù forms of monetary compensation are considered.
As an example, let us consider ad-hoc networks, which are self-organizing wireless infrastructures where mobile nodes cooperatively act via multi-hop routing to transmit data even
when source and destination nodes are out of their transmission ranges. Cooperation can be
achieved by associating a credit balance with each node, so that nodes use credits to pay for
sending their own traffic, and earn credits by forwarding traffic from other nodes to compensate bandwidth and power consumption‚Äîsee, for instance, the work by GoÃàbel, Krzesinski,
and Mandjes (2009) and the references therein. Consider a setting where the nodes s1 , ..., sn
are willing to transmit some data in an ad-hoc network. In a given configuration of the
network, the nodes r1 , ..., rm can be used to forward these data. In particular, whenever
si and rj , with i ‚àà {1, ..., n} and j ‚àà {1, ..., m}, are within their transmission ranges, we
denote by ci,j the credit paid by si to transfer the data via rj ‚Äîa large enough value for ci,j
can be used to state that the transmission is not possible. The resulting cost problem can
be modeled via an allocation scenario where Œ∂i = 1, and where the valuation vector is such
that wi (rj ) = ‚àíci,j , for each i ‚àà {1, ..., n} and j ‚àà {1, ..., m}. Thus, maximizing the social
welfare amounts to minimizing the overall credits paid by source nodes.
It is easily seen that, even if there is no private information and everything is public,
fairness issues emerge in this context, too. Indeed, different sources may want to use the
same routing node to transfer data. For instance, in a simple setting where m = 2 and
n = 2 and where r1 is the most preferred routing node for both s1 and s2 (i.e., c1,1 < c1,2
and c2,1 < c2,2 ), the node that is ‚Äúforced‚Äù to transfer data via r2 might perceive the
given allocation as unfair. This suggests that the credits paid by source nodes should be
‚Äúadjusted‚Äù via payment rules that, in our proposal, are based on the Shapley value of the
coalitional games described in Section 5.
A setting similar to the one discussed above emerges in wireless cooperative file sharing
systems, where mobile subscribers cluster together by downloading (portions of) files of
interest over long-range cellular links, and by exchanging them over short-range radio communications in a wireless local area network. These systems are cooperative environments,

444

Mechanisms for Fair Allocation Problems

whose benefits can be appreciated not only in terms of increased throughput and reduced
energy consumption, but also in terms of economic advantages both for users and content
providers. In order to be effective, however, fair allocation protocols have to be designed,
whose goal is to encourage cooperation (see, e.g., Militano, Iera, & Scarcello, 2013).

References
AbdulkadirogÃÜlu, A., SoÃànmez, T., & UÃànver, M. U. (2004). Room assignment-rent division:
A market approach. Social Choice and Welfare, 22, 515‚Äì538.
AÃägotnes, T., van der Hoek, W., Tennenholtz, M., & Wooldridge, M. (2009). Power in
normative systems. In Proc. of AAMAS‚Äô09, pp. 145‚Äì152.
Alcalde, J., & BarberaÃÄ, S. (1994). Top dominance and the possibility of strategyproof stable
allocations to matching problems. Economic Theory, 4, 417‚Äì435.
Alkan, A., Demange, G., & Gale, D. (1991). Fair allocation of indivisible goods and criteria
of justice. Econometrica, 59 (4), 1023‚Äì39.
Andersson, T. (2009). A general strategy-proof fair allocation mechanism revisited. Economics Bulletin, 29 (3), 1717‚Äì1722.
Andersson, T., & Svensson, L.-G. (2008). Non-manipulable assignment of individuals to
positions revisited. Mathematical Social Sciences, 56 (3), 350‚Äì354.
Andersson, T., Svensson, L.-G., & Ehlers, L. (2010). Budget-balance, fairness and minimal
manipulability. Working papers 2010:16, Lund University, Department of Economics.
Aragones, E. (1995). A derivation of the money rawlsian solution. Social Choice and
Welfare, 12, 267‚Äì276.
Archer, A., & Tardos, E. (2007). Frugal path mechanisms. ACM Transactions on Algorithms, 3, 1‚Äì22.
Auletta, V., De Prisco, R., Penna, P., & Persiano, G. (2009). The power of verification for
one-parameter agents. Journal of Computer and System Sciences, 75, 190‚Äì211.
Auletta, V., De Prisco, R., Penna, P., Persiano, G., & Ventre, C. (2006). New constructions
of mechanisms with verification. In Proc. of ICALP‚Äô06, pp. 596‚Äì607.
Auletta, V., Penna, P., Persiano, G., & Ventre, C. (2011). Alternatives to truthfulness are
hard to recognize. Autonomous Agents and Multi-Agent Systems, 22 (1), 200‚Äì216.
Aumann, R. J., & Maschler, M. (1985). Game-theoretic analysis of a bankruptcy problem
from the talmud. Journal of Economic Theory, 36 (2), 195‚Äì213.
Bachrach, Y., Markakis, E., Resnick, E., Procaccia, A. D., Rosenschein, J. S., & Saberi, A.
(2010). Approximating power indices: theoretical and empirical analysis. Autonomous
Agents and Multi-Agent Systems, 20, 105‚Äì122.
Bachrach, Y., & Rosenschein, J. S. (2008). Coalitional skill games. In Proc. of AAMAS‚Äô08,
pp. 1023‚Äì1030.
Bachrach, Y., & Rosenschein, J. (2009). Power in threshold network flow games. Autonomous Agents and Multi-Agent Systems, 18 (1), 106‚Äì132.

445

Greco & Scarcello

Bachrach, Y., Zuckerman, M., Wooldridge, M., & Rosenschein, J. (2013). Proof systems
and transformation games. Annals of Mathematics and Artificial Intelligence, 67 (1),
1‚Äì30.
BeviaÃÅ, C. (1998). Fair allocation in a general model with indivisible goods. Review of
Economic Design, 3, 195‚Äì213.
Bouveret, S., & Lang, J. (2008). Efficiency and envy-freeness in fair division of indivisible goods: Logical representation and complexity. Journal of Artificial Intelligence
Research, 32, 525‚Äì564.
Brams, S. J., & Kilgour, D. M. (2001). Competitive fair division. Journal of Political
Economy, 109 (2), 418‚Äì443.
Brandt, F., Conitzer, V., & Endriss, U. (2012). Multiagent Systems, chap. Computational
Social Choices. MIT Press.
Caragiannis, I., Elkind, E., Szegedy, M., & Yu, L. (2012). Mechanism design: from partial
to probabilistic verification. In Proc. of EC‚Äô12, pp. 266‚Äì283.
Clarke, E. (1971). Multipart pricing of public goods. Public Choice, 8, 1933.
Deng, X., & Papadimitriou, C. H. (1994). On the complexity of cooperative solution concepts. Mathematics of Operations Research, 19, 257‚Äì266.
Dobzinski, S., & Dughmi, S. (2009). On the power of randomization in algorithmic mechanism design. In Proc. of FOCS‚Äô09, pp. 505‚Äì514.
Dunne, P. E. (2005). Extremal behaviour in multiagent contract negotiation. Journal of
Artificial Intelligence Research, 23, 41‚Äì78.
Dunne, P. E., Wooldridge, M., & Laurence, M. (2005). The complexity of contract negotiation. Artificial Intelligence, 164 (1-2), 23‚Äì46.
Endriss, U., Maudet, N., Sadri, F., & Toni, F. (2006). Negotiating socially optimal allocations of resources. Journal of Artificial Intelligence Research, 25, 315‚Äì348.
Feige, U., & Tennenholtz, M. (2011). Mechanism design with uncertain inputs: (to err is
human, to forgive divine). In Proc. of STOC‚Äô11, pp. 549‚Äì558.
Ferrante, A., Parlato, G., Sorrentino, F., & Ventre, C. (2009). Fast payment schemes for
truthful mechanisms with verification. Theoretical Computer Science, 410, 886‚Äì899.
GoÃàbel, J., Krzesinski, A., & Mandjes, M. (2009). Incentive-based control of ad hoc networks:
A performance study. Computer Networks, 53 (14), 2427‚Äì2443.
Greco, G., & Scarcello, F. (2013). Fair division rules for funds distribution: The case of the
italian research assessment program (vqr 2004-2010). Intelligenza Artificiale, 7 (1),
45‚Äì56.
Green, J., & Laffont, J. (1977). Characterization of satisfactory mechanisms for the revelation of preferences for public goods. Econometrica, 45 (2), 427‚Äì438.
Groves, T. (1973). Incentives in teams. Econometrica, 41, 617631.
Haake, C.-J., Raith, M. G., & Su, F. E. (2002). Bidding for envy-freeness: A procedural
approach to n-player fair-division problems. Social Choice and Welfare, 19 (4), 723‚Äì
749.
446

Mechanisms for Fair Allocation Problems

Hurwicz, L. (1975). On the existence of allocation systems whose manipulative nash equilibria are pareto optimal. Unpublished paper, presented at the third World Congress
of the Economic Sosciety, Toronto.
Jain, K., & Vazirani, V. (2001). Applications of approximation algorithms to cooperative
games. In Proc. of STOC‚Äô01, pp. 364‚Äì372.
Jalali, A., Padovani, R., & Pankaj, R. (2000). Data throughput of cdma-hdr a high efficiencyhigh data rate personal communication wireless system. In Proc. of IEEE VTC‚Äô00,
Vol. 3, pp. 1854‚Äì1858.
Kalai, E., & Samet, D. (1983). On weighted Shapley values. Discussion papers 602, Northwestern University, Center for Mathematical Studies in Economics and Management
Science.
Klijn, F. (2000). An algorithm for envy-free allocations in an economy with indivisible
objects and money. Social Choice and Welfare, 17 (2), 201‚Äì215.
Krysta, P., & Ventre, C. (2010). Combinatorial auctions with verification are tractable. In
Proc. of ESA‚Äô10, pp. 39‚Äì50.
Liben-Nowell, D., Sharp, A., Wexler, T., & Woods, K. (2012). Computing Shapley value in
supermodular coalitional games. In Proc. of COCOON‚Äô12, pp. 568‚Äì579.
Lindner, C. (2010). A market-affected sealed-bid auction protocol. In Proc. of SETN‚Äô10,
pp. 193‚Äì202.
Lipton, R. J., Markakis, E., Mossel, E., & Saberi, A. (2004). On approximately fair allocations of indivisible goods. In Proc. of EC‚Äô04, pp. 125‚Äì131.
Maniquet, F. (2003). A characterization of the Shapley value in queueing problems. Journal
of Economic Theory, 109 (1), 90‚Äì103.
Maskin, E. (1987). On the Fair Allocation of Indivisible Goods, pp. 341‚Äì349. MacMillan.
Meertens, M., Potters, J., & Reijnierse, H. (2002). Envy-free and pareto efficient allocations
in economies with indivisible goods and money. Mathematical Social Sciences, 44 (3),
223‚Äì233.
Militano, L., Iera, A., & Scarcello, F. (2013). A fair cooperative content-sharing service.
Computer Networks, 57 (9), 1955‚Äì1973.
Mishra, D., & Rangarajan, B. (2007). Cost sharing in a job scheduling problem. Social
Choice and Welfare, 29 (3), 369‚Äì382.
Moulin, H. (1992). An application of the Shapley value to fair division with money. Econometrica, 60 (6), 1331‚Äì49.
Moulin, H. (1999). Incremental cost sharing: Characterization by coalition strategyproofness. Social Choice and Welfare, 16 (2), 279‚Äì320.
Moulin, H. (2003). Fair Division and Collective Welfare. MIT Press.
Moulin, H., & Shenker, S. (2001). Strategyproof sharing of submodular costs: budget balance
versus effciency. Economic Theory, 18 (3), 511‚Äì533.
Nagamochi, H., Zeng, D.-Z., Kabutoya, N., & Ibaraki, T. (1997). Complexity of the minimum base game on matroids. Mathematics of Operations Research, 22, 146‚Äì164.
447

Greco & Scarcello

Nisan, N., & Ronen, A. (2001). Algorithmic mechanism design. Games and Economic
Behavior, 35, 166‚Äì196.
Nisan, N., Roughgarden, T., Tardos, EÃÅ., & Vazirani, V. V. (2007). Algorithmic Game
Theory. Cambridge University Press, Cambridge, UK.
Ohseto, S. (2004). Implementing egalitarian-equivalent allocation of indivisible goods on
restricted domains. Economic Theory, 23, 659‚Äì670 (2004).
Osborne, M. J., & Rubinstein, A. (1994). A Course in Game Theory. The MIT Press,
Cambridge, MA, USA.
Papadimitriou, C. H. (1993). Computational Complexity. Addison-Wesley.
Pathak, A.P., S. T. (2013). Comparing mechanisms by their vulnerability to manipulation.
The American Economic Review, 103 (27), 80‚Äì106.
Penna, P., & Ventre, C. (2012a). Collusion-resistant mechanisms with verification yielding
optimal solutions. ACM Transaction on Computation Theory, 4 (2), 1‚Äì17.
Penna, P., & Ventre, C. (2012b). Optimal collusion-resistant mechanisms with verification. Games and Economic Behavior, in press, electronically available with doi
10.1016/j.geb.2012.09.002.
Porter, R., Shoham, Y., & Tennenholtz, M. (2004). Fair imposition. Journal of Economic
Theory, 118 (2), 209‚Äì228.
Potthoff, R. F. (2002). Use of linear programming to find an envy-free solution closest
to the bramskilgour gap solution for the housemates problem. Group Decision and
Negotiation, 11, 405‚Äì414.
Quinzii, M. (1984). Core and competitive equilibria with indivisibilities. International
Journal of Game Theory, 13, 41‚Äì60.
Sakai, T. (2007). Fairness and implementability in allocation of indivisible objects with
monetary compensations. Journal of Mathematical Economics, 43 (5), 549‚Äì563.
Sandholm, T. (1998). Contract types for satisficing task allocation: I theoretical results. In
AAAI Spring Symposium: Satisficing Models.
Schrijver, A. (2003). Combinatorial Optimization: Polyhedra and Efficiency. SpringerVerlag.
Shioura, A., Sun, N., & Yang, Z. (2006). Efficient strategy proof fair allocation algorithms.
Journal of the Operations Research Society of Japan, 49 (2), 144‚Äì150.
Shoham, Y., & Leyton-Brown, K. (2009). Multiagent Systems. Cambridge University Press.
Su, F. (1999). Rental harmony: Sperners lemma in fair division. American Mathematical
Monthly, 106, 930942.
Svensson, L.-G. (1983). Large indivisibles: An analysis with respect to price equilibrium
and fairness. Econometrica, 51 (4), pp. 939‚Äì954.
Svensson, L.-G. (2009). Coalitional strategy-proofness and fairness. Economic Theory, 40,
227‚Äì245.

448

Mechanisms for Fair Allocation Problems

Tadenuma, K., & Thomson, W. (1991). No-envy and consistency in economies with indivisible goods. Econometrica, 59 (6), 1755‚Äì67.
Tadenuma, K., & Thomson, W. (1993). The fair allocation of an indivisible good when
monetary compensations are possible. Mathematical Social Sciences, 25 (2), 117‚Äì132.
Tadenuma, K., & Thomson, W. (1995). Games of fair division. Games and Economic
Behavior, 9 (2), 191204.
Thomson, W. (2011). Fair allocation rules. In Kenneth J. Arrow, A. S., & Suzumura, K.
(Eds.), Handbook of Social Choice and Welfare, Vol. 2, pp. 393‚Äì506. Elsevier.
Valiant, L. G. (1979a). The complexity of computing the permanent. Theoretical Computer
Science, 8 (2), 189‚Äì201.
Valiant, L. G. (1979b). The complexity of enumeration and reliability problems. SIAM
Journal on Computing, 8 (3), 410‚Äì421.
Vickery, W. (1961). Counterspeculation, auctions and competitive sealed tenders. Journal
of Finance, 8‚Äì37.
Willson, S. J. (2003). Money-egalitarian-equivalent and gain-maximin allocations of indivisible items with monetary compensation. Social Choice and Welfare, 20, 247‚Äì259.
Yang, Z. (2001). An intersection theorem on an unbounded set and its application to the fair
allocation problem. Journal of Optimization Theory and Applications, 110, 429‚Äì443.
Yengin, D. (2012). Egalitarian-equivalent groves mechanisms in the allocation of heterogenous objects. Social Choice and Welfare, 38 (1), 137‚Äì160.
Young, H. P. (1985). Monotonic solutions of cooperative games. International Journal of
Game Theory, 14, 65‚Äì72.
Young, H. P. (1994). Equity in Theory and Practice. Princeton University Press.

449

Journal of Artificial Intelligence Research 49 (2014) 241-267

Submitted 08/13; published 02/14

An Empirical Evaluation of Ranking Measures
With Respect to Robustness to Noise
Daniel Berrar

berrar.d.aa@m.titech.ac.jp

Interdisciplinary Graduate School of Science and Engineering
Tokyo Institute of Technology
4259 Nagatsuta, Midori-ku, Yokohama 226-8502, Japan

Abstract
Ranking measures play an important role in model evaluation and selection. Using
both synthetic and real-world data sets, we investigate how different types and levels of
noise affect the area under the ROC curve (AUC), the area under the ROC convex hull, the
scored AUC, the Kolmogorov-Smirnov statistic, and the H-measure. In our experiments,
the AUC was, overall, the most robust among these measures, thereby reinvigorating it
as a reliable metric despite its well-known deficiencies. This paper also introduces a novel
ranking measure, which is remarkably robust to noise yet conceptually simple.

1. Introduction
Various metrics exist to evaluate the performance of a predictive model, but it is often not
so clear which one we should actually choose for a concrete problem at hand (Hand, 2006;
Prati, Batista, & Monard, 2011; HernaÃÅndez-Orallo, Flach, & Ferri, 2012; Bradley, 2013;
Parker, 2013). It is also known that different metrics quantify different aspects of a model
(Caruana & Niculescu-Mizil, 2004; Ferri, HernaÃÅndez-Orallo, & Modroiu, 2009). In practice,
the fair and objective comparison of predictive models is therefore not trivial, particularly
when the data are affected by noise. Here, we consider the problem of binary classification
and ranking problems, which are pervasive in numerous applications (Prati et al., 2011),
ranging from web-based recommender systems and search engines to biomedical classifiers.
The goal of this study is to investigate how robust various ranking measures are to
different types and different levels of noise. Particularly, we are interested in the robustness
of the widely used AUC and whether recently proposed alternatives, such as the H-measure
(Hand, 2009), are indeed preferable. In addition, we present a novel performance measure,
called the truncated average Kolmogorov-Smirnov statistic (taKS). This measure is derived
from the distance between the true positive rate (TPR) and false positive rate (FPR) curves,
similarly to the ‚Äúclassic‚Äù Kolmogorov-Smirnov statistic (KS).
Surprisingly few experimental studies focused on the comparison of performance measures for predictive models. Of course, different measures may quantify different aspects of
a model, and it may make little sense to compare them across different classes. But it does
make sense to compare metrics within the same class, for example, ranking measures and
their robustness to noise. To our knowledge, the most comprehensive study to date was
carried out by Ferri et al. (2009) who investigated the relations between various performance
measures and observed that these measures essentially measure quite different aspects; an
observation also made by Caruana and Niculescu-Mizil (2004). A recent comparative study
c
2014
AI Access Foundation. All rights reserved.

Berrar

by Parker (2013) focused on ranking measures; this study recommends the H-measure and
advises against the AUC. A more theoretical approach to the comparison of performance
measures can be found in the work of Flach (2003). HernaÃÅndez-Orallo et al. (2012) provide
a comprehensive view of how the different measures are related to each other.
In summary, our main insights are the following. First, among the conventional measures, the AUC was arguably the most robust across a wide range of noise levels and types.
This result confirms that in fact, the AUC can be a reliable measure, although it has been
criticized as incoherent and potentially misleading (Hilden, 1991; Lobo, JimeÃÅnez-Valverde,
& Real, 2008; Hand, 2009; Hand & Anagnostopoulos, 2013; Parker, 2013). Therefore, our
experiments lend further empirical support to the AUC as a robust measure for model
evaluation and selection. Second, overall, the magnitude of the differences in robustness
between the commonly used measures were not that dramatic for relatively low noise levels.
Third, the proposed new measure, taKS, was also remarkably robust to noise, and it is
conceptually simple with a neat geometrical interpretation.
This paper is organized is follows. First, we briefly review the ranking measures that
we included in our comparative study. Then, we give the rationale for the new measure,
beginning with an introductory example and then describing the formal details. In Section 4,
we report the results of the comparative study involving both synthetic and real-world data
sets. Section 5 concludes the paper with a discussion.

2. A Brief Review of the Investigated Ranking Measures
Let a data set contain k instances (or cases) xi , i = 1..k. With each case, exactly one class
y is associated, i.e., (y, xi ), y ‚àà {0, 1}, where 1 denotes the positive class and 0 denotes
the negative class. Commonly, predictive models generate a numeric score s for each xi ,
which quantifies the degree of class membership of that case to a class, for example, a
class posterior probability. If the data set contains only positive and negative examples,
then a predictive model can either be used as a ranker or as a classifier. If the scores are
expressed on an ordinal scale, the model can use the scores to rank the cases from the most
to the least likely positive. By setting a threshold t on the ranking score, s(x), such that
C{s(x) ‚â• t} = 1, we can turn the ranker into a (crisp) classifier.
2.1 Area Under the ROC Curve (AUC)
Arguably the most commonly used ranking measure is the AUC. It has been used for model
selection in various applications, ranging from data mining competitions to biomedical tests
(Berrar & Flach, 2012). The AUC is the area under the ROC curve, which depicts the tradeoffs between the false positive rate (or 1 minus specificity, depicted on the x-axis) and the
true positive rate (or sensitivity, depicted on the y-axis). These trade-offs correspond to
all possible binary classifications that any dichotomization of the continuous outputs of
a model would allow. The AUC is equivalent to a Wilcoxon rank-sum statistic (Bamber,
1975; Hanley & McNeil, 1983) and can be interpreted as a conditional probability: given any
randomly selected positive and negative case, the AUC is the probability that the classifier
assigns a higher score to the positive case (i.e., ranks it before the negative case).
Let P denote the probability that a randomly selected actual positive case, x+ , has a
higher ranking score, s+ , than a randomly selected negative case, x‚àí , i.e., s+ > s‚àí . Here,
242

An Empirical Evaluation of Ranking Measures With Respect to Robustness to Noise

a higher ranking score means that x+ is ranked before x‚àí , and f (s+ ) and g(s‚àí ) are the
distribution functions of these scores (Hilden, 1991). Following Hilden‚Äôs notation, the AUC
can then be written as
Z

ZZ
AUC = Pr{s+ > s‚àí |x+ and x‚àí } =

f (s+ )ds+ g(s‚àí )ds‚àí =

F (s‚àí ) dG(s‚àí ).

(1)

s+ >s‚àí

The AUC can be calculated in different ways from an empirical ROC curve; for a practical guide, see the tutorial by Fawcett (2004). ROC analysis is now an integral part of the
evaluation of machine learning algorithms (Bradley, 1997). Whereas ROC curves are widely
(and rightly so) considered useful, both theoretical and practical shortcomings of the AUC
have been pointed out (Hilden, 1991; Adams & Hand, 1999; Bengio, MarieÃÅthoz, & Keller,
2005; Webb & Ting, 2005; Lobo et al., 2008; Hand, 2009; Hanczar, Hua, Sima, Weinstein,
Bittner, & Dougherty, 2010; Hand & Anagnostopoulos, 2013; Parker, 2013). A particular
problem of the AUC is that it can be incoherent, in the sense that it assumes different
cost distributions for different classifiers (Hand, 2009). One of the first criticisms, with an
insightful example showing that area comparisons can be misleading, can be found in the
work of Hilden (1991). Hand (2009), too, considers the AUC fundamentally incoherent. Recently, however, Hand and Anagnostopoulos (2013) showed that the AUC can be a coherent
measure, but only under certain assumptions that may not hold for real applications.
2.2 Scored Area Under the ROC Curve (sAUC)
The AUC measures only how well positive and negative cases are ranked relative to each
other, but it does not consider the actual ranking scores. This means that the margin
between scores is irrelevant. Intuitively, however, it seems reasonable to take the scores
somehow into account. Various alternatives of the AUC have been suggested that do just
that; an example is the scored AUC (sAUC) (Wu & Flach, 2005; Wu, Flach, & Ferri, 2007).
Let n+ denote the total number of positive cases and n‚àí denote the total number of
negative cases. Let {s1+ , ...sn+ } denote the predicted ranking scores for the positive cases
and {s1‚àí , ...sn‚àí } denote the scores for the negative cases, where s1+ ‚â• ... ‚â• sn+ and
s1‚àí ‚â§ ... ‚â§ sn‚àí . Both si+ and sj‚àí are assumed to be normalized between [0, 1], with
i = 1, ...n+ and j = 1, ...n‚àí . Let I(¬∑) be an indicator function with I(true) = 1 and
I(f alse) = 0. The sAUC is then defined as
n+ n‚àí
1 XX
sAUC =
(si+ ‚àí sj‚àí )I(si+ > sj‚àí ).
n+ n‚àí

(2)

i=1 j=1

The indicator function I(si+ > sj‚àí ) assesses the ranking ability, while the factor (si+ ‚àí
sj‚àí ) evaluates the score differences. Without this factor, Equation 2 is equivalent to the
AUC. There exist further variants such as the soft-AUC (Calders & Jaroszewicz, 2007) and
the probabilistic AUC (pAUC) (Ferri, Flach, HernaÃÅndez-Orallo, & Senad, 2005); however,
according to Vanderlooy and HuÃàllermeier (2008), none of the proposed alternatives to the
AUC are effective for model evaluation. We therefore do not consider any further variants
in our comparative analysis.
243

Berrar

2.3 Area Under the ROC Convex Hull (AUCH)
The ROC convex hull is defined as the convex hull that encloses the operating points of the
ROC curve (Provost & Fawcett, 2001; Flach, 2010). Note, that a curve is called convex if
any straight line interpolating between two points on the curve is never above the curve.1
The ROC convex hull results from the interpolation between the following k points, which
are ordered based on increasing values of their abscissa: the origin (xi , yi ) = (0, 0), the
minimum set of points spanning the concavities, and the point (1,1). The area under
the ROC convex hull, AUCH, is always at least as large as the AUC. The AUCH can be
calculated as shown in Equation 3.

AUCH =

k‚àí1
X

yi (xi+1 ‚àí xi ) + 0.5(yi+1 ‚àí yi )(xi+1 ‚àí xi ).

(3)

i=1

2.4 H-measure
In order to address the incoherence of the AUC, Hand (2009) proposed the H-measure. Let
t denote the classification threshold, and let TPR(t) and FPR(t) denote the corresponding
true positive and false positive rate, respectively. The overall misclassification loss is then
c+ œÄ+ (1‚àíTPR(t))+c‚àí œÄ‚àí (FPR(t)), where c+ is the cost associated with the misclassification
of a positive case and c‚àí is the cost associated with the misclassification of a negative case,
and œÄ+ and œÄ‚àí are the prior probabilities of positive and negative cases, respectively.
R

H‚àímeasure = 1 ‚àí
œÄ+

œÄ
R‚àí

Q(T (c), c)u(c)dc
,
R1
cu(c)dc + œÄ‚àí (1 ‚àí c)u(c)dc

(4)

œÄ‚àí

0

with c = c+ /(c+ + c‚àí ); T (c) = arg mint {cœÄ+ (1 ‚àí TPR(t)) + (1 ‚àí c)œÄ‚àí FPR(t)}; Q(t, c) =
R1
{cœÄ+ (1 ‚àí TPR(t)) + (1 ‚àí c)œÄ‚àí FPR(t)}(c+ + c‚àí ); u(c) = c(1 ‚àí c)/ c(1 ‚àí c)dc.
0

The H-measure is a measure of the overall misclassification loss and a function of both
the class-specific misclassification costs and the prior probabilities of positive and negative
cases (Hand, 2009). By contrast, the AUC measures the ranking performance over the entire
space of classification thresholds and is independent of costs and class priors. Flach et al.
(2011) showed that, with two small variations, the H-measure is a linear transformation of
the area under the cost curve, which was proposed by Drummond and Holte (2006).
2.5 Kolmogorov-Smirnov Statistic (KS)
The Kolmogorov-Smirnov goodness-of-fit test for a single sample is a test of ordinal data
and assesses whether the distribution of n scores follows a specific theoretical or empirical
distribution (Sheskin, 2007). The test statistic, KS, is defined as the maximum value of
the absolute difference between two cumulative distributions. When we assess the ranking
1. In mathematical terms, this curve is considered ‚Äúconcave‚Äù, whereas the standard machine learning
terminology uses ‚Äúconvex‚Äù.

244

An Empirical Evaluation of Ranking Measures With Respect to Robustness to Noise

Figure 1: (a) Classification result on a toy data set of 4 cases, (a) optimal model with 3
possible thresholds, (b) another model with 3 possible thresholds; and (c) yet another model,
with 5 possible thresholds (Real: real class label, 1 = positive class; Score: predicted score
for the positive class).

ability of a classifier, these distributions are given by the true positive rates and false
positive rates for all classification thresholds. The KS statistics has a simple geometrical
interpretation as the maximum distance between the TPR and FPR curves,

KS = max{|TPRi ‚àí FPRi |},

(5)

where TPRi and FPRi denote the true positive rate and false positive rate for the ith
threshold, respectively (see Figure 2 for an example).

3. Truncated Average Kolmogorov-Smirnov Statistic (taKS)
In this section, we propose a new ranking measure, called the truncated average KolmogorovSmirnov statistic (taKS).
3.1 Introductory Example
Let us consider the prediction results on an arbitrary test set comprising k cases, which
belong to either the positive or the negative class. The optimal model will assign a score
of 1 to each positive and a score of 0 to each negative case. Consider now another model
that assigns the scores s1 , s2 ...sk to the k test cases, where s1 ‚â• s2 ‚â• ... ‚â• sk , which also
happen to lead to a perfect ranking. Figure 1 illustrates this idea using a toy data set with
k = 4 cases. All models in this example have indeed the same ranking performance, and
consequently, ranking measures like the AUC do not distinguish between them. There is
nothing wrong with that ‚Äì all what matters is the relative ordering of the cases, irrespective
of the actual ranking scores. Note, that the optimal model always allows exactly three
possible thresholds: (1) one threshold separating the positive and negative cases (t2 in
Figure 1a); (2) one ‚Äútop‚Äù threshold (i.e., TPR = 0 and FPR = 0; t1 in Figure 1a); and (3)
one ‚Äúbottom‚Äù threshold (i.e., TPR = 1 and FPR = 1; t3 in Figure 1a).
Assume now that these three models enter a data mining competition. Let us further
assume that we, the judges, can only see the final predictions as shown in Figure 1. We
do not have any other information about the models such as the calibration of the scores,
except that higher scores reflect more relative confidence that a case belongs to the positive
class. No other assumptions shall be allowed for now.
245

Berrar

We can probably agree that model (a) is the winner, but which one should be the runnerup, (b) or (c)? To answer this question, we could consider the ranking scores, for example,
by calculating the sum of squared errors (SSE) or a related measure such as the Brier score.
This approach would tell us that model (c) with SSE = 0.18 is preferable to model (b) with
SSE = 0.50. However, this approach makes the tacit assumption that both models, (b) and
(c), produce comparable scores in the same range, maybe posterior probabilities in [0, 1].
This is of course often a reasonable assumption, but it does not necessarily have to be the
case. In addition, any such assumption was actually not allowed.
What if we allowed assumptions about the calibration? Let us speculate that ‚Äì by
design ‚Äì model (b) could not have produced probabilities larger than 0.7 or smaller than
0.4. Based on minimum message length theory, it may indeed make sense to prevent a
model from making overly confident predictions, for example, by limiting the estimates to
a specific range only (Korb, Hope, & Hughes, 2001).2 Under this particular assumption, we
may look at the performance of model (b) in a new light. In fact, the difference between
the scores in (a) and (b) then reduces merely to a different scaling. Could model (b) be
such a ‚Äúcareful‚Äù model? Granted, the assumption that all scores are comparable (e.g., from
[0, 1]) is more plausible. But the point is that if we take the actual scores into account,
then we have to make some assumptions about the models‚Äô calibrations. Furthermore, the
scores of model (c) have a higher level of granularity than those of model (b), so we might
say that model (c) is more refined than the ‚Äúcoarser‚Äù model (b). But does this refinement
necessarily indicate the ‚Äúbetter‚Äù model?
Let us now look at another difference between the models: the number of possible
thresholds. This number depends on the refinement of the scores, but not on the actual
values. The idea is to combine this number and the ranking performance.
3.2 Formal Details
Visually, we can represent the class discrimination by plotting both the true positive and
the false positive rates as a function of the threshold in the same diagram. By interpolating
the points, we obtain the corresponding TPR and FPR curves (henceforth referred to as
TPR-FPR plot, also known as Kolmogorov-Smirnov chart).
Figure 2 illustrates the TPR-FPR curves for a toy data set. Each test case above the
threshold is classified as a positive case. For instance, the threshold t6 leads to 4 true
positive and 1 false positive classifications because 4 positive cases and 1 negative case are
located above the threshold; the corresponding rates are TPR = 54 and FPR = 15 (Figure 2b).
Class discrimination could now be quantified in terms of the area between the curves (ABC).
Definition 1. Area between the curves
The area between the curves (ABC) is the area between the TPR curve and the FPR curve,
which result from interpolating the true positive and false positive rates based on the n
2. The reason is that overly confident predictions that turn out to be incorrect lead to a dramatic
information-theoretic penalty; for example, the penalty for an incorrect prediction with confidence 1 is
‚àí‚àû. Korb et al. (2001), for example, allowed the range [min, max] = [0.5(n + 1)‚àí1 , (n + 0.5)(n + 1)‚àí1 ],
where n is the number of test cases.

246

An Empirical Evaluation of Ranking Measures With Respect to Robustness to Noise

Figure 2: (a) An example of a binary classification task involving ten test cases. (b) The
ranking scores allow 11 classification thresholds, each corresponding to one TPR and one
FPR point. Interpolation through these points gives the TPR and FPR curves. The distance
between the curves is maximal for t6 .

possible thresholds,
Zn
ABC = |

Zn
TPR(x)dx ‚àí

1

FPR(x)dx|.

(6)

1

Note, that the absolute value is necessary if we accept that a model could also perform
worse than random guessing, which means that the FPR curve could be above the TPR
curve, thereby leading to a negative value of ABC. In the remainder of the paper, we will
work with the signed ABC, though. Applying the trapezoidal rule, we obtain

ABC =

n‚àí1
X
i=1
n‚àí1
X

1
TPRi (xi+1 ‚àí xi ) + (TPRi+1 ‚àí TPRi )(xi+1 ‚àí xi )
2

1
FPRi (xi+1 ‚àí xi ) + (FPRi+1 ‚àí FPRi )(xi+1 ‚àí xi )
2
i=1


n‚àí1
X
1
1
=
(xi+1 ‚àí xi ) TPRi + (TPRi+1 ‚àí TPRi ) ‚àí FPRi ‚àí (FPRi+1 ‚àí FPRi )
2
2

‚àí

=

i=1
n‚àí1
X

1
2

(xi+1 ‚àí xi ) [(TPRi ‚àí FPRi ) + (TPRi+1 ‚àí FPRi+1 )],

i=1

247

Berrar

where TPRi and FPRi denote the true positive rate and false positive rate for the ith
threshold, respectively. We now require that the thresholds on the abscissa be equidistant in
[0, 1], where the first threshold is linearly mapped to 0 and the last threshold, n, is mapped
1
. Denoting j = i + 1, we
to 1 (this condition will be relaxed later). Then (xi+1 ‚àí xi ) = n‚àí1
obtain

ABC =

n‚àí1

n‚àí1

i=1

j=2

1 X
1 X
(TPRi ‚àí FPRi ) +
(TPRj ‚àí FPRj ).
2n ‚àí 2
2n ‚àí 2

Note, that the true positive and false positive rates are always zero for the first threshold,
TPR1 = FPR1 = 0, so we obtain
n‚àí1

ABC =

1 X
(TPRi ‚àí FPRi )
n‚àí1

(7)

i=2

Consider the optimal model that assigns the score 1 to all cases of class 1 and the score
0 to all cases of class 0. Here, n = 3, and ABC = 12 ¬∑ (1 ‚àí 0) = 12 . It is now possible that
another, suboptimal model has a larger ABC. For example, assume that the ranking score
of the i = 2 ranked positive case is identical to the score of the i = 1 ranked positive case.
Figure 3c shows such an example of a suboptimal model with ABC > 0.5. Thus, for model
evaluation and selection, the ABC should not be used directly.
1
However, we can use the ABC to derive a performance measure. Consider the factor n‚àí1
in Equation 7. By replacing ‚àí1 in the denominator with ‚àí2, we obtain the average of the
distances between the points spanning the TPR and FPR curves, excluding the start-point
and end-point. If we consider now this new measure, then the value for the optimal model
is always 1, and no other model can score higher. This is immediately obvious because the
thresholds on the abscissa are scaled from [0, 1], and the FPR and TPR on the ordinate
range from 0 to 1. Any area within these boundaries cannot be larger than 1. Thus, the
distance between any pair (TPRi ,FPRi ) cannot be larger than 1, and therefore the average
of the distances cannot be larger than 1.
Definition 2. Truncated average Kolmogorov-Smirnov statistic (taKS)
Let a model produce ranking scores sa ‚àà R, a = 1..k for k test cases belonging to either the
positive or the negative class, and sa‚àí1 ‚â• sa ‚â• sa+1 and ‚àÉsa , sb : a 6= b ‚àß sa 6= sb . Let n > 2
denote the number of possible thresholds ti , ti‚àí1 < ti < ti+1 , that these scores allow. Let
TPRi and FPRi denote the true positive and false positive rates, respectively, which result
from a particular threshold ti . The taKS is then defined as the average of the distances
between the true positive rates and the false positive rates, excluding the points (0,0) and
(1,1),
n‚àí1

1 X
taKS =
(TPRi ‚àí FPRi ).
n‚àí2
i=2

248

(8)

An Empirical Evaluation of Ranking Measures With Respect to Robustness to Noise

Note, that we can now relax the condition that the thresholds on the x-axis should be
equidistant in [0, 1]. All what matters for the taKS are the distances between the points
spanning the TPR and FPR curves; the scaling of the x-axis is irrelevant. A pseudocode
for deriving taKS is given in Appendix A (algorithm 1).
3.3 Illustration of taKS
Figure 3 shows the TPR-FPR plots with the resulting taKS for nine different prediction
results. The scores of the models in Figure 3a-b are different, but the relative order of the
cases is the same, so the models have the same ranking performance. Here, the taKS is 1.0
for the optimal model (Figure 3a), 0.556 for the model allowing 11 thresholds (Figure 3b),
and 0.600 for the model allowing 10 thresholds. These examples also illustrate that using
the ABC can be misleading: we would erronously prefer the model in Figure 3c with ABC
= 0.533 over the optimal model with ABC = 0.500.
The scores in (b) and (c) are different only for case #1. Model (b) predicted 0.95
whereas model (c) predicted 0.80. Provided that both models are equally calibrated, one
could argue that (b) is better (has a smaller SSE), and that taKS is therefore misleading.3
This reasoning is plausible, but the opposite could also have happened. Suppose that the
model in (c) produces 0.95 for cases #1 and #2. The value of taKS remains the same
(i.e., 0.60), but now it points us to the better model. Either scenario is equally likely a
priori, so correct and incorrect decisions should balance, on average, for this example. This
means that about half the time, taKS points us to the better model. Note, that the AUC is
indifferent in these examples, making no difference between (b) and (c) in either scenario.
If we used the AUC, then we could only guess which one is better, (b) or (c); thus, we would
be correct about half the time, too. Consequently, compared with the AUC (or any other
ranking measure, for that matter), taKS does not provide any advantage in this example ‚Äì
but it does not provide any disadvantage, either.
Figure 3d-f shows three models that make some ranking errors. These models have the
same ranking performance, but they score a different taKS. The model in Figure 3e scores
a larger taKS than the model in Figure 3f. The model in Figure 3e assigned the same score
(0.80) to the cases #1 and #2; the model in Figure 3f assigned the same score (0.60) to the
cases #4 and #5. While both models allow for the same number of thresholds (i.e., 10),
the taKS identifies the model in Figure 3e as the preferable one because of its larger ABC.
Figure 3g-i show three particular models. The model in Figure 3g is the perfect ‚Äúanti‚Äùmodel that predicts like the mirror image of the optimal model. Consequently, the resulting
taKS is ‚àí1. Another particular model is shown in Figure 3h. This model assigns the same
score to all cases; thus, it allows only two possible thresholds, so that the TPR and FPR
curves fall onto the same line. As the ABC is then not defined, the taKS is not defined,
either. Figure 3i shows the results of a random prediction, leading to a taKS and ABC of
zero. Note, that the AUC is defined for Figure 3h; the AUC is here 0.5 and the same as the
AUC in Figure 3i.

3. Thanks to an anonymous reviewer for pointing this out!

249

Berrar

Figure 3: TPR (red) and FPR (black) curves and taKS for nine classification results (cf.
inset table). (a) Best possible predictions with 3 possible thresholds; (b) perfect ranking
with 11 possible thresholds; (c) perfect ranking with 10 possible thresholds; (d) prediction
with ranking errors and 11 possible thresholds; (e) prediction with ranking errors and 10
possible thresholds; (f) prediction with ranking errors and 10 possible thresholds; (g) worst
possible prediction with 3 possible thresholds; (h) all cases have the same ranking score,
and neither ABC nor the taKS is defined; (i) random prediction with 5 possible thresholds.

250

An Empirical Evaluation of Ranking Measures With Respect to Robustness to Noise

3.4 Some Further Notes on taKS
As the name implies, the taKS is closely related to the Kolmogorov-Smirnov (KS) statistic,
i.e., the maximum distance between the TPR and FPR curves (cf. Equation 5). By contrast,
the taKS is the average distance between these curves, excluding start- and endpoint at
(0,0) and (1,1), respectively. In the introductory example (Figure 2), we have KS = 35 (for
threshold t6 ) and taKS = 13 .
For the optimal model, which scores 1 for all positive cases and 0 for all negative cases,
taKS = 1. Also, each model that assigns the same score s+ to all positive and the same
score s‚àí to all negative cases has taKS = 1. Thus, the taKS does not distinguish between
the optimal model and another model that, say, assigns 0.7 to all positive cases and 0.4
to all negative cases. For the worst-possible model (i.e., one that assigns 1 to all cases of
class 0 and 0 to all cases of class 1), taKS = ‚àí1. The expected value of taKS for a random
model is 0. If a model assigns the same score to all cases, then taKS is not defined because
the number of possible thresholds is then n = 2, which would lead to a division by zero in
Equation 8. Graphically, the TPR and FPR curves are then straight lines through (0,0)
and (1,1). Note, that conventional ranking measures such as the AUC are defined in this
case.
Like the AUC, taKS is an aggregate measure of performance for a final classification
result. An advantage of ROC plots is that they can visualize the performance of more than
just one classifier in the same diagram, in contrast to the TPR-FPR plots as used in this
paper. A further limitation of taKS is the following. If two models are equally calibrated,
then in some scenarios taKS can be larger for a model with a larger SSE, thereby leading
us to the potentially inferior model (for an examle, cf. Figure 3b-c). On average, however,
such scenarios should balance against scenarios where the opposite is the case.

4. Robustness Analysis
We considered both synthetic and real-world data sets to study the robustness of the ranking
measures to various types and levels of noise. We adopted an approach similiar to the one
described by Ferri et al. (2009). The main idea is the same: we consider two models, C1
and C2 , where C1 is the truly better model. Then, we progressively added noise. The
question is whether the performance measures can still identify C1 as the better model. A
measure Xi can be considered more robust than another measure Xj if Xi is less affected
by the increasing levels of noise. Below, we describe the different types of noise that we
investigated in our experiments. All experiments are described in pseudocode in Appendix
A and were carried out in R2.10.1 (R Development Core Team, 2009).
4.1 Synthetic Data Sets
We considered the predictions of two classifiers, C1 and C2 , on a hypothetical test set
comprising 100 cases, as described by Ferri et al. (2009). The number of test cases has
arguably little influence on the experiments, provided that it is not too small. We then
generated a vector of 100 real numbers by randomly sampling from a uniform distribution
[0, 1], which represent the ranking scores for the positive class. These membership scores
can be interpreted as class posterior probabilities. The positive class label was then assigned
251

Berrar

to all numbers ‚â• 0.5, and the negative class label was assigned to the remaining numbers.
Next, we randomly selected 10 scores and replaced them by a real number, which was again
randomly sampled from [0, 1]. The resulting numbers represented the predictions of C1 .
For a threshold of 0.5, the expected accuracy of C1 is 95% because we expect that half of
the new scores (i.e., 5 of 10) are wrong.
The predictions of C2 were the same as those of C1 , except that we selected 10 further
predictions at random and replaced them by a real number, again randomly sampled from
[0, 1]. Thus, without noise, C2 was expected to perform worse than C1 , with expected
accuracy of only 90%. The difference between the two classifiers, however, was expected to
become blurred for increasing levels of noise.
We considered three types of noise: misclassification noise, probability noise, and class
proportion noise (described below). For each level of noise, we generated each model, C1 and
C2 , n = 10000 times, each time evaluating its performance based on the different ranking
measures. For each measure, we then counted how many times it erroneously indicated
that C2 was better than C1 . Let X(¬∑) denote the value of a performance measure X for a
classifier. The error rate of a measure X is then given by

Ô£±
1
if X(C2 ) > X(C1 )
Ô£¥
Ô£¥
Ô£¥
Ô£¥
n
Ô£≤
1X
0.5 if X(C2 ) = X(C1 )
(X) =
Œ¥(X), with Œ¥(X) =
Ô£¥
n
Ô£¥
i=1
Ô£¥
Ô£¥
Ô£≥
0
if X(C2 ) < X(C1 )

(9)

By plotting (X) for the measures as a function of the noise level, we can compare their
resilience to noise. For example, if a measure Xi is more robust than a measure Xj , then
the error rate of Xi should be consistently lower; hence, the curve for Xi should be below
the curve for Xj .
4.1.1 Misclassification Noise
First, we considered noise that affects the class labels. This experiment evaluates how
sensitive the measures are with respect to mislabelings. We investigated noise levels ranging
from 0% (i.e., no class label was altered) to 100% (i.e., each class label was altered and
determined by the flip of a coin). For a noise level of 0%, we expect that all error scores
are 0 because C1 is clearly better than C2 . By contrast, if all class labels are random, then
we expect no difference between the models, so the error score should be around 0.5.
For a class label noise between 0% and about 70% (Figure 4), the error rates of the
H-measure and the Kolmogorov-Smirnov statistic are slightly below those of the other
measures. The AUCH is slightly more robust than the AUC and the taKS but less robust
than the H-measure and the Kolmogorov-Smirnov statistic. sAUC is the least robust in this
experiment. Overall, however, we do not see dramatic differences between the measures.
As expected, all measures are around 0.5 for a noise level of 100%. The error rates of the
AUC and the taKS are virtually identical in this experiment.
252

An Empirical Evaluation of Ranking Measures With Respect to Robustness to Noise

Figure 4: Synthetic data, experiment #1. Robustness to misclassification noise.

4.1.2 Probability Noise
This noise affects the class membership scores. This experiment evaluates how sensitive
the measures are when the posterior class probabilities are not well estimated. The noise
was randomly sampled from a uniform distribution [‚àíx, x], where x ranged from 0 (i.e., no
noise) to 0.5 (i.e., 100% noise) in a stepsize of 0.005. The noise was added to all ranking
scores.
When the noise affects the class posterior probabilities (Figure 5), the sAUC performs
the worst. The Kolmogorov-Smirnov statistic is the next least robust, followed by the Hmeasure. The AUC and the taKS are the most robust in this experiment; their error rates
are again almost identical. The AUCH is slightly less robust than these two measures.
4.1.3 Class Proportion Noise
This noise affects the class proportions. The experiment evaluates how sensitive the measures are to changes in the class distribution drifts. We changed the class frequencies by
progressively deleting x% of the cases of the positive class. The noise x% ranged from 5%
to 95%.
When the noise affects the class frequencies (Figure 6), all measures except sAUC perform very similarly. For AUC, AUCH, H-measure, KS, and taKS, the error rates are remarkably low for noise levels up to around 80%. Thus, in contrast to sAUC, these measures
can cope quite well even when the classes are heavily imbalanced.
253

Berrar

Figure 5: Synthetic data, experiment #2. Robustness to probability noise.
4.2 Real-World Data Sets
In the experiments with the synthetic data sets, we investigated a wide range of noise levels,
including some that are arguably unrealistically high for real-world data sets. Therefore,
we limited the next experiments to a noise level that was neither too small to cause any
noticeable effect nor too large to be unrealistic. We assumed that a noise level of 10% would
meet this requirement.
In the experiments with the synthetic data sets, we observed that the class proportion
noise has very little effect on the performance measures, except for unrealistically high noise
levels. Therefore, we excluded this type of noise in the following experiments. Instead, we
considered a new type of noise that we could not study before: attribute noise, which affects
the attributes either in the training set or the entire data set.
We used naive Bayes learning to construct our base classifier. The concrete learning
algorithm was assumed to have little influence on the experimental results. We denoted the
predicted scores of this classifier as C1 . We then randomly selected 10% of these scores and
replaced each score by a random number, which was uniformly sampled from [0, 1]. The
result was C2 . Without noise, C1 is clearly better than its corrupted competitor, C2 . We
used ten benchmark data sets from the UCI repository (Bache & Lichman, 2013).
Experiment #1: Misclassification Noise Affecting the Entire Data Set
In the first experiment, we investigated the resilience to noise affecting the class labels of
the entire data set. For each data set, we selected 10% of the class labels and randomly
254

An Empirical Evaluation of Ranking Measures With Respect to Robustness to Noise

Figure 6: Synthetic data, experiment #3. Robustness to class proportion noise.

assigned either a positive or a negative label. Then, we compared the performance of C1
and C2 in 10-fold cross-validation. We repeated this experiment 1000 times and recorded
how many times C2 was declared the better model by the respective ranking measure (see
Appendix A, algorithm 2).
Experiment #2: Misclassification Noise Affecting the Training Set
In the second experiment, we investigated the resilience to noise affecting the class labels of
only the training set. For each training set, we selected 10% of the class labels and randomly
assigned either a positive or a negative label. Then, we compared the performance of C1
and C2 in 10-fold cross-validation. We repeated this experiment 1000 times and recorded
how many times C2 was declared the better model by the respective ranking measure (see
Appendix A, algorithm 3).
Experiment #3: Attribute Noise Affecting the Entire Data Set
In the third experiment, we investigated the resilience to noise affecting the attribute values in the entire data set. For each data set and each attribute, we selected 10% of the
values and randomly permuted them. Then, we compared the performance of C1 and C2 in
10-fold cross-validation. We repeated this experiment 1000 times and recorded how many
times C2 was declared the better model by the respective ranking measure (see Appendix
A, algorithm 4).

255

Berrar

Experiment #4: Attribute Noise Affecting the Training Set
In the fourth experiment, we investigated the resilience to noise affecting the attribute values in the training set only. For each training set and each attribute, we selected 10% of the
values and randomly permuted them. Then, we compared the performance of C1 and C2 in
10-fold cross-validation. We repeated this experiment 1000 times and recorded how many
times C2 was declared the better model by the respective ranking measure (see Appendix
A, algorithm 5).

Table 1 shows the error rates of the ranking measures for the real-world data sets. We
can make several interesting observations. First, for most data sets, the error rates of the
performance measures are relatively small and not drastically different from each other.
The only exception is sAUC, whose error rates are indeed remarkably high (between 64.8%
and 78.3%) for the data sets Liver and Transfusion in all four experiments. The Liver and
Transfusion data sets have only 6 and 4 attributes, respectively, and they are comparatively
more difficult to classify than the other data sets.4 For the data sets Liver and Transfusion,
the error rates are the highest in all four experiments, whereas the error rates are virtually
neglibile for the data set Credit. We speculate that if some data sets are intrinsically very
easy to classify, then the injected noise has a negligible effect on the ranking measures. If
a data set is easy to classify, then we can expect that our classifier produces scores close
to 0 and 1, with fewer scores around 0.5. Now, we created C2 by randomly selecting some
scores from C1 and re-assigning a random number from [0, 1] to those scores. This means
that we can expect that a larger number of more extreme scores (which are likely to be
correct, as the classification tasks are relatively easy) are mapped to less extreme scores.
Consequently, it is quite easy to identify C1 as the better model, regardless of whichever
measure is being used. By contrast, if a data set is intrinscically difficult to classify, then
even tiny amounts of added noise may wreak havoc. This seems to be the case for sAUC
in particular. Note, that the sAUC implicitly rewards a classifier‚Äôs ‚Äúboldness‚Äù: the sAUC
of a classifier with scores close to 0 and 1 can be larger than the sAUC of a classifier with
less extreme scores, although the latter may make fewer ranking errors; Vanderlooy and
HuÃàllermeier (2008, p.252) give an illustrative example.
Second, the error rates are, overall, higher when the noise affects the entire data sets
than the error rates for the noise that affects only the training sets. This is not unexpected
because in the latter case, a portion of the original data remains intact.
Third, overall, we observe a positive correlation between the measures, but the differences in error rates are remarkable for some data sets. For example, in experiment #2, the
error rate of the H-measure (10.9%) is more than three times the error rate of the AUC
(3.3%) for the data set Spect; on the other hand, the error rate of the AUC (1.4%) is seven
times that of the H-measure (0.2%) for the data set House. Interestingly, the H-measure
has, on average, slightly higher error rates than the AUC or taKS when the noise affects
the class labels. This is somewhat unexpected because the H-measure performed relatively
well in the corresponding experiments with the synthetic data sets (Figure 4). However, the
differences of the average error rates are relatively small and might perhaps be explained by
4. We checked this by analyzing all (uncorrupted) data sets in 100 times repeated 10-fold cross-validation.
The naive Bayes classifier achieved the lowest AUC for Liver and Transfusion.

256

An Empirical Evaluation of Ranking Measures With Respect to Robustness to Noise

Experiment #1

Experiment #2

Experiment #3

Experiment #4

Sonar
Spect
Heart
Liver
Ionosphere
House
Cylinder
Credit
Transfusion
Pima
Sonar
Spect
Heart
Liver
Ionosphere
House
Cylinder
Credit
Transfusion
Pima
Sonar
Spect
Heart
Liver
Ionosphere
House
Cylinder
Credit
Transfusion
Pima
Sonar
Spect
Heart
Liver
Ionosphere
House
Cylinder
Credit
Transfusion
Pima

H-measure
10.30
13.90
1.50
18.10
0.40
0.20
0.40
0.00
5.50
0.40
5.30
10.90
0.30
16.50
0.00
0.20
0.00
0.00
3.40
0.10
8.00
8.80
0.50
18.40
0.10
0.00
0.30
0.00
3.90
0.10
5.50
12.10
0.50
14.90
0.10
0.00
0.00
0.00
2.50
0.10

AUC
8.80
8.90
1.80
16.70
0.70
1.50
1.00
0.00
3.40
0.00
4.80
3.30
0.20
13.20
0.00
1.40
0.10
0.00
0.90
0.00
6.30
3.50
0.90
15.60
0.30
0.00
0.20
0.00
2.10
0.00
4.50
4.00
0.90
13.40
0.00
0.00
0.00
0.00
1.50
0.10

AUCH
11.70
9.60
1.70
17.50
0.80
1.20
1.20
0.00
4.60
0.10
6.70
4.50
0.30
14.00
0.00
1.30
0.10
0.00
1.30
0.00
8.10
3.40
0.70
16.80
0.30
0.00
0.30
0.00
3.00
0.00
5.80
5.20
0.70
13.70
0.00
0.00
0.10
0.00
2.00
0.10

sAUC
10.70
2.80
0.10
70.20
0.00
0.00
7.90
0.10
78.30
0.10
6.30
0.70
0.00
66.50
0.00
0.00
2.90
0.00
69.80
0.10
6.00
1.40
0.10
64.80
0.00
0.00
2.50
0.10
74.60
0.00
5.40
1.20
0.00
65.90
0.00
0.00
5.10
0.00
70.80
0.20

KS
12.05
11.20
1.60
19.30
0.50
0.10
0.80
0.00
7.30
1.00
7.15
6.95
0.70
17.10
0.10
0.20
0.10
0.00
2.90
0.30
9.40
6.90
1.00
21.00
0.10
0.00
0.70
0.00
3.80
0.20
5.85
7.10
0.80
17.20
0.20
0.00
0.30
0.00
2.80
0.10

taKS
7.00
8.80
1.80
16.80
0.50
1.00
0.80
0.00
3.20
0.00
3.40
2.80
0.20
13.20
0.00
1.10
0.00
0.00
0.70
0.00
4.30
2.90
0.90
15.60
0.30
0.00
0.00
0.00
1.70
0.00
3.20
3.40
0.90
13.50
0.00
0.00
0.00
0.00
1.50
0.10

Table 1: Error rates [%] of the ranking measures. Experiment #1: 10% of the class labels
in each data set were randomly assigned; Experiment #2: 10% of the class labels in each
training set were randomly assigned; Experiment #3: 10% of the values of each attribute
were randomly permuted in each data set; and Experiment #4: 10% of the values of each
attribute were randomly permuted in each training set. Each data set was analyzed 1000
times in 10-fold cross-validation. Lowest error rates are shown in boldface.

257

Berrar

statistical fluctuations. An alternative explanation is that in the experiments with synthetic
data, all ranking scores from [0, 1] were equally likely. In the experiments with real-world
data sets, however, that was not the case. These data sets are relatively easy to classify.
Therefore, we can expect to see more scores concentrated towards 1 and 0 and fewer scores
around 0.5, which might have a negative effect on the H-measure; however, this is only
speculation.

5. Discussion and Conclusions
Ranking measures play an important role in model evaluation and selection. Using both
synthetic and real-world data sets, we compared the robustness of various ranking measures
to different types and levels of noise. The AUC has recently been criticized as an incoherent
measure (Hand, 2009; Hand & Anagnostopoulos, 2013; Parker, 2013); nonetheless, it was
arguably the most robust among the conventional measures in our experiments. This is an
important finding, as it lends further empirical credibility to the AUC and complements its
recently published vindications (Flach, HernaÃÅndez-Orallo, & Ferri, 2011; HernaÃÅndez-Orallo
et al., 2012; Bradley, 2013). The AUC was also more robust than the sAUC, which confirms
the observations by Vanderlooy and HuÃàllermeier (2008) that the sAUC is not an efficient
alternative to the AUC.
In our experiments with the synthetic data sets, KS and the H-measure performed best
under misclassification noise. Under probability noise, however, they performed worse than
the AUC and AUCH. All metrics except the sAUC performed more or less similarly under
class proportion noise. Overall, the differences between the metrics with respect to their
resilience to noise were rather small for relatively low noise levels in the synthetic data. Also,
in most of the investigated real-world data sets, the magnitude of the difference was arguably
not that dramatic. The sAUC should be used with caution, though, because it performed
poorly in the experiments with synthetic data (notably class proportion noise, Figure 6)
and in the experiments with the more difficult real-world data sets (Liver, Transfusion).
These observations confirm earlier results, which showed that the sAUC is not robust to
noise (Ferri et al., 2009).
Our experiments do not allow the conclusion that the H-measure is preferable to the
AUC with respect to robustness. Also, we believe that the H-measure is arguably more
intricate than the other measures, and its geometrical interpretation is not as straightforward as that of the AUC. This does of course not mean that the H-measure is not useful or
that the AUC can always be trusted. Hilden (1991) describes an interesting example where
the AUC is in fact misleading. Also, note that Parker (2013) comes to a conclusion that
is different from ours: he recommends the H-measure, both on empirical and theoretical
grounds. However, Parker evaluated a measure based on its (dis-)agreement with other
measures, not based on its robustness to noise.
We also proposed a novel ranking measure, called taKS. A key characteristic of this
measure is its simplicity. The taKS is easily derived, and it has a simple geometrical interpretation as the average distance between two curves: the true positive and the false positive
rate curve, each plotted as a function of the classification threshold. In our study, taKS
was remarkably robust to noise. However, we caution that the arguments against the AUC
(Hilden, 1991; Hand, 2009; Hand & Anagnostopoulos, 2013) should not be dismissed light258

An Empirical Evaluation of Ranking Measures With Respect to Robustness to Noise

heartedly. Particularly, Parker (2013) has recently extended Hand‚Äôs analysis, showing that
related metrics (the area under Cohen‚Äôs Œ∫ curve and average precision) are similarly incoherent. According to Parker‚Äôs theorem 1, the problem is that these measures result from the
integration over all possible classification thresholds. As taKS is measured via a normalized
summation, it could be similarly incoherent. Our experimental results are promising, but
more research is needed to elucidate the usefulness of taKS. Many open questions remain,
for example, what is the precise relation between taKS and other measures, for example,
the partial AUC (McClish, 1989)? When is taKS (in-)coherent? And particularly, what is
the role of data set idiosyncrasies for the selection of a ranking measure? We also remember that the results of empirical studies should not be viewed in isolation but against the
backdrop of previous research. The AUC was remarkably robust in our experiments, and it
has been successfully used in numerous studies; in addition, it has recently been vindicated
theoretically (HernaÃÅndez-Orallo et al., 2012). Taken together, we therefore conclude that
the AUC might still be a good choice for practical applications.
Finally, we note that all investigated metrics share an important caveat: as scalars, they
cannot paint the full picture of a classifier‚Äôs performance. By condensing the performance
into a single number, we are bound to lose important information about the behavior of
a model over a range of operating conditions, which is generally better described by twodimensional plots such as ROC curves. One should always be wary of reading too much
into a single number. A single number can be misleading. On the other hand, scalars have
the obvious advantage that they allow us to tabulate the results of various classifiers easily.
This is desirable when we compare a very large number of models, as it is generally the case
in data mining competitions, for example.

Acknowledgments
We thank the three anonymous reviewers very much for their detailed and constructive
comments that have greatly helped improve the manuscript.

259

Berrar

Appendix A. Pseudocodes
Algorithm 1 Pseudocode for taKS.
Require: A matrix X with k rows (one for each test case) and 2 columns (first column: real class
label; second column: predicted score for positive class, s+ ). X is ordered based on decreasing
values of s+ ; at least two scores must be different. # if all scores are identical then taKS is not
defined.
1: TPR, FPR ‚Üê < 0 > # lists, each containing one element: 0
2: tp, fp ‚Üê 0
3: np ‚Üê number of positive cases in X; nn ‚Üê number of negative cases in X
4: i ‚Üê 1
5: while (i ‚â§ number of rows of X) do
6: threshold ‚Üê i
7: ii ‚Üê i
8: scorei ‚Üê s+ of the ith case
9:
while (scoreii+1 == scorei ) and (ii +1 ‚â§ number of cases in X) do
10:
threshold ‚Üê threshold + 1
11:
ii ‚Üê ii + 1
12:
end while
13: tp ‚Üê number of positive cases at or above threshold
14: fp ‚Üê number of negative cases at or above threshold
15: push tp/np onto TPR; push fp/nn onto FPR
16: i ‚Üê threshold + 1
17: end while
18: taKS ‚Üê mean(TPR\{first, last} ‚àí FPR\{first, last})
19: return taKS

260

An Empirical Evaluation of Ranking Measures With Respect to Robustness to Noise

Algorithm 2 Real-world data set, experiment #1. Corrupt 10% of the class labels in the
data set
Require: data set D
1: for i = 1 to 1000 do
2: Randomly select 10% of the cases from D and randomly assign class label.
3:
for k = 1 to 10 do
4:
Sample k-th training and k-th test set from corrupted D.
5:
Build naive Bayes classifier from k-th training set.
6:
Apply classifier to k-th test set and obtain output C1k .
7:
Derive Xk (C1k ).
8:
Randomly select 10% of the prediction scores of C1k .
9:
Replace each selected score by a random number from [0, 1] to obtain C2k .
10:
Derive Xk (C2k ).
11:
end for
12:
X(C1 ) ‚áê average of Xk (C1k ).
13:
X(C2 ) ‚áê average of Xk (C2k ).
14:
if X(C2 ) > X(C1 ) then
15:
(X) ‚áê (X) + 1
16:
else
17:
if X(C2 ) == X(C1 ) then
18:
(X) ‚áê (X) + 0.5
19:
else
20:
(X) ‚áê (X) + 0
21:
end if
22:
end if
23: end for
24: return (X)

261

Berrar

Algorithm 3 Real-world data set, experiment #2. Corrupt 10% of the class labels per
training set
Require: data set D
1: for i = 1 to 1000 do
2:
for k = 1 to 10 do
3:
Sample k-th training and k-th test set from D.
4:
Randomly select 10% of the training cases.
5:
Randomly assign a class label to the selected cases.
6:
Build naive Bayes classifier from k-th corrupted training set.
7:
Apply classifier to k-th test set and obtain output C1k .
8:
Derive Xk (C1k ).
9:
Randomly select 10% of the prediction scores of C1k .
10:
Replace each selected score by a random number from [0, 1] to obtain C2k .
11:
Derive Xk (C2k ).
12:
end for
13:
X(C1 ) ‚áê average of Xk (C1k ).
14:
X(C2 ) ‚áê average of Xk (C2k ).
15:
if X(C2 ) > X(C1 ) then
16:
(X) ‚áê (X) + 1
17:
else
18:
if X(C2 ) == X(C1 ) then
19:
(X) ‚áê (X) + 0.5
20:
else
21:
(X) ‚áê (X) + 0
22:
end if
23:
end if
24: end for
25: return (X)

262

An Empirical Evaluation of Ranking Measures With Respect to Robustness to Noise

Algorithm 4 Real-world data set, experiment #3. Corrupt 10% of the attribute values in
the data set
Require: data set D
1: for i = 1 to 1000 do
2: Randomly select 10% of the values of each attribute of D.
3: Randomly permute the selected values per attribute.
4:
for k = 1 to 10 do
5:
Sample k-th training and k-th test set from corrupted D.
6:
Build naive Bayes classifier from k-th corrupted training set.
7:
Apply classifier to k-th test set and obtain output C1k .
8:
Derive Xk (C1k ).
9:
Randomly select 10% of the prediction scores of C1k .
10:
Replace each selected score by a random number from [0, 1] to obtain C2k .
11:
Derive Xk (C2k ).
12:
end for
13:
X(C1 ) ‚áê average of Xk (C1k ).
14:
X(C2 ) ‚áê average of Xk (C2k ).
15:
if X(C2 ) > X(C1 ) then
16:
(X) ‚áê (X) + 1
17:
else
18:
if X(C2 ) == X(C1 ) then
19:
(X) ‚áê (X) + 0.5
20:
else
21:
(X) ‚áê (X) + 0
22:
end if
23:
end if
24: end for
25: return (X)

263

Berrar

Algorithm 5 Real-world data set, experiment #4. Corrupt 10% of the attribute values
per training set
Require: data set D
1: for i = 1 to 1000 do
2:
for k = 1 to 10 do
3:
Sample k-th training and k-th test set from D.
4:
For the training set only: select 10% of the values of each attribute.
5:
Randomly permute the selected values per attribute.
6:
Build naive Bayes classifier from k-th corrupted training set.
7:
Apply classifier to k-th test set and obtain output C1k .
8:
Derive Xk (C1k ).
9:
Randomly select 10% of the prediction scores of C1k .
10:
Replace each selected score by a random number from [0, 1] to obtain C2k .
11:
Derive Xk (C2k ).
12:
end for
13:
X(C1 ) ‚áê average of Xk (C1k ).
14:
X(C2 ) ‚áê average of Xk (C2k ).
15:
if X(C2 ) > X(C1 ) then
16:
(X) ‚áê (X) + 1
17:
else
18:
if X(C2 ) == X(C1 ) then
19:
(X) ‚áê (X) + 0.5
20:
else
21:
(X) ‚áê (X) + 0
22:
end if
23:
end if
24: end for
25: return (X)

264

An Empirical Evaluation of Ranking Measures With Respect to Robustness to Noise

References
Adams, N., & Hand, D. (1999). Comparing classifiers when the misallocation costs are
uncertain. Pattern Recognition, 32 (7), 1139‚Äì1147.
Bache, K., & Lichman, M. (2013).
UCI machine learning repository.
[http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of
Information and Computer Science.
Bamber, D. (1975). The area under the ordinal dominance graph and the area below the
receiver operating characteristic curve. Journal of Mathematical Psychology, 12, 387‚Äì
415.
Bengio, S., MarieÃÅthoz, J., & Keller, M. (2005). The expected performance curve. Proceedings
of the ICML 2005 workshop on ROC Analysis in Machine Learning, 9‚Äì16.
Berrar, D., & Flach, P. (2012). Caveats and pitfalls of ROC analysis in clinical microarray
research (and how to avoid them). Briefings in Bioinformatics, 13 (1), 83‚Äì97.
Bradley, A. (1997). The use of the area under the ROC curve in the evaluation of machine
learning algorithms. Pattern Recognition, 30 (3), 1145‚Äì1159.
Bradley, A. (2013). ROC curve equivalence using the Kolmogorov-Smirnov test. Pattern
Recognition Letters, 34 (5), 470‚Äì475.
Calders, T., & Jaroszewicz, S. (2007). Efficient AUC optimization for classification. In Kok,
J., Koronacki, J., de MaÃÅntaras, R., Matwin, S., MladenicÃÜ, D., & Skowron, A. (Eds.),
Proceedings of the 11th European Conference on Principles and Practice of Knowledge
Discovery in Databases, pp. 42‚Äì53. Springer.
Caruana, R., & Niculescu-Mizil, A. (2004). Data mining in metric space: An empirical
analysis of supervised learning performance criteria. In Proceedings of the 10th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.
69‚Äì78. ACM Press.
Drummond, C., & Holte, R. (2006). Cost curves: An improved method for visualizing
classifier performance. Machine Learning, 65, 95‚Äì130.
Fawcett, T. (2004). ROC graphs: Notes and practical considerations for researchers. Kluwer
Academic Publishers, 1‚Äì38.
Ferri, C., Flach, P., HernaÃÅndez-Orallo, J., & Senad, A. (2005). Modifying ROC curves
to incorporate predicted probabilities. In Proceedings of the 2nd Workshop on ROC
Analysis in Machine Learning. Bonn, Germany.
Ferri, C., HernaÃÅndez-Orallo, J., & Modroiu, R. (2009). An experimental comparison of
performance measures for classification. Pattern Recognition Letters, 30, 27‚Äì38.
Flach, P. (2003). The geometry of ROC space: understanding machine learning metrics
through ROC isometrics. In Proceedings of the 20th International Conference on
Machine Learning, pp. 194‚Äì201. AAAI Press.
Flach, P. (2010). ROC analysis. In Sammut, C., & Webb, G. (Eds.), Encyclopedia of
Machine Learning, pp. 869‚Äì874. Springer.
265

Berrar

Flach, P., HernaÃÅndez-Orallo, J., & Ferri, C. (2011). A coherent interpretation of AUC
as a measure of aggregated classification performance. In Proceedings of the 28th
International Conference on Machine Learning, pp. 69‚Äì78.
Hanczar, B., Hua, J., Sima, C., Weinstein, J., Bittner, M., & Dougherty, E. (2010). Smallsample precision of ROC-related estimates. Bioinformatics, 26, 822‚Äì830.
Hand, D. (2006). Classifier technology and the illusion of progress. Statistical Science,
21 (1), 1‚Äì14.
Hand, D. (2009). Measuring classifier performance: a coherent alternative to the area under
the ROC curve. Machine Learning, 77, 103‚Äì123.
Hand, D., & Anagnostopoulos, C. (2013). When is the area under the receiver operating characteristic curve an appropriate measure of classifier performance?. Pattern
Recognition Letters, 34 (5), 492‚Äì495.
Hanley, J., & McNeil, B. (1983). A method of comparing the areas under receiver operating
characteristic curves derived from the same cases. Radiology, 148 (3), 839‚Äì843.
HernaÃÅndez-Orallo, J., Flach, P., & Ferri, C. (2012). A unified view of performance metrics:
Translating threshold choice into expected classification loss. Journal of Machine
Learning Research, 13, 2813‚Äì2869.
Hilden, J. (1991). The area under the ROC curve and its competitors. Medical Decision
Making, 11 (2), 95‚Äì101.
Korb, K., Hope, L., & Hughes, M. (2001). The evaluation of predictive learners: Some
theoretical and empirical results. In DeRaedt, L., & Flach, P. (Eds.), Lecture Notes
in Artificial Intelligence, pp. 276‚Äì287. Springer.
Lobo, J., JimeÃÅnez-Valverde, A., & Real, R. (2008). AUC: a misleading measure of the
performance of predictive distribution models. Global Ecology and Biogeography, 17,
145‚Äì151.
McClish, D. (1989). Analyzing a portion of the ROC curve. Medical Decision Making, 9 (3),
190‚Äì195.
Parker, C. (2013). On measuring the performance of binary classifiers. Knowledge and
Information Systems, 35, 131‚Äì152.
Prati, R., Batista, G., & Monard, M. (2011). A survey on graphical methods for classification predictive performance evaluation. IEEE Transactions on Knowledge and Data
Engineering, 23 (11), 1601‚Äì1618.
Provost, F., & Fawcett, T. (2001). Robust classification for imprecise environments. Machine
Learning, 42 (3), 203‚Äì231.
R Development Core Team (2009). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0.
Sheskin, D. (2007). Handbook of Parametric and Nonparametric Statistical Procedures. 4th
Edition, Chapman and Hall, London/New York.
Vanderlooy, S., & HuÃàllermeier, E. (2008). A critical analysis of variants of the AUC. Machine
Learning, 72, 247‚Äì262.
266

An Empirical Evaluation of Ranking Measures With Respect to Robustness to Noise

Webb, G., & Ting, K. (2005). On the application of ROC analysis to predict classification
performance under varying class distributions. Machine Learning, 58 (1), 25‚Äì32.
Wu, S., & Flach, P. (2005). A scored AUC metric for classifier evaluation and selection.
Proceedings of the Second Workshop on ROC Analysis in Machine Learning.
Wu, S., Flach, P., & Ferri, C. (2007). An improved model selection heuristic for AUC. In
Kok, J., Koronacki, J., de MaÃÅntaras, R., Matwin, S., MladenicÃÜ, D., & Skowron, A.
(Eds.), Proceedings of the 18th European Conference on Machine Learning (ECML
2007), pp. 478‚Äì489. Springer.

267

Journal of Artificial Intelligence Research 49 (2014) 111-142

Submitted 10/13; published 02/14

Towards Minimizing Disappointment in Repeated Games
Jacob W. Crandall

JCRANDALL @ MASDAR . AC . AE

Masdar Institute of Science and Technology
Abu Dhabi, United Arab Emirates

Abstract
We consider the problem of learning in repeated games against arbitrary associates. Specifically, we study the ability of expert algorithms to quickly learn effective strategies in repeated
games, towards the ultimate goal of learning near-optimal behavior against any arbitrary associate
within only a handful of interactions. Our contribution is three-fold. First, we advocate a new
metric, called disappointment, for evaluating expert algorithms in repeated games. Unlike minimizing traditional notions of regret, minimizing disappointment in repeated games is equivalent to
maximizing payoffs. Unfortunately, eliminating disappointment is impossible to guarantee in general. However, it is possible for an expert algorithm to quickly achieve low disappointment against
many known classes of algorithms in many games. Second, we show that popular existing expert
algorithms often fail to achieve low disappointment against a variety of associates, particularly in
early rounds of the game. Finally, we describe a new meta-algorithm that can be applied to existing
expert algorithms to substantially reduce disappointment in many two-player repeated games when
associates follow various static, reinforcement learning, and expert algorithms.

1. Introduction
Many real-world environments require machines to interact repeatedly with other independent, selfinterested entities, including both people and other machines. These finitely repeated interactions
endure for unknown periods of time ranging from minutes, to hours, days, months, or even years.
To be successful in these interactions, machines must employ algorithms that quickly learn good
strategies against arbitrary (likely adaptive) associates.
Many algorithms for repeated games have been developed over the last several decades, including reinforcement learning algorithms (e.g., Watkins & Dayan, 1992; Littman, 1994, 2001; Bowling
& Veloso, 2002; Greenwald & Hall, 2003; Crandall & Goodrich, 2011), opponent modeling algorithms (e.g., Fudenberg & Levine, 1998; Ganzfried & Sandholm, 2011), algorithms for computing
desirable equilibria (e.g., Littman & Stone, 2005; Cote & Littman, 2008; Johanson, Bard, Lanctot,
Gibson, & Bowling, 2012), and expert algorithms (e.g., Auer, Cesa-Bianchi, & Fischer, 2002; de
Farias & Megiddo, 2004; Auer, Cesa-Bianchi, Freund, & Schapire, 1995). While sometimes successful, these algorithms typically have one or more of the following shortcomings which preclude
their use. First, many of these algorithms learn very slowly. They achieve successful behavior only
after thousands of interactions, even in simple games (e.g., Crandall & Goodrich, 2011). Second,
existing algorithms are often myopic. They fail to learn profitable strategies in long-term interactions. Third, many algorithms are successful only against a limited set of associates.
Our long-term goal is to identify algorithms that learn near-optimal behavior against any arbitrary associate within only a handful of interactions. In this paper, we focus on the potential of
expert algorithms to achieve this goal in two-player normal-form games. In each round, an expert
algorithm selects an expert from a predefined set of experts to dictate the agent‚Äôs behavior in that
c
2014
AI Access Foundation. All rights reserved.

C RANDALL

round. This algorithmic structure has several potential strengths. First, it offers a simple and flexible design process. One can create experts that perform well in specific scenarios that the agent is
likely to encounter (such as being paired with a particular associate) without worrying that any one
expert must perform well in all scenarios. The expert algorithm is responsible for finding the best
expert for the specific scenario during run time. Second, experts can be as complicated as necessary.
Though we focus on normal-form games in this paper, experts that compute complex equilibria or
execute sophisticated algorithms can also be derived for stochastic and dynamics games. These two
advantages give rise to a third potential advantage: expert algorithms have the potential to learn
effective strategies quickly, particularly when experts employ precomputed strategies.
An expert algorithm is evaluated based on its ability to learn to select successful experts. Several
metrics have been defined and used in the literature to measure the success of an expert algorithm
in this process, perhaps the most popular of which is regret (Foster & Vohra, 1999; Greenwald &
Jafari, 2003; Bowling, 2004; Gordon, Greenwald, & Marks, 2008). Loosely, an expert algorithm‚Äôs
(external) regret is the difference between the payoffs the agent would have received had it always
followed its best expert against its associates‚Äô observed actions and the payoffs it actually received.
In the context of repeated games, regret is a desirable notion since it provides a simple generalizable
benchmark of success. Unfortunately, minimizing regret does not always correspond to maximizing
payoffs. In this paper, we advocate an alternative metric, called disappointment, which is equivalent
to maximizing payoffs while still providing a simple generalizable benchmark of success.
While there are many algorithms that are guaranteed to achieve no regret (e.g., Bowling, 2004;
Foster & Vohra, 1999; Gordon et al., 2008), we show that it is impossible for an algorithm to be
guaranteed to have no disappointment against an unknown associate. However, it is possible for
an algorithm to quickly achieve and maintain low disappointment against classes of algorithms in
many repeated games. We first evaluate the effectiveness of several existing expert algorithms to
achieve low disappointment when paired with various (1) non-adapting, (2) reinforcement learning,
and (3) expert algorithms in two-player games. Finally, we describe a new meta-algorithm for
enhancing expert algorithms. We show that it substantially reduces the disappointment of these
algorithms against these same associates in both short- and long-term interactions.
In Section 2, we discuss the evaluation of expert algorithms. In so doing, we define disappointment and establish several theoretical results, from which we derive a research agenda. In Section 3,
we define a method for generating an effective set of experts for repeated normal-form games. We
then evaluate the ability of existing expert algorithms to select effective experts across ten different repeated games in Section 4. We propose a meta-algorithm for enhancing expert algorithms in
Section 5, and evaluate its effectiveness in Section 6. We conclude and reflect in Section 7.

2. Evaluating Expert Algorithms
In this section, we review existing evaluation metrics for repeated games, define and discuss disappointment, and compare it to existing metrics. Finally, we formally state our research agenda.
2.1 Notation and Terminology
We consider two-player repeated normal-form games, which consist of a set of joint actions A =
A1 √ó A2 , where Ai is player (or agent) i‚Äôs action set, and a payoff function M : A ‚Üí R2 . In
each round t, each agent i independently selects an action
ati ‚àà Ai . The resulting joint action

t
t
t
t
t
a = (a1 , a2 ) produces the payoff pair m1 (a ), m2 (a ) , where mi (at ) is the payoff to agent i. For
112

T OWARDS M INIMIZING D ISAPPOINTMENT

IN

R EPEATED G AMES

simplicity, we assume that, ‚àÄa ‚àà A, mi (a) ‚àà [0, 1], maxa‚ààA mi (a) = 1, and mina‚ààA mi (a) = 0.
Play repeats an unknown number of rounds. We refer to the two agents as i and ‚àíi.
We use the terms policy and strategy to refer to how agents select their actions. Agent i‚Äôs
policy is a probability distribution over its action set Ai . This probability distribution specifies the
probability that agent i selects each action. A strategy is a rule that defines an agent‚Äôs policies in each
state of the world. In the context of the experts used in this paper, world states are typically defined
by the previous joint action played by the agents in the game. We use the notation ui (œÄi , œÄ‚àíi ) to
denote agent i‚Äôs expected payoff in a round in which it plays policy œÄi and its associate plays œÄ‚àíi .
We use the notation ¬µi (œÅi , œÅ‚àíi ) to denote agent i‚Äôs average expected per-round payoff over time
when it perpetually plays strategy œÅi and its associate perpetually plays strategy œÅ‚àíi .
We make several assumptions. First, we initially assume that the game is played with perfect information: both players know each other‚Äôs payoff matrix. We later relax this assumption to account
for occasions in which the players incorrectly assess their associate‚Äôs payoffs. Second, we focus on
general-sum repeated games, but do not specifically address constant-sum games. However, many
of the concepts discussed herein also apply to constant-sum games.
In each round t, an expert algorithm employed by agent i selects an expert œÜti from a set of
experts Œ¶i . This expert then dictates the policy executed by agent i in round t. For simplicity in
analysis, the literature often only considers experts that always play a single action or policy. We
make no such assumption in this paper. Experts can also be sophisticated automata or learning
algorithms.
2.2 Metrics
Currently, there is no universally accepted metric for evaluating how well expert algorithms select
experts in repeated games. Existing metrics typically define a desirable performance benchmark an
algorithm should achieve and then compare the payoffs obtained by algorithms to this benchmark.
To be reliable indicators of success, such metrics should be payoff comparable.
Definition 2.1 (Payoff comparable): Let A and B be two distinct algorithms, and let ¬µo,M,T
and
A
o,M,T
¬µB
denote the average per-round payoff obtained by A and B, respectively, against associate o
in a repeated game of length T with payoff matrix M . An evaluation metric is payoff comparable if,
for any scenario defined by o, M , and T , it rates A higher than B if and only if ¬µo,M,T
> ¬µo,M,T
.
A
B
In words, an evaluation metric is payoff comparable if and only if success as defined by the metric
implies success with respect to maximizing the agent‚Äôs average per-round payoff.
We seek an evaluation metric that (1) defines a generalizable (and desirable) performance benchmark and (2) is payoff comparable. We discuss several metrics with respect to these two attributes.
2.2.1 R EGRET
Regret has become a popular performance metric for evaluating the effectiveness of learning rules
in repeated games. This notion has been re-discovered independently several times under various
names (Foster & Vohra, 1999). Several forms of regret have been formulated, including external
regret and internal (or swap) regret. For simplicity of argument, we focus on external regret.
113

C RANDALL

Formally, agent i‚Äôs total external regret after round T is
RTi = max
œÜ‚ààŒ¶i

T
X

uti (œÜ, at‚àíi ) ‚àí

T
X

mi (at ),

(1)

t=1

t=1

where uti (œÜ, at‚àíi ) is agent i‚Äôs expected payoff in round t if, in each round œÑ ‚àà {1, ¬∑ ¬∑ ¬∑ , t}, agent i
were to have followed expert œÜ while agent ‚àíi played its observed action aœÑ‚àíi . Agent i‚Äôs average
external regret through round T is
RÃÑTi =

RTi
.
T

(2)

RÃÑTi ‚â§ 0 means that the expert algorithm has done at least as well as it would have done had it
always followed its best expert given that the associate would have still played its observed actions.
Agent i is said to have no regret when limT ‚Üí‚àû RÃÑTi ‚â§ 0. When all agents use no-regret learning
rules, play converges to correlated equilibria (Greenwald & Jafari, 2003; Gordon et al., 2008).
The performance benchmark (the estimated payoffs of its best expert) used to calculate regret is
simple and generalizable to any scenario. However, regret is not payoff comparable. The assumption
that agent i‚Äôs behavior does not affect agent ‚àíi‚Äôs future actions is clearly violated when agent ‚àíi
executes a learning algorithm or even a simple automata. This limiting assumption means that regret
minimization does not imply payoff maximization. In fact, as we demonstrate in Section 2.3, low
regret sometimes strongly correlates with low payoffs.
Several alterations to regret have been made in attempt to alleviate its shortcomings. For example, Chang (2007) proposed a modified form of regret which considers multi-period strategies.
This modification provides more effective evaluations against simple automata such as tit-for-tat (at
the expense of increased computation complexity), but still does not address the general deficiency
that regret minimization does not imply payoff maximization when one‚Äôs associate learns. Alternatively, Bowling (2004) embraced regret as a minimum criterion despite its limitations, but advocated
for more: an algorithm should also converge or achieve negative regret in self play. Though this
addition makes the metric stronger, it does not make the metric payoff comparable.
2.2.2 E XPERIENCED R EGRET
An alternative metric devised by de Farias and Megiddo (2003, 2004), which we refer to as experienced regret (e-regret), compares the agent‚Äôs average payoff over all rounds with the actual average
payoff obtained by its most successful expert in rounds it was followed. Let xTi (œÜ) be the average
payoff obtained by agent i in each round that it followed expert œÜ up to round T , given by1
PT
I(œÜ, œÜti )mi (at )
,
(3)
xTi (œÜ) = t=1
PT
t)
I(œÜ,
œÜ
t=1
i

where I(œÜ, œÜti ) is an indicator function that returns 1 if œÜ = œÜti and 0 otherwise. Then, i‚Äôs e-regret is
T
1X
mi (at ).
E¬ØiT = max xTi (œÜ) ‚àí
œÜ‚ààŒ¶i
T t=1

1. As an exception, if œÜ has never been played up to time T , then xTi (œÜ) = 0.

114

(4)

T OWARDS M INIMIZING D ISAPPOINTMENT

IN

R EPEATED G AMES

As with regret (Eq. 1), the minuend of Eq. (4) is not independent of the subtrahend. In the
general case, xTi (œÜ) can be different depending on the sequence of experts (and, hence, actions) that
the agent plays. As such, maxœÜ xTi (œÜ) is not guaranteed to be a stable metric of success to which
the agent‚Äôs performance can be compared. Lower e-regret is not guaranteed to correspond to higher
payoffs (and often does not; see Section 2.3). Thus, e-regret is not payoff comparable in general.
As an exception, de Farias and Megiddo (2004) showed that minimizing e-regret in the limit as
T ‚Üí ‚àû can directly translate into maximizing payoffs against ‚Äúflexible opponents,‚Äù or associates
against whom the agent‚Äôs average payoff between rounds t and t + s converges (as s ‚Üí ‚àû) to a
limit that is independent of the history of play prior to round t. Against flexible opponents, if œÜ
is followed for a sufficiently large number of rounds s (approaching infinity) when selected, xti (œÜ)
does not vary substantially depending on the sequence of experts chosen (and, hence, the minuend
of Eq. (4) is independent of the subtrahend). In such circumstances, minimizing e-regret equates
to maximizing payoffs. Using such reasoning, de Farias and Megiddo also established well-defined
performance guarantees (in the limit) for the expert algorithm EEE against flexible opponents.
While the performance bounds provided by de Farias and Megiddo (2004) are appealing in this
special case, we do not adopt e-regret as an evaluation metric in repeated games for two reasons.
First, though the set of ‚Äúflexible opponents‚Äù includes useful classes of algorithms, it does not include
many algorithms an agent is likely to encounter, such as many expert algorithms (including EEE
itself) and other learning algorithms. Second, the performance bounds of EEE against flexible
opponents are true only in the limit. Since most interactions are not infinite, we are interested in a
metric that can provide an accurate measure of success over any time interval.
In summary, regret and e-regret are desirable since they provide generalizable performance
benchmarks for expert algorithms. Unfortunately, these evaluation metrics are not payoff comparable against arbitrary associates. Hence, we adopt an alternative (though related) metric for
evaluating how effectively expert algorithms select experts in repeated games.
2.2.3 D ISAPPOINTMENT
External regret and e-regret imply a simple notion of success: an expert algorithm should perform
at least as well as it would have performed had it always followed its best expert. Disappointment
targets this same notion, minus the assumption that agent i‚Äôs actions do not impact agent ‚àíi‚Äôs future
t (œÜ) be the policy that agent ‚àíi would have played in round t had agent i
actions. Formally, let œÄ‚àíi
always followed expert œÜ up to round t. Agent i‚Äôs total disappointment2 up to round T is
DiT = max
œÜ‚ààŒ¶i

t (œÜ))
uti (œÜ, œÄ‚àíi

T
X

t
uti (œÜ, œÄ‚àíi
(œÜ)) ‚àí

T
X

mi (at ),

(5)

t=1

t=1

where
is agent i‚Äôs expected payoff in round t if, in each round œÑ ‚àà {1, ¬∑ ¬∑ ¬∑ , t},
œÑ (œÜ). Agent i‚Äôs average
agent i had followed expert œÜ and agent ‚àíi had acted according to œÄ‚àíi
disappointment up to round T is
DÃÑiT =

DiT
.
T

(6)

2. After this paper was accepted for publication, we became aware of recent work defining policy regret (Arora, Dekel,
& Tewari, 2012; Cesa-Bianchi, Dekel, & Shamir, 2013). Disappointment captures the same notion as policy regret,
except that disappointment allows for experts that implement complex (even adaptive) algorithms rather than just
actions or action sequences. While this generalization is somewhat trivial, the term policy regret does not seem to fit
given such experts. Rather than continue to overload the term regret, we refer to this metric as disappointment.

115

C RANDALL

c
d

C
0.60, 0.60
1.00, 0.00

D
0.00, 1.00
0.20, 0.20

c
d
a 0.84, 0.84 0.33, 1.00
b 1.00, 0.33 0.00, 0.00

(a) Prisoners‚Äô Dilemma

(b) Chicken

Table 1: Payoff matrices for the PD and Chicken. In each cell, the row player‚Äôs payoffs are listed
first, followed by the column player‚Äôs.
Agent i is said to have no disappointment when limT ‚Üí‚àû DÃÑiT ‚â§ 0.
We make several observations about Eq. (5). First, the minuend and subtrahend are independent.
Unlike Eqs. (1) and (4), the computation of an agent‚Äôs best expert as measured in Eq. (5) is independent of how agent i played. The minuend is simply a constant specifying how well agent i would
have done had it always followed its best expert, and hence is a stable benchmark of success. An
agent‚Äôs disappointment is this benchmark minus its accumulated payoffs. Hence, disappointment is
payoff comparable: algorithms that receive higher payoffs against a given associate achieve lower
disappointment than algorithms that receive lower payoffs against that associate (and vice versa).
Second, when agent ‚àíi is not influenced by agent i‚Äôs actions, at‚àíi is a good approximation
t (œÜ). In such cases, disappointment and external regret are essentially equivalent. This is
of œÄ‚àíi
desirable since minimizing regret is equivalent to maximizing payoffs in such cases.
Third, like external regret, disappointment can be negative. Negative disappointment indicates
that an expert algorithm performed better than it would have performed had it always followed
its best expert. However, while sometimes possible, achieving negative disappointment can be
extremely difficult against unknown associates. Thus, as an immediate goal, we focus on finding
expert algorithms that achieve (or come close to achieving) no disappointment.
Finally, a strength of both regret and e-regret is that they can be computed during run time in
a repeated game against an unknown associate. Thus, regret can be used as part of an algorithm
in addition to being an evaluation metric. Indeed, regret is used as an algorithmic tool by various
no-regret algorithms. On the downside, since minimizing regret does not necessarily correspond
to maximizing payoffs, the use of regret as an algorithmic tool can lead to low payoffs in some
scenarios. On the other hand, the minuend of Eq. (5) cannot be computed during run time against
an unknown associate. Thus, disappointment is limited to being a metric to evaluate algorithms in
repeated games; it is not clear how it could be used as an algorithmic tool.
2.3 Examples: Regret vs. Disappointment
We illustrate differences between disappointment and regret with several examples. First, consider
an expert algorithm playing a repeated prisoners‚Äô dilemma (PD; Table 1a) against tit-for-tat (TFT;
Axelrod, 1984). The expert algorithm has at its disposal two experts: AC, an expert that always recommends cooperate, and AD, an expert that always recommends defect. Figure 1 shows the average
payoff, average regret, and average disappointment after 20 rounds when the algorithm always cooperates or always defects. Since always following AC would produce a higher payoff than always
following AD (0.6 as opposed to 0.24), AC is the best expert. As such, always cooperating has zero
disappointment and always defecting has high disappointment. On the other hand, always cooperating has high regret, while always defecting has zero regret. Thus, in this scenario, minimizing
regret does not correspond to maximizing payoffs, but minimizing disappointment does.
116

T OWARDS M INIMIZING D ISAPPOINTMENT

IN

R EPEATED G AMES

Against TFT in the Prisoners‚Äô Dilemma
0.7
Average payoff
Average regret
Average disappointment

0.6
0.5
0.4
0.3
0.2
0.1
0
Always Cooperate

Always Defect

Figure 1: Comparison of average payoff, regret, and disappointment in the PD (T = 20).

Similar results are observed when learning algorithms play against each other in the repeated
PD. For example, Figures 2a and 2c plot the average payoffs of six different learning algorithms
(Exp3, UCB1, EEE, S, BR1, and BR2) against the algorithms‚Äô corresponding average regret and
average disappointment, respectively, when paired with four different associates3 . Figure 2a shows
that the algorithms that achieved lower regret tended to have higher performance against BR1,
WoLF-PHC, and Exp3, but not against S. Against S, algorithms with higher regret received substantially higher payoffs. On the other hand, Figure 2c shows that decreasing disappointment against
each of the four algorithms resulted directly in higher payoffs in the PD.
Regret is even less indicative of performance in Chicken (Table 1b) against these associates. In
this game, lower regret tends to lead to higher payoffs against WoLF-PHC, but not against Exp3,
BR1, or S (Figure 2b). As in the PD, lower regret against S correlates with lower payoffs in Chicken,
while lower disappointment always translates directly into higher payoffs (Figure 2d).
Figures 2e‚Äì2h demonstrate the deficiencies of e-regret as an evaluation metric in the PD and
in Chicken against these same associates. After 1000 rounds, lower e-regret against BR1 in both
the PD and in Chicken does not correspond with higher payoffs (Figures 2e and 2f). Discrepancies
are even more pronounced by 50,000 rounds. In the PD, all algorithms have low e-regret against S,
but with wildly different average payoffs (Figure 2g). Similar, though not identical, trends occur in
Chicken (Figure 2h).
2.4 Research Agenda
In the absence of a single, universally accepted, evaluation metric for repeated games, sets of performance criteria have been proposed (e.g., Powers & Shoham, 2005a; Crandall & Goodrich, 2011).
Researchers advocating these agendas have argued that successful algorithms should simultaneously satisfy all criteria in the identified set. For example, Powers and Shoham (2005a) argued that
successful algorithms should simultaneously satisfy three performance criteria:
‚Ä¢ Targeted Optimality: Against any member of the target set of associates, the algorithm achieves
within Œµ of the expected value of the best response to the associate‚Äôs actual play.
3. See Appendix A for implementation details. The experts used by Exp3, UCB, EEE, and S are described in Section 3.
The regrets and disappointments of BR1 and BR2 are computed with respect to these same experts.

117

0.60

0.90

0.55

0.85

0.50

Average Payoff

Average Payoff

C RANDALL

0.45
0.40
0.35
0.30
against BR1
against WoLF‚àíPHC
against S
against Exp3

0.25
0.20
0.15
0

0.05

0.1

0.15

0.2

0.25

0.3

0.80
0.75
0.70
0.65

against BR1
against WoLF‚àíPHC
against S
against Exp3

0.60
0.55

0.35

0

0.05

Average Regret

(a) PD: RÃÑ1000
vs. ¬µÃÑ1000
i
i
0.90

0.50

against BR1
against WoLF‚àíPHC
against S
against Exp3

0.85

Average Payoff

Average Payoff

0.55

0.15

(b) Chicken: RÃÑ1000
vs. ¬µÃÑ1000
i
i

against BR1
against WoLF‚àíPHC
against S
against Exp3

0.60

0.1

Average Regret

0.45
0.40
0.35
0.30

0.80
0.75
0.70
0.65

0.25
0.60

0.20
0.15
0

0.55
0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45

0

Average Disappointment

0.85

0.50

Average Payoff

Average Payoff

0.90

0.55

0.45
0.40
0.35
0.30
Against BR1
Against WoLF‚àíPHC
Against S
Against Exp3

0.15
0

0.05

0.1

0.15

0.35

0.75
0.70
0.65

0.55

0.2

0

0.05

0.1

0.15

0.2

e‚àíregret

(f) Chicken: EÃÑi1000 vs. ¬µÃÑ1000
i
0.90

Against BR1
Against WoLF‚àíPHC
Against S
Against Exp3

0.80

Average Payoff

Average Payoff

0.3

0.60

Against BR1
Against WoLF‚àíPHC
Against S
Against Exp3

0.50

0.25

0.80

(e) PD: EÃÑi1000 vs. ¬µÃÑ1000
i

0.55

0.2

Against BR1
Against WoLF‚àíPHC
Against S
Against Exp3

e‚àíregret

0.60

0.15

(d) Chicken: DÃÑi1000 vs. ¬µÃÑ1000
i

0.60

0.20

0.1

Average Disappointment

(c) PD: DÃÑi1000 vs. ¬µÃÑ1000
i

0.25

0.05

0.45
0.40
0.35
0.30

0.70
0.60
0.50

0.25
0.40

0.20
0.15
0

0.05

0.1

0.15

0.30
0

0.2

e‚àíregret

0.05

0.1

0.15

0.2

e‚àíregret

(g) PD: EÃÑi50000 vs. ¬µÃÑ50000
i

(h) Chicken: EÃÑi50000 vs. ¬µÃÑ50000
i

Figure 2: Average regret (a‚Äìb), disappointment (c‚Äìd), and e-regret (e‚Äìh) plotted against average
payoffs (¬µÃÑ1000
) for various pairings in the PD and Chicken. Each point is an average of 50 trials.
i
E-regret cannot be computed for BR1 and BR2, so those results are excluded in (e‚Äìh).

118

T OWARDS M INIMIZING D ISAPPOINTMENT

IN

R EPEATED G AMES

‚Ä¢ Compatibility: In self-play, the algorithm achieves at least within Œµ of the payoff of some
Nash equilibrium that is not Pareto dominated by another Nash equilibrium.
‚Ä¢ Safety: Against any associate, the algorithm always receives at least within Œµ of the security
value of the game.
When the set of experts satisfies certain properties, an expert algorithm that is guaranteed to have
no disappointment will also satisfy these desirable performance criteria. Let œÜSafety
, œÜComp
, and
i
i
œÜTargetOpt
be
behavior
rules/algorithms
that
would
satisfy
the
Safety,
Compatibility,
and
Targeted
i
Optimality properties, respectively, if they were always followed. Then, an expert algorithm that
has no disappointment will satisfy the Safety, Compatibility, and Targeted Optimality properties,
respectively, if these experts are in the set of available experts Œ¶i .
Proposition 2.1 Let A be an expert algorithm, let œÜSafety
, œÜComp
, œÜTargetOpt
‚àà Œ¶i , and let A(Œ¶i )
i
i
i
the algorithm that uses A to select experts from Œ¶i . If A is guaranteed to have no disappointment,
then A(Œ¶i ) will satisfy the Safety, Compatibility, and Targeted Optimality performance criteria.
Proof. The proof follows directly from Eq. (5). Against all associates in all possible games, we
know that if A is guaranteed to have no disappointment, then A(Œ¶i )‚Äôs average payoff will be at
least as high in the limit as its security value minus Œµ, since A will do at least as well in the limit as
. Thus, it satisfies the Safety criteria. Similarly,
it would have done if it had always followed œÜSafety
i
in self play, A(Œ¶i ) will perform at least as well in the limit as œÜComp
, and it will do at least as well
i
TargetOpt
as œÜi
against all associates from the targeted set of associates. Hence, it will also meet the
Compatibility and Targeted Optimality performance criteria. 
We argue that it is not difficult to find œÜSafety
, œÜComp
, and œÜTargetOpt
. For example, œÜSafety
i
i
i
i
can be an expert that always plays the agent‚Äôs maximin strategy. œÜComp
can
be
an
expert
that
i
TargetOpt
follows Littman‚Äôs and Stone‚Äôs Godfather strategy (Littman & Stone, 2005). œÜi
could be
any number of algorithms that derive a best response to memory bounded opponents, such as the
algorithm described by Chakraborty and Stone (2010).
Similar arguments can show that an expert algorithm guaranteed to have no disappointment will
also meet other performance criteria, such as the performance criteria advocated by Crandall and
Goodrich (2011). Additionally, suppose that Œ¶i consists of three experts: Expert 1 acts optimally
against associates from behavior class X, Expert 2 acts optimally against associates from behavior
class Y, and Expert 3 acts optimally against associates from behavior class Z. Then, if A is an
expert algorithm guaranteed to have no disappointment, then A(Œ¶i ) will learn to act optimally
when playing against associates from behavior classes X, Y, and Z.
In short, if we can find (1) an expert algorithm that is guaranteed to have no disappointment
and (2) a good set of experts, we will have an agent that performs very well in repeated games.
Given these results, a tempting research agenda is to find an algorithm that always achieves no
disappointment. Unfortunately, unless an agent is omniscient, such a goal is impossible.
Proposition 2.2 Against an unknown associate, an expert algorithm can guarantee an average
disappointment of no less than 1 ‚àí vimm , where vimm = maxœÄi minœÄ‚àíi ui (œÄi , œÄ‚àíi ) is its maximin
(i.e., security) value.
119

C RANDALL

Proof. We adapt an example from de Farias and Megiddo (2003). Let œÉ ‚àó be a particular string of
actions of length 100. Consider agent i playing a repeated PD against an associate (agent ‚àíi) who
has the following strategy. In rounds t ‚â§ 100, always cooperate. For t > 100, always cooperate
if agent i‚Äôs actions matched œÉ ‚àó in all rounds t ‚â§ 100; otherwise, always play the attack policy
attack = arg min
œÄ‚àíi
œÄ‚àíi maxœÄi ui (œÄi , œÄ‚àíi ) (defect). Suppose that agent i has an expert that plays the
string œÉ ‚àó in the first 100 rounds, and then defects in all rounds thereafter. For large T , this best
expert (against this associate) would get an average payoff near 1. But, without omniscience, an
expert algorithm cannot know that it should follow this expert for the first 100 rounds, and therefore
is very unlikely to follow œÉ ‚àó for all t ‚â§ 100. Thus, the maximum payoff it can guarantee itself is no
more than vimm = 0.2. Thus, DÃÑiT ‚â• 1 ‚àí vimm as T ‚Üí ‚àû. 
A less ambitious, but still extremely challenging, research agenda is to find an algorithm that
quickly achieves low disappointment when associating with any member of a target set of associates
across many repeated games. The most common target set of algorithms has been memory-bounded
algorithms (e.g., de Farias & Megiddo, 2004; Chakraborty & Stone, 2010; Arora et al., 2012; CesaBianchi et al., 2013), since theoretical guarantees are easier to establish against such algorithms.
However, we find it more appealing to target a set of associates that is broad enough to cover
many state-of-the-art algorithms published in the literature, which would presumably be the set of
likely associates an agent would face. For example, algorithms from the literature include static
(memory-bounded) algorithms and automata, reinforcement learning algorithms, and expert algorithms. Thus, in this paper, we focus on finding expert algorithms that achieve low disappointment
against associates from these three classes of algorithms.
A theoretical treatment of this aim is extremely challenging and is beyond the scope of this
paper. Rather, as a starting point, we empirically evaluate the disappointment of algorithms against
various static, reinforcement learning, and expert algorithms from the literature. In so doing, we
seek an algorithm that quickly achieves low disappointment against each algorithm we consider.
Let Œò be the set of opponent algorithms considered. Then, the max disappointment of algorithm A
with respect to Œ¶ and Œò after T rounds is
T
T
(Œ∏, Œ¶),
DÃÇA
(Œò, Œ¶) = max DÃÑA
Œ∏‚ààŒò

(7)

T (Œ∏, Œ¶) is the average disappointment of algorithm A against associate Œ∏ ‚àà Œò.
where DÃÑA
Though it is tempting to focus on asymptotic performance (as T ‚Üí ‚àû), interactions repeat no
more than tens or hundreds of times in many realistic scenarios. As such, we seek to identify expert
algorithms that quickly achieve low disappointment against static associates, reinforcement learning
algorithms, and expert algorithms. Thus, we primarily focus on the disappointment achieved by
algorithms over the first 1000 rounds, though we also consider longer-term performance.
In Section 4 we evaluate several existing algorithms with respect to disappointment. Before
doing so, we describe a method for computing a good set of experts for repeated general-sum games.

3. Computing a Set of Experts for Arbitrary Repeated Games
The success of an agent employing an expert algorithm depends on both the expert algorithm‚Äôs ability to select the most effective experts (and, thus, minimize disappointment) and the effectiveness
of the set of experts Œ¶i . Given the importance of Œ¶i , we give considerable attention to computing a
120

T OWARDS M INIMIZING D ISAPPOINTMENT

IN

R EPEATED G AMES

c
d
a 0.33, 0.33 0.67, 1.00
b 1.00, 0.67 0.00, 0.00

c
d
a 1.00, 1.00 0.00, 0.75
b 0.75, 0.00 0.50, 0.50

c
d
a 0.84, 0.33 0.84, 0.00
b 0.00, 1.00 1.00, 0.67

c
d
a 0.00, 0.00 0.00, 1.00
b 1.00, 0.00 0.00, 0.00

(a) Leader

(b) Stag hunt

(c) Security Game

(d) Offset Game

c
d
a 1.00, 1.00 0.00, 0.00
b 0.00, 0.00 0.50, 0.50

c
d
a 0.00, 0.00 0.67, 1.00
b 1.00, 0.67 0.33, 0.33

c
d
a 0.00, 1.00 1.00, 0.67
b 0.33, 0.00 0.67, 0.33

(e) Common Interest

(f) Battle of the Sexes

(g) Tricky Game

a
b
c

d
0, 0
0, 1
1, 0

e
1, 0
0, 0
0, 1

f
0, 1
1, 0
0, 0

(h) Shapley‚Äôs Game

Table 2: Payoff matrices for eight different games.
good set of experts Œ¶i for repeated normal-form games. For each potential associate and game an
agent might encounter, this set of experts should include at least one expert that will perform well.
Littman and Stone (2001) grouped algorithms for repeated games into two classes: leaders
and followers. Leaders are typically effective when associating with follower algorithms, such
as standard reinforcement learning and opponent modeling algorithms. Follower algorithms are
typically more effective against leader strategies and other static algorithms. Thus, in order to have
a high-performing expert for each scenario the agent might encounter, a good set of experts must
contain both leader and follower experts.
3.1 Leader Experts
Based on the premise that the associate will play a best response, leader strategies are designed to
play strategies that cause associates to play their portion of a desirable solution (sometimes called
a targeted pair) (Littman & Stone, 2001). A solution s is a sequence of joint actions that the
agents repeatedly play. Each solution s produces an expected payoff profile v(s) = (vi (s), v‚àíi (s)).
For example, the solution s = <(c, C), (d, C)> in the PD corresponds to the situation in which the
column player always cooperates while the row player alternates between cooperating and defecting.
It produces the expected payoff profile v(s) = (0.8, 0.3) for the payoffs given in Table 1a.
Since it is often unclear which solution an agent should target, we define a set of potential target
solutions ‚Ñ¶. ‚Ñ¶ contains all solutions that satisfy the following three criteria. First, ‚Ñ¶ consists only
of solutions for which each agent‚Äôs expected payoff vi exceeds its maximin value vimm . Second, in
line with Occam‚Äôs razor, ‚Ñ¶ consists only of solutions with sequences of one or two joint actions. Solutions with longer sequences are likely to be too complex for many potential associates to identify,
especially in interactions that last only tens of rounds, so we exclude them. Third, solutions in ‚Ñ¶
must be enforceable. It must be possible to make playing the solution the associate‚Äôs best response.
Formally, when agent i plays strategy œÅi , agent ‚àíi‚Äôs best response is
br‚àíi (œÅti ) = arg max ¬µ‚àíi (œÅi , œÅ‚àíi ).
œÅ‚àíi

(8)

Recall that ¬µ‚àíi (œÅi , œÅ‚àíi ) is ‚àíi‚Äôs average per-round payoff when the strategies œÅi and œÅ‚àíi are played.
The cardinality of ‚Ñ¶ (|‚Ñ¶|) varies from game to game. For example, in the PD, |‚Ñ¶| = 6, while
|‚Ñ¶| = 9 in Shapley‚Äôs Game (Table 2h). Furthermore, in a set of 50 randomly-generated 5-action
games, we found |‚Ñ¶| to vary between 18 and 187.
121

C RANDALL

No.
1
2
3
4

Target solution (s)
<(c, C)>
<(d, C), (c, D)>
<(c, C), (d, D)>
<(d, D)>

v1 (s)
0.60
0.50
0.40
0.20

Strategy
If g2t > 0, play œÄiattack . Otherwise, play own
portion of the current joint action in the
sequence s.

Table 3: Leader experts generated for player 1 in the PD.
Though enforceable, some solutions can only be made an associate‚Äôs best response if the associate conditions its strategy on many previous joint actions. Because of the curse of dimensionality,
most algorithms designed to learn quickly condition their strategy on only a few previous joint actions. Thus, we form a separate leader expert (œÜ) only for those target solutions s ‚àà ‚Ñ¶‚Ä≤ ‚äÜ ‚Ñ¶ for
which br‚àíi (œÜ) requires the associate to remember only a single joint action. These leader experts
incentivize associates to play their portion of the target solution s by punishing deviations from s
using a similar mechanism to those used by previously defined leader strategies (e.g., Littman &
Stone, 2005; Crandall & Goodrich, 2005). When the associate has conformed with s or has not
benefited from deviating from it, the leader expert plays its portion of s. However, if the associate
has benefited from deviating from s, the leader expert plays its attack policy œÄiattack .
t that defines how much its
Formally, each leader expert keeps track of a guilt parameter g‚àíi
1
associate has benefited from deviating from s. Initially, g‚àíi = 0. Subsequently,
t+1
g‚àíi

‚Üê



0
max

t
0, g‚àíi

+ m‚àíi

(at )

t
t
t
 if a‚àíi = b‚àíi and g‚àíi = 0
‚àí v‚àíi + Œ¥t otherwise

(9)

where bt‚àíi is agent ‚àíi‚Äôs current action defined by the target solution s and Œ¥t is a small nonnegative
t = 0 and Œ¥ = 0.0 otherwise. When g t > 0, agent ‚àíi has recently
value. We use Œ¥t = 0.1 if g‚àíi
t
‚àíi
benefited (or at least not been hurt) by deviating from s. To discourage such behavior, the leader
t = 0, the leader expert plays its portion of s.
expert plays its attack policy œÄiattack . When g‚àíi
Table 3 lists the four leader experts generated for the PD. Note that no expert is created for
the target solutions <(c, C), (d, C)> ‚àà ‚Ñ¶ and <(c, C), (c, D)> ‚àà ‚Ñ¶, since these solutions are not
enforceable against an associate that learns a best response conditioned only on the last joint action.
3.2 Follower Experts
Followers learn to play a best response to the estimated strategy of their associate. We consider
three types of follower experts, each of which estimates its associate‚Äôs strategy in a different way.
The first of these experts models its associate using the fictitious play assessment conditioned on
the last joint action played. Let Œ∫t‚àíi (a, a‚àíi ) be the number of times that agent ‚àíi has played a‚àíi
given the previous joint action a up to round t. Then, the estimated probability that agent ‚àíi plays
a‚àíi given the previous joint action a is
t
Œ≥‚àíi
(a, a‚àíi ) = P

Œ∫t‚àíi (a, a‚àíi )
.
t
b‚àíi ‚ààA‚àíi Œ∫‚àíi (a, b‚àíi )

(10)

The expert then computes the automaton that best responds to the agent‚Äôs future discounted reward
t . We refer to this expert as œÜ‚àó .
(we use discount factor Œ≥ = 0.95) given Œ≥‚àíi
i
122

T OWARDS M INIMIZING D ISAPPOINTMENT

No.
1
2

Target solution (s)
(none)
(none)

v1 (s)
¬µ1 (br1 (Œ≥2t ), Œ≥2t )
v1mm = 0.20

3
4
5
6
7
8

<(c, C), (d, C)>
<(c, C)>
<(d, C), (c, D)>
<(c, C), (d, D)>
<(c, C), (c, D)>
<(d, D)>

0.80
0.60
0.50
0.40
0.30
0.20

IN

R EPEATED G AMES

Strategy
br1 (Œ≥2t )
œÄ1mm
If at‚àí1 is in s, play own portion of the next
joint action in s. Otherwise, randomly select a
joint action from s and play own portion of that
joint action.

Table 4: Follower experts generated for player 1 in the PD.
A second follower expert, called œÜmm
i , assumes that its associate is trying to exploit it. Thus, its
best response is to play its maximin strategy (or policy), which is given by
œÄimm = arg max min ui (œÄi , œÄ‚àíi ).
œÄi

œÄ‚àíi

(11)

Finally, the associate could also be using a leader strategy, such as those computed in Section 3.1. Thus, we include in our set of experts a follower expert for each s ‚àà ‚Ñ¶. These experts
always play their part of s. If a joint action in the solution was not played in the previous round,
these experts randomly select a joint action from the solution sequence and play the agent‚Äôs corresponding action.
Table 4 lists the eight follower experts generated for the PD by this method.
3.3 Set of Experts
The set of experts Œ¶i used by the expert algorithms in the remainder of this paper consists of the
follower and leader experts just described. In most scenarios we have encountered, at least one of
these experts is capable of performing effectively. If an associate employs a follower algorithm, an
expert algorithm can select any number of leader experts from Œ¶i that could induce desirable behavior from the associate. Similarly, if the associate employs a leader algorithm, such as Godfather or
Bully (Littman & Stone, 2001), the expert algorithm can select the corresponding follower expert.
Additionally, the diversity of the experts in Œ¶i is such that a good expert algorithm should be able
to obtain high payoffs against other expert algorithms, including those that select from a similar set
of experts. In this case, the expert algorithms must negotiate follower and leader roles.
An illustration of the performance of these experts against three different associates in several
repeated games is provided in Appendix B. In each example, the set of experts contains at least
one high-performing expert. However, these results also illustrate that identifying a best expert
during run time can be extremely difficult. Sometimes the best performing expert must be followed
consistently for many rounds before it produces high payoffs.

4. Results ‚Äì Existing Expert Algorithms
Existing expert algorithms have typically been evaluated in terms of regret. In this section, we evaluate several of these algorithms in repeated normal-form games in terms of disappointment. Specifically, we analyze the average disappointment of four existing expert algorithms against twelve
123

C RANDALL

different algorithms across ten repeated games. Recall that our goal is to find an expert algorithm
that quickly achieves and maintains low disappointment against static algorithms, reinforcement
learning algorithms, and other expert algorithms.
The four expert algorithms are Exp3, UCB1, EEE, and S. Exp3 and UCB1 are well-known expert algorithms with well-defined regret bounds in multi-armed and adversarial multi-armed bandit
problems, respectively. Both of these algorithms have been shown to perform effectively in some repeated games (Bouzy & Metivier, 2010). EEE is an Œµ-greedy expert algorithm designed for repeated
games played against learning associates. As T ‚Üí ‚àû, it has been shown to have no e-regret (de
Farias & Megiddo, 2003, 2004). S is an aspiration-based algorithm which has been shown to perform well as an expert algorithm (Bouzy, Metivier, & Pellier, 2011), though it was not originally
designed as such. In the interest of space, we omit detailed overviews of these algorithms. Instead,
we refer the reader to Appendix A, which provides references to descriptions and analysis of these
algorithms, and also specifies the parameter values used to generate the results in this paper.
We compare the average disappointment of these four expert algorithms when paired against
twelve representative algorithms: four static algorithms (A0, Random, Godfather, and Bully), four
reinforcement learning algorithms (BR1, BR2, Q-learning, and WoLF-PHC), and each other. Implementation details for each of these twelve algorithms are also supplied in Appendix A. Comparisons
are made across the ten games shown in Tables 1 and 2. These are well-studied games from the literature, each representing a different challenge. To be successful in all of these games, an algorithm
must be able to learn to make and accept profitable compromises in many different situations.
The average disappointment across all games for each pairing is shown in Figure 3. The figure
shows that, over the first 1000 rounds, S typically has lower average disappointment against each
associate than the other three expert algorithms. Additionally, despite its popularity and theoretical
properties, Exp3 performs the worst of the four expert algorithms against each associate.
Results vary somewhat by associate. While all four expert algorithms eventually obtain low
disappointment against each static algorithm, Figures 3a‚Äì3d show that only S reaches an average
disappointment of less than 0.05 within 1000 episodes against all four static associates. The other
expert algorithms are unable to do so, in large part due to the relatively large number of experts
in Œ¶i . S‚Äôs mechanism for selecting experts allows it to find the best expert faster than the other
algorithms. As an exception, S is not as effective against Random, especially in the long run, as S
sometimes never does learn to play the best expert. For example, S does not learn to play a best
response against Random in the PD, while the other three algorithms eventually do.
Because reinforcement learning algorithms adapt over time, achieving low average disappointment against these associates is more difficult than against static algorithms. This statement is
confirmed by the performance of the four expert algorithms against BR1, BR2, WoLF-PHC, and
Q-learning (Figures 3e‚Äì3h). Against these associates, none of the four expert algorithms achieves
low disappointment during the first 1000 rounds. Their average disappointment quickly increases
over the first 50-200 rounds, and then slowly decreases or plateaus thereafter.
The four expert algorithms also have high disappointment against each other (Figures 3i‚Äì3l). In
self play, S manages to achieve an average disappointment of about 0.05, but none of the other three
algorithms does so over the first 1000 rounds against any expert algorithm. In fact, against Exp3,
the average disappointment of each expert algorithm increases throughout the first 1000 rounds.
In short, none of the four expert algorithms quickly achieves low disappointment against static
algorithms, reinforcement learning algorithms, and expert algorithms. Thus, in the next section, we
describe a new meta-algorithm designed to enhance the performance of existing expert algorithms.
124

T OWARDS M INIMIZING D ISAPPOINTMENT

0.2
0.15
0.1
0.05

200

400

600

800

0.25
0.2
0.15
0.1
0.05
0
0

1000

0.35
EEE
S
UCB1
Exp3

0.3

200

400

T

0.1
0.05

600

800

0.2
0.15
0.1

EEE
S
UCB1
Exp3

0.05
0
0

1000

200

400

0.1

EEE
S
UCB1
Exp3

0.05

600

800

EEE
S
UCB1
Exp3
200

400

0.2

0.1
0.05

200

400

600

800

0.2
0.15
0.1
0.05

200

400

0.1
0.05

600

T

(j) Against UCB1

800

1000

800

1000

(i) Against Exp3
0.35

0.3
0.25
0.2
0.15
0.1

EEE
S
UCB1
Exp3

0.05
0
0

600

T

200

400

600

T

(k) Against EEE

800

1000

Average Disappointment

0.15

Average Disappointment

0.2

1000

EEE
S
UCB1
Exp3

0.25

0
0

1000

0.35

0.25

800

0.3

(h) Against Q-learning
EEE
S
UCB1
Exp3

0.3

600

0.35
EEE
S
UCB1
Exp3

T

0.35

400

0.1
0.05

(f) Against BR2

0.15

(g) Against WoLF-PHC

200

0.2

T

0.25

0
0

1000

1000

0.15

0
0

1000

Average Disappointment

0.2
0.15

Average Disappointment

Average Disappointment

800

0.3

T

Average Disappointment

600

0.35

0.25

800

0.3

(e) Against BR1

0.3

600

0.25

T

0.35

0
0

400

0.35

0.25

(d) Against Godfather

400

200

(c) Against Bully

0.3

T

200

0.1
0.05

T

Average Disappointment

0.2
0.15

Average Disappointment

Average Disappointment

0.25

400

0.2
0.15

0
0

1000

0.35
EEE
S
UCB1
Exp3

0.3

200

0.25

(b) Against Random

0.35

0
0

800

EEE
S
UCB1
Exp3

0.3

T

(a) Against A0

0
0

600

Average Disappointment

0.25

0
0

R EPEATED G AMES

0.35
EEE
S
UCB1
Exp3

0.3

Average Disappointment

Average Disappointment

0.35

IN

EEE
S
UCB1
Exp3

0.3
0.25
0.2
0.15
0.1
0.05
0
0

200

400

600

800

1000

T

(l) Against S

Figure 3: The average disappointment DÃÑ T over time of four expert algorithms against twelve associates. Results are an average of 50 trials in each of the ten selected games.

125

C RANDALL

In Section 6, we demonstrate that this meta-algorithm improves Exp3, UCB1, EEE, and S so that
they consistently achieve much lower disappointment against these same associates.

5. Enhancing Existing Expert Algorithms
The failure of these expert algorithms to consistently achieve low disappointment against adapting
agents appears to be tied to the algorithms‚Äô exploration strategies. Early in the game, these expert
algorithms tend to spend many rounds following ineffective experts as they seek to determine from
experience which experts are most effective. As a result, they receive low average payoffs in early
rounds of the game. Furthermore, the frequent changes in behavior caused by cycling through many
experts are incoherent to an outsider. Thus, associates are unlikely to determine how to coordinate
behavior or strike mutually beneficial compromises. Against adaptive associates whose internal
models are conditioned somewhat on their associate‚Äôs behavior, this process often leads to low
payoffs (and, hence, high disappointment) in both the short and the long term. In this section, we
describe a rather simple meta-algorithm designed to overcome this deficiency.
5.1 A New Meta-Algorithm
The purpose of the meta-algorithm is to help the expert algorithm explore the effectiveness of its set
of experts Œ¶i more effectively. It does this by computing the highest expected payoff (or potential)
of each expert, and then supplying the expert algorithm with the subset of experts whose potential
meets some performance threshold. Experts with lower potential are only followed once experts
with higher potentials have demonstrated an inability to meet their potentials.
The performance threshold in each round t is determined using aspiration learning (Karandikar,
Mookherjee, R., & Vega-Redondo, 1998; Stimpson, Goodrich, & Walters, 2001; Chasparis, Shamma,
& Arapostathis, 2010). In aspiration learning, agent i maintains an aspiration Œ±ti . In our algorithm,
Œ±1i is initialized to the potential (see Section 5.3) of the expert with the highest potential. Formally,
let zit (œÜ) denote the potential of expert œÜ in round t. Then,
Œ±1i = max zi1 (œÜ).
œÜ‚ààŒ¶i

(12)

As with S (Appendix A), each round that a new expert is selected, Œ±ti is updated as follows:
Œ±ti = ŒªœÑ Œ±t‚àíœÑ
+ (1 ‚àí ŒªœÑ )rÃÑit ,
i

(13)

where rÃÑit is the average payoff received by agent i in the last œÑ rounds, œÑ is the number of consecutive
rounds the expert was followed, and Œª ‚àà [0, 1] is a learning rate (we use Œª = 0.99).
The performance threshold in round t is the agent‚Äôs minimum aspiration level Œ±ki over some
time interval k ‚àà [Œ∂, t], where 1 ‚â§ Œ∂ ‚â§ t. Each expert whose potential meets or exceeds this
performance threshold is considered for selection in round t; the other experts are not.4 Formally,
in round t, experts are selected from the set
Œ¶‚Ä≤i (t) = {œÜ ‚àà Œ¶i : zit (œÜ) ‚â• min Œ±ki }.
k‚àà[Œ∂,t]

(14)

When Œ¶‚Ä≤i (t) as defined in Eq. (14) is empty, a default expert is added to Œ¶‚Ä≤i (t). For the set of experts
defined in Section 3, we add the expert that plays a best response to the fictitious-play assessment
conditioned on the previous joint action. That is, Œ¶‚Ä≤i (t) = {œÜ‚àói } for such cases.
4. As before, disappointment is always computed using the full set of experts Œ¶i .

126

T OWARDS M INIMIZING D ISAPPOINTMENT

IN

R EPEATED G AMES

Algorithm 1 A meta-algorithm (for agent i) to enhance expert algorithms.
Input:
A (the expert algorithm), Œ¶i (the set of experts), and M (the payoff matrix)
Initialize:
t=1
Compute zit (œÜ) for each œÜ ‚àà Œ¶i
Initialize Œ±ti = maxœÜ‚ààŒ¶i zit (œÜ)
repeat
Compute Œ¶‚Ä≤i (t) = {œÜ ‚àà Œ¶i : zit (œÜ) ‚â• minœÑ ‚àà[Œ∂,t] Œ±œÑi }, where 0 ‚â§ Œ∂ < t
Execute (and update) A(Œ¶‚Ä≤i (t)) for œÑ rounds, where œÑ is specified by A. Observer rÃÑit+œÑ .
t= t+œÑ
Update Œ±ti = ŒªœÑ Œ±t‚àíœÑ
+ (1 ‚àí ŒªœÑ )rÃÑit
i
t
Compute zi (œÜ) for each œÜ ‚àà Œ¶i
until Game Over

The complete meta-algorithm is stated in Algorithm 1. We make two observations. First, the
meta-algorithm embodies the optimism-in-uncertainty principle (Brafman & Tennenholtz, 2003).
The aspiration level is initially set high, and each expert is presumed to be able to meet its highest
potential. Only after experts fail to meet their highest potentials (which causes the aspiration level
to fall; Eq. 13) does the algorithm consider selecting experts with lower potential. Second, while
aspiration updates are similar in nature to previous work on aspiration learning (Karandikar et al.,
1998; Stimpson et al., 2001; Chasparis et al., 2010), the aspiration level is used differently in this
meta-algorithm. Rather than use Œ±ti to determine whether an action or expert should be repeated in
the next round, we use Œ±ti to prune the set of selectable experts.
5.2 Properties of the Reduced Set Œ¶‚Ä≤i (t)
Selecting experts from Œ¶‚Ä≤i (t) rather than from Œ¶i leaves open the possibility that Œ¶‚Ä≤i (t) might not
contain the best expert. However, for certain parameter settings, our meta-algorithm will either
obtain average payoffs no less than the best expert in the limit, or the best expert will be contained
in œÜ‚Ä≤i (t) for all t > œÑ . Let vi‚àó (œÜ) denote the highest possible average per-round payoff that agent i
could ever obtain if it were to always follow expert œÜ, let œÜ‚Ä≤ denote the best expert for the game and
associate in question, and let ¬µti be the average payoff obtained by agent i up to time t. Then we
have the following proposition.
Proposition 5.1 If Œ∂ = 1 and ‚àÄœÜ ‚àà Œ¶i , zit (œÜ) ‚â• vi‚àó (œÜ), then one of the following must hold:
Condition 1: limt‚Üí‚àû ¬µti ‚â• zit (œÜ‚Ä≤ )
Condition 2: ‚àÉœÑ, œÜ‚Ä≤ ‚àà Œ¶‚Ä≤i (t) for all t ‚â• œÑ
Condition 1 equates with the expert algorithm having no disappointment, while Condition 2 says
that œÜ‚Ä≤ will eventually enter (and then perpetually remain in) Œ¶‚Ä≤i (t). The proof of the proposition is
straightforward. Since the aspiration level Œ±ti is the fading average payoff of the agent, it must at
some time œÑ fall below zit (œÜ‚Ä≤ ) if its average payoff ¬µti is perpetually below zit (œÜ‚Ä≤ ). In which case, in
accordance with Eq. (14), œÜ‚Ä≤ would be in Œ¶‚Ä≤i (t) thereafter.
As is typical in aspiration learning (Karandikar et al., 1998), we use Œ∂ = t (i.e., the performance
threshold is Œ±ti ) in generating the results shown in the next section. This means that our implemen127

C RANDALL

tation does not technically satisfy the conditions of Proposition 5.1. However, we have observed
that, in practice, the algorithm achieves similar results for both Œ∂ = 1 and Œ∂ = t.
5.3 Computing Potential
Most strategies and learning processes have a target value, such as the value of an equilibrium
solution or the current expected value of the learning process. While not always conforming with
the pre-condition of Proposition 5.1, such target values are often sufficient for determining the
potential of experts in practice. We demonstrate how to define the potential of experts using our set
of experts, which was defined in Section 3.
Let Œ¶lead
‚äÜ Œ¶i denote the set of leader experts in Œ¶i . Under the assumption of a rational
i
(follower) associate, the highest expected payoff (or potential) that we can reasonably expect each
leader expert œÜ ‚àà Œ¶lead
to obtain is the expected per-round payoff the agent will receive when its
i
opponent plays a best response to its strategy. Formally, for all œÜ ‚àà Œ¶lead
and for all t,
i
zit (œÜ) = ¬µi (œÜ, br‚àíi (œÜ)).

(15)

Let Œ¶follow
‚äÜ Œ¶i denote the set of follower experts in Œ¶i excluding the maximin and besti
response experts (œÜmm
and œÜ‚àói , respectively). If the associate appears to be playing its portion of
i
the target solution corresponding to expert œÜ ‚àà Œ¶follow
, then œÜ‚Äôs potential is the agent‚Äôs expected
i
payoff when the target solution corresponding to that expert is played. Let st‚àíi denote the observed
strategy employed by agent ‚àíi in round t. Then,

¬µi (bri (st‚àíi ), st‚àíi ) if œÜi ‚àà bri (st‚àíi )
t
zi (œÜ) ‚Üê
(16)
0
otherwise
To determine which strategies its associate could be playing, agent i models agent ‚àíi‚Äôs actions
given the previous joint action a. Since agent ‚àíi could change st‚àíi at any time, only a partial
estimate of st‚àíi may be available. The agent remembers the last round k that agent ‚àíi changed its
action given any previous joint action a ‚àà A. All actions taken from round k onward define st‚àíi .
Formally, agent ‚àíi‚Äôs estimated strategy given a is
 j
a‚àíi if ‚àÉj ‚àà [k, t) : aj = a
st‚àíi (a) ‚Üê
(17)
‚àß
otherwise
If agent i‚Äôs estimate of st‚àíi is consistent with the leader strategy corresponding to œÜi ‚àà Œ¶follow
, then
i
t
the agent assumes it is playing that leader strategy when computing zi (œÜ).
mm and z t (œÜ‚àó ) =
The potentials of the maximin and best-response experts are zit (œÜmm
i ) = vi
i i
t
t
¬µi (bri (Œ≥‚àíi ), Œ≥‚àíi ), respectively.
5.4 Safety
Safety, the guarantee that expected average payoffs will not be substantially below the maximin
value vimm , is perhaps the oldest objective in repeated games (Fudenberg & Levine, 1998). An
expert algorithm that is guaranteed to have no regret is guaranteed to have safety if some œÜ ‚àà Œ¶i
is safe. However, for those expert algorithms that do not have well-defined regret bounds, we add
a mechanism to our meta-algorithm that ensures safety. We call this mechanism the safety override
since it overrides Eq. (14) under certain conditions.
128

T OWARDS M INIMIZING D ISAPPOINTMENT

IN

R EPEATED G AMES

The safety override is adopted from a security mechanism described by Crandall and Goodrich
(2011). If the sum of the agent‚Äôs payoffs is ever less than a constant Ci below
have
PTwhat it would
œÑ
achieved had it always received its maximin value (i.e., if ‚àÉT ‚â§ t such that œÑ =1 mi (a ) + Ci <
T vimm ), then Œ¶‚Ä≤i (t) = {œÜmm
i }. This guarantees that the agent‚Äôs expected average payoff will be no
mm
less than vi as t ‚Üí ‚àû, regardless of the game or associate. The proof of safety provided by this
mechanism is given by Crandall and Goodrich (2011). We used Ci = 100 in our implementation.
5.5 Best Response
Previous work has shown the value of properly balancing optimistic, best response, and secure
attitudes in repeated games (Crandall & Goodrich, 2011). Eqs. (12‚Äì14) induce an optimistic attitude, while the safety override is a secure attitude. Finally, we add a best-response override. The
algorithm sets Œ¶‚Ä≤i (t) = {œÜ‚àói } when the following two conditions are met:
1. œÜ‚àói ‚àà Œ¶‚Ä≤i (t); see Eq. (14) in conjunction with the safety override.
2. The historical average per-round payoff for playing œÜ‚àói is as high as that of any other expert.
Formally, let xti (œÜ) be the weighted average per-round payoff5 received by agent i in each
round that it has followed expert œÜ up to round t. Then, ‚àÄœÜ ‚àà Œ¶‚Ä≤i (t), xti (œÜ‚àói ) ‚â• xti (œÜ).
This override is of the most use for algorithms like S, which learn effectively against many algorithms, but sometimes do not learn a best response against static agents (such as Random).

6. Results ‚Äì Enhanced Expert Algorithms
We enhanced the four expert algorithms evaluated in Section 4 with the meta-algorithm. We call
the enhanced versions of these algorithms Exp3++, UCB++, EEE++, and S++, respectively. These
algorithms are identical to the original algorithms except that they select experts from Œ¶‚Ä≤i (t) rather
than Œ¶i . We first evaluate whether these expert algorithms consistently achieve low disappointment
against the same twelve associates as before. We also compare the payoffs of these algorithms to
top-performing algorithms from the literature in both perfect and imperfect information settings.
6.1 Against Static, Reinforcement Learning, and Expert Algorithms
The average disappointment of the enhanced and original algorithms against all twelve associates
across all ten games is shown in Figure 4. Each enhanced algorithm has substantially less average
disappointment than the original algorithm. The enhanced algorithms quickly reach low levels of
disappointment (Figure 4a). By 1000 rounds, the average disappointment of each of the enhanced
algorithms is below 0.05 (Figure 4b), which is substantially less than the original algorithms. Similar improvements are present in terms of max disappointment (Figure 5).
The meta-algorithm consistently produces substantial decreases in disappointment against all
associates and in all ten games (not shown). Figure 6 shows the average disappointment of EEE++
and Exp3++ against each associate across all games. Against static associates (Figures 6a‚Äì6d),
EEE++ and Exp3++ both achieve very low average disappointment well before 1000 rounds. The
meta-algorithm produces even greater decreases in average disappointment against the reinforcement learning algorithms (Figures 6e‚Äì6f) and the expert algorithms (Figures 6i‚Äì6l). The algorithms
5. Initially, x1i (œÜ) = 1. xti (œÜ) is updated after each round that œÜ is followed: xti (œÜ) = Œ≤it xt‚àí1
(œÜ) + (1 ‚àí Œ≤it )mi (at ),
i
where Œ≤it = max(1/Œ∫ti (œÜ), 2(1 ‚àí Œª)) and Œ∫ti (œÜ) is the number of times agent i has played œÜ up to round t.

129

0.2
EEE
S
UCB1
Exp3
EEE++
S++
UCB1++
Exp3++

0.15

0.1

0.05

0
0

50

100

150

200

250

300

350

Average Disappointment (T = 1000)

Average Disappointment

C RANDALL

0.15

0.1

0.05

0

T

(a) Average disappointment over time

Original
++

0.2

Exp3

UCB1

EEE

S

(b) Average disappointment (T=1000)

Figure 4: Average disappointment DÃÑ T across all selected games and associates.

0.3

EEE
S
UCB1
Exp3
EEE++
S++
UCB1++
Exp3++

0.25
0.2
0.15
0.1
0.05
0
0

50

100

150

200

250

300

350

Max Disappointment (T = 1000)

Max Disappointment

0.35

0.35

0.25
0.2
0.15
0.1
0.05
0

T
(a) Max disappointment over time

Original
++

0.3

Exp3

UCB1

EEE

S

(b) Max disappointment (T=1000)

Figure 5: Max disappointment DÃÇ T (Eq. 7).

enhanced by the meta-algorithm also achieve higher payoffs in self play than do the original algorithms (Figure 7).
One interesting exception to the previously stated trend is that EEE performs better against
Bully over the first 100 rounds than does EEE++ (Figure 6c). Against Bully, the best-performing
experts (see Figure 11 in Appendix B) tend to not have high potential in some games, while experts
with high potential often do not achieve high payoffs well. Hence, it takes many rounds before
the agent‚Äôs aspiration level falls far enough for the best expert to be in Œ¶‚Ä≤i (t). This causes EEE++
to have higher disappointment in early rounds. Once the best expert appears in Œ¶‚Ä≤i (t), its average
disappointment quickly decreases.
Finally, Figure 8 shows the average payoffs obtained by the algorithms against these associates
over 50,000 rounds. Even after 50,000 rounds, the enhanced expert algorithms all outperform the
original algorithms. While UCB1 has the best performance of the four original algorithms after
50,000, all the enhanced algorithms have substantially higher payoffs. Thus, not only do the enhancements improve the algorithms in the near term, but also in the long term.
130

T OWARDS M INIMIZING D ISAPPOINTMENT

0.2
0.15
0.1
0.05

200

400

600

800

0.25
0.2
0.15
0.1
0.05
0
0

1000

0.35
EEE
EEE++
Exp3
Exp3++

0.3

200

400

T

0.1
0.05

600

800

0.2
0.15
0.1
0.05
0
0

1000

200

400

0.1
0.05

600

800

0.1
0.05

200

400

0.2

0.1
0.05

200

400

600

800

0.2
0.15
0.1
0.05

200

400

0.15
0.1
0.05

600

T

(j) Against UCB1

800

1000

0.25

800

1000

(i) Against Exp3
0.35

EEE
EEE++
Exp3
Exp3++

0.2
0.15
0.1
0.05
0
0

600

T

Average Disappointment

0.2

Average Disappointment

0.25

1000

EEE
EEE++
Exp3
Exp3++

0.25

0
0

1000

0.35
0.3

800

0.3

(h) Against Q-learning
EEE
EEE++
Exp3
Exp3++

0.3

600

0.35
EEE
EEE++
Exp3
Exp3++

T

0.35

400

0.2

(f) Against BR2

0.15

(g) Against WoLF-PHC

200

EEE
EEE++
Exp3
Exp3++

T

0.25

0
0

1000

1000

0.15

0
0

1000

Average Disappointment

0.2
0.15

Average Disappointment

Average Disappointment

800

0.3

T

Average Disappointment

600

0.35

0.25

800

0.25

(e) Against BR1
EEE
EEE++
Exp3
Exp3++

0.3

600

0.3

T

0.35

0
0

400

0.35
EEE
EEE++
Exp3
Exp3++

0.25

(d) Against Godfather

400

200

(c) Against Bully

0.3

T

200

0.1
0.05

T

Average Disappointment

0.2
0.15

Average Disappointment

Average Disappointment

0.25

400

0.2
0.15

0
0

1000

0.35
EEE
EEE++
Exp3
Exp3++

0.3

200

0.25

(b) Against Random

0.35

0
0

800

EEE
EEE++
Exp3
Exp3++

0.3

T

(a) Against A0

0
0

600

Average Disappointment

0.25

0
0

R EPEATED G AMES

0.35
EEE
EEE++
Exp3
Exp3++

0.3

Average Disappointment

Average Disappointment

0.35

IN

200

400

600

T

(k) Against EEE

800

1000

EEE
EEE++
Exp3
Exp3++

0.3
0.25
0.2
0.15
0.1
0.05
0
0

200

400

600

800

1000

T

(l) Against S

Figure 6: Average disappointment DÃÑ T over time against each associate across the selected games.
Results are an average of 50 trials in each of the ten selected games.

131

C RANDALL

Average Payoff (T = 1000)

0.9
Original
++
0.8
0.7
0.6
0.5
0.4
0.3

Exp3

UCB1

EEE

S

Figure 7: Average payoffs in self play over 1000 rounds across all selected games.

Average Payoff

0.8
EEE
S
UCB1
Exp3
EEE++
S++
UCB1++
Exp3++

0.75

0.7

0.65

0.6
0

10,000

20,000

30,000

40,000

50,000

T

Figure 8: Average payoffs over time across all selected games and associates.

6.2 Why the Meta-Algorithm Works
The meta-algorithm has two different components: (1) a method for distinguishing experts that
are likely to be successful from those that are not and (2) best response and safety overrides. The
combined impact of the overrides on the max disappointment of the enhanced algorithms is shown
in Figure 9. While the overrides typically lower the algorithms‚Äô max disappointment by a small
margin, only in a small number of cases (e.g., S against static agents) are the overrides responsible
for substantial improvements. In many scenarios, the overrides are not invoked.
Thus, the meta-algorithm improves the original algorithms primarily via its mechanism for distinguishing successful experts from unsuccessful experts. The original expert algorithms tend to
spend many rounds following ineffective experts as they seek to determine from experience which
experts are most effective. The resulting frequent changes in behavior caused by cycling through
many experts are incoherent to associates, making it difficult for the agents to coordinate behavior
or strike mutually beneficial compromises. On the other hand, the enhanced algorithms select from
fewer experts in early rounds of the game, which produces more predictable behavior that associates
can more easily model and adapt to. This, in turn, allows them to quickly find mutually beneficial
compromises, which lead to higher payoffs both in the short and the long term.
132

Original
+ (no overrides)
++

0.2
0.15
0.1
0.05
0

Exp3

UCB1

EEE

S

(a) Against static algorithms

IN

0.3
Original
+ (no overrides)
++

0.25
0.2
0.15
0.1
0.05
0

Exp3

UCB1

EEE

S

(b) Against RL algorithms

R EPEATED G AMES

Max Disappointment (T = 1000)

0.3
0.25

Max Disappointment (T = 1000)

Max Disappointment (T = 1000)

T OWARDS M INIMIZING D ISAPPOINTMENT

0.3
Original
+ (no overrides)
++

0.25
0.2
0.15
0.1
0.05
0

Exp3

UCB1

EEE

S

(c) Against expert algorithms

Figure 9: Max disappointment DÃÇ 1000 after 1000 rounds against each class of associates.

6.3 Comparison to Top-Performing Algorithms
We have shown that our meta-algorithm helps expert algorithms quickly achieve and maintain low
disappointment against a variety of static, reinforcement learning, and expert algorithms across
many repeated games. To further illustrate the value of this contribution, we now compare the average payoffs of these enhanced algorithms to that of top-performing algorithms in repeated games.
To compare our algorithms with state-of-the-art learning algorithms, we ran several round-robin
tournaments involving eight algorithms: S++, EEE++, and six algorithms from the literature: MQubed, Manipulator-Bully, Manipulator-Godfather, BR1, Godfather, and Bully (see Appendix A).
These algorithms were chosen due to their elite attributes. For example, Bully and Godfather are
leader algorithms with well-understood equilibrium characteristics (Littman & Stone, 2001, 2005).
These algorithms precompute desirable behaviors, and are good standards of performance in early
rounds of repeated games. M-Qubed is a reinforcement learning algorithm that has demonstrated
superior asymptotic performance in several empirical studies (Crandall & Goodrich, 2011; Bouzy
& Metivier, 2010). This makes it a good standard of comparison for long-term (or asymptotic)
performance. Manipulator combines Godfather or Bully with BR1 and the maximin strategy œÄimm to
provide theoretical guarantees with respect to targeted optimality, safety, and compatibility (Powers
& Shoham, 2005a).
To this point, we have assumed perfect information, wherein each player has perfect knowledge
of the other player‚Äôs payoffs. We now relax this assumption and consider scenarios in which the
players have normally distributed errors in their assessments of the other player‚Äôs payoffs. Thus, we
conducted three separate round-robin tournaments: one with perfect information (No Noise), and
two with different sizes of errors in payoff assessment (œÉ = 0.15 and œÉ = 0.30, respectively).
In each tournament, each algorithm was paired with itself and the other seven algorithms in
50 random 3-action repeated games. Each game consisted of 50,000 rounds. We compare the
algorithms by their average per-round payoff over each of these eight pairings in both the short term
(over the first 100 and 1000 rounds) and asymptotically (over the last 1000 rounds). While random
games tend to produce less variation in average payoffs than do selected games6 , we use random
games as a guard against overfitting the selected games.
The results of the tournaments are summarized in Figure 10.
6. One comparison of 16 algorithms showed just a 0.07 difference in the average payoffs between the best- and worstperforming algorithms (Crandall & Goodrich, 2011)

133

C RANDALL

Average Payoff

0.85
0.8
S++
EEE++
M‚àíQubed
BR1
Bully
Godfather
Manipulator‚àíBully
Manipulator‚àíGF

0.75
0.7
0.65
0.6
0.55

No Noise

Noise: œÉ = 0.15 Noise: œÉ = 0.30

(a) Averaged over the first 100 rounds

Average Payoff

0.85
0.8
S++
EEE++
M‚àíQubed
BR1
Bully
Godfather
Manipulator‚àíBully
Manipulator‚àíGF

0.75
0.7
0.65
0.6
0.55

No Noise

Noise: œÉ = 0.15 Noise: œÉ = 0.30

(b) Averaged over the first 1000 rounds

Average Payoff

0.85
0.8
S++
EEE++
M‚àíQubed
BR1
Bully
Godfather
Manipulator‚àíBully
Manipulator‚àíGF

0.75
0.7
0.65
0.6
0.55

No Noise

Noise: œÉ = 0.15 Noise: œÉ = 0.30

(c) Averaged over rounds 49,001 ‚Äì 50,000

Figure 10: Average payoffs in perfect (No Noise) and imperfect (Noise: œÉ = 0.15 and œÉ = 0.30,
respectively) information round-robin tournaments involving 50 random 3-action games.

134

T OWARDS M INIMIZING D ISAPPOINTMENT

IN

R EPEATED G AMES

6.3.1 P ERFECT I NFORMATION
Unsurprisingly, Godfather and Manipulator-GF (which are identical in the first 100 rounds) had the
highest average payoffs over the first 100 rounds (Figures 10a). However, the average payoffs of
S++ and EEE++ under perfect information were not far behind. T-tests show that the difference in
average performance between S++ and Godfather was not statistically significant after 100 rounds
(p = 0.194), though the difference between EEE++ and Godfather was (p = 0.004). The payoffs of
the other algorithms were substantially lower. By 1000 rounds, both S++ and EEE++ outperformed
all the other six algorithms under perfect information (Figure 10b). Through 1000 rounds, the difference between S++ and EEE++ was not statistically significant, though the differences between S++
and each of the other six algorithms was (p < 0.010). Thus, the short-term performances of S++
and EEE++ were very favorable compared with these other algorithms under perfect information.
The long-term performance of S++ and EEE++ matches that of M-Qubed in these perfect information games (Figure 10c). Given M-Qubed‚Äôs top asymptotic performance in previous studies
(Crandall & Goodrich, 2011; Bouzy & Metivier, 2010), these results speak to the effectiveness of
the enhanced algorithms.
6.3.2 I MPERFECT I NFORMATION
Both Godfather and Bully compute equilibrium strategies based on knowledge of the payoffs of
associates. By using precomputed strategies, these algorithms quickly achieve high payoffs. MQubed and BR1, on the other hand, seek to learn effective behaviors from experience. While MQubed is eventually quite effective, its learning processes often takes many rounds. S++, EEE++,
Manipulator-Godfather, and Manipulator-Bully each do some of both. They combine the computation of equilibria before the game begins with learning from experience. As a result, they tend to
perform effectively both in the short-term and the long-term under perfect information.
However, since computing (and agreeing upon) an equilibrium sometimes requires perfect information, these algorithms are potentially limited given that perfect knowledge of an associate‚Äôs
payoffs is uncommon. However, it is not uncommon for agents to have estimates of the payoffs of
others, though sometimes these estimates are in error. We model such scenarios in the imperfect
information tournaments. The results of these tournaments are also shown in Figure 10.
The figure shows that in both early and later rounds of the game, Godfather, Bully, ManipulatorGodfather, and Manipulator-Bully are all negatively impacted by errors in their assessments of
their associates‚Äô payoffs. Their average payoffs fall substantially (and statistically significantly) for
both medium errors in assessment (œÉ = 0.15) and large errors in assessment (œÉ = 0.30). On the
other hand, S++ and EEE++ are only slightly affected by both moderate and high errors in their
assessments. Thus, they continue to perform on par with M-Qubed asymptotically (Figure 10c),
while maintaining the highest performance in early rounds of the game.
While the Manipulator algorithms, S++, and EEE++ all utilize precomputed strategies and learn
from their experiences, S++ and EEE++ are not substantially affected by imperfect information,
while the Manipulator algorithms are. There are two primary differences between the Manipulator
algorithms and the enhanced expert algorithms that cause this difference. First, the Manipulator algorithms each compute a single equilibrium strategy, while the enhanced expert algorithms compute
many equilibria strategies (the various experts). Second, the Manipulator algorithms execute experts
serially. These algorithms first execute their respective leader strategy. If the respective leader strategy fails to produce desired payoffs, they switch to following BR1, and so on. On the other hand,
135

C RANDALL

S++ and EEE++ continue to evaluate multiple experts (essentially in parallel). As a results, these
algorithms are much more robust to errors in the assessments of their associate‚Äôs payoffs.

7. Conclusions and Discussion
In this paper, we introduced a new metric, called disappointment, for evaluating expert algorithms
in repeated games. Disappointment is similar to regret, except that disappointment is not built on
the assumption that an agent‚Äôs actions do not influence its associates‚Äô future behavior. As a result,
minimizing disappointment is always equivalent to maximizing accumulated payoffs in a repeated
game, whereas this is not the case with regret. While we showed that it is impossible to create an
algorithm that is guaranteed to have no disappointment in all scenarios without omniscience, it is
possible to create algorithms that quickly achieve (and maintain) low disappointment against many
algorithms in many repeated games.
To accomplish this goal, we presented a new meta-algorithm that can be used to enhance existing
expert algorithms. This algorithm reduces the set of selectable experts by combining aspiration
learning and equilibrium computation. We showed that the resulting algorithms quickly achieve
and maintain low disappointment when associating with various static algorithms, reinforcement
learning algorithms, and expert algorithms. We also showed that these expert algorithms, given a
good set of experts, outperform top-performing algorithms from the literature.
7.1 Reflections on Learning Using Experts
The meta-algorithm we presented for enhancing expert algorithms alters the order in which experts
are selected. It does this by computing the highest expected payoff (or potential) of each expert, and
then supplying the expert algorithm with the subset of experts whose potential meets some performance threshold, which is determined by aspiration learning. Experts with lower potential are only
selected once experts with higher potential have demonstrated an inability to meet their potentials.
This application of the optimism-in-uncertainty principle (Brafman & Tennenholtz, 2003) allows
expert algorithms to learn effective strategies very quickly while ensuring that the expert algorithm
has access to the best expert in the long term.
Several previously proposed algorithms (e.g., Powers & Shoham, 2005a, 2005b; Knobbout &
Vreeswijk, 2011) select experts in a serial fashion, beginning with the expert with the highest potential. Our results under imperfect information advocate for a more integrated approach in which
the experts are evaluated in parallel. This allows the agents to better negotiate leader and follower
roles, which in turn leads to better chances for profitable cooperation and compromise.
7.2 Extensions to Stochastic Games
The expert algorithms we have discussed and analyzed can also be applied to two-player repeated
stochastic games in at least two different ways. First, Pepper (Crandall, 2012) can be used to extend
any algorithm designed for repeated normal-form games to repeated stochastic games. Pepper uses
a separate instance of a learning algorithm designed for normal-form games in each stage game of
the stochastic game. Future work should determine how quickly our enhanced expert algorithms
learn in stochastic games when extended with Pepper.
Second, experts can be as complex as is necessary. Several algorithms for computing equilibrium strategies in stochastic games have recently been developed (e.g., Cote & Littman, 2008;
136

T OWARDS M INIMIZING D ISAPPOINTMENT

IN

R EPEATED G AMES

Johanson et al., 2012). These and other equilibrium strategies could define the set of experts used by
the enhanced expert algorithms in stochastic games. Future work involves identifying an effective
set of experts for stochastic games.

Acknowledgments
I would like to thank three anonymous reviewers who provided detailed and constructive feedback.
Their suggestions, questions, and comments greatly improved the paper.

Appendix A. Specification of Algorithms
Table 5 states the parameters and settings used by each algorithm used in the paper. Each algorithm
was carefully analyzed to ensure that it behaved as defined. Parameters for the algorithms were
selected by balancing two objectives. First, we desired the algorithms to perform as they were
intended by the algorithms‚Äô authors. Second, we sought to optimize each algorithm‚Äôs short-term
performance (i.e., the first 1000 rounds) while not compromising long-term performance.
Since our implementation of S as an expert algorithm is not provided in the literature (though
we maintain the general principles of the algorithm), we provide further details of that algorithm.
Though not originally designed as an expert algorithm, S has been shown to be effective when
learning to select among learning experts in repeated games (Bouzy et al., 2011). S learns an
aspiration level, and then searches for an expert that obtains payoffs that meets this aspiration.
S sets its initial aspiration Œ±1i to one, its highest payoff. It then randomly selects some expert
œÜi ‚àà Œ¶i , and follows œÜi until a joint action is played twice. This is determined by comparing the
latest joint action with Hc , the set of joint actions played so far. Œ±ti is then updated as follows:
t‚àí|Hc |

Œ±ti ‚Üê (Œªi )|Hc | Œ±i

+ (1 ‚àí (Œªi )|Hc | )rÃÑit ,

(18)

where Œªi ‚àà (0, 1) is the learning rate and rÃÑit is the average payoff obtained by agent i since the last
expert was selected. After updating Œ±ti , S selects a new expert:
(
(t‚àí|Hc |)
œÜi
with prob. f (Œ±ti , rÃÑit )
t
œÜi ‚Üê
(19)
random(Œ¶i ) otherwise
Here, random(Œ¶i ) denotes a random selection from Œ¶i , and f (Œ±ti , rÃÑit ) is the agent‚Äôs inertia given by
f (Œ±ti , rÃÑit ) = min(1, rÃÑit /Œ±ti

|Hc |

).

(20)

Eq. (20) specifies that the agent selects the expert that it played in the previous episode if rÃÑit
|H |
meets or exceeds Œ±ti . If not, it randomly selects a new expert with probability rÃÑit /Œ±ti c . Hc is
then reset to include only the last joint action played, and the process repeats.

Appendix B. Performance of Individual Experts
To illustrate the effectiveness of the experts defined in Section 3, we plot the running average payoff
of each of the experts against three different associates in Chicken, PD, Offset, and Tricky (Tables 1
and 2). These results are shown in Figure 11. We make several observations. First, in each scenario,
137

C RANDALL

Algorithm
A0
Random
Bully
Godfather

Description and Parameters
An agent that always selects its first action.
Randomly selects its action from the uniform distribution over the action set Ai .
The leader strategy (Section 3.1) for the solution s‚àó = arg maxs‚àà‚Ñ¶‚Ä≤ vi (s).
mm
).
The leader strategy (Section 3.1) for the solution sÃÇ = arg maxs‚àà‚Ñ¶‚Ä≤ (vi (s) ‚àí vimm )(v‚àíi (s) ‚àí v‚àíi
(a) Static algorithms

Algorithm
BR1

Description and Parameters
A model-based reinforcement learning algorithm that encodes its state as the previous joint action.
The algorithm estimates the associate‚Äôs behavior using the fictitious-play assessment conditioned on
the current state (Eq. 10), and then computes a best response using value iteration (discount factor
1
Œ≥ = 0.95). It uses Œµ-greedy exploration, Œµ = 10+t/10
.

BR2

Identical to BR1 except that it encodes state as the previous two joint actions.

WoLF-PHC

1
4
See Bowling and Veloso (2002). For Figure 2, Œ± = 100+t/10000
, Œµ = 0.05, Œ¥l = 20000+t
,
1
1
1
2
1
Œ¥w = 20000+t . Otherwise, Œ± = 10+t/100 , Œµ = 10+t/100 , Œ¥l = 100+t/100 , Œ¥w = 100+t/100

Q-learning

A model-free reinforcement learning algorithm proposed by Watkins (1992). Our implementation
encodes state as the previous joint action and uses Œµ-greedy exploration. Q-values are initialized to
1
1
1
the highest possible value 1‚àíŒ≥
. Œµ = 10+t/10
, Œ≥ = 0.95, Œ± = 10+t/100
(b) Reinforcement learning algorithms

Algorithm
Exp3

Description and Parameters
An expert algorithm (Auer et al., 1995) with well-defined regret bounds for the multi-armed bandit
problem. It was shown to be effective in some repeated games (Bouzy & Metivier, 2010; Crandall &
Goodrich, 2011; Chang & Kaelbling, 2005). Our implementation evaluates
p an expert for
œâ = |Ai ||A‚àíi | rounds before selecting a new expert. Œ∑ = Œ≥/|Œ¶i |, Œ≥ = |Œ¶i |ln(|Œ¶i |)/(e ‚àí 1)T0 ,
where T0 is the expected number of rounds in the game.

UCB1

An expert algorithm (Auer et al., 2002) with well-defined regret bounds for the adversarial multiarmed bandit problem. It has proved effective in some repeated games (Bouzy & Metivier, 2010).
Our implementation evaluates an expert for œâ = |Ai ||A‚àíi | rounds before selecting a new expert.

EEE

An Œµ-greedy expert algorithm designed for repeated games played against adaptive associates
1
(de Farias & Megiddo, 2004). œâ = |Ai ||A‚àíi |, Œµ = 10+t/10

S

An aspiration-based algorithm original proposed by Karandikar et al. (1998), and also analyzed by
Stimpson et al. (2001). See the text of Appendix A. Œªi = 0.99
(c) Expert algorithms

Algorithm
M-Qubed

Description and Parameters
An RL algorithm with the highest asymptotic performance in several studies (Crandall & Goodrich,
2011; Bouzy & Metivier, 2010). Parameters set as in the work of Crandall and Goodrich (2011).

ManipulatorBully

A la Powers and Shoham (2005a), this algorithm serially follows three experts. For the first 150
rounds, it follows Bully. Thereafter, if its payoffs become lower than expected, it switches to BR1.
After 300 rounds, if its payoffs ever drop below vimm ‚àí Œµ, it plays œÜmm
thereafter.
i

ManipulatorGodfather

A la Powers and Shoham (2005a), this algorithm serially follows three experts. For the first 150
rounds, it follows Godfather. Thereafter, if its payoffs become lower than expected, it switches to
BR1. After 300 rounds, if its payoffs ever drop below vimm ‚àí Œµ, it plays œÜmm
thereafter.
i
(d) Other algorithms: standards of comparison

Table 5: Algorithmic parameters used in the paper.

138

T OWARDS M INIMIZING D ISAPPOINTMENT

Chicken: Experts Against Bully

0.6
0.5
0.4
0.3
0.2
0.1
200

400

600

800

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

1000

1

Running Average Payoff

Running Average Payoff

Running Average Payoff

0.7

200

400

T

PD: Experts Against Bully

0.7
0.6
0.5
0.4
0.3
0.2
0.1
600

800

0.7
0.6
0.5
0.4
0.3
0.2
0.1
200

400

400

800

0.6
0.5
0.4
0.3
0.2
0.1
600

800

0.7
0.6
0.5
0.4
0.3
0.2
0.1
200

400

0.7
0.6
0.5
0.4
0.3
0.2
0.1
200

(g)

400

600

800

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

1000

200

400

Tricky: Experts Against BR1

0.5
0.4
0.3
0.2
0.1
600

800

1000

Tricky: Experts Against S

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

1000

1

Running Average Payoff

Running Average Payoff

0.6

800

(i)

1

0.7

600

T

(h)

0.8

1000

Offset: Experts Against S

T

0.9

800

(f)

0.8

Tricky: Experts Against Bully

600

1

T

1

1000

T

0.9

0
0

1000

800

0.8

0
0

1000

Running Average Payoff

0.7

600

0.9

Offset: Experts Against BR1

0.8

(j)

200

(e)

0.9

T

0.1

T

Running Average Payoff

Running Average Payoff

600

1

400

0.2

PD: Experts Against S

0.8

Offset: Experts Against Bully

200

0.3

(c)

0.9

0
0

1000

1

400

0.4

1

(d)

200

0.5

T

Running Average Payoff

Running Average Payoff

Running Average Payoff

0.8

400

0.6

PD: Experts Against BR1

0.9

200

0.7

0
0

1000

1

T

Running Average Payoff

800

0.8

(b)

1

0
0

600

0.9

T

(a)

0
0

Chicken: Experts Against S

1

0.8

0
0

R EPEATED G AMES

Chicken: Experts Against BR1

1
0.9

0
0

IN

200

400

600

800

1000

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

200

400

T

(k)

600

800

1000

T

(l)

Figure 11: Running average payoffs of each expert against BR1, S, and Bully in four different
games. Results are an average of 50 trials.

139

C RANDALL

there is at least one expert that performs well against that opponent. For example, in Chicken, an
ideal algorithm should be able to eventually achieve an average payoff near 1 against both BR1
and S after many rounds. Several experts achieve this performance level (Figures 11b and 11c).
Against Bully, the best possible average payoff is 0.33, which several experts achieve (Figure 11a).
Furthermore, in the PD, there is an expert that eventually obtains the average payoff of mutual
cooperation (0.6) against each associate (Figures 11d‚Äì11f).
Second, Figure 11 illustrates that different payoffs are possible against the different associates.
For example, in both Chicken and Tricky, the best expert against Bully has substantially lower
payoffs (despite playing optimally) than the best experts against BR1 and S. We note that disappointment normalizes performance in each of these cases so that an average disappointment of zero
indicates effective play relative to this set of experts.
Third, reaching the highest possible payoff sometimes requires a lot of patience. For example,
against S in Chicken (Figure 11c), the best expert after 100 rounds is not the best expert after 1000
rounds. Only after repeatedly following the same expert for many rounds does the associate learn to
accept the equilibrium offered by this expert. This illustrates how difficult it is for expert algorithms
to maintain low disappointment over time.

References
Arora, R., Dekel, O., & Tewari, A. (2012). Online bandit learning against an adaptive adversary:
from regret to policy regret. In Proceedings of the 29th International Conference on Machine
Learning, pp. 1503‚Äì1510.
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multi-armed bandit
problem. Machine Learning, 47, 235‚Äì256.
Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. E. (1995). Gambling in a rigged casino:
the adversarial multi-armed bandit problem. In Proceedings of the 36th Symposium on the
Foundations of Computer Science, pp. 322‚Äì331.
Axelrod, R. (1984). The Evolution of Cooperation. Basic Books.
Bouzy, B., & Metivier, M. (2010). Multi-agent learning experiments in repeated matrix games. In
Proceedings of the 27th International Conference on Machine Learning, pp. 119‚Äì126.
Bouzy, B., Metivier, M., & Pellier, D. (2011). Hedging algorithms and repeated matrix games.
In ECML Workshop on Machine Learning and Data Mining in and Around Games, Athens,
Greece.
Bowling, M. (2004). Convergence and no-regret in multiagent learning. In Advances in Neural
Information Processing Systems 17, pp. 209‚Äì216.
Bowling, M., & Veloso, M. (2002). Multiagent learning using a variable learning rate. Artificial
Intelligence, 136(2), 215‚Äì250.
Brafman, R. I., & Tennenholtz, M. (2003). R-max ‚Äì a general polynomial time algorithm for nearoptimal reinforcement learning. Journal of Machine Learning Research, 3, 213‚Äì231.
Cesa-Bianchi, N., Dekel, O., & Shamir, O. (2013). Online learning with switching costs and other
adaptive adversaries. In Advances in Neural Information Processing Systems 26, pp. 1160‚Äì
1168.
140

T OWARDS M INIMIZING D ISAPPOINTMENT

IN

R EPEATED G AMES

Chakraborty, D., & Stone, P. (2010). Convergence, targeted optimality, and safety in multiagent
learning. In Proceedings of the 27th International Conference on Machine Learning, pp.
191‚Äì198.
Chang, Y., & Kaelbling, L. P. (2005). Hedge learning: Regret-minimization with learning experts.
In Proceedings of the 22nd International Conference on Machine Learning, pp. 121‚Äì128.
Chang, Y.-H. (2007). No regrets about no-regret. Artificial Intelligence, 171(7), 434‚Äì439.
Chasparis, G., Shamma, J., & Arapostathis, A. (2010). Aspiration learning in coordination games.
In Proceedings of the 49th IEEE Conference on Decision and Control, pp. 5756‚Äì5761.
Cote, E. M. D., & Littman, M. L. (2008). A polynomial-time Nash equilibrium algorithm for
repeated stochastic games. In Proceedings of the 24th Conference on Uncertainty in Artificial
Intelligence, pp. 419‚Äì426.
Crandall, J. W. (2012). Just add Pepper: extending learning algorithms for repeated matrix games to
repeated markov games. In Proceedings of the 11th International Conference on Autonomous
Agents and Multiagent Systems, pp. 399‚Äì406.
Crandall, J. W., & Goodrich, M. A. (2011). Learning to compete, coordinate, and cooperate in
repeated games using reinforcement learning. Machine Learning, 82(3), 281‚Äì314.
Crandall, J. W., & Goodrich, M. A. (2005). Learning to teach and follow in repeated games. In
AAAI workshop on Multiagent Learning, Pittsburgh, PA.
de Farias, D., & Megiddo, N. (2003). How to combine expert (or novice) advice when actions
impact the environment. In Advances in Neural Information Processing Systems 16.
de Farias, D., & Megiddo, N. (2004). Exploration‚Äìexploitation tradeoffs for expert algorithms in
reactive environments. In Advances in Neural Information Processing Systems 17, pp. 409‚Äì
416.
Foster, D. P., & Vohra, R. (1999). Regret in the on-line decision problem. Games and Economic
Behavior, 29, 7‚Äì35.
Fudenberg, D., & Levine, D. K. (1998). The Theory of Learning in Games. The MIT Press.
Ganzfried, S., & Sandholm, T. (2011). Game theory-based opponent modeling in large imperfectinformation games. In Proceedings of the 10th International Conference on Autonomous
Agents and Multiagent Systems, pp. 533‚Äì540.
Gordon, G. J., Greenwald, A., & Marks, C. (2008). No-regret learning in convex games. In Proceedings of the 25th International Conference on Machine Learning, pp. 360‚Äì367.
Greenwald, A., & Jafari, A. (2003). A general class of no-regret learning algorithms and gametheoretic equilibria. In Proceedings of the 16th Annual Conference on Computational Learning Theory, pp. 2‚Äì12.
Greenwald, A., & Hall, K. (2003). Correlated Q-learning. In Proceedings of the 20th International
Conference on Machine Learning, pp. 242‚Äì249.
Johanson, M., Bard, N., Lanctot, M., Gibson, R., & Bowling, M. (2012). Efficient Nash equilibrium
approximation through Monte Carlo counterfactual regret minimization. In Proceedings of
the 11th International Conference on Autonomous Agents and Multiagent Systems, pp. 837‚Äì
846.
141

C RANDALL

Karandikar, R., Mookherjee, D., R., D., & Vega-Redondo, F. (1998). Evolving aspirations and
cooperation. Journal of Economic Theory, 80, 292‚Äì331.
Knobbout, M., & Vreeswijk, G. A. (2011). Sequential targeted optimality as a new criterion for
teaching and following in repeated games. In Proceedings of the 10th International Conference on Autonomous Agents and Multiagent Systems, pp. 517‚Äì524.
Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In
Proceedings of the 11th International Conference on Machine Learning, pp. 157‚Äì163.
Littman, M. L. (2001). Friend-or-foe: Q-learning in general-sum games. In Proceedings of the 18th
International Conference on Machine Learning, pp. 322‚Äì328.
Littman, M. L., & Stone, P. (2001). Leading best-response strategies in repeated games. In IJCAI
workshop on Economic Agents, Models, and Mechanisms, Seattle, WA.
Littman, M. L., & Stone, P. (2005). A polynomial-time Nash equilibrium algorithm for repeated
games. Decision Support Systems, 39, 55‚Äì66.
Powers, R., & Shoham, Y. (2005a). Learning against opponents with bounded memory. In Proceedings of the 19th International Joint Conference on Artificial Intelligence, pp. 817‚Äì822.
Powers, R., & Shoham, Y. (2005b). New criteria and a new algorithm for learning in multi-agent
systems. In Advances in Neural Information Processing Systems 17, pp. 1089‚Äì1096.
Stimpson, J. R., Goodrich, M. A., & Walters, L. C. (2001). Satisficing and learning cooperation
in the prisoner‚Äôs dilemma. In Proceedings of the 17th National Conference on Artificial
Intelligence, pp. 535‚Äì544.
Watkins, C. J., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279‚Äì292.

142

Journal of Artificial Intelligence Research 49 (2014) 49-78

Submitted 07/13; published 01/14

Robustness and Stability in Constraint Programming under
Dynamism and Uncertainty
Laura Climent

LCLIMENT @ DSIC . UPV. ES

Instituto de AutomaÃÅtica e InformaÃÅtica Industrial
Universidad PoliteÃÅcnica de Valencia, Spain.

Richard J. Wallace

R . WALLACE @4 C . UCC . IE

INSIGHT Center for Data Analytics
Department of Computer Science. University College Cork, Ireland.

Miguel A. Salido

MSALIDO @ DSIC . UPV. ES

Instituto de AutomaÃÅtica e InformaÃÅtica Industrial
Universidad PoliteÃÅcnica de Valencia, Spain.

Federico Barber

FBARBER @ DSIC . UPV. ES

Instituto de AutomaÃÅtica e InformaÃÅtica Industrial
Universidad PoliteÃÅcnica de Valencia, Spain.

Abstract
Many real life problems that can be solved by constraint programming, come from uncertain
and dynamic environments. Because of the dynamism, the original problem may change over time,
and thus the solution found for the original problem may become invalid. For this reason, dealing
with such problems has become an important issue in the fields of constraint programming. In some
cases, there is extant knowledge about the uncertain and dynamic environment. In other cases, this
information is fragmentary or unknown. In this paper, we extend the concept of robustness and
stability for Constraint Satisfaction Problems (CSPs) with ordered domains, where only limited
assumptions need to be made as to possible changes. We present a search algorithm that searches
for both robust and stable solutions for CSPs of this nature. It is well-known that meeting both
criteria simultaneously is a desirable objective for constraint solving in uncertain and dynamic
environments. We also present compelling evidence that our search algorithm outperforms other
general-purpose algorithms for dynamic CSPs using random instances and benchmarks derived
from real life problems.

1. Introduction
Constraint programming is a powerful tool for solving many artificial intelligence problems that
can be modeled as CSPs. Much effort has been spent on increasing the efficiency of algorithms for
solving CSPs, as reflected in the literature. However, most of these techniques assume that the set
of variables, domains and constraints involved in the CSP are known and fixed when the problem is
modeled. This is a strong limitation when we deal with real life situations because these problems
may come from uncertain and dynamic environments. Due to the dynamism in the environment,
both the original problem and its corresponding modeled CSP may evolve. In addition, since the
c
2014
AI Access Foundation. All rights reserved.

C LIMENT, WALLACE , S ALIDO & BARBER

real world is uncertain in its nature, information about the dynamism of the environment may be
incomplete, erroneous or even may not exist. In such situations, a solution that holds for the original
model can become invalid after changes in the original problem.
The approaches that deal with this situation can be classified as: (i) reactive approaches, whose
main objective is to obtain a new solution as similar as possible to the previous solution (the solution
found before the changes occurred) in a efficient way, and (ii) proactive approaches, which use
knowledge about possible future changes in order to avoid or minimize their effects (for a survey
see Verfaillie & Jussien, 2005). Thus, proactive approaches are applied before the changes occur,
while reactive approaches are only applied when the changes invalidate the original solution.
Reactive approaches re-solve the CSP after each solution loss, which consumes computational
time. That is a clear inconvenience, especially when we deal with short-term changes, where solution loss is very frequent. In addition, in many applications, such as online planning and scheduling,
the time required to calculate a new solution may be too long for actions to be taken to redress the
situation. In addition, the loss of a solution can have several negative effects in the modeled situation. For example, in a task assignment of a production system with several machines, it could cause
the shutdown of the production system, the breakage of machines, the loss of the material/object
in production, etc. In a transport timetabling problem, a solution loss, due to some disruption at a
point, may produce a delay that propagates through the entire schedule. All these negative effects
will probably entail an economic loss as well.
Proactive approaches try to avoid the drawbacks just stated and, therefore, they are highly valued for dealing with problems in uncertain and dynamic environments. Given the advantages that
proactive approaches potentially offer, in this paper we restrict ourselves to this approach. Heretofore two main types of proactive approaches have been considered, which can be distinguished on
the basis of the characteristics of the solutions that they obtain, which are called robust and flexible
(see Section 2). In an important survey on constraint solving in uncertain and dynamic environments
(Verfaillie & Jussien, 2005), the authors mention the possibility of developing proactive strategies
that combine the solution features of robustness and flexibility. They state: ‚ÄúThe production of
solutions that are at the same time robust and flexible, that have every chance to resist changes and
can be easily adapted when they did not resist, is obviously a desirable objective.‚Äù In this paper, we
present an algorithm that meets the objective of combining solution robustness and stability. The
solution feature of stability is a special case of flexibility.
Many proactive approaches proposed in the literature assume the existence of knowledge about
the uncertain and dynamic environment (see Section 3). In these cases it is difficult to characterize
the robustness of the solutions when detailed information about possible future changes is not available. We consider situations where there is an added difficulty stemming from the fact that the only
limited assumptions about changes can be made. Our discussion focuses on CSPs with ordered
and discrete domains that model problems for which the order over the elements of the domain
is significant. In these cases, a common type of change that problems may undergo is restrictive
modifications over the bounds of the solution space. These assumptions and their motivations were
introduced by Climent et al. (2013). Moreover, examples of real life problems that exhibit this
type of dynamism were described, specifically, temporal reasoning-based problems, spatial and geometric reasoning problems, and design problems. In temporal problems, delays are an inherent
feature, which implies restrictive modifications of the bounds involved with such disruptions. For
instance, Fu, Lau, Varakantham, and Xiao (2012) stated that unexpected external events such as
manpower availability, weather changes, etc. lead to delays or advances in completion of activities
50

ROBUSTNESS AND S TABILITY IN C ONSTRAINT P ROGRAMMING UNDER DYNAMISM AND U NCERTAINTY

in scheduling problems. In spatial and geometric reasoning problems, the constraints can be readjusted due to measurement errors. The latter can also occur in design problems, in which the data is
not completely certain.
In this paper, we present an algorithm that searches for solutions to CSPs with ordered domains,
which are robust and are also stable because they can often be repaired using a value of similar
magnitude if they undergo a value loss. The paper is organized as follows. The next section recalls
some general definitions. Section 3 gives a brief account of earlier proactive procedures. Section 4
presents a new conception of robustness and stability when there exists an order over the elements of
the domain. Sections 5 and 6 describe the main objective for finding solutions that meet the stability
and robustness criteria simultaneously. Then, in Section 7 the search algorithm that meets these
objectives is explained. Section 8 presents a case study of scheduling problems. Section 9 describes
experiments with various types of CSPs, showing the effectiveness of the present approach for
finding solutions that are both stable and robust. Section 10 gives conclusions.

2. Technical Background
In this section we give some basic definitions that are used in the rest of the paper, following standard
notations and definitions in the literature.
Definition 2.1 A Constraint Satisfaction Problem (CSP) is represented as a triple P = hX , D, Ci
where X is a finite set of variables X = {x1 , x2 , ..., xn }, D is a set of domains D = {D1 , D2 , ..., Dn }
such that for each variable xi ‚àà X there is a set of values that the variable can take, and C is a
finite set of constraints C = {C1 , C2 , ..., Cm } which restrict the values that the variables can simultaneously take. We denote by DC the set of unary constraints associated with D.
Definition 2.2 A tuple t is an assignment of values to a subset of variables Xt ‚äÜ X .
If a tuple t is feasible we call it s. This means that s is an assignment of the domain values to
some variables that does not violate any constraint. If s is a complete assignment (it involves all the
variables of the CSP), then it is a solution of the CSP. Xs is the subset of variables that are involved
in s. Then X \Xs is the set of unassigned variables in s. The value assigned to a variable x in s
is denoted as s(x). In addition, we denote Ds (x) ‚äÜ D(x) to the subset of domain values of the
variable x that are consistent with s.
The number of possible tuples of a constraint
Q Ci ‚àà C is composed of the elements of the
Cartesian product of the domains of var(Ci ): xj ‚ààvar(Ci ) Dj , where var(Ci ) ‚äÜ X is the set of
variables involved in Ci (scope of Ci ).
Definition 2.3 The tightness of a constraint is the ratio of the number of forbidden tuples to the
number of possible tuples. Tightness is defined within the interval [0,1].
Inferential processes for CSPs narrow the search space of possible partial solutions. In this work
we use one of the most known and used consistency procedure: arc-consistency.
Definition 2.4 A CSP is arc-consistent (Mackworth, 1977a) iff for any pair of constrained variables
xi and xj , for each value a in Di there exists at least one value b in Dj such that the partial
assignment (xi = a, xj = b) satisfies all the constraints related to both xi and xj . Any value in
51

C LIMENT, WALLACE , S ALIDO & BARBER

the domain of a variable which is not arc-consistent can be eliminated as they can not be part of
any solution. The domain of a variable is arc-consistent iff all values are arc-consistent. Thus, a
problem is arc-consistent iff all its arcs are arc-consistent:
‚àÄ Cij ‚àà C, ‚àÄa ‚àà D(xi ), ‚àÉb ‚àà D(xj ): a and b satisfy Cij .
In the following, several properties associated with the solutions of problems that come from
dynamic environments are defined.
Definition 2.5 The most robust solution of a CSP within a set of solutions is the one with the highest
likelihood of remaining a solution after a given set of changes in the CSP.
Definition 2.6 A flexible solution is anything (a partial solution, complete solution, conditional
solution, set of solutions, etc.) that, in case of change, can be easily modified to produce a solution
to the new problem (Verfaillie & Jussien, 2005).
A more specific concept of flexibility is the concept of stability.
Definition 2.7 A solution s1 is more stable than another solution s2 if and only if, in the event of
a change that invalidates them, a closer alternative to s1 than to s2 exists (modified from the work
presented by Hebrard, 2006).
The main difference between Definition 2.7 and Definition 2.6 is that the former introduces the
concept of ‚Äòcloser‚Äô solution. The measurement of this closeness is made by calculating distances
between solutions. More concrete information about the distance equations is explained in following
sections. We would like to remark that Definition 2.5 does not consider the alterations in the original
solution but only its resistance to changes in the problem. On the other hand, Definition 2.6 and
Definition 2.7 do consider changes to the original solution when a new solution is produced after a
change in the problem.

3. Related Work: Proactive Approaches
Several approaches have been proposed in the past for handling this type of problem, which can be
classified based on the kind of solutions they obtain. Thus, there are techniques that search for robust
solutions and others that search for flexible solutions (for a survey see Verfaillie & Jussien, 2005).
In this section we describe some techniques that search for robust solutions and their limitations.
Then we discuss a technique that searches a certain type of stable solutions: super-solutions.
3.1 Searching for Robust Solutions
Many earlier approaches that search for robust solutions use additional information about the uncertain and dynamic environment in which the problem occurs, and most often this involves probabilistic representations. In one example of this type, information is gathered in the form of penalties
when values have been invalidated after changes in the problem (Wallace & Freuder, 1998). Nevertheless, in the Probabilistic CSP model (PCSP) (Fargier & Lang, 1993), there exists information
associated with each constraint, expressing its probability of existence. Other techniques focus on
the dynamism of the variables of the CSP. For instance, the Mixed CSP model (MCSP) (Fargier,
52

ROBUSTNESS AND S TABILITY IN C ONSTRAINT P ROGRAMMING UNDER DYNAMISM AND U NCERTAINTY

Lang, & Schiex, 1996) and the subsequent Uncertain CSP model (UCSP) (Yorke-Smith & Gervet,
2009) consider the dynamism of certain uncontrollable variables that can take on different values
of their uncertain domains. A related model, uses Simple Temporal Networks, but adds data about
the time uncertainties and preferences, which represent the starting or ending times of events (STPPUs) (Rossi, Venable, & Yorke-Smith, 2006). The Stochastic CSP model (SCSP) (Walsh, 2002)
also considers probability distributions associated with the uncontrollable variables. The Branching
CSP model (BCSP) considers the possible addition of variables (with a certain associated gain) to
the current problem (Fowler & Brown, 2000).
In most of these models, the form of the algorithm is dependent on detailed knowledge about the
dynamic environment. For this purpose, a list of possible changes is required or there is an explicit
representation of uncertainty, often in the form of an associated probability distribution. As a result,
these approaches cannot be used if the necessary information is unknown. In many real problems,
however, knowledge about possible further changes is either limited or non-existent. Hence, there
is an important need for techniques that find robust solutions in this kind of environment.
For instance, Climent et al. (2013) cope with CSPs that model problems for which the order over
the domain elements is significant. Specifically, these CSPs are modeled as Weighted Constraint
Satisfaction Problems (WCSPs) (Larrosa & Schiex, 2004) by penalizing valid tuples based on their
coverings. Instead of requiring extra detailed dynamism information, the authors only make limited
assumptions concerning the changes that might occur, which is related to the nature of CSPs with
ordered domains. Specifically, dynamism is assumed to take the form of restrictions on the bounds
of the solution space. In this paper, we make the same assumptions about the dynamism. The
previous WCSP modeling approach computes robustness based on feasible neighbours that compose
a covering which surrounds the analyzed value with respect to each constraint boundary. Thus, in
cases in which a neighbour is feasible with respect to one bound but is not for another bound, this
neighbour is not feasible in the solution space. For this reason, this approach obtains robustness
approximations in problems in which there is a high relation between constraints. On the other
hand, the algorithm described in this paper computes feasible assignments with respect to the entire
solution space, which avoids the weakness of the WCSP modeling approach explained above. A
comparison between these approaches is found in Section 9.
3.2 Searching for Super-Solutions
Techniques that search for stable solutions of a certain type, which are denoted as super-solutions,
were presented by Hebrard (2006). The goal is to be able to repair an invalid solution after changes
occur, with minimal changes that can be specified in advance. Since this is another approach that
does not require detailed additional information about changes in a problem, it is of interest to
compare it to the search algorithm introduced in this paper.
Definition 3.1 A solution is an (a, b)-super-solution if the loss of values of at most a variables can
be repaired by assigning other values to these variables and changing the values of at most b other
variables (Hebrard, 2006).
For CSPs, a major focus has been on finding (1, 0)-super-solutions. This is because of the high
computational cost of computing b > 0 or a > 1. This is one of the reasons why we analyze
this particular super-solution case in this paper. The other reason is given by Verfaillie and Jussien
(2005), where the authors state that a desirable objective is to ‚Äúlimit as much as possible changes
53

C LIMENT, WALLACE , S ALIDO & BARBER

in the produced solution‚Äù, which motives the search of (a, 0)-super-solutions. In general, it is unusual to find (1, 0)-super-solutions where all variables can be repaired. For this reason, Hebrard
(2006) also developed a branch and bound-based algorithm for finding solutions that are close to
(1, 0)-super-solutions, i.e., where the number of repairable variables is maximized (also called maximizing the (1, 0)-repairability).

4. Extending Robustness and Stability to CSPs with Ordered Domains
In this section we extend the original definition of solution robustness (Definition 2.5) and solution
stability (Definition 2.7) to consider CSPs with ordered domains, where only limited assumptions
are made about changes in the problem that are derived from their inherent structure. Given this
framework and therefore the existence of a significant order over the values in the domains, it is
reasonable to assume that the original bounds of the solution space can only be restricted or relaxed,
even if this does not cover all possible changes. The bounds of the solution space are delimited
by the domains and constraints of the CSP. Note that the possibility of solution loss only exists
when changes over the original bounds of the solution space are restrictive. For this reason, a
solution that is located farther away from the bounds is more likely to remain a solution. Given
these assumptions, we specialize Definition 2.5 for this framework as follows.
Definition 4.1 The most robust solution of a CSP with ordered domains without detailed dynamism
data is the solution that maximizes the distance from all the dynamic bounds of the solution space.
Furthermore, the definition of stable solutions for CSPs with ordered domains can be made more
precise because it is possible to define a more specific notion of closeness between two solutions
due to the existent order over the domain values. Hebrard (2006) measures the level of dissimilarity
of two solutions by counting
Pn the number of variables that take different values in both solutions, i.e.,
the Hamming distance ( i=1 (s1i 6= s2i )). Later, Hebrard, O‚ÄôSullivan, and Walsh (2007) consider
another similarity measure: the Manhattan distance. This
P measure uses the sum of the absolute difference of values (of each variable) for both solutions ( ni=1 |s1i ‚àís2i |). Note that unlike Hamming
distance, Manhattan distance requires an order over the elements in order to calculate the absolute
difference of the values. In the following definition, we apply the Manhattan distance to the notion
of stable solutions for CSPs with ordered domains.
Definition 4.2 Given an order relationship over the values of a set of solutions, a solution s1 is
more stable than another solution s2 iff, in the event of a change that invalidates them, there exists
an alternative solution to s1 with lower Manhattan distance than the Manhattan distance of any
alternative solution to s2.
Furthermore, we present an extension of Definition 3.1 for CSPs with ordered domains by fixing
a maximum Manhattan distance between the original solution and the repaired solution, which is
called c.
Definition 4.3 A solution is an (a, b, c)-super-solution if the loss of values of a variables at most,
can be repaired by assigning other values whose Manhattan distance with respect the original
values is lower or equal to c, and this involves changing the values of b variables at most.
The above definition also holds for the (1, 0, c)-super-solutions and the (1, 0, c)-repairability,
which are the main focus of the stability analysis in this paper.
54

ROBUSTNESS AND S TABILITY IN C ONSTRAINT P ROGRAMMING UNDER DYNAMISM AND U NCERTAINTY

5. Searching for Robust and Stable Solutions: General Main Objective
In order to find robust and stable solutions for CSPs with ordered domains under our assumptions,
we combine the robustness and stability criteria presented in Section 4. As mentioned, calculating
distances is required for the search of robust solutions in this framework. However, a measure of
the distance from the dynamic bounds of the solution space is not always obvious or easy to derive,
since the bounds are delimited by the domains and the constraints of the CSP, and the latter may
be extensionally expressed. However, some deductions about minimum distances to the bounds can
be made based on the feasibility of the neighbours of a solution. This idea is first motivated with a
very simple example and then it is formalized.
Example 5.1 Figure 1 shows two solution spaces (one convex and the other non-convex) whose
dynamic bounds are marked by contiguous lines. The most robust solutions according to Definition
4.1 are highlighted. Note that there are two contiguous feasible neighbours on both sides of each
assignment (discontinuous lines).

(a) Convex Solution Space

(b) Non-convex Solution Space

Figure 1: Most Robust solutions for different solution spaces.
From this example, we can conclude that we can only ensure that a solution s is located at
least at a distance d from a bound in a certain direction of the n-dimensional space if all the tuples
at distances lower or equal to d from s in this direction are feasible. Therefore, the number of
feasible contiguous surrounding neighbours of the solution is a measure of the robustness of the
solution in the face of restrictive changes that affect the original bounds of the solution space (see
Definition 4.1). In addition to fulfilling the main objective of finding solutions whose values have
a high number of feasible neighbours close to each assignment, this criterion can be used to obtain
solutions with high stability. This is because if the value assigned to a variable has at least one
of these feasible neighbour values, then this variable is repairable. That is, if its assigned value is
lost, it can easily be repaired by assigning the neighbour value (since this value is consistent with
the rest of the values of the assignment). Regarding the stability notion of Definition 4.2, note that
the difference between the lost value and the repairable value is very low, since they are immediate
neighbours. In fact, their value difference is one, which is the minimum possible.
55

C LIMENT, WALLACE , S ALIDO & BARBER

The set of feasible contiguous neighbour values of the value v that have differences not greater
than k with respect to v in increasing, or decreasing, or both directions with respect to the order
relationship is denoted as Nk (x, v, s, ‚äï). Value v is a feasible value for variable x in the feasible
partial/complete assignment s. Here, when we say that other values are feasible, we mean that they
are also feasible with respect to s. (Recall that we use Ds (x) ‚äÜ D(x) the subset of domain values
that are consistent with the feasible partial assignment s.) The list of operators ‚äï is composed of a
set of paired elements, or operator pairs. Each operator pair is denoted as ‚äïi ‚àà {{>, +}, {<, ‚àí}}.
The operator pairs fix the order directions to analyze. Thus, the set {>, +} refers to values greater
than v (increasing direction) and the set {<, ‚àí} refers to values lower than v (decreasing direction).
For each operator pair, the operator in position j is referenced as ‚äïij . For instance, if the list of
operators is ‚äï = {{>, +}, {<, ‚àí}}, the operator pair ‚äï1 references {>, +} and the operator ‚äï12
references the operator +. Given this notation, we define Nk (x, v, s, ‚äï) as:
Nk (x, v, s, ‚äï)= {w ‚àà Ds (x) : ‚àÉ‚äïi , w ‚äïi1 v ‚àß |v ‚àí w| ‚â§ k ‚àß

(1)

‚àÄ ‚äïz ‚àÄj ‚àà [1 . . . (|v ‚àí w| ‚àí 1)], (v ‚äïz2 j) ‚àà Ds (x)}
The first condition of Equation 1 ensures that the value w is greater or lower than v according
to the operator ‚äïi1 ‚àà {>, <} and that the distance between these values is less or equal to k. The
second condition ensures that all values that are closer to v than w are also feasible values for s. If
at least one of them is not, the value w cannot belong to Nk (x, v, s, ‚äï). As mentioned previously,
the set of feasible neighbours of a value has to be contiguous. Otherwise, there is an infeasible
space between this value and another feasible value. For instance, in Figure 1(b) the value 5 does
not belong to Nk (y, 2, {x = 2}, ‚äï) for any ‚äï or k because the value 4 is not a feasible value and
therefore it is outside the bounds of the solution space.
For the general case of CSPs with ordered domains in which we assume that all the bounds
are dynamic, the desirable objective is to find contiguous surrounding feasible neighbours on both
sides. For this reason ‚äï = {{>, +}, {<, ‚àí}}. For this list of operator pairs, the last condition of
Equation 1 checks that all the values in both directions that are closer to v than w, are also feasible
values for s. For instance, in Figure 1(b), Nk (y, 2, {x = 2}, {{>, +}, {<, ‚àí}}) = {1, 3} for any k
value. Note that these neighbours are on both sides the value 2 with respect to the y axis. In Section
8, we will show a specific case for which it is desirable to apply only one operator pair due to the
nature of the problem.
To apply Equation 1 to domains that are not ordered in Z, a monotonic and order-preserving
function has to be applied. For instance, if we consider D = {f reezing, cold, mild, warm, hot,
boiling}, a monotonic function that assigns greater values to values with higher temperatures could
be defined. For example, f (f reezing) = 1, f (cold) = 2, f (mild) = 3, f (warm) = 4, f (hot) =
5 and f (boiling) = 6.

6. Objective Function
In Section 5 we stated that the main desirable objective for a selected value is to have as many contiguous feasible neighbours in a certain direction, because they determine the minimum distance of
this value from the bound in this direction. For approximating the distance of several values assigned
(partial or complete assignment), we compute the number of neighbours of each value. Therefore,
56

ROBUSTNESS AND S TABILITY IN C ONSTRAINT P ROGRAMMING UNDER DYNAMISM AND U NCERTAINTY

we define as an objective function of our search algorithm the sum of the size of Nk (x, v, s, ‚äï)
(denoted |Nk (x, v, s, ‚äï)|) for each variable x ‚àà X . If s is an incomplete assignment, we calculate
the maximum |Nk (x, v, s, ‚äï)| for each v ‚àà Ds (x) of each unassigned variable x ‚àà X \Xs (upper
bound). Note that the maximum size of the set of neighbour values for each variable is | ‚äï | ‚àó k,
where | ‚äï | is the number of pair operators. Thus, the maximum size of the set of neighbour values
is 2k if ‚äï is composed of two operator pairs or k if ‚äï is composed of only one operator pair. Note
also that it is not necessary to check all the values of Ds (x) if, for at least one of them, the size of
the set is the maximum possible. In the following equation, we formalize the objective function that
is used by our search algorithm.

f (s, k, ‚äï) = {

X

max{|Nk (x, v, s, ‚äï)|, ‚àÄv ‚àà Ds (x)} +

X

|Nk (y, s(y), s, ‚äï)|}

(2)

y‚ààXs

x‚ààX \Xs

For Example 5.1, for the most robust solutions of both Figures 1(a) and 1(b) (highlighted solutions) f (s, k, {{>, +}, {<, ‚àí}}) = 4, for k ‚â• 1, since every value assigned to each solution has
two contiguous neighbours on both sides.
Next, we give a formal rationale for using the total number of neighbours of the solution (sum
of feasible surrounding neighbours of each value of the solution) as a measure of robustness.
For k = 1, in a convex solution space, each value has either zero, one or two feasible neighbours. Here we can discount the case of zero neighbours because if an assignment has zero feasible
neighbours, then it must be part of a singleton domain, and it will be part of all solutions. So we
need only consider values with one or two feasible neighbours.
In this case, a solution with a greater sum is one whose assignments have more feasible neighbour pairs. This can be easily seen if we consider the difference between a solution all of whose
values have only one feasible neighbour and any other solution; this difference will be equal to the
number of feasible neighbour pairs associated with the latter‚Äôs assignments.
Proposition 1. If we assume that having two feasible neighbours confers greater robustness
than having one and that the probabilities of single changes are independent, then a solution with a
greater feasible neighbour-sum than another will also be more robust, and vice versa.
In the non-convex case, it is unfortunately possible for one assignment to have zero feasible
neighbours, while other assignments to the same variable have one or two. In this case, we cannot
assume Proposition 1. However, as the number of variables in the problem increases, it becomes
increasingly unlikely that a variable with an assignment having zero feasible neighbours will be
associated with the largest neighbour-sum for the remaining variables.
Regarding the measure of stability, a solution that maximizes the (1, 0, k)-repairability (see
Definition 4.3) also maximizes the number of variables that can be repaired by a neighbour value
at a distance less or equal to k (without modifying any other variable). However, to obtain robust
solutions we maximize the sum of neighbour values of each value of the solution. Note that even
if both maximization criteria are not identical, as mentioned, when the number of variables in the
problem increases, it becomes increasingly unlikely that a non-repairable variable will be associated
with the largest neighbour-sum for the remaining variables. So in this work we will use the same
technique for finding robust and stable solutions for CSPs with ordered domains. Nevertheless, the
basic units of measure for both criteria are different.
57

C LIMENT, WALLACE , S ALIDO & BARBER

7. Search Algorithm
In this section we present an algorithm for finding robust and stable solutions according to the main
objective described in Section 6. For this purpose, we have incorporated this optimization criterion
into a Branch & Bound algorithm (Algorithm 1) that maximizes the objective function f (s, k, ‚äï)
(see Equation 2). As mentioned, this function sums |Nk | of each assigned variable and the maximum
possible |Nk | of each unassigned variable. Note that this computation is an upper bound of the final
total number of feasible contiguous neighbours of the solution.
Algorithm 1 (B&B-Nk ) is an ‚Äòanytime‚Äô algorithm that uses an inference process and prunes
the branches whose objective function value is lower or equal to the current maximum function
value obtained, referred to as lb (lower bound). The process stops when all the branches have been
explored or pruned, providing the solution s with the maximum f (s, k, ‚äï). On the other hand, we
can limit the search time and therefore the quality of the best solution found by fixing a time cutoff.
Of course, the more time Algorithm 1 spends searching, the more robust and stable the solution
provided can be. In addition, we compute the maximum possible objective function value, which is
the maximum number of neighbours for each variable multiplied by the number of variables of the
CSP, denoted as ub (upper bound). Thus, if the objective function value of a new solution found is
equal to ub, the algorithm stops, since this solution is optimal.
Algorithm 1: B&B-Nk : Branch & Bound anytime algorithm
Data: P = hX , D, Ci, ‚äï, k, scale, m, time cutoff (optional)
Result: s, Nk , lb
s ‚Üê ‚àÖ; // Partial assignment
Xs ‚Üê ‚àÖ; // Set of variables assigned
Nk ‚Üê ‚àÖ; // Set of contiguous surrounding neighbours
lb ‚Üê ‚àí1; // Maximum f (s, k, ‚äï) for the solutions
ub ‚Üê | ‚äï | ‚àó k ‚àó |X |;
i ‚Üê 1;
GAC3-Nk (P, s, Xs , Nk , ‚äï, k, lb);
repeat
if restarting-scratch ‚àß new solution found then
i ‚Üê 1;
C ‚Üê scale ‚àó mi ; //number of fails cutoff
i ‚Üê i + 1;
until time cutoff ‚à® not MGAC3-Nk (P, s, Xs , Nk , ‚äï, k, lb, 0, C, ub);
We have implemented the Branch & Bound algorithm using a Geometric restart strategy (Walsh,
1999) in order to reduce the repetition of fails in the search due to very early wrong assignments
(thrashing). Thus, each time the number of failures (referenced as nbF ) reaches the number-offails cutoff value condition (C) that is checked in Algorithm 3, the algorithm restarts the search
from scratch, except for the constraint weights stored by the dom/wdeg heuristic variable selection
(Boussemart et al., 2004). The value of the number of fails cutoff is increased geometrically in Algorithm 1 according to the scale factor (referred to as scale) and the multiplicative factor (referred to
as m). We have implemented two different options to carry out after a solution is found. In the first,
called restarting-completion, when the first solution is found, the algorithm continues to search until
58

ROBUSTNESS AND S TABILITY IN C ONSTRAINT P ROGRAMMING UNDER DYNAMISM AND U NCERTAINTY

completion (this is done by assigning a huge number representing ‚àû to the number of fails cutoff).
In the second option, called restarting-scratch, after each solution found, the algorithm restarts the
search from scratch and also restarts the number of fails cutoff computation (the constraint weighs
remain the same). For instances with very large domain sizes, this restarting option can be effective
because it avoids spending a large amount of time in a specific branch. The latter happens when
Algorithm 1 checks many domain values of variables located at low levels of the search tree, because the objective function of the partial assignment is better than the current maximum (lb). In
this case, if there exists a time cutoff, Algorithm 1 could not analyze other branches of the tree that
may contain solutions of better quality.
The inference process is carried out by Algorithm 2 (GAC3-Nk ), which is an extension of the
well-known AC3 (Mackworth, 1977b) that performs Generalized Arc Consistency (GAC) (Mohr
& Henderson, 1986; Bessiere, 2006). Some specific notation has been included, as var(c), which
is the scope of c ‚àà C. The original seekSupport function of GAC3 searches for a support
for each domain value. We have modified this function slightly by providing the set of values to
be analysed as a parameter of the function. Thus, if any of these values is deleted because there
does not exist any consistent support with respect the partial assignment, seekSupport returns
false. This function is first called with the values of the domain of the variables (for checking if
the partial assignment s is GAC3) and later with Nk just for assigned variables (for checking if each
Nk (x, s(x), s, ‚äï) is GAC3 with respect s). In order to ensure the contiguity of the values in Nk ,
Algorithm 2 checks the consistency of subsets of Ni ‚äÜ Nk , where i is equal to one initially, and it
is increased by one unit until at least one of the values of Ni is inconsistent or i reaches the value
of k. The complexity of updating Ni can be reduced to | ‚äï | ‚àó i if the domains are ordered. Note
that in the case where both greater and lower values are candidates to be in the set, the updating
cost is 2 ‚àó i. After composing the set of contiguous neighbour values that are GAC3 with respect s,
Algorithm 2 analyzes if the objective function f (s, k, ‚äï) is greater than lb. If it is not, or s is not
GAC3, it returns false.
Algorithm 3 (MGAC3-Nk ) performs a Maintaining GAC3 procedure by assigning to each variable x ‚àà X a new value v ‚àà D(x), until the value selected is GAC3-Nk with respect s. We have
implemented two value selection heuristics: lexicographical order and selection of the value that
maximizes |Nk (x, v, s, ‚äï)|, starting from intermediate values. There are some real life problems
for which the lexicographical selection order is effective in finding feasible solutions quickly. An
example is scheduling problems, whose domain values represent time units; hence the importance
of selecting low values in order not to exceed the maximum fixed makespan. However, if it is not
important to select low values, the heuristic that starts with intermediate values may offer better
results because it is selecting values that maximize the objective function at the current node of the
search tree. Furthermore, since search starts with intermediate values, the likelihood of selecting
values located far from the domain bounds is higher.
Algorithm 3 is also responsible for updating the set of assigned variables Xs , the partial assignment s and the maximum objective function value lb (for each solution found). Furthermore, it
stores the domains and the set of neighbours of all the variables before making an assignment. Note
that after a variable x is assigned, D(x) contains a single value that is the value assigned to x. If Algorithm 2 (GAC3-Nk ) returns false, then Algorithm 3 (MGAC3-Nk ) carries out the backtracking
process and also restores the domains and set of neighbours of all the variables.
To reduce computational time when we deal with CSPs with convex domains, we have implemented Bounds Arc Consistency for discrete CSPs (Lhomme, 1993). The main feature of this
59

C LIMENT, WALLACE , S ALIDO & BARBER

Algorithm 2: GAC3-Nk : Global Arc Consistency algorithm
Data: P, s, Xs , Nk , ‚äï, k, lb, nbF
Result: D, Nk , nbF
Q ‚Üê {(x, c), ‚àÄc ‚àà C, ‚àÄx ‚àà var(c)} // var(c) is the scope of c
while Q 6= ‚àÖ do
(x, c) ‚Üê takeElement(Q);
seekD ‚Üê seekSupport(x, D(x), c); // Found support for all D(x) for c?
if D(x) = ‚àÖ then
nbF ‚Üê nbF + 1; // number of failures
return false
if not seekD then
Q ‚Üê Q ‚à™ {(y, c‚Ä≤ ), ‚àÄc‚Ä≤ ‚àà C ‚àß c‚Ä≤ 6= c ‚àß ‚àÄx, y ‚àà var(c‚Ä≤ ) ‚àß x 6= y}
if x ‚àà Xs then
i ‚Üê 1;
repeat
update Ni (x, s(x), s, ‚äï) applying Equation 1;
seekN ‚Üê seekSupport(x, Ni (x, s(x), s, ‚äï), c);
i ‚Üê i + 1;
until seekN = false ‚à® i > k;
Nk (x, s(x), s, ‚äï) ‚Üê Ni (x, s(x), s, ‚äï)
return f (s, k, ‚äï) > lb // See Equation 2

consistency technique is that the arc consistency is restricted with respect to the bounds of each
convex domain. Thus, including it in the search algorithm only affects to the seekSupport function, which instead of seeking for a support for all the set of values, just checks the minimum and
maximum bounds. Note that this implementation is not necessary for the search of robust and stable solutions; however it allows a significant reduction of the search time. We only apply bounds
consistency to the tentative values of the assignment but not to their set of neighbours, since they
require a complete consistency check. Otherwise there could exist infeasible gaps, which would
break the contiguity requirement that ensures minimum distances to the bounds.

8. Case Study: Searching for Robust and Stable Schedules
There are some types of real life problems whose structure can provide us with specific information
about their dynamism. In this section we analyze a well known type of problem from the literature:
scheduling problems. These problems can be converted into satisfiability problems by fixing a
maximum makespan, and they can then be modeled as CSPs. The CSP modeling usually consists
of associating the start or end time of each task with a particular variable (in this paper we use the
start time). The domain associated with a variable represents the possible time units, and by means
of them it is possible to fix a maximum desired makespan. Finally, the duration of the tasks and
their order (if it exists) can be fixed by means of CSP constraints.
In this section, we will first explain some robustness scheduling measurement units, and then
we describe the objective function for CSPs that model scheduling problems and give an example
of its application.
60

ROBUSTNESS AND S TABILITY IN C ONSTRAINT P ROGRAMMING UNDER DYNAMISM AND U NCERTAINTY

Algorithm 3: MGAC3-Nk : Maintaining Global Arc Consistency
Data: P, s, Xs , Nk , ‚äï, k, lb, nbF, C, ub
Result: s, Nk , lb
select x ‚àà X \Xs ; // dom/wdeg heuristic
Xs ‚Üê Xs ‚à™ x;
save D and Nk ;
while D(x) 6= ‚àÖ ‚àß nbF < C do
select min(v) ‚àà D(x); // Heuristic 1: lexicographical value order
select v ‚àà D(x), max{|Nk (x, v, s, ‚äï)|} starting by intermediate values; // Heuristic 2
s ‚Üê s ‚à™ {x = v}
D(x) ‚Üê v;
if GAC3-Nk (P, s, Xs , Nk , k, lb, nbF ) then
if Xs = X then
// New solution found
lb ‚Üê f (s, k, ‚äï);
if lb = ub then
return true // Best possible sum achieved
C ‚Üê ‚àû; // restarting-completion
return false // restarting-scratch
if MGAC3-Nk (P, s, Xs , Nk , k, lb, nbF, C, uB) then
return true
restore D\D(x) and Nk ;
s ‚Üê s\{x = v};
Xs ‚Üê Xs \x;
return false

8.1 Robustness Measurement in Scheduling
In this section, we introduce several criteria for measuring the scheduling robustness. For such
purpose, we use the terms buffers and slack to refer to the spare time between related tasks. There
are two main factors that enhance the capability of a schedule to absorb unexpected delays in its
activities: the number of buffers and their duration. Ideally, according to the robustness criterion,
a buffer time should be as long as possible because the longer it is, the longer are the delays that
is able to absorb. For this reason another straight-forward robustness measurement was proposed
by Leon et al. (1994) as the slack average in the schedule. The combination of the duration of
the buffers and their distribution across the schedule provides a more accurate robustness measure
s
denoted as Rslack
. It is a slight variant of a measure introduced by Surico et al. (2008) that consists
in maximizing the slack average (shorted as avg) and minimizing their standard deviation (shorted
as std) for a schedule s. For regulating the importance of the standard deviation term, the authors
use a parameter called Œ±, which can take any value in the interval [0.2,0.25], according to the authors
considerations.

s
Rslack
= avg(slack) ‚àí Œ± std(slack)

61

(3)

C LIMENT, WALLACE , S ALIDO & BARBER

Another means of measuring the robustness of a system, defined by Kitano (2007) is related
to its resistance to perturbations having a certain probability of occurrence. This approach was
extended by Escamilla et al. (2012) to scheduling problems in which the probabilities of task delays
s , where Z is the discrete set of unexpected
are unknown. The robustness measure is denoted as RF,Z
delays in the duration of tasks, F measures whether the schedule s is still feasible after the disruption
1
, ‚àÄz ‚àà Z is the probability for
(F (z) = 1 when is satisfiable, otherwise F (z) = 0) and p(z) = |z|
an instance z ‚àà Z (i.e., all delays are considered to have the same probability of occurrence).
s
RF,Z
=

X

p(z) ‚àó F (z)

(4)

z‚ààZ

8.2 Objective Function for Scheduling
In CSP models of scheduling problems, the fact that domain values represent time units has implications with respect to measures of robustness and stability. For these problems, when a value of the
solution is lost, lower values cannot be used for replacing this unfeasible value because they represent time units that have already taken place. Thus, if there is an incident, and the time point t is not
available, neither are the values lower than t. Therefore, having lower feasible neighbours does not
improve the robustness nor the stability of a solution of a CSP that models a scheduling problem
(since they cannot absorb delays nor be used as repairable values). Given these characteristics, the
main desirable objective is to search for neighbours greater than the value assigned. To do this, we
fix the set of operators to ‚äï = {{>, +}} for scheduling problems. This is illustrated below.
Example 8.1 We consider a toy scheduling problem with two tasks: T0 and T1 . Both have a duration of two time units and they must be executed in the order listed. The maximum makespan
allowed is six time units. In Figure 2 we can see the associated CSP model and its solution space.
The variables X0 and X1 represent the start times of tasks T0 and T1 , respectively. The domain
of both variables (represented by discontinuous lines) is [0 . . . 4], which preserves the maximum
makespan of six time units (the maximum start time of a task is the maximum makespan minus the
duration of the aforesaid task). There is one constraint controlling the execution order of the tasks
(T0 must start before T1 ), which is C0 : X1 ‚â• X0 + 2. The solution space is represented by a dark
gray area, where there are six solutions (black dots).
If no specific information is given about the dynamic environment, which schedule is the most
robust? As stated in Section 8.1, the greater the number of time buffers and the greater their duration, the more robust the schedule is. But how can we determine which solution of the modeled
CSP meets these requirements? The answer is obtained by determining the feasible contiguous
neighbours with greater values, located at distances less or equal to k from the solution. However,
depending on the value of k, we will either prioritize the selection of schedules with a large number
of short time buffers or we will prioritize the selection of schedules with lower number of long time
buffers. The number of greater feasible neighbours associated with a value of a variable corresponds
to the total amount of slack that is located after the task represented by this variable. Thus, the slack
is able to absorb a delay in the previous task as long as itself, without modifying the other tasks of
the present schedule (robustness feature). Furthermore, if the slack following a task is not sufficient
to absorb a delay, the start of the following task can be delayed (after repairing the broken assigned
value) if there is a long enough buffer associated with this later task (stability feature).
62

ROBUSTNESS AND S TABILITY IN C ONSTRAINT P ROGRAMMING UNDER DYNAMISM AND U NCERTAINTY

3

4

X1

0

1

2

C0

0

1

2

3

4

X0

Figure 2: CSP model associated with Example 8.1 and its solution space.
For the above example, which is a two-dimensional CSP representing a scheduling problem
with two tasks, there are three schedules that are most robust according to the criteria stated above.
If we maximize the sum of distances for greater values located at a distance one (k = 1) from
each value of the assignment, we obtain the solution shown in Figure 3(a), whose sum is f (s0 , k =
1, {>, +}) = 1 + 1. The first number is Nk (x0 , v0 , s, {>, +}) and second is Nk (x1 , v1 , s, {>, +}),
where v0 and v1 are the values assigned to the variables x0 and x1 respectively. Note that the sums
for the neighbours greater than the solution values and located at a distance one from them are
f (s1 , k = 1, {>, +}) = 1 + 0 and f (s2 , k = 1, {>, +}) = 0 + 1, respectively. In the following (a)
figures, the greater neighbours are indicated by an elipse, with an arrow pointing to the solution (the
circled dot). In the associated (b) figures, the schedules equivalent to the solutions marked in (a)
are shown. Note that the greater neighbours indicated in the (a) figures correspond to the slack in
the (b) figures. For instance, in Figure 3(b) each task has an associated slack of duration one, which
corresponds to the existence of one greater neighbour for each value assignment in Figure 3(a).
On the other hand, if we maximize the sum of greater neighbors values for k > 1, the three
solutions represented in Figures 3(a), 4(a) and 5(a) are all classified as best solutions according to
our objective function. The computation of the sum of neighbours located at a distance lower or
equal to k, for k > 1 is: f (s0 , k > 1, {>, +}) = 1 + 1 (Figure 3(a)), f (s1 , k > 1, {>, +}) = 2 + 0
(Figure 4(a)) and f (s2 , k > 1, {>, +}) = 0 + 2 (Figure 5(a)). Note that the schedules in Figures
4(b) and 5(b) each have only one time buffer, but its duration is two time units, unlike the schedule
represented in Figure 3(b) that has two time buffers of one unit each. Thus, by fixing k = 1 we
prioritize the seek of a high number of time buffers. However, for greater k values, we prioritize
their duration, even if in this case their distribution may not be optimal.
If we consider the stability of the solutions, small modifications in the solutions are always
preferred. In this case, if it is not possible that a task starts at the scheduled time, by reassigning its
start time to a closer greater neighbour, we are composing another schedule that is very similar to the
original one. Therefore, the search of the feasible greater neighbours (which introduces buffers into
the tasks of the schedule) are improving both, the robustness and stability of the obtained schedules.
The search of schedules with buffers that are up to k time units can also be achieved with model
reformulation techniques. This is achieved by adding two variables to each original variable (the
variables that represent the start time of the tasks). One variable represents the slack that is following
the task and the other variable represents the sum of this slack and the original starting time. For
63

C LIMENT, WALLACE , S ALIDO & BARBER

0

1

2

X0

4
3

Slack

T0

2

X1

0
(a) Solution space.

1

2

T1
3

4

Slack
5

6

(b) Schedule of the marked solution.

Figure 3: Robust schedule s0 = (x0 = 0, x1 = 3) for Example 8.1 and its greater neighbours for
k ‚â• 1.

0

1

2

X0

4
3

Slack

T0

2

X1

0
(a) Solution space.

1

2

3

T1
4

5

6

(b) Schedule of the marked solution.

Figure 4: Robust schedule s1 = (x0 = 0, x1 = 4) for Example 8.1 and its greater neighbours for
k > 1.

0

1

2

X0

4
3

T0

2

X1

0
(a) Solution space.

1

Slack

T1
2

3

4

5

6

(b) Schedule of the marked solution.

Figure 5: Robust schedule s2 = (x0 = 0, x1 = 2) for Example 8.1 and its greater neighbours for
k > 1.

64

ROBUSTNESS AND S TABILITY IN C ONSTRAINT P ROGRAMMING UNDER DYNAMISM AND U NCERTAINTY

instance, let pi be the starting time of the task xi . Thus, we would add the constraint p‚Ä≤i = pi + si ,
where si represents the slack associated to task xi . In addition, depending on the maximum desired
duration of the buffers, another constraint may be added, such as si ‚â§ k. In this case, the delay is
up to k timeP
units. In addition, an objective function that express the goal of maximizing the total
slack (max ni=1 si ) must be defined.
Furthermore, other proactive specific approaches for scheduling problems that do not involve
CSP can be found in the Herroelen and Leus (2005) survey. The main advantage of the approach
presented in this paper over these proactive alternatives for scheduling problems is that our approach
can be applied when all slack-values require a consistency check. This requirement is necessary in
scheduling problems where intermediate non-valid slack values are possible. Examples of this type
of problem are scheduling problems with limited machine availability (see, for instance Schmidt,
2000). In these cases, some machines are unavailable in certain time intervals; for this reason, tasks
that require these resources cannot be executed in such time units. The same happens with some
scheduling with operators, where the workers have some breaks during the day. Moreover, there
also exist reactive approaches, which re-schedule the activities when a disruption invalidates the
original schedule found. For example, solving dynamic Resource-Constrained Project Scheduling
Problems (RCPSP) (Elkhyari, GueÃÅret, & Jussien, 2004).

9. Experimental Results
In this section, we present results from experiments designed to evaluate the performance of Algorithm 1. Solutions obtained by the restarting-completion procedure are referred to as ‚Äúneighbour
solutions‚Äù in the graphs and tables throughout this section. Solutions obtained by restarting-scratch
are referred to as ‚Äúneighbour solutions(R)‚Äù. Experiments were done with random problems and
benchmarks presented in the literature. The random instances generator (RBGenerator 2.0), the
benchmarks and the parser for the XCSP instances can be found on Christophe Lecoutre‚Äôs web
page 1 .
In addition to assessing search Algorithm 1, we also evaluated two other proactive methods
that do not require specific additional information about the dynamism. One of them is the WCSP
modeling technique (Climent et al., 2013), which is based on the same dynamism assumptions
as in the present work. The solutions obtained by this technique are referred to as ‚ÄúWCSP-mod
solutions‚Äù. We have not evaluated this approach with scheduling problems because it does not
consider the adaptation for scheduling problems that we have presented in Section 8.2 (neighbouring values that are lower in magnitude are not considered). The other proactive approach
maximizes the (1, 0)-repairability (see Section 3.2). To implement this technique, we have modified Algorithm 1 (B&B-Nk ) by exchanging MGAC3-Nk and GAC3-Nk algorithms for MAC+ and
GAC+ (Hebrard, 2006), respectively. The solutions obtained by this technique are referred to as
‚Äú(1, 0)-super-solutions‚Äù. In addition, solutions of an ordinary CSP solver have been analyzed (referred to as ‚Äúsimple solutions‚Äù), in order to detect whether there are cases in which all solutions
have similar robustness and/or stability.
In addition, we added the geometric restart (restarting-completion) and bounds consistency techniques explained in Section 7 to the ordinary CSP solver and the super-solutions solver in order to
provide them with these computational advantages. For the approach that models CSPs as WCSP,
1. http://www.cril.univ-artois.fr/ lecoutre/index.html

65

C LIMENT, WALLACE , S ALIDO & BARBER

we have used the same solver as the one used by Climent et al. (2013): ToulBar22 . It was necessary
to use a different solver for the evaluation of this technique because this approach requires a WCSP
solver. For these other approaches evaluated, values were selected in lexicographical order and a
time cutoff was fixed to 100 seconds. Experiments were run on an Intel Core i5-650 Processor (3.20
Ghz). In addition, for the geometric restart, the scale factor was fixed to 10 and the multiplicative
factor to 1.5.
The evaluation is based on the two main features of solutions obtained by proactive approaches:
stability and robustness. In all the tables of this section, the best robustness/stability results obtained
are marked in bold. In accordance with the assumptions laid out in the previous sections, we use
the robustness and stability measures described in Section 4. Here, we note that the ordinary CSP
solver and the super-solutions solver do not consider the same dynamism assumptions as the WCSP
modeling technique and the approach presented in this paper. That is to say, they do not consider
possible future restrictive modifications over the bounds of the solution space of CSPs with ordered
domains. Regarding stability, only the technique that maximizes the (1, 0)-repairability searches for
stable solutions according to Definition 3.1. However, as mentioned above, in this paper we analyze
a more precise concept of stability for CSPs with ordered domains, the (1, 0, c)-repairability (see
Definition 4.3).
9.1 Robustness Analysis with General CSPs
In this section we analyze the robustness and stability of solutions obtained over a wide range of
tightness values. For this purpose, random CSPs were generated by the RBGenerator 2.0, which
have non-convex constraints represented extensionally. Because of the non-convexity of the domains, the bounds consistency technique cannot be used. The CSPs generated have 25 variables
with domain size 30 and 200 binary constraints. Domain values are integer values in the interval
[0, 29]. The tightness values analyzed are 0.1, 0.2, and 0.3. (Note: 0.34 is the critical value of the
tightness of this CSP typology.) For each tightness we generated 10 random instances that were
solved by Algorithm 1 for k = 1. Because in this analysis we deal with the general case of CSPs
with ordered domains (see Section 5.1) we have fixed the set of operators for our search algorithm
to ‚äï = {{>, +}, {<, ‚àí}} and the value selection is heuristic 2 (see Section 7), which maximizes
|Nk (x, v, s, ‚äï)| starting from intermediate values.
As mentioned previously, it is not usually feasible to compute the complete set of solutions of
a CSP. For this reason, in order to measure the robustness of the solutions obtained with the four
approaches, we have sampled the closest surrounding neighbours (k = 1). Thus, if a closest surrounding neighbour is not a solution of the CSP, it means that the analyzed solution could become
infeasible after a change of magnitude one or greater to the original bound/s that invalidate such
neighbour. On the other hand, if the neighbour is a solution of the CSP, this means that this restrictive modification would not invalidate the analyzed solution. Therefore, satisfiability checking of a
random sample of the neighbours of the solutions provides an estimation of the likelihood that the
solutions will remain valid, that is to say, an estimation of their robustness.
For sampling the feasibility of the neighbourhood of the solutions, we made a certain number
of random modifications of magnitude k over the values assigned to the variables of the solutions.
The number of values assigned to the variables of the solutions that are modified, is denoted as
nbV arM od ‚àà [1 . . . 10]. For each value of nbV arM od, we sampled 500 neighbours over the
2. http://carlit.toulouse.inra.fr/cgi-bin/awki.cgi/ToolBarIntro

66

ROBUSTNESS AND S TABILITY IN C ONSTRAINT P ROGRAMMING UNDER DYNAMISM AND U NCERTAINTY

solution analyzed and checked their feasibility. The average number of feasible neighbours for each
type of solution are shown in Table 9.1. It can be observed that Algorithm 1 with either restarting
option dramatically outperformed the ordinary CSP solver and the technique that maximizes the
(1, 0)-repairability. It also outperformed the WCSP modeling approach for tightness 0.2 and 0.3.
The weakness of the latter approach is that it obtains robustness approximations in problems in
which there is a high relation between constraints, because it computes feasible neighbours for
each constraint boundary. Thus, the higher the tightness, the higher the likelihood of the existence
of neighbour tuples that are feasible for one constraint/domain but not for another one. These
conflicting situations are less frequent in very unconstrained instances. For this reason, for tightness
0.1 the performance of the modeling approach is better. However, it only obtains better robustness
results than Algorithm 1 for highly unrestricted instances for high nbV arM od values. In regard
to our Algorithm 1, the restarting-completion option provides better results than restarting-scratch
(differentiated with ‚ÄúR‚Äù) for very unconstrained instances, while they preform similarly for higher
tightness values. In Figure 6(b) we selected the nbV arM od = 2 to emphasize trends in robustness
and stability as a function of varying tightness.
tightness

0.1

0.2

nbV arM od

2

Approach

Average Number of feasible neighbours in the sample

simple
super
WCSP-m
neigh
neigh(R)

7.2
8.8
152.4
206.8
191.6

4

0.6
0.6
60.2
75
74.4

6

0
0
24.5
27.4
24.4

8

0
0
12.8
10.8
7.9

10

0
0
5.8
3.8
2.9

2

0.2
1
5.7
36.2
33.9

0.3
4

0
0
0.1
2.5
2

6

0
0
0
0
0.5

8

10

2

4

6

0
0
0
0
0.1

0
0
0
0
0

0
0.4
0.4
2.8
2.5

0
0
0.1
0.1
0

0
0
0
0
0

Table 1: Robustness Analysis Based on the tightness (< 2, 25, 30, 200, tightness >).
For the stability measurement, (1, 0, 1)-repairability is used (see Definition 4.3), which measures the number of variables that can be replaced by a value located at a distance of one from the
value assigned without modifying the rest of values in the solution. Stability results are shown in
Figure 6(a). As mentioned, if a solution value is lost, the objective is to find the closest repairable
values. For this reason, our algorithm does not consider feasible values that are k units greater
or smaller than the value assigned, since this could result in future solutions where the Manhattan
distance between the new solution and the original one would be exaggeratedly great (see Section
4). On the other hand, the technique that maximizes the (1, 0)-repairability considers any value
as a repairable value. This fact represents a disadvantage when searching for repairable values in
ordered domains. This can be observed in Figure 6(a), where we can see the poor performance of
super-solutions for the (1, 0, 1)-repairability.
We would like to note that for CSPs that are very highly restricted, the stability and robustness
of the solutions obtained by all the evaluated methods are very similar. This is due to the fact that in
these cases the CSPs have very few solutions and consequently the distances of all solutions from
the bounds is very low. For most of these instances, the number of solutions is so low that the
solutions are scattered within the tuple-space, so the likelihood of a solution being located on the
bounds of the solution space is very high. For the same reason, the likelihood that a variable has a
67

C LIMENT, WALLACE , S ALIDO & BARBER

25
neighbours solution
neighbours (R) solution
simple solution
(1,0)-super-solution
WCSP-mod solution

(1, 0, 1)-repairability

20

15

10

5

0
0.1

0.2
Tightness

0.3

Average sampling of the number of neighbour solutions

(a) Stability analysis
neighbours solution
neighbours (R) solution
simple solution
(1,0)-super-solution
WCSP-mod solution

200

150

100

50

0
0.1

0.2
Tightness

0.3

(b) Robustness analysis for nbV arM od = 2

Figure 6: Combined robustness-stability based on the tightness (< 2, 25, 30, 200, tightness >). The
curves are shifted for improving the clarity of the graph.

68

ROBUSTNESS AND S TABILITY IN C ONSTRAINT P ROGRAMMING UNDER DYNAMISM AND U NCERTAINTY

feasible repairable value that is near-by is very low. It can even be the case that none of the solutions
has an assignment with feasible neighbours located at a distance k. In this case, all the solutions are
equally robust and stable for this k value.
9.2 Scheduling Benchmarks Evaluation
In this section, we evaluate the approaches with scheduling benchmarks from the literature in order
to determine the robustness of schedules obtained over a wide range of k values. We analyzed five
sets of 10 job-shop CSP instances, studied by Sadeh and Fox (1996). Each instance is composed
of 10 jobs of five tasks each and there are five resources. Each job has a random linear sequence
of resources to visit, with the exception of the bottleneck resources, which are visited after a fixed
number of operations (in order to further increase resource contention).
Because this analysis deals with scheduling problems (see Section 8) we have fixed the set of
operators for our search Algorithm 1 to ‚äï = {{>, +}} and the value selection is done with heuristic
1, in which values are selected in lexicographical order. Regarding the other proactive technique
evaluated, the author of the (1, 0)-repairability approach (Hebrard, 2006) made an extension to the
concept of breakage (the loss of an assigned value) for scheduling problems. A breakage for this
kind of problem was considered a delay of duration d in a task. Therefore, only values that are
greater than the value assigned in d time units are considered repairable values. For this reason, for
the evaluation of scheduling problems, we have incorporated this condition to the (1, 0)-repairability
approach. For a proper comparison of this approach with our approaches, we used the same values
for k and d parameters. In the following, in order to avoid term repetition, we assume that d = k.
Note that an ordinary CSP solver does not use this parameter, so it obtains the same schedule for
any value of k.
For measuring the robustness of the schedules obtained, we used the robustness measures introduced in Section 8.1. A first robustness assessment is made by measuring the total slack whose
duration does not exceed k, which is denoted as tS(k). In addition, a more accurate measure is
s
(k) (see Equation 3), which measures the average total slack, minus the standard
also used, Rslack
deviation multiplied by the Œ± parameter. The Œ± parameter was fixed to 0.25, which is inside the
interval that the authors consider appropriate for this parameter. Another robustness measure used
s
is based on the resistance of a schedule faced with perturbations, and is denoted as RF,Z
(see Equation 4), where Z is the set of incidents that consist in delays of durations up to maxd over the tasks.
We have used 2 different values for maxd : 1 and k. In each case, we independently simulated 500
delays up to maxd units with equal probability over the entire schedule and checked if the schedule
remained valid. For the stability measurement, again, (1, 0, 1)-repairability is used (see Definition
4.3), which is equivalent to the measurement of the number of buffers of the schedule, denoted as
nbB. Note that the desired objective is that in cases where repairs are necessary, the start time of a
task is delayed for as short a time as possible.
The following figures and tables show the evaluation for two of the Sadeh problem sets. For the
other problem sets we obtained similar results. We show results for the e0ddr1 and e0ddr2 benchmarks in order to compare robustness and stability of schedules obtained with different numbers of
bottlenecks in the problem (other parameters are fixed). Sadeh stated that the e0ddr1 benchmark
contained just one bottleneck and on the other hand, e0ddr2 benchmark contained two bottlenecks.
Tables 9.2 and 9.2 show the means for the robustness and stability measures for scheduling problems. In addition, other measurements are shown, including the number of schedules obtained nbS,
69

C LIMENT, WALLACE , S ALIDO & BARBER

total number of restarts done by the search algorithm nbR, the total number of nodes explored nbN
and the total number of failures nbF . Figure 7 shows the stability and robustness measurements
s
(k) for the e0ddr1. The horizontal axis
(vertical axis): the mean number of buffers and mean Rslack
of the figures represents the value of the ratio of parameters k.
k

Approach

nbS

nbR

nbN

nbF

nbB

tS(k)

s
Rslack
(k)

s
RF,Z
(1)

s
RF,Z
(k)

1

simple
super
neigh
neigh(R)

1
9.1
12.7
15.5

3.1
3.1
3.1
28.8

208
7770.4
10171.1
2820.4

85
3043.7
2465.9
628.1

16.1
21.4
27.8
31.3

16.1
21.4
27.8
31.3

0.208
0.308
0.436
0.509

0.328
0.434
0.562
0.628

0.338
0.43
0.555
0.618

3

simple
super
neigh
neigh(R)

1
7.3
15.9
15.5

3.1
3.1
3.1
27.7

208
8138.2
5880.5
2406.5

85
2619.2
2485.1
670.5

16.1
18.9
20.9
22.3

44.3
52.4
59.4
62.1

0.555
0.702
0.832
0.886

0.328
0.384
0.424
0.448

0.311
0.36
0.409
0.413

5

simple
super
neigh
neigh(R)

1
7.1
19
12.9

3.1
3.1
3.1
23.8

208
8373.2
3947.7
2082.3

85
2654.2
1674.7
517.8

16.1
19.1
19.5
20.2

67.8
82.9
86.3
87.3

0.832
1.101
1.159
1.182

0.328
0.388
0.396
0.406

0.288
0.343
0.364
0.35

7

simple
super
neigh
neigh(R)

1
6.5
19.9
11.5

3.1
3.1
3.1
21.2

208
8319.1
3205.9
1871.8

85
3219.5
1032.6
489.5

16.1
18.1
18.9
18.7

88.1
101.8
107.8
108.6

1.057
1.298
1.4
1.413

0.328
0.368
0.384
0.376

0.271
0.303
0.331
0.314

9

simple
super
neigh
neigh(R)

1
5.7
20.8
11.4

3.1
3.1
3.1
19.7

208
8715.7
2793.6
1711.4

85
2620.7
974.2
462.8

16.1
17.6
18.6
18.2

105.7
117.6
126.5
126

1.242
1.452
1.602
1.588

0.328
0.358
0.378
0.368

0.257
0.277
0.303
0.293

11

simple
super
neigh
neigh(R)

1
6
19
7.9

3.1
3.1
3.1
16.9

208
8019.9
2518.5
1593.9

85
1775.8
844.4
435.5

16.1
18.2
18.1
18.4

120.5
133.6
140.2
138.8

1.389
1.629
1.72
1.693

0.328
0.37
0.368
0.374

0.244
0.256
0.28
0.277

Table 2: Evaluation of ‚Äòe0ddr1‚Äô benchmark.
As expected, schedules obtained by all of the approaches for the e0ddr1 benchmark are more
robust and stable than those for the e0ddr2 benchmark (see Tables 9.2 and 9.2) From the robustness
analysis, we see that our algorithm for k = 11 (for both restarting options) increased the robustness
s (k) by more than 0.5 units for problems with only one bottleneck. Therefore, as
measure RF,Z
expected, the fewer bottlenecks a scheduling problem has, the more robust the schedule obtained
by our algorithm. Detailed results for all robustness measures are found under columns tS(k),
s
s (1) and Rs (k) in the tables. For instance, for the largest k value analyzed (k =
Rslack
(k), RF,Z
F,Z
11), the total sum of all the buffer times of duration up to k of the schedule obtained by Algorithm
1 for restarting-completion is 140.2 time units for the e0ddr1 benchmark and 109.67 time units for
the e0ddr2 benchmark (more than 30 time units difference). Regarding the stability analysis, our
algorithm for k = 1 restarting-scratch (differentiated with ‚ÄúR‚Äù) found schedules with four mean
number of buffers (nbB) more for the problems with one bottleneck than for the problems with two
70

ROBUSTNESS AND S TABILITY IN C ONSTRAINT P ROGRAMMING UNDER DYNAMISM AND U NCERTAINTY

k

Approach

nbS

nbR

nbN

nbF

nbB

tS(k)

s
Rslack
(k)

s
RF,Z
(1)

s
RF,Z
(k)

1

simple
super
neigh
neigh(R)

0.9
6.5
10.1
12.4

4
3.9
4
25.4

227
8024.33
11264.22
2555.67

98.89
1975.67
1865.22
709.33

14.11
19.89
24.33
27.44

14.11
19.89
24.33
27.44

0.17
0.28
0.37
0.43

0.28
0.4
0.49
0.55

0.28
0.41
0.49
0.55

3

simple
super
neigh
neigh(R)

0.9
4.9
16.4
11.9

4
3.9
3.9
22.2

227
8602.67
7141.78
2282.67

98.89
1198.67
1711.33
503.89

14.11
17.33
20.22
20.11

37.56
47.22
56
55.22

0.44
0.61
0.77
0.76

0.28
0.35
0.4
0.4

0.25
0.32
0.37
0.37

5

simple
super
neigh
neigh(R)

0.9
4.4
17.3
8.6

4
3.9
3.9
18.9

227
9102.78
5755.22
2036.11

98.89
743.67
1657.33
452.78

14.11
17.11
18.11
17.89

55.11
70.89
76.67
73.89

0.63
0.89
0.99
0.95

0.28
0.34
0.36
0.36

0.22
0.29
0.31
0.3

7

simple
super
neigh
neigh(R)

0.9
3.8
15.2
7.7

4
3.9
3.9
17.5

227
9721.67
4903
1827.44

98.89
914.22
1272.56
428.78

14.11
15.78
16.89
17.22

68.22
82.22
88.78
88.67

0.75
0.97
1.09
1.09

0.28
0.32
0.34
0.35

0.2
0.25
0.26
0.26

9

simple
super
neigh
neigh(R)

0.9
3.1
15.7
6.4

4
3.9
3.9
16.2

227
9971
4344.44
1657.56

98.89
959.56
1161.78
449.22

14.11
15.56
16.78
16.78

78.67
92.89
101.11
100.44

0.84
1.06
1.2
1.19

0.28
0.31
0.34
0.34

0.18
0.21
0.24
0.23

11

simple
super
neigh
neigh(R)

0.9
2.3
14.7
5.6

4
3.9
3.9
14.4

227
10698.22
4251.78
1588.89

98.89
1090.44
1223.56
390

14.11
15.22
16.22
16.11

87.44
98.89
109.67
107.67

0.91
1.08
1.25
1.22

0.28
0.3
0.32
0.32

0.16
0.19
0.21
0.2

Table 3: Evaluation of ‚Äòe0ddr2‚Äô benchmark.

71

C LIMENT, WALLACE , S ALIDO & BARBER

32
neighbours solution
neighbours solution (R)
simple solution
(1,0)-super-solution

30

Mean number of buffers

28
26
24
22
20
18
16
1

3

5

7

9

11

k

(a) Stability analysis
1.8
neighbours solution
neighbours solution (R)
simple solution
(1,0)-super-solution

1.6

Mean of RslackS(k)

1.4
1.2
1
0.8
0.6
0.4
0.2
1

3

5

7

9

11

k

(b) Robustness analysis

Figure 7: Combined robustness-stability for k parameter: mean measures for the e0ddr1 benchmark.

72

ROBUSTNESS AND S TABILITY IN C ONSTRAINT P ROGRAMMING UNDER DYNAMISM AND U NCERTAINTY

bottlenecks for the best case. Therefore, as expected, the fewer bottlenecks a scheduling problem
has, the more stable the schedule obtained by our algorithm.
In both tables and the figure, we can see that Algorithm 1 with either restarting option outperformed both the ordinary CSP solver and the technique that maximizes the (1, 0)-repairability.
Furthermore, the analysis of the k parameter shows that when these parameters have the lowest values, the number of buffers of the schedules found by our algorithm are markedly greater than these
two techniques (see Figure 7(a)). In contrast, the improvement in robustness for our algorithm with
respect to the ordinary solver is a little more marked for greater k values. The comparison with the
(1, 0)-repairability technique shows the same tendency for the e0ddr2 benchmark (see nbB in Table
9.2).
Regarding the other robustness measures that are not plotted in the figure but are shown in
s (1) measure and the number of
Tables 9.2 and 9.2, we see that there is a correlation between the RF,Z
s (1) were
buffers. This relation is expected, since the random incidents generated for measuring RF,Z
delays of one unit time. Therefore, the more buffers there are (whatever is their duration) the greater
s
the likelihood that a schedule can absorb delays of one time unit. In addition, the tS(k), Rslack
(k)
s
and RF,Z (k) measures are correlated. Recall that tS(k) is the total slack whose duration does not
s
(k) is its average minus the standard deviation multiplied by an Œ± parameter.
exceed k and Rslack
Therefore, unless the distribution of the slack is very poor, the two values must be proportional.
s
(k), the greater the proportionality with respect the
Note that the lower the Œ± parameter for Rslack
s
other two robustness measures. The RF,Z (k) measure is calculated by generating random delays
up to duration k over the schedule. For this reason, this robustness measure is strongly related
with the two aforementioned. A example of the relation of all the aforementioned measurement
units can be observed in Table 9.2 for k = 11, where the schedules obtained with restarting-scratch
s (1) values, and schedules
option (differentiated with ‚ÄúR‚Äù) have greater numbers of buffers and RF,Z
s
s (k) values. This
obtained with restarting-completion option have greater tS(k), Rslack
(k) and RF,Z
means that the latter has a greater total slack whose duration does not exceed k, but its distribution
is more limited.
In Tables 9.2 and 9.2 we also observe measurements that are not correlated with robustness or
stability, but important information can still be extracted from them. For k > 1, the restartingcompletion for our algorithm finds the greater mean number of solutions (nbS). Only for k = 1
does the restarting-scratch (differentiated with ‚ÄúR‚Äù) find more solutions. The greater k is, the easier
it is to find new solutions whose objective function is better than the maximum one (if the instance
is not highly restricted). For this reason, the mean number of solutions found is greater for high
k values. For both restarting options, the mean number of solutions is considerably higher than
for the approach that maximizes the (1, 0)-repairability. This effect is stronger for greater values
of k because the condition of a repairable value for the latter technique becomes more restrictive.
Moreover, this technique considers all feasible values in the domains as repairable values; as a result,
feasibility checking is slower than for techniques that assume only k neighbours (as our technique
does). As expected, the mean number of restarts (nbR) is much greater for the restarting-scratch
option because the other techniques only restart until finding the first solution. As a consequence,
their mean number of nodes explored (nbN ) and mean number of failures (nbF ) is lower.
The schedules obtained by Algorithm 1 for the lowest k value had the highest number of buffers.
On the other hand, the robustness measures are greater for the greater k values. Depending on the
dynamic nature of the problem, it would be desirable to prioritize between a higher number of
buffers of short duration and a lower number of buffers of long duration (if the two features cannot
73

C LIMENT, WALLACE , S ALIDO & BARBER

both be maximized). Thus, if it is known that the possible future delays will have a duration of at
least d time units, it does not make sense to compute k values lower than d because the obtained
time buffers could not absorb the delay. On the other hand, if it is known that possible future delays
cannot have a duration greater than d, then it does not make sense to compute k values greater than
d time units because this may decrease the number of buffers. Hence, the more information about
possible future changes we have, the better the robustness results we can obtain. However, even
if this information is unknown, we can obtain a schedule with certain level of both robustness and
stability by setting k to an intermediate value in Algorithm 1.
34
neighbours solution
neighbours solution (R)
simple solution
(1,0)-super-solution

32

Mean number of buffers

30
28
26
24
22
20
18
16
10

20

30

40

50
60
Time(s)

70

80

90

100

70

80

90

100

(a) k = 1
34
neighbours solution
neighbours solution (R)
simple solution
(1,0)-super-solution

32

Mean number of buffers

30
28
26
24
22
20
18
16
10

20

30

40

50
60
Time(s)

(b) k = 7

Figure 8: Mean number of buffers over the time intervals for the e0ddr1 benchmark.
The above evaluation consists of analyzing the best results obtained for each technique for the
fixed cutoff time. However, we also wanted to analyze the change in the degree of robustness and
stability of the schedules found over the time. For this evaluation, we used the e0ddr1 benchmark
and determined the mean for 50 instances for each interval of time with a discretization of 10
74

ROBUSTNESS AND S TABILITY IN C ONSTRAINT P ROGRAMMING UNDER DYNAMISM AND U NCERTAINTY

seconds. Figures 8(a) and 8(b) show the mean number of buffers found by each approach for k = 1
and k = 7. Other measures are not shown since similar trends were found in these cases. We would
like to highlight that after 20 seconds the simple solution technique does not find better schedules
because it only searches for one schedule for each instance (which is done in less or equal to 20
seconds). The most remarkable aspect is that for k = 1 Algorithm 1 for both restarting options
obtains a greater number of buffer times than the approach that maximizes the (1, 0)-repairability,
for k = 1 for all time intervals (see Figure 8(a)).
On the other hand, Figure 8(b), which represents k = 7, shows more unstable results. Since it is
difficult to find buffers with up to seven time units, it may happen than our algorithm sacrifices some
shorter buffers in order to find one buffer of seven time units. Thus, even if the overall tendency
is for the measure to increase over the time, it is not entirely uniform. On the other hand, the
upward shape of the trend for the approach that searches for super-solutions is due to the fact that
it considers values as repairable if there is any possible alternative for the start time of a task that
follows a task sharing the same resource, which is not equivalent to have a slack associated to this
task in the schedule. For this reason, schedules that are better for this technique may contain lower
number of buffers. This feature is more marked for greater values of k, since the repairable values
have to be at least k unit times greater than the assigned values, and therefore it is more unlikely to
find repairable values that are close to the assigned ones.
It can be concluded that in general the approach that maximizes the (1, 0)-repairability finds
solutions with lower robustness and stability (considering the closest repairable values) than our
approach for the aforementioned reason. Another disadvantage is that it only assumes that delays
are of duration d. Thus, only values greater than this value are considered as repairable values.
However, we consider up to k neighbours and therefore, slacks of duration lower than k are also
valued by our objective function in contrast to the (1, 0)-repairability objective function.
On the basis of this evaluation, we can conclude that the difference in performance between the
two restarting options (restarting-completion and restarting-scratch) is not very significant. Sometimes, the time needed to restart from scratch after each solution makes this option less effective
than restarting-completion. In other cases, the restarting-completion option loses time in branches
in which there are no better solutions, while restarting-scratch explores other branches. For instance,
for the random experiments, restarting-completion provided slightly better results generally (see Table 9.1 and Figure 6(b)), while for the scheduling problems, restarting-scratch obtained schedules
that were a bit more robust and stable for lower k values (see k ‚àà [1, 5] in Figure 7). For greater k
values, both restarting options gave similar results.

10. Conclusions
In this paper we extend the concept of robustness and stability for CSPs with discrete and ordered
domains where only limited assumptions can be made about changes in these problems. In particular, there are no uncertainty statistics nor probabilities about the incidences that can occur in the
original problem. In this context, it is reasonable to assume that the original bounds of the solution
space may undergo restrictive modifications, such as introduced by Climent et al. (2013). Therefore, the main objective in searching for robust solutions is to find solutions that maximize all the
Euclidean distances from the dynamic bounds of the solution space. On the other hand, the main
objective in searching for stable solutions in terms of repairable variables is to find solutions whose
repairable values are as close as possible to the broken assignments.
75

C LIMENT, WALLACE , S ALIDO & BARBER

In this paper, we present a new search algorithm that combines criteria for both robustness
and stability in this framework. The algorithm developed in this paper searches for a solution
that maximizes the sum of contiguous feasible surrounding neighbours at distances of k or less
from the values of the solution. The obtained solutions have a high probability of remaining valid
after possible future restrictive changes over the constraints and domains of the original problem
(robustness criterion), and they also have a high number of variables that can be easily repaired with
a value at a distance lower or equal to k if they undergo a value loss (stability criterion).
We have evaluated the new algorithm in experiments on well-known scheduling benchmarks
as well as random CSPs. We have shown that both versions of the new algorithm outperform
three other approaches evaluated: the ordinary CSP solvers, the technique that maximizes the
(1, 0)-repairability, and the approach that models CSPs as WCSPs under many conditions where
there are real differences in the robustness of solutions that might be obtained. The latter occurs
when the problem is not so constrained that there are only a few valid solutions. With respect
to the two restarting options developed for our algorithm, we found that their performance is not
significantly different, although in certain situations there is an advantage of one over the other.
For slightly constrained CSPs, our algorithm obtains solutions with the greatest number of closer
neighbour solutions, the greatest (1, 0, 1)-repairability and highest values for specific measures of
scheduling robustness. Furthermore, we have shown that by increasing k for large problems, we
can also increase the robustness, although it may happen that (1, 0, 1)-repairability decreases. For
instance, with scheduling problems the schedules obtained with lower k values tend to maximize
the number of buffers even if their size is small. However, the computation of higher k values tends
to give priority to the duration of the buffers and as consequence, the number of buffers obtained
can be lower. Therefore, depending on the dynamic nature of the problem, it would be desirable to
prioritize between a higher number of short buffers or a lower number of long buffers (if it is not
possible to maximize both features).
The extension of the robustness and stability definition for CSPs with discrete and ordered
domains and the development of a search algorithm for finding robust and stable solutions in this
context, are useful and practical in many real life situations where problems can undergo restrictive
changes and there is the added difficulty that information about the possible future changes is limited
or non-existent. Even under these difficult conditions, our search algorithm is able to provide stable
and robust solutions. Finding solutions located far away from the dynamic bounds is important
when we face restrictive modifications over the bounds of the solution space. Moreover, in cases
where a value is lost, it is important to replace it by a nearby value in order to have a solution as
similar as possible to the original one. This closeness feature is handled by our algorithm but is not
by the approach that searches for super-solutions.

Acknowledgments
This work has been partially supported by the research project TIN2010-20976-C02-01 and FPU
program fellowship (Min. de Ciencia e InnovacioÃÅn, Spain). We wish to thank Dr. Christophe
Lecoutre and Dr. Diarmuid Grimes for their assistance.
76

ROBUSTNESS AND S TABILITY IN C ONSTRAINT P ROGRAMMING UNDER DYNAMISM AND U NCERTAINTY

References
Bessiere, C. (2006). Constraint propagation. Foundations of Artificial Intelligence, 2, 29‚Äì83.
Boussemart, F., Hemery, F., Lecoutre, C., & Sais, L. (2004). Boosting systematic search by weighting constraints. In Proceedings of the 16th European Conference on Artificial Intelligence
(ECAI-04), Vol. 16, p. 146.
Climent, L., Wallace, R. J., Salido, M. A., & Barber, F. (2013). Modeling robustness in CSPs as
weighted CSPs. In Proceedings of the 10th International Conference on Integration of Artificial Intelligence and Operations Research techniques in Constraint Programming (CPAIOR13), pp. 44‚Äì60.
Elkhyari, A., GueÃÅret, C., & Jussien, N. (2004). Constraint programming for dynamic scheduling
problems. Hiroshi Kise, editor, ISS‚Äô04 International Scheduling Symposium, pp. 84‚Äì89.
Escamilla, J., Rodriguez-Molins, M., Salido, M., Sierra, M., Mencƒ±ÃÅa, C., & Barber, F. (2012). Robust solutions to job-shop scheduling problems with operators. In 24th IEEE International
Conference on Tools with Artificial Intelligence (ICTAI-12), pp. 209‚Äì306.
Fargier, H., & Lang, J. (1993). Uncertainty in Constraint Satisfaction Problems: a probabilistic approach. In Proceedings of the Symbolic and Quantitative Approaches to Reasoning and Uncertainty (EC-SQARU-93), pp. 97‚Äì104.
Fargier, H., Lang, J., & Schiex, T. (1996). Mixed Constraint Satisfaction: A framework for decision
problems under incomplete knowledge. In Proceedings of the 13th National Conference on
Artificial Intelligence (AAAI-96), pp. 175‚Äì180.
Fowler, D., & Brown, K. (2000). Branching Constraint Satisfaction Problems for solutions robust
under likely changes. In Proceedings of the International Conference on Principles and
Practice of Constraint Programming (CP-2000), pp. 500‚Äì504.
Fu, N., Lau, H., Varakantham, P., & Xiao, F. (2012). Robust local search for solving RCPSP/max
with durational uncertainty. Journal of Artificial Intelligence Research, 43, 43‚Äì86.
Hebrard, E. (2006). Robust Solutions for Constraint Satisfaction and Optimisation under Uncertainty. Ph.D. thesis, University of New South Wales.
Hebrard, E., O‚ÄôSullivan, B., & Walsh, T. (2007). Distance constraints in Constraint Satisfaction. In
Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07),
pp. 106‚Äì111.
Herroelen, W., & Leus, R. (2005). Project scheduling under uncertainty: Survey and research potentials. European Journal of Operational Research, 165(2), 289‚Äì306.
Kitano, H. (2007). Towards a theory of biological robustness. Molecular Systems Biology, 3(1).
Larrosa, J., & Schiex, T. (2004). Solving weighted CSP by maintaining arc consistency. Artificial
Intelligence, 159, 1‚Äì26.
Leon, V., Wu, S., & Robert, H. (1994). Robustness measures and robust scheduling for job shops.
IIE transactions, 26(5), 32‚Äì43.
Lhomme, O. (1993). Consistency techniques for numeric CSPs. In Proceedings of 13th the International Joint Conference on Artificial Intelligence (IJCAI-93), Vol. 13, pp. 232‚Äì232.
77

C LIMENT, WALLACE , S ALIDO & BARBER

Mackworth, A. (1977a). Consistency in network of relations. Artificial Intelligence, 8, 99‚Äì118.
Mackworth, A. (1977b). On reading sketch maps. In Proceedings of the 5th International Joint
Conference on Artificial Intelligence (IJCAI-77), pp. 598‚Äì606.
Mohr, R., & Henderson, T. C. (1986). Arc and path consistency revisited. Artificial intelligence,
28(2), 225‚Äì233.
Rossi, F., Venable, K., & Yorke-Smith, N. (2006). Uncertainty in soft temporal constraint problems:
a general framework and controllability algorithms for the fuzzy case. Journal of Artificial
Intelligence Research, 27(1), 617‚Äì674.
Sadeh, N., & Fox, M. (1996). Variable and value ordering heuristics for the job shop scheduling
Constraint Satisfaction Problem. Artificial Intelligence, 86(1), 1‚Äì41.
Schmidt, G. (2000). Scheduling with limited machine availability. European Journal of Operational
Research, 121(1), 1‚Äì15.
Surico, M., Kaymak, U., Naso, D., & Dekker, R. (2008). Hybrid meta-heuristics for robust
scheduling. ERIM Report Series Reference No. ERS-2006-018-LIS, Available at SSRN:
http://ssrn.com/abstract=902747.
Verfaillie, G., & Jussien, N. (2005). Constraint solving in uncertain and dynamic environments: A
survey. Constraints, 10(3), 253‚Äì281.
Wallace, R., & Freuder, E. (1998). Stable solutions for Dynamic Constraint Satisfaction Problems.
In Proceedings 4th International Conference on Principles and Practice of Constraint Programming (CP-98), pp. 447‚Äì461.
Walsh, T. (1999). Search in a small world. In Proceedings of the International Joint Conference on
Artificial Intelligence, Vol. 16, pp. 1172‚Äì1177.
Walsh, T. (2002). Stochastic Constraint Programming. In Proceedings of the 15th European Conference on Artificial Intelligence (ECAI-02), pp. 111‚Äì115.
Yorke-Smith, N., & Gervet, C. (2009). Certainty closure: Reliable constraint reasoning with incomplete or erroneous data. Journal of ACM Transactions on Computational Logic (TOCL),
10(1), 3.

78

