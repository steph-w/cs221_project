
	
 
			 ! #"$ % 
'&)(+*, --.0/2113541361

789:;  <=,,5>
-
1?A@9	%&<B.0>
-.

CEDGFHJILKMFNPORQTSUOVHJDGF0DWQX+HJY[Z\QO^]TX_Y;OV`bacOVDdX
e HJfhgiOVj0NVXlknmnHJYoQpF0OVH

e jqSrgiKMD

sitou'vuwowyx{z}|~ยuย

ยยยยยdยยpยVยยย2ย+ย=ยยยย

ย$ย5ย)ยJยยย8ยdยiยยยpยยยAยยยยย$ย+ย)ยยยยกยยฃยขยคยฅยฆ0ยง
ยจยคยฉยฆR
ยช ยยยซยขยฌยฉยญยฆยฎยฎยคยฆ0ยงBย ยยฐยฏ
ยฑยฒยยฑ

ยงยยฆ)ยณ_ยดยตยถ#ยท8ยธยทยAยยนยยยซยขยฉยยญยฆ0ยฎยฎยคยฆยงยบยยปยฆ
ยดยยฆ6ยผnยฝiยพยยฟ$ยง5ยตยขยร6ยฆ

รรร)รยรยฃรรpรยกร
รยรAรรยร ร5รรยฃรรร8รรรรรยรรรยกรรรรร8รAร6ร5รรร8รยรยรยฃรรรรรรรยร
รยฃรรBร6รnรรรยฌร0ร6ร8รยร5รVร5รรiร5ร8รยร6ร
รรร0รยบร0รรรรยฌร0รยรยรก[รรยรยฃรข
ร 5ร ร
รAรรรยรBร6รร6รรร รกรยร0ร5รรรยรรครฃร6รยรรฅรร8รฆรร8รง$รAรฆรรรรจiร6รรรฆJรฉ$ร5รยรฆรรPรฆรรยรชยรPรร5รVรยรกyยจยฉยงยรยฉยยฃยง6ยตยฎรฃร6รยรฃยกรร5ร5รรร8ร+รAรก#ร5รร
รรยรร ร5ร
รAรรรยร#รรรรซ#รร5รชยรงรAร6รรร
ร#ร6รAรชร+รรรยร5รรรยร8ร0รยรรร)ยจยฆ0รฌรญยตยขยฌยฉยครoรฃร5รรฃยฌร8ร ร6รรร!รยรยรกWร5รรBรรยรร ร5ร
รAรรรยร6ร8รฉรจรรร8ร รข
รยรฅรรฅรรVรร5ร5รรฆรรรรรจBร5รรAรBยตยฎยฎAร5รร_รรยรร ร5ร
รAรรรยร6รGรฃยกรร6ร ร!ร5รpร5รร+รจยรรรยรรรรฃร6รยรฃยกรร5รรยรฎdรฏยฐรรร5รรรฐรยปรฃยฌรAรฃยกรร!รฉรซoร;รฃร6รยรฃยกรร5ร
รiรรรซรฑรร8ร0รรฆรรฃยฌรยร รรร5รรรยรรครฆรรร5รรยฃรรครยกรรร0รฒร6รรรรจรณรกรดร6รยรฆยรยฌรยร5รcร ร8รฆรรยรยร5รรฐรรรฃร6รยรฃยกรร5ร5รรร8ร_รAรกdรต
ยยฃยขยร0ยฉยคยซยขยยตยฎยฌรรยรยฃรข
ร ร5ร
รAรรรยร6รรรถรดรรยรรณรท0ยค รธยฆ
รยฉยคยฅยฆรรรยรร ร5ร
รAรรรยร6ร
รน_รยรรcรร6ร5รร0ร5รร
รAรฅGรฃร6รยรฃยกรร5ร5รรร8ร)รยรกยร5รรรญรร0รรซoรยร6รชยรง2รกรดรร5ร5รร8ร5รฆรรยร6รยรฉ
รรยรVรAรฅรรฅpร6รรรณรรยรร ร5ร
รAรรรยร6รBรร8ร8รcร5รยรยฌรรญรกรดรรรร6รรรรรAรฅรฎJรบรรiร5รรรซMร5รรAร=รรรรร=ร5รยรฆรรiรรยรรยฃรรร5รรรยรยฌรรฉdร5รร
รรปรรฐร ร5รรยฌร0รPรAรกoร5รยรฅรรยฃร6รรรรร+ร8รAรรรยฌรรรจยรยฌรAร
รAรยร5ร8ร8ร2รฎRรบcรPรฒร6ร รRร
รรAร
รยร0ร5รร6รรรผรVรรณรฃรยร ร6รรรรรฅรรยร)ร5รรร5ร0รBรAรก#ร5รร
รรAร6รรฐรAรรฅรร8ร8รฉยปรซ;รรรฐร
รรฝรซoรJรรAรฆรรJรรฝยง5ยซ!ยซยฉรยจ0ยฆยฉรฎรฝรบcรรณร6รรรรรรรยร5ร6รรรร0รiยญยกยคยฅยซยฉรร6ยซAยขยจยครฐยจยฉ'ยฆยขยกรรพรฉ[รยรร8รซrรฅรรยฃรรAรฅ
รรยรร5รรร ร5ร8รร0รPรซ;รรรฐร
รรญรรรoรRรซoร8รยรชรรกรดรยร6รฆUรAรก$รฃรAร5รยบร0รรร5รรร ร5ร8รร0รรรAรรรญร8รAรรญรยกร+รร
รรรร8รยร!รรรรรiรฟรรถ !รนยร0รรฆVรข
รฃรฅรร0รปยฃรรรรยรถรดรรรร ร5ร!รยรรณรAรกรฟรรถ !รนยรกรดรรoรฃยฌรร5รJร0รรร5รรร ร5ร8รร0รรนรฉรยรรยบรซoรBรฃร6ร8ร5รรยร[รร5ร5รยฃร0รรฐรร6ร8รยบรฃร5รรฃยฌร8ร ร6รรร!รรงรรร
รฃรยร ร6รรรรรฅรฐรAร!รฉรซoรรณร5รรรซUร5รยฌรรPรยรรรรรยรร5รรฐรร6รรยร=รรรรร
รAรยร5รรฐรร6รรรรรรAรก;ร6รรiร6รรAรรร รรPรรยรรรยกรiรฅรรรรร!รAร6รฅรรรรรปยรข
ร6รรรยฃร!รรร5รnรJร รรฅรรยฃร6รรรร$รฉ2รซ;รรรฐร
รรรฅรร8รรร)ร5รยนร6รรรญรฃร6ร8ร5รรยร6รAร5รรรยรยรยรกยร6รรรญรรกรดรยร6รรฆรร8รร6รรรรร8รยรรรซUรฆรร0ร6รรยฃร
รกรดรรyร รรฅรรรรรรจรรยรรณรยฃร8รรยรฆรรฃยฌรยร รรรรจPรกรดรรร0ร5รรรยรรย	
รฅ 
ยฃร8รฎ



รยฃร_รยกร


"!$#&%(')+*",.-/012,43506%7,.'0489%(*:!',;!=<>"1,?;-=%('@)=!-=A*B!=<C-/">"%ED	06%-=A%('F>G,.AA(%7)=,.'04,=H;IJ3506%7,.'0489%(*
K-/">"%06LA(-/A78M%(N!=">"-='O>P<Q!=P",.*G,.-/01R,6*C%('O>G,6",.*G>G,.S+%('504!'*G>G-=%'F>T*"-/>"%(*G<U-=04>"%7!'V"!=W	A7,.;*YXZN[.\*]^
-=A(*G!_06-=AA7,.Sa`b/cdfeUgGh/iQc	e5cNj6eUklb/gmndfHop'q!=SR,6r>G!s-=01K%7,6#=,t>"1%(*r-=%r^Y#/-/%7!LK*r-/"!-=01R,.*u1-n#=,
WN,6,.'_>G%(,.S^P*L01v-=*;D	A(>G,6%('R)9>G,.01K'%(w&LR,.*xXU;-=%'A78y-/0u-='SsK-/>"1s04!'K*"%(*G>G,.'06%(,.*]rXUzx!'F>"-='-/%{^
|.}~$Rย

zx-=0ยOยP!=">"1^

|.}~=~Oย

zx!1Rยยยยย,.'KSR,6*G!'^

WK-=0ยO>G-=0ย;"!04,.*"*ยXUยY-/-=A%(0"ย;ยยIA(A(%7!=>6^ |.}=ย/ยยย

|.}=ย=ย&ยNย

,.*"*%U,6
ย ",=^

|.}=} ]ย!=ย%(!n#=,.,.'O>"*ย>G!V>"1R,

,.01F>G,6ยยยยย,.-/A^ |.}=ย=ย&ย	ย

%('K*GWN,6")R^ |.}=}=ย&ย ยJ"!**G,6.^

|.}=}=ย ]HTยย>"1R,6Tย!="ยV04!'04,6'R,.S+>"1,ย01-/-=04>G,6%(ย.-/>"%7!'r!=<ย06A(-=**G,.*T!=<ยN!A78'R!;%(-=AK"!=WKA(,.;*6^FW	-=*G,.S
!'x>"1R,ย*%7ย6,ย!=<J>"1,.%7YSR!?-=%('*MX ย
XUยR",.LKSR,6.^
`6ยขReยฃd.j6eยX ย

,.01F>G,6n^

|.}=}=ย ]Y!=Y!'ย>"1R,ย*G>GL04>"LR,ย!=<J>"1R,ย04!'*G>G-=%'F>ย'R,6>ยยP!="ย

|.}~/ย ]^A7,.-=SK%('R)r>G!ย>"1R,?",.*G,.'O>"-/>"%7!'@!=<CSR,.04!N!*"%(>"%7!'@,6>"1R!S*B*"LK01t-=*ย>"1R,x`6ย`.ย7j6ยก
,.01O>G,6.^

|.}=}/ย ];!=?>"1R,teUg"jj9`.ยยคยข&d4eยj4gfiQcOยฅยฆX ย

,.01O>G,6ยยยงยย,.-/A{^

|.}=ย=} ],6>"1!&S*.Hsยจยฉ!=",

",.04,.'O>ย-/!-=01@04!'*"%*G>"*ย!=<>"-/ย%('R)ย%'F>G!r-=0604!L'O>d6j6ยช;h/cKeUi{`:"!=N,6">"%(,.*Y!=<P>"1R,;04!'*">G-=%('O>"*;XU-=*
!=N!*G,.S2>G!ยซd4eUgยข`neUยข&gGh/ยยฌ!=VebยญRb/ยEbยฅ=i{`h/ยยฎ"!=N,6">"%(,.*B!=<C>"1R,+'R,6>ยยP!="ย]:>G!@-=01%7,6#=,+-/0504!'K*"%(*G>G,.'048
,43506%7,.'O>"A78+<ยฃ!=ย*GN,.06%EDยฏ0:06A(-=*"*G,.*Y!=<J04!'*G>G-=%'F>"*ยXยฃยฐJ-='ยยY,.'O>G,.'R"80"ยยฏ^
ย

,6#%(A(A7,=^	ยยฒยฑย,.')R^

ยยณzx-=*"%'%{^ |.}=ย=ย ]^R!=P>G!ย01-/-=04>G,6%(ย6,ย*G!;,Y>G-=04>"-/W	A7,Y06A(-=**G,.*l!=<ย"!=WKA7,.?*ยXยฃ#-='
,6,6ย+ย
ย

ย

,.01F>G,6.^

|.}=}RยKยด

%7"!LK*"%(*6^ |.}=}=ย&ยNยต

!&!=N,6.^ ยต

!1R,.'^ยยทยถ=,.-.#=!'K*6^

ย

|.}=}=ย&ย

,6,6ย	^

zx!1R

|.}=}=ย&ย #/-='

|.}=} ]Hlยธ@,ย",.*G,.'O>>"1,.*G,

",.*"LKA7>"*T<ยฃL">"1R,6ย%('+>"1%*CK-/N,6.H
ยน !;,Y<ยฃ",.w&LR,.'F>"A(8,.'04!L'O>G,6",.Sr04!'*G>G-=%'F>"*T-/",Pยบยขc	`neUiUb=c	h/ยO04!'K*G>G-=%('O>"*6^<Q!=T%('*">"-='04,ย%'?N,6Rยป
>"%(SR,ยผ*G8&'O>"1R,.*"%*ยX{ยถ-='**G,.'^ยฏยถย,6
ยฝ )=!L^ยฏยพY!LR)L%(,6.^ยฐย%(A(-/",.r^ยฏย

ร

ยต

-=*G>G!R^

|.}=}/ย ]ย!=ย%('

 , --.oยก %%	ร!<6ร
<TรB0
Pรp
!ร:y
lรยก9!	%&%ยร		ย&0%W%ร0 <$ร

ยต

!'*G>G-=%'F>ยยฟย!=)%(0

รยร$รยฏร7ร

รJรร=ร=รร=ร;ร;ร(รรรยฃรJร=ร?รยร.รOรGร.รRร"รร"รMร6รlร=ร{ร7ร	ร.ร=ร=รกรขรlรฃxร=รOรMร4รรรคGรGรร=ร(รOรรฅรฆKร=รคGร.รง?รคGร&รค"รGร.ร;รคJร=รรงร(ร=รรรจร/ร=ร.รค
รจรคGรYรฉยฃรจKรร4ร"ร7รรร=ร	ร4รรรคGรGรร=ร(รOร"รค6รRร(รร6รรจรงร(รRรBรชยฏรซรญรฌQรฎยฌรฏรญรฐOรฑยรฒ@รUรณPร=รรร(รรRรยฏร.ร=รดรnรขรยฏรฏยรฑรญรตรฎรญรถ	รช@รUรฃuร=ร6รรค.ร	รทยร(รจรคGร=รรธ
รJร=รรงRร6รยรนNร=รรงRร.รรรญร.ร=ร=รกรขยร=รรงยรบยรฑรญรฐFรถKรฌรปยรผยรฝnรพNรผยรฟรถ@รRร"ร6ร.ร;ร=รRรฅยรณร.รKรคGรรยรธaรณร=รรร(รRรRรรญร.ร=ร=รกรขร

รยรร(รคKรNร6ร.ร	PรรคGร"รจรงRรuรฉยฃรจรKร4ร"ร7รรร=รยร4รรรคGรGรร=ร(รOร"รค
ยรรยรร.ร(ร/ร"ร7รรรคYรRร6รuรรร"ร.รคGร.รOรยร/รร	ร/ร"ร"ร(ร=ร
รฉUรจรร4ร"ร7รรรคยรUรRร=ร"ร(ร4รยรRรรคGรยร/ร"รยรรยผรค"ร=รรMร=รค Yร4รรรคGรGรร=รรFร"รค
CรคGร6รMรคGร.ร4ร"ร7รรKรคยรก&ร(รMร=รรงยรก&ร รกรขร
รฃxร=ร"รร"ร.ร6ร(รค"ร.ร7ร=รPรยรคGร"รจรงRร2รRรร"รNร6ร"ร"ร7ร.รคยรร!=ร.ร_รGร@รRร.ร

&%'$%()	*/ร,+@รuรร=ร"ร/รฆKร7ร2รคRร-

รฆOรyร@รRร

ร7รร6ร=รCร4รรรค"รรคGรGร.รร4ร=ร#"$

รKร/ร6รPรจรKรงRร6ร;รคGรรรยร4รรรงKร7ร"ร7รรรค6รรร(รคMร7รร6ร=รยร4รรรค"รรคGรGร.รร4รยฆรรจร/รGรฅ

ร=รOรGร6ร.รค;รRร+ร.Rร(รคGรGร.รร4รrร=รฉยรคGรร(รจRร"ร(รรรคMร=รรง

ร;ร/ร=ร.รครRร.ร

ร.ร=รคร7ร6ร;รGร0/	รรง1
2/Kรรค"ร6ร3Pร+ร'ร/รร=ร4รGร6รร546รxร

รค"รจRรฆ	รคGร6รยร=รฉJรRร6ร/รร(ร/รฆKร7ร.รค6ร	ร6ร=ร(ร7ร.รงยร879:%ยร=รKรงxรงRร.รร=รGร.รง<;>=	Pรยผรร.รuรคRร?ยณรKร/รยร7รฉยฎรRรยรRร6ร)Pร=ร"ร
ร(รค@Kร!=ร=รCร4รรรคร(รคGรGร.รOร6รRรRร.รrร=รFรVร4รรรค"ร(รค"รGร.รFรยร(รKรคGร"ร=รFร"รร/ร"ร7รรVร=รฉA;

ร6ร=รrรฆNรยร(ร(รRร.ร/รร7รร.&รGร.รKรงRร.รง+รGร;รMรคGร/รฅ

ร(รจRร"ร(รรร&ร"ร?&ร(รงร.รงrรRรยผรรรคGร"ร=รOร"ร(ร/ร"ร7รรrร=รรงร6รYร(รคB;<C9D"FEGH5MรIKJ-J(รKร(รLNรรค"รคGร.รค"รคGร.รคYรคGรรรยผรGรยฏรร(ร=รร(ร6ร=ร

ร"รNร6ร"ร"ร(ร.รคCร=รค"รคGรร6ร(ร/รGร.รง8ยร(รM;
+@รยรRร.ร

ร=รรงrรร:รฉยฃรจรKร4ร"ร7รรร=รร4รรรคGรGรร=ร(รFร"รคfรขร

ร(รFรGรร&รงรจKร4รuร2รร

รร6รRรรงvรฉQร=ร+รคGรร!ร(รRรtร=รOร_รฉUรจรร4ร"ร7รรร=รยรพยฏรฝ.รฟ_รฆ&ร_รงRร.ร4รรNรรค"ร(รรยซร7ร

ร(รOรGร+ร)Pรยรค"รจRรฆNรร=รฆKร7ร.ร;รค
@/KรรคGร6ร/	รรงร(รร?ร+ร4รรรค"ร(รคGรGร.รOร:ร(รรคGร"ร=รOร"ร(ร/ร"ร7รรยร=รฉรRรยผรรOร=ร:รคGร6ร6รยร=รKรงยรค"ร.ร4รรรงร
ร.รGร.รรงร(รRรรKร(รค:Kร/ร"ร"ร(ร=รรรรคGร"ร=รOร"ร(ร/ร"ร7รร+รGร5รรค"รร(รจRร"ร7รรtรUร(รuรMรฆKร=ร"ร&รGรร=ร"รOรฅรฉยฃร"ร6รยร?ร=รรRร6รfรขร

รVร;ร=รรยซรงรQPVร6รจร7รยรtรรคยผร6ร7ร.ร/รร7ร@รGร</ยฏรรง2รยร4รรรค"รรคGรGร.รFรMร(รรค"ร"ร=รFร"ร(ร/ร"ร(รรยซร=รฉยรร5ร"ร&ร=รรคGร6ร6ร#Bรร
O

ร"ร.ร;ร=รรรครญร.FNรรRร.รFร"รร=รFร(รBร(ร"รคยฌรค"ร546ร=ร

O

RรPรร6รRรรงRPร3รร.รคGร.รFรSRร6ร"รPร(รครญรRร6รร6รฉQร=ร"รPร=ร(รรรรร=ร"รรP5ร6ร7ร.รOร

รค"ร(รKร4ร:รRร:ร"ร&ร=รยรคGร6รยร(รคTร=รฉรค"ร?ร=ร(รรค"ร!46ร=ร

T

รRร=รRร6รrร=รค9Nร.ร4รMรUยร(รค_รGรVNรร(รFร5รรจRรrร(รค5รร/ร6รYรจรKร(ร7ร=รuรรรค"รVร=รฉยรRร>Pร=ร"รsรงร.ร=ร(ร(รRรWยร7ร

ร4รรรคGรGรร=ร(รFร"รคRBร(ร'0ยฏรรครคGร.รค"รคBรคGรร;ร;รค9Nร.ร6รQ/	รร"รNร6ร"ร"ร7ร.รค.รรร(รค:รร6รRรรงtรRร=รยรรร7รxรXKร(ร7ร.รคYBRร.ร,EH$H
รRรยร4รรรคGรGรร=ร(รOร"รคNรรค"รค"ร.รค"รคJรRรยรร!=ร.รZKร"รNร6ร"รยรrรยฃร=ร[Kร"รNร6ร"ร"ร7ร.รครขรรฆ	รจRรlร=ร(รคGร\BRร.ร;รรKร7รZ%DZlร=รฉรRร.ร
รงRรRร

O

Kร(รค]KรNร6รBร(รค:ร=ร"รร=รKร!46ร.รงtร=รคBรฉยฃรร(ร7ร?ยรค
BรคGร.ร4ร"ร(รร2รก^/KรรคGรBร(รOรGร"ร&รงKรจร4ร.รคยรRร;รฆKร=รคร(รMรงRร/	รร7ร"ร(รรรค:ร=รรง

รRร=ร"ร/ร"ร7รรKรคยผรร6รร/รรงร(รRรtรพNรฝ6รฟรค\Bร(ร'_Yร(ร(รJรฆNรVรจรคGร.รงyร(รtรรVรฉยฃรร(ร7ร?ยร(รRรRร>+@ร+รRร.รyรงRร/	รRรVรฉยฃรจKรร4ร"ร7รรร=ร
ร4รรรคGรGรร=ร(รFร"รคร=รรง;รงรรค"ร6รจรค"รค`รร&ร7รรจKรค#ร=รรMร=รKรง;ร7ร"รคJรร.ร(ร/ร"ร7รรรค#Yร7รMรร(รค#	รยฏร6รnร

O

Rรยร(ร=รคGร	ร/ร"รJร=รฉNรRร

รคGร.ร4ร"ร7รรUรร.รคGร.รFร"รคยรRร=ร"ร(รรรคYร=รครคGร&ร6รร/รGร.รง>Yร7รยรฉUรจรร4ร"ร7รรKร=รยฌร4รรรค"รGรร=ร(รOร"รค6ร	ร(รยร/ร"รMร;ร=ร(รร(รVรฉยฃรรจรรงxร(ร
ร=รรXยรRร6ร=รร=รa+@รรRร.ร9ร(รFรGรร&รงรจKร4ร\Kร!=ร=ร:ร4รรรคร(รคGรGร.รร4รuร=รKรงยร=รcb?รd1e(fgenรขYร=ร7ร=ร=รร7รร

ร=ร(ร7ร?ยร(รRร5รGร

ร=รKร7ร=รCรRรCร(ร/รGรGร6รJร(รMรคGร.ร4ร"ร7รร2h&รFรฆยฏร6รฉยฃร=ร"ร@ร"ร.รคGร.รOร"ร(รRรยร(รMรคGร.ร4ร"ร7รรjiยรRร:รรยฏร6รร"ร7ร.รคยฎรค"รจ&NKรร7ร.รง:รGรBรBรพNรฝ6รฟ
รฆ&รยรKร(รคJร4รรรค"รรคGรGร.รร4ร=ร#+@รL/ยฏรร=ร(ร7ร6ร"รNรรคGรยรยร;ร6รRร&รง;รฆKร=รคGร.รง;รร?รRรรคGรร"รNร6ร"ร"ร7ร.รคยฎรฉยฃร=รรคGรร5&ร(รรยผรพยฏรฝ.รฟรค
ร4รรNรรคGร.รง5ร=รฉรฉยฃรจKรร4ร"ร7รรร=ร	ร4รรรคGรGรร=ร(รOร"รคยรยร7ร;ร=ร@ยร7รรรจRรlร=รงรงร(ร"ร7รรร=รรRรรRรฅรฉยฃรจรKร4ร"ร7รรร=รร4รรรค"รGรร=ร(รOร"รครขร

k`lmnpoXq)rtsur)vwxnyr)oXz
{|g}`~S$ย?ย	~uยxยKยuยย$E7'*;รพNรฝ6รฟ<ยย$%6ยyย3ย8*5รย<ยย>ย'ยYยยยรขย:ย`ยF7
ย

ยยยยยยYยย9ย
ย

ยยยยขย?ยฃ[ยยยยย-ยยฃSยIยยยย-ยยฃSยNย>ยค%ยF%ยยj%]dยยpD2E&%'ยBย`ยF7MEpยยฃSย3ยยขย-ยฅyยยยยฆ'ยtยยยยยคย<$%

e

ยยยย?ย9ย[ยยยยยยย9ย:ยXย^$%RยF\%@ยย\%LdยยกE7(EgH5(%J

ย&%]9ยยFR"F%%'GH5MEHยจยงN%Bย7ZE7'KEyH!ย:ยtยฉRยF%'5ยช-M9ยยF^H!E7Kยซg%(RยpD2Eยยยยยฌ$%
'EHH!ยjf	J
ยยญยค%0ยFU%2ยยU%ยฎ,&%(79EX%ยฏย?ยฐ ย ยยยย-ยยฐยฒยฑyยยยย-ยยฐxยณยยฉaย70ยFU%Eยกยด_ยย<%($D"	HยจK*?ยE
ย

ย

'&%7ยEXยฐxยฑaยยยย:ยtย9ยยตยกยMNยกHยจ-gยซ^E7'KEyH!%Lย@ยBE	ยย`ยต$%6ยy)ยยฐยยถยตpJ
ยยข$%Yย&]%ยย]ยFยฎ7H!E&%6Eยก%%KE)ย2ย#ย^ยF6&%(79EX%ย3ย`ย&7RยFR7H!EMยทXยยถยต$%
ย&Y%'ยงX%#ยย]ยFZยธ#E7()%'KEj"	79ยยงN-ยนยฃSยXยบยฃยฒยต6%G"N ย(*ยกgยซZยF["FE$7'%a9ยDjยง&ยงNEH$Hยจ*2D"FEpKH!
EHยจยงN%J

ยป-ยป?ยผ

ยฝ[ยพยจยฟXรXรUร@รXรXร'ยพ5รรFรyรXรyร0รยกรXร8ร`รNร	รyร&ยพ5ร	ร&รXรZรรรgยฝ:ร

ร,รร'รรXร2รcรรรร<ร'ร6รSรรร5ร!รรกรขร&รRรฃรครฅ&รฆ(รงรจ9รฉรชรฅXรง1รซรจ9รฉ'รฌXรญ]รรรฎjรฏรรรรฐรฐรฑรฒรรณ5รรข9รรกjรดรณ!รขรรตรถpรดBร&รรร:รขร&ร:รทรรรขรณยครรรฐ
รรฎNรกยฏรขรNร2รรก&รรรฐรรฑรรรรรฐ'รYรรรฐ9รxรรรขรณ!รทรร!รธUรรรNรรรฐ9รรฎgรขRรขร&รZรทยกรรรณยครรฏXร!รรฐaรรฎNรกยฏรขรNรZรรฑpรฎNรฐ9รข9รรรณยครฎyรขรฐรน^รรฎNรฑรขร&รร


 
 รณ!ร&รร
	RรรรNรรรฐ9รรฎgรขรฐรรขร&รRรรฑpรฎXรฐ9รข9รรรณ5รฎgรขBรรฎNรก8รรฑpรฎXรฐรณ5รฐ9รข9รรฎNรรธ^รรรรXรXรฐรรฑรบยนรขรNรRรบ$รฑpรยคร!รฑ-รดBรณยครฎ&รjรFรร	ร!ร

รรรรXร1รถ#รรรNรรรฐ9รรฎgรขรณ5รฎ&รUรขร&รMรรร5รรขรณ5รฑpรฎNรฐรฑรบBรขร&ร8รปxรผรฝยฒรถรรรฎรพรร5รฐรฑUรฏxรMรก&รรฟ	รฎ&รรก 2รขร&รcรฃรครฅNรฆ'รช$รฆ(รง รฅรฃ <รซรจ9รฉ'รฌXรญgรน



ยนรฉรซรฅ	รฃZรครจ'รฆรฆ(รงรชรฅ	รฉรงรชKรครฅ&รฆ\รงGรคZรชรงรฆ\รฃรฒรฆรงGรค รจ'รฆ!#"Lรฉรจ'รช$รฆ%$'&Aรครฅ(pรครฅ)$*<รฉยกรฆ'รญ+
,$.-/0+1Xรครจ320รฉรฅ465cรฉรจ'รช78$รฉรฅ(9รฌ(Qรค?รฆ8รงรญyรจ:%รซFรช7(รฆรงGรค_รฉyรฃรฃรคBรฌ&รฉรฅ)Wรงรญ,
!
. รชKรฃ3$'
; รค=j
< รฉรฅ(?ยน
> รญyรจ(รชยครฆ@ A
A รญB( รฆ]
 รซF รช7( รฆ@รฌ	รจ9รค- รช7j
 รงรญ,6
 รช$รฅC รครจD2
 รฉรงรชรครฅ9<%Q รค0E!
รงรจ9รฉ

รชรฅgรซyรงGรครฅ

F . รชKรฃ3G0#รช$รฆรญBรฆ6รงGรคjรซyรค^รงGรคH"Lรฉรจ(รชยครฆ6รครจ-0+81	รครจ%2I$J<&รง#รฆ'รญBjรครฅ)KรฆGรฌ,รฉ82-รฆ.LยฒรจMรฅ	รฃรญB@
; รคM
< รฆGรฌ, รฉ8-2 รฆ
:
N รฅgรซ รช$รฆรญVรฉรฅ(OยฒL รจM รฅ	รฃรญ\
$ รฉรฅ4Pp รคQ รฆรฅ	รคpรง.3
0 รฉรฅXรงaรงGรค8รซgรค0รงGรคOc
5 รฉ รจ'รช7SRT<&รง
F Y
รฉyรฃรฃ3I
 รฌรงรฆรฉรฅUM
 รครงรญ, รจ6รฃรชรงVDW@
> รญgรจ'รช$รฆรครฅ)K2
 รฆ)รฌ, รฉ-2 รฆYX รฌ&รฉรฅNรช$รฆรญL
$ รฉรฅ(
 รจ:VCDg รฆ( รฆ\รงGรคjรซyรค^รงGรคMรฉรฅ)
 รฃรชรงVZ<F รง[-0+8	1 รครจ%2@
F ยน
A รญ,Y2รฉรฅรฉรซ=รจY03รคB\]ยจรช^2RรงGรค_รฅ4รฉ`1รงรญBรรฌFรคยกรฆ'รฆ'รชa<รช` รชรงรชa(รฆYbVCรง3รงGรค2รญyรชc/$รช7@`Q@d$0`รญyรชKรฃรญZรซรฒรช7
A
,
 4 O0`รญyรชKรฃรญcรฃรชรงVI$#0#รชรงรญ?0`รญyรชรฃรญ_รฃFรจ'รจ:รฅรฃ8$e2?รฅ	รค0#รช$รฅgรซUรงรญรฒรฉpรง.รฉpรฃรญ8รซFรช7Hfรฒรฆ(รง
รคM
C รฃรครฒรจ'รฆ\รฆ)รฌ,รฉ2^รงรญB!รฉรฅgรซ&รฉ-รซ=jรคCรงรญBjรฃรคFรฅXรงรจDรญBZRtรครจ\รฆรญBW -รช$รฆ'รชรงรฆ%$:รฉรฅ(MรงรญBjรฃรฒรจ(รจ:รฅ	รฃ
fรฒ รฆ(รงe<%
 รงรญFรฉรงรรค\
C รงรญBj
 รฃ'รคF รฅXรงรจ3@
g

รญ รฃ'รฉรฅWรฆ รฅ <รงGรค



รNรณยครฐยฒรNรรฑรฏXร5ร

F g

ihjFรถรรฑรxรฑpรฐ9รรก6รฑรบFรฟXรทร3รทรรรณ5รรฏXร5รรฐAรรฎNรกRรฟXรทร@รรฑpรฎXรฐ9รข9รรรณ5รฎgรขรฐ

รรรฎ6รฏร3รรฎNรรฑรฒรกNรรก\รฏรฒรธaรขร&ร:รปxรผรฝRรต

ร&รRรฐ9รรขLรฑรบ`รขร&ร]รฟXรทร\รทยกรรรณยครรฏXร!รรฐรรณ5รฐ

lk,mino7prqts8uvo3w=oVqxsIuxvxyinzew={oVqts8uvn{{xq(zivoVqts8u=|}rzJmin}EmUqxsI~

รยร

F g



_ยrย8ยcย4ยBย รlk8ยจรชรฃD]R`eWQ$[;Yรค<ZRV;WI$_>ยนรญyรจ(รชยครฆ]RD>(WQ~
" รฉรจ(รชยครฆ3$[1& รครฅ(p รครฅ,$H<
* รฉยกรฆ'รญyรชรฅgรซpรงGรครฅ)$[-0+81	รครจ%2I$'5cรฉรจ'รช78~
_ยยMยIยcยย รยk8L
_ยUยrย8ยย8ย)ยcยย รยk8xL รจ9รฉรฅ	รฃ3$ยE;#$GยrXBย$X รฌFรฉรชรฅ,~
 ยtย8ย(ย)ยBย[ย8ยcยย รยk8xL รจ:L$Jย.$Eย$'"ย รฆ%~
N รฅgรซ รช$รฆรญ$X รฌFรฉรฅXรชยครฆ'รญ~
ยยtยยrยIย'ย=ยBย รlk8ยฒL รจM รฅ	รฃรญ$:
ร&รรณ!ร:รฟXรทร\รกNรฑ 2รรณ5รฎNรฐรรรร


ยkIย ยrย8ยcย4ยยQยQย8ยMยIยcยย ร%ย ย8ยMย8ย^ยBยQยย)ยrยยย8ยUย^ยBย ร%ย ยrย8ยcย4ยBยQยยย4ยยrย8ย'ย=ยย ร
ย ย)ยrยยย8ยUย^ยBยQยย4ยยUยUยยย8ย^ยBย ร%ย ย)ยrยยย8ย)ยcยยQยยยtยยrยIย'ย=ยBย ~
รดร&รรร2รขร&รรฐรZรรฑpรฎNรฐ9รข9รรรณยครฎyรขรฐaรรรฐ9รxรรรขรณ!รทรร!รธ0รรรNรรรฐรรฎyรขaรขร&รรรณ!รขรณ!รรฐaรขร&รZรN
 รณ5รก&รรฐaรดรณ5รฐร,รรฑร6รรรรรXรข'ร
รข9รฑUรทFรณ5รฐรณ!รขรถยนรขร&รZรรฑN
 รฎgรข9รรณ!รรฐ6รขร&รรรณ5รขรณ!รรฐRรฏxรร!รฑpรฎ&รรข9รฑ&รถ`รขร&ร^ร5รรฎ&รN รรรรฐรฐ9รxรฑย รรฎcรฏรฒรธยฏรขร&ร2รX รณ5รก&รรฐรถSรขร&ร
รN
 รรรรฎNรรณ!รรฐN รฐ9รรกWรณ5รฎยฏรขร&ร^รรฑN รฎyรข9ร'รณ!รรฐรถ#รรฎNรกcรขรNรZรฑ
ย รรณ5รร[ร5รรฎ&รN รรรรฐ6รฑรบรรขร&ร^รรฑN รฎyรข9ร'รณ!รรฐรนย
ย รฑรขรณ5รร
รขรNรรร6รณ5รฐรฎ&รฑZรF
 รXร5รณยครรณ!รขLรรฑpรฎNรฐ9รข9รรรณยครฎyรขรฏรรขtรด[รรรฎUรขร&รร& รรรรฎXรรณ!รรฐBรรฎNรก<รขร&ร\รX รณ5รก&รรฐ@รขรNรณ5รฐรรฑpรฎNรฐ9รข9รรรณ5รฎgรข
รดรณ5ร5รNรฏxรYรณ5รฎNรก)N
 รรรกZรฏgรธZรขร&รYรรณ5รขรณ!รรฐYรรรฎXรกรขรN รฐ[รขร&ร]รรฑN รฎyรข9ร'รณ!รรฐ'รรทรฒรณ5รฐรณ!รข9รรกZรฏรฒรธ2รขร&รYรN รณ5รกNรรฐ3รฑpรฎ^รขร&รYรฑpรฎ&ร
รXรรฎNรกยฒรถNรรฎNรกรฏรฒรธ^รขร&รRร&
 รรรรฎXรรณ!รรฐรรฑรบSรขร&ร\รรฑN รฎgรข9รรณ!รรฐรรฑpรฎ8รขร&รRรฑรขร&รรBรXรรฎNรกยฒรน

F g

F g

ร&รRรฐ9รรขLรฑรบ`รขร&ร]รฟXรทร6รรฑpรฎNรฐ9รข9รรรณ5รฎgรขรฐLรณ5รฐ

รยร


ย ยrย8ยcย4ยBยQยย8ยMย8ย^ยBย รlkURV รชKรฃ3$b"Bรฉรจ(รชยครฆยWQ$_RV รชKรฃ3$b-/0+1Xรครจ32%WI$#R`;Yรค<D$^"Lรฉรจ'รช$รฆWI$R`;Yรค<$b&Aรครฅ(pรครฅWQ$
RV;Yรค<D$*<รฉยกรฆ'รญyรช$รฅรฒรซpรงGรครฅWI$.RV;Yรค<D$^-0+8	1 รครจ%23WQ$ยRDยน
> รญyรจ'รช$รฆ%$^-/0+X1 รครจ32%WQ~
; รค<$bยฒL รจM รฅ	รฃรญWI$RV
; รค=<D$^:
N รฅgรซ รช$รฆรญWI$
ยEยrย8ยcย4ยBยQยยย4ยยrย8ย'ย=ยย รlkUR`ยจ รชKรฃ3D$^xL รจ: รฅรฃรญWI$_R`Y
RD>ยนรญyรจ(รชยครฆ3$cXรฌ&รฉรฅNรช$รฆรญWI~
ร&ร]รฟXรทร\รรฐรฐรฑรฒรรณ5รรข9รรก<รรร5รรขรณ!รฑpรฎXรฐรรรร

ยQยIย

ยยยIยก4ยขdยฃ
ยค4ยฅยฆUยฆ)ยงBยจยค8ยฉ^ยงBยช
ยซ)ยฌยยญ
ยฎ
ร
ยฏ8ยฐcยฏ

ยจ ยณrยฅ ยฒ ยณ=ยงยช
ยฑยฒ 
ยฏ8ยฐ^ยฌยยดยต7ยถ
ยง ยด3ยท:ยธ ยนยบยญcยถ
ยช%ยป3ยผ ยดยนยบยญ^ยถ
ยค)ยฝrยฅยจยพ8ยฆUยฉ^ยงBยช

รร%รQรIรรรDรยบรรiร:รรร=ร:รรรร
รQร=ร%รร%รQรKรร
ร3ร`ร%รKรรรร=ร:รKรรร

ยฏ8ยฐ ยผ ยดยตVยฌ ยณยฟรยฅยช ยฒ ยช%ยป3ยผ ยนยบยด

ยค8ยฉMยพ8ยฉ^ยงBยช
ยซ ยผ ยฐ7ยนรยญ
ยฑ8รMยดร%รMยด
ร ยผ ยญcยถ3ยนยบยด3ยทMรdรMยด
ยจ ยฌร=รTรร:ยฐ`ร
ร ยผ รMยฐ7ยนKร
ยฒ

ยฟ

ยณrยฅIยฉ`ร(ยงBยช
ยค

ร[รdรร,ร:ร
รกรขiรฃJรครฅ)รฆMรงMร%รจร^รฅรงeรร%รจรฉ(รชยรจรฅ)รซOรฃJรครฅ)รฆ%รbรฆMรงMรรฅ)รฌรญZรร%รจรฉUรชรฎรครฏ'รฐiรฑรฒ
รณ ยค8ยฉ:ยพIยฉ^ยงBยชQรดยค)ยฝrยฅยจยพ8ยฆ)ยฉcยงยชรถรตยรทUรธ`รน#รบรป3รผcรฝ%รพ^รฟtรปMรบQรพYรธ	

,รพIรพรธOรบ8รฝ=รผ
,รพ!"Iรพ
	รธ #$&%')(
รป *Iรพ +,-Qรพย	รธ . /
รบ รปD0รผ 8รพ /1Bรบ2รผ 354
รณ ยค)ยฝrยฅยจยพ8ยฆ)ยฉcยงยชQรดยคtยฅ8ยฆ(ยฆ)ยงBยจ[ยค8ยฉcยงยช รตlรทUรธ`รฟxรปรบ67Dรพ^รฟtรป:รฟQรพYรธ#รพ	8"Qรพ_รธ5!.รพ	9Qรพยรธ31Bรบรผ2,รพ^รน:Dรฝ;54
รณ ยค)ยฝrยฅยจยพ8ยฆ)ยฉcยงยชQรด ยฑยฒ ยจยณrยฅ ยฒ ยณ=ยงBยชSรตlรทUรธVรฟtรปMรบ7รพbรฟxรป&5QรพYรธ#รพ=<"/>ยบรผcรฝ?Iรพรธ5,ยรพ=<"/>ยบรผcรฝ?Iรพ
รธ 31Bรบ2รผ ,รพ231BรบUรผ^รฝ7?54
@Tรฏรง:รช)รร:ร6รbรฆ

รฅ,รคPรฌรครฅ)รฆMรงMร3รจรbรฅรงBAtรรงDCJรรรฅFEรจร%รbรจAHGbรรฆJILKYรจรฅUรซMINPO0Q6KRNTUW
S VYX7Z รจ3G[G#รฉUรจรdร%รฆHรครฏ\Eรจ3Gbร,รรฆ]รจร:ร
รง:รช,รร:รรฏ`รคร:รGรจ3G=Gdรค5CJรรซxรข รณ KRN รต^] K`_ ] Na รณ KRNYรbรฆ#รง:รช,รรฅยรฌรจ3G[Gdรรซรฎรจcb Uรผd3รป3รฝรบ>รปe>dรบ3Vรผ0
 a
f รช)ร.รฅ)รครง:รdรครฅ รครฏ รฝ b 1/1,
รป รbรฆJร:ร?G^รจรงMรรซรฎรงMรคHรง:รช,รยรฉUรจรdร3รฆ รง:รชUรจรงgAtร?Gdรครฅ,ร/รครยรฅ,รครงeรงMรคHรง:รช,รยร:ร?G^รจรง:รdรครฅ)รฆรข`CJรYรฆ:รจรญ
รง:รช)รจรงยh
รจ E83รจ G^ร,$
ร ikj U ] jHรbรฆ.รจ รฝ b 11,
รป  รคl
รฏ O`รครYรฆ:ร,รฉUรฉ4รคร%รง:รฆ X B
รจ E83รจ G^ร,m
ร iK U ] Kรฏ`รครรง:รช)รfรฌรครฅUรฆMรงMร%รจรbรฅ=n
รง Q K j
รbรฏ
รจรฅ)รซ รคoรฅ Gdรญยรb:
รฏ O	iKqpeikj XlU รณ K ja\rsGbรฆMรค Z รง:รช,m
ร E83รจ G^ร,m
ร ikj U ] j
รbรฆรจZรฆ%ร,รฉ)รฉtรคร:รง#รครฏ รง:รช,รรฉ(รจรdt
ร O	iKqpei)N XlU รณ KuNfรbรฏ
รจรฅ)รซรฎรคHรฅ Gdรญ]รd`
รฏ ikjYAtรครง:รชรฎรฆ:ร,รฉUรฉ4รคร%รง:รฆ iK'รฏcรครยรง:รช,รGรฌรครฅ)รฆMรงMร3รจรbรฅs
รง Q K j/รจรฅ)c
รซ iNยรฏ`รคร#รง:รช,รGรฌรครฅUรฆMรงMร%รจรbรฅ=:
รง Q N ja
v รฆMรง:รจรฅ)รซUรฆยรฏcรครรจรฅรญ รฆ:!
ร A(รฆMรรงยรค"
รฏ w Z รจรฅUx
รซ vj]ร:รรฉ)ร%รรฆMรรฅรง:รฆYรง:รช,ร รฆMรรงYรครฏiรง:รช,z
ร yUร%รฆMY
รง {|Eรจร%รbรจ AHGbรรฆยรคL
รฏ w Z
C}a ?ร a &รง a6รจรฅ รคร%รซ,รร/ร ~BH
รฉ Gbรจรbรฅ,รรซPรbรฅ รง:รช,รZรฌรครฅรงMร ~B&รง a f รช,ร]รซ,รรฏVรจoร GdรงGรคร%รซ,รY
ร C_[ร G[GAtz
ร Il?pIยยp&ย&ย&ย5pIยยยย&pI"ยHa
rsGbรฆMรค Z รง:รช,รYรคร3รซ,รร#รฏ`รคร#รฐJรฑยt
รฒ Cย[ร G[G6Ath
ร ย"ย3ย0ย+ยยย)ยยkย7ยย	ยยย)ยยยยย"ย3ย"ยยย	ยยย)ยยย3ยยยยยยkย	ยยยยย,ยย"ย"ย3ยยHยยยa
ย [ร Eรย
รฅ vย รต รท I  p&ย&ย&ยpIยย 4 รจ รฆ%!ร AUรฆMรรง รคz
รฏ w Z รง:รช,ร รจรฆ:รฆ:รdรoรฅ ย
รรฅรง รงMรค รรจรฌ3ย
รช E8รจร3รbรจ AHGdP
ร ILK
รคB
รฏ vHย
รครฏGย
รจ E83รจ Gbร)ย
ร i K รครฏ ] K รbรฆZรฌ3รจ G[Gdรรซ รจ ยรธ 1Bรบรป VรผVรบ > Sรผ )รฝaรบ HVรผ73รบ V0รผ 
ย
qย v ย aยยBร)รฌ3รช รจรฅ รbรฅ)รฆMรง:รจรฅ=รง:รbรจรง:รdรคยข
รฅ ยกย รต
O	i  p&ย&ย&ย?peiKqp&ย&ย&ยpei)N/p&ย&ย&ย?peiย X รbรฆ รฌรครฅ)รฆ:รbรฆMรงMรรฅ=รงiรdรฏxรจรฅ)รซ รคH
รฅ GdรญHรdรฏx&ร Eรร:รญ รฌรครฅ)รฆ:รงMร%รจรbรฅ=รง รbรฅ)&รฌ G^ร)รซ,รรซ
รbB
รฅ vHย'รbรฆEรฆ%รจรง:รbqรฆ ยฃ
yUรย
รซ ยค=รฏcรค7ร ย
3รจ G[Gdรญ Zยฅ6ยฆ pDยงzยจFยฉ[?รฆ a &รง aQ KRN U|VtZ O	i K pei N XยU รณ KRN aร,รคร ร^รฅ)รฆMรง:รจรฅ)รฌร Z รฅ,รรdรง:รช,รร 	รธ s>K	รผ Dยรพ 
/
/ รฅ,รคร
2รธ s
ยชรพiรน#รบรป3รผcรฝ%รพ 31,รบรผ / รจร:ร
รฌรครฅUรฆ:รbรฆMรงMรรฅ=รง Z Cยรช)รร:รรจรฆ 2รธ s>K0รผ 7DรพJรนยรบรปDรผ^;รฝ  รจรฅ)รซ รธ ยซ+รป3รผcรฝ%รพ #$&%')(H
รป *IรพJ:
รน D?รฝ &aรบ8รพ
t
A

รค
:
รง
รช

รจ
:
ร
ร
3
ยฌ
a
r
b
b
ร
/
รฆ
:
รง
,
รช

ร
:
ร

ร
`
รฏ

รค
:
ร
ร
,
รซ
ร

(
y
,
รฅ

ร
9
รซ

รจ
f
รฆ
?
รจ

รฌ

รค
)
รฅ
:
รฆ
b
ร
M
รฆ
M
รง

ร
=
รฅ
f
รง
b
ร
)
รฅ
M
รฆ
:
รง

รจ
=
รฅ
:
รง
b
ร

รจ
:
รง
d
ร

รค
รฅ

รค
รฏ
:
wPaยญr
31B3
รบ )รผ^7รฝ ?
&รฝ 
> V0รผ 

รฆM/รค Gbร)รง:รdรครฅ รงMz
รค w รbรฆiรง:รช=รUรฆ 2รธ s>K0รผ 7Dรพ'รนยรบรปDรผ^รฝ3รพ[รฟtรปMรบ DรพรฟxรปMรฟรพรฟteรป &? a
ยฎ)ยฏ?ยฐ

ยฑLยฒยดยณHยตHยถ|ยทยยตHยธHยน7ยฒ[ยนยถ,ยบยธHยปยผยญยฝ)ยตHยพcยฟ`รoยธยปยถ!ยฒ[ยตยธ!รHรรยท-รยฑ"ยน

ร
ร=ร
ร[ร

รoรeร0รรร
ร:รeรร0รยดร
ร)รร?รรร
รรDร รร รeร2ร
ร"รรร7รรร7รรยรร
ร

ร
ร

รร
ร

ร

รฉ รฎรซรฎรฏ รง

รฐtรฑmร7รฒ

รก)ร=รeร?รข2ร
รฃHรค รฅ6รค
รฆ รค รง)รค รจ+รค

รฉoรช รฆ)รรฌรซ)รญHรฎ=รฏ รง

รณ`รดยรต/รถ!รทรธYรนkรบรป-รผยรฝzรธรพ,รฟยรธ-รฝ	2รถ
eรดยรฝ
oรฟรฝ
รทรฟ3รด


	!#"$%&(')"*,+-!.!%/*0+13254768	9:;"*
<ยฌรธ>=Hรท?@BAoรธ=$
!รธnรผ(CoรฟD"รผยรธE&รฟGF?HGI$J0K/L/MI$NDOJ?MDIQPRK/S@NDL#IK#P3T	รณ`รดยรต/รถ!รทeรธ}รนUoรทeรธeรธ
7V;รผLรฝzรธรพ!รฟW[รธ"รฝeรถ?C
รฝ
@รท7รฟ3รด
7RX?Y[Z6รธ-2รฝ3รทeรธ\AHรด&รถรด
!รตยฌรทeรธ[รฟDรธAFรผLรฝ3รท^]$_`<ยฌรธaC!รธ
Mรทรธ&รฟ(รฝzรธยรต3รทรฟD$CbC!รธ&รฝ3รทcb
oรฝeรดยรฝ

Zยรธ-รฝ3รทรธE^Coรฟรทรฟรธ&รท7รดed?รด
!รตรรฟoรทeรฝยรธ&รทfcg
!รธรธeรฟรท^cg2รฝ3รทhC!รธ)Jรธ-C!รฝGA รผLรธ>Hรทeรธรธ
:รด
iCoรดVรฟD6รธ&รท0_
2546j	;!#"$k9ml L#nDo-IpK/qrMsnDNDSRL/N7t-OeoPUuBvWNDI$w5uVxyEqzo\w7oI*MKfo{uVv}| uVx~L ยF-MDSiNO;Ojย7vUL#Iยย	vfy
K#ยGo-S@o5L9PยNKยยM:PKEMDI*o{ยยxsL#IยยjxยP?HJ-ยยK#ยยNKET/ยvยยยยxX{ยยย v xยยยยogK#ยQoIpw7o-I$MKfoWยยxยยmย v xT/ยvfXยยยMDS
ยยxยย`ย vย xยT/ย7vยXfยL F3K#ย7L;PEn:NDOยHoWoยL;PRK#P?y,ยยMK#ยGo-S?q8L;P-o0ย
ย J^MIQPRK/S@NDL#IKยกย v xgL;PB	รถ
eรด[รฝ
oรฟ8L puBv|ยขu[x5MDS[uVxย|ยฃuzv@ยhยค?IยK#ยGorF-MDO#OยฅMDq8L#IยฆyยกqzoEq8L#O#O,J^NDO#OQuzv
K#ยGolรฝ3รทรด[รต/รด
ยNDI*wUuBx5K#ยGoยงeรฟรทรต3รธ-WM@F3K#ยGozFRHยI*JK/LMDI$NDOjJ^MDIQPK/SยจNL;IKju v |ยขu[xย
รณ!รทeรฝยฃCHรด{A!รธ=$
oรดeeรด[รฝ
Y+รผยรธ5&รฟ
ยฉA!รธAoรถรธ รฟaHรฟรทeรดeรดยรฝ
ยรฝยงC!รธiรฝ
@รทรฟ3รด
Wรธ-tรฝ(C!รธ\ยชยซ-ยฌรบzรฝ
ยญC!รธ
รฝ
!รธCoรฟ
AsยVยฎยY,C!รธรธ-\รฝrCoรธW2รถ
eรดยรฝ
oรฟjรฝ
@รทรฟ3รด9
7-Yยรฝ
~C!รธtรฝC!รธ&รท3Coรฟ
A~ยBยฏ:YC!รธJรฝC!รธ&รท?T#C!รธ

!รฝ
s	รถ
eรด[รฝ
oรฟยกรฝ
@รท7รฟ3รด
7RX?_gยฐmยชยซ-ยฌxรด9Eeรฟ3รดAsรฝ\Z6รธยฑ2รถ
eรดยรฝ
oรฟรดe"รดe3รฝ
eรฟ3รด
PMDยยฑo32รถ
eรดยรฝ
oรฟ
รฝ
@รท7รฟ3รด
7ยTยยVยฎยยยด
ยฒ ยณX?Y รฟ
AยญPK/S?LJK/Oยยต3FRHGI$JK/LMDI$NDOยรดeยรดeยฑMDIOยยตรฝ
eรฟ3รด
3รถ^C~รฝ
@รทรฟ3รด
&รบUยยดยยถยVยฎ
Tยย ยฏ ย`ยณX?_

9ยธยทg4ย&#%Q!4ยนยปยบ".:ยผ
ยฝยจ
bC!รธ5=รธA รฝEรฝ
@รท7รฟ3รด
7ยeรฟDeรด@	รฟeรดยรฝ
ยปoรทeรฝZยรธยฑ-Yรฝ
!รธยรฝ>C!รธ\JรฝBรดWยรฝ3รทeรฟ
zรฝZGยพรธeรดeยฟ3รธhรด
รฝcรทeรธAHรถรธยฑรฝWHรถeรฟDeรดยรฝ
seรดJรธ_gรยรฟ
cs@eรถAoรด[รธ>Coรฟยฟ3รธยZ6รธ&รธ
ย&รฟรทeรทรดยรธAยฌรฝ/รถEรฝ รฟ^CHรดยรธ-ยฟ3รธยฑCoรด9ยฑT/รธ&รธ2รฝ3รท
รด
eรฟ
รธzCoรธยรทรธ-รธ&รทeรธ
รธรด
3Coรธ"รด9
7รทeรฝGAoรถeรด[รฝ
}รฝCHรด,รฟD6รธ&รทX?_ รปhCoรธ-cEยรธ&รท^zรดeรฝsรทรธAoรถรธYรฝJรธ-eรด9Jรธ
A!รทรฟeรด&รฟecYjC!รธieรดJรธhรทeรธรkรถoรดยรทeรธAยรฝsรฝeยฟ3รธ\ยชยซ-ยฌ-_ยรlรฝ5รผยรธ-ยฟ3รธ&รทYLรฟ
 รฝZeรฟ-ยรธg=รดzรดe3C!รธ?รดยรทtรธรย&รดยรธ
c6รบ
C!รธ-cยญรฟรทeรธzรต3รธ
!รธ&รท7รฟยกJรธ-C!รฝGA-Yรฟ
AYรฟ3รถ^CY,C!รธ-csA!รฝย
!รฝ>eรฟD]3รธBรด9
7รฝcรฟ-รฝ/รถ
3Coรธยฑ@ยรธ&รดยฅ=$&รดeeรด[รธlรฝ
C!รธ>oรทรฝZยรธBรฝWZยรธ>รฝeยฟ3รธAY
!รฝeรฟDZ$ecรรผE_uรท0_ร-_VP-oยNDI$K/L/J-Pยรฝ	C!รธ>รฝ
@รทรฟ3รด9
7-_LรปhCoรดVA!รธ=$&รด[รธ
cรรผ"รฟ
รฝ0ยฟ3รธ&รท?รฝJรธWZc\ยฟ)รฟรทรด[รฝ/รถUT	รฟ
AYรฝ3รท>Jรฝ@sรฝrCoรธ5Yรทรธรธ
7?Xlรทeรธรถe-_sรปhC!รธ-ccรธรพQCoรดeZHรด(รฝJรธW-[รฟeรธlรฝ
รฝ
@รท7รฟ3รด
7sรผ(CHรด^CยยรฝรธยงHรฟรทeรด&รถ[รฟรทยงHรทeรฝยรธ&รทeรดยรธ-_sรปhC!รธรธmรทeรธeรถ(	รฟ รด
7รฝgDรผยรฝ5-[รฟรธ?รบ:รฝ
aC!รธ
รฝ
!รธgCoรฟ
AY	C!รฝรธgoรทeรฝยรฝeรด
oรตcรฟยรต3รฝ3รทรดeCยฑ3@ยรธ&รดยฅ=$&รฟ9ec|รฟAHรฟDรธAรรฝaรฝzรธย-[รฟeรธYรฝ(รฝ
@รทรฟ3รด
-Y
Jรฝ3รทeรธYรธรย&รดยรธ
hCoรฟ
cรต3รธ
oรธ&รทรฟรฟยรต3รฝ3รท7รดeC-Yoรฟ
A รฝ
5C!รธ}รฝCoรธ&รทยงCoรฟ
AรYQC!รฝรธ)?Coรฟรทรฟรธ&รทรดd?รด
!รตยฑ-[รฟ^รธ
รฝ3รยจรทรฟeรฟDZ$ยรธรioรทeรฝZ[รธ-_(ยฝยจ
ยCoรด}รธeรดยรฝ
ยฌรฟnรด
ยC!รธ$รทรธ@lรฝzCoรดยงHรฟDยรธ&รทYHรผยรธtรผ:รด,eรถQยรฝรธ)CoรฟD-Y
รถ
[รธ"รฝC!รธ&รทeรผ:รด9รธ>@eรฟDรธAYCoรธ3
!รธ-Dรผยรฝ3รท]G:รฟรทeรธ>Zรด
oรฟรทc_
ร:รร

ร(ร!ร*รeร

ร รร{รร^ร@รEร-รรร^รEรรร7รรร9รรEรรQรรรร@รร^รรรรรรร>ร(รรร?รsรกรDรขรยรร@รWรรฃVรรQรiรค-รฅ-รฆWรงรจรฉ/รช/รซ-รค>รรฃBรรรรรรGรฌ
h
ร@ร@ร^รร9ร7รร[ร@รยฑรญรรรญรร@รEร@รญรร-รยฅร$ร}รญร^รยรรรร^รรQรฎร-รฏQรรรกWรรรฐgรDร^ร3รรรรรร@ร@รรรรฐยฑร$รร@ร-ร^รรQรฎร-รฑ
ร รQรVรรรด รณ รรรรร,รตยจรถรทรธรธรนrรญรรรญรร@รhร@ร{รกWรGรดรeรฃ#รฐ)รบ(รปzรฌยรผUร@รUร9รกWรญรร!รฝร(รeรรยกรรพgร-รeรรรรฐWรรaรฅ^รฟรงยรช#รฉ
รฒยญรณ 
รช#รจ*รฅ^รฟรงยรช#รฉ\รรรด	Dรช;รค-รฅ^รฟรงยรช#รฉ\รรรร@ร@ร^รรรรร-รฑkรhรQร-รฐรรรรดรรรยรร9รกWร\รรรกWรญรร
Gรeรยรฐยญรรฃยรบ(รปBรฌยรผยรฃ#รรรก
 รต !รนBรรiรฎรรQร-ร^รรร-รร@รรฏร@ร  รต GรนVรฃ#รรhรรQรร@ร3รรญ*รร-รeร$รยรรรร@ร@ร^รรรรร-รฑ
รฒ รบ(รปBรฌยรฏGรUรQร-รยรDร^รRรฌfรรรรรรร@รรรรฐยฑรรeรฎรร^รรรรก5รฏร9รยกรญรรรรร7ร@รรดรฐยกรรยงรร7ร@รรQร^รฐยร^รขaรฅ-รฉ8รงยงรตยจรถรทรทรน?รฑ
รบ}รรรรรรรฐรฏ,รบ(รปzรฌ iรร>รQรรEรรsรรรฎรร^รeรรรก >รeร>รรรรรรดยร^รDรรQร-!
ร รWรรรรรรดร-รรรดยรรEร5รกWรGรดQรร	ร(รeรร
รร-รฝร-ร^รรรญรรร"ร รeรzรรญ*รร-ร9รรร #รDรรeรรรรฏรร-รรร^รดรรQรฎ>ร@ร3รรQรhร-รรรร@รรยกรรฃรรรร@ร@ร^รรรรร8ร@รEรญร^รยรรรรhรต$รฅ &%
' Gรจ$รซรฉ/รช (Dรจ$
รง *)รรงDรจรฉ/,รช + ' Gรจ$รซ0รฉ/รช (Dรจ*รง ยรรรดWรฆ (D.
รจ (/รฉ (Dรจรช/รซBรรรร@ร@ร?รรร7ร1
ร 0ยขรQรรรรร8รรรDรรรรQรร@รrรรรร@ร@ร^รรรรร
ร^รร@รญรรรรeรฝรรeรฐ{รรรรรร@รญรรรดWร@ร รณ รรQรzรรรด รณ รรรรร 2 ร}รฅรฟ รงยรช/รฉ 34Dรช;รค-รฅรฟ รง รช/รฉ Wรรรด5รช#รจ*รฅรฟ รง รช/รฉ {รรรGรฌ
รร@ร^รรรรร?รน?รฑzรบ(รปzรฌ รร^รรeร-รฝรร}รDร^รRรฌfรรรร^รร@ร@รรรรฐยรร  รต GรนVรฃ#รรhรรQรร@ร3รรรร@ร@ร^รรรรร-รฏร(รรร^รยร9ร[รรรGรฌ
ร^รร@ร@รรรhร(รeรรiรรQร>รฃ#รร^รกWร-ร(รรรรรeร-รฑ5
รhรรยร@รรรรรดรร-รรรรUรรรรรรรร)รรQรร@รgร@รร^รรร9รรQรร)ร(รร9ร^รยร9รดQรร7รรรฃ;รฐยรรรร@ร@ร^รร9ร7รรUรฃ#รร)ร(รรร^รยร@รรกWร
eร รGร-รรjรรรรรร@ร@รรรรฐ\รร}รรGรพยร-รeรรร}ร@รgรฎรรDร^รร7ร@ร-รWรฎรรรรรรรรรร@ร@รรรรฐรฏรรยรญร^รรญ*รรร3รญรรeรฐGรQรรกรรรรรeรฎรDรฌ
ร^รeรรรกรVร@รยฑร@รรeรฝรEรรรกWร3ร-รรรรรรhรรฃ	รญรรรeรรกยฑร-รฑ
รฒ รฝDรร76zร-ร-รขpรตยจรถรทรทรน)รรร0ร(ร3รรรDร3รญรDรรยรรรรรรร@รรรรฐ~รรรดรรรรยรฎรeรรรrรรรรรร@ร@รรรรฐsรeรฃVรรQรยรรรGรฌ
รร@ร^รรรรรhรDรร 89(:ย;รซ (D=
รจ <D?รฅ >5รฑ @
รฒ รฝDรรA6Bร-ร-รขยรรรดCB}รร^รร@ร-รiรตยจรถรทรท:รผรนEรรQร0ร รรรDรEรeรฃยกรรQรรรรร@ร@ร^รรรรรEรDรรDiรฌยรรeรฎรร9EรฏรรQรร~รร@รรรQรฎ
,รต DGFHรนfรฌfรรรรรรร@รรรรฐ5รรรดรรรรVรฎร
ร รรรรรรรรร@รรรรฐรฑ5I
รฒ รบ

@ร ร^รรรรรรร-รรรร>รรฃJรรรDรรฐยรรรร@ร@ร^รรรรรEรรยรรดรร7รรยฅร$รรดKยรฐยรปVรรรญร-รยรฅ-รฉยงรงLยรตยจรถรทรท:รผรน?รฏNMPOยรถ$O:รร9ร
รรรร@ร@ร^รรรรร-รฑ4Qยรฃ*รฃ#รรzรรรฐWรญรรeรzรรฃรฝ:รDร?รรรeรร4RTSVU9R4Wรฏ7ร-รฝร-รรฐยฑรฝ:รรรรhรรฃYXNSรรrรรรญรญรรร@รรดยรฐUรรรรQร-ร
MGรฏVรถรรi
รง ,รฝ:รรรรร>รZ
รฃ X W รฏ	รรQรรsรรQร-รรยร 
Gร9ร@รรEรรยรรรฎรร^รeรรรก5รฏ	ร-รรรeร\
รด [^]`_ /รต a*ร-ร^
ร bcยรQ3ร b:รบ}รร;รน?รฏ
รยงรรร^รยรรeรรQร-ร3ร$รรดรEรยร@รรรQรรรรรฏ,รร{รรQรฃ#รร^รกรยรรรDร{รQร\รรรรQรรeรรยร 
Gรรรร3รรsรญรรeรฐGรQรรกร9รรjรรรกWร
รต/รรรกWรรeรฐ  รต *,รต deFfGรน@รน@รน?g
รฑ QยรWรรWรรร@รยรรQร!ร(รยญรรรDรรรรฐยรรรQร-รร-รรร^ร{รรฃยงรรรร@ร@ร?รรร7รรWร-รรร@รรด
รรรดQร-ร[รญร-ร^รกUรQรรDรรeรรยรรรดiร^รร@ร@ร^รรรรeรรยรรฃรรQร)รดQรรกรรรร[ร9i
ร hkjยกรฌfรรรกWรญรeร-ร@รรฑ

รท mรนrรญรรร@รรรรzร)รญรรeรฐGรQรรกรรรQรรeรฎรร?รeรรรก ร@ร{ร@รรeรฝ!
ร n`opรrร(รeรร\รช;k
รฆ q.ยรช/รซ?รงรฉ/รช (Dรจ$รงยรรรGรฌ
รฒ\l รeรรรรร9ร(รตยจรถรท
รร@ร^รรรรร3รต#ร(รรร?รรฏQรฃ#รรZรรรDร^รฐยรรรร@ร@ร?รรร7รรรฏรDรรEรรQร3ร^รรกWรEรรrMPOยรถ$O:รรรรรรร@ร@ร^รร9ร7รร?s0รน?รฑ
tvuxw y3z*{|y~}y3vย3ย&ยrz5ย~ยย;{VยYยย4{9ยยย5ย5ย9ยยVย ย?ยยVยยLยยยu
ยu=ยยยยz5}xยยย5}ย}y9ยยยยย/yย?ย9ยvz5ยยย/yย.ยยยย*ยย;ย{ยยย}ยย|ย;z*ยยxย5vยยz5{`ย3ย/ยยยย|ยยยiยiz5ย3ยยก?ยขยVยยยยzยฃย/ยยคย3vยZย;z5ย}ยย?ยL;ย=ยYย/ยย}ยย/z5{Vย/ย9ยย{ยย*ยย}}
ยยยย;}z*{1{Vvย}ยย|ย;z5ยย/}ยยvยย$ยVยฅยยยยJย;ย/{Vยฅยฆ{ยยย}z5}ย/ยยย3{VยงJยยย5ยv;ย/z5ย/y3ยi}xย3ย/?ยขz&ย$ย1ยNย$ยย{ยz5}z5vยยจย$ย/{ยย9ย$ย3ย/ยยคยยy3v}ย1{ยยยiยย5ยVยฉ$zยฃย,ยง
z5}1ย/yยย}/ย;ยiย4ยย}ยย/yยย{Vvยiยย5ยVยฉ$zยฃย,ยงZ;ย=ยชYยซ.ยฅยฌยยยยยiยยยยฃยงrยญยจยฎ"ยฏ9ยฐ$ยฑu
ยฒ$u.ยช ยz5ยย?ยยงZ{ยvย3}ยย|ยยz5ยยYยณยด ยต4z*}.ย/?ยยถ{VvยยขยVยฉizยฃยทยธยยนยยYยยยยงiยบ$ยดยผยป!ยฝยผยดย$ยยย5ยzยฃย/}`}ย3ยย;ยย/}1ย;ย/ยย{Vvย}ยV{ยย3ย/z5ยขยยกz*ย~ยฝ^ยตย$ย3ย/?ยขz*ย3ย9ย
ย/yยย/ยยยVยฉ$z5}ย/}ยกย;ยยพยฎz5ยiยย5z5{ยzยฃยยฑยคยย|ย$ยVย/z5ยยยจ;ยยผย/yย4ย$vยZยยz5ย3}ยu
ยฟ u.ยชgย3z5ยย;ยยงร{ยยย}ยย|ยยz5ยยยจยณ ยด ยต z5}Jรrยฅ"ย/z*ยยyยยz5ยทยยยขvยVยยงรยขยยยย5ย3ยiยบ ยด ยปยฝ ยด z5}J}ย3ยย;ยย/ย9ยรยยงรยยzยฃย/yยVยJย;ยยยiย}ยJรรยขvย;ย*ย3ยย}ยz5ย
ยฝ ยต ยยยiรvรLรยขยยยย5ย3ยย}ยยย1ยฝ ยต ยยยยยPยP{ยvยยขยย/}ยยยยฃยงยยVยขยVยยงยธยขยยยย5ยยJยบ ยต ยป-ยฝ ยต ยยzยฃย/yยVย4yยย}4ย?ย4ยiv}ยยรร}ย3ยย;ยย/}ยz5ยยพยฝ ยด ยย4z5}
}ยย3ยยยย/ย9ยrยยงรรvรLรยขยยยย5ยยย}ย;ย=ยฝ=ยดu
ยฌu.ร`yยยงรยLย$ยย/yยVย/ยi;ย/ยJ}y3?ยey9ยeย/yz5}Nย/ยV}ยยยฃยยยยย3ยย5z5ยย}Nย/rยยย3ยฅยฆยz5ยย?ยยงรย3ยVย,ยยคยย/ร}ยรยกยยนยยยย;ยยง!รรยรรยยyย}ยT{ยยย}ยย|ย;z*ยย/}
yย9ยขยยย;ย/zยฃย,ยงkรT;ยNย5ยย}}ยกยยยยรย;ย/ยยย;ยYยiย}ยยกรrยฅ"ย/z*ยยyยJยฎ"ย;ยรร?ยฅ,ย?ยยงr{ยvย3}ยย|ยยz5ยยNz5}Yรkยฅ"ย/z5ยvyยยzยฃยทยพย;ย*ยzยฃย/}ยกย3z*ยย;ยยงrย3ย/?รยV{Vย/z5vย}
ย;ย/ย4รrยฅ"ย/z*ยยyยยฑย3zยฃยรย/y3ย4รรยรrz5}Y}ยย/ยยยยย5ยงรยฎยฎรรรt?ยฑVยฎรNร-t?ยฑร t9ยฑยยฅยฆ{ยvย3}z*}ย/ยVยย9ย$ย/yยVย~zยฃยYz*}ยยยย*ยยย;ย*ยยฃยง~{ยvย3}z*}ย/ยVยย9u
ร$u.ร;ยย/y3ย=}/ย;รยxยย$}z5ยiยย5z5{ยzยฃย,ยงยกย`ยxยยz*ย5ยย3ยยย3ย/ยย}ยยยยยยย3ยฅยฆยz5ยย?ยยงNz5ยiย3ย*z5{9ย?ย/z5vยย;ย{ยยย}ยย|ยยz5ยย/}ยคยฎLยยนยยยยyz5{yยย;ยยยญยจยฎยฐ$รVรรรPยฏ;ยฑ
ยยย5ยยยย/zยฃย/yยรz5}ยย3ย/ยย}ยยยย/ย9ยยฑย$ยยยยกยTยย;ย|ย;ย*ย5ยยยยยยย5ยv;ย/zยฃย/yยรยยยย5}i}ย/ยย$z*ยยย~z5ย~yz5}ยยย;ยยVย9u
รร$ร

รJรยนร=ร=รกรฃรขTร=รค=รฅยร"รฅ;รกรรฆPรค=รงPรจeรฉร=รชยถรซยรฌxรค.รงPรกยผร"ร.รคยผรญ=รฎรขiรฏรยจรฅ

รฐ7รฑ=รฒยรณ9รดยรตรถ^รท-รทรธรนรดNรบรป`รถ^รผยผรดYรด?รฝยผรถ^รณ9รธยรฒ;รธรณ?รผxรพรด?รณรฟยรบrรตรพ"รบรณ?รณย
รถ ยผรป"รนxรบรฒ รรตรถ^รนxรณ9รด9รฒ;
รบ ยฆรนPรด?
รณ "รณยรบrรณ;รผยผรป1รตรพ"รบรณ?รณ1รถยผรบรพ"รพรด?รฝxรถ^รณ9รธ
?
รฒ

รธ
?
รณ

รธ
P
รน
9
รด
	
รธ
รฃ


รบ
`
รป

รถ




รธ
1

x
รน

รบ

รท

รธ

รพ





	











r


รต
^
รถ
=
รน
9
รณ
9
รด
;
รฒ

รบ
"


รน
?
รด

รณ
ยจ
รฟ
?
รด
ยผ
รฝ

รธ
ยถ


รบ
?
รฒ

รธ


/


รด


^

P
รฝ

รด


!

"
$#&%รบรพ"รพ(')`รบรน* รฒ?รถ,+รรตรถ^รน-
รธ/.10

รบ "รนPรด?รณยรบรฒ?5
รธ 4รรนยผ
รถ +rรน-รด9รถ!รป`รธiรด9รฒ;รบรตรด?รบรป=รพ
รธ 076kรถรด "รตรธZรด?รฝxรบรดJรตรถ^รนxรณ9รด9รฒ;
รบ "รนรด?รณยรตรบรพ"รพ"	
รธ 98):<;=;>@?รป-
2  39รธรตรด 
รธrรตรถ^รนxรณ9รด9รฒย

รบรน 2 รธรธ 4A#B	CCD$'4รบ*
รน ร"
รป FEยรบH
รน Grรธรนรด9รธรนยผรฒ รรต 4IJ>@?LKM#B	CCD$'ยรบรฒ?รธZรบรตรด?รผxรบรพยฆรพ / 		(@4รตรถ^รนxรณ9รด9รฒ;
รบ "รนรด?Oรณ N,0QPรรน
รด?รฝยผรธยพรถรด?รฝยผรธรฒยธรฝxรบR
รน Sยค
รบ รผxรนxรตรด รถ^รนxรบรพNรตรถ^รนxรณ9รด9รฒย
รบ "รนPU
รด TWVYXZT\[]"รณ~รนยผ	
รธ รด?รฝxรธรฒรรนxรธรตรธรณ?รณ?รบOรฒ "^รพ _/รด ^รฝP
รด ยรนxรถ`
รฒ !"รบรพยฆรพ
#รณ9รธa
รธ ยรถรฒk/รธ .รรบรท  รพรธรรตรถ^รนxรณ9รด9รฒย
รบ "รนPM
รด bdcfe"gh"I
รน i\^รผยผรฒ;a
รธ D$')=รป=รผxM
รด j Vk[ รฒ?รธรท-
รบ "รน=รณiรฒ?
รถ + รตรถ^รน 
/รธ .l#,
รธ 
รธรน รด?รฝxรถ^m
รผ ^รฝ
j<[VZรท	รบ gรนxรถรดรรป`,รธ ')0 2 รผยผรดรบรต;R
รฝ 
รธ 
<"m
รน   รบรด?รฝ\รตรถ^รนxรณ ยฆรณ9รด9รธรนx/รต "รน7รบรฃรนยผรธnรด +Tรถรฒ 4+kรฝยผรถ^รณ9รธรตรถ^รนxรณ9รด9รฒ;
รบ "รนรด?รณยพรบรฒ?รธ รบรพ"รพ
รผxรนxรตรด รถ^รนxรบรพยกรท-	
รบ รฃรตรฒ?รธรบรด9รธรนยผรถ^o
รน ,รผ=รนxรตรด รถ^รนxรบรพยกรตรถ^รนxรณ9รด9รฒ;
รบ "รนรด?p
รณ #,รถรฒรรด9รฒ;รบรนxqรณ ,รถรฒ;รท รณ9รถ^รทr
รธ ,รผxรน=รตรด รถ^รนxรบรพNรตรถ^รนxรณ9รด9รฒ;
รบ "รนรด
"รนรด9รถ รนยผรถ^o
รน ,รผxรน=รตรด รถ^รนxรบรพNรถ^รนยผรธ)รณ ')0a
รบรน 2 รธ
รธ 4s5รณรรฒ?รธรณ?รผxรพ"M
รด ยผรถยรธรณยธรนยผรถรดยธรบ R รพ Kรบ!
รน รรทรถรฒ?I
รธ #รณ "รน=รตรธรรด?รฝxรธรฒ?รธรบรฒ?รธ-รนยผรถ^รน
รฒ?,
รถ + รตรถ^!
รน 
/รธ .รตรถ^รน=รณ9รด9รฒ;
รบ "รนรด?)รณ ')0Yixรผxรนxรตรด "รถ^รนxรบรพxรตรถ^รนxรณ9รด9รฒย
รบ "รนPรด?	
รณ Pรผxรนx^รพ ^4รธJ*
รป  3ยรธรตรด 
รธZรตรถ^รน=รณ9รด9รฒ;
รบ "รนรด?
รณ รบรฒ;5
รธ "รนPรด9รฒยรบรตรด?รบรป=รพรธ
"t
รน รธรนยผรธรฒยรบรพยรตรบรณ9
รธ 0
รน  รตรถ^รท-รทรธรนt
รด "รณ รท-รถรฒ?v
รธ "รท  รถรฒ;รด?รบรนPรดรฟCรบรพ"รพr
รถ ยธรด?รฝยผรถ^รณ9รธรฃรฒ?รธรณ?รผxรพรด?รณ รบรณ?รณ?รผxรท-รธ รด?รฝxรบw
รด >@?(?4
รถ !รด?รฝยผรธ
u รฝxรธKรณ9รธรตรถ^R
รตรถ^รนxรณ9รด9รฒย
รบ "รนPรด?รณ!
รถ Jรด?รฝยผรธ-รนxรธnรด +Tรถรฒ 4Kรป`รธรพรถ^R
รน  รด9รถยถt
รบ $^
รธรนAรตรพ"รบรณ?H
รณ #Kyx-K^zt/รด ^^รฝP
รด J!"รบรพ"{รพ Yรฒ;
รถ + รตรถ^รน 
/รธ .|0,00)')
รบR
รน รรถ^รนxรพ !รบ * รพ h"รนรรด?R
รฝ "รณNรตรบรณ9
รธ 0 u รฝยผW
รธ RL}`รธรฒ?รธรนxรต
รธ รบR
รน S$"รนรรถ^รผxรฒNรถ  "R
รน รถ^
รน รบp
รน "รนรด9รธรฒ?รธรณ9รด "m
รน U"รท  รฒ;
รถ 
รธรทรธรนรด 
"รณยธรด?รฝ=รบรด!รด?รฝยผรธ-รฒ?รธรณ;รผxรพรด?`
รณ +Tรธ  รฒ?รธรณ9รธรนF
รด "รนeรด?*
รฝ "รณ  รบ  รธรฒรรบรพ"รณ?รถยถรบ R รพ v+rรฝยผรธรนCรถ^รนx^รพ ~=@Hยธ
รถ ยจรด?รฝยผรธรตรถ^รน=รณ9รด9รฒ;
รบ "รนรด?รณ
รบรฒ?h
รธ รผxรนxรตรด "รถ^รนxรบ{รพ 0
รฒ #ย~
รบ 
"ยย	CCย$')aI
รด ยฆรณ รณ?รฝยผ
รถ +kรน รด?รฝxรบย
รด "รน รบ  รบรด?รฝ รตรถ^รนxรณ ยฆรณ9รด9รธรนPย
รด ย|ย	ยยธรบ!
รน 
ย รน รบ  รฒ;รธ 
"รถ^รผ=รณ  รบ  รธ
รตรถ^รนxรณ ยฆรณ9รด9รธรนP7
รด ยฆรนxรณ9รด?รบรนรด "รบรด รถ^รนรตรบรนรป`รธยจ/รธ .ยรด9รธ*
รน ยผ	
รธ ยพรด9รถรรบยธรณ9รถ^รพ"รผxรด รถ^]
รน ย,รถรฒ4
รธ 
รธรฒ รนยผรถ^<
รน nยฆรนxรณ9รด?รบรนรด "รบรด9	
รธ ]
รบ)รฒ "รบรป=รพ
รธ 
รด?รฝยผรธรฒ?รธK/รธ .<ยฆรณ9รด?รณ รบ7รณ9	
รธ ยยรผยผรธรนxรตรธK
รถ h,รผ=รนxรตรด รถ^รนxรบรพiรถรฒ *
รป  39รธรตรด 
รธรฃรตรถ^รนxรณ9รด9รฒ;
รบ "รนรด?ย
รณ ยรฒ?รถ^รท รบย
รน #รบ!
รน R'ยยฆรนxรณ9รด?รบรนรด "รบรด9	
รธ 

รบOรฒ "รบรป=รพรธรด9รถรฃรด?R
รฝ "รณรรนยผรถ^<
รน n"รนxรณ?รด?รบรนPรด "รบรด9	
รธ 
รบ)รฒ "รบรป=รพ
รธ 0 รฐGรณ?รผยผรป=รณ?รธรด!
รถ iรด?รฝยผH
รธ 
รบOรฒ "รบรป.รพรธh
รณ "รณรรด?รฝรผxa
รณ ยผรธรฑ.รนยผ	
รธ Sยกรณ?รผxรตยรฝ
รด?รฝxรบรด4รบรน ร
รถ *รด?รณยรตรถ^รนxOรณ "รณ9รด9รธรน\
รด ยฆรนxรณ9รด?รบรนรด "รบรด รถ^รนรรตรบรนยพรปยครธT^รพ "รนยผรธรบรฒยรพ ร/รธ .ยรด9รธR
รน x	
รธ ยพรด9รถรรบ~รณ9รถ^รพ"รผxรด รถ^
รน 0ยกรฐ~รตรด?รผxรบรพ"^รพ   รบรด?รฝ
รตรถ^รนxรณ ยฆรณ9รด9รธรนx/รต ย"รณ~รนยผรถรดรรนยผรธรตรธรณ?รณ?รบOรฒ .รฟ~H
รบ +Tรธ@
รบ 4รธรฒรรพรถรรตรบรพยกรตรถ^รน=รณ "รณ9รด9รธรนx/รต ย(ย=$ยO=@;R~)~n/;|ย@S"รณ~รณ?<
รผ ย 
รต รธรน
รด 1รบรณ
+Tรธรรนยผรถ +  รฒ?รถ  รถ^รณ9รธยธรด9รถรณ?รฝxรถ +h0

ย1ย^ยยยJยยยย,ย(ยยย*ยยยaย|ยยขยกย,ยฃ,ย*ยย|ยยกHย*ยยขยคยฆยฅIย(ยฃ,ยง"ยยยง"ยคdยจยฃยยRยฉJยชยก
ย รรบ`ย1ยยhยซยฆ"รณSรผxรนxรตรด"รถ^รนxรบรพ{,"รด?รณYรตรถ^รนxรณ9รด9รฒยรบ"รนPรดรฒ;รบ  รฝรรตรบรนรรป`รธQR^
"^xรธ	9"รนPรด9รถkรดn+Tรถ~รณ;รผยผรป=รณ9รธรด?รณ0YยฌWยญZรตรถ^รนรด?รบ"รนxรณยกรด?รฝยผรธ
#Rรฒ;รธรตรด9รธ	'ยจรบรฒ;รตรณrรฒ?รธ  รฒ?รธรณ9รธรนรด"รนmรด?รฝยผรธhรผxรนxรตรดรถ^รนxรบรพ1รตรถ^รน=รณ9รด9รฒ;รบ"รนรด?รณ*+kรฝยผรธรฒ?รธ!รบรน รบรฒยรตF"รณ5*รฒ?รธรตรด9รธ	A,รฒ?รถ^รทยฎTยฏVยรด9รถ
TWยฐhL}ATWVXยฑTWยฐ-ร
 รบรน*tยฌยฏZ
ยฒ รตรถ^รนรด?รบ" รนxรณJรด?รฝยผรธ~รธ	m รธรณh#ยณ{K(K^ร
 รผxรนRR" รฒ?รธรตรด9รธ	')!,รถรฒยจรนยผรถ^รน,รผxรน=รตรดรถ^รนxรบรพ=รตรถ^รน=รณ9รด9รฒ;รบ"รนรด?รณ0
 รธรนยผรถรด9รธFยต_cยถ#(ยทยยธ)ยฌWยญUยนIยฌยฏยฒT
' รด?รฝR" รณW
 รฒ;รบ  รฝx
 รบรนRยยต1ยญhcยถ#(ยทvยธ)ยฌWยญ-'W รด?รณยบ* รฒ?รธรตรด9รธ	ยถ
 รณ?รผxรปR รฒ;รบ  รฝ0
ยด รธax
 รถ$" รนRย,รผxรฒ?รด?รฝยผรธรฒ	mT
+ รธรรนยผรธรธ	 รด9รถ-รฒ?รธรตรบรพ"รพยกรณ9รถ^รทรธh รฒยรบ  รฝยถรด?รฝยผรธรถรฒO
 รนxรถรด รถ^รนxรณรฟยรด?รฝxรธยป!/~/);ยป$>@;ย
 รถยรบ
2 รธ,รถรฒ?รธF

รธรฒ?รด9รธ/.ยผ#(
 รบรฒO" รบรป=รพ"รธ,')x
 รบรนR
 รด?รฝยผรธยยฝq=	=Y
 รถยรบHR รฒ?รธรตรด9รธ	t รฒ;รบ  รฝ1รฟ
 รธรณ;รตรธรนRx
 รบรนรดw=q8ยT V (;ร>รยป@(ยฝqO	nOยปxยฝq>)ย*ร
ยฅAยง-ยพ\ยJยย,ย(ยยยฟย1ย^ยรรรrยง"ยฃยร*ยงรvร-รร<รรยร /ยฝ/nรยTWยฐร~o>wยผ
#(ยทยยธ)ยฌWยญ"'F รร(ร<ยฝqยร$ย~/(~a(;vยฌWยญw>Fย<>(ร_#(TรVยณยธqT7[ร/ยธรรรยธqTY[ยร@ยธรรร	ยธqT7[{ร!ยธqTWยฐ'F~):R/รย(ร">$YTรV\XรT\[Oร/ยธรรรยธ
TY[ ร XยฑTY[ยรnร ร ยธรรรยธqTY[ ร XรTWยฐรร|รaร@รร รรร$K
ร /ยฝ/nรTยฏรกAย~ย> รฒ?รถยรถรดt=q8H>oยปยฝnยปIx$ยฝB>)ยรยผ#(ยทยยธ)ยฌWยญ!'ย รรข>@;Rยย=(ร<ยฝH@ยฝnOรoย~H>ยยป!/~/);ยป$>@;
8ยฝB=@รฃTยฏรก@K
รน m+Jรธ9ยผรธรฑ.รนxรธรรบยฝq=	=ร~	ยครถNรบ]Rรฒ;รธรตรด9รธ	tรฒ;รบ  รฝ0 ย รด5"รณTรธรบรณq รด9รถยพรฑยครนRรถ^รผxรดiรด?รฝxรบรดZรบยฝB=	=$S"รณiรบ
2 /รธ .รรด9รธรนxรณ รถ^S
รฒ?รถยรถรดrรณ9รธรด5+kรฝR"รต;รฝ รตรถ^รนรด?รบ"รนxรณrรบ-รณ"รนm^รพ"รธU
รธรฒ;รด9รธ/.|0

> รฒ?รถยรถรดยจรณ?รธรด]=B8F>ยป@(ยฝqO	nOยปrx$ยฝB>ย*รv#(ยทยยธ)ยฌWยญ!'9 รย>@;Rยย@/ยฝ/nรt=B8
ยฅ ยง-ยพ\ยJยย,ย(ยยยฟย1ยรคยร ~:R/~	Sรฅรฆ=B8ยบยทZย~Fร
A
ยทรงร_รฅรฆ~r>Iยป!/~//;ยป$>;*18)ยฝq=@รจ>@;l?H/;*ยบ=B8MรฅoKMรฅรฆ~aO>@?(?Oยป-รทยยฆรนR"รท-รบรพQ รยถ(ร<ยฝqHยป$=~r;=ยOร~
>ยฝq=	=ยฏ~	ยขรฅยรฉY~):Rรย(ร">JรฅHรฉรชรซรฅoKWรฅรฌ~aO>@?(?^
ยป รทยยฆ รนR" รทรรผxรทรญ รย(รm/ยฝpยป$=/~F;=5ร$ย~/ยบ>Hยฝq=	=ยฏ~	JรฅHรฉ
~):RรA(ร<>5รฎ รฅ รฉ รฎ"รฏรฐรฎ รฅยรฎLK
รฑ	รฒรณvรดยณรตรถยรทรธ{รนnรบOรปรผรถ	รธJรฝ$รพ รฟQรป รท


รป Bรดยณรธ&รปYรป	FรปkรธSรป รท
,รถยรดยณรธ&รปรผรต)รถ@รบ
 รน&รต1รพmรธ&รตRรฟ1รพRรฟ!"
 รน&รต#mรฟ7รธ&รต|รพ$Rรฟ"%Rรฟ&ยรฒ
')(+*

,-/.102

314!56567893):;7=<

S!T 8UP4 T

UV7=<

314!56567893):;7=<

3ONP4!89Q!5O:;7=<

S!T 89UP4 T

UV7<

3ONP4!89Q!5O:;7=<

3):RQ/:;7=<

UP4):XW17<

Y

3):RQ/:;7=<

>?A@CBEDGFIHJKFL&M

UP4/:W17<

>/H?A@CBEDGFIH+M

Z"[]\^I_R`bacEd=^OeO\f_&g!h6ijgfkOlm_&nVnfopR`qornfsPtuwv
xEyzG{$|~}/ยยzยVzย|~}/ยย~ยyfยยVย$zย|ย})ยย9yยย~|ยยยzย|ย})mย=ย$ยxyfยยx6|ย}
nf_
314!56567893):;7=<

S!T 8UP4 T

UV7=<

ยzยVzย|ย})~xyfzG{P|ย})ย~ยyfยยVย$zย|ย})VยยPยExyfยxO|~}/Pย9yfยย|ยยEยzย|~}

g!_R`ยeยnfoRijยยยยยnยยhยg!oR[e6ย`fย
xEyzG{$|~}/ยยzยVzย|~}/ยยยPยExyfยยx6|~}/ยย~ยyfยยยVยzย|ย})ยย9yfยย~|1ยEยzย|~}

3ONP4!89Q!5O:;7=<

[]phO_R`ยOย=`+ljeยยยยยe6^Ioย[oยก[;pยกkOnfoยยยยยยnยยh6g!oR[]e6ย`fc
Y

3):RQ/:;7=<

iOgfpยkInยขgfk6ย`+pยฃoยฃnf_

[]kยoRiO[]pยnf_&lI`q_&[]kO\ยฅp&^Oย&i#oRiOg!o

BEยค

ย=ย$ยxyfยยx6|ย}
BEยคยกยฆ

ย=ย$ยยง

xEyยยx6|ย} ย

UP4):XW17<

xEyzG{$|~}/ยจย=ย$ยxEyยยx6|ย})ยฉย~ยEyยยVยzย|~}/ยย9yfย$ย~|ยยยzย|ย})ยยzยVzย|~}
[]pยkInforยAยยยnยยh6g!oR[e6ย]`fcO[or[]pยkInforh6_R`ยOย=`+ljeVยยยยจย
Z"[]\^I_R`ยซยชIcยฌยAยยยnยยh6g!oR[e6ย]`ยgfkOlยkInkmยAยยยnยยh6g!oR[eยย`ยซnf_&lI`q_&[;kI\pnfsPtuwv

Z[\^I_R`ยซaยก[]ย]ย]^6pยฃoยฃ_&g!oยฃ`+pยฌoRiO`ยญkOnfoR[nkOpKnfsPlO[_&`+ยoยฃ`+lยp&^IeO\f_&g!h6iยgfkOlยฎnfs$_Rnnfoยpยฃ`qoยยฏยiI`q_R`fยIgยยฐยยฑXยฒOยฑยฐยยณ=ยฐยด_Rnnfo
pยฃ`qoยตยถg!hOh6ย;[`+lยoยฃnjn^I_ยญ`ยIgfยยh6ย`ยกtEuwv=ยยยทnfoR[;ย`ยกoRi6g!oยธoRiO`ยg!_ยยน[]pยธnkOย]ยยlO[_R`+ยoยฃ`+lยบs_Rnย
ย9yfย$ย~|ยยยzย|ย} ยOgfkOlยkInfors_Rnย
gfkOl

ย9yยย~|1ยEยzย|ย}
ยฆ

ย9yยย~|1ยEยzย|ย}

ย9yยย~|1ยEยzย|ย}
ยฆ

oยฃn

ย~ยyfยยยVยzย|ย}

ย~ยEyยยVยzย|~}

oRiIn^I\iยยปK`bi6g+ยผf`beยnfoRi

oยฃn

ย~ยyยยVยzย|~}

ย~ยyยยVยzย|~} ย$ยฝiO[]pยซ[]pยซnk6ยยยsnf_ยoRiI`ยฎpRg!ยพf`AnfsยpR[]ยยh6ย][]ยq[owยยc

oRiO[]pย[;kIsXnf_ยยg!oR[nkjยป[]ย;ย1kOnfoยe1`ย^6pยฃ`qs^6ย~[]kjoRiI`ยซsnย]ยnยฟยปยถ[]kI\Iย
รร[oRiยจgยฎ_Rnnfoยถpยฃ`qoยถยยจย1ยปK`ยกgfpRpยฃn=ยq[]g!oยฃ`ยกยยจรยฃร&ร!ยฐยถร=รfรยยฑรรยฑรรยฑรยร!ย~gยhO_&nfh1`q_&oยยmoRiOg!oยธยqgfkยจeย`ยยผf`q_&[ย6`+lยeยยgfk
nf_&lI`q_[]kI\ยnfsรยขc
รmรVรร"รXร/รรยรรรยร;รยดรยญยฒยฅร!รยฃรรรqรยฑยฒVรยรรรร
รจรรฉยรช1รซยฌรฌรฎรญ ย
รฒรฉยรช~รณยรดรฎรญ ย

รญรฐรฏ
B

ยครฑ

ย

รญรฐรฏรตรถยรท#รณ

Bร+รกยฃBรข!รกqรฃqรฃqรฃ+รกยฃBรค

ยฑ;รฅbรร!รXร]รRรยกยยรยnยยh6g!oR[e6ย`Aยฑ รฆยถรง

ร!ยฒยร
รฏ

รฅยณOรรธยรรธรร

B9รนยถยฆรบBEรป รฉ

รผรkยnfoRiI`q_EยปKnf_lOpqยรoRiO`rย6_&pRoยฌยผ)g!_[]g!e6ย`+pยฌ[;kยoRiO[]pยnf_&lI`q_&[]kO\ยg!_R`oRiInpR`ยปiO[;ย&iยeย`+ยnkO\ยซoยฃnยนยรฝยฏยปK`ยธoRiI`+kยฎpRg+ย
oRiOg!oยoRiO[;pยnf_&lO`q_&[]kI\ย[]pยรยรRรยรพPรฟรRรยซeยAยmยตยOgfkOljgfkรยยฎnfoRiI`q_rยผ!g!_&[]g!e6ย]`
[]kjoRiI`bnf_&lI`q_&[]kO\mยฏ B9รน

[;pยnf_&lI`q_R`+ljeย`qsnf_R`

Bยรป ยตp&^Oย&imoRi6g!o

Bรนยธยฆ

Bยรป
Bรป

iOgfpยg!orย`+gfpRoEnkO`ยญgfk6ย`+pยฃoยฃnf_

B9รน

ยฏยpยฃ`q`ยกZ[\^I_R`ยญยชVยตย

	




!"ย[ยผfnfoยถยnkOpR[]pยฃoยฃ`+k6ยยยlI`qhย`+kOl6pnkยoRiI`ยgfpRp&[\k=ย

Z[_&pยฃoยnfs9gfย]ยรยยปยถiรยmlO[]lยยปE`bยqgfย]ย[]oEรยยฑ !รfรยร&ร!ยฒOรฅยฑXรฅรยรยฒ1รqร

$# &%ยญkยบ`qยผf`q_Rย

ยย`+kVoยซnf_&lI`q_&[;kI\ ยฟย

pยฃoยฃ`qh

nfsEoRiI`ยgfpRp&[\kOยย`+kVoqยoยยปEnm`+ย`+ยย`+kVoRpยiOgยฟยผf`ยoยฃnยeย`ยlO[]pยฃoR[;kI\^O[]pRiO`+l~c

ย6_&pRoRยยfยรoRiI`ยญpยฃ`qoEnfs~oRiI`ยธsXnf_&ยย`q_&ยยย[]kOpยฃoRgfkVoR[]g!oยฃ`+lยฎยผ)g!_[]g!e6ย`+pยธยฏoRiI`ยนยฑยฒIรฅรรร!ยฒ6รยยฑGรfรยรRรยรฅqร+ร;ยตยgfkOlยpยฃ`+ยnkOl6ยยยoRiI`

'!(*)!+-,.+-/10324/658749.:+624;=<>74+19@?03/A0B74CD<=9@?FE	+-EG74;=?<H5I03?:J0K:F03A70B74+1LK;M<=9K?E	+1EG74;H?F<H5ONQP;=<CR74;H2SUTV;W0324/QXZYW[K'@']\>(
^F_R^

`badcefgheij@akj3fVlminmoqpFertsuviwnmfxakewixyz&g.{Z`|j

}x~]Vย|ยFยย3ยยยยยQ~ยยWยยยย~Iยk}vยWยBย}ZยBยkยยW~ยยยbยI}v~DยRย}ยย]ย$}vย3ยkยx~Rย|ยยQยยย|ย]ย$}vยBยkยBยW~ย}vย]ยยย|ยยยvยBยยย~RยBย>ยยยย~Rยยh~R~ย}
~ยยย3ยยยk}vยWยBย}ZยBยkยยW~ยยยยW~Rยยย}vยยยBยx~ย}x~]VยยยFยย3ยยยยยQ~ยWยยย~ยยk}vยWยBย}ZยBยkยยW~ยยยยยxยยย~ยยย3ยยยยยยBยx~ยยW~ยยW~RยBยยยกbย
ย]ย$}vยWยWย@ยยk}mยDยvยยยย&ยยยBยBยkยRยขยkยย8ยBย$ยQ~ยฃยWย&ยยkย!ยwยคhยBยx~ยฃย]ย$}ยBยkยWยW~ย}vย]ยtย@ยx~ยยBยฅVยDยBยขxย3}ยย$}ยฆยQยRย.ยง8ยvยkยยย]ย$}vยWยWย3ยยk}Zย
ยkย|ยBยยขvย8ยRยยkยk~ยยยยBยx~IยจwยฉยซยชFยฌ$ยญยยยBยv~ยฃยW~RยRย
ยฎยฏ	ยฐยฒยฑยดยณFยตยถยยตยท*ยธยนxยธยบยซยปwยท
ยvยขxย3ยBยx~Rยยย}ยยBยvยkยยW~ยย]ยBยQย$}ยก.ยh~ยยยkยkย.ยk}ZยWยBยVยvยขvย]~ยฆยBยx~ยฆยx~]ยผ*}vยQยBยQย$}ยฝยยยพยwยQยยยย]ย$}vย3ยkยWยW~ย}vย]ยยยk}รยฟรยBยW~RยยRยค
ย]ย$}vยBยยยWยW~ย}vย]ยยยย~Rยยh~R~ย}ยฟยยFยย@ยkยยยQ~ยยRยกwยBยx~ย}tยwยQยยยยยยรยยBยขxยยW~RยยยยMรรยกยย}vยtยผ*}vยยkยQยยยQยยยJย]ย$}ยBยkยWยW~ย}vย]ย
ยยยBยx~OรยรRรย
ร Zยต รยทรยบยธยบยซยปwยทรยฎยฏ	ยฐรรร ยญรรhร>รWรรรรWร.รยรยฆรรรKรvร]รรยญยซรรยญรร ร รmร3ร ร รOรรรยร8ร ร รรรร*รยร ร รtรร ร ร|รZรกยซย]ย$รขยwยยBยQยยQ~
ยฉ รฃรครรvรฅยญGรยจwรฆ ร ยฌ=รง.รจvร-รยรรรยพรยญรฆ ร รรKยญรยฌร ร ร@รยจ$ยจVยฌรKยญยฉร&รฉรยรง]ยฌร.ร ร รรร*รยฃร ร รZร|รชรซยฌรKรฌยรรฆรฆdรฅFรญยรฎรฏGรฐZรHรBรฐรรฑbรรจร-รFรญ
รฒ รฐรยดรยฆรฉรรRรยยญ4รยพรฏGรฐZร=รBรฐZรรฑhรรรจ ร รtรรwรยรฏGรฐรรBรฐรรฑbรรจ ร ร
ยxย$ยขxยDย]ย$รขยรข~ย}ZยBยยยย3ยยยW~ยพยย3ย$รขรณยBยvยkย.ยx~]ยผ*}vยQยBยQย$}ยค
รด ยยยงยยx~O~]xยkยWยW~ย}ย]~Oยยยย]ย$}vยWยWย@ยยk}mยDยย~Rยยh~R~ย}qรbรbย}vยรรMรยยkยJ}xยยDย]ย$รขยยยขvยkยWยย3ยwยค.ยBยx~ยยB~ยยkยยBยkย$}รจร-ร
รข&ยยยยยย~ยดยขv}vยQย~Rย3ย3ยย	ย
รต ยJร ร รIย}vยร ร รDยยB~ย}xยยb}x~ยย]~ยยBยBยย3ยยยQยยยยรทรถ*~Rย3~ย}mยRยรธHยย}vยยRยกร ร รIรขOยขvยBยรยย~8รhรยรน@ร3ยฌรฌJยจVร$ยญGยฉ	รบRรฆ ร ยJยQยBยยยQยBยW~ยยkย>รป
ยย}รยBยvยยยยRยยW~ยb~ยยยBยBยkรข&ยkยkยยW~ยยBยx~ยยยQยยรฏGรฐmรHรBรฐZร4รฑยพยย8ยBยx~ยB~ยยkยยBยQย$}ยรจvรรทร8ยWยยBยx~ยFยยkยขv~รฐZร.ยย8ยBยx~
ยvย$รขยยยk}รรฉร=ย
ยฟยยยงยยvยkยhยx~]ยผw}vยQยBยkย$}&รขยย!ย&ยย~JยW~R~ย}ยยย|ยยยQยVยRยยย~Rย@ยBยQย$}ยยยWยWย3ย$}xรผยยฟรนย]ย$}ยBยkยWยW~ย}vย]ยqรฏ รต ย}vยยยฟยฃย]ย$}vย3ยkย=รน
ยW~ย}ยRยQ~ยย@รฑ@ยค
รฝ รต รนย]ย$}ยBยkยWยW~ย}vย]ย*ยคยQยMยkยยvยขx~hยWยยพยBยx~|ยB~ยรขยยยBยฅยยย*ยย~ยครย}ZยยยFยยkยขv~hยk}รฉรwรขยยขvยWยMยvย!ย~8ยยพยBยขxยvยยยยBย
ยย}tรฉรmย
รฝยฝยฟรนย]ย$}ยBยkยWยW~ย}vย]ย*ยครย}Zยยย]ย$}vยBยยยWยW~ย}mยยยk}vยBยBย}mยBยkยยBยkย$}ยยยรพยร|รHรWรรรฟqรข&ยยยยยย~ร~]VยW~ย}vยx~ยยรยWยยฝย
ยBยยQย3ยยยยย3ยkยยยQ~ยกxยx~Rย3~ยพร|รmย
ยยฃ}xย!ยยยย}xรผยยBยvยยยยยBยยย]ย$}vย3ยkยWยW~ย}vย]ยยฃยยย~ยขvยQยยยQ~ย}ZยยWยยดยฟรนย]ย$}vยBยยยWยW~ย}vย]ยยก$ยh~.ยRย}ยยv~ยยvยขvย]~hยBยvยยยยยBย
ย]ย$}ยBยkยWยW~ย}vย]ยยยRย}ยยย~ยพยB~Rยยย3ยQยWยW~ย}ยยk}ยยW~Rย3รข&ย8ยยรร|รยรนย]ย$รขยยยBยQยwยkยkยQย>ยwยค
 รยรยรรยฉรbยจxรยญยซรยฆร3ยฌรvร@ยฉรKยญ ร รwยญยฉ รฃรรงRยฌรOรรฆยซรฆxร|ร>รWรรรรWร.รยฃยจVรยฉยซร Mยฉยร ร รยฉรKยญGยฉยซรwรยยญยซรญรรพยรhรHรWร|รmรฟยรรwร&รพยรMรรWร|รmรฟ
รร ร รบ3ยฌยญยซรยร|ร$รกWร3ยฌรฌJยจVรยญGยฉ4รบ]รฆ ร ร

 ~ยยQยB~ยยยxยtย3ยยkยtยBยvยยIยWย$รข~ยย]ย$}vยWยWย@ยยk}mยBยIยRยยkยk~ยยรยBยx~ยดยจwยฉยซยชFยฌยญยซรJยยยย~ย&ยwยยBยBยkยRยขvยkยยยยยยBยDยWยยยยยยย*รปwยh~
}xยยรคยx~]ยผw}x~ยพยBยv~ยรขยยค

ร Zยต รยทรยบยธยบยซยปwยทรยฎยฏ
	 ร ร ยญ ยร รรwรร ร รยรร รRรยยญ4ร|รhร |
ร ร$รญรซร.รOรรร รร*รยรbรรยร|รhร รhร
ยฉรOรยฃยwยQยยยยยฌ=รง ยฉ รฃ รฎwร รยพร Rร รยยญ4ร|ร ร รยฃq
ร รยรญร ร รรรรwรยร ร t
ร รร ร ร|รZรก=ร@ยฌรฌJยจVรยญGยฉ4รบ]รฆ ร ร
รธ=}ยยยBยx~RยDยbยย3ยยRยกvรผ$ยQย~ย}ยฆย}mยยยvยBยยย~Rย8ยBยขvยยW~Rย ยยรรยก*ยยยซยข}vย]ยBยQย$}vยยรซย]ย$}ยWยWย3ยยk}Zย8ร ร  ร|รWย]ย$รข&ยk}xรผ
ย$ยขxย รยยBย$
รข  รฏยซรbรI
ร  ย}ยรร|รรร 
ร ยรฑยดยkยยฃยรยยQยยยยดยย ยkยhย}vยรย$}vยQยUยQยhยยยยย}Zยรย]ย$}vยBยkยWยW~ย}Zย
ยk}vยBยBย}mยBยkยยBยkย$} รฏGรฐ ร รBรฐ ร รฑยยยยดร ร ย}vย ยยยฃย}ZยยยยBยx~RยยยFยย@ยkยยยQ~tร ร ยk} &ยก8ยBยv~RยB~ยฆ~]xยkยWยBยยยยยQ~ยยยWยย$}x~
ยยยkยขx~IรฐZรOยk}รฉรยฃยBยขvย3ยยBยvยยยฃรฏGรฐZรHรBรฐFรFรฑยRย}ยยย~D~]ยW~ย}ยx~ยยยยWยยยย]ย$}ยBยkยWยW~ย}Zย.ยk}vยWยBย}ZยBยkยยBยQย$}UรฏGรฐZรHรBรฐรรBรฐรรฑรยย
รพยร|ร>รWรMรรWร.ร$รฟ$"
ย !JยยพยยซยยยดยGยขv}vย]ยBยQย$}ยยMย]ย$}vยWยWย3ยยk}ZยBยRยกรซยh~&ยRยยkยรhรhยBยx~รยฌรK$ยฉ #$ยฉรย}vยqร.รยยBยx~tยญ4ร%ร # ร ยญรยยhยBยx~
ยยQยยย.ร|ร  ร|รmย

&(')'

*+-,/.10

6:

28:

5:
287
67 57
97

243
53

ยงยช

63

ยHย1ยMยยย_ย`ยยยXยยยpย1ยยยยยยAยMยยยย
ยยยยยยaยยกย/ยขยฃ

ยค8ยช

ยฆยช
ยค8ยจ
ยฆยจ
ยง
ยจ
ยฉยจ

;=<?>A@CBEDF@HGEI
JHKMLONMPRQ)S)TCSU>@CBADF@HGIPRVC@HGXWY@HZ
JHKMP$VX[]\_^`PRVHNaPbQ-STCcS)deVfP$VgThL_d(Tji/ThKMLOkNal)PbmnVSU8opBfG-qrTFKalT
P$Vsut B DFt Gjv qwsux B DFt Gjv ldayzs{x B Dh| Gjv l}R}pKalAQ-Ll~VFMNMNยS)mFTยPRd
ย Z s
mFLVfNยLjcTFPRQ)L_}Rย`t Z qet Z l)dayยx Zjvย
ย dOThKMLS)TFKMLjm8Kal)dayยqAPbT8P$VpdMLjPยThKML_m/NalTFKc_S)daVFPRVfTFLjd(TjqEVfPRdac_L
sux G Dhx Zjv Kel)VยdaSยVFMNMNยS)mFTpPbd ย B qEdMS-m8l)mhcwc_S)daVFPRVfTFLjd(TjqEVfPRdac_L
xยGยKal-VHdMSยVfaNMNeS-mfTยPRd ย B ย

ยค4ยฅ
ยฆยฅ

;=<?>A@CBEDF@HGEI
JHKMLONMPRQ)S)Tยc_ldeyยPRyalETFLยS)Uยซ>A@HBDF@GIOP$VC@HGยWY@HZ ย
JHKMP$VH[]\j^ยP$VdaSTยซNaPbQ-STยซc_S)daVFPRVfTFLjd(TjiHs
tMBEDnxยG v Kal)VยซdMSVFMNยยฌ
NยS)mFTยPbd ย Z ย

ยงยฅ
ยยยญยMยยยย`ยฎยยXยยยยยMยยฏยpยยญยยยยยยAยMยยยยยญยยยยยยeยยฐย4ยขEยฃ

ยฑ ย)ยฒยณยยดยยตrยยถยยดยยยฐยpย1ยยยยยยถยยยทยตยธยยยยนpยยยEยยยบยปยยยทยผยดpยตยยXยน4ยEยยยพยฝยยAยฟ8ยยยยฝรaยฒยถยยฐยEยตยยยฏยยญยeยยยย]ยฝยยยยAยยยดยยยฐยยยยยยยญยMยรยยยท

ร8ร{ร(รยรยร_รยรยรjรuรรfรEร8รEร ยยยทHยตรย4ยขEยฃย
รยฏรeรHรรuร-ร{ร8รรรwรยญรรร ร{รrรEรยพร ย4ยขEยฃรรยรยปร{ยผรรรรjรยฐรรก`รข_รฃ ร~รคยรรยรยซรรEรยรฅรnรฆ ร{ยผรรjร`รข รrร8รงยรrรยธรฅรจยร_รrรฉร]รMรรชร%รซEรฌ1ร
ร(รjรjร1รญยรยรฉรรAร8รรฎรrรคยรงaรAรคร{รeรญรฏรฐ ร รฏยรฑ รEรฒEรฒEรฒร รฏยรณ รฃรฎรด รยฏรEรrร ร ร
ร ยpย1ยยยยย"ยAยMยยย_ยยญยยยยยยeยzรดXรต รค รต ร รต ร{รถaรuรรrรคnรงeรAรครuรeรญรทร รธ
รน4รบรผรปรฝ รรฟรพ รฅ รพยรฃ  รบ ร รต ร รต รฏ 	 รฏ 
ยพร
รรฎร~ร8ร{ร(รMรยถรnรฆ ยบ 
 รฐ ร รฏรฐ ร รฏยถรฑ รEรฒEรฒEรฒร รฏ 
 รฐ รต
nยยยทuยยย"ยตยร]ยEยยยEยยพยrยตrย_ยยญยตrยน1ยยกยฒยดpยยญย_ยดยฏยฝยยยยยยยยยยยน/ยยญยMยยยยยยย รฅ ย
ยยยยดยย~ยยตrยยยยEยยยยทยต ย8ย1ยยยยยOยฒยดยยMยยยยยยยjย1ยMยยญย
ยยญยOยน4ยEยท{ยยยย`ย1ยย
ยยฏยยดยยรฎยตยย_ยย1ยMย ยยeยOยยย_ยฝยยEยjยยญยยยย ยHย1ยMยยยยยย ยตยยpยฝรยฎ ยยดยย)ยฒ=ยต"ยpย1ยยยยยยAยMยpยยยญยยยยยยeยยฐย4ยขEยฃzยตยยpยฝยต
ยยยMยยฏย8ย1ยยยยยยAยMยยยยยญยยยยยaยยฐย4ยขยฃ
ย ย1ยยยยยzยAยMยยยยยญยยยยยยย
 ยยยทยธยตย/ยขยฃรยยยญ ย1ยยรผยMย ยยดยย ยย ยยญยยยยยยยยAย ยยยทยยต ยยยEยยฏยยยท`ยทรชยยยยยAยย1ยMยpยตยฐ
 ยAยMยpยยยยย_ยตยยยญยeยย
ย4ยMยยยยยย_ยยญยยย~ยpยตrย_ยยยญยEยยญ ยตrยยยยยยยย4ยEยยย1ยยEยยยดยยยpย1ยยยยยยยร ยฐ
 ยAยฟ8ยยย1ยย1ยMย"!#$ยธ
! ยตrยน/ย-ยยย-รข&
% ยดยยยยEร]ยต รฉยร{รยรuรฉ(]' รฉ ยยยEยยยยยท
ยทรชยยยยยAยย1ยMยยยต4 ยAยMยยยยยย_ยตยยยญยeยยยEยตยยยฏยน4ยยย_ยดยยตrย_ยตยยAยยยEยjยE) ยยฝwรpยยย ยยดpยตrยยยดยยรฎย4ยขEยฃยฏยยญยยถยpยยญยยยยยยAยMยยยยยญยยยยยยeย*&% ยดpยยญยยยยญยยยยดยย
ยpยยย_ย/ยMยยยฐยยยทยยดยย~ยยดยยยEย~ยAยMยยยฝยย1ยยยญยMยยยOยยยยตrยยยยฝย
ย+
 ยAยฟ/ยยย1ยย1ยMย!#-ย, ย
. ยAยMยpยฝยย1ยย1ยMย0/ยฐยยยยยยยยยยถยยดยยEย_ยยฐยยยยญยยยยยยตยธยpย1ยยยยยยถยท{ยยยOยยตยย_ยดยฏยบ 
1 รฐ ร รบยพรป รฝ ยCยยดยยยยยยEยFยฒยยยย2รผยยญยยยยดยยEยยEยท{ยยยย
ย8ย1ยยยยยยAยMยยยย
ยยยยยยaย*3
. ยAยMยpยฝยย1ยย1ยMยยย54ยกยตยยpยฝ6!71ยยตยยยยฐยยยยยยยยAยย_ยยตrยยยAยMยยยยยยย_ยตยยยญยeยยCยMยยยEยยMยรฎยMยยยยpยยญยยยยยยยยยEยยฝยยCยยย8nยยตrยยยยEย9
ยยตยยjยด รฏ
;: ยผ=< รฅ รpยตยยยยฝzยยย ยpย1ยยยยยยยญยOย_ยe> ยยยยญยยยฝยพยยญยยยยยญยฝยย รฅ 
?A@B

CEDGFHI0JKHLMDMNIPOQLRQSUTAHVWXLYRQIDHYLZ[6J\]CM
^+_]`acbedbgfYaihjkmlngoqp*rts6uwv*xUy{z}|g~ยยยย{zยยยยยยยKย1ย	ยNยยsยยยยยยยย*p*ยยยยsrยยยยยNยqยยยPsยย nยย*ยp
sAยยnยrย6prYย6ยqยยยQpยยner#ยยกยยยข*ยยยฃ*ยค*ยค*ยคยยEยฅย"sqrYยยฆsยงยp*ยยจยยชยฉยซยฉยฌPrYย1ย n ยrYsqยยญยNยqrยยย ยยsqnerYยgย+ยฎยฐยฏยฑยย]ย6n ยฉ0ยgยฒp
ยฉ*ยqยeยยqยณยดngr]ย+ยgยฒQยpNpยจยNยqrYยngย nยยqrยยซsยยp	ย*sย nยตยgยถKpย
ยทQยธยน ยKยบ;ยป~=ยผยกยยยฝ#ยยพ	ยฟรยยบ	ยปยยฎรย ยธ ย ยธรยรยฆร sqrYย;ยKยพ	ยฟรยKยบยneยยYngoAยยยยชยฉยรรยบร ยข
ร sqrร"oAsqยยnยsQยยp(ยยชยฉย~รยผรยรnยตย	ยgยฒpยซยรsqย ย#p*ยยยชยฉ;sยญยYngoAยยGร
ร#ยธยน ยKยบ;ยป~=ยผยกยย5รยยพ;ยฟรยยบ	ยปยยฎรsqrรยยซยรรยฟรยKยบ;ยปยยฎ(ร6ร ร zรร
ร sqrร"oAsqยยnยsQยยp(ยยชยฉย~รยผรยรnยตย	ยgยฒpยซยรsqย ย#p*ยยยชยฉ;sยEยยจยAยยยยqrwpยยYneoqยยGร
ย รยญยปยยยร ยฝ#ยKรรยฟรยยดรรยปยยฎ
ร]ยธยน 
ร rYย"oAsยnยsQยยp(ยยชยฉยยรneย	ยgยฒPpยซยรsqยยย]pยยยยฉ;s7ยรneoqยยGร
ยgยฒPp*rยยฎรneย	ยNsqยgยpNย+s;รรรรรกยรขยรฃ*รกยจยยชยฉยญยgยฒPpรuwvxwยEsqrYย;yรคnยตยKยYngoAยยยยยqrยยnยตยยรฅprย ยธ
รฆรรฃ6รงรจ1รรฃ6รรฉ]รกยรชรPรซรฌรญรฃรซUรจรฉรฃ*รฎรฐรฏรPรญ*รจรฏยดรญรยรฉรขรยตรขยรกยรฃรฉรญรฑรฒรรรรรรกยญรญรยรฉรขรรขยรกยรฃรฉรญรฑรณยจรดยรรฎEรฃ*รรฃ*รชรฒรจรขรรตgรรชยญรจรฉQรฑรรฏรPรญ*รจรฏ
รญรยรฉรขรยตรขยรกยรฃรฉรญรฑรฒPรจ;uwv*xรทรถรฃรฉรฃ*รชรจรฏรฏรฑรทรซร#รฃรข&รฉรรก&รขNรจqรกรรขยรตgรฑ(รรก*รณcรธยรงรฃยรรชรรนรฏรฃรบรฐรงรจรข5รกรงรฃรฉรทรกยร	รนwรฃ5ยถ5ยรปยรฅpยpยรรรฉ(รรชรซรฃ*รช
รกยร6รรนรกรจรรฉยรรกรขยYngoAยยยยqrยneยยยรฅp*rยยย*ยยAยยยฌ#ยpรฒwรฎยรงรรญNรงรยตรขยรรชรฃรขรฃรฉQรกยรฃรซ0รรฉ+รกรงรฃยซรฉรฃรผPรกยรขยรฃรญรกรรยรฉรรณยรธยรงรรขรฝรรฏรกยรฃ*รชNรรฉรถ
รรข7รจรญNรงรรฃ*รรฃรซรรฎยญรณ-รชรณ-รก*รณ(รจยรถยรรรฃรฉUรรรรรก7รขรฃ*รกยญยฎรทรพ7รกรงรรข7รรขรรฎยรง]รฑยรฎKรฃยจรฉร1รฎยรรชNรรรรยรขรฃยซรกรงรฃยจรซรฃรฝYรฉรรกรรยรฉรรตKรรรรรก
รญรยรฉรขรยตรขยรกยรฃรฉรญรฑ0ยณยดngยgยฒยpยยรฅยpย1ยKยรยยsยญยYngoAยย&ย*pยรฟยฎยจรพ
^+_]`acbedbgfYaihj รlngoqp*r sยuwvxยกyรz |g~ยยยยรย	ยNย	sยยยยยยรยp*ยKย ยยชยฉ"|g~ยย	ยNยยญsqrUยยยยยNยqยยยPsยย nยย*ยp
sAยยnยrย6prYยรฟยqยยยQpยยngr]ยยยยยขยยยฃq*ยค*ยค*ยคยยยฅยsrYย	sยซยNยqrยยย ยยsqnerYยรยp*ยยฎยยฏ ยยซยยณEpย*sqรยyรฐneยรรรรรกรญรยรฉรขรรขยรกยรฃรฉ]รก
รฎยรรกรงยรชNรฃรขยรwรฃรญรกรกยร ยฎรฐรจรฉรซรกรงรรขรรชNรซรฃ*รชรรฉรถn iยฎรneย	sรยรneoqยยEย*p*ยยยชยฉy ยธ
h j	bรfd	;fรa
b
d1_#aรdgf
_
รรฃ*รกยญรฌรขรรจรขรขรฌรบรทรฃรทรจuwv*xUyรรรข7รฉรรกรรYรรรรกรรญรยรฉรขรรขยรกยรฃรฉ]รก*รฒ รจรฉรซUรกรงรจqรกรรฎEรฃรทรฎยรยตรขรงยรกยรรบ6รจรฃ6รรกรรขยรรณยซรฆรรฃรทรกรงรฃรฉ
รรนรกรจรรฉ รจยรฉรฃ*รฎ{รรชNรรนรฏรฃรบ+รฒcรขรจรฑ 
y Pรฒรนรฌรก	รฎKรฃ6รฎรจรฉQรกยซรกรงรรข	รรชรรนรฏรฃรบ รกยร0รบยจรฃ*รฃ*รกยซรขยรยรบยจรฃ6รรชรรwรฃ*รชรกรรฃรข*รณ+รธยรงรฃ
รรฌรชNรรรยรขรฃรทรรตKรกรงรรขยญรขยรฃรญรกรรยรฉ รรขรรกยรUรขยรwรฃรญ*รรตgรฑยรกรงรฃรขยรฃยจรรชรรwรฃ*รชรกรรฃรข*รณ6รฆรรฃ รฝรชNรขยรก;รซรฃรฝYรฉรฃรทรกรงรฃยจยYngoAยยรยNยrยneยยยรฅp*rย
ย*ยยAยยยฌ#ยpยรรตรจยจรถยรรรฃรฉยuwv*xรพ
^+_]`acbedbgfYaihj  y  zย|g~ยย  ย  ย  ยรnยตย;ยsqยeยpย"ยgยฒPpยรYรรรรกยรญรยรฉรขรรขรกยรฃรฉQรกยรญ*รฏรยรขรฌรชรฃยยยชยฉยญยgยฒPpรuwv*xยy z
|g~ยยยยรย	ย;n 
y  y ร รยรณ-รฃรณรปย ยน  ยป qยค"ยค !ยด$ #  ร %
ยฏ #cร ยย ยฏi
ย ]ยEsqrYย ยน  รฅร6ยป qยค"ยค !ยด$ &  ร ร %
ยฏ &ร ร+ยq(
ย 'รร รรทยปU
ร ยรร1ยEsrYย
ยทQยธ 
ร#ยธ y  neยยYneoqยยยNยqrยยnยตยยรฅprยgยsqrYย
y  nยตยรรบยจรจAรผรรบยจรจ*รฏ )ยซยgยฒPp*ยยp ยยย1pย(rรยยย,p +ยnยตยยยด.
y -รฟยYngoAยยรยNยqrยยnยตยยรฅprยยยฌยยฒUยgยฒ#sย5
y  .
y -/ y sqrรย
ร]ยธ 
y  zi
ร y -ยธ
0Kรฃ*รตeรรชNรฃยรรชรฃรขรฃรฉQรกรรฉรถรรก รฎEร;รรชรรwรฃ*รชรกรรฃรขรรตwรกรงรฃยรรรรรก5รญรยรฉรขรยตรขยรกยรฃรฉQรกEรญ*รฏรยรขรฌรชรฃ
y ยญรรตwรจยซuรvxรทyรทรฒQรฎKรฃยรรฉ]รกยรชรPรซรฌรญรฃ
รกรงรฃยญรตgรยรฏรฏรรฎยรรฉรถ(รฏรฃรบยจรบ6รจPรฒรฎยรงรรญรงยรฎEรฃ	รฎยรรฏรฏรรฌรขยรฃยญรฏรจqรกยรฃ*รชรรรฉยรยรฉรฃรรรตรกรงรฃยญรรชร]รรต รข*รณ
1K3
_ 242 5รhร7j 698 pยยyยฑz |g~ยยยยรย	ยรยNpUsรuwv*x ยธ 8รฟp*ยยซย ยณ&ยUuwvxwยรทy ยข z |g~ยย ยข ย ยข ย ยข 
ย รคy
sqrYยy ยฃ zร|g~ยย ยฃ ย ยฃ ย ยฃ :
ย รy ยp;ยYngoAยยย	ยNยqrยneยยยรฅprYยยญยณ ยธ ย ยธ ย ยธ sรยยยยยรยp*ยยยย;s"ยYngoAยยย7ย*pยยยฎ z
ร
ย ;=< ?ร >รรปAยฅ @ Cยบ B ยฟรยยบยย ยนw
y G7z
ร DFE ย E รยsrYย+sqrยยยยชยยqยยยPsย nรยยpยจsAยยnยrย6prYยยยยชยQp*ยngr]ย ยธ 8 p*ยยgยฒPp	uรvxยH
ยข I ย ยฃ ย G z ย /
ยข K ย ยฃ ย G z ย J
ยข I ย ยฃ ยNยยยณยดngยgยฒ+ย G z ย J
ยข I ย ยฃ zรรL# รG zM# ร ยข ยN# รยฃ
|g~ยย G z ย J
ยป qยค"ยค !ร+srYย(.
ย Gยz ย ยข I ย ยฃ z Lร &OรG ร P
z & รยข ร 	
ย & ร-ยฃ ร ยน  รฅรยจ4
ยป qยค"ยค ! ย ยธ ย ยธ '=รG ร ยป0.
ย Gร :
y G
ยน 4
ยธ Q ยฒPp*r(ยถ5ยNยย ย รยซH
neยEยรneoqยยKยยqrยยnยตยยรฅprยEยณ ยธ ย ยธ ย ยธ ยgยฒPpยยpยญยsqย6p	ย*pยgยยรsrYยยซยฎรsqrรยยยgยฒQnยตย7ย*sqย6pยยยยยยNยqยยยPsย nรยยpรทsAยยnยrย6p*rย
ยqยยยQpยยngr]ยqยsqrYย ย*pNยNยqrรยPย รยซy ยข ยy ยฃ  H
y GR y ยธ
SUTLV

WYXLZO[]\

^H_U`a`b$cedgfihj$kmlnk,oqprsl,outlnvHwyxk{zx]|}p}ln~Cpยuk$xkmlmfยยlยยยยยfยlnยk:ยqfยqp}lmfNยยย=ยยยยย$ยยยยยยยยยย
p}j,f:ยuj$x]fCยย}ยย
ย

ยยย?ยยยย

ยยยLยก

ยukยฆk,oqprl,outlยฆยจp}jยฅt}ยฉยฉยฒยชยณยฌsยญ ยฏยฐยญยฑยยดย
l,outl.ยบ ยธ ยยปeยผ
ร ยธ ยง ยยดยยยยยรรยดร ยจ
ย7ยฟ ยธ}ร ยฟ3ย ยก

ยผ

ยคยดยฝ

ยป4ร ยธ ยค ย

ย7ยฟ ยธ}ร ยฟ ยยยยยยย ยก

ยยยยยยยย

ยปPร ยธ ยค ยยดยยยยยรร

ย l,oqfยj,fยยจรp}j,f

ร ยธw ย ยก

ยป4ร ยธ ยง ยยย?ยยย ยยฆยบ ยธ ย

ยLยท
xรvHwย
ยค
ยพqp}jรt}ยฉยฉยยชรยฌรยญ ยฏgยญยฑยul,oqfยj,f.fCรqxkml,kยย

ยฟยย

xkยฆzx]|}p}l{~Cpยuk,xkmlmfยยlยยยฒl,oufยj,f	fCรรxkml,k

ย

ยรย

ยยยยยยย

ยยท
ย ยท
xยฃvHwยยยfยlยฆยยดยธnxeยถ L
k,ยu~ยนo
ยค
ยค
ย7ยฟ ยธยร ยฟ ย
ย ย?ยยย ยก ยปยฃร ยธ ยค ยยย?ยCย p}j ย7ยฟ ยธยร ยฟ ยยดยยยยย ยก ยป

ร ยธw ยด
ย ยยยยย ยOfยx]l,oqfยj

ย7ยฟ ยยย?ยยยยร ยฟยย ยก

t}ย

t}uยNย

ยป

ยค

t}kv

ยยย p}jยย
ย

xยฃvยฅยคยฆt}uยgvHยงยยจp}jt}ยฉยฉยซยชeยฌยฎยญ ยฏยฐยญยฑยnยfยl

xkยตl,oqf:zx|}p}lยตp}ยจยถ

ยJย

ย7ยฟ ยธ}ร ยฟ ยยยยยยย ยก

ยงUย(ยพqp}jHt}ยฉยฉ

ย7ยฟ ยธ ร ยฟ ยยย?ยยย ยก

ย

l,oufzx]|}p}l.rYoupยkmfl,tj,ยข}fยlxk.ย

ยปยร

ย l,oqfยj,fยยจรp}j,f

ยค ยยยยยย
ยย

ร wยยยยยยยย ยก ย

ยยร ~Cp ย

tj,f{~Cpยukmfยร3ยqfยยl,ยฉ]ยyย

ยยค

xยร

k,ยu~ยนo4l,outl

ร lHxkรl,oqf{k,t ย

ztl,x]ยยฉ]f ร

ย

fxยจ
ย

ยรย

ยยยยยยย

xk

zx]|}p}lยp}ยจ/ยถ

ยค รmv

ยยfยl(ยukqpLrPk,oqpLrยl,outlv

ร

ยป

รOร

ร ร]ร"ร

ยรยดย(
w ร

ยพยqj,l,oqfยj ย
ยผ
ร

w

ร

ยผ
ร

ยผ

ยคยยฝ

ร

รOร ร*รก

ยปeร ร]ร"ร

ร

รOร ร*รก

ยป9ร ร]ร"ร
ยผ
ร

ยค ร
ย.

ยผ

ยผ
ร

w

ยผ
ร

ยผ

ยคt}uย

k,ยu~ยนo	l,outl(ยบ

ย

t}ยyร
ยผ

ยผ
ร

ยปยฐยผ

ยw ยธ

k,ยu~ยนogl,outlยฆยบ

รยดยw

ยคt}uย

ยยธ

ร ย ยธ}ยurYoux~ยนo	x ย
ร

w

ยยร ยก ย

dgfยqfยยuย~Cfยฆยจรj,p ย

ยรร ยก

t}uย

ยผ

ย ย
ร

ยยรCร ยก
ยรร ยก

ยงUย

ร ยยค ยธ
ร

ยยร ยก

รยดยw ย

ยรร ~Cpยukmfยร3ยqfยAl,ยฉย}ยรยดย.
w ร
ร

ยผ

ยผ
ร

ร ยยง ยธ
ร

w

ยผ
ร

ร ยยค ยธ

kmp

ร ยw ยธ

ยยqp	rรคfnoutย|}f

ร

ยผ

ยคยฝ

ยรร ร ยก

ยงUย

ร ยw ยธ
ร

ร ยยง ยธ

t}uย

ร ย ยธLรฅยยfยlยฆยบ
ร ยw ยธ

w ยarJfยฅl,oqfยj,fยยจรp}j,foutย|}f

ร ยw ยธ

ยรรข ยก

ยง ร
ยR
ร

w ย ร ยw ยธ

zยฉx]fยk

ยง ร
ย.

t}uยร

ยงUยqrJfยตoutย|}f

ยปรฃยผ

vHw.xkรzx]|}p}l(~Cpยuk,xkmlmfยยlยย
ร

ยค

ร

ยค ร
ย.
ร

ยLยท

zx|}p}lp}ยจยยถ

v

w ยก ยยตรจJยi~Cpยukmlmjยนยu~Cl,x]pยยฐp}ยจรครฉ

ร ยยง ยธ

t}ย

ยผ

ยงLยkmp

ร

vHw

ยยง ยk,pnร
ร

p}j,f}ยurJfยตoutย|}fร

ยฉkmpqยk,xu~Cf

ยรงยผ

ยค ร
ยยฒ
ร

ยง.ร

ย

ยรย

ยยย?ยยย

ยยธ

ยปยณยผ

ร ยยค ยธ
ร

ร ยw ยธ ย
ร

ยรรข ยก

ร ยยง ยธ ยยฆรชYpLr
ร

ยปรฃยผ

ยรฆoqfยยฃยบ=ยw ยธ
ร ยยค ยธ

ร ย ยธย
ร

ยรรขUร ยก

ยคยตร

l,outlRv

w

ร ยยธ
ร

vHwt}uยNv

ยง{ร

ร

vHwยยt}ยivHw

ยยรCร ยก ย

vรซยจj,p ย

ยรรUร ยก

ยรรข ร ยก ย

t}uย

รฌ

dgfยฆ~ยt}yqprรญzuj,fยkmfยยlยl,oqfHzuj$p}zOfยj$l,x]fยkJrรคfยตt}qpยยuu~Cfยยxรl,oqf.ย=fยยขยxuuxuยขยฆp}ยจยดl,ouxkยk,fย~Cl,x]pยaย
^H_U`รฎยซรฏ3_Lรฐยนรฑ%รฒยรณรดยฎรตรถรรทยรธmรทยตรนรบuรปยรผ:รท,รฝ}รพรฟรรฟ.รนรบOรทยรฝยรพรปรv

ร
v
	รพรน$รน}รบqรฟยนรพรฟ*รทยรบCรนรธยรพ}รทCรบรรธยรนรน/รฟยรท
Uรฟ$รฟ !รพ ยรบ":รทCรบยรนรธ#AรทCรธ รพรรบ$&%

ยฏยรพ}รทCรบ	รพรนรครฟยรทรบยรพ}รทCรบ
^H_U`a`b$c('รยqzzOpยk,f{l,outlยตv
ร xk

l,outlJv

*

(ยฝ

ร ร ยผ
qp}l

ย

t}ยฉt}uย

ยผ.ร ร,รฉ53

tUรรx ย
ย

tUรqx ย

รฉ



xkHqp}lยuuxร3ยqf ร

l,o3ยukยยยl,oqfยj,f:fCรqxkml,kยตv
ร-ร ,

zx]|}p}lรค~Cpยk,xkmlmfยยlยยqt}uย:v

t}ยฉรยรrYoux~ยนo	~CpยAlmj$t}ยx~Cl,kl,oqfยตzuj,fย|รx]pยยukยt}k,k,ย ย



xkzOfยjยนoutzkJl,oqf

^H_U`รฎยซรฏ3_Lรฐยนรฑ%รฒยรณรฒ
6รรนรรถ.0/1=รฟยv7}รบยฆv

pยk,lYx ย
ย

ย)
ร

ร+*

ร ร ยผ

ร ร,รฉ

vรยยซยยfยlรยukยฒยยuxยฉย{l,ouf.0/1:v

ร ยก ยรรฆoxk.0/1Nxk(zx]|}p}lร~Cpยuk,xkmlmfยยlRt}uยyv

รฆoqfHk,fย~Cpยuย	zuj,p}z=fยj,lรยnp}ยจยดv

ร

ร
v

รร
ร

ร ยก

ย ยf ยย
v

ร

vยฎk,ยu~ยนo
ย)

รร ร

ร+*243
v

รข ย ร ยก ร
t

zul,x]pยaย

xk
รฌ

z=p}j,l,t}ยl ร

รธ,รท{รท+893รพ:}รป]รทยรบยซรพรบ;รรถ<4รรถรรทยรผ{รถ<=}รท2รรถรรทHรฟ>=:รทยตรฟยรทรครน?Rรฟยรนรป@9<A


7รพรรนรบqรฟ>%
^H_U`a`b$cยฒยยfยlยยukยqfยup}lmfB

B

ย v

ยก ยรคยยfยlยukรqpLrยk,oqpr

p}ยจv
ยJI ย ร

I

rYoux~ยนo
ยธ ยก

ย v
ยก

t}uยCB

l,outl5B

xk	qp}ltยณkmpยยฉยql,x]pย

ยปรญร ย ยธ
ย

G
ย

ย v

ย v ยก
ยก

ร

p}ยจยฆv

xk{tยฐkmpยยฉยql,x]pย4p}ยจ(v

k,ยqzzOp}j$l(ยจรp}jHtlHยฉ]fยt}kml.pยqf{~Cpยuk,lmj$t}xยl ร
l,oqfยj,fยยจรp}j,fJoutย|}f

ยJI ย ร ILย ยก

ยปN
, รuย]ย

p}j

l,oqfยx]jj,fยkmz=fย~Cl,x]|}fรคk,fยl,kp}ยจukmpยยฉยql,x]pยukยยEDรยย|รx]pยยuk$ยฉ]ย}ยFB

B

ยJI ยธ}ร L
I ย


ย

ยก ร

ย v ยก ร

k,ยqzuz=pยkmfรl,oqfยj,fRfCรqxkml,kHG ย

รฆYoqfยLK ร ร*รก

t}k{vยฐxk

Kuยชikยย"lยย
, ร ยธ ย
ยก ยปi
ร

ร

G

ยฟ3ย
ย

รฌ

P:QR

ยป

ร ร]ร"ร

ย

tUรqx ย
ยป
ร

ย ย

k,ยu~$oยl,outl

t}ยฉรงยยซl,outl
ยJI ย ร ยฟ3ย ยก

ย

ยJI
ร

ยJI ย ร

ยค

ร รยรยร ร

I

ยธ ยก

fยt}ukยฅl,otl

ยป4
, รย]ย

p}j

I

ยป,

ย ยก

ยJI ย ร

ร ย ยธ ย

I

ยJI ยธ}ร ยฟ3ย ยก

xkqp}lยtYkmpยยฉยql,x]pย.p}ยจqv{ย}l,outlยxkMB

ยON

ย v ยก
ร

kmpยยฉยql,x]pย
รชYpLr

ยธ ยก

out}k{qp

ยก

ยป4
, ร ยธ ย ยdgf
ร

B

ย v ยก ย

ST@UVWYX4VZ[\T][^W`_&Za&b;c:VdeEf"Za&WT]VZghX5i$SH[

j kl"mFnpo`lmFnpq+r"s=qq+r"kt+kuk>v`wyx#q+xzs{"l"w]|<{k}~ยYยยยMยsยyยยkย;q+rkยยยย:ยยย^ย=ยยยyย>ยยย>ยยzยยยย:ยย<ย+ยzmย
ยuยx+{ย^rq+r"s=q5ย ย0ย ยuยย ย wyx5ยs:v`wyยsยยsl"ย(ย ย w]xHยwยยmqย>mยlx+w]x#q#kl$qยยยยmt+kmFยktยq+r"w]xยก}0~ยrsx5q+rk
x+sยukzx#kq4mยยขx#mยย]{"q+wยmยl"xยsx4ยuย`q+rkย"t+mยฃย]kยยคwยqHย>mยยukx4ยt^mยยยยฅH{qย$kยkl(q+r"mย{ยฆยrยงw]qw]x4w]l$q#kt+kx#q+w]lยฆยq#m
o`lmFnยจq+r"s=qzx+{"ย\rยs}0~ยยk>v`w]x+q+xยw]qยฉx#kkยxt+ksx+mยl"s=ยฃยยk2q#mยงnยฉw]x+rq#m(mยฃ"q+swylยwยqยช5q+rs=qw]xยฉq+rk2ย{t+ย0mยx#k
mยEq+rklk>v`qยกx#kย>q+wยmยlMยnยกr"w]ย\rย"t+mย0mยx#kx5slsย]ยฆmt^wยq+r"ยยซsย\r"wยkย`w]lยฆsยw]ยmqยกย>mยl"x+w]x+q#kl&qHยฌย]q#kt^w]lยฆย
ยญยยฎ]ยญยฐยฏยฒยฑยกยณOยดยตFยถ<ยท:ยณOยธยบยนยปยฏ(ยดยนยผยท=ยณยตยพยฝยบยฟ
รEwยt^x#qยslmq+s=q+wยmยlMยชq+rk2x#kqยกmยEq+rkยย{l"ย>q+wยmยl"sยMย>mยlx#q#t^sw]l$q+xรHร2รรร4รย\rmยx#klยq#mยฃ0kq+rkยยwยยmq+xmย
ksย^rรรwyxยบยsย]ย]kย2sยยยยยขย?รรยยย=ยยยบย\ร=ยร=ยJรยรยยย>ย\ย$slย2ย"klmq#kยuรยรยEรยฉยq#kt-q+rk4ยฌย]q#kt^w]lยฆย=รยรLรรรuยq+rk
ยwยยmqx#kqยยกj k2nยกw]ยyยยx+{ย"ย0mยx#kCwylq+rk2ย>mย{t^x#kยmยq+r"w]xยฉx#kย>q+wยmยlยq+r"s=qยฉยฃ0mq+rรq+rkยsx+x^wยยฆยl"ยukl$qยกmt^ย"kt^w]lยฆ
sl"ยร2รps=t+ko`lmFnยฉlยรnkCk>v<ยยysw]lrmFnLq+rkรs=t+kmยฃ"q+sw]l"kยw]ls=ย"ย0kl"ย"w!vรรCยรร<ย
ร r"kยงsยยยฆmt^w]q+r"ยรw]xยย>mยยuย0mยx#kยยปmยx+kยkt^sย4ย"t+m`ย>kย"{t+kxย ร r"kwยtCย"w!ร0kt+kl$qCยยkยkย]xยt+kย"t^kx#kl&qยq+rk
q+rt+kk4x#q#kยxยบยmยยyยยmFn4kยq#mzยk>ยฌlkยยwยยmqยบย>mยlx+w]x#q#kl"ย>รยรj k4ย"t+kx#kl$qยขq+rkx+kย"t^m<ย>kย"{"t+kxMw]lยslยsx+ย>klย"w]lยฆ
nHsรยช
ร ยยยYs=okCq+r"kCย>mยl"x#q#t^swyl&q+xยกรร\รCslยรร0รรรรร4รCรรร4รยร5รรยย>mยยuยs=q+w]ยฃยยkย
ร<ย4รHmยยuย{q#ksuยwยยmq5ร4ร2รยฒร4รยยmtยกsย]ย0ร5รยwyl(รรขรกรครฃ
รฅ ยยรย^rwยkยkCยw]ยmqย>mยl"x+w]x#q#klย>รยงยmtq+rk2}0~ยยย
รฆ t^m<ย>kย"{"t+kยรง<รจยพรฉรช"รซ&รฌรญFรฎรฏ&รฐรฑรยร`รฒ#ร5ร$รฒ#ร ร:รณ ยs=okxzย>mยl"x#q#t\sw]l&q+xยฉร5รรรดรHรusl"ยยรร\รรHรรยย>mยยuยs=q+w]ยฃยยkยช5wยq

t+kยumยพยkxCยt+mยยรถรตร^รq+rmยx#kq+{"ยยยkxznยกr"wyย^rรคยml"mqCr"sยkรsย>mยยยumยlรทx+{ย"ย0mt+qw]lรทรธEรยmtq+rkx#kยงqยn4m
ย>mยl"x#q#t\sw]l&q+xยรนรบยElkย>kx+x^s=t+รยwยqย>t+ks=q#kxยฉq+rkย>mยl"x+q#t^sw]l$qยฉร0ร^รzยฃ0kqยn4kklq+r"kย:s=t\w]s=ยฃยยkxHรHรusl"ยร ร ย
รป=รผยรฝยรพFรฟรผรฟรฝ	
:รฟ
&!รฟ "
#รฝF$
รผ 
%'&  &  (*) + รฝ

&-,/.!'&021
#3&  &  (5
4 )  รฝยพรผ6&  &  (5
4 )  7&รฟ8"%98รผยรฟ	::;&  &  <#:รผยรฝ=) +
รฟ!"0>#=รฝยพรผ

รฟ!"0
รฆt^m<ย>kย"{"t+@
k ?
รญ A`รจ&รฌzรฑร4ร`รฒ#ร5ร รณ ยs=okxsย]ยย>mยl"x#q#t\sw]l&q+xรร\รย>mยl$q+sw]l"w]lยฆรHรsl"ย(x+{"ย\rรq+r"s=q5ร รCB รรDFE
รHรรยย>mยยuยs=q+wยยฃยยk5nยกw]q+rร4รCร ร4รยฉยฃ$รx+{"ยย>kx^x+wยยkยกยsย]ยyx-q#mq+rkยกx^{ยฃ"t+mย{q+wylkHรง<รจ:รฉ`รชรซยรฌรญยพรฎรฏ$รฐยฉรฑรร`รฒ#รHร&รฒ#ร ร รณ ย
รยฉยq#ktwยq+x5ย>mยยuย{q+s=q+w]mยlยย"รHรCรยฒร5รยw]xHq+r"kt+kยmt^kCsuยwยยmq5mยEรร!DFEย
รป=รผยรฝยรพFรฟรผรฟรรปGรฝHII!
&!รฟ "
#รฝF$
รผ 
JK-(<LMNPO:RQSTQKUVW(5XYรฝ;รฝ
!Z:รฟ['	\K
รฟ!"0
ร rkย}0~ยw]x5ยwยยmqย>mยlx+w]x#q#kl$qยกw!ร;sl&ร(ย=s=t^w]s=ยฃยยkzรHรCmยEร7รก

รฃ7wyxHq+rkq+s=t+ยฆkqยฉmยรsuยwยยmq5mย-รรDFE>ย

]!^`_`aWbdce8fgdeihkjlcemjonqprtsuwvKectvSxyy0cwzShKjle*{P|n}cwz~Zr+|8mยยgย!gdecยhkgยwr+|ยยpriยRยยย8m0uweย	mcยn`bcxzkvSgยยยprยSยwย[ยย
ย ย r+|
uweยยยยpwยยSยwย\ยย
ย ย ย |8^
ยย8ย

ยยยยยยยย

ยยย	ย!ย%ย!ย!ยกยข0ยยขiย	ยฃ!ยก0ยJยคMย!ยฅ!ย	ยฃ!ยฆยยก0ยง
ยจยฃ!ยงยยก
ยค0ย!ยฆ$ยฉยยชยฌยซ3ย	ยยฎยญยฏ;ยฐยยฑย
ยฒ\ยณ ยช ยด!ยฆยยงยยกยตยยค;ยยถยฃยธยทVยย0ยย%ย!ยน!ยก	ยฑZยiยฑ0ยนย	ยฃยยบยถยยข!ยฃJย	ยน!ยฆยง	ยฃย=ยยข ยฒยป
ยMยย	ยยยฝยผ ยฒ ยณยพ ยฒ ยป8ยฟ
ยฃ!ยก0ยฑ>ยคยยฆ
ยฃ!ยก0ยฑ
รCรรqรร`ร!ร'รรรร`ร
รKรรรรรรoรร0รwรรรรรรรร	รรรkรร0รรยฝร`รรรร0รยฝรยรรPรwร'รร2รรยรiรรรรรรรรรรรwรรรรรร'รZรCร`รรรรกiรยรiรรขร'รยรรรVรHรยร'รร0ร
รยร0รVรwร'รร2รiรqร-รiรยรรwรร+ร5รdรฃtรคYรฅ[รฆรงรiรฆรkรฆCรรรร5รรMรรiรร;รจ	ร	รkรรฉรฆ

	

)*$+
*

  "!$#&%
* -,

รCรรFรqรชยรซ<รฌยฎรญรฏรฎiรฐยรญ8รฑรฒยรฐรงรณรดรรญiรญรถรต+รทqรฑรตรรฑรธ'รตรรญiรฒ*รฎรน	รบ<รปqรผVรต+รฝรพVรฟ
รฏรฝรงรดรรฑ รปqรฝ 0รน0รตCรน0รธ
รน0รฒ+รญiรน 0รญiรฒ -รน	รพqรฐยรณ>รต+รทVรญยรพPรญ8รฎรญ8รด+รด+รฑรฒ+รณ%รด+รผVรปPรปqรฒ+รญ8รด+รด+รฝยรน	รพqรดยรฑรฒ+รญ รปยรญiรฒยรธ'รน0รฒยรบรถรญ
รฑ0รฎiรฎรน0รฒ PรฝรงรพVรฟยฎรตรรน;รต+รทPรญ Vรญ `รพPรฝยรต+รฝรงรน	รพยตรน0รธ
Pรฑรถรป`รฑ0รฝยรฒ@รฝรงรด@รด+รผVรปPรปqรฒ+รญ8รด+รดรรญ รธรรฒ+รน	รบYรฑ รฒยรญ8รฐรงรฑรต+รฝยรน	รพยรน	รพqรฐยรณ รฝยรธรฝยรต@รทPรฑ0รดWรพVรน รด+รผVรปPรปRรน0รฒ+รตtรฝรงรพ รต+รทVรญรรต+รฑรฒยรฟ0รญiรตWรน0รธ
รต+รทVรญรรฎiรผPรฒ+รฒ+รญ8รพZรต-รปqรฝ 0รน0รตtรฎiรฑ0รพ Pรฝ Pรฑรตรรญ
รฌยฎรญ รฎรน	รพqรดรรญ ZรผPรญ8รพรต+รฐยรณยธรด+รฝรงรบ<รป`รฐยรณยรทPรฑ 0รญ รตรรนยรด+รทVรน
รต+รทqรฑรตCรฑ0รพรณJรด+รผVรปPรปqรฒ+รญ8รด+รด+รฝยรน	รพ PรผVรญ5รตรรน
@รทPรฑ0รด
รพVรน รฝรพ `รผVรญ8รพPรฎรญ รน	รพรต+รทVรญCรป`รฝ 0รน0รต+รด
รธรรน0รฒ
รน0รฒ+รญยรปPรฒ+รญ8รฎiรฝรงรด+รญ8รฐยรณ qรฝยรธ
Iรฝรงรดtรฑ รปqรฝ 0รน0รตtรน0รธ
รญiรธ'รน0รฒยรญCรฎรน	รบ<รปqรผVรต+รฝรงรพPรฟ
Pรต+รทPรญ8รพยรฝยรต ยรฝรงรฐรฐ`รฒยรญ8รบรถรฑ0รฝรงรพ รดรรน
Vรน0รฒยฝรต+รทVรญtรด+รฑ 0รญ@รน0รธ รฒ+รญ รฝยรต2รณ รญ @รฝรงรฐรงรฐ Vรญ8รพVรน0รตรรญยรฝรพ5รต+รทVรญยรฎรน	รผVรฒยรด+รญtรน0รธRรต+รทPรฝรงรด\รปPรฒ+รนMรน0รธ
รต+รทVรญยรฎรน	รบ<รปqรผPรต+รฑรต+รฝยรน	รพ
รน0รธยรต+รทVรญยรปPรฒยรนMรฎรญ PรผPรฒ+รญ
Kรญiรต
รญยรด+รผPรฎยรท รต+รทPรฑรต
ยรฝรดรถรฑยฎรปqรฝ 0รน0รต รน0รธ
รญiรธรรน0รฒ+รญ
รผVรปqรปยรน	รด+รญ
Hรฝรงรด5รพVรนJรฐยรน	รพVรฟ0รญiรฒรถรฑยธรปqรฝ 0รน0รต*รน0รธ
5รฑรธ'รตรรญiรฒ
ยฝรน	รพqรดรรญ ZรผPรญ8รพรต+รฐยรณ [รต+รทVรญiรฒ+รญ
รญ Vรฝรงรดรรต+รด
รด+รผPรฎยรทยธรต+รทPรฑรต
Tรฑ0รพ
\รฑรฒ+รญ<รพVรน0รต
2รฎรน	รบ<รปqรฑรต+รฝ รฐยรญ
*รด oรต
รน0รฒ
ยรทPรฑ0รด รต+รทMรผPรด รด+รผPรปPรปPรฒ+รญ8รด+รด+รญ
Wรธรรฒ+รน	รบ
\รน0รฒ
@รธรรฒ+รน	รบ
ยรทPรฝรงรดยรฝรงรด
รฝรงรบ<รปRรน	รด+รดยรฝ รฐยรญ 0รด+รฝรงรพqรฎรญ รณ Vรญ ยรพPรฝยรต+รฝยรน	รพ5รปPรฒยรนMรฎรญ PรผPรฒ+รญ
\รน	รพPรฐยรณ5รบ<รน Pรฝ qรญ8รด[รฎรน	รพPรดรรตรรฒยรฑ0รฝรงรพZรต+รด
ยรฝยรต+รท
\รด+รน%ร*รฃiรรรรรรรรร
Mรน
PรนZรญ8รดยรพVรน0รต*รฝรพ `รผVรญ8รพPรฎรญ<รป`รฝ 0รน0รต+รด
[รธรรน0รฒ
tรทqรฝรงรด-รฑ0รฐยรฟ0รน0รฒยรฝยรต+รทqรบยฌรต+รทVรญiรฒ+รญiรธรรน0รฒ+รญรรฎรน	รบ<รปqรผPรตรรญ8รด-รต+รทVรญCรปqรฝ 0รน0รตtรฎรน	รพPรด+รฝรงรดรรตรรญ8รพZรตยรฎiรฐยรน	รดยรผVรฒ+รญ รน0รธ\รฑ

'

( )
)*
. "/0213 (4 536

2* 7* 8%
)9
)
(:
;*  <	
=8
>
 ?@ABC EDGFIHJ%='
8 G@ABC
 KJC!$#
L
M 3A

=:
%
N
O L & 8P: ?: *
RQSJ
TH2
)* 	
U=8%WV XD L
X @  C
 AJC!$# L
QSR
TH2%ZY
[@ C
 \JC!$# XQ]SJ
TH2%_^ )9
8
L 8+Ei$
kj e ljP@<\mon e @ &% &%[
kj e ljPCkqmo
-` =abcD
]deC *fd @ C
EChg
p nUeC
r
kjP@8ljPCksmt
p n @ CT%[QSR
TH2
)*Z
kj e ljUCu vneC r
kj3@3ljPCu wn @ CT%[x
L 8
L s* -,
)* =	
=
* h,
dR)yzP:
{ |}b~HJ
{ |MbDย
THยbยDย%fY ยQSR
TH2ย*
>
 ย@ย C
DยFยHJ%ยx
ย
rยRย&ย$%
ย
ย รน0รต+รฝรงรฎรญรรต+รทqรฑรตยรผPรพPรฐรงรฝย0O รญยรฑรฒwรฎ รน0รฒtรปqรฑรต+รทรฎรน	รพPรด+รฝรงรดรรตรรญ8รพqรฎiรฝยรญ8รด=ย: รทPรฝรฎยรท รน	รพPรฐยรณ รพVรญiรญ)*OรพVรน":ยรฐยรญ)*Vรฟ0รญรรฑ L รน	รผVรต-รต+รทPรญMยRย&ย$
@
รต+รทVรญรปqรฝ0
 รน0รต รฎรน	รพPรดยรฝรงรดรรตรรญ8รพZรตรฏรฎiรฐยรน	รด+รผPรฒ+รญ รฝรงรดรถรฝรพ รฑ8*2P* รฝยรต+รฝรงรน	รพtV* รญ-`, รพVรญ)*t:]o% รฒ)o% รต&% รฑ;รฒ+รนMรน0รตรฏรด+รญiรต&t รฑ;รปqรฝ0 รน0รตรฏรดรรญiรตรฏรฑ0รพ23
* รฑ0รพ
รฑ0รด+รด+รฝรงรฟ	รพPรบ<รญ8รพZรต รน0รฒV
* รญiรฒยรฝรงรพVรฟ+Z\N รฝยรฟ	รผVรฒ+รญoยต
ย รด+รทVรน"ย: รดยรทVรน"ร
: รต+รทVรญ;รฎยรทPรน	รฝรงรฎรญยฎรน0รธรรต+รทVรญยฎรป`รฝ0 รน0รต(ร
 รด รบรถรฑ!รณ6รฝรงรพ`> รผVรญ8รพPรฎรญยธรต+รทVรญ
,`รฐยรตรรญiรฒยรญ)* รปPรฒ+รน L รฐยรญ8รบ%
ย รยยย
ย$ยยยย รIรยรย รย	Vย รรรรยร ร!ร>ร"ยย;ยEยยยยต
dย รพยฎรต+รทPรฝรงรดCรด+รญ8รฎรต+รฝยรน	รพ$ย
: รญ รฝรฐรงรฐรงรผPรดรรตรรฒwรฑรตรรญ*รต+รทPรญs`, รฐยรตรรญiรฒยรฝรพVรฟ รฑ0รฐยรฟ0รน0รฒยรฝยรต+รทPรบ รผPรด+รฝรงรพVรฟ รต+รทPรญรถรตรรฒยรฑ)0 รญ8รฐยฝรฑรฟ0รญ8รพPรฎรณยฎรญ-V` รฑ0รบ<รปqรฐยรญ8รฏ% รฌยฎรญ
,qรฒยรด+รต-รฒ+รญ8รฎiรฑ0รฐรงรฐFรต+รทVรญยรฝรงรพPรฝยรต+รฝรงรฑ0รฐ`รปPรฒ+รน L รฐยรญ8รบ รฑ0รพ2* รญ-` รปRรน	รดรรญ รต+รทVรญยq* รฑรต+รฑรถรพVรญ8รฎรญ8รด+รด+รฑรฒยรณ รตรรนรฏรต+รทVรญรรฑ0รฐยรฟ0รน0รฒwรฝยรต+รทPรบvร
 รฒ+รนMรน0รต@รดรรญiรตยยยก
รดรรญiรตtรน0รธIรปqรฝ0
 รน0รต@รฎiรฑ0รพ2P* รฝยP* รฑรตรรญ8รดยQqยข รฑ0รพ2*ยฃยX2g รฎรน	รบ<รปqรฑรต+รฝ L รฐยรญ รน0รฒยคV* รญiรฒยรฝรพVรฟP%
ย~ยฅยงยฆ2ยจ\ยฉยชยฌยซยญยฎ3ยฏKยฐ2ยช<ยฑPยชยฒยญยฎยดยณ
Qqยขยตยฅยตยฆ2ยฐ2ยช"ยฑUยชzยญยฎX ยฐGยถEยฉยทMยฑPยธยยชzยญยฎU
ยฐGยถ\ยฉยทqยฑUยธKยชยฒยญยฎs ยฐยฉยธKยธยยญ$ยท]ยฐยชยฒยญยฎP
ยฐGยถ\ยฉยทqยฑUยธKยชยฒยญยฎsยบยน ยปKยท]ยจ\ยฉ2ยป\ยจยดยญยฎยดยณ
ยผ รพย[2g รฎรน	รบ<รป`รฑรต+รฝ L รฐยรญ รน0รฒยคP* รญiรฒยรฝรงรพVรฟ<รฝรงรด&+
ยจEยฉยชzยซยญยยฎ3ยฏKยฐ2ยช"ยฑUยชzยญยยฎ3ยฏยยฐGยถEยฉยทMยฑPยธยยชzยญยฎPยฏKยฐยฉยธKยธยยญ$ยท]ยฐ2ยชzยญยฎPยฏยน ยปKยท]ยจ\ยฉ2ยป\ยจยดยญยฎ
dย รพ รน0รฒยคV
* รญiรฒFรตรรนยรฑ0รฎwรทPรฝยรญ&0 รญรปqรฝ0 รน0รตรฎรน	รพPรด+รฝรดรรตรรญ8รพPรฎรณ8<: รญ[รพPรญiรญ)* รตรรนยรฎรน	รบ<รปqรผPรตรรญย3E
ยฝยฐGยถEยฉยทMยฑUยธKยชzยญยฎPยฏยพยนยปยยทยจEยฉยป\ยจยดยญยฎย
<	A
ยฝยฐGยถ\ยฉยทqยฑUยธKยชยฒยญยฎ3ยฏยยฐยฉยธยยธยยญ$ยทยฐ2ยชยฒยญยฎ$ยฝ รฑ0รพ2*3E
ยฝยฐ2ยช<ยฑPยชยฒยญยฎ3ยฏยยฐGยถEยฉยทMยฑUยธKยชzยญยฎย%
ยฟ"ร(ร

รรยพรรรfรรรรรยรยครร3รร3รรร<รรรGร2ร ร3รรยร รรรXร=รPรร

รรรZร-รยร&รzรGร)รยฒรGร&ร
รGร	รกรขยฒร	รฃzรกรกรคPรฅ<รฆzรฆzรงรฉรจ3รชรฉรกรค	รงยพรซ(รฅ(รขยฒรฆยรฌ)รฃยฒรก=รKรยรญ~รยรฎ(รฏ3รGรยรญ~รยรฎ?รฌ)รฐPรฑMรGรยรญ~รยรฎ"รฒ
รGร	รก?รฌ"รฆzรฆzรงรฉรณ"รฐ	รดAรกยครฐ<รขGรฅ(รฃรตรฑ8รกยครฃยฒรงรฉรฐ3รณ\รถยรกร3รฌ-รซ(รก=รทรตร	รฅ<รฆยฌรกรฐยรงhรฆKร ร รzร ร รzร ร รยฒร รฎ รฒ
รฝkรพ}รถรก รทรตร	รฅรฅ<รฆยฌรกfรขzร3รก รค	รงรฉรซ"รฅ)รขยกรGรcรญ รยรฎรรขzร	รกรรฟ3รชรฉรขzรกรฃ
	
รธยฝรบ
รธยฝรป
รงยพรฐ	รณ รทยครฃzรกยฝรฌ&รขยฒรกรฆXรขzร	รก รทlรฅ"รฐ3รฆยฌรขzรฃรตรฌ)รงยพรฐ"รข รยฒร รฏAรฌ)รฐ3รฑยรฑ8รฅรกรฆยรฐ	รฅ(รข
รทรตร3รฌ(รฐ	รณ(รกยรฌ)รฐ <รขzร	รงยพรฐ	รณรกยครชhรฆยฌรก)รถยรกรฅ"รจ8รขยฒรฌ(รงรฉรฐยรขzร	รกGรค	รฃยฒรฅ(รจ	รชยพรกยครดยร




รธยฝรผ

&รธ รน
รฝkรพ	รถยรกรทรตร	รฅรฅ"รฆzรก$รขยฒร	รกยดรค	รงยพรซ(รฅ(รขJร ร รญ~ร รฎ รขzร	รกยดรฟ3รช รขยฒรกยครฃยฒรงรฉรฐ3รณรฑ8รฅรกรฆ
รฐ	รฅ(รข$รฆ  รค	รค3รฃzรกยฝรฆzรฆ$รฌ(รฐ<รขยฒร	รงรฉรฐ3รณ)รขยฒร	รกยรฃยฒรกรฆ  รช รขยฒรงรฉรฐ3รณรค	รฃยฒรฅ(รจ3รชรฉรก
รด Kร
รงhรฆยรขzร	รกรฃzรกยครพยรฅ(รฃยฒรกรขยฒร	รก?รฆยฒรฌ)รดAรก=รฌ(รฆยรขzร3รก=รงรฉรฐ	รงรฉรขzรงhรฌ)รชรฅ(รฐ	รก ]รฒ





!#"%$'&)()+*-,-.0/1,2!3#+3#.4"2!/156&782:96;"2!/1"<,-=.#>;"@?A+*-,-.

BDCEAE!F:GHBI8F:J
K!LNM
O
w
PQRP

G UVC T U'FWJ
ST H
PQ8LNXY[Z
F X]\#^ _`MRZ
Jba]c X_`M8Z

B!dVCGHeEAI8F:J
xybz{z|1}x~`|1ยย#ย}ย'y#ยย|1

PQ c XYfL

UWghCJ T

Jba]c _`X

x{ย'yb}Wยbz{~u|1
x]~sยb~u|1ยย'y#~uย|1

BI4eI8F:J
K c Q[_iM
Sj4Xkbj4X
l c MRZ]_`X]\4mj4X
G L
n'oqpj#Qsr
t c k4Q[_uk

UVCIsv6F:J
T
g
B

;#"ยยW&ยย,2!34.4bย-+2'.ยย-2!ยย/1,2!3b+34.4"2!/15-bย?A>A3ย,-=ย.#>!"ย+2!.#8ย-ยAยยย
ย

ยยย

ย0ยยDยย

ยDยยAย!ย:ยHยยก8ย:ยข
ยฃ!ยคNยฅ
ยฆ
ร
ยงยจRยง

ย ยซVย ยช ยซ'ยWยข
ยฉยช H
ยงยจ8ยคNยฌยญ[ยฎ
ย ยฌ]ยฏ#ยฐ ยฑ`ยฅRยฎ
ยขbยฒ]ยณ ยฌยฑ`ยฅ8ยฎ

ย!ยดVยยHยตยAยก8ย:ยข
รรbร{รร1รรร`ร1รยร#รรร'ร#รรร1ร

ยงยจ ยณ ยฌยญfยค

ยซWยถhยยข ยช

ยขbยฒ]ยณ ยฑ`ยฌ

ร{ร'รbรWรbร{รuร1ร
ร]รsรbรuร1รยร'ร#รuรร1ร

ยยก4ยตยก8ย:ยข
ยฃ ยณ ยจ[ยฑiยฅ
ยฉยท4ยฌยธbยท4ยฌ
ยน ยณ ยฅRยฎ]ยฑ`ยฌ]ยฏ4ยบยท4ยฌ
ย ยค
ยป'ยผqยฝยท#ยจsยพ
ยฟ ยณ ยธ4ยจ[ยฑuยธ

ยซVยยกsร6ย:ยข
ยช
ยถ
ย

รรรร;ร#รยรWรรร0รsร4รรยร6รร;รรรกรรขรฃรฅรค;รฆ%รงWรจ7รฉqรชVรซรฌuรญ6รฎ7รฆ@รฏรฐรค!รฎรฅรฏรฑรชยรซรณรฒ

 6ร;ร#รปร#รรทรถ!รฟร

รดรตร%รถ!รท{รธรบรน;รร#รป-ร8รผรฝร#รพ;รรฟ4ร 1รท

 รทA ร;ร#รปร#รรทรถ รท-รVร6รร;รรรฐรรขรฃรฐรค:รฆ%รงWรจ7รฉqรชVรซรฌuรญ6รฎ7รฆ รฏรฅรค!รฎรฅรฏ รชVรซรณรฒ<รfรรรร;รbรยรรฒ]ร
รดรตร	รฝรร#รsรท-ร
'รรAร รรขรฃรฐรค:รฆ รง'รจ)รฉNรชVรซ'รฌuรญAรฎ7รฆ รฏรฐรค;รฎรฅรฏ รชVรซรฌ รขรฃรฐรค:รฆ รงWรจ7รฉNรชVรซรฑรฒยรธรฅรพ!รbรพ รน;รทWรรฟรกรถ;รท-ร
 รทWรน!ร+ร รป-รถWร#รพ!ร+รถ;ร)รป-รถAรน'รรAร รรขรฃรฐรค:รฆ%รงWรจ7รฉqรชVรซรฌuรญ6รฎ7รฆ รฏรฅรค!รฎรฅรฏ รชVรซรฌ รฏรฐรค:รฉ!รชยรซรณรฒ"รธ0รพ!ร]รพ#1ร#ร%$
รปร4รรฟรฅร#รพ;ร&1รทรถ!รฟ#ร4รbรป-ร+รถ'ร('
' รฏรฐรค:รฉ!รชยรซรฌHรขรฃรฅรค;รฆ%รงWรจ7รฉqรชVรซ*)+
 รทA ร;ร#รปร#รรทรถ รท-รVร6รร;รรรฐรรขรฃรฐรค:รฆ%รงWรจ7รฉqรชVรซรฌ รขรค;รจ7รจ)รชรณรฆ รข;รฉqรชVรซรณรฒ]ร
รดรตร,รฝ
 รร#รRรท-ร"-W ร ร6รย รรขรฃรฐรค:รฆ%รงWรจ7รฉqรชVรซรฌ รขรค;รจ7รจ)รชรณรฆ รข;รฉqรชVรซรฌ รขรฃรฐรค:รฆ รงWรจ7รฉNรชVรซรฒ รธ0รพ!ร]รพ รน;รทWรรฟ รถ;รท-ร
 รทWรน!ร+ร รป-รถW ร#รพ!ร+รถ;ร+
 รทA ร;ร#รปร#รรทรถ รท-รVร6รร;รรรฐรรข!รฉรงWรฉNรชยรซรฌ รขรฃรฐรค:รฆ%รงWรจ7รฉqรชVรซรณรฒยรfรรรร;ร#ร/.10 รฒ]ร
รดรตร2รฝ
 รร#รsรท-ร
3W ร ร6ร4 รรข;รฉรงWรฉNรชVรซ'รฌ รขยรฃรฐรค;รฆ%รงWรจ7รฉNรชยรซรฌ รข!รฉรงWรฉNรชยรซรณรฒ%รธ0รพ!ร5bรพ รน;รทWรรฟ รถ;รท-ร6]รพ!รป-รถ;ร-รรป-รถ$
ร#รพAร+รถ;ร'
 รป-รถAรน2W ร ร6ร4ย รรข!รฉรงWรฉNรชVรซ'รฌ รขรฃรฅรค;รฆ%รงWรจ7รฉqรชVรซรฌ รฏรฐรค:รฉ!รชยรซ-รฒ": รธ0รพ!ร]รพ7 รท:รน!ร968 รรฟHร#รพ;ร:1รทรถ!รฟ4ร4รbรป-ร+รถ'ร
' รฏรฐรค:รฉ!รชVรซ'รฌ7รข!รฉรงWรฉNรชVรซ;)+
<>=5?A@BCEDFHG6DEI*JBJKF4LMI;NPORQMST@VUFWXGYDI;J4BZJ1F4LI*NPO
รดรตร[A
 ร#ร1W\ ร+รทร!รฟ#รผ]ย รฟbรป-ร+รน ร#รพ!รปร)รปร"^$
รป-รถ!รน$_A รปร#รพ`1รทรถ!รฟ#ร+รฟ4ร4รรถ%%รธยรป-รฟ)รถ;รท-รรรถ;รK1รรฟ#รฟ#รปรa ร4รท รร\-รรฅรฟ4รท ร[!ร#รทPรฝรร#ร#รรรฟ
ร4รท รป รsรAรถ1ร#รรทรถ!รป-รผcbed1f>;
 รป-รถ!รน ร#รพ!รปรg6 ร]-\ รท-รg1รทรถ!รฟ#ร+รฟ4ร4รรถ% รธยรป-รฟ รฟ#รh`รรรถ'ร1H+ รดรตร(8Aร]รฟ4รรฟbรพ;รท{รธ ร+รถ ร#รพ!ร8รฟรฟ#รK1ร#รรทรถ
ร#รพ!รปร/6
 ร]-\ รท-ร71รทรถ!รฟ#ร8รฟ4ร4รรถ%ร+รฟ รปรตรธรรปj-i รรถ;รรน รsรท-ร
 รท-ร:A รปร#รพk1รทรถ!รฟ#ร+รฟ4ร4รรถ%Pย รป-รถAรน ร+รถรฟ4รK1ร#รรทรถkl รธร รธ0ร+รผ+รผ
!ร#รรฟ#รรถรยรฟ4รท ร&! ร#รทPรฝ รร#ร#รรรฟ รป-รถAรน รป รร#รพ;รท:รนยรsรท-ร0รฟ4รทรผ]:\ ร+รถ;ร รfร!รถ1ร#รรทรถAรป-รผ>bedK!f รฟnAm รป-รฟ4รรน รรฝ รทรถ ร#รพ!รKH+
o รท-ร4ร ร#รพ!รปร1 รฟ#ร+รถ1รpA ร-\ รท-รq1รทรถ!รฟbร+รฟ4ร4รรถ% ร+รฟVรนAรร#รK1ร4รรน>- รทรถ;รn1รทร!รผ+รน รธ0ร8รฟ#รพ ร4รท(1รทA รปรbรยร+รยรถ;รท-รรทรถ!รผ@ รธ0รร#รพ
รฅ
r รfร!รผ+รผs(6 รปร#รพ21รทรถAรฟ#ร+รฟ4ร4รรถ%PMAm ร;รรป-รผ+รฟ4รท รธ0รร#รพutjvwaxay4z{v{|P}~jMA รปร#รพ`1รทรถAรฟ#ร+รฟ4ร4รรถ%PH+ รดรตร<ร#รพ;รรbรรRรท-ร#รย! ร#รทPรฝ รทรฟ4รรฅร4รท
!ร#รรฟ#รรถรรฅร#รรผ+รปร#รรทรถ!รฟbรพ!ร]A รฟยรฝm รร
รธรรรถuA ร]-\ รท-ร[1รทรถ!รฟbร+รฟ4ร4รรถ% รป-รถ!รนยรฝm รท-ร#รพยA รปร#รพย1รทรถ!รฟ#ร+รฟ4ร4รรถ% รป-รถ!รน รน!ร+ร#รK1ร#รรทรถ!รป-รผ
Aรปร#รพY1รทรถ!รฟ#ร+รฟ#ร4รรถ%P+ รดรตรรรธ0ร8รผ+รผ-ร#รพWร!รฟVรฟ#รรรร#รพ!รปรยA ร-\ รท-ร;1รทรถ!รฟ#ร8รฟ4ร4รรถ% ร+รฟยรป0รbรรฟ4ร4รbร1ร4รรน6-\ รรbรฟ#รรทรถ%รท-รAรน!ร+ร#รK1ร#รรทรถ!รป-รผ
Aรปร#รพย1รทรถAรฟ#ร+รฟ4ร4รรถ%รป-รฟยรธรรผ+รผย+
ยยย

ยpยยยยยuยยยยย"ยย
ยยยยยยยยยยยยยยยยยยยยย7ยgยยnย
ยEยjยยยกยขยฃยยค5ยกยฅ
ยฆยง!ยจ
ยฉ
ร
ยชยซยช

ยฌjยญ ยฃยข ยฎ;ย ยญ ยฎยกMยฅ
ยชยซ5ยง!ยฏKยฐยยฑ
ยก ยฏ"ยฒaยณ ยดยตยจยฑ
ยฅ
ยถ"ยท ยฏ1ยดยตยจ5ยฑ
ยยธ;ยjยขยฃยนยยค5ยกยฅ

รjร
ร4รร%รร^รยตร%รnรaรjรรรaรรKร%ร

ยชยซ ยท ยฏKยฐ{ยง ยฎMยบยปยjยฅ ยญ ยฅ
ยถ"ยท ยดยตยฏ

ร4รร
รMร
ร4รยร%ร
ร"รZร
รยร%รรรรaรยรjร%ร

ยยคยผยนยค5ยกยฅ
ยญ ยฎ;ยยคZรยกยฅ
ยฆยฌยพยผยทยฏKยซยยฟ
ยดยฝยพยผยจ ยฏ
ยบ
ร ยข ยท ยจยฑ"ยดยตยฏ"ยฒยผร]ยพยผยฏ
ย
ยงรรรรรKร ยพaยซZยท ร ยฟยผยซยยดยยฟ
รqรรรรaร/ร1รรกรฃรขVรครฅยผร1รVรฆรงjรจรฉรช:รซ^รฌรญรฎMรญ!รฏ;รฐรฑ_รฌqรฒยรณรด6รฎMรตรถรญ!รฏqรฐ>รท"รกรKรธรนHรบPรค;รฅaรปร(รผรฝ]รฅยผร1ร
ร5รธร
รยร]ร
รพยผรฅ1รฟรฝร1รฅ รรพgรaร  รฝรฝ  รaร  ร	pร
  รนร[รปรKรธHรนร%รผรธรรธร  รบ  รฅaรรรฝ5ร]รฅEรก
!#"%$'&)(+*-,/.01!2&32.4&# 5762,'89$/:;: 
$'.?>KH  D BCJ $/83 IL *M,/N#"%$'&OQP2:R3S

รข รพaรฝร]รรปรฅ รนรUTeร1รaรKรธ  ร

).=<

eร ร
รพ Vร]รฅaรปHรนร]รaร รฅaรรบรธ รฝ

$/.?>
6,/89$/:): A@%BCAD ;.E<GFIH I@%BCI'J

รฅaรป รบรธรพ
รรพยผรฅยผรKรธ Eรก

 V  

  

 
WX!Y>/)8C3M*&OZ,/.?$/:?"%$'&)(G*M,'.0-;1&3.4& 5[6,/8Y$/:): 
  ) .\<]$/.^>_62,'8`$/:): A@%BCD ).\<
h
F
H
/
$
?
.
K
>
H
/
$
C
8
3
, N#"%$'&OQP2:R3S
I@%BCI'J
ADiBCAjJ
Ij
 L *-/

 bacBCdfe7g

รพ
ร H
รป รฅaรป รฅ
kรฃรRlPรบPรฅ  รบรธรพ
รรพยผรฅยผรKรธ  kรนรบรKรพ2รธรบPรฅ รธร1รKรน รพยผรบX  รธjร
รmรรรaรรKรธรฅaรพnRรยร]ร
รพยผรฅ1รฟoAp  รบY  รฅaรรรฝร]รฅq#ร5รพ7รบรธรฝR
รaรmMรร]รaรKรนยรครบPrร l  ร"ร  รฝ]รKรพVรรธ <tsvu n[รรรaรฅaรปร1รMรบPรaรPรฟEรครบPรVร ' รปยรบPรคยฃรฅaรปรKรพaรwรฟeรธรบPรฅ  รฝ5รฝcรบPรคยฃรฅaรปร
Eรบรพ
รพaรรฝ]ร
  ร]ร"รพTรบPรค  รบรธรพaรฅยผร  รรธรฅaรพ  รaรHรผรฝรฅยผร1รaรKรน>รก  รพรฅaรปรรพ  รบรธรพaรรพยผรฅยผรKรธ   รรพรนร]รaร  รฅยผรKรน>รฟxAp  รบY  รฅaรรรฝร]รฅq,รรพTรบรธรฝR
รพ [รปรบรพยผรgรผร
รพaรฅyl  ร"ร  รฝ]รIร
ร  รKรนรKรพyรซZรฅaรปรVรพยผร  รบรธรน/รบรธรรทรถรรธรฅaรปร  รพ
รพaร]รรธVรKรธรฅ
' รปร]ร lPรKรน7รคZรบPร  รบรธรพยผรฅยผร  รรธรฅay
รบPร
รนร1ร"รรธ0ร n6รยรรธ  รฝรฝ Pรฟ>รฅaรปรYรผรฝ]รฅยผร1ร"รรธร รรพ:รบรธรฝ  ' รปร]ร lPรKG
รน n ร n รฅ nTรฅaรป`
ร ร lPรบPรฅ  รธรนรรน  รฅยผ`
ร [รปรบรพยผรรฅ  ร
รPร1รฅรรพ
2รซ{รพ  \A@{z#|}j-รท nf~รรธHรบPรฅaรปร1
ร pรบPร"รนรพ1รฟ
 ^v;I"?)ยย,&*-,/.01!2&32.4&A 5X6,/8`$/:;: A ;.ย<tsGuย$/.?>b62,/8$/:;:  D ;.ย<ย1ยV*2(ย&)(ย$'&
ย
dxe7g FยH @pz'BCI'J $/.?>_H DยBCIJ $/83 jL *-,/N#"%$'&OQP2:R31F4"?8i,'ยpZ>j3>
&)(ย$'& Ap
@ zr|}Iwยยยย S

ย รบYรพaร4 ร0cรฟรบรธร  รธ`ร
รบรรรปรฝรพ  /รฅaรป  รฅยรครบPรยร ' รปยรฟ%  รฅaรป  รบรธรพaรรพaรฅยผรKรธ  /รธร1รKรนรพpรฅยผรบ  รบรธรพaรรนร1รIย/รซ)ยcย4รท
รฅยผร
ร  รธรรฝ]รKรพ1รฟยฃรนร]รaร  รฅaรรบรธ  รฝf  รฅaรป  รบรธรพ
รรพยผรฅยผรKรธ  Xย7รซZg ย รท  รธรน+รlPรบPรฅ  รบรธรพaรรพยผรฅยผรKรธ  ยย/รซZgรท-n ย รปร2รนรUTeร1รaรKรธ  ร
eร1รฅ ยร1รKรธG  รฅaรป  รธรนยรRlPรบPรฅ  รบรธรพaรรพaรฅยผรKรธ  ร]รKรพ  รฝ]ร  ร"รฝ  4Eร  ร"รพ(รบรธ รฅaรปร/รฅยผร  lPรKรฝ  รPรKรธ   ร2ย  รฝ]รPรก&รบรธรฝR
รฅยรบ  รบรธรพยผรฅยผร  รรธรฅaรพ  ร
ร  รฝ]รฅยผร1รaรKรนยn ย รปร7รผร
รพยผรฅTรบรธร รซqยยยยรณรญqยยรฏqรฐรฑVรฌยรฒยรณรดYรฎMรตรถรญ!รฏ;รฐยย1รทรรพ  รaร  รฅยผรKรน>รฟ  รธรน#รฅaรปร
รพยผร  รบรธรน#รบรธร qรซ ยยยยรณรญ ยnรฏ;รฐรฑVรฌรญรฎMรญ!รฏqรฐยย1รท6รรพ`รบรนร9รผรKรน>รฟh[รปร1รaร  รพ  รฅaรป  รบรธรพaรรพยผรฅยผรKรธ  Xรบรนร9รผรKรน,รฅaรปร2รผ4lPร
ร2ยรรพยผรฅaรรธร  รบรธรพยผรฅยผร  รรธรฅaรพ1รฟ  รธรน  รaร  รฅยผรKรนยรฅaรปร4รผ lPรรบPรฅaรปร1rร eรบรพaรพaร รฝ]ร  รบรธรพยผรฅยผร  รรธรฅaรพ1รฟ^  Mร5รธร/รฅaรปร  รบรธรพยผรฅยผร  รรธรฅ
รPร  รป  รบ Yรฝ]ร1รฅยผรยรซ{รพaร1ร`รยร]รรรaรยรPร4-รท nยยVpรบ ยร lPร1รKรฟยรฅaรปรรพ(2ร ย  Yรฝ]ร/รนรบรKรพ6รธรบPรฅรปร]รรปรฝร]รรปรฅ  ยรธ  รนUร Teร1รaรKรธ  ร
eร1รฅ ยร1รKย
รธ ร lPรบPรฅ  รบรธรพaร5รพยผรฅยผรKรธ    รธรนยรนรรaร  รฅaร]รบรธ  ยรฝ   รฅaรป  รบรธรพaร5รพยผรฅยผรKรธ  'n&รยร]รรรaร ร ย	รaรKรพaรKรธรฅaรพ:รป{รบ ยร lPรบPรฅ
รบ รธรพaร5รพยผรฅยผรKรธ    รธรน รนร]รaร  รฅaร]รบรธ  ยรฝ   รฅaรป  รบรธรพaร5รพยผรฅยผรKรธ  ย   รนUร TEร1ร
รKรธรฅaรฝ ยรบรนร])รค ยรฅaรปร  รบรธรพยผรฅยผร  ร5รธรฅ&รPร  รป
รบPรค   [รปรKย
รธ รaรบ  รKรพaรพaรรธร  รธรบMรนbร ยn
ยก{ยขpยฃ

ยคoยฅ{ยฆ^ยงยจ

ยฉ^ยช/ยซ4ยซVยฌ%ยญhยฉยยฎ!ยฌ%ยฏ

ยดยยต;ยด

ยณ
ร

ยญ ยธยยช ยท ยธยยฌยยฏ
ยถ/ยท h
ยดยยต!ยฑqยนยบZยป
ยฌ ยน-ยผยฝ ยพยฟยฒ;ยป
ยฏMร-ร ยนยพยฟยฒ!ยป

ยฐVยฑqยฒ

ยฉVรยยช/ยญhรยยซ4ยฎ!ยฌ%ยฏ
ยดยยต ร ยนยบOยฑ

ยธยรยช/ยฏ ยท

ยฏMร-ร ยพยฟยน

ยฉยยฎCรยยฎ!ยฌ%ยฏ
ยฐ ร ยตZยพรยฒ
ยถยรCยนรMรCยน
ร ร ยฒ;ยป-ยพยฟยน-ยผCรรCยน
ยญ ยฑรยรยรรยต)ร
ร ร รCยตZยพรร

ร/รMรpร{ร2รjร1รยฟร2รรร/รjรยรรjรร2ร

ยธยยช{ยฎ)ร?ยฌ%ยฏ
ยท
ร
ยฉ

รpรยรMรยรMรpรรร2ร
ร-ร)รMรรร2รรรยรรรร/ร2ร

รรขรกRรฃรค0รฅรฆรง'รง/รจfรฉยรช/รซรฌ=รญ2รฎรฏ4รฐรกRรฐCรซCรฆรฏยรซoรญรฑรฎรฐรคVรฅรฆbรฎ'รฒยรณรดqรต
รถ รธ

รถ รทiรธ

Pivot consistency

รถ รบ

รถ รป

Directional path consistency

รถ รน
รถ รธ

รถ รธ

รถ0รทiรธ
รถ%รบ

รถVรทยรธ
รถVรป

รถยรบ

รถ0รน

รถ0รป
รถยรน

รยรกรฃรค0รฅรฆรงรผยรจ#รฉyรฅรฎ%รญ2รฆรฐรฐรกRรฏVรฃbรฝ4รกรพ'รฎ'รซfรญ2รฎรฏVรฐMรกRรฐCรซCรฆรฏVรญ2รฟYรช'รฏ4รกรฅรฆรญ2รซรกรฎรฏVรช'รฑ%รฝ?รช/รซรฌ9รญ2รฎรฏVรฐรก!รฐCรซCรฆรฏVรญ2รฟรฎรฏ9รฏ0รฎ0รฆ	
oรกรซรฌ
รฆ ^รฎรฑ รฆ 0รฃ'รฆรฐยรฅรฆรฝ4รฅรฆรฐCรฆรฏยรซยรซรฌ0รฆรญ2รฎรฏVรฐCรซCรฅ-รช'รกRรฏjรซรฐรขรฝรฎรฐรฐ!รก 4รฑรฟ_รญ2รฅรฆรช/รซCรฆ "ยรฟwรฝ4รกรพ'รฎ'รซ
   รจยรซรฌ0
รญ2รฎรฏVรฐรกRรฐCรซCรฆรฏ4รญ2#
รฟ 
%$ รฅ $ &
รซ $'(*),+-  %รซรฌ0,
รฆ 0รฎ'รซCรซCรฆ ยรฆ 0รฃ'รฆรฐรซรฌ0รฆ_/รฆ .%รซCรฅMรชรฎรฏ0รฆรฐAรฝรฎรฐรฐ0รก 4รฑRรฟ`รญ2รฅMรฆรช/รซCรฆ
ย1
รฟ VรกรฅMรฆรญ2รซรกรฎรฏVรช'รฑ^รฝ4รช/รซรฌ=รญ2รฎรฏ4รฐรกRรฐCรซCรฆรฏVรญ2รฟ

243 2

57698:;#<:=>?6!>;@	=A	BDCE:FHGJI=KA	;L6!:K=LMNO<PQ5>

R SUWV%X/YZUW[\0V/]LS0^`_aSbT\0Y*cV&dfeYgdf[KS0hgYg^fX/YZibSb^V iX/_a^jkgiHeYgdf[Kkl^jmX/YZibSb^V iX/_aYgdonS!dV X/^S0YZikg\
T
[kl^jHX/YZibS!b^V iX/_gpKkgbTcfS!\\KiLY4crqV%bjLY4cfits
utv!wyx{z}|~Jยย	ยยยยยยย
ย d YยX/V nยdVaยยlยย	ยS!b"X/YZUW[KยL^V nยV/]kgX/^\!_ยYZiX/VmeยYgdWV kgX?jยhEkld?S!klq\0VaยยmS!iยยยยยยยยS!^ย^jV iยX&kg\!\!b
kl^ยUWYZb^ยยOยยขยกW^SUWV b,[dYX/V nยLdVยฃยย4ยคLยฅยฆZยKย*ยงKยจQยฉยซยชYZiX/VeYgd%V kgXjYge7^jVยยยขยกWeYgdUWV&dยhEkldSklq\0V b?ยฌ?s
ยญ XjS0V&hS!iLยฎยยย&ยฏX/YZUW[Kkl^S0qS!\!S!^_qV&^cV&V ia^cYOX/YZib^dkgS!iQ^bTยฐยฑ*ยฒย{kgin1ยฐ}ยณยดย{X/YgddV b[YZinb7^YWkgX?jS0V&hS!iLยฎ
[kl^j1X/YZibS!b^V iX/_eYgddV \!kl^S!YZiaยต}ยฑ&ยณยc%sยถd sยถ^&sยทhlkldS!klq\!Vfยย	sยทRTjS!bยธYg[V&dkl^S!YZi1iLV&V nbTยนยชยปยบQยผ*ยฌยธSiยฎgV iV&dkg\ยฝs
ยพ ยL^1bS!iX/V#^jLVDX/YZib^dkgSi	^Hยฐยฑ*ยฒ?ยยซS!beยiX/^S0YZikg\ยฝpf^jS!bยฟX/YZUW[\0V/]LS0^_S!baiLY*cรยนยชยปยบQร*ยฌ?sรRTjLV#Y*hgV&d?kg\!\
X/YZUW[\!V/]S0^`_YgeJ^jLV,ร}\0^V&dS!iLยฎWS!b^jV&dV&eยYgdV
ยนยชรร ร ยชยฝย"ยยยก*ยฌรยบ ร ยฌยธรรยนยชยชร ร ยรร ร ยฌรยบ ร ยฌ
ย&ร}รรรร
ยชยปS!ib^V kgnYge'ยนยชร ยผ ยบ ยผ ยฌยธeYgd[kl^jaX/YZibS!b^V iX/_OYgdTnS!dV X/^S0YZikg\K[kl^jaX/YZibS!b^V iX/_ รร ยรรS!b7^jLVรiQยUยqV&d
YgeJhEkld?S!klq\0V b&pLยบS!b^jLV%bS!ร&V%Ygeร^jVยnLYZUkgS!ibkgin1ร"S!b^jLV%bS!ร&V%Ygeร^jV%dYยYg^TbV&^?ยฌ?s
รยทรร'ร	รยร

รรfรJรรtร,รEรยธรก4รขgรรรรขZรฃรคร}รขZรฃรครรรรรขgรรฅรรงรฆรจรยร`รฉ'รขZรฃรครรร

RTjLVS!i	^d?S!ibS!XยXjkldkgX/^V&dS!b^SXYgekm\0YX&kg\ยทX/YZibS!b^V iX/_รS!bย^YHV ibยLdV^jkl^ยkm[kld^S!kg\ยทSib^kgiQ^S!kl^S0YZi
X&kgiยq}VยV/]ย^V inV nH^YakOiLV&cรชhEkldSklq\0VgsรซรV%รdb^T[dV bV iQ^f^jVย[dYg[V&d^S!V bYgeJ[S0hgYg^oX/YZibS!b^V iX/_gp}S!i
[kld^SX&ย\!kldยท^jLVรฌX/YZinS0^S!YZibยธยinLV&dยทcfjSXjOk{X/YZibS!b^V i	^7[kld^S!kg\KS!ib^kgiQ^S!kl^S0YZiUk _WqVTV/]^V inLV n^Y
kabYZ\!ยL^S!YZitsfรซรVW^jLV i#V/][\!kgS!iยjLY*cรญ^Y1X/YZUW[ย^Vย^jVยnkl^kยฟdV รฎยยS0dV nmqย_H^jLV{รK\0^V&dSiLยฎkg\0ยฎgYgd?S0^jU1p
kginaร}ikg\!\0_[dV bV iQ^TkUWV&^jLYn1eYgdfbYZ\0hS!iLยฎ"eยปยiX/^S0YZikg\ยรฏรฐ รฑb&s
รฒรณvยฝรดรถรต%รทEz~ยธยQรท4ย4ยยQรธ
รซรVร[dYX/V&V n1S!i^cYb^klยฎgV b&รนยธรdb^&pย^jLVรkgnnS0^S!YZiOYgeรจkWiLV&cรงhEkldSklq\0Vf^YW^jV,X&ยddV iQ^Sib^kgiQ^S!kl^S0YZitp
^jLV i1^jV%V/]ย^V ibS0YZi1edYZUรบ^jLVรdYQYg^fbV&^T^YOkbYZ\ยL^S0YZits
รฑ 	&ย t
ร  รผ/รฝรผรฝ/รฝรฝ gรฝ !#"%$ ยtร	
รต%รทEz~ยทยยรท4ย?ยยรฒยvยฝรดรงรปรรผ&รฝJรพ รรบยชยยรฟmรฟรฌรฟยยฌ รผ รฏรฐ&

ย )
'& ย(
รผ *},รผ +.-/0  1 รผ"รฝ23ยรฝ4รฝZรฝรผ6587"{รฝ39Lรผ0รผOรผ:;/รฝ3 ยยฑ< $ ย=tร >?,9mรฝ39@gรฝ ยยฑBAรยย C
ย tร 	 3รฝ 9รผ  &ย tรIH J  รผWรผ:	รฝรผ'&Qรผ&Kรฝ &ย4	 24ยรฝรผ,รฝK3/รฝรฝ gรฝ4LF"G$ ย 5

3

/
g

รฝ
E
F

G
"
$
D
รต%รทEztN
z MO*P V&^ยยb{bjLY4cร^jkl^"kgi	_ยซX/YZib^dkgS!iQ^{S!iX&\!ยnLV nรS!i $ ยaS!b{bkl^S!b รKV nts P V&^ยยยฑQ< $ ยtรยbยX?j
^jkl^ยยฑRA ยย#SbOkยซ[KS0hgYg^Yge $ ย=tร/pTkginTSQยฑยซS!^bhlkg\!ยLVmS!iU&ย=tร&sWVYZibV Qรฎ ยLV iQ^\0_gp^jLV&dVHV/]Sb^b
Sยย,.
ร Xlยฑ Yรย	ยชSยยฑQยฌ?s P V&^TยbTnLV iLYg^VZ&ยรรรถยชS ร รฟ,[,[,[ รฟSยยฑLรฟ,[,[,[ รฟSยย=tร รฟSยยgยฌ?s
ยกls ยญ Qi _OX/YZib^dkgSi	^bkl^S!b รKV nOqย_&ยtร7kgincfjS!X?jnYQV biLYg^X/YZi	^kgS!iaยยรS!b7^jLV&dV&eYgdVรฌYgqQhS0YZยb\!_
kg\bYObkl^S!bรV naqQ_2&ยQs
\ยG
s ]Ygdkg\!K
\ ^)_ ยยb&sยถ^&sยฐ}ยณ0ยQ<W"s(]JS0db^&p'ย ยณ < $ ยtร/รน\0V&^CS ยณ qVยฟS0^bยhEkg\!ยVยฟS!i`&ยtร&sbaV X/YZintp
ย
ยฑ A ยยWS!b,k[S0hgYg^,Yge $ =ย tร&s%ยฐ}ยณ0ยkgin#ยยฑA ยยOkldVWX/YZibV รฎยยLV i	^\!_Hยย*ยฏX/YZUW[kl^S!q\0Vgs,รซรV
^jยยbTjk hgVยฟยช S ยณ รฟ SQยZc
ยฌ <ยยตKยณ0ยZยbYLpKยฐ}ยณ0ย{S!bbkl^S!b รV n1qย2
_ &ยZs
d

hgV&d_ยฟX/YZib^dkgS!iQ^TS!iX&\!ยnV nS!i
$

ยยS!bbkl^S!b รV naqย_&ยQsfefV iX/VZ&ยยS!bkWX/YZibS!b^V iQ^TS!ib^kgiQ^S!kl^S0YZiรณsEg

h#i=j'kmlnopnqrnKrstq/uvoFw6uvx/y{z=u}|oFs~nuvlx/rยย/rnqIsFlxย uยยno~x/s~ยIlxCยยย/xs~nuvlxrยย/ยFย=ยfuvยKยnuยv'ยEยยยยยย6ยFยrxzIx/lnKยEยยยยยFย=ย#ยยย
ย uvx/sFoยย|ยn'ยยย/x/sยnuยlxrsFlxยn |truvx,nยยr|oยxln'x/oFs~oFย ยr|uv}ยzu}|oFsยno#zpnq/oย|uvyq,nNยยrFย%ยยยยqo~|oยrยยยย/xs~nuvlxr/sFlx/ยn |truยx,n
uvยBย3ย=|lย4o~|}ย=ยBz=u}|oFs~no#zu}ยmu}nยยl|uvyuยxuvยEยก4o~ยยl|o%u}nยEntr|yo~nยuvxnqoยขuvx/ยntrx,nuยฃrnuvlx*l|tzo~|uvxy,ยยยยrxzย oFsFlxzx/lx
ยยยx/s~nuvlxrยsFlx/ยn |truvx,nยยยคrFยยฅยก4oms~|o#rnoFzยขยก,ยGnqoย|l,s~oFย ย uvx/yj
ยฆยงยจ

ยฉGยชยซยยฌยยญ

ยฎQยฏIยฐ?ยฑ=ยฒยดยณ?ยตยฏ6ยถ#ยฏ6ยฐยยทยยทยธ?ยฏ8ยทยธยฏ,ยฑ4ยตยฏ6ยนยปยบยทยทยธยฏ8ยผยฏ6ยฐยฝยท#ยฏ,ยตยฅยฑ4ยพmยทยธยฏZยน)ยฏ,ยทยธ?ยฑ@ยฟรยฒpยฏZรรยฐยฝยท#ยตยฑรยฟ?ร?ยผยฏ%ยพรยตยทยธยฏ,ยต6ร
รCรยร@รNร/ร@ร

รรรยร,รEรรรรร3รรรรรรยขรรZรรGรรรรกร

รข(รฃFรค=รค4ร{รฅ6ร,ร3รZรขรฆ'รงLรขรฆbรรรจ#รฉรค4รชGรซรข4รรฌ รญ,รฎยรรฏรข/รฅรฅรฌยรฐ4รฆ?รชร,รฆNร

รครฃ#รงยรรฃรฌ3รฆยฝรฐLรฅรฑ?รฉรฒรณร3รฒ@รข4รยฅรรดรฌรฅCรซ'รฌ3รต/รค4รCรฉรครฆรฅรฌรฅรtรรฆNรIรถKรทรฃรทรรทWร3รฒรรชรทรนรธรบรปรข`รฉรครฆ?รฅรฌรฅรtรรฆ'รZรฌ3รฆรฅรรข4รฆNรรฌรข;รรฌรครฆWรค#รบ)ร
รรผ;รฌรฅร3รฅรcร3รฒรรฆ(รฌ3รEรฉรข4รฆbรญรBรรผ;รtร,รฆ'รงยรรง2รรค*รขรรญรข;รฉรฝรรฃFรข;รฉ,รฝรจยรบรฃรรCรฅรครฎยรฑรรฌรครฆยรท
รพ ร/รยรNรฟ ยธยฏยบยตรรยบ	ยยฏ6ยถยยฑ4ยพ ร
bร

ยฑ;ยฐ*ยฐ?ยฏ,ยฏ6ยฟรปยท#ยฑ
ยฏ8รรยฐNยถ#ยทยบ4ยฐยยทรยบยท#ยฏ6ยฟรยบยยฑ;ยฐยฒGรยยทยธรยทยธยฏ

ยฑ4ยตยฟยฏ,ยตรรยฐยบ4ยถcยฏ6ยบ4ยผยธรยฑ4ยพยยทยธยฏ6ยนรกรรยถcยทยธยฏยขยทยบยต4ยฏ,ยทEยฑ4ยพยยบCยณNร4ยฑ4ยท ร ยถรรยฐ?ยผยฏ ร
รรยถ4ยฏ,ยตรNยฏ6ยฟรปยถ#ยท#ยฏ,ยณ

!รยถ#ยท#ยฏ,ยณรปยพ3ยตยฑ;ยน

ยทยธยฏ8รยฐ?ยถ#ยทยบ4ยฐยฝยทรรยบยทรยยฑ;ยฐรยฑ4ยพ ร

$&%('*),+ ร	-/.1032146579895:รร<;;รยร>=Kร?@032A4CB

ร ยผยฑ;ยน)ยณNยบยทรยยฏ

รรยถKยณ'ร4ยฑ4ยทpยผยฑ;ยฐNยถรรยถ#ยท#ยฏ6ยฐยฝยท ร ยตยฑ4ยณ
ยฏ,ยตยทร

ยท#ยฑยบยถ#ยฑ"รรยทรรยฑ;ยฐยร

#

ร<DรยรFE

GFยฐ)ยถ#ยฏ6ยผยทรยยฑ;ยฐร ร3 ยธยฏ,ยฑ4ยตยฏ6ยนH ร ยฒcยฏยฅยถยบ6ยฒRยทยธNยบยท  ;ร4ยฏ6ยฐยบZยผยฑ;ยฐ?ยถรรยถ#ยท#ยฏ6ยฐยฝยทfรรยฐ?ยถยทยบ4ยฐยยทรรยบยทรรยฑ;ยฐBยฑ4ยพ รI ยบ4ยฐ?ยฟBยณ?ยตยฑ@รรรยฟยฏ6ยฟ
ยทยธ?ยบยทยทยธยฏpยณ?ยตยฑรยฏ6ยน

รรยถmยณNร4ยฑ4ยทmยผยฑ;ยฐ?ยถรรยถ#ยท#ยฏ6ยฐยฝยทmยฒZร}ยต6ร}ยท,รmยบ4ยฐ

ร ยผยฑ;ยน)ยณNยบยทรรยฏpยบ4ยถยถร;ยฐ?ยน)ยฏ6ยฐยฝยทยยฑ4ยตยฟยฏ,ยตรรยฐ%ยบ4ยฐ?ยฟCยบยขยณNร4ยฑ4ยท

ยถ#ยฏ,ยท  ยฒpยฏ*ยผ,ยบ4ยฐ`ยฐยฑ4ยทZยฑ;ยฐJ;ร?ยบยตยบ4ยฐยฝยท#ยฏ,ยฏ*ยทยธ?ยฏ,ยตยฏรรยถIยบรยถ#ยฑ"รยทรยยฑ;ยฐbยท#ยฑยทยธยฏ*ยฒGยธยฑ"ยยฏยณNยตยฑยยฏ6ยน

 NรยทZยบยถ#ยฑ

'ยฐNยฟbรยยท

ยฒGรยยทยธ?ยฑ;รยทยยบ4ยฐKLNยบ4ยผMยฝยท#ยตยบ4ยผM@รยฐรfยฎQยฏIยผ,ยบ4ยฐรปยทยธยฏ,ยตยฏ,ยพ3ยฑ4ยตยฏZยฟยฏ6ยฟ?รNยผยฏ%ยบยน)ยฏ,ยทยธยฑรยฟรยพยตยฑ;ยนรกยทยธ?รรยถEยณ?ยตยฑ4ยณ
ยฏ,ยตยท4รfยฎQยฏZยฐยฑยฒ
รรยฐยฝยท#ยตยฑรยฟ?ร?ยผยฏ{ยทยธ?รรยถGยน)ยฏ,ยทยธยฑรยฟ  ยฟยฏ6ยผยฑ;ยน)ยณ
ยฑ;ยถ#ยฏ6ยฟรรยฐยยท#ยฑยพยฑ;รยตGยณNยธ?ยบ4ยถ#ยฏ6ยถ  ยบ4ยฐ?ยฟรยพ3ยฑ4ยตGยฏ6ยบ4ยผยธยฑ4ยพยทยธยฏ6ยน

 ยฒpยฏZยณNยตยฏ6ยถ#ยฏ6ยฐยฝยทยทยธยฏ

ยทรรยน)ยฏ8ยผยฑ;ยนยณยยฏONรยยทP2ยบ4ยฐ?ยฟรยทยธยฏZยตยฏ6ยถร6ยยทEยฑ4ยพรยยทยถยผยฑ;ยน)ยณ'รยทยบยทรยยฑ;ยฐรยฑ;ยฐรยทยธยฏ{ยฏONรยบ4ยนยณยยฏ4ร
รพ

รAQ6?6ร`รSR ยฑ;ยน)ยณNรยทยบยทรยยฑ;ยฐ`ยฑ4ยพยขยบยตยฑยฝยฑ4ยทยถ#ยฏ,ยท

รI ยบรฏยณNร4ยฑ4ยท)ยผ,ยบ4ยฐ?ยฟ?รรยฟNยบยท#ยฏ2ยถยฏ,ยทUT
ร

ยบ4ยฐ?ยฟรณยบ4ยฐ

ร ยผยฑ;ยน)ยณNยบยทรยยฏ

ยบ4ยถยถร;ยฐ?ยน)ยฏ6ยฐยฝยทยยฑ4ยตยฟยฏ,ยตรรยฐร

 ยธ?รรยถยณNยธ?ยบ4ยถ#ยฏรฏยผ,ยบ4ยฐรยตยฑ;ร;ยธ6V
ยฏรฏยฑ;รยทรรรยฐยฏ6ยฟTยบ4ยถW'ยฐ?ยฟ?รรยฐ6bยทยธยฏ(ยถ#ยฑ;รยตยผยฏ6ยถ*ยฑ4ยพZยบC4ยตยบยณNยธTยฒGยธยฏ,ยตยฏยฏ6ยบ4ยผยธ
ยถยท#ยตยฑ;ยฐ"รปยผยฑ;ยฐ?ยฐ?ยฏ6ยผยท#ยฏ6ยฟยผยฑ;ยน)ยณ
ยฑ;ยฐยฏ6ยฐยฝยท%รรยถยตยฏ6ยฟNร?ยผยฏ6ยฟรปยท#ยฑยฑ;ยฐยฏCยฐ?ยฑ@ยฟยฏ
ยถยฏ,ยทKยฑ4ยพ
ยฟ?รยยตยฏ6ยผยท#ยฏ6ยฟยบยตยผ,ยถKยผยฑX4ยฏ,ยตรรยฐZยบ(?ยฐยฑรยฟยฏ6ยถfรรยฐ
ยถยฑ4ยตยท

รY
ร

ร T

ร ยผยฑ;ยน)ยณNรยทยบยทรยยฑ;ยฐยฑ4ยพ ร2ร 'ยฐ?ยฟNรรยฐยบ

รZร ยบ4ยฐ?ยฟ)ยทยธยฏ6ยฐยณยยฏ,ยตยพยฑ4ยตยนรรยฐยขยท#ยฑ4ยณยยฑ"รยฑ;รรยผ,ยบ

ร ยบ4ยถยถร;ยฐ?ยน)ยฏ6ยฐยฝยทยยฑ4ยตยฟยฏ,ยตรรยฐ6 ร ร[Zยฑ4ยตยฅยน)ยฑ4ยตยฏZยฟยฏ,ยทยบ4รรยถ  ยถยฏ,ยฏZยบยณ?ยณ
ยฏ6ยฐ?ยฟ?รรยผยฏ6ยถ]\Iร8ยบ4ยฐ?ยฟ^\Zร`_@ร

a รค4รชGรซยรฎยรรผ4รฌร3bcedรgf]hjiยร ยท#ยฑ2ยผยฑ;ยนยณNรยท#ยฏ
ยทยธ?ยฏZยบ4ยถยถร;ยฐNยน)ยฏ6ยฐยยทยยฑ4ยตยฟยฏ,ยตรรยฐ

 ยธยฏ{ยตยฑ@ยฑ4ยทGยถ#ยฏ,ยทGรรยถ ร

ร

ร ยบยณNยณยยฏ6ยฐNยฟ?รkN

\Zร ร ยบ4ยฐNยฟยบรยถยฑ

dรgfhjiยร ยพ3ยฑ4ยตlT ร

ยบ4ยฐ?ยฟ

ร ยบยณ?ยณ
ยฏ6ยฐ?ยฟNรkNW\Iร`_ ร รm@ยฑ  ยพ3ยฑ4ยตยทยธยฏZยฒGยธยฑ"ยยฏ%ยณNยธ?ยบ4ยถยฏ dรgfhCiยร ร

รon6prqtsuwvAx<y{z6sX|!sv}x1~ ร

 ยธยฏยยณNร4ยฑ4ยทfยผ,ยบ4ยฐ?ยฟNรรยฟ?ยบยท#ยฏGยถ#ยฏ,ยทfรยถ[T รWรn6z6sย|<sยvAxยยยzยยrqtยย|!ย[sยvAxAFzยยrqtยย|!ย[svAxeยยzwqtย[ยยvFย]ย
z6sยvAx<[zยยrqtยย|!ย[svAxยยยย{ยprqยp1vAx<~
 ยธยฏ ร ยผยฑ;ยน)ยณ'ยบยทรยยฏ8ยบ4ยถยถร;ยฐ?ยน)ยฏ6ยฐยฝยทยยฑ4ยตยฟยฏ,ยตรยฐBยฒcยฏZยผยธยฑ;ยถ#ยฏCรรยถ

รพ

prqsPuvAxKy[z6sX|!sยvAxKy{zยยrqtยย|!ย[svAx<y[zwqtย[ย{v&ยz6svAx<y	ยย[ยeprqยp1vAxtย
รAQ6?6ร

'

 ร4ยฑ4ยทGยผยฑ;ยฐ?ยถรรยถ#ยท#ยฏ6ยฐยฝยท]	ยยท#ยฏ,ยตรยฐ

a รค4รชGรซยรฎยรรผ4รฌร3bcยdร#รยi&ย
ยยย6รยยKย=ร#ร{ร ยถ#ยฏ,ยฏยย@ร`ย ร

รพ

 ยธยฏ{ยณNร4ยฑ4ยทยผยฑ;ยฐ?ยถรรยถ#ยท#ยฏ6ยฐยฝยทยณ?ยตยฑรยฏ6ยน
รAQ6?6รJย

รรยถยยผยฑ;ยน)ยณNรยท#ยฏ6ยฟรรยฐรยถ#ยฏ6ยผยทรยยฑ;ยฐIย@รยร

GFยฐ?ยถ#ยทยบ4ยฐยฝยทรรยบยทรยยฑ;ยฐรปยฑ4ยพ ร

a รค4รชGรซยรฎยรรผ4รฌร3bcยdรgfยยยยKย6ร ยฒยฅยธยฏ,ยตยฏ fยย

รยถยยทยธยฏ8ยฐ@ร?ยนย
ยฏ,ยตยยฑ4ยพยยผยฑ;ยฐ?ยถ#ยท#ยตยบ4รรยฐยยทยถGรรยฐ?ยผยรร?ยฟ?ยฏ6ยฟ*รยฐ
ร

ร

 ยธยฏ,ยตยฏ8ยบยตยฏ	4ยฏZยผยฑ;ยฐ?ยถรรยถยท#ยฏ6ยฐยยทรรยฐNยถ#ยทยบ4ยฐยยทรยบยทรยยฑ;ยฐ?ยถยยฑ4ยพ รรกรon6prqsPuvAxKy{z6sย|<sยvAx1~ 
nย3ยยขรฎยรฌรฉรร(ยรขรฃรฌรฅยก=รยขยยยฃยขรคยรญรยGรขรฃรฌรฅPยกรยคย3ยฃยขรคยรญรรยรครฆยรง;รครฆยกรยคย3ยฃยฅรคยฝรญรยยฅรรข/รฅรฒยฝรฌรฆยฝรฐยรรครฆยกรยคย3ยฃยฅรคยฝรญร(ยฆIร,รถยรจยยงNรค4รฃรฝยก@~
ยจXยฉ@ยฉ

ยชยยซยญยฌยฎยฏยฑยฐwยฎยฒยณยซยณยฏtยดKยฒยตKยถJยทยยฎยธ

ยพยยฟAร6รรรยฑรรยคร6รยขรรร<รรรรรร"ร
รรรรรรรรรรร3ร3รรรรรกยรข

ยนยยบ6ยฒ	ยตKยฏยซยฎ	ยฒยปยผยฐ]ยฝ<ยชยณ

รร}รรรยร
รjรฃKรครฅยรฆOรงtรยขรฆรร6รจ6ร(รรฉ

ร^รชOร"ร6รรรยขรยขรฆรร<รยรร6รรรรKรรรรรร"รรซรร]รรฌรยขรยฑร^รยขร"รญ(รฎรรร"รรซร"ร6รญรฏ

ร6รฆยรฆรรจ6รยรญรร6รฆรรรฐ]รรรฑรฒรฆรณ
รดรรต รฆeรยขร"รญรฎรรร"ร6รรรAร รต รฆeรถ6รฐรรทรญรฆรรฑรธรรฐรฆรฅ
รนยรบ ยญร รgรปรรฝรผ(รพยรฟรรผยขรฟ	รปรฝรรฝรผ	รรผร
รรป

รน  รOรผรพรรฟรฝรรผยครฟรรปรรฝรผ	[รผยขร	รป


รน 
ร รฝรผ &
ร K
ร รผ   
ร รฝรผ[รผ !"ร ร"
รน 
ร รฝรผ#^รฟ$"<ร!Kร รรผ
%'& รบ รผ(รผ)!รยญร"
รน  รรฝรผ*ยร+-,$.ร"/Xรผ0%1& รบ รผ(รผ !"ร ร"
รข 4 รรฃ 4 รค76 498;:< 6>=รรครณ รดรรต รรยรฑรฒรฆยร รต รtรจ
รดยรต รฆ]ร32รฆยรฐรรญรญรร(รฑรฒรฆยรชOร"รฑรฒรถรญรฆOรงรรรฏยรรรร รต รรรรฑรฒรฆยร รต ร!รจรฒรร[ร รต รฆยรฐรฆยรยรรฐรฆ รรกยขรกย5
รยรรฐร"รรฆยรยขร"รญรฎรรร"ร รก รรฐรรญรญ/รค]รร[รร3รฎ6ร6รชOรรร"รรรญ@?AB^รยขรWร รต ย
รฆ ร"รรฆUรร6รจ6รฎรชOรฆรรจ <
รท รฏ

ร รต รฎ6รรรฐรฆรรจ6รฎ6รชOรฆรรร รต รฆยรรฆรรรฐรช รต

รรรlรฐร!รรeรยขรฆยรlร^รณDCrรร รต รฆUรรรฑรฒรฆUรฐรฆE!รฎ6รรฐรฆรรจยฑรรทF2!รร"รฎ6รรญรฏ^รจ6รฆยรถรรฆรรรจ6รรร"รยร รต รฆรฒรรHGยรฆรฒรร{รJI5Kwรฆรรฑรฑรฒรฆรรจ6ร(รรยขรฆรรญรฏ
รยขรฆยรฆ รต รKSรร<รยขรฆยรฐรฆรรยขรรร6รฉยรรยร(ร[รยขรUรชOร"รฑรฒรถรฎรยขรฆrรยรฑรรรรฑยรฎรฑ
ร รต รฆยรรฑรรญรญรฆยรฐรร

รยขรQPtรรKoรรรยรรHGยรฆeรรรยขร!ร"ร

C

รรHGยรฆรรจรฐร<รรรยขรฆยรยรณLC

รจ6รรฐรฆรรชOร{รชOร"ร6รยขรฆE!รฎรฆรร6รชOรฆlรร{ร รต รร

รรI6ร รต รฆยรฑรฒรรฐรฆยรฆ
MWรชยรรฆรร<รรร รต รรยรฑรฒรฆยร รต รtรจNKรรญรญ9รท9รฆรณยรยรรรชOร"ร6รรฆE<รฎรฆรร<รรญรฏ

รญรรรรรฐรฆรรฑรรฐ"P

รรยรถ9ร"รรรรทรญรฆรฅ}ร รต รร รต รร]รท9รฆeรจร"ร6รฆeรรLรถ รต รรยขรฆQRรณ

รยขรฆยรฆรรฑรOKwรรฐร รต

K รต รรญรฆ

รทรรฆยรยรรฐรฆยรชOร"ร6รชยรญรฎ6รจ6ร(รรฉรฅ{ร รต รฆยรฐรฆรรรจรฆยรฐrรถ6รฐรรท	รรทรญรฏLรรรรรชOรฆรรจIร รต รรร รต รฆยร!รฎ6รฑยรท9รฆยรฐรร[รชOร"รTS

รรรรยขรฆรรKรUรร6รยขรรร<รรรรรร"รรeรรยร

รรยร รต รฆLรรรฑรฒรฆLรรยร รต รฆรร!รฎ6รฑยรทรรฆยรฐยรรรรยขร"รญรฎ6รรร"ร6รeรรรร รต รฆUK รต ร"รญรฆรถ6รฐรรทรญรฆรรฑ

รณ

รดยรต ร(ร]รร]รรรรรรฒรชOร"รรรชยรรจรฆรร6รชOรฆI6รรVKยรฆยรรฆยรฆeรท9รฆรรญรKeรฅ

ร `bร[ร	ร cยรร[รป
ร dรฝร OรP

ร รยรผ[รฟUรรยร ร
รรยรeยครรร"รfยรรรhgji'tรkmltรQร
รonOยรรpltร3รgรd
ยพXW$Y9Z[รFW3[]\ ร'^_ Aรยa
oร ne` "ร ql6รฟรrยรtรsmltรQร
UรonยรปรdรรฝรPรรรdรฝร รฟ	ร3ร3รฟร3รgรdรฒรon ร รยรรฟ"รป
Nยรรpltร3รgรยรยรtรรร"รPรdรgร รon
รร<รฟ"รปรร3รยญรรรรรUรonยร!รยรยรdรฝร รฟร3ร3รฟ"ร3ร3รm
g

รกz9{|}}}|	z รค[รท9รฆรรยรชOร"รรรรยขรยขรฆรร<รยรรรยขรรรKรร(รรรร"รรรFร รต รฆrรฐร!รรยรยขรฆยรwร
{|}}}3|
รณ
=Oy
=
y~ย
ยx=3ย
CrรชยรชOรรฐรจ6รรรฉยฑรยขร รดยรต รฆยรรฐรฆรรฑยRIยw รชยรรCรท9รฆรฆOรงtรยขรฆรร6รจรฆรรจCรยขรรซรรรฒรญรฆรรรยขรยร"รรฆLรยขร"รญรฎรรร"ร รยขร ` รณJร รรยรยฑรฐร!รร
=
รยขรฆยรรฒรรk{
ย รณยr
C รKรฏ
รรCรรฌร ร,รรUร รต รฆยรฐรฆยรยรรฐรฆ รรซรจรฆรรรชOรฆรรรจ6รรKรUรยรฐร"รฑ
รย2ยรรฐรรรทรญรฆ
รรร^รณ รดรรต รฆยรฐรฆ
ยยย
ย =7ย
|	z รค
รชOร"ร6รยขรฆ!
E รฎรฆรร<รรญรฏ>รฆOรงรรยขรรรยฑรฎร6รE!รฎรฆย2รรญรฎรฆ z
รรฎ6รช รต ร รต รร รกz
Iwรร6รจI{รรdIยรรjร"ร6รญรฏ
ยยยยfย
= ย ย ยยย = ย ย
รชOร"ร6รร(รยขรยขรฆรรKรรฒรรรยขรรรKรร(รรรร"รCรรรยย
รร6รชยรญ(รฎ6รจ6รรรฉJw I[ร รต รฆยรฐรฆยร/รรฐรฆ ร"ร6รญรฏยร"รรฆ รร"รญรฎรรร"รCรร6รชยรญรฎรจ6รรรฉยw รณ
ย)ย
=
=
ยIรรฐรฆยร
2 รฆยรฐ[
I รรยรยw
K รIรจรยร
ย รฆยรฐรฆรรKรยร(ร6รยขรรร<รรรรรร"ร6รรรร รรทF!
2 รร"รฎรรญรฏIรชยรรรรรยรท9รฆรฆOรงtรยขรฆรร6รจรฆรรจ>รยขร^ร รต รฆรรรรฑรฒรฆ
รยขร"รญรฎ6รรร"ร
I ร รต รฆยรฐรฆยรชOร"ร6รยขรฆ!
E รฎรฆรร<รรญรฏLรฆOรงtรรรรรรร"ร6รฆeรยขรร"รรฆยรฑรรถ6รถร(รรฉUรทรรฆยรยย
K รฆยรฆรรIร รต รฆยรยขรฆยรรรรยร รต รฆยรชOร"ร6รรรยขรยขรฆรร<ร
รร6รรรรKรรรรรร"ร6รรรยรรรร6รจ ร รต รฆeรยขรฆยรOย รรAร รต รฆยรยขร"รญรฎรรร"รรยรยขร ` รณ
ย
ยพXW$YY9t"u 1
v รฆยรxw

ยfยยยยย1ยยยdยกยยขxยฃยคhย'ย
รด รP!รร6รฉ

รร<รยขรCรรชยรชOร"รฎรKร

รยขรฆรรฑรร<รรรชยฑรถ6รฐรรถ9รฆยรฐรรรฆรรรรeร รต รฆJรชOร"ร6รยขรยขรฐรรรKรรLรรWรVรยขรรรญรญ]รฐรฆรรชOรฆรรKร

รรถ6รถ6รฐร"รรช รต

รยขร

รร6รชOรฐรฆรรรยขรฆร รต รฆeรฆ
MWรชยรรฆรร6รชOรฏWรร@ยฅ	รรจ6รรรฉUรยขร"รญรฎรรร"ร6รรยขรย?AB6รรรณ รดรรต รรรถรรถ9รฆยรฐ]รท9รฆรรญร"รรฉ"รรยขรร รต รร]รยรฐรรฑรฒรฆรณ

ยฆรซรฆsยฅ	รฐรยขรรร<รยขรฐร!รจรฎ6รชOรฆรรจJรรร6รฆK

รญร!รชยรรญรรชOร"รรรรยขรยขรฆรร6รชOรฏI1รถรH2รรeรชOร"ร6รร(รยขรยขรฆรร6รชOรฏI&รยขร

รชOร"ร6รยขรยขรฐรรรKรรรร/รรฐrรรญร3KยรฆยรฐlรชOร"รยขรร รต รร
รรรรยขรฆรร6รชOรฏรซรรeรNKwรฆรรPรซรยรรฐรฑ

รร]รถรร รต

รถรร รต

รชOร"ร6รรรรยขรฆรร6รชOรฏ

รจรฆรรรญfKรร รต

รกย
K รฆยรยรฎรฐร รต รฆยรฐรฑรฒรรฐรฆยร รต รKwรฆรรจ

รยรฎร6รชOรรร"ร6รรญ

ร รต รรรรถรH2รรรชOร"รTS

รชOร"ร6รรรรยขรฆรร6รชOรฏรครณยงยฆรซรฆรร รต รฆรรยรถรฐรรถ9ร"รยขรฆรรจยรร>รรญรฉรรฐรร รต รฑ

รยขรIรรช รต รรฆ2รฆ

รถรH2รรยรชOร"ร6รรรยขรยขรฆรร<รLยฅรรญรยขรฆยรฐรรรฉรณ@v1รรยขรฆยรฐIFKwรฆรยขรรฎ6รจรรฆรรจรฒรยขร"รฑรฒรฆรถ6รฐรรถ9รฆยรฐรรรฆรรยร รต รรยรถรH2รร{รชOร"รรรรยขรยขรฆรร6รชOรฏUรถ6รฐร2tรรจรฆรร
รรรยรฎร6รชOรรร"ร6รรญ-?ABยKรรร รต F
I รรIรถรรฐรร(รชยรฎ6รญรรฐlรชOร"ร6รจ6รรรร"รรrรฎร6รจรฆยรฐยจK รต รรช รต

รท9รฆรฆOรง!รยขรฆรรรจรฆรรจ

รLรชOร"ร6รรรยขรยขรฆรร<รlร(ร6รยขรรร<รรรรรร"รJรชยรร

รยขรรรร"รญรฎรรร"รFรณ

ยฆรซรฆยฉKยรฆยรฐรฆยร รต รฆรร^รญรฆรรจ รยขรรถ6รฐรฆรรยขรฆรร<รรรรจ6รฆรรชOร"รฑรฒรถ9ร"รรรรร"ร รฑรฆยร รต ร!รจ รทรรยขรฆรรจ ร"ร ร รต ร"รยขรฆยรถรฐรรถ9รฆยรฐรรรฆรรITK รต ร(รช รต
รจรฆรรชOรฐรฆรรรรฆรรร รต รฆรชOร"รฑรฒรถรญรฆOรงรรPรฏรรAรยขร"รญHt
2 รรรฉยรยรฎร6รชOรรร"ร6รรญ?AB6รยรณ-CrรLรรKรยขรฆยรฐรฆรรยขรรรรฉยรถ9ร"รรKรรรยร รต รร]ร รต รฆlรยขรฆรรรฐรช รต
ยช3ยซ$ยฌ

ยญVยฎ3ยฏaยฐHยฑ

ยฒยณยดยยตยณ>ยถยทdยธ	ยนHยณ>ยบmยต-ยธยณjยธ	ยปdยผeยฒยณยด"ยฝQยผยดยยพmยด	ยณยฟยถHยผยฝรยนยตLยด	ยผรmยทmร
ยผรยยธยณรยธ	ยปdยผยจยตยผรยด"ร]ยปยยฒยณยด ยตยณ>ยถยทdยธ	ยนHยณ>ยบ9ยต-ยธยณยฉยธ	ยปmยผeยต	ยทdยฟ9ยพmยด	ยณยฟ9ยถHยผยฝ
ยนยบmร9ยทmร
ยผรjยฟรjรkยพ9รยด	ยธ	ยนรยทmยถรยดfยต"ยทdยฟ9ยตยผยธร>ยธ	ยปmยผ)ยด	ยณยณยธLยต	ยผยธรยธ	ยปmยนยต-ยบmยผรรยพmยด	ยณยฟ9ยถHยผยฝรรรยบsยฟยผ)ยตยณ>ยถHรยผรQยฟรjรยบรDยฝsยผยธ	ยปdยณTรร
ยนยบmรยถยทmรmยนยบdรjยปdยผยทdยด"ยนยตยธ	ยนรยตร
ร ยทmยด	ยธ	ยปdยผยด"ยฝsยณยด"ยผร5รxยผQยฝjยทmยตยธXรร9รรยธ	ยปmรยธรยธ	ยปmยนยตkยฝsยผยธ	ยปdยณTรรรdยผรยถยตkรeยนHยธ	ยปรยพmยด	ยณยฟ9ยถยผยฝQยตkยนยบยรVยปmยนร]ยปยยบmยณยธXรยถยถfยธ	ยปdยผ
ร
ยณ>ยบmยตยธยด]รยนยบยธ	ยตDยปmรรยผยงยธยณJยฟยผยยฒยทmยบ9ร
ยธ	ยนHยณ>ยบmรยถeรรยตjรยยผ;ยต	รรรยนยบยยธ	ยปdยผยยผ
รdรยฝsยพ9ยถยผ3ร]รรรVยปmยนยตรยนยตรยนยบรยณ>ยทdยดjยณยพ9ยนยบmยนHยณ>ยบรรยบ
ยนยบยธยผยด	ยผยตยธ	ยนยบmรยยนยฝQยพmยด	ยณรยผยฝQยผยบยธรVยฝQยณ>ยตยธkยพmยด	ยผรTยนHยณ>ยทmยตOรยยณยด	รยรdยผรยถยนยบdรยรVยนHยธ	ยปJยพmยด	ยณยพยผยด	ยธ	ยนHยผยตVยณยฒxยตยพยผรยนยรaรรรยถรยต	ยต	ยผยตeยณยฒ
ร
ยณ>ยบmยตยธยด]รยนยบยธ	ยตsรยต"ยต	ยทmยฝsยผยตjยธ	ยปmรยธQรยถยถxยธ	ยปdยผ;ร
ยณ>ยบmยตยธยด]รยนยบยธDยณยฒeยธ	ยปdยผ;ยบdยผยธยรยยณยด	รรยพaยณ>ยต"ยตยผยต	ยตDรยร>ยนHรยผยบยยพmยด	ยณยพยผยด	ยธยรรยรยบmร
ยณ>ยบmยถHรยรยพmยพ9ยถยนยผยต)ยนยบยงยธ	ยปmยนยต)รรยตยผร
ร ยณรยยผรยผยดรxยต	ยณ>ยฝsยผยยพ9ยด	ยณยฟ9ยถHยผยฝQยตรยด	ยผยฝQรยนยบ5ร ร ยนHยด"ยตยธร-รยบmรรยธ	ยปmยนยตยฉยนยตjรJรยถรยต"ยต	ยนรรยถxยพmยด	ยณยฟ9ยถHยผยฝรรยตjยฒรยดDรยตjยพmยด	ยผ
ร
ยพmยด	ยณTร
ยผยต	ยต	ยนยบdร>ยตxรยด"ยผkร
ยณ>ยบ9ร
ยผยด"ยบdยผรรLยนยบยรVยป9รยธxยผ
รTยธยผยบยธeยนยตยยธ	ยปmยผkรยพ9ยพ9ยถยนรรยธ	ยนHยณ>ยบยยณยฒ'ยธ	ยปmยผkยฝQยผยธ	ยปdยณFรยรยยผkยพmยด	ยผยตยผยบยธ)ยปdยผยด	ยผ
ยทmยตยผยฒยทmยถรรกรbยพรยด	ยธ	ยนรยถ)รยบmยตรยยผยดยรรยบรขยฟaยผ;ร>ยนรยผยบรขยธยณรฃยธ	ยปmยนยตsรคยทdยผยต	ยธ	ยนHยณ>ยบร;รยยผยยนยบmรdยผยผรรยต"รรยยธ	ยปmรยธUยณ>ยบdยผNยณยฒkยธ	ยปdยผ
รรdรรยบยธ	รรยผยตยจยณยฒfยธ	ยปmยนยตVยฝsยผยธ	ยปdยณTรNยนยต)ยธ	ยปmรยธVยธ	ยปdยผรร
ยณ>ยตยธeยณยฒ@ยธ	ยปmยผยฉยตยผรยด"ร]ยปยยนยตOรFยบdยณ3รVยบ;ยผรยด"ยถรยงยนยบ;ยธ	ยปdยผXยพmยด"ยณFร
ยผยต	ยตรmยณ>ยบdยผ
รรยบNยธ	ยปFยทmยต)ยผยต	ยธ	ยนยฝQรยธยผรยนHยฒ1ยธ	ยปdยผรร
ยณ>ยบยธ	ยนยบFยทmรยธ	ยนHยณ>ยบ;ยณยฒ@ยธ	ยปdยผXยพmยด"ยณFร
ยผยต	ยตVยนยต รxยณยด"ยธ	ยปรVยป9ยนยถHยผkยณยดeยบdยณยธร
รฅkยบยยธ	ยปdยผjยณยธ	ยปdยผยดXยปmรยบ9รรยพ9ยนHรยณยธkร
ยณ>ยบmยต	ยนยตยธยผยบ9ร
รยรยบmรยยธ	ยปdยผsรยต	ยตยณTรยนรยธยผรรร
ยณ>ยบmยต	ยนยตยธยผยบmร
รNยพmยด"ยณยพaยผยด"ยธ	ยนHยผยตeรรยบรยณยฒ
ร
ยณ>ยทdยด"ยต	ยผรยฟaยผรรยผยบmยผยด"รยถยนHรฆยผรยงยธยณUยบmยณ>ยบยงยฟ9ยนยบmรยด	ร;รงรจรฉmยตรTรยผยธร9ยต	ยณ>ยฝsยผXยพmยด	ยณยฟยถHยผยฝQยต รยพmยพยผรยดรfร9ยด"ยต	ยธยยณยฒ-รยถยถhรรaยบmรmยนยบdรDร
ยฝQยนยบ9ยนยฝยฉยท9ยฝรชยด	ยณFยณยธยยตยผยธยยฟaยผร
ยณ>ยฝQยผยต-ยผ
รFยพยณ>ยบdยผยบยธ	ยนรยถ'รยนยธLยนยตfยธ	ยปdยผVยต	รยฝQยผ)ยพmยด	ยณยฟ9ยถHยผยฝรซรยตLรยบmรmยนยบdรยจรjรฌ>รญ
รฎ ยณยฒยฝQยนยบmยนยฝยฉยทmยฝ
ยต	ยนHรฆยผยยนยบรยธ	ยปdยผsรยผยถรรยณยฒ)ยด	ยผยถรยธ	ยนยณ>ยบmรยถLรmรยธ	รยฟ9รยต	ยผยตร-รฏ'ยทmรร]ยปdยผยต	ยนยรฐรฑรฅkยตยฟยณยด"ยบรยรฒรณ>รดรต>ร]รยยฝsยณยด	ยผยณ3รยผยดร-รยตรรยยผยยปmรรยผ
ยตยผยผยบรยตยณ>ยฝsยผDร
ยณ>ยบmยตยธยด]รยนยบยธ	ยตยจยฝQรรNยฟยผรยนยบmรmยท9ร
ยผรNยฟFร;ยธ	ยปdยผXรยถยธยผยด"ยนยบdรQยพ9ยด	ยณFร
ยผยต"ยต	ยนยบdรdรaยนHยฒfยธ	ยปmยนยตVยนยตVยบdยณยธkรUยพ9ยด	ยณยฟ9ยถHยผยฝ
ยฒยณยดOยฟ9ยนยบmรยด"รยงรงรจรฉmยตXรยธ	ยปmยผรยบdยผรรชร
ยณ>ยบmยต	ยธยด"รยนยบยธ	ยตOรยด	ยผรยตยธ	ยนยถยถaยฟ9ยนยบmรยด	รmร]รdยนยบยงรถรยรยด	รยรงรจรฉmยตรTยต	ยณ>ยฝsยผรร
ยณ>ยบmยตยธยด]รยนยบยธ	ยตVยฝQรร
ยนยบmร9ยทmร
ยผยฉยบdยผรรยณ>ยบdยผยตยจยณยฒxรยด	ยผรยธยผยดXรยด"ยนHยธยรรรVยปmยนร]ยปJยฝQรรยยฒยด	ยณ>ยฝรทยตยธยผยพรยธยณNยตยธยผยพรยถHยผรรยยธยณ;รยบJยผ
รTยพ9ยถHยณ>ยต"ยนHยณ>ยบยยณยฒxยธ	ยปdยผ
รงรจรฉ;รยด"ยนยธยรร
ร ยนยบmร9ยนยบdรQรยบmรJร"ยปmรยด"รร
ยธยผยด]ยนHรฆยนยบdรยงรยถรยต	ยตยผยตeยณยฒ-ยพmยด	ยณยฟยถHยผยฝQยตOยปmรรFยนยบdรQยด	ยณFยณยธยจยต	ยผยธ	ยตVยณยฒLยต	ยฝUรยถยถ'ยต	ยนรฆยผยฉรยตeรxยผยถยถ1รยต
ยพmยด	ยณยฟยถHยผยฝQยตOรeยปdยณ>ยตยผยฉรยด]ยนHยธยรNรdยณFยผยตยจยบdยณยธeรยด"ยณรรรVยปdยผยบJรร"ยปmยนHยผรTยนยบdรยยพ9ยนHรยณยธยจร
ยณ>ยบmยต	ยนยตยธยผยบ9ร
รNรยด	ยผยฉยธ	ยปmยผยด	ยผยฒยณยด"ยผjยนยบยยธ	ยปdยผ
ร
ยณ>ยบยธ	ยนยบยท9ยนHยธยรยยณยฒfยธ	ยปdยผรรxยณยด"รยรยยผXยพmยด"ยผยตยผยบยธยผรยยนยบยงยธ	ยปmยนยต ยพ9รยพยผยดร
ร1ยณยฉร
ยณ>ยบmรยถยทmรdยผรยธ	รรTยนยบdรjยนยบยธยณรรรร
ยณ>ยทmยบยธยยธ	ยปmยผVยตยผยฝQรยบยธ	ยนรยตxยณยฒ5ร
ยณ>ยบmยตยธยด"รยนยบยธ	ยตยยตยผยผยฝQยตxยธยณรยฟยผVรยบยยนยบยธยผยด"ยผยตยธ	ยนยบdร
ยด	ยผยตยผรยด]ร"ยปQรยด	ยผรTรรยบmรรรยต-ยต	ยท9ร"ยปร>รยยผ)รรยบsยธ	ยปmยนยบmรXยณยฒ9ยผ
รTยธยผยบmรmยนยบmรยจยธ	ยป9ยนยต@รFยนยบ9รรยณยฒยตยธ	ยทmรdรยรร"ยปmรยด"รร
ยธยผยด]ยนHรฆรยธ	ยนHยณ>ยบQยณยฒ
ยพmยด	ยณยพยผยด	ยธ	ยนยผยต1รยบmรยฉยณยฒ9ยพ9ยด	ยณFร
ยผยต"ยต	ยนยบdร>ยต1ยตยพยผรยนยรรLยธยณรรkรยถรยต	ยต@ยณยฒ9ร
ยณ>ยบ9ยตยธยด"รยนยบยธ	ยต]ร'ยธยณkยณยธ	ยปdยผยด-รยถรยต	ยตยผยต@ยณยฒร
ยณ>ยบ9ยตยธยด"รยนยบยธ	ยตร

 รน
	รฝ'รป		
	
"!$#&%('*),+-.0/1.324'56'879/;:<2=5>2=)-)@?''8.BACD.E
รธรรนยรน)รบรปยรผยรฝhรพรธยรฟ

รOยปdยผยยผ
รTรยฝsยพยถHยผยยทmยตยผร ยนยบรยธ	ยปmยนยตQยพ9รยพยผยดUยนยตQยบdยณยธยยต	ยทmยนHยธยผรรขยธยณยยธ	ยปdยผยยนยถยถยทmยตยธยด]รยธ	ยนHยณ>ยบยยณยฒkยธ	ยปdยผยร
ยณ>ยฝsยพ9ยทmยธ	รยธ	ยนHยณ>ยบรขรยยผ
ยพmยด	ยผยต	ยผยบยธOยปdยผยด"ยผรmรxยผรยธ	ยปdยผยด	ยผยฒยณยด	ยผรยทmยตยผXรยบmยณยธ	ยปdยผยดOยณ>ยบdยผยร ร ยนHร>ยทmยด	ยผยตXรฒ Dยธยณยรฒ >ร]ร

GF

GH

IKJLNMOPRQPTSUJVWJ3XNYZ\[^]0_3`Oa3_\`;b8]0QMdceJ3XNYf*g
รฒรh ยณ>ยฝsยพ9ยทdยธยผjilkmonpoqororor\qsnt>
u รยธ	ยปmยผยจยตยผยธยยณยฒ5ยธ	ยปdยผยจยตยธยด	ยณ>ยบdร>ยถรQร
ยณ>ยบmยบdยผร
ยธยผร;ร
ยณ>ยฝsยพยณ>ยบdยผยบยธ	ยตยยณยฒYfr
v รh ยณ>ยฝsยพ9ยทdยธยผยรยด"รยพ9ยปwYZk ร=xqzyZ ร]รLรVยปdยผยด	ยผยยผรร"ยปยรยผยด	ยธยผ
ร|{UV} ยณยฒjx ยนยตยฉยธ	ยปdยผยยด"ยผรmยทmร
ยธ	ยนHยณ>ยบรยณยฒVร
ยต	ยธยด	ยณ>ยบdร>ยถHรยร
ยณ>ยบ9ยบdยผร
ยธยผรรร
ยณ>ยฝsยพยณ>ยบdยผยบยธ(n~o} รLรยบmรรยธ	ยปdยผยด"ยผยยผ
รdยนยตยธ	ยตDรยบยรยด"รยยฒยด"ยณ>ยฝ{ยO
} ยธยณย{$ย
ย ยนยฒVรยบmร
ยณ>ยบ9ยถHรยยนยฒ1รยบNรยด"รXยผ
รTยนยต	ยธยผร;ยฒยด"ยณ>ยฝยรsรยผยด	ยธยผ
ร;ยณยฒn } ยธยณUรsรยผยด	ยธยผ
รNยณยฒ>n ย ร
IKJLNMOPRQPTSUJVWJ3XยPsc_ย
J*O]0a\_ย"J3XNYZ\[ยQV`6Psc_]0JยJPยย
_PยJ3XNYยfg
F รh ยณ>ยฝsยพ9ยทdยธยผรยธ	ยปdยผยฉยตยผยธยยZยkmG{sยRยoqorororGqT{Tยย\รu ยณยฒ@ยธ	ยปdยผรยตยณ>ยทmยด"ร
ยผยต)ยณยฒYZ ร
F
ย ร)h ยปdยณยณ>ยต	ยผรยฒยณยดeยผรร"ยปยยต	ยณ>ยทdยด"ร
ยผย{TยUXย ยณยฒ>ยkZ รยบรยรยผยด"ยธยผ
รNยณยฒnยยย ร9รVยปmยนร]ยปยงรVยนยถยถaยด	ยผยพmยด"ยผยตยผยบยธOยนยธร
H รVรVยปdยผXยตยผยธ E ยพmยด	ยณTร
ยผยต	ยตยผรNยนยบยงยธ	ยป9รยธOรยรรNยนยต)รsยด	ยณFยณยธOยตยผยธOยณยฒYยFf ร
F
ย\ย0ย

ยยยยย8ย8ยยยย8ย8ยzยยยยยยยDย8ยกDยขยยฃ3ย8ยคBยฅdยฆ~ย*ยกDย1ยยย*ย1ยง8ยจยฉยยยชยยซย

ยฌ1ยญ

ยฌยด

ยฎdยถ

ยฎdยฏ

ยฎdยฐ

ยฎdยต

ยฎยญ

ยทยธยนยบGยปยธยฝยผ
ยทยพjยนยบGยปยยพ\ยฟTยปยร3ยฟTยปยซร3ยผ
ยท ร ยนยบGยปยร0ยผ
ยท ร ยนยบGยปยร0ยผ
ยทรjยนยบGยปยร0ยฟTยปยร3ยฟTยปยร\ยผ

ยฌ1ยฑ

ยฎยด
ยฌ1ยณ

ยฎยณ

ยฎยฒ
ยฌ8ยฒ

ยฎยฑ

รรยรร1รRรรรGรรรรTรoร,รรรRร1รรรTรTรยรร1รรรรรรรรร~ร1รGรรรTรGรรรรรรกรรรร~รGรDรRร
รขร

รขร
รขยพ

รขร
รขยธ

รฅ รค ยนยบGรฆzยธGยฟTรฆ ยพ ยผ

รฃรค

รdรรรร1รRรรรรรง1รยซรรTรoร8รjรจรฉ
ร8รรรรรRร1ร^รRรGร8ร~รรรGรNร
รยรฉร8รรรฉ
ร~รNรรรRรยรTรร~รยรรรGร

รช1รฌ

รช1รซ

รชยรณ

รชยรฑ

รช1รฒ





  


$



&

รช~รญ

รชยรฎ






(

)%

รdรรรร1รยรรร รรรTรoร8รยรงรฉ
ร~ร รรฉรกรร ร~รยรกยร~รก&รRรร
ร9รTรoร
-

.-

/ 01

!



% 

'



+*

รต \รถ Uรถ ยรท ยรถยรฝ Uรถ
รท\รพUรถยรน Kรท\รน \รพ
Uรท Uรถzรฝ ยรถzรพUรถยรน ยณ

  	 






รชรฐ
รช1รฏ

รดdรต
รถรธรท0รนDรบรผรปรฝรท\รพยรพรฟ รบยรถ
รพUรถยรน ยฒ รฟยรพ ยฎ ยฒ
\รถ Uรถ รฟยรพ
ยณ ยฎยณ ยฎยฑ ยฎยด
รท รฟรผรนDรต รพ 0รน Rรถ ยฎ ยณ
รถ รพdรท 0รฟรผรน





 


,

 

" #



ยฎยฒ ยฎยณ


 

!

24357698

:<;>=@?ACBCD9EFHGJIDKAC?ALNMO"PRQTS"OSPVUWIC?G)XYX9Z\[]+^R_`^R]bac]edgff)PhOSPYi
?I?lGJIC?"G)mnBC=`G)XKXKZBoNEp;c=@?AqBCDKEF`ALsrtIACBu`DKA'UvGwICE>E)B4AC?Bxi.G)FtyHD9z{ACE@utDKA|B k

j4k

?"rtIAqB|A k
~

?Bย=`AยA k
ย

U.ย

ยvย

E oB k

E oยB k

GJBยB k

?lAq?B4UeoN?"mnEยwยt=@Bq?yHDKAยGยICEcE)B4Aq?BL

GJBยG)FZ
ย)?ICBq?nย	DKAยGยy@?ACmn?F`ytG)FBยzRICEยeG)F	?X9?ยw?FBยE)zยU
ย

ย

FtD9BCD9EFยuTย

DYA'B k

?+ACG)ยw?ยAqBqICEF`ยX9ZHmnEF`F@?mnBq?yยmnEยยยยEF@?F>BยG)AยยhยยยยhZHy@?nr`ย

?FHGwy@?Amn?F`y`G)F>B4z}IEยยB k

ยยยยE.?XK?ยย?FB+DKFยกUยขยย?X9EF@ยA"BqE.B k
y`?F@E)Bq?|ยฆtยยB k
B k

?BยยNยยยย?bGย)?ICBq?nย	E)z
ย

ohEwยยEACACDKยtDKXKD9BCDK?AL
j

ย ยย:ยF
?X9?ยย?F>BยE)zยUWยย?X9EF@ยAยBqEB k

ย

DYA|Aq?Bl]b^}_t^}]baT]"i

?"ย)IGJย

ยงqยฌยญยฎDYF

DKA'?X9?ยw?FBย

?ยAqBqIEF@ยX9ZยmnEFtF@?mnBq?ymnEยwยยEF`?FBo

k	ยจยยฉ ยยยชcE@u7ยงยยDKAยF@E)BยGAqE=@Imn?ยE)zB k
A=`m k

ยจยยฉ

B k

ย ย

?ยฃACG)ยย?ACBqICEF@ยX9ZยคmnEF`F@?mnBq?yยฅmnEยยยยEF@?F>BยG)Aยย
k

DKm k

DKA|ย)IGJย

kยยซ

GJB'ยง%ย{DKA'Gby@?ACmn?F`ytG)FB'zRICEยยฏยงgยฌยญgย

I?ย`IC?Aq?F>B4ยงqยฌยญDKFHU.ยย:ยmmnE)Iy`DKF`ยยBqEwB k

mnEF>BCG)DKF`AยยNย%u>G)F`ybยง%ยยD9BCAIC?y`=tmnBCD9EFยDKF
mnEF`AC?;>=@?F>BCX9Z)uยB k

?B'ย|ยฌยญยฎยย?ยB k
ย

?B+=`A
ย

?"y@?nrยF`D9BCD9EFยฃE)zsB k

?IC?ย?nยTDKACBCAยGpACE=@Imn?

?ยย)?IBq?nยยฃE)z

?lIC?yt=`mn?yยฐย)IGJย

oN?lm k
ย

EAq?ยBqE

u@ยฑcF@E oยDKF@ยยB k
k

GJB4ยงย

DYAยGยy@?Amn?F`y`G)F>BยzRICEยvยงqยฌยญ%uยฒG)F>Z.?X9?ยย?F>BยE)zยฆtยhDKAยGy@?Amn?F`y`G)F>BยzRICEยWG)F>Z.?X9?ยย?F>BยE)zยยฆยยฌยญgu
G)FtyยฐB k

?IC?zRE)IC?lzRICEยยย|ยฌยญgยยยNยsDKA4mnEF`AC?;>=@?F>BCX9ZยGwy@?ACmn?Fty`G)FBยz}ICEยยณG)Fยฐ?X9?ยย?F>B4E)zยฎU.ย

UeDYA4GยICEcE)B4Aq?Bย
~

?ยF@EoยตA k

ยด

EoยถB k

GJBยUยทDKA|]b^R_`^}]baT]ยนยธ%^ยปยบRSยบKu>B k

?Bh=`Aยy@?F`E)Bq?ยยพ"ยฟรร Uยรรยย:ยACAC=tยย?|B k
ย

ยย?ยB k

?lAq?B4E)zยฎB k

?+ยชTBqICEF@ยX9ZยฐรhEF`F@?mnBq?y
ร'EยยยยEF@?FBCA4B k

X9?B'รยธRUรKยฝยยย?ยB k
ยพยAqE=@Imn?Aย

j4k

?ยAq?BhE)zยฒB k

?DKINIC?yt=`mnBCD9EF`AhDKF

?B{=`AVy@?F`E)Bq?hรยร"รpยธยปร`ยฝยB k

E)zยฒB k

?ยIC?y`=tmn?ypย)IGJย
k

u>B k

k

EAq?ยACD9ยผ?ยDKAยพJรยรรยพcย

GJBยmnEF>BCG)DKFHG)XKXยB k

ยจยยฉ ยยยhZpy@?nr7F`DKBCD9EFwE)zยฒU
uTB k

?IC?ย?nย@DKAqBCA4GJBยX9?G)AqB|EF@?"ACE=@Imn?ยยงqยฌร"DKF

ร รยยธRUpรYยฝร>รรยพJร7รรยพยฝxย'รhEF`Aq?;c=@?F>BCX9Z)u@B k
ย

?IC?ยy@Ec?AยF`E)Bย?nยTDYAqBยG)FZยIE>E)BยAC?BยE)zยACยwG)XKX9?IยAD9ยผ? ยฝxL

?IC?ย?nยTDKACBCAยGยICEcE)BNAC?BยUpรTo

o

ยจยยฉ

DKm k
k

?B'รยรยรwยธRUpรKยฝ
ย

?"?X9?ยย?F>BCA|E)zsU

?ยI?y`=`mn?yย)IGJย

y@Ec?A4F@E)B|ยย?X9EF`ยยBqEwรยฃยธRU

G)A

ร ยฝยยธรADKF`mn?

zRICEยรG)Fยฐ?XK?ยย?FBยE)zsรยฃยธRUpรYยฝhBqEwยงqยฌ ร ย

?IC?l?nย@DKAqBCAยF@EยยtGJB k

?ยACBqICEF@ยX9ZยmnEF`F@?mnBq?y+mnEยยยยEF@?F>BยฒIC?y`=`mn?ylBqEยยง ยฌร ยsร@ICEยB k
?IC?ย?nย@DKAqBCA'F@E+ยtGJB k

ร utG)F`y

kยยจ7ยฉยk

z}IEยยฏG)Fยฃ?X9?ยw?FBhE)zยฒU
ร

?ยy@?nrยF`D9BCD9EF

BqEbG)F?X9?ยย?F>B'E)zยฎรยรยรwยธยปร`ยฝxย

รhEF`AC?;>=@?F>BCX9Z)u@UรยDYA|F@E)BยGยICEcE)B4Aq?Bย
UeDYA|B k
ร

=tA4Gยฐ]b^}_t^}]baT]รICEcE)B4Aq?Bย

fJ]ยร7รKSCร^RPรรรยรBยDKAยB k

mnEยยยยEF@?F>BCAu@B k

ยด

?4ACG)ยย?ยG)AยB k

?4mnEยยยtXK?nยTD9BZยI?;>=`DKIC?ybz}E)INmnEยยยt=@BCDKF`ยยB k

GJB4DKA4รpยธยปรยรยกร{ยฝยยธ j

GJIรรCG)Fยu

ยรรJย ยฝxu7D9zยฎรlDKAhB k

?4AqBqICEF@ยXKZ+mnEF`F`?mnBq?y

?ยFc=`ย"ยย?I'E)zยฎ?y@ย)?A|G)F`yยรยB k

?ยF>=tย"ยย?I

E)zsย)?ICBCDKmn?Aย

Upรถ รจรฆ7รณรทรด{รฏ`รซ รงRรธยฎรนRรฉยครบwรปรฑ{รฉcรป

รก
รขYรฃยณรค+รฅVรฆยรงRรจรฉ
รฆ7รช4รซ รฅ{รฉ.รฌยรงYรญยรฆ`รซรฎรฏtรฐ{รฑรฒรคยรฆยรณ\รดsรตVรซรฏ@รซ รงRรฆ7รฐรรฆtรชยรฏ7รฐ
?ยF@EoWy@?ACmnIDKยย?
~

E)Iy@?IxDKF@ย@ย

?ยrtIxAqBยย`IC?AC?FBยB k
~

G)FรG)X9ย)E)IxD9B k

EovBqEยฅm k
k

ยรผB k

E>EAC?
B k

?
ยtD9ย)E)BยฃmG)F`y`DKy`GJBq?AG)F`yยถBqEรทmnEยยยt=`Bq?	G)FรUwยmnEยยยtGJBCDKยtX9?

?ยmnEF`y`DKBCD9EF`AB k

GJBpmnEยwยt=@Bq?AยยยE)B kรรฝ

?ยยtD9ย)E)BNmG)Fty`DKy`GJBq?ANยย=`ACBยACGJBCDKAqzRZ

รรพG)F`yรG)FรUpยmnEยยยtGJBCD9ยtXK?ยฐE)Iy@?IDKF`ย@ย

ยtD9ย)E)B4mnEFtACDKAqBq?F`mnZยฐDKยwยtXKD9?ANBohEmnEF`ytD9BCD9EF`A'EFยฐB k

ย ยย:ยF>Zpย'รฟ"DKF
DYA|B k

ย

Uรย+=`AqB'ยย?ยB k

oh?ยB k

?Fpย`IC?Aq?F>B

?Hy@?nr7F`D9BCDKEFรทE)z

?lยtD9ย)E)BCAL

?ยBCGJICย)?B4E)zVEF`?ยG)FtyEFtX9ZwEF@?ยยtD9ย)E)Bu@G)F`yยฐF`EยยJGJIDKGJยtXK?ยE)z{U

?ยBCGJICย)?BยE)zsGbย7D9ย)E)Bย



ย4ร%zยฎย

ร@E)Iย?G)m k

ยยย

ยซ

j4k

ย|รฟ4DKF

ยhรฟ+DKA|GยยtDKย)E)Bu`B k

ย

ย

U
u)oN?
k



?Fยฐย

Gย)?|BqElm k

ยDKA'ยย?z}E)I?ยย'รฟ+DKFยB k



?nย@DKAqBCAutADKF`mn?ยย|รฟยy@Ec?A4F@E)Bยยย?XKEF@ยยBqEwUยยฝxย
ohE=`XKyยฅยย?ยฃDKF,mnEF>BqIG)y`DKmnBCDKEF,oยD9B k



EcEAq?|GยยGJIxDKGJยtX9?Nย
.E)I?Eย)?Iu

mnEF`y`D9BCDKEF

	


ย

รฝ

ยDKF

?ยE)Ixy@?IDKF@ย@ย

ย

AqEยB k



GJBsย



ยขย|รฟ"ยธRย

ยF`?mn?ACACGJIDKXKZ

รยย+=`AqBยF`E)BยmnEFBCG)DYF.G)F>ZยmDKIm=`D9BuTo
k

DYm k

ยยฅ:รo'GZยฅBqEยคย`IC?ย)?F>BwmD9Im=`DKBCAยDKA+BqEยกยwGJIยฑ\?ย)?ICZ

!"#%$'&()*)*+,-./
021435146872149;:=<=>?<=@31	AB<	C(9D72EF02143514687G<HC*1IJ:=<=>?<=@)351K<	LMENC*OH7QP*1!R(C(LS<=>T	149DENC*140AUIUP(?6PV?0W7QP*1
7Q<=>QO	17E	XY<ZX[RC(687Q?5ENC(<	3)68ENC(0272><	?C/7IUP*EN021\E	>?5ON?CW?0<	35>Q14<	9*]SLS<=>T	149_^a`BP(?0bX[RC(687Q?5ENC(<	3)68ENC(0272><	?C/7
?07QP*14CG?C(63R9*149-?C/72EdcZedA<	C(9G?57Q07Q<=>QO	17f[7QP*1gC*1I;:"<=>?<=@351ihb?07QP*14CGLS<=>QT	149_^a`BP(1\0217cZejI1
E	@(7Q<	?Ck7QP(?0lI<4]k?m0U7QP*1d0217nE	Xa7QP*1po?5:	E	7n6<	C(9(?m9(<=72140lR02149K@q]%7QP*1po?5:	E	7n68ENC(0Q?m027214C(68]K<	35O	E	>?57QP(L
fr0214687Q?5ENCSsq^tsNh^a`UP(?0'0217u?C(9(R68140v<\o<=>Q7Q?m<	3/E	>9*1>wBxyENCdz{f[I1ENC(3]P(<4:	172E|<	99Z7QP(172><	C(0Q?57Q?5:?57}]
68ENC(0272><	?C7Q0~h^-ยCยE	>9*1>|72E0Q<=7Q?02X[]7QP*1-68ENC(9(?7Q?5ENC(0nE	Xย\18ย)C(?57Q?5ENCยsq^tsqA'7QP*1W<	0Q0Q?5ONC(LS14C7gE	>9(1>?C*O
68ENC(0214ยqR*14C/7Q35]ยP(<	072EW@ย1|<-3?C*14<=>18ย7214C(0Q?5ENCKE	Xv7QP(1|o<=>Q7Q?<	3ยE	>9*1>|wBxยo>Q18ย(ย149G@/]GยK^ยl687QR<	335]	A
68ENLMoR(7Q?C*OMwBxF?0aC(E	7aC*1468140Q0Q<=>Q]ยยucZeย?m0u0QRยW6?514C/7u72E68ENLMo)R*721B7QP*1U3m?C*14<=>u18ย7214C(0Q?5ENC_^u`BP(1>Q1B<=>Q1
C*ES68ENC(9?57Q?5ENC(0ENCย7QP*1g:=<=>?<=@)35140E	Xvยkย*7QP*1<	35O	E	>?57QP(Lย6<	Cย7QPqR(0@ย1|9*1468ENLMoยEN02149%?mC72Ed7ยIEW0Q721o0ย
ย ^-ยย2ยยยย[ย/ย%ยยยย
ย R(LZ@ย1>BXย>ENL ย 72ESยยยขยก ย!ยก7QP*1g:"<=>?<=@35140E	X'ยJ<	C9%LS<=>QTยฃ7QP*14L
ยค ^-ยย2ยยยย[ย/ย%ยยยzยฆยฅยย
ยง 1oย14<=7
ยจ P*EqEN021g<MC*1IยฉR(C(LS<=>T	149G:"<=>?m<=@351\ยชยซZ?Cยฃzยฌยฅ!ยJ0^ยญ7^a7QP(1>Q1n18ย?0Q7Q0<MLS<=>QT	149ยยชยฎd0^ยญ7^
ยชยฎยฏยฐยชยซ
ย R(LZ@ย1>B<	C(9%LS<=>TWยชยซ
ยฑ C/7Q?3ย<	33ย:=<=>?<=@3140<=>Q1LS<=>QT	149
ยฒ ?5ONR*>1 ย4ยณ o(>Q140214C/7Q0<	CW<	3O	E	>?57QP(Lยด68ENLMoR*7Q?mC*O@ยE	7QPW7QP*1\0Q17E	XYo?5:	E	76<	C9(?9(<=72140cZeยต<	C(9ยฃ<	C
ยSยถย68ENLSo<=7Q?5@351l<	0Q0Q?5ONC(LS14C7aE	>9*1>?C*O*^uยทY17bR(0a>1oย14<=77QP(<=7<	C/]S3?C(14<=>a18ยq7214C0Q?5ENC-?m0Q0QR*149MX[>QENLยธcZe
?0<	CKยSยถย68ENLMo<=7Q?@351gE	>9*1>?C*O*^
ยน `UP*1|0217ยบ(ยปNยผ/ยฝ(ยพยฟW68ENC7Q<	?C07QP*1|:=<=>?<=@31407QP(<=7UP(<ร:	1<	35>Q14<	9*]ยฃ@ย114Cย72>Q14<=72149
ยน `UP*1%0217Mร*ยพร/ร/ร*รร/ร/รรร)ร/ยพ!68ENC7Q<	?C0M7QP*1KC*18ย7M:=<=>?<=@35140p7QP(<=7W6<	CF@ย1%6P*EN0214CยA7QP(<=7W?0d7QP*1
RC(LS<=>QT	149WENC*140IUP(?6P-<=>Q1\7QP*1\7Q<=>QO	17E	Xย<pX[RC(687Q?5ENC(<	3)68ENC(0272><	?C/7bIlP*EN021\E	>?5ON?CW?0L-<=>QT	149
ยน `UP*1ย0217Q0-ร	ยผ)ร	ร(รiรBร[รNรy68ENC7Q<	?Cร7QP(1ยฃE	>?5ON?C(0pE	XUXrR(C(687Q?ENC(<	368ENC0272><	?C/7Q0dIUP*EN021G7Q<=>QO	17Q0W<=>Q1
9(140Q68>?5@ย149ย<=@ยEร:	1Wf[ร*ยพ/รรqรรรqรรiรร/ยพh
ยน รรร%>Q1o>Q140214C/7Q07QP*1CqR(LZ@ย1>E	Xร7QP*16R(>Q>Q14C/7:"<=>?m<=@351g?CG7QP*1<	00Q?5ONC(LM14C/7E	>9*1>?C*O
ร ย=ย~ย2ยร4รrยยย8รรร
ยนDร 17UcZedย
ร ย	ย)ย=ยrรrยrย=ยยรNรร<lX[R(C687Q?5ENC(<	3N68ENC(0Q72><	?C/7vยชยฎยฏรยชยซ?0Y<	9(9(14972E\cZeV14<	6Pp7Q?LM1<lC*1Iร:"<=>?<=@351
ยชยซn?0a6P*EN0214C_ย'I1U7QP*1>Q1X[E	>Q1UC(1149S72Eo>QEร:	1l7QP(<=7<	C/]d:"<=>?m<=@351E	XยzรยฅWยยด?0b02143146872149ยQร/รNร4รrรรก
ENC681	AN<	C(9|7QP(<=7รC*ENC*1b?m0Y021435146872149ZX[>QENLยฉยk^ร`BP(1a:"<=>?<=@35140v<=>Q102143146872149X[>QENLยฉร*ยพ/รรqรรรqรรiรร/ยพA
IlP(?6PGENC35]ยฃ68ENC/7Q<	?C(0lR(C(LS<=>T	149G:"<=>?m<=@35140ย(0Q?mC(681|C*ESLS<=>QT	149K:"<=>?<=@351gLS<ร]ยฃ@ย1|R(CLS<=>QT	149
@)<	6QTยA'<	C9ย1:	1>Q]y0Q1435146872149y:=<=>?<=@)351S?0L-<=>QT	149_Aร1:	1>Q]ย:=<=>?<=@351S6<	Cร@ย1-021435146872149H<=7dLMEN027
ENC681	^
ยทY17UR(0C*EiIยo(>QEi:	1|7QP(<=71:	1>Q]G:=<=>?<=@351\E	Xรzรขยฅ!ยJ?0|รร4รrรฃ(ร=ร[รรครก-fr<=7B3514<	0Q7ENC(681ih021435146872149ยยb<=7
<	C/]7Q?LM1	Aยร*ยพร/รqรรร/ร/รiรร/ยพยฃ?0\7QP*1S0217gE	X<	3m3u9(?5>Q14687g9*140Q6814C9(<	C7Q0nE	X7QP*1SLS<=>T	149!:"<=>?m<=@35140
7QP<=7d9(EC*E	7p@ย1435ENC*Ok72Eยkย<=7p14<	6PDENR*721>M35E/E	oHE	Xl0Q721o ยค Aa<!C*1Iยด:=<=>?<=@351ยฃ?0p18ยq72><	6872149
X[>QENLยดร*ยพร/ร/ร*รร/ร/รรร)ร/ยพ<	C(9GLS<=>T	149_ย*Xย>ENLยธ7QP(1\9*18ยยC(?57Q?5ENC-E	XYยKA*<	33ย:=<=>?<=@3140bE	XยzรฅยฅkยยฌIU?33
7QP(1>Q1XยE	>1|@ย1g>Q14<	6P*149k<	C(9%?C021>Q72149ย?CGร*ยพร/รqรรร/ร/รiรร/ยพ(^
รฆ=รง=รจ

รฉUรชiรซยรฌ5รญ
รฎ(รฏ/รฐรฑรรฒ
รณรตรดรถ5รทQรถรธ	รนรถ5รบ4รธ=รทQรถรปNรด(รผ
รฝ*รพรฟ (รฏ

รฏ 	
รฑiรฎ รฏ
 รฟjรฏ/
รพ kรฑi
รฒ 
(')*+

!Nรฟ)รฑ=รฐรฑรรฒ#"$$%&

, .รท -0/ 13254687 -09รถรด:dรป<;>=

? ยรฑ(/รฏ รฝ(รพNรฟ*รฏAB @ = C
D EE	รฏGFIHJ'qรฒqรฝ(รพรฟ(รฏยตรฑiรฒ =
รฝ*รพรฟ&FIHLKNMPOFIHQ
C รฝ(รพรฟ(รฏRMTS
('
) รฎรฏรฟUF H ? รฑT
'()
O*VW
CU')
  รฟjรฏ/รพXF HZY F\[]>^N
I^ F_[ รฑCW'qรฒqรฝ(รพNรฟ(รฏ
OF [ 
U
 รฏ	
/
 รฑรรฎ
 รฏ
OFIHa
N! รฟรฑ	รฐรฑรรฒ#"dc%
รฏ=รฒ   รฟ
รฏ=รฒ ? ย รฑ รฏ

, .รท -0/Le 13254687 -09รถรด:dรป<;>fhgi=kjmlรปNรด(รผQรท.9 4n

=รฏ รฒ
ย'รถ: 4

รพ=รฒF`[ab @
=

C

รทQรถ5รปNรดยรป<;_o8p

o8p 
? ยรฑ(/รฏ รฝ(รพNรฟ*รฏAB @ f C
D EE	รฏGFrq รฑiรฒtsรพ	รฒ รฟ*รฏT)uรฏยตรฑ
  รฟCT)wvW*รฏ	
CE/รฑiรฎE/รฏ
รฝ*รพรฟ&Frq&KNMxOFrqy
C รฝ(รพNรฟ(รฏRMTS
('
) รฎรฏรฟUFrq ? รฑT
&(')
O*VW
CU')
D EE	 รฏGFrj
z รฑรรฒts รพ=รฒ รฟ*รฏ){(*
u รฏjรฑT
  รฟ)|vx	! รฟ)รฑ=รฐรฑiรฒ}"~C%
OFIz Y Frqy
 o8p
  รฟjรฏ/รพXFยq Y F_ย&_^$
r^ F_ย
ย รฑCyq
' รฒqรฝ(รพรฟ(รฏUC
OF_ยG
CU*
 รฏ	
CE รฑiรฎE/
 รฏ
OFrqy
&N! รฟรฑ	รฐ(รฑiรฒ#"ย%
รฏ=รฒ   รฟ
รฏ=รฒ ? ย รฑ รฏ
9ย- ยย1 lรป 6 / 4

รทQรธ=รทQรถ5รปNรด%รป<;รรธSรผย-รทBรป<;|/)รถย	รป	รท n 	รธ รดย(รถยย(รธ=รท.-4รผ}o8pยขรธ	รดยยรธ	รดย=ยย n รป 6 /รธ=รทQรถ 7 รนย-gรป<9ยย-09รถรด:

ยก

ย`ย<ยยยยยยยยยยยยย 7ยXn รปNรด(รผ2รท.9 4En รทQรถ5รปNรดยรป<;dรทยย-yรผ.-รทQรผ !	รฟ)รฑ	รฐ(รฑiรฒ}"ยc(%ย รธPย"รธ9รถรธ 7 รน- FIH รถรผ%รถรด(รผย-09Qรท.-ยยยรถรด
!Nรฟ)รฑ=รฐรฑรรฒ#"ยc(% รถ 6Q6 -ยย(รถmรธ=รท.-4รน ย รธ;[รท.-09 F H ย(รธ	รผ 7 -0-4รดVรด 4687 -09ย-ยยยยรทยย(รถรผSรปNรด(รน ย รธ//รนรถ-4รผJ;[รป<9-รทยย- 4 รดCย
6 รธ9ยย<-ยย F [<ย , รป ยEF [ รถรผรด- n -4รผQรผQรธ9รถรน ย รด 4687 -09ย-ยยยรธ;ยรท.-09 FrH ย

รณยรผรทยย(รถmรผBรธ	รผQรผQรถ:Nรด 6 -4รดรทรป<9ยขย-09รถรด:J=ยย n รป 6 /รธ=รทQรถ 7 รนย-ยฃ
รท ย=รธ 9รถรธ 7 รน-4รผรธ9ย-|รป 7 ยqรถรป 4 รผQรน ย รทยย*รปNรผ.-|รทQรธย<-4รดยง;d9Qรป
ยค ย-ยฆยฅE9รผ2ย
รธ ย(รถ 9ย- n รท n รปNรด(ยรผ -ยยฉ 4 -4รด n -|รป<; n รปNรดย(รถ5รทQรถ5รปNรดยedรธ 7 รปย<- ย
ย
ยชยซยยฌ

6

=

K รผ2รท.-0/ ยจS ย ยค ย-gรป	รทยย-09 n รปNรดย(รถรทQรถ5รปNรดยรถรผ

ยญยฏยฎยฑยฐEยฒEยณยตยดIยฒEยถEยทยขยฎยยทยยณCยธยถEยนยบยผยปTยฒEยฝยยพ_ยฟยถยนยณยฎยยฒยถรEรยยดยรยญrยท

ร รรรรรยรยร(รยรยร(ร
`
รIร<รยรรร.ร.ร0รรรรรยร<รรรยรรQรยร<รaรรร.ร.รยรรรรยรรยร<รรรยรรร(ร.รรรรขรกร0รฃ*รTรรยขรคยรรฅEรฆรJรงIรจรฉ|รQรรยรชยผร<รกรซiรกรรรฌรฅรญร0รรรคร0รฉ>ร<รกรซ
รฎ ร<รยรยร<รยร]ร รฎ รครยรLรฏยรEรกรQรรยรช<รยรซรฐaรซรครยรยรรรยรซรยรยรรรยรกรซร<รกรยร0รฉยรซรรร.ร(รQรยตรฑQรฏรณรฒยจรฐaร<ร{ร0รยขรรยรคร(รกรLรฏยรซEรรยรคยรกรดรตร.ร.ร0รรถรฉ
รรรรร(รกEร.ร.รยร<รคยรกรยรคยรQร<รซEรซรยรซxร.รรรทรนรธรบรฃ#รEรคยรยรxร<รฆยร.รiรยรยรปรรครยรยรรผรฑQรฏรณรฒยจรฐ.รฐยขรฝXรพ}รรยรร<ร0รยร<รฆยรฆ#รยรคยรaรยงรรร(รaรรฆรรรฟCรคยร Pรคยร
รยรร0รยร รฎ ร<รยร
รฑQรฏ
รฃ#รร0รยร





รจ

รฏรณรฒ	
 rรฏรฐ
รฐ.รฐ&รฑQรฏTรฐ

รฏรฐrรคยรrรยรรรฌร.ร0ร}ร รฎ รยรรรรซรคยรยรยรรร}รซรยรยรรรยรกรซEร<รกรยร รฎ รยร(ร


รงrรจรฝ

 "!$#%"&(')%"*+%",.-

ร รฎ รยรรCBIร(รaรEรร.ร0รEDCร0รครยรกรรรGF|รรฅรญร<รยรร.ร<รHยผร รฎJI ร(รกรLK
รรญรยรฆยรฆยรคร0ร รฎ ร<รrรยรรยรครIรรยรฆร รฎ รรฆร<รซรรคยรรร<รฉร<รกรซรรยรรยฆร<รกร(รกMรaร(รEรยฏรยร0รรคร0รฃIร0รยร รฎ ร<รrรยรรยรคยรIรรยร รฎ รรฆรรร(รQรaรยรกรยรยร<รกรซ
รยรรด<รด<รยรยรยรคร(รกร0รฝN/ยร<รฆยร.รรรยรร<รกรชPOรรกEรกรรฌรrรรยร<รฆยรฆร0รยฆร<รกR
รซ Q_ร<รยร0ร<T
รฆ S(รรEM
ร รฎ ร<รรรรญร(รฆยรคยรยรรคยรกรดรนรยรรVUยฏรกรด(รฆยรคยรยรยรฝ3รพ}รร
ร<รฆยรฆรรยรฆยร{รยรซJรคยรQรรยรยจร<รrรยรรคยร_รEรรรญร0รยรฝ>รพ#รรคยร>รยรยร.รยรรยรยขรaรฃrร<ร`รEรรยรยรคยร<รฆยWรฆ รฌรยรรEร{ร<รยร.รยรซ8X
รฅ รนรยรZ
ร Y[:=2[\]>@< รยรยร.รยรรยรยขร
รรยร_^.รยรรรG03257Y`baMc[d?Aรนร รฎ รยรรV2e6?:)7รฝ
/ยฆรฃ#รคยรยรยผร.รยงรยรร<รกรชยผรยรร103254)6798;:=<?>@6)8=7Aรผร.รยร<ร

%[gL%[h%"b%"f

รIร0รยรด<ร<รฉ5Bยฆรฝwรฏรณรฒ9ij_kรฐยขรฝmlNn]o ร"pCรrqaรยรTsรรยขรร

ร pCรrq N
nut.n]o "
รฝ vยฆรรกรCรซยรฉ[Q_รรยรคยร0รฝ

รIรยรยรยรคuร0w รยร<รฉMBยฆรฝรฏรณรฒ9i.i_xรฐยขรฝXOรรยรyK$รรร(รกรยรคยร.ร.รยรกรrรร<รกรซ8รรยรyK$รรร(รกรยรคยร.ร.รยรกรrรรรด(ร<รคยรกยรฝbz{n รยร |}ยร o .ร ~ยEร$รรรยรยฑร t รre} ร รฉ[ย.ย|รฏรณรฒยจรฐยขรฉ
รฒ jยiยย9รฒ iยkCรฝ
ย
รIร<รยรกรคยรกรดรฉยO8รฝรฌรฏรณรฒ9i.ยCรฒยจรฐยขรฝkรพ}รรQ\รยร<รด<รยขร<รQรQรคยรกรดF|ร<รกรด(รรรด<รยO5ร.รรญรยรรรยรรขร รฎ รพ}รEรคยรกรดFwรรฅwรฉยOยBrร(รกร.ร.รยร<รคยรกรLK
ย รยรครยรกร.รยC
ร ยยย nLo [q o }ยจรยรยยร [q8ยร ยย n ร tnLo รJรJร tยย?o  tย[o9t รrq o ;ย
รซ DCรคยร8รรฆยรรยรคร(ย
รก Fwรรฅรญร<รยรร.ร<3ร <รฝยz ?
ยย
ร qรร$รรm
ร q ย
รฉ ย>รฏ xรฐยข5
รฉ ย.ย.ยยยXย.ยjรฝ
BIรร<รรญร0รยรฉ I
zยn

รฝ B5รฝรฉbBIร(รรยรกยรฉvรนรฝ.OรฌรฝรฉXยยS<รยรยร<ร(รกEร0รฉMQ>รฝMยรนรฝรฏรณรฒ9i.i_xรฐยขรฝ?Brรรรยร<รรร.ร0รยรคยรยรคยรกรด5ร.รยร<รรรยรรฅEรฆร}รรร(รกEร.ร.รยร<รคยรกรยร0รฝ
ย
รยร |}0ร o ร;~ยร$รรรยร ร t รe}ยขร รฉย.ยรฉ;ย_xMj9ยXย.ยCรฒรฝ

vยฆรยรCรคยรซยรฉยQ|รฝ3รฏรณรฒ9i.i.ย(รฐยขรฝยยกรรยรก
ย

ร }ยขรยร3ยร t qaรLยข{~ยยฃยร z
n ย

รฎ รรกรรรยรคร(รกร<รฆ_ร<รกรซiรฅEรค ^.รยรรรยรคร<รQรรร(รกร.ร.รยร<รคยรกรยรรรยรรช<รยรRBZDbQXรรญร(รฆWCรกร(รQรคยร<รฆ รฝV/รณรก
~ยยค]ยฅ ย<รฉรรwรฝEรถ<_
รถ x_ยรถ<.รถ i?
รฝ Bยรร<รรฌN
รฅ ร0ยฆ 3ร <รฉ"ยงรยร<รกรรร<รฝ

รฎ ร<ร	BIร(รกร.ร.รยขร<รคยรกรQ3รยรรรรยรยรยรคยรกรดร>รIร<รยรช9^ยรรaรรคยรกรดรฉยFwรยรรยรกbK
รคยรกรดรฉร<รกรซ$Brรรยร.ร0รZv5รยรรร(รaรรญร(รยรครยรคร(รกยรฝzยn รยร |}0ร o ร;~ยร$รรรยร ร t รe}ยขร รฉ"ยฉ"ยช>รฏuย(รฐยขรฉรญรถjยยยยXยCรฒยรถรฝ

v5รยรยรร.ร0รยรฉbยจรรฝEรฏรณรฒ9i.iยkรฐยขรฝ?U3รกEรร<รกรรรยรaรยรกรDCรยรรยรaรยร

v5รยรยรร.ร0รยรฉeยจรฌรฝwรฏรณรฒ9i.i<รถ(รฐยขรฝยงรยร(รkรฆรCร0ร<รฆรญร.รQรด(รฆร<รฅEร<รฆยรรร(รกรยรคยร.ร.รยรกรr<รฝz{n

รยร |}0ร o e
ร ~ยEร$ร0รdรยฑร t รre} ร รฉ)ยย>รฏรณรฒยจรฐยขรฉ;ยj9ยรฒkยjรฝ

รฅร<ร.รยรซรตรรยรรยรคยร.รยรคยร0ร รฎ ร<รยฆรรร(รกร.ร.รยร<รคยรกรรรยรรยรคยร รฎ ร<รรรยรคร(รกยตรรยร<รฅยฎK
ร ~ยEร$ร0รdรยฑร t รre} ร รฉ?ย9ยฉrรฏรณรฒยจรฐยขรฉ_รฒยยXย.ยรฝ
รฆยรยรQร0รฝz{n รยร |	}0ร o ;

v5รยรยรร.ร0รยรฉ?ยจรรฝรฉ=ยยซQ|รยรรยขรฆ 5
รฉ Sรฝ>รฏรณรฒ9i.ย.ย(รฐยขรฝยญยฌรร0ร$รฃIร<รยรชยK

v5รยรยรร.ร0รยรฉ5ยจรรฝรฉeยยฏQ>รยรรยรฆ ;
รฉ Sรฝwรฏรณรฒ9i.ย.i(รฐยขรฝยรพ|รยร0รรฌร0รฆยรEร.ร.ร0รยรคยรกรด

ร e}
t r

ร รฉ)ยยยฐ>รฏuย(รฐยขรฉ=ย.ย.ยยยXย.ย.ยรฝ

ยงรยร0รยรยร<รกbK$รIรยรกร.ร(รกยรฉ5รรรฝNยฌ8รฝรฉNย

ร<รยขรครยรกร.รยรซLรฆยร<รกรด(รรEรรด<ร<รฝ/รก

รฎ ร<ร{BIร(รกEร.ร.รยร<รคยรกรZยฌ#ร0รNรฃยฏร<รยรชร0รฝZz{n รยร |	}0ร o ;
ร ~ยEร$ร0รdรยฑร@ยค

รยฏร<รยขรกรคยรกรดรฉZO8รฝยฆรฏรณรฒ9i.i<รถ(รฐยขรฝยฑ/รณรกร.ร0รด<รยรรยรคยรกรดWรรร(รกEร.ร.รยร<รคยรกรยรรผรฃ#รครยรGร<รก]ร<รฅb^.รยรรรLK
ย n ย
ร }ยขรย3ร ยร  t qรน]ร ยข{ยฒรN
ร ยณยญยณ ยJยคLยฅยยด รฉรรZรฝE.รถ ย.ยยย.รถ ย.ยรฝ
ยตยยถยท

ยธยบยนยยป;ยผWยฝ

ยพยฎยฟHร9ร"รยฎรยฟ9รยฎรZร"ร{ร=รLร9รรยรรยร	รXรbรยรHร[ร9รHรWร9รรยฎรยร	รร[ร]ร]ยฟยร.รรยรNรรbร[ยฟHร9ร3รHรWรร[รร{รรยรEรmรbร[รuร3รกรขรฃรรฃรยร[รคVรLรฅยญรขรฆbรงยรจยร?รฉรชร
รซXรฌ รLร.รยรยร?ร.รญ.รยรฎXร.รฏ.รฏXร

 รงรeรยรงZ รงyรค9รงHรกยรผ]รrรฆยร
 
{รยยฟ3ร
	รรธ3รบeร รรฒรณรWร ร		รร.รร รฐ ร	รด(รยรLร9ร.รยรยรLร[รธrยฟHร9ร.รHรร[รยร]ยฟHรร$รHร9รยยฟ3รธ3ร รรธรWร9ร[รธrรร.ยฟCรธrรร[ร]ร]ยฟ3ร.รรMร
ร3รยรHรรรฃ ร.รธrรHรWรร ร[ยฟ3ร.รฑ	Wร9รทยรร(รจ{รผrรขรฃร รพ	รรuรกยรฝ;รฟยร"รขรงรฝ@รฝ ร  รงrรeรyรงrร รฌ ร.รฏ.รตยรฎXรตbร9รตXร
ร.ร"รHร]ร9ร=ร รWร ร ร.รร?ร!ย รWร#ย
" รรยฎรร[รWรยฟ9ร	รน{รWรย
$ ร	 รยยฟ3ร9รทPรรฒรณร % ร{รWร# รNร.ร]ร]ยฟHรยฎรJรน{รยบรLร9ร.รยรยรร'&("*)+-,/.
, รHรHร ร]ร]ร9รรชรยฎร9รHรWรร$ร
 ร5รร[รHรร[รVร]รbรยรHร[ร9รHรรยร	 ร.ร[รร1m
{
0 รงย
2 รปยฎรยรbรผยรeรก.รฝ รLรฅCรรฆยฎรงrรEร รคrรขรฃรผ4ย3 ร รฌ รLร รยรTร.รฏ.รยรฎ
รฐ

รร"ร]รฑ5รยฟHรยฎรMรฒรณรMรด ร5รLร9ร.ร.รตรยรTรถ{รXร[ร.รทGรรธยบรนNร.รธHรบXร]ยฟ3ร.รธ3รบXรร[รยฎร รปยฎรยรbรผยรeรก.รฝ=ร]รฅJรจ{รผrรขรฃร รพรรuรกยรฝ[รฟยร"รขรงrรฝรฝ ร
รฌ ร .รญยรฎ ยรฏXร

รรยรฏXร

5รWยฟ3รร[รHรรรJรด รรฒรณรยญรLร9ร.ร.รตรยร ยพ"ร.ร]รCร"รยยฟ3ร
		Wร	ยบรธrรร[ร]ร]ยฟ3ร.รรMรCรHรยรHรรรฃร.รธrรHรWรร=ร รจ{รผrรขรฃร รพร9รรฃรก.รฝNรฟยร"รขรงรฝ@รฝ ร  รงrรeรyรงrร76  ร
รM
 ร9รฎeร9รฏbร
รด)ร"รธรธ3รยฎร9รHร รTร{ร)รด รWร 9ยญ
8 ร]รฑ5ร.ยฟ3ร=รยร;รรด(รรLร9รรยรรยร$รNร.ร"ร[รร[รยร]รยรบ.รรbร:ร.ยฟ ยฟ3ร	รยรHรWรร[รรRรป<;รร.ร=;รbรขรงรผยรกยร?>
@ ย3 รคrรขรงrร @ รร รงrร;รยรงyรคyร รฌBA รC รยรD รEย รฎF รยรXร
รฒ ร.รธHรบF
G ร.ยฟHรHร?รV
, รHย
5 รZรLร9รร.ร.รยร รNรร[รHรร]ร]ร9ร"รธrร1รรIยบ" รรJG ร.ยฟ3รบXรยร
*Z ร	 รยรHรรร[รร รจ{รผrรขรฃร รพรรuรกยรฝ รฟยร"รขรงrรฝรฝ ร  รงrรeรyรงrร
K รLรยรยร=ร.รยรฎeร.ร9รXร
รฒ$รรยฎยฟ9ร#
 รWร!Lย
 ร9ร[รยฎรยฟ3ร]รร?ร!
) รNร{รยรLร9ร.ร.รฏรยรMยบ, ยฟ3รธ$ร.ร[รN รยรHร+ร	รร"รHรร]ร]ร9ร[รธrรยบ รPXO รร3รWร]ร9ร=รรจ{รผyรขรฃร รพร9รรฃรกยรฝ
รฟยรeรขรงrรฝรฝ ร  รงรeรยรงร รซK รC
 รยรD
. รญยรฎF. รต.รตXร
รฒ$รรยฎยฟ9ร+ รWรย
 รฒ$ร.รHร ร[รuร รฐ รยญรLร9ร.ร.รรยรQย
 ร[ร[ร[รร[รยร
 รธรWร9รMรRW	 ร รยยฟยรธ รธrรร"รHรร]ร]ร9ร[รธrร.รS ร @ ย3 ร"รข รกยร9รขรฃรuรรณรกยร?>
@ รขรฃรผยร[ร9รขรฃรbรผ]รกยรฝDยบT รก.รขuรขรงrรผyรUZ รง3ร3ร  ร[รรขรฃรuรยร[ร;ร[ร?รVb รยร9รฎF. รตbรยร)รXร[ยฟยรรยฎร.รยฟ%W$ รยฟX	 รยรยฎรbรน	รยฟX	 รรYยบ ร9ร รยฎรW	 รฑ5รยฟHรยฎร
รฒ$รรยรHร.ร[รยยฟยรuร[V
Z ร รLร9รร\M รยร]ย
" รร^	G ร.ยฟHรบbรร
Z ร	รร[ร]ร]ยฟ3ร.ร รยรHรP.(รฃ ร[ร[ร[ร.รทยร9รยรHร
T	 ร[ยฟ3ร.ร;รยฟ3รHรWร9รยร.ร[รยรยร[ร	 รรธรยรHรWรร[ร
ร]รGร"รรธrรHรยฎยฟHรยญร"ยฟHรXรธrร9ร3รHรรยฎรยฎร รฟยร9รฅรยรผyรยรกรขรฃรรฃรยร @ รร รงrรeรyรงyรคyร A รLรยรยร=ร.รญยรฎeร9รต
X
ร
รฒ$ร.รยฟ3รรTรนยร[, รWร รฐ รร[ร]ร.รยรถmร_, รWร`Q(
$ ร.ร[ร[รยฟ75a ร.ร[รยฎร9ร?ร)รน{รรLร9ร.ร
 รยรPรถยร9รธP	 รยยฟ3รยรHรb.O รGร"ยฟHร.ร.ยฟ3ร.รทยรทยร รยฎรPรรยร
ร"ยฟHร.ร]ร.รรXร5ร
% รร[ร]รHร.ร[รธrร รHรXร]ร]ร9รทcย
. ร.รฑBLd ร9รธrรe% ร.ยฟ3รร9รยร]ร9ร ร[ยฟHร.ร.ยฟ3ร.รทยรทGรรยฎรfยบG รWรHรยฎรรยฎรgZG ยฟยรWรHรรยฎร รทยรรHรยฎรbร[รร
 รUJT รผLร9รyรง3รงXย> ร@ร  รคEร]รฅihjhT @<k รจjlm รซ รยฎร[ร)ร?ร9รE_ รฎF
b ร?($ ร.ร"รธrรรB.O รยฟ9ร;รNร.ร"ร.ร[รbร
L
(ยฟ3รรHร]รยฟ9รย รMรLร9ร.ร.รตรยร'ย
 รMรฑ"ยฟ3รรยญร
W	 ร.ร.ยฟ3รรHร[รทยรD ร.ยฟ)รHรยฎร รธrรร"ร]ร]ยฟ3ร.รรMร)รHรยรHรรR ร.รธrรHรWรร ร[ยฟHร.รฑ	 ร9รทPร5รร.ร;= รbรข รกรขรฃรรฃรยร;รกยรฝ
รฟยรeรขรงrรฝรฝ ร  รงรeรยรงร[
m รuรตรยรD. รฏ.รยรฎF. ร.รXร
)ยรยยฟn]d ร.ร?ร+
 รรLร9รร รยร รถ{รร[รHร<%C"o ยฟ3รHรCรHร9รยยฟ3รธ3รยร.ร[รp	 ร รยฎร9รยยฟ รฐ ยฟ3รยร"ร ร
W	 ร.ร.ยฟ3รรHร[รทยรร @ รฟ]รจ{รฉ รปยฎรยรbรผยรeรก.รฝยญร.ร
รร.ร;
= รbรขรฃรร  ร รฌ รC รยรรย รฏยรฎeร9รฏb ร
Oยร.ร รน	รรรบ;รq ร(รLร9ร.ร
 รยร/ย
8 ร$รHรยฎรEรฒ$ร ร[รรทยร
	 รรรCร.ร[ร$รถ{ร9รธrรรทยร5รรHรยรฑeร	 รWร รPร
	 ร	รร[ร]ร]ยฟยร.รรยร(ย
" รร^	G ร.ยฟHรบXร9ร*L ร
T รผLรยรยรง3รงXย> รร  รคยรLรฅ{รจ{รจยรจ{รฟ4lem รซ รยฎร[ร?ร<
M ร9รฎย รญ
X ร)รbร.รr รร]ร.ร;รNร
	 ร@ ร.ยฟ3ร"รรbร
J
Oยร.ร1รน	รรรบeรHย รWร  รถ{ร9รธ3รMร]รยฟ9รs
 รรLร9ร.รEM รยร$รNรร[ร]ร]ยฟ3ร.รรMร:Z) รรรยรHรยฎร9ร3รj.O รยฟ3รHร[ร รฐ W	 ร.รฑ"ร
		 ร	รร[รHรรHร]ร9ร[รธrร.ร]L ร
T รผLรยรยรง3รงXย> รร  รคยรLรฅ*tj*lm  ร"รน	รร[ร?ร รฐ รยฟ3รทยร.รMร.ร
J
$(ร.รuย
 ร9รMร]ร9รยฎยฟHรbรธHรบ;รFย รWรMรถ{รPbO ร	W	 ร.รE
& รWรFv)) ร9รยฎรยฎรยฎร{ร % รฒรณรeรLร9ร.ร
 รยรw+
, ร.ร9รยฎรยฟ3รรธJรยยฟ3รธx% รธrรร"รHรร]ร]ร9ร[รธrรยร
W	 ร.ร.ยฟยรWรHร[รท
ร.ร"ร รWรHรZร]ร5ร9รธรร
	 รWร9รยรHรWรร[รร รจ{รผrรขรฃร รพรรuรกยรฝeรฟยร"รขรงรฝ@รฝ ร  รงrรeรyรงrร[y A รCย
 รฎXรตรยรw. รbรยรฎXรต
b รยร
z{z

Journal of Articial Intelligence Research 2 (1995) 541-573

Submitted 9/94; published 5/95

Pac-learning Recursive Logic Programs:
Negative Results
William W. Cohen

AT&T Bell Laboratories
600 Mountain Avenue, Murray Hill, NJ 07974 USA

wcohen@research.att.com

Abstract

In a companion paper it was shown that the class of constant-depth determinate k-ary
recursive clauses is eciently learnable. In this paper we present negative results showing
that any natural generalization of this class is hard to learn in Valiant's model of paclearnability. In particular, we show that the following program classes are cryptographically
hard to learn: programs with an unbounded number of constant-depth linear recursive
clauses; programs with one constant-depth determinate clause containing an unbounded
number of recursive calls; and programs with one linear recursive clause of constant locality.
These results immediately imply the non-learnability of any more general class of programs.
We also show that learning a constant-depth determinate program with either two linear
recursive clauses or one linear recursive clause and one non-recursive clause is as hard as
learning boolean DNF. Together with positive results from the companion paper, these
negative results establish a boundary of ecient learnability for recursive function-free
clauses.

1. Introduction
Inductive logic programming (ILP) (Muggleton, 1992; Muggleton & De Raedt, 1994) is
an active area of machine learning research in which the hypotheses of a learning system
are expressed in a logic programming language. While many dierent learning problems
have been considered in ILP, including some of great practical interest (Muggleton, King,
& Sternberg, 1992; King, Muggleton, Lewis, & Sternberg, 1992; Zelle & Mooney, 1994;
Cohen, 1994b), a class of problems that is frequently considered is to reconstruct simple
list-processing or arithmetic functions from examples. A prototypical problem of this sort
might be learning to append two lists. Often, this sort of task is attempted using only
randomly-selected positive and negative examples of the target concept.
Based on its similarity to the problems studied in the eld of automatic programming
from examples (Summers, 1977; Biermann, 1978), we will (informally) call this class of
learning tasks automatic logic programming problems. While a number of experimental
systems have been built (Quinlan & Cameron-Jones, 1993; Aha, Lapointe, Ling, & Matwin,
1994), the experimental success in automatic logic programming systems has been limited.
One common property of automatic logic programming problems is the presence of recursion . The goal of this paper is to explore by analytic methods the computational limitations
on learning recursive programs in Valiant's model of pac-learnability (1984). (In brief, this
model requires that an accurate approximation of the target concept be found in polynomial time using a polynomial-sized set of labeled examples, which are chosen stochastically.)
While it will surprise nobody that such limitations exist, it is far from obvious from previous
c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Cohen

research where these limits lie: there are few provably fast methods for learning recursive
logic programs, and even fewer meaningful negative results.
The starting point for this investigation is a series of positive learnability results appearing in a companion paper (Cohen, 1995). These results show that a single constant-depth
determinate clause with a constant number of \closed" recursive calls is pac-learnable. They
also show that a two-clause constant-depth determinate program consisting of one nonrecursive clause and one recursive clause of the type described above is pac-learnable, if some
additional \hints" about the target concept are provided.
In this paper, we analyze a number of generalizations of these learnable languages. We
show that that relaxing any of the restrictions leads to dicult learning problems: in particular, learning problems that are either as hard as learning DNF (an open problem in
computational learning theory), or as hard as cracking certain presumably secure cryptographic schemes. The main contribution of this paper, therefore, is a delineation of the
boundaries of learnability for recursive logic programs.
The paper is organized as follows. In Section 2 we dene the classes of logic programs and
the learnability models that are used in this paper. In Section 3 we present cryptographic
hardness results for two classes of constant-depth determinate recursive programs: programs
with n linear recursive clauses, and programs with one n-ary recursive clause. We also
analyze the learnability of clauses of constant locality, another class of clauses that is paclearnable in the nonrecursive case, and show that even a single linearly recursive local
clause is cryptographically hard to learn. We then turn, in Section 4, to the analysis of
even more restricted classes of recursive programs. We show that two dierent classes of
constant-depth determinate programs are prediction-equivalent to boolean DNF: the class
of programs containing a single linear recursive clause and a single nonrecursive clause, and
the class of programs containing two linearly recursive clauses. Finally, we summarize the
results of this paper and its companion, discuss related work, and conclude.
Although this paper can be read independently of its companion paper we suggest that
readers planning to read both papers begin with the companion paper (Cohen, 1995).

2. Background

For completeness, we will now present the technical background needed to state our results;
however, aside from Sections 2.2 and 2.3, which introduce polynomial predictability and
prediction-preserving reducibilities, respectively, this background closely follows that presented in the companion paper (Cohen, 1995). Readers are encouraged to skip this section
if they are already familiar with the material.

2.1 Logic Programs

We will assume that the reader has some familiarity in logic programming (such as can
be obtained by reading one of the standard texts (Lloyd, 1987).) Our treatment of logic
programs diers only in that we will usually consider the body of a clause to be an ordered
set of literals. We will also consider only logic programs without function symbols|i.e.,
programs written in Datalog.
The semantics of a Datalog program P will be dened relative to to a database , DB ,
which is a set of ground atomic facts. (When convenient, we will also think of DB as a
542

Pac-Learning Recursive Logic Programs: Negative Results

conjunction of ground unit clauses). In particular, we will interpret P and DB as a subset
of the set of all extended instances . An extended instance is a pair (f; D) in which the
instance fact f is a ground fact, and the description D is a set of ground unit clauses. An
extended instance (f; D) is covered by (P; DB ) i
DB ^ D ^ P ` f
If extended instances are allowed, then function-free programs can encode many computations that are usually represented with function symbols. For example, a function-free
program that tests to see if a list is the append of two other lists can be written as follows:

Program P :

append(Xs,Ys,Ys)
null(Xs).
append(Xs,Ys,Zs)
components(Xs,X,Xs1) ^
components(Zs,X,Zs1) ^
append(Xs1,Ys,Zs1).

Database DB :

null(nil).
Here the predicate components(A,B,C) means that A is a list with head B and tail C; thus
an extended instance equivalent to append([1,2],[3],[1,2,3]) would have the instance fact
f = append (list12 ; list3 ; list123 ) and a description containing these atoms:
components(list12,1,list2), components(list2,2,nil),
components(list123,1,list23), components(list23,2,list3),
components(list3,3,nil)
The use of extended instances and function-free programs is closely related to \attening"
(Rouveirol, 1994; De Raedt & Dzeroski, 1994); some experimental learning systems also
impose a similar restriction (Quinlan, 1990; Pazzani & Kibler, 1992). Another motivation
for using extended instances is technical. Under the (sometimes quite severe) syntactic
restrictions considered in this paper, there are often only a polynomial number of possible
ground facts|i.e., the Herbrand base is polynomial. Hence if programs were interpreted
in the usual model-theoretic way it would be possible to learn a program equivalent to any
given target by simply memorizing the appropriate subset of the Herbrand base. However,
if programs are interpreted as sets of extended instances, such trivial learning algorithms
become impossible; even for extremely restricted program classes there are still an exponential number of extended instances of size n. Further discussion can be found in the
companion paper (Cohen, 1995).
Below we will dene some of the terminology for logic programs that will be used in this
paper.
2.1.1 Input/Output Variables

If A B1 ^ : : : ^ Br is an (ordered) denite clause, then the input variables of the literal Bi
are those variables which also appear in the clause A B1 ^ : : : ^ Bi,1 ; all other variables
appearing in Bi are called output variables .
543

Cohen

2.1.2 Types of Recursion

A literal in the body of a clause is a recursive literal if it has the same predicate symbol
and arity as the head of the clause. If every clause in a program has at most one recursive
literal, the program is linear recursive . If every clause in a program has at most k recursive
literals, the program is k-ary recursive . If every recursive literal in a program contains no
output variables, the program is closed recursive.
2.1.3 Depth

The depth of a variable appearing in a (ordered) clause A B1 ^ : : : ^ Br is dened as follows.
Variables appearing in the head of a clause have depth zero. Otherwise, let Bi be the rst
literal containing the variable V , and let d be the maximal depth of the input variables of
Bi ; then the depth of V is d +1. The depth of a clause is the maximal depth of any variable
in the clause.
2.1.4 Determinacy

The literal Bi in the clause A B1 ^ : : : ^ Br is determinate i for every possible substitution
 that unies A with some fact e such that
DB ` B1  ^ : : : ^ Bi,1 

there is at most one maximal substitution  so that DB ` Bi . A clause is determinate
if all of its literals are determinate. Informally, determinate clauses are those that can be
evaluated without backtracking by a Prolog interpreter.
The term ij -determinate (Muggleton & Feng, 1992) is sometimes used for programs that
are depth i, determinate, and contain literals of arity j or less. A number of experimental systems exploit restrictions associated with limited depth and determinacy (Muggleton
& Feng, 1992; Quinlan, 1991; Lavrac & Dzeroski, 1992; Cohen, 1993c). The learnability of constant-depth determinate clauses has also received some formal study (Dzeroski,
Muggleton, & Russell, 1992; Cohen, 1993a).
2.1.5 Mode Constraints and Declarations

Mode declarations are commonly used in analyzing Prolog code or describing Prolog code;
for instance, the mode declaration \components (+; ,; ,)" indicates that the predicate components can be used when its rst argument is an input and its second and third arguments
are outputs. Formally, we dene the mode of a literal L appearing in a clause C to be a
string s such that the initial character of s is the predicate symbol of L, and for j > 1
the j -th character of s is a \+" if the (j , 1)-th argument of L is an input variable and a
\," if the (j , 1)-th argument of L is an output variable. (This denition assumes that all
arguments to the head of a clause are inputs; this is justied since we are considering only
how clauses behave in classifying extended instances, which are ground.) A mode constraint
is a set of mode strings R = fs1 ; : : :; sk g, and a clause C is said to satisfy a mode constraint
R for p if for every literal L in the body of C , the mode of L is in R.
We dene a declaration to be a tuple (p; a0; R) where p is a predicate symbol, a0 is an
integer, and R is a mode constraint. We will say that a clause C satises a declaration if
544

Pac-Learning Recursive Logic Programs: Negative Results

the head of C has arity a0 and predicate symbol p, and if for every literal L in the body of
C the mode of L appears in R.
2.1.6 Determinate Modes

In a typical setting, that facts in the database DB and extended instances are not arbitrary:
instead, they are representative of some \real" predicate, which may obey certain restrictions. Let us assume that all database and extended-instance facts will be drawn from some
(possibly innite) set F . Informally, a mode is determinate if the input positions of the
facts in F functionally determine the output positions. Formally, if f = p(t1 ; : : :; tk ) is a
fact with predicate symbol p and p is a mode, then dene inputs (f; p) to be hti1 ; : : :; ti i,
where i1 , : : : , ik are the indices of  containing a \+", and dene outputs (f; p) to be
htj1 ; : : :; tj i, where j1, : : : , jl are the indices of  containing a \,". We dene a mode
string p for a predicate p to be determinate for F i
k

l

fhinputs (f; p); outputs (f; p)i : f 2 Fg
is a function. Any clause that satises a declaration Dec 2 DetDEC must be determinate.
The set of all declarations containing only modes determinate for F will be denoted
DetDEC F . Since in this paper the set F will be assumed to be xed, we will generally omit
the subscript.
2.1.7 Bounds on Predicate Arity

We will use the notation a-DB for the set of all databases that contain only facts of arity
a or less, and a-DEC for the set of all declarations (p; a0; R) such that every string s 2 R is
of length a + 1 or less.
2.1.8 Size Measures

The learning models presented in the following section will require the learner to use resources polynomial in the size of its inputs. Assuming that all predicates are arity a or
less for some constant a allows very simple size measures to be used. In this paper, we will
measure the size of a database DB by its cardinality; the size of an extended instance (f; D)
by the cardinality of D; the size of a declaration (p; a0; R) by the cardinality of R; and the
size of a clause A B1 ^ : : : ^ Br by the number of literals in its body.

2.2 A Model of Learnability
2.2.1 Preliminaries

Let X be a set. We will call X the domain , and call the elements of X instances . Dene a
concept C over X to be a representation of some subset of X , and dene a language Lang
to be a set of concepts. In this paper, we will be rather casual about the distinction between
a concept and the set it represents; when there is a risk of confusion we will refer to the
set represented by a concept C as the extension of C . Two sets C1 and C2 with the same
extension are said to be equivalent . Dene an example of C to be a pair (e; b) where b = 1 if
e 2 C and b = 0 otherwise. If D is a probability distribution function, a sample of C from
545

Cohen

X drawn according to D is a pair of multisets S + ; S , drawn from the domain X according
to D, S + containing only positive examples of C , and S , containing only negative ones.
Associated with X and Lang are two size complexity measures , for which we will use
the following notation:

 The size complexity of a concept C 2 Lang is written j C j .
 The size complexity of an instance e 2 X is written j ej .
 If S is a set, Sn stands for the set of all elements of S of size complexity no greater
than n. For instance, Xn = fe 2 X : j ej  ng and Langn = fC 2 Lang : j C j  ng.
We will assume that all size measures are polynomially related to the number of bits needed
to represent C or e; this holds, for example, for the size measures for logic programs and
databases dened above.
2.2.2 Polynomial Predictability

We now dene polynomial predictability as follows. A language Lang is polynomially
predictable i there is an algorithm PacPredict and a polynomial function m( 1 ; 1 ; ne ; nt)
so that for every nt > 0, every ne > 0, every C 2 Langn , every  : 0 <  < 1, every
 : 0 <  < 1, and every probability distribution function D, PacPredict has the following
behavior:
t

1. given a sample S + ; S , of C from Xn drawn according to D and containing at least
m( 1 ; 1 ; ne ; nt) examples, PacPredict outputs a hypothesis H such that
e

Prob(D(H , C ) + D(C , H ) > ) < 
where the probability is taken over the possible samples S + and S , and (if PacPredict
is a randomized algorithm) over any coin ips made by PacPredict;
2. PacPredict runs in time polynomial in 1 , 1 , ne , nt , and the number of examples;
and
3. The hypothesis H can be evaluated in polynomial time.
The algorithm PacPredict is called a prediction algorithm for Lang, and the function m( 1 ; 1 ; ne ; nt ) is called the sample complexity of PacPredict. We will sometimes
abbreviate \polynomial predictability" as \predictability".
The rst condition in the denition merely states that the error rate of the hypothesis
must (usually) be low, as measured against the probability distribution D from which the
training examples were drawn. The second condition, together with the stipulation that the
sample size is polynomial, ensures that the total running time of the learner is polynomial.
The nal condition simply requires that the hypothesis be usable in the very weak sense
that it can be used to make predictions in polynomial time. Notice that this is a worst case
learning model, as the denition allows an adversarial choice of all the inputs of the learner.
546

Pac-Learning Recursive Logic Programs: Negative Results

2.2.3 Relation to Other Models

The model of polynomial predictability has been well-studied (Pitt & Warmuth, 1990), and
is a weaker version of Valiant's (1984) criterion of pac-learnability . A language Lang is
pac-learnable i there is an algorithm PacLearn so that
1. PacLearn satises all the requirements in the denition of polynomial predictability,
and
2. on inputs S + and S , , PacLearn always outputs a hypothesis H 2 Lang.
Thus if a language is pac-learnable it is predictable.
In the companion paper (Cohen, 1995), our positive results are all expressed in the model
of identiability from equivalence queries, which is strictly stronger than pac-learnability;
that is, anything that is learnable from equivalence queries is also necessarily pac-learnable.1
Since this paper contains only negative results, we will use the the relatively weak model
of predictability. Negative results in this model immediately translate to negative results
in the stronger models; if a language is not predictable, it cannot be pac-learnable, nor
identiable from equivalence queries.
2.2.4 Background Knowledge in Learning

In a typical ILP system, the setting is slightly dierent, as the user usually provides clues
about the target concept in addition to the examples, in the form of a database DB of
\background knowledge" and a set of declarations. To account for these additional inputs it
is necessary to extend the framework described above to a setting where the learner accepts
inputs other than training examples. Following the formalization used in the companion
paper (Cohen, 1995), we will adopt the notion of a \language family".
If Lang is a set of clauses, DB is a database and Dec is a declaration, we will dene
Lang[DB ; Dec] to be the set of all pairs (C; DB ) such that C 2 Lang and C satises Dec .
Semantically, such a pair will denote the set of all extended instances (f; D) covered by
(C; DB ). Next, if DB is a set of databases and DEC is a set of declarations, then dene
Lang[DB ; DEC ] = fLang[DB ; Dec ] : DB

2 DB and Dec 2 DECg

This set of languages is called a language family .
We will now extend the denition of predictability queries to language families as follows.
A language family Lang[DB; DEC ] is polynomially predictable i every language in the set
is predictable. A language family Lang[DB; DEC ] is polynomially predictable i there is
a single algorithm Identify(DB ; Dec ) that predicts every Lang[DB ; Dec] in the family
given DB and Dec .
The usual model of polynomial predictability is worst-case over all choices of the target
concept and the distribution of examples. The notion of polynomial predictability of a
language family extends this model in the natural way; the extended model is also worstcase over all possible choices for database DB 2 DB and Dec 2 DEC . This worst-case
1. An equivalence query is a question of the form \is H equivalent to the target concept?" which is answered
with either \yes" or a counterexample. Identication by equivalence queries essentially means that the
target concept can be exactly identied in polynomial time using a polynomial of such queries.

547

Cohen

model may seem unintuitive, since one typically assumes that the database DB is provided
by a helpful user, rather than an adversary. However, the worst-case model is reasonable
because learning is allowed to take time polynomial in the size of smallest target concept
in the set Lang[DB ; Dec ]; this means that if the database given by the user is such that
the target concept cannot be encoded succinctly (or at all) learning is allowed to take more
time.
Notice that for a language family Lang[DB ; Dec] to be polynomially predictable, every
language in the family must be polynomially predictable. Thus to show that a family is not
polynomially predictable it is sucient to construct one language in the family for which
learning is hard. The proofs of this paper will all have this form.

2.3 Prediction-Preserving Reducibilities

The principle technical tool used in our negative results in the notion of prediction-preserving
reducibility , as introduced by Pitt and Warmuth (1990). Prediction-preserving reducibilities
are a method of showing that one language is no harder to predict than another. Formally,
let Lang1 be a language over domain X1 and Lang2 be a language over domain X2.
We say that predicting Lang1 reduces to predicting Lang2, denoted Lang1  Lang2 , if
there is a function fi : X1 ! X2 , henceforth called the instance mapping , and a function
fc : Lang1 ! Lang2 , henceforth called the concept mapping , so that the following all hold:
1. x 2 C if and only if fi (x) 2 fc (C ) | i.e., concept membership is preserved by the
mappings;
2. the size complexity of fc (C ) is polynomial in the size complexity of C |i.e., the size
of concept representations is preserved within a polynomial factor;
3. fi (x) can be computed in polynomial time.
Note that fc need not be computable; also, since fi can be computed in polynomial time,
fi(x) must also preserve size within a polynomial factor.
Intuitively, fc (C1) returns a concept C2 2 Lang2 that will \emulate" C1|i.e., make
the same decisions about concept membership|on examples that have been \preprocessed"
with the function fi . If predicting Lang1 reduces to predicting Lang2 and a learning
algorithm for Lang2 exists, then one possible scheme for learning concepts from Lang1
would be the following. First, convert any examples of the unknown concept C1 from
the domain X1 to examples over the domain X2 using the instance mapping fi . If the
conditions of the denition hold, then since C1 is consistent with the original examples,
the concept fc (C1) will be consistent with their image under fi ; thus running the learning
algorithm for Lang2 should produce some hypothesis H that is a good approximation of
fc (C1). Of course, it may not be possible to map H back into the original language Lang1,
as computing fc ,1 may be dicult or impossible. However, H can still be used to predict
membership in C1: given an example x from the original domain X1, one can simply predict
x 2 C1 to be true whenever fi (x) 2 H .
Pitt and Warmuth (1988) give a more rigorous argument that this approach leads to a
prediction algorithm for Lang1 , leading to the following theorem.
548

Pac-Learning Recursive Logic Programs: Negative Results

Theorem 1 (Pitt and Warmuth) Assume Lang1  Lang2. Then if Lang1 is not polynomially predictable, Lang2 is not polynomially predictable.

3. Cryptographic Limitations on Learning Recursive Programs

Theorem 1 allows one to transfer hardness results from one language to another. This is
useful because for a number of languages, it is known that prediction is as hard as breaking
cryptographic schemes that are widely assumed to be secure. For example, it is known
that predicting the class of languages accepted by deterministic nite state automata is
\cryptographically hard", as is the class of languages accepted by log-space bounded Turing
machines.
In this section we will make use of Theorem 1 and previous cryptographic hardness
results to show that certain restricted classes of recursive logic programs are hard to learn.

3.1 Programs With n Linear Recursive Clauses

In a companion paper (Cohen, 1995) we showed that a single linear closed recursive clause
was identiable from equivalence queries. In this section we will show that a program with
a polynomial number of such clauses is not identiable from equivalence queries, nor even
polynomially predictable.
Specically, let us extend our notion of a \family of languages" slightly, and let
DLog[n; s] represent the language of log-space bounded deterministic Turing machines with
up to s states accepting inputs of size n or less, with the usual semantics and complexity
measure.2 Also let d-DepthLinRecProg denote the family of logic programs containing
only depth-d linear closed recursive clauses, but containing any number of such clauses. We
have the following result:
Theorem 2 For every n and s, there exists a database DB n;s 2 1-DB and declaration
Dec n;s 2 1-DetDEC of sizes polynomial in n and s such that
DLog[n; s]  1-DepthLinRecProg[DB n;s ; Dec n;s ]
Hence for d  1 and a  1, d-DepthLinRecProg[DB; a-DetDEC ] is not uniformly polynomially predictable under cryptographic assumptions.3
Proof: Recall that a log-space bounded Turing machine (TM) has an input tape of length
n, a work tape of length log2 n which initially contains all zeros, and a nite state control
with state set Q. To simplify the proof, we assume without loss of generality that the tape
and input alphabets are binary, that there is a single accepting state qf 2 Q, and that the
machine will always erase its work tape and position the work tape head at the far left after
it decides to accept its input.
At each time step, the machine will read the tape squares under its input tape head and
work tape head, and based on these values and its current state q , it will
2. I.e., a machine represents the set of all inputs that it accepts, and its complexity is the number of states.
3. Specically, this language is not uniformly polynomially predictable unless all of the following cryptographic problems can be solved in polynomial time: solving the quadratic residue problem, inverting the
RSA encryption function, and factoring Blum integers. This result holds because all of these cryptographic problems can be reduced to learning DLOG Turing machines (Kearns & Valiant, 1989).

549

Cohen






write either a 1 or a 0 on the work tape,
shift the input tape head left or right,
shift the work tape head left or right, and
transition to a new internal state q 0
A deterministic machine can thus be specied by a transition function

 : f0; 1g  f0; 1g  Q ,! f0; 1g  fL; Rg  fL; Rg  Q
Let us dene the internal conguration of a TM to consist of the string of symbols
written on the worktape, the position of the tape heads, and the internal state q of the
machine: thus a conguration is an element of the set
CON  f0; 1glog2 n  f1; : : :; log2 ng  f1; : : :; ng  Q

A simplied specication for the machine is the transition function

0 : f0; 1g  CON ! CON
where the component f0; 1g represents the contents of the input tape at the square below
the input tape head.
Notice that for a machine whose worktape size is bounded by log n, the cardinality of
CON is only p = jQjn2 log2 n, a polynomial in n and s = jQj. We will use this fact in our
constructions.
The background database DB n;s is as follows. First, for i = 0; : : :; p, an atom of the
form coni(ci ) is present. Each constant ci will represent a dierent internal conguration of
the Turing machine. We will also arbitrarily select c1 to represent the (unique) accepting
conguration, and add to DB n;s the atom accepting(c1). Thus
DB n;s  fcon i (ci)gpi=1 [ faccepting (c1)g

Next, we dene the instance mapping. An instance in the Turing machine's domain is
a binary string X = b1 : : :bn ; this is mapped by fi to the extended instance (f; D) where

f  accepting (c0 )
D  ftruei gb 2X :b =1 [ ffalsei gb 2X :b =0
i

i

i

i

The description atoms have the eect of dening the predicate truei to be true i the i-th
bit of X is a \1", and the dening the predicate falsei to be true i the i-th bit of X is
\0". The constant c0 will represent the start conguration of the Turing machine, and the
predicate accepting(C) will be dened so that it is true i the Turing machine accepts input
X starting from state C.
We will let Dec n;s = (accepting ; 1; R) where R contains the modes coni (+) and coni (,),
for i = 1; : : :; p; and truej and falsej for j = 1; : : :; n.
Finally, for the concept mapping fc , let us assume some arbitrary one-to-one mapping
 between the internal congurations of a Turing machine M and the predicate names
550

Pac-Learning Recursive Logic Programs: Negative Results

con0,: : : ,conp,1 such that the start conguration (0log2 n ; 1; q0) maps to con0 and the accepting conguration (0log2 n ; 1; qf ) maps to con1. We will construct the program fc (M )
as follows. For each transition  0(1; c) ! c0 in  0, where c and c0 are in CON , construct a
clause of the form

accepting(C)

conj (C) ^ truei ^ conj 0 (C1) ^ accepting(C1).

where i is the position of the input tape head which is encoded in c, con j =  (c), and
con j 0 =  (c0). For each transition  0(0; c) ! (c0) in  0 construct an analogous clause, in
which truei is replaced with falsei.
Now, we claim that for this program P , the machine M will accept when started in
conguration ci i
DB n;s ^ D ^ P ` accepting (ci )
and hence that this construction preserves concept membership. This is perhaps easiest to see by considering the action of a top-down theorem prover when given the goal
accepting (C ): the sequence of subgoals accepting (ci ), accepting (ci +1 ), : : : generated by the
theorem-prover precisely parallel the sequence of congurations ci , : : : entered by the Turing
machine.
It is easily veried that the size of this program is polynomial in n and s, and that the
clauses are linear recursive, determinate, and of depth one, completing the proof.
There are number of ways in which this result can be strengthened. Precisely the
same construction used above can be used to reduce the class of nondeterministic log-space
bounded Turing machines to the constant-depth determinate linear recursive programs.
Further, a slight modication to the construction can be used to reduce the class of log-space
bounded alternating Turing machines (Chandra, Kozen, & Stockmeyer, 1981) to constantdepth determinate 2-ary recursive programs. The modication is to emulate congurations
corresponding to universal states of the Turing machine with clauses of the form
accepting(C)
conj (C) ^ truei ^
conj 10 (C1) ^ accepting(C1) ^
conj 20 (C2) ^ accepting(C2).
where conj1 0 and conj2 0 are the two successors to the universal conguration conj . This is
a very strong result, since log-space bounded alternating Turing machines are known to be
able to perform every polynomial-time computation.

3.2 Programs With One n-ary Recursive Clause

We will now consider learning a single recursive clause with arbitrary closed recursion.
Again, the key result of this section is an observation about expressive power: there is
a background database that allows every log-space deterministic Turing machine M to
be emulated by a single recursive constant-depth determinate clause. This leads to the
following negative predictability result.
551

Cohen

Theorem 3 For every n and s, there exists a database DB n;s 2 3-DB and declaration
Dec n;s 2 3-DetDEC of sizes polynomial in n and s such that
DLog[n; s]  3-DepthRec[DB n;s ; Dec n;s ]
Hence for d  3 and a  3, d-DepthRec[DB n ; a-DetDEC ] is not uniformly polynomially
predictable under cryptographic assumptions.

Proof: Consider a DLOG machine M . As in the proof of Theorem 2, we assume without
loss of generality that the tape alphabet is f0; 1g, that there is a unique starting congura-

tion c0, and that there is a unique accepting conguration c1. We will also assume without
loss of generality that there is a unique \failing" conguration cf ail; and that there is exactly
one transition of the form
 0(b; cj) ! c0j
for every combination of i 2 f1; : : :; ng, b 2 f0; 1g, and cj 2 CON , fc1; cf ailg. Thus on
input X = x1 : : :xn the machine M starts with CONFIG=c0 , then executes transitions
until it reaches CONFIG=c1 or CONFIG=cf ail, at which point X is accepted or rejected
(respectively). We will use p for the number of congurations. (Recall that p is polynomial
in n and s.)
To emulate M , we will convert an example X = b1 : : :bn into the extended instance
fi(X ) = (f; D) where

f  accepting (c0 )
D  fbit i (bi)gni=1
Thus the predicate bit i (X ) binds X to the i-th bit of the TM's input tape. We also will
dene the following predicates in the background database DB n;s .

 For every possible b 2 f0; 1g and j : 1  j  p(n), the predicate statusb;j (B,C,Y) will
be dened so that given bindings for variables B and C , statusb;j (B,C,Y) will fail if
C = cf ail; otherwise it will succeed, binding Y to active if B = b and C = cj and
binding Y to inactive otherwise.
 For j : 1  j  p(n), the predicate nextj (Y,C) will succeed i Y can be bound to
either active or inactive. If Y = , then C will be bound to cj ; otherwise, C will be
bound to the accepting conguration c1.
 The database also contains the fact accepting (c1 ).
It is easy to show that the size of this database is polynomial in n and s.
The declaration Dec n;s is dened to be (accepting ; 1; R) where R includes the modes
status bj (+; +; ,), next j (+; ,), and bit i (,) for b 2 f0; 1g, j = 1; : : :; p, and i = 1; : : :; n.
Now, consider the transition rule  0(b; cj ) ! c0j , and the corresponding conjunction
TRANSibj  biti (Bibj ) ^ statusb;j (C,Bibj ,Yibj ) ^ nextj 0 (Yibj ,C1ibj ) ^ accepting(C1ibj )
552

Pac-Learning Recursive Logic Programs: Negative Results

Given DB n;s and D, and assuming that C is bound to some conguration c, this conjunction
will fail if c = cf ail. It will succeed if xi 6= b or c 6= cj ; in this case Yibj will be bound to
inactive, C1ibj will be bound to c1, and the recursive call succeeds because accepting(c1) is in
DB n;s . Finally, if xi = b and c = cj , TRANSibj will succeed only if the atom accepting(cj 0 )
is provable; in this case, Yibj will be bound to active and C1ibj will be bound to cj 0 .
From this it is clear that the clause fc (M ) below
^
accepting(C)
TRANSibj
i

2f1;:::;ng; b2f0;1g
j 2f1;:::;pg

will correctly emulate the machine M on examples that have been preprocessed with the
function fi described above. Hence this construction preserves concept membership. It is
also easily veried that the size of this program is polynomial in n and s, and that the
clause is determinate and of depth three.

3.3 One k-Local Linear Closed Recursive Clause

So far we have considered only one class of extensions to the positive result given in the
companion paper (Cohen, 1995)|namely, relaxing the restrictions imposed on the recursive
structure of the target program. Another reasonable question to ask is if linear closed
recursive programs can be learned without the restriction of constant-depth determinacy.
In earlier papers (Cohen, 1993a, 1994a, 1993b) we have studied the conditions under
which the constant-depth determinacy restriction can be relaxed while still allowing learnability for nonrecursive clauses. It turns out that most generalizations of constant-depth
determinate clauses are not predictable, even without recursion. However, the language of
nonrecursive clauses of constant locality is a pac-learnable generalization of constant-depth
determinate clauses. Below, we will dene this language, summarize the relevant previous
results, and then address the question of the learnability of recursive local clauses.
Dene a variable V appearing in a clause C to be free if it appears in the body of C but
not the head of C . Let V1 and V2 be two free variables appearing in a clause. V1 touches V2
if they appear in the same literal, and V1 inuences V2 if it either touches V2, or if it touches
some variable V3 that inuences V2. The locale of a free variable V is the set of literals that
either contain V , or that contain some free variable inuenced by V . Informally, variable
V1 inuences variable V2 if the choice of a binding for V1 can aect the possible choices of
bindings for V2.
The locality of a clause is the size of its largest locale. Let k-LocalNonRec denote the
language of nonrecursive clauses with locality k or less. (That is, k-LocalNonRec is the
set of logic programs containing a single nonrecursive k-local clause.) The following facts
are known (Cohen, 1993b):
 For xed k and a, the language family k-LocalNonRec[a-DB; a-DEC] is uniformly
pac-learnable.
 For every constant d, every constant a, every database DB 2 a-DB, every declaration
Dec 2 a-DetDEC , and every clause C 2 d-DepthNonRec[DB ; Dec ], there is an
553

Cohen

equivalent clause C 0 in k-LocalNonRec[DB ; Dec] of size bounded by kj C j , where k
is a function only of a and d (and hence is a constant if d and a are also constants.)
Hence
k-LocalNonRec[DB; a-DEC]
is a pac-learnable generalization of

d-DepthNonRec[DB; a-DetDEC ]
It is thus plausible to ask if recursive programs of k-local clauses are pac-learnable. Some
facts about the learnability of k-local programs follow immediately from previous results.
For example, an immediate consequence of the construction of Theorem 2 is that programs
with a polynomial number of linear recursive k-local clauses are not predictable for k  2.
Similarly, Theorem 3 shows that a single recursive k-local clause is not predictable for k  4.
It is still reasonable to ask, however, if the positive result for bounded-depth determinate
recursive clauses (Cohen, 1995) can be extended to k-ary closed recursive k-local clauses.
Unfortunately, we have the following negative result, which shows that even linear closed
recursive clauses are not learnable.

Theorem 4 Let Dfa[s] denote the language of deterministic nite automata with s states,

and let k-LocalLinRec be the set of linear closed recursive k-local clauses. For any constant s there exists a database DB s 2 3-DB and a declaration Dec s 2 3-DEC , both of size
polynomial in s, such that
Dfa[s]  3-LocalLinRec[DB s ; Dec s ]

Hence for k  3 and a  3, k-LocalLinRec[a-DB ; Dec] is not uniformly polynomially
predictable under cryptographic assumptions.

Proof: Following Hopcroft and Ullman (1979) we will represent a DFA M over the alphabet

 as a tuple (q0; Q; F;  ) where q0 is the initial state, Q is the set of states, F is the set of
accepting states, and  : Q   ! Q is the transition function (which we will sometimes
think of as a subset of Q    Q). To prove the theorem, we need to construct a database
DB s of size polynomial in s such that every s-state DFA can be emulated by a linear
recursive k-local clause over DB s .
Rather than directly emulating M , it will be convenient to emulate instead a modication of M . Let M^ be a DFA with state set Q^  Q [ fq(,1); qe ; qf g, where q(,1) , qe and qf
are new states not found in Q. The initial state of M^ is q(,1) . The only nal state of M^ is
qf . The transition function of M^ is
[
^   [ f(q(,1); a; q0); (qe; c; qf )g [
f(qi; b; qe)g
2

qi F

where a, b, and c are new letters not in . Note that M^ is now a DFA over the alphabet
 [ fa; b; cg, and, as described, need not be a complete DFA over this alphabet. (That
is, there may be pairs (qi ; a) such that ^(qi ; a) is undened.) However, M^ can be easily
554

Pac-Learning Recursive Logic Programs: Negative Results

M

1


q


0

?

0 

M^

1



q



- ?

1

0

1

1







q 
q
q
q
q
  

,1

0

?

a

-

0 

M0




-

0

?

1

b,c,0,1








1







b

-

a,b,c
1

-

c

e


q

?

r

a,b,c

-

f

a,b,c,0,1
a,b,c,0,1


6

a,b,
0,1







q
q
q
q
q









,1

a



0 

?

-



0
0

-

?

1





,
,

,
, b -



e

c

-

f

Figure 1: How a DFA is modied before emulation with a local clause

555

Cohen

made complete by introducing an additional rejecting state qr , and making every undened
transition lead to qr . More precisely, let  0 be dened as
0  ^ [ f(qi; x; qr) j qi 2 Q^ ^ x 2  [ fa; b; cg ^ (6 9qj : (qi ; x; qj ) 2 ^)g
Thus M 0 = (q(,1); Q^ [fqr g; fqf g;  0) is a \completed" version of M^ , with Q0 = Q^ [fqr g. We
will use M 0 in the construction below; we will also let Q0 = Q^ [ fqr g and 0 =  [ fa; b; cg.
Examples of M , M^ and M 0 are shown in Figure 1. Notice that aside from the arcs into
and out of the rejecting state qr , the state diagram of M 0 is nearly identical to that of M .
The dierences are that in M 0 there is a new initial state q(,1) with a single outgoing arc
labeled a to the old initial state q0 ; also every nal state of M has in M 0 an outgoing arc
labeled b to a new state qe , which in turn has a single outgoing arc labeled c to the nal
state qf . It is easy to show that

x 2 L(M ) i axbc 2 L(M 0)
Now, given a set of states Q0 we dene a database DB that contains the following
predicates:
 arcq ;;q (S,X,T) is true for any S 2 Q0, any T 2 Q0, and any X 2 0, unless S = qi,
X = , and T 6= qj .
 state(S) is true for any S 2 Q0.
 accept(c,nil,qe,qf ) is true.
As motivation for the arc predicates, observe that in emulating M 0 it is clearly useful to be
able to represent the transition function  0. The usefulness of the arc predicates is that any
transition function  0 can be represented using a conjunction of arc literals. In particular,
the conjunction
^
arc q ;;q (S; X; T )
i

j

i

(q ;;q )20
i

j

j

succeeds when  0 (S; X ) = T , and fails otherwise.
Let us now dene the instance mapping fi as fi (x) = (f; D) where

f = accept (a; xbc; q(,1); q0)
and D is a set of facts that denes the components relation on the list that corresponds to
the string xbc. In other words, if x = 1 : : :n , then D is the set of facts
components(1 : : :n bc; 1; 2 : : :n bc)
components(2 : : :n bc; 2; 3 : : :n bc)
..
.
components(c,c,nil)
The declaration Dec n will be Dec n = (accept ; 4; R) where R contains the modes
components (+; ,; ,), state (,), and arc q ;;q (+; +; +) for qi , qj in Q0 , and  2 0 .
Finally, dene the concept mapping fc (M ) for a machine M to be the clause
i

j

556

Pac-Learning Recursive Logic Programs: Negative Results

accept(X,Ys,S,T)
V
(q ;;q )20 arcq ;;q (S,X,T)
^ components(Ys,X1,Ys1) ^ state(U) ^ accept(X1,Ys1,T,U).
where  0 is the transition function for the corresponding machine M 0 dened above. It is
easy to show this construction is polynomial.
In the clause X is a letter in 0, Ys is a list of such letters, and S and T are both states
in Q0 . The intent of the construction is that the predicate accept will succeed exactly when
(a) the string XYs is accepted by M 0 when M 0 is started in state S , and (b) the rst action
taken by M 0 on the string XYs is to go from state S to state T .
Since all of the initial transitions in M 0 are from q(,1) to q0 on input a, then if the
predicate accept has the claimed behavior, clearly the proposed mapping satises the requirements of Theorem 1. To complete the proof, therefore, we must now verify that the
predicate accept succeeds i XYs is accepted by M 0 in state S with an initial transition to
T.
From the denition of DFAs the string XYs is accepted by M 0 in state S with an initial
transition to T i one of the following two conditions holds.
 0(S; X ) = T , Ys is the empty string and T is a nal state of M 0, or;
 0(S; X ) = T , Ys is a nonempty string (and hence has some head X 1 and some tail
Ys1) and Ys1 is accepted by M 0 in state T , with any initial transition.
The base fact accept(c,nil,qe,qf ) succeeds precisely when the rst case holds, since in
M 0 this transition is the only one to a nal state. In the second case, the conjunction of the
arc conditions in the fc (M ) clause succeeds exactly when  (S; X ) = T (as noted above).
Further the second conjunction in the clause can be succeeds when Ys is a nonempty string
with head X 1 and tail Ys1 and X1Ys1 is accepted by M 0 in state T with initial transition
to any state U , which corresponds exactly to the second case above.
Thus concept membership is preserved by the mapping. This completes the proof.
i

j

i

j

4. DNF-Hardness Results for Recursive Programs

To summarize previous results for determinate clauses, it was shown that while a single
k-ary closed recursive depth-d clause is pac-learnable (Cohen, 1995), a set of n linear closed
recursive depth-d clauses is not; further, even a single n-ary closed recursive depth-d clauses
is not pac-learnable. There is still a large gap between the positive and negative results,
however: in particular, the learnability of recursive programs containing a constant number
of k-ary recursive clauses has not yet been established.
In this section we will investigate the learnability of these classes of programs. We will
show that programs with either two linear closed recursive clauses or one linear closed recursive clause and one base case are as hard to learn as boolean functions in disjunctive
normal form (DNF). The pac-learnability of DNF is a long-standing open problem in computational learning theory; the import of these results, therefore, is that establishing the
learnability of these classes will require some substantial advance in computational learning
theory.
557

Cohen

4.1 A Linear Recursive Clause Plus a Base Clause

Previous work has established that two-clause constant-depth determinate programs consisting of one linear recursive clause and one nonrecursive clause can be identied, given
two types of oracles: the standard equivalence-query oracle, and a \basecase oracle' (Cohen,
1995). (The basecase oracle determines if an example is covered by the nonrecursive clause
alone.) In this section we will show that in the absence of the basecase oracle, the learning
problem is as hard as learning boolean DNF.
In the discussion below, Dnf[n; r] denotes the language of r-term boolean functions in
disjunctive normal form over n variables.

Theorem 5 Let d-Depth-2-Clause be the set of 2-clause programs consisting of one

clause in d-DepthLinRec and one clause in d-DepthNonRec. Then for any n and
any r there exists a database DB n;r 2 2-DB and a declaration Dec n;r 2 2-DEC , both of sizes
polynomial in n and r, such that
Dnf[n; r]  1-Depth-2-Clause[DB n;r ; Dec n;r ]

Hence for a  2 and d  1 the language family d-Depth-2-Clause[DB; a-DetDEC ] is
uniformly polynomially predictable only if DNF is polynomially predictable.

Proof: We will produce a DB n;r 2 DB and Dec n;r 2 2-DetDEC such that predicting
DNF can be reduced to predicting 1-Depth-2-Clause[DB n;r ; Dec n;r ]. The construction
makes use of a trick rst used in Theorem 3 of (Cohen, 1993a), in which a DNF formula is
emulated by a conjunction containing a single variable Y which is existentially quantied
over a restricted range.
We begin with the instance mapping fi . An assignment  = b1 : : :bn will be converted
to the extended instance (f; D) where
f  p(1)
D  fbit i (bi)gni=1
Next, we dene the database DB n;r to contain the binary predicates true1 , false1, : : : , truer ,
falser which behave as follows:

 truei(X,Y) succeeds if X = 1, or if Y 2 f1; : : :; rg , fig.
 falsei(X,Y) succeeds if X = 0, or if Y 2 f1; : : :; rg , fig.
Further, DB n;r contains facts that dene the predicate succ(Y,Z) to be true whenever
Z = Y + 1, and both Y and Z are numbers between 1 and r. Clearly the size of DB n;r is
polynomial in r.
Let Dec n;r = (p; 1; R) where R contains the modes bit i (,), for i = 1; : : :; n; true j (+; +)
and false j (+; +), for j = 1; : : :; r, and succ (+; ,).
Now let  be an r-term DNF formula  = _ri=1 ^sj =1 lij over the variables v1 ; : : :; vn.
We may assume without loss of generality that  contains exactly r terms, since any DNF
formula with fewer than r terms can be padded to exactly r terms by adding terms of the
i

558

Pac-Learning Recursive Logic Programs: Negative Results

Background database:

for i = 1; : : :; r
truei (b; y ) for all b; y : b = 1 or y 2 f1; : : :; rg but y 6= i
falsei (b; y ) for all b; y : b = 0 or y 2 f1; : : :; rg but y 6= i
succ(y,z)
if z = y + 1 and y 2 f1; : : :; rg and z 2 f1; : : :; rg

DNF formula: (v1 ^ v3 ^ v4) _ (v2 ^ v3) _ (v1 ^ v4)
Equivalent program:
p(Y) succ(Y,Z)^p(Z).
p(Y) bit1 (X1 ) ^ bit2 (X2 ) ^ bit3 (X3 ) ^ bit4 (X4 ) ^
true1 (X1,Y) ^ false1 (X3 ,Y) ^ true1(X4 ,Y) ^
false2 (X2,Y) ^ false2 (X3,Y)^
true3 (X1,Y) ^ false3 (X4 ,Y).
Instance mapping: fi(1011) = (p(1); fbit1(1); bit 2(0); bit3(1); bit4(1)g)
Figure 2: Reducing DNF to a recursive program
form v1 v1. We now dene the concept mapping fc () to be the program CR; CB where CR
is the linear recursive depth 1 determinate clause

p(Y ) succ(Y; Z ) ^ p(Z )
and CB is the nonrecursive depth 1 determinate clause
s
n
^
^r ^
p(Y )
bit k (Xk ) ^
Bij
i

i=1 j =1

k =1

where Bij is dened as follows:

Bij 

(

truei (Xk ,Y) if lij = vk
falsei (Xk ,Y) if lij = vk

An example of the construction is shown in Figure 2; we suggest that the reader refer
to this gure at this point. The basic idea behind the construction is that rst, the clause
CB will succeed only if the variable Y is bound to i and the i-th term of  succeeds (the
denitions of truei and falsei are designed to ensure that this property holds); second, the
recursive clause CR is constructed so that the program fc () succeeds i CB succeeds with
Y bound to one of the values 1; : : :; n.
We will now argue more rigorously for the correctness of the construction. Clearly, fi ( )
and fc () are of the same size as  and  respectively. Since DB n;r is also of polynomial
size, this reduction is polynomial.
Figure 3 shows the possible proofs that can be constructed with the program fc ();
notice that the program fc () succeeds exactly when the clause CB succeeds for some value
559

Cohen

p(1)

 A@
A@
AA @
@
B(1)





succ(1,2) p(2)


 @
A


A@


AA @
@
B(2)

succ(2,3) p(3)


 A
@


A@


AA @
@
B(3)
:::

p(n-1)

B (i)  V bit (X ) ^ V V B
i

i

ij


 A
@


A@


AA @
B(n-1)
@

succ(n-1,n) p(n)
B(n)

Figure 3: Space of proofs possible with the program fc ()
Vs l must be true; in
of Y between
1
and
r
.
Now,
if

is
true
then
some
term
T
i =
j =1 ij
V
V
s0
s
this case j =1 Bij succeeds with Y bound to the value i and j =1 Bi0 j for every i0 6= i also
succeeds with Y bound to i. On the other hand, if  is false for an assignment, then each Ti
fails, and hence for every possible binding of Y generated by repeated use of the recursive
clause CR the base clause CB will also fail. Thus concept membership is preserved by the
mapping.
This concludes the proof.
i

i

i

4.2 Two Linear Recursive Clauses

Recall again that a single linear closed recursive clause is identiable from equivalence
queries (Cohen, 1995). A construction similar to that used in Theorem 5 can be used to
show that this result cannot be extended to programs with two linear recursive clauses.
Theorem 6 Let d-Depth-2-Clause0 be the set of 2-clause programs consisting of two
clauses in d-DepthLinRec. (Thus we assume that the base case of the recursion is given
as background knowledge.) Then for any constants n and r there exists a database DB n;r 2
2-DB and a declaration Dec n;r 2 2-DEC , both of sizes polynomial in n, such that
Dnf[n; r]  1-Depth-2-Clause0[DB n;r ; Dec n;r ]
Hence for any constants a  2 and d  1 the language family
d-Depth-2-Clause0 [DB; a-DetDEC ]
560

Pac-Learning Recursive Logic Programs: Negative Results

is uniformly polynomially predictable only if DNF is polynomially predictable.

Proof: As before, the proof makes use of a prediction-preserving reducibility from DNF to

d-Depth-2-Clause0[DB ; Dec ] for a specic DB and Dec . Let us assume that  is a DNF
with r terms, and further assume that r = 2k . (Again, this assumption is made without
loss of generality, since the number of terms in  can be increased by padding with vacuous
terms.) Now consider a complete binary tree of depth k + 1. The k-th level of this tree has
exactly r nodes; let us label these nodes 1, : : : , r, and give the other nodes arbitrary labels.

Now construct a database DB n;r as in Theorem 5, except for the following changes:
 The predicates truei (b,y) and falsei(b,y) also succeed when y is the label of a node at
some level below k.
 Rather than the predicate succ, the database contains two predicates leftson and
rightson that encode the relationship between nodes in the binary tree.
 The database includes the facts p(!1), : : : , p(!2r), where !1, : : : , !2r are the leaves
of the binary tree. These will be used as the base cases of the recursive program that
is to be learned.
Let  be the label of the root of the binary tree. We dene the instance mapping to be

fi (b1 : : :b1)  (p(); fbit1 (b1); : : :; bit n (bn )g)
Note that except for the use of  rather than 1, this is identical to the instance mapping
used in Theorem 5. Also let Dec n;r = (p; 1; R) where R contains the modes bit i (,), for i =
1; : : :; n; true j (+; +) and false j (+; +), for j = 1; : : :; r; leftson (+; ,); and rightson (+; ,).
The concept mapping fc () is the pair of clauses R1; R2, where R1 is the clause
s
n
^
^r ^
p(Y )
bit k (Xk ) ^
Bij ^ leftson(Y; Z ) ^ p(Z )
i

k =1

and R2 is the clause

p(Y )

n
^
k =1

bit k (Xk ) ^

i=1 j =1

s
^r ^
i

i=1 j =1

Bij ^ rightson (Y; Z ) ^ p(Z )

Note that both of these clause are linear recursive, determinate, and have depth 1. Also,
the construction is clearly polynomial. It remains to show that membership is preserved.
Figure 4 shows the space of proofs that can
V be constructed
V V with the program fc (); as
in Figure 3, B (i) abbreviates the conjunction bit i (Xi) ^ Bij . Notice that the program
will succeed only if the recursive calls manage to nally recurse to one of the base cases
p(!1), : : : , p(!2r ), which correspond to the leaves of the binary tree. Both clauses will both
succeed on the the rst k , 1 levels of the tree. However, to reach the base cases of the
recursion at the leaves of the tree, the recursion must pass through the k-th level of the tree;
that is, one of the clauses above must succeed on some node y of the binary tree, where
y is on the k-th level of the tree, and hence the label of y is a number between 1 and r.
The program thus succeeds on fi ( ) precisely when there is some number y between 1 and
561

Cohen

p()

"
, b
@b
"

H

"H
,
@bb
"
" ,
@ b
" ,
@ bb
"
"
b
,
@
"
b
"
,
@
b
"
b
,
@
B() p(L)
B()
p(R)
 Z
 Z
`
` \
Z
X \\
Z

X
\
 
Z

Z
 
\ Z
 
\ Z
 
 
\ Z
\
\ Z


E
X
X
X
EX


E


E


E

:::

:::

:::

B
 B
 B




B
B

:::

:::

B(1) p(LL: : : L) B(1) p(LL: : : R)

p(!1 )

:::



J
 J

J


J

J

B
 B
 B

B

B

B(n) p(RR: : : LR) B(n) p(RR: : : R)

p(!2 )

p(!2 ,1 )
r

p(!2 )
r

Figure 4: Proofs possible with the program fc ()

r such that the conjunction B(i) succeeds, which (by the argument given in Theorem 5)
can happen if and only if  is satised by the assignment  . Thus, the mappings preserve

concept membership. This completes the proof.

Notice that the programs fc () used in this proof all have the property that the depth
of every proof is logarithmic in the size of the instances. This means that the hardness
result holds even if one additionally restricts the class of programs to have a logarithmic
depth bound.

4.3 Upper Bounds on the Diculty of Learning

The previous sections showed that several highly restricted classes of recursive programs
are at least as hard to predict as DNF. In this section we will show that these restricted
classes are also no harder to predict than DNF.
We will wish to restrict the depth of a proof constructed by a target program. Thus, let
h(n) be any function; we will use Langh(n) for the set of programs in the class Lang such
that all proofs of an extended instance (f; D) have depth bounded by h(j Dj ).
562

Pac-Learning Recursive Logic Programs: Negative Results

Theorem 7 Let Dnf[n; ] be the language of DNF boolean functions (with any number

of terms), and recall that d-Depth-2-Clause is the language of 2-clause programs consisting of one clause in d-DepthLinRec and one clause in d-DepthNonRec, and that
d-Depth-2-Clause0 is the language of 2-clause programs consisting of two clauses in
d-DepthLinRec.
For all constants d and a, and all databases DB 2 DB and declarations Dec 2 a-DetDEC ,
there is a polynomial function poly (n) such that

 d-Depth-2-Clause[DB ; Dec]  Dnf[poly (j DB j ); ]
 d-Depth-2-Clause0h(n) [DB ; Dec]  Dnf[poly (j DB j ); ] if h(n) is bounded by c log n
for some constant c.
Hence if either of these language families is uniformly polynomially predictable, then Dnf[n; ]
is polynomially predictable.

Proof: The proof relies on several facts established in the companion paper (Cohen, 1995).
 For every declaration Dec, there is a clause BOTTOM d(Dec) such that every nonrecursive depth-d determinate clause C is equivalent to some subclause of BOTTOM d .
Further, the size of BOTTOM  d is polynomial in Dec . This means that the language of subclauses of BOTTOM  is a normal form for nonrecursive constant-depth
determinate clauses.

 Every linear closed recursive clause CR that is constant-depth determinate is equivalent to some subclause of BOTTOM  plus a recursive literal Lr ; further, there are
only a polynomial number of possible recursive literals Lr .
 For any constants a, a0, and d, any database DB 2 a-DB, any declaration Dec =
(p; a0; R), any database DB 2 a-DB , and any program P in d-Depth-2-Clause[DB ; Dec ],
the depth of a terminating proof constructing using P is no more than hmax, where
hmax is a polynomial in the size of DB and Dec .
 At can be assumed without loss of generality that the database DB and all decsriptions
D contain an equality predicate , where an equality predicate is simply a predicate
equal(X,Y) which is true exactly when X = Y .
The idea of the proof is to contruct a prediction-preserving reduction between the two
classes of recursive programs listed above to and DNF. We will begin with two lemmas.

Lemma 8 Let Dec 2 a-DetDEC , and let C be a nonrecursive depth-d determinate clause

consistent with Dec. Let SubclauseC denote the language of subclauses of C , and let
Monomial[u] denote the language of monomials over u variables. Then there is a polynomial poly 1 so that for any database DB 2 DB,
SubclauseC [DB ; Dec]  Monomial[poly 1(j DB j )]

563

Cohen

Proof of lemma: Follows immediately from the construction used in Theorem 1 of

Dzeroski, Muggleton, and Russell (Dzeroski et al., 1992). (The basic idea of the construction is to introduce a propositional variable representing the \success" of each connected
chain of literals in C . Any subclause of C can then be represented as a conjunction of these
propositions.)
This lemma can be extended as follows.

Lemma 9 Let Dec 2 a-DetDEC , and let S = fC1; : : :; Crg be a set of r nonrecursive depth-

d determinate clauses consistent with Dec, each of length n or less. Let SubclauseS denote
the set of all programs of the form P = (D1; : : :; Ds) such that each Di is a subclause of
some Cj 2 S .
Then there is a polynomial poly 2 so that for any database DB 2 DB,
SubclauseS [DB ; Dec]  Dnf[poly 2 (j DB j ; r); ]

Proof of lemma: By Lemma 8, for each Ci 2 S , there is a set of variables Vi of size
polynomial in j DB j such
every clause in SubclauseC can be emulated by a monomial
Sr that
over
V
V
.
Clearly,
jV j is polynomial in n and r, and every clause in
i . Let V =
i
i=1
S
i

i

SubclauseC can be also emulated by a monomial over V . Further, every disjunction

of r such clauses can be represented by a disjunction of such monomials.
Since the Ci 's all satisfy a single declaration Dec = (p; a; R), they have heads with the
same principle function and arity; further, we may assume (without loss of generality, since
an equality predicate is assumed) that the variables appearing in the heads of these clauses
are all distinct. Since the Ci's are also nonrecursive, every program
P 2 SubclauseS can
S
be represented as a disjunction D1 _ : : : _ Dr where for all i, Di 2 ( i SubclauseC ). Hence
every P 2 SubclauseS can be represented by an r-term DNF over the set of variables V .
i

i

Let us now introduce some additional notation. If C and D are clauses, then we will use
C u D to denote the result of resolving C and D together, and C i to denote the result of
resolving C with itself i times. Note that C u D is unique if C is linear recursive and C and
D have the same predicate in their heads (since there will be only one pair of complementary
literals.)
Now, consider some target program

P = (CR; CB ) 2 d-Depth-2-Clause[DB ; Dec]
where CR is the recursive clause and CB is the base. The proof of any extended instance
(f; D) must use clause CR repeatedly h times and then use clause CB to resolve away
the nal subgoal. Hence the nonrecursive clause CRh u CB could also be used to cover the
instance (f; D).
Since the depth of any proof for this class of programs is bounded by a number hmax
that is polynomial in j DB j and ne , the nonrecursive program

P 0 = fCRh u CB : 0  h  hmax g
564

Pac-Learning Recursive Logic Programs: Negative Results

is equivalent to P on extended instances of size ne or less.
Finally, recall that we can assume that CB is a subclause of BOTTOM d ; also, there
is a polynomial-sized set LR = Lr1 ; : : :; Lr of closed recursive literals such that for some
Lr 2 LR , the clause CR is a subclause of BOTTOM d [ Lr . This means that if we let S
be the polynomial-sized set
S1 = f(BOTTOM d [ Lr )h u BOTTOM d j 0  h  hmax and Lr 2 LR g
then P 0 2 SubclauseS1 . Thus by Lemma 9, d-Depth-2-Clause  Dnf. This concludes
the proof of the rst statement in the the theorem.
To show that
d-Depth-2-Clause0h(n) [DB ; Dec]  Dnf[poly (j DB j ; ]
a similar argument applies. Let us again introduce some notation, and dene
MESHh;n (CR1 ; CR2 ) as the set of all clauses of the form
p

i

i

i

i

CR 1 u CR 2 u : : : u CR 0
where for all j , CR = CR1 or CR = CR2 , and h0  h(n). Notice that for functions
h(n)  c log n the number of such clauses is polynomial in n.
Now let p be the predicate appearing in the heads of CR1 and CR2 , and let C^ (respectively
^ ) be a a version of C (DB ) in which every instance of the predicate p has been replaced
DB
with a new predicate p^. If P is a recursive program P = fCR1 ; CR2 g in d-Depth-2-Clause0
^ ,
over the database DB , then P ^ DB is equivalent4 to the nonrecursive program P 0 ^ DB
i;

ij

where

i;

i;h

ij

P 0 = fC^ j C 2 MESHh;n (CR1 ; CR2 )g
e

Now recall that there are a polynomial number of recursive literals Lr , and hence a
polynomial number of pairs of recursive literals Lr ; Lr . This means that the set of clauses
[
S2 =
fC^ j C 2 MESHh;n (BOTTOM d [ Lr ; BOTTOM d [ Lr )g
i

i

(L

ri

e

2 

;Lrj ) LR LR

j

i

j

is also polynomial-sized; furthermore, for any program P in the language d-Depth-2-Clause,
P 0 2 SubclauseS2 . The second part of the theorem now follows by application of Lemma 9.
An immediate corollary of this result is that Theorems 6 and 5 can be strengthened as
follows.
Corollary 10 For all constants d  1 and a  2, the language family
d-Depth-2-Clause[DB; a-DetDEC ]
is uniformly polynomially predictable if and only if DNF is polynomially predictable.
For all constants d  1 and a  2, the language family
d-Depth-2-Clause0 [DB; a-DetDEC ]
is uniformly polynomially predictable if and only if DNF is polynomially predictable.
4. On extended instances of size n or less.
e

565

Cohen

Thus in an important sense these learning problems are equivalent to learning boolean
DNF. This does not resolve the questions of the learnability of these languages, but does
show that their learnability is a dicult formal problem: the predictability of boolean DNF
is a long-standing open problem in computational learning theory.

5. Related Work
The work described in this paper diers from previous formal work on learning logic programs in simultaneously allowing background knowledge, function-free programs, and recursion. We have also focused exclusively on computational limitations on ecient learnability
that are associated with recursion, as we have considered only languages known to be paclearnable in the nonrecursive case. Since the results of this paper are all negative, we have
concentrated on the model of polynomial predictability; negative results in this model immediately imply a negative result in the stronger model of pac-learnability, and also imply
negative results for all strictly more expressive languages.
Among the most closely related prior results are the negative results we have previously
obtained for certain classes of nonrecursive function-free logic programs (Cohen, 1993b).
These results are similar in character to the results described here, but apply to nonrecursive
languages. Similar cryptographic results have been obtained by Frazier and Page (1993) for
certain classes of programs (both recursive and nonrecursive) that contain function symbols
but disallow background knowledge.
Some prior negative results have also been obtained on the learnability of other rstorder languages using the proof technique of consistency hardness (Pitt & Valiant, 1988).
Haussler (1989) showed that the language of \existential conjunction concepts" is not paclearnable by showing that it can be hard to nd a concept in the language consistent with a
given set of examples. Similar results have also been obtained for two restricted languages
of Horn clauses (Kietz, 1993); a simple description logic (Cohen & Hirsh, 1994); and for the
language of sorted rst-order terms (Page & Frisch, 1992). All of these results, however, are
specic to the model pac-learnability, and none can be easily extended to the polynomial
predictability model considered here. The results also do not extend to languages more
expressive than these specic constrained languages. Finally, none of these languages allow
recursion.
To our knowledge, there are no other negative learnability results for rst-order languages. A discussion of prior positive learnability results for rst-order languages can be
found in the companion paper (Cohen, 1995).

6. Summary
This paper and its companion (Cohen, 1995) have considered a large number of dierent
subsets of Datalog. Our aim has been to be not comprehensive, but systematic: in particular, we wished to nd precisely where the boundaries of learnability lie as various syntactic
restrictions are imposed and relaxed. Since it is all too easy for a reader to \miss the forest
for the trees", we will now briey summarize the results contained in this paper, together
with the positive results of the companion paper (Cohen, 1995).
566

Pac-Learning Recursive Logic Programs: Negative Results

Local
Clauses

Constant-Depth Determinate
Clauses

nCR,

nCR,

nCR jCB,

nCR ; CB,

k  nCR,

n  nCR,

kCR,

kCR+

kCRjCB+

kCR; CBDNF

k  k0CRDNF

n  kCR,

1CR,

1CR+

1CRjCB+

1CR; CB=DNF

2  1CR=DNF

n  1CR,

Table 1: A summary of the learnability results
Throughout these papers, we have assumed that a polynomial amount of background
knowledge exists; that the programs being learned contain no function symbols; and that
literals in the body of a clause have small arity. We have also assumed that recursion is
closed , meaning that no output variables appear in a recursive clause; however, we believe
that this restriction can be relaxed without fundamentally changing the results of the paper.
In the companion paper (Cohen, 1995) we showed that a single nonrecursive constantdepth determinate clause was learnable in the strong model of identication from equivalence
queries . In this learning model, one is given access to an oracle for counterexamples|that
is, an oracle that will nd, in unit time, an example on which the current hypothesis is
incorrect|and must reconstruct the target program exactly from a polynomial number of
these counterexamples. This result implies that a single nonrecursive constant-depth determinate clause is pac-learnable (as the counterexample oracle can be emulated by drawing
random examples in the pac setting). The result is not novel (Dzeroski et al., 1992); however
the proof given is independent, and is also of independent interest. Notably, it is somewhat
more rigorous than earlier proofs, and also proves the result directly, rather than via reduction to a propositional learning problem. The proof also introduces a simple version of the
forced simulation technique, variants of which are used in all of the positive results.
We then showed that the learning algorithm for nonrecursive clauses can be extended
to the case of a single linear recursive constant-depth determinate clause, leading to the
result that this restricted class of recursive programs is also identiable from equivalence
queries. With a bit more eort, this algorithm can be further extended to learn a single
k-ary recursive constant-depth determinate clause.
We also considered extended the learning algorithm to learn recursive programs consisting of more than one constant-depth determinate clauses. The most interesting extension
was to simultaneously learn a recursive clause CR and a base clause CB , using equivalence
queries and also a \basecase oracle" that indicates which counterexamples should be covered
by the base clause CB . In this model, it is possible to simultaneously learn a recursive clause
and a nonrecursive base case in all of the situations for which a recursive clause is learned
567

Cohen

Language Family
d-DepthNonRec[a-DB; a-DetDEC]
d-DepthLinRec[a-DB; a-DetDEC]
d-Depth-k-Rec[a-DB; a-DetDEC]
d-Depth-2-Clause[a-DB; a-DetDEC]
kd-MaxRecLang[a-DB; a-DetDEC ]
d-Depth-2-Clause[a-DB; a-DetDEC]
d-Depth-2-Clause [a-DB; a-DetDEC ]
d-DepthLinRecProg[a-DB; a-DetDEC ]
d-DepthRec[a-DB; a-DetDEC ]
k-LocalLinRec[a-DB; a-DEC ]
0

B
1
0
0
1
1
1
0
0
0
0

R
0
1
1
1
1
1
2

L/R Oracles
, EQ
1
EQ
k
EQ
1
EQ,BASE
k
EQ,BASE
1
EQ
1
EQ
n 1
EQ
1 n
EQ
1 1
EQ

Notation Learnable
CB
yes
1CR
yes
kCR
yes
1CRjCB yes
kCRjCB
yes
1CR; CB =DNF
2  1CR =DNF
n  1CR no
nCR
no
1CR
no

Table 2: Summary by language of the learnability results. Column B indicates the number
of base (nonrecursive) clauses allowed in a program; column R indicates the number of recursive clauses; L/R indicates the number of recursive literals allowed in
a single recursive clause; EQ indicates an oracle for equivalence queries and BASE
indicates a basecase oracle. For all languages except k-LocalLinRec, all clauses
must be determinate and of depth d.
alone; for instance, one can learn a k-ary recursive clause to together with its nonrecursive
base case. This was our strongest positive result.
These results are summarized in Tables 1 and 2. In Table 1, a program with one rary recursive clause is denoted rCR, a program with one r-ary recursive clause and one
nonrecursive basecase is denoted rCR; CB , or rCRjCB if there is a \basecase" oracle, and
a program with s dierent r-ary recursive clauses is denoted s  rCR . The boxed results
are associated with one or more theorems from this paper, or its companion paper, and
the unmarked results are corollaries of other results. A \+" after a program class indicates
that it is identiable from equivalence queries; thus the positive results described above are
summarized by the four \+" entries in the lower left-hand corner of the section of the table
concerned with constant-depth determinate clauses.
Table 2 presents the same information in a slightly dierent format, and also relates the
notation of Table 1 to the terminology used elsewhere in the paper.
This paper has considered the learnability of the various natural generalizations of the
languages shown to be learnable in the companion paper. Consider for the moment single
clauses. The companion paper showed that for any xed k a single k-ary recursive constantdepth determinate clause is learnable. Here we showed that all of these restrictions are
necessary. In particular, a program of n constant-depth linear recursive clauses is not
polynomially predictable; hence the restriction to a single clause is necessary. Also, a single
clause with n recursive calls is hard to learn; hence the restriction to k-ary recursion is
necessary. We also showed that the restriction to constant-depth determinate clauses is
necessary, by considering the learnability of constant locality clauses . Constant locality
clauses are the only known generalization of constant-depth determinate clauses that are
pac-learnable in the nonrecursive case. However, we showed that if recursion is allowed,
568

Pac-Learning Recursive Logic Programs: Negative Results

then this language is not learnable: even a single linear recursive clause is not polynomially
predictable.
Again, these results are summarized in Table 1; a \," after a program class means that
it is not polynomially predictable, under cryptographic assumptions, and hence neither
pac-learnable nor identiable from equivalence queries.
The negative results based on cryptographic hardness give an upper bound on the expressiveness of learnable recursive languages, but still leave open the learnability of programs
with a constant number of k-ary recursive clauses in the absence of a basecase oracle. In
the nal section of this paper, we showed that the following problems are, in the model of
polynomial predictability, equivalent to predicting boolean DNF:
 predicting two-clause constant-depth determinate recursive programs containing one
linear recursive clause and one base case;
 predicting two-clause recursive constant-depth determinate programs containing two
linear recursive clauses, even if the base case is known.
We note that these program classes are the very nearly the simplest classes of multi-clause
recursive programs that one can imagine, and that the pac-learnability of DNF is a longstanding open problem in computational learning theory. These results suggest, therefore,
that pac-learning multi-clause recursive logic programs is dicult; at the very least, they
show that nding a provably correct pac-learning algorithm will require substantial advances
in computational learning theory. In Table 1, a \= Dnf" (respectively  Dnf) means that
the corresponding language is prediction-equivalent to DNF (respectively at least as hard
as DNF).
To further summarize Table 1: with any sort of recursion, only programs containing
constant-depth determinate clauses are learnable. The only constant-depth determinate
recursive programs that are learnable are those that contain a single k-ary recursive clause
(in the standard equivalence query model) or a single k-ary recursive clause plus a base
case (if a \basecase oracle" is allowed). All other classes recursive programs are either
cryptographically hard, or as hard as boolean DNF.

7. Conclusions

Inductive logic programming is an active area of research, and one broad class of learning
problems considered in this area is the class of \automatic logic programming" problems.
Prototypical examples of this genre of problems are learning to append two lists, or to
multiply two numbers. Most target concepts in automatic logic programming are recursive
programs, and often, the training data for the learning system are simply examples of the
target concept, together with suitable background knowledge.
The topic of this paper is the pac-learnability of recursive logic programs from random
examples and background knowledge; specically, we wished to establish the computational
limitations inherit in performing this task. We began with some positive results established
in a companion paper. These results show that one constant-depth determinate closed k-ary
recursive clause is pac-learnable, and that further, a program consisting of one such recursive
clause and one constant-depth determinate nonrecursive clause is also pac-learnable given
an additional \basecase oracle".
569

Cohen

In this paper we showed that these positive results are not likely to be improved. In
particular, we showed that either eliminating the basecase oracle or learning two recursive clauses simultaneously is prediction-equivalent to learning DNF, even in the case of
linear recursion. We also showed that the following problems are as hard as breaking (presumably) secure cryptographic codes: pac-learning n linear recursive determinate clauses,
pac-learning one n-ary recursive determinate clause, or pac-learning one linear recursive
k-local clause.
These results contribute to machine learning in several ways. From the point of view
of computational learning theory, several results are technically interesting. One is the
prediction-equivalence of several classes of restricted logic programs and boolean DNF; this
result, together with others like it (Cohen, 1993b), reinforces the importance of the learnability problem for DNF. This paper also gives a dramatic example of how adding recursion
can have widely diering eects on learnability: while constant-depth determinate clauses
remain pac-learnable when linear recursion is added, constant-locality clauses become cryptographically hard.
Our negative results show that systems which apparently learn a larger class of recursive
programs must be taking advantage either of some special properties of the target concepts
they learn, or of the distribution of examples that they are provided with. We believe that
the most likely opportunity for obtaining further positive formal results in this area is to
identify and analyze these special properties. For example, in many examples in which
FOIL has learned recursive logic programs, it has made use of \complete example sets"|
datasets containing all examples of or below a certain size, rather than sets of randomly
selected examples (Quinlan & Cameron-Jones, 1993). It is possible that complete datasets
allow a more expressive class of programs to be learned than random datasets; in fact, some
progress has been recently made toward formalizing this conjecture (De Raedt & Dzeroski,
1994).
Finally, and most importantly, this paper has established the boundaries of learnability
for determinate recursive programs in the pac-learnability model. In many plausible automatic programming contexts it would be highly desirable to have a system that oered some
formal guarantees of correctness. The results of this paper provide upper bounds on what
one can hope to achieve with an ecient, formally justied system that learns recursive
programs from random examples alone.

Acknowledgements
The author wishes to thank three anonymous JAIR reviewers for a number of useful suggestions on the presentation and technical content.

References
Aha, D., Lapointe, S., Ling, C. X., & Matwin, S. (1994). Inverting implication with small
training sets. In Machine Learning: ECML-94 Catania, Italy. Springer-Verlag. Lecture
Notes in Computer Science # 784.
570

Pac-Learning Recursive Logic Programs: Negative Results

Biermann, A. (1978). The inference of regular lisp programs from examples. IEEE Transactions on Systems, Man and Cybernetics, 8 (8).
Chandra, A. K., Kozen, D. C., & Stockmeyer, L. J. (1981). Alternation. Journal of the
ACM, 28, 114{113.
Cohen, W. W. (1993a). Cryptographic limitations on learning one-clause logic programs. In
Proceedings of the Tenth National Conference on Articial Intelligence Washington,
D.C.
Cohen, W. W. (1993b). Pac-learning non-recursive Prolog clauses. To appear in Articial
Intelligence.
Cohen, W. W. (1993c). Rapid prototyping of ILP systems using explicit bias. In Proceedings
of the 1993 IJCAI Workshop on Inductive Logic Programming Chambery, France.
Cohen, W. W. (1994a). Pac-learning nondeterminate clauses. In Proceedings of the Eleventh
National Conference on Articial Intelligence Seattle, WA.
Cohen, W. W. (1994b). Recovering software specications with inductive logic programming. In Proceedings of the Eleventh National Conference on Articial Intelligence
Seattle, WA.
Cohen, W. W. (1995). Pac-learning recursive logic programs: ecient algorithms. Journal
of AI Research, 2, 501{539.
Cohen, W. W., & Hirsh, H. (1994). The learnability of description logics with equality
constraints. Machine Learning, 17 (2/3).
De Raedt, L., & Dzeroski, S. (1994). First-order jk-clausal theories are PAC-learnable.
In Wrobel, S. (Ed.), Proceedings of the Fourth International Workshop on Inductive
Logic Programming Bad Honnef/Bonn, Germany.
Dzeroski, S., Muggleton, S., & Russell, S. (1992). Pac-learnability of determinate logic
programs. In Proceedings of the 1992 Workshop on Computational Learning Theory
Pittsburgh, Pennsylvania.
Frazier, M., & Page, C. D. (1993). Learnability of recursive, non-determinate theories: Some
basic results and techniques. In Proceedings of the Third International Workshop on
Inductive Logic Programming Bled, Slovenia.
Haussler, D. (1989). Learning conjunctive concepts in structural domains. Machine Learning, 4 (1).
Hopcroft, J. E., & Ullman, J. D. (1979). Introduction to Automata Theory, Languages, and
Computation. Addison-Wesley.
Kearns, M., & Valiant, L. (1989). Cryptographic limitations on learning Boolean formulae
and nite automata. In 21th Annual Symposium on the Theory of Computing. ACM
Press.
571

Cohen

Kietz, J.-U. (1993). Some computational lower bounds for the computational complexity
of inductive logic programming. In Proceedings of the 1993 European Conference on
Machine Learning Vienna, Austria.
King, R. D., Muggleton, S., Lewis, R. A., & Sternberg, M. J. E. (1992). Drug design by
machine learning: the use of inductive logic programming to model the structureactivity relationships of trimethoprim analogues binding to dihydrofolate reductase.
Proceedings of the National Academy of Science, 89.
Lavrac, N., & Dzeroski, S. (1992). Background knowledge and declarative bias in inductive
concept learning. In Jantke, K. P. (Ed.), Analogical and Inductive Inference: International Workshop AII'92. Springer Verlag, Daghstuhl Castle, Germany. Lectures in
Articial Intelligence Series #642.
Lloyd, J. W. (1987). Foundations of Logic Programming: Second Edition. Springer-Verlag.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory and methods.
Journal of Logic Programming, 19/20 (7), 629{679.
Muggleton, S., & Feng, C. (1992). Ecient induction of logic programs. In Inductive Logic
Programming. Academic Press.
Muggleton, S., King, R. D., & Sternberg, M. J. E. (1992). Protein secondary structure
prediction using logic-based machine learning. Protein Engineering, 5 (7), 647{657.
Muggleton, S. H. (Ed.). (1992). Inductive Logic Programming. Academic Press.
Page, C. D., & Frisch, A. M. (1992). Generalization and learnability: A study of constrained
atoms. In Inductive Logic Programming. Academic Press.
Pazzani, M., & Kibler, D. (1992). The utility of knowledge in inductive learning. Machine
Learning, 9 (1).
Pitt, L., & Warmuth, M. K. (1988). Reductions among prediction problems: On the difculty of predicting automata. In Proceedings of the 3rd Annual IEEE Conference
on Structure in Complexity Theory Washington, D.C. Computer Society Press of the
IEEE.
Pitt, L., & Valiant, L. (1988). Computational limitations on learning from examples. Journal
of the ACM, 35 (4), 965{984.
Pitt, L., & Warmuth, M. (1990). Prediction-preserving reducibility. Journal of Computer
and System Sciences, 41, 430{467.
Quinlan, J. R., & Cameron-Jones, R. M. (1993). FOIL: A midterm report. In Brazdil, P. B.
(Ed.), Machine Learning: ECML-93 Vienna, Austria. Springer-Verlag. Lecture notes
in Computer Science # 667.
Quinlan, J. R. (1990). Learning logical denitions from relations. Machine Learning, 5 (3).
572

Pac-Learning Recursive Logic Programs: Negative Results

Quinlan, J. R. (1991). Determinate literals in inductive logic programming. In Proceedings
of the Eighth International Workshop on Machine Learning Ithaca, New York. Morgan
Kaufmann.
Rouveirol, C. (1994). Flattening and saturation: two representation changes for generalization. Machine Learning, 14 (2).
Summers, P. D. (1977). A methodology for LISP program construction from examples.
Journal of the Association for Computing Machinery, 24 (1), 161{175.
Valiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27 (11).
Zelle, J. M., & Mooney, R. J. (1994). Inducing deterministic Prolog parsers from treebanks:
a machine learning approach. In Proceedings of the Twelfth National Conference on
Articial Intelligence Seattle, Washington. MIT Press.

573

Journal of Articial Intelligence Research 2 (1994) 131-158

Submitted 4/94; published 12/94

Wrap-Up: a Trainable Discourse
Module for Information Extraction
Stephen Soderland

Wendy Lehnert
Department of Computer Science, University of Massachusetts
Amherst, MA 01003-4610

soderlan@cs.umass.edu
lehnert@cs.umass.edu

Abstract

The vast amounts of on-line text now available have led to renewed interest in information
extraction (IE) systems that analyze unrestricted text, producing a structured representation of selected information from the text. This paper presents a novel approach that
uses machine learning to acquire knowledge for some of the higher level IE processing.
Wrap-Up is a trainable IE discourse component that makes intersentential inferences and
identies logical relations among information extracted from the text. Previous corpusbased approaches were limited to lower level processing such as part-of-speech tagging,
lexical disambiguation, and dictionary construction. Wrap-Up is fully trainable, and not
only automatically decides what classiers are needed, but even derives the feature set for
each classier automatically. Performance equals that of a partially trainable discourse
module requiring manual customization for each domain.

1. Introduction

An information extraction (IE) system analyzes unrestricted, real world text such as newswire
stories. In contrast to information retrieval systems which return a pointer to the entire
document, an IE system returns a structured representation of just the information from
within the text that is relevant to a user's needs, ignoring irrelevant information.
The rst stage of an IE system, sentence analysis, identies references to relevant objects
and typically creates a case frame to represent each object. The second stage, discourse
analysis, merges together multiple references to the same object, identies logical relationships between objects, and infers information not explicitly identied by sentence analysis.
The IE system operates in terms of domain specications that predene what types of information and relationships are considered relevant to the application. Considerable domain
knowledge is used by an IE system: about domain objects, relationships between objects,
and how texts typically describe these objects and relationships.
Much of the domain knowledge can be automatically acquired by corpus-based techniques. Previous work has centered on knowledge acquisition for some of the lower level
processing such as part-of-speech tagging and lexical disambiguation. N-gram statistics have
been highly successful in part-of-speech tagging (Church, 1988; DeRose, 1988). Weischedel
(1993) has used corpus-based probabilities both for part-of-speech tagging and to guide
parsing. Collocation data has been used for lexical disambiguation by Hindle (1989), Brent
(1993), and others. Examples from a training corpus have driven both part-of-speech and
semantic tagging (Cardie, 1993) and dictionary construction (Rilo, 1993).

c 1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Soderland and Lehnert

This paper describes Wrap-Up (Soderland & Lehnert, 1994), the rst system to automatically acquire domain knowledge for the higher level processing associated with discourse
analysis. Wrap-Up uses supervised learning to induce a set of classiers from a training corpus of representative texts, where each text is accompanied by hand-coded target output.
We implemented Wrap-Up with the ID3 decision tree algorithm (Quinlan, 1986), although
other machine learning algorithms could have been selected.
Wrap-Up is a fully trainable system and is unique in that it not only decides what
classiers are needed for the domain, but automatically derives the feature set for each
classier. The user supplies a denition of the objects and relationships of interest to the
domain and a training corpus with hand-coded target output. Wrap-Up does the rest with
no further hand coding needed to tailor the system to a new domain.
Section 2 discusses the IE task in more detail, introduces the microelectronics domain,
and gives an overview of the CIRCUS sentence analyzer. Section 3 describes Wrap-Up,
giving details of how ID3 trees are constructed for each discourse decision, how features
are automatically derived for each tree, and requirements for applying Wrap-Up to a new
domain. Section 4 shows the performance of Wrap-Up in two domains and compares its
performance to that of a partially trainable discourse component. In Section 5 we draw
some conclusions about the contribution of this research. A detailed example from the
microelectronics domain is given in an appendix.

2. The Information Extraction Task

This section gives an overview of information extraction and illustrates IE processing with
a sample text fragment from the microelectronics domain. We then discuss the need for
trainable IE components to acquire knowledge for a new domain.

2.1 An Overview of IE

An information extraction system operates at two levels. First, sentence analysis identies
information that is relevant to the IE application. Then discourse analysis, which we will
focus on in this paper, takes the output from sentence analysis and assembles it into a
coherent representation of the entire text. All of this is done according to predened guidelines that specify what objects from the text are relevant and what relationships between
objects are to be reported.
Sentence analysis can be further broken down into several stages, each applying dierent
types of domain knowledge. The lowest level is preprocessing, which segments the text into
words and sentences. Each word is assigned a part-of-speech tag and possibly a semantic
tag in preparation for further processing. Dierent IE systems will do varying amounts of
syntactic parsing at this point. Most research sites that participated in the ARPA-sponsored
Message Understanding Conferences (MUC-3, 1991; MUC-4, 1992; MUC-5, 1993) found
that robust, shallow analysis and pattern matching performed better than more elaborate,
but brittle, parsing techniques.
The CIRCUS sentence analyzer (Lehnert, 1990; Lehnert et al., 1992) does shallow syntactic analysis to identify simple syntactic constituents, and to distinguish active and passive
voice verbs. This shallow syntactic analysis is sucient for the extraction task, which uses
132

Wrap-Up: a Trainable Discourse Module

local linguistic patterns to instantiate case frames, called concept nodes (CN's) used by
CIRCUS.
Each CN denition has a trigger word and a syntactic pattern relative to that word.
Whenever the trigger word occurs in the text, CIRCUS looks in one of the syntactic buers
for appropriate information to extract. Some CN denitions will extract information from
the subject or from the direct object, rst testing for active or passive voice. Other CN
denitions look for a prepositional phrase with a particular preposition. Examples of CN
extraction patterns from a particular domain are shown in Section 2.3.
Discourse analysis starts with the output from the sentence analyzer, in this case a set
of concept nodes representing locally extracted information. Other work on discourse has
often involved tracking shifts in topic and in the speaker/writer's goals (Grosz & Sidner,
1986; Liddy et al., 1993) or in resolving anaphoric references (Hobbs, 1978). Discourse
processing in an IE system may concern itself with some of these issues, but only as a
means to its main objective of transforming bits and pieces of extracted information into a
coherent representation.
One of the rst tasks of discourse analysis is to merge together multiple references to
the same object. In a domain where company names are important, this will involve recognizing the equivalence of a full company name (\International Business Machines, Inc.")
with shortened forms of that name (\IBM") and generic references (\the company", \the
U.S. computer maker"). Some manually engineered rules seem unavoidable for coreference
merging. Another example is merging a domain object with a less specic reference to that
object. In the microelectronics domain a reference to \DRAM" chips may be merged with
a reference to \memory" or an \I-line" process merged with \lithography."
Much of the work of discourse analysis is to identify logical relationships between extracted objects, represented as pointers between objects in the output. Discourse analysis
must also be able to infer missing objects that are not explicitly stated in the text and in
some cases split an object into multiple copies or discard an object that was erroneously
extracted.
The current implementation of Wrap-Up begins discourse processing after coreference
merging has been done by a separate module. This is primarily because manual engineering
seems unavoidable in coreference. Work is underway to extend Wrap-Up to include all of IE
discourse processing by incorporating a limited amount of domain-specic code to handle
such things as company name aliases and generic references to domain objects.
Wrap-Up divides its processing into six stages, which will be described more fully in
Section 3. They are:
1. Filtering out spuriously extracted information
2. Merging objects with their attributes
3. Linking logically related objects
4. Deciding when to split objects into multiple copies
5. Inferring missing objects
6. Adding default slot values
At this point an example from a specic domain might help. The following sections introduce the microelectronics domain, then illustrate sentence analysis and discourse analysis
with a short example from this domain.
133

Soderland and Lehnert

2.2 The Microelectronics Domain

The microelectronics domain was one of the two domains targetted by the Fifth Message
Understanding Conference (MUC-5, 1993). According to the domain and task guidelines
developed for the MUC-5 microelectronics corpus, the information to be extracted are microchip fabrication processes along with the companies, equipment, and devices associated
with these processes. There are seven types of domain objects to be identied: entities (i.e.
companies), equipment, devices, and four chip fabrication processes (layering, lithography,
etching, and packaging).
Identifying relationships between objects is of equal importance in this domain to identifying the objects themselves. A company must be identied as playing at least one of four
possible roles with respect to the microchip fabrication process: developer, manufacturer,
distributor, or purchaser/user. Microchip fabrication processes are reported only if they are
associated with a specic company in at least one of these roles. Each equipment object
must be linked to a process which uses that equipment, and each device object linked to
a process which fabricates that device. Equipment objects may point to a company as
manufacturer and to other equipment as modules.
The following sample from the MUC-5 microelectronics domain has two companies in the
rst sentence, which are associated with two lithography processes from the second sentence.
GCA and Sematech are developers of both the UV and I-line lithography processes, with
GCA playing the additional role of manufacturer. Each lithography process is linked to the
stepper equipment mentioned in sentence one.
GCA unveiled its new XLS stepper, which was developed with
assistance from Sematech. The system will be available in
deep-ultraviolet and I-line configurations.

Figure 1 shows the ve domain objects extracted by sentence analysis and the nal
representation of the text after discourse analysis has identied relationships between objects. Some of these relationships are directly indicated by pointers between objects. The
roles that companies play with respect to a microchip fabrication process are indicated by
creating a \microelectronics-capability" object with pointers to both the process and the
companies.

2.3 Extraction Patterns

How does sentence analysis identify GCA and Sematech as company names, and extract the
other domain objects such as stepper equipment, UV lithography and I-line lithography?
The CN dictionary for this domain includes an extraction pattern \X unveiled" to identify
company names. The subject of the active verb \unveiled" in this domain is nearly always
a company developing or distributing a new device or process. However, this pattern will
occasionally pick up a company that fails the domain's reportability criteria. A company
that unveils a new type of chip should be discarded if the text does not specify the fabrication
process.
Extracting the company name \Sematech" is more dicult since the pattern \assistance
from X" is not a reliable predictor of relevant company names. There is always a trade-o
between accuracy and complete coverage in deciding what extraction patterns are reliable
134

Wrap-Up: a Trainable Discourse Module

A. Five concept nodes extracted by sentence analysis.
Entity
Type: company
Name: GCA

Equipment
Type: stepper
Name: XLS

Lithography
Type: UV

Lithography
Type: I-line

Entity
Type: company
Name: Sematech

B. Final representation of the text after discourse analysis.
Template
Contents:
ME-Capability
Manufacturer:
Developer:
Process:

ME-Capability
Manufacturer:
Developer:
Process:

Entity
Type: company
Name: GCA

Entity
Type: company
Name: Sematech
Lithography
Type: UV
Equipment:

Lithography
Type: I-line
Equipment:

Equipment
Type: stepper
Name: XLS
Manufacturer:
Status: in-development

Figure 1: Output of (A) sentence analysis and (B) discourse analysis
enough to include in the CN dictionary. Including less reliable patterns increases coverage
but does so at the expense of spurious extraction. The more specic pattern \developed
with assistance from X" is reliable, but was missed by the dictionary construction tool
(Rilo, 1993).
For many of the domain objects, such as equipment, devices, and microchip fabrication
processes, the set of possible objects is predened and a list of keywords that refer to these
objects can be created. The extraction pattern \unveiled X" looks in the direct object
of the active verb \unveiled", instantiating an equipment object if a keyword indicating
an equipment type is found. In this example an equipment object with type \stepper" is
created with the equipment name \XLS". The same stepper equipment is also extracted by
135

Soderland and Lehnert

the pattern \X was developed", which looks for equipment in the subject of the passive verb
\developed". This equipment object is extracted a third time by the keyword \stepper"
itself, which is sucient to instantiate a stepper equipment object whether or not it occurs
in a reliable extraction pattern.
The keyword \deep-ultraviolet" and the extraction pattern \available in X" are used to
extract a lithography object with type \UV" from the second sentence. Another lithography
object of type \I-line" is similarly extracted. Case frames are created for each of the objects
identied by sentence analysis. This set of objects becomes input for the next stage of
processing, discourse analysis.

2.4 Discourse Processing

In the full text from which this fragment comes, there are likely to be other references to
\GCA" or to \GCA Corp." One of the rst jobs of discourse analysis is to merge these
multiple references. It is a much harder task to merge pronominal references and generic
references such as \the company" with the appropriate company name. This is all part of
the coreference problem that is handled by processes separate from Wrap-Up.
The main job of discourse analysis is to determine the relationships between the objects
passed to it by sentence analysis. Considerable domain knowledge is needed to make these
discourse-level decisions. Some of this knowledge concerns writing style, and specic phrases
writers typically use to imply relationships between referents in a given domain. Is the
phrase \<company> unveiled <equipment>" sucient evidence to infer that the company
is the developer of a microelectronics process? The word \unveiled" alone is not enough,
since a company that unveiled a new DRAM chip may not be the developer of any new
process. It may simply be using someone else's microelectronics process to produce its chip.
Such inferences, particularly those about what role a company plays in a process, are often
so subtle that two human analysts may disagree on the output for a given text. A human
performance study for this task found that experienced analysts agreed with each other on
only 80% on their text interpretations in this domain (Will, 1993).
World knowledge is also needed about the relationships possible between domain objects. A lithography process may be linked to stepper equipment, but steppers are never
used in layering, etching, or packaging processes. There are delicate dependencies about
what types of process are likely to fabricate what types of devices. Knowledge about the
kinds of relationships typically reported in this domain can also help guide discourse processing. Stories about lithography, for example, often give the developer, manufacturer,
or distributor of the process, but these roles are hardly ever mentioned for packaging processes. Companies associated with packaging tend to be limited to the purchaser/user of
the packaging technology.
A wide range of domain knowledge is needed for discourse processing, some of it related
to world knowledge, some to writing style. The next section discusses the need for trainable components at all levels of IE processing, including discourse analysis. Wrap-Up uses
machine learning techniques to avoid months of manual knowledge engineering otherwise
required to develop a specic IE application.

136

Wrap-Up: a Trainable Discourse Module

2.5 The Need for Trainable IE Components

The highest performance at the ARPA-sponsored Fifth Message Understanding Conference
(MUC-5, 1993) was achieved at the cost of nearly two years of intense programming eort,
adding domain-specic heuristics and domain-specic linguistic patterns one by one, followed by various forms of system tuning to maximize performance. For many real world
applications, two years of development time by a team of half a dozen programmers would
be prohibitively expensive. To make matters worse, the knowledge used in one domain
cannot be readily transferred to other IE applications.
Researchers at the University of Massachusetts have worked to facilitate IE system development through the use of corpus-driven knowledge acquisition techniques (Lehnert et
al., 1993). In 1991 a purely hand-crafted UMass system had the highest performance of
any site in the MUC-3 evaluation. The following year UMass ran both a hand-crafted system and an alternate system that replaced a key component with output from AutoSlog, a
trainable dictionary construction tool (Rilo, 1993). The AutoSlog variant exhibited performance levels comparable to a dictionary based on 1500 hours of manual coding. Encouraged
by the success of this one trainable component, an architecture for corpus-driven system
development was proposed which uses machine learning techniques to address a number
of natural language processing problems (Lehnert et al., 1993). In the MUC-5 evaluation,
output from the CIRCUS sentence analyzer was sent to TTG (Trainable Template Generator), a discourse component developed by Hughes Research Laboratories (Dolan, et al.,
1991; Lehnert et al., 1993). TTG used machine learning techniques to acquire much of the
needed domain knowledge, but still required hand-coded heuristics to turn this acquired
knowledge into a fully functioning discourse analyzer.
The remainder of this paper will focus on Wrap-Up, a new IE discourse module now
under development which explores the possibility of fully automated knowledge acquisition
for discourse analysis. As detailed in the following sections, Wrap-Up builds ID3 decision
trees to guide discourse processing and requires no hand-coded customization for a new
domain once a training corpus has been provided. Wrap-Up automatically decides what
ID3 trees are needed for the domain and derives the feature set for each tree from the output
of the sentence analyzer.

3. Wrap-Up, a Trainable IE Component

This section describes the Wrap-Up algorithm, how decision trees are used for discourse
analysis, and how the trees and tree features are automatically generated. We conclude
with a discussion of the requirements of Wrap-Up and our experience porting to a new
domain.

3.1 Overview

Wrap-Up is a domain-independent framework for IE discourse processing which is instantiated with automatically acquired knowledge for each new IE application. During its training
phase, Wrap-Up builds ID3 decision trees based on a representative set of training texts,
paired against hand-coded output keys. These ID3 trees guide Wrap-Up's processing during
run time.
137

Soderland and Lehnert

At run time Wrap-Up receives as input all objects extracted from the text during sentence analysis. Each of these objects is represented as a case frame along with a list of
references in the text, the location of each reference, and the linguistic patterns used to
extract it. Multiple references to the same object throughout the text are merged together
before passing it on to Wrap-Up. Wrap-Up transforms this set of objects by discarding
spurious objects, merging objects that add further attributes to an object, adding pointers
between objects, and inferring the presence of any missing objects or slot values.
Wrap-Up has six stages of processing, each with its own set of decision trees designed
to transform objects as they are passed from one stage to the next.
Stages in the Wrap-up Algorithm:
1. Slot Filtering
Each object slot has its own decision tree that judges whether the slot contains reliable
information. Discard the slot value from an object if a tree returns \negative".
2. Slot Merging
Create an instance for each pair of objects of the same type. Merge the two objects
if a decision tree for that object type returns \positive". This stage can merge an
object with separately extracted attributes for that object.
3. Link Creation
Consider all possible pairs of objects that might possibly be linked. Add a pointer
between objects if a Link Creation decision tree returns \positive".
4. Object Splitting
Suppose object A is linked to both object B and to object C. If an Object Splitting
decision tree returns \positive", split A into two copies with one pointing to B and
the other to C.
5. Inferring Missing Objects
When an object has no other object pointing to it, an instance is created for a decision
tree which returns the most likely parent object. Create such a parent and link it to
the \orphan" object unless the tree returns \none". Then use decision trees from the
Link Creation and Object Splitting stages to tie the new parent in with other objects.
6. Inferring Missing Slot Values
When an object slot with a closed class of possible values is empty, create an instance
for a decision tree which returns a context-sensitive default value for that slot, possibly
\none".

3.2 Decision Trees for Discourse Analysis

A key to making machine learning work for a complex task such as discourse processing
is to break the problem into a number of small decisions and build a separate classier
138

Wrap-Up: a Trainable Discourse Module

for each. Each of the six stages of Wrap-Up described in Section 3.1 has its own set of
ID3 trees, with the exact number of trees depending on the domain specications. The
Slot Filtering stage has a separate tree for each slot of each object in the domain; the Slot
Merging stage has a separate tree for each object type; the Link Creation stage has a tree
for each pointer dened in the output structure; and so forth for the other stages. The
MUC-5 microelectronics domain (as explained in Section 2.2) required 91 decision trees: 20
for the Slot Filtering stage, 7 for Slot Merging, 31 for Link Creation, 13 for Object Splitting,
7 for Inferring Missing Objects , and 13 for Inferring Missing Slot Values.
An example from the Link Creation stage is the tree that determines pointers from
lithography objects to equipment objects. Every pair of lithography and equipment objects
found in a text is encoded as an instance and sent to the Lithography-Equipment-Link tree.
If the classier returns \positive", Wrap-Up adds a pointer between these two objects in
the output to indicate that the equipment was used for that lithography process.
The ID3 decision tree algorithm (Quinlan, 1986) was used in these experiments, although
any machine learning classier could be plugged into the Wrap-Up architecture. A vector
space approach might seem appropriate, but its performance would depend on the weights
assigned to each feature (Salton et al., 1975). It is hard to see a principled way to assign
weights to the heterogeneous features used in Wrap-Up's classiers (see Section 3.3), since
some features encode attributes of the domain objects and others encode linguistic context
or relative position in the text.
Let's look again at the example from Section 2.2 with the \XLS stepper" and see how
Wrap-Up makes the discourse decision of whether to add a pointer from UV lithography to this equipment object. Wrap-Up encodes this as an instance for the LithographyEquipment-Link decision tree with features representing attributes of both the lithography
and equipment objects, their extraction patterns, and relative position in the text.
During Wrap-Up's training phase, an instance is encoded for every pair of lithography
and equipment objects in a training text. Training instances must be classied as positive or
negative, so Wrap-Up consults the hand-coded target output provided with the training text
and classies the instance as positive if a pointer is found between matching lithography and
equipment objects. The creation of training instances will be discussed more fully in Section
3.4. ID3 tabulates how often each feature value is associated with a positive or negative
training instance and encapsulates these statistics at each node of the tree it builds.
Figure 2 shows a portion of a Lithography-Equipment-Link tree, showing the path used
to classify the instance for UV lithography and XLS stepper as positive. The parenthetical
numbers for each tree node show the number of positive and negative training instances represented by that node. The a priori probability of a pointer from lithography to equipment
in the training corpus was 34%, with 282 positive and 539 negative training instances.
ID3 uses an information gain metric to select the most eective feature to partition
the training instances (p.89-90, Quinlan, 1986), in this case choosing equipment type as
the test at the root of this tree. This feature alone is sucient to classify instances with
equipment type such as modular equipment, radiation source, or etching system, which have
only negative instances. Apparently these types of equipment are never used by lithography
processes (a useful bit of domain knowledge).
The branch for equipment type \stepper" leads to a node in the tree representing 202
positive and 174 negative training instances, raising the probability of a link to 54%. ID3
139

Soderland and Lehnert

(282 pos, 539 neg)

Equipment-type
modularequipment

...
radiationsource

(0 pos, 11 neg)

etchingsystem

...

Stepper

lithographysystem

(0 pos, 125 neg)

(0 pos, 15 neg)

(80 pos, 141 neg)

(202 pos, 174 neg)

Lithography-type
...
G-line

...

E-beam

I-line

optical

UV
(15 pos, 27 neg)

(6 pos, 25 neg)
(2 pos, 31 neg)

(87 pos, 20 neg)

(27 pos, 14 neg)

Distance
-2

...

...

0

-1
(0 pos, 1 neg)

(18 pos, 12 neg)

(4 pos, 0 neg)

Figure 2: A decision tree for pointers from lithography to equipment objects.
recursively selects a feature to partition each partition, in this case selecting lithography
type. The branch for UV lithography leads to a partition with 27 positive and 14 negative
instances, in contrast to E-beam and optical lithography which have nearly all negative
instances. The next test is distance, with a value of -1 in this case since the equipment
reference is one sentence earlier than lithography. This branch leads to a leaf node with
4 positive and no negative instances, so the tree returns a classication of positive and
Wrap-Up adds a pointer from UV lithography to the stepper.
This example shows how a decision tree can acquire useful domain knowledge: that
lithography is never linked to equipment such as etching systems, and that steppers are
often used for UV lithography but hardly ever for E-beam or optical lithography. Knowledge
of this sort could be manually engineered rather than acquired from machine learning, but
the hundreds of rules needed might take weeks or months of eort to create and test.
Consider another fragment of text and the tree in Figure 3 that decides whether to add
a pointer from the PLCC packaging process to the ROM chip device.
: : :a

new line of 256 Kbit and 1 Mbit ROM chips.
available in PLCC and priced at : : :

They are

The instance which is to be classied by a Packaging-Device-Link tree includes features for
packaging type, device type, distance between the two referents, and the extraction patterns
used by sentence analysis.
140

Wrap-Up: a Trainable Discourse Module

(325 pos, 750 neg)

Distance

(0 pos, 12 neg)

...

...

-50

-20

(7 pos, 40 neg)

50

0

-1
(60 pos, 70 neg)

(130 pos, 93 neg)

(0 pos, 12 neg)

Device-type
...
EPROM
(6 pos, 2 neg)

memory

(0 pos, 11 neg)

...
DRAM

ROM
(13 pos, 2 neg)

none

(1 pos, 4 neg)

(0 pos, 19 neg)

pp-available-1
true

false

(13 pos, 0 neg)

(0 pos, 2 neg)

Figure 3: A tree for pointers from packaging to device objects.
ID3 selects \distance" as the root of the tree, a feature that counts the distance in sentences between the packaging and device references in the text. When the closest references
were 20 or more sentences apart, hardly any of the training instances were positive. The
distance is -1 in the example text, with ROM device mentioned one sentence earlier than
the PLCC packaging process. As Figure 3 shows, the branch for distance of -1 is followed
by a test for device type. The branch for device type ROM leads to a partition with only
15 instances, 13 positive and 2 negative. Those with PLCC packaging found in the pattern
\available in X" (encoded as pp-available-1) were positive instances.
These two trees illustrate how dierent trees learn dierent types of knowledge. The
most signicant features in determining whether an equipment object is linked to a lithography process are real world constraints on what type of equipment can be used in lithography.
This is reected in the tree in Figure 2 by choosing equipment type as the root node followed by lithography type. There is no such overriding constraint on what type of device
can be linked to a packaging technique. Here linguistic clues play a more prominent role,
such as the relative position of references in the text and particular extraction patterns.
The following section discusses how these linguistic-based features are encoded.

3.3 Generating Features for ID3 Trees

Let's look in more detail at how Wrap-Up encodes ID3 instances, using information available
from sentence analysis to automatically derive the features used for each tree. Each ID3
tree handles a discourse decision about a domain object or the relationship between a pair
of objects, with dierent stages of Wrap-Up involving dierent sorts of decisions.
141

Soderland and Lehnert

The information to be encoded about an object comes from concept nodes extracted
during sentence analysis. Concept nodes have a case frame with slots for extracted information, and also have the location and extraction patterns of each reference in the text.
Consider again the example from Section 2.2.
GCA unveiled its new XLS stepper, which was developed with
assistance from Sematech. The system will be available in
deep-ultraviolet and I-line configurations.

Sentence analysis extracts ve objects from this text: the company GCA, the equipment XLS stepper, the company Sematech, UV lithography, and I-line lithography. One of
several discourse decisions to be made is whether the UV lithography uses the XLS stepper
mentioned in the previous sentence. Figure 4 shows the two objects that form the basis of
an instance for the Lithography-Equipment-Link tree.
Equipment
Type: stepper
Name: XLS

Lithography
Type: UV
Extraction Patterns:
pp-available-in
keyword-deep-ultraviolet

Extraction Patterns:
obj-active-unveiled
subj-passive-developed
keyword-stepper

Figure 4: Two objects extracted from the sample text
Each object includes the location of each reference and the patterns used to extract
them. An extraction pattern is a combination of a syntactic pattern and a specic lexical
item or \trigger word" (as explained in Section 2.1). The pattern pp-available-in means that
a reference to UV lithography was found in a prepositional phrase following the triggers
\available" and \in".
Figure 5 shows the instance for UV lithography and XLS stepper. It encodes the attributes and extraction patterns of each object and their relative position in the text. WrapUp encodes each case frame slot of each object using the actual slot value for closed classes
such as lithography type. Open class slots such as equipment names are encoded with the
value \t" to indicate that a name was present, rather than the actual name. Using the
exact name would result in an enormous branching factor for this feature and might overly
inuence the ID3 classication if a low frequency name happened to occur only in positive
or only in negative instances.
Extraction patterns are encoded as binary features that include the trigger word and
syntactic pattern in the feature name. Patterns with two trigger words such as \pp-availablein" are split into two features, \pp-available" and \pp-in". For instances that encode a pair
of objects these features will be encoded as \pp-available-1" and \pp-in-1" if they refer to
the rst object. The count of how many such extraction patterns were used is also encoded
142

Wrap-Up: a Trainable Discourse Module

(lithography-type . UV)
(extraction-count-1 . 3)
(pp-available-1 . t)
(pp-in-1 . t)
(keyword-deep-ultraviolet-1 . t)

(equipment-type . stepper)
(equipment-name . t)
(extraction-count-2 . 3)
(obj-unveiled-2 . t)
(subj-passive-developed-2 . t)
(keyword-stepper-2 . t)

(common-triggers . 0)
(common-phrases . 0)
(distance . -1)

Figure 5: An instance for the Lithography-Equipment-Link tree.
for each object. The feature \extraction-count" was motivated by the Slot Filtering stage
since objects extracted several times are more likely to be valid than those extracted only
once or twice from the text.
Another type of feature, encoded for instances involving pairs of objects, is the relative
position of references to the two objects, which may be signicant in determining if two
objects are related. One feature easily computed is the distance in sentences between
references. In this case the feature \distance" has a value of -1, since XLS stepper is found
one sentence earlier than the UV lithography process. Another feature that might indicate
a strong relationship between objects is the count of how many common phrases contain
references to both objects. Other features list \common triggers", words included in the
extraction patterns for both objects. An example of this would be the word \using" if the
text had the phrase \the XLS stepper using UV technology".
It is important to realize what is not included in this instance. A human making this
discourse decision might reason as follows. The sentence with UV lithography indicates
that it is associated with \the system", which refers back to \its new XLS stepper" in the
previous sentence. Part of this reasoning involves domain independent use of a denite
article, and part requires domain knowledge that \system" can be a nonspecic reference
to an equipment object. The current version of Wrap-Up does not look beyond information
passed to it by sentence analysis and misses the reference to \the system" entirely.
Using specic linguistic patterns resulted in extremely large, sparse feature sets for most
trees. The Lithography-Equipment-Link tree had 1045 features, all but 11 of them encoding
extraction patterns. Since a typical instance participates in at most a dozen extraction
patterns, a serious time and space bottle neck would occur if the hundreds of linguistic
patterns that are not present were explicitly listed for each instance. We implemented a
sparse vector version of ID3 that was able to eciently handle large feature spaces by only
tabulating the small number of true-valued features for each instance.
As links are added during discourse processing, objects may become complex, including
many pointers to other objects. By the time Wrap-Up considers links between companies
and microelectronics processes, a lithography object may have a pointer to an equipment
object or to a device object, and the equipment object may in turn have pointers to other
objects. Wrap-Up allows objects to inherit the linguistic context and position in the text
of objects to which they point. When object A has a pointer to object B, the location and
143

Soderland and Lehnert

extraction patterns of references to B are treated as if they references to A. This version
of inheritance is helpful, but a little too strong, ignoring the distinction between direct
references and inherited references.
We have looked at the encoding of instances for isolated discourse decisions in this
section. The entire discourse system is a complex series of decisions, each aecting the
environment used for further processing. The training phase must reect this changing
environment at run time as well as provide classications for each training instance based
on the target output. These issues are discussed in the next section.

3.4 Creating the Training Instances

ID3 is a supervised learning algorithm that requires a set of training instances, each labeled
with the correct classication for that instance. To create these instances Wrap-Up begins
its tree building phase by passing the training texts to the sentence analyzer, which creates
a set of objects representing the extracted information. Multiple references to the same
object are then merged to form the initial input to Wrap-Up's rst stage. Wrap-Up encodes
instances and builds trees for this stage, then repeats the process using trees from stage one
to build trees for stage two, and so forth until trees have been built for all six stages.
As it encodes instances, Wrap-Up repeatedly consults the target output to assign a
classication for each training instance. When building trees for the Slot Filtering stage
an instance is classied positive if the extracted information matches a slot in the target
output. Consider the example of a reference to an \Ultratech stepper" in a microelectronics
text. Sentence analysis creates an equipment object with two slots lled, equipment type
stepper and equipment name \Ultratech". This stage of Wrap-Up has a separate ID3 tree
to judge the validity of each slot, equipment type and equipment name.
Suppose that the target output has an equipment object with type \stepper" but that
\Ultratech" is actually the manufacturer's name and not the equipment model name. The
equipment type instance will be classied positive and the equipment name instance classied negative since no equipment object in the target output has the name Ultratech.
Does this instance include features that capture why a human analyst would not consider
\Ultratech" to be the equipment name? The human is probably using world knowledge
to recognize Ultratech as a familiar company name and recognize other names such as
\Precision 5000" as familiar equipment names. Knowledge such as lists of known company
names and known equipment names is not presently included in Wrap-Up, although this
could be derived easily from the training corpus.
To create training instances for the second stage of Wrap-Up, the entire training corpus
is processed again, this time discarding some slot values as spurious according to the Slot
Filtering trees before creating instances for Slot Merging trees. An instance is created for
each pair of objects of the same type. If both objects can be mapped to the same object in
the target output, the instance is classied as positive. For example, an instance would be
created for a pair of device objects, one with device type RAM and the other with size 256
KBits. It is a positive instance if the output has a single device object with type RAM and
size 256 KBits.
By the time instances are created for later stages of Wrap-Up, errors will have crept in
from previous stages. Errors in ltering, merging, and linking will have resulted in some
144

Wrap-Up: a Trainable Discourse Module

objects retained that no longer match anything in the target output and some objects that
only partially match the target output. Since some degree of error is unavoidable, it is
best to let the training instances reect the state of processing that will occur later when
Wrap-Up is used to process new texts. If the training is too perfectly ltered, merged, and
linked, it will not be representative of the underlying probabilities during run time use of
Wrap-Up.
In later stages of Wrap-Up objects may become complex and only partially match anything in the target output. To aid in matching complex objects, one slot for each object
type is identied in the output structure denition as the key slot. An object is considered
to match an object in the output if the key slots match. Thus an object with a missing
equipment name or spurious equipment name will still match if equipment type, the key
slot, matches. If object A has a pointer to an object B, the object matching A in the output
must also have a pointer to an object matching B.
Such recursive matching becomes important during the Link Creation stage. Among the
last links considered in microelectronics are the roles a company plays towards a process. A
company may be the developer of an x-ray lithography process that uses the ABC stepper,
but not developer of the x-ray lithography process linked to a dierent equipment object.
Wrap-Up needs to be sensitive to such distinctions in classifying training instances for trees
in the Link Creation and Object Splitting stages.
Instances in the Inferring Missing Objects stage and the Inferring Missing Slot Values
stage have classications that go beyond a simple positive or negative. An instance for the
Inferring Missing Objects stage is created whenever an object is found during training that
has no higher object pointing to it. If a matching object indeed exists in the target output,
Wrap-Up classies the instance with the type of the object that points to it in the output.
For example a training text may have a reference to \stepper" equipment, but have no
mention of any process that uses the stepper. The target output will have a lithography
object of type \unknown" that points to the stepper equipment. This is a legitimate inference to make, since steppers are a type of lithography equipment. The instance for the
orphaned stepper equipment object will be classied as \lithography-unknown-equipment".
This classication gives Wrap-Up enough information during run time to create the appropriate object.
An instance for Inferring Missing Slot Values is created whenever a slot is missing from
an object which has a closed class of possible values, such as the \status" slot for equipment
objects, that has the value of \in-use" or \in-development". When a matching object is
found in the target output, the actual slot value is used as the classication. If the slot is
empty or no such object exists in the output, the instance is classied as negative. As in
the Inferring Missing Objects stage, negative is the most likely classication for many trees.
Next we consider the eects of tree pruning and condence thresholds that can make
the ID3 more cautious or more aggressive in its classications.

3.5 Condence Thresholds and Tree Pruning

With any machine learning technique there is a tendency toward \overtting", making
generalizations based on accidental properties of the training data. In ID3 this is more
likely to happen near the leaf nodes of the decision tree, where the partition size may
145

Soderland and Lehnert

grow too small for ID3 to select features with much predictive power. A feature chosen
to discriminate among half a dozen training instances is likely to be particular to those
instances and not useful in classifying new instances.
The implementation of ID3 used by Wrap-Up deals with this problem by setting a pruning level and a condence threshold for each tree empirically. A new instance is classied by
traversing the decision tree from the root node until a node is reached where the partition
size is below the pruning level. The classication halts at that node and a classication of
positive is returned if the proportion of positive instances is greater than or equal to the
condence threshold.
A high condence threshold will make an ID3 tree cautious in its classications, while
a low condence threshold will allow more positive classications. The eect of changing
the condence threshold is more pronounced as the pruning level increases. With a large
enough pruning level, nearly all branches will terminate in internal nodes with condence
somewhere between 0.0 and 1.0. A low condence threshold will classify most of these
instances as positive, while a high condence threshold will classify them as negative.
Wrap-Up automatically sets a pruning level and condence threshold for each tree using
tenfold cross-validation. The training instances are divided into ten sets and each set is
tested on a tree built from the remaining nine tenths of the training. This is done at
various settings to nd settings that optimize performance.
The metrics used in this domain are \recall" and \precision", rather than accuracy.
Recall is the percentage of positive instances that are correctly classied, while precision is
the percentage of positive classications that are correct. A metric which combines recall
and precision is the f-measure, dened by the formula f = ( 2 + 1)P R=( 2P + R) where 
can be set to 1 to favor balanced recall and precision. Increasing or decreasing  for selected
trees can ne-tune Wrap-Up, causing it to select pruning and condence thresholds that
favor recall or favor precision.
We have seen how Wrap-Up automatically derives the classiers needed and the feature
set for each classier, and how it tunes the classiers for recall/precision balance. Now
we will look at the requirements for using Wrap-Up, with special attention to the issue of
manual labor during system development.

3.6 Requirements of Wrap-Up

Wrap-Up is a domain-independent architecture that can be applied to any domain with
a well dened output structure, where domain objects are represented as case frames and
relationships between objects are represented as pointers between objects. It is appropriate
for any information extraction task in which it is important to identify logical relationships
between extracted information. The user must supply Wrap-Up with an output denition
listing the domain objects to be extracted. Each output object has one or more slots, each of
which may contain either extracted information or pointers to other objects in the output.
One slot for each object is labeled as the key slot, used during training to match extracted
objects with objects in the target output.
If the domain and application are already well dened, a user should be able to create
such an output denition in less than an hour. For a new application, whose information needs are not established, there is likely to be a certain amount of trial and error in
146

Wrap-Up: a Trainable Discourse Module

developing the desired representation. This need for a well dened domain is not unique
to discourse processing or to trainable components such as Wrap-Up. All IE systems require clearly dened specications of what types of objects are to be extracted and what
relationships are to be reported.
The more time consuming requirement of Wrap-Up is associated with the acquisition of
training texts and most importantly, hand-coded target output. While hand-coded targets
represent a labor-intensive investment on the part of domain experts, no knowledge of
natural language processing or of machine learning technologies is needed to generate these
answer keys, so any domain expert can produce answer keys for use by Wrap-Up. A
thousand microelectronics texts were used to provide training for Wrap-Up. The actual
number of training instances from these training texts varied considerably for each decision
tree. Trees that handled the more common domain objects had ample training instances
from only two hundred training texts, while those that dealt with the less frequent objects
or relationships were undertrained from a thousand texts.
It is easier to generate a few hundred answer keys than it is to write down explicit and
comprehensive domain guidelines. Moreover, domain knowledge implicitly present in a set
of answer keys may go beyond the conventional knowledge of a domain expert when there
are reliable patterns of information that transcend a logical domain model. Once available,
this corpus of training texts can be used repeatedly for knowledge acquisition at all levels
of processing.
The architecture of Wrap-Up does not depend on a particular sentence analyzer or a
particular information extraction task. It can be used with any sentence analyzer that uses
keywords and local linguistic patterns for extraction. The output representation produced
by Wrap-Up could either be used directly to generate database entries in a MUC-like task
or could serve as an internal representation to support other information extraction tasks.

3.7 The Joint Ventures Domain

After Wrap-Up had been implemented and tested in the microelectronics domain, we tried
it on another domain, the MUC-5 joint ventures domain. The information to be extracted
in this domain are companies involved in joint business ventures, their products or services,
ownership, capitalization, revenue, corporate ocers, and facilities. Relationships between
companies must be sorted out to identify partners, child companies, and subsidiaries. The
output structure is more complex than that of microelectronics, with back-pointers, cycles
in the output structure, redundant information, and longer chains of linked objects.
Figure 6 shows a text from the joint ventures domain and a diagram of the target output.
With all the pointers and back-pointers, the output for even a moderately complicated text
becomes dicult to understand at a glance. This text describes a joint venture between a
Japanese company, Rinnai Corp., and an unnamed Indonesian company to build a factory
in Jakarta. A tie-up is identied with Rinnai and the Indonesian company as partners
and a third company, the joint venture itself, as a child company. The output includes an
\entity-relationship" object which duplicates much of the information in the tie-up object.
A corporate ocer, the amount of capital, ownership percentages, the product \portable
cookers", and a facility are also reported in the output.
147

Soderland and Lehnert

RINNAI CORP., JAPAN'S LEADING GAS APPLIANCE MANUFACTURER, WILL SET UP
A JOINT VENTURE IN INDONESIA IN AUGUST TO PRODUCE PORTABLE COOKERS FOR
LOCAL USERS, PRESIDENT SUSUMU NAITO SAID MONDAY.
THE NEW FIRM WILL BE CAPITALIZED AT ONE MILLION DOLLARS, OF WHICH
RINNAI IS SCHEDULED TO PUT UP 50 PCT AND A LOCAL DEALER 50 PCT, HE SAID.
IT WILL MANUFACTURE 3,000 TO 4,000 UNITS A MONTH INITIALLY AT A PLANT
IN A 26,000-SQUARE-METER SITE IN JAKARTA, NAITO SAID, ADDING RINNAI AIMS
TO START FULL-SCALE PRODUCTION NEXT SPRING.
THE NAGOYA-BASED COMPANY HAS NOW SEVEN OVERSEAS PRODUCTION UNITS.

Template
Doc-Nr: 1485
Content:

Tie-Up
Status: existing
Entity:
Joint-venture:
Ownership:
Activity:

Activity
Site: (
Industry:

Entity
Type: company
Location: Indonesia
Relationship:
Facility:

Entity
Type: company
Name: Rinnai Corp
Aliases: "Rinnai"
Location: Nagoya, Japan
Relationship:
Person:

Entity
Type: company
Nationality: Indonesia
Relationship:

Facility
Type: factory
Location: Jakarta,
Indonesia

Person
Name: Susumu Naito
Position: pres
Entity:

Relationship
Entity-1:
Entity-2:
Relation: child
Status: future

)

Industry
Type: Production
Product:
"portable
cookers"

Ownership
Capital: 1000000 $
Ownership-Percent: (
Owned:

50) (

50)

Figure 6: A sample text and target output from the joint ventures domain.

148

Wrap-Up: a Trainable Discourse Module

Some special handling was required for the joint ventures domain since the output
structure dened for the MUC-5 evaluation included some slots such as activity site and
ownership percent whose values had a mixture of extracted information and pointers. These
slot values have their own internal structure and can be thought of as pseudoobjects, an
activity site object with pointers to a facility object and a company, and an ownership
percent object with a pointer to a company and another slot giving a numeric value. These
pseudoobjects were reformulated as standard objects conforming to the requirements of
Wrap-Up, the activity site slot pointing to an activity site object and so forth. These were
then transformed back into the complex slot lls when printing the nal representation of
the output.
The output specications for joint ventures were less well-behaved in other ways, with
graph cycles, back pointers, and redundant objects whose content must agree with information elsewhere in the output. Modications to Wrap-Up were needed to relax some implicit
requirements for the domain structure, allowing graph cycles and giving special handling
to any pointer slot which the user has labeled in the output denition as a back pointer.
Joint ventures also has some implicit constraints on relationships between objects. A
company can play only a single role in a tie-up or a joint venture relationship: it cannot be
both a joint venture child and also a parent or partner company. Wrap-Up had diculty
learning this constraint and performed better when certain pointer slots were labeled with
a \single-role" constraint in the output denition.
This strategy of letting the user indicate constraints by annotating slots in the output
denition was implemented in an ad hoc fashion. A more general approach would allow the
user to declare several types of constraint on the output. A pointer slot may be required
or optional, may have at most one pointer or allow several. Some slots of an object may be
mutually exclusive, an entry in one prohibiting an entry in another slot. There may be a
required agreement between the value of a slot in one object and a slot in another object. A
fully domain-independent discourse tool needs a mechanism to implement such generalized
constraints.

4. System Performance

As a point of comparison for the performance of Wrap-Up, the UMass/Hughes system was
run with the TTG discourse module, which had been used in the ocial MUC-5 evaluation. Overall system performance with Wrap-Up was compared to performance with TTG,
holding the rest of the system constant.
Wrap-Up takes the idea of TTG and extends it into a fully trainable system. TTG
used decision trees to acquire domain knowledge, but often relied on hand-coded heuristics
to apply that acquired knowledge, in particular the decisions about splitting or merging
objects, which Wrap-Up handles during its Object Splitting stage; inferring missing objects,
which Wrap-Up does in its Inferring Missing Objects stage; and adding context sensitive
default slot values, which Wrap-Up does in its Inferring Missing Slot Values stage.
Several iterations of hand tuning were required to adjust thresholds for the decision trees
produced by TTG, whereas Wrap-Up found thresholds and pruning levels to optimize recall
and precision for each tree automatically. After a day of CPU time devoted to decision tree
training, Wrap-Up produced a working system and no further programming was needed.
149

Soderland and Lehnert

The comparison with TTG was made for both the microelectronics domain and the
joint ventures domain. The metrics used here are recall and precision. Recall is the percentage of possible information that was reported. Correctly identifying two out of ve
possible company names gives recall of 40. Precision is the percent correct of the reported
information. If four companies are reported, but only two of them correct, precision is 50.
Recall and precision are combined into a single metric by the f-measure, dened as f =
( 2 + 1)P R=( 2P + R), with  is set to 1 for balanced recall and precision.

4.1 The Microelectronics Domain

Wrap-Up's scores on the ocial MUC-5 microelectronics test sets were generally a little
higher than to those of TTG, both in overall recall and precision.
Wrap-Up
Rec. Prec. F

TTG
Rec. Prec.

Part 1
Part 2

32.3 44.4 37.4
36.3 38.6 37.4

27.1 39.5 32.1
32.7 37.0 34.7

Part 3

34.6 37.7 36.1

34.7 40.5 37.5

Avg.

34.4 40.2 36.8

31.5 39.0 34.8

F

Figure 7: Performance on MUC-5 microelectronics test sets
To put these scores in perspective, the highest scoring systems in the MUC-5 evaluation
had f-measures in the high 40's. This was a dicult task both for sentence analysis and
discourse analysis.
Another way to assess Wrap-Up is to measure its performance against the baseline
provided by output from sentence analysis. Lack of coverage by the sentence analyzer
places a ceiling on performance at the discourse level. In test set part 1 there were 208
company names to be extracted. The CIRCUS analyzer extracted a total of 404 company
names, with only 131 correct and 2 partially correct, giving a baseline of 63% recall and 33%
precision for that slot. Wrap-Up's Entity-Name-Filter tree managed to discard a little over
half of the spurious company names, keeping 77% of the good companies. This resulted in
49% recall and 44% precision for this slot, raising the f-measure by 5 points, but doing so
at the expense of recall.
Limited recall for extracted objects is compounded when it comes to links between
objects. If half the possible companies and a third of the microelectronics processes are
missing, discourse processing has no chance at a large proportion of the possible links
between companies and processes.
Although precision is often increased at the expense of recall, Wrap-Up also has mechanisms to increase recall slightly. When the Inferring Missing Objects stage infers a missing
process from an equipment object or the Object Splitting stage splits a process that points
to multiple equipment, Wrap-Up can sometimes gain recall above that produced by the
sentence analyzer.

150

Wrap-Up: a Trainable Discourse Module

4.2 The Joint Ventures Domain

In the joint ventures domain Wrap-Up's scores on the MUC-5 test sets were a little lower
than the ocial UMass/Hughes scores. Wrap-Up tended to have lower recall but slightly
higher precision.
Wrap-Up
Rec. Prec. F

TTG
Rec. Prec.

Part 1
Part 2

23.5 52.9 32.5
22.7 53.6 31.9

26.0 53.9 35.1
26.0 52.1 34.7

Part 3

23.3 51.4 32.1

27.7 49.7 35.6

Avg.

23.2 52.7 32.2

26.5 52.0 35.1

F

Figure 8: Performance on MUC-5 joint ventures test sets
The performance of Wrap-Up and TTG is roughly comparable for each of the two
domains. Both systems tend to favor the domain in which they were rst developed, WrapUp developed in microelectronics then ported to joint ventures, while the opposite was true
for TTG. A certain amount of bias has probably crept into design decisions that were meant
to be domain independent in each system. The higher scores of TTG for joint ventures are
partly due to hand-coded heuristics that altered output from TTG before printing the nal
output, something that was not done for TTG in microelectronics or for Wrap-Up in either
domain.
The most noticeable dierence between Wrap-Up and TTG output in the joint ventures
domain was in the ltering of spuriously extracted company names. Discourse processing
started with 38% recall and 32% precision from sentence analysis for company names. Both
systems included a ltering stage that attempted to raise precision by discarding spurious
companies, but did so at the expense of discarding some valid companies as well. Each
system used threshold settings to control how cautiously or aggressively this discarding is
done (as in the example from Section 3.5). TTG's were set by hand and Wrap-Up's were
selected automatically by cross-validation on the training set. TTG did only mild ltering
on this slot, resulting in a gain of 2 precision points but a drop of 6 recall points. Wrap-Up
chose aggressive settings and gained 13 precision points but lost 17 points in recall for this
slot.
As a result, Wrap-Up ended up with only two thirds as many correct companies as
TTG. This in turn meant two thirds as many pointers to companies in tie-ups and entity
relationships. For other objects Wrap-Up scored higher recall than TTG, getting more than
three times the total recall for activity, industry, and facility objects.

5. Conclusions

With the recent accessibility of large on-line text databases and news services, the need for
information extraction systems is growing. Such systems go beyond information retrieval
and create a structured summary of selected information contained within relevant documents. This gives the user the ability to skim vast amounts of text, pulling out information
151

Soderland and Lehnert

on a particular topic. IE systems are knowledge-based, however, and must be individually
tailored to the information needs of each application.
Some research laboratories have focused on sophisticated user interfaces to ease the
burden of knowledge acquisition. GE's NLToolset is an example of this approach (Jacobs et
al., 1993), while BBN typies systems that combine user input with corpus-based statistics
(Ayuso et al., 1993). The University of Massachusetts has been moving in the direction
of machine learning to create a fully trainable IE system. The ultimate goal is a turnkey
system that can be tailored to new information needs by users who have no special linguistic
or technical expertise.
Wrap-Up embodies this goal. The user denes an information need and output structure,
and provides a training corpus of representative texts with hand-coded target output for
each text. Wrap-Up takes it from there and instantiates a fully functional IE discourse
system for the new domain with no further customization needed by the user. Wrap-Up
is the rst fully trainable system to handle discourse processing, and it does so with no
degradation in performance. It automatically decides what classiers are needed based on
the domain output structure and derives the feature set for each classier from sentence
analyzer output.
The most intriguing aspect of Wrap-Up is the automatic generation of features. How
eective was this, and what did the trees actually learn? The greatest leverage seems to
come from features that encode attributes of domain objects. The trees in microelectronics
often based their classication on probabilities conditioned on the device type, equipment
type, or process type. The example tree in Section 3.2 rst tested the equipment type and
lithography type in determining whether a piece of equipment was used for a lithography
process. This type of real world domain knowledge was the most important thing that
Wrap-Up learned about microelectronics.
Useful knowledge was also provided by features that encoded the relative position of
references in the text. Distance, measured in number of sentences apart, played a prominent
role in many classications, with other trees relying on more ne-grained features such as
the number of times both references were in the same noun phrase or had overlapping
linguistic context.
An enhancement to Wrap-Up's feature generation would be to increase its expressiveness
about relative position. In addition to direct references to object A and object B, Wrap-Up
could look for indirect references to A (pronominal or anaphoric) found near references to
B and vice versa. The instance shown in Section 3.3 is an example where features for such
indirect relationships might be useful.
Wrap-Up currently encodes an instance for each pair of objects that might be related,
but is incapable of expressing the rule \attach object B to the most recent object of type A."
It is blind to the existence of other objects that are alternate candidates to the relationship
being considered. Features could be encoded to reect whether object A is the most recently
mentioned object of its type.
The features that were least successful and most tantalizing were those that encoded the
local linguistic context, the extraction patterns. These included an exact lexical item and
were nearly all of such low frequency that they added noise more often than aiding useful
discriminations. Tree pruning was only a partial solution, and an experiment in combining
semantically similar terms only caused a sharp drop in classication accuracy.
152

Wrap-Up: a Trainable Discourse Module

Low frequency terms are a built-in problem for any system that processes unrestricted
text. Dunning (93) estimated that 20-30% of typical English news wire reports are composed
of words of frequency less than one in 50,000 words. Yet the discourse decisions made
by a human reader often seem to hinge on the use of one of these infrequent terms. It
is a challenging open question to nd methods to utilize local linguistic context without
drowning in the noise produced by low frequency terms.
Finding a mechanism for choosing appropriate features is more critical than which machine learning algorithm is applied. ID3 was chosen as easy to implement, although other
approaches such as vector spaces are worth trying. It is not obvious, however, how to craft
a weighting scheme that gives greatest weight to the most useful features in the vector
space and nearly zero to those not useful in making the desired discrimination. Cost and
Salzberg (1993) describe a weighting scheme for the nearest neighbor algorithm that looks
promising for lexically-based features. Another candidate for an eective classier is a back
propagation network, which might naturally converge on weights that give most inuence
to the most useful features.
We hope that Wrap-Up will inspire the machine learning community to consider analysis
of unrestricted text as a fruitful application for ML research, while challenging the natural
language processing community to consider ML techniques for complex processing tasks.
In a broader context, Wrap-Up provides a paradigm for user customizable system design,
where no technological background on the part of the user is assumed. A fully functional
system can be brought up in a new domain without the need for months of development
time, signifying substantial progress toward fully scalable and portable natural language
processing systems.

Appendix A: Walk-through of a Sample Text

To see the Wrap-Up algorithm in action, consider the sample text in Figure 9. The
desired output has the company, Mitsubishi Electronics America, Inc., linked as purchaser/user to two packaging processes, TSOP and SOJ packaging. Each of these processes
point to the device, 1 Mbit DRAM. The packaging material, plastic, should be attached
to TSOP but not SOJ. All other details from the text are considered extraneous to the
domain.
After sentence analysis, followed by the step that merges multiple references, there are
eight objects passed as input to Wrap-Up. Sentence analysis did fairly well in identifying
the relevant information, only missing \1 M" as a reference to 1 MBits. Three of the
eight objects are spurious and should be discarded during Wrap-Up's Slot Filtering stage.
According to domain guidelines, the name \Mitsubishi Electronics America, Inc." should
be reported, not \The Semiconductor Division ...". The packaging material EPOXY and
the device MEMORY should also be discarded.
The Slot Filtering stage creates an instance for each slot of each object. The EntityName-Filter tree classies \Mitsubishi Electronics America, Inc." as a positive instance,
but \The Semiconductor Division ..." as negative and it is discarded. The most reliable
discriminator of valid company names is \extraction-count", which was selected as root
feature of this tree. Training instances participating in several extraction patterns were
twice as likely to be valid as those extracted only once or twice. This held true in this text.
153

Soderland and Lehnert

The Semiconductor Division of Mitsubishi Electonics America, Inc. now offers
1M CMOS DRAMs in Thin Small-Outline Packaging (TSOP*), providing the
highest memory density available in the industry. Developed by Mitsubishi,
the TSOP also lets designers increase system memory density with standard and
reverse or "mirror image," pin-outs. Mitsubishi's 1M DRAM TSOP provides
the density of chip-on-board but with much higher reliability because the
plastic epoxy-resin package allows each device to be 100% burned-in and fully
tested. *Previously referred to as VSOP (very small-outline package) or USOP
(ultra small-outline package). The 1M DRAM TSOP has a height of 1.2 mm, a
plane measurement of 16.0 mm x 6.0 mm, and a lead pitch of 0.5 mm, making
it nearly three times thinner and four times smaller in volume than the 1M
DRAM SOJ package. The SOJ has a height of 3.45 mm, a plane dimension of
17.15 mm x 8.45 mm, and a lead pitch of 1.27 mm. Additionally, the TSOP
weighs only 0.22 grams, in contrast with the 0.75 gram weight of the SOJ.
Full text available on PTS New Product Announcements.

Figure 9: A microelectronics text

Entity
Type: company
Name:Mitsubishi Electronics
America, Inc.

Entity
Type: company
Name: The Semiconductor Division of
Mitsubishi Electronics America, Inc.

Device
Type: DRAM

Packaging
Type: TSOP

Device
Type: MEMORY

Packaging
Material: EPOXY

Packaging
Material: PLASTIC

Packaging
Type: SOJ

Figure 10: Input to Wrap-Up from the sample text

154

Wrap-Up: a Trainable Discourse Module

\Mitsubishi Electronics America, Inc." had extraction count of 5, while the spurious name
was extracted from only 2 patterns.
As the Slot Filtering stage continues, the packaging material EPOXY is classied negative by the Packaging-Material-Filter tree, whose root test is packaging type. It turns out
that EPOXY was usually extracted erroneously in the training corpus. This contrasts with
the material PLASTIC which was usually reliable and is classied positive. Both TSOP and
SOJ packaging types are classied positive by the Packaging-Type-Filter tree. Instances for
these types were usually positive in the training set, particularly when extracted multiple
times from the text. The Device-Type-Filter tree, with root feature device type, nds that
DRAM is a reliable device type but that MEMORY was usually spurious in the training
corpus. It should usually be merged with a more specic device type.
The Slot Merging stage of Wrap-Up then considers each pair of remaining objects of
the same type. There are three packaging objects, one with type TSOP, one with material
PLASTIC, and one with type SOJ. The Packaging-Slotmerge tree easily rejects the TSOPSOJ instance, since packaging objects never had multiple types in training. After testing
that the second object has no packaging type, the feature \distance" is tested. This led to a
positive classication for TSOP-PLASTIC, which are from the same sentence, and negative
for SOJ-PLASTIC, with nearest references two sentences apart. At this point four objects
remain:
Entity
Type: company
Name:Mitsubishi Electronics
America, Inc.

Packaging
Type: TSOP
Material: PLASTIC

Device
Type: DRAM

Packaging
Type: SOJ

The Link Creation stage considers each pair of objects that could be linked according
to the output structure. The rst links considered are pointers from packaging to device
objects. Separate instances for the Packaging-Device-Link tree are created for the possible
TSOP-DRAM link and for the possible SOJ-DRAM link. Although only 25% of the training
instances were positive, the tree found that 78% were positive with packaging type TSOP
and \distance" of 0 sentences, and 77% were positive with packaging type SOJ and device
type DRAM. After testing a few more features, the tree found each of these instances
positive and pointers were added in the output. Notice how this tree interleaves knowledge
about types of packaging and types of devices with knowledge about relative position of
references in the text.
The next Link Creation decision concern the roles Mitsubishi plays towards each of the
packaging processes. The output structure has a \microelectronics-capability" object with
one slot pointing to a lithography, layering, etching, or packaging process, and four other
slots (labeled developer, manufacturer, distributor, and purchaser/user) pointing to companies. Wrap-Up accordingly encodes four instances for Mitsubishi and TSOP packaging,
one for each possible role. The same is done for Mitsubishi and SOJ packaging.
Instances for Mitsubishi in the roles of developer, manufacturer, and distributor were all
classied as negative. Training instances for these trees had almost no positive instances.
155

Soderland and Lehnert

Template
Doc-Nr: 2523814
Contents:
ME-Capability
Purchaser/User:
Developer:
Process:

ME-Capability
Purchaser/User:
Process:

Packaging
Type: TSOP
Material: plastic
Device:

Packaging
Type: SOJ
Device:

Entity
Type: company
Name: Mitsubishi
Electronics
America, Inc.

Device
Type: DRAM

Figure 11: Final output after links have been added
It seems that stories about packaging processes in this corpus are almost exclusively about
companies purchasing or using someone else's packaging technology.
There are seldom explicit linguistic clues about the relationship of a company to a process
in this corpus, so the Packaging-User-Link tree tests rst for the relative distance between
references. Only 20% of training instances were positive, but when distance was 0 it jumped
to 43% positive. Mitsubishi is in the same sentence with TSOP and the Mitsubishi-SOJ
instance also has distance of 0 by inheritance. Even though the nearest reference to SOJ is
two sentences after Mitsubishi, SOJ is linked to DRAM which occurs in the same sentence
as Mitsubishi. Both instances are classied positive after further testing for packaging type
and other features.
The last discourse decision in the Link Creation stage is to add pointers to each microelectronics capability from a \template object", created as a dummy root object in this
domain's output. The Object Splitting stage nally gets to make a decision, albeit a vacuous
one, and decides to let the template object point to multiple objects in its \content" slot.
There were no \orphan" objects or missing slot values for the last two stages of Wrap-Up
to consider. The nal output for this text is shown in Figure 11.

Acknowledgements
This research was supported by NSF Grant no. EEC-9209623, State/Industry/University
Cooperative Research on Intelligent Information Retrieval.

156

Wrap-Up: a Trainable Discourse Module

References

Ayuso, D., Boisen, S., Fox, H., Gish, H., Ingria, R., & Weischedel, R. (1992). BBN: Description of the PLUM System as Used for MUC-4. In Proceedings of the Fourth Message
Understanding Conference, 169-176. Morgan Kaufmann Publishers.
Brent, M. (1993). Robust Acquisition of Subcategorization Frames. In Proceeding of the
Association for Computational Linguistics.
Cardie, C. (1993). A Case-Based Approach to Knowledge Acquisition for Domain-Specic
Sentence Analysis. In Proceedings of the Eleventh National Conference on Articial
Intelligence, 798-803.
Church, K. (1988). A stochastic parts program and noun phrase parser for unrestricted text.
In Proceedings of the Second Conference on Applied Natural Language Processing of
the ACL, 136-143.
Cost, S., & Salzberg, S. (1993). A Weighted Nearest Neighbor Algorithm for Learning with
Symbolic Features. Machine Learning, 10(1), 57-78.
DeRose, S. (1988). Grammatical Category Disambiguation by Statistical Optimization.
Computational Linguistics, 14(1), 31-39.
Dolan, C. P., Goldman, S. R., Cuda, T. V., & Nakamura, A. M. (1991). Hughes Trainable
Text Skimmer: Description of the TTS System as used for MUC-3. In Proceedings of
the Third Message Understanding Conference, 155-162. Morgan Kaufmann Publishers.
Dunning, T. (1993). Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguistics, 19(1), 61-74.
Grosz, B., & Sidner C. (1986). Attention, intention and the structure of discourse. Computational Linguistics, 12(3), 175-204.
Hindle, D. (1989). Acquiring Disambiguation Rules from Text. In Proceeding of the Association for Computational Linguistics, 118-125.
Hobbs, J. (1978). Resolving Pronoun References. Lingua, 44(4), 311-338.
Jacobs, P., Krupka, G., Rau, L., Mauldin, M., Mitamura, T., Kitani, T., Sider, I., &
Childs, L. (1993). GE-CMU: Description of the SHOGUN System used for MUC5. In Proceedings of the Fifth Message Understanding Conference, 109-120. Morgan
Kaufmann Publishers.
Lehnert, W. (1990). Symbolic/Subsymbolic Sentence Analysis: Exploiting the Best of Two
Worlds. Advances in Connectionist and Neural Computation Theory. vol. 1.. Norwood,
NJ: Ablex Publishing, 151-158.
Lehnert, W., Cardie, C., Fisher, D., McCarthy, J., Rilo, E., & Soderland, S. (1992).
University of Massachusetts: Description of the CIRCUS System as Used for MUC-4.
In Proceedings of the Fourth Message Understanding Conference, 282-288. Morgan
Kaufmann Publishers.
157

Soderland and Lehnert

Lehnert, W., McCarthy, J., Soderland, S., Rilo, E., Cardie, C., Peterson, J., Feng, F.,
Dolan, C., & Goldman, S. (1993). UMass/Hughes: Description of the CIRCUS System
as Used for MUC-5. In Proceedings of the Fifth Message Understanding Conference,
257-259. Morgan Kaufmann Publishers.
Liddy, L., McVearry, K., Paik, W., Yu, E., & McKenna, M. (1993). Development, Implementation, and Testing of a Discourse Model for Newspaper Texts. In Proceedings of
the Human Language Technology Workshop, 159-164. Morgan Kaufmann Publishers.
MUC-3. (1991). Proceedings of the Third Message Understanding Conference. Morgan Kaufmann Publishers.
MUC-4. (1992). Proceedings of the Fourth Message Understanding Conference. Morgan
Kaufmann Publishers.
MUC-5. (1993). Proceedings of the Fifth Message Understanding Conference. Morgan Kaufmann Publishers.
Quinlan, J.R. (1986). Induction of Decision Trees. Machine Learning, 1, 81-106.
Rilo, E. (1993). Automatically Constructing a Dictionary for Information Extraction
Tasks. In Proceedings of the Eleventh National Conference on Articial Intelligence,
811-816.
Salton, G., Wong, A., & Yang, C.S. (1975). A vector space model for automatic indexing.
Correspondences of the ACM, 18(11), 613-620.
Soderland, S., & Lehnert, W. (1994). Corpus-Driven Knowledge Acquisition for Discourse
Analysis. In Proceedings of the Twelfth National Conference on Articial Intelligence,
827-832.
Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L., & Palmucci, J. (1993). Coping
with Ambiguity and Unknown Words Through Probabilistic Models. Computational
Linguistics, 19(2), 359-382.
Will, C. (1993). Comparing human and machine performance for natural language information extraction: Results for English microelectronics from the MUC-5 evaluation.
In Proceedings of the Fifth Message Understanding Conference, 53-67. Morgan Kaufmann Publishers.

158

Journal of Articial Intelligence Research 2 (1995) 361{367

Submitted 8/94; published 3/95

Research Note

On the Informativeness of the DNA
Promoter Sequences Domain Theory

Julio Ortega
Computer Science Dept., Vanderbilt University
P.O. Box 1679, Station B
Nashville, TN 37235 USA

julio@vuse.vanderbilt.edu

Abstract
The DNA promoter sequences domain theory and database have become popular for
testing systems that integrate empirical and analytical learning. This note reports a simple
change and reinterpretation of the domain theory in terms of M-of-N concepts, involving no
learning, that results in an accuracy of 93.4% on the 106 items of the database. Moreover,
an exhaustive search of the space of M-of-N domain theory interpretations indicates that
the expected accuracy of a randomly chosen interpretation is 76.5%, and that a maximum
accuracy of 97.2% is achieved in 12 cases. This demonstrates the informativeness of the
domain theory, without the complications of understanding the interactions between various
learning algorithms and the theory. In addition, our results help characterize the diculty
of learning using the DNA promoters theory.

1. Introduction
The DNA promoter sequences domain theory and database, contributed by M. Noordewier
and J. Shavlik to the UCI repository (Murphy & Aha, 1992), have become popular for
testing systems that integrate empirical and analytical learning (Hirsh & Japkowicz, 1994;
Koppel, Feldman, & Segre, 1994b; Mahoney & Mooney, 1994, 1993; Norton, 1994; Opitz &
Shavlik, 1994; Ortega, 1994; Ourston, 1991; Towell, Shavlik, & Noordewier, 1990; Shavlik,
Towell, & Noordewier, 1992). The original domain theory, as usually interpreted, is overly
specic in that it classies all of the promoter sequences in the database as negative instances. Since the database consists of 53 positive instances and 53 negative instances, the
accuracy over this database is 50%. The learning systems cited above take advantage of
the initial domain theory in order to achieve higher accuracy rates, especially with fewer
training examples, than the rates achieved by purely inductive methods such as C4.5 and
backpropagation. Thus, the informativeness of this theory is acknowledged, despite its 50%
accuracy rate using a naive interpretation. However, the extent to which the theory is
informative is not easily ascertained; this is implicit in the interactions between each of the
learning algorithms and the theory. This note reports a simple change and reinterpretation
of the domain theory in terms of M-of-N concepts, which involve no learning, that results
in an accuracy of 93.4% on the 106 data items. Moreover, an exhaustive search of the space
of M-of-N interpretations reveals some that achieve 97.2% accuracy.

c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Ortega

promoter

contact

conformation

minus_35

p-37=c
p-36=t
p-35=t
p-34=g
p-33=a
p-32=c

p-36=t
p-35=t
p-34=g
p-32=c
p-31=a

p-36=t
p-35=t
p-34=g
p-33=a
p-32=c
p-31=a

minus_10

p-36=t
p-35=t
p-34=g
p-33=a
p-32=c

p-14=t
p-13=a
p-12=t
p-11=a
p-10=a
p-9=t

p-13=t
p-12=a
p-10=a
p-8=t

p-13=t
p-12=a
p-11=t
p-10=a
p-9=a
p-8=t

p-12=t
p-11=a

p-7=t

p-47=c
p-46=a
p-45=a
p-43=t
p-42=t
p-40=a
p-39=c
p-22=g
p-18=t
p-16=c
p-8=g
p-7=c
p-6=g
p-5=c
p-4=c
p-2=c
p-1=c

p-45=a
p-44=a
p-41=a

p-49=a
p-44=t
0-27=t
p-22=a
p-18=t
p-16=t
p-15=g
p-1=a

p-45=a
p-41=a
p-28=t
p-27=t
p-23=t
p-21=a
p-20=a
p-17=t
p-15=t
p-4=a

Figure 1: DNA Promoters Theory

2. The DNA Promoter Sequences Database and Domain Theory
The sources of the UCI promoter sequences database and domain theory are described
by Towell (1990). The rules of the theory were derived from the biological research of
O'Neill (1989). The negative examples are contiguous strings from a long DNA sequence
believed not to contain promoters. The positive examples of promoters were taken from
the compilation of Hawley and McClure (1983). This database has been recently augmented (Harley & Reynolds, 1987; Lisser & Margalit, 1993) and new theories of promoter
action are still appearing (Lisser & Margalit, 1993). Nevertheless, the UCI promoter sequences database of examples and domain theory has remained a prominent testbed for
evaluating machine learning methods.
The DNA promoters domain theory obtained from the UCI repository is shown in
Figure 1 as an AND-OR tree. Each box at a leaf of the tree in Figure 1 is usually interpreted
as a conjunction of conditions. Each condition in a box requires that a particular nucleotide
appear at a particular position in the sequence. According to this theory, a DNA sequence
can be classied as a promoter if two regions of the DNA sequence can be identied. The
rst region is called a contact, and the second a conformation. For a conformation region to
be identied, any one of the four specic nucleotide sequences shown at the right-hand side
of Figure 1 need to be present. For a contact region to be identied, both a minus 10 and
a minus 35 region also need to be identied. Again, for a minus 10 or a minus 35 region to
362

On the Informativeness of the DNA Promoter Sequences Domain Theory

be identied, one of their respective four specic nucleotide sequences need to be present.
If a sequence is not classied as a promoter by the domain theory, then that sequence is
classied as negative (i.e., not a promoter).
As noted earlier, the promoters domain theory is coupled with a database of 106 items
in the UCI repository: 53 examples of promoters and 53 non-examples. When interpreting
each leaf of the domain theory as a logical conjunction, the theory classies all of the data
items as negative. Thus, it is clearly too restrictive: No sequence in the database satises
all the conditions specied in the theory. Two pieces of domain knowledge suggest ways
to loosen the conditions of the domain theory. First, the conformation condition has very
weak biological support. This was implied by the initial KBANN experiments (Shavlik
et al., 1992), where none of the learned rules referenced the conformation conditions. In
addition, the EITHER system (Ourston, 1991) eliminated rules involving conformation
altogether from the domain theory. Eliminating conformation was also supported by a
domain expert (Ourston, 1991). The second piece of domain knowledge is that the concepts
in this domain tend to take the form of M-of-N concepts. Some of the nal rules extracted
in the KBANN approach take this form. This was also made clear in the NEITHER-MofN
system (Baes & Mooney, 1993), which added a mechanism to handle M-of-N concepts to
the original learning mechanism of the EITHER/NEITHER system.

3.

M-of-N

Interpretations of the DNA Promoters Theory

We can modify the original DNA domain theory as follows, and allow more sequences to be
positively classied as promoters: a) eliminate the conformation condition altogether from
the theory, and b) reinterpret the conjunctions of conditions in the leaves of Figure 1 as
M-of-N concepts. As usually interpreted, each of the leaves of Figure 1 is equivalent to a
concept of the form (N-of-N c1c2:::cN ). For example, the conjunction (p-37=c ^ p-36=t ^
p-35=t ^ p-34=g ^ p-33=a ^ p-32=c) of the leftmost leaf is logically equivalent to (6-of-6
p-37=c p-36=t p-35=t p-34=g p-33=a p-32=c).
Progressively less restrictive theories can be created by lowering the number of conditions
that need to be satised in each leaf of the theory. Thus, a new theory can be constructed
where each of the N-of-N concepts is substituted by (N { 1)-of-N, (N { 2)-of-N, etc. The
variable N (i.e., the number of conditions at a leaf) is decremented by a constant value i to
obtain the value M for each of the M-of-N concepts at the leaves of the theory. Figure 2
shows the accuracy of the theories constructed in this manner over all of the examples in the
database. As the number of conditions that have to be met for each of the M-of-N concepts is
lowered, the number of false negatives decreases, and the number of false positive increases.
The total number of misclassications (false negatives plus false positives) is minimized
when each leaf is interpreted as a (N { 2)-of-N concept, resulting in an accuracy of 93.4%.
Even better accuracies can be obtained if we remove the constant decrement restriction.
That is, we allow greater exibility in choosing dierent values of M for each of the leaves
corresponding to the minus 35 and minus 10 regions in Figure 1. By an exhaustive search
through all of the 388800 possible combinations of M values we found twelve theories that
correctly classify 103 of the 106 examples in the database (i.e., the accuracy of these theories
is 97.2%), and 5148 theories of accuracy equal or better than 93.4%. Figure 3 shows the
probabilities of obtaining theories of dierent accuracies when the value of M for each of
363

Ortega

Correct
False
False
Percent
M-of-N criteria
Predictions Positives Negatives Accuracy
N-of-N (original theory)
53
0
53
50.00%
N-of-N w/ conformation rule removed
57
0
49
53.77%
(N { 1)-of-N
78
0
28
73.58%
(N { 2)-of-N
99
1
6
93.40%
(N { 3)-of-N
90
16
0
84.91%
(N { 4)-of-N
62
44
0
58.49%
Figure 2: Accuracy of DNA promoters theory under dierent M-of-N interpretations
0.06
P[accuracy]
0.05

Probability

0.04

0.03

0.02

0.01

0
0

0.2

0.4

0.6

0.8

1

Accuracy

Figure 3: Probability distribution of DNA-theory-interpretation accuracies
the contact leaves of Figure 1 is chosen at random (but under the restriction that M  N,
where N is the total number of conditions at a particular leaf). The probabilities in Figure 3
were computed by counting the total number of combinations of M values that produced
theories of specic accuracies. The mean accuracy of a randomly chosen theory is 76.5%,
with a standard deviation of 9.3%.
These results show that when leaves are interpreted as appropriate M-of-N concepts,
the existing DNA domain theory possesses a large amount of predictive information, a fact
that has also been pointed out by Koppel et al. (1994a). It is much better than the null
power suggested by its initial 50% accuracy, which would be equivalent to random guessing
with no theory at all. At the least, the theory allows us to make a single random guess of
an M-of-N interpretation with an expected accuracy of 76.5%. As shown in Figure 2 a few
random guesses allow us to do much better than that.
364

On the Informativeness of the DNA Promoter Sequences Domain Theory

4. Learning with the DNA Promoters Theory

The accuracies of various systems that integrate analytical and empirical learning are around
93% (Baes & Mooney, 1993). These results are typically means computed over multiple
trials with 80-85 training examples and 21-26 tests examples. Our reported accuracies of
93.4% and 97.2% are not based on such splits of training and test data. Instead, they
represent the maximum accuracies (relative to the database of 106 examples) that could be
obtained by learning algorithms with certain representational biases. For example, 93.4%
is the maximum accuracy that may be achieved by a learning system that identies (N
{ i)-of-N concepts at the leaves, where i is constant across leaves. This approach can be
converted into a learning task where the learner identies the optimal value of i given a set
of training examples, and evaluates the resultant classier using a test set. The results of
this algorithm, averaged over 100 trials, produce mean accuracies of 88.7% with 10 training
examples, and 92.5% with 85 training examples (on a test set of 21 examples). These results
are similar to the best of the algorithms reported by Baes and Mooney (1993).
Koppel et al. (1994a) also show that there is considerable information in a \reinterpreted" promoters domain theory. In their DOP (Degree of Provedness) classication
methodology, the logical operations of a propositional domain theory (AND, OR, or NOT)
are replaced by arithmetic equivalents that contain a degree of uncertainty. Rather than
directly returning a truth value indicating whether an example is positive, the system rst
calculates the DOP numerical score for that example. If the DOP score value is greater
than a pre-specied threshold value, then the example is considered \suciently" proved
and thus classied as a positive example by the theory. Otherwise, the example is classied
as negative. Koppel et al. determine the threshold value with two pieces of knowledge: a)
the distribution of the DOP score over all examples, and b) the proportion (n%) of positive
examples in the database. The DOP values for all examples are sorted, so that the threshold
value can be set to the value that separates the n% of the examples with highest DOP values
from the rest. An important assumption is that the domain theory be of a certain proofadditive nature, so that DOP values will be higher for positive examples than for negative
examples. The DOP classication methodology achieves a high accuracy (92.5%) when applied to the DNA promoter sequences domain theory. As with our approach, this accuracy
is not based on a split of the available data into training and test sets, and represents an
upper bound on the accuracy that could be obtained if their method was converted into a
learning algorithm. The DOP classication methodology could be converted into a learning
algorithm by estimating the distribution of DOP values over a set of training examples.

5. Concluding Remarks

This note does not detail a new learning algorithm. Rather, it demonstrates that a suitable
learning model in the promoters domain is nding the correct number, M, for each of the
M-of-N concepts at the leaves of the original domain theory. 1 Assessing the diculty of
learning using the available theory is usually complicated by the need to understand the
learning algorithms that exploit the theory. The theory-accuracy distribution of Figure 3
1. Some algorithms may introduce some structural modication to the theory (i.e., add/delete clauses and
conditions). However, the increase in accuracy due to these structural modications is negligible in the
case of the promoters domain, as illustrated by the high accuracies that can be obtained without them.

365

Ortega

helps characterize learning complexity in this domain (under the M-of-N model) and provides a dimension along which to evaluate the performance of learning algorithms that use
the DNA promoter's theory as their testbed.

Acknowledgements
This research was supported by a grant from NASA Ames Research Center (NAG 2-834) to
Doug Fisher. I am grateful for the suggestions of Doug Fisher, Stefanos Manganaris, Doug
Talbert, Jing Lin, as well as comments and pointers from Larry Hunter and anonymous
JAIR referees.

References

Baes, P. T., & Mooney, R. J. (1993). Symbolic revision of theories with M-of-N rules. In
Proceedings of the Thirteenth International Joint Conference on Articial Intelligence
Chambery, France.
Harley, C. B., & Reynolds, R. P. (1987). Analysis of e. coli promoter sequences. Nucleic
Acids Research, 15 (5), 2343{2361.
Hawley, D. K., & McClure, W. R. (1983). Compilation and analysis of escherichia coli
promoter DNA sequences. Nucleic Acids Research, 11 (8), 2237{2255.
Hirsh, H., & Japkowicz, N. (1994). Boostraping training-data representations for inductive
learning: A case study in molecular biology. In Proceedings of the Twelfth National
Conference on Articial Intelligence, pp. 639{644 Seattle, WA.
Koppel, M., Segre, A. M., & Feldman, R. (1994a). Getting the most from awed theories.
In Proceedings of the Eleventh International Conference on Machine Learning, pp.
139{147 New Brunswick, NJ.
Koppel, M., Feldman, R., & Segre, A. M. (1994b). Bias-driven revision of logical domain
theories. Journal of Articial Intelligence Research, 1, 159{208.
Lisser, S., & Margalit, H. (1993). Compilation of e. coli mRNA promoter sequences. Nucleic
Acids Research, 21 (7), 1507{1516.
Mahoney, J. J., & Mooney, R. J. (1993). Combining connectionist and symbolic learning
to rene certainty-factor rule-bases. Connection Science, 5 (3{4), 339{364.
Mahoney, M. J., & Mooney, R. J. (1994). Comparing methods for rening certainty-factor
rule-bases. In Proceedings of the Eleventh International Conference on Machine Learning, pp. 173{180 New Brunswick, NJ.
Murphy, P. M., & Aha, D. W. (1992). UCI Repository of Machine Learning Databases.
Department of Information and Computer Science, University of California at Irvine,
Irvine, CA.
366

On the Informativeness of the DNA Promoter Sequences Domain Theory

Norton, S. W. (1994). Learning to recognize promoter sequences in e. coli by modeling
uncertainty in the training data. In Proceedings of the Twelfth National Conference
on Articial Intelligence, pp. 657{663 Seattle, WA.
O'Neill, M. C. (1989). Escherichia coli promoters I: Consensus as it relates to spacing class,
specicity, repeat structure, and three dimensional organization. Journal of Biological
Chemistry, 264, 5522{5530.
O'Neill, M. C., & Chiafari, F. (1989). Escherichia coli promoters II: A spacing-class dependent promoter search protocol. Journal of Biological Chemistry, 264, 5531{5534.
Opitz, D. W., & Shavlik, J. W. (1994). Using genetic search to rene knowledge-based
neural networks. In Proceedings of the Eleventh International Conference on Machine
Learning, pp. 208{216 New Brunswick, NJ.
Ortega, J. (1994). Making the most of what you've got: using models and data to improve learning rate and prediction accuracy. Tech. rep. TR-94-01, Computer Science
Dept., Vanderbilt University. Abstract appears in Proceedings of the Twelfth National
Conference on Articial Intelligence, p. 1483, Seattle, WA.
Ourston, D. (1991). Using Explanation-Based and Empirical Methods in Theory Revision.
Ph.D. thesis, University of Texas, Austin, TX.
Shavlik, J. W., Towell, G., & Noordewier, M. O. (1992). Using neural networks to rene
existing biological knowledge. International Journal of Human Genome Research, 1,
81{107.
Towell, G. G. (1990). Symbolic Knowledge and Neural Networks: Insertion, Renement,
and Extraction. Ph.D. thesis, University of Wisconsin, Madison, WI.
Towell, G. G., Shavlik, J. W., & Noordewier, M. O. (1990). Renement of approximate
domain theories by knowledge-based neural networks. In Proceedings of the Eighth
National Conference on Articial Intelligence, pp. 861{866 Boston, MA.

367

Journal of Articial Intelligence Research 2 (1995) 475-500

Submitted 10/94; published 5/95

Adaptive Load Balancing: A Study in Multi-Agent
Learning
Andrea Schaerf

aschaerf@dis.uniroma1.it

Dipartimento di Informatica e Sistemistica
Universita di Roma \La Sapienza", Via Salaria 113, I-00198 Roma, Italy

Yoav Shoham

Robotics Laboratory, Computer Science Department
Stanford University, Stanford, CA 94305, USA

Moshe Tennenholtz

Faculty of Industrial Engineering and Management
Technion, Haifa 32000, Israel

shoham@flamingo.stanford.edu
moshet@ie.technion.ac.il

Abstract
We study the process of multi-agent reinforcement learning in the context of load balancing in a distributed system, without use of either central coordination or explicit communication. We rst dene a precise framework in which to study adaptive load balancing,
important features of which are its stochastic nature and the purely local information
available to individual agents. Given this framework, we show illuminating results on the
interplay between basic adaptive behavior parameters and their eect on system eciency.
We then investigate the properties of adaptive load balancing in heterogeneous populations,
and address the issue of exploration vs. exploitation in that context. Finally, we show that
naive use of communication may not improve, and might even harm system eciency.

1. Introduction
This article investigates multi-agent reinforcement learning in the context of a concrete
problem of undisputed importance { load balancing. Real life provides us with many examples of emergent, uncoordinated load balancing: trac on alternative highways tends to
even out over time; members of the computer science department tend to use the most powerful of the networked workstations, but eventually nd the lower load on other machines
more inviting; and so on. We would like to understand the dynamics of such emergent
load-balancing systems and apply the lesson to the design of multi-agent systems.
We dene a formal yet concrete framework in which to study the issues, called a multiagent multi-resource stochastic system, which involves a set of agents, a set of resources,
probabilistically changing resource capacities, probabilistic assignment of new jobs to agents,
and probabilistic job sizes. An agent must select a resource for each new job, and the
eciency with which the resource handles the job depends on the capacity of the resource
over the lifetime of the job as well as the number of other jobs handled by the resource over
that period of time. Our performance measure for the system aims at globally optimizing
the resource usage in the system while ensuring fairness (that is, a system shouldn't be made
ecient at the expense of any particular agent), two common criteria for load balancing.
c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Schaerf, Shoham, & Tennenholtz

How should an agent choose an appropriate resource in order to optimize these measures?
Here we make an important assumption, in the spirit of reinforcement learning (Sutton,
1992): The information available to the agent is only its prior experience. In particular,
the agent does not necessarily know the past, present, or future capacities of the resources,1
and is unaware of past, current, or future jobs submitted by the various agents, not even
the relevant probability distributions. The goal of each agent is thus to adapt its resourceselection behavior to the behavior of the other agents as well as to the changing capacities
of the resources and to the changing load, without explicitly knowing what they are.
We are interested in several basic questions:

 What are good resource-selection rules?
 How does the fact that dierent agents may use dierent resource-selection rules aect
the system behavior?

 Can communication among agents improve the system eciency?
In the following sections we show illuminating answers to these questions. The contribution of this paper is therefore twofold. We apply multi-agent reinforcement learning to the
domain of adaptive load balancing and we use this basic domain in order to demonstrate
basic phenomena in multi-agent reinforcement learning.
The structure of this paper is as follows. In Section 2 we discuss our general setting.
The objective of this section is to motivate our study and point to its impact. The formal
framework is dened and discussed in Section 3. Section 4 completes the discussion of this
framework by introducing the resource selection rule and its parameters, which function as
the \control knobs" of the adaptive process. In Section 5 we present experimental results
on adaptive behavior within our framework and show how various parameters aect the
eciency of adaptive behavior. The case of heterogeneous populations is investigated in
Section 6, and the case of communicating populations is discussed in Section 7. In Section 8
we discuss the impact of our results. In Section 9 we put our work in the perspective of
related work. Finally, in Section 10 we conclude with a brief summary.

2. The General Setting

This paper applies reinforcement learning to the domain of adaptive load balancing. However, before presenting the model we use and our detailed study, we need to clarify several
points about our general setting. In particular, we need to explain the interpretation of
reinforcement learning and the interpretation of load balancing we adopt.
Much work has been devoted in the recent years to distributed and adaptive load balancing. One can nd related work in the eld of distributed computer systems (e.g., Pulidas,
Towsley, & Stankovic, 1988; Mirchandaney & Stankovic, 1986; Billard & Pasquale, 1993;
Glockner & Pasquale, 1993; Mirchandaney, Towsley, & Stankovic, 1989; Zhou, 1988; Eager,
Lazowska, & Zahorjan, 1986), in organization theory and management science (e.g., Malone,
1. In many applications the capacities of the resources are known, at least to some extent. This point will
be discussed later. Basically, in this paper we wish to investigate how far one can go using only purely
local feedback and without the use of any global information (Kaelbling, 1993; Sutton, 1992).

476

Adaptive Load Balancing: A Study in Multi-Agent Learning

1987), and in distributed AI (e.g., Bond & Gasser, 1988). Although some motivations of
the above-mentioned lines of research are similar, the settings discussed have some essential
dierences.
Work on distributed computer systems adopts the view of a set of computers each of
which controls certain resources, has an autonomous decision-making capability, and jobs
arrive to it in a dynamic fashion. The decision-making agents of the dierent computers
(also called nodes) try to share the system load and coordinate their activities by means of
communication. The actual action to be performed, based on the information received from
other computers, may be controlled in various ways. One of the ways adopted to control
the related decisions is through learning automata (Narendra & Thathachar, 1989).
In the above-mentioned work each agent is associated with a set of resources, where both
the agent and the related resources are associated with a node in the distributed system.
Much work in management science and in distributed AI adopts a somewhat complementary
view. In dierence to classical work in distributed operating systems, an agent is not
associated with a set of resources that it controls. The agents are autonomous entities which
negotiate among themselves (Zlotkin & Rosenschein, 1993; Kraus & Wilkenfeld, 1991) on
the use of shared resources. Alternatively, the agents (called managers in this case) may
negotiate the task to be executed with the processors which may execute it (Malone, 1987).
The model we adopt has the avor of models used in distributed AI and organization
theory. We assume a strict separation between agents and resources. Jobs arrive to agents
who make decisions about where to execute them. The resources are passive (i.e., do not
make decisions). A typical example of such a setting in a computerized framework is a set
of PCs, each of which is controlled by a dierent user and submits jobs to be executed on
one of several workstations. The workstations are assumed to be independent of each other
and shared among all the users. The above example is a real-life situation which motivated
our study and the terminology we adopt is taken from such a framework. However, there
are other real-life situations related to our model in areas dierent from classical distributed
computer systems.
A canonical problem related to our model is the following one (Arthur, 1994): An agent,
embedded in a multi-agent system, has to select among a set of bars (or a set of restaurants).
Each agent makes an autonomous decision but the performance of the bar (and therefore of
the agents that use it) is a function of its capacity and of the number of agents that use it.
The decision of going to a bar is a stochastic process but the decision of which bar to use is
an autonomous decision of the respective agent. A similar situation arises when a product
manager decides which processor to use in order to perform a particular task. The model we
present in Section 3 is a general model where such situations can be investigated. In these
situations a job arrives to an agent (rather than to a node consisting of particular resources)
who decides upon the resource (e.g., restaurant) where his job should be executed; there is
a-priori no association between agents and resources.
We now discuss the way the agents behave in such a framework. The common theme
among the above-mentioned lines of research is that load-balancing is achieved by means
of communication among active agents or active resources (through the related decisionmaking agents). In our study we adopt a complementary view. We consider agents who
act in a purely local fashion, based on purely local information as described in the recent
reinforcement learning literature. As we mentioned, learning automata were used in the
477

Schaerf, Shoham, & Tennenholtz

eld of distributed computer systems in order to perform adaptive load balancing. Nevertheless, the related learning procedures rely heavily on communication among agents (or
among decision-making agents of autonomous computers). Our work applies recent work
on reinforcement learning in AI where the information the agent gets is purely local. Hence,
an agent will know how ecient the service in a restaurant has been only by choosing it as a
place to eat. We don't assume that agents may be informed by other agents about the load
in other restaurants or that the restaurants will announce their current load. This makes
our work strictly dierent from other work applying reinforcement learning to adaptive load
balancing.
The above features make our model and study both basic and general. Moreover, the
above discussion raises the question of whether reinforcement learning (based on purely
local information and feedback) can guarantee useful load balancing. The combination of
the model we use and our perspective on reinforcement learning makes our contribution
novel. Nevertheless, as we mentioned above (and as we discuss in Section 9) the model we
use is not original to us and captures many known problems and situations in distributed
load balancing. We apply reinforcement learning, as discussed in the recent AI literature,
to that model and investigate the properties of the related process.

3. The Multi-Agent Multi-Resource Stochastic System

In this section we dene the concrete framework in which we study dynamic load balancing.
The model we present captures adaptive load balancing in the general setting mentioned
in Section 2. We restrict the discussion to discrete, synchronous systems (and thus the
denition below will refer to N , the natural numbers); similar denitions are possible in
the continuous case. We concentrate on the case where a job can be executed using any of
the resources. Although somewhat restricting, this is a common practice in much work in
distributed systems (Mirchandaney & Stankovic, 1986).

Denition 3.1 A multi-agent multi-resource stochastic system is a 6-tuple hA; R; P ; D; C;
SRi, where A = fa1; : : :; aN g is a set of agents, R = fr1; : : :; rM g is a set of resources,
P : A  N ! [0; 1] is a job submission function, D : A  N ! < is a probabilistic job size
function, C : RN ! < is a probabilistic capacity function, and SR is a resource-selection
rule.

The intuitive interpretation of the system is as follows. Each of the resources has a
certain capacity, which is a real number; this capacity changes over time, as determined by
the function C . At each time point each agent is either idle or engaged. If it is idle, it may
submit a new job with probability given by P . Each job has a certain size which is also
a real number. The size of any submitted job is determined by the function D. (We will
use the unit token where referring to job sizes and resource capacities, but we do not mean
that tokens come only in integer quantities.) For each new job the agent selects one of the
resources. This choice is made according to the rule SR; since there is much to say about
this rule, we discuss it separately in the next section.
In our model, any job may run on any resource. Furthermore, there is no limit on the
number of jobs served simultaneously by a given resource (and thus no queuing occurs).
However, the quality of the service provided by a resource at a given time deteriorates with
478

Adaptive Load Balancing: A Study in Multi-Agent Learning

the number of agents using it at that time. Specically, at every time point the resource
distributes its current capacity (i.e., its tokens) equally among the jobs being served by it.
The size of each job is reduced by this amount and, if it drops to (or below) zero, the job is
completed, the agent is notied of this, and becomes idle again. Thus, the execution time
of a job j depends on its size, on the capacity over time of the resource processing it, and
on the number of other agents using that resource during the execution of j .
Our measure of the system's performance will be twofold: We aim to minimize timeper-token, averaged over all jobs, as well as to minimize the standard deviation of this
random variable. Minimizing both quantities will ensure overall system eciency as well as
fairness. The question is which selection rules yield ecient behavior; so we turn next to
the denition of these rules.

4. Adaptive Resource-Selection Rules
The rule by which agents select a resource for a new job, the selection rule (SR), is the
heart of our adaptive scheme and the topic of this section. Throughout this section and
the following one we make an assumption of homogeneity. Namely, we assume that all
the agents use the same SR. Notice that although the system is homogeneous, each agent
will act based only on its local information. In Sections 6 and 7 we relax the homogeneity
assumption and discuss heterogeneous and communicating populations.
As we have already emphasized, among all possible adaptive SRs we are interested in
purely local SRs, ones that have access only to the experience of the particular agent. In our
setting this experience consists of results of previous job submissions; for each job submitted
by the agent and already completed, the agent knows the name r of the resource used, the
point in time, tstart , the job started, the point in time, tstop , the job was nished, and the
job size S . Therefore, the input to the SR is, in principle, a list of elements in the form
(r; tstart; tstop ; S ). Notice that this type of input captures the general type of systems we
are interested in. Basically, we wish to assume as little as possible about the information
available to an agent in order to capture real loosely-coupled systems where more global
information is unavailable.
Whenever agent i selects a resource for its job execution, i may get its feedback after
non-negligible time, where this feedback may depend on decisions made by other agents
before and after agent i's decision. This forces the agent to rely on a non-trivial portion of
its history and makes the problem much harder.
There are uncountably many possible adaptive SRs and our aim is not to gain exhaustive understanding of them. Rather, we have experimented with a family of intuitive and
relatively simple SRs and have compared them with some non-adaptive ones. The motivation for choosing our particular family of SRs is partially due to observations made by
cognitive psychologists on how people tend to behave in multi-agent stochastic and recurrent situations. In principle, our set of SRs captures the two most robust aspects of these
observations: \The law of eect" (Thronkide, 1898) and the \Power law of practice" (Blackburn, 1936). In our family of rules, called 
, which partially resembles the learning rules
discussed in the learning automata literature (Narendra & Thathachar, 1989), and partially resembles the interval estimation algorithm (Kaelbling, 1993), agents do not maintain
complete history of their experience. Instead, each agent, A, condenses this history into
479

Schaerf, Shoham, & Tennenholtz

a vector, called the eciency estimator, and denoted by eeA . The length of this vector is
the number of resources, and the i'th entry in the vector represents the agent's evaluation
of the current eciency of resource i (specically, eeA (R) is a positive real number). This
vector can be seen as the state of a learning automaton. In addition to eeA , agent A keeps
a vector jdA, which stores the number of completed jobs which were submitted by agent A
to each of the resources, since the beginning of time. Thus, within 
, we need only specify
two elements:
1. How agent A updates eeA when a job is completed
2. How agent A selects a resource for a new job, given eeA and jdA
Loosely speaking, eeA will be maintained as a weighted sum of the new feedback and the
previous value of eeA , and the resource selected will most probably be the one with highest
eeA entry except that with low probability some other resource will be chosen. These two
steps are explained more precisely in the following two subsections.

4.1 Updating the Eciency Estimator
We take the function updating eeA to be

eeA (R) := WT + (1 , W )eeA (R)
where T represents the time-per-token of the newly completed job and is computed from
the feedback (R; tstart; tstop; S ) in the following way:2

T = (tstop , tstart)=S
We take W to be a real value in the interval [0; 1], whose actual value depends on jdA(R).
This means that we take a weighted average between the new feedback value and the old
value of the eciency estimator, where W determines the weights given to these pieces of
information. The value of W is obtained from the following function:

W = w + (1 , w)=jdA(R)
In the above formula w is a real-valued constant. The term (1 , w)=jdA(R) is a correcting
factor, which has a major eect only when jdA (R) is low; when jdA (R) increases, reaching
a value of several hundreds, this term becomes negligible with respect to w.

4.2 Selecting the Resource

The second ingredient of adaptive SRs in 
 is a function pdA selecting the resource for a
new job based on eeA and jdA . This function is probabilistic. We rst dene the following
function
(
if jdA(R) > 0
A (R),n
0
pdA(R) := ee
,
n
E [ee ]
if jd (R) = 0
A

A

2. Using parallel processing terminology, T can be viewed as a stretch factor, which quanties the stretching
of a program's processing time due to multiprogramming (Ferrari, Serazzi, & Zeigner, 1983).

480

Adaptive Load Balancing: A Study in Multi-Agent Learning

where n is a positive real-valued parameter and E [eeA] represents the average of the values
of eeA (R) over all resources satisfying jdA (R) > 0. To turn this into a probability function,
we dene the pdA as the normalized version of pd0A :

pdA(R) := pd0A(R)=
where  = Rpd0A (R) is a normalization factor.3
The function pdA clearly biases the selection towards resources that have performed
well in the past. The strength of the bias depends on n; the larger the value of n, the
stronger the bias. In extreme cases, where the value of n is very high (e.g.,  20), the agent
will always choose the resource with the best record. This strategy of \always choosing
the best", although perhaps intuitively appealing, is in general not a good one; it does not
allow the agent to exploit improvements in the capacity or load on other resources. We
discuss this SR in the following subsection, and expand on the issue of exploration versus
exploitation in Sections 6 and 7.
To summarize, we have dened a general setting in which to investigate emergent load
balancing. In particular, we have dened a family of adaptive resource-selection rules,
parameterized by a pair (w; n). These parameters serve as knobs with which we tune the
system so as to optimize its performance. In the next section we turn to experimental
results obtained with this system.

4.3 The Best Choice SR (BCSR)

The Best Choice SR (BCSR) is a learning rule that assumes a high value of n, i.e, which
always chooses the best resource in a given point. We will assume w is xed to a given
value while discussing BCSR. In our previous work (Shoham & Tennenholtz, 1992, 1994),
we showed that learning rules that strongly resemble BCSR are useful for several natural
multi-agent learning settings. This suggests that we need to carefully study it in the case
of adaptive load balancing. As we will demonstrate, BCSR is not always useful in the load
balancing setting.
The dierence between BCSR and a learning rule where the value of n is low, is that
in the latter case the agent gives relatively high probability for the selection of a resource
that didn't give the best results in the past. In that case the agent might be able to notice
that the behavior of one of the resources has been improved due to changes in the system.
Note that the exploration of \non-best" resources is crucial when the dynamics of the
system includes changes in the capacities of the resources. In such cases, the agent could not
take advantage of possible increases in the capacity of resources if it uses the BCSR. One
might wonder, however, whether in cases where the main dynamic changes of the system
stem from load changes, relying on BCSR is sucient. If the latter is true, we will be
able to ignore the parameter n and to concentrate only on the BCSR, in systems where
the capacity of resources is xed. In order to clarify this point, we consider the following
example.
3. If for all R we have jdA (R) = 0, (i.e., if the agent is going to submit its very rst job), then we assume
the agent chooses a resource randomly (with a uniform probability distribution).

481

Schaerf, Shoham, & Tennenholtz

Suppose there are only two resources, R1 and R2 , whose respective (xed) capacities,
cR1 and cR2 , satisfy the equality cR1 = 2cR2 . Assume now that the load of the system varies

between a certain low value and a certain high one.
If the system's load is low and the agents adopt BCSR, then the system will evolve in
a way where almost all of the agents would be preferring R1 to R2. This is due to the
fact that, in the case of low load, there are only few overlaps of jobs, hence R1 is much
more ecient. On the other hand, when the system's load is high, R1 could be very busy
and some of the agents would then prefer R2, since the performance obtained using the
less crowded resource R2 could be better than the one obtained using the overly crowded
resource R1. In the extreme case of a very high load, we expect the agents to use R2 one
third of the time.
Assume now that the load of the system starts from a low level, then increases to a
high value, and then decreases to reach its original value. When the load increases, the
agents, that were mostly using R1, will start observing that R1's performance is becoming
worse and, therefore, following the BCSR they will start using R2 too. Now, when the load
decreases, the agents which were using R2 will observe an improvement in the performance
of R2, but the value they have stored for R1 (i.e., eeA (1)), will still reect the previous
situation. Hence, the agents will keep on using R2, ignoring the possibility of obtaining
much better results if they moved back to R1. In this situation, the randomized selection
makes the agents able to use R1 (with a certain probability) and therefore some of them
may discover that the performance of R1 is better than that of R2 and switch back to R1.
This will improve the system's eciency in a signicant manner.
The above example shows that the BCSR is, in the general case, not a good choice.
This is in general true when the value of n is too high.
In the above discussion we have assumed that the changes in the load are unforeseen. If
we are able to predict the changes in the load, the agents can simply use the BCSR while
the load is xed and then use a low value of n during the changes. In our case, instead,
without even realizing that the system has changed in some way, the agents would need to
(and, as we will see, would be able to) adapt to dynamic changes as well as to each other.

5. Experimental Results
In this section we compare SRs in 
 to each another, as well as to some non-adaptive,
benchmark selection rules.
The non-adaptive SRs we consider in this paper are those in which the agents partition
themselves according to the capacities and the load of the system in a xed predetermined
manner and each agent uses always the same resource. Later in the paper, a SR of this
kind is identied by a conguration vector, which species, for each resource, how many
agents use it. When we test our adaptive SRs, we compare the performance against the nonadaptive SRs that perform best on the particular problem. This creates a highly competitive
set of benchmarks for our adaptive SRs.
In addition, we compare our adaptive SRs to the load-querying SR which is dened as
follows: Each agent, when it has a new job, asks all the resources how busy they are and
always chooses the less crowded one.
482

Adaptive Load Balancing: A Study in Multi-Agent Learning

5.1 An Experimental Setting
We now introduce a particular experimental setting, in which many of the results described
below were obtained. We present it in order to be concrete about the experiments; however,
the qualitative results of our experiments were observed in a variety of other experimental
settings.
One motivation of our particular setting stems from the PCs and workstations problem
mentioned in Section 2. For example, part of our study is related to a set of computers
located at a single site. These computers have relatively high load with some peak hours
during the day and a low load at night (i.e., the chances a user of a PC submits a job
is higher during the day time of the week days than at night and on weekend). Another
part of our study is related to a set of computers split all around the world, where the
load has quite random structure (i.e., due to dierence in time zones, users may use PCs in
unpredictable hours).
Another motivation of our particular setting stems from the restaurant problem mentioned in Section 2 (for discussion on the related \bar problem" see Arthur, 1994). For
example, we can consider a set of snack bars located at an industrial park. These snack
bars have relatively high loads with some peak hours during the day and low load at night
(i.e., the chances an employee will choose to go to a snack-bar is higher during the day
because there are more employees present during the day). Conversely, we can assume a
set of bars near an airport where the load has quite random structure (i.e., the airport
employees may like to use these snack-bars in quite unpredicted hours).
Although these are particular real-situations, we would like to emphasize the general
motivation of our study and the fact that the related phenomena have been observed in
various dierent settings.
We take N , the number of agents, to be 100, and M , the number of resources, to be
5. In the rst set of experiments we take the capacities of the resources to be xed. In
particular, we take them to be c1 = 40; c2 = 20; c3 = 20; c4 = 10; c5 = 10. We assume
that all agents have the same probability of submitting a new job. We also assume that all
agents have the same distribution over the size of jobs they submit; specically, we assume
it to be a uniform distribution over the integers in the range [50,150].
For ease of exposition, we will assume that each point in time corresponds to a second,
and we consequently count the time in minutes, hours, days, and weeks. The hour is our
main point of reference; we assume, for simplicity, that the changes in the system (i.e., load
change and capacity change) happen only at the beginning of a new hour. The probability
of submitting a job at each second, which corresponds to the load of the system, can vary
over time; this is the crucial factor to which the agents must adapt. Note that agents can
submit jobs at any second, but the probability of such submission may change. In particular
we concentrate on three dierent values of this quantity, called Llo ; Lhi and Lpeak , and we
assume that the system load switches between those values. The actual values of Llo ; Lhi and
Lpeak in the following quantitative results are 0:1%, 0:3% and 1%, which roughly correspond
to each agent submitting 3.6, 10.8, and 36 jobs per hour (per agent) respectively.
483

Schaerf, Shoham, & Tennenholtz

load

conguration
time-per-token
Llo
f100; 0; 0; 0; 0g
38.935
Lhi
f66; 16; 16; 1; 1g
60.768
Lpeak f40; 20; 20; 10; 10g
196.908
Figure 1: Best non-adaptive SRs for xed load
In the following, when measuring success, we will refer only to the average time-pertoken.4 However, the adaptive SRs that give the best average time-per-token were also
found to be fair.

5.2 Fixed Load

We start with the case in which the load is xed. This case is not the most interesting for
adaptive behavior; however, a satisfactory SR should show reasonably ecient behavior in
that basic case, in order to be useful when the system stabilizes.
We start by showing the behavior of non-adaptive benchmark SRs in the case of xed
load.5 Figure 1 shows those that give the best results, for each of the three loads.
As we can see, there is a big dierence between the three loads mentioned above. When
the load is particularly high, the agents should scatter around all the resources at a rate
proportional to their capacities; when the load is low they should all use the best resource.
Given the above, it is easy to see that an adaptive SR can be eective only if it enables
moving quickly from one conguration to the other.
In a static setting such as this, we can expect the best non-adaptive SRs to perform better than adaptive ones, since the information gained by the exploration of the adaptive SRs
can be built-in in the non-adaptive ones. The experimental results conrm this intuition,
as shown in Figure 2 for Lhi . The gure shows the performance obtained by the population
when the value of n varies between 2 to 10 and for three values of w: 0.1, 0.3, and 0.5.
Note that for the values of (n; w) that are good choices in the dynamic cases (see later in
the paper, values in the intervals [3; 5] and [0:1; 0:5], respectively), the deterioration in the
performance of the adaptive SRs with respect to the non-adaptive ones is small. This is an
encouraging result, since adaptive SRs are meant to be particularly suitable for dynamic
systems. In the following subsections we see that indeed they are.

5.3 Changing Load

We now begin to explore more dynamic settings. Here we consider the case in which the
load on the system (that is, the probability of agents submitting a job at any time) changes
over time. In this paper we present two dynamic settings: One in which the load changes
according to a xed pattern with only a few random perturbations and another in which the
load varies in some random fashion. Specically, in the rst case we x the load to be Lhi
4. In the data shown later we refer, for convenience, to the time for 1000 tokens.
5. The non-adaptive SRs are human-designed SRs that are used as benchmarks; they assume knowledge of
the load and capacity, which is not available for the adaptive SRs we design.

484

Adaptive Load Balancing: A Study in Multi-Agent Learning

A
v
e
r
a
g
e
T
i
m
e
p
e
r
T
o
k
e
n

6
67





 Weight: w = 0.5
Weight: w = 0.3
 Weight: w = 0.1

66





65
64





63
62
61
2



 

 


 


















3
4 5
6 7
8
9 10
Exponent of the Randomization Function: n

-

Figure 2: Performance of the adaptive Selection Rules for xed load
for ten consecutive hours, for ve days a week, with two randomly chosen hours in which
it is Lpeak , and to be Llo for the rest of the week. In the second case, we x the number
of hours in a week for each load as in the rst case, and we distribute them completely
randomly in a week.
The results obtained for the two cases are similar. Figure 3 shows the results obtained
by the adaptive SRs in the case of random load. The best non-adaptive deterministic
SR gives the time-per-token value of 69:201 obtained with the conguration (partition of
agents) f52; 22; 22; 2; 2g; the adaptive SRs are superior. The load-querying SR instead gets
the time-per-token value of 48:116, which is obviously better, but is not so far from the
performances of the adaptive SRs.
We also observe the following phenomenon: Given a xed n (resp. a xed w) the average
time-per-token is non-monotonic in w (resp. in n). This phenomenon is strongly related to
the issue of exploration versus exploitation mentioned before and to phenomena observed
in the study of Q-learning (Watkins, 1989).
We also notice how the two parameters n and w interplay. In fact, for each value of
w the minimum of the time per token value is obtained with a dierent value of n. More
precisely, the higher w is the lower n must be in order to obtain the best results. This means
that, in order to obtain high performance, highly exploratory activity (low n) should be
matched with giving greater weight to the more recent experience (high w). This \parameter
485

Schaerf, Shoham, & Tennenholtz

A
v
e
r
a
g
e
T
i
m
e
p
e
r
T
o
k
e
n

6
71
70
69


 Weight: w = 0.5




Weight: w = 0.3
 Weight: w = 0.1













68



 

    






67
66
65
2













3
4 5
6 7
8
9 10
Exponent of the Randomization Function: n

-

Figure 3: Performance of the adaptive Selection Rules for random load
matching" can be intuitively explained in the following qualitative way: The exploration
activity pays because it allows the agent to detect changes in the system. However, it is
more eective if, when a change is detected, it can signicantly aect the eciency estimator
(i.e., if w is high). Otherwise, the cost of the exploration activity is greater than its gain.

5.4 Changing Capacities
We now consider the case in which the capacity of the resources can vary over time. In
particular, we will demonstrate our results in the case of the previously mentioned setting.
We will assume the capacities rotate randomly among the resources and, in ve consecutive
days, each resource gets the capacity of 40 for one day, 20 for 2 days, and 10 for the other
2 days.6 The load also varies randomly.
The results of this experiment are shown in Figure 4. The best non-adaptive SR
in this case gives the time-per-token value of 118:561 obtained with the conguration
f20; 20; 20; 20; 20g.7 The adaptive SRs give much better results, which are only slightly
6. Usually the capacities will change in a less dramatic fashion. We use the above-mentioned setting in
order to demonstrate the applicability of our approach under severe conditions.
7. The load-querying SR gives the same results as in the case of xed capacities, because such SR is
obviously not inuenced by the change.

486

Adaptive Load Balancing: A Study in Multi-Agent Learning

A
v
e
r
a
g
e
T
i
m
e
p
e
r
T
o
k
e
n

6




92.5




90





87.5



82.5

    




80




85

77.5



2








 Weight: w = 0.5

Weight: w = 0.3
 Weight: w = 0.1




-

3
4
5
6 7 8
9 10
Exponent of the Randomization Function: n

Figure 4: Performance of the adaptive Selection Rules for changing capacities
worse than in the case of xed capacities. The phenomena we mentioned before are visible
in this case too. See for example how a weight of 0:1 mismatches with the low values of n.

6. Heterogeneous Populations
Throughout the previous section we have assumed that all the agents use the same SR, i.e.
Homogeneity Assumption. Such assumption models the situation in which there is a sort
of centralized o-line controller which, in the beginning, tells the agents how to behave and
then leaves the agents to make their own decisions.
The situation described above is very dierent from having an on-line centralized controller which makes every decision. However, we would like now to move even further from
that and investigate the situation in which each agent is able to make its own decision about
which strategy to use and, maybe, adjust it over time.
As a step toward the study of systems of this kind, we drop the Homogeneity Assumption
and consider the situation in which part of the population uses one SR and the other part
uses a second one.
In the rst set of experiments, we consider the setting discussed in Subsection 5.1 and
we confront one with the other, two populations (called 1 and 2) of the same size (50 agents
each). Each population uses a dierent SR in 
. The SR of population i (for i = 1; 2) will
487

Schaerf, Shoham, & Tennenholtz



A
v
e
r
a
g
e
T
i
m
e
p
e
r
T
o
k
e
n

6



67
66



65























63
61





 


  

64
62


 : T1
 : T2



2 3
4 5
6 7
8
9 10
Exponent of the Randomization Function (n2 )

-

Figure 5: Performance of 2 populations of 50 agents with n1 = 4 and w1 = w2 = 0:3
be determined by the pair of parameters (wi; ni ). The measure of success of population i
will be dened as the average time-per-token of its members, and will be denoted by Ti .
Figure 5 shows the result obtained for w1 = w2 = 0:3, and n1 = 4, and for dierent
values of n2 , in the case of randomly varying load.
Our results expose the following phenomenon: The two populations obtain dierent
outcomes from the ones they obtain in the homogeneous case. More specically, for 4 
n2  6 , the results obtained by the agents which use n2 are generally better than the results
obtained by the ones which use n1 , despite the fact that an homogeneous population which
uses n1 gets better results than an homogeneous population which uses n2 .
The phenomenon described above has the following intuitive explanation. For n2 in
the above-mentioned range, the population which uses n2 is less \exploring" (i.e., more
\exploiting") than the other one, and when it is left on its own it might not be able to
adapt to the changes in a satisfactory manner. However, when it is joined with the other
population, it gets the advantages of the experimental activity of agents in that population,
without paying for it. In fact, the more exploring agents, in trying to unload the most
crowded resources, make a service to the other agents as well.
It is worth observing in Figure 5 that when n2 is low (e.g., n2  3) the agents that use
n2 take the role of explorers and lose a lot, while the agents that use n1 gain from that
situation. Conversely, for high values of n2 (e.g., n2  7) the performances of the exploiters,
488

Adaptive Load Balancing: A Study in Multi-Agent Learning

A
v
e
r
a
g
e
T
i
m
e
p
e
r
T
o
k
e
n



6
67
66



65


 : T1
 : T2

 





 


64
63
62










61
2
















3
4 5
6 7
8
9 10
Exponent of the Randomization Function: n2

-

Figure 6: Performance of 2 populations of 90/10 agents with n1 = 4 and w1 = w2 = 0:3
which use n2 , deteriorate. This means that if the exploiters are too static, then they hinder
each other, and the explorers can take advantage of it.
For a better understanding of the phenomena involved, we have experimented with an
asymmetric population, composed of one large group and one small one, instead of two
groups of similar size. Figure 6 shows the results obtained using a setting similar to the
one above, but where population 1 is composed of 90 members while population 2 consists
of only 10 members. In this case, for every value of n2  4, the exploiters do better than
the explorers. The experiments also show that in this case, the higher n2 is the better T2
is, i.e. the more the exploiters exploit, the more they gain.
The above results suggest that a single agent gets the best results for itself by being noncooperative and always adopting the resource with the best performance (i.e., use BCSR),
given that the rest of the agents use an adaptive (i.e., cooperative) SR. However, if all of
the agents are non-cooperative then all of them will lose.8 In conclusion, the selsh interest
of an agent does not match with the interest of the population. This is contrary to results
obtained in other basic contexts of multi-agent learning (Shoham & Tennenholtz, 1992).
What we have shown is how, for a xed value of w, coexisting populations adopting
dierent values of n interact. Similar results are obtained when we x the value of n and
8. This is in fact an illuminating instance of the well-known prisoners dilemma (Axelrod, 1984).

489

Schaerf, Shoham, & Tennenholtz

A
v
e
r
a
g
e
T
i
m
e
p
e
r
T
o
k
e
n

6
67




 : T1
 : T2

66
65
64




















63

























62
61

-

0.01 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Weight of the estimator parameter (w2)

1

Figure 7: Performance of 2 populations of 50 agents with n1 = n2 = 4 and w1 = 0:3
use two dierent values for w. In such cases, the agents adopting the lower value of w are
in general the winners, as shown in Figure 7 for n1 = n2 = 4 and w1 = 0:3. When w is very
low then the corresponding agents get poor results and they are no longer the winners, as
in the case of very high n in Figure 5.
Another interesting phenomenon is obtained when confronting adaptive agents with
load-querying agents. Load-querying agents are agents who are able to consult the resources
about where they should submit their jobs. A load-querying agent will submit its job to the
most unloaded resource at the given point. When confronting load-querying agents with
adaptive ones, the results obtained by the adaptive agents are obviously worse than the
results obtained by the load-querying ones, but are better than the results obtained by a
complete population of adaptive agents. This means that load-querying agents do not play
the role of \parasites", as the above-mentioned \exploiters"; the load-querying agents help
in maintaining the load balancing among the resources, and therefore help the rest of the
agents. Another result we obtain is that agents who adopt deterministic SRs may behave
as parasites and worsen the performance of adaptive agents.
These assertions are supported by the experiments described in Figure 8, where a population of 90 agents, each of which uses an adaptive SR with parameters (n; w), is faced with
a minority of 10 agents which use dierent SRs, as stated above. In particular, in the four
cases we consider, the minority behaves in the following ways: (i) they choose the resource
490

Adaptive Load Balancing: A Study in Multi-Agent Learning

90 agents
10 agents
T1
(.3,4)
(.3,20)
65.161
(.3,4)
(.1,4)
64.630
(.3,4) Load-querying 62.320
(.3,4)
Using Res. 0 65.499

T2

59.713
63.818
47.236
55.818

Figure 8: Performance of 2 populations of 90/10 agents with various SRs
which gave best results, (ii) they are very conservative in updating the history, (iii) they
are load-querying agents, (iiii) they all use deterministically the resource with capacity 40
(in our basic experimental setting).

7. Communication among Agents
Up to this point, we have assumed that there is no direct communication among the agents.
The motivation for this was that we considered situations in which there were absolutely
no transmission channels and protocols. This assumption is in agreement with the idea of
multi-agent reinforcement learning. In systems where massive communication is feasible
we are not so much concerned with multiple agent adaptation, and the problem reduces to
supplying satisfactory communication mechanisms. Multi-agent reinforcement learning is
most interesting where real life forces agents to act without a-priori arranged communication channels and we must rely on action-feedback mechanisms. However, it is of interest to
understand the eects of communication on the system eciency (as in Shoham & Tennenholtz, 1992; Tan, 1993), where the agents are augmented with some sort of communication
capabilities. Our study of this extension led to some illuminating results, which we will now
present.
We assume that each agent can communicate only with some of the other agents, which
we call its neighbors. We therefore consider a relation neighbor-of and assume it is reexive, symmetric and transitive. As a consequence, the relation neighbor-of partitions the
population into equivalence classes, that we call neighborhoods.
The form of communication we consider is based on the idea that the eciency estimators of agents within a neighborhood will be shared among them when a decision is made
(i.e., when an agent chooses a resource). The reader should notice that this is a naive
form of communication and that more sophisticated types of communication are possible.
However, the above form of communication is most natural when we concentrate on agents
that update their behavior based only on past information. In particular, this type of
communication is similar to the ones used in the above-mentioned work on incorporating
communication into the framework of multi-agent reinforcement learning.
We suppose that dierent SRs may be used by dierent agents in the same population,
but we impose the condition that within a single neighborhood, the same SR is used by all
its members.
We also assume that each agent keeps its own history and updates it by itself in the
usual way. The choice, instead, is based not only on the agent eciency estimator, but on
491

Schaerf, Shoham, & Tennenholtz

A
v
e
r
a
g
e
T
i
m
e
p
e
r
T
o
k
e
n

6
71
70














68





67



66

 

65
2














69









  






 : 5 CNs of 20 agents
: 20 CNs of 5 agents
 : 50 CNs of 2 agents

3
4 5
6 7
8
9 10
Exponent of the Randomization Function: n

-

Figure 9: Performance of the adaptive Selection Rules for random load prole for communicating agents
the average of the eciency estimators of the agents in the corresponding neighborhood.
Such average is called the neighborhood eciency estimator. The neighborhood eciency
estimator has no physical storage: Its value is recalculated each time a member needs it.
In order to compare the behavior of communicating agents and non-communicating ones,
we assume that in a single population there might be, aside from the neighborhoods dened
above, also some neighborhoods that do not allow the sharing of eciency estimators among
its members. The members of these neighborhoods behave as described in the previous
sections, i.e., each agent relies only on its own history. The only thing that is common
among the members of such a neighborhood is that all its members use the same SR.
We call communicating neighborhood (CN), a neighborhood in which the eciency estimators are shared when a decision is taken and non-communicating neighborhood (NCN),
a neighborhood in which this is not done.
The rst set of experiments we ran, regards a population composed of only CNs, all
of the same size. In particular, we considered CNs of various sizes, starting from 50 CNs
of size 2, going to 5 CNs of size 20. The load prole exploited is the random load change
dened in Subsection 5.3, the value of w is taken to be 0:3, and n is taken to have various
values. The results obtained are shown in Figure 9.
492

Adaptive Load Balancing: A Study in Multi-Agent Learning

The results show that such communicating populations do not get good results. The
reason for this is that members of a CN tend to be very conservative, in the sense that they
mostly use the best resource. In fact, since they rely on an average of several agents, the
picture they have of the system tends to be much more static. In particular, the bigger is
the CN the more conservative its members tend to be. For example, consider the values of
(n; w) that give the best results for non-communicating agents, those values give quite bad
performance for CNs since they turn to be too conservative.
Using more adaptive values of (n; w), the behavior of a communicating population improves and reaches a performance that is just slightly worse than the performance of a
non-communicating population. Tuning the parameters using a ner grain, it is possible to
obtain a performance that is equal to the one obtained by a non-communicating population.
However, it seems clear that no obvious gain is achieved from this form of communication
capability. The intuitive explanation is that there are two opposite eects caused by the
communication. On the one hand, the agents get a fairer picture of the system which prevents them from using bad resources and therefore getting bad performance. On the other
hand, since all of the agents in a CN have a \better" picture of the system, they all tend
to use the best resources and thus they all compete for them. In fact, the agents behave
selshly and their selsh interest may not agree with the interest of the population as a
whole.
The interesting message that we get is that the fact that some agents may have a
\distorted" picture of the system (which is typical for non-communicating populations),
turns out to be an advantage for the population as a whole.
Sharing the data among agents leads to poorer performances also because in this case
the agents have common views of loads and target jobs toward the same (lightly loaded)
resources, which quickly become overloaded. In order to protably use the shared data,
we should allow for some form of reasoning about the fact that the data is shared. This
problem however is out of the scope of this paper (see e.g., Lesser, 1991).
In order to understand the behavior of the system when CNs and NCNs face each other,
we consider an NCN of 80 agents together with a set of CNs of equal size, for dierent values
of that size. The results of the corresponding experiments are shown in Figure 10. The
members of the CNs, being more inclined to use the best resources, behave as parasites in
the sense explained in Section 6. They exploit the adaptiveness of the rest of the population
to obtain good performance from the best resources. For this reason they get better results
than the rest of the population, as shown by the experimental results.
It it interesting to observe that when the NCN uses a very conservative selection rule,
the CNs obtain even better results. The intuitive explanation for this behavior is that
although all groups, i.e., both the communicating ones and the one with high value of n,
tend to be conservative, the communicating ones \win" because they are conservative in a
more \clever" way, that is making use of a better picture of the situation.
The conclusion we draw in this section is that the proposed form of communication
between agents may not provide useful means to improve the performance of a population
in our setting. However, we do not claim that communication between agents is completely
useless. Nevertheless, we have observed that it does not provide a straightforward signicant
improvement. Our results support the claim that the sole past history of an agent is a
493

Schaerf, Shoham, & Tennenholtz

80 agents
(.3,4) 1 NCN
(.3,4) 1 NCN
(.3,4) 1 NCN
(.3,4) 1 NCN
(.3,10) 1 NCN
(.3,10) 1 NCN
(.3,10) 1 NCN
(.3,10) 1 NCN

20 agents
(.3,4) 1 CN
(.3,4) 2 CNs
(.3,4) 5 CNs
(.3,4) 10 CNs
(.3,4) 1 CN
(.3,4) 2 CNs
(.3,4) 5 CNs
(.3,4) 10 CNs

T1

65.287
65.069
65.091
64.895
68.419
68.319
68.529
68.351

T2

63.054
63.307
62.809
63.840
60.018
59.512
60.674
61.711

Figure 10: Performance of CNs and NCNs together
reasonable information on which to base its decision, assuming we do not consider available
any kind of real-time information (e.g., current load of the resources).

8. Discussion
The previous sections were devoted to a report on our experimental study. We now synthesize our observations in view of our motivation, as discussed in Sections 1 and 2.
As we mentioned, our model is a general model where active autonomous agents have
to select among several resources in a dynamic fashion and based on local information.
The fact that the agents use only local information makes the possibility of ecient loadbalancing questionable. However, we showed that adaptive load balancing based on purely
local feedback is a feasible task. Hence, our results are complementary to the ones obtained
in the distributed computer systems literature. As Mirchandaney and Stankovic (1986) put
it: \: : : what is signicant about our work is that we have illustrated that is possible to design
a learning controller that is able to dynamically acquire relevant job scheduling information
by a process of trial and error, and use that information to provide good performance."
The study presented in our paper supplies a complementary contribution where we are able
to show that useful adaptive load balancing can be obtained using purely local information
and in the framework of a general organizational-theoretic model.
In our study we identied various parameters of the adaptive process and investigated
how they aect the eciency of adaptive load balancing. This part of our study supplies
useful guidelines for a systems designer who may force all the agents to work based on a
common selection rule. Our observations, although somewhat related to previous observations made in other contexts and models (Huberman & Hogg, 1988), enable to demonstrate
aspects of purely local adaptive behavior in a non-trivial model.
Our results about the disagreement between selsh interest of agents and the common
interest of the population is in sharp contrast to previous work on multi-agent learning
(Shoham & Tennenholtz, 1992, 1994) and to the dynamic programming perspective of
earlier work on distributed systems (Bertsekas & Tsitsiklis, 1989). Moreover, we explore
how the interaction between dierent agent types aects the system's eciency as well as
494

Adaptive Load Balancing: A Study in Multi-Agent Learning

the individual agent's eciency. The related results can be also interpreted as guidelines
for a designer who may have only partial control of a system.
The synthesis of the above observations teaches us about adaptive load balancing when
one adopts a reinforcement learning perspective where the agents rely only on their local
information and activity. An additional step we performed attempts to bridge some of the
gap between our local view and previous work on adaptive load balancing by communicating
agents, whose decisions may be controlled by learning automata or by other means. We
therefore rule out the possibility of communication about the current status of resources
and of joint decision-making, but enable a limited sharing of previous history. We show
that such limited communication may not help, and even deteriorate system eciency. This
leaves us with a major gap between previous work where communication among agents is the
basic tool for adaptive load balancing and our work. Much is left to be done in attempting
to bridge this gap. We see this as a major challenge for further research.

9. Related Work
In Section 2 we mentioned some related work in the eld of distributed computer systems
(Mirchandaney & Stankovic, 1986; Billard & Pasquale, 1993; Glockner & Pasquale, 1993;
Mirchandaney et al., 1989; Zhou, 1988; Eager et al., 1986). A typical example of such work
is the paper by Mirchandaney and Stankovic (1986). In this work learning automata are
used in order to decide on the action to be taken. However, the suggested algorithms heavily
rely on communication and information sharing among agents. This is in sharp contrast
to our work. In addition, there are dierences between the type of model we use and the
model presented in the above-mentioned work and in other work on distributed computer
systems.
Applications of learning algorithms to load balancing problems are given by Mehra
(1992), Mehra and Wah (1993). However, in that work as well, the agents (sites, in the
authors' terminology) have the ability to communicate and to exchange workload values,
even though such values are subject to uncertainty due to delays. In addition, dierently
from our work, the learning activity is done o-line. In particular, in the learning phase the
whole system is dedicated to the acquisition of workload indices. Such load indices are then
used in the running phase as threshold values for job migration between dierent sites.
In spite of the dierences, there are some similarities between our work and the abovementioned work. One important similarity is the use of learning procedures. This is in
dierence from the more classical work on parallel and distributed computation (Bertsekas
& Tsitsiklis, 1989) which applies numerical and iterative methods to the solution of problems
in network ow and parallel computing. Other similarities are related to our study of the
division of the society into groups. This somewhat resembles work on group formation
(Billard & Pasquale, 1993) in distributed computer systems. The information sharing we
allow in Section 7 is similar to the limited communication discussed by Tan (1993). In
the classication of load-balancing problems given by Ferrari (1985), our work falls into
the category of load-independent and non-preemptive pure load-balancing. The problems we
investigate can be also seen as sender-initiated problems, although in our case the sender
is the agent and not the (overloaded) resource.
495

Schaerf, Shoham, & Tennenholtz

One may wonder how our work diers from other work on adaptive load balancing
in Operations Research (OR) (e.g., queuing theory Bonomi, Doshi, Kaufmann, Lee, &
Kumar, 1990). Indeed, there are some commonalities. In both OR and our work, individual
decisions are made locally, based on information obtained dynamically during runtime. And
in both cases the systems constructed are suciently complex that the most interesting
results tend to be obtained experimentally. However, a careful look at the relevant OR
literature reveals an essential dierence between the perspective of OR on the topic and our
reinforcement-learning perspective: OR permits free communication within the system, and
thus there is no signicant element of uncertainty in that framework. In particular, the issue
of exploration versus exploitation, which lies at the heart of our approach, is completely
absent from work in OR.
Some work on adaptive load balancing and related topics has been carried out also by
the Articial Intelligence community (see e.g., Kosoresow, 1993; Gmytrasiewicz, Durfee, &
Wehe, 1991; Wellman, 1993). This work too, however, tends to be based on some form of
communication among the agents, whereas in our case the load balancing is obtained purely
from a learning activity.
This article is related to our previous work on co-learning (Shoham & Tennenholtz,
1992, 1994). The framework of co-learning is a framework for multi-agent learning, which
diers from other frameworks discussed in multi-agent reinforcement learning (Narendra &
Thathachar, 1989; Tan, 1993; Yanco & Stein, 1993; Sen, Sekaran, & Hale, 1994) due to
the fact that it considers the case of stochastic interactions among subsets of the agents,
where there is purely local feedback revealed to the agents based on these interactions. The
framework of co-learning is similar in some respects to a number of dynamic frameworks in
economics (Kandori, Mailath, & Rob, 1991), physics (Kinderman & Snell, 1980), computational ecologies (Huberman & Hogg, 1988), and biology (Altenberg & Feldman, 1987). Our
study of adaptive load balancing can be treated as a study in co-learning.
Relevant to our work is also the literature in the eld of Learning Automata (see Narendra & Thathachar, 1989). In fact, an agent in our setting can be seen as a learning automaton. Therefore, one may hope that theoretical results on interconnected automata and
N-player games (see e.g., El-Fattah, 1980; Abdel-Fattah, 1983; Narendra & Wheeler Jr.,
1983; Wheeler Jr. & Narendra, 1985) could be imported in our framework. Unfortunately,
due to the stochastic nature of job submissions (i.e., agent interactions) and the real-valued
(instead of binary) feedback, our problem does not t completely in to the theoretical
framework of learning automata. Hence, results concerning optimality, convergence or expediency of learning rules such as Linear Reward-Penalty or Linear Reward-Inaction, can
not be easily adapted into our setting. The fact that we use a stochastic model for the
interaction among agents, makes our work closely related to the above-mentioned work on
co-learning. Nevertheless, our work is largely inuenced by learning automata theory and
our resource-selection rules closely resemble reinforcement schemes for learning automata.
Last but not least, our work is related to work applying organization theory and management techniques to the eld of Distributed AI (Fox, 1981; Malone, 1987; Durfee, Lesser,
& Corkill, 1987). Our model is closely related to models of decision-making in management
and organization theory (e.g., Malone, 1987) and applies a reinforcement learning perspective to that context. This makes our work related to psychological models of decision-making
(Arthur, 1994).
496

Adaptive Load Balancing: A Study in Multi-Agent Learning

10. Summary

This work applies the idea of multi-agent reinforcement learning to the problem of load
balancing in a loosely-coupled multi-agent system, in which agents need to adapt to one another as well as to a changing environment. We have demonstrated that adaptive behavior
is useful for ecient load balancing in this context and identied a pair of parameters that
aect that eciency in a non-trivial fashion. Each parameter, holding the other parameter
to be xed, gives rise to a certain tradeo, and the two parameters interplay in a non-trivial
and illuminating way. We have also exposed illuminating results regarding heterogeneous
populations, such as how a group of parasitic less adaptive agents can gain from the exibility of other agents. In addition, we showed that naive use of communication may not
improve, and might even deteriorate, the system eciency.

Acknowledgments

We thank the anonymous reviewers and Steve Minton, whose stimulating comments helped
us in improving on an earlier version of this paper.

References

Abdel-Fattah, Y. M. (1983). Stochastic automata modeling of certain problems of collective
behavior. IEEE Transactions on Systems, Man, and Cybernetics, 13 (3), 236{241.
Altenberg, L., & Feldman, M. W. (1987). Selection, generalized transmission, and the
evolution of modier genes. I. The reduction principle. Genetics, 117, 559{572.
Arthur, W. (1994). Inductive reasoning, bounded rationality and the bar problem. Tech. rep.
94-03-014 (working paper), Santa Fe Institute. Appeared also in American Economic
Review 84.
Axelrod, R. (1984). The Evolution of Cooperation. New York: Basic Books.
Bertsekas, D., & Tsitsiklis, J. (1989). Parallel and Distributed Computation: Numerical
Methods. Prentice Hall.
Billard, E., & Pasquale, J. (1993). Eects of delayed communication in dynamic group
formation. IEEE Transactions on Systems, Man, and Cybernetics, 23 (5), 1265{1275.
Blackburn, J. M. (1936). Acquisition to skill: An analysis of learning curves. IHRB Report
No. 73.
Bond, A. H., & Gasser, L. (1988). Readings in Distributed Articial Intelligence. Ablex
Publishing Corporation.
Bonomi, F., Doshi, B., Kaufmann, J., Lee, T., & Kumar, A. (1990). A case study of
adaptive load balancing algorithm. Queuing Systems, 7, 23{49.
Durfee, E. H., Lesser, V. R., & Corkill, D. D. (1987). Coherent cooperation among communicating problem solvers. IEEE Transactions on Computers, 36, 1275{1291.
497

Schaerf, Shoham, & Tennenholtz

Eager, D., Lazowska, E., & Zahorjan, J. (1986). Adaptive load sharing in homogeneous
distributed systems. IEEE Transactions on Software Engineering, 12 (5), 662{675.
El-Fattah, Y. M. (1980). Stochastic automata modeling of certain problems of collective
behavior. IEEE Transactions on Systems, Man, and Cybernetics, 10 (6), 304{314.
Ferrari, D. (1985). A study of load indices for load balancing schemes. Tech. rep. Ucb/CSD
86/262, Computer Science Division (EECS), Univ. of California, Berkeley.
Ferrari, D., Serazzi, G., & Zeigner, A. (1983). Measurement and Tuning of Computer
Systems. Prentice Hall.
Fox, M. S. (1981). An organizational view of distributed systems. IEEE Transactions on
Systems, Man, and Cybernetics, 11, 70{80.
Glockner, A., & Pasquale, J. (1993). Coadaptive behavior in a simple distributed job
scheduling system. IEEE Transactions on Systems, Man, and Cybernetics, 23 (3),
902{907.
Gmytrasiewicz, P., Durfee, E., & Wehe, D. (1991). The utility of communication in coordinating intelligent agents. In Proc. of the 9th Nat. Conf. on Articial Intelligence
(AAAI-91), pp. 166{172.
Huberman, B. A., & Hogg, T. (1988). The behavior of computational ecologies. In Huberman, B. A. (Ed.), The Ecology of Computation. Elsevier Science.
Kaelbling, L. (1993). Learning in Embedded Systems. MIT Press.
Kandori, M., Mailath, G., & Rob, R. (1991). Learning, mutation and long equilibria in
games. Mimeo. University of Pennsylvania.
Kinderman, R., & Snell, S. L. (1980). Markov Random Fields and their Applications.
American Mathematical Society.
Kosoresow, A. P. (1993). A fast rst-cut protocol for agent coordination. In Proc. of the
11th Nat. Conf. on Articial Intelligence (AAAI-93), pp. 237{242.
Kraus, S., & Wilkenfeld, J. (1991). The function of time in cooperative negotiations. In
Proc. of the 9th Nat. Conf. on Articial Intelligence (AAAI-91), pp. 179{184.
Lesser, V. R. (1991). A retrospective view of FA/C distributed problem solving. IEEE
Transactions on Systems, Man, and Cybernetics, 21 (6), 1347{1362.
Malone, T. W. (1987). Modeling coordination in organizations and markets. Management
Science, 33 (10), 1317{1332.
Mehra, P. (1992). Automated Learning of Load-Balancing Strategies For A Distributed
Computer System. Ph.D. thesis, Department of Electrical and Computer Engineering,
University of Illinois at Urbana-Champaign.
498

Adaptive Load Balancing: A Study in Multi-Agent Learning

Mehra, P., & Wah, B. W. (1993). Population-based learning of load balancing policies for a
distributed computer system. In Proceedings of Computing in Aerospace 9 Conference,
AIAA, pp. 1120{1130.
Mirchandaney, R., & Stankovic, J. (1986). Using stochastic learning automata for job
scheduling in distributed processing systems. Journal of Parallel and Distributed
Computing, 3, 527{552.
Mirchandaney, R., Towsley, D., & Stankovic, J. (1989). Analysis of the eects of delays on
load sharing. IEEE Transactions on Computers, 38 (11), 1513{1525.
Narendra, K., & Thathachar, M. A. L. (1989). Learning Automata: An Introduction.
Prentice Hall.
Narendra, K., & Wheeler Jr., R. M. (1983). An N-player sequential stochastic game with
identical payos. IEEE Transactions on Systems, Man, and Cybernetics, 13 (6), 1154{
1158.
Pulidas, S., Towsley, D., & Stankovic, J. (1988). Imbedding gradient estimators in load balancing algorithms. In Proceedings of the 8th International Conference on Distributed
Computer Systems, IEEE, pp. 482{489.
Sen, S., Sekaran, M., & Hale, J. (1994). Learning to coordinate without sharing information.
In Proc. of the 12th Nat. Conf. on Articial Intelligence (AAAI-94).
Shoham, Y., & Tennenholtz, M. (1992). Emergent conventions in multi-agent systems: initial experimental results and observations. In Proc. of the 3rd Int. Conf. on Principles
of Knowledge Representation and Reasoning (KR-92), pp. 225{231.
Shoham, Y., & Tennenholtz, M. (1994). Co-learning and the evolution of social activity.
Tech. rep. STAN-CS-TR-94-1511, Dept. of Computer Science, Stanford University.
Sutton, R. (1992). Special issue on reinforcement learning. Machine Learning, 8 (3{4).
Tan, M. (1993). Multi-agent reinforcement learning: Independent vs. cooperative agents.
In Proceedings of the 10th International Conference on Machine Learning.
Thronkide, E. L. (1898). Animal intelligence: An experimental study of the associative
processes in animals. Psychological Monographs, 2.
Watkins, C. (1989). Learning With Delayed Rewards. Ph.D. thesis, Cambridge University.
Wellman, M. P. (1993). A market-oriented programming environment and its application to
distributed multicommodity ow problems. Journal of Articial Intelligence Research,
1, 1{23.
Wheeler Jr., R. M., & Narendra, K. (1985). Learning models for decentralized decision
making. Automatica, 21 (4), 479{484.
499

Schaerf, Shoham, & Tennenholtz

Yanco, H., & Stein, L. (1993). An adaptive communication protocol for cooperating mobile robots. In From Animals to Animats: Proceedings of the Second International
Conference on the Simulation of Adaptive Behavior, pp. 478{485.
Zhou, S. (1988). A trace-driven simulation study of dynamic load balancing. IEEE Transactions on Software Engineering, 14 (9), 1327{1341.
Zlotkin, G., & Rosenschein, J. S. (1993). A domain theory for task oriented negotiation. In
Proc. of the 13th Int. Joint Conf. on Articial Intelligence (IJCAI-93), pp. 416{422.

500

	
		
	

	!"!#%$'&)(*+*-,+.0/-(*21-/+34

A

BDCFEHGJILKNMPOQKSRUTWVXTYKSRUTYK[Z\A^]+_[CJ`9IZ9aUE

kmlnPomnqpsrmtQuwv

56789":;3-<2*+=>?7
#	@:;(+<*-,

b	CJ`dce]2GfKgAdRUGYVhZiGWZ9I2CjK

xzyz{z|~}ย[ยz}ยย ยยy}6xย){ยยยz{9ย ยยย

ยยrmtiยnยยYk;ยยยยnzยย

ยFยยยยi[ยz}ยย ยยy}6xzย){ยยยz{9ย ยยย

ยยยย0ยยยยกFย+ยขยยยคยฃยฅFยฆ9ยฃยกยยยจยงยย@ย+ยยคยฉzยช2ยซย2ยขยจยชยFยยขยจยฌFยญยยขยยฎยซ)ยขยจยย2ย6ยซ)ยขยยฎ
ยฏยข0ยซ)ยฐย+ยยฑ+ยซยฒยยณFยฃยฅUยดhยLยฑยตยถยซ)ยขยยฎLย!ยฃยข
ยฉzยย-ยยยทยฒยยธยนยดยคยบยผยปยฝยยพยยปยฟ

รรรยรยรรรmรยจร
ร;รรยรPรยรรรยรยรยฒร	รFรmรรร!รรรร0รร"รยร@รNรร!รยฒร	รPรยรรร)รรร0รรร0รPรยร+รร!รรรยจรร@รLรNร0รรยรรรยฒรรยจร	รรPรJร0รรยรNรยร+รร0ร	ร	รยฒรร
ร รรรwรรรยรยร	รยฒร	ร[รรรร@รรร+ร!รFรกยรรรข;รรฃรยร[ยยจยทยฒยยขรย	ยฌยยรย0ย@ยLยยซยฃยขรฅรครฆรFรรยรรงร"รจยรยฒรรรฅร	รยร!ร+รPรรยฒร!รรร0รรรยรXร	ร)รรฉรร)รรXร@ร
ย
รยฒรJร@ร	รยฒรยร+รfรรฉรร+รขรชรร@รยรรยฒร+รUรซยรฌรรญร@ร0รรรJรPรยรPร+รFรข9รNร0ร@รรยร)รยรSรยรยร	รNรยรยฒรยรรยฒรPรยร+รPร6รPรยร+รยรFรรยฒร	รยร@รยฒร@รรรฎร"ร	ร
รร)รยรรฏร	รรยรรรรยร@รยฒร	รรPรยร+รFรยรร@ร!รรรยร@รJร@รPรยร%รยฒรยครรรยคร@รยรรPรzรร2ร	รFรรยฒร+ร!ร	รzรรPรhร!รจยร@ร@ร6รNรร@ร)รรรรPรยรร	รFรยจรร@รYรรร
ร!รFร	ร@ร0ร+รยร	รรยรรรรยร@รยฒร	รhรรยฒร	รยร@รยฒร@ร0รFรยรยฒรNร!รรยรยฒรยฒร@ร+รรร!รร@ร	รซ
รฐ ร0รfรยรรร@รยร	ร!รรญรยฒรJรPรร!ร-รรฑร	รรรUรยรยฒร+รขรฒรยรยรร)รรรรยฒรรยรรFร@รLรรรร!รรยฒรรXรรฉร	รรรยรรรญรยร%รPรยร@ร!รรยรยร0รรยรร-รซ
รณ ร6รร+รรรยร@รยฒร	รJรร)รร0รรยฒรรNร!รรรร!ร@รยรรWร!รร[รยรรรยรรรด รWร@รยร	รWรรPรรฉรFรร	ร+ร'ร"ร@รยรยรรรยรFร@รรฅรรรยรJรร@รยฒรรhรร)รรยร
!ร รรตPรร+รFร6รยถรยคร	รยจร+รรรยร@รยร@ร-รซwรฌรhรร)รรรรยฒรรYรยถรจNรยรรร0รรรร!รยฒร	รรรYรยฒรรรรรร!รจFรร)รรยรครถรรNรยร@รรยฒร!รรรร!รจNรรรยรWรยฒรNร!รร
รร)รรXรยรรรยรรยรครทรยฒร%ร!รรNร!รรรยร@ร@รยฒรรรฉรPรยรรรยรWรร	รยร!รรNร!ร-รรร@รรยรรยจรยร@รรรฅรร)รรยร@ร	รรยรรรรยร@รยฒรรXรรยฒร	รยร@รยฒร@รรdร+รร
รร0รรยฒรจSรยจร	ร!รรฏร!รรFรรรยรFร[ร!รรตPรร6รFร+รยรยร	รยจร+รรรยร@รยร@ร[รLร	รยรรร)รยรรรยร!รUรNรยร+รร+รรร!รรรยร[ร0รรยรรร+รยรยรPรรฏร6รรรญรรยฒร!ร
ร@ร6ร@รรรยรร;รร	รร!ร@รรรยฒรยร@ร;รรPรJร!ร@ร+รรwร"ร!ร	รรธร!รรยร0รรยรรซ รฐ รร;รรยฒร	รยร@รยฒร@ร0รUรด รiรร	รFร0รรร6ร@ร+รร6ร@ร9ร+รร@ร0ร@รLรยร!รPรรiร!รร
ร	ร0รรรรร@รยฒรยรSรรยฒรยร	ร@รยฒร!รรรฒรข;รยฒรยฒรPร+รยร+รยร@รPรยรยฒรรรจรฅร@ร-รยรรร!รUร!รรยร+รยร@รยฒร!รยร	รรรรUรรยจรNรยฒร!ร;ร@รจยร@ร!ร+รNรร!รร2รรรรจรฅร+รร!รร@ร+ร
ร@รยจรร;รยฒรยรข;รยฒรยฒรรยรFร!ร[รข;รยฒร@ร0ร	รร;ร!ร-รยรรPร0รรยร@รยฒรจNร!ร-รรร!รรยฒรรNรรยรจNรยจรร@ร!ร;รรiร@ร0ร%รยรรรยรรรซ
รนiรบhรปยถรผ รรรฝ;รพยรฟยครยจร@รฝ รผ

 	
			
		
 !""#$&%'()
*+,+#"#
+$
-	+.
-/0%'12

.34$65879
-.$:"#

-
$;	$<=<=	/>+?#@().;A>CBD	+E;-$+%
F	/>F+?+	
-%'<D	,	-/G2H+
	C%'/>"#5<I$"#4:"#/G		.
3
"#/G#@
J+	/GD#K=	+EL=+K$!
	M%'/ON.K	!
"P
$Q$R
-$
/G;#@		
S	

=	T&
-U+%*=		/V5FI(!/>KP=+KM+,+-	
K"+W
SXY3
C=
	-/
DL+-
	ZZ!/>3XW	/V[?=G+%J	+	
	Z\	P
>	
]\=	\"#	G.B45
^H

->P#XKF=_/?+DW	E`	P/G2H
-	a 
+
-SF"PD#-G$/,M+?&

:
=	#%'b=	Pb/>3>)Y++D#+MG&
	W""#$$c>!+M	Ed	P/>5
7e;#@e+/W:"2'$f		.;D
'"3XG""#/WX
=f=	#
-.Ba
f=	b	=!$[
gih?jMkJh?ljmbnfo [9p
q?T%1

-
38"#

c+f3:	P
M%'/r=	sX
	.+P
W&
/,
X'+;+tu	C=+=cE;B$W
-V"P
."#	/>#+"#$P=+$/,bP=	b
			.65v;=	
P
+3*	=J/?$?3,
-D+/GJ	!Nw"P
3K/G(
xNw"+
?+%K=	bx
	.+W+*:%'
#@+/G-:++/,
	J"#.+.+,/>+B
	b=	FX
-	.+b*yz9


'3eJD3w"#
!

/?+."P=>=	s
			T)$"P
XNw"+
-5
gin ย nf{Jk;nbkCl3|a} [(~	
X%'f=	JP
$A+?t 5ย5X:eDV	
-	ย+A/G+
	>6:
>"P=+	+
	?f.#:;D?/G	%
	,S+
'+M

	DFtย	
X*=	bP$
	,
"P=
$F=	s
			3 5
 (*+*-,wz@@!"	ย2	:ร!ย	:<ยW+<ยmรง8
ยF

2_ย07
)#Hยยz

ย#"0!" ย6:+ย

ย

yz{z|~}aยยย

ยย	ย

gยยยjM}VjMh>nfoFl3ยQnCkJl3|G} [pM	.3X
U;=	c	E;Y"#P$+$,>+,P<
-<<M	E0"

>P=	X
-	.+?ยH	+
$a
F
;ยย"P
G
Xย*F%/ย+;"#	PD-,
?=	MX
	.ย#5
vย=
C+W%	"#$WL=	\	+	.+!
]	("#$PC
L=		.3"#D#@	,+%JZ"2'!$
+	
-	q	/Vย=	+E;$:4	,	+	.
Z3
=	/ย"#ย?#%F%J.&%/>+&
S
+?V		+
	f	/>;;E;#X5
ย ยย
ย ยOย

l	ยozrzl	ย

ย
t

ยZPB?
q"!2'$+	
	?=M=
!
"SXX>)ย"#"#$A
\+!
"#+T	X
"+
-V2
/>3
-:T+ย=?P$0]%'("#?0	P$D#+
ย.+=	fP=+ย3!
=	/,
"?
'	$5ยกv;=	
$$."=f	$P$;	-/>QX
BME=+F%'$+	$F+%8sX
-	.+,+f/?+BMD	a

"#$F%'_!		2
$R	D!
S3:$=	+Eยข%$+P	$4+%=	QX
	.T'+Y"+,	$!1#ย*$"#
<	+	.+!
J#++
$6:
+fW*5
ยฃ 	E;Bb#M/>3
	2-
-D)D<S
- =	/ยค%9+Y	+	.+
-*:$+s
4P=	#%
"#/G/WD.w[Q
M	+
'$T?"#/G/GV+!%P/ยฅE;
=fEc=
"=\	,"+Z+3UYV"#/G
=	\S+!
?	$!D.
0"=	/W$f+0	+	.+!
`!.+
$:WE;#Xs?#@(
0=	
+#"#=	C
S1	#N.M%<=	Y"!2'$\+		D"=G+	
-	5bI$"#
-bยฆG+ยง	5zยจ

"#MP=	aยฉ9ยช*ยซ	ยฌ\ย ยญM+/G/W4:1ยฎ3ยฏยงยฏย+ยขยฐ	ยฑยฒXยณKยฑLยยดb+/,	=+/G+
<ยตยกยญ-$:<ยฎ$ยฏยฏยจย_	/?
&
	?	c%./GE;B4:(ZI$"#!
qยถe5XยฎJ"P=+."#!
U$;	$"P
!#f=	J)
3	#N.M+%<'+
	+	.
?.ย+f	.
*5
v;=
'+)b	$.M+Z3!
=	/V:wยทยฐยณuยHP=	Vยธ(!/>+
"s+q	+	.ยน(ยK%Tย	+	2
.+
-ยP=+C
G	4:;"#/G-:_u	/>
"+5LI		$G/W$+,=+W=	f			G'+

M+#+DP$\V+!
&%'V=	,DS :*"#/GP	$b/G$b=+bP=	G+		CE;
XX_3-Eย6(cNยข
+	!
ZZ
X%	?#@
.WยH+.$s+%=	?X
	#+V'+ย	PS
$ยขDยข=	>P
6ย.:K+
	/>+!
"P
HW/G$+F=+P=	3-
P=	/ย#@	$;P=	J"#b+%8	+	.+
-F		2'$		+
ย'
V=	$:
;E;
XX*	b"#&
'T+V	+	#+
?/GPT=f"#+ย.5
I	/>
"P
&\
,=	
"PB
$,	)ย\.+:_]%'GE;qP$5ย79
.$:Q=	
	+	F#+$
-ยC"#+%4
"#/GM+5 (Gยบ "P=W
"#/GP+>"G#@(W
DPJ+
#@()	
3;D	/,C+%T"#/G!
ยK	/>
"P
&V$R
$s=+JP=	>	+	.
Z3
=	/
	"#&
;E;b
"#/G'+F=+=+cf	b"#/G
-*:E=	P$;"#/G	$P
$R
$M=+b\
SQ"#/W
\)ย"# 
$45fI$"#4:*+]	+	.+!
VP$R
P$
G"#/,
+!
?+%1#"#
	,	
;'+	
	>$"P
 
cย "P=	+
"#J+?#
	G+%+V6:


	?+%KS+
'+$;E;
-=
f=	,"#
VP"=	/>#ย.:4;E;#X8ย/>+B
	G	Eย$"P
&
65MI(P/G2
+
"P
- ,$R
$;P=+TG$"P
 
*:"#JP."#$4:	MJ"#&
$+D3
*5
ยฃ 	;%'.+/GE;PB,%+	
	WDV		.+
+?
F!$f E;,	/,
'$;=3
-	YP>>Eย
=
=	b+	b%QP$?+c+?=	+Eย=	f+b/>+
-	+$4[
X
	#+J'+G;";
FP$G<C"#/GPT+?"#&
PDK,%_+-
	bP=	T	
-
	/V5fv;=
s+ย"#.3
-M=	ย!J+ย.
	b=+,+$VP=	>	
-b	/
3-	fE;
=L	
!
3_"#!.3
.b+]3"#Z
-%P/>+
qP=+J$"#.ZEc=DZP=	
PYZ#
	DC+	$+,=	P5 ยป 	
	ZV"fq	Eยก	-/ยN#b
+-$
3ยผ
-	ย=	AX
	.L+ยย/?+."P=u=	

!
3T+ยD3b"#

G+%JP=	ย"#	PD
	/V:4?	("#$PM=+M	("#$C>"#&
M		M
"#/GPY'+*5bv;=	G	+	.+!

		"#$/G	.F?"#/GbP=
F+*5

gยยป

ยฝ.ยพยฟรWรรร$รร#ร;รรรรร'รbร6รรรPรGรรรGร+รbรSรPร'ร-รรร!รzรรรYร#ร'ร$รร'รรรรรรGรร.รรHร!รรร?รSรPร'ร-รรร!รzรรร,ร ร#ร$ร'ร'รHร!รรร3รรfร.รร-รXรร6รรรรร*รร3รfรยร!ร
ร'รรร6รรร'ร_รPร$ร6รรร-รรร.ร+รร	ร'ร'รร$ร*ร#ร*รร.ร3ร'ร'รHรรรรร'ร*รXร#ร4รรร4ร'รรPรHรรรรร.รKร'ร3ร<ร#รรร!รยพ
ร3ร$ร

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

ร

ยzy	รฃยยyยยzย"ยz{

v =		+	.+
-G		"#$;"#&
!.K+%KY!.+	+.W;+%1+	2'P#N	/GD;).+.;ยHP=	D
g ;
=+_	f;+?"#.S
D#K,=	M+4ยKF=	J+
xX
H,,P."#<P=	b#N	/G.
/>bE=	=	MX
	.G+fEF!
+
3xรW	.+$45
ย J
EdP=	Y	.3K+	
	f	/รงT?$+."P=\=		=q?
$"#$.	=V+%;+
S
ยข
+65ยv;=	f#+	=*yzC,	$.,=	fX+u\
#b-$$$JP	$,N
=	$รจ+5
pT	#+
?+	
-	ย!.+.C+,=	?DY%T=	f.	=i+]$+#"=	$C%'G	(\ย&+4ยb=+
+!
&N$M=	GD3 5W^HJ	.+$M=	?.	=qZ""#$&
-#\#N
	ยขย "##3

		ย;=	G'+*5
v;=	P
+3	=%รฉ+>	+	.+
-	2'$s+		$:G=	=	K=+4:P	<>+
P.+
		,
Z=	,#+	=*:1+=	?	+	b)+
J!$+."P=
	?%P/รชP=+b+
-D$5,^&b/,b>G
$+."P=V+EVP=	b.+	=AX
Bb,	.
b		T		TSG/,)Y+-b>$+#"="BE+.
=		=>=	b.+	=?DGP."#
-	Y"#P.3
.:	("P
-	G/Gb+P."#<'+5 ยฃ 	T"#/G-
+u!(P/>+
"?	+	#+
uS
X=	/รซ
W+VPย$+."P=LL		f
-u=	.+	=LE;
-=		
"#&
!
	>+>		b/GPb=+f"#5
ยZF=3K
/W/G$b	K3
X=	/ $ 
Wรฌ</G/WJรญ)
,,รฎTรฏc^HรฐยE;B	.+&
9+b$!$

bย!.3<	/ย/>3
-5 ยบ @	
-/GD#3_
$b"#/G+G	J3!
=	/rVV&
-/,
X+
#ย*$:;ยฐ	ยฑยฒxยณKยฑ0ย ยดb+/,	=+/G
Tยตรฑยญ3:bยฎ$ยฏยฏยจย#5 ยฃ 	G$!.,=	+EรชZ	/?+
"G!$	
%P/ยก+fs%T"#.3
-ย&
-/GJ+?+	/ยฅ"P$65
ยฃ 	E;PBGVยท+ยฐยณ]/>+B$F=	s%X$Eย
	Y"#
-		
[
ยฃ 	
 K3
X=	/ย"+		$9=	F$"#<%(=	F+	2H		.+
T		"#$9E;
=
-J+b#@(P/G#
 
/G%'.+/GE;PBw5 ยป *"P=b
รฉ
*+/GK;%'/>S3	&
:$b	+
$*F%'.+/GE;PB
Eย
=VEc=
"=ยข>+3+PG=	J/?3
	2-
)D,3!
=	/>;x
B,=	?ยฐ	ยฑยฒXยณKยฑย!(P/
ยยดb+/,	=+/G+
Kยตรฒยญc$:Kยฎ3ยฏยฏยจย.:)+ff+3-DU,/>3
	2H)	b	$.+!

f	+	.+!
f.+P+
$5
g

g ยยขTM=	c%#+/GE;B,s
D$
D Mยฉ*ยช9ยซ	ยฌ(yzF#+&%'/>+!
ร3w+		D"=?J'+ย	+	2
#+
*:++G!=	SEL=	+E0ยฉ*ยช9ยซ	ยฌ(yzQ3
<!.+
$1"#,M	$GPJยท+ยฐยณqK!$+."P=	2H"#DP+
=		!

"65
g ยยขY+3-DU,=	C.+ย]E;q'+V	#+
\+V	+	.+
-*:w"P=+."#
U#
	W=	
 
/,
XH
-HG$R
$??=	Jf
+3*	
-	b,	("#,$	*5
g ยยขYPM\/G
!
"3K#@()
/G.M+\/G#+C%?&
/G-a"P'c+%<	/?
f!(P/>+
"s#+!
=
\) E;ย"#/G		.+
-V
-/GG+\=	> 
/,
X
X f)E_ยข=	

-				/r+?=	MX
	#+Gf=	Y		.+
>3
=	/ยก
;E;
P=*5
v =	,+)b		"#$	Jc%'+X+ET6[;E_CN.!T
Eย	
ME;PB?
\+	
	fq	+	!
	
;
3
-
	G	
c+	
-5<รฏc#@(ME;b
Eย=	s$2H"#/G/,
/WD;	.
M+	
	
3!
=	/ยZEc=
"=]ยท+ยฐยณย
b!$4:*
-i+
-	qA
DP("P
-	V/>+\+%Tยท+ยฐยณ<yzC	+."#	P$5
I$"#
?รณ,=	f#@('3
F=	b.3
XQ+%1			.+
>3
=	/V59^H\I$"#
-ยรดCE;M	+TP=+
ยท+ยฐยณ\
!	4:"#/G-J+f	/>
"+5
I+
"#f=	V!$L%	+	.
ย)W]=	VRSX
Z+%TP=	f+L!
$ย%/รตP=	
X
	.+P:
-ย"Vs%รถ;G)%/ยฅ	.
M+	
	W=+V+P/G	G	+	M+?
+		2
	
+sX
	.>'+*ย	
-ZI$"#
qยถWE_,+3UJP=
P.+ยย+\3!>
"#M/Wb
$!
	

DP."#
<)E_\=	,3
X=	/>Q%		.+
ย+A+AN
	5vย=	f
ยขI$"#
ยขยฆJE;
รทยพwรธ6รร$รbรรรรนรDร'รFรบรป$รผ$รฝรพPรฟ#รพ	รพ
รผ	#รป<รXร#ร)รรรรXร#ร'รรร-รรร.รMร.รJรPรรรรรร-รรร$รFรXร'รรKร'ร.ร3ร'รร<รรร$รKร#รXรยพ
ร3ร

ย

yz{z|~}aยยย

ยย	ย

=	+EO=	SE .+&%'/?+
+31'+		.M"=\MยญT+/W/G4yzsย.ยฎ$ยฏยฏย<ยฉ*ยช9ยซ	ยฌย	/รฑ"+V)G++2
U$G 
	J	Q%#+/GE;B45KI$"#
-ยยงs.K	</G
!
"3w
-$5 ยป %F
E;
	b#'+$
E;B?ยI$"#
-ยยฏ(ย.:)I$"#
\ยฎ,
"#!$<		P$<+?)D$;R	$
Q%'F%	P	TP$$+."P=*5
ร "#<@รฝwรฟยร%$
ยรบ! 

รฝiร&รฒรฝ รผ

รรรพยร(';ร)#*"

,+

ร รผยรผ

รผ.-

v;=	M
$C+%+	
	,>	+	.
G=<)f
?=	cX
#+	;%';/>+G$.:	+a
>/?+D

Xย*F%P/>5^Hf=
'_!$"#
fE;M
-Eย=
FE;BG	!
 /:
	,,/G
++b+?		;

#$"#
-J	M"#	E;BGVยทยฐยณ<5
vย=	b&
"c
$a=
+	
-	GDf	+	#+
ยHM&
/,
X'$;E;BW
V"!2'$f'+	
	:
.&%/>+
XS+	
	:;+	
	CD?+	
-G3)ย4
FY!+Ts	Eย	-/ยfย.ยฎ+ย
!

-	,%P/ยฅ/G/GยW	-/รช==f)Z+$?	P
&:	=	]ย ยจ(ย_	+	
-	G=	
+f!+	
>,=	C	Eย	-/V5
vย=	Vยฉ*ยช*ยซ	ยฌZ	/ ยยญT+/W/G4:ยฎ$ยฏยฏ ยM
,Z"2'$L		G=+G+-$C	/>Y

=	V/>S
ย%YIU$"P=DE+u"#B
	5ยขย`=	L
uD3\		"#q\
!=LE;
=]+
'"#+
	)
$:ยฉ9ยช*ยซ	ยฌbN.F
$Q1
 024365	7598:036;;+Y	P/>Q_"#
 /	
'"#.K=</,
=D<+!
;%'/ร=	
	EยกD3 :+]$C=+,+3	&
s=
 <;>36<5	;?>;C%/ย/G/GZq"
	+f+	!
]ยH#x
	
+4ย.5Kv;=	b#X
	M+W
F=	?/>+
	$G>%
 @BAC5 DE5	2GFbS
-=	/ย=+;
-$<PY+!
&%'
+ย	ErD3'G+LP3
G	/>,=>
L	ย+!
?
L=	#X
-	q"#+!
5ย^H?=	
#@($"#	P$=	b'+*:	+W
X%K#@($"#	
-f$#K
-?%รถ3
x	b1
 <;H8:05	<_S
-=	/รฒ+SDU$ย=	M%รถS
X	
+W$F=	TP$;+%*=++3-(&
'C
/G	+P=	
#@W%F=
;+-	
>!J=+Q
;E;
XX	
C
$W
V&

+FE=	b
-_Eย
XX4%รถS
X1+D3
*5
ยฉ9ยช*ยซ	ยฌZ	$!$G\E;
f#+	?+%	P/>s
/G).D,Z"2'$L	
	[?=	+Eยฅ
+
"P
+PV	-/>:=	+ErL!
q]+-	
รจ%/ =	]"X
	.:;=	+Eรชu	+	>
/G	
X%f++f+-	
*:f=	+EยG!J#@	$"#	
A%รถ3
x	b,
-/G	+J	$R	TP
SS5
ยฃ 	Jยท+ยฐยณu	/r	
/>
XV	P$$M=	,	+	.+!
V	P/V:4+
ยขI$"#
]ยฆ,E;Y!Y	
%#+/GE;BGPa3UJยฉ*ยช9ยซ	ยฌ(yzย/G(
xNw"+
V!.+
$
V!/GJ.3
x 5
v;=	c
ยฐ Iยซ JLKwยทc(!/ยย ยป P/>+*:ยฎ$ยฏยงยง(ยw"#%'.*P=	<	-/d+%Kยธ	+	
<+	
-	:zยนc		
3,	$!$<P=	T	/ย+.
% <MG2ONP365	@B;T	+	#+
,C+W%รถS
X	5Q
ยฐ Iยซ JLKwยทC+		D"P=	$K'+
	+	.
ZEย
=ย\"#/,
+!
\+%T."#
"3_"#ย+]&

Z/?+."P=
	5fย0=	LV'+
%รถS
X	c
$"#P$?
F
"P'&
XN$f;#
	W#
=	TC%H3
Xx
	,	$"#

-*:	C%รถS
XX
	,	."#/G:	
"%9
xยรฉ!
	J3:+;JPG	<%รฉ.$5^H	
	JP=	T)$"#.F+%K
ยฐ Iยซ JLKwยทM=<$3*E;
=

"#/G-J+W
"#P$"#;BD	+E;$:DP=	b	./VyzF/>3
?3
-ย!.+,
+$F'"P
	
,%H3
X-$>'+qPVE;
P=V	C=+/,
-=DM"=
-J=	,+/GC		5b
ยฐ Iยซ JQKwยท,$M>!/>+
"
	 E;PBG,	$T+!."#
-ย"P'$;%8"#
<P=+T"P=
M=	J+/Wb		D!GยHE3B
X	
+f

	>+Pb=?
-.+"#$;%8#+.+
Xf"#
-:%_#@+/G-+ย.5
v;=	S
 RTK(
ยฑ Uw)ยฒ Kwยท,	/รฑยPI+
/G/G6:*ยฎ$ยฏยงยง(ย1
'V
 36<02OWXDA<@B0365XA240Y8(YZ02424;><.5Fย0=
Xb=	J
xยรฉP2
"#GE;Z?.+&%'/?+
X31'+		J+Z?"2'!$\+		b=M	bZ	$"P
'#
#N	$4:8?/?+ยผ
xยรฉP"#>"#"#s=	SEยP=	YE;? $s+%_'+		.Mb=	?.+!
	G
D
%F+f	+	.
*5Qรฌ_$2'!$>	/?K;=
Q+?
'b!
S34+%1C_!+	
,%P/
MX
	#+:+		[
 R*TQ(
ยฑ Uw)ยฒ KwยทT"#/Y
-	$</>SX+W%#+/G.9%'<
Xย*ยH=	)#%xC
ย
$"##+%<P=	Y"#		/V[
5 RTK(
ยฑ Uw)ยฒ Kwยท,
Xย*.ย%/รชยฉ9ยช*ยซ	ยฌ,
VE;,=	ME$	[N#T%Q3x :
R*TQ(
ยฑ Uw6ยฒ K4ยทGD$c	M%'/ย=
 02\365X7>5]80365XA2ย3	&
;\=	C+*:)
	?G
'D
x%?	-
#J)#%?X
	.\
+3 5\^&$ยข
G""#	#JP=	?%รถ"#,=J=	fP
$ยข+]E;
XX;
/w$E;$4:*+\"#	.M
#S
b=		
!
"M?+."P=V
-$5Jยฉ9ยช*ยซ	ยฌ	:4\=	G=	M=+4:	/G$
ร3ร$ร

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

ร

ยzy	รฃยยyยยzย"ยz{

=+C=	?
$X
	.VยE;
xX<f\"PDG		=ZN,=	G	Eยก	-/ !V=+sX
2
,C	q		.+
VE;
Xx<G	$"#$P+5I$"#4:/,"=Z%_=	1
 RTQ
ยฑ U46ยฒ KwยทWE;BA
J$\
#
	>Gc+%13
-#+.Q%'TR+
XN$f+?/GP
"MS
+$65
vย=	G/>3
ย
$f)=
]=	>ยทยฐรยณ0	/รต!+.+$
b%'/รฑP=	>=	P>	/>s/GD!
	$
+)S[G=+,=	?		"#$,+%M+^
 0C08360365XA2ย
,A%รถ3
-Z&
/G->#@	&
ย\=	f	P("#$C+%
+%
 F;2\;<0365	A25 ยป ;C"#$R	"#TE;T"+?	/GM=+KP=	T3
=	/ย=K	.+$8X
	.
+ t`+f=	,"#P	J+%K=	D!J+ tL
P=	J+/G,;=	,	+	.+!
V3
=	/รช+A=	
+V!"#	$ย
;	.$5F^HVP=	Yยท+ยฐยณZ
EJ:e+f	.
?
8ยผ MG$"P
'38"C+%K+
	+	#+
ยH	M
-fE=
"P=f=	M
;	W
$f"#	Pb,.@	+
-ย.5
v;E;G
-$"#$+%KE;Bf#)$\+=	,+/GC
/GJMยท+ยฐยณL	T 
/,
Xย	/G	
-[F=	
ยฐ	ยฑยฒXยณKยฑu	/ ย ยดb+/,	=+/G
ยยตรฑยญc$:ยฎ3ยฏยฏยจยT+]P=	V
 _STQ`8)ยฒ aยcยฒ bu	/ eย dK#D!:<ยฎ$ยฏยฏยจ	:
ยฎ$ยฏยฏ+รณeย.5
v;=	b/?3
f
Xย*"#J) E;Vยทยฐรยณ]+\ยฐ	ยฑยฒxยณKยฑ?
;=	b	-
	W+	
	>S
-=	/V[1ยทยฐยณ
$GZ"#.3
-D2')D
X	\$"P=	
R	V&
/,
X'+J]รฌ<=	/>+*yzWย.ยฎ$ยฏยงยฆf
ย b(gfยซQ
ยณ hยคa/G(
xN$]D
~\" ยป X-$+V
 iยDX
Mย.ยฎ$ยฏยฏ	ยฎ+ย.:E=	$Mยฐ	ยฑยฒxยณKยฑV$c,S+!
+_
% jTQjkIS)ยฒ jZย v+:4ยฎ$ยฏ(ยฆยฆย.:
=
.+#"=
"Sรฉ'+		$5;I$"#
-VยงG"#/G$;=	$bE;,+		._
V/GJ#3
X 5
v;=	l
 _STL`K6ยฒ acยฒ bq	/ยฅ3!Y#+B$;,$+."P=	2'
$?+		D"=?,+	
	)51^&T
Xย*.Q%'/
ยท+ยฐยณ 
L=	VP+V]"V'$	C
LP=	V	-/G2H+
	Z	P("#$65 ยป X
-	.+]+0ย "+ยJ
-u
.+ %P/>+
X3f"2'$2'+	
	ยข%'.+/GE;BL$>ยข+	
-LLZ	
-G	-/
3	\E;
-=ยZ	/W/>+\+%bE=+C	Eยฅ	/?b
-YE;'Z)Vq!
.+f+-	
\%'$:Q		,

"#.3
WX
-X
%/>+
X0+	V=	m
 8(<PA>7;nWWGP=+f	.+P$uP=	ย+	!
*p
5 oM
S
3
+3:fP=	b=	;=+4:P$;	.D
'31$"#
	
-F+%1=	!
 C;75cW5XA2OWK=+;$P$W

=	Y!+	
*5;^H\+
'"#+$
: dK#-DyzM	/r$"#.	c/GPb
%'/>
XV+M$"=\"=	
"#J)+

=+ย$Jยทยฐยณ<[YยX
M+%;%H3
X$\3P+
$:)%b#@e+/W5fv;=	G#
,#ย*$"#
	$PM+%=	
E_?+		D"P=	$b/>MP>=
	YV=	,#@	DbP>E=
'"=+f'+	
	V$"P
&
-bย	D$
G=	b'+;=	/>!#$#ย<"+b	.PD	V+A#@(-+
$a
V&
/a
X+;+	
-	G
	$5
^&>	/G/>:+E;ย"#&
<	KE_Bb>ยทยฐรยณfPbT"#/W/G.+CT/WDK#@

	bE;PBM

.+ %P/>+
X3	;"2'$W+	
-	5Kv;=	;+PK=<"#"#D#+$W>#
	J=		
'2

"3xf#ย*$"#
C	/ย.ย%b+
"#'+/>3
5aรฌ_2'!$2'+	
	f$!$+."P=q=
3,#@	$?P=	b	/ยฅ%8=	+Eย,P
b"$F%'/ยฅP=	b+?X
	#+tL
>
"#_P=	
	/ยก+%*=	+E0b
#@?=	/ยก#ยรฉ$"#!
#5ยท+ยฐยณ<:?=	T=	;=+4:
<,/>S
	2-

3!
=	/V:	+fD$c	T	$;=	bP
+3*;
#@
	G	/>F
VDfVE35
v;=	,/>3
-V	ยผ$"#
$;+%_P=
E;PBV+Gย#ยฎ+ย_P>#@	,=	s
$?=+M+Z		.+
+A
T
%รถ3
-/,
-	4	$.+
-3D+JS
X=	/a
"*++
D4+%	=	K&
'"	/ย%(b	.+
-*:
ย ยจยKG	+
M	#X
/,
-+G
"#JP=+;=
;
Eย+%+V	+	#+
W
F/G
!
"3XW
:
+Lย qยT\	+
?V=	f"#/G/,	
Z+ยข
/G/WD.
Z%ci3-
P=	/ย=+CE;
XX3X+E
#ยรฉ$"#!
M	/รช!+.FWJ	
x;$ff=
F
$e5
ยZC	SEย)+
\=	G#	/GDM+%<	M%'.+/WE_B>Eย
=qf$P"#
	
V+%<=	,	-
	
%./GE;B,%';		#f	#+
M+	
-	5
r;รบ%s "

" รรร)#*"
รผ 

!+

ร รผยรผ

รผ.-Lt

รuv"^wLxVy{z
ร

+|-

รฝiร รu.}

I+
"#>P=	ยยทยฐรยณ03
=	/r
C+]#@( 
Z+%T+!
32'+.$:"##3
2'D!
	:)$Y"#/W2
/ 
/Gb	.
,+	
-	q3
X=	/V:4E;G)+
Z\	$
	VP=	G	.+!
Z3
=	/
,

.#X%5,ยญ+E;$:4E;GVV 
	f=	G	#+
+\+%_P=	>ยท+ยฐยณu	/V:*
Z=	,		"#$
DP2
ร3ร$ร

ย

yz{z|~}aยยย

ยย	ย

"#?/>+V+%=	>	.f"#P	$J+q%	"#
-b	$$ยขf
-/G/Gb=	,%'X_	+	#+

3
=	/V5 ยฃ 	b$+/WDc
c	
#%-t`,#E=	?ย ~\" ยป X$PTp
ยต i;DX
3:1ยฎ$ยฏยฏ	ยฎ(
ย ~_
ยตยยยข#4:4ยฎ$ยฏยฏ+รณD(ย1%';/GPJ.3
X 5
 ยย

ยยrzlr

k~lยยkยยlยยkย	nzv

 07>365XA2a
GP"=	/>
"M	$!D.
?+%<+f#+T3+3
X+-T,=	C+		$5 ยป \"#

ยป =
"#&
'.b%cย2\0@B;D:KV!Y%8(<P;7A24C5	365	A2WD:+ย0CCยYย5ยW3':KยC;>YZ;>36;ยYย5ยW3.:K+]V!Y%ยย5X2\C5	2GF
7A2W36<05X2\3	WD5ยvย=	fN.C%'	>+Pย
 ;ยย8(<P;nWW5	A2WJP=+G"+`"#.3
-ย
 ?>0<5	0ยYZ;nWD5]ยZV!VR	$!

/>+PB(cW
D!
X%f++
$ย
: ยย?%'c
.+"#ย
5 ~Q

	V"#.3
-D.c+J!$VPG

'"+,=+
?+!
"#+MS+!
+G"+		b)>)	\V?
"#"#!.+bbV/WY=	bS+!
+5
ยญcM
+V"#
V"#P$
	GGG&
-/Gb	"B	E;'B
 ยย:ยยยย>#+$[
ยยยยย4ยย ยยยยยยยกย6ย ยยขkย
ยฃย ยยย:ยยยยคยยยฅยยยฆยง
ยฃยGย ยยยยชยยยยฅยยซ(ยง ยnยยยฌOยย ยจยฅยยยยง ยnยยยฌOยGย ยจยฅยยฆ(ยงGยง
ย6ยยจ ยGย ย
ย ย ยOยฉ
ยฃยGย ยยยยคยยยยฅยยฆ(ยง ยnยยยฌOยGย ยจยฅยยซ(ยงยง
ย ยยGยOยฉ
ย ยฉ
ยฃยGย ยยยยคยยยยฅยยซ(ยง ยnยยยฌOยGย ยจยฅยยฆ(ยงยง
ย ยOยยยฌOย ย 
ย ย ยยย ยฉ ยGยจ ย ยnยย ยฉpยฃยGยnยญGยฎ ยยยยฅยยฆ(ยง ยnยญGยฎ ยยยยคยยซ(ยง ยยญยฎ ยยยฆยฅยยซ(ยง
ยยญยฎ ยยย^ยฏGยฐยฑGยฒยณ(ยง ยยญยฎ ยยฆยดยฏGยฐยฑGยฒยณ(ยงGยงยง

 Wยถ36;H8w5^H#+D!
+
X	b+Y"#!
b
-D$
ยป C
.+"#F+%รฉY"#
-b
9
-$
DPMYKยต
ย.ยฎ+ยK+

	,	
R	J+/W$;G=	b++
$F
f=	J"#
*:	+ยย ยจย<&
-
	,=	JVa	
R	

#@]
ยP=	f+*:<Z+L"+u"#.3
-Z/G?=+]	?
!.+"#?+%T=	V/Gย"#
*5 ยป
A
P=	#%Y+A
."#J+%_+\"#
?
$ย
>W+VEย
=qf
#@V=T	
R	#


XN$F
35
% 7A2W36<05X2\3	WD:3E=
"P=G#
=	<"#!.3
,=	;#<+%4 E;b8

ยป +>3'"#D.S
KJKยท
=	;+,<"#!.3
C=	;

	D%wS
+$9
-YP=	ย!5 ยป ,#
	J"#P.3
*.+B$=	
%ยน
/ ยธ\ยบยทยปยผยธยฝS:+E=	{
 ยธ4ยบ4+%
 ยธยฝ__P:D+s

'"+$9=+K=	,E;
-=b
#!
@ ยพ4/,K	""#	
#%';=	,E;
-=b
#ย
@ ยฟ45 ยป 
-
	J"##3
*
9+%4=	F%P/ยยย รยร ( ร $ ยw;ย รย
ร ร ( ร $ ย.:+Ec=	
ร ( 
'T?++
+-Y+	)$+
	W
Z/GGPV
-V=	,+Z+m
 ร $ 
c#
=	JWS+
'+JJ"#.+
+	$
	,
fP=	b+*5 /
ยZZ3'i		.+0"#.S
D?E;
P=ยคยข$"##u%YE=L
fEW"#$รจ
0=	\*5
v;=	#%'f+*yz,"#P.3
.M
'J"#SXZ\,+%T3
#J%T=	W%Pร
/ ร[ร+v
: รGรรE=	V
 รY
,
#
=	Tf.
	W;

	>"#!.3
$:	+ย
 รs
ร
 <P;0ยWA2G	+.,P"#	,ย #N	$)#+Ebย.5
v;=	CN3_"#/G	J%ย?+
bf!b+%ย"+P3fYย5X2OรW :$"P=Z+%;=	C%/รยธ ยบร ร ยธ ยฝ :*Ec=	
ยธ4ยบ<+รยธยฝJ+J!:4+ยร 
+f#@		$&
-*5_vย=	bX
	B?$"##	=	M%H"#P=+	C		D!J+%
ยธ49
ยบ 
f=	b+?
'<PG/>+BยรยP	:	E=	Sรย
;,	$"#
-
?+%รยธยฝS5K^รถ%Q,'+V"#.3
;sX
	B
ยธ ยบ ร ร ยธ ยฝ 
-_/,M3G"#.3
?=	C.
	%ยธ ยบ ยป^ยธ ยฝ 5

+V"#&
'.F+%<G+%<6:(W+%<"##3
.:DVGc+%*X
	B	5
 รLรc*
: ร;
: ยฐ ย ย(ยยยย ยฉ รbS
5 ร0
'JV!J+%;#@(	$P&
C$"#

	
ยป 8Yย0242\5X2Fl8<AยYZ;>@ร
J?P
V
=	V	/Vyzs

!
3T"#

-ร
: ร 
GZW+%#@		$ 
?$"#

	ยข=	V	/Vyz,D3 :
+ยยฐ ย ยยยยยย ยฉ 
'b=	fJ%c3+3
X+-a"#
5?ยZf!	/GG=+l
 ยฐ ย ย(ยยยย ยฉ 
C3S3
X'+,VP=	
3!
=	/ ,+3*S+
'+5
ยป

รยพ49ร$รยร6รSรพรยถ>|$รพยรSร&รรรยรรรTรPรรPรร-รรร#รร$รร'รร-รรร3ร-รรร#รร!รรร'รFรร.รรHร!รรร3ร*รรรร$รรรร$ร;รร.ร3ร'ร'รHรรรรร'รรรeรร,รรรรรรรฉรรร4รรรรรรร
ร!รรรรร*ร'ร;ร'ร3รรรกรฃV
รข รค ร รค ร4
รฅ ร3ร.รHรPร-รรร#รbร$ร'รรcรรรMร'ร$รKร'รร'ร*รPรร'ร$รKรSรPร+รรยพ

ร3รรฆ

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

ร

ยzy	รฃยยyยยzย"ยz{

รฏc#@	E;J#@	+
?.+	+#?	$.+
-3*
"PB>+"#TW+	
	W	/ยฅP>
+f?	
X
	>a+f=+"#.3
-
ยฎ5J?E;
P=>+/Wย5	245	365X0Yร:
#@ร	:=3
	J	,	$"#
-
F	#cX
!$:+		E;
P=>+
	WX
"# 

	,+%K=	b	/VyzF
-

3*"#

-
 ร:
ยจe5J?E;
P=>+/WรงFA0Yร:+
-#@รฉรจu:E;
P=G	$"#
!
;"#&
!
	J%รฉP=	T3w#@		$&
-
ร<:			c/G	ย	+f#Mx
.:
qe5;=	J&
	+-T#
	>"#!.3
{ยธ
4

ยป^ยธรช?:

รณ)5;	GS+!
+2'

-	a"#.3
-D.6:
รดe5;	,X
	B	5
ยบ fV/,b"#D#3
q$TP=	JE;ยPT+=	,#
	:4+fE;G"3X=	,+
E;
P=fG=
'"#P	b=	!24MY	Y8Yย025
vยE_c
$!
		)
$%wM+G+Q
.K+ยท
% Aยถ8:;2ยต8<;7A2\C5	365XA2OW_+s
.K+L
% 3	รซG<;03	N
;2\;C%Yย5X2OรW5Kv;=	b%'/GF
ย=	Yc+%8#@		$ 
;=T+	$;
qDf*yz;	P$"#

f		
=3b	>"+S9!		cE;
=
?=	b*ย	=	M+PF
F=	J+%K#@	X
"P
"+SรฉP#+
=

=+b/,
-=DMWDXx
XN$D=	Jc
-qP=	Y'+*5Y7e/>SXV+\)ย"#

-f
]G*:
	.+P$^
 ร ร ยธOยฝS:4
bf!รฌ
 ยธยฝb
\=	W+\=+b=M	P$"#

รญ
 รW:1+A%'bE=
"P=\=	,
'M	
X
	BW
f=	bf+%8P=	M%P/รฎยธ\ยบร ร ยธยฝ%'T+ย!mยธ4ยบ5
ยป X
	B?+%<=	M%'/ยกยธ\ยบร ร ยธยฝT
'ย3	รซG<;036;24;C;ยผ;
\"b=	Pb
	=	bmยธ4รฏQ
f=	J'+
"P=V=+
ยฎ5;P=	Y'+*yzc.
	V"#.3
-D.cE;3X+Eยคยธ4รฏ;f)G.P$qS%lยธ4ยบ;+\#%'%ยธยฝ+:

ยจ	5{ยธ\รฏ=JAD!."#

-LยH#
P=	Y	 = Y#+ยM=+C=	>'+*yzb++
2'

	\"#	2
!.3
.QE_f3X+EยG	
X%'>Eย
=ยรG5
ยป +\E;
=	V)\	$"#

-b+	V=	P$+	$AX
	B	c
M"3X$\รWAYยMG365	A2ZPf=	
	"P
+P$G	
	G	-/V5
79
SX]E_A
	"#V=	รฌ
 <;0ยWยถA2]	+.]"#P	:			$"#$P+L%'?	.+!
f+	
-	
		J$!D
'3<%'J	+	#+
*5 ยบ Pq!
/G>fP*:*x
	B\Y"#!.3
M
b	$ยข\f+]+
	"P
+P$1
 <;0ยWA2b$"##	
.		PD5 ยป $!>"#&
'.+%4 E;M+.6[<ยฎ+ยKbD/,)+$"#.
-	
E=>=	b"#P.3
;Eย_	$ยH#
=	;L
ยณ UkU(รฐ nยท bยซ	ยฐรฉ:)ยซDnยท bDL
ยณ รฑ\ISยฒ-ยท+ยช1:	Tยฐ	
ยฑ TkbยซD\
ยฉ bย.:+fยจ(ย8#
-=	
X
	B4:	*:	;=	P$+F
f=	b+?
'D
x%
	,=	b+;+%=	b+f)#
	G3
-$45;I$"#
V
 q	5ยรณ

"#!$;$F
f/GPJ.3
X 5
รฒ$ยพ4รธร#ร_รKร+รร#รรรรQรSร3รรรรwรร#ร$รร'รร-รรรร'ร6รรร-รรร.รFร'รSรPร*รณรด)ร'ร$ร#รรรรTร'ร3ร'ร!รPร'รรSรณรตรท รถ รณ>รธKรzรรรร4ร+รร*รนยร.รcรรร'รLรบรปรป9รzรรร'รยพ(ยฟ9รXร'รรรฉร!รzร-ร
ร'ร$รร$ร'รร'รร$รรKรร4รณe
รด ร$รรร'รรผ ร*ร$ร'รร#ร รรkA
รน รXร'ร.ร]ร+ร&รรร$รFร'ร'ร$รL*
รฝ ร3รรfรณ>*
รธ รรรwรรพรรร$ร'รรยพ(รฟร$ร4ร$ร|รSร6รรร-รรร.รรรPร$ร#ร$ร'รรcรรร'ร#ร
ร ยฟ9รzรรรร'ร'ร รรร3ร 4ร.ร'รรร6รzรรร'รQ'รก ยฝ ยฝย&รฅ ร+รรร4ร3ร รรร'รHรPร'รbร'ร_รร3ร'ร$ร'ร<ร'รร'ร'รรรPร-รรร&รรร-ร#ยพwรธรร[รก KรPร<ร3รSรPร_ร+รร-ร-รDยฝ ร4
รฅ รXร#ร1ร

ร6รรร'รร$ร'ร-รรร.รยพ







ร3ร
	



ย

 
k 

ย

n

{



ยrmt9tiย!t

n


ย

ย ยยยl

yz{z|~}aยยย

ยย	ย



v;=	,	.
b	
	ยS
-=	/ยก
c$A\=	b
$?%_.+!
ร	WE;
==	,DX+Z+
""#$ 
#ย
 <P;2\5X2FM
MD\"=	D&
-	ย,
 /w3EยกยH)Z"#

Vb=	P$+	$AX
	B)ยยq	
	
	EdP:X
	B	:	ย"#.3
-D.FaN	@>
35<v;=	J3
=	/ยก/,
-+$;#
P=	_Ec=	qG"#/W
+s
)%'	Wย ""#$รยw9E=	G3XDP&
F#N	/GK	
รฉ=$K,#@(=$Wย'%H3
X	+ย#5
รฌF&
<M	
	b	/รE;
=C

-
3("#


 >ยkยย(ย ยยฌ ,D3
 ย ยยฌ 59ยZTP	/G
=		=		<P=	+;=K=	!<+%1"#
-K3S3
X'+;JP=	+		$
: ยฐ ย ย(ยยยยย ยฉ :+
'N	@	$45KยZ
	+Eย#N	JJP	2-#4%'	"#
*
:  ยฌย ย ย ย ย ยจ ย ย(ย
 ยยยฌ ยฆw:E=
"P=G

-
3X
XU$F=	b$+."P=f+f"3XF
%'	"#
f=%'/?<P=	J#N	/G		"#$5

ยฎ

 ยยtkยยl	ย ย t { ยrmt ย ntmn:ยยrzl	ยomnยย !ยkยย(ย ยยฌ#" Gย ยยฌ%$ [<8+?LD05	YZM<;
&\[ยรยรฌ<P"#;=	JX*+?%'/'>ยkยย(ย ยยฌ +(ย ยยยฌ
ยยnPlยยkย0t*) ยย ยย ยยขkย ย:ยยฒยGยยย ย (
&ยง

ยจ

) ยย ยnย ยยขkย ยยGยฒยยยยW!$+."P=	$K=		=>P=	"#T+%+
S+Q%_,+	!
G*:D
	
=	q!$+."P=L=	
U]
-DP3X:K$"=L
-/GV"=	D&
-	ย\+uu"3Xx
	+) ยยย ยnย ย  ยฌย ย*:KE=
"P=
"=	D$c+?3
#G&
	[
 /w3E0
-f=+*5
ยฎ

 ยยtkยยl	ย ย t h n-,Wt~n  ntl o ยwย/. &ย ยฌGยฌ  ยฌOย ย $ [<>QDยถ05	YยMG<;
0ยจ:ยยย:ย(ย ย ยจยย2143
&ย ยฌยฌ  ยฌย ย/5
ย ย4ย.  ย ยยnยจomn:ย6
ย  
0 ยจ:ยยย:ย(ย ย ยจ,
';/G	รฏl  nt=ย	nยจlยยkยt,Dยถ05	YยMG<;
 ย7?
^
1 #$"#Tf#/G;%P/80ยจยยยย(ย ย ยจ
oM#9?
 %'/80 ยจยยย:ย(ย ย ยจ
ย  W
 
G	
ยl  ntยยยnPlยยkย0t:
nzยvn>	fSX*#/G.;+%#) ยยย ยย ย  ยฌOย ย ย (
 ยงM;0ยจ:ยยยยย

ยจ
รณ

q

ยถ

รด
ยฆ

ย ยจ

i;#N
-	aCย"# 
.Q+%1E;J+#[1#-$"#
	G{
 /w3Eย
?=	T'+Vย+>?	$"#

-
JP=	$+	$x
	Bย#:*=	Z	.+
	\3X<)D&
->"#$"#!
M=	B/w3EJ5>vย=	ย#-$"#
\+%
E=
"Pย
= /w3EยกZ"#P$"#Y	$]	Y)f$"#&
'$4:<		,=	?/>+		a
ยEc=
"=ยข
C
,"#P$"#$
/,
=;=3b,:)E=
"P=?
;E=f3X*DP&
J"#P$"#
;+PJ	$f,=	,$+."P=?%'
$5

ยฎ

 ยยtkยยl	ย ย t h n-,Wt~n { ยrmt< $ [<รญ)
!_%8
0 7ย 1AI#$"#Tl/w3Eย%'/8
ยยnPlยยkย0t:
= ยยจยจ ยย ย>0 ยฌย@L? ย 0 " (ยง

ยจ

รฌ<P$"#
	Gl/w3Ed+/G	D.F,$!+
	G+V)V"#

-f$!+
	G,=	$+3[

ยฎ
q

ยจ

 ยยtkยยl	ย ย tBA ย ยยยยnยยl
CยยrD(E0 "  $ [<รญ)
!+%1
ย  a
0 
+f)V	$"#

Xl  nt
ยยnPlยยkย0t*) ยGยฉ ย ยฌ  ยGF ย ย ย ย 0 " (ยง
nzยvn1ยยnยจlยย ย0t*) ยGยฉ ย ยฌ  ย ยฏ-
H ยจ ยย ย ย 0 "  ยง
ร3ร
I

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

ร

ยzy	รฃยยyยยzย"ยz{

ยป G)ย"#

?"+Gb		)$W#
=	FDG"P=	DD 
	Y+W#@D
!
	J?=+<.Q=	
	)D&
!
?ย	
-	>,	EdPf=+T$[

ยฎ

 ยยtkยยl	ย ย t h nยv ย ยomn | . ntKร:J L ยฝ "  $ [<รญ)
!+%K+
 ย ยC$"P=V L ยบ "#	PD-G
(Xย ย
ย  L ยบ8"VC.$?)#% L ยฝS:	+ L ยบK		G"#
-
f	
X%'
	,E;
=
J l  nt
"J L ยฝ " ยง
ย ย ยยn:ยยl L ยยGย4ยยจGย ย L ยบ "
 ย ยC$"P=V"#
%ยฐW
ยยฐ ย ย(ยยยย ยฉ E=	DJ	ยX
"#.3
-_W"#

>	
X%'
	GE;
P= J ย
ย LM :-N ยT[ยร ยฐ ยGย L ย ย ย ย ยฐ " (
ยง
ย
"
" L ยฝ "  Nยง
ย ย ยยn:ยยl L ยยGย4ยยจGย LM
J
ยยnยจlยย ย0tV=	sX
;+%+"#+x$"#$f+;x
	$รqG+Vยถe5

ยจ
q

รณ
รด
ยถ

ยฆ

vย=	G%'	"#
 ยฐ ยย L ย ย ยZ.+B$J+ย"#
ยZi
			.:/>B$J\"#\+%T=	G'+*:

#+D!
+$;=	,"#
?
-DfGP*:w+\		ย
PG=	,+E;
=f=	C$RD
-$f#
	f+


	>"#.3
-D.65*^HM	P;P=f=	b	E;f	$V!q+?P=	b	E;f"#
$f'+*5

ยฎ

 ยยtkยยl	ย ย t n ยยย'kmln . ยยฐ "  $ [;ยPI*:'+4ย
L M [ยรd,	EยfEย
=f"#
ยยฐf+f+?
-#@V	
R	bPO
) [ยรd,	Eย$f+%K=	M%'/'ยP ยณLUkU(รฐ ยทnbยซ	ยฐ LM
Q
\
 N [ยรยY"#D?+%#
ยป 	 L M R N
ยป 	?$"=+%Lยฐwyz%ย ย ยยย ยฉ ยยจ ย ยnย:ย ยฉ RN :$"P=V#+$WE;
=()
ยป 	?=	J#
	D L 4 ยป LGM + LM ยป L รชรON :e=f#+$?E;
-=;)
ยยnPlยยkย0tZย LM :%XN ย

ยจ
q

รณ
รด
ยถ

ยฆ

รฏc+E L ยGยยOยยจยa		_C"+34X
-	BY)E_?E;J#@

	,Q
GP=	T L ยบ + L ยฝ :(S	
E;
=fP=	b$R
-$f#
	>+?
-
	ย"#.3
-D.658รฏc
'"#b=+=	Pb/,
=C/GbP=+
	GE3VAX
	B\=	GE;Vs$"+!>=	PG/,
=b?/GPG=+Z	GD!."#

-\+% L ยบ
=+;"+>	
x%,E;
P=G=	cX
	B,	)D&

- 5v;=
K.+!
,
8

"34J=	cEย3Gยยท jยIยฐa		
J
"+39X
	B	<#@"#	M=+;=	Y"#.3
-D.F+J+		.+$?E;
-=ยร
 <P;0ยWA2GP"#	5

ยฎ
ยจ
q

รณ
ยถ

รด
ยฆ
ยฏ

ยง

 ยยtkยยl	ย ย t k*ย .<.ย ยยl@ L ยบ " J " L ยฝ "  $ [Kรญ)
c+%8'+
 ย ยC$"=V!+%K

	Dรยฑf"+&
-	 L ยบKG! J ย ย
fN ย2>
1 G"#D?+%#
ยฒ^ย7?
1 C	EยX
-	B L ยบร:S L ยฝ
) ย7?
^
1 C	Ed$TรP ยซDยทnbDยณLรฑ\ISยฒ-ยท+ยชยยฒ Q
ยป 	%ยฒ>UGN
ยป 	?=	J#
	G"##3
 L ยบ ยป L ยฝ R N :	#+$WE;
=()
ยป 	%ยฑ>UG'N :.+$?E;
=V)
ย ย ยยn:ยยlW N
ยยnPlยยkย0tV=	,+%1'+"#+X-$"#$f+TPVยง

ร3ร
X

ย

ย

yz{z|~}aยยย

L :L

ยย	ย

iย$"3X	=+<M=	$1bcX
	B ยบร ร ยฝK
KT!%ยธ4รฏ4=+<"+G"#&
'b;#$,E;
ยบ_+
ยฝaq"ย"#&
!D!VPb#
=	Sรยก *รrb?)D."#
-
]ย'
 5 5J#
P=	J		

L

L

RY

#$ ร,ย.5ยZbbP=	b	.+
-Vร%ยธ4ยบ>ร ร ยธยฝ@Zยธ4รฏยร0G	b=
'<P=	$+$5Kv;=	b=	PbD 

E3(M$+->fP=	$+tu	/G
*:*/G
*:1+]+.
tL
+-SG	
-	q.
	
+?

-	ย"#!.3
.QG=	b*[

ยฎ
ยจ

รณ

q

ยถ

รด
ยฆ
ยง
ยฏ

ยฎ
ยฎ ยฎ
ยฎ$ยจ
ยฎq
ยฎรณ
$ยฎ รด

 ยยtkยยl	ย ย t h nยv ย ยomn k ยยnยจrzl@ร L ยบร:S L ยฝ[Z L รฏร "  $ [<รญ)
;+%K+
) ย71?C	EdP$TPรยฐ	ยฑTkbยซDยฉ\bรฌร L ยบร:S L ยฝ\Z L รฏร Q
^
ย  L รฏ<"+V)J"#&
!D!G.P$f#%' L ยบ9l  nt
fN ย2>
1 W"#D?+%#
ยป 	?=	Y"#.3
-D L รฏ.ยป L ยบR N :	#+$WE;
=()
ย ย ยยn:ยยlW N
ย  L รฏ<"+q"#&
,)J.P$V3%' L ยฝJl  nt
 N ย2>
1 W"#D?+%#
	

?

=	Y"#.3
-D L ยฝยยป L รฏKPOG-N :	.+$GEย
=()
ยป
ย ย ยยn:ยยlWGN
 ย ยC$"=V!+%K

	Dรยฑ?=+;	D. L รฏyzF#ย*$"#.F%'/ 	
X%
	,E;
P= J
fN ย2>
1 G"#D?+%#
	

f

"#.S
D# L ยบ ยป L รฏ + L รฏ ยป L ยฝ U N :P=V.+$GEย
=()
ยป
	

%

>
ยฑ
UG'N :.+$?E;
=V)
ยป
ย ย ยยn:ยยlW N
ย

ย

ยยnPlยยkย0tq3X*	EO+"#X$"#$f+FX
-	$รด	:	ยฏ	:+\ยฎ6รณ>++

รฏcM=+FX
	G>ยฎ b
',
		-b$"+CP=V"#($&
-+
ย+?	"#($&
-+
V"#	2
.S
D#,/,>)Z	$45 , 7e>#@+/G-:=	\+ E;ย
xยรฉPD?/,

/?3.W+%J

-	
"##3
.F=+;/,,	$fPG	$"#ยตยธ ยบ & ร ร ]|ร#]|ร . ยธ ยฝ %'/ยฅG!ยยธ รฏ P=+#$ ย ยยยยคย ย
ย
^kยงW
[ 3ย ร`
ร _ย/_ ย ย 5D:4+a
 3ยย ร`_ย_ ย ย!:>ย รb
ร _ยฆc_^*2ย 5D5Jรญw
	Vยฎ$ยจa
M3W
$
-	Gt =	G"#P.3
.
L
ยบ
ยป
<
รฏ
+




L
รฏ
ยป
b
ยฝ
+



J



	



$

ย




f



.

P>	PJ(!/>+
'"P
H5
L L
L L
 ย

C

ย ย



rย

n ยยl	ยnยv-6รฅk ย ยยtiย;tmnzv	ved
{ ย ย. 

A ย .

-d 

ยnPlnt~nzv	v 9k 9vln





rzl	ย6ยยl

~\" ยป X-$+ViยDX
Mย.ยฎ$ยฏยฏ	ยฎ+ย1	PSb=	b	)
$ย+%8P=
;3!
=	/V[
ยฎ5MI		$[0%'Z+0
			\	-/ Eย
=0

-
3Y"#
-
f	:DS):d"#!

ยฐ ย ยยยยยย ยฉ :ย
X
%  ยฌOย ย ย ย ย ยจ ย ยยg ยยฌ ยฆ ย  " 4ยงZ""#$P&%xuP	>]'+i
 hG:c=	0#@($"#	2

-	J=	JPK
-;
 hO
f+>&
P+
+>
&%'
-	O
 JE;
xXรฉ3-Eย3			"#YJ!.+c
?E=
"Pj
= 

'<P	5
ยจ	5MรฌF/G	$P[< ยฌOย ย ย ย ย ยจ ย ยยk ยยฌ ยฆYEย
XX4NVG!+	
>?
X%1	C#@D
!.5

l ยฌOย ย

q	5MI	/?+
"P
&w[

k

ย ย ย ยจ ย ยย

-+ยK/Gb=f"#5

ยยยฌ ยฆ,E;
XX4	T"#&
'_P=	J+/GM+ยH+!
3wย"#/G2

รฌ /G	$+C(!/>+
'"P
"+C;#@	3
	$s%'	=	Db
E;
	9 ยฌย ย ย ย ย ยจ ย ย(ย
 ยยยฌ ยฆ
<
$."=
-	>G
$"#P$f.	=f+%8
34+5;v;=	b#+	=V=c,	
R	J$:=	CDX**:

mยพ/nร3ร1ร'รรรรกXรร#รo\รธรFรรร'รรDยฝEรรฅ4รXร#รรฉรPรJร!รรร'รร'รSรPร-รรร#ร<รร3ร$ร'รรร&รยพ
ร3ร
p

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

ร

ยzy	รฃยยyยยzย"ยz{

+]\"3X_P:
 ) ยยย ยnย ย  ยฌย ยq	#+$,	(yz,"P=
X]ย"=	D&
-	qร/w3E Z	#+
	

.C""#$.WยH=	?+
S<+C$!
	?%P/ "#&

-	q3X;DP&
GE3	b+%N	@
	A
ย.5
79
	Pยจ?ยK+ย
 qqย
XX-.+P$<=	+E0#N	/GM"#$Y%D
-_		JEย
=W
."P=
X*5
vย=	T"#/GP	$$F/G$+K&
-/GJP=+KG+-	
C+>+	)$+.FKM$S%w		;

=
ย.+	=*:	+A=+ { ยrmt ย nt~nยยrzl	ยomnยยE;
XX*3X-G
'&
;W$3%K	(M
x%8	$"#$P+5
I	/?+
"P
&M
/GX
-$=+K=
K
-$"#$,#+	=C

C%H"#<!
 36<;;$:K79
-	ยจb	$!.5Kv;=	
.	=f=F=
;	G)$"+JP=	J"=
X'f+%8,
34+f		J+J3x9S+!
รN	@	$
%'J%
 /w3r
E qetu$"P=i"P=
X=CV
Xย*J*:9.
	):4J

-	q"#P.3
	$\fN	@
q5 ยป >&
-"#J	$R	D;#N	/G.;-a	?/GT"#.3
-D.6:$"P=>% 5X3	W<"P=
X?
-	=	

=
b"#/W/,
/GMf=	+s
E qL=	,N	@	$45Gv;=	#%'>D+\\=	C%PD!
bE;
XX<
Xย*
%P/ยฅ?=	c+ff=	s%PD!
F
f=	JE3,
ยN	@(${
 WA@B;ร/w3EJ:)+f=	JP+/Gb+fEย
XX
	T+	)$+Mf=	M%'
;/GbP=+f"#5

ut8vXv	ย!t/xwร;0ยWA2OWยl ย

ย

n ย ย ยยย
h :

h

e,

n Wtmn



ntlรยยnยยvยย

ย

tiv

 E;;/G
	$,+)+:+=	;$a	.b"#P	F
			$"#$+Pb
>c+		F=+%'/?
ยป 
?#N	/Gb#+
65_ยทยjkIยฐ:e%c#@e+/W:w$c	,=	/V5ยญc+E;$:=	f	+

=	,&
c%'bP."#
	?J$"P
'&
ME=
'"=
M?	$"#$+Pย"#/G	b+%;+Z	+	.

?
"#$i
uP=	V	#@	ย$"#!
*5 ~F#%V#@	3

-	ย=	."#
]		"#$:=	+E;$:FE;
	/G/>
Uf=	รฌ
 <;0ยWยถA2Z	.ย!"#	$a=+?$"#.รจ=	SEr+LE=uยข+uEa#N	$45 ยป

Xย*_P$VP"#	M
ย$?%';$"P=f+%8P=	J=	b$;%8#N	/WD$[
IPd	

-*5ยย0=	ย]	EรตPยค
 ยธ4ยบC
f	$0PuL+ ย'%	"#!
^
 ยฐ ยย L ย ย ย4ย.:c=	
++
2'

	V"#!.3
.	"P
+$E;
P=V
-.T"#
Z"P=	/>f+G3'V	$4:1S	
Eย
=? E;,.!
	G"#.S
D#<!	
	G=_P=	b	EยPf(""#	.c3%P=	M

!
3รฉ!
V)#%,=	J3Q!*5bv;=	,$M""#/W+D
	?=	$G"#P.3
.M+YSX8%<=	
%''
/ PยQ
ยณ UkUรฐยท bยซ	V
ยฐ ยธ\ยบ Q 5
g

รฌ;+3*x
	Bย	

*5;ย0=	\,X
-	B>+%<P=	b%'/ยกยธ4ยบ>ร ร ยธOยฝT
c	$G=	,+\ยรถ%	"#
-
 ยธ ยบ ยป^ยธ ยฝ 
'J3!ย	$4:K3-	>Eย
=\S+!
+2'

-	
L ยยGย4ยยจย)ย.:wZ.
-	V"#.S
Dl
"#.3
-D.9	
-	J=+KP=	T#$"#$aD#"#

Y*
% ยธ4ยบ*"#SXJ!.=		PD 
2
!
?$R
-$?D?=	,#$"#$?	$"#

f
% ยธยฝ+5<v;=	$J"#!.3
.FE;
Xxรฉ)J+		.$
g

Eย
=f,$V"#	Pb+%8P=	M%P/yPรยซDยทbยณLรฑ4ISยฒ-ยท+ยชยยธ ยบ ร ร ยธ ยฝ Q 5

v =	P$+J$!+	
*5fย0=	iAX
	Bยยธ4ยบร ร ยธOยฝY
'b=	$	$Zย\ ยธ4รฏ:K=	WX
	BZ"+]
g ;
$!+$รจย'%'	"#
T) ยยฉ ย ยฌ  ย ยฏeHยจ ยย ยยMDย	
	\	f+%T=	PV.s+%"#!.3
.[G
#
	\+%b=	?%/ ยธ4รฏLยป^ยธ4ยบ:;+u#
	\+%b=	f%'/ ยธยฝยยป^ยธ4รฏ:;,S+
'+2'

	
"#!.3
.	!
	,=+<P=	T=	P$+
	CD#"#

G+.
% ยธ4รฏD$F	"#SXb%H3 
X%
=	cX
	B4yzF	)D&
!
1
 รW58vย=	$J"##3
.KE;
xXw)J+		.+P$GE;
-=ยC$f"#	P
+%=	b%''
/ Pรยฐ	
ยฑ TkbยซD\
ยฉ bรฌร L ยบ>ร:
S L ยฝ\Z L รฏร Q 5
v;=
M"#/G-$M	b
Eย+%;	.+
-GยHP#N	/GD6ย+	
	:*!fE_,	+Eย	\fP=	
#@( 
;=_P	V=
'<'+		;
>+V	+	
b3
=	/V5

z;รบ!!+ ร
รผ

รรรพยร(''รยรร@รฝ รผ

v;=	J+bE;,/>+ยผ
Xย*"#$;)E_	.+!
b+f	+	
-b+	
	)[
ร3ร
{

ย

yz{z|~}aยยย

ยย	ย

Null Plan

Retraction

Fitted
Library Plan

Extension
Working Plan?

79
	,ยฎ[<8+?#N	/G?."#!
f$+#"=?


.

ยrmtZ"#5

Plan Extension

79
	Pยจ	[M>#N	/WDP"#$C+?.+$ยU(T(gยjVE;
-=ยW;+%8	E
+V	
!
3*fT"#.3
-D$5

+:$"P=fE;
=

ยฎ5;^Hu	+	!
f+	
-	Z=	?
W=Yย5)ยยถ<0<Eย<;36<5X;>?0Y	=!?
LE=
"P=L=	f+]x
	.+ย

$+."P=	$?%'TW+f="P#?/>+."P=	$;=	s
			
-

31+AD39%P/>ย	=	sX
	.
+?
F=	ย
 0C2|MW36;CsG/>+#"=f=	J"#	PDc	/V5
ยจe5 ยป 	
 +	!
J'+	
	f
bE;
-=V=
'M
$qq3ยผ P$V
3K+*:*q"Z2
."#'+	
	f"#.S
D#T	$E=	\=	,VEc
+
-3ร>	.+$4ย4	#+

+	
	,
Eย
=f,+fE;
-=f	>"#P.3
.;+f"+f-ย	A	Eย	$5
&^ >=	;E_.	:)=f	#+
M+>	+	
M+	
-	Y+b$+#"=
	C%'_,	
W+

ย?PG+%;+
S<+:9(	C	.+!
,+	
	V!.+.bb=	fยH	
R	+ยTPDb%_=	G'+
?E=	$,		
G'+	
	V)+
GY/Wย+P
.+PV"#W
]=	?ยHDP&
Z+,
+	!
*:DP&
>=	b$:&
f+/WM
DP
;		+ย.5
79
	Yยฎb=	+ETย=+	+	.
f.+P.<+M+?

F	(:)+fG+-	
?/,
=+	$
ยธ.#-SEMยน\
W
0=	\\W
ยL
Xย*ย!		Z3=	$5 ยป f]$?=	Z	+	.

3
=	/ย/,Cf+?V/G+ยธ.	ยน=	?WDย
 <;>@ยA?5	2GF?"#.S
D#c%/ย=	?+ย
E;#X1;/G+Wยธร+Eยน,=	bPbDm
 0CC5X2FJ"#P.3
.5
79
	$bยจf+ย
 q=	+Eย=	GE3V=
M/WS/GDM
'b""#/GX
'=	$4[b+ <P;2\;@B;2\34
s=	
ยH)ยK#+
f)%'/G$ADf,	.
b		$5<^&.+B$,+!
3*+f=	b=	
-U
+"#$c
-bE;
==+M+*yzB
 7nรซG5XYยC<;>2:4fb+%<+c
'D
'"38P>=	C
			C+#@e"#	
%=3
	,	b/W{
 /w3Eย3
-$45
8+!
 <;36<07>365XA2M.B$*c+JJ=	F=	
U,+J"P=	D$T"+P3X
	B4:38!1+%"#P.3
.
ย
 <;@BA?>;$5ยv;=G+]
W"#$LL=	=	
ULE;
=L=	+Vย&E=
"P=]
,/>+PB$]%'
	

-3<#"#
4ย#:13	fE;
=ZP=	>*yzJ&
-X
	DaยH	$
	\3Xf
 0Yย36;><2\0365X?>;,E$	M+%
ร3ร$ร

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

ร

ยzy	รฃยยyยยzย"ยz{

Plan Retraction

79
	Pรงq	[ML."#!
]"#$?+ K9ยฐย.+$]+ยE;
=u+	=	VK9ยฐย+0+u!.3
&
X
-	Y'+;.+$ย
 U(T(gยj15

2-N	@
	?=	ย/w3EยE=	bN	@VE;P."#$W%'/ =	J4ย.5_vย=	Y&
x
	D+Pb=	V.$?%'
	
!
3*#N	/G$5
^&LI$"#
ยรด?E;>=	+Eย=+b=
'J&
/WG"=	/W#t`+	/G
	\?	.+
-,+		$yzs2
N	/Gq+
xX
LE;
=0=	Z+
xX
LL#"#V"##3
.&tL
?ยย"P
f]
/W/Gf+
	+	#+
*59^H?=	FE_.	:E;	PS=+K=	b	+	!
;+		;E;
Xxw
XX,	("#MSSX

+-	
Mย		$#ย#:
b"+fN>	
f	W/>+cE=	b
-V=	C+q!"#J=	sX
	.
+L'"#$C
C

!
3XXLย "#/G	$รย.:<+ยข
G!
XXTD$!*yยG#@	f+$a+%T=	fL
$		+ย 	/?+
"P
&ย#5

tยย ย 
k 
n

n

ยwr

.

l	ยomn

{



ยrmt9tiย!t

n


ย

ย ยยยl

<

v;=	b	+	.
G3
=	/ย)%'/>;J!.+	+.,	P$=	2-N.$."=*:/>3
.3
-
	b=	b$+."P=
%PD!
8<bF+%w3
-.$"P=G+%wP=	;%/ ร}h>:K*ยฐ(รL%'/ ร}h?:UT(gยj*รC5^HG#
P=	_"}h0
'
?ยH)D&
-,
"#/G-+ย<+f+รญ
 K*ยฐGย
 UT gยjf

"$F=	JE3G,/>+
-	+M=	J'+f
	.+PT=	b'+*yzF	#
=#F
?=	J$+."P=V"#[ UT(gยjf/G$+ย	.+W
 hGyรน_!""#$.ยD
%	P=	#N
	C
Mย 	
	G	EdP_c
 ~$T"#!.3
.#ย#@e"#>Q
f	.+
T	
	ย
K*ยฐ,/G$+F	.+P
 hGyz;""#$P.FG."#
	J	T+%=	TP#N	/GD#</?TEc=	f=	T'+
EF
+
-3XG"#P"#$45

ยฎ
ยจ

รณ
รด

q

 ยยtkยยl	ย ย t { ยrmt n ยir . l	ยomnยย !ยkยย(ย ยยฌ#" Gย ยยฌ#" ยฒ(ย!^ยจ ย ยจยฆ $ [<8+?LD05	YZM<;
ยฒ(ย!^% ยฌOย ยยย71ร<;36<5X;>?;MJ'+f%'9ยkย>ยย ยยยฌ +*ย ยยฌ %'/ ยฒ(ยg^:ยจ ย ยจยฆ
ยฐ ยGย ย ยฉ ย ยยย  ยฌOย ย ย2ร
1 0C2| MW3kยฒ(ย!^% ยฌย ย>,/>+#"=+>ย ย>ย(ย ยยฌ +ยGย ยยยฌ #@e"#!
& ย\?  ยฌOย ยยย7,
1 ยฐ ยย ย:ย ย ยยยยยยยฒยGยยย ย ยฒยg^> ยฌOย ย ยง
ย 36A<P;ย& ย@?  ยฌOย ยW
ยยฒยg^ ยจ ย ยจGยฆ

ยยnPlยยkย0t*& ย\?  ยฌOย ย

ยlยยย%ยยยbย

xยJยซbยฑยฒ'ยซeยยณLI%ย#ยBUKยKwยทbaMยซj4bย*ยณLjkUยยOb

I3ยณLj

TQยฑ	ยณkRKยซ

ยฃ 	< 
";+	2'P
+3w3
=	/ 
<R
M&
/G[9E;T"+GP=	+,x
	.+:+/?+."P=
	M%/>

,=	FX
	.T'+*yzDSDE;
=,=	F
			KD3 54ยZ;P
F=	FX
	.+Pb+E;
P=J=	;$$
D	/,)T+%K/>+#"=	$:=		$+B?
$;f"#	D!
	G=	b	/,M+%8/?+."P=	$;E;f=	b
-			



'3e"#

F+,=	ย


Se"#
-
Q+%w=	M
$C5KvF
$
-G=		/,;+%w/?+."P=	$
%P=fD31+W

-
3*#@		$&
+M	B\+
P.+&
X5
ร3ร

ย

yz{z|~}aยยย

ยย	ย

vย=
b		"#$,#$"##JV 
	+,x
	.++*:		C
.c

-
3_+ยขDS_"#

b	$ยข	
/>+#"=?=	M
			;
-

3*+?DSw#@		$&
;#@e"#5Kv;=	!0C2|MWยถ36@B;2\34		"#$		;D3F
=	sX
	.+PG+=++	$
=	M
			MD3*		b+C	T3P$G
-f=	J*:+V#-$
DSc%P/รฑ=	,X
-	.++\=+,V	J+	)$+b
-Z=	,
			CD3<#@		$ 
*5>vย=	,X
	.
+*yรน4

-
3("#

9+"=	$,&
/,
X'$!TPT/>."=C=	K
-			K	/ย$"#
	!
*51vย=	
"+S1X
-	B(M+G3ยผ$4[M,X
-	Bf
=	CX
	.+Pf+V%<=	C%P/ ยธ4ยบ ร ร ยธOยฝE=	P1
 ยธ\ยบ<=M)
#$A$"#/W$b+V)Z"#

f+%<=	b%'/ ร ร ยธยฝSยe
X%[ยธยฝb=cZ#$4:)=	sX
	B?
.!#X%
"+Z)>P/G+$45>รฏcEยกย"#

C+>SV	$%',+D\	EรD3K%'/>65>v;=
s	E
+A
c+.+$?GW#N	/Gb+%<=	,Dx8f%'TP=	a"#	PDM	/:4		T	X
-B
=	FX
	.b+C

	1	$"#$P+
X,"#/GP5<IsI$"#
Gยถc%'1/W.3
X'*W=	;
+3
+f3ยผ/GT3-
=	/>65
v;=	=	Y	+	.+
->	=b
_


$4:E=
"P=V/G	
XN$c=	JP
$?f>	P("#,
+	!
V'+%bP=	G	Eย	/5Yv;=
'b+	
-
M$fP=	Jx
	.+ Wยถ36A<P0>F;b	
	:
E=
"P=q$"P
$MEc=	=	f=	+EdP>PJ=	C+f%'!J
-V	$R	Db	
	>
'($65
v;=	VR	$
]+O
% ยรซG;3	รซ;<CZf\	E;-i		$ย!+	
ย"PBยข
DPV=	f]X
	.+PV
'
+
/G).+M	:1&
-"#G=3
	?/GPY'+c
Z=	,Vx
	.+/>+B$c=	,x
	.+2'!
ร+3
		"#$C.+BW	35 ยฃ L=	?=	,=+4:<P
	/>+q'+s
ยP=	GX
-	.+
"#$$C=	
"="#$=_	bE;
XX*,G"PD!b/>+."P=fGG	$R	
			c	/V5
^$3Xb=	;Jx
	.+J!=	,"#&
!1+%wMP#+
#!/>3X+%<ยธรR3X
-.+&
#b
Xย*.ยน
+	!
Fยธร"#/G/WG	""#	
	ยนJ	P/>:		TW"=+#"#
U$
X>%8RSX
.+!
+#G
Xย*2
DM+?+%<"#/G/G->	""#	
	?"+fC=+.?G"#/Gb5;ยท+ยฐยณZ/>+B$<	?"#DP
		
>W=	
R	$!
?+%1E=ย!=	?+	$+ย
>P=	TGX
-	.+:+?	;/G

'"3wE;Ba
VI$"#!
fยงY2
	/G$;b	P$/,
	$?'+,X
	#+,E=
"P=,
Q	+	/G$G	
-	YP=	T#@	
-/GD#3w!
35
IGeย dK#-D:4ยฎ$ยฏยฏยจย%+?
xX	/,

	b
-D$!
D+!
ร>+%K=	$M
'	$5

tยย R
k 

n

n

ยir

.

lrzl	ย

ย

t

o ยwย/.

v;=	ย
 ยฐ ยย ยย ย ยยยยยย:ยฒยGยยยL%'	"#
ย
f&
-/,
X+?]
#?	.+
-Z"#	DP+(
 ) ยย ยnย ยยขkย ยยGยฒยGยยย
#@e"#	ย
>P=	M+;"M?+f#$"#$W%';#N	/GF
Q%	P=	#N	$45K^&f=	J"M+%
	+	.
*:*?+P
3K+Z/a
=bG/>B$A%b#N	/GYJ3
+#f%'!
 <;36<07>365XA2:
+\=	?3!
=	/r/Y!bBยP."PBV+%;E=
"P=*5>v;=b=	a%D
-)$"#/G$CVC+%_3
-.M+%
=	C%P/ รยhGK
: ย\รยE=	Pย
 hร
MW+
S1+\++
 ยW
T?/,+<	
	fร
 C5X<;7365	A2:)#
=	
UT(gยjVf
 K9ยฐ58vย=	B
 UT(gยjq"T/W$+;#N	b=	M+?%'	=	3:
->E=
'"=f"b=	9
 ) ยย ยย ย  ยฌOย ย
%	"#
-f
M"3X-$4:	#@e"#f=	,+/G,;
-V	.
*5 ยป 
P$"#
+
% K9ยฐยP$.F
-q?"3XK
) ย ยGยจ ยย ย ยฌOย ยรฉ:E=
"P=?
#N	$#+EJ5

ยฎ
ยจ

รณ

q

ยง

ยถ

รด
ยฆ

 ย9tkยยl	ย ย t n ยir . lrzl	ย ย t o ย*ย/. !>ยkยย(ย ยยฌ  ยฌOย ย $ [<8+fLD05	YยMG<;
0ยจยยย:ย(ย ย ยจยย213Gรยยkยย(ย ยยยฌ  ยฌOย ยรฉ:OK*ยฐ4รb:kรW>ยkยย(ย ยยฌ  ยฌOย ย*:U(T(gยj*ร95
ย ย4ย.  ย ยยnยจo~nย6
ย  
0 ยจยยย:ย(ย ย ยจ,
ย/G	รฏl  nt=ยยnยจlยย ย0t!D05	YยMG<;
ร}w
 :ย รยกย2>
1 !#$"#T+f#/WD;%'/80ยจ:ยยยยย ย ยจ
oM#Bรย4
 :ย รยค%/ย0 ยจยยยยย ย ยจ
ย  G
 
'G+-	
ยl  ntยย	nยจlยยkยt+
ย  ,
ย ร UT(gยjรฑl  nt
 ย ยb$"P=V( ยบ 		$?() ยย ยย ย  ยฌOย ย ย 4 ยงNย ย
ร3ร$ร

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

ยฏ

ยฎ
ยฎ ยฎ

tยย 

nzยvnรฏย

h

/

nPlยยr4ยยl	ย!t

h

 ย!ร

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

ร

ยzy	รฃยยyยยzย"ยz{

ยป 	mรยยบ : UT(gยj*ร0P;0ยจยยย:ย(ย ย ยจ

K9ยฐUl



nt

	fSX*#/G.;+%#) ย ยยจ ยย ย) ยย ยย ยยข(ย ยย ย (
 ยงM;0ยจ:ยยย:ย(ย ย ยจ

-,

n %tmn



ntl	v

^H!$ยข+%ย	
-	q+ยข	$"#!
	V"+3FX
	B	:*P."#
-\/G+$b"P=	+
"#$b/?GE=	]=	
X
	#+ZuEC

3Xยข	.+$45`ย+?R) ยย ยย ย  ยฌOย ยu#$"##Gร/w3Eยก
L=	\"#	
+?+G		QJ=	ย%D
-<3Xw
Xย*KE$	+%4N	@
	J=	 /w$EC:e) ย ยยจ ยย ยG) ยย ยnย ยยขkย ย:ยC.+B$
?	
b#N	/GY"P=	+
"#:*!$M=	>!("P
$$ZP"#	,V"#/W#/G+,=+
#N	/G$:+\		Mf=	C%'
b3XK+%<=	>3-+
XCE$	c=+M=	G#N	/Gb/,
=D
=3b/>5
: K*
ยฐ ร;Eย
=Z
ยป b79
	P qย
XX!.+$:4P."#
qP"#$bVR		?DPV+%=	,%'/ ร}hGQ
ยธ.+PD.ยน?ย
% h?yz,ย3f.$ K*ยฐยb3	fEย
=ยJ+ย
% hGyzC&
X
-	D:*$"=].+$ย
 U(T(gยj15 ยป
	$"P
M#N

W+%;ยธร&
X
-	Dยน;
'K=	T!<+%4#N	/G.FW
 hGyzKDF=+<+P	[
 5cWยถA@BA<)84รซG5	7
36AWh>5QยZJ#N	b
!/G	=
'/ยฅF%'+X+E[

e,
ย t#6OยยLAl8(YZ02W}h ( 024Cยh $ 0<;c
/G	=
"|MWยถ3v5X2m70ยWยถ;
ย-ย ย(36;84Wย0F<P;;@ย
g 3	รซ;<;!5cWS0 ย ย ย @B0885X2FยD<A@ Wยถ36;H84WS5X2Vh ( 024Cยh $ WยถM7รซV3	รซG037A<<;W|8:A2\C5X2F
W36;8\Wรซ0?;l5XC;>24365	70Y240@B;WRยe360ยรย;ย3	รซG;ย7A<<;nWe8A24C;>247;f36A,ยย;{ยธ ( Zยธ $ Z
ยkย!ยยยธยร36A
ยก ( ZEยก $ Z
ย
ย!ย2ยก9ย@ยข
ยฃ>ยยฅยค 5	2รW{0F<;;@ย
ยก ยบร ร ยก ยฝยฅยฆ(h $
g ยธ\ยบร ร ยธยฝWยฆ(h ( 5 ยงb{
ยจ>ยยฉ <C;><5	2GFยWรง0>F<;;[ย
ยก ยบยปiยก ยฝยฆ(h $
g ยธ\ยบ*ยป^ยธยฝWยฆ(h ( 5 ยงยช{
ยซยยฅยฌ 5	24C5	2GF,7A2W36<P05X2\3	Wรง0F<;;[ย
ยญ ยยฎยฆ(h ( 5 ย
ยง ยยรb_ ยจGยบ9ยญ ยยฎยฆ4h $kยฏ ย รซ;<;ย_ ยฉ ยบ[5ยWย0ย?0<5	0ยยถYย;B5X2รWยถ36;H8รยพ ADh (
g ยรรย_ ยฉ ยบย)
024CU_ ยจ ยบ 5cWรง3	รซG;,7A<<;W|8:A2\C5	2GF!?>0<5X0ยYย;ย5	2รWยถ36;H8รยพ{ADยh $ 024Cยย
ยญ 5ยWS0%7A2W360243
ย 5ยW;LDA<Mยรb
ร _ ยฉ ยบ9)
ยญ ยรง02\CGย>รb
ร _ ยจยบ)
ยญย
g Yย5cร;kL
ยง ยยรb_ ยจยยบย_ ยจ ยฝ$ยTยฆยฐh $!ยฏ ย รซ;<;T_ ยฉ ยยบย024Ca_ ยฉ  ยฝ 0<P;
g ยรรย_ ยฉ ยยบย_ ยฉ  ยฝ+ย:ยฆยฐh ( 5 รฑ
?>0<5X0ยYย;Wย5X2รWยถ36;H84W ยพ 024C ยฟรฉAHDยh ( <;nWe8;7>365X?>;Y E ยฏ 024CU_ ยจย:ยบ}_ ยจG
 ยฝ 0<P;l3	รซG;B7A<<;N
W|8:A24C5	2GF,?0<5X0:ยยถYย;WS5	2ยW36;84Wfยพ{02\Cfยฟ ADยh $ <;W|8:;7365	?;>YZE
ย 5ยW;LDA<Mยรb
ร _ ยฉ ยยบย_ ยฉ 
 ยฝ$ยย02\C?ยร`
ร _ ยจย4ยบ[ยจG
 ยฝ$ย ย
g Yย5cร;kL

ยยn Wtiยl	ย

v;=
'J#N

-\
/Gx
$M=+C E;?
!/G	=
'"J'+b=3GP=	ย/GG)i"#
-
C+
=	$+P	$fx
	B(McE;#X 5TรฏcC=+ME_W+c/>3f=$Y"#P$
-	VM+A
D!
"3
.
-	D<+ร
 24A3	)
'/GP	=
"+:+=	+E_$:D&
-"#=	G"+f
Xย*KG	MK/GPT"+3)X
	B	5
ร3ร$ร

ย

yz{z|~}aยยย

ยย	ย

vย=	JR	$!
V	+Ed
$;;PGE=
"P=q$"P
 
"+C.$AE=	V/WS
	,	Eย.W

=	G"#,+%<
3+5Jvย=	a 
/G$b+E;c
c=+9) ย ยGยจ ยย ย) ยยย ยย ยยถยขยย ยGยf/,T)G+
C#X
/,
T+>$"P
&
->P=+"#W=$/>MDO) ยยย ยnย ย  ยฌย ยรฉ5i;#N	/GT$"P
&
-
/>bV
 ) ยยย ยnย ย  ยฌย ยย"+fP$;
fP=	M%+x+E;
	J#-/GD#;#
	f	$??,+*[
g]ยป &
	+,"+3X
	B4:	bV.!
	ย"#.3
-D-

-	ย"#!.3
.F
-$
N	@q+V)Z"#

*5<^&qP=
",3X1P=	a"#.3
-D.FE;
xX1,.$?E;
P=f=	J$
PรยซDยท bLยณ รฑ4I+ยฒยทรฉ
ยช ยธ\ยบร ร ยธยฝ Q 5
	EรชL-Y\"+P3;X
	B4:
P$]N	@i+L)u"#

*5\^HuP=
,"? E;
.
	G"#!.3
.;+G+%K

	f"#P.3
.;	"P
+P$GE;
-=f=	Y!fE;
XX*)
.+$aE;
=?=	b$!:
 PL
ยณ UkUรฐยท bยซ	ร
ยฐ ยธ Q :+?+f.!
	G"#.S
D;+f,$"#f;+%

-
	>"#P.3
.KEย
XX*,	$V3-	,E;
=f=	b	EยคX
	B4:+)+5

g]ยป

 #
	Y"#.3
-D
!$WMN	@ยbP=	$+<#
-=	<G	/W
,;/G
-*51v;=
'
gยยป G
"#.3
-DbE;
xX<?.+$VEย
=iPรยฐ	ยฑTkbยซDยฉ\bรฌร L ยบ>ร:
S L ยฝ\Z L รฏร Q E=	ยยธ4รฏย
b=	?=	$
	
!*5
 C+%T++
+-2'

	q"#.3
-D.M-bE;f.
	\"##3
.c
P$ZAN	@ย
P=	$+<G+.
*5Kv;=	$b"#P.3
.E;
XxwM.+$,E;
P=*Pรยฐ	ยฑ:TยbยซDยฉ\bยร L ยบ%ร:
S L ยฝ\Z L รฏร Q 5

gยยป

 
	+J"3xรฉPO) ย ยยจ ยย ย) ยย ยย ยยขkย ยGย>=	?=	#%'bP."#	J!"=#N	/G$"P
 
*:
E=
"P=V+/G	.F,/G+
	,=	J!("P
$f;+%K.
	:

	>"#!.3
.::)+
X
	B	;%'/ =	J*5Tรฏc
"#C=+T?$"P
&
q"#P$	M"PD!#f>?M+%K

"3
 <;0ยWA2
"#P	$F
\,+*:!G."#
	G>$"P
'&
?%'/รชa+f$3xย/G	.;,/G+
	>G!
+%<"#.S
D#KE;
P=?
D!
"3*.+D6:(S	,E;
=fP=	#
	"P
+$Wx
	B(;+5
v;=	b	J#@"#	
G=
c"#$)"#C
;=	s%รถ"#=+P=	J$"P
&
>	G
G'+]ยH$ยฑ
 PยL
ยณ UยUรฐยท bยซ	:
ยฐ ยkย
ย Q ย;
'S
 0YuยL0E>W/>aM+M+%_>$"P
'&
\f	\,x
	BZยHP$
PรยซDยท bLยณ รฑ\ISยฒยทยฑ
ยช ย
ยkย Q ย.:,=	$ME_,$"P
&
<=	'G)b."#P$G;b3
-_FE;#X 59ยZTEย
XXwP$+
=	JE;GM+.+PJ$"P
&
6:			T	3-
X=	/ยกE;
Xx1	C=+T?A
;/G+$W%'/r
+VcDVF
.Q"+3*x
	B,
;P."#$45
ยป =		=f=	G"=	+
'"#J+%_G$"P
'&
>P."#;
'/>J	P/,

!
"3X:(
T"+		M
/>V+P
.+!
ร-:8 
"#f=	f+		G"#']	,=$f	.+$ยข=	V$"P
 
C
L+ย.$5
7	?#@+/G:;E=	ย	
X
	]+B
 h>:;=	+		?/,
=G=3\"#$+$0X
-	^
B ยธ4ยบร ร ยธOยฝq+
+ย
DP("#$\?M+%<.
-	>M

	f"#P.3
.ย
 ยฒร?	$"#M=
ยX
	B?%'/r#
-	
=	$+P	$]DL+	=	>!^
 ยธ4รฏ5ivย=	V#"#
]3!
=	/ย/,WV+fPqP."#,#
=	
$"P
&
ยขย #C=	bx
	B>T=	,"##3
.#ย.:		M=	$JE;>$"P
 
M+b	b/G/G
'"+5;^'%
ยฒ 
;#$4%: ยณL)$"#/G$;=	P$+	$?+D3
*:		;
Xยด% ยณยข
'_#-$4:=	*
 ยฒO$"#/W$	H /	5
v9Z	$"#f+DS
C$3
-	Z=	V'+uE;
-=`	) /	fP:FX
	B	:Fย"#.3
-D.6:KE;
3X+E0=	J3
=	/ยกW."#;-f=	DJ$"P
 
;=T+!
 ;nยย8:AยWยถ;C5F^H%'/>3x:	G$"P
&


F#@()D$W
X%*	GP=	ย"#.3
-D.9
->=	b'+ย)f?=	b"#P		$?,=	T'+>
=+$"P
&
*5<v;=	M%'/>31#N
!
f+%K#@()D$W
c.+$W
-f/>F+[
% <;0ยWA2OW8Eย
=
VC+*:
&
"#YE;J	$V+)+Y$"P
 
M	V"#!.3
.;P>G'+V=T+C.+$AE;
=?
'D
'"3
$65
ยป

ร3รรฆ

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

-,
ยeย ยก

ยยn %tiยl	ย

ยฃย ยก

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

#6ยถยตp<;0ยWA2(ยก 5cW#@()D$ย5X2 8Yย02Vhยก5 D
5cWSADS3	รซG;vDยถA<@ยทรP ยฐ	ยฑ:TยbยซDยฉ\bยยธ4ยบร ร ยธยฝ Q DA<ยWA@B;BYZ5	2รlยธ4ยบร ร
ย

ร

ยzy	รฃยยyยยzย"ยz{

t

ยทPรยซDยทbยณLรฑ\ISยฒยทยช=ยธ

5cWSADS3	รซG;vDยถA<@

Q

ยบ รร ยธ ยฝ

ยธยฝ

ยฏ



DยถA<{WA@B;BYย5X2Oรยยธ ยบ ร ร ยธ ยฝ 02\C

ย|0ยขThรฎ7A243605	2Wย24A%<;0ยWยถA2mADS3	รซ;รDยถA<@8Pรยฐ	ยฑ:Tยb1ยซDยฉ\b ยธ ยบร ร ยธ ยฝ Q ยฏ 02\C
Hย ยยธรฌ
ยข ;5	3	รซG;<,ยธ4ยบ{8:0<365	7598:036;nWร5	2ย024A3	รซG;><ยYย5X2Oรยยธ4ยบ ร ร ยธcยน ยฏ A<,ยธ4ยบ!CA;nWY	V088;0<%5	2
024E{8(<PA36;7>36;C%3	รซG<;03.ADB3	รซG;vDยถA<@ยทรP ยฐ	ยฑ:TยbยซDยฉ4b ร%ยธยน*ร ร ยธยบZยธ4ยบร Q ยฏ 
ยจย ยก

ยทPยยณLUkU(รฐ ยทnbยซ	ยฐรฉยธ M
Q

5cWSADS3	รซG;vDยถA<@

Rh

024C

7A2\3605	2Wย24A%YZ5	2ร!ADS3	รซ;รDยถA<@pยธ

M รร ยธ ย

v =	GN.!a]=
.]"$,+W%H3
ZP.3
=%'+E+4[G"#.S
D#b=+,$!+f=	$+
ย
"+V3-Eย3	K)J#"#$4:	+fGPV"+fG)J/WS$,
X%
;	,	;+
"P
-+$
-V+D
"+S*X
	B	5
vย=	$"#f"J$$_!/GM#@(+
*:=	+E_$5Kv;=	MN.	4"J3	<P=+_sX
	B
"+		Fb#$C%'/ยกM+>	,=	M+T"#!.3
.9
W=	+W	$"#
	M
-K%'/
,=	$tu=	Eย
M=	J"##3
.<	$A,$+-TP=	b=	$+;E;?)$"#/GJ	) /	5
v;=	T$"#>	4"M+.	F+D3
-K=	;%'+X+E;
X	J)$"P
3*"[K		TP=+ยฎ
 hd"#D.S

E_cX
	B	
: ยธ4ยบร ร ยธยฝ_+,
 ยธยน<ร ยป ยธยบ5K7		=	P/G:		)D;=+ร
 ยธ\ยบ$,M=	$+Kl
 ยธยน<ร ยป ยธยบ:+		<
	
K$"P
&
-J$!+$C=+=	$$5 ยฃ 	/a
=1)_P/G	$Cb/G+F=	FX
	BBยธ4ยบ>ร ร ยธOยฝ_+
3	bEย
=,
KP=	Tร
 ยธ4ยบ:D&
"#ยต
 ยธ4ยบ4E_,	b-	<T+Y		PDc
GP=	+*5L~F	_+
-	
fE;A$3G	) /	J!"#	C
ยข=	,+*:*/G#VP=	a"#.3
-D.c=+bE;G	$
,$+-=	b=	P$+ย
 ร%ยธยนร ยป ยธยบGZnยธ4ยบรJ5 ยฃ 	T#N
-
W%_#@	$?+.D$QN.!_P=+Y!
E;
XX<)>P/G+$E=		C
J"#$$sq!>?		)DW
Z=	W+*yzC"+3_P"#	fยรถ
 5ย5
E=		Q
.9!X
	Bb
'K/G+$)ย#:+		<=+<
	JJEย
XX		K$3	) /	;"#P.3
.

f=	b*5
รฏc+E`P=	T#;
?E=
"P=V$"P
&
;"+f)JP."#$?"+fJ!.+$?&
/G-w[K,$"P
&
V"+
bP."#$?-,
X%*
#_	"P
+$W$?
F#@()D$45 ยฃ )
	?=
F#
	,/G$+;P=+;=	
+\E;
Xx<	Y"#.3
-q	) /	,"##3
.:)X
	B	:4J!ย$R
-S3E;Y/a
=J3
=+."#
	-b#@()D$,$"P
&
K"#$)	P=	_P.F.
CE=
"P=ac	.

+		S
 @B5ยFยรซG3vรซG0?;B@B0C;M=	DJ$"P
'&
;
+
SXX5
รฌ<.3
-
	b."#
GPJ	""#	;
?=
K.;/,
=_/ยก,)T+-YP$
"#!
:,E;
/>+B;E;
-/GP.+K++
65179
.!$:$	=+KP=	.K+%4."#
C
ร
 2\A3	"#.S
	$
J)b=	M.M+%1P=	T#_!$GE=	fP=	X
-	.+,+?E<"#P$+$atย,=	M.M+%
A24;c+%1=	,$"P
&
	2'.
	<P=+ย
 7AMYยCJ=3M\$?P>"#$+bP=	MX
	.+W+*5FI$"#4:
E_?
$"#b=	GP$J\I$"#!
LยฆD:9E=
"P=\#@(S
b=	+Eยยฉ9ยช*ยซ	ยฌf3
,.+
X$:4"#	$ย
ยท+ยฐยณZ=		
!
":	"#&
$Pf=	$b$P
"#
<f"#
	G;/?"#G).+.5
รฏc#@(CE_W	$J=	O
 ) ย ยGยจ ยGย ย) ยยย ยnย ยยถยขยย ยยf%'	"#
*5fรฏc
"#,=	+Eย=	?#N

ยข/,
.
=++%K
-.;	.
J"#	ย
 ) ยย ยย ย  ยฌOย ย*[K=	s+Pย"P=	D$Ml
 /w3Eย+A	MCX
'
=+K
-"P$3X4DP&
E3	K+%*N	@
	C
$:=	c%P/G_"P=	D$;+>#@	$f$"P
&
*:4<;@BA?>;W
=	J"##3
.F=+;

3XCN	@	$?
3:(fR		$T3xรฉP=	J3
ME$	F+%N	@D
	a
$5

ร3ร
	

ย

ยฎ
q

รด
ยถ

ยย	ย

 ยยtkยยl	ย ย t h nPlยยr4ยยl h ne,Wtmn  ntl@ $ [<รญ)
c+% ร8+*:oc
$"#!
kร
)\[ยรd#$"#M+f#@()D$A$
ย  =	Pb
F	G#@	D!$f$!Xl  nt=ยยnPlยยkย0tT3e5D5
ย7)
0 : N ย[ยรยผ) ยยข ย- ย L ยGยจย ย ยยยจ ย ย ) " ยง
ย ย ยยn:ยยlร} N Z K9ยฐ:ร
 ย ยC$"P=f+(NยฝN 	P	$fj= ยยจยจ ยGย ย>0 ยฌOย\L? ย 0 " NยยงSย ย
ย   NยฝN 
F	;
/G	=
"cOย
 l  ntยย ย ยยnยยlรย NยฝN Z UT(gรj ร

ยจ
รณ

yz{z|~}aยยย

ยฆ

ยยnยจlยย ย0tq3xรฉ'+*:
$"#!
f3
#_"#X$"#$W
?x
	$;รณ>+fยถe5

v =	fEย3ZPqP/G+>P=	V"#	PยP("P
'+$ยขE;
=u+]#@	D$รจ$u	WL=	
ย
D)>%ยP=	>$*5Vv;=	W%'	"#
a) ยยข ยe ย L ยยจย ย ยยยจ ย 	s=	!/w3Eยฅ!("P
$\E;
P=Z=	

			b$qME_#xQc=	,+\		"#$\V/WS
	?=	G+			!
+J"#!.3
.:eX
	B	:
+Z!5fรฏ
"#G=J=	>"#	X
	V)E_ยขX
	B\+]ย$"P
 
M
'b/>W=	[JEc=	
=	sFX
	BfP>GA
#$A=	Y!f
c#$fPD5;7	=
;P$E_J?	=3C
=+GP=	>"G%ยP/G+
	VA$q%ยP=	,%ยพ
/ PยL
ยณ UkU(รฐ nยท bยซ	ยฐVI Q [,fZ)$"#/G$C#@	D$
GEc=	Vbx
	B,
;#$4:e		_P=
Q%	"#
-f/G+$Q=	JW
/G/G$
'+#5FIGC$?+%
=	M%'ยฟ
/ PยQ
ยณ UkUรฐยท bยซ	ยฐVI Q E;
XX*	T	$+#@	$?
Va+*5

ยฎ
ยจ
q

รณ
รด
ยถ

ยฆ
ยง
ยฏ

ยฎ
ยฎ ยฎ
ยฎ$ยจ
ยฎq

 ยยtkยยl	ย ย t h n  ย o~nkmlยยkยยlยยkยยn) "  $ [;ย793EJ:D8+4ย
ย  W
) 
;+%K=	M%'/ยฟPรยฐ	ยฑTkbยซDยฉ\bรฌร%ยธ4ยบร ร ยธยฝ\Z L รฏยร Q l  nt
0 [ยรยนร%ยธ4ยบร ร ยธยฝ\Z L รฏยร

 N [ยรยY"#D?+%#
oM#c%'/8*
N 3X1"##3
.K.$?E;
P=;)
ยยnPlยยkย0tZย7)
0 :% N ย
nzยvnรฏย  G
) 
F+%K=	M%'/ยฟรP ยซDยทbยณLรฑ4ISยฒ-ยท+ยช ยธ4ยบร ร ยธOยฝ Q l  nt
0 [ยรรฎร:ร L ยฝ

1N [ยรยY"#D?+%#
oM#lยธ4ยบ>ร ร ยธยฝc%/8GN
oM#c%'/8 N 3X1"##3
.K.$?E;
P=;)
ย  รฉ
N "#D.S
F	,X
	B?+%K=	M%/ L ยบ>ร:ร LM %'T+f LM +A#@(	$P&
Vรรฎl  nt
# L ยบ9%/8GNยฝ4
N 3	,E;
-=V3X1"#P.3
.K#+$?E;
-=:P ยณLUkUรฐยทbยซ	ยฐ L ยบ Q
ยยnPlยยkย0tZย7)
0 :%XN ย

v =
'T"#"P-$b=	>$P"#
	
V+%;=	>ยท+ยฐยณu3-
X=	/Vย4E;,	#@	b#@e+/a
	,=	G3!
=	/Vyz
;
%/>3	
-$:	+
	C=+Q
K
F	Aย +,+W
<P	;"#
P	$<C+	
-GC=	

				
	G	-/fย.:	"#/GPJย'
x%1=	s
{
 024Eb+-	
?,=	M
			'+	
	G	/V:
ยท+ยฐยณuEย
XX<3X-fN
3:*D.$M+%=	aX
	.+PV+ย
J"=	D$sV	+	6ย.:1	/?+
"
ยH=	J		.+
>E;
xX*	T"#&
'T,+!
3*+f/GTP=+f"#+ย.5

รยรบVร

ร9รรฏรฝk}^'

รฝwรฟ รผ รพ รผ 
" รยถร

+ "zร"

ร

" รยร ยร รผ รพ
รผ 

ร<ร

รยรย"\}eรร!ร	ร

ร

v*,	+c%'/>34	)
$;%1=	Jยท+ยฐยณZ3
=	/ยกE;T)+
f>"=."#
-U#
	Gbx
X%P$G#&

+%YP=	q	.+
q3
=	/ #)$0D0~\" ยป X-$f+ i;D!DX
-$yzAย.ยฎ$ยฏยฏeยฎ+ยY3
=	/
ร3ร
I

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

ร

ยzy	รฃยยyยยzย"ยz{

ยH=	$S%,"3X$\ยยท jkIยฐ	ยM
ZP/>M+%TV$."=ยข=		=ยข=	ย!"#G+%+!
3K+5?ยZ?=	
"#&
'>P."#
u,E;#X 5ยv;=
'>
P"#&
L!$?/>+ย+%JP=	q"#"#	.fL/?,%'/
I$"#
V
 qG$"#

-	G+?+	
	G	P/>5
รฌF&
,q
-$"#$].	=ยs
L79
	\ยฎ?E=	fV		f	$!D.,+L+]+L+."
	$!D.?]+	2'P#N	/GD?).+35ยยZZ"+0#N	Z=	Z"=
xu%YZ		]ยH+4R
ย hG:
		ยผ$"#?,	/a


'"T"P=	+
"#:K%'+X-SEM[

-,
ย t#6OยยรซG;b"=
X'รฌADS0ย8(YZ02;hยก0<;B;ยย0736YยE!3	รซ;W;@ย
ยeยWร Dยถhยก5cWS7A@[8(YZ;>36;B3	รซG;2m5	3LรซG0ยWS2\A17nรซG5	YZC<P;>2 ย
ยฃยOยฉ 3	รซG;<Lย 5ยW;lWยถ;>Yย;7>3LA24;BADยhTร WSA8:;>2m7A2\C5	365XA2OWรงA<S3	รซG<;036;2\;C%Yย5X2OรW ย
ยจยWร Dย3	รซG;%7รซA5X7;!5cWย3	รซG;%A8;2ย7A2\C5	365XA2 ยฏ ร ร ยธยฝ ยฏ 3	รซ;2ยhTร WB7รซG5	YยC<;2ย0<;%0Y	Y8Yย02OWย3	รซG03
702,ยย;[7A2W36<M736;ClยยถEย0CC5	2GFย0lYZ5	2รรยธ ยบ>ร ร ยธ ยฝ ยฏ 02!A<C;><5	2GF{ยธ ยบ ยปยผยธ ยฝ ยฏ 024Cl0ย@B5X2\5X@B0Y
?0<5X0:ยยถYย;รยยถ5	24C5	2GFร7A2W36<P05X2\3ยฎร ยฏ 
ย รซG;<;%ยธ4ยบS5cW1;>5X3	รซG;><102รฌ;ย5cW365X2FยW36;8ยA<%0m24;
รย YยE
7><;036;C%W36;8 3	รซ03[702ย7A2W5ยW36;2\36YยE%ยH;%A<C;><P;Cl8<5XA<!36Aยยธยฝ ยฏ 024CV3	รซ03f0CCยWBWยถA@ย;
8(<PAยถ8:AยW5X365	A2ยi ยฏ 
ย รซG;><;Siยl
ร รbร ย
ยซยยฉ 3	รซ;<Lย 5ยW; ยฏ 5 Dร3	รซ;รฉ7nรซGA5	7;m5cWร3	รซ;V3	รซG<;03 ยฏ ร{ยธ4ยบ>ร ร ยธOยฝ ยฏ ยธ4รฏยร ยฏ 3	รซ;2ย3	รซ;รฉ2\AC;รรซG0ยWV3	รซG;

ยยn %tiยl	ย

7nรซG5	YZC<P;>2VAย3605	24;CยยE

ยe0รยข
Hย ยยธยข
|ย 72ยข

0CC5	2GF%3	รซG;BA<C;<5	2GF%ยธ4รฏLยป^ยธ\ยบ
0CC5	2GF%3	รซG;BA<C;<5	2GF%ยธยฝfยปยผยธ4รฏ
0CC5	2GF%3	รซG;!A<C;><5	2GFยWfยธ ยบ ยปยผยธ รฏ 02\C!ยธ รฏ ยปยผยธ ยฝ 5X2m0CC5X365	A2m36Aย0ย@B5X2\5X@B0Y(?>0<5cN
0:ยยถYย;,ยยถ5	24C5	2GF%7A2W36<05X2\3
3	รซG03\DA<7;nWS0Y	YยDยถA<@lW ยน5	2รยธ4รฏ WS0CCV024C%C;Yย;36;
Yย5cWยถ3 i รCA;nWยถ2 3vMG2\5 DE L5	3	รซ

w
eร
ยฏรยฏ
cร
Oย Tร ย
ยฏ ยร
8(<PA?5	C;C%3	รซG;nWยถ;B0<;B7A2W5ยW36;2\3#L
ย 5X3	รซย3	รซG;!7A2W36<05	243	Wย7M<<;2\36YยEย5	2Vh ย

~ " ยป X$b+mi;D!DX
-Jย#ยฎ$ยฏยฏ	ยฎ+ย"P3
-/รช=	PY	P!
$M+%_P=
M	$.+!
qqS2
\

-=	/V[
g

I		$[<s$3%	(J"#P$	;YC+P
34+*:	+>"#/G!
?+%1E=
"P=?E;
XX)

%H"#+!
&%'G=	M
			MD35
g

รฌF/G	$Pย
[ 02\E>'+L=+>!+$,P=	f+	
	Z	P/ 
a$3X
U$ยข
-u=	f#+	=
 Yq$3%b	(5uvย=	#%V+iP.+ย%?$+."P=
	\=	f.	=L=+,
C.+$

P>"#&
Mf		,3X?E;
Xx*NV?+	!
f?=	J'+	
	G	/ยฅ
x%<	
#@
#5
g

I	/?+
"P
&w[cE_V
'
"#M	($
Z=	,#+	=Z	$!Db		2-
'/GP	=
"C+:+
%'	=	P/G:=	T.+	=G	#+$WD>C	
	J	/ร
<C5Kv;=	P#%PC$+."P=
%_=	G'+Z.	=Z=+,D$s	J)$+YA	(WE;
XX<	,"#&
,f+P
3<q
DG%1
#;#N	/G.;/Gb=+f"#5
ร3ร
X

ย

ร
ย

ย
k

yz{z|~}aยยย

ยย	ย

ย ย9tQย'tmnยvยv

v;=	,		$c	)?%Mยท+ยฐยณZ%'+X-$ET;
$"#!G%'/rยทยjkIยฐyรนT		$:4&
"#Y!		$ย

	Ta	P ?+%K=	J3
=	/Vyz;$."=V!.+:D		M"#/G/G.;->fP=	b+	C+%1-$3%
		$;ย "#/W#ย.5KI+
-"#ยทยฐรยณ\#N	$F+K+G!+	
9
G=	TP+/G;E$,<ยทยjkIยฐ:ยท+ยฐยณ
,
;	45

ร ย รA ย  .

ยnยจlntmnยvยv

รฌ</W	$:$"SX :	"#&
!.;+%KE_,"P3
/?[
ยฎ5;=+bZ	
VP=	G+	
	f	P/รฑ
'M$3X
U$ยขb?$S%_		>+%=	W.+	=*:
+
ยจe5;=+;=	J$+#"=V3-
P=	/รฒEย
XXwDSX?
 
;W$3%1		b
fP=	b.+	=*5
vย=	N.!_"#
!
G"#b+DS
>$<	_?>=	ME3J=	M.+	=W
F$+."P=	$4:=	2
%'c
F
KP	b+%1ยท+ยฐยณZ$"M
F
F	M+%8ยทยjkIยฐ5<v;=	b$"#f"#

W
8$;"P$+$:=	+E;$[
ยท jยIยฐY/?+B$<!	
-_"#+.Q=	T
P.+	=WD>!.+
-	J+<=	M_,#@(
	,=	T.+	=
ย
+EEย.A
ยf	/>+!
"s%รถ!=
*:*Ec=	$Jยทยฐยณ`..M+Y+Z+P
.+PV
DM
-Z=	G#+	=
+?.3#$Q
;
f)=V
$"#!
5
 h ยบ 	2
ยป 	PD+%M+%"#/G-	$G+/W	D#bZ/G!.+
-	V=+C%'G+Z+
'3_ยฑ
$
	]=	V)+
	
	]
DW%'>ยท+ยฐยณ4tuP=	ย"Zย'x
	.+]+4ยG		X
-$Lu=	
+3
/G$"P=+
/,tuP=	>3
X=	/รชEย
XXKDP3XfP."#J"#P.3
.ย%/รช=	W+\	D!
XK
M
 
.
=	MD_		CยHDxรฉ'+4ย.:Df+
	Y!aSb
-/GX
$F=+Q
;E;
XX
&
-_3X1		P$F+%*=	b
	(,;E;#X 5~M%'/>3xG.+$4:eE_b=3[
kR

n ย ยยn 
ย 5X3	รซรฌ0ยYย5)ยยถ<0<E!8(YZ02ThยพLย 5	YXYร70MW;V;>?;><E,8:0<365	0Y
ย 6*ยต 70YXYv36AVยฐ ยย ยยG ยฌย ยiL
xยe;>?;><E!2\AC;B5	2V3	รซG;รง8Yย02ยF<084รซยC;ยร24;CยยEWhTร Wร8Yย0242\5X2F{8<AยYZ;>@ยยข136Aยยย;B?5cWยถ5	36;C ย

8Yย02

ยZc,
"#
+P	/GKJ	+c=
KP=	/V:=	+E;
	b=+F=	b		.+	=WD$
+ }h ยบ 
"#/W#G#@	P$4:(?=+;=	b3!
=	/ยEย
XX%'+X-SE0,+=?	f,=	bMยHX
+4ย<#@	!
	,f		.+	=?
-f=	J		"#$5
ยZf)+
ยDu/G.+!
	
%/>3X-Z=+?ID ยป yรนG/GP=	(รจ+% <;245	2GFfย+
Sย'+
ย 	
	?"#.S
D#_;	$f,P."#
	eย1
';$R
-S3W=	b.+	=V!$+."P=V	.B
DLยยท jยIยฐ5LPย iย$"3X=+Gยท+ยฐยณย.+P$J]/>+
	
	qยข$+."P=Z%'
,E=	>
-$Y
ร}h>\
: UT(gรjยทรO+m
 รยhG(
: K94
ยฐ รJ:"#P$
	>P$$"#
-#?>	
	?+V#!
	>"#P.3
.
%Pร
/ h>5รย

ASยrย  ย 6 kย รซG; ;2\36<5	;W F;>24;<036;C ยยถEยยทยฐยณร W 8<A7;WW5X2Fยน02ร;>2436<E AHDยน3	รซG;รฌDA<@
ร}h ยฏ UT gยj*ร 7A<<P;nWe8A24Cรฌ;nย07>36YยEรฌ36Aย3	รซG;VยทยjkIยฐยF<084รซ AD!80<365X0Yv8(YZ02Wรฉ<A>A36;C 03Wh ยฏ
0ยWWยถM@B5X2Fย3	รซG;BW0@B;17nรซGA5	7;,5ยWB@ย0C;!0ยW!36A(ยรซG03 7A2\C5	365XA2BยeA8;2ยA<S3	รซG<;03รยขร36Aร<P;nWยถAYZ?>;
03ร;07รซ%Wยถ360F; ย

^&Tยย"#$;Pa!=	SE =+;=	b	Eย
$F	#+$?ยยทยฐรยณย
f$)b>fDPG+%K=	
%/ ร}h?\
: UT(gยj*รย"#P$VP>=	G/GJ+P
31'+P=+"#/G	
ยh>yzM"P=
XA
\=	
.+	=?K#N	$f++Mย K[
 qqยฆย.5v;=	E;c=	+#Kb=	M#N

*<
[ hd"#/G: h
#N	$fย"P=	D&
	>f\"#

>?+
 %
: hย#N	$ADf"=	D&
-	>,=	$_PG$+-5
^H,=	;";=+l
 hu
'1"#/Wk: h`=*	J"P=
X*:++sX
BE;
<ยทยฐรยณ?/a
+$9	.+!
	
	G	Ed
-$5
ร3ร
p

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

K)

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

ร

ยzy	รฃยยyยยzย"ยz{



ยฃ P=	E;
;ยท+ยฐยณV"3X' ยย ยnย ย ยฌOย รฉ
ย :Ec=
"=G"P=	D$KT"#

CM$+_+C	.$	E
UT(gรj>
$:+	%'K$"P=G&
c$+	!
*5*รฏc=	P#%P=+< UT(gยj?DPJ	.$
A2\YZEยตUT(gยf
j 
$6ย
->P=	_E;#	<P#N	/GDcE;
XX4G	#+M/GM#N	/G.ยผ;


$"#P$?+=?
f=	C.+	=?-$	;G""#$ 
#?/GJ"#.3
-	$?+5
^&,=	$"#G"!ยY>"#
-
s
K"=	*ยe) ยย ยnย ย  ยฌOย ย	#+$	E^UT(gรj>
$
%'SX8#@
!
	VM)D&
-f	
Mf=	,)ย"#

-q+q%b3X<"#
-P=+J	=	
)q"#
!
*yzF	)D&
!
*5Kv;=
;"#P$	c#@"#GP>",Pย q(ยQ+5
^&G=	;'<"Mb=	$<"#

>ยMX
	B,+Gs=	$+
-	Y4ย4
'<"P=	D*ยe) ยยย ยnย ย  ยฌย ย
		s=	G.!
	Db+c
 ~$b

	q"#.3
-D.c=+C	b=	G=	P$+$:*#@"#\c
ย"
ยHรณ	ย<+)S5
ยญM3
	b
XN$W=+_ยทยฐยณq	.+$Q=	
-/G/G$
+PT"=
xG+%Kb+!
3+W
>C/>		
$R
-S3fuยยท jkIยฐ:J+รจ%'	=	/W\=3
	L	$0=+?
-fD#?=	$ย"P=
X0 =	
%PD!
GE;
P^
= UT(gยj0.+D,WE;#XJย !Z=	#
>"P=
XPuE;
XxcSZ)q#@	$)ย.:;P=	f%'+X+E;
X	
/G/>s%'+X+ETF
$"#-G%'/rรฌKS
/รฑยฎ+:	=	J"#/G	$ย+%Qยยท jkIยฐ:fJP$
"#!
?
=	J$+#"=V3-
P=	/ย	$A#+EJ[
o n ยฑ r ย 6 ร Dยยทยฐยณ ;>?;<ร0CCยWm36Aรฌ3	รซG;BD<A24365	;<ร3	รซG;ย;2\36<Eยร}h ยฏ UT(gรjยทรร3	รซG;>2ย5	3RยL5XY	Y

<hรย|5	247YยMC5	2GFยh

;?>;2436M0Y	YยE!;ยร8YยA<;!0Y	Y8:0<365	0Y8(YZ02WS7A243605	24;C%5X2m3	รซG;SF<084รซV<AA36;C%03
5X3	W;Y D

kยข ย

ยฃ 	G/,b)G	$"P
!a	bE=M
b/G$+M]ยธ##@($ยนf?+
'31+*:9b$R
S3-D-
\ยธ.
&
-.ยนC=	Y"#P$
	G.	=V		5รยฐ ยOย ยย ยฌOย ยf"#D#3
C-DA
fE=
"P=f
-T#$"#.c+
DPb%'/ย=	ย%D
-;ย'
 5ย5b+V
 ~b
$"#
YS
ย.:"P=	$"B	
-%<"#/G	$ยยH/a
+
X	

X%w	ย#:S+s=	Eย
<P#N	$=	<*5<IWยธ.#@	
-	Dยน;cยธร"#&
'
	Dยนc+C/G$+K#-$"#
	
=	*yzDP,G=	T!$+."P=,%PD!
$5*รญ4/G/?GยฎT"#3XC#X
$>J$."=	2H"#DP+e!.+
=+Q
K.+$GDSXJPY"#&
'_G,?=	%'
351v;=
F"#$!	F,
$+."P=>P.+bP=+<E;
xX(DSXJ
&
-K,	(c
Gb.+	=G+
-,		=?
/G#tL
,P=	
E;.	:	b=E;
XX*	)\+?
N
J/G	_%8
-/G
-VG		.	=VE;
P=		;#@(
	
=	,+$M+%=	W.+	=*5fยยท jยIยฐyzM
-.+
-2H
	\$+."P=ยP.+=bP=
M	)ย
D$Mยท+ยฐยณ<yรน<	P$=	2-N.M$+."P=*5
v;=	K<"%'1"#/W	$)%'+X-SEM
$"#!%'/ยรญ4/W/>bยฎ<+MP=	K%H"#4=+kยฐ ยย ยยG ยฌย ย



'3XXG		.;)ย
= ร}hยบ\
: K*
ยฐ รยm
 รยhรยบ \
: UT gยj*รยf=	M%PD!
$[
o
n

r 6Vยkรซ;SWMOย)F<P0ยถ84รซร<A>A36;Cย03KhรยบยยL5XY	Yยย;vDยถMYXYยE!;nยย8YยA<;C ย
ยฑ a

รฏ+EยE;b"+q!.+M=	M
"#
-V"#

VC/W/>	[
o n ยฑ r  6 ร Df0{80<365X0Yย8Yย02Oh 5cWkDMGY	YยES;nยย8YยA<;C ยฏ 024C9hร 5cWf3	รซG; 8:0<365X0Y8(YZ02!F;2\;<036;C

*ย
รยข
รh ยฏ 3	รซG;>2=3	รซG;,WMยeF<084รซ
<hร;ย
ย ย
v;=	s%รถ"#c=+9hยฎรb
'T"# 
$\W$M+%<GP."#
-?%P/8hยก/G$+c=+P=	J
ร}h>:\K*ยฐรยกEW"#&
$4:F$-
	
uZ"3xยPT) ย ยยจ ยย ย) ยย ยย ยยข(ย ยย\%'/ E=
"P=xhV
ร E
	.+$LJP=	f+G	((hยK
N 
LP=	V"3X+) ย ยยจ ยย ย) ยย ยย ยยขkย ย:ย)5ยv9ย!=	SE =+Rhร yz
0ยW!0V<;WMGYย3รAD |24A2\C;36;><@B5	245cWยถ365	70YXYยE 1<;36<07365	2GFร0V7รซA5X7; Dยถ<A@
<PA>A36;C103
L5	YXYยย;vDยถMY	YZE,;ยย8(YZA<P;C%0ยW L;Y	Y

		.+	=?
'F%x>#@	P$?E_b	$AG=	+Eย=+
ยฎ5}h

ร


;
&
$4:

ยจ	5;=	J!		.+	=f)+
	
	f+}hย
Q%'X?#@(-$4:

ยดh ร

q	5;=ยSX*+%

yz"P=
XยA3	รซG;<;=(hรฒ+M%'XW#@($45
ร3ร
{

ย

yz{z|~}aยยย

ยย	ย

v;=	MN.!;
;P	b$"+ย
 ) ย ยยจ ยย ย) ยย ยย ยยขkย ย:ย,	#+$;=	bm
 ร}hรD(
: K*4
ยฐ รb:	Ec=
"=f/W$+
=+9hรYE;
xX13X->)Y
&
-$45Tv;=	G$"#q"#

A
;=	C
"#!
V==	$ 
5bv;=	
=
#ย"#

-ย+/G	.?u/G.+!
	L=+\ย.ยฎยG=	ย"P=
XยP		$0`
 ) ย ยGยจ ยย ยcร
) ยย ยย ยยขkย ย:ยf"#SXf	$!D9 h ร yzT"P=
XPยT#N	$Z+:4+ยขย ยจย;=+M=	$G"=
x
E;
XxรฉP=	/>#$ยM%'X?#@($45
vย=	bN.c
;$&
X?
XN$4}
[ ) ย ยGยจ ยGย ย) ยยย ยnย ยยถยข(ย ยยG
/W/G$
+#->"3Xย
 =GยยจGยจ ยย ย%0 ยฌOย@? =	
/w3Eรซ
q"P=	D$0."#3:bE=
"P=ย
'f#@"#ยP=	Z%'	"#
ย"3X-$ยb
 ) ยย ยย ย  ยฌOย ย0ย2
$A=	m
 /w3Eรฑ
-ย=	\N.!V"#5 ^HO=	E_.	:M=	Z	Eรซ	($	.$ย%'(
 hรยD
ย

ย
ย

ย
ย

ย
k
ยข
ย
ย
ย
ย
ย
O
ยฌ
ย
) ยยจ ยG) ยnย ย:ยV+PJ#@"#=	DC=+bE;AG	#+$Dย
 )
ยย 
ย*:	E=
"P=\D
รฌKS
/รฑยฎJ+W
 hรyz"P=
X*5
 hร
.!#X%_+C		b
ยป ย%M=	G"P=D
x\#
	?%'Xf#@	P$4:*3X1=	G"P=
X\#@"#	M%
=	b%'
;E;
-=ยร
 UT(gยj\.+:	+A=	#%'bยรญรฉ/G/>ยฎbE;
XX*)b%'X?#@($4#
5 hย
-.#X%*
'
%x>#@	P$?Df	/W	
*:E=
"P=V"#"P$c=	b	+%K+%<รญ4/G/>!
 qe5
79
SX\E;>	$รจZ/G#+?=	V"3Xm
 ยฐ ยย ย:ย ยฌย ยwรย h.ยบ'ยb3X-Z."#.C\=	
.+	=*yรนf$5r79
.+%>3X :M=	\N#q"3xYรฌ
 ยฐ ยย ยยG ยฌย ยย	.+P$V+ย0+%G=	ยข%'/
ร}hรยบ (
: K94
ยฐ รb:1+		"#$&
	\+\DPV+%;=	C%/ ร}hรยบ (
: K94
ยฐ รย	.+P$b+\DPV+%;=	,%'/
ร}hรยบรร ( (
: K94
ยฐ รJ:E=	ย
 hยบรร ( 	$!D.ย=	b."#
?+%<G&
	+-"#!.3
F%Pร
/ hยบ5
v;=	<"SX[
 ยฐ ยย ย:ย ยฌย ยwรย h.ยบ'ยP=	#%K	.+P$wc$R	"#F+%	D!
$4+%DP=	K%'ยค
/ ร}h ( (
: K*4
ยฐ รb:
ร}h $ 4
: K9
ยฐ รbย
: ยkย
ย#[
: รยh M : K*4
ยฐ รb:<Ec=	j
 รยข
'JP=	fD	/,W+%s$"P
'&
 3 
a
 hรยบ 5ยข^HuP=
,$R	"#
h ( รรhรยบT+a
 h M =,	ย"#P.3
.5Z7e	=	/G:<รญ4/G/?Zยจ\#X'Ja=+,=	V		#+	=
$V
 h ( 
'c%X-f#@($V+\รญ4/W/>ย
 qGP#XcM=+M=	,$M+%<=	R
 hรยบ<		.+	=b
%x>#@	P$f;E;#X 5
v;=	ยN31R	$!
,
QE=	=	}
 h M :	b'+GE;
P=G	G"#.S
D#:3
K	$"#$P+
X-YP=	TXw'+
ย #N	$f+)SJ)s+GEย
=bยผ<P=	

-
3w+YN31<G=	M&
	+M"#.S
DK2

	M

-
3D)#%FN3Hย.5ยZ;B	SE]=+<"SX9
 ) ย ยGยจ ยGย ย) ยยย ยnย ยยถยขยย ยยME;
XX	3XJ#-
3X8"3X
	B	+\3X1.
	Dc=+cE_PY	$\c=	JP$M+%<	$"#!
	>W=	$+$5MI	2
 /	_!;ย !K=K=$	YP("P
'+$sX
	B)ย8+a.
	FยH=+KE;T	$GE;
-=		
V"#P$
	f=	$+,"#

-4ย_/a
=J+	$+s
T
 hรยบ:*=	+E;$:*++
 ) ย ยGยจ ยGย ย) ยยย ยnย ยยถยขยย ยย
E;W	;N?=	/V<
5 h M :e=	*:E_>"#D.S
>	,/GTP."#
-G	
-:		;E_G	
C=	bDX+*5
ยZ\"+LN	@u=
',$&
X-ย		=*:;#
=	?DL$R

-	Z=	fx
	.+2'!
S3/>"=
	PL
		ZbE;
P=		Y	) /	G!,+ย"#.3
-D.6:4,D
P
	qย#@	X
"P
,"=	$"PB

+
 ) ย ยGยจ ยย ย) ยยย ยย ยยถยขยย ยยf=+M/G+$b	) /	Cb+\"#P.3
.cE=	Z=	G+PY	
/Gb	!
FGP."#$5
v;=	?%'/GW/,
=G	G)q$&
-.+[=	fx
	.+ยข+u/a
=a"#D.S
u,P=+>*yย



SXXa	$+Pa!M=	bD3:		;_"#/WM
>=wย-$$
	C=	/ยก
?=	b+?/G$+
=	T'+		<	$W	<P2-
DP("#M=	/ 
DPJ=	'+*5Kv;=	'+K	!
,
8
	#@	&
:(

"#SX,
/W/G$W
f	T"#	5;IWI$"#
Vยถ	5 qb%';%'	=	M
"#&
-f+%1=
'F
	5
 h M 
P=	JX8'+*:P=	a"#/G	$c	+%K
ยN
=	$4[cE_G=	+E;$fP=+
ยป 	/,
	?=+ย
"3XX
	ย
 ยฐ ยOย ยยG ยฌOย ย4ยย hยบรถย;%X-V#@	$
.M+EZ		.	=*:1+%'	=	P/G,	.$J?+P=
,=	T.+	=*yzQDTย&=	TX4+4ยK	
	,=ยSXw		$F#+E0=	M+=f+M
 
$a
>P=	
	("#$P5

รยพ

ร#ร'ร<ร3ร'รรHรรร'ร&รรร#ร$ร'ร$รKรร3ร<ร+รร1รPรรรรร'ร-รรร3ร รรร|รบร7ร!รKร'ร'ร'ร3ร ร'ร3ร'รรยพ
รnรฆSร

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

ร
ย

/


k ยvln

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

ร

ยzy	รฃยยyยยzย"ยz{

E



rzl	ย6ยยl

I 	/>+!
"P
:9X
Bf"#/GP	$:
,\E;2'+PY"P3
-/V5Zv;=	?N.a
s%P/>3 [,=YP=	f+
.	=G
FC#tL
?=	;E;.	:=+F=	T)+X
"#W+%*	.+
-	as	(yรน;+.KG/>B
	,
	P/,

!
"F		KN	@	$>"P=	+
"#;+%1b"#
!
fยHGKP=	$+ย*bP$+:=	G	#+
	
=	\		yzV"P=
X0`+	-
	L3Xb&
\E3(W]$+-\=+f"#

`/W$+?=+
+fE_?

"#c+V		$M	$!DM		2-
/G	=
"s+5vย=	a!$"#V"PS
/ยฅ
';=+M=	
#+,%'$+#"=
	W=	b.+	=f	T
&
#_Cf	(C/Gb=f"#5
vย=	N.K"P3
/ย+	X
-$ยผ !1c=	%/>3(#N
-
b+%	=	F+b.	=*:Sc=	_	/?+
"P
&
+%<ยยท jkIยฐ>ยย"#$c,	+b=	J	/>
"P
,+%<ยท+ยฐยณ<5
v9?
X%'f=	G$"#\"P3
-/รชE;,	$VPย=	+EO=+c%+V
3+*
 h?:wยทยฐยณLE;
XX
	.+Pb=+;+,ยผ"#5FยZJ/G.+PT=
'F
fE;Y.[

ยฑ Tt<6Wร

n
r
<A>7;nWW5ย2GF,02ย;>2436<E!ADย3	รซ;รDยถA<@
F;>24;<036;C%0F05	2

o

ย

ยh

ยh ยฏ
ร

รยL5XY	Y24;?>;<ย70MWยถ;9hยก36AรยH;

UT gยj*ร

ยh

v;=
';
c	C$"+!G	.+!
	Vร G: UT(gยj*ย
ร "+!$ ?yzT"P=
XPV?,	.$E;
=
UT(gยjZ#+D:*+\f*5 ยบ q!""#$&
,		G=+b.M	.+P$VEย
XXK=$a!
"#->/G

"##3
.F;/GMX
-	B(;P=+(hG:+?=	P#%PbE;
XX*	)b
!/G	=
'"+5

ร 6xร<A>7;nWW5X2F 02^;2\36<EยAHDV3	รซG;BDA<@ รยh ยฏ K*ยฐ4รรยL5	YXY[2\;?>;<ย70MW;Oh 36A ยH;
F;>24;<036;C%0F05	2 ย
Q("#$P&
	ยร}?
h :(K*ยฐ4รรช"$OhGyรนG+PDOh ร Z\	#+$LEย
=u+ยดK*ยฐย.ย+xhGyรน
&
X
-	DJ)	.$,E;
P=>BUT gยj>.51รฏcP=+#ย
h 
'ร24A3		.+$?+D3
-aK=
K)+
$5
รฏcb%'	=	;#@	&
W+%1Y 
X
	C+%Kย
h "+?FM
/G	=
"FRG
h :&
"#M=	GE;
Xxรฉ
Xย*
ย +9$6ย,=	_!#$"#
C+%รฉM+-	
bM=	"#
!
JP$+$sE;Rhยฎ
ร +s
.*"P=
XP*5
รญ)
BEย
:	,&
X
-	J+%Kh
ร "+G_)TP#N	$GCc
/W	=
";PG
h :D 
"#
-<E;
Xxw
Xย*K%'/
hยฅย <-$ย
f=	J"#.3
-D;=+.+$}h ร %P/ยก
.;&
-X
	D5
o

n

ยฑ

r

v;=	P#%P>s	\J+ย
C	J#@	X
"P
!\DP$Zยข=	G%'
CE;
=ยข= UT(gรj
+ K*ยฐf.+D6:
ME;
XXK	bG"# 
$/GC=+\"#5 ยป "#3x>P=	JNP$AX
	.+P>*:
hรยบ :
Q


Sร-J$W>P=	R		TEย
=G)ร
= UT(gยjV+ร
 K*ยฐ,.+D:Yยทยฐยณq/?$G"# 
<=
'
+
'38'+\/G,=+\"#:+A
M=	#%',	J!
"#-V(!/>+
'"+5 ยบ PV=	b
3
+*:)=	+E_$:
c	.+P$V	
	f+
#+
Xf+%<=	sf
ร
 ยฐ ยOย ย:ย ยฌOย ยรฉ:	Ec=
"=	.+P$
$"P=`%b
.,G-L"#:#
P=	V
 K*ยฐLร
 UT(gรj15รIย=	\ยท+ยฐยณย.	=	2H$+."P=`3-
=	/ 
'
	/>+!
"#@"#	c%;=	s%รถ"#_P=+;
;/,
=D"#&
';
.F
-

34+fE;
"#5

ร;รบhรปยถรผ

!รก1"4" ยช
รผ รข

ร"รรmรยจร@รฝ รผ รXร["ร

"zรร|"#iร +

ร รผ รพ\รรรพยร(''รยรร@รฝ รผ

ย0=
XJP=	G	BV%_	M$$+."P=\=M]$\P>=	G		.+
V	=,+%;=	,+	
	
	("#$P:4
M
'c
/G&
,V"#&
b=
M	=>"#/G-#f
-V
'++!
+*5J^&Z=
b$"#!
\E;
"#&
?=	f#@	$"#$ย	#Nf%s		.+
iaE_#xG/GV		A
."#
-SaE;
	+	.+!
>+?P
+3 5K79
.;E;b"#/G+bP=	J"#/G#@
G+%1'+V	+	.
+>Eย
=?=+;+%
+V	.+
?%/รช"#.+#"=*ย)=
;#+
,	+
$TV$
-/>+b+%K=	+Eย"P-Db=	sX
	.+P>
/Y!K/>+."P=,=	T"#	_ 
+
-b
W.%'<	+	.
,b);%รถK=+W	.+
-*51รฏc#@(
E_W	X
	W=	SEยกJ+?$ย
ยZP
$%'/ ยท+ยฐยณ<yzX
	.5V79
3xVE;>$"#!

/GM
$!
	C
."#
-SF) E;=	b		"#$$c+%1!
S3K+f	+	.
*5
รnรฆG

ย

รฃ
ย

ย
k



ย ยQยย ย

tmnรrยir

yz{z|~}aยยย

ยย	ย

. @l รค

ยป X *+		.F=+;!b+?"$Q%รถ"#T=	M%'		+/G.3*	/ยก+%8/a

	,E=
"P=>'+
+Wยธ.		ยนJPJ!
:
 5ย5X:E=
"P=>'+;"+fJ"P=	$+>	+	$?C=	J"#	;	/V5^&
=
C$"#
ZE;>	$!D,q 
/G>3	&
M+%=	f"#

-b	,E=
"P=L	+	.+!
+Z+%T+
#@

	G"!M
FX
B#GG)b/Gb#@	$
-
;P=+f	.+!
b+	
-	5
vย=	J&
'"b
'$,
c=+b+b+V		:*	+	.+!
f=M#@"#fP=	Y+/WY	
M	2
+
-b+	
	):	c=	,	P	
>PG."#TG	P
T$"P
 
*5v;=P=	Y$+#"=\"#
	.+"P=
	,%H"#<
';	b$F%M	+	.+!
?=+?%';	.+
-b+	
	)5
I		)Db=_P=	b	.+
-T	#+"=
-	,%รถ"#K
'ยถ
 รฅbfJE;PB
	C+f+%9	*
= รฆ#@
.
ย
%'G=	f	Er	/5L^Hย=
G"!:;=	V"#DG%	.
]
U
 รฅ 5uรฏcSEr		D!V=+W=	
X
	.D2'P
+3/G(-qP	?Zu=+f"+ย\#@(P$uPiยขE;B
	Z+ยE;
`
= ร
	+	.
ย=
'J"#P$	a	=\=	f	

Z+ยฅ
% รยข	EยฅCJP=	>'"#/G
+% M$ 
+			!
+,5?v;=DC	+	.

cX
-B#VPf,%HbP=+Z	.
,+	
-	
E=		
ย7รฅ `
รง ยฎย

M ยปiรฅ ย

v;=
'K
-	$R3x
,

&N$?E=		

รฆ

ร

Gรจ ร ( รฅ

ยป]

 รฅ?
"#$$:;=	AD+!
=	/ 
"#P$$W+Eย.	,ยขX
/,
,+%J	5
ยป GP=	V	.+"P=
	ยข%รถ"#(
v =_!/>3X	."=
	s%รถ"#.Q#@e"#F=	M$+$!K	?W=	 ย M #+
5k~F	&
"#M	.

;
+	
	Z3-/GDG3E3	b=,\	."=
	%H"#,+%b+C$!qZ+L 
"#G- = qยร ยรยฆSยฏ	:E;
"#"P,=+b	+	#+
?
';X
-B#?	#%#+,E=		b=	C
-S3K/G	JP	b>"
=+P$R
P$T+c/GDT
ยง รฉรฒ;/?+Df/W(
XNw"
;c	.+!
b+	
-	GE;?$R
5 ยป
"#++
>$!
/>+f	$.C=+,=
,"#P$	GZAN$ยขx
	.+ยข+ยข
LE=
"P=u+
/GDcG
รณ รฉยก+%8P=	Y"#
-+s
+			
+5Fย0=
XCE_,"PBD	+E;$b=+=
'T+S( 
F/Y!
;.+BJ-DD!#:$E;F#X
<
-1	+
$#%'D
-D
-
4YP=	ย"2HRSX
TP$R
P$JPT/>B
	+	.
?E;=E=
XX5

รฃ ย kR

nยยยnPlยยยnยจozrย

. 

rvยn

ยฃ 	;/G(#*+%*P
SSรฉ+?	+	.
+,
Q$W>P=		/,
!T=<=	bยท+ยฐยณqS
-=	/O
.#x%
	.+P$
.9x
	.+C+5K8+K	.+$,Gยท+ยฐยณq	/>+!
"3XXJ=3M$CE;
-=G=	/ยก3X
=	G"ร
$c
	"#$f
-qP=	J		"#$M+%<	
X'
	>P=	Y'+*:)
 5ย5M3XK+%K
.M"+3X
	B	
+f"#P.3
.5
~D+%*M+*yz9	)D&

-1;S+
'+
XX
U$C)#%P_P=	+C
KP$M
-G=	FX
	.tuE;
>P=	JS
+
Xx
U$+
X?
\G	P/G2H)$"P
XNw"C/>+		$:)		M=	J	#31
'	J%8E=T+P.;+%
b>CS+!
+
XX
-Uย">)T
E;$><b	P/ย%1#@('++
	2'$,	.SX
U$+
-S*:+C


"#$?GยดM$	+2รฌ_#XX
1+?~\"3รฌ_+P=D?ย.ยฎ$ยฏยงยฆย1+?>ยดb+/,	=+/G
รฉ+?ยดM$	+Mย.ยฎ$ยฏยฏ	ยฎ+ย.5
รญ)
	#+b!
S3
1cE;2Hb		"#$[+
-YM+%	


'3(+sD3("#

6:$=	3-2

=	/รN.ย
D!
XN$;=	C/GD;	/a
&
	CX
	#+G*:	=	q$/GJ=3X-SE0/G	
XNw"+!

G/>+BT=	C+*yzQ

-
3*+f31"#

</?+."P=f=	M
			.5
รnรฆSร

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

@รฌ

รช/ยรรซ/ยยย

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

ร

ยzy	รฃยยyยยzย"ยz{

eย

`8ยฒ)รฑยฑ	ยณKยฑ 0ยฑ4ยซbยฑeยฒรถยซ ยณLI

v;=	AN.G	=!q%=	
-S3T	P("#$W$W#
=	>`+	X
'"+
	2H!		X
$]/G=		L>
/>S
	2-
<3
=	/ย&
-/,
X+9=	F	;$bYยดb+/,	=+/W+
(+Cยญ-;ย.ยฎ$ยฏยฏยจ(ย
G#$"#"+
'	+b+65879
-.<P=	M
			DS<+PT/>."=	$fD3
F=	b$"P=Gx
	.+W+*yz
DS:*+\P=	GX
-+V'+ME;
=ยข=	G$$b	/Y)J+%/>."=	$C+,
'D
xN$45>v;=
C"+
$-M
\/>+q"+
	$:*&
"#?.3K+b"Z/>+."P=*:*+\f&
	,+Z"+\/>+."P=

\	/,?+%s
xยรฉPDGE3	5Lv*ย"P=	Dq/G	Z=	fP/>3

	]3
+$C=	q3-
=	/
#@+/,
	$C=	?/>+."P=Z) E;L=	W

!
3_"#
!
5^HG"#/G		P$b%'C$"=uS+!
รG=	
	/Y),+%T)i"#
-
C"#$+$ยขDยข"P
	=	WX
	.+PV+*yรนb
-

3_"#

sE;
=
=	A
			G
-

3T"#
!
5ยv;=
',
Y
DP$uPZ/G$	PV=	\+/G	>%	
	ยE;PB
	$"#$+PL\G=	A
			,

!
3E;].+P>ยข=	V.+Pf#@()$"#$ยD]=	?X
	#+Z*5
^H>"#	D.a=	fD	/,)>+%b)`"#
!
s%G$"P=L	
L+ย"=	D$,P=	f+LE;
P=L=	
/,

/,	/V:	$B
	W
$;+
P.+
XX5

รช/ยรรซ/ยรรซรฎรญ

ยฒยb(b*ยฒ6j(Rยb1ยช9ยซZยฑ4ยซb1ยฑยฒรถยซeยKยซUuยฐI3ยณLj

ยญT3
	,/>+."P=	$fCX
-	.+G'+*:N
-	J
-,=	J	E

	/ยก
'&
/G-[

ยฎ5;^&.+
=	cX
	.G+?E;
P=>=	MS
+M

	<	P("#$ADGP=	b/>+."P=ย+5
ยจ	5[iย"#b=	sX
	.+PG+*yzF31"#

;E;
=?=	b	EOD31"#
-
5
q	5MรฌF$+,G	Eย)Z"#

f%'T$"=\DS1	)D&

-f=+b+	$.c
\=	J	EยD3

!			;
-f=	MX
	.G+*yรน;D3135

รณ5[iย"#M=	;X
	#+,+*yz9
-

3w"#

QE;
=,=	M	E0	/yz9


'3w"#

-5
รด	57e;$"P=q"+S*X
	B?=+,ยธ"#	/G$ยนW,	)D&

-G%'/ =	b+W

-
31"#
!
:
X%
P=+b	)D&

-\
C+b%'/ยP=	G	Eร


S_"#

:*=	L#W=	Gx
	BZ+
	fG"#$!
	W	Eย\"#

-*5
ยถ	57e<$"=V"+P34X
	BG=bยธ.		"#$ยนGC	D&
-
W%<=	b+'G3รฉ"#
-
:
X%=+
	PD 

W
;+;%'/ยฅP=	b	EยD3':=	V#C=	MX
	B45

รฃ
ย



A
ย

9nt~nย ย ยivย,;ll	ย!t

tQvยn:ยยozrzl	ยomnรฏo9vย

v;=	G3
=	/รง+Y$M	?		
	f+%_	)/	C[c=	JqP		$\"+Z"#.3

M=T#@
$]ยธ#	("#$ยนf"3KX
	B	ย%b	PD 

ย
\=	CX
	#+f+*yรน3Q!
=+G+f	G+G%T=	V	E D365ยยญc"#V=	AN$]+u"u"#D#3
ยขX
	B	:_!:;+
"##3
.;E=
"P=Z+?ย +	D!ยQ
#++T?=	G"#	b	-/V5 ยฃ %ย"#	.:4	
XK=	
	+	.
G3
=	/ย"#SXJP	
Q
9
/G)D&
-JP#XE=	=	F=	$T.+%*=	;x
	.+
+JEย
XX("#P3XM	C	1PT)<#%' 5^'%4/G+$b	
	M=	FN
-			"#$:+=	;	+	.+!

3!
=	/รฒ/a
=ย
'"#+=+F
c	$	,2'	.+b=	JP+/GJP"#	$5
v;=	JR	$
V=	#%'J
$;GE=	P=	T=	sN
	?3!
=	/ !=	V#-Y3x8"P=
X
	B	:$)
3xรM/G+
	;/>+!1J"#P.3
.9ย "#!S+!
<#+)ย.:=	

$3;=	/ย
W=	T'+,=	
	JP=+<=	,E;
xX3XC	+;#%'8ยM	F+		D"P=4ย _
ยฃ 	ย"Y$&
XJ"#!"#K"$9
,E=
"P=J#
-=	<.M%'/>9E;#X+C=	;=	%'/>
5
รnรฆSร

ย

yz{z|~}aยยย

ยย	ย

ยยข	$?++b+,
-DP$
	s
DP."#
Y)E_?=	M	;P.+,+W		+	2
.+!
YS
- =	/5kยฐ ยย ย:ย ยฌย ยรฉyรน4."#
,3!
=	/ย
P=	<
-D#;+%w#@	&
*:+E=
"P=Y/W$+
=+
\"+ย0P."#Z$"P
&
VP=+f
\/,
-=D=3i"#3X0/>i	
-	`#@	&
-*5
ยฐ ยย ย:ย ยฌย ยiEย
XX
&]	>	.+\ย	) /	?`*:uLX
	#+]+
"#.3

-	J	 /	FX
	B	K;F"#W	<=3)f		"#$f
$"#,,=		+	$5
^'%<
: ยฐ ยOย ย:ย ยฌOย ยa/a
=_	_)J+MJP."#3X4=	b+	
-	a$"P
'&
Q
?=	cX
	.+PY'+*:
+
'M=	#%'G	J"#/G5AยI+
"#,
J"		JP."#J3X<	P
M+	
	\$"P
&
6:4

"+		F."#<3X=	ME$C"PBYPb=	Xw*:D+,P=	#%/>3C%รถS
Xb#@	P=	D!

+f"#5รย
ย i;$"3X4%'/ I$"#
fรด	5zยจC=+F=	M."#!
>3-
P=	/ย	$$a
I$"#
?รณC

,"#/GFE=	G$Y
Y"#+ยผ	"#
YEย
=Gb"#++
KN
-	P.+:+83-+
-#X
DC/G	
X%
	M=	ยฎ
 ) ย ยGยจ ยGย ย) ยยย ยnย ยยถยขยย ยยb"#(c
K#$K	) /	<!&t`=	1P=+
=	M

!
31+?DS9!;=+MG	;		"#a,"+P3*X
	BtL%'/ยฅ+>?
	P5

รฏ O
รบ รฐ

kรฑ

รร รผ ร +รฝiร}eรร!รฝ รผ ร

+

รรรพยร(''รยรร@รฝ รผ

~D?	
-?E;BLย"2'$0+	
-	L=f"#"#.+$ยยN
	LD	รจ
#@D
-	
"P=	/G$C%'J=	f'+Zx
	.+:E;
-=Z=	?
$=Y
	q]

	\+			
'+>"!$
E;ยข/,

/,
-UG=	f	$ยข%'G	+	.+!
*5ยZย"L		=	#$CfP=	ยยทยฐรยณยค%./GE;B\
+3UJP=	Y	+	#+
V"#/G	M+%<=	b	/>5vย=	YP3
TP.++
-$F
"P$A
=	
ยฉ9ยช*ยซ	ยฌย	/รซย ยญM+/G/G4:Kยฎ$ยฏยฏ ย#:%T#@+/G:*!$"P
X%'V#+&%'/>+&
;=+b"+\>$"#/G2
D!$W
DPY$R	"#$%8ยท+ยฐรฌ
ยณ <;ย2\;_m
 <P;>36<073)	
/,
-
$5 ยฃ 	T+3-(&
'K	+$F#%'4
>E;

Xย*_E3	[
ยฎ5;^&ย!=	SEM,=	+Eรชยฉ9ยช*ยซ	ยฌ(yรน,
#@
	L+LP3
f.+P+
X$G"#รจ\#@(-+
$ยข
ย=	qยทยฐยณ
%'.+/WE_B,D?	+
'
	G=		
!
"M$+."P=	2H"#+4
%'/>
*5
ยจ	5;^&>/G!.+$a=	SEรฑยท+ยฐยณ<yรนG&
/G-V"#	Pq"+L)q!$Lย3UV/GPV"#/G#@
	+	.+
-qP.++
-$:*+\!
/>+#-V"#\)>$ยq"#/W+G3P+
,=	
$
%1+fS
$5
ย ?.+ME;
P=ยf$"#
ย	/G/>
U#
	fยฉ9ยช*ยซ	ยฌ(yรนJ$&
*5>v;=	ยข
LI$"#
-]ยฆD5zยจ?E;>"#&
'
Z

.;P3
TP.++
X$!$R	
3x:	$"#/G&
	G=	/ 
Gยท+ยฐยณ]..5ยI$"#
\ยฆD5 q,	+$
=+Jยฉ9ยช*ยซ	ยฌ	yzbb+%<P3
.c
c
-"#/G:*ZI$"#!
]ยฆD5ยรณV
P"#$ME3(cf"#	>ยฉ9ยช*ยซ	ยฌ(yรน
=		
!
"F
VยทยฐรยณFyzQ%./GE;B45<I$"#!
\ยฆD5zรด,
P"#$c=	SE0	M+3	&
;"#?)J#@	$
=	P.+&%'/>H
-3w'+		."P=q[
 R*TQ(
ยฑ Uw)ยฒ Kwยท,ยI+
-/G/G:4ยฎ$ยฏยงยงย.5

รฒ
ย

ย
{

ยrmt

rยir

.

lrzlยถย

ย

tรธย!tiยฉ9ยช*ยซ	ยฌ

ยฉ9ยช*ยซ	ยฌV$,?N2H.+W	("#$Pb%'Y	+	
	\+Z#@

	+ZPq"P=
G	Eยก35fยฉ9ยช*ยซ	ยฌ
N..B$CX
-	.+G'+*:N.F
-,=	J	EO	-/V:	+V&
-/Y'+$Q
.;#@	$"#	
*:	&
	GP=	
	Eย
-

3_"#

C+ยขD36V
5 i;	=-ย$B
	):8ยฉ9ยช*ยซ	ยฌ	yzs%H3
X	$C"#$!ยขq\ยทยฐรยณ
+fE;
P=V+F$;	ยต
 /w$Eยt`,=	$+P	$WX
	B?;	$"#

-*5
ยฉ9ยช*ยซ	ยฌT	#@	<!$9%PEย.,+C"BE+.,"P=3

	MJ+3-DU;=	;%H3
X-	:S
'"#+
	M=
	D
X
BCE=+TPqPT"+!$VP=	b%H3
X	:4+AE=+MD3ย=	D,ME;Y!
'"P
	5Tvย=	
$9
*M"+3		E;Bb"#$)
	MM=	_"+P3Dx
	B("#P"#$bJยท+ยฐยณW
JP=	_	P("#$
รnรฆรฆ

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

ร

ยzy	รฃยยyยยzย"ยz{

+%J	.
*รด5 รณWยท+ยฐยณ>P=	#%<)%/>9=	FN. E;T!.+$4%wยฉ*ยช9ยซ	ยฌ(yzK	+	.+
-b		"#$

f=	b	P("#$c+%K+f	#+
*5
ยฉ9ยช*ยซ	ยฌP=	Y!$=	"+3D#@	+
C!#$"#1	_%w&
X@	J	$!$C
+	D!$:S"SX$
v ยฃ FFยID"=	Bw:)ยฎ$ยฏยงยจย#5 ยป v ยฃ Z3b"#.3
Kb!K+%wS
8!.+
$&tu+C.+ %P/>+
+
=+M/,
=D#x
/,
+Pb=	C%รถ3
x	#t`+A$"=3
b#+?=M+qP("P
'+$fP$TPย"P=	$"B

.,+	x
"+
XX
-H5ยยฉ9ยช*ยซ	ยฌ	yz,v ยฃ ยIย+V

$ยข
fNV"P$6[G%H3
X-	$J	Z&
f#ย*$"#.:
$&
P$G#ย*$"#.:	&
'%'$+	$6:D$&
P$,%'$+	$6:(>P>+#+/G.651v;=	Jnยท b1ยฑยฒรถยฐDยทT"#
?	2
$.+!
J!$,D,ยท+ยฐยณq$K	<

	
=J)E_W	ยผ$"#%$+P	$K+C=	K	D&
-

+D$M	M3X+Eย+#+/G
-U$f:4?f=	sN.E;>"PP$%Qv ยฃ FTJ#-S+
?	T+3-(&
'5;^HZ+V":)=	$C E;>"P'$M+b=	,/G;
/G).D$:)&
"#JP=	V""#	
%/GC=+f=3x%<+%8P=	Yv ยฃ F5v;=	Y
'
"#
-fE;q?&
C#ยรฉ$"#M+V?$&
$A#ย*$"#

;.S
=% E+#4[1&
b#ย*$"#.c+M.;D!."#

-F=+*yยT!		Mbx
	Bw:E=
x
$&
$A#ย*$"#.TW=$Y,		)DC
f=	C+*5รฏM+	.SX:=	Y!T+%<+			
+bP3
.

Xย*<
-f=	JE;G"$5
ยป %P1"=	D&
-	Tv ยฃ :+ยฉ*ยช9ยซ	ยฌ;
.+
 $รฉ=	_P("P
'+$c3
K.+
X$4&
	c=	_.S
X
+%1=	J"#	PD;
	,.B45K7	;$"P=fDP&
M3
ยฉ9ยช*ยซ	ยฌG	;,P$;GM
X%1=	M3
-<
'
+	X
":	&
	?=	,$-+%<=
c$cG
-.+
3b=	C3
357	M#@e/G:)=	J$!%'
+q."#M3
-T"#$!
	fP,
!
V%_\ยธ.Ec=
bB
=.ยนfยPรฌ<=+	/>+*:ยฎ$ยฏยง(ยฆย<E;
/,
-	bE=
"P=V"#'f$P;=	J$&
P$f	)D&
!
*5
79
SX:$ยฉ9ยช*ยซ	ยฌ$+%	=		
!
"K-$4.	Bb=	<++
w
.+
$P3
.:+"P=	DD!$
=	M$$:	+?+	X
-$K
-$5 ยฃ "#b=	T'+G
'K.&%/G$4:+
-K
;&
/,$,"#b+D3
-*ยD$"#
-
+%<,	E0%H3
X	b..F=	Y"#	"PJ+	EC5

รฒ ย 
{

lยยrmtQv

ยrmt

ย ย 

rzl	ย

ย
t

ย!tiยฉ9ยช*ยซ	ยฌ

I ?+%1ยฉ*ยช9ยซ	ยฌ(yzF?3
;.+P+
$KJ	_+	,C	_ยทnbยฑeยฒรถยฐDยท2-x
BT"#!
,	$	2
.+
-*5K7	#@+/G-:	=	b3
-=+T3ยผ #;=	J	.+!
f+%<+V"#
-?
F
	X
"+J 
"#
3X<ยท bยฑยฒ'ยฐDยท3"#
+G!	/G$f?	""#	b
#+D#+	&5Mv;=	JP$b+%<=
M$"#
-q$"#!
$
=	b#SD;3
-.+?$"#$c=	/ยฅP>ยท+ยฐยณZ	
-/,

$5
7 	*S
.*	M	EL9P=	/w$E;$M*51^&J$"P=J"F=	<'+M%รถS
X	<"#P$	
g e
PaYX
	Bรยธ%ร ร ร ยธรตF=	$+P	$fย+	=	Tmยธ\รฏ 5
ยฎ5ยรถ%รท
รธรนeรบkรท@รปXt ยป 	LV	ErL3%P1ยธ\รฏT=YEย
XX_P/G+G=	V&
2'#ย*$"#,	)D&
!

Y*ร #%'JP=	J"#	/,
	?*:(ยธรต#:
'<#@	$"#	$45

ยจ	5}รผ}รฝ รพยธรท@รปรฟยรทkรพeรปรท#t ยป 	O]	Eย0=V"=+	$f+0	$&
$0P.3
,
-D]	ZP=+
/>."=	$F=	CD35

ยรถ%รท รนeรบkรท!รฟ รท kรพ eรปรท#t

	

q	5




'"+5

  

ยป 	fC	Eยค!>P=+#$;+?	ยผ$"#$yzK	$ 
$f"P=+."#!
2

ยพwรธ6ร.ร;ร;รPร
รผ ร9รรรรนรรร3ร'รKรPรพร6รรรPรSรPร-รรร#ร$ร<รPร'ร<ร;ร#ร'ร<รรพร$ร'รร'ร-รรร#ร;ร'รSรPรรผ ร*รรร3รHรร)รzรรรร*ร-รรร$รร<ร'ร3รKรรรPร'ร'รร1รรPร$ร3ร.ร
ร รพร3ร'ร ร'รMร;รรHรร#รรร!รรรMร'ร$ร&รVรรbรร#รรรรรรร$!
ร รฝ*รPร'ร'รHรXร6รQรPรร-รรร#ร$ร
ยพ *รร'รJรPร$ร?รรร?ร'ร$รCร'รร'รรร<ร#ร$รbรPรSร!รรรร-รรร
ร รฝ4รbร ร#ร ร
ร-รรร$รร<ร#รรรรbร'ร$ร;รXรรPร'ร$ร'รรKร
ร 
Jร'รSรPร<ร+รร'รHรรรรJร'
ร "!#";ร6รรรPร$ร6รรร$ร3%
ยพ $Jรcร!รPร$ร-รรร.รCร'ร$รร'รรPร$รร<ร'ร+รรKรรPรรJรPร

รผ ร4รรร$ร3รร.รPร-รรร#ร$ร<รPร'รKร'รHรรรร.รรรร#รรรรcรรรMร$ร#
ร ร&'!("Qร$ร#รยร!รรร3ร<รร3รbร'รร$รร'รSรPร1ร#ร$รKรPรSร!รรรร-รรรรรรcร$รรร ร'ร-รรรร#ร+รรร
รรร$รร#ร_ร6รรรร'ร#ยพ
รnรฆ\	

ย

yz{z|~}aยยย

ยย	ย

v;=	$G=	P>S
.J,

"3K%'/ ยท+ยฐยณ<yzs.!$"#
:8 
"#ยยทnbยฑeยฒรถยฐDยท?D$C	,
2

	
!=ย) E;Leยผ $"#C%'$+	$G]=	,D)$,+%b	&

5]I+
"#VP=	f3

.+P,
;P=	J+/Gs
f=	JP=	J"$:(
;
;	"P-$+TP=+=	J
!
"#
Vยฉ*ยช9ยซ	ยฌY/>B$
E;f	ยผ$"#.+W%'$+	$ย	+
$c+DW#%<"#+*B	+E;$c
f=	$,"$5
ยบ "P=b+%=	$F3
#*"#$!	9=	Q
	"#
b+%aยธ.E=
KB
=$5zยน ยป ""#/Gx
=	2

	,=	$b3
-.	/>
"3ร,$R
$;P."#
-	YP=	b=	$+	$Wx
	BG=	V	
-	>
	Eยx
	B\	("#$]Z	E !LยH.=	J=i&
-/GZ	
	=	>	EยฅP4ย.5Vv;=
ยท+ยฐยณq"+f&
/,=	$M=	c.+&%'/?+
+KEย
=>s."#P2'#N	!$R	"#:S=		=
	

3_P."#
Z/,
=J)f	$$L\#X
-/,
+f$"P
&
-C=+G$LL=	

+
Sw=	$	$WX
	B45
รณ5%)+*-รฝ-, รพรรฟ'.0/@รฟ2รปรท1ยรนรป'ctยoc

'JG?
,E;a
'
"#ย=+;P=	ย"=
b=	
$ 
$?$-.;+%1P=	J
-+
3H5
^&ยยท+ยฐยณย/,
	:)
M
'b"P$+b=+C=	>PqPV?X
$:ยธรD:*/,J)G		"P
	VE;
"3X
	B	:*&
"#,
-
b""#/GX
!=
	>E;?		D!$5Gv;=bยท+ยฐยณu"+Z#ย*$"#b=
MP3

JP."#
-	b=	=	$	$CX
	BWยHE=
"P=>+	P/>+
'"3รb/G+$K/GS
+;

-	
,.
-	Y"#P.3
.#ย:	
	,b	Eยรยธ

ร

2

:+GP=	>	
	Gb	EรจX
	B%ยธ

ร

2

รร ยธ

รต5

g vยE_,#+&%'/>+&
<P"#J+f#@

-	a!?
f=	C+*5
รด	5}รผ}รฝ รพยธรท@รปรฟ3*eรฝ .54761,7/@รท!รฟยรท18Kรท
รธรพHtu^&f=
F"M=	;%H3
X	F
FMX
-	B1ยธรร ร ยธรตKE=
'"=,
'K=	$	$
`	=	q!ยค
 ยธ รฏ E=	D!ZD!."#

-ยผ
 Y*ร 
?	?
D+$รจ
ย+	P=	fX
	B45
vย=	f3
,
',Z'"#m
 ยธ4รฏbE;
P=u+	=	G*{
: ยธ รฏ92 =+?D$*y G=3+
 Y*รยW
)D."#

*5
ยถ	5}รผ}รฝ รพยธรท@รปรฟ3*eรฝ .54:*@รปรท
รธรน;.0/<, รพ, รน=.tยคv;=
'%H3
X	Q
b%ยธรต9E=
"P=J#
=	K=K+,,	$"#	2

-
1
 รO<E=	D!T	$"#

a
;		P$G>C=	$	$,x
	Bw5Kv;=	M3
-
	
P.+&%'/>H
-G"#${
 ยธรตFE;
=VC	Eยf=ย$;	c=$รง
 รรC	$"#
2
!
*5

vย=	$,.+&%'/?+
X;=3,=	,$!b
S1%'M	+

-	ยยทยฐรยณLEย
=q!$+."P=	2H"#DP+
=		!

"65ยน~F=u%Y=	$\S
.?/>+Bq]"#/GD\ย<P;>36<073a%+x+E<$L <;24;ย
Pq?x
	B\
ZP=	>/,
'	>%c\"+P3_	E;B4V
5 i;$"3x :=	SE;3:*=+,ยท+ยฐยณ0\/>+B$
"P=+	$?Pย=	ยยธ%'
	$ยนZ+%,=	\	E_Bw[]ยท+ยฐยณยLP."#.?$"P
&
-?=+?=3\	
=	$"P
'&
M(
	f\=	/V5J7eM#@e+/W:w"#&
b=	C%'+X+E;
X	G#.

%ยP=	(
 รผ}รฝ รพยธรท@รปรฟ3*eรฝ .54#6>,7/@รท!รฟยรท18Kรท
รธรพ,#@e+/Wย+5]I		f=+C=	V"#	,+L"#D#3

E;>	
-
3K$"P
&
6[=	G$"P
&
-V?$.X
=\f"+3X
	m
B ยธ4รฏ>ร ร ยธ+?VยHP=
;
'E=
"$?=	c
"P&
f+.
% ยธ4รฏ,+
fEย
=4ย8f3G,$"P
&
>,	$"#=
QX
-	B,%P/
+	=	;=	$+
-	a!m
 ยธ M 5ยI+
"#b=	M+P"=	+
'"#J	cf=	bW#@D
!"#b+%
=	cX
	B4:=	b$"P
&
?G	ร
 ยธ4รฏ,=	T'+ย_!		ย%{
 ยธ4รฏ ร ร ยธ+?Y"+		;)b."#P$
	
X*=	J$"P
 
fG	P$"#F
=ยVP."#$45
ยบ /Y'+
+M+%(P=	#รผ}รฝ รพยธรท@รปรฟ3*eรฝ .54#6>,7/@รท!รฟยรท8Kรท
รธรพ19รผ}รฝ รพยธรท@รปรฟ3*eรฝ .54@*\รปรท
รธรน;.0/<, รพ, รน;..+ %P/>+
X)E;
$!*
-ย
 รฆรง>ยฎ;ยท+ยฐยณf#"#*).+
4%'+X3E;$MDR
 รฆรง>ยฎ<P#N	$:+E=	ย
 รฆa
*P=	_	/,+%
)Db$"P
&
F
f=	,"+3*	E;B451^&V=	J"#	PDMยท+ยฐยณZ
-/G/G.+
-S*:=	P
รnรฆ\I

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

ร

ยzy	รฃยยyยยzย"ยz{


F	J%H"P
Xx
J%'F=
FD)b+%1/>"#PY.+P$:		c=	$Jยฉ9ยช*ยซ	ยฌJ#+&%'/>+!
+_!	$
=	J	
xX
>%_
-	G=	G$R	"#Y%_$"P
&
MP."#$ย
VP=	Y"#	.!Y%<	
	+
	รยธรต
b=	c%
-	:D+W=	?3
	b=	$b$"P
&
KE;
P=>b		"#$;&
/a
X+Y!
S+!
3
+3fยรฌ_+P	#X:ยฎ$ยฏ
ยง q	
ย dK#D>ยตรรฌ;+	#X :4ยฎ$ยฏยฏ qeG
ย dK#-D:4ยฎ$ยฏยฏยจย.5
ยฃ 	J3
-ย"P=+	$ย=	J*yzFS+
'+2'

	?"#.S
D#5
g

ยฆ5}รผ}รฝ รพยธรท@รปรฟ3, รพรรทt ยป 	Er	ยผ$"#W
G	!
	$รจZ#X
/a
+f	E+DP$]%'$+	$,Ec=
X
/>3
.3
X
	G$&
P$f"=+#"#
!
"5
v =
93
<"+,)$,J"#$"#KMD	/,F+%รฉยทยฐยณ?%รถ3
x	$[4=	P$+	$sX
	B	:3
"# 
2
;
D"#!.3
.:	+A=	M
+
xX
,>		)b+f\"#

-Vย	JG		P$+++
=	$+.รย.5Gยท+ยฐยณ`"#\#ย*$"#J=	W3
b\."#!
	f=	>$"P
 
Z=J	$ZP=	>"#	2
.3
-D.FยH/WDKx
B#,=	J	
-
G%8,"+34X
	B)ย8?#N
	G,&
/,
X'+_$"P
 
?=+
)	Va	Eย	ยผ$"#$5
g vย=	\.+&%'/?+&
,/G	
X%'u=	\*yz?/G).3J"##3
.:;#
	]#@D
!
	
!5
ยง	5}รผ}รฝ รพยธรท@รปรฟ3*eรฝ @รธรทรท.kรพ>4-ยรพยธรท@รปXt`vย=
M3
M"#$)	cG	/W
f+A$R
-$;	?2
P."#
*5
ยฏ	5}รผ}รฝ รพยธรท@รปรฟ3*eรฝ @รธรทรท.kรพ>4:Aeรท1ยรนรปรท#t`v;=
P3
K"#$)	KP/G
,+,3P$R
P$	
P."#
-*5

ยฎ	5ยรถ%รท!รนรปB/@รท@รปxt`v;=	G#b
ZEc=
"=ยข E;VPJ+WV)>	ย
b.$45fv;=
C"+Z
""#/GX
!=	$]DLP."#
	ยข=	q$"P
 
uP=+>	$รจ=	V!
+
3#
	L+

-	YP=	b	D 
b.!
	5
v =
'Y3	&
b%sยฉ9ยช*ยซ	ยฌZ3
	s
L	,	.!.+
	Z%T.+ %P/>+
X3;+		.Y
ยE;
;
E$	579
.Q
_"P
XN$Kยฉ9ยช*ยซ	ยฌ	yzF.
*:	+

-	YJ 
/G#@	+
G%w
-.KS
_P.+2
+
$C+Z=	+E;
	E=+J.b%_.&%/>+
-Sc
-Y"+ยย"+		Y""#/WX
=*5I$"#ย

3(Q=	b	E_B,%'K
"##+
	Gยฉ9ยช*ยซ	ยฌ	yz;.+P+
$
-D,ยท+ยฐยณ<yz;		.+
ย3-
X=	/ 

=	M%'/ยฅ+%K"#DP+*+x
"P
$MยI$"#!
\ยฆD5ยรณeย.5

รฒ
ย



n ย ย
kR ย

 .

ยnPlnt~nzv	v

ย



ยฉ9ยช*ยซ	ยฌ

ยฃ 	\$?+%Y+3-DU#
	Lยฉ9ยช*ยซ	ยฌ	yzGP3
f.+P+
$C
0ยท+ยฐยณ<yza%'.+/GE;Bยข
?ย/W.

=+,ยฉ*ยช9ยซ	ยฌ(yzs+
'"#+JC+%3
.
M
-"#/G#tuP=+M
:=	?+>/G>"#/,
+!

+%<x
	.+f'+b+
			C	/?c%ME=
"P=ยยฉ9ยช*ยซ	ยฌfE;
Xx8)G	+GP>	#+>f
-.+
$R	"#J+%K3
#5Fรฌ<&
3:%_#@+/G-:	=	J"+S8"#P	J=	+E?
V79
	Tรณ)5
ยป 	/G\=f.
	u"#.3
-D.W$P
"#?=	Z+ยP?L=	\N	PyzW#%'2'2'
-=D
.
-	ย4
\P=	JE;.	b		)DGP=+b=	G-q"#&
'MX
	$+
-U$+
XV%ยP=
M+Z..
E;
ร
= ยธCP=	V
 ยธ4รฏ=	ร
 ยธ รจ +?J*5<v;=	b++ETK	P"+P3X
-	B(:		;,E_Mx
	B(Q=3
+)#$fEย
=f=	C	&

f	("#$ยขยHP=	J=	#+s
#++ย.5cI+
"#!
 ยธ4รฏ;#$%
 D
รจ
รจ
+ร
 ยธ $R
-$Q
$:
K
'_"P-$+_P=+{
 ยธ4รฏ1P=	$+ ยธC+ร E ยธ 5;I+
"#B
 ยธF?G"#	/W$ย
 YD,:eG
= Dย
YDd+P#%'w#ย*$"#.<,=	=	P$+</,K/>."=W	+%1ยฉ9ยช*ยซ	ยฌ	yz<$&
-$,#ย*$"#_v ยฃ F5^HW%รถ"#$:
79
	Gรณ
,"P&
'"G#@+/G?+%=	H
 Aeรฝ รน>รธIรท/@รฟ3*@รปรท
รธรน;./<, รพ, รน;.Zv ยฃ OE=
"P=]=CZE;VS

รnรฆ\X

yz{z|~}aยยย

ย

P

S

Sa

ยย	ย

Goal

b
S

P

St

u

79
	Mรณ[<ยฉ9ยช*ยซ	ยฌ,3
-."+		ยN	@A$&
-$2'#ย*$"#T?%รถ3
x	$FE;
=?=
c"+38!"#	5
#++
$6
[ รถ>รท
รธรนeรบkรท@รป1+U
 รผ}รฝ รพยธรท@รปรฟ3*eรฝ .54@*\รปรท
รธรน;.0/<, รพ, รน;.D5F^H?+
"#'+$:ยฉ9ยช*ยซ	ยฌb
8%
	W%/ย
	
=	Rรผ}รฝ รพรรท@รปรฟ3*eรฝ7.54761, /@รท!รฟยรท18Kรท
รธรพbS
 
"#J=	,=	P$+TP$.ย%P/รง?$&
P$V#ย*$"#Cย'%'ยยธ ? ย;	b
&
b#ย*$"#$5Kv;=
';/G$+F=+ยฉ9ยช*ยซ	ยฌGE;
xXw	M"#&
P"P
	%ยธ4รฏE;
=V,Pf=+D$!*yย
#J
 D,:1Z=		=ยข=+b/>3V)>P=	G\E3VV"P=
GAE_B
-	f+*5>v9V?=+
=
ย.+&%'/?+
XW
"++-J+%K$!
	C
q,E;PB
	,*:		C=+;=	Y"P=	+
"#C+ร
% ยธF?G
		)JP=	GD3</?$\=3,ยข
"#P$"#$5G^&ย=	JE;.	6:)
J/>3?D 
GfP"#
ยธF?GEย
=V+	P=	T=+D$c	M$R
R
 YD,:)E=
"P=fE_?/>+BbP=	b%H3
X	JW&
2'#ย*$"#
%H3
X	c
P$?+%<G$&
$2'#ย*$"#;%H3
X-	:	+?E;?+B
 ยธ4รฏyรน;"#/WD$5
ย0=+b+,=	C
/Gx
"+
;+%_P=
M$g _รจQ+-Vยฉ*ยช9ยซ	ยฌ(yzc
-"#/G	$Pc
M+%_/a
	
"#$R	"#:<$!$"P
3xย&
"#fP=+J	ยผ $"#3yzb3_EC	("#V=		
'
"3xZ$R
J%_
-#@D
	\+ยข.+&%'/?$!
Z.+
X$M.+=	C=+ยย%/>3X-V
xNw+G3
=	/V5
ยป V3	&
QX
Bc=
F
F		P=	#$F
!"#
T&
"#M
;/>B$F	$"P
bE=_P.+ย1;ยฉ9ยช*ยซ	ยฌ(yรน
3
=	/d/?+B$5^H<"G;
-"#
-<J;
B ยรซGEM
"#83-
P=	/ย
	!	4:+
"#/G2
:)		/?+
"+:fE=+b++D#+$F
#@(	P$&
C+E;M#@()$"#$%'/?+"#
+3
	$CDG"#
XNw"P
	J/G;%'/>3		)5ยZ#x
=<+>3
=	/Vyzรฉ%'/>3	P	2

$F	+
b	b+%K,D	/,)+%KEย3	F,	..+A=	J3
X=	/VyzQ=3
$:		TG	
"#
-	M=	J!
/>+M.+	+#?D?E=
"P=V+qS
X=	/yzK+3	Y!=	?cยผ $45
ยZ	#@(?	ย]=	V
"f+%J=	+ErZ\=	\ยฉ9ยช*ยซ	ยฌยS
>#++
$aE;
=
ย=	qยทยฐยณ
%#+/GE;BGPY
b=	Y	+	.+
-ยS
X=	/5

รฒ uย tยฅยฉ9ยช*ยซ	ยฌXlยยrmtQv  ย ย 

rzl	ย

ย

tivรฅrv>ยทยฐรยณ



n\ย ย
ย ยvl	ย6ย0v

 =	=
=	$a# :;ยฉ9ยช*ยซ	ยฌย+uยทยฐยณย#+f
-LL
Xย*DWEย3	5uยฉ9ยช*ยซ	ยฌย#+.CE;
P=u
ยป G
"#/GPf+uP=+,%H3
XCยP+
&%'Z=	D3T+รจ$W.+&%'/?+
XCZ	#+Vยข	E
"#/GP+*5<ยฉ9ยช*ยซ	ยฌ,/,
.Q	J	!
G+%1C+!
3+f+,	,#@	X
"P
F	
G+%*."#
	
L"#/G/,
-/G$5รรฌ<.?=
?Eย
=uP=	ย		D"P=0.+B0`ยท+ยฐยณ<:ME=
'"=ย"+0#"#ยD
	
K'+	
	Y$"P
'&
*:$-
	M
>G
"#/G#G)$"P
XN$W+*5Kv;=:J+E`ยทยฐยณ
E;
=]$+."P=	2H"#+<=		
'
"b"#P$
	V\ยฉ9ยช*ยซ	ยฌ(yรนb.&%/>+
X6:4E_W	$]V"=S

P=	Yยท+ยฐยณ<yรนc("S_#N	@ ~$P."#,$"P
&
sV#ย*$"#,]ยธHยผ	/Gยน?%'/รฑ	ย+P$f+%=	G'+
"#b?+	=	$5
v;=	,&
/G-$E3>+%<

-	>ยท+ยฐยณ]=
M"++
XX
- a
P>#%'/, Jยทยฐยณ<yzc	2-#8"#DP+
D,%P/รฒs	$=	2-N.!<#@	.+!
,+%*=	b"#+%*ย&&
	J,R		MF	

- JR		+ย
>?	=	2-N.Mc
.+!
ร2H
	f	=	2-N.b$+#"=ยขยH&
	fG."B)ย.5F^HZ"P=q?"=	/W
) ยยย ยnย ย  ยฌย ย>E;?	W	R		,3X1P=	J	EO+;P		$fj
 =ยยจGยจ ยGย ย%0 ยฌOย\? ย
$

TE;'q"P=	D,=	ยธ.$!.ยนf""#$b+]ยH&
	f/WY=		!

"s.+	B
	?
%'/>
+4ยย
#@(C
$:)$3
	>P=	aS+P$\=	?."PB?%M+M#@(-.+
-f
X%<"B."PB
	>	PS$
	$"#$+9
5 ) ย ยยจ ยGย ยG) ยยย ยnย ยยขkย ย:ย?E;f?X
-BE;
bEย
=f=	,#"#$f		yzM&
X
-	D5v;=

รnรฆ\p

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

ร

ยzy	รฃยยyยยzย"ยz{

/G	
XNw"+
-G,ยท+ยฐยณ<yzF	2--#รฉ"#	D>#X
-/,
+$P=	T	$W%'b+-3w'+	2'S3-+

=		
!
"+:&
	,
-$?=	M%'+X-SEย
	b%'	=	B	F%=		
!
"T"#DP+*BD	+E;$[

ยฎ5;ย`=	ยข=	O) ย ยยจ ยย ยG) ยย ยnย ยยขkย ย:ยf		"#$	W
b
ZfZf#"#$:4=		
!
"C
%'2
/>+
G
F		=DcG$G$"P
JEc=
"=V$"P
'&
fW."#35

ยจe5 ยป '% <) ย ยGยจ ยย ย) ยยย ยย ยยถยขยย ย:ยM	.+P$9
.	EL+9
-K$=		
!
"J"P=	DD!_Ec=	=	
Y"#
	M."#!
	G"#.S
D#9%/รฒP=	b+__Ec=	=	;,#N	J,"=
X'fย ,
X%

"=	D$;P=	M+3:DEc=
"=V 
X
	,GP#N	+ย.5

})

qe5

ยยย ยnย ย  ยฌย ยJx
BE;
!$;=		
'
"ย
%'/>+!
รGG/,
-	TE=
'"=?V"#

W
=	$+	$Wx
	Bย!=	?,	$!$WN.$5

รณ)5 ยป %'<) ยยย ยnย ย  ยฌย ยT	#+$รฉ
.""#$+รฉ
$9=	ร	!

"9b#$"#E=
"P=Y&
-X
	
Pa"#D
-D	b#N
-	5
รฌ<&
JP=	f.
Z%T=
,	=	2-N.!GS+!
+J+%ยทยฐยณ<:K+
ย=	W

!
3<NP$Z'+
.+$]=^K9ยฐi+ UT gยj85 i;qยจยข+	X
$:;&
"#=
G"P=	+
"#fP$R
P$G$"P

	]E;
."#
0+ย#@( 
*5ยยZZ"#'u"#	Lยฉ*ยช9ยซ	ยฌ(yz?P3
f.+P+
$,u/>B
	ยiยZยจ
#@e/,
	?=	ย"#	PDC+*yz,"+P3ยP"#	:?=+,"#	P>\"=	Df+L+			!
+
v ยฃ :+0"=	DZ]S
ย!.+5 ยป f$"#
-$ย
0=	\	
f!$"#
:;$"=ยP3

.L"+0\"#($ >]/?"#Z).+>+%JP#N	$V+รจ."##,t =	$Z"#'L
E
PL
L]+SXXu""#$&
-q/W/Gu+P$ย+ ยธ.P$ย+ยยยน\`!	$R	VP$5 ยป
รถ>รท
รธรน-รบ
รท@รป1P3
</a
=_#@	+?,C E;2H!ย!$R	"#[KP."#FX
	B4:P#N	MX
	B4.5 i;bยจ,E;
"=	DZP."#G=	AN$รจ+*:;=	^
 i;LยฎfE;ย"=	Df=		-$/GAX
	B]Z
."#$4:=	ย
 i;bยจbE;'G"=	DM=	T"P=
XW"#$)
	,,	
	JP=	P>$"P
xN$?D
=	ย
 รถ>รท
รธรน-รบ
รท@รป43
$5 ยป K=
)+
$:+=	;/>"#Pb.E_C=$_)ย"#/G#b#@	$"#	$45
I+
"#M=
Q	Eย"#DP+wP"#	M$F,=	b.+	+#Gยท+ยฐยณV'+>/W(
XNw"
G.+P.
+f>P	cE=	V=	,M+%8Z"#

-+A=	$+	$ยX
	B	+bX :)		$

?/>3
-D.S
	$45ยฅI+
-/,
X+!:W	uf	=	2-N.A
.+!
2H
-	u$+."P=0
?$4:M=
'
+		D"=Z	P$$,ยท+ยฐยณ<yรนY"#/G	$65\I(!/>+
'"P
f
C
++P$VZ=	??+%
P.+
X2

	0$+."P=*:;=	+E;$:+ย=	A
?+	=	?	/ E;
P=`	/>
"P
&ย	f=
'
+		D"=f;E;#X [9/,
M3
-.;"+		;	$"#$+!
X?b!%P/G$W
f$R	"#5;v;=	M
	/ยฅPร/?K%'/ยก=	M%H"#;=+3xรฉยฉ9ยช*ยซ	ยฌ,3
.Q
+-#N	$?/GDF
+."#.
%+x+E<$WDGP#N	$L
5 KK$:=	MG'+F		$?>,"3X4R
 ) ยยย ยnย ย  ยฌย ยaT#+$ร
 UT(gยj
+?==	f"+		c=$J,.+ %P/>+&
W
+
	JP."#
-ย	X
$?GP=	/รฑยHE;
P=		

+
X	G	/>+!
"P
H)ย.5v;=	Y	$+PG,.3*)D&
-J+	
-F,=
;	/V[
g oM#3>/G	
-	Y+>P3
.F=+MG	F
+M#"#
*:"P=V}รผ}รฝ รพรรท@รปรฟ3*eรฝ7@รธรทรท.
รพ4
ยรพยธรท@รป++
 รผ}รฝ รพรรท@รปรฟ3*eรฝ7@รธรทรท.
รพ4:A@รท1 รนรปรท:1	
X+	=	bP3
JP=+!
 CA;nWb."#b=J)i	2
X
-$45
g %'/ยฅ3xรฉP."#
-Q


Sร-:#%'C
	G+>#@	&
V	+	.+!
57M
g ^&	C=	%K*ยฐย+ UT gยjq#+D+\3x+EยP=V#@	&
\+."#
V+b+f	(5
ย0=
X;=
'<+		"=G"#
XNw"#$<!(P/>+
"P
- :+P=	=	c
=<=	TS+.+$%9!$+."P=
"#s
P$"#$ยD0ยฉ*ยช9ยซ	ยฌS2H\.+ %P/>+
XยE;
XXb+ย1?=	
"#$$ &
U\+%YP=	
+.SXรฉ$."=V!"#5K^H\+ย"!:	=	J+		"=V!
XX4+.+$"#/G	$65
ยพ49รรรร9รSรPรzรรรร;รรร*รรพร'ร'รร;ร&รรรร-รรร<รรนรรรPร4ร'ร_ร'ร3ร<รร+รร3รHรร-รรร.รbร'รร&ร$รรรรร$รรร$ร'รรbรรO!(	ยพ
N

รnรฆ\{

ย

รฒ ยร

jQP

 l 

lntQย9ย!t

yz{z|~}aยยย

ยย	ย



nรrmtmrย ยv	ยv

v;=
b!$"#
Z"+\\B."P=\=	G)D&

xX

X$ย%M
-D.+
	W=	,
$M%_.&%/>+&
S
+		#
M=	Tยท+ยฐยณA%./GE;B45*7			c$$+."P=,E;
XX	
-/G/GK=	$F
$F+,$!KJ
E=	=	bP=	VE;BVMP=	Y	P
b$"#
\	$!.5 ยป 
/G/G.+!
fE;\3!ย3x+E
+
,
$QS3
	M=	#
F	
XX
b+%*
Xย*K3
K=		
'
"5ยZ!$"#;=+
รผ}รฝ รพรรท@รปรฟ3*eรฝ7.54761, /@รท!รฟยรท8Kรท
รธรพ<+ย
 รผ}รฝ รพยธรท@รปรฟ3*eรฝ .54@*\รปรท
รธรน;.0/<, รพ1, รน;.bE;s	+
;=	c$+$!1
'	+"#:		KE;
E;
XxรฉP$;=
;)#X
#%5
^&<E;W3b)
$!
	b,	X
"+c	_+S( 
+%1ยฉ9ยช*ยซ	ยฌb%'<P=	<.&%/>+&
S
+		#5KยZb)#X
b=
'<E;'G)Y,.S
=%E_+#,#@(#"P
M
f/>D>"$5<7e;#@e+/W:
=	WN.G]=+!
 RTK(
ยฑ Uw)ยฒ KwยทยI+
-/G/G:;ยฎ$ยฏยงยงยT#+B$CE=	L		+
	ยZ
s\	
X]
"+SรฉP"#	;X
-B=		ยทยฐรยณ	
X	5ยI+
"#รง
 RTQ
ยฑ U46ยฒ Kwยทsย'X
-BTยฉ9ยช*ยซ	ยฌDย$;b
'"=>!<+%1P3

=		
!
"=+K/>."=s%รถY"+SeP"#	$:E;ย!$"#F=+K=	,"+GM$"#/G$C

ยท+ยฐยณ*2-X
-B	
/a

$K<E;bยฉ9ยช*ยซ	ยฌ	yz5 ยฃ 	J
Xยย"#C
GP=
;+3	&
E;?"#"#ย
 RTK(
ยฑ Uw)ยฒ Kwยทyรน
/G	=&
'Z"#!
;E;
-=V/GP
"b#ย*$"#.65JI+
-"#aยทยฐยณ<yzMยท bยฑยฒ'ยฐDยทC	$!D.
\D$M	b3X-SE
"#

-3<#ย*$"#.,ย&	J=	>"#/G		P$ZZ+
=	/W
"s%	"#!
#ยb?N.J!ยE;'\?
	Zยทยฐรยณ92HW."#!
VPV=	ย
 K4ยฉ*G
ยฐ TQยฐZย P=DZยตยกยยข#4:;ยฎ$ยฏยฏยจย;S
 R
ยซ jT ย =Zยต
ยZ#4:ยฎ$ยฏยฏ q(ย+		.651ย0=
X-ร
 Kwยฉ9G
ยฐ TKยฐM=+$K"#
!
3#ย*$"#.+b	
.3RD
xNw"+
*:

J$b	C/>+."Pย
= R*TQ(
ยฑ Uw6ยฒ K4ยท,
ยข#@(	$P&
	$6T
5 R
ยซ jTF:*=	+E;$:9=+$s/G
"C#ย*$"#.C+
"#
	"P=+	5
U;รบWV

9ร

}^'<@ร@รzร +

ร

รรฟยครพ

ยZb=A E;,DS
V"#"#
	?/G

"S9!
$[
ยฎ5;PT/>B;/G;	$"P
!_P=	+	+C#@	K+%รฉ)$	>P=+<"#Cc$3X
U$CJ&
-	
x
	.+2'#N;+	
-	:	+
ยจ	5;Pย"#/G+Vยทยฐรยณ0รจยฐ	ยฑeยฒxยณKยฑ0ย ยดb+/,	=+/W+
Tยตรฑยญ-$:Tยฎ3ยฏยฏยจย.:E=
'"=L		.!	$>&
-/,
X+

'$;E;
P=
ยW
Xย*D;	
	,%'.+/GE;B45
v;=	cE_B,fยฐ	ยฑยฒXยณQยฑย"PD!#,+.3X-#	<+E*[=	MBC
$
>)=>"$Q
K=<J	2
+ 
T	
	G3!
=	/รฒ"fJ!$?>	+	FX
-	.+G'+:	+
'$?=+Mย.ยฎ+ยK=	b		
BM/GC$"#.A+%8P=	J$!F%'
-."P=	+
"#$:\ย ยจยF=	J		"+."#T;E;#X8
/>+BW#N	/GY"P=	+
"#$5I+
"#fยฐ	ยฑยฒxยณKยฑiยยทยฐรยณ0=+WG+%TVnยท bยฑeยฒรถยฐDยท2-x
B,	$.+!
*:
E;E;T+;CX
";=	#@	
/WD.Q	.+B>Gยดb+/,	=+/G+
wGยญ-Tย#ยฎ$ยฏยฏยจย
+f"#/Gb	$!.;E;
-=>P=	#
. * 5
X

ย

ย

{ ย ยZY

ยn



vlrzln



ntl

7 
.,/GW"B	4[,=	Vยฐ	ยฑยฒxยณKยฑL#@()
/G.sfE_f	.3ย"P'$C+%	"B2H."B
	
9
	/?:K+/G$\[Oยฑ L +][Oยฑ L_^ 5][ยข
,+ยข
?ยH.+	
	f%'/ qLยฎ$ยจย$ 
+
-	qP=	
D	/,M+%1-("PB(_
D+$a
f=+c	/V5
v;=	sN.b"P:e5ยF
5 `ยฑ L :
+-$+A


S9"#N	.+
-?
VEc=
"=\3X1P=	J	"B	M+J
=	>#+>+]"P$+35qv;=	?DS<
ZP=	a
 [Oยฑ L_^ 	-/>M
,3'V	
X]V#"Bยข+%=	#
=b
 [*:
		Y3XK=	G-("PB(b,	J"P$+bยข=	,.+C

!
3XX?
ZP=	$,	/> t`/GG-("PB(b"

ยพwรธ6รร_รธรรร-รรร#รWKรXร#ร1รFรรรร'ร ร3ร'ร-รรร#รbรร	ร'ร$รKร6ร รร'รร$รรร1ร+รรXรฝ4รรรbร'ร$รKร	รฝ4ร<ร'รร'ร'รร;รยพ
-c

รk	$ร

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

A

B

C

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

A
B
C

J

K

ร

I
L

3BS

ยzy	รฃยยyยยzย"ยz{

I
K
J
L
LL

LL
5BS1

79
	Jรด	[Kv;E;1~Q	"PB(E;!x?KP/>
W\\+%<$"P=Z=	35Jv;=	C


'3Q!.+s%dยฑ LZ^ :	%'b#@+/G:)
-

3xรf		#M("PBT?
L%T	"ย
B ยฒw5 ยฃ =	J
 [ยฑ LZ^ 	/?J=3fE;VG=	f3
-.C+%T	"B	G#"B$\

-
3X-:
=		=V=	,+C	G

-
31."B	;+%K=	CT/WJ-("PB(6579
	JรดG!=	SEM;

-
31+ยN3
.$4%'E_M#$"#$C	-/>Kย"#/G;$"P
xNw"+
9%'=	L
 ยยฑ LZ^ 	/>K"+C;%'	
#Ec=	,ย ยญM+	B(cยตยยZ#4:4ยฎ$ยฏยฏยจ	ย	ยดb+/,	=+/G+!
9ยตรยญ-$:*ยฎ$ยฏยฏยจย.5
vย=	\ยฐ	ยฑeยฒxยณKยฑu#@	!
/G.C
++$]"#/G+!
	Z=	f		$yzC%'/?+"#Vu	/
E=	ย=	f+LEC	.$ยข%P/ "#.."=LEย
=ยข
.C%'/?+"#VE=	LP=	q+-	
]Z
/>SX	/รชEc$\MWX
	.>'+*5%eยฑ L,รgf ยฑ L +H`ยฑ LBร dยฑ LZ^ +PJE;G#@e/G
#@	
/WD.6587e;#@e/G;
: `ยฑ LBร dยฑ LZ^ 
D+$K"#/W+
	,=	M
/GM$R
$WJ	#+b
+W%ย!+
	J=	
 dยฑ LZ^ %'/ยฅ"#.+#"=?E;
=W=	b
/GM$R
$a%ย!+
	J=	h
 dยฑ L_^ 	-/
.+P
	,E;
=fG	
W%'i
 `ยฑ L 5

รฏcGP=+J=	$!>#@	
-/GD#b
-DG=	f	+	#+
\		"#$,tuP=	>	/ย+%b2
$"#
	G+\+			!
+cX
	.G+fEF	M"#&
$45

/ยยย%ยยย ยJยซ	ยฐ	ยฑ4ยซDยท+ยซj\bDยณb*ยฒ	TLj
j

I$ยณLjRQKยณkRKยซ

ยZ!
$,M
/a
.+cยฐ	ยฑยฒXยณKยฑ*yzF	P$D#+
C	+K"P#G&
[)=G	P$D#+2

)=3KE;;	$
"+$6: F &b+ย
 =ยฒยณGยฐ)ยฏ F w:#+bE;<	!
/,

-8"#
g: kยฏรยยฑยฒ F =eยญcร F &cรยฑยฒ F =eยญ
+V
 k:ยฏcรยฑยฒ F =-ยญร F &รยยฏGยฐยฑยฒGยณ5 (4
ยฐ	ยฑยฒxยณKยฑY!$K=
#+."P=
"3D	P$D#+
*:6
"P
-	T		2'	!
/,

-_"#!
4#@		$&
-	J"#	2
#" 	.cX
Bfยธ.ffยฐmยฑw:eN.b	#+G?+V"P$+{ยฐw:4P=	q	.+G?+\f"P$+{ยฑ4:
=	]#@($"#	fP=	VยH	!
/,

-S+ย9kยฏcรยฑยฒ F =-ยญร F &cรยฑยฒ F =-ยญq"#
-*5zยนยฅยทยฐรยณFyzC	$.+
-ย"#&
'.
b+%w$P"#
	
)%1=	FE_c	
/,
!
K"#
651v;=	"PD!$1+3-	
,ยท+ยฐยณf=
#+."P=
"3
/>3
-	2H$"P
XNw"cB	SEย$c
K=	M	
-G+%1$+#"=	2H"#	
%/>+
-*[1+	x
"+
	2H		X
$
%	"#!
M=+JP/,
	,E=
"P=\	(a
\=	G.	=q%<+
S8MV"#&
b	#@	$:4E=
"#
<P,
	"#:
fE=+;#$:	=	+Eย	$"#
-
;+bPG,"=
-$4:	+V,*5
ยฝlยพ\*ร$รKร3ร.รร!รรรMร'ร$รร.ร'รcร$ร'รร'รรร'ร รMรรรยรกKรPร<ร$ร+รร;ร+รร-รoH*รร$ร6รรรรรยฝ.รท6ร+ยฟ1ร3รSรร3รรZรพ9nรฅ9รรรร'ร<ร;รรร-รรร#ร$ร*รรรHรPรFรรร$ร
รร3รfรรรรร รรรฉรP[
ร รฝ4ร&รzร8รPร;ร.รPร-รรร#ร$รร'ร6รรรร;รzร:.รMร$ร#ร'รรรร$ร,รร.ร6รรร?ร+รm1naรsรรรHรรFรรรยพqรธรรรร$รรMร$ร,รรรHรPร<รรร$ร รSร#ร$ร'รร
รรรGร'ร3รbPร รพร+รร-รรร;รรร'รร3ร'ร ร'รรร'รรWรรย
ร รกKรPร<ร3รSรPร_ร+รร-ยฎ
ร oo*รร3รรรรรรยฝ #รทร*รธรรร-รรร#ร 	 รฅ&(
ร รฝwรcร.รFรรร'ร'รร?ร'ร$รร รรร'ร#ร
ร.ร3ร_ร'รร3ร'ร ร'รรรHรPร-รรร#รp
ยพ !-":รผ รร'รร$ร'รร'รรรHรร-รรร.ร?รรรร'รbรรร3ร&รรร$ร3รร<ร'รร.รรHร!ร1ร$ร#รรรรร>รร รพ#รรร#ร_รร)ร.ยพ ร$ยพ	ร#ร$รMร'รSรPร;ร$|ร รSร$รร
ร am1nGรPร$ร#ร'ร$รร]
ยพ fร3รร รMร$ร#รMร$ร'รร#รรร$รMรXร.รcร$ร#รยร!รรรVรร รพ#รรร#ร;รร1ร'B
ร รฝ4ร
qr1stuv m1wYรPรMร'ร$รJรPร$ร'รร$รรJรPร<ร#ร$รJร6รรร
รรร$รร#ร'รSร#รHรPร'ร รbร'ร+รร9รรร6รXร.ร'รรPร-รรร#รรรรร'ร_ร'ร3ร<รรร-รรร.รbรร3รbร$ร'ร#รรรร รLร3|ร รSรรรร-รรร#ร$รยพ
รk	

ย

/ยยย%ยรรซ
j

yx

yz{z|~}aยยย

ยย	ย

TQj4bยฑ:TQI>ยฒ)j*ยฌTQยฑa;ยณb*ยฒXTQj

v;=	C
M	fD
M"#$!"#?E;]ยฐ	ยฑeยฒxยณKยฑ*yzs=
.+."P=
"3K+\	P$D#+
\+
ยท+ยฐยณ<yรน"#+4BD	+E;$:,=	JR	$
?
-/G/G$
+P#>+;,E=+T"#DP+4
%/>+
X
E;Y=		+
s
VP		
	>P=	Y#@	!
/G.5Tยทยฐยณu"+V#@	+
b/>S
	2HJ"#+

%'/>
+G
-f=	J'"#$[
ยฎ5;>$"P
C=	SE G/>+#"=feยผ $"#._
V=	WยH+
4ย1x
	.+?+VD3
;=	C	ยผ$"#.F
=	

				-/VyzQ


'31+?D34%'/>6:
ยจe5;G$"P
JE=
'"=f+P
3*+fPa"#&
	#@	$:+
qe5;G$"P
JE=
'"=f+P+%1=	C+
'31ย'
-"#/G+ยF+f,E;BGf	#@($5

v =	bN.c
$"#J+%</>S
	2HJ"#+4
%'/>
W
+$F=	+EO,N=	sX
	.
ย
+?,=	b	E0	-/V: (+( E=
"P=G
-D$<"P=	D&
	G"##+D#
?=	M
			;	-/ยฅY!		2

-	;%'"#.D.Q
?=	MX
	#+,+*5ยZJ	$GP=	J+/GM+x
"#>;
fยดb+/,	=+/W+

+Vยญc$[;"P=	DD!J=	J	!
	
-f=+/?3@D
-/,
U$;=	b	/,M+%K
			D3*%'/>ย=+
"#3x>+	)$+
-f=	J#+&%'/G$aX
	.G+*:)+?
fP=	Y"b%Q,!
J"=	DC=	J	!
2
	
G=+F/>3@
/,
U$F=	MD	/,)_%*


'3w"#

-
?=	c
			;	/ยก=_+	)$+F

=	b.&%/G$WX
	#+G*5
v;=	,	/ยฅ
'=N
	fP=	Y	
/>3/>+	
	V"+\)aR
C#@()&
[c
X%<P=	J
-			
	/รฑ/WD
-
 รฆ]	ยผ $"##T+ย=	CX
	.+Pf	/รฑ/G

 ร	ยผ$"#.:)N
-	f=	G$!
/>+	
	M/>3M
+K#@e+/a

	b3X{z

ร

ย

<|

)D&
-
XX

X$5v;=	_3	&
)
?ย ยญM+	B	1ยตยยZ#'4:Dยฎ$ยฏยฏยจย

 /GP.+$;P=	Y)
31"#D!T+%</?+	
	?&
	G=	,#@+/GC+%<+
	G=	 f ยฑ L 	-/

&
	>!""#$&
#-G;X
-	.+G'+5<v;=	,"#/G#@
G+%<"#/G		!
	G=	b	!
/>3*/>+	
-	
+ETs#@()	D!
3X\E;
-=Z=	f&
U?+%T=	WX
	#+\+]VP=	>)+
YEc=	ย!+
	V=	 f ยฑ L
	/รซ&
	ZV!+	
ZV#@"#-qP=	ย/GG	-/ JAX
	.VZ
C"#3xV/G
#@()&
C=+&
	>?/>3xFX
	.+P>ZยHfP=		=f
-$R
$;	f	+	.
V+M3XHย.5
ยZG	PG=+b=
'c
J&
-/,
X+Mf=	ร
 MG365	Yย5X36E 8<AยยถYย;@ยฅ	$$\Z~A
DP
Z=	>"#D#@	b+%
ยบ ~;รญ0ย~f
-D*:Kยฎ$ยฏยงยงย#5,^Hย	!$R	,#@()
/G.bE;,$]?=		
!
"+:1/?3
	2H$:
X
	$+P2'
/GM/>+	
	?3!
=	/V:	$"#
)$?
\ย ยญM+	B	ยตOยZ#4:9ยฎ$ยฏยฏยจย#5
ยป "#+;+x
"#%,=	f$"#]$"P
&
-ยP$R
P$J=
X%'
	A%P/ย	$=	2-N.G$+#"=]
f)$2-N.GP.+5v;=	G-	JG
"#!$J	Y#+	B
-	f%'	"#
ยข
ย.S
X 5Vv9Z"#	2
+<$"P
&
-M+%_P=	G=
.,ย&E=+ /w$EO
-qP=	>"#	b+\V	P$M	#@(6ยE_,	
X-J
$+."P=	2H"#+=		!

"F=<$
SXM
/G-/GDP$Gb)+X
"#,+%<ยธ#	
XG#"B	9%'/ยP=	P2
/r	*5zยน0ยZG"PBD	+E;-$J=T=	G	
-
+%_/>3
-q)$"P
XNw",=		
!
"M"#/GX
"$P=	
"#/G+
'?E;Vยท+ยฐยณ<yzF!%P/>+"#J+?=_%<ยฐ	ยฑยฒXยณKยฑ*:			cE_b+	b=+;=
;	
!


,ยธP%รถ3
-.ยน,$"?ยฐ	ยฑeยฒxยณKยฑ\$=		
!
"M
%'/?+
?
-.#X%5^&ยยฐ	ยฑยฒxยณKยฑ*yzM",=	Y/>3
\2
"P
XNw"B	+E;$cDBJ=	c%/ย+%1C<+%*.BD2'P$"#
>P"=	/>.,ยรฌF=+
+B,ยตd~\" oM/W$:
ยฎ$ยฏยง+รณ	ย4.+P=	K=+G.	B
	M%'	"#
:+		<)=>	/?=		
!
""#(B	+E;$5รฎ	2
%P	+#:e
c
M	$+-?
/G&
CV!$P=	a"#$)"#GE;Z=	,E;,%'/>c+%
/>3
fB	+E;$:e		T	#x
/,
+P>#@	
-/GD#:%'#@+/Gs
\Pย ~;+PTยตยยZ#'4:*ยฎ$ยฏยฏ+รณ4ย.:
=	+Ed=T.BD2'P$"#
V"P=	/>+#>"+\	+
J'+		!$	Z=
QยผT&
-
XNw"+
;=+;	#3
	$?ยยทยฐรยณFyz;.+	B
	,%'	"#
5

ยฝ#ยฝ.ยพKรPร<ร3รSรPร;รSรPร-รQรPร$รT9รร$ร6รรรร;ร!ร!รzรร'รรรรFร'ร$รS}.รบ~~5uรย;ร$ร'ร#รรรรรยยWรรรFรรรรรฝ4ร&รzรร3รยรฝ*รWรรร,ร'ร$รcรzรรร'รรHรร'ร3ร'ร.ร*รPร$ร,รรร
ร6รรร'รร$ร'ร'รรMรรS
ร รกรธ6รHรSรPร Wo\ยฟร+รHรรร'ร.รรDยฝ  	>	 รฅ&G
ร รกรธ6รHร+รร +ร	ยฝ 1N.รท รฅ&รรร3B
ร รก9ยร รร'ร3รรรDยฝ >N#รท รฅ4รXร#รwร รพ$รPร;รรรร#ยพ
รk	$ร

ย รยคyzย){	รกรข{zยยรฃย{zยย0{ย ร
รรร 	

KP/
q~cI ร
qc
~ I ร
รณGc
~ I ร
รณGc
~ I ร
รดc
~ I ร
รณGc
~ Iยฎ ร
รณGc
~ I ร
รดc
~ I ร
ยถc
~ I ร
ยฆc
~ I ร
รณGc
~ I ร
ยฆc
~ I ร
ยงc
~ I ร
q~cI ร
รดc
~ I ร
ยฎ>c
~ I ร

รณG~cIยฎ
รด ~cIยฎ

รด ~cIยฎ

ยถ ~cIยฎ

ยฆ ~cIยฎ
ยง ~Iยฎ

ยง ~cIยฎ

ยง ~cIยฎ

ยฏ ~cIยฎ

ยฏ ~cIยฎ

ยฎ~Iยฎ
ยฎ~Iยฎ
ยฎ~Iยฎ
ยฎ$
ยจ ~Iยฎ
ยฎ$
ยจ ~Iยฎ
ยฎ$ยจ ~cIยฎ

ยยถยmยeรคzย)ยzx	ร0รฅยถย	รคยขรฆยยยyz{

KP("+5K!
/GCยH/>$"3ย
ยทยฐยณ
ยฐ	ยฑยฒxยณKยฑ
ยฎ5รยฆ
ยจ	5 รณ
รณ)5 
รณ5 q
ยจe5zยฏ
q	5รนยจ
ยถe5zยฏ
ยฎยฎ5รนยถ
ยฎยฎ5zยจ
ยฎยฎ5xยฎ
ยฎ$ยงe5zยถ
ยจยจ	5รนยจ
ยจ	ยฎ5 q
ยฎ$รด	5 รณ
ยฎ$ยฏe5zยจ
ยฎ 	5xยฎ
qe5zยจ
ยฎ$ยง	5xยฎ
ยจ+รณ)5zยฏ
ยฎยฎ5 รณ
ยถ	ยฎ5รยฆ
รดยจ	5รนยฏ
รณ e5รยฆ
G
ยจ q	5 รณ

qรดe5 
ยฎรณ5รนรด
ยฎ qqe5zยจ
ยฆยฆD5xยฎ
ยฎยฎรณ)5 
รด	ยฎ5รนยง
รด qe5Xยฎ

ยจ	ยฎ5รนยจ

ร

ยzy	รฃยยyยยzย"ยz{

I)$	\w"#5
ยท+ยฐยณ
ยฐ	ยฑยฒxยณKยฑ
รด
ยฏ รฉ
รณ รฉ
G
รดรฉ
รณD
ยฏ รฉ
ยถ%
รณ รฉ
ยถ
ยจ รฉ
รดqรฉ
q+%
รณ รฉ
รด
ยง รฉ
ยฆD@ยฎ รฉ
รด
รด รฉ
ยฆS
ยจ รฉ
รณ
ยฏ รฉ
ยง	@ยฎ รฉ
รด%
รณ รฉ
ยงeยฆ รฉ
รด qรฉ

ยฏ รฉ

ยถe@ยฎ รฉ
ยฏ+%
รณ รฉ
รณรฉ
ยงeยฆ รฉ
ยถe@ยฎ รฉ
ยฏ+%
รณ รฉ
ยถ
ยถ รฉ
ยฏ
ยถ รฉ
ยฎ3
ยง รฉ
ยฏ
ยถ รฉ
qรฉ
ยฏeยฆ รฉ
ยถ(eยฆ รฉ
ยฏ
ยฏ รฉ

v+,ยฎ[;รฌ</G+.+!
)%/>+"#:ยทยฐยณ1+\ยฐ	ยฑยฒxยณKยฑ
X

รA ย  .
ย

r4ยยrzl	ยomnVย	nzvยยiยl	v

vย=	,N.C=	f"#+	/Gs+%Tv1+-Vยฎ>=	+Eร=	SEยกยท+ยฐยณ<yzs%'/?+"#ย"#/G+$sZยฐ	ยฑยฒxยณKยฑ*yz

\++	Pb/>5 (+$ ยZ,"+	
f$.M+D3
-_ 
	>P=
F
%'/>
?>.3Eย+>	
"#"P&
-<+)	K=	M#+!
+_/W
.+%*=	c E;b+		D"=	$[=	c E;M	#+/>E;E!


u
Xย*C+	+$:K	Lu
xยรฉPD,/>"P=
	$:;+]	#
P=	GEC	
/,
-U$]\	("#
=	J)$MD 
b.3E0%'/?+"#J	/,.5 (/ รฏc	=	#$P;E;J	bP=+=	,++	Pb
/G
D	/,).TJ"#/G+#+[<ยทยฐรยณ]$AGE;BW%รถ;q!/>3X;	/>:4ยฐ	ยฑยฒxยณKยฑV)M
+_	P/>:		M=	J	+.WG	T	$;=+#
-=		.+/ร
;"P$+-ย	)
35
ยดb+/,	=+/W+
8+\ยญcJ!$bยฐ	ยฑยฒxยณKยฑ*yzM!%P/>+"#JP#+
bG
-.+E=3
c

	.+!
	,+Q%P/ยฅ"#.+."P=*5<v;=
'<	/,$:)"3X$W=	B
 W0?5	2GFยWL8:;><P7;2\360F;3:
;#N	$f,
J
 ย3ย;ย :4E=	W
 ยC
'=	,
-/GJ$R
$Af+,?	/:5ย)5 ^ย ยฑ L_^ :e%P/รงP"#.+."P=q+m
 รC
'
ย
=	b
/W$R
$?G+-=+/GT	P/ยก&
	G,/>3xFX
	.+PY'+*:5ย5	M%%
 `ยฑ L 5
v;=	M%'	=V+ยN	%P=V"#+	/G;%Qv+Gยฎb"#/GJยท+ยฐยณL+ยฐ	ยฑยฒXยณKยฑf=
;/W
"+5
v;=	?R	$!
]=	#%'>+!
$JbE=Dรจยฐ	ยฑeยฒxยณKยฑ*yzC$	ยD	/,#Y>"#&
'q!
/,"=C+P
,/>+
-;=+>ยทยฐรยณFyz:++!
"#+Xba+	-/>:+GP=		=>+!+	
%'/?+"#G
'b	G&

XNw"+V)$5\v;=	f+E;J=JPqVEย
=]=	>	/>6y*#

%'/?+"#f
L'+	
	ยข%/ P"#.+."P=*5 ยป ?79
	VยถZ/W.$:cยฐ	ยฑeยฒxยณKยฑ*yzW%'/?+"#
.$c/,"=%Hc=+qยทยฐยณ<yzcV	#+
b#B	5ยZC=3J	?#@('++
?%Cยฐ	ยฑยฒXยณQยฑรฉyรน
=3
$:			c
.ย#ยรฉ$"#M=	Y3
	D2')."#.+b	/,c
T"P-$+$[K=	$!J	/Y).b+b=
-=
$"+!Vยฐ	ยฑยฒxยณKยฑ*yzC%'/>"#>ยข	.+
-G.!B(b#$C/Y"P=Z/G>R
"B\=+L$
ยฝรทยพ)ยฟ*รรนรDร+ร ร-รXร#ร'รรร3ร รFรร$รFรSรร'ร9รXร.รZ!-Gรร3รSรรPร*รรรยรกKรPร<ร$ร+รร;ร+รร-รo9รร$ร6รรรรรDยฝ.รท6รรธ6รร ร-รรร.ร 	 รฅ&ยพ
ยฝยรยพ4รธร[
ร รก9ยรร3รรรร รWoย9ร'ร$ร;ร;ร.ร3รร)ยฝ>lรฅรร3รBรก91รPรรรGรPรzรXรร+ร[oย_ย>ยยยยย;ยยย1ย1ย>ย_ยยย>ยยยยยยย3ยย3ยLยย:ย&ยยยกย&ยยยขย1ยhย>ยhยฃ&ยย
ย3ยคOยย:ยยยขยยยยฅยฆย3ยง>ยยยฅยขยกยยฃย:ย>ยยจยยFยยฅ@ยยยยยขยยฉQยย&ย1ยฉ>ย'ยยคOย3ยช

ยซยฌยซ

ยญOยฎ;ยฏ;ยฐยฑยณยฒยตยดยทยถยธยน

1800000
1600000
1400000

CPU Msec

1200000
1000000
800000

SPA
PRIAR

600000
400000
200000
0
4

6

8

10

12

Problem size

ยบยผยป9ยฝยพ;ยฟรhร;รOรร;ร3รรรรรFรยฟ3ร&รยฟรTร"รFร1รยจรBรยฟQยฝรร;รยฟ>รร3ยปรรiร<ร&รร;ร<ยป9ร;ยฝ

ยป9ร>รรรFรรFรรยปรยฟiรรรยฟร1รFรiร1รร3ร;รร{ร"ยพFร3รhรรWรรWร;รFรรยป9รรรร<ยปBรOยฟร1รBร"ร3ยป9รรFร3ร<ยปรรรรรBรยฟiรร;รSรรQร;รรรรกรQรFรpร;ยฟรร;รข
ร9รรรครฃ;รฅFรฆรจรงLรฅรรฉ-รรชร;ยฟรยฆร1รรรยป9ร;ยฝรซร3ยป9รWรJยปBรรชรรฌ<รญรฎรรรฏรฃรงยรฉ(รรรฐยบ=รยฟTรฌรQร;รรรรกร%รFรรฑยป9รpยปBรJรฌรก<รญรฎร"รรฒรฏ"รฃรงยรฉ-รรoยบ;รยฟ
รรณรQร;รรรรกร%รFรยจรร;รยจร<ยพ;รbร+รยฟยยปBรยรด5ร<รญTรตร;ยพ;รยร&รยฟยยฝรร;รยฟ>รร3ยปร;ยฝรรถรทรธ;รนZรถQรBยฟรรรบรร1ยฟ>ร"ร>รรรฑรฃ;รฅ=รฆรจรงLรฅรยฟยพ;รFรOรร+รยพ;ร
รรกรฑร3ยป9รWรรpรร9รรปOรยฟรTรผOร<ยป&รhยฟรร3ยพ<รรhรFรยป9ร<ร>รรฒรยพ;รรรFร"รรร;รJรSยพFร3รhยพFร3รJร1รฝยฆรยฟรรWรaรร"ยพ;ร3ยป9รรรพยป9รรซรร0รรยพFรร3ยป9ร;ยฝ
รร<รaรยรยฆร3รรรรบรFรร3รรฟรรรรรร;รร3รhยฟร1รBรรยยป9รรhร3รFรรรฟ<ยพ;รรฑรFยฝยพ;ยฟรรรตFรยยป9รFร1รhรร;รรรรยฟรรร1รยพFร0รยร9รWรWรรร3ยพ;ยฟ3ยป9ร;ยฝSรร<ร9ร

	
	


รร;ร



รรFรรยรร;รยจร;ยฟรร<ร9รร

ร+รร3รป{รรรรซร3รปOร

ร3รรFรยฟ1รรรTร1รรรชรFรร;รร<ร>รSรรiรHรยป9ร;ยฝ"ร9รTร3ร;ร3รรรGร

รรรฟ<รรยปBรฟยป9ร;ยฝSรป%ร;รรJรรยฝรร;รยฟ1รรร%ร<รBร"รFรLร&ยฟรร



รSยพFร3ร%ร>ร"รร%ยปร5รรWรรร1รยพ;ร5ร%รร;รhร _รร1รยยป9รรร;รรรOร"ร

รWรรBร3รรพรFรยป9ร<ร>รรฒรยพ;ร

รร1ยฟ>รร1รรSยปรFร3รรรรฟSร"รยผรรฟ;รร;ร3ยป9ร;ยฝpรร;รร

รร;รhยพ;รFรฟ<รยฟยร9รยปร;ยฝWยฝรร;รยฟ>รรยยป9รรรรWรรรFรร<ยปBร3รรพร

 "!#!$&%'
(


ยพ;ยฟOร3รป{รpยฝร5รร&รรรป{รยฟรiรรpรร3ร>รร<รรจยปBร3รTรpร3รยฆรยรรรTรร3ยป&รOยฟร1รBรรยยป9รรFร3ร<ยป9รSร+รรยรปOรรรรชรยยป9ร;ยฟ>รยฟรbยพFรยร

รรFรฟรชร;ยฟรร<รรรWรข

)

ร3รร9รยป9ร;ยฝJร รรยฟรรตรรรFรฟรพรรGร1รรWรFรยฟรSรยพ;ยฟร3ร;ร3รรรรพรฉ-รยจรFรยฟ3ร&รยฟรJรรFร1รWรรJรรFรรhรรOรร;รWรยป9รSยปรจรBรยฟpรฃ;รฅ=รฆรจรงLรฅรร

'ร

รร;รQรFยฟ>ร3ร{รรร3ร%รปOรiร;รรร%รรFรรยรรTร1รยฟร1รยป9รWร;ยฟรร<ร9รรTรรตรWร5ร3รยร;รร>รร<ร9รSรร;รQรFร%ร;ร\รTรQรยฟร1รFร>รรต<รร;รยฟรOยป&ร
รJยฟรยฝยพ<รBร"ยฟ

*

รรFรฟHร3ร;ร3รรรJรร3ยปBรรฒยฟร1รBรร3ยปรรFร3ร<ยป9รรพรFรร3รปOรรร



,+

รWรรรยยพ;ยฟรรฟรพยฟรยพ;ยฝร<ร9รรพร5รรพรร;รWรฟยป รรยฟรรFร1รWร+รรยรปOรรร

213)4

ร;ยฟรร<ร9รรรพร

รร;รSรFรhรFรร3รปOรรรรพรยยป9ร;ยฟ>รยฟรGรรFรฟรฑยป9ร;ร;ยพ;รhร;ยฟรร<ร9รรJร

.-/

]รรFรฟ

0

iร"รFรฟGรร;รWร3ยป9รWรpยฟร ยพ<ยป9ยฟรรฟHรรรร3รรรรSรร;ร

ร{รยร;รยพ<รBรฟhร;รรรรตร;รรป{รรรยฟรตรรFรรยผรร;ร{รยปรWร<ร9รยรFร"รยพ;ยฟรLรรFรร;รOรฟ<รรTรยปรรรFรฟhรร;รยร;ยฟรร<ร9รรJร

)5
7

รรฟ<รSยป9ร1รhรJรFรยฟร3ยปBร1ยพ<ร&รยฟ3ร9รรรร<รยป9รยพFรยจรWรรรยยพ;ยฟรSรร

รรHร9รรรSยฟรยฝยพ<รBร"ยฟWร;ยฟรร<ร9รรรชรข'ร3รร9รยป9ร;ยฝรร1รรFรยฟ3ยปร5รร

26

รฟ<รยฝยฟรรSรรOรFรรต รรยรรรร;รร3รWยฟรรยยพ<ร9ร>รยจรTรรGร;รรhร1รฝยฆรรรFรฟ
รรรร;รGร3รร1รรFรฟรรรร3รรพรปOรGรฟ<รรWรรFร3รยฟ1รรรรฟรซรรFรรรชรร;ร

รFรยฟ3ร&รยฟรJรรFร1รรรรhรร;รGร3รปOรร3รยฆรยรรรTรSรป%รรpยฟรยพ;ยฝร<ร9ร

ร1รรWรFรยฟ>รร<รรTร+รรร\ยปรรรรFร3รร9ยพ;รรรรรยฟรTรWร"รFรฟ

ยป9ร

รรยฟรTรรรรรร;รpยฟร1รBรร3ยปรร%ร0ร0ร9ยพ;รรรLยฟร1รFรรยยป9ร;ยฝFร

8 :9 <;>= ? A@CB
ย ยช ย3ย

ยย ย

I

D

ยยฅยขยย;ยยย1ย >ยยผยยย1ย

FE HG HG
J 7J
L

ยOยย3ยยยย3ยยยย0ยยยฅ ยยยขย3ยยยขย

ยฃ&ยยยยยค{ย1ยกยยฃรยยFยฃยยขยคOยย&ย ยกยยขย&ย3ยhยฃ&ยOย&ยยฅ:ยง>ยยยยยย&ย ยฅ:ย3ยค

รยยยขยย

FJ E

K

ยยยย3ยง1ย'ยฅ:ย>ยSยOย&ย3ยฉ>ย&ย3ย&ยย:ย>ยpยค{ยยยยฅยฃ&ย0ยยฃรยย&ย3ยย:ยยยฃ&ย

ยซ
0ยฌ ยย&ยยยhย1ยhยฃ&ยยรย:ยยยกยฃยยยยจยฅ(ย ย'ยย %ยยฅ@ยยiยย ยย3ย3ยช

MONQPR S TVU XW
ยฎ ยยฏ

4

&Y M [Z P]\ SY^ R`_[P\Ca

&ยฏ;ยนFยถ ยถยฏ;ยนFยถยฏ

ยธ

ยน;ยฎ

ยฎ

ยฏ

dc

รbรSยพFร3รhรรFร1รTรยฝ5รยปร

รรฟ<รยปBร3รWรรยพ;ร3ยปรรรพยป9รรพยป9ร5รรยฟร;ยฟรร3ยป9ร;ยฝJรร;รร3รรชยฟรร3ยพ<ร9ร>รร

รร;รรรพร;ยฟรรยปBรฟ<รSรWร;ยฟร1รยยป9รSยป9รFร"ยฟรTรรรยยปBรฟ;ร"ร3ยป9รรรรร{รฏ"รฃรงยรฉ-รยจรร9ยฝรยฟยยป9รร;ร
รFรยฟรรฟpรรpรฃ;รฅFรฆยรง

M W	Y bYS P

ยธยฎ;ยฏ

ร+รรรรยปร

Qร9รร;รยพ;ยฝร

รปOรSรFร1รรจยป9รรร

ร"รFร3รร9ยพ;รรhรรยฟรTรยจรรFรฟHร1รรWรข

รฅ_รฉ(รรร<ยป9รยฟ>รยฟ>รร<ยปBรรรFรร;ร;ยฟร<รรรรรตรร;รยร'รร1รยผยปBรรรร;รOร1รฝ;รFรยฟ3ยป9รรชรร5ร>รยผรปOรยฟร%ร1รรFรฟ<ยพFร1รรรฟpยปรยณรQรรยฟร

ยฟรยฝยพ<ร&รยฟ%รรFรฟTรยยป9รWร<ร9รhร;ยฟรร<ร9รร

รฟ<รรTรยป9รรรต<รป%ร<ยป&รรรรรยร9รรปOรรฟSยพFรOรรยณรรFรยฟ>รร1รรยฟ3ยป9รรยจรร;รhรยป9รรhรร

รpร;ยฟรร<ร9รร

รยฟOร<รBรรJยพFรยป9ร;ยฝWรSรยป9ร;ยฝ"ร9รiร<ยพ;รSรFรยฟรต+รรFรฟSร&ยพ;ยฟรร;รยฟยจรรยร9รรปยรรฟยณยพFรOรรWรรFร"ยฟ>รร1รรยฟ3ยปรรiรร;รยจร1รฝ;รรร5รOรรSรป%ร<ยป&รรรร
รยยป9ร;ยฟ1รยฟรJร<รBรรHรปOรยพ<รBรฟJรFรJร3ยพ<ยป9ร>รร<รรยจรBรยฟยจยพFร3รpยป9รHร3รร9รยป9ร;ยฝรรSรBร"ยฟยฝรยฟOยป9ร;ร;ยพ;รhร;ยฟรร<ร9รร

ร5รGร1รรรชรFรยฟ3ยป9ร;ยฝJรร;ร

ร<ยพ;รbร+รยฟ>ร%รรร3รยฆรยป&รรรรฟJรปOยป9รรTรร;รpร3รป{รSร<รBร"รFรร
ยบ=ยพ;รยพ;ยฟรรรปOรยฟร

รbยพFรยรWรร;รยฟร1ร&รยฟรรร1รรFร1รร<รยฟ>รรรรรร\รยรปOรรยฟรรรรJรร;รGรป%ร;ร"ร9รTร;ยฟรร<ร9รร

ยฟรรยฟยยป9รรรhรยฝร5ร;รฟWร<ร&รรJรBยฟรร
รร{รรรยรรรยป9ร;ยฝFรตยฆยป9ร

gc

ร;ยฟรร<ร9รรรพร

*
e5

รร;ร%รรจยป9ร;ยฟ>รยฟร

ยตรยยป9รSยปยรBร"ยฟ{รรFรรรยฆรยป&ร{ร"ร;รFรรยฟ>ร{ยป9ร

*ih

ร<รร<ร9รยฟรตรร

รรรยพ;ยฟรชร0รปQรรรตLยป9ร



รร1ร3ยป9รร

|{~}

รรSร;รFรรWรFรรยยป

รFรยฟร3ยป&ร1ยพ<รBรยฟpยป9ร>รSยพFรยรGรรhรร;รGยฝรร;รยฟ>รร3ยป9รรTร<รBร"ร;ร;รยฟTรร

รฟ<รรFรรFรฟ<รรZรยป9รรhรรFร"รรรรรพรBรรรยฟhร+รWยฟรรยฟ>รร1รรรฟZรSรฃ;รฅFรฆยรง

รรFรฟรพรรรยปร;ร;ยพ;ร

รด >ร

*ih

รSรFรรรbรรยฟรรรฟ<รGรรชรร5ร3ยปรร;รรฟรรร;รSรปOรยฟรรรร]รฃ;รฅFรฆรจรงLรฅ

รร

รฅรยป9ยฝร;รยฟร >รต5รรFรฟJรร;รiร;ยฟรร5รรร

f6
jXj /

รฟ<รยฝยฟรรpรรLรFร Sร+รร3รป{รรรรยณรยยป9ร;ยฟ>ร"ยฟรTร<ร&รร

kglnmo&prq&s	out`vxwy	z
4

b/

รปQร<ยปBรรรร+รรรรรฏรฃรง]รรFรฟรฑรฃ;รฅFรฆยรง

รรชยฟรรรยยป&ร3ร3ยป9รhรฟ<รรTรยปรรรตFรร;ร

รรhร;รรป

j	j /
ยย

QรรFรฟร9รยฟรตLร

รก รรรiรร9ร<ร3ร

ร;ยฟรรยปBรฟ<รJรรจยป9ร;ยฟ>รยฟร

ร<รBรรFรJรรFรฟ

รฅ]รรFรฟรฏรฃรงรรร&ร3รaรยรFรยฟรSรร;รTรร"รWรWรฏ รรฅFรฆ&รฃ5รฏรขรยยป9รร

ยฟรร;ยฟรร3รร<ร>รร3ยป9รรรรรผOร;รWรTร0ยป9รรฟยป รรยฟรรFร1รWรFรร3รปOรรรHรร;รWร3รปOรaร"ร;ร;ยฟร5รรร;รรQยปBรยจรร;รWยพ;รFรฟ<รยฟยร9รยปร;ยฝ

ย0
* j	j b/

ยย

ร<รBรร;ร<ยป9ร;ยฝGร0ร9ยฝรยฟ3ยปยรร;รรพร%รฏรฃรง\ยพFร3รรhรรร1รรFร3รยฟ>รยป9ร<รรข&รFร<ร3ร3ยป9ร;ยฝWรรรร;ร<ยป ยพ;รTรยปรSยปยรBรยฟQรร

ย:ยยย gย
ยยย|ยย ย *
รง

ยย c

LรตยฆรรOรWร;รฟยปยรFรรฟJร5ร

jย	ยX/

รฆ

รผรรรรรตZร

ยย

Hร %รรจร9รร3รรยฟOรรFรฟ

Oร5ร3รร<ร<รยยป9รร

ร

ยรFรร;รTรรรรฉ-ร

* j	ยย	/
ร

;ร 1รตรป%ร;รยฟรรรยจรฃ;รฅFรฆยรงLรฅรยพFร3รรQรSร0รยฟยยปBรร<รยรร

>รต5รSร<ยปรยฟ>รยฟ>รร<ยปBรรรรร<รBรร;ร;รยฟร

*

รฃ;รฅFรฆรจรงLรฅรรฉ-รยจร<รBรรHยฟรร;ยฟรรยรร5ร>ร"ร3ยป9รรรรตZรรFรฟรพรร<ยพFร%รร;รWรรยฝรยฟ3ยป9รร;รTรOรรFรรhรJรร<ยป9ร;ยพ<รBรรรรร;รรGรตรร"ยฟรรWรยฟร



ร1รรWร<รยยป&รรรรรฟhรรFรรยรฏรฃรงยรฉ-รรรผOร;รยฟร%รยฟร{รร;ยฟรร%รฟยป รรยฟรร5รรร3ร5ร+รรรรรรรรยยปBรฟ;ร"ร3ยป9รรFร

:/

ย

ยฟร1รBรร3ยปรรFร3ร<ยป9รFรยผรFรร3รปOรรร

ย

ร;รยฆรฟ<รรยผยปรรร;รOร<รBรรpยฝยฟ>รร;ร 1รตร&รยฟรร1รฝ=รรรชร<ร9ร \ร;ร9รรยฟLร1รรFรฟยป9ร3ยปรรรรตร;ยฟรร1รรFรฟยป9ร3ยป9รรรรตรรFรฟhร;รFรร<รรร

ย

<5

รปOร1รยรรรยรฟยป _รยฟรร5ร

V6


ยฝร5ร0ร รฐรร

>ยฟรรฟ<ยพFร1ร3ยป9รรSรรรร1รBร Qร&รยฟยรร;ร%ร<รBรรWรรFรรยยฟรร;ยฟรร3รร5ร1รยรhร<ยปรยฟ>รยฟ>รร<ยปBรรรรฟ<รร1รรรชรFร5รยปร3ยป9รร

|ย

รรFยปร>รรยรยฟยพFร1รยพ;ยฟรรตร0ร9รร;ยฝยจรปOยป9รรรฒรFรรiรฟยป รรยฟรร<ร

รยรยฟ>รรรยฝ"ยป9รรZร&รยฟรยฟรรFรยป9ยฟยยป9ร;ยฝiรรรยยปBรฟ;ร"ร3ยป9รรรฒรรยปรจร9ยพ;ยฟรรร

ยรร<รยฟ>รร3ร

รร<ยปBรOยฟรร;ยฟรร3รร<ร>รร3ยป9รรTรปOยปรรGรฏรฃรงยรฉ-รรร<รBรรรยฟรร;ยฟรรยรร5ร>ร"ร3ยป9รรรร1รรFรยปBรยร3ยป9ร;ยฝSรรยรรยพFรรรรรรจยป9ร;รยฆรOรรFรฟรพร3รรร;รข&รยฟ1รฟ<รยฟ
ร1รรFร3รยฟ1รยป9ร<ร>รร

h

* j	j
ย5

รฃ;รฅFรฆรจรงLรฅรรฉ-รรรWรยฟร%ร1รรรชร<รยยปBรรรรรฟpร<รBร"ร;ร<ยป9ร;ยฝรรFรฟSรรรยยปBรฟ;รรยยป9รรbรยรยฟยพFร1รยพ;ยฟรOรTร"รรรยผยป9รLรFรยฟ>รฟ<รยฟLรรhรร0รร9ยพFร"รร
รร;รbร0ร9ยฝรยฟ3ยปยรร;ร

ร&รยฟรTร0รยร9รร

ย}

รรSร;รFรรWรFรรยยป

รรFรฟ

QรรFรฟร9รยฟ

ร

รรFรฟWรยฟยฝยพ;รยยป9ร<รBรยฟรTรรยรรยจรBรยฟยรยจร;ยฟรรFรยฟร3รpรยยป9รรรร1รรWร<ร9รรรร;รรรร

	jย/

รก;รตรรร%ร

ร;ยฟร0รรรWร3รยพ;รFรฟ<ร;รรรQยฟรร3ยพ<รร

>รปOร%รรBรยป9ร

รรFรรLรยพ;ยฟรร&ยฟ>รรWรรปOรยฟรSร1รรรยฟ>ร

รรยร%รFร<รรยป9ร<ร9รJรWร;รฟยปยรรรร3ยปรรFรรฒรBรยฟSร<รBร"รFรSรรFรรWรยฟรรรฟ<รรร1ยฟ3ยปรFรร<ร9รรรปOยป9รร<ยป9ร]รร;รGรร1ร3ยป9รรรซยฟรร;ยฟรรยรร5ร>ร"ร3ยป9รร

26ย

รฟ<รรร1ยฟ3ยป9ร+รรฟรพยป9รรร<ยปBร%รFร"รFรยฟร

รhยปBรQร;รรรร9รร"ยฟhรร;รWร1รฝ=รร1รhยฟร1รBรร3ยปรรFร3ร<ยป9รรพรFรร3รปOรรรรร<ยปBรยจร;ยฟรรFรยฟร3ร

ร"รFรฟ

รยพ;ยฟiร1รรWร<รรรรร;รรรOร;ยฟรรFรยฟร3รร

nยยยยย * }

รผOร;รOรปOรยฟรhรรWรรฟ;ร"ร;ร>รร3ยป9รรยจรBรยฟยรรร3รรข&รFรร3รรฟpร<รBรร;ร<ยปร;ยฝhรFรรรรTรยป9ร<รรhรFรรรTร1รรFร1รยฟร;รรฟSรปOยป9รรpรFรFรฟยป9ร;ยฝ
ยฝร5ร;รฟSร3รยฟ>ร"รรยฝยป9รร_รBรยฟยรร;ร<ร9รยป9ร;ยฝ



xย

รรฟ;รร;ร>รร3ยปรรFรร รรรรร1ร3ยป9รร

%รปOร%รฟยปBรร1ยพFรร3รรฟ

j	j /

iรรรชรWรรFรฟZรต;ร

ยย

รณ

ยป9รรฟ<รร>รยปยรยรตรร"รFรร9ร<ร1ยป9ร;ยฝJยป9รQยป9รรรยฟรJร%รร%รฏรฃรงยรฉ-รhรรฟ;รร;ร>รร3ยปรรGร;ยฟ3ยปรSยป9ร3ยป9รรรรSรยป9รFร1รTรฏ"รฃรงรยพFร3รรยจรร;รJรฏ รรฅFรฆ&รฃ5รฏ

ยฟรร;ยฟรร3รร<ร>รร3ยปรรaร"รFรฟGรรร;ร;รรQยฟรร;ยฟรร3รร<รiรยป9รSยพ<ร9ร1รร;รรยพFรOรร1ร3ยป9รรFรยรยฟiรร1ร3ยป9รรFรยรปรยป9รรJรรรWรFรยฟ>รรร1รฝ;รรร<รรต

&ยยยยย

รป{รLรปOรยฟรยรร<ร9รhรร<ร9รLรรยจร1รรFรยปBรฟ<รยฟรรรรhรร

}

ยฆรฉ(รร3รรรร5รรรรpยฟรรFรยป9ยฟรร3รยฟ>รรรยฝยป9รรร Qรรป{รรรยฟรตรปOรยร1รรFรยปBรฟ<รยฟ

ยป9รLยป9ร<รรยฟรรยร3ยป9ร;ยฝhรรFรรยร<ยป9ร;รยจรรรรร;รร3รยจรยฟ>รรFรร&รยฟรJรร3ยป9รรFรยรร"รWรFรhรรFร1ร;รฟ<รรฟรรยป9รWร<รรยณรรLร1ยป9รร;รยฟยรร;รยจรยฟยร3รปOร
รรFรยปร;รรฟรรฏรฃรง]ร;ยฟยยป9รSยป9ร3ยปร0รรร

j	j / ย

รรร1ร3ยป9รร

ร

ร 1ร

ร;ยฟรร<ร9รร

* j	j /

รกJรรBร3รรรฟยปBรร1ยพFรรยรรฟรฃ

ร1ร9ร5รยร

ร

ย ย	ยgย * c
รฏ

j	ย	ย/ Cย ย|ย ยย ย *ยกย

Qร9รรยฟรTรรรรตรร

รก ร0รBร3รbรฟ<รรร1ยฟ3ยป9รFรรQรhรWรรรFรร<ยปBร3ร

0

OรรFรฟ

รฆ %รฆ

ยข{ย

ร1ร9ร<ร3ร

{ร"ยฟรFรร;ร1รรจรยรต

ร<รWรป%ร<ยปBรรรรรร3รยจรWรรWรยฟรpยปBรยร1รฝ;รรรFรฟ<รรฟรรฟ<ยพ;ยฟ3ยป9ร;ยฝ

ร3รร9รยป9ร;ยฝFรต=ยป9รFรร9ยพFรฟยปร;ยฝWร9รร"ยฟร<ยป9ร;ยฝJรรรร;ร<ยป ยพ;
ยซ ร

ยฌรQ
ยฌ ร&รยฟOยป9รรชร;ยฟรรยป9ร;ยฝJรร;รSรยป9รSยปยร&รยฟ3ยป9รรTรWรรยฟ3ยปBรhยพFรยรรฟรยปร

ยญOยฎ;ยฏ;ยฐยฑยณยฒยตยดยทยถยธยน

รยยป9ร;ยฟ1รยฟรยฟรรยฟ3ยป9รรรรยรLรผOร;รร3รOยป&รร3ยพ;รรยรFรรร%รFรรรรร1รรWร<รรรร1ร9รpยป9ยฝร;รยฟรรฟpยป9รWรยพ;ยฟOรฟ<รรร1ร9รร;รรชรร5รยรรรฏรฃรงยรต<ร;ยพ;ร
ยป9รยยปBรOร+ร5รรยปร<ร9รhรรFรร%รร;รรaร1รยพ<ร&รฟJรFรbรรฟ;รฟ<รรฟรรรSรยพ;ยฟiรยรยฆร3รรรGร

* j	j / ยฃc

n}

รรรWร%รรร3รรข&รFรร3รรฟยณร<รBรร;ร<ยป9ร;ยฝhรปOรยฟรZรตร&รยฟLร1รฝ=ร"รWร<ร9รOร5ร

ยจรรWรWรรFรฟ

ร

รณ รรรFรฟ

* j	ย	ย/

Qร9รรยฟรTร"ร

ร

uยค

1รต

รรBรยรTรรฟ;รฟ<ยฟรรรยรรiรยปรยพFรร3ยป9รรFรยยปรรรป%ร<ยปBรรGรร;รpร<รBรร;ร;รยฟรฉ-รQรฟ<รรTรยป9รรพรWร;รฟ<ร1รรยปBรรยป9รFร1รรWร<ร9รรรรรFรฟ รยฟOยปรFร1รยฟรข

*

ยฟรร1รรHรยรรรรซรร%รร;รร3รรร3ร;ร3รรรTรpยฝรร;รยฟ>ร"รรTรรพร<รBรรรซยพFรยป9ร;ยฝHรGร;ยฟรยฆร1รรรpรรยจยฟรรยฟ3ยป9รรรรรร"รFรฟ]รรฟ;ร"ร;ร>รร3ยป9รรรรต

7ยยฅbfยฆยจยง

รร;รร

<

รฒรร;รSร<รBรรรร

u/

&รOร1รฝยฆรร1ยพ;ร3ยปรรรพรรยปรจรBร

รรรร;รยพ;ยฝรรพรร;รSรWรยฆรฟ<ร1รLยปรFร1รยฟยฟรร1ร3รรTร;ยฟรรฟยป&ร1รรรฟHรรFรรQยป9ร

รปOรยพ<รBรฟรพร3ยพFรร1รรรฟ >รตZรร;รร3รWร3ร;ร3รรรJรOรยฟรรรรรชร9รรยฟรGรร;รpยฟรรร3รรFรQรป%ร<รรตร"รFรฟ
รยฟรร3รรพรร;รTร3ร;ร3รรร

รฟ<ร5รรpร;รรhรJรรรWรร;รรรรรรชรWรSยปBร3ร1รรรWรยฝ5รยปรรรTรฏรฃรงoรยปBรฟ<รร3รรรFรรฒรร<ยปBรpรรFรรรจร9รร;ยฝยป9ร;ยฝ

ร;ยฟรร<รรรGรต{รยยป9รFร1รรยป9รJรรฟ;รฟ<ยฟรรร3รรรชรร<ร9ร\รร;รรร;ยฟรร<ร9รร
ร;ยฟรร<รรร

รยรรยฟรpรร;รhร'รยปยร9ยพ;ยฟรhยป9รรพรWรรWรข

ย

รรรร;รรรฟ<รข&รร&รข&ร3ยป9รWรรร<ร&รรรยฝรร;รยฟ1รร3ยป9รร รร;รรWรร;ร

Hย

รรLร1รฝยฆรร1ยพ;ร3ยปรรGรรFรฟJรยฟยฟรยฟ%ยฟรร1รรรยฟรรยรผOร;ร

รรฆ&รฆรร<รBร"ร;ร;รยฟ

*Vยฉ

ยช
ยฌ Qย

{ยซ4

ร"รBรฟ<รรรรต Lรร1ยป9รร<ยปยรต

ยพFร3รรยจรWร<รBร"ร;ร<ยป9ร;ยฝWร&ยฟ>รรรชรรป{รยฟรaรยยป9รSยปยรBรยฟ%รรTรฏรฃรงยรฉ-รรตรฟ<รรร1รรร<ยป9ร;ยฝTรรชยฟรร;ยฟรร3รร<ร>รรยยป9รรGรรFรฟรพรร9ยฝรยฟยยป9รร;ร
ยฝรร;รยฟ>รรยยป9รร%ร<รBร"ร;ร<ยป9ร;ยฝhยป9รรชรร;ร%ร;ยฟรร3รรFร1รhร"รยปรFร1รรWร<ร9รรรรยป9ร<รBรยฟรTรร3ยปยรร รร;ร
รป%รFรร%รFร"ยฟร3ยปBรร+ยป9ร<ร&รยฟรTรรยยป9รรรชยป9รOรFรรยยปBร%ร1รยฟยฟรร1รรต<ร;รรปOรรรยฟร



4

รรชรWรร5รยยป9รร;รรฟรฑยป9ร\รรร1ร3ยปรร]รกรรรFรรhรยพ;ยฟยฝร5รรBรQยป9ร

รฟยป รรยฟรร<รรรBยฟรร

jXj /

ร1รBรฟZรตZร

รด

ร&รยฟ

รฆ&รฆ+ร<รBร"ร;ร;รยฟ{ร3ร3ยปรจรยรรรร3ยพ;รรชรร

ร;ยพ<ยปยรBรฟยป9ร;ยฝรรร;รJรฏรฃรงรฐร3ร;ร3รรร

รปOรยฟรTร3รรรชรรป%รFรร

รWร<ร3รLรปOรยฟรhยปรยณรรฟ;รร;ร3ยป9รรOร<รBร"ร;ร<ยป9ร;ยฝFรรรยพ;ยฟรยป9ร5รรร5รรยปBรรรรFรรยรรLรQร&รยฟรTรร<ร&ยฟ>รรWรรปOรยฟรรฏ"รฃรง

รรรHรFรWยพFรยรรฟHรรรรรFรร9ร<รรWรรร3รรข&รFรร3รรฟ

c

ร<รBรร;ร;รยฟ>รยจรรJยพ;รFรฟ<รยฟ>ร3ร1รรFรฟHร;รรป

ร;ยฟรร<ร9รร

รฟ<รรTร0ยป9รFรร hรรรรhยป9รรชร<ร9รรWรร<รรรฟhร3ร;ร3รรร

ร;ยฟรร<ร9รร

ร3รร9รรยฟ>รรGรผOร;รJรรรรพยปBรรฒรยยป9รร1ร9รGรรรพรFรรรร;รรรรฟ;รฟยป9รยยป9รรHรร

รร;รร

รยยพFรร1รรรฟรพยป9รHรFรยฟร3ยป&ร1ยพ<รBรยฟ

ยฎยญยฏยง
>ยฐ 

รป{รยร;รรFรยรรFรรLรฏรฃรงรรรรร+ร{ยพFรยรรฟhรร

ยร รรร1ร3ยป9รร

รฟ<รรJรยป9ร;รข'รฟ<รรFรรFรฟ<รร<รTรรรยรรข&ยฟรรยฟ3ยป9รรรร

รร9ยฝรยฟ3ยป9ร3ร;รTรOรรFรฟรร;รยพ;ยฟยยปBร3ร3ยปBรยจร1รร<รยฟรรร3รยฟ>รรรยฝยปรรร

4

ยฑ&ยฒgl.ยณxwยดยถยตpยกยทยถยธ[ยนrwยดAยธ
รJรFรรรTร;ยฟรรยรร5รรรฟรซรร;รaรฏ"รฃรง

รรยฝรยฟ3ยป9ร3ร;รGรตLรร\รร;ร;ยฟร<รรร

*

รรรรร3รรข&รFรร3รรฟรซร<ร&รร;ร<ยป9ร;ยฝGรFรร3รรฟรซรรรซรร;ร

ยปBรฟ<รรpรรFรรOรร;รรรฟ;ร"ร;ร>รร3ยป9รรTรรรร;ยฟรรยปรยพFรยร<รBรร;ร<ยป9ร;ยฝWรร<ยป&ร3รยฆรฟ<รร

/

รยยป9ร;ยฟ>ร"ยฟรSร<รBรรFร รยปBรยยฟรรรรจร9รTรpร;ยฟร;ร1รรรOรร

รร;ร;ยฟรร;ยฟยยปBรรร1ร9รpยฟรรยฟ>รร1ร3ยป9ร;ยฝร"รBรฟSร<รBรร;ร<ยปร;ยฝยณรฟ<รรยป&รยป9รรFรOรรFรฟJรรฟ;รฟยป9ร;ยฝpร;รรป
ยป9รรรยฟ>รฟ<รยฟQรรWรTรรรiรร;รยจรรจยป9ร;ยฟ>รยฟรรชร<รBรรGร3รร1รรรรรGร3รรรรiรร;รร;ยฟรร<รรร
รผOร;รSรร9ยฝรยฟ3ยปยรร;ร

ยบ

ร3ร;ร3รรรTรรยยปBรยป9ร'รร
รรFรฟ\รร;ร

ยปBรiรยยป9รWร<ร9รรตFร"รFรฟJรFรร%ร<ยป&ร1รhร&รยฟรTรรรร;ยฟรรFรยฟร3ยปรรร{รยรยพ;รFรฟ<ร;รรรรตZร1รรWร<ร9รรรร;รรรรตZรรFรฟ

'รรร&ร3รWรTรรรรiรร9รร"ยฟiรร;รWรฟยปBรยร3ยป9รFร1ร3ยป9รรGรFรร3รปOรรร

รฟ<รรTรยปร;รข'รฟ<รรFรรFรฟ<รร<ร

(

รฟ<รรTรยป9ร;รขยป9รFรฟ<รร+รรFรฟ<รร;รรร9ยฝรยฟ3ยป9ร3ร;รTร

ยปc

รร;ร<รยยป&รรร3ยป9รร\ร"รรฒรฟ<รรJรยป9ร;รข'รฟ<รรFรรFรฟ<รร<รGร1รร<รยฟรรยจร5ร;รรปOร9รรฟ<ยฝรร

รรร<ยปBรร9รpร&รยฟhร3รยพFรฟ<รยป9ร;ยฝรรร;รSร;ยฟรร<ร9รร



รยรรรFรรตรยยป9ร;ร;รยรรFรฟJร1รรFร3รยฟ>รยป9ร<ร>ร

ร"ร%รFรรFรฟZร

hรTรยยพFรรยทยป9รSยปBรWร"รรร1รฝFร1ร1รยร9รร<ร

ร"รรรรร3รรข&รFรร3รรฟรพร<รBรร;ร<ยป9ร;ยฝJยป9รHรร;รWรรFรยรยฟ>รร1รhรรFรฟรฑรBรยฟรรFรร9ร<ร1ยป9ร;ยฝ

ร3รยฟ>รรรยฝยปรรLร&รยฟ%ร<ร&รรรยฟรรFรยป9ยฟร

ยพ;ยฟhร1รฝ;รFรยฟ3ยปรWรร5ร1รรยยฟรร3ยพ<ร9ร1ร%รร3ร1รร<รยยปBร3ร;รรฟHรรร3ร;ร3รรรJรร3ยปBรรฒยฟร1รBรร3ยปรรFร3ร<ยป9รรพรFรร3รปOรรร]ร1รรWร;ยพ;ร>ร"ร3ยป9รรFรร

0

ร รรยฟรLยฟร ยพ<ยป9ยฟรรฟWร"รFรฟSรร;ร%ร1รฝ;รรร5รยรรhรป%ร<ยปBรรTรQรยยปร;ยฟ>รยฟรpร<รBรรWยฟรรยรรbร<รรรรรร;รOยป9ร;ร;ยพ;รยร;ยฟรร<ร9รรGรต<รรFรฟWรร&ร3ร
ร1รรWรFรยฟรรฟpรยพ;ยฟLร3ร;ร3รรรGรฉ(รรร+รยฟ3รBรยฟรTรรFร1รOรรhรรFร"รรรFรร;รรรยยป9รSยปยรBร"ยฟLรฃ;รฅFรฆยรงLรฅรรยรผรร;รรรยรยฆร3รรรGรฉ-รยผรFรยฟ3ร&รยฟรTร"รFร1ร

ยปBรยจรรFร1รยพ;ยฟ>รยฝ"ยป9ร;ยฝFรตZร;ยพ;รรปOรSร;รรรรฟ

รรFรรhรร;รSยฟรร3ยพ<รร>รhร3ร;รยพ<รBรฟ

รFรSยป9ร<รรยฟร;ยฟรรรรฟรพรปOยป9รร<ยป9รHรร;รWร1รร<รร1รฝ;รhรร

รร;รรยป9รWร<รรรรFรฟJยฟรยฝยพ<รBร"ยฟ%ร;ยฟรร<ร9รรJรLยปรรรป%ร<ยปBรรGรร;รรJรปOรยฟรร1รรFรฟ<ยพFร1รรรฟZร

ยผยฝยผOยพ.ยฟ7รbรยรรยกร:%X!ร$&รยถร"%ยจรยร ร%ยจรรรยรรร&รQ$&รรuรย%bรรbร"!รร
4
ร

g}

ร%ร3ร;รยพ<รBรฟpร1รรWรWรร<รLร;ยฟ3ยป9ร FรhรรSรร;รยยป9รรชร<รยยปBรรร3ยปยรรFร_รรรยพ;ยฟLรร9ยฝรยฟ3ยป9ร3ร;รGรฉ-รรBรยฟรTรร5ร;ยฟรรFรยฟร3ยปรรร

iรรยป9ร;ยฝ

ร;ยฟรรFรยฟร3ยป9รร_รยยป9รรยร1รรรชร<ร9รรรร;รรรLรรFรฟpร3ร;ร3รรรTรรยยปBรยป9ร'รhรฟ<ร<รรรร;รรร;รร1รรรรยฟ3ยปยรรiรTร"รร{รรSรร9ยฝรยฟ3ยป9รร;รoยฝร<รยฆรฟZรต
ร;รยฟ

รฟ<ร<รรยจรร;รSรรFร3รรFร1รWรรยรร;รร3รร;ยฟรรFรยฟร3ยปรร%ร;รร1รรรรยฟ3ยปยร9รรรJรรรbร"ร

รรร&ยฟ>รรรชรรป{รยฟรรพรBรยฟpร<รBรร;ร<ยป9ร;ยฝHรSยพFร3รSยพ<ร9ร3ยปรTรรร1ร9รHร+รรรWรรร3ยพ;ยฟรรฟ

iย

ร;ยฟรร<ร9รรTร รรรSร;ยฟรรยปBรฟ<รยจร1ร0รรยฟ>รยฝร%ร"ร
รยป9รรรตiร"รFรฟ

ร

ร3รรซรรรร

ยป9ร

ร0ร9ยฝรยฟ3ยปยรร;รรรFรรฟZรhรผOร;รSร0รร9ยพ;รpรร
ยป9ร>รSรร<ยปยรรจยป9ร'รHรรร3รรรรSยป9ร<รรยฟรร3รยยป9ร;ยฝ

รรรชยป9ร5รรยฟรร3ร3ยป9ร;ยฝSรฟ<รรTรยปรรรตรรWรรรรรiรรpร;ยฟรร<ร9รรJรยร"รยฟรรร3รรFรร<ร9ร

รรยพ;รFรฟ<ร;รรรรตhร1รรWร<ร9รรรร;รรรรตhรรFรฟoร3ร;ร3รรรTรรยยปBรยป9ร'รรร"ยฟรHร;ร1ยป9รร;รยฟรร;รร1รรรร"ยฟรรร;รยฟ



ร3ยพ aรยป9รร<ร%รรSร;ยพ<ยปยรBรฟรพรรรร รรร1ร3ยป9รรยจร<รBรร;ร;รยฟร

ยซยฌ

ร

MONQPR S TVU XW
ยฎ ยยฏ

}

ยธ

"4

ยน;ยฎ

ยฎ

ยฏ

pร<รBรร;ร<ยป9ร;ยฝJรร9ยฝรยฟยยป9รร;รTรรต<ร;รรปOรรรยฟรต<รป%ร<ยปBรร

oรFรรรยปBรยยป9รiร"รFรยพ;ร



M W	Y bYS P

ยธยฎ;ยฏ

ยยงuยฐ	รย	&ยฐ
รยยยยย

QรรปOรรรยฟรต<รร;รร;ยฟรรFรยฟรยยป9รร%รฟ<รJร;ร1ร9รGยพFรOรร

0

ยปBรOร ยพFรรยรรWยป9รรชรFรยฟร1รร5รร

4

&Y M [Z P]\ SY^ R`_[P\Ca

&ยฏ;ยนFยถ ยถยฏ;ยนFยถยฏ



:ร

WรรFรร%รTรรฟ<รยจยปร%ร รรร1ร3ยป9รรยจยป9รJยปร>ร%ร1ร<รรยปร;ยฝTรฟ<รรTรยปร

ร

oรFรรZยปBร_รร;รLรรร3รร<ร3ยปBรร;รฟยป รรยฟรรFร1รLรFรร3รปOรรรรร;รQรฃ;รฅFรฆยรงLรฅSรรFรฟยจรร;ร{รฏ"รฃรงSรBยฟ1รรWรรปOรยฟร;ร Wยบ;รยฟรJรร5รรFรร9ร;รยปBร
รร%รรรร9ยฝรยฟยยป9รร;ร



ยฃ4

รรรHร;ยฟรรยปBรฟ<รpยป9รFรยยป9ยฝร<ร%ยป9ร<รรรรป%รFร"รhรTรรรรรยป9รhร รรร1ร3ยป9รรร

dยยยยย

รWร3ร;รรปOรรฟรพรรFรร

;รฉ-ร

รยฟ>ร"รFรรBรยฟรTรร3ยปยรรGรยรยฟ>รรรยฝ"ยป9รร%ร1รรWรSรรยจรร;รbร1ร5รยรiรร{ร"รรยป9รFร1รรรชร<ร9รรรbร0ร9ยฝรยฟ3ยปรยร;รรพรต;ร;ยพ;รhยพ;รFรฟ<รยฟ>ร3ร>ร"รFรฟยป9ร;ยฝ
รป%รFรรpรFรยฟร>รยจร"รรรร;รTร3รรยฟ>รร]ร3รFรร1รWรร;รรร1รฝFรร9ยพFรฟ<รTรรรรซร;ร1ร9รยพFรรฒรFรรรรยฟbยพ;รFรฟ<รยฟ1ร3ร>รรFรฟHร;รรป

รร;รรรยฟร



ร รรร1ร3ยป9รรร
ยบ=รยฟรTร0ร

ร

ร;ยฟรรFรยฟร3ยปรรรรร]ร0รBร3รGรร1รSรรรร

รฟยป aร1ยพ<รรWรรรรรร9ยพFรรรร



ยบ=รรป

ร รรร1ร3ยป9รรรชรFรยฟ3ร&รยฟรTร"รFร1รร

1Vร (

ร]รร;รรชรรร;รยฟรFร"รFรฟZรตรยป9รยจยปBรรฒรร5รยป9รยพFรยร9รรยป9รรชรFรยฟร1รร5รhรรรพรTรรรJร3ยพ;ยฟรJรรFรร

รWร<ร&รร;รข'รรฟ;รร;ร>ร"ร3ยป9รรGรรยฝรยฟ3ยป9รร;ร
รร3ยปBรpรร9ยฝรยฟ3ยป9รร;ร

ยปBรฟ<รรรยยป9รร"ร3ยป9รรHรรiรGรฟ<รรยป9ยฟ1รร<ร9รWร;ยฟรรFรยฟร3รGรรFรรhยปBรยจรรชรยฟร

รปOรยพ<ร&รฟ\รยฟยฝยพ;รรตยร&รยฟWร1รฝFรรWร<ร9รรตยรรFรรรร3ร;ร3รรรTร"ร3ยปBรยปยร3รHยปBรรชร;รร1รรรรยฟรรซร&รยฟ

รฟ<ร5รรยจร;รร

ร1ร;รร9รรตรรรFรฟรพรปOรbรรรรรQร9รรรยร%ยฝยพFรยฟ>ร"ร5รรรpรรFรรhรTรยรยฆร3รรรWรข

1ร

รปOยปยรยรLร;รรร1ร;รร9รSรรรยฟhรFรยฟร3ยป&รรร<รBรรFรร

0
ยผยฝยรรร|:รbร%ยจรยร)ร:%[ร

รรGรยรยฆร3รรรTรร3ยปBรยปร'รรรSยป9ยฝร5รยจรFรWรร5รGร3รยฟรร;ยฝรร

ยฟร ยพ<ยป9ยฟรรWรร<รรรBรยฟยรรTรร9ยฝรยฟ3ยป9ร3ร;รGรตร;ยพ;รOรรLรร;รiรร"รWรOร3ยป9รWรยยป9รLร;ยฟรรยปBรฟ<รรLรรWรรFรฟยณรFรยป9ร<รLยปรยณรhรยรFรร1รยฟยพ;รรพร

(

ย0

ยพ;ยฟiรปOรยฟรWยฟ>รยปBรยรรยรJรร5ร

ร c

ยพ;รร3ร3ยป9รรFรรรรFรร%ร3ยพ;ยฝยฝรรยรiรรรร5ยพ;รร{รBรยฟยร&ยพ;รยพ;ยฟรhยฟรรยรรยฟ>รรรร

Qร9รร;รยพ;ยฝรTรร;รยฟรรยฟรยจรTรร<รWร;ร<รรยฆร{รBรยฟ%รฟ<รรJรยป9ร;รข'รฟ<รรFรรFรฟ<รร<รQยป9ร<รBรยฟรTรร3ยปยรรยณยป9รรรยพ;ยฟ%รรฟ;รร;ร>ร"ร3ยป9รร

n0

ร0ร9ยฝรยฟ3ยปยรร;รรพรต;รปOรhรFรรรร;รรhร3รยฟ3ยป9รยพFรร9รJร1รฝยฆร<ร9รยฟรรฟรรร;ร

}

Qรรป

ร1รร5รรร<ยปรร5รJยปBรรชยปรรรร

ยพFรรยยป9ร3รWร"รยรร;รbร3รร"ยฟ>รร;รข'ร1รร5รยฟรรรยป9ร<รรยฟ3ร'รร1รร

รยรFรรยปยร&รoร;รยพ;ยฟ3ยปBร3ร3ยป&รรJรร]ยฝยพ<ยปBรฟ<ร]รรฟ;รร;ร1รร3ยป9รร

:ร

ยป9ร

รรซรWรยฟรHยฟรร0รยยปBร3ร3ยป9ร

รฟ<รรTรยป9ร

ร (

*

ยพ;ยฟLรรFรรรยฆรยป&รZรรFรยฟ>ร"รFรรBรยฟรTรร3ยปร0รFร0ร5ร<รBรร;ร<ยป9ร;ยฝhร3ร;ร3รรรTร

รรSยป9รWร<ร9รรรชรร5รยจรร;รbร"ร;ร;ยฟร5รรร

รร ย :ร ยย *

รยฟ>รรFรร&รยฟรTร"ร'ยปรรFรรร<ร&รร;ร;รยฟ>รร
รผรร;ร%ยป9ร<รรยฟร<ร&รรSร+รรยรปOรรรรรฟ<รรยปBรยป9รรFรยรJรรฟ<ร

รฅ รฆ รฏ

ร (

ยร (

ร9ร1ร&ร%รรTร1รรFรยยปBรฟ<รยฟร

ยย

*

รรรรร;รรฏ รรฅFรฆ&รฃ5รฏSรร1รยยป9รรJยฟรร;ยฟรร3รร<ร>รร3ยปรรรร

ยป&รฟ<รรรรรรiรยจรWรยฟรLร1รฝ;ร;ยฟรรรยยป9รรยรBรร;ยฝยพFร"ยฝร

ยยย ย *iร

รรฟ;รฟยป9ร;ยฝรยฟรรยฟ>รร1ร3ยป9รรHรร

ร ย :ร ยย รข/

รฅ รฆ รฏ >ร

ร
Vรฃ &รค
E
Vรจ &รฉ

ยผรฃ

รยจร;รรรฟJรรWร1รร<รBยฟรร5รยรร;รQยปBรร3ยพ;รร

รฃ

รhรJรรJรFรSรร<ร9รhรรรชยฝรร;รยฟ>รรยยปรรhรยพ;ยฟ

rรง

X;>รฅ ยถJ
fJ
ยรฆ

ย ยช ยQยรจยย3ยฃZยยยคOยยยขยยยขยยยยฅ5ย3ยง1ยยขยยยยย3ย>ย

E

Lยยค

*

ยพ;ยฟยจรรFรร9ร;รยปBร

ร'รร1รรฟ



b/

ร3รร1รยยป9รรรร;รยร รยปBรOรรJยป9ร<รรยฟรรยร3ยป9ร;ยฝ

ยป9ร\รร<ยป&รSรFรรFรยฟยณยปBรpยป9ร>รSรฟ<รร+รรFรฟ;รรFร1ร

|ร ย *iร

'รยจรป{รยพ<รBรฟJรFรpรร3รFรรยปBร0รยร9รรชยป9ร5รรยฟรร3ร3ยป9ร;ยฝSรรSร1รฝ;รรรFรฟรรยพ;ยฟ
ร&รยฟร1รฝFรรWร<รรรต0ร3รรWรรร<ยป9ร;ยฝQรยยป9รรLรง

รก{O4

รร5ร+รยฟรร<ร

ร0ร9ยฝรยฟ3ยปยรร;ร

j	j /

ร1ร&รฟZรตยร

รผOร;รSร<รBร"ร;ร<ยป9ร;ยฝTร1รร3รJยปBรยจรร9ร5ร3ร1รรรยฟร1รBรรรรฟรฑรรJรรFรรยจรร{รฟ<รรยป9ยฝร

4

%รยฟรรร;รยฟ

4

ร;รรพรรiรร;รรพร;ยฟรร<ร9รรJรรปรยป9รร\รร;รGรร;ร;ยฟร5รรรรรรฟ<รร;รรรรรฟ

ร<ร

รยป9รWรรชรรFรรตOร

รป%รFรรOรTรรรรOรSยฝร5ร;รฟJร<รBรรรรรSยฟรรยฟ3ยป9รรรรต;รรFรฟJรป%รFร"รยยป&รOรร;รhรFรรยร

รป%รรWรรpรFร%รรFรรOร<รBรรJร&รยฟOรร;รhร<รBรรGรรฟ;ร"ร;รรยฟ
รยร>รยฟรรตร;ยพ;รiรSยพFรรJยปBร

;รฉ-ร%รยฟ>รรFรร&รยฟรข

'รhรปOรยพ<รBรฟHรร&ร3รTร+รSยป9ร<รรยฟรข

รฟ<ยพ;ยฟยยป9ร;ยฝSรร;รiร<ร&รร;รข&ยฟรรยฟ3ยปรร0รรZร;ยฟรยฆร1รรร%รรFรฟรชรร;รiร<ร&รร;รข

รรฟ;รร;ร>รร3ยปรรSร;ยฟรยฆร1รรรยรFรรรยจร;รรOรFรรรรรปOร1รยรZร1รฝยฆร<รรยฟรรฟZร
ร<รรรรยร_รรร3รรข&รFรร3รรฟรฑร<รBรร;ร;รยฟ>รร

รยยยยย
j	ย	ยย/

)

ร1รรWร;ยพ;ร>รร3ยปรรFรรร+รร;ร1รFรร

รรยร3ยป9ร;ยฝGรรGรFรยฟยรBรยฟรรฎรร;รรรรรWรJร3ร5ร+รTร"รQร"รFรร9ร;รยปBรhรร

4

=ยปBรรร;ยฟร1รยยป9รSยป9รFร"ยฟรร รยร;รรFร

รฟ<รรร1ยฟยยป9รFรรฟรพรร;รยฟรWรรFรฟรพรฟ<รรรยฟรSยปร;รbรปQร<ยปBรรรพรร

รJรร3ยป9รรFร0รยฟรรFรยป9ยฟ1ร%ร;ยฟรรยปBรฟ<รSรร;รSยฝยฟรรรรร3ร

ร

nย	/

รรร1ร3ยปรร

jXย	ย/

รรฟ<รFรยพ<ร9รรต5ร

*

รก >รต+รยฟhรร;รSรBร"ร;ยฝยพFรยฝรSยพFร3รรฟHร<ร

/

ร+รรรHรยฟรbรยร5ร<รร;รรยปBรยจรร1ร3ยปรยป9รยยป9รร >ร

รรJรรฟ;รฟ<ยฟรรรยจรรร3รรข&รFรรยรรฟGรฟ<รรยป9ยฝรรรรรยพ;รWรFรรฟ<รข

E

ยรฆ

ย0ยยคOย0ยยฃย9ย<ยยย1ย1ย>ยยยฃ&ย3ยยยยผยฃ&ยยย&ยกยฉ1ยฉ1ยยย&ยฃยผยฃ&ย0ยยฃรย ย&ยฃ&ย3ยครยยฃยยขยยยยยฅยขยฉ1ย1ยยยขยฃ&ยยคOยรยย&ยยยย3ยฃ&ยก

FE

ยยยฅ-ยฅ Oยฅยขย3ย&ยรย ย3ย3ยฃยยขยง1ยLย>ยยจย3ย>ยคOยคOย>ยhยย&ย ยฅยขย3ยคOย3ยช

[G

ย ยช ยผยย1ยกยฉ1ย%ย3ยง>ย3ย%ยรย ย&ยฃ&ย3ยครยยฃยยขยยผยยฅ@ยย ย&ย0ยยยยยผยยฅ@ยยยย3ยยยยOย&ยยยยยยฃ&ยยยยฅ ยยฉ>ย3ยย3ย'ยยฃ&ยยยฅ@ยยย+ยฃ&ย0ยยฃFยย&ยยยกย3ยยผยยขยย3ยยฃย:ยยยยยฅ Zย1ยยฅยขย
ย&ยฃ'ยยฃ&ย3ย3ยช

ยซยฌ

รช

ยญOยฎ;ยฏ;ยฐยฑยณยฒยตยดยทยถยธยน

รFรยฟ>รรWรรรยฟ

*4

รฟ<รรยปBร1รรรยพFรยป9ร;ยฝยจยป&รฟ<รรรZร&ยฟรรyรยรยฆร3รรรยตรฟ<ร<รFรรSยปBรร

j	j /
ร

j	j ยฌ	รซ

รยปรจรยรยยป9รรTรรตร

รณ

ย{ยป4

Qรรยปยรยร9ร

ร1รBรฟZรต

รก 1ร

รฌ ยต	zgยดAw]vยpยกoutAรญouรฎรฏouยดs	ยธ
ร

รผOร<ยป&รรยฟรร3รรยฟ>รรSรป%รรยผยป9รWร;ยฟรรรรฟpร<รWรฟยปBรร1ยพFรรยปรรFรรรปOยป9รรWรผยผรร<รbรOรยฟยฟรรรรต

( รยช

h

ย

ยฟรร

รร1ยป9รร<ยป3รตรรรFรฟ

%รร

ยฃรฐ
c

ร"รbร;รFร"รWรFรร3ยปยร

xรฐ
dรฐ

ยจรร<ยปBร3ร

ยจยฟ>ร"รFรยฟรร9รรร;รรฟ

ยย
ยรฑ

ยฟ3ยปBร"ร3ยป9ร;ยฝWยพFรรตยผร;ยพ;รhร;ยฟร;รฟ<ยพFรยป9ร;ยฝGรรรพยป9รWร;ยฟรรรรฟรพร3ร;ร3รรรGร

hรรยปBรฟ

รรFรรรยฆรยป&รรTรผOรFร"ร;รยฆรSรรBรยรรรรรรรรร

รรSยป9รรรรต

xย

รยป9รรปOรยฟ>รรตOรป%ร;ร5รยร

รฑยป9ร5รรรรรต QรยยปBร1รร

รยยพ;ยฝยฝรร3ร3ยปรรFรSยปรWร;ยฟรรรรฟ\รร;ร

nรซ

รปOรยฟรpรปรรรรร&ยพ;รFรฟ<รรฟSยป9รรชรFรยฟรยร<ร

iยรฒ ยกj Xย ย
j

ยยย ยกj

'รข รณรณ ร 0รณ;รต+รรFรฟ

ร;ยฟรร3รร5ร1รร3ยป9รร

(ร
Hรณ

รดรขรรข'รณรณรรณ;รต<รรFรฟรร<รรรSยฝยฟ>รร<รยรBยฟรร

|รซ

aร1รpรร

รร;ร

gย

ยจรร0รร

รรฐ

ยจยฟ>ร"รFรยฟรต

ยพ;ร]ร3รรWรSรรOรร;รTร1ร;รฟ<รรตZยป9ร<ร&ยพ;รข

Hรรฟยปยฝ5รรGร;ร1รรFรรฟHรปOยปรรGรร;รSรรWร<ยปยฟ3ยปBรรร

iร"ร3ยป9รรFรรZร5รยป9รรFร1รยบ=รยพ;รFรฟ;รร3ยป9รร

'รข รรณรรด5ร;รรต5ร<ร

รฐ

รยพ<รรยรรรWรรต ยจรร<ยปBร3ร

%ยปร;ยฝรยพ;รรรตยรรFรฟ

รร;รaร"ร;รร5ร<รWรยพFรรฒยฟรรข

รรรร<ยปBรรชรFรร+รยฟGร3ยพ;รFร3ร>รร<ร3ยปBร0รยร9รร

ยฉ

ยฉ

ยฟ>รร<ร>ร

Oรร3รรยฟ>รร

iยรฒ ยกย	j
Xiยย ยกย	j ย
Qj j
,รซ
'รข

ยฟ>รร<ร>ร

รณรกรณ;รรณ;รต

รข

รผรร<ยปBร

รฌ รรณรก;รต

รณ"รข'รรขร รณรดWรรFรฟ

iรณรณรณ;รรดรข

iรยฟรรฝGร1รยฟร+รยฟ>รรยยป9รรรร

moรดVo&yXo&ยดAยตouยธ
h
c
}
{
	c * ยช / * ยจj	j / รรตร	ยฐ	
รถbQ
.รทร	u&
	รถ &ย
ยย
uยAc
c
ย * j	ย	ยย/ c
<รธAรถ	u
>
รนรปรบยยฆ
uยฆf ยรผ[รฝ 	j ยจรพ
gc A{รฟ4 ยรฐ * jXj / ร
ยขยช
ร
H
 ยยฆ
	 f&ยจ>
รถ	ยจยฆf  * b/ รย รพ
c {ยซ4 &รฐ * j	j :/
 รรทรยจยฆ
	gรผ[รฝ	H		
ร  !"$#&%'#)('+*,ย+-!.รข+/#&0212
รธยถ	 	รX 		"c
* j	ย / nรฐ
ย
ย0
*
 xย
	ย ยช / รทรยฆfยฐ	
รถยฃ3 ร5 4A
>รย, fu&	
	u	6ร	ยฆ 
&87fXfu
>	รถ:9ย	&;+<
 >=
g
?=
ย
ย
	รฐ * j	ยยX/ ร

@
Hf
 รยฆยจ
>X" fu
FรถX&ยฆร A	รฝ * / [รพ ย	ย
ย
ยช {รฟย ยรฐ
รฐ * ยจj	ย / Bf&ยจยฐ	ยงยฆยจ
>XรC รปf
 รยฆยจ
>	D &
FรถXยฆ ยฎc
ร
4
.ย
&ย
&ย,c
ยฉ
bรฐ * j	ย / bc
	 nรทรยจยฆffยฐX
>รถb
E dF
 fu&	
	&X"รธยถ	 ยuยฆVdXรHรธ G	ยญV&ยจ
>ยฆ ร	uยฐ.รบยยฆยจ
>ยจI G ยชAยชยถยช
ยฉ
h Aยช
( ยถ{ 4 Aรฐ * j	j ]/ (
g รทย
ยฆ 	gรผbรฝ	  X 	AรธA	 	"	 	J	
h * j	ยXj/ nรธยถbยฏK <)LQbยfยฐ.รทย	&u
>รถ MCN"
+ Og
รถยขรทร	&u
>	รถeb B 6ร+ P)	QGSR&bQ ; <c
}
ร
h * j	j / Qยช
}
$H
 ยยฆ
	 fu
FรถX&ยฆร >TU
รขย ยจรพ bย
Qรยร9รรรรต5ร=รยรต QรรFรฟร9รยฟรต<ร;รยรต
ร5รร

HรรรรFรต

Qร9รรยฟรTรรรรต hร
รOรยฟยฟรรรรต

รผรรรรรต Sร

ยรฟ;รร >ร

ร

bรรจรต

ร1รBรฟZรต

Sร

ร

รด5ร ร

รOรยฟยฟรรรรต Sรยรต

ร1รBรฟZรต Sร

{รยฟร+รร;ร1รยรยรตรร;ร

ร

ร 1ร

ร

รร<รร;รยฟรต Sร

>ร

ร&ยฟรร

_ร

&รยรรจยป9ร;รยปBรOรร

QยฟรFรรFร"รข ยรFรรWรFรยปยฝรรร

ร

รต

รด >ร

ยรรWรFรร<รรต Oรรรฟยป9ร;ยฝFรต

รยรต

รร1ยป9รร<ยปยรต

รยรต

ร1ร&รฟZรต

iร"รWรWรรFรฟZรต

ร

รฟ<รรยณยปBร

iร"รWรWรรFรฟZรต

ร

Sร

ร

5ร

รฟ;รฟยปBรยรร;รข

Sร

รก ร รร3รยฟยพFร1รยพ;ยฟรยรTรร;ร<ยป9ร;ยฝiร"ร;ร;ยฟร5รรรรรยจรรFรร9รยฝร

รJรรFรยฝรรWรร<รยยป9รรร<รBรร;ร<ยปร;ยฝFร

ร >รตรรร ยฆร
ร

รรFรฟรฒรWรร>รร;ร;รยฟร ร
ร

รร&รฟ<รรรรต

ร

ยณรร

รBรร;ร<ยป9ร;ยฝQรBรยฟยร1รร ยยพ;รFร1รยยป9รรOยฝร5รร&รร

ยพ;ร<รยยปBรยร<ยป9ร;ยฝ

ร

ร0รรยพFรร3ยป9ร;ยฝHรFร<รรยป9ร<ร9รรร รรยป9รรFร1ร

"รดร >รLรผรร3ร5รข'รฟ<รร1รรWรFร5รยยป9ร3ยป9รรJรยปBรSร<รBรรรรFร"ยฟ>รยป9ร;ยฝFร

Hร ยจรยฟรWรรรรต Sร

รรร9รร

รตFร ร 5รด5รก;รร

ยรฟZร 1รต

Zร %ร<ยปรรยฟ>รยป9ร3รSรร
ร

รต

ยจรยฟ3ยป9ร0ร"ร3ยป9รรFรร{ร"รFรร9รยฝรรฑยป9รร;ยฟรร<ร9รรgร3ร"ร9รยปร;ยฝGรรFรฟร<ร;รรปOร9รรฟ<ยฝรWรร ยพ<ยปBร3รยยป9รรรร

รฑยปBรรFรรBร3รยปยรต hร

%รยรต

รยพ<รBรJรร;รรรต

ร >รต 5ร รรรก;ร

ยจร0รยปยร&รร<ร9รhรยปBรSยบLรผ

ยรFรยฟร<ยป&รรรต

ร รพรยฟยฝ<รร

รยฟร3ยป&รร%รยฟ1รฟ<รยฟWร<รBรร;ร<ยป9ร;ยฝ+ร

รต

ยรFรร;รTร"รรรต Sร

รณ >ร

>ร hรฟ;รร;ร3ยป9รรhร<รBรร;ร<ยป9ร;ยฝFร

ยฝ<รยป9รFรร

ร

ร

Sร

รด >ร

%ร

รWร<ยป9ร+รรรรFร1รรรปOยปรร;รยพ;รSรรWร<ยปBรรยป9รรFร1รรรรรรFร3รยฟ

'ร

>ร

0ร

hรรรข

ยฟรรรร

ร

ร 0ร ยฆรกรก ;ร

ร

รณ 1ร

รรฝ;ร<รBรยป9ร<ยป9ร;ยฝJรรFรฟรพยฟรรFรยป9ยฟ3ยปร;ยฝWร<รBรรFรQรรFรรQรร0ยปยรยร
ยซยฌ

V

รต

รต

MONQPR S TVU XW
ยฎ ยยฏ

ร

}

ยจรร;รยฆรรตFรFรรจรต

&Y M [Z P]\ SY^ R`_[P\Ca

&ยฏ;ยนFยถ ยถยฏ;ยนFยถยฏ

ย ยช {Oย

รรยรBรรรZรต รซร %รยรต

ยรร;รรรรต

}

ยป9รWรร<ร>รร3ยป9รรรรต;รรFรฟJรร;รรฟ<รรยยป9ยฝรGร"ร

&{ร4
Aย

ยจรร;รยฆรรตZรFรยรต

h

รWรร5รQรร

ร

ร &ย * j	j /
ร รฒร

ร

M W	Y bYS P

ยธยฎ;ยฏ

ยน;ยฎ

ยฎ

WX6รรถX
Yยฏ
&รตยรผZT

|c

รก >รยรผรร;รร3รยฆรยรรรTรร3ยป&รยจร<รBรรGรรฟ;รร;ร1รรรยฟร

)j 	j

ร<รBรร;ร<ยปร;ยฝFรGรผยผรรร;ร<ยปBรร0รรยฟรรFรยฟร

ยยช

ยรรรชร;ยพ;รรยฟhร5รยป9รรFร1รSรรFรฟ

ยฏ

ร รรยรรFรร;รJรยฟร;รรต;รรร3รรFรรฟ;รรตFร1รร5รยฟรรยรยรรฟWร1รฝ;รFรยฟรข

รยฝรร<รiรยฟ>รร<ยป9รรร1รยพ;ยฟรรร

* jXj /

:รฐ

ร1รBรฟZรต SรZรFร

รรร3รรข&รFรร3รรฟ

ยธ

รด >ร

ร&รยฟรTร0รรรBรยพ;รFรฟ;รร3ยป9รรJร&รยฟ

[=Qร<ยป9รรยฟ1รยป9ร3รHรรQ4]รร3ร<ยป9ร;ยฝรรรรรต"รฐยจรรFร"ยฟรรข

รกรข'รณ "รข'รณรดFรต

ร;ยฝยป9ร;รรยฟ3ยป9ร;ยฝ+ร

* /

* j	j ย/ (
\=
b
# รทยยฆfVยฐX
>รถbx3
^]ยรธDW3<^_
A
Xย ยจรพ 	ย
h
* j	j / c
{ }
`H
 ยยฆ
	fu
FรถXยฆ UU j [รพ 	ย
h
* ยจj	j / ยc
h * ยช / uรทรยฆ2	
 ย}
รฝ	&ยฐaf&	gรธA	K +	"	 	JK	ยรทยFX&&
รถรbรบ Gยจย+ Pยฃ
bรพ Xj c<cรc<
ร {รฐ
ร
c
gย * jXj /
รยช
n
d HรตQรท e9ย	&;ยป	f
 f&u	รนX
>รนยจg ยXย	ยฆ ยnยรทย	&u
>รถ
hAรบ:ยฆยยฐ	ยง
>รถ
hรX&ยฐรรธยถ	&	
h
	jยจรพ ย
ย c
รฐ { ย
รฐ * j	j b/
g รรทยiยจ
ยฆ 	_[ H	 	
รธยถ	 	รX 		
bรพ Xj
* j	ย	ย/ i
ย
 รทร2ยฆ 	/ย H	 	AรธA	K +	"	 	JK	
bรพ 	j
รซ
&รฐ &{ร4 &รฐ * j	j / g
ยถ j9ย	^;b
>kรถ H	ย
E dg
 8?5$4	&รบ G
PQbย
>ยง P 	: dHยย
FรถXg ย"P~รทยถ Gยจยฏ
ยฆfX:รทร
>uยฆ
ยย
ร
bยช * jXย	ย/

*
รธยถ" PยบยยงX
>	u	 fu
FรถX&ยฆร T / ยจรพ ย
ร
*
{ 4 รฐ * ยจj	j / uc
/ Q 7รทรยฆVยฐ	
รถbdE รg ?85)"รผ _"_lA,bรบ G
Pยบยรขยฏ
ยง Pรฟ	f
 4	ยงuยฐ		
	dE 5รปยง" P)	
ยฆ
รทย	&u
>	รถ MmRxรธยถFรขfย
>ยฆร	 ย	:iยจ	ยฆยย	ukยฐ Lรฒ Gb	&ยฐ
ยจรพ
ร
ย{~4 รฐ * j	j / W=Hย ร ( ร :c
cHWรฐ c ยฎ รทร2ยฆ 	nAXiยฐ:fu 	QรธA	 	ร	Cรทร
&ยฆ
ยยn3 aod&" Oยfยฐยจรถ	ยฎรตร:i ยฏยจ&	
	#	&ยฐ,รตยf
 <
ร pb !q#&%'r#(>'*s + -!.ยจq+ #&0"1

ย	u
>รถ
ยจรพ c
ร bย [{
	รฐ * j	j /
 eรทรยฆ2	
รผXรผ[  X 	AรธA	 		C 	J	 j [รพ j	j
ย * jXย / tduGbu" P)
>5ยฆ 6ร+ P)	QGMnvR	QG)3 รรตย+ P)
&ยฐ	
รถ)	uFยฐ 7fXfu
>รถd
รรธA PQ:ยงยจV
ร
X&ยฐยฎรทรย ยถย
u=
&ย &{ยซc
&ย * jย	ยX/ gรบ:ยฆ
 ยQ hgรทร	]w h xQX2Q hg	&z
ยฐ y &ยฐ	ยจVย	uยฐ	
>รถ ยช
ย * j	ย	ย/ Hc
H รทร2ยฆ 	X \ H	 	
รธยถ	 	รX 		 &j bรพยj	j
รรSร;รFรรWรFร"ร3ยปยรตยจรFร

ร

ร >ร

รoรร;รGยพ;ร3ยปรจรยยป9ร3ร]ร"รbร3รยฆรยรรรTรร3ยป&รยป9ร'รZร

QรFรฟ<รยฟ>ร3ร1รรFรฟยป9ร;ยฝ\รร;รรพรยฟ>รรฟ<รร ร

รFรร3รปOรรร\ยฟรรฟ<ยพ;รFรฟ;รรFร1รรรรFรฟ\ร1รรรชรSยป9รรWรร<รpยป9ร\รFรยฟรยยปBรร9รข&รยฟรฟ<รยฟSร<รBรร;ร<ยปร;ยฝFร

'ร

รต<ร;รรรรร รณ รร รฌ=ร

รรSร;รFรรWรFร"ร3ยปยรต+รFรยรต

QรรFรฟร9รยฟรตFร=ร

ร

รก >ร

ร0ร0รยยปBรฟ;รร3ยปรรaรยรยฟยพFร1รยพ;ยฟรhรFรร3รรฟJรร;รรยฟรJรรLร<รBรรรรWร;รฟ<รข

ยปยรรรร3ยป9รรaรรFรฟJยฟรยพFร3รร

ร<รร<ร9รยฟรตFร;ร

ร

รด >ร

รต

iรรยปBรฟยป9ร;ยฝpร<ยป9ร3ร'รรยรBร

0รตรร ร ยฆรกรฌ ;ร

ยป9รGรรร3รรข&รFรรยรรฟJร<รBรร;ร<ยป9ร;ยฝFร

ร

iรรรชรWรรFรฟZรต

รตร;รรรรรณรด รรณ 5ร

รรร;ยฝรรรรต

รยรต

ยจยฟยพ;รWรWรรFรฟZรต

รซร

ร

รณ >รWรผยผรรปรร"ยฟ>รฟHรร

ร

ยรฟZร >รต

>ร

รรฝ;รFรยฟยยป9รWรร<ร>รร%ร5รยป9รรFร1รJรร

รBรร;ร<ยป9ร;ยฝ+ร

'ร

:รต

ร;รรรยผรรณ

รรรดFร รพรยฟยฝ5รร

Hร %รยรรร3รรยฟรต Sรยรต

รยพ<รBรJรรรร

รร5ร3รร<ร<รยยป9รรรต Sร

ร

;ร รยรร;ร3รรรTรรยยปBรยจร;รร<รยยป9ร;รร"ยฟ{ร<ร&รร;ร<ยป9ร;ยฝFร

ร

@รต<ร;รรรFรรรด ยฆรร ;ร

รฑยป9ร5รรรรรตรFร

ร

1ร %ยพFรร<ร3ยป9ร>ร"ร3ยป9รรยยฟรรยยพ<ร9ร>รรร1รรFร1รยฟร<ยป9ร;ยฝpรร;รOยพ;ร3ยปยรยยป9ร3รยจรรร1รฝ;ร<รBรรFร"ร3ยป9รร;รข&รFรรยรรฟยจร9รรยฟร<ยป9ร;ยฝFร

ร

@รต<ร;รรรFรฌร"รด ยฆรฌร =ร

%รรยปยรยรรรต Sรยรต

ร1รBรฟZรต Sร

ร

รก 1ร

'ร;ร;รร0ร"ร3ยป9รรSรฟ<รรยป9ยฝรHรร%ร3ร;ร3รรรTร"ร3ยปBรhร3รรยฟ1รรรร

'ร

ร

รรฟ<รFรยพ<ร9รรต %ร

ร

>ร<รร<ร5รร;รรยป9ร1ยป9ร;ยฝยจร<รBรรFรรรรFร"รร1รร<ร>รยป9รSรร1ร3ยป9รรFร_รปOยป9รรSร1รร5รร1รฝยฆรรข'รฟ<รร+รรFรฟ<รร5รยร รรร1ร>รร
รต

รร5ร+รยฟรร<รรต;ร;ร+รFรยรต
ยฟรรFรยฟร >ร

ร1รBรฟZรต Sร;รFร

รด >รต;รรฌร ;ร 0รก;ร

ร

ร ร

ร;รรป

รร;ร;ยฟร5รรรJรรรรรรชรFรยฟ>ร0รร<รBรร;ร<ยปร;ยฝ

ร;ยฟร1รรจยป9รSยป9รFรยฟร

ร

รต<ร;รรรรรรรก Zรรร<ร

รร5ร+รยฟรร<รรตร;รรจรต
Lร

ร1รBรฟZรต Sร

ร

รก >ร

ร

ร3รยพ;รFรฟZรตรร1รรWร<ร9รรรรตยผรFรยฟร3ยป&รรรยฟ>รฟ<รยฟhร<ร&รร;ร;รยฟhร&รยฟ

'ร

รตร;รรรยผรรณร รรรดFร ยจร0ร0ยปยรBรร<ร9รhรยปBรSยบLรผ

รรรรต รซรยรต

รรSยปรรรรต Sร

ร

ร

Wรร

รร

ร ร5รผOร;ยฟรร"รรข&ยฟรรWรร0ร0ร=ร3รยฟ>รรรยฝยปรร+รBรยฟLรFรยฟร3ยปBร0ร9รข&รยฟ>รฟ<รยฟร<ร&รร;ร<ยป9ร;ยฝFร 'ร
ยรตร;รรร+รด รก 5รด

ร5รรFร"ร;รรต hร

รBยฟรร

<ร

รก >ร
ร

{รรSร;ยฟ3ยป&รฟ<ยฝร

ร5รรFร"ร;รรต รฒรยรต

ยจรFร1ร&ร3รรรรต hร

รยป9รWรWรรFรรต hร

ร

ร

Qร<ยป9รรยฟ1รยป9ร3ร

ร

ยฟรรรร

>ร

ร

รร;รรยฟรGรร%รฟ<รร;ยพ;ยฝยฝยป9ร;ยฝรพร<รBรรFรhรรFรฟรฑยป9ร<รรยฟร;ยฟรร>รร3ยป9รรFรร

@รต<ร;รรร "รด

;ร

ยซยฌ

{

'ร

Lยฟ3ร9รFร"ยพ;รGร

ยญOยฎ;ยฏ;ยฐยฑยณยฒยตยดยทยถยธยน

:c * jย	ยX/ ยฉ
@
H 7รทรยจยฆ
	 U[|f&	D]ยX
>uHรธA	K+	A	|F		
ย	ย	ยยจรพยยXj
* j	j / รฐ
ย ย{ ย
gc
ยย
ย ร รt} Hc
c ย0
:=
6รXยฆย
&~7fXfu
>รถ :รผ
 j[รพ ย[ย
"ย * jXj / k7f	&
	รถ,ยญGยHu	fรถ	
ยฆ	รตยfรขยฏ	u
>รถย
ยxQยจ&	 รทยยญยฏ+Pรฟรบ:	รน
รถ ร 2รฐ
uย
dย =
c
รปยรย|= ย ยกj bย
[ย * j	j /
:c
 รทรยจ
ยฆ 	gรผ[รฝ	 H	 	ยรธA	 		 F		 	j ยจรพ
* j	j / :
รฐ
 Cรทร2ยฆ 	ย[  X 	AรธA	 	"	 	J	
j[รพ
ยฃ} <{
Hรฐ * ยจj	j /
ย
ย ยฃย ยกj รฐ
รย

รผรรรรรต Sร

ร

>ร

รร;รยฟ1รร3ยป9ร;ยฝรร;ยฟร ยรร1รhร;รร3รปOรยฟร;รร

ร

ยรตZร;รรร

ย

0ร=ร

ร1ร9ร5รยรFรต รซรยรต

{รยฟร+รร;ร1รยรยรตรร;ร

ร

ย

ร ยพ<ยปBรยป9รยยป9รรรรต;รรรยฟ>รยฝรรต;รรFรฟ

ร1ร9ร5รยรFรต

รซร

ร

ร1ร9ร5รยรFรต รซร

ร

ร<ร9รรร

ยรร;ยฝFรต

รต

รฆ

รตFรกรด

ยร

ยฆรก

ยจยพ;รรรTรรยยป9ร;ยฝ

Qร<ยป9รรยฟ>รยยป9ร3รร ยจร0รยปยร&รร<ร9รรรOรรรร;ร<ยปBรร0ร_ยฟรรFรยฟร

Oรร3ร

;ร

รรร Sร

%รข %รรข รกรขร รดFร

"รด >รยบยผร9ร1รฝ<ยป9ร<ร9รOร3รยฟ>รรรยฝร%รรรยฟร<ยป9ร;ยฝFร ยจรFรร9รยฝยปBรรร5ยฟรร<รBรรiรรFร;ยฟรร<ร9รรyร3รรรยป9ร;ยฝQรร<ยปBร3ร;รฟ<รรร
ยรต5ร;รรร+รฌ รฌ ยฆรรณรณ;ร

ยปยรยรยยป&รรJรรต5รยจร

รฑ

iรFร0ร9รยฝรJยป9รรซรฃ;รฅ

ร

รพร1รยร9รร

'ร

4

ยจรยฟยยป9ร0รรยยป9รรFรร

รก ร

รร;รรยปBรรต {ร"ยฟร;รยฝยป9ร

ย

ร 1ร

%รยยปยรยยป9รรรยป9รรรร

รยรต

ร

รณ >ร 'ร<รรยฟ>รร1ร3ยป9รร;รข&รFรรยรรฟSยป9ร<รรร<ร3ยป9รรรร

'ร

ยรต<ร;รรรFรรด

ยบยผยป&ร3ร;รยฟรต

Sร

รรFรยฟ>ร"รรยฟ>รรยรผรรรร;ร<ยปBรรร

=%ร<ยป9รรยฟ>รยป9ร3รZร

ร

รก >ร

รรรFรยฟร

ยจรรยป9ยฝร<ยปร;ยฝร;รรร1รรฟ<รรยป&ร1รรยรBยฟรร

รยปรSยปยรBรยฟยยป9ร'ร<รข&รFรร3รรฟoยฟรรยฟ3ยป9รรรรbรรFรฟ

%รรข รก"รขรร;รต ยจรรFรยฟรรรชรร5ร%ร"ร

ยซ

ร+ย

รFยฟ>รยร{ร;ยฟยยป9รFรยป9รข

ยฆรรฌร<ร

รFรยฟร3ยปBร0รยฟรยพFร3รรรWรTรร1ยฟรรข

ยรรWร;ยพ;รรยฟhร5รยป9รรFร1รรต

รรFรฟ<รยฟร<ยปรจร9ร

Journal of Articial Intelligence Research 2 (1994) 227-262

Submitted 10/94; published 12/94

Total-Order and Partial-Order Planning:
A Comparative Analysis
Steven Minton
John Bresina
Mark Drummond

Recom Technologies
NASA Ames Research Center, Mail Stop: 269-2
Moett Field, CA 94035 USA

minton@ptolemy.arc.nasa.gov
bresina@ptolemy.arc.nasa.gov
med@ptolemy.arc.nasa.gov

Abstract

For many years, the intuitions underlying partial-order planning were largely taken for
granted. Only in the past few years has there been renewed interest in the fundamental
principles underlying this paradigm. In this paper, we present a rigorous comparative
analysis of partial-order and total-order planning by focusing on two specic planners that
can be directly compared. We show that there are some subtle assumptions that underly
the wide-spread intuitions regarding the supposed eciency of partial-order planning. For
instance, the superiority of partial-order planning can depend critically upon the search
strategy and the structure of the search space. Understanding the underlying assumptions
is crucial for constructing ecient planners.

1. Introduction

For many years, the superiority of partial-order planners over total-order planners has been
tacitly assumed by the planning community. Originally, partial-order planning was introduced by Sacerdoti (1975) as a way to improve planning eciency by avoiding \premature
commitments to a particular order for achieving subgoals". The utility of partial-order
planning was demonstrated anecdotally by showing how such a planner could eciently
solve blocksworld examples, such as the well-known \Sussman anomaly".
Since partial-order planning intuitively seems like a good idea, little attention has been
devoted to analyzing its utility, at least until recently (Minton, Bresina, & Drummond,
1991a; Barrett & Weld, 1994; Kambhampati, 1994c). However, if one looks closely at
the issues involved, a number of questions arise. For example, do the advantages of partialorder planning hold regardless of the search strategy used? Do the advantages hold when the
planning language is so expressive that reasoning about partially ordered plans is intractable
(e.g., if the language allows conditional eects)?
Our work (Minton et al., 1991a, 1992) has shown that the situation is much more interesting than might be expected. We have found that there are some \unstated assumptions"
underlying the supposed eciency of partial-order planning. For instance, the superiority of
partial-order planning can depend critically upon the search strategy and search heuristics
employed.
This paper summarizes our observations regarding partial-order and total-order planning. We begin by considering a simple total-order planner and a closely related partialorder planner and establishing a mapping between their search spaces. We then examine
c 1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Minton, Bresina, & Drummond

the relative sizes of their search spaces, demonstrating that the partial-order planner has
a fundamental advantage because the size of its search space is always less than or equal
to that of the total-order planner. However, this advantage does not necessarily translate
into an eciency gain; this depends on the type of search strategy used. For example, we
describe a domain where our partial order planner is more ecient than our total order
planner when depth-rst search is used, but the eciency gain is lost when an iterative
sampling strategy is used.
We also show that partial-order planners can have a second, independent advantage
when certain types of operator ordering heuristics are employed. This \heuristic advantage"
underlies Sacerdoti's anecdotal examples explaining why least-commitment works. However,
in our blocksworld experiments, this second advantage is relatively unimportant compared
to the advantage derived from the reduction in search space size.
Finally, we look at how our results extend to partial-order planners in general. We
describe how the advantages of partial-order planning can be preserved even if highly expressive languages are used. We also show that the advantages do not necessarily hold for
all partial-order planners, but depend critically on the construction of the planning space.

2. Background
Planning can be characterized as search through a space of possible plans. A total-order
planner searches through a space of totally ordered plans; a partial-order planner is dened
analogously. We use these terms, rather than the terms \linear" and \nonlinear", because
the latter are overloaded. For example, some authors have used the term \nonlinear"
when focusing on the issue of goal ordering. That is, some \linear" planners, when solving a
conjunctive goal, require that all subgoals of one conjunct be achieved before subgoals of the
others; hence, planners that can arbitrarily interleave subgoals are often called \nonlinear".
This version of the linear/nonlinear distinction is dierent than the partial-order/totalorder distinction investigated here. The former distinction impacts planner completeness,
whereas the total-order/partial-order distinction is orthogonal to this issue (Drummond &
Currie, 1989; Minton et al., 1991a).
The total-order/partial-order distinction should also be kept separate from the distinction between \world-based planners" and \plan-based planners". The distinction is one
of modeling: in a world-based planner, each search state corresponds to a state of the
world and in a plan-based planner, each search state corresponds to a plan. While totalorder planners are commonly associated with world-based planners, such as Strips, several
well-known total-order planners have been plan-based, such as Waldinger's regression planner (Waldinger, 1975), Interplan (Tate, 1974) and Warplan (Warren, 1974). Similarly,
partial-order planners are commonly plan-based, but it is possible to have a world-based
partial-order planner (Godefroid & Kabanza, 1991). In this paper, we focus solely on the
total-order/partial-order distinction in order to avoid complicating the analysis.
We claim that the only signicant dierence between partial-order and total-order planners is planning eciency. It might be argued that partial-order planning is preferable
because a partially ordered plan can be more exibly executed. However, execution exibility can also be achieved with a total-order planner and a post-processing step that removes
unnecessary orderings from the totally ordered solution plan to yield a partial order (Back228

Total-Order and Partial-Order Planning

strom, 1993; Veloso, Perez, & Carbonell, 1990; Regnier & Fade, 1991). The polynomial
time complexity of this post-processing is negligible compared to the search time for plan
generation.1 Hence, we believe that execution exibility is, at best, a weak justication for
the supposed superiority of partial-order planning.
In the following sections, we analyze the relative eciency of partial-order and totalorder planning by considering a total-order planner and a partial-order planner that can
be directly compared. Elucidating the key dierences between these planning algorithms
reveals some important principles that are of general relevance.

3. Terminology

A plan consists of an ordered set of steps, where each step is a unique operator instance.
Plans can be totally ordered, in which case every step is ordered with respect to every other
step, or partially ordered, in which case steps can be unordered with respect to each other.
We assume that a library of operators is available, where each operator has preconditions,
deleted conditions, and added conditions. All of these conditions must be nonnegated propositions, and we adopt the common convention that each deleted condition is a precondition.
Later in this paper we show how our results can be extended to more expressive languages,
but this simple language is sucient to establish the essence of our argument.
A linearization of a partially ordered plan is a total order over the plan's steps that is
consistent with the existing partial order. In a totally ordered plan, a precondition of a plan
step is true if it is added by an earlier step and not deleted by an intervening step. In a
partially ordered plan, a step's precondition is possibly true if there exists a linearization in
which it is true, and a step's precondition is necessarily true if it is true in all linearizations.
A step's precondition is necessarily false if it is not possibly true.
A state consists of a set of propositions. A planning problem is dened by an initial
state and a set of goals, where each goal is a proposition. For convenience, we represent a
problem as a two-step initial plan, where the propositions that are true in the initial state
are added by the rst step, and the goal propositions are the preconditions of the nal
step. The planning process starts with this initial plan and searches through a space of
possible plans. A successful search terminates with a solution plan, i.e., a plan in which all
steps' preconditions are necessarily true. The search space can be characterized as a tree,
where each node corresponds to a plan and each arc corresponds to a plan transformation.
Each transformation incrementally extends (i.e., renes) a plan by adding additional steps
or orderings. Thus, each leaf in the search tree corresponds either to a solution plan or
a dead-end, and each intermediate node corresponds to an unnished plan which can be
further extended.

1. Backstrom (1993) formalizes the problem of removing unnecessary orderings in order to produce a \leastconstrained" plan. He shows that the problem is polynomial if one denes a least-constrained plan as a
plan in which no orderings can be removed without impacting the correctness of the plan. Backstrom
also shows that the problem of nding a plan with the fewest orderings over a given operator set is a
much harder problem; it is NP-hard.

229

Minton, Bresina, & Drummond

TO(P; G)
1. Termination check: If G is empty, report success and return solution plan P.
2. Goal selection: Let c = select-goal(G), and let Oneed be the plan step for which c is a precondition.
3. Operator selection: Let Oadd be an operator in the library that adds c. If there is no such Oadd , then
terminate and report failure. Choice point: all such operators must be considered for completeness.
4. Ordering selection: Let Odel be the last deleter of c. Insert Oadd somewhere between Odel and
Oneed , call the resulting plan P . Choice point: all such positions must be considered for completeness.
5. Goal updating: Let G be the set of preconditions in P that are not true.
6. Recursive invocation: TO(P ; G ).
0

0

0

0

0

Figure 1: The to planning algorithm
Plan P
O

S

del

A

B

Oneed

F

+ O

add

S

O

del

Oadd

S

Oneed F

B

A

O

del

A

Oadd

B

S

O

del

Oneed F

A

B

Oadd

Oneed F

Figure 2: How to extends a plan: Adding Oadd to plan P generates three alternatives.

4. A Tale of Two Planners
In this section we dene two simple planning algorithms. The rst algorithm, shown
in Figure 1, is to, a total-order planner motivated by Waldinger's regression planner
(Waldinger, 1975), Interplan (Tate, 1974), and Warplan (Waldinger, 1975). Our purpose
here is to characterize the search space of the to planning algorithm, and the pseudo-code
in Figure 1 accomplishes this by dening a nondeterministic procedure that enumerates
possible plans. (If the plans are enumerated by a breadth-rst search, then the algorithms
presented in this section are provably complete, as shown in Appendix A.)
230

Total-Order and Partial-Order Planning

to accepts an unnished plan, P , and a goal set, G, containing preconditions which are

currently not true. If the algorithm terminates successfully then it returns a totally ordered
solution plan. Note that there are two choice points in this procedure: operator selection
and ordering selection. The procedure does not need to consider alternative goal choices.
For our purposes, the function select-goal can be any deterministic function that selects
a member of G.
As used in Step 4, the last deleter of a precondition c for a step Oneed is dened as
follows. Step Odel is the last deleter of c if Odel deletes c, Odel is before Oneed , and there is
no other deleter of c between Odel and Oneed . In the case that no step before Oneed deletes c,
the rst step is considered to be the last deleter.
Figure 2 illustrates to's plan extension process. This example assumes that steps A
and B do not add or delete c. There are three possible insertion points for Oadd in plan P ,
each yielding an alternative extension.
The second planner is ua, a partial-order planner, shown in Figure 3. ua is similar to
to in that it uses the same procedures for goal selection and operator selection; however,
the procedure for ordering selection is dierent. Step 4 of ua inserts orderings, but only
\interacting" steps are ordered. Specically, we say that two steps interact if they are
unordered with respect to each other and either:
 one step has a precondition that is added or deleted by the other step, or
 one step adds a condition that is deleted by the other step.
The only signicant dierence between ua and to lies in Step 4: to orders the new step
with respect to all others, whereas ua adds orderings only to eliminate interactions. It is
in this sense that ua is less committed than to.
Figure 4 illustrates ua's plan extension process. As in Figure 2, we assume that steps
A and B do not add or delete c; however, step A and Oadd interact with respect to some
other condition. This interaction yields two alternative plan extensions: one in which Oadd
is ordered before A and one in which Oadd is ordered after A.
Since ua orders all steps which interact, the plans that are generated have a special
property: each precondition in a plan is either necessarily true or necessarily false. We
call such plans unambiguous. This property yields a tight correspondence between the two
planners' search spaces. Suppose ua is given the unambiguous plan U and to is given
the plan T , where T is a linearization of U . Let us consider the relationship between
the way that ua extends U and to extends T . Note that the two planners will have the
same set of goals since, by denition, each goal in U is a precondition that is necessarily
false, and a precondition is necessarily false if and only if it is false in every linearization.
Since the two plans have the same set of goals and since both planners use the same goal
selection method, both algorithms pick the same goal; therefore, Oneed is the same for both.
Similarly, both algorithms consider the same library operators to achieve this goal. Since T
is a linearization of U , and Oneed is the same in both plans, both algorithms nd the same
last deleter as well.2 When to adds a step to a plan, it orders the new step with respect to
2. There is a unique last deleter in U . This follows from our requirement that for any operator in our
language, the deleted conditions must be a subset of the preconditions. If two unordered steps delete
the same condition, then that condition must also be a precondition of both operators. Hence, the two
steps interact and will be ordered by ua.

231

Minton, Bresina, & Drummond

UA(P; G)
1. Termination check: If G is empty, report success and return solution plan P.
2. Goal selection: Let c = select-goal(G), and let Oneed be the plan step for which c is a precondition.
3. Operator selection: Let Oadd be an operator in the library that adds c. If there is no such Oadd , then
terminate and report failure. Choice point: all such operators must be considered for completeness.
4. Ordering selection: Let Odel be the last deleter of c. Order Oadd after Odel and before Oneed.
Repeat until there are no interactions:
 Select a step Oint that interacts with Oadd .
 Order Oint either before or after Oadd .
Choice point: both orderings must be considered for completeness.
Let P be the resulting plan.
5. Goal updating: Let G be the set of preconditions in P that are necessarily false.
6. Recursive invocation: UA(P ; G ).
0

0

0

0

0

Figure 3: The ua planning algorithm

Plan P
A
S

O

O

F

need

del

B
+ O

add

S

O

O add

A

add

O

O del

need

S

F

A
O

O del

need

F

B

B

Figure 4: How ua extends a plan: Adding Oadd to plan P generates two alternatives. The
example assumes that Oadd interacts with step A.

232

Total-Order and Partial-Order Planning

all existing steps. When ua adds a step to a plan, it orders the new step only with respect
to interacting steps. ua considers all possible combinations of orderings which eliminate
interactions; hence, for any plan produced by to, ua produces a corresponding plan that
is less-ordered or equivalent.
The following sections exploit this tight correspondence between the search spaces of
ua and to. In the next section we analyze the relative sizes of the two planners' search
spaces, and later we compare the number of plans actually generated under dierent search
strategies.

5. Search Space Comparison

The search space for both to and ua can be characterized as a tree of plans. The root
node in the tree corresponds to the top-level invocation of the algorithm, and the remaining
nodes each correspond to a recursive invocation of the algorithm. Note that in generating
a plan, the algorithms make both operator and ordering choices, and each dierent set of
choices corresponds to a single branch in the search tree.
We denote the search tree for to by treeTO and, similarly, the search tree for ua by
treeUA . The number of plans in a search tree is equal to the number of times the planning
procedure (ua or to) would be invoked in an exhaustive exploration of the search space.
Note that every plan in treeUA and treeTO is unique, since each step in a plan is given
a unique label. Thus, although two plans in the same tree might both be instances of a
particular operator sequence, such as O1  O2  O3, the plans are distinct because their
steps have dierent labels. (We have dened our plans this way to make our proofs more
concise.)
We can show that for any given problem, treeTO is at least as large as treeUA , that is,
the number of plans in treeTO is greater than or equal to the number of plans in treeUA .
This is done by proving the existence of a function L which maps plans in treeUA into sets
of plans in treeTO that satises the following two conditions.
1. Totality Property: For every plan U in treeUA , there exists a non-empty set
fT1; : : :; Tmg of plans in treeTO such that L(U ) = fT1; : : :; Tmg.
2. Disjointness Property: L maps distinct plans in treeUA to disjoint sets of plans in
treeTO ; that is, if U1; U2 2 treeUA and U1 6= U2, then L(U1) \ L(U2) = fg.
Let us examine why the existence of an L with these two properties is sucient to prove
that the size of ua's search tree is no greater than that of to. Figure 5 provides a guide for
the following discussion. Intuitively, we can use L to count plans in the two search trees.
For each plan counted in treeUA , we use L to count a non-empty set of plans in treeTO .
The totality property means that every time we count
P a plan in treeUA , we count at least
one plan in treeTO ; this implies that j treeUA j  U 2treeUA j L(U ) j. Of course, we must
further show that each plan counted in treeTOPis counted only once; this is guaranteed by
the disjointness property, which implies that U 2treeUA j L(U ) j  j treeTO j. Thus, the
conjunction of the two properties implies that j treeUA j  j treeTO j.
We can dene a function L that has these two properties as follows. Let U be a plan
in treeUA , let T be a plan in treeTO , and let parent be a function from a plan to its parent
233

Minton, Bresina, & Drummond

ua search tree

h

L

h

?
,@
,
	
@
@
R
@

h

to search tree

L

h

h

L

- f
- f

L

- f
g

H
HH



j
H

h

hg

,@
, @
	
R

h

hg

- f

A





h

A
AA
U

hg

Figure 5: How L maps from treeUA to treeTO
plan in the tree. Then T 2 L(U ) if and only if (i) T is a linearization of U and (ii) either
U and T are both root nodes of their respective search trees or parent(T ) 2 L(parent(U )).
Intuitively, L maps a plan U in treeUA to all linearizations which share common derivation
ancestry.3 This is illustrated in Figure 5, where for each plan in treeUA a dashed line is
drawn to the corresponding set of plans in treeTO .
We can show that L satises the totality and disjointness properties by induction on the
depth of the search trees. Detailed proofs are in the appendix. To prove the rst property,
we show that for every plan contained in treeUA , all linearizations of that plan are contained
in treeTO . To prove the second property, we note that any two plans at dierent depths in
treeUA have disjoint sets of linearizations, and then show by induction that any two plans
at the same depth in treeUA also have this property.
How much smaller is treeUA than treeTO ? The mapping described above provides an
answer. For each plan U in treeUA there are j L(U ) j distinct plans in to, where j L(U ) j is
the number of linearizations of U . The exact number depends on how unordered U is. A
totally unordered plan has a factorial number of linearizations and a totally ordered plan
has only a single linearization. Thus, the only time that the size of treeUA equals the size of
treeTO is when every plan in treeUA is totally ordered; otherwise, treeUA is strictly smaller
than treeTO and possibly exponentially smaller.

6. Time Cost Per Plan
While the size of ua's search tree is possibly exponentially smaller than that of to, it does
not follow that ua is necessarily more ecient. Eciency is determined by two factors: the
3. The reader may question why L maps U to all its linearizations in treeTO that share common derivation ancestry, as opposed to simply mapping U to all its linearizations in treeTO . The reason is that
our planners are not systematic, in the sense that they may generate two or more plans with the same
operator sequence. We can distinguish such plans by their derivational history. For example, suppose
two instantiations of the same operator sequence O1  O2  O3 exist within a treeTO but they correspond to dierent plans in treeUA . L relies on their dierent derivations to determine the appropriate
correspondence.

234

Total-Order and Partial-Order Planning

Step Executions Per Plan TO Cost UA Cost
1
1
O(1)
O(1)
2
1
O(1)
O(1)
3
<1
O(1)
O(1)
4
1
O(1)
O(e)
5
1
O(n)
O(e)
Table 1: Cost per plan comparisons
time cost per plan in the search tree (discussed in this section) and the size of the subtree
explored during the search process (discussed in the next section).
In this section we show that while ua can indeed take more time per plan, the extra
time is relatively small and grows only polynomially with the number of steps in the plan,4
which we denote by n. In comparing the relative eciency of ua and to, we rst consider
the number of times that each algorithm step is executed per plan in the search tree and
we then consider the time complexity of each step.
As noted in the preceding sections, each node in the search tree corresponds to a plan,
and each invocation of the planning procedure for both ua and to corresponds to an attempt
to extend that plan. Thus, for both ua and to, it is clear that the termination check and
goal selection (Steps 1 and 2) are each executed once per plan. Analyzing the number of
times that the remaining steps are executed might seem more complicated, since each of
these steps is executed many times at an internal node and not at all at a leaf. However,
the analysis is actually quite simple since we can amortize the number of executions of each
step over the number of plans produced. Notice that Step 6 is executed once for each plan
that is generated (i.e., once for each node other than the root node). This gives us a bound
on the number of times that Steps 3, 4, and 5 are executed.5 More specically, for both
algorithms, Step 3 is executed fewer times than Step 6, and Steps 4 and 5 are executed
exactly the same number of times that Step 6 is executed, that is, once for each plan that
is generated. Consequently, for both algorithms, no step is executed more than once per
plan, as summarized in Table 1. In other words, the number of times each step is executed
during the planning process is bounded by the size of the search tree.
In examining the costs for each step, we rst note that for both algorithms, Step 1,
the termination check, can be accomplished in O(1) time. Step 2, goal selection, can also
be accomplished in O(1) time; for example, assuming the goals are stored in a list, the
select-goal function can simply return the rst member of the list. Each execution of
Step 3, operator selection, also only requires O(1) time; if we assume the operators are
indexed by their eects, all that is required is to \pop" the list of relevant operators on each
execution.
4. We assume that the size of the operators (the number of preconditions and eects) is bounded by a
constant for a given domain.
5. Since Steps 3 and 4 are nondeterministic, we need to be clear about our terminology. We say that Step
3 is executed once each time a dierent operator is chosen, and Step 4 is executed once for each dierent
combination of orderings that is selected.

235

Minton, Bresina, & Drummond

Steps 4 and 5 are less expensive for to than for ua. Step 4 of to is accomplished
by inserting the new operator, Oadd , somewhere between Odel and Oneed . If the possible
insertion points are considered starting at Oneed and working towards Odel , then each execution of Step 4 can be accomplished in constant time, since each insertion constitutes one
execution of the step. In contrast, Step 4 in ua involves carrying out interaction detection
and elimination in order to produce a new plan P 0 . This step can be accomplished in O(e)
time, where e is the number of edges in the graph required to represent the partially ordered
plan. (In the worst case, there may be O(n2 ) edges in the plan, and in the best case, O(n)
edges.) The following is the description of ua's ordering step, from Figure 3, with some
additional implementation details:
4. Ordering selection: Order add after del and before need . Label all steps preceding add and
O

O

O

O

all steps following Oadd . Let stepsint be the unlabeled steps that interact with Oadd . Let Odel be the
last deleter of c. Repeat until stepsint is empty:



Let Oint = Pop(stepsint )
if Oint is still unlabeled then either:
{ order Oint before Oadd , and label Oint and the unlabeled steps before Oint ; or
{ order Oint after Oadd , and label Oint and the unlabeled steps after Oint .
Choice point: both orderings must be considered for completeness.

Let P be the resulting plan.
0

The ordering process begins with a preprocessing stage. First, all steps preceding or following Oadd are labeled as such. The labeling process is implemented by a depth-rst traversal
of the plan graph, starting with Oadd as the root, which rst follows the edges in one direction and then follows edges in the other direction. This requires at most O(e) time. After
the labeling process is complete, only steps that are unordered with respect to Oadd are
unlabeled, and thus the interacting steps (which must be unordered with respect to Oadd)
are identiable in O(n) time. The last deleter is identiable in O(e) time.
After the preprocessing stage, the procedure orders each interacting step with respect to
Oadd, updating the labels after each iteration. Since each edge in the graph need be traversed
no more than once, the entire ordering process takes at most O(e) time (as described in
Minton et al., 1991b). To see this, note that the process of labeling the steps before (or
after) Oint can stop as soon as a labeled step is encountered.
Having shown that Step 4 of to has O(1) complexity and Step 4 of ua has O(e) complexity, we now consider Step 5 of both algorithms, updating the goal set. to accomplishes this
by iterating through the steps in the plan, from the head to the tail, which requires O(n)
time. ua accomplishes this in a similar manner, but it requires O(e) time to traverse the
graph. (Alternatively, ua can use the same procedure as to, provided an O(e) topological
sort is rst done to linearize the plan.)
To summarize our complexity analysis, the use of a partial order means that ua incurs
greater cost for operator ordering (Step 4) and for updating the goal set (Step 5). Overall,
ua requires O(e) time per plan, while to only requires O(n) time per plan. Since a totally
ordered plan requires a representation of size O(n), and a partially ordered graph requires
a representation of size O(e), designing procedures with lower costs would be possible only
if the entire plan graph did not need to be examined in the worst case.
236

Total-Order and Partial-Order Planning

7. The Role of Search Strategies

The previous sections have compared to and ua in terms of relative search space size
and relative time cost per node. The extra processing time required by ua for each node
would appear to be justied since its search space may contain exponentially fewer nodes.
However, to complete our analysis, we must consider the number of nodes actually visited
by each algorithm under a given search strategy.
For breadth-rst search, the analysis is straightforward. After completing the search to
a particular depth, both planners will have explored their entire trees up to that depth.6
Both ua and to nd a solution at the same depth due to the correspondence between their
search trees. Thus, the degree to which ua will outperform to, under breadth-rst, depends
solely on the \expansion factor" under L, i.e., on the number of linearizations of ua's plans.
We can formalize this analysis as follows. For a node U in treeUA , we denote the number
of steps in the plan at U by nu , and the number of edges in U by eu . Then for each node U
that ua generates, ua incurs time cost O(eu ); whereas, to incurs time cost O(nu )  j L(U ) j,
where j L(U ) j is the number of linearizations of the plan at node U . Therefore, the ratio
of the total time costs of to and ua is as follows, where bf (treeUA ) denotes the subtree
considered by ua under breadth-rst search.
P
cost(tobf ) = u2bfP(treeUA ) O(nu )  j L(U ) j
cost(uabf )
u2bf (treeUA ) O(eu )

The analysis of breadth-rst search is so simple because this search strategy preserves
the correspondence between the two planners' search spaces. In breadth-rst search, the two
planners are synchronized after exhaustively exploring each level, so that to has explored
(exactly) the linearizations of the plans explored by ua. For any other search strategy which
similarly preserves the correspondence, such as iterative deepening, a similar analysis can
be carried out.
The cost comparison is not so clear-cut for depth-rst search, since the correspondence is
not guaranteed to be preserved. It is easy to see that, under depth-rst search, to does not
necessarily explore all linearizations of the plans explored by ua. This is not simply because
the planners nondeterministically choose which child to expand. There is a deeper reason:
the correspondence L does not preserve the subtree structure of the search space. For a plan
U in treeUA , the corresponding linearizations in L(U ) may be spread throughout treeTO .
Therefore, it is unlikely that corresponding plans will be considered in the same order by
depth-rst search. Nevertheless, even though the two planners are not synchronized, we
might expect that, on average, ua will explore fewer nodes because the size of treeUA is less
than or equal to the size of treeTO .
Empirically, we have observed that ua does tend to outperform to under depth-rst
search, as illustrated by the experimental results in Figure 6. The rst graph compares
the mean number of nodes explored by ua and to on 44 randomly generated blocksworld
problems; the second graph compares the mean planning time for ua and to on the same
problems and demonstrates that the extra time cost per node for ua is relatively insignicant. The problems are partitioned into 4 sets of 11 problems each, according to minimal
6. For perspicuity, we ignore the fact that the number of nodes explored by the two planners on the last
level may dier if the planners stop when they reach the rst solution.

237

Minton, Bresina, & Drummond

10000

7500

Time to Solution

Nodes Explored

50

TO
5000

UA
2500

40
TO
30
UA
20

10

0

0
3

4

5

6

3

Depth of Problem

4

5

6

Depth of Problem

Figure 6: ua and to Performance Comparison under Depth-First Search
solution \length" (i.e., the number of steps in the plan). For each problem, both planners
were given a depth-limit equal to the length of the shortest solution.7 Since the planners
make nondeterministic choices, 25 trials were conducted for each problem. The source code
and data required to reproduce these experiments can be found in Online Appendix 1.
As we pointed out, one plausible explanation for the observed dominance of ua is that
to's search tree is at least as large as ua's search tree. In fact, in the above experiments
we often observed that to's search tree was typically much larger. However, the full story
is more interesting. Search tree size alone is not sucient to explain ua's dominance; in
particular, the density and distribution of solutions play an important role.
The solution density of a search tree is the proportion of nodes that are solutions.8 If the
solution density for to's search tree is greater than that for ua's search tree, then to might
outperform ua under depth-rst search even though to's search tree is actually larger. For
example, it might be the case that all ua solution plans are completely unordered and that
the plans at the remaining leaves of treeUA { the failed plans { are totally ordered. In this
case, each ua solution plan corresponds to an exponential number of to solution plans, and
each ua failed plan corresponds to a single to failed plan. The converse is also possible:
the solution density of ua's search tree might be greater than that of to's search tree, thus
favoring ua over to under depth-rst search. For example, there might be a single totally
ordered solution plan in ua's search tree and a large number of highly unordered failed
7. Since the depth-limit is equal to the length of the shortest solution, an iterative deepening (Korf, 1985)
approach would yield similar results. Additionally, we note that increasing the depth-limit past the
depth of the shortest solution does not signicantly change the outcome of these experiments.
8. This denition of solution density is ill-dened for innite trees, but we assume that a depth-bound is
always provided, so only a nite subtree is explicitly enumerated.

238

Total-Order and Partial-Order Planning

UA Search Tree

*

*

TO Search Tree

*

*

*

*

* = Solution plan

Figure 7: Uniform solution distribution, with solution density 0.25
plans. Since each of these failed ua plans would correspond to a large number of to failed
plans, the solution density for to would be considerably lower.
For our blocksworld problems, we found that the solution densities of the two planners'
trees does not dier greatly, at least not in such a way that would explain our performance
results. We saw no tendency for treeUA to have a higher solution density than treeTO . For
example, for the 11 problems with solutions at depth six, the average solution density9 for
to exceeded that of ua on 7 out of the 12 problems. This is not particularly surprising
since we see no a priori reason to suppose that the solution densities of the two planners
should dier greatly.
Since solution density is insucient to explain ua's dominance on our blocksworld experiments when using depth-rst search, we need to look elsewhere for an explanation.
We hypothesize that the distribution of solutions provides an explanation. We note that
if the solution plans are distributed perfectly uniformly (i.e., at even intervals) among the
leaves of the search tree, and if the solution densities are similar, then both planners can
be expected to search a similar number of leaves, as illustrated by the schematic search
tree in Figure 7. Consequently, we can explain the observed dominance of ua over to by
hypothesizing that solutions are not uniformly distributed; that is, solutions tend to cluster.
To see this, suppose that treeUA is smaller than treeTO but the two trees have the same
solution density. If the solutions are clustered, as in Figure 8, then depth-rst search can be
expected to produce solutions more quickly for treeUA than for treeTO .10 The hypothesis
9. In our experiments, a nondeterministic goal selection procedure was used with our planners, which meant
that the solution density could vary from run to run. We compared the average solution density over 25
trials for each problem to obtain our results.
10. Even if the solutions are distributed randomly amongst the leaves of the trees with uniform probability
(as opposed to being distributed \perfectly uniformly"), there will be some clusters of nodes. Therefore,
to will have a small disadvantage. To see this, let us suppose that each leaf of both treeUA and treeTO
is a solution with equal probability p. That is, if treeUA has NUA leaves, of which kUA are solutions,

239

Minton, Bresina, & Drummond

UA Search Tree

*

TO Search Tree

*

*

**

*

* = Solution plan

Figure 8: Non-uniform solution distribution, with solution density 0.25
that solutions tend to be clustered seems reasonable since it is easy to construct problems
where a \wrong decision" near the top of the search tree can lead to an entire subtree that
is devoid of solutions.
One way to test our hypothesis is to compare ua and to using a randomized search
strategy, a type of Monte Carlo algorithm, that we refer to as \iterative sampling" (cf.
Minton et al., 1992; Langley, 1992; Chen, 1989; Crawford & Baker, 1994). The iterative
sampling strategy explores randomly chosen paths in the search tree until a solution is
found. A path is selected by traversing the tree from the root to a leaf, choosing randomly
at each branch point. If the leaf is a solution then search terminates; if not, the search
process returns to the root and selects another path. The same path may be examined
more than once since no memory is maintained between iterations.
In contrast to depth-rst search, iterative sampling is relatively insensitive to the distribution of solutions. Therefore, the advantage of ua over to should disappear if our hypothesis is correct. In our experiments, we did nd that when ua and to both use iterative
sampling, they expand approximately the same number of nodes on our set of blocksworld
problems.11 (For both planners, performance with iterative sampling was worse than with
depth-rst search.) The fact that there is no dierence between ua and to under iterative
sampling, but that there is a dierence under depth-rst search, suggests that solutions are
and treeTO has NTO leaves, of which kTO are solutions, then p = kUA =NUA = kTO =NTO . In general,
if k out of N nodes are solutions, the expected number of nodes that must be tested to nd a solution
is :5N=k when k = 1 and approaches N=k as k (and N ) approaches 1. (This is simply the expected
number of samples for a binomial distribution.) Therefore, since kTO  kUA , the expected number of
leaves explored by to is greater than or equal to the expected number of leaves explored by ua, by at
most a factor of 2.
11. The iterative sampling strategy was depth-limited in exactly the same way that our depth-rst strategy
was. We note, however, that the performance of iterative sampling is relatively insensitive to the actual
depth-limit used.

240

Total-Order and Partial-Order Planning

indeed non-uniformly distributed. Furthermore, this result shows that ua is not necessarily
superior to to; the search strategy that is employed makes a dramatic dierence.
Although our blocksworld domain may be atypical, we conjecture that our results are
of general relevance. Specically, for distribution-sensitive search strategies like depth-rst
search, one can expect that ua will tend to outperform to. For distribution-insensitive
strategies, such as iterative sampling, non-uniform distributions will have no eect. We note
that while iterative sampling is a rather simplistic strategy, there are more sophisticated
search strategies, such as iterative broadening (Ginsberg & Harvey, 1992), that are also
relatively distribution insensitive. We further explore such strategies in Section 8.2.

8. The Role of Heuristics

In the preceding sections, we have shown that a partial-order planner can be more ecient
simply because its search tree is smaller. With some search strategies, such as breadthrst search, this size dierential obviously translates into an eciency gain. With other
strategies, such as depth-rst search, the size dierential translates into an eciency gain,
provided we make additional assumptions about the solution density and distribution.
However, it is often claimed that partial-order planners are more ecient due to their
ability to make more informed ordering decisions, a rather dierent argument. For instance,
Sacerdoti (1975) argues that this is the reason that noah performs well on problems such
as the blocksworld's \Sussman anomaly". By delaying the decision of whether to stack A
on B before or after stacking B on C, noah can eventually detect that a conict will occur
if it stacks A on B rst, and a critic called \resolve-conflicts" can then order the steps
intelligently.
In this section, we show that this argument can be formally described in terms of our
two planners. We demonstrate that ua does in fact have a potential advantage over to
in that it can exploit certain types of heuristics more readily than to. This advantage is
independent of the fact that ua has a smaller search space. Whether or not this advantage
is signicant in practice is another question, of course. We also describe some experiments
where we evaluate the eect of a commonly-used heuristic on our blocksworld problems.

8.1 Making More Informed Decisions

First, let us identify how it is that ua can make better use of certain heuristics than to.
In the ua planning algorithm, step 4 arbitrarily orders interacting plan steps. Similarly,
Step 4 of to arbitrarily chooses an insertion point for the new step. It is easy to see,
however, that some orderings should be tried before others in a heuristic search. This is
illustrated by Figure 9, which compares ua and to on a particular problem. The key in
the gure describes the relevant conditions of the library operators, where preconditions are
indicated to the left of an operator and added conditions are indicated to the right (there
are no deletes in this example). For brevity, the initial step and nal step of the plans
are not shown. Consider the plan in treeUA with unordered steps O1 and O2 . When ua
introduces O3 to achieve precondition p of O1 , Step 4 of ua will order O3 with respect to
O2, since these steps interact. However, it makes more sense to order O2 before O3, since O2
achieves precondition q of O3. This illustrates a simple planning heuristic that we refer to
as the min-goals heuristic: \prefer the orderings that yield the fewest false preconditions".
241

Minton, Bresina, & Drummond

UA

TO

O1

O1

O1
O1

O2

O1

O2

O2
O1
O2

O3

O1

O1

O2

q O3

p

O3

O3

O2

O3

O1

O3

O2

O1

O2

KEY
p

O1

r

O2 q

Figure 9: Comparison of ua and to on an example.
This heuristic is not guaranteed to produce the optimal search or the optimal plan, but it
is commonly used. It is the basis of the \resolve conicts" critic that Sacerdoti employed
in his blocksworld examples.
Notice, however, that to cannot exploit this heuristic as eectively as ua because it
prematurely orders O1 with respect to O2 . Due to this inability to postpone an ordering
decision, to must choose arbitrarily between the plans O1  O2 and O2  O1, before the
impact of this decision can be evaluated.
In the general case, suppose h is a heuristic that can be applied to both partially ordered
plans and totally ordered plans. Furthermore, assume h is a \useful" heuristic; i.e., if h
rates one plan more highly than another, a planner that explores the more highly rated
plan rst will perform better on average. Then, ua will have a potential advantage over to
provided that h satises the following property: for any ua plan U and corresponding to
plan T , h(U )  h(T ); that is, a partially ordered plan must be rated at least as high as any
of its linearizations. (Note that for unambiguous plans, the min-goals heuristic satises this
property since it gives identical ratings to a partially ordered plan and its linearizations.)
ua has an advantage over to because if ua is expanding plan U and to is expanding a
corresponding plan T , then h will rate some child of U at least as high as the most highly
rated child of T . This is true since every child of T is a linearization of some child of U ,
and therefore no child of T can be rated higher than a child of U . Furthermore, there may
be a child of U such that none of its linearizations is a child of T , and therefore this child of
U can be rated higher than every child of T . Since we assumed that h is a useful heuristic,
this means that ua is likely to make a better choice than to.
242

Total-Order and Partial-Order Planning

5000
TO
UA
TO-MG
UA-MG

Nodes Explored

4000

TO without MinGoals
UA without MinGoals
TO with MinGoals
UA with MinGoals

TO

3000
UA
2000
TO-MG
1000
UA-MG
0
3

4

5

6

Depth of Problem

Figure 10: Depth rst search with and without min-goals

8.2 Illustrative Experimental Results
The previous section showed that ua has a potential advantage over to because it can better
exploit certain ordering heuristics. We now examine the practical eects of incorporating
one such heuristic into ua and to.
First, we note that ordering heuristics only make sense for some search strategies. In
particular, for breadth-rst search, heuristics do not improve the eciency of the search in a
meaningful way (except possibly at the last level). Indeed, we need not consider any search
strategy in which to and ua are \synchronized", as dened earlier, since ordering heuristics
do not signicantly aect the relative performance of ua and to under such strategies. Thus,
we begin by considering a standard search strategy that is not synchronized: depth-rst
search.
We use the min-goals heuristic as the basis for our experimental investigation, since it is
commonly employed, but presumably we could choose any heuristic that meets the criterion
set forth in the previous section. Figure 10 shows the impact of min-goals on the behavior
of ua and to under depth-rst search. Although the heuristic biases the order in which the
two planners' search spaces are explored (cf. Rosenbloom, Lee, & Unruh, 1993), it appears
that its eect is largely independent of the partial-order/total-order distinction, since both
planners are improved by a similar percentage. For example, under depth-rst search on
the problems with solutions at depth six, ua improved 88% and to improved 87%. Thus,
there is no obvious evidence for any extra advantage for ua, as one might have expected
from our analysis in the previous section. On the other hand, this does not contradict our
theory, it simply means that the potential heuristic advantage was not signicant enough
to show up. In other domains, the advantage might manifest itself more signicantly. After
all, it is certainly possible to design problems in which the advantage is signicant, as
243

Minton, Bresina, & Drummond

Nodes Explored

100
TO-IB
UA-IB
TO-IS
UA-IS

75

TO Iterative Broadening
UA Iterative Broadening
TO Iterative Sampling
UA Iterative Sampling

TO-IB

UA-IB
50

TO-IS

25

UA-IS

3

4

5

6

Depth of Problem

Figure 11: Iterative sampling & iterative broadening, both with min-goals
our example in Figure 9 illustrates. Our results simply illustrate that in our blocksworld
domain, making intelligent ordering decisions produces a negligible advantage for ua, in
contrast to the signicant eect due to search space compression (discussed previously).12
While the min-goals heuristic did not seem to help ua more than to, the results are
nevertheless interesting, since the heuristic had a very signicant eect on the performance
of both planners, so much so that to with min-goals outperforms ua without min-goals.
While the eectiveness of min-goals is domain dependent, we nd it interesting that in these
experiments, the use of min-goals makes more dierence than the use of partial orders. After
all, the blocksworld originally helped motivate the development of partial-order planning
and most subsequent planning systems have employed partial orders. While not deeply
surprising, this result does help reinforce what we already know: more attention should be
paid to specic planning heuristics such as min-goals.
In our analysis of search space compression in Section 7, we described a \distribution
insensitive" search strategy called iterative sampling and showed that under iterative sampling ua and to perform similarly, although their performance is worse than it is under
depth-rst search. If we combine min-goals with iterative sampling, we nd that this produces a much more powerful strategy, but one in which to and ua still perform about
equally. For simplicity, our implementation of iterative sampling uses min-goals as a pruning heuristic; at each choice point, it explores only those plan extensions with the fewest
goals. This strategy is powerful, although incomplete.13 Because of this incompleteness, we
note there was one problem we removed from our sample set because iterative sampling with
12. In Section 9.2, we discuss planners that are \less-committed" than ua. For such planners, the advantage
due to heuristics might be more pronounced since they \delay" their decisions even longer than ua.
13. Instead of exploring only those plan extensions with the fewest goals at each choice point, an alternative
strategy is to assign each extension a probability that is inversely correlated with the number of goals,

244

Total-Order and Partial-Order Planning

min-goals would never terminate on this problem. With this caveat in mind, we turn to the
results in Figure 11, which when compared against Figure 10, show that the performance
of both ua and to with iterative sampling was, in general, signicantly better than their
performance under depth-rst search. (Note that the graphs in Figures 10 and 11 have very
dierent scales.) Our results clearly illustrate the utility of the planning bias introduced by
min-goals in our blocksworld domain, since on 43 of our 44 problems, a solution exists in
the very small subspace preferred by min-goals.
These experiments do not show any advantage for ua as compared with to under the
heuristic, which is consistent with our conclusions above. However, this could equally well
be because min-goals was so powerful, leading to solutions so quickly, that smaller inuences
were obscured.
The dramatic success of combining min-goals with iterative sampling led us to consider
another search strategy, iterative broadening, which combines the best aspects of depthrst search and iterative sampling. This more sophisticated search strategy initially behaves
like iterative sampling, but evolves into depth-rst search as the breadth-cuto increases
(Langley, 1992). Assuming that the solution is within the specied depth bound, iterative
broadening is complete. In its early stages iterative broadening is distribution-insensitive;
in its later stages it behaves like depth-rst search and, thus, becomes increasingly sensitive
to solution distribution. As one would expect from our iterative sampling experiments, with
iterative broadening, solutions were found very early on, as shown in Figure 11. Thus, it is
not surprising that ua and to performed similarly under iterative broadening.
We should point out that the results presented in this subsection are only illustrative,
since they deal with only a single domain and with a single heuristic. Nevertheless, our
experiments do illustrate how the various properties we have identied in this paper can
interact.

9. Extending our Results
Having established our basic results concerning the eciency of ua and to under various
circumstances, we now consider how these results extend to other types of planners.

9.1 More Expressive Languages
In the preceding sections, we showed that the primary advantage that ua has over to is that
ua's search tree may be exponentially smaller than to's search tree, and we also showed
that ua only pays a small (polynomial) extra cost per node for this advantage. Thus far we
have assumed a very restricted planning language in which the operators are propositional;
however, most practical problems demand operators with variables, conditional eects, or
conditional preconditions. With a more expressive planning language, will the time cost
per node be signicantly greater for ua than for to? One might think so, since the work
required to identify interacting steps can increase with the expressiveness of the operator
language used (Dean & Boddy, 1988; Hertzberg & Horz, 1989). If the cost of detecting step
and pick accordingly. Given a depth bound, this strategy has the advantage of being asymptotically
complete. We used the simpler strategy here for pedagogical reasons.

245

Minton, Bresina, & Drummond

interaction is high enough, the savings that ua enjoys due to its reduced search space will
be outweighed by the additional expense incurred at each node.
Consider the case for simple breadth-rst search. Earlier we showed that the ratio of
the total time costs of to and ua is as follows, where the subtree considered by ua under
breadth-rst search is denoted by bf (treeUA ), the number of steps in plan a U is denoted
by nu , and the number of edges in U is denoted by eu :

P

cost(TObf ) = U 2bfP(treeUA ) O(nu )  j L(U ) j
cost(UAbf )
U 2bf (treeUA ) O(eu )
This cost comparison is specic to the simple propositional operator language used so
far, but the basic idea is more general. ua will generally outperform to whenever its cost
per node is less than the product of the cost per node for to and the number of to nodes
that correspond under L. Thus, ua could incur an exponential cost per node and still
outperform to in some cases. This can happen, for example, if the exponential number of
linearizations of a ua partial order is greater than the exponential cost per node for ua. In
general, however, we would like to avoid the case where ua pays an exponential cost per
node and, instead, consider an approach that can guarantee that the cost per node for ua
remains polynomial (as long as the cost per node for to also remains polynomial).
The cost per node for ua is dominated by the cost of updating the goal set (Step 5) and
the cost of selecting the orderings (Step 4). Updating the goal set remains polynomial as
long as a plan is unambiguous. Since each precondition in an unambiguous plan is either
necessarily true or necessarily false, we can determine the truth value of a given precondition
by examining its truth value in an arbitrary linearization of the plan. Thus, we can simply
linearize the plan and then use the same procedure to uses for calculating the goal set.
As a result, it is only the cost of maintaining the unambiguous property (i.e., Step 4) that
is impacted by more expressive languages. One approach for eciently maintaining this
property relies on a \conservative" ordering strategy in which operators are ordered if they
even possibly interact.
As an illustration of this approach, consider a simple propositional language with conditional eects, such as \if p and q, then add r". Hence, an operator can add (or delete)
propositions depending on the state in which it is applied. We refer to conditions such as
\p" in our example as dependency conditions. (Note that, like preconditions, dependency
conditions are simple propositions.) Chapman (1987) showed that with this type of language it is NP-hard to decide whether a precondition is true in a partially ordered plan.
However, as we pointed out above, for the special case of unambiguous plans, this decision
can be accomplished in polynomial time.
Formally, the language is specied as follows. An operator O, as before, has a list of preconditions, pre(O), a list of (unconditional) adds, adds(O), a list of (unconditional) deletes,
dels(O). In addition, it has a list of conditional adds, cadds(O), and a list of conditional
deletes, cdels(O); both containing pairs hDe ; ei, where De is a conjunctive set of dependency conditions and e is the conditional eect (either an added or a deleted condition).
Analogous with the constraint that every delete must be a precondition, every conditional
delete must be a member of its dependency conditions; that is, for every hDe ; ei 2 cdels(O),
e 2 De.
246

Total-Order and Partial-Order Planning

Figure 12 shows a version of the ua algorithm, called ua-c, which is appropriate for this
language. The primary dierence between the ua and ua-c algorithms is that in both Steps
3 and 4b an operator may be specialized with respect to a set of dependency conditions.
The function specialize(O, D ) accepts a plan step, O, and a set of dependency conditions,
D; it returns a new step O0 that is just like O, but with certain conditional eects made
unconditional. The eects that are selected for this transformation are exactly those whose
dependency conditions are a subset of D. Thus, the act of specializing a plan step is the
act of committing to expanding its causal role in a plan.14 Once a step is specialized, ua-c
has made a commitment to use it for a given set of eects. Of course, a step can be further
specialized in a later search node, but specializations are never retracted.
More precisely, the denition of O0 = specialize(O; D), where O is a step, D is a conjunctive set of dependency conditions in O, and n is the set dierence operator, is as follows.







pre(O0) = pre(O) [ D.
adds(O0) = adds(O) [ fe j hDe; ei 2 cadds(O) ^ De  Dg.
dels(O0) = dels(O) [ fe j hDe ; ei 2 cdels(O) ^ De  Dg.
cadds(O0) = fhDe ; ei j hDe ; ei 2 cadds(O) ^ De 6 D ^ De = De nDg.
cdels(O0) = fhDe ; ei j hDe ; ei 2 cdels(O) ^ De 6 D ^ De = De nDg.
0

0

0

0

The denition of step interaction is generalized for ua-c as follows. We say that two
steps in a plan interact if they are unordered with respect to each other and the following
disjunction holds:
 one step has a precondition or dependency condition that is added or deleted by the
other step, or
 one step adds a condition that is deleted by the other step.
The dierence between this denition of step interaction and the one given earlier is indicated by an italic font. This modied denition allows us to detect interacting operators
with a simple inexpensive test, as did our original denition. For example, two steps that
are unordered interact if one step conditionally adds r and the other has precondition r.
Note that the rst step need not actually add r in the plan, so ordering the two operators
might be unnecessary. In general, our denition of interaction is a sucient criterion for
guaranteeing that the resulting plans are unambiguous, but it is not a necessary criterion.
Figure 13 shows a schematic example illustrating how ua-c extends a plan. The preconditions of each operator are shown on the left of each operator, and the unconditional
adds on the right. (We only show the preconditions and eects necessary to illustrate the
specialization process; no deletes are used in the example.) Conditional adds are shown
14. For simplicity, the modications used to create ua-c are not very sophisticated. As a result, ua-c's space
may be larger than it needs to be in some circumstances, since it aggressively commits to specializations.
A more sophisticated set of modications is possible; however, the subtlies involved in eciently planning
with dependency conditions (Pednault, 1988; Collins & Pryor, 1992; Penberthy & Weld, 1992) are largely
irrelevant to our discussion.

247

Minton, Bresina, & Drummond

UA-C(P; G)
1 Termination check: If G is empty, report success and return solution plan P.
2 Goal selection: Let c = select-goal(G), and let Oneed be the plan step for which c is a precondition.
3 Operator selection: Let Oadd be an operator schema in the library that possibly adds c; that is,
either c 2 adds(O), or there exists an hDc ; ci 2 cadds(O). In the former case, insert step Oadd and in
the latter case, insert step specialize(Oadd ; Dc). If there is no such Oadd , then terminate and report
failure. Choice point: all ways in which c can be added must be considered for completeness.
4a Ordering selection: Let Odel be the (unconditional) last deleter of c. Order Oadd after Odel and
before Oneed .
Repeat until there are no interactions:
 Select a step Oint that interacts with Oadd .
 Order Oint either before or after Oadd .
Choice point: both orderings must be considered for completeness.
Let P be the resulting plan.
4b Operator role selection: While there exists a step Ocadd with unmarked conditional add hDc ; ci
and a step Ouse with precondition c, such that Ouse is after Ocadd and there is no (unconditional)
deleter of c in between Ouse and Ocadd .
 Either mark hDc; ci, or replace Ocadd with specialize(Ocadd ; Dc ).
Choice point: Both options must be considered for completeness.
5 Goal updating: Let G be the set of preconditions in P that are necessarily false.
6 Recursive invocation: UA-C(P ; G ).
0

0

0

0

0

Figure 12: The ua-C planning algorithm
underneath each operator. For instance, the rst operator in the plan at the top of the
page has precondition p. This operator adds q and conditionally adds u if t is true. The
gure illustrates two of the plans produced as a result of adding a new conditional operator
to the plan. In one plan, the conditional eects [u ! s] and [t ! u] are selected in the
specialization process, and in the other plan they are not.
The new step, Step 4b, requires only polynomial time per plan generated, and the time
cost of the other steps are the same as for ua. Hence, as with our original ua algorithm,
the cost per node for the ua-c algorithm is polynomial.
to can also handle this language given the corresponding modications (changing Step
3 and adding Step 4b), and the time cost per plan also remains polynomial.15 Moreover,
the same relationship holds between the two planners' search spaces { treeUA is never larger
than treeTO and can be exponentially smaller. This example illustrates that the theoretical
advantages that ua has over to can be preserved for a more expressive language. As we
pointed out, our denition of interaction is a sucient criterion for guaranteeing that the
resulting plans are unambiguous, but it is not a necessary criterion. Nevertheless, this
conservative approach allows interactions to be detected via a simple inexpensive syntactic
test. Essentially, we have kept the cost per node for ua-c low by restricting the search space
it considers, as shown in Figure 14. ua-c only considers unambiguous plans that can be
generated via its \conservative" ordering strategy. ua-c is still a partial-order planner, and
15. In fact, Step 4b be implemented so that the time cost is O(e), using the graph traversal techniques
described in Section 6. As a result the ua-c implementation and the corresponding to-c implementation
have the same time cost per node for this new language as they did for the original language, O(e) and
O (n), respectively.

248

Total-Order and Partial-Order Planning

q
r O
s
p

q

O
[t

u]

O
O

Add Operator:
[u

O
p
[t

[u

q

O

r
s]

r
s]

q
r O
s

u
p

u]

t

O

q

O

r
s

q
r O
s

Ou
O

Figure 13: An example illustrating the ua-c algorithm
it is complete, but it does not consider all partially ordered plans or even all unambiguous
partially ordered plans.
The same \trick" can be used for other languages as well, provided that we can devise
a simple test to detect interacting operators. For example, in previous work (Minton et al.,
1991b) we showed how this can be done for a language where operators can have variables in
their preconditions and eects. In the general case, for a given ua plan and a corresponding
to plan, Steps 1,2, and 3 of the ua algorithm cost the same as the corresponding steps of
the to algorithm. As long as the plans considered by ua are unambiguous, Step 5 of the
ua algorithm can be accomplished with an arbitrary linearization of the plan, in which case
it costs at most O(e) more than Step 5 of the to algorithm. Thus, the only possibility for
additional cost is in Step 4. In general, if we can devise a \local" criterion for interaction
such that the resulting plan is guaranteed to be unambiguous, then the ordering selection
step can be accomplished in polynomial time. By \local", we mean a criterion that only
considers operator pairs to determine interactions; i.e., it must not examine the rest of the
plan.
Although the theoretical advantages that ua has over to can be preserved for more
expressive languages, there is a cost. The unambiguous plans that are considered may have
more orderings than necessary, and the addition of unnecessary orderings can increase the
size of ua's search tree. The magnitude of this increase depends on the specic language,
domain, and problem being considered. Nevertheless, we can guarantee that ua's search
tree is never larger than to's.
The general lesson here is that the cost of plan extension is not solely dependent on
the expressiveness of the operator language, it also depends on the nature of the plans that
249

Minton, Bresina, & Drummond

partially ordered plans
unambiguous
partially ordered plans
unambiguous partially
ordered plans produced by
conservative ordering strategy
totally ordered
plans

Figure 14: Hierarchy of Plan Spaces
the planner considers. So, although the extension of partially ordered plans is NP-hard for
languages with conditional eects, if the space of plans is restricted (e.g., only unambiguous
plans are considered) then this worst-case situation is avoided.

9.2 Less Committed Planners

We have shown that ua, a partial-order planner, can have certain computational advantages
over a total-order planner, to, since its ability to delay commitments allows for a more
compact search space and potentially more intelligent ordering choices. However, there
are many planners that are even less committed than ua. In fact, there is a continuum
of commitment strategies that we might consider, as illustrated in Figure 15. Total-order
planning lies at one end of the spectrum. At the other extreme is the strategy of maintaining
a totally unordered set of steps during search until there exists a linearization of the steps
that is a solution plan.
Compared to many well-known planners, ua is conservative since it requires each plan
to be unambiguous. This is not required by noah (Sacerdoti, 1977), NonLin (Tate, 1977),
Totally
Ordered
TO

Completely
Unordered
UA

Figure 15: A continuum of commitment strategies
250

Total-Order and Partial-Order Planning

MT(P; G)
1. Termination check: If G is empty, report success and stop.
2. Goal selection: Let c = select-goal(G), and let Oneed be the plan step for which c is a precondition.
3. Operator selection: Let Oadd be either a plan step possibly before Oneed that adds c or an operator
in the library that adds c. If there is no such Oadd , then terminate and report failure.
Choice point: all such operators must be considered for completeness.
4. Ordering selection: Order Oadd before Oneed. Repeat until there are no steps possibly between
Oadd and Oneed which delete c:
Let Odel be such a step; choose one of the following ways to make c true for Oneed
 Order Odel after Oneed .
 Choose a step Oknight (possibly Oadd ) that adds c that is possibly between Odel and Oneed;
order it after Odel and before Oneed .
Choice point: both alternatives must be considered for completeness.
Let P be the resulting plan.
5. Goal updating: Let G be the set of preconditions in P that are not necessarily true.
6. Recursive invocation: MT(P ; G ).
0

0

0

0

0

Figure 16: A Propositional Planner based on the Modal Truth Criterion
nor Tweak (Chapman, 1987), for example. How do these less-committed planners compare
to ua and to? One might expect a less-committed planner to have the same advantages
over ua that ua has over to. However, this is not necessarily true. As an example, in
this section we introduce a Tweak-like planner, called mt, and show that its search space
is larger than even to's in some circumstances.16
Figure 16 presents the mt procedure. mt is a propositional planner based on Chapman's
Modal Truth Criterion (Chapman, 1987), the formal statement that characterizes Tweak's
search space. It is straightforward to see that mt is less committed than ua. The algorithms
are quite similar; however, in Step 4, whereas ua orders all interacting steps, mt does not.
Since mt does not immediately order all interacting operators, it may have to add additional
orderings between previously introduced operators later in the planning process to produce
correct plans.
The proof that ua's search tree is no larger than to's search tree rested on the two
properties of L elaborated in Section 5. By investigating the relationship between mt and
to, we found that the second property, the disjointness property, does not hold for mt,
and its failure illustrates how mt can explore more plans than to (and, consequently, than
ua) on certain problems. The disjointness property guarantees that ua does not generate
\overlapping" plans. The example in Figure 17 shows that mt fails to satisfy this property
because it can generate plans that share common linearizations, leading to considerable
redundancy in the search tree. The gure shows three steps, O1, O2, and O3 , where each Oi
has precondition pi and added conditions gi, p1, p2, and p3 . The nal step has preconditions
g1, g2, and g3, but the initial and nal steps are not shown in the gure. At the top of the
gure, in the plan constructed by mt, goals g1 , g2, and g3 have been achieved, but p1 , p2,
and p3 remain to be achieved. Subsequently, in solving precondition p1, mt generates plans
which share the linearization O3  O2  O1 (among others). In comparison, both to and
16. We use Tweak for this comparison because, like ua and to, it is a formal construct rather than a realistic
planner, and therefore more easily analyzed.

251

Minton, Bresina, & Drummond

O1
O2
O3

O1

O2

O2

O3

O3

O2

O1

O3

O1

O3

O2

O1

KEY
p O1
1

g1
p
1
p2
p3

p2 O
2

g2
p
1
p2
p3

p3 O3

g3
p
1
p2
p3

Figure 17: \Overlapping" plans.
ua only generate the plan O3  O2  O1 once. In fact, it is simple to show that, under
breadth-rst search, mt explores many more plans than to on this example (and also more
than ua, by transitivity) due to the redundancy in its search space.
This result may seem counterintuitive. However, note that the search space size for a
partial-order planner is potentially much greater than that of a total-order planner since
there are many more partial orders over a set of steps than there are total orders. (Thus,
when designing a partial-order planner, one may preclude overlapping linearizations in order
to avoid redundancy, as discussed by McAllester & Rosenblitt, 1991 and Kambhampati,
1994c.)
Of course, one can also construct examples where mt does have a smaller search space
than both ua and to. Our example simply illustrates that although one planner may be
less committed than another, its search space is not necessarily smaller. The commitment
strategy used by a planner is simply one factor that inuences overall performance. In
particular, the eect of redundancy in a partial-order planner can overwhelm other considerations. In comparing two planners, one must carefully consider the mapping between
their search spaces before concluding that \less committed ) smaller search space".

10. Related Work
For many years, the intuitions underlying partial-order planning were largely taken for
granted. Only in the past few years has there been renewed interest in the fundamental
principles underlying these issues.
252

Total-Order and Partial-Order Planning

Barrett et al. (1991) and Barrett and Weld (1994) describe an interesting and novel
analysis of partial-order planning that complements our own work. They compare a partialorder planner with two total-order planners derived from it, one that searches in the space
of plans, and the other that searches in the space of world states. Their study focuses
on how the goal structure of the problem aects the eciency of partial-order planning.
Specically, they examine how partial-order and total-order planning compare for problems
with independent, serializable, and non-serializable goals, when using a resource-bounded
depth-rst search. They rene Korf's work on serializable goals (Korf, 1987), introducing a
distinction between trivially serializable subgoals, where the subgoals can be solved in any
order without violating a previously solved subgoal, and laboriously serializable subgoals,
where the subgoals are serializable, but at least 1=n of the orderings can cause a previously
solved subgoal to be violated. Their study describes conditions under which a partial-order
planner may have an advantage. For instance, they show that in a domain where the goals
are trivially serializable for their partial-order planner and laboriously serializable for their
total-order planners, their partial-order planner performs signicantly better.
Our study provides an interesting contrast to Barret and Weld's work, since we investigate the relative eciencies of partial-order and total-order planning algorithms independent
of any particular domain structure. Instead, we focus on the underlying properties of the
search space and how the search strategy aects the eciency of our planners. Nevertheless,
we believe there are interesting relationships between the forms of serializability that they
investigate, and the ideas of solution density and clustering that we have discussed here.
To illustrate this, consider an articial domain that Barret and Weld refer to as D1S 1,
where, in each problem, the goals are a subset of fG1; G2; : : :G15g, the initial conditions
are fI1; I2; : : :I15g, and each operator Oi2f1;2;:::;15g has precondition Ii , adds Gi , and deletes
Ii,1. It follows that if a solution in D1S 1 contains operators Oi and Oj where i < j , then Oi
must precede Oj . In this domain, the goals are trivially serializable for their partial-order
planner and laboriously serializable for their total-order planners; thus, the partial-order
planner performs best. But note also that in this articial domain, there is exactly one
solution per problem and it is totally ordered. Therefore, it is immediately clear that, if
we give ua and to problems from this domain, then ua's search tree will generally be
much smaller than to's search tree. Since there is only single solution for both planners,
the solution density for ua will clearly be greater than that for to. Thus, the properties
we discussed in this paper should provide a basis for analyzing how dierences in subgoal
serializibility manifest their eect on the search. This subject, however, is not as simple as
it might seem and deserves further study.
In other related work, Kambhampati has written several papers (Kambhampati, 1994a,
1994b, 1994c) that analyze the design space of partial-order planners, including the ua
planner presented here. Kambhampati compares ua, Tweak, snlp (McAllester & Rosenblitt, 1991), ucpop (Penberthy & Weld, 1992), and several other planners along a variety of
dimensions. He presents a generalized schema for partial order planning algorithms (Kambhampati, 1994c) and shows that the commitment strategy used in ua can be viewed as a
way to increase the tractability of the plan extension (or renement) process. Kambhampati
also carries out an empirical comparison of the various planning algorithms on a particular problem (Kambhampati, 1994a), showing how the dierences in commitment strategies
aects the eciency of the planning process. He distinguishes two separate components
253

Minton, Bresina, & Drummond

of the branching factor, bt and be , the former resulting from the commitment strategy for
operator ordering (or in his terms, the \tractability renements") and the latter resulting
from the choice of operator (\establishment renements"). Kambhampati's experiments
demonstrate that while \eager" commitment strategies tend to increase bt, sometimes they
also decrease be , because the number of possible establishers is reduced when plans are more
ordered. This is, of course, closely related to the issues investigated in this paper.
In addition, Kambhampati and Chen (1993) have compared the relative utility of reusing
partially ordered and totally ordered plans in \learning planners". They showed that the
reuse of partially ordered plans, rather than totally ordered plans, result in \storage compaction" because they can represent a large number of dierent orderings. Moreover, partialorder planners have an advantage because they can exploit such plans more eectively than
total-order planners. In many respects, these advantages are fundamentally similar to the
advantages that ua derives from its potentially smaller search space.

11. Conclusions
By focusing our analysis on a single issue, namely, operator ordering commitment, we have
been able to carry out a rigorous comparative analysis of two planners. We have shown
that the search space of a partial-order planner, ua, is never larger than the search space of
a total-order planner, to. Indeed for certain problems, ua's search space is exponentially
smaller than to's. Since ua pays only a small polynomial time increment per node over
to, it is generally more ecient.
We then showed that ua's search space advantage may not necessarily translate into
an eciency gain, depending in subtle ways on the search strategy and heuristics that are
employed by the planner. For example, our experiments suggest that distribution-sensitive
search strategies, such as depth-rst search, can benet more from partial orders than can
search strategies that are distribution-insensitive.
We also examined a variety of extensions to our planners, in order to demonstrate
the generality of these results. We argued that the potential benets of partial-order
planning may be retained even with highly expressive planning languages. However, we
showed that partial-order planners do not necessarily have smaller search spaces, since
some \less-committed" strategies may create redundancies in the search space. In particular, we demonstrated that a Tweak-like planner, mt, can have a larger search space than
our total-order planner on some problems.
How general are these results? Although our analysis has considered only two specic
planners, we have examined some important tradeos that are of general relevance. The
analysis clearly illustrates how the planning language, the search strategy, and the heuristics
that are used can aect the relative advantages of the two planning styles.
The results in this paper should be considered as an investigation of the possible benets
of partial-order planning. ua and to have been constructed in order for us to analyze the
total-order/partial-order distinction in isolation. In reality, the comparative behavior of two
planners is rarely as clear (as witnessed by our discussion of mt). While the general points
we make are applicable to other planners, if we chose two arbitrary planners, we would not
expect one planner to so clearly dominate the other.
254

Total-Order and Partial-Order Planning

Our observations regarding the interplay between plan representation and search strategy raise new concerns for comparative analyses of planners. Historically, it has been
assumed that representing plans as partial orders is categorically \better" than representing plans as total orders. The results presented in this paper begin to tell a more accurate
story, one that is both more interesting and more complex than we initially expected.

Appendix A. Proofs
A.1 Denitions

This section denes the terminology and notation used in our proofs. The notion of plan
equivalence is introduced here because each plan step is, by denition, a uniquely labeled
operator instance, as noted in Section 3 and Section 5. Thus, no two plans have the same
set of steps. Although this formalism simplies our analysis, it requires us to dene plan
equivalence explicitly.

 A plan is a pair h; i, where  is a set of steps, and  is the \before" relation on ,
i.e.,  is a strict partial order on . Notationally, O1  O2 if and only if (O1; O2) 2.
 For a given problem, we dene the search tree treeTO as the complete tree of plans
that is generated by the to algorithm on that problem. treeUA is the corresponding








search tree generated by ua on the same problem.
Two plans, P1 = h1; 1 i and P2 = h2 ; 2i are said to be equivalent, denoted
P1 ' P2, if there exists a bijective function f from 1 to 2 such that:
{ for all O 2 1, O and f (O) are instances of the same operator, and
{ for all O0; O00 2 1, O0  O00 if and only if f (O0)  f (O00).
A plan P2 is a 1-step to-extension (or 1-step ua-extension) of a plan P1 if P2 is
equivalent to some plan produced from P1 in one invocation of to (or ua).
A plan P is a to-extension (or ua-extension) if either:
{ P is the initial plan, or
{ P is a 1-step to-extension (or 1-step ua-extension) of a to-extension (or uaextension).
It immediately follows from this denition that if P is a member of treeTO (or treeUA ),
then P is a to-extension (or ua-extension). In addition, if P is a to-extension (or
ua-extension), then some plan that is equivalent to P is a member of treeTO (or
treeUA ).
P1 is a linearization of P2 = h; 2i if there exists a strict total order 1 such that
2  1 and P1 ' h; 1i.
Given a search tree, let parent be a function from a plan to its parent plan in the tree.
Note that P1 is the parent of P2 , denoted P1 = parent(P2 ), only if P2 is a 1-step
extension of P1 .
255

Minton, Bresina, & Drummond

 Given U 2 treeUA and T 2 treeTO , T 2 L(U ) if and only if plan T is a linearization
of plan U and either both U and T are root nodes of their respective search trees, or
parent(T ) 2 L(parent(U )).
 The length of the plan is the number of steps in the plan excluding the rst and last
steps. Thus, the initial plan has length 0. A plan P with n steps has length n , 2.
 P1 is a subplan of P2 = h2; 2i if P1 ' h1; 1i, where
{ 1  2 and
{ 1 is 2 restricted to 1, i.e., 1 = 2 \ 1 1.
 P1 is a strict subplan of P2, if P1 is a subplan of P2 and the length of P1 is less than
the length of P2.
 A solution plan P is a compact solution if no strict subplan of P is a solution.

A.2 Extension Lemmas
TO-Extension Lemma: Consider totally ordered plans T0 = h0; 0i and T1 = h1; 1i,
such that 1 = 0 [ fOadd g and 0  1 . Let G be the set of false preconditions in T0.

Then T1 is a 1-step to-extension of T0 if:
 c = select-goal(G), where c is the precondition of some step Oneed in T0, and
 Oadd adds c, and
 (Oadd; Oneed) 21, and
 (Odel; Oadd) 21, where Odel is the last deleter of c in T1.

Proof Sketch: This lemma follows from the denition of to. Given plan T0, with false
precondition c, once to selects c as the goal, to will consider all operators that achieve c,
and for each operator to considers all positions before c and after the last deleter of c.
UA-Extension Lemma: Consider a plan U0 = h0; 0i produced by ua and plan
U1 = h1; 1i, such that 1 = 0 [ fOaddg and 0  1. Let G be the set of false
preconditions of the steps in U0 . Then U1 is a 1-step ua-extension of U0 if:
 c = select-goal(G), where c is the precondition of some step Oneed in U0, and
 Oadd adds c, and
 1 is a minimal set of consistent orderings such that
{ 0  1, and
{ (Oadd; Oneed) 21, and
{ (Odel; Oadd) 21, where Odel is the last deleter of c in U1, and
{ no step in U1 interacts with Oadd
256

Total-Order and Partial-Order Planning

Proof Sketch: This lemma follows from the denition of ua. Given plan U0, with false

precondition c, ua considers all operators that achieve c, and for each such operator ua then
inserts it in the plan such that it is before c and after the last deleter. ua then considers
all consistent combinations of orderings between the new operator and the operators with
which it interacts. No other orderings are added to the plan.

A.3 Proof of Search Space Correspondence L
Mapping Lemma: Let U0 = h0; u0i be an unambiguous plan and let U1 = h1; u1i
be a 1-step ua-extension of U0. If T1 = h1; t1i is a linearization of U1 , then there exists

a plan T0 such that T0 is a linearization of U0 and T1 is a 1-step to-extension of T0 .
Proof: Since U1 is a 1-step ua-extension of U0, there is a step Oadd such that 1 = 0 [
fOaddg. Let T0 be the subplan produced by removing Oadd from T1; that is, T0 = h0; t0i,
where t0 = t1 \ 0 0 . Since u0 = u1 \ 0 0  t1 \ 0 0 = t0 , it follows that T0
is a linearization of U0.
Using the TO-Extension lemma, we can show that T1 is a 1-step to-extension of T0.
First, T0 is a linearization of U0 , so the two plans have the same set of goals. Therefore, if
ua selects some goal c in expanding U0 , to selects c in extending T0. Second, it must be
the case that Oadd adds c since Oadd is the step ua inserted into U0 to make c true. Third,
Oadd is before Oneed in T1, since Oadd is before Oneed in U1 (by denition of ua) and since
T1 is a linearization of U1. Fourth, Oadd is after the last deleter of c, Odel, in T1, since Oadd
is after Odel in U1 (by denition of ua) and since T1 is a linearization of U1. Therefore, the
conditions of the TO-Extension lemma hold and, thus, T1 is a 1-step to-extension of T0.
Q.E.D.

Totality Property For every plan U in treeUA , there exists a non-empty set fT1; : : :; Tmg
of plans in treeTO such that L(U ) = fT1; : : :; Tmg.
Proof: It suces to show that if plan U1 is a ua-extension and plan T1 is a linearization

of U1 , then T1 is a to-extension. The proof is by induction on plan length.
Base case: The statement trivially holds for plans of length 0.
Induction step: Under the hypothesis that the statement holds for plans of length n, we
now prove that the statement holds for plans of length n + 1. Suppose that U1 is a uaextension of length n + 1 and T1 is a linearization of U1 . Let U0 be a plan such that U1 is a
1-step ua-extension of U0 . By the Mapping lemma, there exists a plan T0 such that T0 is a
linearization of U0 and T1 is a 1-step to-extension of T0. By the induction hypothesis, T0
is a to-extension. Therefore, by denition, T1 is also a to-extension. Q.E.D.

Disjointness Property: L maps distinct plans in treeUA to disjoint sets of plans in treeTO ;
that is, if U1 ; U2 2 treeUA and U1 =
6 U2, then L(U1) \ L(U2) = fg.
Proof: By the denition of L, if T1; T2 2 L(U ), then T1 and T2 are at the same tree depth
d in treeTO ; furthermore, U is also at depth d in treeUA . Hence, it suces to prove that if
plans U1 and U2 are at depth d in treeUA and U1 6= U2 , then L(U1 ) \ L(U2 ) = fg.

Base case: The statement vacuously holds for depth 0.
Induction step: Under the hypothesis that the statement holds for plans at depth n, we
prove, by contradiction, that the statement holds for plans at depth n + 1. Suppose that
257

Minton, Bresina, & Drummond

there exist two distinct plans, U1 = h1; 1 i and U2 = h2 ; 2i, at depth n + 1 in
treeUA such that T 2 L(U1) \L(U2). Then (by denition of L), parent(T ) 2 L(parent(U1))
and parent(T ) 2 L(parent(U2 )). Since parent(U1 ) 6= parent(U2 ) contradicts the induction
hypothesis, suppose that U1 and U2 have the same parent U0 . Then, by the denition
of ua either (i) 1 6= 2 or (ii) 1 = 2 and 1 6=2 . In the rst case, since the two
plans do not contain the same set of plan steps, they have disjoint linearizations and,
hence, L(U1 ) \ L(U2 ) = fg, which contradicts the supposition. In the second case,
1 = 2; hence, both plans resulted from adding plan step Oadd to the parent plan. Since
16=2, there exists a plan step Oint that interacts with Oadd such that in one plan Oint
is ordered before Oadd and in the other plan Oadd is ordered before Oint . Thus, in either
case, the linearizations of the two plans are disjoint and, hence, L(U1 ) \ L(U2 ) = fg,
which contradicts the supposition. Therefore, the statement holds for plans at depth n + 1.
Q.E.D.

A.4 Completeness Proof for TO

We now prove that to is complete under a breadth rst search control strategy. To do so, it
suces to prove that if there exists a solution to a problem, then there exists a to-extension
that is a compact solution. Before doing so, we prove the following lemma.

Subplan Lemma: Let totally ordered plan T0 be a strict subplan of a compact solution Ts.

Then there exists a plan T1 such that T1 is a subplan of Ts and T1 is a 1-step to-extension
of T0.
Proof: Since T0 is a strict subplan of Ts and Ts is a compact solution, the set of false
preconditions in T0, G, must not be empty. Let c = select-goal(G), let Oneed be the
step in T0 with precondition c, and let Oadd be the step in Ts that achieves c. Consider the
totally ordered plan T1 = h0 [ fOadd g; 1i, where 1  s . Clearly, T1 is a subplan of
Ts. Furthermore, by the TO-Extension Lemma, T1 is a 1-step extension of T0 by to. To
see this, note that Oadd is ordered before Oneed in T1 since it is ordered before Oneed in Ts .
Similarly, Oadd is ordered after the last deleter of c in T0 since any deleter of c in T0 is a
deleter of c in Ts , and Oadd is ordered after the deleters of c in Ts . Thus, the conditions of
the TO-Extension Lemma hold. Q.E.D.
TO

Completeness Theorem: If plan Ts is a totally ordered compact solution, then Ts

is a to-extension.
Proof: Let n be the length of Ts. We show that for all k  n, there exists a subplan of Ts
with length k that is a to-extension. This is sucient to prove our result since any subplan
of exactly length n is equivalent to Ts . The proof is by induction on k.
Base case: If k = 0 the statement holds since the initial plan, which has length 0, is a
subplan of any solution plan.
Induction step: We assume that the statement holds for k and show that if k < n the
statement holds for k + 1. By the induction hypothesis, there exists a plan T0 of length k
that is a strict subplan of Ts . By the Subplan Lemma, there exists a plan T1 that is both a
subplan of Ts and a 1-step to-extension of T0. Thus, there exists a subplan of Ts of length
k + 1. Q.E.D.
258

Total-Order and Partial-Order Planning

A.5 Completeness Proof for UA

We now prove that ua is complete under a breadth-rst search strategy. The result follows
from the search space correspondence dened by L and the fact that to is complete. In
particular, we show below that for any to-extension T , there exists a ua-extension U such
that T is a linearization of U . Since ua produces only unambiguous plans, it must be the
case that if T is a solution, U is also a solution. From this, it follows immediately that ua
is complete.

Inverse Mapping Lemma: Let T0 = h0; t0i be a totally ordered plan. Let T1 =
h1; t1i be a 1-step to-extension of T0. Let U0 = h0; u0i be a plan produced by ua such
that T0 is a linearization of U0 . Then there exists a plan U1 such that T1 is a linearization
of U1 and U1 is a 1-step ua-extension of U0 .
Proof: By the denition of to, 1 = 0 [ fOaddg, where Oadd added some c that is a
false precondition of some plan step Oneed in U0 . Consider U1 = h1 ; u1 i, where u1 is a
minimal subset of t1 such that:
 u0  u1, and
 (Oadd; Oneed) 2u1, and
 (Odel; Oadd) 2u1 , where Odel is the last deleter of c in U1, and
 no step in U1 interacts with Oadd
Since u1  t1 , T1 is a linearization of U1. In addition, U1 is an extension of U0 since
it meets the three conditions of the UA-Extension Lemma, as follows. First, since c must
have been the goal selected by to in extending T0, c must likewise be selected by ua in
extending U0 . Second, Oadd adds c since Oadd achieves c in T0. Finally, by construction,
u1 satises the third condition of the UA-Extension Lemma. Q.E.D.
UA Completeness Theorem: Let Ts be a totally ordered compact solution. Then there

exists a ua-extension Us such that Ts is a linearization of Us .
Proof: Since to is complete, it suces to show that if T1 is a to-extension, then there
exists a ua-extension U1 such that T1 is a linearization of U1 . The proof is by induction on
plan length.
Base case: The statement trivially holds for plans of length 0.
Induction step: Under the hypothesis that the statement holds for plans of length n, we
now prove that the statement holds for plans of length n + 1. Assume T1 is a to-extension
of length n + 1, and let T0 be a plan such that T1 is a 1-step to-extension of T0. By
the induction hypothesis, there exists a ua-extension U0 of length n such that T0 is a
linearization of U0. By the Inverse Mapping Lemma, there exists a plan U1 that is both a
linearization of T1 and a 1-step ua-extension of U0 . Since U1 is a 1-step ua-extension of
U0, it has length n + 1. Q.E.D.

259

Minton, Bresina, & Drummond

Acknowledgements
Most of the work present in this paper was originally described in two conference papers
(Minton et al., 1991a, 1992). We thank Andy Philips for his many contributions to this
project. He wrote the code for the planners and helped conduct the experiments. We also
thank the three anonymous reviewers for their excellent comments.

References

Backstrom, C. (1993). Finding least constrained plans and optimal parallel executions
is harder than we thought. In Proceedings of the Second European Workshop on
Planning.
Barrett, A., Soderland, S., & Weld, D. (1991). The eect of step-order representations on
planning. Tech. rep. 91-05-06, Univ. of Washington, Computer Science Dept.
Barrett, A., & Weld, D. (1994). Partial-order planning: Evaluating possible eciency gains.
Articial Intelligence, 67 (1), 71{112.
Chapman, D. (1987). Planning for conjunctive goals. Articial Intelligence, 32, 333{377.
Chen, P. (1989). Heuristic Sampling on Backtrack Trees. Ph.D. thesis, Dept. of Computer
Science, Stanford Univ., Stanford, CA.
Collins, G., & Pryor, L. (1992). Achieving the functionality of lter conditions in a partial
order planner. In Proceedings of the Tenth National Conference on Articial Intelligence.
Crawford, J., & Baker, A. (1994). Experimental results on the application of satisability
algorithms to scheduling problems. In Proceedings of the Twelfth National Conference
on Articial Intelligence.
Dean, T., & Boddy, M. (1988). Reasoning about partially ordered events. Articial Intelligence, 36, 375{399.
Drummond, M., & Currie, K. (1989). Goal-ordering in partially ordered plans. In Proceedings of the Eleventh International Joint Conference on Articial Intelligence.
Ginsberg, M., & Harvey, W. (1992). Iterative broadening. Articial Intelligence, 55, 367{
383.
Godefroid, P., & Kabanza, F. (1991). An ecient reactive planner for synthesizing reactive
plans. In Proceedings of the Ninth National Conference on Articial Intelligence.
Hertzberg, J., & Horz, A. (1989). Towards a theory of conict detection and resolution
in nonlinear plans. In Proceedings of the Eleventh International Joint Conference on
Articial Intelligence.
Kambhampati, S. (1994a). Design tradeos in partial order (plan space) planning. In
Proceedings of the Second International Conference on AI Planning Systems.
260

Total-Order and Partial-Order Planning

Kambhampati, S. (1994b). Multi contributor causal structures for planning: A formalization
and evaluation. Articial Intelligence, 69, 235{278.
Kambhampati, S. (1994c). Renement search as a unifying framework for analyzing plan
space planners. In Proceedings of the Fourth International Conference on Principles
of Knowledge Representation and Reasoning.
Kambhampati, S., & Chen, J. (1993). Relative utility of EBG-based plan reuse in partial ordering vs. total ordering planning. In Proceedings of the Eleventh National Conference
on Articial Intelligence.
Korf, R. (1985). Depth-rst iterative deepening: An optimal admissible tree search. Articial Intelligence, 27, 97{109.
Korf, R. (1987). Planning as search: A quantitative approach. Articial Intelligence, 33,
65{88.
Langley, P. (1992). Systematic and nonsystematic search strategies. In Proceedings of the
First International Conference on AI Planning Systems.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proceedings of
the Ninth National Conference on Articial Intelligence.
Minton, S., Bresina, J., & Drummond, M. (1991a). Commitment strategies in planning: A
comparative analysis. In Proceedings of the Twelfth International Joint Conference
on Articial Intelligence.
Minton, S., Bresina, J., Drummond, M., & Philips, A. (1991b). An analysis of commitment
strategies in planning: The details. Tech. rep. 91-08, NASA Ames, AI Research
Branch.
Minton, S., Drummond, M., Bresina, J., & Philips, A. (1992). Total order vs. partial order
planning: Factors inuencing performance. In Proceedings of the Third International
Conference on Principles of Knowledge Representation and Reasoning.
Pednault, E. (1988). Synthesizing plans that contain actions with context-dependent eects.
Computational Intelligence, 4, 356{372.
Penberthy, J., & Weld, D. (1992). UCPOP: A sound, complete, partial-order planner for
adl. In Proceedings of the Third International Conference on Principles of Knowledge
Representation and Reasoning.
Regnier, P., & Fade, B. (1991). Complete determination of parallel actions and temporal
optimization in linear plans of action. In Hertzberg, J. (Ed.), European Workshop
on Planning, Vol. 522 of Lecture Notes in Articial Intelligence, pp. 100{111 Sankt
Augustin, Germany. Springer.
Rosenbloom, P., Lee, S., & Unruh, A. (1993). Bias in planning and explanation-based learning. In Minton, S. (Ed.), Machine Learning Methods for Planning. Morgan Kaufmann
Publishers.
261

Minton, Bresina, & Drummond

Sacerdoti, E. (1975). The nonlinear nature of plans. In Proceedings of the Fourth International Joint Conference on Articial Intelligence.
Sacerdoti, E. (1977). A Structure for Plans and Behavior. American Elsivier, New York.
Tate, A. (1974). Interplan: A plan generation system which can deal with interactions
between goals. Tech. rep. Memo MIP-R-109, Univ. of Edinburgh, Machine Intelligence
Research Unit.
Tate, A. (1977). Generating project networks. In Proceedings of the Fifth International
Joint Conference on Articial Intelligence.
Veloso, M., Perez, M., & Carbonell, J. (1990). Nonlinear planning with parallel resource
allocation. In Proceedings of the Workshop on Innovative Approaches to Planning,
Scheduling and Control.
Waldinger, R. (1975). Achieving several goals simultaneously. In Machine Intelligence 8.
Ellis Harwood, Ltd.
Warren, D. (1974). Warplan: A system for generating plans. Tech. rep. Memo 76, Computational Logic Dept., School of AI, Univ. of Edinburgh.

262

Journal of Articial Intelligence Research 2 (1995) 411-446

Submitted 11/94; published 4/95

Rerepresenting and Restructuring Domain Theories:
A Constructive Induction Approach
Steven K. Donoho
Larry A. Rendell

Department of Computer Science, Univeristy of Illinois
405 N. Mathews Ave., Urbana, IL 61801 USA

donoho@cs.uiuc.edu
rendell@cs.uiuc.edu

Abstract

Theory revision integrates inductive learning and background knowledge by combining
training examples with a coarse domain theory to produce a more accurate theory. There
are two challenges that theory revision and other theory-guided systems face. First, a
representation language appropriate for the initial theory may be inappropriate for an
improved theory. While the original representation may concisely express the initial theory,
a more accurate theory forced to use that same representation may be bulky, cumbersome,
and dicult to reach. Second, a theory structure suitable for a coarse domain theory may
be insucient for a ne-tuned theory. Systems that produce only small, local changes to
a theory have limited value for accomplishing complex structural alterations that may be
required.
Consequently, advanced theory-guided learning systems require exible representation
and exible structure. An analysis of various theory revision systems and theory-guided
learning systems reveals specic strengths and weaknesses in terms of these two desired
properties. Designed to capture the underlying qualities of each system, a new system uses
theory-guided constructive induction. Experiments in three domains show improvement
over previous theory-guided systems. This leads to a study of the behavior, limitations,
and potential of theory-guided constructive induction.

1. Introduction
Inductive learners normally use training examples, but they can also use background knowledge. Eectively integrating this knowledge into induction has been a widely studied research problem. Most work to date has been in the area of theory revision in which the
knowledge given is a coarse, perhaps incomplete or incorrect, theory of the problem domain,
and training examples are used to shape this initial theory into a rened, more accurate
theory (Ourston & Mooney, 1990; Thompson, Langley, & Iba, 1991; Cohen, 1992; Pazzani
& Kibler, 1992; Baes & Mooney, 1993; Mooney, 1993). We develop a more exible and
more robust approach to the problem of learning from both data and theory knowledge by
addressing the two following desirable qualities:

 Flexible Representation. A theory-guided system should utilize the knowledge contained in the initial domain theory without having to adhere closely to the initial
theory's representation language.

 Flexible Structure. A theory-guided system should not be unnecessarily restricted by
the structure of the initial domain theory.

c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Donoho & Rendell

Before giving more precise denitions of our terms, we motivate our work intuitively.

1.1 Intuitive Motivation
The rst desirable quality, exibility of representation, arises because the theory representation most appropriate for describing the coarse, initial domain theory may be inadequate
for the nal, revised theory. While the initial domain theory may be compact and concise in
one representation, an accurate theory may be quite bulky and cumbersome in that representation. Furthermore, the representation that is best for expressing the initial theory may
not be the best for carrying out renements. A helpful renement step may be clumsy to
make in the initial representation yet be carried out quite simply in another representation.
As a simple example, a coarse domain theory may be expressed as the logical conjunction
of N conditions that should be met. The most accurate theory, though, is one in which any M
of these N conditions holds. Expressing this more accurate theory in the DNF representation
used to describe the initial theory would be cumbersome and unwieldy (Murphy & Pazzani,
1991). Furthermore, arriving at the nal theory using the renement operators most suitable
for DNF (drop-condition, add-condition, modify-condition) would be a cumbersome task.
But when an M-of-N representation is adopted, the renement simply involves empirically
nding the appropriate M, and the nal theory can be expressed concisely (Baes & Mooney,
1993).
Similarly, the second desirable quality, exibility of structure, arises because the theory
structure that was suitable for a coarse domain theory may be insucient for a ne-tuned
theory. In order to achieve the desired accuracy, a restructuring of the initial theory may be
necessary. Many theory revision systems act by making a series of local changes, but this
can lead to behavior at two extremes. The rst extreme is to rigidly retain the backbone
structure of the initial domain theory, only allowing small, local changes. Figure 1 illustrates
this situation. Minor revisions have been made { conditions have been added, dropped,
and modied { but the rened theory is trapped by the backbone structure of the initial
theory. When only local changes are needed, these techniques have proven useful (Ourston
& Mooney, 1990), but often more is required. When more is required, these systems often
move to the other extreme; they drop entire rules and groups of rules and then build entire
new rules and groups of rules from scratch to replace them. Thus they restructure, but
they forfeit valuable knowledge in the process. An ideal theory revision system would glean
knowledge from theory substructures that cannot be xed with small, local changes and
use this in a restructured theory.
As an intuitive illustration, consider a piece of software that \almost works." Sometimes
it can be made useful through only a few local operations: xing a couple of bugs, adding
a needed subroutine, and so on. In other cases, though, a piece of software that \almost
works" is in fact far from full working order. It may need to be redesigned and restructured.
A mistake at one extreme is to try to x a program like this by making a series of patches in
the original code. A mistake at the other extreme is to discard the original program without
learning anything from it and start from scratch. The best approach would be to examine
the original program to see what can be learned from its design and to use this knowledge
in the redesign. Likewise, attempting to improve a coarse domain theory through a series
of local changes may yield little improvement because the theory is trapped by its initial
412

Rerepresenting and Restructuring Domain Theories

Initial Theory

a

b

c

j

d e

h i

Refined Theory

n

k l m

s

a

b

c

o p q r

j

w e

f g

i

n

k l

u

o

p

v

r

t f g

Figure 1: Typical theory revision allows only limited structural exibility. Although conditions have been added, dropped, and modied, the revised theory is much
constrained by the structure of the initial theory.
structure. This does not render the original domain theory useless; careful analysis of the
initial domain theory can give valuable guidance for the design of the best nal theory.
This is illustrated in Figure 2 where many substructures have been taken from the initial
theory and adapted for use in the rened theory. Information from the initial theory has
been used, but the structure of the revised theory is not restricted by the structure of the
initial theory.
Initial Theory

a

b

c

n

j

d e

h i

Refined Theory

k l m

k

o p q r

l

f

d

g

m

e

h

i

s

o

p

u

v

f g q r

f g

Figure 2: More exible structural modication. The revised theory has taken many substructures from the initial theory and adapted and recombined them for its use,
but the structure of the revised theory is not restricted by the structure of the
initial theory.

413

Donoho & Rendell

1.2 Terminology

In this paper, all training data consist of examples which are classied vectors of feature/value pairs. We assume that an initial theory is a set of conditions combined using the
operators AND, OR, and NOT and indicating one or more classes. While it is unreasonable
to believe that all theories will always be of this form, it covers much existing theory revision
research.
Our work is intended as an informal exploration of exible representation and exible
structure. Flexible representation means allowing the theory to be revised using a representation language other than that of the initial theory. An example of exible representation
is the introduction of a new operator for combining features | an operator not used in
the initial theory. In Section 1.1 the example was given of introducing the M-of-N operator
to represent a theory originally expressed in DNF. Flexible structure means not limiting
revision of the theory to a series of small, incremental modications. An example of this
is breaking the theory down into its components and using them as building blocks in the
construction of a new theory.
Constructive induction is a process whereby the training examples are redescribed using
a new set of features. These new features are combinations of the original features. Bias
or knowledge may be used in the construction of the new features. A subtle point is that
when we speak of exible representation, we are referring only to the representation of the
domain theory, not the training data. Although the phrase \change of representation" is
often applied to constructive induction, this refers to a change of the data. In our paper,
the term exible representation is reserved for a change of theory representation. Thus
a system can be performing constructive induction (changing the feature language of the
data) without exhibiting exible representation (changing the representation of the theory).

1.3 Overview

Theory revision and constructive induction embody complementary aspects of the machine
learning research community's ultimate goals. Theory revision uses data to improve a
theory; constructive induction can use a theory to improve data to facilitate learning. In
this paper we present a theory-guided constructive induction approach which addresses the
two desirable qualities discussed in Section 1.1. The initial theory is analyzed, and new
features are constructed based on the components of the theory. The constructed features
need not be expressed in the same representational language as the initial theory and can
be rened to better match the training examples. Finally, a standard inductive learning
algorithm, C4.5 (Quinlan, 1993), is applied to the redescribed examples.
We begin by analyzing how landmark theory revision and learning systems have exhibited exibility in handling a domain theory and what part this has played in their performance. From this analysis, we extract guidelines for system design and apply them to the
design of our own limited system. In an eort to integrate learning from theory and data,
we borrow heavily from the theory revision, multistrategy learning, and constructive induction communities, but our guidelines for system design fall closest to classical constructive
induction methods. The central focus of this paper is not the presentation of \another new
system" but rather a study of exible representation and structure, their manifestation in
previous work, and their guidance for future design.
414

Rerepresenting and Restructuring Domain Theories

Section 2 gives the context of our work by analyzing previous research and its inuence on our work. Section 3 explores the Promoter Recognition domain and demonstrates
how related theory revision systems behave in this domain. In Section 4, guidelines for
theory-guided constructive induction are presented. These guidelines are a synthesis of the
positive aspects of related research, and they address the two desirable qualities, exibility
of representation and exibility of structure. Section 4 also presents a specic theory-guided
constructive induction algorithm which is an instantiation of the guidelines set forth earlier
in that section. Results of experiments in three domains are given in Section 5 followed
by a discussion of the strengths of theory-guided constructive induction in Section 6. Section 7 presents an experimental analysis of the limits of applicability of our simple algorithm
followed by a discussion of limitations and future directions of our work in Section 8.

2. Context and Related Work

Although our work bears some resemblance in form and objective to many papers in constructive induction (Michalski, 1983; Fu & Buchanan, 1985; Utgo, 1986; Schlimmer, 1987;
Drastal & Raatz, 1989; Matheus & Rendell, 1989; Pagallo & Haussler, 1990; Ragavan &
Rendell, 1993; Hirsh & Noordewier, 1994), theory revision (Ourston & Mooney, 1990; Feldman, Serge, & Koppel, 1991; Thompson et al., 1991; Cohen, 1992; Pazzani & Kibler, 1992;
Baes & Mooney, 1993), and multistrategy approaches (Flann & Dietterich, 1989; Towell,
Shavlik, & Noordeweir, 1990; Dzerisko & Lavrac, 1991; Bloedorn, Michalski, & Wnek, 1993;
Clark & Matwin, 1993; Towell & Shavlik, 1994), we focus only upon a handful of these systems, those that have signicant, underlying similarities to our work. In this section we
analyze Miro, Either, Focl, LabyrinthK , Kbann, Neither-MofN, and Grendel to
discuss their related underlying contributions in relationship to our perspective.

2.1

Miro

(Drastal & Raatz, 1989) is a seminal work in knowledge-guided constructive induction. It takes knowledge about how low-level features interact and uses this knowledge to
construct high-level features for its training examples. A standard learning algorithm is
then run on these examples described using the new features. The domain theory is used
to shift the bias of the induction problem (Utgo, 1986). Empirical results showed that
describing the examples in these high-level, abstract terms improved learning accuracy.
The Miro approach provides a means of utilizing knowledge in a domain theory without
being restricted by the structure of that theory. Substructures of the domain theory can
be used to construct high-level features that a standard induction algorithm will arrange
into a concept. Some constructed features will be used as they are, others will be ignored,
others will be combined with low-level features, and still others may be used dierently in
multiple contexts. The end result is that knowledge from the domain theory is utilized,
but the structure of the nal theory is not restricted by the structure of the initial theory.
Miro provides exible structure.
Another benet is that Miro-like techniques can be applied even when only a partial
domain theory exists, i.e., a domain theory that only species high-level features but does
not link them together or a domain theory that species some high-level features but not
others. One of Miro's shortcomings is that it provided no means of making minor changes
Miro

415

Donoho & Rendell

in the domain theory but rather constructed the features exactly as the domain theory
specied. Also the representation of Miro's constructed features was primitive | either
an example met the conditions of a high-level feature or did not. An example of Miro's
behavior is given in Section 3.2.

2.2

,

, and LabyrinthK

Either Focl

The

Either

(Ourston & Mooney, 1990), LabyrinthK (Thompson et al., 1991), and
Focl (Pazzani & Kibler, 1992) systems represent a broad spectrum of theory revision
work. They make steps toward eective integration of background knowledge and inductive
learning. Although these systems have many supercial dierences with regard to supervised/unsupervised learning, concept description language, etc., they share the underlying
principle of incrementally revising an initial domain theory through a series of local changes.
We will discuss Either as a representative of this class of systems. Either's theory
revision operators include: removing unwanted conditions from a rule, adding needed conditions to a rule, removing rules, and adding totally new rules. Either rst classies its
training examples according to the current theory. If any are misclassied, it seeks to repair
the theory by applying a theory revision operator that will result in the correct classication
of some previously misclassied examples without losing any of the correct examples. Thus
a series of local changes are made that allow for an improvement of accuracy on the training
set without losing any of the examples previously classied correctly.
Either-type methods provide simple yet powerful tools for repairing many important
and common faults in domain theories, but they fail to meet the qualities of exible representation and exible structure. Because the theory revision operators make small, local
modications in the existing domain theory, the nal theory is constrained to be similar in
structure to the initial theory. When an accurate theory is signicantly dierent in structure from the initial theory, these systems are forced to one of the two extremes discussed
in Section 1. The rst extreme is to become trapped at a local maximum similar to the
initial theory unable to reach the global maximum because only local changes can be made.
The other extreme is to drop entire rules and groups of rules and replace them with new
rules built from scratch thus forfeiting the knowledge contained in the domain theory.
Also, Either carries out all theory revision steps in the representation of the initial
theory. Consequently, the representation of the nal theory is the same as that of the initial
theory. Another representation may be more appropriate for the revised theory than the
one in which the initial theory comes, but facilities are not provided to accommodate this.
An advanced theory revision system would combine the locally acting strengths of Eithertype systems with exibility of structure and exibility of representation. An example of
Either's behavior is given in Section 3.3.

2.3

Kbann

and Neither-MofN

The Kbann system (Towell et al., 1990; Towell & Shavlik, 1994) makes unique contributions to theory revision work. Kbann takes an initial domain theory described symbolically
in logic and creates a neural network whose structure and initial weights encode this theory.
Backpropagation (Rumelhart, Hinton, & McClelland, 1986) is then applied as a renement tool for ne-tuning the network weights. Kbann has been empirically shown to give
416

Rerepresenting and Restructuring Domain Theories

signicant improvement over many theory revision systems for the widely-used Promoter
Recognition domain. Although our work is dierent in implementation from Kbann, our
abstract ideologies are similar.
One of Kbann's important contributions is that it takes a domain theory in one representation (propositional logic) and translates it into a less restricting representation (neural
network). While logic is an appropriate representation for the initial domain theory for the
promoter problem, the neural network representation is more convenient both for rening
this theory and for expressing the best revised theory. This change of representation is
Kbann's real source of power. Much attention has been given to the fact that Kbann combines symbolic knowledge with a subsymbolic learner, but this combination can be viewed
more generally as a means of implementing the important change of representation. It may
be the change of representation that gives Kbann its power, not necessarily its specic
symbolic/subsymbolic implementation. Thus the Kbann system embodies the higher-level
principle of allowing renement to occur in an appropriate representation.
If an alternative representation is Kbann's source of power, the question must be raised
as to whether the actual Kbann implementation is always the best means of achieving this
goal. The neural network representation may be more expressive than is required. Accordingly, backpropagation often has more renement power than is needed. Thus Kbann may
carry excess baggage in translating into the neural net representation, performing expensive
backpropagation, and extracting symbolic rules from the rened network. Although the full
extent of Kbann's power may be needed for some problems, many important problems may
be solvable by applying Kbann's principles at the symbolic level using less expensive tools.
Neither-MofN (Baes & Mooney, 1993), a descendant of Either, is a second example
of a system that allows a theory to be revised in a representation other than that of the
initial theory. The domain theory input into Neither-MofN is expressed in propositional
logic as an AND/OR tree. Neither-MofN interprets the theory less rigidly | an AND
rule is true any time any M of its N conditions are true. Initially M is set equal to N (all
conditions must be true for the rule to be true), and one theory renement operator is to
lower M for a particular rule. The end result is that examples that are a close enough
partial match to the initial theory are accepted. Neither-MofN, since it is built upon
the Either framework, also includes Either-like theory revision operators: add-condition,
drop-condition, etc.
Thus Neither-MofN allows revision to take place in a representation appropriate
for revision and appropriate for concisely expressing the best rened theory. NeitherMofN has achieved results comparable to Kbann on the Promoter Recognition domain,
which suggests that it is the change of representation which these two systems share that
give them their power rather than any particular implementation. Neither-MofN also
demonstrates that a small amount of representational exibility is sometimes enough. The
M-of-N representation it employs is not as big a change from the original representation
as the neural net representation which Kbann employs yet it achieves similar results and
arrives at them much more quickly than Kbann (Baes & Mooney, 1993).
A shortcoming of Neither-MofN is that since it acts by making local changes in an
initial theory, it can still become trapped by the structure of the initial theory. An advanced
theory revision system would incorporate Neither-MofN's and Kbann's exibility of
417

Donoho & Rendell

representation and allow knowledge-guided theory restructuring. Examples of
and Neither-MofN's behavior are given in Sections 3.4 and 3.5.

2.4

's

Kbann

Grendel

Cohen (1992) analyzes a class of theory revision systems and draws some insightful conclusions. One is that \generality [in theory interpretation] comes at the expense of power."
He draws this principle from the fact that a system such as Either or Focl treats every
domain theory the same and therefore must treat every domain theory in the most general way. He argues that rather than just applying the most general renement strategy
to every problem, a small set of renement strategies should be available that are narrow
enough to gain leverage yet not so narrow that they only apply to a single problem. Cohen
presents Grendel, a toolbox of translators each of which transforms a domain theory into
an explicit bias. Each translator interprets the domain theory in a dierent way, and the
most appropriate interpretation is applied to a given problem.
We apply Cohen's principle to the representation of domain theories. If all domain
theories are translated into the same representation, then the most general, adaptable representation has to be used in order to accommodate the most general case. This comes
at the expense of higher computational costs and possibly lower accuracy due to overt
stemming from unbridled adaptability. The neural net representation into which Kbann
translates domain theories allows 1) a measure of partial match to the domain theory 2) different parts of the domain theory to be weighted dierently 3) conditions to be added to
and dropped from the domain theory. All these options of adaptability are probably not
necessary for most problems and may even be detrimental. These options in Kbann also
require the computationally expensive backpropagation method.
The representation used in Neither-MofN is not as adaptable as Kbann's | it does
not allow individual parts of the domain theory to be weighted dierently. NeitherMofN runs more quickly than Kbann on small problems and probably matches or even
surpasses Kbann's accuracy for many domains | domains for which ne-grained weighting
is unfruitful or even detrimental. A toolbox of theory rerepresentation translators analogous
to Grendel would allow a domain theory to be translated into a representation having the
most appropriate forms of adaptability.

2.5 Outlook and Summary

In summary, we briey reexamine exible representation and exible structure, the two
desirable qualities set forth in Section 1. We consider how the various systems exemplify
some subset of these desirable qualities.
 Kbann and Neither-MofN both interpreted a theory more exibly than its original
representation allowed and revised the theory in this more adaptable representation.
A nal, rened theory often has many exceptions to the rule; it may tolerate partial
matches and missing pieces of evidence; it may weight some evidence more heavily
than other evidence. Kbann's and Neither-MofN's new representation may not
be the most concise, appropriate representation for the initial theory, but the new
representation allows concise expression of an otherwise cumbersome nal theory.
These are cases of the principle of exible representation.
418

Rerepresenting and Restructuring Domain Theories

 Standard induction programs have been quite successful at building concise theories
with high predictive accuracy when the target concept can be concisely expressed using
the original set of features. When it can't, constructive induction is a means of creating
new features such that the target concept can be concisely expressed. Miro uses
constructive induction to take advantage of the strengths of both a domain theory and
standard induction. Knowledge from the theory guides the construction of appropriate
new features, and standard induction structures these into a concise description of
the concept. Thus Miro-like construction coupled with standard induction provides
a ready and powerful means of exibly restructuring the knowledge contained in an
initial domain theory. This is a case of the principle of exible structure.

In the following section we introduce the DNA Promoter Recognition domain in order
to illustrate tangibly how some of the systems discussed above integrate knowledge and
induction.

3. Demonstrations of Related Work
This section introduces the Promoter Recognition domain (Harley, Reynolds, & Noordewier,
1990) and briey illustrates how a Miro-like system, Either, Kbann, and NeitherMofN behave in this domain. We implemented a Miro-like system for the promoter domain; versions of Either and Neither-MofN were available from Ray Mooney's group;
Kbann's behavior is described by analyzing (Towell & Shavlik, 1994). We chose the promoter domain because it is a non-trivial, real-world problem which a number of theory
revision researchers have used to test their work (Ourston & Mooney, 1990; Thompson
et al., 1991; Wogulis, 1991; Cohen, 1992; Pazzani & Kibler, 1992; Baes & Mooney, 1993;
Towell & Shavlik, 1994). The promoter domain is one of three domains in which we evaluate
our work, theory-guided constructive induction, in Section 5.

3.1 The Promoter Recognition Domain

A promoter sequence is a region of DNA that marks the beginning of a gene. Each example in the promoter recognition domain is a region of DNA classied either as a promoter
or a non-promoter. As illustrated in Figure 3, examples consist of 57 features representing a sequence of 57 DNA nucleotides. Each feature can take on the values A,G,C, or T
representing adenine, guanine, cytosine, and thymine at the corresponding DNA position.
The features are labeled according to their position from p-50 to p+7 (there is no zero
position). The notation \p-N " denotes the nucleotide that is N positions upstream from
the beginning of the gene. The goal is to predict whether a sequence is a promoter from its
nucleotides. A total of 106 examples are available: 53 promoters and 53 non-promoters.
The promoter recognition problem comes with the initial domain theory shown in Figure 4 (quoted almost verbatim from Towell and Shavlik's entry in the UCI Machine Learning
Repository). The theory states that promoter sequences must have two regions that make
contact with a protein and must also have an acceptable conformation pattern. There are
four possibilities for the contact region at minus 35 (35 nucleotides upstream from the beginning of the gene). A match of any of these four possibilities will satisfy the minus 35
contact condition, thus they are joined by disjunction. Similarly, there are four possibilities
419

Donoho & Rendell

DNA Sequence

p-50

p+7

C G A C T T
Figure 3: An instance in the promoter domain consists of a sequence of 57 nucleotides
labeled from p-50 to p+7. Each nucleotide can take on the values A,G,C, or T
representing adenine, guanine, cytosine, and thymine.
for the contact region at minus 10 and four acceptable conformation patterns. Figure 5
gives a more pictorial presentation of portions of the theory. Of the 106 examples in the
dataset, none matched the domain theory exactly, yielding an accuracy of 50%.

3.2

Miro

in the Promoter Domain

A Miro-like system in the promoter domain would use the rules in Figure 4 to construct new high-level features for each DNA segment. Figure 6 shows an example of this. A
DNA segment is shown from position p-38 through position p-30. The minus 35 rules from
the theory are also shown, and four new features (feat minus35 A through feat minus35 D)
have been constructed for that DNA segment, one for each minus 35 rule. The new features feat minus35 A and feat minus35 D both have the value 1 because the DNA fragment
matches the rst and fourth minus 35 rules. Likewise, feat minus35 B and feat minus35 C
both have the value 0 because the DNA fragment does not match the second and third
rules. Furthermore, since the four minus 35 rules are joined by disjunction, a new feature,
feat minus35 all, is created for the group that would have the value 1 because at least one
of the minus 35 rules matches.
New features would similarly be created for the minus 10 rules and the conformation
rules, and a standard induction algorithm could then be applied. We implemented a Mirolike system; Figure 7 gives an example theory created by it. (Drastal's original Miro used
the candidate elimination algorithm (Mitchell, 1977) as its underlying induction algorithm.
We used C4.5 (Quinlan, 1993).) As opposed to theory revision systems that incrementally
modify the domain theory, Miro has broken the theory down into its components and has
fashioned these components into a new theory using a standard induction program. Thus
Miro has exhibited the exible structure principle for this domain { it was not restricted
in any way by the structure of the initial theory. Rather, Miro exploited the strengths of
standard induction to concisely characterize the training examples using the new features.
420

Rerepresenting and Restructuring Domain Theories

Promoters have a region where a protein (RNA polymerase) must make contact and
the helical DNA sequence must have a valid conformation so that the two pieces
of the contact region spatially align. Prolog notation is used.
promoter :- contact, conformation.
There are two regions "upstream" from the beginning of the gene at which the
RNA polymerase makes contact.
contact

:- minus_35, minus_10.

The following rules describe the compositions of possible contact regions.
minus_35
minus_35
minus_35
minus_35

:- p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.
:p-36=t, p-35=t, p-34=g,
p-32=c, p-31=a.
:p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=a.
:p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.

minus_10
minus_10
minus_10
minus_10

:- p-14=t, p-13=a, p-12=t, p-11=a, p-10=a, p-9=t.
:p-13=t, p-12=a,
p-10=a,
p-8=t.
:p-13=t, p-12=a, p-11=t, p-10=a, p-9=a, p-8=t.
:p-12=t, p-11=a,
p-7=t.

The following rules describe sequences that produce acceptable conformations.
conformation :- p-47=c,
p-18=t,
p-1=c.
conformation :- p-45=a,
conformation :- p-49=a,
conformation :- p-45=a,
p-15=t,

p-46=a, p-45=a, p-43=t, p-42=t, p-40=a, p-39=c, p-22=g,
p-16=c, p-8=g, p-7=c, p-6=g, p-5=c, p-4=c, p-2=c,
p-44=a, p-41=a.
p-44=t, p-27=t, p-22=a, p-18=t, p-16=t, p-15=g, p-1=a.
p-41=a, p-28=t, p-27=t, p-23=t, p-21=a, p-20=a, p-17=t,
p-4=t.

Figure 4: The initial domain theory for recognizing promoters (from Towell and Shavlik).
A weakness Miro displays in this example is that it allows no exibility of representation
of the theory. The representation of the features constructed by Miro is basically the same
all-or-none representation of the initial theory; either a DNA segment matched a rule, or it
did not.

3.3

Either

in the Promoter Domain

An Either-like system renes the initial promoter theory by dropping and adding
conditions and rules. We simulated Either by turning o the M-of-N option in Neither
and ran it in the promoter domain. Figure 8 shows the rened theory produced using a
randomly selected training set of size 80. Because the initial promoter domain theory does
not lend itself to revision through small, local changes, Either has only limited success.
421

Donoho & Rendell

DNA Sequence

p-50

p+7

Contact at minus_35

Contact at minus_10

-37 -36 -35 -34 -33 -32 -31

C T

T G A C

*

-14 -13 -12 -11 -10 -9

-8 -7

T A A T

* *

T

A

OR

OR

* T T G * C A

*

T A

OR

T *

OR

* T T G A C A

*

T A

OR

* T T G A C

* A *
T A A

T *

OR

*

*

*

T A *

*

* T

Figure 5: The contact portion of the theory. There are four possibilities for both the
minus 35 and minus 10 portions of the theory. A \*" matches any nucleotide.
The conformation portion of the theory is too spread out to display pictorially.
In this run, the program exhibited the second behavioral extreme discussed in Section 1;
it entirely removed groups of rules and then tried to build new rules to replace what was
lost. The minus 10 and conformation rules have essentially been removed, and new rules
have been added to the minus 35 group. These new minus 35 rules contain the condition
p-12=t previously found in the minus 10 group and the condition p-44=a previously found
in the conformation group.
Either's behavior in this example is a direct result of its lack of exibility of representation and exibility of structure. It is dicult to transform the minus 10 and conformation
rules into something useful in their initial representation using Either's locally-acting operators. Either handles this by dropping these sets of rules, losing their knowledge, and
attempting to rediscover the lost knowledge empirically. The end result of this loss of
knowledge is lower than optimal accuracy shown later in Section 5.

3.4

Kbann

in the Promoter Domain

Figure 9, modeled after a gure by Towell and Shavlik (1994), shows the setup of a Kbann
network for the promoter theory. Each slot along the bottom represents one nucleotide
in the DNA sequence. Each node at the rst level up from the bottom embodies a single
domain rule, and higher levels encode groups of rules with the nal concept at the top. The
links shown in the gure are the ones that are initially high-weighted. The net is next lled
out to be fully connected with low-weight links. Backpropagation is then applied to rene
the network's weights.
422

Rerepresenting and Restructuring Domain Theories

A DNA segment fragment:
:::

p-38=g, p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=t, p-30=t

:::

The minus 35 group of rules and corresponding constructed features:
minus 35 :- p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.
minus 35 :p-36=t, p-35=t, p-34=g,
p-32=c, p-31=a.
minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=a.
minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.

feat minus35 A = 1
feat minus35 B = 0
feat minus35 C = 0
feat minus35 D = 1

feat minus35 all = (feat minus35 A _ feat minus35 B _ feat minus35 C _ feat minus35 D) = 1

Figure 6: An example of feature construction in a Miro-like system. The constructed
features for the rst and fourth rules in the minus 35 group are true (value = 1)
because the DNA segment matches these rules. The constructed feature for the
entire group, feat minus35 all, is true because the four minus 35 rules are joined
by disjunction.
feat_minus10_all
0

1
promoter

feat_conf_B
0

1

feat_minus35_D
0
non-promoter

promoter
1

promoter

Figure 7: An example theory created by a Miro-like system. A DNA segment is recognized
as a promoter if it matches any of the minus 10 rules, the second conformation
rule, or the fourth minus 35 rule.
The neural net representation is more appropriate for this domain than the propositional
logic representation of the initial theory. It allows for a measurement of partial match by
weighting the links in such a way that a subset of a rule's conditions are enough to surpass a
node's threshold. It also allows for variable weightings of dierent parts of the theory; therefore, more predictive nucleotides can be weighted more heavily, and only slightly predictive
nucleotides can be weighted less heavily. Kbann has only limited exibility of structure.
Because the rened network is the result of a series of incremental modications in the
initial network, a fundamental restructuring of the theory it embodies is unlikely. Kbann
423

Donoho & Rendell

promoter :- contact, conformation.
contact

:- minus_35, minus_10.

minus_35
minus_35
minus_35
minus_35
minus_35
minus_35

::::::-

p-35=t,
p-36=t,
p-36=t,
p-34=g,
p-34=g,
p-35=t,

p-34=g.
p-33=a, p-32=c.
p-32=c, p-50=c.
p-12=t.
p-44=a.
p-47=g.

minus_10 :- true.
conformation :- true.

Figure 8: A revised theory produce by Either.
promoter

contact

conformation
minus_35

p-50

minus_10

DNA Sequence

p+7

Figure 9: The setup of a Kbann network for the promoter theory.
is limited to nding the best network with the same fundamental structure imposed on it
by the initial theory.
One of Kbann's advantages is that it uses a standard learning algorithm as its foundation. Backpropagation has been widely used and consequently improved by previous
researchers. Theory renement tools that are built from the ground up or use a standard
tool only tangentially suer from having to invent their own methods of handling standard
problems such as overt, noisy data, etc. A wealth of neural net experience and resources
is available to the Kbann user; as neural net technology advances, Kbann technology will
passively advance with it.
424

Rerepresenting and Restructuring Domain Theories

3.5

Neither-MofN

in the Promoter Domain

renes the initial promoter theory not only by dropping and adding conditions and rules but also by allowing conjunctive rules to be true if only a subset of their
conditions are true. We ran Neither-MofN with a randomly selected training set of size
80, and Figure 10 shows a rened promoter theory produced. The theory expressed here
with 9 M-of-N rules would require 30 rules using propositional logic, the initial theory's
representation. More importantly, it is unclear how any system using the initial representation would reach the 30-rule theory from the initial theory. Thus the M-of-N representation
adopted not only allows for the concise expression of the nal theory but also facilitates the
renement process.
Neither-MofN

promoter :- 2 of ( contact, conformation ).
contact

:- 2 of ( minus_35, minus_10 ).

minus_35 :- 2 of ( p-36=t, p-35=t, p-34=g,
p-32=c, p-31=a ).
minus_35 :- 5 of ( p-36=t, p-35=t, p-34=g, p-33=a, p-32=c
).
minus_10
minus_10
minus_10
minus_10

::::-

2
2
6
2

of
of
of
of

(
(
p-13=t,
( p-14=t, p-13=a,
(
p-13=t,

p-12=t, p-11=a,
p-12=a,
p-10=a,
p-12=t, p-11=a, p-10=a, p-9=t
p-12=a,
p-10=a,

p-7=t
p-8=t

).
).
).
p-34=g ).

conformation :- true.

Figure 10: A revised theory produced by Neither-MofN.
Neither-MofN displays exibility of representation by allowing an M-of-N interpretation of the original propositional logic, but it does not allow for as ne-grained renement
as Kbann. Both allow for a measure of partial match, but Kbann could weight more
predictive features more heavily. For example, in the minus 35 rules, perhaps p-36=t is
more predictive of a DNA segment being a promoter than p-34=g and therefore should be
weighted more heavily. Neither-MofN simply counts the number of true conditions in a
rule; therefore, every condition is weighted equally. Kbann's ne-grained weighting may be
needed in some domains and not in others. It may actually be detrimental in some domains.
An advanced theory revision system should oer a range of representations.
Like Kbann, Neither-MofN has only limited exibility of structure. The rened
theory is reached through a series of small, incremental modications in the initial theory
precluding a fundamental restructuring. Neither-MofN is therefore limited to nding the
best theory with the same fundamental structure as the initial theory.

4. Theory-Guided Constructive Induction

In the rst half of this section we present guidelines for theory-guided constructive induction
that summarize the work discussed in Sections 2 and 3. The remainder of the section
425

Donoho & Rendell

presents an algorithm that instantiates these guidelines. We evaluate the algorithm in
Section 5.

4.1 Guidelines

The following guidelines are a synthesis of the strengths of the previously discussed related
work.
 As in Miro, new features should be constructed using components of the domain
theory. These new features are combinations of existing features, and a nal theory is
created by applying a standard induction algorithm to the training examples described
using the new features. This allows knowledge to be gleaned from the initial theory
without forcing the nal theory to conform to the initial theory's backbone structure.
It takes full advantage of the domain theory by building high-level features from the
original low-level features. It also takes advantage of a strength of standard induction
| building concise theories having high predictive accuracy when the target concept
can be concisely expressed using the given features.
 As in Either, the constructed features should be modiable by various operators
that act locally, such as adding or dropping conjuncts from a constructed feature.
 As in Kbann and Neither-MofN, the representation of the constructed features
need not be the exact representation in which the initial theory is given. For example,
the initial theory may be given as a set of rules written in propositional logic. A
new feature can be constructed for each rule, but it need not be a boolean feature
telling whether all the conditions are met; for example it may be a count of how
many conditions of that rule are met. This allows the nal theory to be formed and
expressed in a representation that is more suitable than the representation of the
initial theory.
 Like Grendel, a complete system should oer a library of interpreters allowing the
domain theory to be translated into a range of representations with diering adaptability. One interpreter might emulate Miro strictly translating a domain theory
into boolean constructed features. Another interpreter might construct features that
count the number of satised conditions of the corresponding component of the domain theory thus providing a measure of partial match. Still another interpreter
might construct features that are weighted sums of the satised conditions. The
weights could be rened empirically by examining a set of training examples. Thus
the most appropriate amount of expressive power can be applied to a given problem
without incurring unnecessary expense.

4.2 A Specic Interpreter

This section describes an algorithm which is a limited instantiation of the guidelines just
described. The algorithm is intended as a demonstration of the distillation and synthesis
of the principles embodied in previous landmark systems. It contains a main module,
Tgci described in Figure 12, and a specic interpreter, Tgci1 described in Figure 11.
The main module Tgci redescribes the training and testing examples by calling Tgci1
426

Rerepresenting and Restructuring Domain Theories

and then applies C4.5 to the redescribed examples (just as Miro applied the candidate
elimination algorithm to examples after redescribing them). Tgci1 can be viewed as a
single interpreter from a potential Grendel-like toolbox. It takes as input a single example
and a domain theory expressed as an AND/OR tree such as the one shown in Figure 13.
It returns a new vector of features for that example that measure the partial match of the
example to the theory. Thus it creates new features from components of the domain theory
as in Miro, but because it measures partial match, it allows exibility in representing
the information contained in the initial theory as in Kbann and Neither-MofN. One
aspect of the guidelines in 4.1 that does not appear in this algorithm is Either's locally
acting operators such as adding and dropping conditions from a portion of the theory.
The following two paragraphs explain in more detail the workings of Tgci1 and Tgci
respectively.
Given: An example E and a domain theory with root node R. The domain
theory is an AND/OR/NOT tree in which the leaves are conditions which can
be tested to be true or false.
Return: A pair P = (F; F ) where F is the top feature measuring the partial
match of E to the whole domain theory, and F is a vector of new features measuring the partial match of E to various parts and subparts of the domain theory.

1. If R is a directly testable condition, return P=(1,<>) if R is true for E
and P=(-1,<>) if R is false for E .
2. n = the number of children of R
3. For each child Rj of R, call Tgci1(Rj ,E ) and store the respective results
in Pj = (Fj ; Fj ).
4. If the major operator of R is OR, Fnew = MAX (Fj ).
Return P = (Fnew ; concatenate(<Fnew >; F1; F2; :::; Fn)).
P
5. If the major operator of R is AND, Fnew = ( nj=1 Fj )=n.
Return P = (Fnew ; concatenate(<Fnew >; F1; F2; :::; Fn)).
6. If the major operator of R is NOT, Fnew = ,1  F1 .
Return P = (Fnew ; F1).
Figure 11: The Tgci1 algorithm
The Tgci1 algorithm, given in Figure 11, is recursive. Its inputs are an example E and
a domain theory with root node R. It ultimately returns a redescription of E in the form
of a vector of new features F . It also returns a value F called the top feature which is used
in intermediate calculations described below. The base case occurs if the domain theory is
a single leaf node (i.e., R is a simple condition). In this case (Line 1), Tgci1 returns the
top feature 1 if the condition is true and -1 if the condition is false. No new features are
returned in the base case because they would simply duplicate the existing features. If the
427

Donoho & Rendell

domain theory is not a single leaf node, Tgci1 recursively calls itself on each of R's children
(Line 3). When a child of R, Rj , is processed, it returns a vector of new features Fj (which
measures the partial match of the example to the j th child of R and its various subparts).
It also returns the top feature Fj which is included in Fj but is marked as special because it
measures the partial match of the example to the whole of the j th child of R. If there are n
children, the result of Line 3 is n vectors of new features, F1 to Fn , and n top features, F1
to Fn . If the operator at node R is OR (Line 4), then Fnew , the new feature created for that
node, is the maximum of Fj . Thus Fnew measures how closely the best of R's children come
to having its conditions met by the example. The vector of new features returned in this
case is a concatenation of Fnew and all the new features from R's children. If the operator
at node R is AND (Line 5), then Fnew is the average of Fj . Thus Fnew measures how closely
all of R's children as a group come to having their conditions met by the example. The
vector of new features returned in this case is again a concatenation of Fnew and all the new
features from R's children. If the operator at node R is NOT (Line 6), R should only have
one child, and Fnew is F1 negated. Thus Fnew measures the extent to which the conditions
of R's child are not met by the example.
Given: A set of training examples Etrain , a set of testing examples Etest , and a
domain theory with root node R.
Return: Learned concept and accuracy on testing examples.
1. For each example Ei 2 Etrain , call Tgci1(R,Ei) which returns Pi =
(Fi ; Fi). Etrain,new = fFig.
2. For each example Ei 2 Etest, call Tgci1(R,Ei). which returns Pi =
(Fi ; Fi). Etest,new = fFi g.
3. Call C4.5 with training examples Etrain,new and testing examples
Etest,new . Return decision tree and accuracy on Etest,new .
Figure 12: The Tgci algorithm
If Tgci1 is called twice with two dierent examples but with the same domain theory,
the two vectors of new features will be the same size. Furthermore, corresponding features
measure the match of corresponding parts of the domain theory. The Tgci main module
in Figure 12 takes advantage of this by creating redescribed example sets from the input
example sets. Line 1 redescribes each example in the training set producing a new training
set. Line 2 does the same for the testing set. Line 3 runs the standard induction program
C4.5 on these redescribed example sets. The returned decision tree can be easily interpreted
by examining which new features were used and what part of the domain theory they
correspond to.

4.3

Tgci1

Examples

As an example of how the Tgci1 interpreter works, consider the toy theory shown in
Figure 13. Tgci1 redescribes the input example by constructing a new feature for each node
428

Rerepresenting and Restructuring Domain Theories

in the input theory. Consider the situation where the input example matches conditions A,
B, and D but not C and E. When Tgci1 evaluates the children of Node 6, it gets the values
F1 = 1, F2 = 1, F3 = ,1, F4 = 1, and F5 = ,1. Since the operator at Node 6 is AND, Fnew
is the average of the values received from the children, 0.20 ((1 + 1 + (,1) + 1 + (,1))=5 =
0:20). Likewise, if condition G matchs but not F and H, Fnew for Node 5 will have the value
0.33 (,1  ((1 + (,1) + (,1))=3)) because two of three matching conditions at Node 7 give
the value ,0:33, and this is negated by the NOT at Node 5. Since Node 2 is a disjunction,
its new feature measures the best partial match of its two children and has the value 0.33
(MAX(0.20,0.33)), and so on.
1
3

NOT
2
4
5

NOT
6

A B C D E

7

F G H

8

I J K

9

L M N O

Figure 13: An example theory in the form of an AND/OR tree that might be used by the
interpreter to generate constructed features.
Figure 14 shows how Tgci1 redescribes a particular DNA segment using the minus 35
rules of the promoter theory. A partial DNA segment is shown along with the four minus 35
rules and the new feature constructed for each rule (We have given the new features names
here to simplify our illustration). For the rst rule, four of the six nucleotides match; therefore, for that DNA segment feat minus35 A has the value 0.33 ((1+1+1+1+(,1)+(,1))=6).
For the second rule, four of the ve nucleotides match; therefore, feat minus35 B has
the value 0.60. Because these and the other two minus 35 rules are joined by disjunction in the original domain theory, feat minus35 all, the new feature constructed for this
group, takes the maximum value of its four children; therefore, feat minus35 all has the
value 0.60 because feat minus35 B has the value 0.60, the highest in the group. Intuitively, feat minus35 all represents the best partial match of this grouping | the extent
to which the disjunction is partially satised. The results of running Tgci1 on each DNA
sequence is a set of redescribed training examples. Each redescribed example has a value for
feat minus35 A through feat minus35 D, feat minus35 all, and all other nodes in the promoter domain theory. The training set is essentially redescribed using a new feature vector
derived from information contained in the domain theory. In this form, any o-the-shelf
induction program can be applied to the new example set.
Anomalous situations can be created in which Tgci1 gives a \good score" to a seemingly
bad example and a bad score to a good example. Situations can also be created where
logically equivalent theories give dierent scores for a single example. These occur because
429

Donoho & Rendell

A DNA segment fragment:
:::

p-38=g, p-37=c, p-36=t, p-35=t, p-34=g, p-33=c, p-32=a, p-31=a, p-30=t

:::

The minus 35 group of rules and corresponding constructed features:
minus 35 :- p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.
minus 35 :p-36=t, p-35=t, p-34=g,
p-32=c, p-31=a.
minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=a.
minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.

feat minus35 A = 0.33
feat minus35 B = 0.60
feat minus35 C = 0.33
feat minus35 D = 0.20

feat minus35 all = max(feat minus35 A, feat minus35 B, feat minus35 C, feat minus35 D) = 0.60

Figure 14: An example of how Tgci1 generates constructed features from a portion of the
promoter domain theory and a DNA segment. Four of the conditions in the rst
minus 35 rule match the DNA segment; therefore, the constructed feature for
that rule has the value 0.33 ((1 + 1 + 1 + 1 + (,1) + (,1))=6). Feat minus35 all,
the new feature for the entire minus 35 group takes the maximum value of its
children thus embodying the best partial match of the group.
is biased to favor situations where more matched conditions of an AND is desirable,
but more matched conditions of an OR is not necessarily better. Eliminating these anomalies
would remove this bias.
Tgci1

5. Experiments and Analysis
This section presents the results of applying theory-guided constructive induction to three
domains: the promoter domain (Harley et al., 1990), the primate splice-junction domain (Noordewier, Shavlik, & Towell, 1992), and the gene identication domain (Craven & Shavlik,
1995). In each case the Tgci1 interpreter was applied to the domain's theory and examples
in order to redescribe the examples using new features. Then C4.5 (Quinlan, 1993) was
applied to the redescribed examples.

5.1 The Promoter Domain
Figure 15 shows a learning curve for theory-guided constructive induction in the promoter
domain accompanied by curves for Either, LabyrinthK , Kbann, and Neither-MofN.
Following the methodology described by Towell and Shavlik [1994], the set of 106 examples
was randomly divided into a training set of size 80 and a testing set of size 26. A learning
curve was created by training on subsets of the training set of size 8, 16, 24, : : : 72, 80,
using the 26 examples for testing. The curves for Either, LabyrinthK , and Kbann were
taken from Ourston and Mooney (1990), Thompson, Langley, and Iba (1991), and Towell
430

Rerepresenting and Restructuring Domain Theories

and Shavlik (1994) respectively and were obtained by a similar methodology1 . The curve
forTgci is the average of 50 independent random data partitions and is given along with 95%
condence ranges. The Neither-MofN program was obtained from Ray Mooney's group
and was used in generating the Neither-MofN curve using the same 50 data partitions
as were used for Tgci2.
42.5
40

EITHER
Labyrinth-k
NEITHER-MofN
KBANN
TGCI
95% confidence of NEITHER-MofN
95% confidence of TGCI

37.5
35
32.5
30
27.5
% Error

25
22.5
20
17.5
15
12.5
10
7.5
5
2.5
0
0

5

10 15 20 25 30 35 40 45 50 55 60 65 70 75 80
Size of Training Sample

Figure 15: Learning curves for theory-guided constructive induction and other systems in
the promoter domain.
Tgci showed improvement over Either and LabyrinthK for all portions of the curve
and also performed better than Kbann and Neither-MofN for all except the smallest training sets. Condence intervals were not available for Either, LabyrinthK , and

1.

used a testing set of size 25 and did not use the conformation portion of the domain theory. The
testing set in LabyrinthK always consisted of 13 promoters and 13 non-promoters.
2. Baes and Mooney (1993) report a slightly better learning curve for Neither-MofN than we obtained,
but after communication with Paul Baes, we think the dierence is caused by the random selection of
data partitions.
Either

431

Donoho & Rendell

Kbann, but in a pairwise comparison with Neither-MofN, the improvement of Tgci was
signicant at the 0.0005 level of condence for training sets of size 48 and larger.

Structure of Initial Promoter Theory

100% of
first
conf.
rule

100% of
first
minus_35
rule

100% of
second
minus_35
rule

100% of
third
minus_35
rule

100% of
fourth
minus_35
rule

100% of
first
minus_10
rule

100% of
second
minus_10
rule

100% of
third
minus_10
rule

100% of
second
conf.
rule

100% of
third
conf.
rule

100% of
fourth
conf.
rule

100% of
fourth
minus_10
rule

Structure of Revised Promoter Theory

>20% of
second
minus_35
rule

>33% of
first
minus_10
rule

>33% of
second
minus_10
rule

>33% of
third
minus_10
rule

>33% of
fourth
minus_10
rule

Figure 16: The revised theory produced by theory-guided constructive induction has borrowed substructures from the initial theory, but as a whole has not been restricted by its structure.
Figure 16 compares the initial promoter theory with a theory created by Tgci. Reasons
for Tgci's improvement can be inferred from this gure. Tgci has extracted the components of the original theory that are most helpful and restructured them into a more
concise theory. Neither Kbann nor Neither-MofN facilitates this radical extraction and
restructuring. As seen in the leaf nodes, the new theory also measures the partial match
of an example to components of the original theory. This aspect is similar in Kbann and
Neither-MofN.
Part of Tgci's improvement over Kbann and Neither-MofN may be due to a knowledge/bias conict in the latter two systems, a situation where revision biases conict with
knowledge in such a way as to undo some of the knowledge's benets. This can occur
whenever detailed knowledge is opened up to revision using a set of examples. The revision
is not guided only by the examples but rather by the examples as interpreted by a set
432

Rerepresenting and Restructuring Domain Theories

of algorithmic biases. Biases that are useful in the absence of knowledge may undo good
knowledge when improperly applied. Yet these biases developed and perfected for pure induction are often unquestioningly applied to the revision of theories. The biases governing
the dropping of conditions in Neither-MofN and reweighting conditions in Kbann may
be neutralizing the promoter theory's potential. We speculate this because we conducted
some experiments that allowed bias-guided dropping and adding of conditions within Tgci.
We found that these techniques actually reduced accuracy in this domain.
45
42.5
40
37.5
c4.5
backpropagation
KBANN
TGCI
95% confidence of TGCI
domain theory

35
32.5
% Error

30
27.5
25
22.5
20
17.5
15
12.5
10
7.5
0

20

40

60

80

100

120

140

160

180

200

Size of Training Sample

Figure 17: Learning curves for Tgci and other systems in the primate splice-junction domain.

5.2 The Primate Splice-junction Domain

The primate splice-junction domain (Noordewier et al., 1992) involves analyzing a DNA
sequence and identifying boundaries between introns and exons. Exons are the parts of a
DNA sequence kept after splicing; introns are spliced out. The task then involves placing a
433

Donoho & Rendell

given boundary into one of three classes: an intron/exon boundary, an exon/intron boundary, or neither. An imperfect domain theory is available which has a 39.0% error rate on
the entire set of available examples.
Figure 17 shows learning curves for C4.5, backpropagation, Kbann, and Tgci in the
primate splice-junction domain. The results for Kbann and backpropagation were taken
from Towell and Shavlik (1994). The curves for plain C4.5 and the Tgci algorithm were
created by training on sets of size 10,20,30,...90,100,120,...200 and testing on a set of size
800. The curves for C4.5 and Tgci are the average of 40 independent data partitions.
No comparison was made with Neither-MofN because the implementation we obtained
could handle only two-class concepts. For training sets larger than 200, Kbann, Tgci, and
backpropagation all performed similarly.
The accuracy of Tgci appears slightly worse than that of Kbann but perhaps not signicantly. Kbann's advantage over Tgci is its ability to assign ne-grained weightings
to individual parts of a domain theory. Tgci's advantage over Kbann is its ability to
more easily restructure the information contained in a domain theory. We speculate that
Kbann's capability to assign ne-grained weights outweighted its somewhat rigid structuring of this domain theory. Theory-guided constructive induction has an advantage of
speed over Kbann because C4.5, its underlying learner, runs much more quickly than
backpropagation, Kbann's underlying learning algorithm.

5.3 The Gene Identication Domain
The gene identication domain (Craven & Shavlik, 1995) involves classifying a given DNA
segment as a coding sequence (one that codes a protein) or a non-coding sequence. No
domain theory was available in the gene identication domain; therefore, we created an
articial domain theory using the information that organisms may favor certain nucleotide
triplets over others in gene coding. The domain theory embodies the knowledge that a DNA
segment is likely to be a gene coding segment if its triplets are coding-favoring triplets or if
its triplets are not noncoding-favoring triplets. The decision of which triplets were codingfavoring, which were noncoding-favoring, and which favored neither, was made empirically
by analyzing the makeup of 2500 coding and 2500 noncoding sequences. The specic articial domain theory used is described in Online Appendix 1.
Figure 18 shows learning curves for C4.5 and Tgci in the gene identication domain.
The original domain theory yields 31.5% error. The curves were created by training on
example sets of size 50,200,400,...2000 and testing on a separate example set of size 1000.
The curves are the average of 40 independent data partitions.
Only a partial curve is given for Neither-MofN because it became prohibitively slow
for larger training sets. In the promoter domain where training sets were smaller than 100,
Tgci and Neither-MofN ran at comparable speeds (approximately 10 seconds on a Sun4
workstation). In this domain Tgci ran in approximately 2 minutes for larger training sets.
Neither-MofN took 21 times as long as Tgci on training sets of size 400, 69 times as
long for size 800, and 144 times as long for size 1200. Consequently, Neither-MofN's
curve only extends to 1200 and only represents ve randomly selected data partitions. For
these reasons, a solid comparison of Neither-MofN and Tgci cannot be made from these
curves, but it appears that Tgci's accuracy is slightly better. We speculate that Neither434

Rerepresenting and Restructuring Domain Theories

45
42.5
40

TGCI
95% confidence of TGCI
C4.5
NEITHER-MofN
domain theory

37.5

% Error

35
32.5
30
27.5
25
22.5
20
0

200

400

600

800

1000 1200 1400 1600 1800 2000

Number training examples

Figure 18: Learning curves for Tgci and other systems in the gene identication domain.
's slightly lower accuracy is partially due to the fact that it revises the theory to
correctly classify all the training examples. The result is a theory which likely overts the
training examples. Tgci does not need to explicitly avoid overt because this is handled
by its underlying learner.
MofN

5.4 Summary of Experiments
Our goal in this paper has not been to present a new technique but rather to understand
the behavior of landmark systems, distill their strengths, and synthesize them into a simple
system, Tgci. The evaluation of this algorithm shows that its accuracy roughly matches or
exceeds that of its predecessors. In the promoter domain, Tgci showed sizable improvement
over many published results. In the splice-junction domain, Tgci narrowly falls short of
Kbann's accuracy. In the gene identication domain, Tgci outperforms Neither-MofN.
In all these domains Tgci greatly improves on the original theory alone and C4.5 alone.
435

Donoho & Rendell

is faster than its closest competitors. Tgci runs as much as 100 times faster than
on large datasets. A strict quantitative comparison of the speeds of Tgci
and Kbann was not made because 1) backpropagation is known to be much slower than
decision trees (Mooney, Shavlik, Towell, & Gove, 1989), 2) Kbann uses multiple hidden
layers which makes its training time even longer (Towell & Shavlik, 1994), and 3) Towell
and Shavlik (1994) point out that each run of Kbann must be made multiple times with
dierent initial random weights, whereas a single run of Tgci is sucient.
Overall, our experiments support two claims of this paper: First, the accuracy of Tgci
substantiates our delineation of system strengths in terms of exible theory representation
and exible theory structure, since this characterization is the basis for this algorithm's
design. Second, Tgci's combination of speed and accuracy suggest that unnecessary computational complexity can be avoided in synthesizing the strengths of landmark systems.
In the following section we take a closer look at the strengths of theory-guided constructive
induction.
Tgci

Neither-MofN

6. Discussion of Strengths
Below a number of strengths of theory-guided constructive induction are discussed within
the context of the Tgci algorithm used in our experiments.

6.1 Flexible Representation

As discussed in Section 1, for many domains the representation most appropriate for an
initial theory may not be most appropriate for a rened theory. Because theory-guided constructive induction allows the translation of the initial theory into a dierent representation,
it can accommodate such domains. In the experiments in this paper a representation was
needed which allowed for a measurement of partial match to the domain theory. Tgci1
accomplished this by simply counting the matching features and propagating this information up the theory appropriately. Either and LabyrinthK do not easily aord this
measure of partial match and therefore are more appropriate for problems in which the best
representation of the nal theory is the same as that of the initial theory. Kbann allows
a ner-grained measurement of partial match than both Neither-MofN and our work,
but a price is paid in computational complexity. The theory-guided constructive induction framework allows for a variety of potential tools with varying degrees of granularity of
partial match, although just one tool is used in our experiments.

6.2 Flexible Structure

As discussed in Section 2.5, a strength of existing induction programs is fashioning a concise
and highly predictive description of a concept when the target concept can be concisely
described with the given features. Consequently, the value of a domain theory lies not in its
overall structure. If the feature language is sucient, any induction program can build a
good overall theory structure. Instead, the value of a domain theory lies in the information
it contains about how to redescribe examples using high-level features. These high-level
features facilitate a concise description of the target concept. Systems such as Either and
Neither-MofN that reach a nal theory through a series of modications in the initial
436

Rerepresenting and Restructuring Domain Theories

theory hope to gain something by keeping the theory's overall structure intact. If the initial
theory is suciently close to an accurate theory, this method works, but often clinging to
the structure hinders full exploitation of the domain theory. Theory-guided constructive
induction provides a means of fully exploiting both the information in the domain theory and
the strengths of existing induction programs. Figure 16 in Section 5.1 gives a comparison of
the structure of the initial promoter theory to the structure of a revised theory produced by
theory-guided constructive induction. Substructures have been borrowed, but the revised
theory as a whole has been restructured.

6.3 Use of Standard Induction as an Underlying Learner

Because theory-guided constructive induction uses a standard induction program as its
underlying learner, it does not need to reinvent solutions to overt avoidance, multi-class
concepts, noisy data, etc. Overt avoidance has been widely studied for standard induction,
and many standard techniques exist. Any system which modies a theory to accommodate
a set of training examples must also address the issue of overt to the training examples. In
many theory revision systems existing overt avoidance techniques cannot be easily adapted,
and the problem must be addressed from scratch. Theory-guided constructive induction can
take advantage of the full range of previous work in overt avoidance for standard induction.
When multiple theory parts are available for multi-class concepts, the interpreter is
run on the multiple theory parts, and the resulting new feature sets are combined. The
primate splice-junction domain presented in Section 5.2 has three classes: intron/exon
boundaries, exon/intron boundaries, and neither. Theories are given for both intron/exon
and exon/intron. Both theories are used to create new features, and then all new features
are concatenated together for learning. Interpreters such as Tgci1 also trivially handle
negation in a domain theory.

6.4 Use of Theory Fragments

Theory-guided constructive induction is not limited to using full domain theories. If only
part of a theory is available, this can be used. To demonstrate this, three experiments
were run in which only fragments of the promoter domain theory were used. In the rst
experiment, only the four minus 35 rules were used. Five features were constructed | one
feature for each rule and then an additional feature for the group. Similar experiments were
run for the minus 10 group and the conformation group.
Figure 19 gives learning curves for these three experiments along with curves for the entire theory and for no theory (C4.5 using the original features). Although the conformation
portion of the theory gives no signicant improvement over C4.5, both the minus 35 and
minus 10 portions of the theory give signicant improvements in performance. Thus even
partial theories and theory fragments can be used by theory-guided constructive induction
to yield sizable performance improvements.
The use of theory fragments should be explored as a means of evaluating the contribution
of dierent parts of a theory. In Figure 19, the conformation portion of the theory is shown
to yield no improvement. This could signal a knowledge engineer that the knowledge that
should be conveyed through that portion of the theory is not useful to the learner in its
present form.
437

Donoho & Rendell

45
C4.5
conformation portion of theory
minus_10 portion of theory
minus_35 portion of theory
whole theory

42.5
40
37.5
35
32.5
30

% Error

27.5
25
22.5
20
17.5
15
12.5
10
7.5
5
2.5
0
0

5

10 15 20 25 30 35 40 45 50 55 60 65 70 75 80
Size of Training Sample

Figure 19: Learning curves for theory-guided constructive induction with only fragments of
the promoter domain theory. The minus 35 portion of the theory, the minus 10
portion of the theory, and the conformation portion of the theory were used
separately in feature construction. Curves are also given for the full theory and
for C4.5 alone for comparison.

6.5 Use of Multiple Theories

Theory-guided constructive induction can use multiple competing and even incompatible
domain theories. If multiple theories exist, theory-guided constructive induction provides
a natural means of integrating them in such a way as to extract the best from all theories.
Tgci1 would be called for each input theory producing new features. Next, all the new
features are simply pooled together and the induction program selects from among them
in fashioning the nal theory. This is seen on a very small scale in the promoter domain.
438

Rerepresenting and Restructuring Domain Theories

% Error

In Figure 4 some minus 35 rules subsume other minus 35 rules. According to the entry in
the UCI Database, this is because \the biological evidence is inconclusive with respect to
the correct specicity." This is handled by simply using all four possibilities, and selection
of the most useful knowledge is left to the induction program.
Tgci could also be used to evaluate the contributions of competing theories just as it was
used to evaluate theory fragments above. A knowledge engineer could use this evaluation
to guide his own revision and synthesis of competing theories.
25
22.5
20
17.5
15
12.5
10
7.5
5
2.5
0

TGCI using C4.5
TGCI using LFC
95% confidence of LFC

5

10 15 20 25 30 35 40 45 50 55 60 65 70 75 80
Size of Training Sample

Figure 20: Theory-guided constructive induction with Lfc and C4.5 as the underlying
learning system. Theory-guided constructive induction can use any inductive
learner as its underlying learning component. Therefore, more sophisticated
underlying induction programs can further improve accuracy.

6.6 Easy Adoption of New Techniques

Since theory-guided constructive induction can use any standard induction method as its
underlying learner, as improvements are made in standard induction, theory-guided constructive induction passively improves. To demonstrate this, tests were also run with Lfc
(Ragavan & Rendell, 1993) as the underlying induction program. Lfc is a decision tree
learner that performs example-based constructive induction by looking ahead at combinations of features. Characteristically, Lfc improves accuracy for a moderate number of
examples. Figure 20 shows the resulting learning curve along with the C4.5 Tgci curve.
Both curves are the average of 50 separate runs with the same data partitions used for each
program. In a pairwise comparison the improvement of Lfc over C4.5 was signicant at the
0.025 level of condence for training sets of size 72 and 80. More sophisticated underlying
induction programs can further improve accuracy.
439

Donoho & Rendell

7. Testing the Limits of Tgci
The purpose of this section is to explore the performance of theory-guided constructive
induction on theory revision problems ranging from easy to dicult. In easy problems
the underlying concept embodied in the training and testing examples matches the domain
theory fairly closely; therefore, the examples themselves match the domain theory fairly
closely. In dicult problems the underlying concept embodied in the examples does not
match the domain theory very well so the examples do not either. Although many other
factors determine the diculty of an individual problem, this aspect is an important component and worth exploring. Our experiment in this section is intended to relate ranges of
diculty to the amount of improvement produced by Tgci.
Since a number of factors aect problem diculty we chose that the theory revision
problems for the experiment should all be variations of a single problem. By doing this we
are able to hold all other factors constant and vary the closeness of match to the domain
theory. Because we wanted to avoid totally articial domains, we chose to start with the
promoter domain and create \new" domains by perverting the example set.
These \new" domains were created by perverting the examples in the original promoter
problem to either more closely match the promoter domain theory or less closely match the
promoter domain theory. Only the positive examples were altered. For example, one domain
was created with 30% fewer matches to the domain theory than the original promoter
domain as follows: Each feature value in a given example was examined to see if it matched
part of the theory. If so, with a 30% probability, it was randomly reassigned a new value
from the set of possible values for that feature. The end result is a set of examples with 30%
fewer matches to the domain theory than the original example set3. For our experiment
new domains such as this were created with 10%, 30%, 60%, and 90% fewer matches.
For some features, multiple values may match the theory because dierent disjuncts
of the theory specify dierent values for a single feature. For example, referring back to
Figure 4, feature p-12 matches two of the minus 10 rules if it has the value a and another
two rules if it has the value t. So a single feature might accidentally match one part of a
theory when in fact the example as a whole more closely matches another part of the theory.
For cases such as these, true matches were separated from accidental matches by examining
which part of the theory most clearly matched the example as a whole and expecting a
match from that part of the theory.
New domains that more closely matched the theory were created in a similar manner. For
example, a domain was created with 30% fewer mismatches to the domain theory than the
original promoter domain as follows: Each feature value in a given example was examined
to see if it matched its corresponding part of the theory. If not, with a 30% probability,
it was reassigned a value that matched the theory. The end result is a set of examples in
which 30% of the mismatches with the domain theory are eliminated. For our experiment
new domains such as this were created with 30%, 60%, and 90% fewer mismatches.
Ten dierent example sets were created for each level of closeness to the domain theory:
10%, 30%, 60%, 90% fewer matches, and 30%, 60%, 90% fewer mismatches. In total, forty
example sets were created which matched the original theory less closely than the original
3. More precisely, there would be slightly more matches than 30% fewer matches because some features
would be randomly reassigned back to their original matching value.

440

Rerepresenting and Restructuring Domain Theories

55
50
45
40

% Error

35

C4.5
TGCI

30
25
20
15
10
5
0
-100

-80

-60

-40

-20

0

20

40

60

80

100

Closeness to theory

Figure 21: Seven altered promoter domains were created, three that more closely matched
the theory than the original domains and four that less closely matched. A
100 on the x-axis indicates a domain in which the positive examples match the
domain theory 100%. A negative 100 indicates a domain in which any match
of the positive examples to the domain theory is purely chance. The accuracy
of C4.5 and Tgci are plotted for dierent levels of proximity to the domain
theory.

example set, and thirty example sets were created which matched the original theory more
closely than the original example set. Each of these example sets was tested using a leaveone-out methodology using C4.5 and the Tgci algorithm. The results are summarized in
Figure 21. The x-axis is a measure of theory proximity { closeness of an example set to the
domain theory. \0" on the x-axis indicates no change in the original promoter examples.
\100" on the x-axis means that each positive example exactly matches the domain theory.
\-100" on the x-axis means that any match of a feature value of a positive example to the
441

Donoho & Rendell

domain theory is totally by chance4 . Each datapoint in Figure 21 is the result of averaging
the accuracies of the ten example sets for each level of theory proximity (except for the
point at zero which is the accuracy of the exact original promoter examples).
One notable portion of Figure 21 is the section between 0 and 60 on the x-axis. Domains
in this region have a greater than trivial level of mismatch with the domain theory but not
more than moderate mismatch. This is the region of Tgci's best performance. On these
domains, Tgci achieves high accuracy while a standard learner, C4.5, using the original
feature set gives mediocre performance. A second region to examine is between -60 and 0
on the x-axis where the level of mismatch ranges from moderate to extreme. In this region
Tgci's performance falls o but its improvement over the original feature set remains high
as shown in Figure 22 which plots the improvement of Tgci over C4.5. The nal two
regions to notice are greater than 60 and less than -60 on the x-axis. As the level of
mismatch between theory and examples becomes trivially small (x-axis greater than 60),
C4.5 is able to pick out the theory's patterns leading to high accuracy that approaches that
of Tgci's. As the level of mismatch becomes extreme (x-axis less than -60) the theory gives
little help in problem-solving resulting in similarly poor accuracy for both methods. In
summary, as shown in Figure 22 for variants of the promoter problem there is a wide range
of theory proximity (centered around the real promoter problem) for which theory-guided
constructive induction yields sizable improvement over standard learners.
20
17.5

error difference

% Error

15
12.5
10
7.5
5
2.5
0
-100

-80

-60

-40

-20

0

20

40

60

80

100

Closeness to theory

Figure 22: The dierence in error between C4.5 and Tgci for dierent levels of proximity
of the example set to the domain theory.

4. The scale 0 to -100 on the left half of the graph may not be directly comparable with the scale 0 to 100
on the right half of the graph since there were not a equal number of matches and mismatches in the
original examples.

442

Rerepresenting and Restructuring Domain Theories

8. Conclusion
Our goal in this paper has not been just to present another new system, but rather to
study the two qualities exible representation and exible structure. These capabilities are
intended as a frame of reference for analyzing theory-guided systems. These two principles
provide guidelines for purposeful design. Once we had distilled the essence of systems
such as Miro, Kbann, and Neither-MofN, theory-guided constructive induction was
a natural synthesis of their strengths. Our experiments have demonstrated that even a
simple application of the two principles can eectively integrate theory knowledge with
training examples. Yet there is much room for improvement; the two principles could be
quantied and made more precise, and the implementations that proceed from them should
be explored and rened.
Quantifying representational exibility is one step. Section 4 gave three degrees of
exibility: one measured the exact match to a theory, one counted the number of matching
conditions, and one allowed for a weighted sum of the matching conditions. The amount of
exibility should be quantied, and ner-grained degrees of exibility should be explored.
The accuracy in assorted domains should be evaluated as a function of representational
exibility.
Finer-grained structural exibility would be advantageous. We have presented systems
that make small, incremental modications in a theory as lacking structural exibility. Yet
theory-guided constructive induction falls at the other extreme, perhaps allowing excessive
structural exibility. Fortunately, existing induction tools are capable of fashioning simple
yet highly predictive theory structures when the problem features are suitably high-level.
Nevertheless, approaches should be explored that take advantage of the structure of the
initial theory without being unduly restricted by it.
The strength discussed in Section 6.5 should be given further attention. Although the
promoter domain gives a very small example of synthesizing competing theories, this should
be explored in a domain in which entire competing, inconsistent theories are available such as
synthesizing the knowledge given by multiple experts. The point was made in Section 6.4
that Tgci can use theory fragments to evaluate the contribution of dierent parts of a
theory. This should also be explored further.
In an exploration of bias in standard induction, Utgo (1986) refers to biases as ranging
from weak to strong and from incorrect to correct. A strong bias restricts the concepts that
can be represented more than a weak bias thus providing more guidance in learning. But as
a bias becomes stronger, it may also become incorrect by ruling out useful concept descriptions. A similar situation arises in theory revision | a theory representation language that
is inappropriately rigid may impose a strong, incorrect bias on revision. A language that
allows adaptability along too many dimensions may provide too weak a bias. A Grendellike toolbox would allow a theory to be translated into a range of representations with
varying dimensions of adaptability. Utgo advocates starting with a strong, possibly incorrect bias and shifting to an appropriately weak and correct bias. Similarly, a theory could
be translated into successively more adaptable representations until an appropriate bias is
found. We have implemented only a single tool; many open problems remain along this line
of research.
443

Donoho & Rendell

The converse relationship of theory revision and constructive induction warrants further
examination | theory revision uses data to improve a theory; constructive induction can
use theory to improve data to facilitate learning. Since the long-term goal of machine
learning is to use data, inference, and theory to improve any and all of them, we believe
that a consideration of these related methods can be benecial, particularly because each
research area has some strengths that the other lacks.
An analysis of landmark theory revision and theory-guided learning systems has led
to the two principles exible representation and exible structure. Because theory-guided
constructive induction was based upon these high-level principles, it is simple yet achieves
good accuracy. These principles provide guidelines for future work, yet as discussed above,
the principles themselves are imprecise and call for further exploration.

Acknowledgements
We would like to thank Geo Towell, Kevin Thompson, Ray Mooney, and Je Mahoney for
their assistance in getting the datapoints for Kbann, LabyrinthK , and Either. We would
also like to thank Paul Baes for making the Neither program available and for advice on
setting the program's parameters. We thank the anonymous reviewers for their constructive
criticism of an earlier draft of this paper. We gratefully acknowledge the support of this
work by a DoD Graduate Fellowship and NSF grant IRI-92-04473.

References
Baes, P., & Mooney, R. (1993). Symbolic revision of theories with M-of-N rules. In
Proceedings of the 1993 IJCAI.
Bloedorn, E., Michalski, R., & Wnek, J. (1993). Multistrategy constructive induction:
AQ17-MCI. In Proceeding of the second international workshop on multistrategy learning.
Clark, P., & Matwin, S. (1993). Using qualitative models to guide inductive learning. In
Proceedings of the 1993 International Conference on Machine Learning.
Cohen, W. (1992). Compiling prior knowledge into an explicit bias. In Proceedings of the
1992 International Conference on Machine Learning.
Craven, M. W., & Shavlik, J. W. (1995). Investigating the value of a good input representation. Computational Learning Theory and Natural Learning Systems, 3. Forthcoming.
Drastal, G., & Raatz, S. (1989). Empirical results on learning in an abstraction space. In
Proceedings of the 1989 IJCAI.
Dzerisko, S., & Lavrac, N. (1991). Learning relations from noisy examples: An empirical
comparison of LINUS and FOIL. In Proceedings of the 1991 International Conference
on Machine Learning.
444

Rerepresenting and Restructuring Domain Theories

Feldman, R., Serge, A., & Koppel, M. (1991). Incremental renement of approximate
domain theories. In Proceedings of the 1991 International Conference on Machine
Learning.
Flann, N., & Dietterich, T. (1989). A study of explanation-based methods for inductive
learning. Machine Learning, 4, 187{226.
Fu, L. M., & Buchanan, B. G. (1985). Learning intermediate concepts in constructing a
hierarchical knowledge base. In Proceedings of the 1985 IJCAI.
Harley, C., Reynolds, R., & Noordewier, M. (1990). Creators of original promoter dataset.
Hirsh, H., & Noordewier, M. (1994). Using background knowledge to improve inductive
learning of DNA sequences. In Tenth IEEE Conference on AI for Applications San
Antonio, TX.
Matheus, C. J., & Rendell, L. A. (1989). Constructive induction on decision trees. In
Proceedings of the 1989 IJCAI.
Michalski, R. S. (1983). A theory and methodology of inductive learning. Articial Intelligence, 20 (2), 111{161.
Mitchell, T. (1977). Version spaces: A candidate elimination approach to rule learning. In
Proceedings of the 1977 IJCAI.
Mooney, R. J. (1993). Induction over the unexplained: Using overly-general domain theories
to aid concept learning. Machine Learning, 10 (1), 79{110.
Mooney, R. J., Shavlik, J. W., Towell, G. G., & Gove, A. (1989). An experimental comparison of symbolic and connectionist learning algorithms. In Proceedings of the 1989
IJCAI.
Murphy, P., & Pazzani, M. (1991). ID2-of-3: Constructive induction of M-of-N concepts for
discriminators in decision trees. In Proceedings of the 1991 International Conference
on Machine Learning.
Noordewier, M., Shavlik, J., & Towell, G. (1992). Donors of original primate splice-junction
dataset.
Ourston, D., & Mooney, R. (1990). Changing the rules: A comprehensive approach to theory
renement. In Proceedings of the 1990 National Conference on Articial Intelligence.
Pagallo, G., & Haussler, D. (1990). Boolean feature discovery in empirical learning. Machine
Learning, 5 (1), 71{99.
Pazzani, M., & Kibler, D. (1992). The utility of knowledge in inductive learning. Machine
Learning, 9 (1), 57{94.
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. San Mateo, CA: Morgan
Kaufmann.
445

Donoho & Rendell

Ragavan, H., & Rendell, L. (1993). Lookahead feature construction for learning hard concepts. In Proceedings of the 1993 International Conference on Machine Learning.
Rumelhart, D. E., Hinton, G. E., & McClelland, J. L. (1986). A general framework for
parallel distributed processing. In Rumelhart, D. E., & McClelland, J. L. (Eds.),
Parallel Distributed Processing: Explorations in the Microarchitecture of Cognition,
Volume I. Cambridge, MA: MIT Press.
Schlimmer, J. C. (1987). Learning and representation change. In Kaufmann, M. (Ed.),
Proceedings of the 1987 National Conference on Articial Intelligence.
Thompson, K., Langley, P., & Iba, W. (1991). Using background knowledge in concept
formation. In Proceedings of the 1991 International Conference on Machine Learning.
Towell, G., & Shavlik, J. (1994). Knowledge-based articial neural networks. Articial
Intelligence, 70, 119{165.
Towell, G., Shavlik, J., & Noordeweir, M. (1990). Renement of approximately correct
domain theories by knowledge-based neural networks. In Proceedings of the 1990
National Conference on Articial Intelligence.
Utgo, P. E. (1986). Shift of bias for inductive concept learning. In Michalski, Carbonell,
& Mitchell (Eds.), Machine Learning, Vol. 2, chap. 5, pp. 107{148. San Mateo, CA:
Morgan Kaufmann.
Wogulis, J. (1991). Revising relational domain theories. In Proceedings of the 1991 International Conference on Machine Learning.

446

Journal of Articial Intelligence Research 2 (1994) 1-32

Submitted 4/94; published 8/94

A System for Induction of Oblique Decision Trees
Sreerama K. Murthy
Simon Kasif
Steven Salzberg

Department of Computer Science
Johns Hopkins University, Baltimore, MD 21218 USA

murthy@cs.jhu.edu
kasif@cs.jhu.edu
salzberg@cs.jhu.edu

Abstract

This article describes a new system for induction of oblique decision trees. This system,
OC1, combines deterministic hill-climbing with two forms of randomization to nd a good
oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision
tree methods are tuned especially for domains in which the attributes are numeric, although
they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and articial data, that analyze OC1's ability to
construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benets of randomization for the construction of oblique
decision trees.

1. Introduction
Current data collection technology provides a unique challenge and opportunity for automated machine learning techniques. The advent of major scientic projects such as the
Human Genome Project, the Hubble Space Telescope, and the human brain mapping initiative are generating enormous amounts of data on a daily basis. These streams of data
require automated methods to analyze, lter, and classify them before presenting them in
digested form to a domain scientist. Decision trees are a particularly useful tool in this context because they perform classication by a sequence of simple, easy-to-understand tests
whose semantics is intuitively clear to domain experts. Decision trees have been used for
classication and other tasks since the 1960s (Moret, 1982; Safavin & Landgrebe, 1991). In
the 1980's, Breiman et al.'s book on classication and regression trees (CART) and Quinlan's work on ID3 (Quinlan, 1983, 1986) provided the foundations for what has become a
large body of research on one of the central techniques of experimental machine learning.
Many variants of decision tree (DT) algorithms have been introduced in the last decade.
Much of this work has concentrated on decision trees in which each node checks the value
of a single attribute (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1986, 1993a).
Quinlan initially proposed decision trees for classication in domains with symbolic-valued
attributes (1986), and later extended them to numeric domains (1987). When the attributes
are numeric, the tests have the form xi > k, where xi is one of the attributes of an example
and k is a constant. This class of decision trees may be called axis-parallel, because the tests
at each node are equivalent to axis-parallel hyperplanes in the attribute space. An example
of such a decision tree is given in Figure 1, which shows both a tree and the partitioning it
creates in a 2-D attribute space.

c 1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Figure 1: The left side of the gure shows a simple axis-parallel tree that uses two attributes.
The right side shows the partitioning that this tree creates in the attribute space.
Researchers have also studied decision trees in which the test at a node uses boolean
combinations of attributes (Pagallo, 1990; Pagallo & Haussler, 1990; Sahami, 1993) and
linear combinations of attributes (see Section 2). Dierent methods for measuring the
goodness of decision tree nodes, as well as techniques for pruning a tree to reduce overtting
and increase accuracy have also been explored, and will be discussed in later sections.
In this paper, we examine decision trees that test a linear combination of the attributes
at each internal node. More precisely, let an example take the form X = x1 ; x2; : : :; xd; Cj
where Cj is a class label and the xi 's are real-valued attributes.1 The test at each node will
then have the form:
d
X
ai xi + ad+1 > 0
(1)
i=1

where a1 ; : : :; ad+1 are real-valued coecients. Because these tests are equivalent to hyperplanes at an oblique orientation to the axes, we call this class of decision trees oblique
decision trees. (Trees of this form have also been called \multivariate" (Brodley & Utgo,
1994). We prefer the term \oblique" because \multivariate" includes non-linear combinations of the variables, i.e., curved surfaces. Our trees contain only linear tests.) It is clear
that these are simply a more general form of axis-parallel trees, since by setting ai = 0
for all coecients but one, the test in Eq. 1 becomes the familiar univariate test. Note
that oblique decision trees produce polygonal (polyhedral) partitionings of the attribute
space, while axis-parallel trees produce partitionings in the form of hyper-rectangles that
are parallel to the feature axes.
It should be intuitively clear that when the underlying concept is dened by a polygonal space partitioning, it is preferable to use oblique decision trees for classication. For
example, there exist many domains in which one or two oblique hyperplanes will be the
best model to use for classication. In such domains, axis-parallel methods will have to ap1. The constraint that x1 ; : : : ; xd are real-valued does not necessarily restrict oblique decision trees to
numeric domains. Several researchers have studied the problem of converting symbolic (unordered)
domains to numeric (ordered) domains and vice versa; e.g., (Breiman et al., 1984; Hampson & Volper,
1986; Utgo & Brodley, 1990; Van de Merckt, 1992, 1993). To keep the discussion simple, however, we
will assume that all attributes have numeric values.

2

Figure 2: The left side shows a simple 2-D domain in which two oblique hyperplanes dene
the classes. The right side shows an approximation of the sort that an axis-parallel
decision tree would have to create to model this domain.
proximate the correct model with a staircase-like structure, while an oblique tree-building
method could capture it with a tree that was both smaller and more accurate.2 Figure 2
gives an illustration.
Breiman et al. rst suggested a method for inducing oblique decision trees in 1984. However, there has been very little further research on such trees until relatively recently (Utgo
& Brodley, 1990; Heath, Kasif, & Salzberg, 1993b; Murthy, Kasif, Salzberg, & Beigel, 1993;
Brodley & Utgo, 1994). A comparison of existing approaches is given in more detail in
Section 2. The purpose of this study is to review the strengths and weaknesses of existing
methods, to design a system that combines some of the strengths and overcomes the weaknesses, and to evaluate that system empirically and analytically. The main contributions
and conclusions of our study are as follows:

 We have developed a new, randomized algorithm for inducing oblique decision trees

from examples. This algorithm extends the original 1984 work of Breiman et al.
Randomization helps signicantly in learning many concepts.

 Our algorithm is fully implemented as an oblique decision tree induction system and
is available over the Internet. The code can be retrieved from Online Appendix 1 of
this paper (or by anonymous ftp from ftp://ftp.cs.jhu.edu/pub/oc1/oc1.tar.Z).

 The randomized hill-climbing algorithm used in OC1 is more ecient than other

existing randomized oblique decision tree methods (described below). In fact, the
current implementation of OC1 guarantees a worst-case running time that is only
O(log n) times greater than the worst-case time for inducing axis-parallel trees (i.e.,
O(dn2 log n) vs. O(dn2)).

 The ability to generate oblique trees often produces very small trees compared to
axis-parallel methods. When the underlying problem requires an oblique split, oblique

2. Note that though a given oblique tree may have fewer leaf nodes than an axis-parallel tree|which is what
we mean by \smaller"|the oblique tree may in some cases be larger in terms of information content,
because of the increased complexity of the tests at each node.

3

Murthy, Kasif & Salzberg

trees are also more accurate than axis-parallel trees. Allowing a tree-building system
to use both oblique and axis-parallel splits broadens the range of domains for which
the system should be useful.
The remaining sections of the paper follow this outline: the remainder of this section
briey outlines the general paradigm of decision tree induction, and discusses the complexity issues involved in inducing oblique decision trees. Section 2 briey reviews some
existing techniques for oblique DT induction, outlines some limitations of each approach,
and introduces the OC1 system. Section 3 describes the OC1 system in detail. Section 4
describes experiments that (1) compare the performance of OC1 to that of several other
axis-parallel and oblique decision tree induction methods on a range of real-world datasets
and (2) demonstrate empirically that OC1 signicantly benets from its randomization
methods. In Section 5, we conclude with some discussion of open problems and directions
for further research.

1.1 Top-Down Induction of Decision Trees

Algorithms for inducing decision trees follow an approach described by Quinlan as top-down
induction of decision trees (1986). This can also be called a greedy divide-and-conquer
method. The basic outline is as follows:
1. Begin with a set of examples called the training set, T . If all examples in T belong
to one class, then halt.
2. Consider all tests that divide T into two or more subsets. Score each test according
to how well it splits up the examples.
3. Choose (\greedily") the test that scores the highest.
4. Divide the examples into subsets and run this procedure recursively on each subset.
Quinlan's original model only considered attributes with symbolic values; in that model,
a test at a node splits an attribute into all of its values. Thus a test on an attribute
with three values will have at most three child nodes, one corresponding to each value.
The algorithm considers all possible tests and chooses the one that optimizes a pre-dened
goodness measure. (One could also split symbolic values into two or more subsets of values,
which gives many more choices for how to split the examples.) As we explain next, oblique
decision tree methods cannot consider all tests due to complexity considerations.

1.2 Complexity of Induction of Oblique Decision Trees

One reason for the relatively few papers on the problem of inducing oblique decision trees is
the increased computational complexity of the problem when compared to the axis-parallel
case. There are two important issues that must be addressed. In the context of top-down
decision tree algorithms, we must address the complexity of nding optimal separating
hyperplanes (decision surfaces) for a given node of a decision tree. An optimal hyperplane
will minimize the impurity measure used; e.g., impurity might be measured by the total
number of examples mis-classied. The second issue is the lower bound on the complexity
of nding optimal (e.g., smallest size) trees.
4

Figure 3: For n points in d dimensions
(n  d), there are n  d distinct axis-parallel splits,
, 
while there are 2d  nd distinct d-dimensional oblique splits. This shows all distinct
oblique and axis-parallel splits for two specic points in 2-D.
Let us rst consider the issue of the complexity of selecting an optimal oblique hyperplane for a single node of a tree. In a domain with
n training instances, each described using
, 
d real-valued attributes, there are at most 2d  nd distinct d-dimensional oblique splits; i.e.,
hyperplanes3 that divide the training instances uniquely into two nonoverlapping subsets.
This upper bound derives from the observation that every subset of size d from the n points
can dene a d-dimensional hyperplane, and each such hyperplane can be rotated slightly
in 2d directions to divide the set of d points in all possible ways. Figure 3 illustrates these
upper limits for two points in two dimensions. For axis-parallel splits, there are only n  d
distinct possibilities, and axis-parallel methods such as C4.5 (Quinlan, 1993a) and CART
(Breiman et al., 1984) can exhaustively search for the best split at each node. The problem
of searching for the best oblique split is therefore much more dicult than that of searching
for the best axis-parallel split. In fact, the problem is NP-hard.
More precisely, Heath (1992) proved that the following problem is NP-hard: given a
set of labelled examples, nd the hyperplane that minimizes the number of misclassied
examples both above and below the hyperplane. This result implies that any method
for nding the optimal oblique split is likely to have exponential cost
(assuming P 6= NP ).
, 
Intuitively, the problem is that it is impractical to enumerate all 2d  nd distinct hyperplanes
and choose the best, as is done in axis-parallel decision trees. However, any non-exhaustive
deterministic algorithm for searching through all these hyperplanes is prone to getting stuck
in local minima.
3. Throughout the paper, we use the terms \split" and \hyperplane" interchangeably to refer to the test
at a node of a decision tree. The rst usage is standard (Moret, 1982), and refers to the fact that the
test splits the data into two partitions. The second usage refers to the geometric form of the test.

5

Murthy, Kasif & Salzberg

On the other hand, it is possible to dene impurity measures for which the problem
of nding optimal hyperplanes can be solved in polynomial time. For example, if one
minimizes the sum of distances of mis-classied examples, then the optimal solution can
be found using linear programming methods (if distance is measured along one dimension
only). However, classiers are usually judged by how many points they classify correctly,
regardless of how close to the decision boundary a point may lie. Thus most of the standard
measures for computing impurity base their calculation on the discrete number of examples
of each category on either side of the hyperplane. Section 3.3 discusses several commonly
used impurity measures.
Now let us address the second issue, that of the complexity of building a small tree.
It is easy to show that the problem of inducing the smallest axis-parallel decision tree is
NP-hard. This observation follows directly from the work of Hyal and Rivest (1976). Note
that one can generate the smallest axis-parallel tree that is consistent with the training
set in polynomial time if the number of attributes is a constant. This can be done by
using dynamic programming or branch and bound techniques (see Moret (1982) for several
pointers). But when the tree uses oblique splits, it is not clear, even for a xed number
of attributes, how to generate an optimal (e.g., smallest) decision tree in polynomial time.
This suggests that the complexity of constructing good oblique trees is greater than that
for axis-parallel trees.
It is also easy to see that the problem of constructing an optimal (e.g., smallest) oblique
decision tree is NP-hard. This conclusion follows from the work of Blum and Rivest (1988).
Their result implies that in d dimensions (i.e., with d attributes) the problem of producing
a 3-node oblique decision tree that is consistent with the training set is NP-complete. More
specically, they show that the following decision problem is NP-complete: given a training
set T with n examples and d Boolean attributes, does there exist a 3-node neural network
consistent with T ? From this it is easy to show that the following question is NP-complete:
given a training set T , does there exist a 3-leaf-node oblique decision tree consistent with
T?
As a result of these complexity considerations, we took the pragmatic approach of trying
to generate small trees, but not looking for the smallest tree. The greedy approach used by
OC1 and virtually all other decision tree algorithms implicitly tries to generate small trees.
In addition, it is easy to construct example problems for which the optimal split at a node
will not lead to the best tree; thus our philosophy as embodied in OC1 is to nd locally
good splits, but not to spend excessive computational eort on improving the quality of
these splits.

2. Previous Work on Oblique Decision Tree Induction
Before describing the OC1 algorithm, we will briey discuss some existing oblique DT
induction methods, including CART with linear combinations, Linear Machine Decision
Trees, and Simulated Annealing of Decision Trees. There are also methods that induce
tree-like classiers with linear discriminants at each node, most notably methods using
linear programming (Mangasarian, Setiono, & Wolberg, 1990; Bennett & Mangasarian,
1992, 1994a, 1994b). Though these methods can nd the optimal linear discriminants for
specic goodness measures, the size of the linear program grows very fast with the number
6

Induction of Oblique Decision Trees

To induce a split at node T of the decision tree:
Normalize values for all d attributes.
L=0
While (TRUE)
L = L+1

Let the current split sL be v  c, where v = Pdi=1 ai xi.
For i = 1; : : :; d
For  = -0.25,0,0.25
Search for the  that maximizes the goodness of the split v , (ai + )  c.
Let  , be the settings that result in highest goodness in these 3 searches.
ai = ai ,   , c = c ,     .
Perturb c to maximize the goodness of sL , keeping a1 ; : : :; ad constant.
If jgoodness(sL) - goodness(sL,1)j   exit while loop.
Eliminate irrelevant attributes in fa1; : : :; ad g using backward elimination.
Convert sL to a split on the un-normalized attributes.
Return the better of sL and the best axis-parallel split as the split for T .
Figure 4: The procedure used by CART with linear combinations (CART-LC) at each node
of a decision tree.
of instances and the number of attributes. There is also some less closely related work on
algorithms to train articial neural networks to build decision tree-like classiers (Brent,
1991; Cios & Liu, 1992; Herman & Yeung, 1992).
The rst oblique decision tree algorithm to be proposed was CART with linear combinations (Breiman et al., 1984, chapter 5). This algorithm, referred to henceforth as CART-LC,
is an important basis for OC1. Figure 4 summarizes (using Breiman et al.'s notation) what
the CART-LC algorithm does at each node in the decision tree. The core idea of the CARTLC algorithm is how it nds the value of  that maximizes the goodness of a split. This
idea is also used in OC1, and is explained in detail in Section 3.1.
After describing CART-LC, Breiman et al. point out that there is still much room for
further development of the algorithm. OC1 represents an extension of CART-LC that
includes some signicant additions. It addresses the following limitations of CART-LC:

 CART-LC is fully deterministic. There is no built-in mechanism for escaping local

minima, although such minima may be very common for some domains. Figure 5
shows a simple example for which CART-LC gets stuck.

 CART-LC produces only a single tree for a given data set.
 CART-LC sometimes makes adjustments that increase the impurity of a split. This
feature was probably included to allow it to escape some local minima.

 There is no upper bound on the time spent at any node in the decision tree. It halts
when no perturbation changes the impurity more than , but because impurity may
increase and decrease, the algorithm can spend arbitrarily long time at a node.
7

Murthy, Kasif & Salzberg

1

OC1

2

1

Initial Loc.

1

2

CART-LC

1

2

2

Figure 5: The deterministic perturbation algorithm of CART-LC fails to nd the correct
split for this data, even when it starts from the location of the best axis-parallel
split. OC1 nds the correct split using one random jump.
Another oblique decision tree algorithm, one that uses a very dierent approach from
CART-LC, is the Linear Machine Decision Trees (LMDT) system (Utgo & Brodley, 1991;
Brodley & Utgo, 1992), which is a successor to the Perceptron Tree method (Utgo, 1989;
Utgo & Brodley, 1990). Each internal node in an LMDT tree is a Linear Machine (Nilsson,
1990). The training algorithm presents examples repeatedly at each node until the linear
machine converges. Because convergence cannot be guaranteed, LMDT uses heuristics to
determine when the node has stabilized. To make the training stable even when the set of
training instances is not linearly separable, a \thermal training" method (Frean, 1990) is
used, similar to simulated annealing.
A third system that creates oblique trees is Simulated Annealing of Decision Trees
(SADT) (Heath et al., 1993b) which, like OC1, uses randomization. SADT uses simulated
annealing (Kirkpatrick, Gelatt, & Vecci, 1983) to nd good values for the coecients of
the hyperplane at each node of a tree. SADT rst places a hyperplane in a canonical
location, and then iteratively perturbs all the coecients by small random amounts. Initially, when the temperature parameter is high, SADT accepts almost any perturbation of
the hyperplane, regardless of how it changes the goodness score. However, as the system
\cools down," only changes that improve the goodness of the split are likely to be accepted.
Though SADT's use of randomization allows it to eectively avoid some local minima, it
compromises on eciency. It runs much slower than either CART-LC, LMDT or OC1,
sometimes considering tens of thousands of hyperplanes at a single node before it nishes
annealing.
Our experiments in Section 4.3 include some results showing how all of these methods
perform on three articial domains.
We next describe a way to combine some of the strengths of the methods just mentioned,
while avoiding some of the problems. Our algorithm, OC1, uses deterministic hill climbing
most of the time, ensuring computational eciency. In addition, it uses two kinds of
randomization to avoid local minima. By limiting the number of random choices, the
algorithm is guaranteed to spend only polynomial time at each node in the tree. In addition,
randomization itself has produced several benets: for example, it means that the algorithm
8

Induction of Oblique Decision Trees

To nd a split of a set of examples T :
Find the best axis-parallel split of T . Let I be the impurity of this split.
Repeat R times:
Choose a random hyperplane H .
(For the rst iteration, initialize H to be the best axis-parallel split.)
Step 1: Until the impurity measure does not improve, do:
Perturb each of the coecients of H in sequence.
Step 2: Repeat at most J times:
Choose a random direction and attempt to perturb H in that direction.
If this reduces the impurity of H , go to Step 1.
Let I1 = the impurity of H . If I1 < I , then set I = I1 .
Output the split corresponding to I .
Figure 6: Overview of the OC1 algorithm for a single node of a decision tree.
can produce many dierent trees for the same data set. This oers the possibility of a new
family of classiers: k-decision-tree algorithms, in which an example is classied by the
majority vote of k trees. Heath et al. (1993a) have shown that k-decision tree methods
(which they call k-DT) will consistently outperform single tree methods if classication
accuracy is the main criterion. Finally, our experiments indicate that OC1 eciently nds
small, accurate decision trees for many dierent types of classication problems.

3. Oblique Classier 1 (OC1)

In this section we discuss details of the oblique decision tree induction system OC1. As
part of this description, we include:
 the method for nding coecients of a hyperplane at each tree node,
 methods for computing the impurity or goodness of a hyperplane,
 a tree pruning strategy, and
 methods for coping with missing and irrelevant attributes.
Section 3.1 focuses on the most complicated of these algorithmic details; i.e. the question of
how to nd a hyperplane that splits a given set of instances into two reasonably \pure" nonoverlapping subsets. This randomized perturbation algorithm is the main novel contribution
of OC1. Figure 6 summarizes the basic OC1 algorithm, used at each node of a decision
tree. This gure will be explained further in the following sections.

3.1 Perturbation algorithm

OC1 imposes no restrictions on the orientation of the hyperplanes. However, in order to be
at least as powerful as standard DT methods, it rst nds the best axis-parallel (univariate)
split at a node before looking for an oblique split. OC1 uses an oblique split only when it
improves over the best axis-parallel split.4
4. As pointed out in (Breiman et al., 1984, Chapter 5), it does not make sense to use an oblique split when
the number of examples at a node n is less than or almost equal to the number of features d, because the

9

Murthy, Kasif & Salzberg

The search strategy for the space of possible hyperplanes is dened by the procedure
that perturbs the current hyperplane H to a new location. Because there are an exponential
number of distinct ways to partition the examples with a hyperplane, any procedure that
simply enumerates all of them will be unreasonably costly. The two main alternatives
considered in the past have been simulated annealing, used in the SADT system (Heath
et al., 1993b), and deterministic heuristic search, as in CART-LC (Breiman et al., 1984).
OC1 combines these two ideas, using heuristic search until it nds a local minimum, and
then using a non-deterministic search step to get out of the local minimum. (The nondeterministic step in OC1 is not simulated annealing, however.)
We will start by explaining how we perturb a hyperplane to split the training set T at
a node of the decision tree. Let n be the number of examples in T , d be the number of
attributes (or dimensions) for each example, and k be the number of categories. Then we
can write Tj = (xj 1 ; xj 2; : : :; xjd; Cj ) for the j th example from the training set T , where xji is
the value of attribute i and Cj is the category label. As dened P
in Eq. 1, the equation of the
current hyperplane H at a node of the decision tree is written as di=1
(a x )+ad+1 = 0. If we
Pd i i
substitute a point (an example) Tj into the equation for H , we get i=1 (aixji )+ ad+1 = Vj ,
where the sign of Vj tells us whether the point Tj is above or below the hyperplane H ;
i.e., if Vj > 0, then Tj is above H . If H splits the training set T perfectly, then all points
belonging to the same category will have the same sign for Vj . i.e., sign(Vi) = sign(Vj ) i
category(Ti) = category(Tj ).
OC1 adjusts the coecients of H individually, nding a locally optimal value for one
coecient at a time. This key idea was introduced by Breiman et al. It works as follows.
Treat the coecient am as a variable, and treat all other coecients as constants. Then
Vj can be viewed as a function of am . In particular, the condition that Tj is above H is
equivalent to
Vj > 0
= Uj
am > am xxjm , Vj def
jm

(2)

assuming that xjm > 0, which we ensure by normalization. Using this denition of Uj , the
point Tj is above H if am > Uj , and below otherwise. By plugging all the points from T
into this equation, we will obtain n constraints on the value of am .
The problem then is to nd a value for am that satises as many of these constraints
as possible. (If all the constraints are satised, then we have a perfect split.) This problem
is easy to solve optimally: simply sort all the values Uj , and consider setting am to the
midpoint between each pair of dierent values. This is illustrated in Figure 7. In the gure,
the categories are indicated by font size; the larger Ui 's belong to one category, and the
smaller to another. For each distinct placement of the coecient am , OC1 computes the
impurity of the resulting split; e.g., for the location between U6 and U7 illustrated here, two
examples on the left and one example on the right would be misclassied (see Section 3.3.1
for dierent ways of computing impurity). As the gure illustrates, the problem is simply
to nd the best one-dimensional split of the U s, which requires considering just n , 1 values
for am . The value a0m obtained by solving this one-dimensional problem is then considered
data underts the concept. By default, OC1 uses only axis-parallel splits at tree nodes at which n < 2d.
The user can vary this threshold.

10

Figure 7: Finding the optimal value for a single coecient am . Large U's correspond to
examples in one category and small u's to another.

Perturb(H,m)
For j = 1; : : :; n
Compute Uj (Eq. 2)
Sort U1; : : :; Un in non-decreasing order.
a0m = best univariate split of the sorted Uj s.
H1 = result of substituting a0m for am in H .
If (impurity(H1 ) < impurity(H))
f am = a0m ; Pmove = Pstag g
Else if (impurity(H) = impurity(H1 ))
f am = a0m with probability Pmove
Pmove = Pmove , 0:1  Pstag g
Figure 8: Perturbation algorithm for a single coecient am .

as a replacement for am . Let H1 be the hyperplane obtained by \perturbing" am to a0m . If
H has better (lower) impurity than H1, then H1 is discarded. If H1 has lower impurity, H1
becomes the new location of the hyperplane. If H and H1 have identical impurities, then
H1 replaces H with probability Pstag .5 Figure 8 contains pseudocode for our perturbation
procedure.
Now that we have a method for locally improving a coecient of a hyperplane, we need
to decide which of the d + 1 coecients to pick for perturbation. We experimented with
three dierent methods for choosing which coecient to adjust, namely, sequential, best
rst and random.
Seq: Repeat until none of the coecient values is modied in the For loop:
For i = 1 to d, Perturb(H; i)
Best: Repeat until coecient m remains unmodied:
m = coecient which when perturbed, results in the
maximum improvement of the impurity measure.
Perturb(H; m)
R-50: Repeat a xed number of times (50 in our experiments):
m = random integer between 1 and d + 1
Perturb(H; m)
5. The parameter Pstag , denoting \stagnation probability", is the probability that a hyperplane is perturbed
to a location that does not change the impurity measure. To prevent the impurity from remaining
stagnant for a long time, Pstag decreases by a constant amount each time OC1 makes a \stagnant"
perturbation; thus only a constant number of such perturbations will occur at each node. This constant
can be set by the user. Pstag is reset to 1 every time the global impurity measure is improved.

11

Murthy, Kasif & Salzberg

Our previous experiments (Murthy et al., 1993) indicated that the order of perturbation
of the coecients does not aect the classication accuracy as much as other parameters,
especially the randomization parameters (see below). Since none of these orders was uniformly better than any other, we used sequential (Seq) perturbation for all the experiments
reported in Section 4.

3.2 Randomization

The perturbation algorithm halts when the split reaches a local minimum of the impurity
measure. For OC1's search space, a local minimum occurs when no perturbation of any
single coecient of the current hyperplane will decrease the impurity measure. (Of course,
a local minimum may also be a global minimum.) We have implemented two ways of
attempting to escape local minima: perturbing the hyperplane with a random vector, and
re-starting the perturbation algorithm with a dierent random initial hyperplane.
The technique of perturbing the hyperplane with a random vector works as follows.
When the system reaches a local minimum, it chooses a random vector to add to the
coecients of the current hyperplane. It then computes the optimal amount by which the
hyperplane should beP perturbed along this random direction. To be more precise, when
a hyperplane H = di=1 ai xi + ad+1 cannot be improved by deterministic perturbation,
OC1 repeats the following loop J times (where J is a user-specied parameter, set to 5 by
default).

 Choose a random vector R = (r1; r2; : : :; rd+1).
 Let  be the amount
by which we want to perturb H in the direction R. In other
Pd
words, let H1 = i=1 (ai + ri )xi + (ad+1 + rd+1 ).
 Find the optimal value for .
 If the hyperplane H1 thus obtained decreases the overall impurity, replace H with H1,
exit this loop and begin the deterministic perturbation algorithm for the individual
coecients.

Note that we can treat  as the only variable in the equation for H1 . Therefore each of the
n examples in T , if plugged into the equation for H1, imposes a constraint on the value of
. OC1 therefore can use its coecient perturbation method (see Section 3.1) to compute
the best value of . If J random jumps fail to improve the impurity, OC1 halts and uses
H as the split for the current tree node.
An intuitive way of understanding this random jump is to look atPthe dual space in which
the algorithm is actually searching. Note that the equation H = di=1 ai xi + ad+1 denes
a space in which the axes are the coecients ai rather than the attributes xi . Every point
in this space denes a distinct hyperplane in the original formulation. The deterministic
algorithm used in OC1 picks a hyperplane and then adjusts coecients one at a time. Thus
in the dual space, OC1 chooses a point and perturbs it by moving it parallel to the axes.
The random vector R represents a random direction in this space. By nding the best value
for , OC1 nds the best distance to adjust the hyperplane in the direction of R.
12

Induction of Oblique Decision Trees

Note that this additional perturbation in a random direction does not signicantly increase the time complexity of the algorithm (see Appendix A). We found in our experiments
that even a single random jump, when used at a local minimum, proves to be very helpful.
Classication accuracy improved for every one of our data sets when such perturbations
were made. See Section 4.3 for some examples.
The second technique for avoiding local minima is a variation on the idea of performing
multiple local searches. The technique of multiple local searches is a natural extension
to local search, and has been widely mentioned in the optimization literature (see Roth
(1970) for an early example). Because most of the steps of our perturbation algorithm
are deterministic, the initial hyperplane largely determines which local minimum will be
encountered rst. Perturbing a single initial hyperplane is thus unlikely to lead to the best
split of a given data set. In cases where the random perturbation method fails to escape
from local minima, it may be helpful to simply start afresh with a new initial hyperplane.
We use the word restart to denote one run of the perturbation algorithms, at one node of
the decision tree, using one random initial hyperplane.6 That is, a restart cycles through
and perturbs the coecients one at a time and then tries to perturb the hyperplane in a
random direction when the algorithm reaches a local minimum. If this last perturbation
reduces the impurity, the algorithm goes back to perturbing the coecients one at a time.
The restart ends when neither the deterministic local search nor the random jump can nd
a better split. One of the optional parameters to OC1 species how many restarts to use.
If more than one restart is used, then the best hyperplane found thus far is always saved.
In all our experiments, the classication accuracies increased with more than one restart.
Accuracy tended to increase up to a point and then level o (after about 20{50 restarts,
depending on the domain). Overall, the use of multiple initial hyperplanes substantially
improved the quality of the decision trees found (see Section 4.3 for some examples).
By carefully combining hill-climbing and randomization, OC1 ensures a worst case time
of O(dn2 log n) for inducing a decision tree. See Appendix A for a derivation of this upper
bound.

Best Axis-Parallel Split. It is clear that axis-parallel splits are more suitable for some

data distributions than oblique splits. To take into account such distributions, OC1 computes the best axis-parallel split and an oblique split at each node, and then picks the better
of the two.7 Calculating the best axis-parallel split takes an additional O(dn log n) time,
and so does not increase the asymptotic time complexity of OC1. As a simple variant of
the OC1 system, the user can opt to \switch o" the oblique perturbations, thus building
an axis-parallel tree on the training data. Section 4.2 empirically demonstrates that this
axis-parallel variant of OC1 compares favorably with existing axis-parallel algorithms.
6. The rst run through the algorithm at each node always begins at the location of the best axis-parallel
hyperplane; all subsequent restarts begin at random locations.
7. Sometimes a simple axis-parallel split is preferable to an oblique split, even if the oblique split has slightly
lower impurity. The user can specify such a bias as an input parameter to OC1.

13

Murthy, Kasif & Salzberg

3.3 Other Details
3.3.1 Impurity Measures

OC1 attempts to divide the d-dimensional attribute space into homogeneous regions; i.e.,
regions that contain examples from just one category. The goal of adding new nodes to
a tree is to split up the sample space so as to minimize the \impurity" of the training
set. Some algorithms measure \goodness" instead of impurity, the dierence being that
goodness values should be maximized while impurity should be minimized. Many dierent
measures of impurity have been studied (Breiman et al., 1984; Quinlan, 1986; Mingers,
1989b; Buntine & Niblett, 1992; Fayyad & Irani, 1992; Heath et al., 1993b).
The OC1 system is designed to work with a large class of impurity measures. Stated
simply, if the impurity measure uses only the counts of examples belonging to every category
on both sides of a split, then OC1 can use it. (See Murthy and Salzberg (1994) for ways of
mapping other kinds of impurity measures to this class of impurity measures.) The user can
plug in any impurity measure that ts this description. The OC1 implementation includes
six impurity measures, namely:
1.
2.
3.
4.
5.
6.

Information Gain
The Gini Index
The Twoing Rule
Max Minority
Sum Minority
Sum of Variances

Though all six of the measures have been dened elsewhere in the literature, in some
cases we have made slight modications that are dened precisely in Appendix B. Our
experiments indicated that, on average, Information Gain, Gini Index and the Twoing Rule
perform better than the other three measures for both axis-parallel and oblique trees. The
Twoing Rule is the current default impurity measure for OC1, and it was used in all of
the experiments reported in Section 4. There are, however, articial data sets for which
Sum Minority and/or Max Minority perform much better than the rest of the measures.
For instance, Sum Minority easily induces the exact tree for the POL data set described in
Section 4.3.1, while all other methods have diculty nding the best tree.

Twoing Rule. The Twoing Rule was rst proposed by Breiman et al. (1984). The value

to be computed is dened as:

k
X

TwoingValue = (jTLj=n)  (jTRj=n)  (

i=1

jLi=jTLj , Ri=jTRjj)2

where jTLj (jTRj) is the number of examples on the left (right) of a split at node T , n is
the number of examples at node T , and Li (Ri ) is the number of examples in category i on
the left (right) of the split. The TwoingValue is actually a goodness measure rather than
an impurity measure. Therefore OC1 attempts to minimize the reciprocal of this value.
The remaining ve impurity measures implemented in OC1 are dened in Appendix B.
14

Induction of Oblique Decision Trees

3.3.2 Pruning

Virtually all decision tree induction systems prune the trees they create in order to avoid
overtting the data. Many studies have found that judicious pruning results in both smaller
and more accurate classiers, for decision trees as well as other types of machine learning
systems (Quinlan, 1987; Niblett, 1986; Cestnik, Kononenko, & Bratko, 1987; Kodrato
& Manago, 1987; Cohen, 1993; Hassibi & Stork, 1993; Wolpert, 1992; Schaer, 1993).
For the OC1 system we implemented an existing pruning method, but note that any tree
pruning method will work ne within OC1. Based on the experimental evaluations of
Mingers (1989a) and other work cited above, we chose Breiman et al.'s Cost Complexity
(CC) pruning (1984) as the default pruning method for OC1. This method, which is also
called Error Complexity or Weakest Link pruning, requires a separate pruning set. The
pruning set can be a randomly chosen subset of the training set, or it can be approximated
using cross validation. OC1 randomly chooses 10% (the default value) of the training data
to use for pruning. In the experiments reported below, we only used this default value.
Briey, the idea behind CC pruning is to create a set of trees of decreasing size from the
original, complete tree. All these trees are used to classify the pruning set, and accuracy is
estimated from that. CC pruning then chooses the smallest tree whose accuracy is within k
standard errors squared of the best accuracy obtained. When the 0-SE rule (k = 0) is used,
the tree with highest accuracy on the pruning set is selected. When k > 0, smaller tree size
is preferred over higher accuracy. For details of Cost Complexity pruning, see Breiman et
al. (1984) or Mingers (1989a).
3.3.3 Irrelevant attributes

Irrelevant attributes pose a signicant problem for most machine learning methods (Breiman
et al., 1984; Aha, 1990; Almuallin & Dietterich, 1991; Kira & Rendell, 1992; Salzberg, 1992;
Cardie, 1993; Schlimmer, 1993; Langley & Sage, 1993; Brodley & Utgo, 1994). Decision
tree algorithms, even axis-parallel ones, can be confused by too many irrelevant attributes.
Because oblique decision trees learn the coecients of each attribute at a DT node, one
might hope that the values chosen for each coecient would reect the relative importance
of the corresponding attributes. Clearly, though, the process of searching for good coecient
values will be much more ecient when there are fewer attributes; the search space is much
smaller. For this reason, oblique DT induction methods can benet substantially by using a
feature selection method (an algorithm that selects a subset of the original attribute set) in
conjunction with the coecient learning algorithm (Breiman et al., 1984; Brodley & Utgo,
1994).
Currently, OC1 does not have a built-in mechanism to select relevant attributes. However, it is easy to include any of several standard methods (e.g., stepwise forward selection
or stepwise backward selection) or even an ad hoc method to select features before running
the tree-building process. For example, in separate experiments on data from the Hubble
Space Telescope (Salzberg, Chandar, Ford, Murthy, & White, 1994), we used feature selection methods as a preprocessing step to OC1, and reduced the number of attributes from 20
to 2. The resulting decision trees were both simpler and more accurate. Work is currently
underway to incorporate an ecient feature selection technique into the OC1 system.
15

Murthy, Kasif & Salzberg

Regarding missing values, if an example is missing a value for any attribute, OC1 uses
the mean value for that attribute. One can of course use other techniques for handling
missing values, but those were not considered in this study.

4. Experiments

In this section, we present two sets of experiments to support the following two claims.
1. OC1 compares favorably over a variety of real-world domains with several existing
axis-parallel and oblique decision tree induction methods.
2. Randomization, both in the form of multiple local searches and random jumps, improves the quality of decision trees produced by OC1.
The experimental method used for all the experiments is described in Section 4.1. Sections 4.2 and 4.3 describe experiments corresponding to the above two claims. Each experimental section begins with a description of the data sets, and then presents the experimental
results and discussion.

4.1 Experimental Method

We used ve-fold cross validation (CV) in all our experiments to estimate classication
accuracy. A k-fold CV experiment consists of the following steps.
1. Randomly divide the data into k equal-sized disjoint partitions.
2. For each partition, build a decision tree using all data outside the partition, and test
the tree on the data in the partition.
3. Sum the number of correct classications of the k trees and divide by the total number
of instances to compute the classication accuracy. Report this accuracy and the
average size of the k trees.
Each entry in Tables 1 and 2 is a result of ten 5-fold CV experiments; i.e., the result of tests
that used 50 decision trees. Each of the ten 5-fold cross validations used a dierent random
partitioning of the data. Each entry in the tables reports the mean and standard deviation
of the classication accuracy, followed by the mean and standard deviation of the decision
tree size (measured as the number of leaf nodes). Good results should have high values for
accuracy, low values for tree size, and small standard deviations.
In addition to OC1, we also included in the experiments an axis-parallel version of OC1,
which only considers axis-parallel hyperplanes. We call this version, described in Section 3.2,
OC1-AP. In all our experiments, both OC1 and OC1-AP used the Twoing Rule (Section
3.3.1) to measure impurity. Other parameters to OC1 took their default values unless stated
otherwise. (Defaults include the following: number of restarts at each node: 20. Number
of random jumps attempted at each local minimum: 5. Order of coecient perturbation:
Sequential. Pruning method: Cost Complexity with the 0-SE rule, using 10% of the training
set exclusively for pruning.)
In our comparison, we used the oblique version of the CART algorithm, CART-LC.
We implemented our own version of CART-LC, following the description in Breiman et
al. (1984, Chapter 5); however, there may be dierences between our version and other
16

Induction of Oblique Decision Trees

versions of this system (note that CART-LC is not freely available). Our implementation
of CART-LC measured impurity with the Twoing Rule and used 0-SE Cost Complexity
pruning with a separate test set, just as OC1 does. We did not include any feature selection
methods in CART-LC or in OC1, and we did not implement normalization. Because the
CART coecient perturbation algorithm may alternate indenitely between two locations
of a hyperplane (see Section 2), we imposed an arbitrary limit of 100 such perturbations
before forcing the perturbation algorithm to halt.
We also included axis-parallel CART and C4.5 in our comparisons. We used the implementations of these algorithms from the IND 2.1 package (Buntine, 1992). The default
cart0 and c4.5 \styles" dened in the package were used, without altering any parameter
settings. The cart0 style uses the Twoing Rule and 0-SE cost complexity pruning with
10-fold cross validation. The pruning method, impurity measure and other defaults of the
c4.5 style are the same as those described in Quinlan (1993a).

4.2 OC1 vs. Other Decision Tree Induction Methods

Table 1 compares the performance of OC1 to three well-known decision tree induction
methods plus OC1-AP on six dierent real-world data sets. In the next section we will
consider articial data, for which the concept denition can be precisely characterized.
4.2.1 Description of Data Sets

Star/Galaxy Discrimination. Two of our data sets came from a large set of astronom-

ical images collected by Odewahn et al. (Odewahn, Stockwell, Pennington, Humphreys, &
Zumach, 1992). In their study, they used these images to train articial neural networks
running the perceptron and back propagation algorithms. The goal was to classify each example as either \star" or \galaxy." Each image is characterized by 14 real-valued attributes,
where the attributes were measurements dened by astronomers as likely to be relevant for
this task. The objects in the image were divided by Odewahn et al. into \bright" and \dim"
data sets based on the image intensity values, where the dim images are inherently more
dicult to classify. (Note that the \bright" objects are only bright in relation to others
in this data set. In actuality they are extremely faint, visible only to the most powerful
telescopes.) The bright set contains 2462 objects and the dim set contains 4192 objects.
In addition to the results reported in Table 1, the following results have appeared on
the Star/Galaxy data. Odewahn et al. (1992) reported accuracy of 99.8% accuracy on the
bright objects, and 92.0% on the dim ones, although it should be noted that this study
used a single training and test set partition. Heath (1992) reported 99.0% accuracy on the
bright objects using SADT, with an average tree size of 7.03 leaves. This study also used
a single training and test set. Salzberg (1992) reported accuracies of 98.8% on the bright
objects, and 95.1% on the dim objects, using 1-Nearest Neighbor (1-NN) coupled with a
feature selection method that reduces the number of features.
Breast Cancer Diagnosis. Mangasarian and Bennett have compiled data on the problem of diagnosing breast cancer to test several new classication methods (Mangasarian
et al., 1990; Bennett & Mangasarian, 1992, 1994a). This data represents a set of patients
with breast cancer, where each patient was characterized by nine numeric attributes plus
the diagnosis of the tumor as benign or malignant. The data set currently has 683 entries
17

Murthy, Kasif & Salzberg

Bright S/G
98.90.2
4.31.0
CART-LC
98.80.2
3.91.3
OC1-AP
98.10.2
6.92.4
CART-AP
98.50.5
13.95.7
C4.5
98.50.5
14.32.2
Algorithm
OC1

Dim S/G
95.00.3
13.08.7
92.80.5
24.28.7
94.00.2
29.38.8
94.20.7
30.410
93.30.8
77.97.4

Cancer
96.20.3
2.80.9
95.30.6
3.50.9
94.50.5
6.41.7
95.01.6
11.57.2
95.32.0
9.82.2

Iris
94.73.1
3.10.2
93.52.9
3.20.3
92.72.4
3.20.3
93.83.7
4.31.6
95.13.2
4.60.8

Housing
82.40.8
6.93.2
81.41.2
5.83.2
81.81.0
8.64.5
82.13.5
15.110
83.23.1
28.23.3

Diabetes
74.41.0
5.43.8
73.71.2
8.05.2
73.81.0
11.47.5
73.93.4
11.59.1
71.43.3
56.37.9

Table 1: Comparison of OC1 and other decision tree induction methods on six dierent
data sets. The rst line for each method gives accuracies, and the second line gives
average tree sizes. The highest accuracy for each domain appears in boldface.
and is available from the UC Irvine machine learning repository (Murphy & Aha, 1994).
Heath et al. (1993b) reported 94.9% accuracy on a subset of this data set (it then had
only 470 instances), with an average decision tree size of 4.6 nodes, using SADT. Salzberg
(1991) reported 96.0% accuracy using 1-NN on the same (smaller) data set. Herman and
Yeung (1992) reported 99.0% accuracy using piece-wise linear classication, again using a
somewhat smaller data set.

Classifying Irises. This is Fisher's famous iris data, which has been extensively studied

in the statistics and machine learning literature. The data consists of 150 examples, where
each example is described by four numeric attributes. There are 50 examples of each of
three dierent types of iris ower. Weiss and Kapouleas (1989) obtained accuracies of 96.7%
and 96.0% on this data with back propagation and 1-NN, respectively.

Housing Costs in Boston. This data set, also available as a part of the UCI ML repos-

itory, describes housing values in the suburbs of Boston as a function of 12 continuous
attributes and 1 binary attribute (Harrison & Rubinfeld, 1978). The category variable (median value of owner-occupied homes) is actually continuous, but we discretized it so that
category = 1 if value < $21000, and 2 otherwise. For other uses of this data, see (Belsley,
1980; Quinlan, 1993b).

Diabetes diagnosis. This data catalogs the presence or absence of diabetes among Pima

Indian females, 21 years or older, as a function of eight numeric-valued attributes. The
original source of the data is the National Institute of Diabetes and Digestive and Kidney
Diseases, and it is now available in the UCI repository. Smith et al. (1988) reported 76%
accuracy on this data using their ADAP learning algorithm, using a dierent experimental
method from that used here.
18

Induction of Oblique Decision Trees

4.2.2 Discussion

The table shows that, for the six data sets considered here, OC1 consistently nds better
trees than the original oblique CART method. Its accuracy was greater in all six domains,
although the dierence was signicant (more than 2 standard deviations) only for the dim
star/galaxy problem. The average tree sizes were roughly equal for ve of the six domains,
and for the dim stars and galaxies, OC1 found considerably smaller trees. These dierences
will be analyzed and quantied further by using articial data, in the following section.
Out of the ve decision tree induction methods, OC1 has the highest accuracy on four
of the six domains: bright stars, dim stars, cancer diagnosis, and diabetes diagnosis. On the
remaining two domains, OC1 has the second highest accuracy in each case. Not surprisingly,
the oblique methods (OC1 and CART-LC) generally nd much smaller trees than the axisparallel methods. This dierence can be quite striking for some domains|note, for example,
that OC1 produced a tree with just 13 nodes on average for the dim star/galaxy problem,
while C4.5 produced a tree with 78 nodes, 6 times larger. Of course, in domains for which
an axis-parallel tree is the appropriate representation, axis-parallel methods should compare
well with oblique methods in terms of tree size. In fact, for the Iris data, all the methods
found similar-sized trees.

4.3 Randomization Helps OC1

In our second set of experiments, we examine more closely the eect of introducing randomized steps into the algorithm for nding oblique splits. Our experiments demonstrate that
OC1's ability to produce an accurate tree from a set of training data is clearly enhanced
by the two kinds of randomization it uses. More precisely, we use three articial data sets
(for which the underlying concept is known to the experimenters) to show that OC1's performance improves substantially when the deterministic hill climbing is augmented in any
of three ways:

 with multiple restarts from random initial locations,
 with perturbations in random directions at local minima, or
 with both of the above randomization steps.
In order to nd clear dierences between algorithms, one needs to know that the concept
underlying the data is indeed dicult to learn. For simple concepts (say, two linearly
separable classes in 2-D), many dierent learning algorithms will produce very accurate
classiers, and therefore the advantages of randomization may not be detectable. It is
known that many of the commonly-used data sets from the UCI repository are easy to
learn with very simple representations (Holte, 1993); therefore those data sets may not be
ideal for our purposes. Thus we created a number of articial data sets that present dierent
problems for learning, and for which we know the \correct" concept denition. This allows
us to quantify more precisely how the parameters of our algorithm aect its performance.
A second purpose of this experiment is to compare OC1's search strategy with that
of two existing oblique decision tree induction systems { LMDT (Brodley & Utgo, 1992)
and SADT (Heath et al., 1993b). We show that the quality of trees induced by OC1 is as
good as, if not better than, that of the trees induced by these existing systems on three
19

Murthy, Kasif & Salzberg

articial domains. We also show that OC1 achieves a good balance between amount of
eort expended in search and the quality of the tree induced.
Both LMDT and SADT used information gain for this experiment. However, we did
not change OC1's default measure (the Twoing Rule) because we observed, in experiments
not reported here, that OC1 with information gain does not produce signicantly dierent
results. The maximum number of successive, unproductive perturbations allowed at any
node was set at 10000 for SADT. For all other parameters, we used default settings provided
with the systems.
4.3.1 Description of Artificial Data

LS10 The LS10 data set has 2000 instances divided into two categories. Each instance is

described by ten attributes x1 ,: : : ,x10, whose values are uniformly distributed in the range
[0,1]. The data is linearly separable with a 10-D hyperplane (thus the name LS10) dened
by the equation x1 + x2 + x3 + x4 + x5 < x6 + x7 + x8 + x9 + x10. The instances were all
generated randomly and labelled according to which side of this hyperplane they fell on.
Because oblique DT induction methods intuitively should prefer a linear separator if one
exists, it is interesting to compare the various search techniques on this data set where we
know a separator exists. The task is relatively simple for lower dimensions, so we chose
10-dimensional data to make it more dicult.

POL This data set is shown in Figure 9. It has 2000 instances in two dimensions, again

divided into two categories. The underlying concept is a set of four parallel oblique lines
(thus the name POL), dividing the instances into ve homogeneous regions. This concept is
more dicult to learn than a single linear separator, but the minimal-size tree is still quite
small.

RCB RCB stands for \rotated checker board"; this data set has been the subject of

other experiments on hard classication problems for decision trees (Murthy & Salzberg,
1994). The data set, shown in Figure 9, has 2000 instances in 2-D, each belonging to one of
eight categories. This concept is dicult to learn for any axis-parallel method, for obvious
reasons. It is also quite dicult for oblique methods, for several reasons. The biggest
problem is that the \correct" root node, as shown in the gure, does not separate out any
class by itself. Some impurity measures (such as Sum Minority) will fail miserably on this
problem, although others (e.g., the Twoing Rule) work much better. Another problem is
that a deterministic coecient perturbation algorithm can get stuck in local minima in
many places on this data set.
Table 2 summarizes the results of this experiment in three smaller tables, one for each
data set. In each smaller table, we compare four variants of OC1 with LMDT and SADT.
The dierent results for OC1 were obtained by varying both the number of restarts and the
number of random jumps. When random jumps were used, up to twenty random jumps
were tried at each local minimum. As soon as one was found that improved the impurity
of the current hyperplane, the algorithm moved the hyperplane and started running the
deterministic perturbation procedure again. If none of the 20 random jumps improved the
impurity, the search halted and further restarts (if any) were tried. The same training and
test partitions were used for all methods for each cross-validation run (recall that the results
20

Induction of Oblique Decision Trees

lr1

ot

rr1

-1

l-1

1

r-1

rl-

1
rr-

r-1

t-1
oo
R

3
4
4
4
77
4
33
333
33
4
33
33
7
3 4 4
4 44 44 4
1 33
3
3 3 3
4 4 7
4 44
3
4
4
44
33 3 3
3 33
4
4
7
4
33 3 3
4 44
111 1 3 33 3
7 7 77 7
4
3
3 3 3 33 3 3
4
1
4 4 444
1
44
7
4 4
3
3
4
4 4
3 333 3
4 44444
44 4 4 44
47
1 1
777
3
33 3 4 4
3
4
7
4 4
444 4
7
4444 4
3
7
334 4 4
44
44
33
1
77 77 777
3
4 4
34 44
1 1
7 777 7
44
113
4 7 777 7
4
7
7
4
1
1
4
1
4 4 44 4 4 44 4 4
1
3
7
1
7
1 111
4
4
1
7
7 7 7 777 7
44
4 4 4447
44 4 4
4
77
7
111 1 1 1 4 4 44 4 4
7 7 77 7
44 4
1
4
1 1
4 44 4
7
444
7 7
44 4 4
1 111 1
4
1
4
1
7 77
11
1 1
44 4
4
4
1
7
7
7
7 77
1 1 11 14
1 1 111
4 4 44 477 7
7
4 44
7
1
77
4 4
77 7 7 7 7 7
11
4
8
4
11
7
1 1 1
4
8
2
77 77
7
4 4 7
1
2 4 44 4
7
11 1
7
8
7 77
7
1
1
1
2
1
2
7
7
8
11
11
22 2444 4 4
7 777 77 7 77 7 7 77 7 7 8
4
1 2 22 2 2
7
1
88 8
ll-2
2
7
77 7
4
1 1 22
8
7 7
22 2 22 44 4 4 77
7
7 77888 8
11
1
22 22
8
888
2 22 22
5
7
1
8
22 222
1
2 2
88
7 77 7
2
2
2
11 2 2
8
22
2
5
7777
8
8
222 22 2
8
222
2 2
2
77 8 8 8
2
2
55
77 8
2 2 22 2
22
8 88 8 8 8
5
55
8
2 22 2
2
2 2
8 88
7 8
2 222
8
5 5 77
888 8
2 2
8
2
5
5
5
5
2
2
8
2
55 55
8
88 8
22 222 2
7
2 22
8
2
25
8 88 8
5
2
22
88
55
5
8 8 8 8 8 88
5 5
2 22 2 22 22 2 22 22
8
8 88 8
5
5
2 2
55
22
8 8 88 8
5
2 22 2 5 5 5
8
2 2
5 56
8
8 8
5
5
8
2
2
55
5
5 5 55 5 6 6
8 88 8888
22
5
2
8
5
5
8 88
6 66 8 8
2 2 2 2 2 2 2 2 5 5 55 5 5
6
8
6
8
8
2 2 2 2
8
55
8
5
6
8
8 8
5
6
55
2 2222 2 5
5
5
5
5
5
6
6
6
5 5 5
5 55
6
55 5 5
222 22 2 22
8 8
6
5
55 5 5
6
6
6
66
8
5
55
8
22
6 66 6
8 8
5 5 5
2 22
55
6
6
6 6
2 2
2
5 55 5 5 5
5
6 66 8 8
6
2
5
66 6
5
6 66
55 5
5
5 6
5
55 5
6 6 66
55
5
22 2
55 5
5 5
66 6 6
6
55
6666
8
5 5
6
5
6
6
6
66
55
6 6 66
55
55
8 8
6 66 6
5 5555
6 66 6
5
6
66
2
5 5555
5
66
6
5
5
8
66 6 6
5 5 5 55 555 5
6 66
55 5
6 68
5
6 6
6
6 6

Ro

-1
rrr

2
2
1
2
11
1
22
222
11
2
22
22
1
2 2 2
1 11 11 1
1 11
2
2 2 2
1 1 1
2 22
2
2
1
11
22 2 2
2 22
2
2
1
2
22 2 2
2 22
111 1 1 11 2
1 1 11 1
1
2
1 2 2 22 2 2
1
1
2 2 222
1
22
1
1 1
2
2
2
2 2
2 222 2
2 22222
22 2 2 11
11
1 1
111
2
22 2 2 2
1
2
1
2 1
222 2
1
2222 2
1
1
222 2 2
22
22
22
1
11 11 111
1
2 2
22 22
1 1
1 111 1
22
111
2 2 222 1
2
1
2
2
1
1
2
1
2 2 22 2 2 22 2 2
1
1
1
1
1
1 111
2
2
1
2
2 1 1 111 1
22
2 2 2222
22 2 2
1
22
2
111 1 1 1 1 1 11 2 2
1 1 11 1
22 2
1
2
1 1
2 22 2
2
222
1 1
22 2 2
1 111 1
2
1
2
1
2 22
11
1 1
22 2
2
1
1
2
1
2
2 22
1 1 11 11
1 1 111
2 2 22 222 2
2
2 22
2
1
22
2 2
22 2 2 2 2 2
11
1
2
2
11
2
1 1 1
2
2
1
22 22
2
2 2 2
1
1 1 11 1
2
21 1
2
2
2 22
2
1
1
1
1
2
1
2
2
2
11
111
11 1111 1 1
2 222 22 2 22 2 2 22 2 2 2
1
11 1
2
1
22 2
1 1
1
2
22 2
1
2 1 11
2
2 2
11 1 11 11 1 1 12
2
2 22222 2
22
2
11 11
2
222
1 11 11
1
2
2
2
22 111
2
1 1
22
2 22 2
1
1
1
22 2 2
2
11
1
1
2222
2
2
111 11 1
2
111
2 2
2
22 2 2 2
2
1
11
22 2
2 2 22 2
11
2 22 2 2 2
1
11
2
1 11 1
2
1 1
2 22
2 2
2 222
2
1 1 11
222 2
2 2
2
1
1
1
1
1
1
1
2
1
11 11
2
22 2
22 222 2
1
2 22
2
1
11
2 22 2
1
2
22
22
11
1
1 2 2 2 2 22
1 1
2 22 2 22 22 2 22 21
2
2 22 2
1
1
2 2
11
22
2 2 22 2
1
2 22 2 1 1 1
2
2 2
1 11
2
2 2
1
1
1
2
2
22
1
1 1 11 1 1 1
1 22 2222
22
2
2
2
1
2
2 22
1 11 1 1
1 2 2 2 2 2 2 2 2 2 22 1 1
1
2
1
1
1
1 1 2 2
2
11
2
1
1
1
2 2
1
1
11
2 2222 2 2
1
1
1
1
2
1
1
1
2 2 2
1 11
1
22 2 2
111 11 1 22
2 2
1
1
22 2 2
1
1
1
11
1
1
22
2
11
1 11 1
1 2
2 2 2
1
1 11
11
1
1
1 1
1 1
1
2 22 2 2 2
2
1 11 1 2
1
1
2
11 1
2
1 11
11 1
2
1 1
2
22 2
1 1 11
22
1
11 1
22 2
1 2
11 1 1
1
22
1111
1
2 2
2
2
1
1
2
11
11
1 1 11
11
22
1 1
1 11 1
1 1111
1 11 1
2
1
11
1
2 2222
1
22
1
1
2
1
22 1 1
1 1 1 11 111 2
1 11
11 1
1 11
1
2 2
1
1 1

Figure 9: The POL and RCB data sets
Linearly Separable 10-D (LS10) data
R:J Accuracy
Size
Hyperplanes
0:0 89.81.2 67.05.8
2756
0:20 91.51.5 55.27.0
3824
20:0 95.00.6 25.62.4
24913
20:20 97.20.7 13.93.2
30366
LMDT 99.70.2 2.20.5
9089
SADT 95.21.8 15.55.7
349067
Parallel Oblique Lines (POL) data
R:J Accuracy
Size
Hyperplanes
0:0 98.30.3 21.61.9
164
0:20 99.30.2 9.01.0
360
20:0 99.10.2 14.21.1
3230
20:20 99.60.1 5.50.3
4852
LMDT 89.610.2 41.919.2
1732
SADT 99.30.4 8.42.1
85594
Rotated Checker Board (RCB) data
R:J Accuracy
Size
Hyperplanes
0:0 98.40.2 35.51.4
573
0:20 99.30.3 19.70.8
1778
20:0 99.60.2 12.01.4
6436
20:20 99.80.1 8.70.4
11634
LMDT 95.72.3 70.19.6
2451
SADT 97.91.1 32.54.9
359112
Table 2: The eect of randomization in OC1. The rst column, labelled R:J, shows the
number of restarts (R) followed by the maximum number of random jumps (J)
attempted by OC1 at each local minimum. Results with LMDT and SADT are
included for comparison after the four variants of OC1. Size is average tree size
measured by the number of leaf nodes. The third column shows the average
number of hyperplanes each algorithm considered while building one tree.
21

Murthy, Kasif & Salzberg

are an average of ten 5-fold CVs). The trees were not pruned for any of the algorithms,
because the data were noise-free and furthermore the emphasis was on search.
Table 2 also includes the number of hyperplanes considered by each algorithm while
building a complete tree. Note that for OC1 and SADT, the number of hyperplanes considered is generally much larger than the number of perturbations actually made, because
both these algorithms compare newly generated hyperplanes to existing hyperplanes before
adjusting an existing one. Nevertheless, this number is a good estimate of much eort
each algorithm expends, because every new hyperplane must be evaluated according to the
impurity measure. For LMDT, the number of hyperplanes considered is identical to the
actual number of perturbations.
4.3.2 Discussion

The OC1 results here are quite clear. The rst line of each table, labelled 0:0, gives the
accuracies and tree sizes when no randomization is used | this variant is very similar
to the CART-LC algorithm. As we increase the use of randomization, accuracy increases
while tree size decreases, which is exactly the result we had hoped for when we decided to
introduce randomization into the method.
Looking more closely at the tables, we can ask about the eect of random jumps alone.
This is illustrated in the second line (0:20) of each table, which attempted up to 20 random
jumps at each local minimum and no restarts. Accuracy increased by 1-2% on each domain,
and tree size decreased dramatically, roughly by a factor of two, in the POL and RCB
domains. Note that because there is no noise in these domains, very high accuracies should
be expected. Thus increases of more than a few percent in accuracy are not possible.
Looking at the third line of each sub-table in Table 2, we see the eect of multiple restarts
on OC1. With 20 restarts but no random jumps to escape local minima, the improvement
is even more noticeable for the LS10 data than when random jumps alone were used. For
this data set, accuracy jumped signicantly, from 89.8 to 95.0%, while tree size dropped
from 67 to 26 nodes. For the POL and RCB data, the improvements were comparable to
those obtained with random jumps. For the RCB data, tree size dropped by a factor of 3
(from 36 leaf nodes to 12 leaf nodes) while accuracy increased from 98.4 to 99.6%.
The fourth line of each table shows the eect of both the randomized steps. Among the
OC1 entries, this line has both the highest accuracies and the smallest trees for all three
data sets, so it is clear that randomization is a big win for these kinds of problems. In
addition, note that the smallest tree for the RCB data should have eight leaf nodes, and
OC1's average trees, without pruning, had just 8.7 leaf nodes. It is clear that for this data
set, which we thought was the most dicult one, OC1 came very close to nding the optimal
tree on nearly every run. (Recall that numbers in the table are the average of 10 5-fold
CV experiments; i.e., an average of 50 decision trees.) The LS10 data show how dicult it
can be to nd a very simple concept in higher dimensions|the optimal tree there is just a
single hyperplane (two nodes), but OC1 was unable to nd it with the current parameter
settings.8 The POL data required a minimum of 5 leaf nodes, and OC1 found this minimalsize tree most of the time, as can be seen from the table. Although not shown in the Table,
8. In a separate experiment, we found that OC1 consistently nds the linear separator for the LS10 data
when 10 restarts and 200 random jumps are used.

22

Induction of Oblique Decision Trees

OC1 using Sum Minority performed better for the POL data than the Twoing Rule or any
other impurity measure; i.e., it found the correct tree using less time.
The results of LMDT and SADT on this data lead to some interesting insights. Not
surprisingly, LMDT does very well on the linearly separable (LS10) data, and does not
require an inordinate amount of search. Clearly, if the data is linearly separable, one should
use a method such as LMDT or linear programming. OC1 and SADT have diculty nding
the linear separator, although in our experiments OC1 did eventually nd it, given sucient
time.
On the other hand, for both of the non-linearly separable data sets, LMDT produces
much larger trees that are signicantly less accurate than those produced by OC1 and
SADT. Even the deterministic variant of OC1 (using zero restarts and zero random jumps)
outperforms LMDT on these problems, with much less search.
Although SADT sometimes produces very accurate trees, its main weakness was the
enormous amount of search time it required, roughly 10-20 times greater than OC1 even
using the 20:20 setting. One explanation of OC1's advantage is its use of directed search, as
opposed to the strictly random search used by simulated annealing. Overall, Table 2 shows
that OC1's use of randomization was quite eective for the non-linearly separable data.
It is natural to ask why randomization helps OC1 in the task of inducing decision trees.
Researchers in combinatorial optimization have observed that randomized search usually
succeeds when the search space holds an abundance of good solutions (Gupta, Smolka,
& Bhaskar, 1994). Furthermore, randomization can improve upon deterministic search
when many of the local maxima in a search space lead to poor solutions. In OC1's search
space, a local maximum is a hyperplane that cannot be improved by the deterministic
search procedure, and a \solution" is a complete decision tree. If a signicant fraction
of local maxima lead to bad trees, then algorithms that stop at the rst local maximum
they encounter will perform poorly. Because randomization allows OC1 to consider many
dierent local maxima, if a modest percentage of these maxima lead to good trees, then it
has a good chance of nding one of those trees. Our experiments with OC1 thus far indicate
that the space of oblique hyperplanes usually contains numerous local maxima, and that a
substantial percentage of these locally good hyperplanes lead to good decision trees.

5. Conclusions and Future Work
This paper has described OC1, a new system for constructing oblique decision trees. We
have shown experimentally that OC1 can produce good classiers for a range of real-world
and articial domains. We have also shown how the use of randomization improves upon
the original algorithm proposed by Breiman et al. (1984), without signicantly increasing
the computational cost of the algorithm.
The use of randomization might also be benecial for axis-parallel tree methods. Note
that although they do nd the optimal test (with respect to an impurity measure) for each
node of a tree, the complete tree may not be optimal: as is well known, the problem of
nding the smallest tree is NP-Complete (Hyal & Rivest, 1976). Thus even axis-parallel
decision tree methods do not produce \ideal" decision trees. Quinlan has suggested that his
windowing algorithm might be used as a way of introducing randomization into C4.5, even
though the algorithm was designed for another purpose (Quinlan, 1993a). (The windowing
23

Murthy, Kasif & Salzberg

algorithm selects a random subset of the training data and builds a tree using that.) We
believe that randomization is a powerful tool in the context of decision trees, and our
experiments are just one example of how it might be exploited. We are in the process of
conducting further experiments to quantify more accurately the eects of dierent forms of
randomization.
It should be clear that the ability to produce oblique splits at a node broadens the capabilities of decision tree algorithms, especially as regards domains with numeric attributes.
Of course, axis-parallel splits are simpler, in the sense that the description of the split only
uses one attribute at each node. OC1 uses oblique splits only when their impurity is less
than the impurity of the best axis-parallel split; however, one could easily penalize the
additional complexity of an oblique split further. This remains an open area for further
research. A more general point is that if the domain is best captured by a tree that uses
oblique hyperplanes, it is desirable to have a system that can generate that tree. We have
shown that for some problems, including those used in our experiments, OC1 builds small
decision trees that capture the domain well.

Appendix A. Complexity Analysis of OC1

In the following, we show that OC1 runs eciently even in the worst case. For a data
set with n examples (points) and d attributes per example, OC1 uses at most O(dn2 log n)
time. We assume n > d for our analysis.
For the analysis here, we assume the coecients of a hyperplane are adjusted in sequential order (the Seq method described in the paper). The number of restarts at a node will
be r, and the number of random jumps tried will be j . Both r and j are constants, xed in
advance of running the algorithm.
Initializing the hyperplane to a random position takes just O(d) time. We need to
consider rst the maximum amount of work OC1 can do before it nds a new location for
the hyperplane. Then we need to consider how many times it can move the hyperplane.
1. Attempting to perturb the rst coecient (a1 ) takes O(dn + n log n) time. Computing
Ui 's for all the points (equation 2) requires O(dn) time, and sorting the Ui 's takes
O(n log n). This gives us O(dn + n log n) work.
2. If perturbing a1 does not improve things, we try to perturb a2 . Computing all the new
Ui 's will take just O(n) time because only one term is dierent for each Ui . Re-sorting
will take O(n log n), so this step takes O(n) + O(n log n) = O(n log n) time.
3. Likewise a3; : : :; ad will each take O(n log n) additional time, assuming we still have not
found a better hyperplane after checking each coecient. Thus the total time to cycle
through and attempt to perturb all these additional coecients is (d , 1)  O(n log n) =
O(dn log n).
4. Summing up, the time to cycle through all coecients is O(dn log n)+O(dn+n log n) =
O(dn log n).
5. If none of the coecients improved the split, then we attempt to make up to j random
jumps. Since j is a constant, we will just consider j = 1 for our analysis. This step
24

Induction of Oblique Decision Trees

involves choosing a random vector and running the perturbation algorithm to solve
for , as explained in Section 3.2. As before, we need to compute a set of Ui 's and sort
them, which takes O(dn + n log n) time. Because this amount of time is dominated by
the time to adjust all the coecients, the total time so far is still O(dn log n). This is
the most time OC1 can spend at a node before either halting or nding an improved
hyperplane.
6. Assuming OC1 is using the Sum Minority or Max Minority error measure, it can only
reduce the impurity of the hyperplane n times. This is clear because each improvement
means one more example will be correctly classied by the new hyperplane. Thus the
total amount of work at a node is limited to n  O(dn log n) = O(dn2 log n). (This
analysis extends, with at most linear cost factors, to Information Gain, Gini Index and
Twoing Rule when there are two categories. It will not apply to a measure that, for
example, uses the distances of mis-classied objects to the hyperplane.) In practice,
we have found that the number of improvements per node is much smaller than n.
Assuming that OC1 only adjusts a hyperplane when it improves the impurity measure,
it will do O(dn2 log n) work in the worst case.
However, OC1 allows a certain number of adjustments to the hyperplane that do not
improve the impurity, although it will never accept a change that worsens the impurity.
The number allowed is determined by a constant known as \stagnant-perturbations". Let
this value be s. This works as follows.
Each time OC1 nds a new hyperplane that improves on the old one, it resets a counter
to zero. It will move the new hyperplane to a dierent location that has equal impurity at
most s times. After each of these moves it repeats the perturbation algorithm. Whenever
impurity is reduced, it re-starts the counter and again allows s moves to equally good
locations. Thus it is clear that this feature just increases the worst-case complexity of OC1
by a constant factor, s.
Finally, note that the overall cost of OC1 is also O(dn2 log n), i.e., this is an upper
bound on the total running time of OC1 independent of the size of the tree it ends up
creating. (This upper bound applies to Sum Minority and Max Minority; an open question
is whether a similar upper bound can be proven for Information Gain or the Gini Index.)
Thus the worst-case asymptotic complexity of our system is comparable to that of systems
that construct axis-parallel decision trees, which have O(dn2 ) worst-case complexity. To
sketch the intuition that leads to this bound, let G be the total impurity summed over all
leaves in a partially constructed tree (i.e., the sum of currently misclassied points in the
tree). Now observe that each time we run the perturbation algorithm on any node in the
tree, we either halt or improve G by at least one unit. The worst-case analysis for one node
is realized when the perturbation algorithm is run once for every one of the n examples,
but when this happens, there would no longer be any mis-classied examples and the tree
would be complete.

Appendix B. Denitions of impurity measures available in OC1

In addition to the Twoing Rule dened in the text, OC1 contains built-in denitions of ve
additional impurity measures, dened as follows. In each of the following denitions, the
25

Murthy, Kasif & Salzberg

set of examples T at the node about to be split contains n (> 0) instances that belong to
one of k categories. (Initially this set is the entire training set.) A hyperplane H divides T
into two non-overlapping subsets TL and TR (i.e., left and right). Lj and Rj are the number
of instances of category j in TL and TR respectively. All the impurity measures initially
check to see if TL and TR are homogeneous (i.e., all examples belong to the same category),
and if so return minimum (zero) impurity.

Information Gain. This measure of information gained from a particular split was pop-

ularized in the context of decision trees by Quinlan (1986). Quinlan's denition makes
information gain a goodness measure; i.e., something to maximize. Because OC1 attempts
to minimize whatever impurity measure it uses, we use the reciprocal of the standard value
of information gain in the OC1 implementation.

Gini Index. The Gini Criterion (or Index) was proposed for decision trees by Breiman et
al. (1984). The Gini Index as originally dened measures the probability of misclassication
of a set of instances, rather than the impurity of a split. We implement the following
variation:
GiniL = 1:0 ,
GiniR = 1:0 ,

k
X
i=1
k
X
i=1

(Li =jTLj)2
(Ri=jTRj)2

Impurity = (jTLj  GiniL + jTRj  GiniR)=n
where GiniL is the Gini Index on the \left" side of the hyperplane and GiniR is that on the
right.

Max Minority. The measures Max Minority, Sum Minority and Sum Of Variances were

dened in the context of decision trees by Heath, Kasif, and Salzberg (1993b).9 Max
Minority has the theoretical advantage that a tree built minimizing this measure will have
depth at most log n. Our experiments indicated that this is not a great advantage in
practice: seldom do other impurity measures produce trees substantially deeper than those
produced with Max Minority. The denition is:
MinorityL =
MinorityR =

k
X
i=1;i6=max Li
k
X
i=1;i6=max Ri

Li
Ri

Max Minority = max(MinorityL; MinorityR)
9. Sum Of Variances was called Sum of Impurities by Heath et al.

26

Induction of Oblique Decision Trees

Sum Minority. This measure is very similar to Max Minority. If MinorityL and MinorityR are dened as for the Max Minority measure, then Sum Minority is just the sum of
these two values. This measure is the simplest way of quantifying impurity, as it simply
counts the number of misclassied instances.
Though Sum Minority performs well on some domains, it has some obvious aws. As
one example, consider a domain in which n = 100; d = 1, and k = 2 (i.e., 100 examples, 1
numeric attribute, 2 classes). Suppose that when the examples are sorted according to the
single attribute, the rst 50 instances belong to category 1, followed by 24 instances of category 2, followed by 26 instances of category 1. Then all possible splits for this distribution
have a sum minority of 24. Therefore it is impossible when using Sum Minority to distinguish which split is preferable, although splitting at the alternations between categories is
clearly better.
Sum Of Variances. The denition of this measure is:
jX
TL j
jX
TL j
VarianceL = (Cat(TLi ) , Cat(TLj )=jTLj)2
VarianceR =

i=1

j =1

jX
TRj

jX
TRj

i=1

(Cat(TRi ) ,

j =1

Cat(TRj )=jTRj)2

Sum of Variances = VarianceL + VarianceR
where Cat(Ti) is the category of instance Ti . As this measure is computed using the actual
class labels, it is easy to see that the impurity computed varies depending on how numbers
are assigned to the classes. For instance, if T1 consists of 10 points of category 1 and 3
points of category 2, and if T2 consists of 10 points of category 1 and 3 points of category
5, then the Sum Of Variances values are dierent for T1 and T2. To avoid this problem,
OC1 uniformly reassigns category numbers according to the frequency of occurrence of each
category at a node before computing the Sum Of Variances.

Acknowledgements
The authors thank Richard Beigel of Yale University for suggesting the idea of jumping in a
random direction. Thanks to Wray Buntine of Nasa Ames Research Center for providing the
IND 2.1 package, to Carla Brodley for providing the LMDT code, and to David Heath for
providing the SADT code and for assisting us in using it. Thanks also to three anonymous
reviewers for many helpful suggestions. This material is based upon work supported by the
National Science foundation under Grant Nos. IRI-9116843, IRI-9223591, and IRI-9220960.

References
Aha, D. (1990). A Study of Instance-Based Algorithms for Supervised Learning: Mathematical, empirical and psychological evaluations. Ph.D. thesis, Department of Information
and Computer Science, University of California, Irvine.
27

Murthy, Kasif & Salzberg

Almuallin, H., & Dietterich, T. (1991). Learning with many irrelevant features. In Proceedings of the Ninth National Conference on Articial Intelligence, pp. 547{552. San
Jose, CA.
Belsley, D. (1980). Regression Diagnostics: Identifying Inuential Data and Sources of
Collinearity. Wiley & Sons, New York.
Bennett, K., & Mangasarian, O. (1992). Robust linear programming discrimination of two
linearly inseparable sets. Optimization Methods and Software, 1, 23{34.
Bennett, K., & Mangasarian, O. (1994a). Multicategory discrimination via linear programming. Optimization Methods and Software, 3, 29{39.
Bennett, K., & Mangasarian, O. (1994b). Serial and parallel multicategory discrimination.
SIAM Journal on Optimization, 4 (4).
Blum, A., & Rivest, R. (1988). Training a 3-node neural network is NP-complete. In Proceedings of the 1988 Workshop on Computational Learning Theory, pp. 9{18. Boston,
MA. Morgan Kaufmann.
Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classication and Regression
Trees. Wadsworth International Group.
Brent, R. P. (1991). Fast training algorithms for multilayer neural nets. IEEE Transactions
on Neural Networks, 2 (3), 346{354.
Brodley, C. E., & Utgo, P. E. (1992). Multivariate versus univariate decision trees. Tech.
rep. COINS CR 92-8, Dept. of Computer Science, University of Massachusetts at
Amherst.
Brodley, C. E., & Utgo, P. E. (1994). Multivariate decision trees. Machine Learning, to
appear.
Buntine, W. (1992). Tree classication software. Technology 2002: The Third National
Technology Transfer Conference and Exposition.
Buntine, W., & Niblett, T. (1992). A further comparison of splitting rules for decision-tree
induction. Machine Learning, 8, 75{85.
Cardie, C. (1993). Using decision trees to improve case-based learning. In Proceedings of
the Tenth International Conference on Machine Learning, pp. 25{32. University of
Massachusetts, Amherst.
Cestnik, G., Kononenko, I., & Bratko, I. (1987). Assistant 86: A knowledge acquisition
tool for sophisticated users. In Bratko, I., & Lavrac, N. (Eds.), Progress in Machine
Learning. Sigma Press.
Cios, K. J., & Liu, N. (1992). A machine learning method for generation of a neural network
architecture: A continuous ID3 algorithm. IEEE Transactions on Neural Networks,
3 (2), 280{291.
28

Induction of Oblique Decision Trees

Cohen, W. (1993). Ecient pruning methods for separate-and-conquer rule learning systems. In Proceedings of the 13th International Joint Conference on Articial Intelligence, pp. 988{994. Morgan Kaufmann.
Fayyad, U. M., & Irani, K. B. (1992). The attribute specication problem in decision tree
generation. In Proceedings of the Tenth National Conference on Articial Intelligence,
pp. 104{110. San Jose CA. AAAI Press.
Frean, M. (1990). Small Nets and Short Paths: Optimising neural computation. Ph.D.
thesis, Centre for Cognitive Science, University of Edinburgh.
Gupta, R., Smolka, S., & Bhaskar, S. (1994). On randomization in sequential and distributed
algorithms. ACM Computing Surveys, 26 (1), 7{86.
Hampson, S., & Volper, D. (1986). Linear function neurons: Structure and training. Biological Cybernetics, 53, 203{217.
Harrison, D., & Rubinfeld, D. (1978). Hedonic prices and the demand for clean air. Journal
of Environmental Economics and Management, 5, 81{102.
Hassibi, B., & Stork, D. (1993). Second order derivatives for network pruning: optimal
brain surgeon. In Advances in Neural Information Processing Systems 5, pp. 164{171.
Morgan Kaufmann, San Mateo, CA.
Heath, D. (1992). A Geometric Framework for Machine Learning. Ph.D. thesis, Johns
Hopkins University, Baltimore, Maryland.
Heath, D., Kasif, S., & Salzberg, S. (1993a). k-DT: A multi-tree learning method. In
Proceedings of the Second International Workshop on Multistrategy Learning, pp. 138{
149. Harpers Ferry, WV. George Mason University.
Heath, D., Kasif, S., & Salzberg, S. (1993b). Learning oblique decision trees. In Proceedings
of the 13th International Joint Conference on Articial Intelligence, pp. 1002{1007.
Chambery, France. Morgan Kaufmann.
Herman, G. T., & Yeung, K. D. (1992). On piecewise-linear classication. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14 (7), 782{786.
Holte, R. (1993). Very simple classication rules perform well on most commonly used
datasets. Machine Learning, 11 (1), 63{90.
Hyal, L., & Rivest, R. L. (1976). Constructing optimal binary decision trees is NPcomplete. Information Processing Letters, 5 (1), 15{17.
Kira, K., & Rendell, L. (1992). A practical approach to feature selection. In Proceedings
of the Ninth International Conference on Machine Learning, pp. 249{256. Aberdeen,
Scotland. Morgan Kaufmann.
Kirkpatrick, S., Gelatt, C., & Vecci, M. (1983). Optimization by simulated annealing.
Science, 220 (4598), 671{680.
29

Murthy, Kasif & Salzberg

Kodrato, Y., & Manago, M. (1987). Generalization and noise. International Journal of
Man-Machine Studies, 27, 181{204.
Langley, P., & Sage, S. (1993). Scaling to domains with many irrelevant features. Learning
Systems Department, Siemens Corporate Research, Princeton, NJ.
Mangasarian, O., Setiono, R., & Wolberg, W. (1990). Pattern recognition via linear programming: Theory and application to medical diagnosis. In SIAM Workshop on
Optimization.
Mingers, J. (1989a). An empirical comparison of pruning methods for decision tree induction. Machine Learning, 4 (2), 227{243.
Mingers, J. (1989b). An empirical comparison of selection measures for decision tree induction. Machine Learning, 3, 319{342.
Moret, B. M. (1982). Decision trees and diagrams. Computing Surveys, 14 (4), 593{623.
Murphy, P., & Aha, D. (1994). UCI repository of machine learning databases { a machinereadable data repository. Maintained at the Department of Information and Computer
Science, University of California, Irvine. Anonymous FTP from ics.uci.edu in the
directory pub/machine-learning-databases.
Murthy, S. K., Kasif, S., Salzberg, S., & Beigel, R. (1993). OC1: Randomized induction of
oblique decision trees. In Proceedings of the Eleventh National Conference on Articial
Intelligence, pp. 322{327. Washington, D.C. MIT Press.
Murthy, S. K., & Salzberg, S. (1994). Using structure to improve decision trees. Tech. rep.
JHU-94/12, Department of Computer Science, Johns Hopkins University.
Niblett, T. (1986). Constructing decision trees in noisy domains. In Bratko, I., & Lavrac,
N. (Eds.), Progress in Machine Learning. Sigma Press, England.
Nilsson, N. (1990). Learning Machines. Morgan Kaufmann, San Mateo, CA.
Odewahn, S., Stockwell, E., Pennington, R., Humphreys, R., & Zumach, W. (1992). Automated star-galaxy descrimination with neural networks. Astronomical Journal,
103 (1), 318{331.
Pagallo, G. (1990). Adaptive Decision Tree Algorithms for Learning From Examples. Ph.D.
thesis, University of California at Santa Cruz.
Pagallo, G., & Haussler, D. (1990). Boolean feature discovery in empirical learning. Machine
Learning, 5 (1), 71{99.
Quinlan, J. R. (1983). Learning ecient classication procedures and their application to
chess end games. In Michalski, R., Carbonell, J., & Mitchell, T. (Eds.), Machine
Learning: An Articial Intelligence Approach. Morgan Kaufmann, San Mateo, CA.
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1, 81{106.
30

Induction of Oblique Decision Trees

Quinlan, J. R. (1987). Simplifying decision trees. International Journal of Man-Machine
Studies, 27, 221{234.
Quinlan, J. R. (1993a). C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, San Mateo, CA.
Quinlan, J. R. (1993b). Combining instance-based and model-based learning. In Proceedings
of the Tenth International Conference on Machine Learning, pp. 236{243 University
of Massachusetts, Amherst. Morgan Kaufmann.
Roth, R. H. (1970). An approach to solving linear discrete optimization problems. Journal
of the ACM, 17 (2), 303{313.
Safavin, S. R., & Landgrebe, D. (1991). A survey of decision tree classier methodology.
IEEE Transactions on Systems, Man and Cybernetics, 21 (3), 660{674.
Sahami, M. (1993). Learning non-linearly separable boolean functions with linear threshold unit trees and madaline-style networks. In Proceedings of the Eleventh National
Conference on Articial Intelligence, pp. 335{341. AAAI Press.
Salzberg, S. (1991). A nearest hyperrectangle learning method. Machine Learning, 6,
251{276.
Salzberg, S. (1992). Combining learning and search to create good classiers. Tech. rep.
JHU-92/12, Johns Hopkins University, Baltimore MD.
Salzberg, S., Chandar, R., Ford, H., Murthy, S. K., & White, R. (1994). Decision trees for
automated identication of cosmic rays in Hubble Space Telescope images. Publications of the Astronomical Society of the Pacic, to appear.
Schaer, C. (1993). Overtting avoidance as bias. Machine Learning, 10, 153{178.
Schlimmer, J. (1993). Eciently inducing determinations: A complete and systematic
search algorithm that uses optimal pruning. In Proceedings of the Tenth International
Conference on Machine Learning, pp. 284{290. Morgan Kaufmann.
Smith, J., Everhart, J., Dickson, W., Knowler, W., & Johannes, R. (1988). Using the
ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings
of the Symposium on Computer Applications and Medical Care, pp. 261{265. IEEE
Computer Society Press.
Utgo, P. E. (1989). Perceptron trees: A case study in hybrid concept representations.
Connection Science, 1 (4), 377{391.
Utgo, P. E., & Brodley, C. E. (1990). An incremental method for nding multivariate
splits for decision trees. In Proceedings of the Seventh International Conference on
Machine Learning, pp. 58{65. Los Altos, CA. Morgan Kaufmann.
Utgo, P. E., & Brodley, C. E. (1991). Linear machine decision trees. Tech. rep. 10,
University of Massachusetts at Amherst.
31

Murthy, Kasif & Salzberg

Van de Merckt, T. (1992). NFDT: A system that learns exible concepts based on decision
trees for numerical attributes. In Proceedings of the Ninth International Workshop on
Machine Learning, pp. 322{331.
Van de Merckt, T. (1993). Decision trees in numerical attribute spaces. In Proceedings of
the 13th International Joint Conference on Articial Intelligence, pp. 1016{1021.
Weiss, S., & Kapouleas, I. (1989). An empirical comparison of pattern recognition, neural
nets, and machine learning classication methods. In Proceedings of the 11th International Joint Conference of Articial Intelligence, pp. 781{787. Detroit, MI. Morgan
Kaufmann.
Wolpert, D. (1992). On overtting avoidance as bias. Tech. rep. SFI TR 92-03-5001, The
Santa Fe Institute, Santa Fe, New Mexico.

32

Journal of Articial Intelligence Research 2 (1995) 501-539

Submitted 9/94; published 5/95

Pac-Learning Recursive Logic Programs:
Ecient Algorithms
William W. Cohen

AT&T Bell Laboratories
600 Mountain Avenue, Murray Hill, NJ 07974 USA

wcohen@research.att.com

Abstract

We present algorithms that learn certain classes of function-free recursive logic programs in polynomial time from equivalence queries. In particular, we show that a single
k-ary recursive constant-depth determinate clause is learnable. Two-clause programs consisting of one learnable recursive clause and one constant-depth determinate non-recursive
clause are also learnable, if an additional \basecase" oracle is assumed. These results immediately imply the pac-learnability of these classes. Although these classes of learnable
recursive programs are very constrained, it is shown in a companion paper that they are
maximally general, in that generalizing either class in any natural way leads to a computationally dicult learning problem. Thus, taken together with its companion paper, this
paper establishes a boundary of ecient learnability for recursive logic programs.

1. Introduction
One active area of research in machine learning is learning concepts expressed in rstorder logic. Since most researchers have used some variant of Prolog to represent learned
concepts, this subarea is sometimes called inductive logic programming (ILP) (Muggleton,
1992; Muggleton & De Raedt, 1994).
Within ILP, researchers have considered two broad classes of learning problems. The
rst class of problems, which we will call here logic based relational learning problems,
are rst-order variants of the sorts of classication problems typically considered within
AI machine learning community: prototypical examples include Muggleton et al.'s (1992)
formulation of -helix prediction, King et al.'s (1992) formulation of predicting drug activity, and Zelle and Mooney's (1994) use of ILP techniques to learn control heuristics for
deterministic parsers. Logic-based relational learning often involves noisy examples that reect a relatively complex underlying relationship; it is a natural extension of propositional
machine learning, and has already enjoyed a number of experimental successes.
In the second class of problems studied by ILP researchers, the target concept is a Prolog
program that implements some common list-processing or arithmetic function; prototypical
problems from this class might be learning to append two lists, or to multiply two numbers.
These learning problems are similar in character to those studied in the area of automatic
programming from examples (Summers, 1977; Biermann, 1978), and hence might be appropriately called automatic logic programming problems. Automatic logic programming
problems are characterized by noise-free training data and recursive target concepts. Thus a
problem that is central to the enterprise of automatic logic programming|but not, perhaps,
logic-based relational learning|is the problem of learning recursive logic programs.
c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Cohen

The goal of this paper is to formally analyze the learnability of recursive logic programs
in Valiant's (1984) model of pac-learnability, thus hopefully shedding some light on the
task of automatic logic programming. To summarize our results, we will show that some
simple recursive programs are pac-learnable from examples alone, or from examples plus a
small number of additional \hints". The largest learnable class we identify in a standard
learning model is the class of one-clause constant-depth determinate programs with at most
a constant number of \closed" recursive literals. The largest learnable class we identify
that requires extra \hints" is the class of constant-depth determinate programs consisting
of a single nonrecursive base clause and a single recursive clause from the class described
above. All of our results are proved in the model of identication from equivalence queries
(Angluin, 1988, 1989), which is somewhat stronger than pac-learnability. Identication from
equivalence queries requires that the target concept be exactly identied, in polynomial
time, and using only a polynomial number of equivalence queries . An equivalence query
asks if a hypothesis program H is equivalent to the target program C ; the answer to a
query is either \yes" or an adversarily chosen example on which H and C dier. This
model of learnability is arguably more appropriate for automatic logic programming tasks
than the weaker model of pac-learnability, as it is unclear how often an approximately
correct recursive program will be useful.
Interestingly, the learning algorithms analyzed are dierent from most existing ILP
learning methods; they all employ an unusual method of generalizing examples called forced
simulation . Forced simulation is a simple and analytically tractable alternative to other
methods for generalizing recursive programs against examples, such as n-th root nding
(Muggleton, 1994), sub-unication (Aha, Lapointe, Ling, & Matwin, 1994) and recursive
anti-unication (Idestam-Almquist, 1993), but it has been only rarely used in experimental
ILP systems (Ling, 1991).
The paper is organized as follows. After presenting some preliminary denitions, we
begin by presenting (primarily for pedagogical reasons) a procedure for identifying from
equivalence queries a single non-recursive constant-depth determinate clause. Then, in
Section 4, we extend this learning algorithm, and the corresponding proof of correctness,
to a simple class of recursive clauses: the class of \closed" linear recursive constant-depth
determinate clauses. In Section 5, we relax some assumptions made to make the analysis
easier, and present several extensions to this algorithm: we extend the algorithm from linear
recursion to k-ary recursion, and also show how a k-ary recursive clause and a non-recursive
clause can be learned simultaneously given an additional \basecase" oracle. We then discuss
related work and conclude.
Although the learnable class of programs is large enough to include some well-known
automatic logic programming benchmarks, it is extremely restricted. In a companion paper
(Cohen, 1995), we provide a number of negative results, showing that relaxing any of these
restrictions leads to dicult learning problems: in particular, learning problems that are
either as hard as learning DNF (an open problem in computational learning theory), or as
hard as cracking certain presumably secure cryptographic schemes. Thus, taken together
with the results of the companion paper, our results delineate a boundary of learnability
for recursive logic programs.
Although the two papers are independent, we suggest that readers wishing to read both
this paper and the companion paper read this paper rst.
502

Pac-Learning Recursive Logic Programs: Efficient Algorithms

2. Background

In this section we will present the technical background necessary to state our results. We
will assume, however, that the reader is familiar with the basic elements of logic programming; readers without this background are referred to one of the standard texts, for example
(Lloyd, 1987).

2.1 Logic Programs

Our treatment of logic programs is standard, except that we will usually consider the body
of a clause to be an ordered set of literals.
For most of this paper, we will consider logic programs without function symbols|
i.e., programs written in Datalog.1 The purpose of such a logic program is to answer
certain questions relative to a database , DB , which is a set of ground atomic facts. (When
convenient, we will also think of DB as a conjunction of ground unit clauses.) The simplest
use of a Datalog program is to check the status of a simple instance . A simple instance
(for a program P and a database DB ) is a fact f . The pair (P; DB ) is said to cover f i
DB ^ P ` f . The set of simple instances covered by (P; DB ) is precisely the minimal model
of the logic program P ^ DB .
In this paper, we will primarily consider extended instances which consist of two parts:
an instance fact f , which is simply a ground fact, and a description D, which is a nite set
of ground unit clauses. An extended instance e = (f; D) is covered by (P; DB ) i
DB ^ D ^ P ` f

If extended instances are allowed, then function-free programs are expressive enough to
encode surprisingly interesting programs. In particular, many programs that are usually
written with function symbols can be re-written as function-free programs, as the example
below illustrates.

Example. Consider the usual program for appending two lists.
append([],Ys,Ys).
append([XjXs1],Ys,[XjZs1])

append(Xs1,Ys,Zs1).

One could use this program to classify atomic facts containing function symbols
such as append([1,2],[3],[1,2,3]). This program can be rewritten as a Datalog
program that classies extended instances as follows:

Program P :

append(Xs,Ys,Ys)
null(Xs).
append(Xs,Ys,Zs)
components(Xs,X,Xs1) ^
components(Zs,X,Zs1) ^
1. This assumption is made primarily for convenience. In Section 5.2 we describe how this assumption can
be relaxed.

503

Cohen

append(Xs1,Ys,Zs1).

Database DB :
null(nil).

The predicate components(A,B,C) means that A is a list with head B and tail
C; thus an extended instance equivalent to append([1,2],[3],[1,2,3]) would be

Instance fact f :

append(list12,list3,list123).

Description D:

components(list12,1,list2).
components(list2,2,nil).
components(list123,1,list23).
components(list23,2,list3).
components(list3,3,nil).
We note that using extended instances as examples is closely related to using ground
clauses entailed by the target clause as examples: specically, the instance e = (f; D) is
covered by P; DB i P ^ DB ` (f D). As the example above shows, there is also a close
relationship between extended instances and literals with function symbols that have been
removed by \attening" (Rouveirol, 1994; De Raedt & Dzeroski, 1994). We have elected
to use Datalog programs and the model of extended instances in this paper for several
reasons. Datalog is relatively easy to analyze. There is a close connection between Datalog
and the restrictions imposed by certain practical learning systems, such FOIL (Quinlan,
1990; Quinlan & Cameron-Jones, 1993), FOCL (Pazzani & Kibler, 1992), and GOLEM
(Muggleton & Feng, 1992).
Finally, using extended instances addresses the following technical problem. The learning problems considered in this paper involve restricted classes of logic programs. Often, the
restrictions imply that the number of simple instances is polynomial; we note that with only
a polynomial-size domain, questions about pac-learnability are usually trivial. Requiring
learning algorithms to work over the domain of extended instances precludes trivial learning
techniques, however, as the number of extended instances of size n is exponential in n even
for highly restricted programs.

2.2 Restrictions on Logic Programs

In this paper, we will consider the learnability of various restricted classes of logic programs. Below we will dene some of these restrictions; however, we will rst introduce
some terminology.
If A B1 ^ : : : ^ Br is an (ordered) denite clause, then the input variables of the literal
Bi are those variables appearing in Bi which also appear in the clause A B1 ^ : : : ^ Bi,1 ;
all other variables appearing in Bi are called output variables . Also, if A B1 ^ : : : ^ Br is a
denite clause, then Bi is said to be a recursive literal if it has the same predicate symbol
and arity as A, the head of the clause.
504

Pac-Learning Recursive Logic Programs: Efficient Algorithms

2.2.1 Types of Recursion

The rst set of restrictions concern the type of recursion that is allowed in a program.
If every clause in a program has at most one recursive literal, then the program is linear
recursive . If every clause in a program has at most k recursive literals, then the program is
k-ary recursive . Finally, if every recursive literal in a program contains no output variables,
then we will say that the program is closed recursive.
2.2.2 Determinacy and Depth

The second set of restrictions are variants of restrictions originally introduced by Muggleton
and Feng (1992). If A B1 ^ : : : ^ Br is an (ordered) denite clause, the literal Bi is
determinate i for every possible substitution  that unies A with some fact e such that
DB ` B1  ^ : : : ^ Bi,1 

there is at most one maximal substitution  so that DB ` Bi . A clause is determinate
if all of its literals are determinate. Informally, determinate clauses are those that can be
evaluated without backtracking by a Prolog interpreter.
We also dene the depth of a variable appearing in a clause A B1 ^ : : : ^ Br as follows.
Variables appearing in the head of a clause have depth zero. Otherwise, let Bi be the rst
literal containing the variable V , and let d be the maximal depth of the input variables of
Bi ; then the depth of V is d +1. The depth of a clause is the maximal depth of any variable
in the clause.
Muggleton and Feng dene a logic program to be ij -determinate if it is is determinate,
of constant depth i, and contains literals of arity j or less. In this paper we use the phrase
\constant-depth determinate" instead to denote this class of programs. Below are some
examples of constant-depth determinate programs, taken from Dzeroski, Muggleton and
Russell (1992).

Example. Assuming successor is functional, the following program is determinate. The maximum depth of a variable is one, for the variable C in the second
clause, and hence the program is of depth one.
less than(A,B)
less than(A,B)

successor(A,B).
successor(A,C) ^ less than(C,B).
!
A
The following program, which computes C , is determinate and of depth
two.
choose(A,B,C)
zero(B) ^
one(C).
choose(A,B,C)
decrement(B,D) ^
decrement(A,E) ^
505

Cohen

multiply(B,C,G) ^
divide(G,A,F) ^
choose(E,D,F).
The program GOLEM (Muggleton & Feng, 1992) learns constant-depth determinate
programs, and related restrictions have been adopted by several other practical learning
systems (Quinlan, 1991; Lavrac & Dzeroski, 1992; Cohen, 1993c). The learnability of
constant-depth determinate clauses has also received some formal study, which we will
review in Section 6.
2.2.3 Mode Constraints and Declarations

We dene the mode of a literal L appearing in a clause C to be a string s such that the initial
character of s is the predicate symbol of L, and for j > 1 the j -th character of s is a \+" if
the (j , 1)-th argument of L is an input variable and a \," if the (j , 1)-th argument of L
is an output variable. (This denition coincides with the usual denition of Prolog modes
only when all arguments to the head of a clause are inputs. This simplication is justied,
however, as we are considering only how clauses behave in classifying extended instances,
which are ground.) A mode constraint is simply a set of mode strings R = fs1 ; : : :; sk g, and
a clause C is said to satisfy a mode constraint R for p if for every literal L in the body of
C , the mode of L is in R.

Example. In the following append program, every literal has been annotated
with its mode.

append(Xs,Ys,Ys)
null(Xs).
append(Xs,Ys,Zs)
components(Xs,X,Xs1) ^
components(Zs,X,Zs1) ^
append(Xs1,Ys,Zs1).

% mode: null+
% mode: components + ,,
% mode: components + +,
% mode: append + ++

The clauses of this program satisfy the following mode constraint:
f components + ,,; components + +,; components + ,+;
components , ++; components + ++; null +
append + +,;
append + ,+;
append , ++;
append + ++
g
Mode constraints are commonly used in analyzing Prolog code; for instance, they are
used in many Prolog compilers. We will sometimes use an alternative syntax for mode
constraints that parallels the syntax used in most Prolog systems: for instance, we may
write the mode constraint \components + ,," as \components (+; ,; ,)".
We dene a declaration to be a tuple (p; a0; R) where p is a predicate symbol, a0 is an
integer, and R is a mode constraint. We will say that a clause C satises a declaration if
the head of C has arity a0 and predicate symbol p, and if for every literal L in the body of
C the mode of L appears in R.
506

Pac-Learning Recursive Logic Programs: Efficient Algorithms

2.3 A Model of Learnability

In this section, we will present our model of learnability. We will rst review the necessary
denitions for a standard learning model, the model of learning from equivalence queries
(Angluin, 1988, 1989), and discuss its relationship to other learning models. We will then
introduce an extension to this model which is necessary for analyzing ILP problems.
2.3.1 Identification From Equivalence Queries

Let X be a set. We will call X the domain , and call the elements of X instances . Dene a
concept C over X to be a representation of some subset of X , and dene a language Lang
to be a set of concepts. In this paper, we will be rather casual about the distinction between
a concept and the set it represents; when there is a risk of confusion we will refer to the set
represented by a concept C as the extension of C . Two concepts C1 and C2 with the same
extension are said to be (semantically) equivalent .
Associated with X and Lang are two size complexity measures , for which we will use
the following notation:

 The size complexity of a concept C 2 Lang is written j C j .
 The size complexity of an instance e 2 X is written j ej .
 If S is a set, Sn stands for the set of all elements of S of size complexity no greater
than n. For instance, Xn = fe 2 X : j ej  ng and Langn = fC 2 Lang : j C j  ng.
We will assume that all size measures are polynomially related to the number of bits needed
to represent C or e.
The rst learning model that we consider is the model of identication with equivalence
queries . The goal of the learner is to identify some unknown target concept C 2 Lang|
that is, to construct some hypothesis H 2 Lang such that H  C . Information about the
target concept is gathered only through equivalence queries . The input to an equivalence
query for C is some hypothesis H 2 Lang. If H  C , then the response to the query is
\yes". Otherwise, the response to the query is an arbitrarily chosen counterexample |an
instance e that is in the symmetric dierence of C and H .
A deterministic algorithm Identify identies Lang from equivalence queries i for
every C 2 Lang, whenever Identify is run (with an oracle answering equivalence queries
for C ) it eventually halts and outputs some H 2 Lang such that H  C . Identify
polynomially identies Lang from equivalence queries i there is a polynomial poly (nt; ne )
such that at any point in the execution of Identify the total running time is bounded by
poly (nt ; ne ), where nt = j C j and ne is the size of the largest counterexample seen so far, or
0 if no equivalence queries have been made.
2.3.2 Relation to Pac-Learnability

The model of identication from equivalence queries has been well-studied (Angluin, 1988,
1989). It is known that if a language is learnable in this model, then it is also learnable
in Valiant's (1984) model of pac-learnability. (The basic idea behind this result is that
an equivalence query for the hypothesis H can be emulated by drawing a set of random
507

Cohen

examples of a certain size. If any of them is a counterexample to H , then one returns
the found counterexample as the answer to the equivalence query. If no counterexamples
are found, one can assume with high condence that H is approximately equivalent to the
target concept.) Thus identication from equivalence queries is a strictly stronger model
than pac-learnability.
Most existing positive results on the pac-learnability of logic programs rely on showing
that every concept in the target language can be emulated by a boolean concept from
some pac-learnable class (Dzeroski et al., 1992; Cohen, 1994). While such results can be
illuminating, they are also disappointing, since one of the motivations for considering rstorder representations in the rst place is that they allow one to express concepts that cannot
be easily expressed in boolean logic. One advantage of studying the exact identication
model and considering recursive programs is that it essentially precludes use of this sort of
proof technique: while many recursive programs can be approximated by boolean functions
over a xed set of attributes, few can be be exactly emulated by boolean functions.
2.3.3 Background Knowledge in Learning

The framework described above is standard, and is one possible formalization of the usual
situation in inductive concept learning, in which a user provides a set of examples (in
this case counterexamples to queries) and the learning system attempts to nd a useful
hypothesis. However, in a typical ILP system, the setting is slightly dierent, as usually
the user provides clues about the target concept in addition to the examples. In most ILP
systems the user provides a database DB of \background knowledge" in addition to a set
of examples; in this paper, we will assume that the user also provides a declaration. To
account for these additional inputs it is necessary to extend the framework described above
to a setting where the learner accepts inputs other than training examples.
To formalize this, we introduce the following notion of a \language family". If Lang is
a set of clauses, DB is a database and Dec is a declaration, we will dene Lang[DB ; Dec]
to be the set of all pairs (C; DB ) such that C 2 Lang and C satises Dec . Semantically,
such a pair will denote the set of all extended instances (f; D) covered by (C; DB ). Next,
if DB is a set of databases and DEC is a set of declarations, then dene
Lang[DB ; DEC ] = fLang[DB ; Dec ] : DB

2 DB and Dec 2 DECg

This set of languages is called a language family .
We will now extend the denition of identication from equivalence queries to language families as follows. A language family Lang[DB; DEC ] is identiable from equivalence
queries i every language in the set is identiable from equivalence queries. A language
family Lang[DB; DEC ] is uniformly identiable from equivalence queries i there is a single
algorithm Identify (DB ; Dec) that identies any language Lang[DB ; Dec ] in the family
given DB and Dec .
Uniform polynomial identiability of a language family is dened analogously:
Lang[DB; DEC ] is uniformly polynomially identiable from equivalence queries i there is a
polynomial time algorithm Identify (DB ; Dec ) that identies any language Lang[DB ; Dec]
in the family given DB and Dec . Note that Identify must run in time polynomial in the
size of the inputs Dec and DB as well as the target concept.
508

Pac-Learning Recursive Logic Programs: Efficient Algorithms

2.3.4 Restricted Types of Background Knowledge

We will now describe a number of restricted classes of databases and declarations.
One restriction which we will make throughout this paper is to assume that all of the
predicates of interest are of bounded arity. We will use the notation a-DB for the set of all
databases that contain only facts of arity a or less, and the notation a-DEC for the set of
all declarations (p; a0; R) such that every string s 2 R is of length a + 1 or less.
For technical reasons, it will often be convenient to assume that a database contains an
equality predicate |that is, a predicate symbol equal such that equal (ti ; ti) 2 DB for every
constant ti appearing in DB , and equal (ti ; tj ) 62 DB for any ti 6= tj . Similarly, we will
often wish to assume that a declaration allows literals of the form equal(X,Y), where X
and Y are input variables. If DB (respectively DEC ) is any set of databases (declarations)
we will use DB = (DEC = ) to denote the corresponding set, with the additional restriction
that the database (declaration) must contain an equality predicate (respectively the mode
equal (+; +)).
It will sometimes also be convenient to assume that a declaration (p; a0; R) allows only
a single valid mode for each predicate: i.e., that for each predicate q there is in R only
a single mode constraint of the form q. Such a declaration will be called a unique-mode
declaration. If DEC is any set of declarations we will use DEC 1 to denote the corresponding
set of declarations with the additional restriction that the declaration is unique-mode.
Finally, we note that in a typical setting, the facts that appear in a database DB and
descriptions D of extended instances are not arbitrary: instead, they are representative of
some \real" predicate (e.g., the relationship of a list to its components in the example above).
One way of formalizing this is assume that all facts will be drawn from some restricted set F ;
using this assumption one can dene the notion of a determinate mode . If f = p(t1 ; : : :; tk )
is a fact with predicate symbol p and p is a mode, then dene inputs (f; p) to be the
tuple hti1 ; : : :; tik i, where i1, : : : , ik are the indices of  containing a \+". Also dene
outputs (f; p) to be the tuple htj1 ; : : :; tjl i, where j1 , : : : , jl are the indices of  containing
a \,". A mode string p for a predicate p is determinate for F i the relation

fhinputs (f; p); outputs (f; p)i : f 2 Fg
is a function. Informally, a mode is determinate if the input positions of the facts in F
functionally determine the output positions.
The set of all declarations containing only modes determinate for F will be denoted
DetDEC F . However, in this paper, the set F will be assumed to be xed, and thus we will
generally omit the subscript.
A program consistent with a determinate declaration Dec 2 DetDEC must be determinate, as dened above; in other words, consistency with a determinate declaration is a
sucient condition for semantic determinacy. It is also a condition that can be veried with
a simple syntactic test.
2.3.5 Size Measures for Logic Programs

Assuming that all predicates are arity a or less for some constant a also allows very simple
size measures to be used. In this paper, we will measure the size of a database DB by its
cardinality; the size of an extended instance (f; D) by the cardinality of D; the size of a
509

Cohen

declaration (p; a0; R) by the cardinality of R; and the size of a clause A B1 ^ : : : ^ Br by
the number of literals in its body.

3. Learning a Nonrecursive Clause

The learning algorithms presented in this paper all use a generalization technique which
we call forced simulation. By way of an introduction to this technique, we will consider a
learning algorithm for non-recursive constant-depth clauses. While this result is presented
primarily for pedagogical reasons, it may be of interest on its own: it is independent of
previous proofs of the pac-learnability of this class (Dzeroski et al., 1992), and it is also
somewhat more rigorous than previous proofs.
Although the details and analysis of the algorithm for non-recursive clauses are somewhat involved, the basic idea behind the algorithm is quite simple. First, a highlyspecic \bottom clause" is constructed, using two operations that we call DEEPEN and
CONSTRAIN . Second, this bottom clause is generalized by deleting literals so that it covers the positive examples: the algorithm for generalizing a clause to cover an example is
(roughly) to simulate the clause on the example, and delete any literals that would cause
the clause to fail. In the remainder of this section we will describe and analyze this learning
algorithm in detail.

3.1 Constructing a \Bottom Clause"

Let Dec = (p; a0; R) be a declaration and let A B1 ^ : : : ^ Br be a denite clause. We
dene
^
DEEPEN Dec (A B1 ^ : : : ^ Br )  A B1 ^ : : : ^ Br ^ (
Li )
Li 2LD

where LD is a maximal set of literals Li that satisfy the following conditions:
 the clause A B1 ^ : : : ^ Br ^ Li satises the mode constraints given in R;
 if Li 2 LD has the same mode and predicate symbol as some other Lj 2 LD , then the
input variables of Li are dierent from the input variables of Lj ;
 every Li has at least one output variable, and the output variables of Li are all
dierent from each other, and are also dierence from the output variables of any
other Lj 2 LD .
As an extension of this notation, we dene DEEPEN iDec (C ) to be the result of applying
the function DEEPEN Dec repeatedly i times to C , i.e.,
(
if i = 0
i
DEEPEN Dec (C )  C
i,
1
DEEPEN Dec (DEEPEN Dec (C )) otherwise
We dene the function CONSTRAIN Dec as
^
CONSTRAIN Dec (A B1 ^ : : : ^ Br )  A B1 ^ : : : ^ Br ^ (
Li )
Li 2LC

where LC is the set of all literals Li such that A B1 ^ : : : ^ Br ^ Li satises the mode
constraints given in R, and Li contains no output variables.
510

Pac-Learning Recursive Logic Programs: Efficient Algorithms

Example. Let D0 be the declaration (p; 2; R) where R contains the mode
constraints mother (+; ,), father (+; ,), male (+), female (+), and equal (+; +).

Then

DEEPEN D0(p(X,Y) ) 
p(X,Y) mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)
DEEPEN 2D0(p(X,Y) )  DEEPEN D0 (DEEPEN D0 (p(X,Y) )) 
p(X,Y)
mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)^
mother(XM,XMM)^father(XM,XMF)^ mother(XF,XFM)^father(XF,XFF)^
mother(YM,YMM)^father(YM,YMF)^ mother(YF,YFM)^father(YF,YFF)
CONSTRAIN D0(DEEPEN D0(p(X,Y) )) 
p(X,Y)
mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)^
male(X)^female(X)^male(Y)^female(Y)^
male(XM)^female(XM)^male(XF)^female(XF)^
male(YM)^female(YM)^male(YF)^female(YF)^
equal(X,X)^equal(X,XM)^equal(X,XF)^
equal(X,Y)^equal(X,YM)^equal(X,YF)^
equal(XM,X)^equal(XM,XM)^equal(XM,XF)^
equal(XM,Y)^equal(XM,YM)^equal(XM,YF)^
equal(XF,X)^equal(XF,XM)^equal(XF,XF)^
equal(XF,Y)^equal(XF,YM)^equal(XF,YF)^
equal(Y,X)^equal(Y,XM)^equal(Y,XF)^
equal(Y,Y)^equal(Y,YM)^equal(Y,YF)^
equal(YM,X)^equal(YM,XM)^equal(YM,XF)^
equal(YM,Y)^equal(YM,YM)^equal(YM,YF)^
equal(YF,X)^equal(YF,XM)^equal(YF,XF)^
equal(YF,Y)^equal(YF,YM)^equal(YF,YF)

Let us say that clause C1 is a subclause of clause C2 if the heads of C1 and C2 are
identical, if every literal in the body of C1 also appears in C2 , and if the literals in the
body of C1 appear in the same order as they do in C2. The functions DEEPEN and
CONSTRAIN allow one to easily describe a clause with an interesting property.
Theorem 1 Let Dec = (p; a0; R) be a declaration in a-DetDEC =, let X1; : : :; Xa be distinct
variables, and dene the clause BOTTOM d as follows:
BOTTOM d (Dec )  CONSTRAIN Dec (DEEPEN dDec (p(X1; : : :; Xa ) ))
For any constants d and a, the following are true:
 the size of BOTTOM d(Dec) is polynomial in j Decj ;
 every depth-d clause that satises Dec (and hence, is determinate) is (semantically)
equivalent to some subclause of BOTTOM d (Dec ).
0

0

511

Cohen

begin algorithm Force1NR (d ; Dec; DB ):

% below BOTTOM d is the most specic possible clause
let H BOTTOM d(Dec)

repeat

Ans answer to the query \Is H correct?"
if Ans =\yes" then return H
elseif Ans is a negative example then
return \no consistent hypothesis"
elseif Ans is a positive example e+ then
% generalize H minimally to cover e+
let (f; D) be the components of the extended instance e+
H ForceSimNR (H ; f ; Dec; (DB [ D ))
if H = FAILURE then
return \no consistent hypothesis"

end

endif
endif
endrepeat

Figure 1: A learning algorithm for nonrecursive depth-d determinate clauses

Proof: See Appendix A. A related result also appears in Muggleton and Feng (1992).
Example. Below C1 and D1 are equivalent, as are C2 and D2. Notice that D1
and D2 are subclauses of BOTTOM 1 (D0).

C1 : p(A,B) mother(A,C)^father(A,D)^ mother(B,C)^father(B,D)^male(A)
D1 : p(X,Y) mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)^
male(X)^equal(XM,YM)^equal(XF,YF)
C2 : p(A,B) father(A,B)^female(A)
D2 : p(X,Y) father(X,XF)^female(X)^equal(XF,Y)
For C1 and D1, p(X,Y) is true when X is Y 's brother. For C2 and D2, p(X,Y)
is true when X is Y 's daughter, and Y is X 's father.

3.2 The Learning Algorithm

Theorem 1 suggests that it may be possible to learn non-recursive constant-depth determinate clauses by searching the space of subclauses of BOTTOM d in some ecient
manner. Figures 1 and 2 present an algorithm called Force1 NR that does this when Dec is
a unique-mode declaration.
Figure 1 presents the top-level learning algorithm, Force1 NR . Force1 NR takes as
input a database DB and a declaration Dec , and begins by hypothesizing the clause
BOTTOM d (Dec ). After each positive counterexample e+ , the current hypothesis is generalized as little as possible in order to cover e+ . This strategy means that the hypothesis is
512

Pac-Learning Recursive Logic Programs: Efficient Algorithms

begin subroutine ForceSimNR(H ; f ; Dec; DB ):

% \forcibly simulate" H on fact f
if f 2 DB then return H
elseif the head of H and f cannot be unied then
return FAILURE

else

let H 0 H
let  be the mgu of f and the head of H 0
for each literal L in the body of H 0 do
if there is a substitution 0 such that L0 2 DB then
   0, where  0 is the most general such substitution
else
delete L from the body of H 0 , together with
all literals L0 supported (directly or indirectly) by L

end

endif
endfor
return H 0
endif

Figure 2: Forced simulation for nonrecursive depth-d determinate clauses
always the least general hypothesis that covers the positive examples; hence, if a negative
counterexample e, is ever seen, the algorithm will abort with a message that no consistent
hypothesis exists.
To minimally generalize a hypothesis H , the function ForceSimNR is used. This subroutine is shown in Figure 2. In the gure, the following terminology is used. If some
output variable of L is an input variable of L0 , then we say that L directly supports L0. We
will say that L supports L0 i L directly supports L0, or if L directly supports some literal
L00 that supports L0. (Thus \supports" is the transitive closure of \directly supports".)
ForceSim NR deletes from H the minimal number of literals necessary to let H cover e+ . To
do this, ForceSim NR simulates the action of a Prolog interpreter in evaluating H , except
that whenever a literal L in the body of H would fail, that literal is deleted, along with all
literals L0 supported by L.
The idea of learning by repeated generalization is an old one; in particular, previous
methods exist for learning a denite clause by generalizing a highly-specic one. For example, CLINT (De Raedt & Bruynooghe, 1992) generalizes a \starting clause" guided
by queries made to the user; PROGOL (Srinivasan, Muggleton, King, & Sternberg, 1994)
guides a top-down generalization process with a known bottom clause; and Rouveirol (1994)
describes a method for generalizing bottom clauses created by saturation. The Force1 NR algorithm is thus of interest not for its novelty, but because it is provably correct and ecient,
as noted in the theorem below.
513

Cohen

In particular, let d-DepthNonRec be the language of nonrecursive clauses of depth
d or less (and hence i-DepthNonRec[DB; j -DetDEC ] is the language of nonrecursive ij determinate clauses). We have the following result:

Theorem 2 For any constants a and d, the language family
d-DepthNonRec[DB= ; a-DetDEC =1]
is uniformly identiable from equivalence queries.

Proof: We will show that Force1 NR uniformly identies this language family with a polyno-

mial number of queries. We begin with the following important lemma, which characterizes
the behavior of ForceSimNR .

Lemma 3 Let Dec declaration in DetDEC =1 , let DB be a database, let f be a fact, and let

H be a determinate nonrecursive clause that satises Dec. Then one of following conditions

must hold:
 ForceSimNR(H ; f ; Dec; DB ) returns FAILURE, and no subclause H 0 of H satises
both Dec and the constraint H 0 ^ DB ` f ; or,
 ForceSimNR(H ; f ; Dec; DB ) returns a clause H 0, and H 0 is the unique syntactically
largest subclause of H that satises both Dec and the constraint H 0 ^ DB ` f .

Proof of lemma: To avoid repetition, we will refer to the syntactically maximal subclauses
H 0 of H that satisfy both Dec and the constraint H 0 ^ DB ` f as \admissible subclauses"

in the proof below.
Clearly the lemma is true if H or FAILURE is returned by ForceSim NR . In the remaining
cases the for loop of the algorithm is executed, and we must establish these two claims
(under the assumptions that A and f unify, and that f 62 DB ):
Claim 1. If L is retained, then every admissible subclause contains L.
Claim 2. If L is deleted, then no admissible subclause contains L.
First, however, observe that deleting a literal L may cause the mode of some other
literals to violate the mode declarations of Dec . It is easy to see that if L is deleted from
a clause C , then the mode of all literals L0 directly supported by L will change. Thus if C
satises a unique-mode declaration prior to the deletion of L, then after the deletion of L
all literals L0 that are directly supported by L will have invalid modes.
Now, to see that Claim 1 is true, suppose instead that it is false. Then there must
be some maximal subclause C 0 of H that satises Dec , covers the fact f , and does not
contain L. By the argument above, if C 0 does not contain L but satised Dec , then C 0
contains no literals L0 from H that are supported by L. Hence the output variables of L
are disjoint from the variables appearing in C 0. This means that if L were to be added to
C 0 the resulting clause would still satisfy Dec and cover f , which leads to a contradiction
since C 0 was assumed to be maximal.
To verify Claim 2, let us introduce the following terminology. If C = (A B1 ^ : : : ^ Br )
is a clause and DB is a database, we will say that the substitution  is a (DB ; f )-witness
514

Pac-Learning Recursive Logic Programs: Efficient Algorithms

for C i  is associated with a proof that C ^ DB ` f (or more precisely, i A = f and
8i : 1  i  r; Bi 2 DB .) We claim that the following condition is an invariant of the for
loop of the ForceSim NR algorithm.
Invariant 1. Let C be any admissible subclause that contains all the literals in H 0 preceding L (i.e., that contains all those literals of H that were retained on previous
iterations of the algorithm). Then every (DB ; f )-witness for C is a superset of  .
This can be easily established by induction on the number of iterations of the for loop. The
condition is true when the loop is rst entered, since  is initially the most general unier
of A and f . The condition remains true after an iteration in which L is deleted, since 
is unchanged. Finally, the condition remains true after an iteration in which L is retained:
because  0 is maximally general, it may only assign values to the output variables of L, and
by determinacy only one assignment to the output variables of L can make L true. Hence
every (DB ; f )-witness for C must contain the bindings in  .
Next, with an inductive argument and Claim 1 one can show that every admissible
subclause C must contain all the literals that have been retained in previous iterations of
the loop, leading to the following strengthening of Invariant 1:
Invariant 10. Let C be any admissible subclause. Then every (DB ; f )-witness for C is a
superset of  .
Now, notice that only two types of literals are deleted: (a) literals L such that no superset
of  can make L true, and (b) literals L0 that are supported by a literal L of the preceding
type. In case (a), clearly L cannot be part of any admissible subclause, since no superset
of  makes L succeed, and only such supersets can be witnesses of admissible clauses. In
case (b), again L0 cannot be part of any admissible subclause, since its declaration is invalid
unless L is present in the clause, and by the argument above L cannot be in the clause.
This concludes the proof of the lemma.
To prove the theorem, we must now establish the following properties of the identication
algorithm.
Correctness. By Theorem 1, if the target program is in d-DepthNonRec[DB ; Dec],
then there is some clause CT that is equivalent to the target, and is a subclause of
BOTTOM d (Dec ). H is initially BOTTOM  d and hence a superclause of CT . Now consider
invoking ForceSim NR on any positive counterexample e+ . By Lemma 3, if this invocation
is successful, H will be replaced by H 0, the longest subclause of H that covers e+ . Since
CT is a subclause of H that covers e+ , this means that H 0 will again be a superclause of
CT . Inductively, then, the hypothesis is always a superclause of the target.
Further, since the counterexample e+ is always an instance that is not covered by the
current hypothesis H , every time the hypothesis is updated, the new hypothesis is a proper
subclause of the old. This means that Force1 NR will eventually identify the target clause.
Eciency. The number of queries made is polynomial in j Decj and j DB j , since H is
initially of size polynomial in j Dec j , and is reduced in size each time a counterexample is
provided. To see that each counterexample is processed in time polynomial in nr , ne , and
nt, notice that since the length of H is polynomial, the number of repetitions of the for
loop of ForceSim NR is also polynomial; further, since the arity of literals L is bounded by
515

Cohen

a, only anb + ane constants exist in DB [ D, and hence there are at most (anb + ane )a
substitutions  0 to check inside the for loop, which is again polynomial. Thus each execution

of ForceSim NR requires only polynomial time.
This concludes the proof.

4. Learning a Linear Closed Recursive Clause

Recall that if a clause has only one recursive literal, then the clause is linear recursive ,
and that if no recursive literal contains output variables, then the clause is closed linear
recursive. In this section, we will describe how the Force1 algorithm can be extended to
learn a single linear closed recursive clause.2 Before presenting the extension, however, we
would rst like to discuss a reasonable-sounding approach that, on closer examination, turns
out to be incorrect.

4.1 A Remark on Recursive Clauses

One plausible rst step toward extending Force1 to recursive clauses is to allow recursive
literals in hypotheses, and treat them the same way as other literals|that is, to include
recursive literals in the initial clause BOTTOM d , and delete these literals gradually as
positives examples are received. A problem with this approach is that there is no simple
way to check if a recursive literal in a clause succeeds or fails on a particular example. This
makes it impossible to simply run ForceSimNR on clauses containing recursive literals.
A straightforward (apparent) solution to this problem is to assume that an oracle exists
which can be queried as to the success or failure of any recursive literal. For closed recursive
clauses, it is sucient to assume that there is an oracle MEMBERCt (DB ; f ) that answers
the question
Does DB ^ P ` f ?
where Ct is the unknown target concept, f is a ground fact, and DB is a database. Given
such an oracle, one can determine if a closed recursive literal Lr should be retained by
checking if MEMBERCT (DB ; Lr  ) is true. Such an oracle is very close to the notion of a
membership query as used in computational learning theory.
This is a natural extension of the Force1NR learning algorithm to recursive clauses|in
fact an algorithm based on similar ideas has been been previously conjectured to pac-learn
closed recursive constant-depth determinate clauses (Dzeroski et al., 1992). Unfortunately,
this algorithm can fail to return a clause that is consistent with a positive counterexample.
To illustrate this, consider the following example.

Example. Consider using the extension of Force1NR described above to learn
following target program:
append(Xs,Ys,Zs)
2. The reader may object that useful recursive programs always have at least two clauses|a recursive
clause and a nonrecursive base case. In posing the problem of learning a single recursive clause, we are
thus assuming the non-recursive \base case" of the target program is provided as background knowledge,
either in the background database DB , or in the description atoms D of extended instances.

516

Pac-Learning Recursive Logic Programs: Efficient Algorithms

components(Xs,X,Xs1),
components(Zs,Z,Zs1),
X1=Z1,
append(Xs1,Ys,Zs1).
This program is determinate, has depth 1, and satises the following set of
declarations:
components(+,,,,).
null(+).
equal(+,+).
odd(+).
append(+,+,+).
We will assume also a database DB that denes the predicate null to be true
for empty lists, and odd to be true for the constants 1 and 3.
To see how the forced simulation can fail, consider the following positive instance
e = (f; D):

f = append (l12 ; l3 ; l123 )
D = f cons(l123,1,l23), cons(l23,2,l3), cons(l3,3,nil),
cons(l12,1,l2), cons(l2,2,nil),
append(nil,l3,l3) g

This is simply a \attened" form of append([1,2],[3],[1,2,3]), together with the
appropriate base case append([],[3],[3]). Now consider beginning with the clause
BOTTOM 1 and generalizing it using ForceSimNR to cover this positive instance.
This process is illustrated in Figure 3. The clause on the left in the gure is
BOTTOM d (Dec ); the clause on the right is the output of forcibly simulating
this clause on f with ForceSimNR . (For clarity we've assumed that only the
single correct recursive call remains after forced simulation.)
The resulting clause is incorrect, in that it does not cover the given example e.
This can be easily seen by stepping through the actions of a Prolog interpreter
with the generalized clause of Figure 3. The nonrecursive literals will all succeed, leading to the subgoal append(l2,l3,l23) (or in the usual Prolog notation,
append([2],[3],[2,3])). This subgoal will fail at the literal odd(X1), because X1
is bound to 2 for this subgoal, and the fact odd(2) is not true in DB [ D.
This example illustrates a pitfall in the policy of treating recursive and non-recursive
literals in a uniform manner (For more discussion, see also (Bergadano & Gunetti, 1993; De
Raedt, Lavrac, & Dzeroski, 1993).) Unlike nonrecursive literals, the truth of the fact Lr 
(corresponding to the recursive literal Lr ) does not imply that a clause containing Lr will
succeed; it may be that while the rst subgoal Lr  succeeds, deeper subgoals fail.
517

Cohen

BOTTOM 1 (Dec ):
ForceSimNR (BOTTOM 1(Dec); f; Dec; DB [ D) :
append(Xs,Ys,Zs)
append(Xs,Ys,Zs)
components(Xs,X1,Xs1)^
components(Xs,X1,Xs1)^
components(Ys,Y1,Ys1)^
components(Ys,Y1,Ys1)^
components(Zs,Z1,Zs1)^
components(Zs,Z1,Zs1)^
null(Xs)^
null(Ys1)^
null(Ys)^
equal(X1,Z1)^
..
odd(X1)^
.
odd(Y1)^
null(Ys1)^
odd(Z1)^
null(Zs1),
append(Xs1,Ys,Zs1).
equal(Xs,Xs)^
..
.
equal(X1,Z1)^
..
.
equal(Zs1,Zs1)^
odd(Xs)^
..
.
odd(X1)^
odd(Y1)^
odd(Z1)^
..
.
odd(Zs1)^
append(Xs,Xs,Xs)^
..
.
append(Zs1,Zs1,Zs1).

Figure 3: A recursive clause before and after generalization with ForceSimNR

4.2 Forced Simulation for Recursive Clauses
A solution to this problem is to replace the calls to the membership oracle in the algorithm
sketched above with a call to a routine that forcibly simulates the actions of a top-down
theorem-prover on a recursive clause. In particular, the following algorithm is suggested.
First, build a nonrecursive \bottom clause", as was done in ForceSimNR . Second, nd some
recursive literal Lr such that appending Lr to the bottom clause yields a recursive clause
that can be generalized to cover the positive examples.
As in the nonrecursive case, a clause is generalized by deleting literals, using a straightforward generalization of the procedure for forced simulation of nonrecursive clauses. During
forced simulation, any failing nonrecursive subgoals are simply deleted; however, when a
recursive literal Lr is encountered, one forcibly simulates the hypothesis clause recursively
518

Pac-Learning Recursive Logic Programs: Efficient Algorithms

begin subroutine ForceSim (H ; f ; Dec; DB ; h ):

% \forcibly simulate" recursive clause H on f
% 1. check for innite loops
if h < 0 then return FAILURE
% 2. check to see if f is already covered
elseif f 2 DB then return H
% 3. check to see if f cannot be covered
elseif the head of H and f cannot be unied then
return FAILURE

else

let Lr be the recursive literal of H
let H 0 H , fLrg

% 4. delete failing non-recursive literals as in ForceSimNR
let A be the head of H 0
let  be the mgu of A and e
for each literal L in the body of H 0 do
if there is a substitution 0 such that L0 2 DB
then    0, where 0 is the most general such substitution

else

delete L from the body of H 0 , together with
all literals L0 supported (directly or indirectly) by L

endif
endfor

% 5. generalize H 0 on the recursive subgoal Lr 
if Lr  is ground then return ForceSim(H 0 [ fLr g; Lr; Dec; DB ; h , 1)
else return FAILURE

end

endif
endif

Figure 4: Forced simulation for linear closed recursive clauses

519

Cohen

on the corresponding recursive subgoal. An implementation of forced simulation for linear
closed recursive clauses is shown in Figure 4.
The extended algorithm is similar to ForceSimNR , but diers in that when the recursive
literal Lr is reached in the simulation of H , the corresponding subgoal Lr  is created, and
the hypothesized clause is recursively forcibly simulated on this subgoal. This ensures that
the generalized clause will also succeed on the subgoal. For reasons that will become clear
shortly, we would like this algorithm to terminate, even if the original clause H enters an
innite loop when used in a top-down interpreter. In order to ensure termination, an extra
argument h is passed to ForceSim . The argument h represents a depth bound for the forced
simulation.
To summarize, the basic idea behind the algorithm of Figure 4 is to simulate the hypothesized clause H on f , and generalize H by deleting literals whenever H would fail on
f or on any subgoal of f .

Example.

Consider using ForceSim to forcibly simulate the following recursive clause
BOTTOM 1(Dec ) [ Lr
append(Xs,Ys,Zs)
components(Xs,X1,Xs1)^components(Ys,Y1,Ys1)^components(Zs,Z1,Zs1)^
null(Xs)^: : : ^null(Zs1)^
odd(Xs)^: : : ^odd(Zs1)^
equal(Xs,Xs)^: : : ^equal(Zs1,Zs1)^
append(Xs1,Ys,Zs1)
Here the recursive literal Lr is append(Xs1,Ys,Zs1). We will also assume that f
is taken from the extended query e = (f; D), which is again the attened version
of the instance append([1,2],[3],[1,2,3]) used in the previous example; that Dec
is the set of declarations of in the previous example; and that the database DB
is D [ null (nul ).
After executing steps 1-4 of ForceSim, a number of failing literals are deleted,
leading to the substitution3  of fXs = [1; 2], Ys = [3], Zs = [1; 2; 3], X1 = 1,
Xs1 = [2], Y1 = 3, Ys1 = [], Z1 = 1, Zs1 = [2; 3]g and the following reduced
clause:
append(Xs,Ys,Zs)
components(Xs,X1,Xs1)^components(Ys,Y1,Ys1)^components(Zs,Z1,Zs1)^
null(Ys1)^odd(X1)^odd(Y1)^odd(Z1)^equal(X1,Z1)^
append(Xs1,Ys,Zs1)
Hence the recursive subgoal is

Lr  = append (Xs1 ; Ys ; Zs1 ) = append ([2]; [3]; [2; 3])
3. Note that for readability, we are using the term notation rather than the attened notation of Xs = l12,
Ys = l3, etc.

520

Pac-Learning Recursive Logic Programs: Efficient Algorithms

Recursively applying ForceSim to this goal produces the substitution fXs = [2],
Ys = [3], Zs = [2; 3], X1 = 2, Xs1 = [], Y1 = 3, Ys1 = [], Z1 = 2, Zs1 = [3]g
and also results in deleting the additional literals odd(X1) and odd(Z1). The
next recursive subgoal is Lr  = append ([]; [3]; [3]); since this clause is included
in the database DB , ForceSim will terminate. The nal clause returned by
ForceSim in this case is the following:
append(Xs,Ys,Zs)
components(Xs,X1,Xs1)^components(Ys,Y1,Ys1)^components(Zs,Z1,Zs1)^
null(Ys1)^odd(Y1)^equal(X1,Z1)^
append(Xs1,Ys,Zs1)
Notice that this clause does cover e.
As in Section 3 we begin our analysis by showing the correctness of the forced simulation
algorithm|i.e., by showing that forced simulation does indeed produce a unique maximally
specic generalization of the input clause that covers the example.
This proof of correctness uses induction on the depth of a proof. Let us introduce again
some additional notation, and write P ^ DB `h f if the Prolog program (P; DB ) can be
used to prove the fact f in a proof of depth h or less. (The notion of depth of a proof is the
usual one; we will dene looking up f in the database DB to be a proof of depth zero.) We
have the following result concerning the ForceSim algorithm.

Theorem 4 Let Dec be a declaration in DetDEC =1, let DB be a database, let f be a fact,

and let H be a determinate closed linear recursive clause that satises Dec. Then one of
the following conditions must hold:

 ForceSim(H; f; Dec; DB ; h) returns FAILURE, and no recursive subclause H 0 of H
satises both Dec and the constraint H 0 ^ DB `h f ; or,
 ForceSim(H; f; Dec; DB ; h) returns a clause H 0, and H 0 is the unique syntactically
largest recursive subclause of H that satises both Dec and the constraint H 0^DB `h f .

Proof: Again to avoid repetition, we will refer to syntactically maximal recursive (nonrecursive) subclauses H 0 of H that satisfy both Dec and the constraint H 0 ^ DB `h f as

\admissible recursive (nonrecursive) subclauses" respectively.
The proof largely parallels the proof of Lemma 3|in particular, similar arguments
show that the clause returned by ForceSim satises the conditions of the theorem whenever
FAILURE is returned and whenever H is returned. Note that the correctness of ForceSim
when H is returned establishes the base case of the theorem for h = 0.
For the case of depth h > 0, let us assume the theorem holds for depth h , 1 and
proceed using mathematical induction. The arguments of Lemma 3 show that the following
condition is true after the for loop terminates.

Invariant 10. H 0 is the unique maximal nonrecursive admissible subclause of H , and every
(DB ; f )-witness for H 0 is a superset of  .
521

Cohen

begin algorithm Force1 (d ; Dec; DB ):

% below BOTTOM d is the most specic possible clause
let Lr1 ; : : :; Lrp be all possible closed recursive literals for BOTTOM d(Dec)
choose an unmarked recursive literal Lri
let H BOTTOM d(Dec) [ fLri g

repeat

answer to the query \Is H correct?"

Ans

if Ans =\yes" then return H
elseif Ans is a negative example e, then
H

FAILURE

elseif Ans is a positive example e+ then

% generalize H minimally to cover e+
let (f; D) be the components of e+
H ForceSim (H ; f ; Dec; (DB [ D ); (a j Dj + a j DBj )a )
where a0 is the arity of the clause head as given in Dec
0

endif
if H = FAILURE then
if all recursive literals are marked then
return \no consistent hypothesis"
else

mark Lri
choose an unmarked recursive literal Lrj
let H BOTTOM d(Dec) [ fLrj g

end

endif
endif
endrepeat

Figure 5: A learning algorithm for nonrecursive depth-d determinate clauses
Now, let us assume that there is some admissible recursive subclause H . Clearly H  must
contain the recursive literal Lr of H , since Lr is the only recursive literal of H . Further,
the nonrecursive clause H^ = H  , fLr g must certainly satisfy Dec and also H^ ^ DB ` f ,
so it must (by the maximality of H 0) be a subclause of H 0. Hence H  must be a subclause
of H 0 [ fLr g. Finally, if Lr  is ground (i.e., if Lr is closed in the clause H 0 [ Lr ) then by
Invariant 10, the clause H  must also satisfy H  ^ DB ` Lr  by a proof of depth h , 1.
(This is simply equivalent to saying that the recursive subgoal of Lr  generated in the proof
must succeed.)
By the inductive hypothesis, then, the recursive call must return the unique maximal
admissible recursive subclause of H 0 [ Lr , which by the argument above must also be the
unique maximal admissible recursive subclause of H .
Thus by induction the theorem holds.
522

Pac-Learning Recursive Logic Programs: Efficient Algorithms

4.3 A Learning Algorithm for Linear Recursive Clauses

Given this method for generalizing recursive clauses, one can construct a learning algorithm for recursive clauses as follows. First, guess a recursive literal Lr , and make
H = BOTTOM d [ Lr the initial hypothesis of the learner. Then, ask a series of equivalence
queries. After a positive counterexample e+ , use forced simulation to minimally generalize
H to cover e+ . After a negative example, choose another recursive literal L0r , and reset the
hypothesis to H = BOTTOM d [ L0r .
Figure 5 presents an algorithm that operates along these lines. Let d-DepthLinRec
denote the language of linear closed recursive clauses of depth d or less. We have the
following result:
Theorem 5 For any constants a and d, the language family
d-DepthLinRec[DB=; a-DetDEC =1]
is uniformly identiable from equivalence queries.
Proof: We will show that Force1 uniformly identies this language family with a polynomial number of queries.
Correctness and query eciency. There are at most aj Dj + aj DBj constants in
any set DB [ D, at most (aj Dj + aj DB j )a a0 -tuples of such constants, and hence at most
(aj Dj + aj DB j )a distinct recursive subgoals Lr  that might be produced in proving that a
linear recursive clause C covers an extended instance (f; D). Thus every terminating proof
of a fact f using a linear recursive clause C must be of depth (aj Dj + aj DB j )a or less; i.e.,
for h = (aj Dj + aj DB j )a ,
C ^ DB ^ D `h f i C ^ DB ^ D ` f
Thus Theorem 4 can be strengthened: for the value of h used in Force1, the subroutine
ForceSim returns the syntactically largest subclause of H that covers the example (f; D)
whenever any such a subclause exists, and returns FAILURE otherwise.
We now argue the correctness of the algorithm as follows. Assume that the hypothesized recursive literal is \correct"|i.e., that the target clause CT is some subclause of
BOTTOM d [ Lr . In this case it is easy to see that Force1 will identify CT , using an argument that parallels the one made for Force1 NR . Again by analogy to Force1 NR , it is easy to
see that only a polynomial number of equivalence queries will be made involving the correct
recursive literal.
Next assume that Lr is not the correct recursive literal. Then CT need not be a subclause
of BOTTOM d [ Lr , and the response to an equivalence query may be either a positive or
negative counterexample. If a positive counterexample e+ is received and ForceSim is
called, then the result may be FAILURE, or it may be a proper subclause of H that covers
e+ . Thus the result of choosing an incorrect Lr will be a (possibly empty) sequence of
positive counterexamples followed by either a negative counterexample or FAILURE. Since
all equivalence queries involving the correct recursive literal will be answered by either
a positive counterexample or \yes"4, then if a negative counterexample or FAILURE is
obtained, it must be that Lr is incorrect.
0

0

0

0

4. Recall that an answer of \yes" to an equivalence query means the hypothesis is correct.

523

Cohen

The number of variables in BOTTOM d can be bounded by aj BOTTOM d (Dec )j , and
as each closed recursive literal is completely dened by an a0-tuple of variables, the number
of possible closed recursive literals Lr can be bounded by

p = (aj BOTTOM d (Dec )j )a

0

Since j BOTTOM d (Dec )j is polynomial in j Dec j , p is also polynomial in j Dec j . This means
that only a polynomial number of incorrect Lr 's need to be discarded. Further since each
successive hypothesis using a single incorrect Lr is a proper subclause of the previous hypothesis, only a polynomial number of equivalence queries are needed to discard an incorrect
Lr . Thus only a polynomial number of equivalence queries can be made involving incorrect
recursive literals.
Thus Force1 needs only a polynomial number of queries to identify Ct.
Eciency. ForceSim runs in time polynomial in its arguments H , f , Dec, DB [ D
and h. When ForceSim is called from Force1, h is always polynomial in ne and j DB j , and
H is always no larger than j BOTTOM d(Dec)j + 1, which in turn is polynomial in the size
of Dec . Hence every invocation of ForceSim requires time polynomial in ne , Dec , and DB ,
and hence Force1 processes each query in polynomial time.
This completes the proof.
This result is somewhat surprising, as it shows that recursive clauses can be learned
even given an adversarial choice of training examples. In contrast, most implemented ILP
systems require well-choosen examples to learn recursive clauses.
This formal result can also be strengthened in a number of technical ways. One of
the more interesting strengthenings is to consider a variant of Force1 that maintains a
xed set of positive and negative examples, and constructs the set of all least general
clauses that are consistent with these examples: this could be done by taking each of the
clauses BOTTOM d [ Lr1 , : : : , BOTTOM d [ Lrp , forcibly simulating them on each of the
positive examples in turn, and then discarding those clauses that cover one of more negative
examples. This set of clauses could then be used to tractably encode the version space of
all consistent programs, using the [S; N ] representation for version spaces (Hirsh, 1992).

5. Extending the Learning Algorithm

We will now consider a number of ways in which the result of Theorem 5 can be extended.

5.1 The Equality-Predicate and Unique-Mode Assumptions
Theorem 5 shows that the language family

d-DepthLinRec[DB=; a-DetDEC =1]
is identiable from equivalence queries. It is natural to ask if this result can be extended
by dropping the assumptions that an equality predicate is present and that the declaration
contains a unique legal mode for each predicate: that is, if the result can be extended to
the language family
d-DepthLinRec[DB; a-DetDEC ]
524

Pac-Learning Recursive Logic Programs: Efficient Algorithms

This extension is in fact straightforward. Given a database DB and a declaration Dec =
(p; a0; R) that do not satisfy the equality-predicate and unique-mode assumptions, one can
modify them as follows.
1. For every constant c appearing in DB , add the fact equal (c ; c ) to DB .
2. For every predicate q that has k valid modes qs1 , : : : , qsk in R:
(a) remove the mode declarations for q , and replace them with k mode strings for
the k new predicates qs1 , : : : , qsk , letting qsi si be the unique legal mode for the
predicate qsi ;
(b) remove every fact q (t1 ; : : :; ta) of the predicate q from DB , and replace it with
the k facts qs1 (t1 ; : : :; ta ), : : : , qsk (t1 ; : : :; ta).
Note that if the arity of predicates is bounded by a constant a, then the number of modes
k for any predicate q is bounded by the constant 2a , and hence these transformations can
be performed in polynomial time, and with only a polynomial increase in the size of Dec
and DB .
Clearly any target clause Ct 2 d-DepthLinRec[DB ; Dec ] is equivalent to some clause
Ct0 2 d-DepthLinRec[DB 0; Dec0], where DB 0 and Dec 0 are the modied versions of DB
and Dec constructed above. Using Force1 it is possible to identify Ct0 . (In learning Ct0, one
must also perform steps 1 and 2b above on the description part D of every counterexample
(f; D).) Finally, one can convert Ct0 to an equivalent clause in d-DepthLinRec[DB ; Dec]
by repeatedly resolving against the clause equal(X,X) , and also replacing every predicate
symbol qsi with q .
This leads to the following strengthening of Theorem 5:

Proposition 6 For any constants a and d, the language family
d-DepthLinRec[DB; a-DetDEC ]
is uniformly identiable from equivalence queries.

5.2 The Datalog Assumption

So far we have assumed that the target program contains no function symbols, and that the
background knowledge provided by the user is a database of ground facts. While convenient
for formal analysis, these assumptions can be relaxed.
Examination of the learning algorithm shows that the database DB is used in only two
ways.

 In forcibly simulating a hypothesis on an extended instance (f; D), it is necessary to
nd a substitution  0 that makes a literal L true in the database DB [ D. While this
can be done algorithmically if DB and D are sets of ground facts, it is also plausible
to assume that the user has provided an oracle that answers in polynomial time any
mode-correct query L to the database DB . Specically, the answer of the oracle will
be either
525

Cohen

{ the (unique) most-general substitution 0 such that DB ^ D ` L0 and L0 is
ground; or
{ \no" if no such 0 exists.

Such an oracle would presumably take the form of an ecient theorem-prover for DB .

 When calling ForceSim, the top-level learning algorithm uses DB and D to determine

a depth bound on the length of a proof made using the hypothesis program. Again,
it is reasonable to assume that the user can provide this information directly, in the
form of an oracle. Specically, this oracle would provide for any fact f a polynomial
upper bound on the depth of the proof for f in the target program.

Finally we note that if ecient (but non-ground) background knowledge is allowed, then
function symbols always can be removed via attening (Rouveirol, 1994). This transformation also preserves determinacy, although it may increase depth|in general, the depth of
a attened clause depends also on term depth in the original clause. Thus, the assumption
that the target program is in Datalog can be replaced by assumptions that the term depth
is bounded by a constant, and that two oracles are available: an oracle that answers queries
to the background knowledge, and a depth-bound oracle. Both types of oracles have been
frequently assumed in the literature (Shapiro, 1982; Page & Frisch, 1992; Dzeroski et al.,
1992).

5.3 Learning k-ary Recursive Clauses

It is also natural to ask if Theorem 5 can be extended to clauses that are not linear recursive.
One interesting case is the case of closed k-ary recursive clauses for constant k. It is
straightforward to extend Force1 to guess a tuple of k recursive literals Lr1 , : : : , Lrk , and
then to extend ForceSim to recursively generalize the hypothesis clause on each of the facts
Lr1  , : : : , Lrk  . The arguments of Theorems 4 and 5 can be modied to show that this
extension will identify the target clause after a polynomial number of equivalence queries.
Unfortunately, however, it is no longer the case that ForceSim runs in polynomial time.
This is easily seen if one considers a tree of all the recursive calls made by ForceSim; in
general, this tree will have branching factor k and polynomial depth, and hence exponential
size. This result is unsurprising, as the implementation of ForceSim described forcibly
simulates a depth-bounded top-down interpreter, and a k-ary recursive program can take
exponential time to interpret with such an interpreter.
There are at least two possible solutions to this problem. One possible solution is to
retain the simple top-down forced simulation procedure, and require the user to provide
a depth bound tighter than (aj Dj + aj DB j )a , the maximal possible depth of a tree. For
example, in learning a 2-ary recursive sort such as quicksort, the user might specify a logarithmic depth bound, thus guaranteeing that ForceSim is polynomial-time. This requires
additional input from the user, but would be easy to implement. It also has the advantage
(not shared by the approach described below) that the hypothesized program can be executed using a simple depth-bounded Prolog interpreter, and will always have shallow proof
trees. This seems to be a plausible bias to impose when learning k-ary recursive Prolog
programs, as many of these tend to have shallow proof trees.
0

526

Pac-Learning Recursive Logic Programs: Efficient Algorithms

A second solution to the possible high cost of forced simulation for k-ary recursive
programs is to forcibly simulate a \smarter" type of interpreter|one which can execute
k-ary recursive program in polynomial time.5 One sound and complete theorem-prover for
closed k-ary recursive programs can be implemented as follows.
Construct a top-down proof tree in the usual fashion, i.e., using a depth-rst left-to-right
strategy, but maintain a list of the ancestors of the current subgoal, and also a list VISITED
that records, for each previously visited node in the tree, the subgoal associated with that
node. Now, suppose that in the course of constructing the proof tree one generates a subgoal
f that is on the VISITED list. Since the traversal of the tree is depth-rst left-to-right, the
node associated with f is either an ancestor of the current node, or is a descendant of some
left sibling of an ancestor of the current node. In the former case, the proof tree contains
a loop, and cannot produce a successful proof; in this case the theorem-prover should exit
with failure. In the latter case, a proof must already exist for f 0 , and hence nodes below the
current node in the tree need not be visited; instead the theorem prover can simply assume
that f is true.
This top-down interpreter can be easily extended into a forced simulation procedure:
one simply traverses the tree in the same order, generalizing the current hypothesis H as
needed to justify each inference step in the tree. The only additional point to note is that
if one is performing forced simulation and revisits a previously proved subgoal f at a node
n, the current clause H need not be further generalized in order to prove f , and hence it is
again permissible to simply skip the portion of the tree below n. We thus have the following
result.

Theorem 7 Let d-Depth-k-Rec be the set of k-ary closed recursive clauses of depth d.
For any constants a, d, and k the language family
d-Depth-k-Rec[DB; a-DetDEC]
is uniformly identiable from equivalence queries.

Proof: Omitted, but following the informal argument made above.
Note that we give this result without the restrictions that the database contains an
equality relation and that the declaration is unique-mode, since the tricks used to relax
these restrictions in Proposition 6 are still applicable.

5.4 Learning Recursive and Base Cases Simultaneously

So far, we have analyzed the problem of learning single clauses: rst a single nonrecursive
clause, and then a single recursive clause. However, every useful recursive program contains
at least two clauses: a recursive clause, and a nonrecursive base case. It is natural to ask
if it is possible to learn a complete recursive program by simultaneously learning both a
recursive clause, and its associated nonrecursive base case.
In general, this is not possible, as is demonstrated elsewhere (Cohen, 1995). However,
there are several cases in which the positive result can be extended to two-clause programs.
5. Note that it is plausible to believe that such a theorem-prover exists, as there are only a polynomial
number of possible theorem-proving goals|namely, the (aj Dj + aj DB j )a possible recursive subgoals.
0

527

Cohen

begin algorithm Force2 (d ; Dec; DB ):
let Lr1 ; : : :; Lrp be all possible recursive literals for BOTTOM d(Dec)
choose an unmarked recursive literal Lri
let HR BOTTOM d(Dec) [ fLri g
let HB BOTTOM d(Dec)
let P = (HR; Hb)

repeat

Ans answer to query \Is HR ; HB correct?"
if Ans =\yes" then return HR ; HB
elseif Ans is a negative example e, then
P FAILURE
elseif Ans is a positive example e+ then
let (f; D) be the components of e+
P ForceSim2 (HR; HB ; f ; Dec; (DB [ D ); (a j Dj + a j DBj )a )
0

endif
if P = FAILURE then
if all recursive literals Lrj are marked then
return \no consistent hypothesis"
else

mark Lri
choose an unmarked recursive literal Lrj
let HR BOTTOM d(Dec) [ fLrj g
let HB BOTTOM d (Dec)
let P = (HR ; HB )

end

endif
endif
endrepeat

Figure 6: A learning algorithm for two-clause recursive programs

528

Pac-Learning Recursive Logic Programs: Efficient Algorithms

begin subroutine ForceSim2 (HR; HB ; f ; Dec; DB ; h ):

% \forcibly simulate" program HR ; HB on f
if h < 1 then return FAILURE
% check to see if f should be covered by HB
elseif BASECASE (f ) then
return current Hr and generalized HB
return (HR; ForceSimNR(HB ; f ; Dec; DB ))
elseif the head of HR and f cannot be unied then
return FAILURE

else

let Lr be the recursive literal of HR
let H 0 H , fLrg
let A be the head of H 0
let  be the mgu of A and e
for each literal L in the body of H 0 do
if there is a substitution 0 such that L0 2 DB
then    0, where 0 is the most general such substitution
else
delete L from the body of H 0 , together with
all literals L0 supported (directly or indirectly) by L

endif
endfor

% generalize H 0; HB on the recursive subgoal Lr 
if Lr  is ground then
% continue the simulation of the program
return ForceSim2(H 0 [ fLr g; HB; Lr; Dec; DB ; h , 1)
else return FAILURE

end

endif
endif

Figure 7: Forced simulation for two-clause recursive programs

529

Cohen

In this section, we will rst discuss learning a recursive clause and base clause simultaneously, assuming that any determinate base clause is possible, but also assuming that an
additional \hint" is available, in the form of a special \basecase" oracle. We will then
discuss various alternative types of \hints".
Let P be a target program with base clause CB and recursive clause CR. A basecase
oracle for P takes as input an extended instance (f; D) and returns \yes" if CB ^ DB ^ D ` f ,
and \no" otherwise. In other words, the oracle determines if f is covered by the nonrecursive
base clause alone. As an example, for the append program, the basecase oracle should return
\yes" for an instance append(Xs,Ys,Zs) when Xs is the empty list, and \no" otherwise.
Given the existence of a basecase oracle, the learning algorithm can be extended as
follows. As before, all possible recursive literals Lri of the clause BOTTOM d are generated;
however, in this case, the learner will test two clause hypotheses that are initially of the
form (BOTTOM d [ Lri ; BOTTOM d ). To forcibly simulate such a hypothesis on a fact f ,
the following procedure is used. After checking the usual termination conditions, the forced
simulator checks to see if BASECASE(f) is true. If so, it calls ForceSimNR (with appropriate
arguments) to generalize the current hypothesis for the base case. If BASECASE(f) is
false, then the recursive clause Hr is forcibly simulated on f , a subgoal Lr  is generated
as in before, and the generalized program is recursively forcibly simulated on the subgoal.
Figures 6 and 7 present a learning algorithm Force2 for two clause programs consisting of
one linear recursive clause CR and one nonrecursive clause CB , under the assumption that
both equivalence and basecase oracles are available.
It is straightforward to extend the arguments of Theorem 5 to this case, leading to the
following result.

Theorem 8 Let d-Depth-2-Clause be the set of 2-clause programs consisting of one

clause in d-DepthLinRec and one clause in d-DepthNonRec. For any constants a
and d the language family

d-Depth-2-Clause[DB; a-DetDEC ]
is uniformly identiable from equivalence and basecase queries.

Proof: Omitted.
A companion paper (Cohen, 1995) shows that something like the basecase oracle is
necessary: in particular, without any \hints" about the base clause, learning a two-clause
linear recursive program is as hard as learning boolean DNF. However, there are several
situations in which the basecase oracle can be dispensed with.
Case 1. The basecase oracle can be replaced by a polynomial-sized set of possible base
clauses. The learning algorithm in this case is to enumerate pairs of base clauses CBi
and \starting clauses" BOTTOM  [ Lrj , generalize the starting clause with forced
simulation, and mark a pair as incorrect if overgeneralization is detected.
Case 2. The basecase oracle can be replaced by a xed rule that determines when the base
clause is applicable. For example, consider the rule that says that the base clause is
applicable to any atom p(X1; : : :; Xa) such that no Xi is a non-null list. Adopting
530

Pac-Learning Recursive Logic Programs: Efficient Algorithms

such a rule leads immediately to a learning procedure that pac-learns exactly those
two-clause linear recursive programs for which the rule is correct.
Case 3. The basecase oracle can be also be replaced by a polynomial-sized set of rules for
determining when a base clause is applicable. The learning algorithm in this case is
pick a unmarked decision rule and run Force2 using that rule as a basecase oracle. If
Force2 returns \no consistent hypothesis" then the decision rule is marked incorrect,
and a new one is choosen. This algorithm will learn those two-clause linear recursive
programs for which any of the given decision rules is correct.
Even though the general problem of determining a basecase decision rule for an arbitrary
Datalog program may be dicult, it may be that a small number of decision procedures
apply to a large number of common Prolog programs. For example, the recursion for most
list-manipulation programs halts when some argument is reduced to a null list or to a
singleton list. Thus Case 3 above seems likely to cover a large fraction of the automatic
logic programming programs of practical interest.
We also note that heuristics have been proposed for nding such basecase decision rules
automatically using typing restrictions (Stahl, Tausend, & Wirth, 1993).

5.5 Combining the Results

Finally, we note that all of the extensions described above are compatible. This means
that if we let kd-MaxRecLang be the language of two-clause programs consisting of one
clause CR that is k-ary closed recursive and depth-d determinate, and one clause CB that
is nonrecursive and depth-d determinate, then the following holds.

Proposition 9 For any constants a, k and d the language family
kd-MaxRecLang[DB; a-DetDEC ]
is uniformly identiable from equivalence and basecase queries.
5.5.1 Further Extensions
The notation kd-MaxRecLang may seem at this point to be unjustied; although it is the

most expressive language of recursive clauses that we have proven to be learnable, there are
numerous extensions that may be eciently learnable. For example, one might generalize
the language to allow an arbitrary number of recursive clauses, or to include clauses that are
not determinate. These generalizations might very well be pac-learnable|given the results
that we have presented so far.
However, a companion paper (Cohen, 1995) presents a series of negative results showing
that most natural generalizations of kd-MaxRecLang are not eciently learnable, and
further that kd-MaxRecLang itself is not eciently learnable without the basecase oracle. Specically, the companion paper shows that eliminating the basecase oracle leads
to a problem that is as hard as learning boolean DNF, an open problem in computational
learning theory. Similarly, learning two linear recursive clauses simultaneously is as hard
as learning DNF, even if the base case is known. Finally, the following learning problems
are all as hard as breaking certain (presumably) secure cryptographic codes: learning n
531

Cohen

linear recursive determinate clauses, learning one n-ary recursive determinate clause, or
learning one linear recursive \k-local" clause. All of these negative results hold not only
for the model of identication from equivalence queries, but also for the weaker models of
pac-learnability and pac-predictability.

6. Related Work
In discussing related work we will concentrate on previous formal analyses that employ a
learning model similar to that considered here: namely, models that (a) require all computation be polynomial in natural parameters of the problem, and (b) assume either a neutral
source or adversarial source of examples, such as equivalence queries or stochastically presented examples. We note, however, that much previous formal work exists that relies on
dierent assumptions. For instance, there has been much work in which member or subset
queries are allowed (Shapiro, 1982; De Raedt & Bruynooghe, 1992), or where examples are
choosen in some non-random manner that is helpful to the learner (Ling, 1992; De Raedt
& Dzeroski, 1994). There has also been some work in which the eciency requirements
imposed by the pac-learnability model are relaxed (Nienhuys-Cheng & Polman, 1994). If
the requirement of eciency is relaxed far enough, very general positive results can be obtained using very simple learning algorithms. For example, in model of learnability in the
limit (Gold, 1967), any language that is both recursively enumerable and decidable (which
includes all of Datalog) can be learned by a simple enumeration procedure; in the model
of U-learnability (Muggleton & Page, 1994) any language that is polynomially enumerable
and polynomially decidable can be learned by enumeration.
The most similar previous work is that of Frazier and Page (1993a, 1993b). They analyze
the learnability from equivalence queries of recursive programs with function symbols but
without background knowledge. The positive results they provide are for program classes
that satisfy the following property: given a set of positive examples S + that requires all
clauses in the target program to prove the instances in S + , only a polynomial number of
recursive clauses are possible; further the base clause must have a certain highly constrained
form. Thus the concept class is \almost" bounded in size by a polynomial. The learning
algorithm for such a program class is to interleave a series of equivalence queries that
test every possible target program. In contrast, our positive results are for exponentially
large classes of recursive clauses. Frazier and Page also present a series of negative results
suggesting that the learnable languages that they analyzed are dicult to generalize without
sacricing ecient learnability.
Previous results also exist on the pac-learnability of nonrecursive constant-depth determinate programs, and on the pac-learnability of recursive constant-depth determinate
programs in a model that also allows membership and subset queries (Dzeroski et al.,
1992).
The basis for the intelligent search used in our learning algorithms is the technique
of forced simulation . This method nds the least implicant of a clause C that covers
an extended instance e. Although when we developed this method we believed it to be
original, subsequently we discovered that this was not the case|an identical technique had
been previously proposed by Ling (1991). Since an extended instance e can be converted
(via saturation) to a ground Horn clause, there is also a close connection between forced
532

Pac-Learning Recursive Logic Programs: Efficient Algorithms

simulation and recent work on \inverting implication" and \recursive anti-unication"; for
instance, Muggleton (1994) describes a nondeterministic procedure for nding all clauses
that imply a clause C , and Idestam-Almquist (1993) describes a means of constraining such
an implicant-generating procedure to produce the least common implicant of two clauses.
However, while both of these techniques have obvious applications in learning, both are
extremely expensive in the worst case.
The CRUSTACEAN system (Aha et al., 1994) uses inverting implication in constrained
settings to learn certain restricted classes of recursive programs. The class of programs
eciently learned by this system is not formally well-understood, but it appears to be
similar to the classes analyzed by Frazier and Page. Experimental results show that these
systems perform well on inferring recursive programs that use function symbols in certain
restricted ways. This system cannot, however, make use of background knowledge.
Finally, we wish to direct the reader to several pieces of our own research that are relevant. As noted above, a companion paper exists which presents negative learnability results
for several natural generalizations of the language kd-MaxRecLang (Cohen, 1995). Another related paper investigates the learnability of non-recursive Prolog programs (Cohen,
1993b); this paper also contains a number of negative results which strongly motivate the
restriction of constant-depth determinacy. A nal prior paper which may be of interest
presents some experimental results with a Prolog implementation of a variant of the Force2
algorithm (Cohen, 1993a). This paper shows that forced simulation can be the basis of a
learning program that outperforms state-of-the art heuristic methods such as FOIL (Quinlan, 1990; Quinlan & Cameron-Jones, 1993) in learning from randomly chosen examples.

7. Conclusions
Just as it is often desirable to have guarantees of correctness for a program, in many
plausible contexts it would be highly desirable to have an automatic programming system
oer some formal guarantees of correctness. The topic of this paper is the learnability of
recursive logic programs using formally well-justied algorithms. More specically, we have
been concerned with the development of algorithms that are provably sound and ecient in
learning recursive logic programs from equivalence queries. We showed that one constantdepth determinate closed k-ary recursive clause is identiable from equivalent queries; this
implies immediately that this language is also learnable in Valiant's (1984) model of paclearnability. We also showed that a program consisting of one such recursive clause and
one constant-depth determinate nonrecursive clause is identiable from equivalence queries
given an additional \basecase oracle", which determines if a positive example is covered by
the non-recursive base clause of the target program alone.
In obtaining these results, we have introduced several new formal techniques for analyzing the learnability of recursive programs. We have also shown the soundness and
eciency of several instances of generalization by forced simulation . This method may have
applications in practical learning systems. The Force2 algorithm compares quite well experimentally with modern ILP systems on learning problems from the restricted class that
it can identify (Cohen, 1993a); thus sound learning methods like Force2 might be useful as
a lter before a more general ILP system like FOIL (Quinlan, 1990; Quinlan & CameronJones, 1993). Alternatively, forced simulation could be used in heuristic programs. For
533

Cohen

example, although forced simulation for programs with many recursive clauses is nondeterministic and hence potentially inecient, one could introduce heuristics that would make
the forced simulation ecient, at the cost of completeness.
A companion paper (Cohen, 1995) shows that the positive results of this paper are not
likely to be improved: either eliminating the basecase oracle for the language above or
learning two recursive clauses simultaneously is as hard as learning DNF, and learning n
linear recursive determinate clauses, one n-ary recursive determinate clause, or one linear
recursive \k-local" clause is as hard as breaking certain cryptographic codes. With the positive results of this paper, these negative results establish the boundaries of learnability for
recursive programs function-free in the pac-learnability model. These results thus not only
give a prescription for building a formally justied system for learning recursive programs;
taken together, they also provide upper bounds on what one can hope to achieve with an
ecient, formally justied system that learns recursive programs from random examples
alone.

Appendix A. Additional Proofs

Theorem 1 states: Let Dec = (p; a0; R) be a declaration in 2 a-DetDEC = , let nr = j Rj , let
X1; : : :; Xa be distinct variables, and dene the clause BOTTOM d as follows:
0

BOTTOM d (Dec )  CONSTRAIN Dec (DEEPEN dDec (p(X1; : : :; Xa ) ))
0

For any constants d and a, the following are true:

 the size of BOTTOM d(Dec) is polynomial in nr ;
 every depth-d clause that satises Dec is equivalent to some subclause of
BOTTOM d (Dec ).

Proof: Let us rst establish the polynomial bound on the size of BOTTOM d. Let C be a
clause of size n. As the number of variables in C is bounded by an, the size of the set LD

is bounded by

Thus for any clause C
By a similar argument

nr 
|{z}

(|an{z)a,1}

(# modes) (# tuples of input variables)

j DEEPEN Dec (C )j  n + (an)a,1nr

(1)

j CONSTRAIN Dec (C )j  n + (an)anr

(2)

Since both of the functions DEEPEN Dec and CONSTRAIN Dec give outputs that are polynomially larger in size than their inputs, if follows that composing these functions a constant
number of times, as was done in computing BOTTOM d for constant d, will also produce
only a polynomial increase in the size.
Next, we wish to show that every depth-d determinate clause C that satises Dec is
equivalent to some subclause of BOTTOM d . Let C be some depth-d determinate clause,
534

Pac-Learning Recursive Logic Programs: Efficient Algorithms

and without loss of generality let us assume that no pair of literals Li and Lj in the body
of C have the same mode, predicate symbol, and sequence of input variables.6
Given C , let us now dene the substitution C as follows:
1. Initially set
C fX1 = X1; : : :; Xa = Xa g
where X1; : : :; Xa are the arguments to the head of BOTTOM d and X1 ; : : :; Xa are
the arguments to the head of C .
Notice that because the variables in the head of BOTTOM d are distinct, this mapping
is well-dened.
2. Next, examine each of the literals in the body of C in left-to-right order. For each
literal L, let variables T1; : : :Tk be its input variables. For each literal L in the
body BOTTOM d with the same mode and predicate symbol whose input variables
T1; : : :; Tk are such that 8i : 1  i  r; TjC = Tj , modify C as follows:
0

0

0

0

C [ fU1 = U1 ; : : :; Ul = Ul g
where U1 ; : : :; Ul are the output variables of L and U1; : : :; Ul are the output variables
of L .
Notice that because we assume that C contains only one literal L with a given predC

icate symbol and sequence of input variables, and because the output variables of
literals L in BOTTOM d are distinct, this mapping is again well-dened. It is also
easy to verify (by induction on the length of C ) that in executing this procedure some
variable in BOTTOM d is always mapped to each input variable Ti , and that at least
one L meeting the requirements above exists. Thus the mapping C is onto the
variables appearing in C .7
Let A be the head of BOTTOM d , and consider the clause C 0 which is dened as follows:
 The head of C 0 is A.
 The body of C 0 contains all literals L from the body of BOTTOM d such that either
{ LC is in the body of C
{ L is the literal equal (Xi; Xj) and XiC = XjC .
We claim that C 0 is a subclause of BOTTOM d that is equivalent to C . Certainly C 0
is a subclause of BOTTOM d . One way to see that it is equivalent to C is to consider
the clause C^ and the substitution ^C which are generated as follows. Initially, let C^ = C 0
and let ^C = C . Then, for every literal L = equal (Xi; Xj) in the body of C^ , delete L
^ ij and replace ^C with (^C )ij , where ij is the
from C^ , and nally replace C^ with C
substitution fXi = Xij ; Xj = Xij g and Xij is some new variable not previously appearing
6. This assumption can be made without loss of generality since for a determinate clause C , the output
variables of Li and Lj will necessarily be bound to the same values, and hence Li or Lj could be unied
together and one of them deleted without changing the semantics of C .
7. Recall that a function f : X Y is onto its range Y if 8y 2 Y 9x 2 X : f (x) = y.

535

Cohen

in C^ . (Note: by (^C )ij we refer to the substitution formed by replacing every occurrence
of Xi or Xj appearing in ^C with Xij .) C^ is semantically equivalent to C 0 because the
operation described above is equivalent to simply resolving each possible L in the body of
C 0 against the clause \equal(X,X) ".
The following are now straightforward to verify:
 ^C is a one-to-one mapping.
To see that this is true, notice that for every pair of assignments Xi = Y and Xj =
Y in C there must be a literal equal (Xi; Xj) in C 0. Hence following the process
described above the assignments Xi = Y and Xj = Y in ^C would eventually be
replaced with Xij = Y and Xij = Y .

 ^C is onto the variables in C .
Notice that C was onto the variables in C , and for every assignment Xi = Y in C
there is some assignment in ^C with a right-hand side of Y (and this assignment is
either of the form Xi = Y or Xij = Y ). Thus ^C is also onto the variables in C .
 A literal L^ is in the body of C^ i L^^C is in the body of C .
This follows from the denition of C 0 and from the fact that for every literal L from
C 0 that is not of the form equal (Xi; Xj) there is a corresponding literal in C^ .
Thus C^ is an alphabetic variant of C , and hence is equivalent to C . Since C^ is also equivalent
to C 0, it must be that C 0 is equivalent to C , which proves our claim.

Acknowledgements
The author wishes to thank three anonymous JAIR reviewers for a number of useful suggestions on the presentation and technical content.

References
Aha, D., Lapointe, S., Ling, C. X., & Matwin, S. (1994). Inverting implication with small
training sets. In Machine Learning: ECML-94 Catania, Italy. Springer-Verlag. Lecture
Notes in Computer Science # 784.
Angluin, D. (1988). Queries and concept learning. Machine Learning, 2 (4).
Angluin, D. (1989). Equivalence queries and approximate ngerprints. In Proceedings of
the 1989 Workshop on Computational Learning Theory Santa Cruz, California.
Bergadano, F., & Gunetti, D. (1993). An interactive system to learn functional logic programs. In Proceedings of the 13th International Joint Conference on Articial Intelligence Chambery, France.
536

Pac-Learning Recursive Logic Programs: Efficient Algorithms

Biermann, A. (1978). The inference of regular lisp programs from examples. IEEE Transactions on Systems, Man and Cybernetics, 8 (8).
Cohen, W. W. (1993a). A pac-learning algorithm for a restricted class of recursive logic
programs. In Proceedings of the Tenth National Conference on Articial Intelligence
Washington, D.C.
Cohen, W. W. (1993b). Pac-learning non-recursive Prolog clauses. To appear in Articial
Intelligence.
Cohen, W. W. (1993c). Rapid prototyping of ILP systems using explicit bias. In Proceedings
of the 1993 IJCAI Workshop on Inductive Logic Programming Chambery, France.
Cohen, W. W. (1994). Pac-learning nondeterminate clauses. In Proceedings of the Eleventh
National Conference on Articial Intelligence Seattle, WA.
Cohen, W. W. (1995). Pac-learning recursive logic programs: negative results. Journal of
AI Research, 2, 541{573.
De Raedt, L., & Bruynooghe, M. (1992). Interactive concept-learning and constructive
induction by analogy. Machine Learning, 8 (2).
De Raedt, L., & Dzeroski, S. (1994). First-order jk-clausal theories are PAC-learnable.
In Wrobel, S. (Ed.), Proceedings of the Fourth International Workshop on Inductive
Logic Programming Bad Honnef/Bonn, Germany.
De Raedt, L., Lavrac, N., & Dzeroski, S. (1993). Multiple predicate learning. In Proceedings
of the Third International Workshop on Inductive Logic Programming Bled, Slovenia.
Dzeroski, S., Muggleton, S., & Russell, S. (1992). Pac-learnability of determinate logic
programs. In Proceedings of the 1992 Workshop on Computational Learning Theory
Pittsburgh, Pennsylvania.
Frazier, M., & Page, C. D. (1993a). Learnability in inductive logic programming: Some
basic results and techniques. In Proceedings of the Tenth National Conference on
Articial Intelligence Washington, D.C.
Frazier, M., & Page, C. D. (1993b). Learnability of recursive, non-determinate theories:
Some basic results and techniques. In Proceedings of the Third International Workshop
on Inductive Logic Programming Bled, Slovenia.
Gold, M. (1967). Language identication in the limit. Information and Control, 10.
Hirsh, H. (1992). Polynomial-time learning with version spaces. In Proceedings of the Tenth
National Conference on Articial Intelligence San Jose, California. MIT Press.
Idestam-Almquist, P. (1993). Generalization under implication by recursive anti-unication.
In Proceedings of the Ninth International Conference on Machine Learning Amherst,
Massachusetts. Morgan Kaufmann.
537

Cohen

King, R. D., Muggleton, S., Lewis, R. A., & Sternberg, M. J. E. (1992). Drug design by
machine learning: the use of inductive logic programming to model the structureactivity relationships of trimethoprim analogues binding to dihydrofolate reductase.
Proceedings of the National Academy of Science, 89.
Lavrac, N., & Dzeroski, S. (1992). Background knowledge and declarative bias in inductive
concept learning. In Jantke, K. P. (Ed.), Analogical and Inductive Inference: International Workshop AII'92. Springer Verlag, Daghstuhl Castle, Germany. Lectures in
Articial Intelligence Series #642.
Ling, C. (1991). Inventing necessary theoretical terms in scientic discovery and inductive
logic programming. Tech. rep. 301, University of Western Ontario.
Ling, C. (1992). Logic program synthesis from good examples. In Inductive Logic Programming. Academic Press.
Lloyd, J. W. (1987). Foundations of Logic Programming: Second Edition. Springer-Verlag.
Muggleton, S. (1994). Inverting implication. To appear in Articial Intelligence.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory and methods.
Journal of Logic Programming, 19/20 (7), 629{679.
Muggleton, S., & Feng, C. (1992). Ecient induction of logic programs. In Inductive Logic
Programming. Academic Press.
Muggleton, S., King, R. D., & Sternberg, M. J. E. (1992). Protein secondary structure
prediction using logic-based machine learning. Protein Engineering, 5 (7), 647{657.
Muggleton, S., & Page, C. D. (1994). A learnability model for universal representations.
In Wrobel, S. (Ed.), Proceedings of the Fourth International Workshop on Inductive
Logic Programming Bad Honnef/Bonn, Germany.
Muggleton, S. H. (Ed.). (1992). Inductive Logic Programming. Academic Press.
Nienhuys-Cheng, S., & Polman, M. (1994). Sample pac-learnability in model inference.
In Machine Learning: ECML-94 Catania, Italy. Springer-Verlag. Lecture notes in
Computer Science # 784.
Page, C. D., & Frisch, A. M. (1992). Generalization and learnability: A study of constrained
atoms. In Inductive Logic Programming. Academic Press.
Pazzani, M., & Kibler, D. (1992). The utility of knowledge in inductive learning. Machine
Learning, 9 (1).
Quinlan, J. R., & Cameron-Jones, R. M. (1993). FOIL: A midterm report. In Brazdil, P. B.
(Ed.), Machine Learning: ECML-93 Vienna, Austria. Springer-Verlag. Lecture notes
in Computer Science # 667.
Quinlan, J. R. (1990). Learning logical denitions from relations. Machine Learning, 5 (3).
538

Pac-Learning Recursive Logic Programs: Efficient Algorithms

Quinlan, J. R. (1991). Determinate literals in inductive logic programming. In Proceedings
of the Eighth International Workshop on Machine Learning Ithaca, New York. Morgan
Kaufmann.
Rouveirol, C. (1994). Flattening and saturation: two representation changes for generalization. Machine Learning, 14 (2).
Shapiro, E. (1982). Algorithmic Program Debugging. MIT Press.
Srinivasan, A., Muggleton, S. H., King, R. D., & Sternberg, M. J. E. (1994). Mutagenesis:
ILP experiments in a non-determinate biological domain. In Wrobel, S. (Ed.), Proceedings of the Fourth International Workshop on Inductive Logic Programming Bad
Honnef/Bonn, Germany.
Stahl, I., Tausend, B., & Wirth, R. (1993). Two methods for improving inductive logic
programming. In Proceedings of the 1993 European Conference on Machine Learning
Vienna, Austria.
Summers, P. D. (1977). A methodology for LISP program construction from examples.
Journal of the Association for Computing Machinery, 24 (1), 161{175.
Valiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27 (11).
Zelle, J. M., & Mooney, R. J. (1994). Inducing deterministic Prolog parsers from treebanks:
a machine learning approach. In Proceedings of the Twelfth National Conference on
Articial Intelligence Seattle, Washington. MIT Press.

539

Journal of Articial Intelligence Research 2 (1995) 575-609

Submitted 12/94; published 5/95

Provably Bounded-Optimal Agents
Stuart J. Russell

Computer Science Division, University of California
Berkeley, CA 94720, USA

Devika Subramanian

Computer Science Department, Cornell University
Ithaca, NY 14853, USA

russell@cs.berkeley.edu
devika@cs.cornell.edu

Abstract

Since its inception, articial intelligence has relied upon a theoretical foundation centred around perfect rationality as the desired property of intelligent systems. We argue,
as others have done, that this foundation is inadequate because it imposes fundamentally
unsatisable requirements. As a result, there has arisen a wide gap between theory and
practice in AI, hindering progress in the eld. We propose instead a property called bounded
optimality. Roughly speaking, an agent is bounded-optimal if its program is a solution to
the constrained optimization problem presented by its architecture and the task environment. We show how to construct agents with this property for a simple class of machine
architectures in a broad class of real-time environments. We illustrate these results using
a simple model of an automated mail sorting facility. We also dene a weaker property,
asymptotic bounded optimality (ABO), that generalizes the notion of optimality in classical
complexity theory. We then construct universal ABO programs, i.e., programs that are
ABO no matter what real-time constraints are applied. Universal ABO programs can be
used as building blocks for more complex systems. We conclude with a discussion of the
prospects for bounded optimality as a theoretical basis for AI, and relate it to similar trends
in philosophy, economics, and game theory.

1. Introduction

Since before the beginning of articial intelligence, philosophers, control theorists and
economists have looked for a satisfactory denition of rational behaviour. This is needed to
underpin theories of ethics, inductive learning, reasoning, optimal control, decision-making,
and economic modelling. Doyle (1983) has proposed that AI itself be dened as the computational study of rational behaviour|eectively equating rational behaviour with intelligence. The role of such denitions in AI is to ensure that theory and practice are correctly
aligned. If we dene some property P , then we hope to be able to design a system that
provably possesses property P . Theory meets practice when our systems exhibit P in reality. Furthermore, that they exhibit P in reality should be something that we actually care
about. In a sense, the choice of what P to study determines the nature of the eld.
There are a number of possible choices for P :
 Perfect rationality: the classical notion of rationality in economics and philosophy.
A perfectly rational agent acts at every instant in such a way as to maximize its
expected utility, given the information it has acquired from the environment. Since
action selection requires computation, and computation takes time, perfectly rational
agents do not exist for non-trivial environments.
c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Russell & Subramanian

 Calculative rationality: the notion of rationality studied in AI. A calculatively rational

agent eventually returns what would have been the rational choice at the beginning of
its deliberation. There exist systems such as inuence diagram evaluators that exhibit
this property for a decision-theoretic denition of rational choice, and systems such
as nonlinear planners that exhibit it for a logical denition of rational choice. This
is assumed to be an interesting property for a system to exhibit since it constitutes
an \in-principle" capacity to do the right thing. Calculative rationality is of limited
value in practice, because the actual behaviour exhibited by such systems is absurdly
far from being rational; for example, a calculatively rational chess program will choose
the right move, but may take 1050 times too long to do so. As a result, AI systembuilders often ignore theoretical developments, being forced to rely on trial-and-error
engineering to achieve their goals. Even in simple domains such as chess, there is little
theory for designing and analysing high-performance programs.
 Metalevel rationality: a natural response to the problems of calculative rationality.
A metalevel rational system optimizes over the object-level computations to be performed in the service of selecting actions. In other words, for each decision it nds
the optimal combination of computation-sequence-plus-action, under the constraint
that the action must be selected by the computation. Full metalevel rationality is
seldom useful because the metalevel computations themselves take time, and the metalevel decision problem is often more dicult than the object-level problem. Simple
approximations to metalevel rationality have proved useful in practice|for example, metalevel policies that limit lookahead in chess programs|but these engineering
expedients merely serve to illustrate the lack of a theoretical basis for agent design.
 Bounded optimality: a bounded optimal agent behaves as well as possible given its
computational resources. Bounded optimality species optimal programs rather than
optimal actions or optimal computation sequences. Only by the former approach
can we avoid placing constraints on intelligent agents that cannot be met by any
program. Actions and computations are, after all, generated by programs, and it is
over programs that designers have control.
We make three claims:
1. A system that exhibits bounded optimality is desirable in reality.
2. It is possible to construct provably bounded optimal programs.
3. Articial intelligence can be usefully characterized as the study of bounded optimality,
particularly in the context of complex task environments and reasonably powerful
computing devices.
The rst claim is unlikely to be controversial. This paper supports the second claim in
detail. The third claim may, or may not, stand the test of time.
We begin in section 2 with a necessarily brief discussion of the relationship between
bounded optimality and earlier notions of rationality. We note in particular that some important distinctions can be missed without precise denitions of terms. Thus in section 3 we
provide formal denitions of agents, their programs, their behaviour and their rationality.
576

Provably bounded-optimal agents

Together with formal descriptions of task environments, these elements allow us to prove
that a given agent exhibits bounded optimality. Section 4 examines a class of agent architectures for which the problem of generating bounded optimal congurations is eciently
soluble. The solution involves a class of interesting and practically relevant optimization
problems that do not appear to have been addressed in the scheduling literature. We illustrate the results by showing how the throughput of an automated mail-sorting facility
might be improved. Section 5 initiates a discussion of how bounded optimal congurations
might be learned from experience in an environment. In section 6, we dene a weaker property, asymptotic bounded optimality (ABO), that may be more robust and tractable than
the strict version of bounded optimality. In particular, we can construct universal ABO
programs. A program is universally ABO if it is ABO regardless of the specic form of
time dependence of the utility function.1 Universal ABO programs can therefore be used as
building blocks for more complex systems. We conclude with an assessment of the prospects
for further development of this approach to articial intelligence.

2. Historical Perspective
The classical idea of perfect rationality, which developed from Aristotle's theories of ethics,
work by Arnauld and others on choice under uncertainty, and Mill's utilitarianism, was put
on a formal footing in decision theory by Ramsey (1931) and vonNeumann and Morgernstern
(1947). It stipulates that a rational agent always act so as to maximize its expected utility.
The expectation is taken according to the agent's own beliefs; thus, perfect rationality does
not require omniscience.
In articial intelligence, the logical denition of rationality, known in philosophy as the
\practical syllogism", was put forward by McCarthy (1958), and reiterated strongly by
Newell (1981). Under this denition, an agent should take any action that it believes is
guaranteed to achieve any of its goals. If AI can be said to have had a theoretical foundation, then this denition of rationality has provided it. McCarthy believed, probably
correctly, that in the early stages of the eld it was important to concentrate on \epistemological adequacy" before \heuristic adequacy" | that is, capability in principle rather than
in practice. The methodology that has resulted involves designing programs that exhibit
calculative rationality, and then using various speedup techniques and approximations in
the hope of getting as close as possible to perfect rationality. Our belief, albeit unproven, is
that the simple agent designs that fulll the specication of calculative rationality may not
provide good starting points from which to approach bounded optimality. Moreover, a theoretical foundation based on calculative rationality cannot provide the necessary guidance
in the search.
It is not clear that AI would have embarked on the quest for calculative rationality had it
not been operating in the halcyon days before formal intractability results were discovered.
One response to the spectre of complexity has been to rule it out of bounds. Levesque and
Brachman (1987) suggest limiting the complexity of the environment so that calculative and
perfect rationality coincide. Doyle and Patil (1991) argue strongly against this position.
1. This usage of the term \universal" derives from its use in the scheduling of randomized algorithms by
Luby, Sinclair and Zuckerman (1993).

577

Russell & Subramanian

Economists have used perfect rationality as an abstract model of economic entities, for
the purposes of economic forecasting and designing market mechanisms. This makes it
possible to prove theorems about the properties of markets in equilibrium. Unfortunately,
as Simon (1982) pointed out, real economic entities have limited time and limited powers
of deliberation. He proposed the study of bounded rationality, investigating \: : : the shape
of a system in which eectiveness in computation is one of the most important weapons
of survival." Simon's work focussed mainly on satiscing designs, which deliberate until
reaching some solution satisfying a preset \aspiration level." The results have descriptive value for modelling various actual entities and policies, but no general prescriptive
framework for bounded rationality was developed. Although it proved possible to calculate
optimal aspiration levels for certain problems, no structural variation was allowed in the
agent design.
In the theory of games, bounds on the complexity of players have become a topic of
intense interest. For example, it is a troubling fact that defection is the only equilibrium
strategy for unbounded agents playing a xed number of rounds of the Prisoners' Dilemma
game. Neyman's theorem (Neyman, 1985), recently proved by Papadimitriou and Yannakakis (1994), shows that an essentially cooperative equilibrium exists if each agent is a
nite automaton with a number of states that is less than exponential in the number of
rounds. This is essentially a bounded optimality result, where the bound is on space rather
than on speed of computation. This type of result is made possible by a shift from the
problem of selecting actions to the problem of selecting programs.
I. J. Good (1971) distinguished between perfect or \type I" rationality, and metalevel
or \type II" rationality. He denes this as \the maximization of expected utility taking into
account deliberation costs." Simon (1976) also says: \The global optimization problem is
to nd the least-cost or best-return decision, net of computational costs." Although type II
rationality seems to be a step in the right direction, it is not entirely clear whether it can be
made precise in a way that respects the desirable intuition that computation is important.
We will try one interpretation, although there may be others.2 The key issue is the space
over which the \maximization" or \optimization" occurs. Both Good and Simon seem to
be referring to the space of possible deliberations associated with a particular decision.
Conceptually, there is an \object-level machine" that executes a sequence of computations
under the control of a \meta-level machine." The outcome of the sequence is the selection of
an external action. An agent exhibits type II rationality if at the end of its deliberation and
subsequent action, its utility is maximized compared to all possible deliberate/act pairs in
which it could have engaged. For example, Good discusses one possible application of type
II rationality in chess programs. In this case, the object-level steps are node expansions in
the game tree, followed by backing up of leaf node evaluations to show the best move. For
simplicity we will assume a per-move time limit. Then a type II rational agent will execute
whichever sequence of node expansions chooses the best move, of all those that nish before
2. For example, it is conceivable that Good and Simon really intended to refer to nding an agent design
that minimizes deliberation costs in general. All their discussions, however, seem to be couched in terms
of nding the right deliberation for each decision. Thus, type II or metalevel rationality coincides with
bounded optimality if the bounded optimal agent is being designed for a single decision in a single
situation.

578

Provably bounded-optimal agents

the time limit.3 Unfortunately, the computations required in the \metalevel machine" to
select the object-level deliberation may be extremely expensive. Good actually proposes a
fairly simple (and nearly practical) metalevel decision procedure for chess, but it is far from
optimal. It is hard to see how a type II rational agent could justify executing a suboptimal
object-level computation sequence if we limit the scope of the optimization problem to a
single decision. The diculty can only be resolved by thinking about the design of the
agent program, which generates an unbounded set of possible deliberations in response to
an unbounded set of circumstances that may arise during the life of the agent.
Philosophy has also seen a gradual evolution in the denition of rationality. There has
been a shift from consideration of act utilitarianism | the rationality of individual acts | to
rule utilitarianism, or the rationality of general policies for acting. This shift has been caused
by diculties with individual versus societal rationality, rather than any consideration of
the diculty of computing rational acts. Some consideration has been given more recently
to the tractability of general moral policies, with a view to making them understandable
and usable by persons of average intelligence (Brandt, 1953). Cherniak (1986) has suggested
a denition of \minimal rationality", specifying lower bounds on the reasoning powers of
any rational agent, instead of upper bounds. A philosophical proposal generally consistent
with the notion of bounded optimality can be found in Dennett's \Moral First Aid Manual"
(1986). Dennett explicitly discusses the idea of reaching equilibrium within the space of
decision procedures. He uses as an example the PhD admissions procedure of a philosophy
department. He concludes, as do we, that the best procedure may be neither elegant nor
illuminating. The existence of such a procedure, and the process of reaching it, are the
main points of interest.
Many researchers in AI, some of whose work is discussed below, have worked on the
problem of designing agents with limited computational resources. The 1989 AAAI Symposium on AI and Limited Rationality (Fehling & Russell, 1989) contains an interesting
variety of work on the topic. Much of this work is concerned with metalevel rationality.
Metareasoning | reasoning about reasoning | is an important technique in this area,
since it enables an agent to control its deliberations according to their costs and benets.
Combined with the idea of anytime (Dean & Boddy, 1988) or exible algorithms (Horvitz,
1987), that return better results as time goes by, a simple form of metareasoning allows
an agent to behave well in a real-time environment. A simple example is provided by
iterative-deepening algorithms used in game-playing. Breese and Fehling (1990) apply similar ideas to controlling multiple decision procedures. Russell and Wefald (1989) give a
general method for precompiling certain aspects of metareasoning so that a system can eciently estimate the eects of individual computations on its intentions, giving ne-grained
control of reasoning. These techniques can all be seen as approximating metalevel rationality; they provide useful insights into the general problem of control of reasoning, but there
is no reason to suppose that the approximations used are optimal in any sense.
The intuitive notion of bounded optimality seems to have become current in the AI
community in the mid-1980's. Horvitz (1987) uses the term bounded optimality to refer
to \the optimization of computational utility given a set of assumptions about expected
3. One would imagine that in most cases the move selected will be the same move selected by a Type
I agent, but this is in a sense \accidental" because further deliberation might cause the program to
abandon it.

579

Russell & Subramanian

problems and constraints in reasoning resources." Russell and Wefald (1991) say that an
agent exhibits bounded optimality for a given task environment \if its program is a solution
to the constrained optimization problem presented by its architecture." Recent work by
Etzioni (1989) and Russell and Zilberstein (1991) can be seen as optimizing over a welldened set of agent designs, thereby making the notion of bounded optimality more precise.
In the next section, we build a suitable set of general denitions from the ground up, so
that we can begin to demonstrate examples of provably bounded optimal agents.

3. Agents, Architectures and Programs
Intuitively, an agent is just a physical entity that we wish to view in terms of its perceptions
and actions. What counts in the rst instance is what it does, not necessarily what it thinks,
or even whether it thinks at all. This initial refusal to consider further constraints on the
internal workings of the agent (such as that it should reason logically, for example) helps in
three ways: rst, it allows us to view such \cognitive faculties" as planning and reasoning
as occurring in the service of nding the right thing to do; second, it makes room for those
among us (Agre & Chapman, 1987; Brooks, 1986) who take the position that systems can
do the right thing without such cognitive faculties; third, it allows more freedom to consider
various specications, boundaries and interconnections of subsystems.
We begin by dening agents and environments in terms of the actions and percepts
that they exchange, and the sequence of states they go through. The agent is described
by an agent function from percept sequences to actions. This treatment is fairly standard
(see, e.g., Genesereth & Nilsson, 1987). We then go \inside" the agent to look at the agent
program that generates its actions, and dene the \implementation" relationship between
a program and the corresponding agent function. We consider performance measures on
agents, and the problem of designing agents to optimize the performance measure.

3.1 Specifying agents and environments

An agent can be described abstractly as a mapping (the agent function) from percept
sequences to actions. Let O be the set of percepts that the agent can receive at any instant,
and A be the set of possible actions the agent can carry out in the external world. Since
we are interested in the behaviour of the agent over time, we introduce a set of time points
or instants, T. The set T is totally ordered by the < relation with a unique least element.
Without loss of generality, we let T be the set of non-negative integers.
The percept history of an agent is a sequence of percepts indexed by time. We dene
the set of percept histories to be OT = fOT : T ! Og. The prex of a history OT 2 OT
till time t is denoted Ot and is the projection of OT on [0::t]. We can dene the set of
percept history prexes as Ot = fOt j t 2 T; OT 2 OTg. Similarly, we dene the set of
action histories AT = fAT : T ! Ag. The set of action history prexes is At, dened as
the set of projections At of histories AT 2 AT .

Denition 1 Agent function: a mapping
f : Ot ! A
580

Provably bounded-optimal agents

where

AT(t) = f (Ot)
Note that the agent function is an entirely abstract entity, unlike the agent program that
implements it. Note also that the \output" of the agent function for a given percept sequence
may be a null action, for example if the agent is still thinking about what to do. The agent
function species what the agent does at each time step. This is crucial to the distinction
between perfect rationality and calculative rationality.
Agents live in environments. The states of an environment E are drawn from a set X.
The set of possible state trajectories is dened as XT = fX T : T ! Xg. The agent does not
necessarily have full access to the current state X T(t), but the percept received by the agent
does depend on the current state through the perceptual ltering function fp. The eects
of the agent's actions are represented by the environment's transition function fe, which
species the next state given the current state and the agent's action. An environment is
therefore dened as follows:

Denition 2 Environment E : a set of states X with a distinguished initial state X0, a
transition function fe and a perceptual lter function fp such that

X T (0) = X0
X T(t + 1) = fe(AT (t); X T(t))
OT(t) = fp(X T(t))
The state history X T is thus determined by the environment and the agent function. We
use the notation eects(f; E ) to denote the state history generated by an agent function
f operating in an environment E . We will also use the notation [E; At ] to denote the
state history generated by applying the action sequence At starting in the initial state of
environment E .
Notice that the environment is discrete and deterministic in this formulation. We can
extend the denitions to cover non-deterministic and continuous environments, but at the
cost of additional complexity in the exposition. None of our results depend in a signicant
way on discreteness or determinism.

3.2 Specifying agent implementations

We will consider a physical agent as consisting of an architecture and a program. The
architecture is responsible for interfacing between the program and the environment, and
for running the program itself. With each architecture M , we associate a nite programming
language LM , which is just the set of all programs runnable by the architecture. An agent
program is a program l 2 LM that takes a percept as input and has an internal state drawn
from a set I with initial state i0. (The initial internal state depends on the program l,
but we will usually suppress this argument.) The set of possible internal state histories is
IT = fI T : T ! Ig. The prex of an internal state history I T 2 IT till time t is denoted I t
and is the projection of I T on [0::t].
581

Russell & Subramanian

Denition 3 An architecture M is a xed interpreter for an agent program that runs the

program for a single time step, updating its internal state and generating an action:
M : LM  I  O ! I  A
where
hI T(t + 1); AT(t)i = M (l; I T(t); OT(t))
Thus, the architecture generates a stream of actions according to the dictates of the program.
Because of the physical properties of the architecture, running the program for a single
time step results in the execution of only a nite number of instructions. The program may
often fail to reach a \decision" in that time step, and as a result the action produced by the
architecture may be null (or the same as the previous action, depending on the program
design).

3.3 Relating agent specications and implementations

We can now relate agent programs to the corresponding agent functions. We will say that an
agent program l running on a machine M implements the agent function Agent(l; M ). The
agent function is constructed in the following denition by specifying the action sequences
produced by l running on M for all possible percept sequences. Note the importance of the
\Markovian" construction using the internal state of the agent to ensure that actions can
only be based on the past, not the future.

Denition 4 A program l running on M implements the agent function f = Agent(l; M ),
dened as follows. For any environment E = (X; fe; fp), f (Ot) = AT (t) where
hI T(t + 1); AT(t)i = M (l; I T(t); OT(t))
OT (t)
X T(t + 1)
X T(0)
I T(0)

=
=
=
=

fp(X T(t))
fe(AT(t); X T(t))
X0
i0

Although every program l induces a corresponding agent function Agent(l; M ), the
action that follows a given percept is not necessarily the agent's \response" to that percept;
because of the delay incurred by deliberation, it may only reect percepts occurring much
earlier in the sequence. Furthermore, it is not possible to map every agent function to an
implementation l 2 LM . We can dene a subset of the set of agent functions f that are
implementable on a given architecture M and language LM :
Feasible(M ) = ff j 9l 2 LM ; f = Agent(l; M )g
Feasibility is related to, but clearly distinct from, the notion of computability. Computability refers to the existence of a program that eventually returns the output specied by
a function, whereas feasibility refers to the production of the output at the appropriate
point in time. The set of feasible agent functions is therefore much smaller than the set of
computable agent functions.
582

Provably bounded-optimal agents

3.4 Performance measures for agents

To evaluate an agent's performance in the world, we dene a real-valued utility function U
on state histories:
U : XT ! <
The utility function should be seen as external to the agent and its environment. It denes
the problem to be solved by the designer of the agent. Some agent designs may incorporate
an explicit representation of the utility function, but this is by no means required. We will
use the term task environment to denote the combination of an environment and a utility
function.
Recall that the agent's actions drive the environment E through a particular sequence
of states in accordance with the function eects(f; E ). We can dene the value of an agent
function f in the environment E as the utility of the state history it generates:
V (f; E ) = U (eects(f; E ))
If the designer has a set E of environments with a probability distribution p over them,
instead of a single environment E , then the value of the agent in E is dened as the
expected value over the elements of E. By a slight abuse of notation,

V (f; E) =

X

E2E

p(E )V (f; E )

We can assign a value V (l; M; E ) to a program l executed by the architecture M in the
environment E simply by looking at the eect of the agent function implemented by the
program:
V (l; M; E ) = V (Agent(l; M ); E ) = U (eects(Agent(l; M ); E ))
As above, we can extend this to a set of possible environments as follows:

V (l; M; E) =

X

E2E

p(E )V (l; M; E )

3.5 Perfect rationality and bounded optimality

As discussed in Section 2, a perfectly rational agent selects the action that maximizes its
expected utility, given the percepts so far. In our framework, this amounts to an agent
function that maximizes V (f; E) over all possible agent functions.
Denition 5 A perfectly rational agent for a set E of environments has an agent function
fopt such that
fopt = argmaxf (V (f; E))
This denition is a persuasive specication of an optimal agent function for a given
set of environments, and underlies several recent projects in intelligent agent design (Dean
& Wellman,1991; Doyle, 1988; Hansson & Mayer, 1989). A direct implementation of this
specication, which ignores the delay incurred by deliberation, does not yield a reasonable
583

Russell & Subramanian

solution to our problem { the calculation of expected utilities takes time for any real agent.
In terms of our simple formal description of agents introduced above, it is easy to see where
the diculty has arisen. In designing the agent program, logicists and decision theorists
have concentrated on specifying an optimal agent function fopt in order to guarantee the
selection of the best action history. The function fopt is independent of the architecture M .
Unfortunately, no real program in LM implements this function in a non-trivial environment,
because optimal actions cannot usually be computed before the next percept arrives. That
is, quite frequently, fopt 62 Feasible(M ).
Suppose the environment consists of games of chess under tournament rules against some
population of human grandmasters, and suppose M is some standard personal computer.
Then fopt describes an agent that always plays in such a way as to maximize its total
expected points against the opposition, where the maximization is over the moves it makes.
We claim that no possible program can play this way. It is quite possible, using depth-rst
alpha-beta search to termination, to execute the program that chooses (say) the optimal
minimax move in each situation, but the agent function induced by this program is not the
same as fopt. In particular, it ignores such percepts as the dropping of its ag indicating a
loss on time.
The trouble with the perfect rationality denition arose because of unconstrained optimization over the space of f 's in the determination of fopt , without regard to feasibility.
(Similarly, metalevel rationality assumes unconstrained optimization over the space of deliberations.) To escape this quandary, we propose a machine-dependent standard of rationality, in which we maximize V over the implementable set of agent functions Feasible(M ).
That is, we impose optimality constraints on programs rather than on agent functions or
deliberations.

Denition 6 A bounded-optimal agent with architecture M for a set E of environments
has an agent program lopt such that
lopt = argmaxl2LM V (l; M; E)

We can see immediately that this specication avoids the most obvious problems with
Type I and Type II rationality. Consider our chess example, and
suppose the computer has
26
2
a total program memory of 8 megabytes. Then there are 2 possible programs that can
be represented in the machine, of which a much smaller number play legal chess. Under
tournament conditions, one or more of these programs will have the best expected performance. Each is a suitable candidate for lopt. Thus bounded optimality is, by denition, a
feasible specication; moreover, a program that achieves it is highly desirable. We are not
yet ready to announce the identity of lopt for chess on an eight-megabyte PC, so we will
begin with a more restricted problem.

4. Provably Bounded-Optimal Agents

In order to construct a provably bounded optimal agent, we must carry out the following
steps:
 Specify the properties of the environment in which actions will be taken, and the
utility function on the behaviours.
584

Provably bounded-optimal agents

 Specify a class of machines on which programs are to be run.
 Propose a construction method.
 Prove that the construction method succeeds in building bounded optimal agents.
The methodology is similar to the formal analysis used in the eld of optimal control, which
studies the design of controllers (agents) for plants (environments). In optimal control
theory, a controller is viewed as an essentially instantaneous implementation of an optimal
agent function. In contrast, we focus on the computation time required by the agent, and
the relation between computation time and the dynamics of the environment.

4.1 Episodic, real-time task environments

In this section, we will consider a restricted class of task environments which we call episodic
environments. In an episodic task environment, the state history generated by the actions of
the agent can be considered as divided into a series of episodes, each of which is terminated
by an action. Let A?  A be a distinguished set of actions that terminate an episode.
The utility of the complete history is given by the sum of the utilities of each episode,
which is determined in turn by the state sequence. After each A 2 A?, the environment
\resets" to a state chosen at random from a stationary probability distribution Pinit . In
order to include the eects of the choice of A in the utility of the episode, we notionally
divide the environment state into a \conguration" part and a \value" part, such that
the conguration part determines the state transitions while the value part determines the
utility of a state sequence. Actions in A? reset the conguration part, while their \value"
is recorded in the value part. These restrictions mean that each episode can be treated as
a separate decision problem, and translate into the following property: if agent program l1
has higher expected utility on individual episodes than agent l2, it will have higher expected
utility in the corresponding episodic task environment.
A real-time task environment is one in which the utility of an action depends on the
time at which it is executed. Usually, this dependence will be suciently strong to make
calculative rationality an unacceptably bad approximation to perfect rationality.
An automated mail sorter4 provides an illustrative example of an episodic task environment (see Figure 1). Such a machine scans handwritten or printed addresses (zipcodes) on
mail pieces and dispatches them to appropriate bins. Each episode starts with the arrival of
a new mail piece and terminates with the execution of the physical action recommended by
the sorter: routing of the piece to a specic bin. The \conguration part" of the environment corresponds to the letter feeder side, which provides a new, randomly selected letter
after the previous letter is sorted. The \value part" of the state corresponds to the state of
the receiving bins, which determines the utility of the process. The aim is to maximize the
accuracy of sorting while minimizing the reject percentage and avoiding jams. A jam occurs
if the current piece is not routed to the appropriate bin, or rejected, before the arrival of
the next piece.
We now provide formal denitions for three varieties of real-time task environments:
xed deadlines, xed time cost and stochastic deadlines.
4. See (Sackinger et al. 1992; Boser et al. 1992) for details of an actual system. The application was
suggested to us by Bernhard Boser after an early presentation of our work at the 1992 NEC Symposium.

585

Russell & Subramanian

camera
sacks of mail

zipcode
buckets

reject

Figure 1: An automated mail-sorting facility provides a simple example of an episodic,
real-time task environment.
4.1.1 Fixed deadlines

The simplest and most commonly studied kind of real-time task environment contains a
deadline at a known time. In most work on real-time systems, such deadlines are described
informally and systems are built to meet the deadline. Here, we need a formal specication
in order to connect the description of the deadline to the properties of agents running in
deadline task environments. One might think that deadlines are part of the environment
description, but in fact they are mainly realized as constraints on the utility function. One
can see this by considering the opposite of a deadline | the \starter's pistol." The two
are distinguished by diering constraints on the utilities of acting before or after a specic
time.
Denition 7 Fixed deadline: The task environment hE; U i has a xed deadline at time td
if the following conditions hold.
 Taking an action in A? at any time before the deadline results in the same utility:

U ([E; At1]) = U ([E; A(2td ,1)  AT1 (t)])
where \" denotes sequence concatenation, t  td , AT1 (t) 2 A? , and A(1t,1) and A(2td ,1)
contain no action in A?.
 Actions taken after td have no eect on utility:

U ([E; At1])  U ([E; At2]) if U ([E; At1d ])  U ([E; At2d ]) and t  td
4.1.2 Fixed time cost

Task environments with approximately xed time cost are also very common. Examples
include consultations with lawyers, keeping a taxi waiting, or dithering over where to invest
one's money. We can dene a task environment with xed time cost c by comparing the
utilities of actions taken at dierent times.
586

Provably bounded-optimal agents

Denition 8 Fixed time cost:
The task environment hE; U i has a xed time cost if, for
t
t
any action history prexes A11 and A22 satisfying
(1) AT1 (t1) 2 A? and AT2 (t2) = AT1 (t1)
(2) A(1t1 ,1) and A(2t2 ,1) contain no action in A?
the utilities dier by the dierence in time cost:
U ([E; At22 ]) = U ([E; At11 ]) , c(t2 , t1)

Strictly speaking, there are no task environments with xed time cost. Utility values have a
nite range, so one cannot continue incurring time costs indenitely. For reasonably short
times and reasonably small costs, a linear utility penalty is a useful approximation.
4.1.3 Stochastic deadlines

While xed-deadline and xed-cost task environments occur frequently in the design of
real-time systems, uncertainty about the time-dependence of the utility function is more
common. It also turns out to be more interesting, as we see below.
A stochastic deadline is represented by uncertainty concerning the time of occurrence of
a xed deadline. In other words, the agent has a probability distributionPpd for the deadline
time td . We assume that the deadline must come eventually, so that t2T pd (t) = 1. We
also dene the cumulative deadline distribution Pd .
If the deadline does not occur at a known time, then we need to distinguish between
two cases:
 The agent receives a percept, called a herald (Dean & Boddy, 1988), which announces
an impending deadline. We model this using a distinguished percept Od :

OT(td ) = Od
If the agent responds immediately, then it \meets the deadline."
 No such percept is available, in which case the agent is walking blindfolded towards
the utility cli. By deliberating further, the agent risks missing the deadline but may
improve its decision quality. An example familiar to most readers is that of deciding
whether to publish a paper in its current form, or to embellish it further and risk
being \scooped." We do not treat this case in the current paper.
Formally, the stochastic deadline case is similar to the xed deadline case, except that td is
drawn from the distribution pd . The utility of executing an action history prex At in E is
the expectation of the utilities of that state history prex over the possible deadline times.
Denition 9 Stochastic deadline: A task environment class hE; U i of xed-deadline task
environments has a stochastic deadline distributed according to pd if, for any action history
prex At ,
X
U ([E; At ]) = pd (t0)U ([Et ; At ])
t 2T

0

0

where hEt ; U i is a task environment in hE; U i with a xed deadline at t0.
0

587

Russell & Subramanian

The mail sorter example is well described by a stochastic deadline. The time between
the arrival of mail pieces at the image processing station is distributed according to a density
function pd , which will usually be Poisson.

4.2 Agent programs and agent architecture

We consider simple agent programs for episodic task environments, constructed from elements of a set R = fr1 ; : : : ; rn g of decision procedures or rules. Each decision procedure
recommends (but does not execute) an action Ai 2 A?, and an agent program is a xed
sequence of decision procedures. For our purposes, a decision procedure is a black box with
two parameters:

 a run time ti  0, which is an integer that represents the time taken by the procedure
to compute an action.

 a quality qi  0, which is a real number. This gives the expected reward resulting
from executing its action Ai at the start of an episode:

qi = U ([E; Ai])

(1)

Let MJ denote an agent architecture that executes decision procedures in the language J .
Let tM denote the maximum runtime of the decision procedures that can be accommodated
in M . For example, if the runtime of a feedforward neural network is proportional to its
size, then tM will be the runtime of the largest neural network that ts in M .
The architecture M executes an agent program s = s1 : : : sm by running each decision
procedure in turn, providing the same input to each as obtained from the initial percept.
When a deadline arrives (at a xed time td , or heralded by the percept Od ), or when
the entire sequence has been completed, the agent selects the action recommended by the
highest-quality procedure it has executed:

M (s; I T(td); OT(td)) = hi0; action(I T(td ))i
M (s; I T(ts); OT(ts)) = hi0; action(I T(ts))i where ts = Psi2s ti
M (s; I T(t); Od) = hi0; action(I T(t))i

(2)

where M updates the agent's internal state history I T(t) such that action(I T(t)) is the
action recommended by a completed decision procedure with the highest quality. When
this action is executed, the internal state of the agent is re-initialized to i0 . This agent
design works in all three of the task environment categories described above.
Next we derive the value V (s; M; E ) of an agent program s in environment E running
on M for the three real-time regimes and show how to construct bounded optimal agents
for these task environments.

4.3 Bounded optimality with xed deadlines

From Equation 2, we know that the agent picks the action in A? recommended by the
decision procedure r with the highest quality that is executed before the deadline td arrives.
588

Provably bounded-optimal agents

P

Let s1 : : : sj be the longest prex of the program s such that ji=1 ti  td . From Denition 7
and Equation 1, it follows that
V (s; M; E ) = Qj
(3)
where Qi = maxfq1; : : : ; qi g. Given this expression for the value of the agent program, we
can easily show the following:
Theorem 1 Let r = arg maxri 2 R;titd qi . The singleton sequence r is a bounded optimal
program for M in an episodic task environment with a known deadline td.
That is, the best program is the single decision procedure of maximum quality whose runtime
is less than the deadline.

4.4 Bounded optimality with xed time cost

From Equation 2, we know that the agent picks the action in A? recommended by the best
decision procedure in the sequence, since M runs the entire sequence s = s1 : : : sm when
there is no deadline. From Denition 8 and Equation 1, we have

V (s; M; E ) = Qm , c

m
X
i=1

ti

(4)

Given this expression for the value of the agent program, we can easily show the following:
Theorem 2 Let r = arg maxri 2 R qi , cti . The singleton sequence r is a bounded optimal
program for M in an episodic task environment with a xed time cost c.
That is, the optimal program is the single decision procedure whose quality, net of time
cost, is highest.

4.5 Bounded optimality with stochastic deadlines

With a stochastic deadline distributed according to pd, the value of an agent program
: : sm is an expectation. From Denition 9, we can calculate this as
P ps (t=)V s(s;1 :M;
Et), where hEt; U i is a task environment with a xed deadline at t. Aft2T d
ter substituting for V (s; M; Et) from Equation 3, this expression simplies to a summation,
over the procedures in the sequence, of the probability of interruption after the ith procedure
in the sequence multiplied by the quality of the best completed decision procedure:
m
X
Pi
V (s)  V (s; M; E) = [Pd(Pij+1
(5)
=1 tj ) , Pd ( j =1 tj )]Qi
i=1
Rt
where Pd (t) = ,1
pd(t0)dt0 and Pd (t) = 1 for t  Pmi=1 ti.
A simple example serves to illustrate the value function. Consider R = fr1 ; r2 ; r3g. The
rule r1 has a quality of 0.2 and needs 2 seconds to run: we will represent this by r1 = (0:2; 2).
The other rules are r2 = (0:5; 5); r3 = (0:7; 7). The deadline distribution function pd is a
uniform distribution over 0 to 10 seconds. The value of the sequence r1 r2r3 is
V (r1 r2r3 ) = [:7 , :2]:2 + [1 , :7]:5 + [1 , 1]:7 = :25
A geometric intuition is given by the notion of a performance prole, as shown in Figure 2.

589

Russell & Subramanian

q

0.7
0.5
0.2

p(t)
t

2

5

7

Figure 2: Performance prole for r1r2 r3, with pd superimposed.

Denition 10 Performance prole: For a sequence s, the performance prole Qs (t) gives
the quality of the action returned if the agent is interrupted at t:

Qs(t) = maxfqi :

i
X

j =1

tj  tg

For a uniform deadline density function, the value of a sequence is proportional to the
area under the performance prole up to the last possible interrupt time. Note that the
height of the prole during the interval of length ti while rule i is running is the quality of
the best of the previous rules.
From Denition 10, we have the following obvious property:
Lemma 1 The performance prole of any sequence is monotonically nondecreasing.
It is also the case that a sequence with higher quality decisions at all times is a better
sequence:
Lemma 2 If 8t Qs1 (t)  Qs2 (t), then V (s1)  V (s2 ).
In this case we say that Qs1 dominates Qs2 .
We can use the idea of performance proles to establish some useful properties of optimal
sequences.
Lemma 3 There exists an optimal sequence that is sorted in increasing order of q's.

P

Without Lemma 3, there are ni=1 i! possible sequences to consider. The ordering constraint eliminates all but 2n sequences. It also means that in proofs of properties of sequences, we now need consider only ordered sequences. In addition, we can replace Qi in
Equation 5 by qi .
The following lemma establishes that a sequence can always be improved by the addition
of a better rule at the end:
Lemma 4 For every sequence s = s1 : : : sm sorted in increasing order of quality, and single
step z with qz  qsm , V (sz )  V (s).
590

Provably bounded-optimal agents

Corollary 1 There exists an optimal sequence ending with the highest-quality rule in R.
The following lemma reects the obvious intuition that if one can get a better result in
less time, there's no point spending more time to get a worse result:
Lemma 5 There exists an optimal sequence whose rules are in nondecreasing order of ti .
We now apply these preparatory results to derive algorithms that construct bounded
optimal programs for various deadline distributions.
4.5.1 General distributions

For a general deadline distribution, the dynamic programming method can be used to obtain
an optimal sequence of decision rules in pseudo-polynomial time. We construct an optimal
sequence by using the denition of V (s; M; E ) in Equation 5. Optimal sequences generated
by the methods are ordered by qi, in accordance with Lemma 3.
We construct the table S (i; t), where each entry in the table is the highest value of
any sequence that ends with rule ri at time t. We assume the rule indices are arranged
in
P
increasing order of quality, and t ranges from the start time 0 to the end time L = ri 2R ti .
The update rule is:

S (i; t) = maxk2[0:::i,1][S (k; t , ti ) + (qi , qk )[1 , Pd (t)]]
with boundary condition
S (i; 0) = 0 for each rule i and S (0; t) = 0 for each time t
From Corollary 1, we can read o the best sequence from the highest value in row n of the
matrix S .
Theorem 3 The DP algorithm computes an optimal sequence in time O(n2L) where n is
the number of decision procedures in R.

The dependence on L in the time complexity of the DP algorithm means that the algorithm is not polynomial in the input size. Using standard rounding and scaling methods,
however, a fully polynomial approximation scheme can be constructed. Although we do not
have a hardness proof for the problem, John Binder (1994) has shown that if the deadline
distribution is used as a constant-time oracle for nding values of P (t), any algorithm will
require an exponential number of calls to the oracle in the worst case.
4.5.2 Long uniform distributions

If the deadline is uniformly distributed over a time interval greater than the sum of the
running times of the rules, we will call the distribution a long uniform distribution. Consider
the rule sequence s = s1 : : : sm drawn from the rule set R. With a long uniform distribution,
the probability that the deadline arrives during rule si of the sequence s is independent of
the time at which si starts. This permits a simpler form of Equation 5:

V (s; M; E) = Pmi=1,1 Pd (ti+1)qi + qm (1 , Pmi=1 Pd (ti))
591

(6)

Russell & Subramanian

To derive an optimal sequence under a long uniform distribution, we obtain a recursive
specication of the value of a sequence as with a 2 R and s = s1 : : : sm being some sequence
in R.

V (as; M; E) = V (s; M; E) + qaPd (t1) , qmPd (ta)

(7)

This allows us to dene a dynamic programming scheme for calculating an optimal sequence
using a state function S (i; j ) denoting the highest value of a rule sequence that starts with
rule i and ends in rule j . From Lemma 3 and Equation 7, the update rule is:

S (i; j ) = maxi<kj [S (k; j ) + Pd (tk )qi , Pd (ti)qj ]

(8)

with boundary condition

S (i; i) = (1 , Pd (ti))qi

(9)

From Corollary 1, we know that an optimal sequence for the long uniform distribution ends
in rn , the rule with the highest quality in R. Thus, we only need to examine S (i; n); 1 
i  n. Each entry requires O(n) computation, and there are n entries to compute. Thus,
the optimal sequence for the long uniform case can be calculated in O(n2 ).

Theorem 4 An optimal sequence of decision procedures for a long uniform deadline distribution can be determined in O(n2) time where n is the number of decision procedures in
R.
4.5.3 Short uniform distributions

P

When ni=1 Pd (ti) > 1, for a uniform deadline distribution Pd, we call it short. This means
that some sequences are longer than the last possible deadline time, and therefore some rules
in those sequences have no possibility of executing before the deadline. For such sequences,
we cannot use Equation 7 to calculate V (s). However, any such sequence can be truncated
by removing all rules that would complete execution after the last possible deadline. The
value of the sequence is unaected by truncation, and for truncated sequences the use of
Equation 7 is justied. Furthermore, there is an optimal sequence that is a truncated
sequence.
Since the update rule 8 correctly computes S (i; j ) for truncated sequences, we can use
it with short uniform distributions provided we add a check to ensure that the sequences
considered are truncated. Unlike the long uniform case, however, the identity of the last rule
in an optimal sequence is unknown, so we need to compute all n2 entries in the S (i; j ) table.
Each entry computation takes O(n) time, thus the time to compute an optimal sequence is
O(n3).

Theorem 5 An optimal sequence of decision procedures for a short uniform deadline distribution can be determined in O(n3) time where n is the number of decision procedures in
R.
592

Provably bounded-optimal agents

4.5.4 Exponential distributions

For an exponential distribution, Pd(t) = 1,e,t . Exponential distributions allow an optimal
sequence to be computed in polynomial time. Let pi stand for the probability that rule i
is interrupted, assuming it starts at 0. Then pi = Pd(ti ) = 1 , e,ti : For the exponential
distribution, V (s; M; E) simplies out as:

V (s; M; E) =

i
i
h
ij =1(1 , pj ) pi+1qi + mj=1(1 , pj ) qm

mX
,1 h
i=1

This yields a simple recursive specication of the value V (as; M; E) of a sequence that
begins with the rule a:

V (as; M; E) = (1 , pa )p1qa + (1 , pa)V (s; M; E)
We will use the state function S (i; j ) which represents the highest value of any rule sequence
starting with i and ending in j .

S (i; j ) = maxi<kj [(1 , pi )pk qi + (1 , pi)S (k; j )]
with boundary condition S (i; i) = qi(1 , pi). For any given j , S (i; j ) can be calculated in
O(n2). From Corollary 1, we know that there is an optimal sequence whose last element is
the highest-valued rule in R.

Theorem 6 An optimal sequence of decision procedures for an exponentially distributed
stochastic deadline can be determined in O(n2) time where n is the number of decision
procedures in R.

The proof is similar to the long uniform distribution case.

4.6 Simulation results for a mail-sorter

The preceding results provide a set of algorithms for optimizing the construction of an agent
program for a variety of general task environment classes. In this section, we illustrate these
results and the possible gains that can be realized in a specic task environment, namely,
a simulated mail-sorter.
First, let us be more precise about the utility function U on episodes. There are four
possible outcomes; the utility of outcome i is ui.
1. The zipcode is successfully read and the letter is sent to the correct bin for delivery.
2. The zipcode is misread and the letter goes to the wrong bin.
3. The letter is sent to the reject bin.
4. The next letter arrives before the recognizer has nished, and there is a jam. Since
letter arrival is heralded, jams cannot occur with the machine architecture given in
Equation 2.
593

Russell & Subramanian

1

1
mu=9

0.8

0.8

0.6

0.6
P(t)

Accuracy

lambda=0.9

0.4

0.4

0.2

0.2

0

0
0

2

4
6
Computation Time (sec)

8

10

0

2

4
6
Time (sec)

8

10

Figure 3: (a) Accuracy prole (1 , e,x ), for  = 0:9. (b) Poisson arrival distribution, for
mean  = 9 sec
Without loss of generality, we set u1 = 1:0 and u2 = 0:0. If the probability of a rule
recommending a correct destination bin is pi, then qi = piu1 + (1 , pi)u2 = pi . We assume
that u2  u3, hence there is a threshold probability below which the letter should be sent
to the reject bin instead. We will therefore include in the rule set R a rule rreject that
has zero runtime and recommends rejection. The sequence construction algorithm will then
automatically exclude rules with quality lower than qreject = u3. The overall utility for an
episode is chosen to be a linear combination of the quality of sorting (qi ), the probability of
rejection or the rejection rate (given by P (t1), where t1 is the runtime of the rst non-reject
rule executed), and the speed of sorting (measured by the arrival time mean).
The agent program in (Boser et al. 1992) uses a single neural network on a chip.
We show that under a variety of conditions an optimized sequence of networks can do
signicantly better than any single network in terms of throughput or accuracy. We examine
the following experimental conditions:
 We assume that a network that executes in time t has a recognition accuracy p that
depends on t. We consider p = 1,e,t . The particular choice of  is irrelevant because
the scale chosen for t is arbitrary. We choose  = 0:9, for convenience (Figure 3(a)).
We include rreject with qreject = u3 and treject = 0.
 We consider arrival time distributions that are Poisson with varying means. Figure 3(b) shows three example distributions, for means 1, 5, and 9 seconds.
 We create optimized sequences from sets of 40 networks with execution times taken
at equal intervals from t = 1 to 40.
 We compare
(a) BO sequence: a bounded optimal sequence;
(b) Best singleton: the best single rule;
(c) 50% rule: the rule whose execution time is the mean of the distribution (i.e., it
will complete in 50% of cases);
594

Provably bounded-optimal agents

1
BO Sequence
Best Singleton
50% Rule
90% Rule

Average utility per second

0.8

0.6

0.4

0.2

0
5

10

15

20
25
Mean arrival time

30

35

40

Figure 4: Graph showing the achievable utility per second as a function of the average time
per letter, for the four program types.  = 0:9.
(d) 90% rule: the rule whose execution time guarantees that it will complete in 90%
of cases.
In the last three cases, we add rreject as an initial step; the BO sequence will include
it automatically.
 We measure the utility per second as a function of the mean arrival rate (Figure 4).
This shows that there is an optimal setting of the sorting machinery at 6 letters per
minute (inter-arrival time = 10 seconds) for the bounded optimal program, given that
we have xed  at 0.9.
 Finally, we investigate the eect of the variance of the arrival time on the relative
performance of the four program types. For this purpose, we use a uniform distribution
centered around 20 seconds but with dierent widths to vary the variance without
aecting the mean (Figure 5).
We notice several interesting things about these results:
 The policy of choosing a rule with a 90% probability of completion performs poorly
for rapid arrival rates (  3), but catches up with the performance of the best single
rule for slower arrival rates ( > 4). This is an artifact of the exponential accuracy
prole for any  > 0:5, where the dierence in quality of the rules with run times
greater than 6 seconds is quite small.
 The policy of choosing a rule with a 50% probability of completion fares as well as
the best single rule for very high arrival rates (  2), but rapidly diverges from it
thereafter, performing far worse for arrival time means greater than 5 seconds.
595

Russell & Subramanian

1
BO Sequence
Best Singleton
50% Rule
90% Rule

Average utility per second

0.8

0.6

0.4

0.2

0
0

20

40

60
80
Variance in arrival time

100

120

Figure 5: Graphs showing the utility gain per second as a function of the arrival time
variance, for the four program types for the uniform distribution with a mean of
20 seconds.

 Both the best sequence and the best single rule give their best overall performance
at an arrival rate of around 6 letters per minute. The performance advantage of the
optimal sequence over the best single rule is about 7% at this arrival rate. It should
be noted that this is a signicant performance advantage that is obtainable with no
extra computational resources. For slower arrival rates (  7), the dierence between
the performance of the best rule and the best sequence arises from the decreased
rejection rate of the best sequence. With the exponential accuracy prole (  0:5)
the advantage of running a rule with a shorter completion time ahead of a longer rule
is the ability to reduce the probability of rejecting a letter. For high arrival rates
(inter-arrival times of 1 to 4 seconds), it is useful to have a few short rules instead of
a longer single rule.

 Figure 5 shows that the best sequence performs better than the best single rule as the

variance of the arrival time increases.5 The performance of the optimal sequence also
appears to be largely unaected by variance. This is exactly the behaviour we expect
to observe | the ability to run a sequence of rules instead of committing to a single
one gives it robustness in the face of increasing variance. Since realistic environments
can involve unexpected demands of many kinds, the possession of a variety of default
behaviours of graded sophistication would seem to be an optimal design choice for a
bounded agent.

5. The performance of the 50% rule is at because the uniform distributions used in this experiment have
xed mean and are symmetric, so that the 50% rule is always the rule that runs for 20 seconds. The
90% rule changes with the variance, and the curve exhibits some discretization eects. These could be
eliminated using a ner-grained set of rules.

596

Provably bounded-optimal agents

5. Learning Approximately Bounded-Optimal Programs

The above derivations assume that a suitable rule set R is available ab initio, with correct
qualities qi and runtimes ti , and that the deadline distribution is known. In this section, we
study ways in which some of this information can be learned, and the implications of this
for the bounded optimality of the resulting system. We will concentrate on learning rules
and their qualities, leaving runtimes and deadline distributions for future work.
The basic idea is that the learning algorithms will converge, over time, to a set of
optimal components | the most accurate rules and the most accurate quality estimates for
them. As this happens, the value of the agent constructed from the rules, using the quality
estimates, converges to the value of lopt. Thus there are two sources of suboptimality in the
learned agent:
 The rules in R may not be the best possible rules | they may recommend actions
that are of lower utility than those that would be recommended by some other rules.
 There may be errors in estimating the expected utility of the rule. This can cause the
algorithms given above to construct suboptimal sequences, even if the best rules are
available.
Our notional method for constructing bounded optimal agents (1) learns sets of individual decision procedures from episodic interactions, and (2) arranges them in a sequence
using one of the algorithms described earlier so that the performance of an agent using
the sequence is at least as good as that of any other such agent. We assume a parameterized learning algorithm LJ ;k that will be used to learn one rule for each possible runtime
k 2 f1; : : : ; tM g. Since there is never a need to include two rules with the same runtime
in the R, this obviates the need to consider the entire rule language J in the optimization
process.
Our setting places somewhat unusual requirements on the learning algorithm. Like
most learning algorithms, LJ ;k works by observing a collection T of training episodes in E,
including the utility obtained for each episode. We do not, however, make any assumptions
about the form of the correct decision rule. Instead, we make assumptions about the
hypotheses, namely that they come from some nite language Jk , the set of programs
in J of complexity at most k. This setting has been called the agnostic learning setting by
Kearns, Schapire and Sellie (1992), because no assumptions are made about the environment
at all. It has been shown (Theorems 4 and 5 in Kearns, Schapire and Sellie, 1992) that, for
some languages J , the error in the learned approximation can be bounded to within  of
the best rule in Jk that ts the examples, with probability 1 , . The sample size needed
to guarantee these bounds is polynomial in the complexity parameter k, as well as 1 and 1 .
In addition to constructing the decision procedures, LJ ;k outputs estimates of their
quality qi . Standard Cherno-Hoeding bounds can be used to limit the error in the quality
estimate to be within q with probability 1 , q . The sample size for the estimation of quality
is also polynomial in 1q and 1q .
Thus the error in each agnostically learned rule is bounded to within  of the best rule
in its complexity class with probability 1 , . The error in the quality estimation of these
rules is bounded by q with probability 1 , q . From these bounds, we can calculate a bound
on the utility decit in the agent program that we construct, in comparison to lopt :
597

Russell & Subramanian

Theorem 7 Assume an architecture MJ that executes sequences of decision procedures in
an agnostically learnable language J whose runtimes range over [1::tM ]. For real time task

environments with xed time cost, xed deadline, and stochastic deadline, we can construct
a program l such that
V (lopt ; M; E) , V (l; M; E)   + 2q
with probability greater than 1 , m( + q ), where m is the number of decision procedures in
lopt .

Proof: We prove this theorem for the stochastic deadline regime, where the bounded

optimal program is a sequence of decision procedures. The proofs for the xed cost and
xed deadline regimes, where the bounded optimal program is a singleton, follow as a
special case. Let the best decision procedures for E be the set R = fr1 ; : : : ; rn g, and
let lopt = s1  : : : sm  be an optimal sequence constructed from R. Let R = fr1 ; : : : rng be
the set of decision procedures returned by the learning algorithm. With probability greater
than 1 , m, qi , qi   for all i, where qi refers to the true quality of ri . The error in the
estimated quality q^i of decision procedure ri is also bounded: with probability greater than
1 , mq , jq^i , qi j  q for all i.
Let s = s1 : : : sm be those rules in R that come from the same runtime classes as the
rules s1  : : : sm  in R . Then, by Equation 5, we have
V (lopt ; M; E) , V (s; M; E)  
because the error in V is a weighted average of the errors in the individual qi . Similarly, we
have
jV^ (s; M; E) , V (s; M; E)j  q
Now suppose that the sequence construction algorithm applied to R produces a sequence
l = s1 : : : sl . By denition, this sequence appears to be optimal according to the estimated
value function V^ . Hence
V^ (l; M; E)  V^ (s; M; E)
As before, we can bound the error on the estimated value:
jV^ (l; M; E) , V (l; M; E)j  q
Combining the above inequalities, we have
V (lopt ; M; E) , V (l; M; E)   + 2q
0

0

2

Although the theorem has practical applications, it is mainly intended as an illustration
of how a learning procedure can converge on a bounded optimal conguration. With some
additional work, more general error bounds can be derived for the case in which the rule
execution times ti and the real-time utility variation (time cost, xed deadline, or deadline
distribution) are all estimated from the training episodes. We can also obtain error bounds
for the case in which the rule language J is divided up into a smaller number of coarser
runtime classes, rather than the potentially huge number that we currently use.
598

Provably bounded-optimal agents

6. Asymptotic Bounded Optimality

The strict notion of bounded optimality may be a useful philosophical landmark from which
to explore articial intelligence, but it may be too strong to allow many interesting, general
results to be obtained. The same observation can be made in ordinary complexity theory:
although absolute eciency is the aim, asymptotic eciency is the game. That a sorting
algorithm is O(n log n) rather than O(n2) is considered signicant, but replacing a \multiply
by 2" by a \shift-left 1 bit" is not considered a real advance. The slack allowed by the
denitions of complexity classes is essential in building on earlier results, in obtaining robust
results that are not restricted to specic implementations, and in analysing the complexity of
algorithms that use other algorithms as subroutines. In this section, we begin by reviewing
classical complexity. We then propose denitions of asymptotic bounded optimality that
have some of the same advantages, and show that classical optimality is a special case of
asymptotic bounded optimality. Lastly, we report on some preliminary investigations into
the use of asymptotic bounded optimality as a theoretical tool in constructing universal
real-time systems.

6.1 Classical complexity

A problem, in the classical sense, is dened by a pair of predicates  and such that output
z is a solution for input x if and only if (x) and (x; z ) hold. A problem instance is an
input satisfying , and an algorithm for the problem class always terminates with an output
z satisfying (x; z ) given an input x satisfying (x). Asymptotic complexity describes the
growth rate of the worst-case runtime of an algorithm as a function of the input size. We
can dene this formally as follows. Let Ta (x) be the runtime of algorithm a on input x,
and let Ta (n) be the maximum runtime of a on any input of size n. Then algorithm a has
complexity O(f (n)) if
9k; n0 8n n > n0 ) Ta (n)  kf (n)
Intuitively, a classically optimal algorithm is one that has the lowest possible complexity.
For the purposes of constructing an asymptotic notion of bounded optimality, it will be
useful to have a denition of classical optimality that does not mention the complexity
directly. This can be done as follows:
Denition 11 Classically optimal algorithm: An algorithm a is classically optimal if and
only if
9k; n0 8a0; n n > n0 ) Ta (n)  kTa (n)
To relate classical complexity to our framework, we will need to dene the special case of task
environments in which traditional programs are appropriate. In such task environments,
an input is provided to the program as the initial percept, and the utility function on
environment histories obeys the following constraint:
Denition 12 Classical task environment: hEP ; U i is a classical task environment for
problem P if
(
l outputs a correct solution for P
V (l; M; EP ) = u0 (T (l; M; EP )) ifotherwise
0

599

Russell & Subramanian

where T (l; M; EP ) is the running time for l in EP on M , M is a universal Turing machine,
and u is some positive decreasing function.
The notion of a problem class in classical complexity theory thus corresponds to a class of
classical task environments of unbounded complexity. For example, the Traveling Salesperson Problem contains instances with arbitrarily large numbers of cities.

6.2 Varieties of asymptotic bounded optimality

The rst thing we will need is a complexity measure on environments. Let n(E ) be a suitable
measure of the complexity of an environment. We will assume the existence of environment
classes that are of unbounded complexity. Then, by analogy with the denition of classical
optimality, we can dene a worst-case notion of asymptotic bounded optimality (ABO).
Letting V (l; M; n; E) be the minimum value of V (l; M; E ) for all E in E of complexity n,
we have
Denition 13 Worst-case asymptotic bounded optimality: an agent program l is timewise
(or spacewise) worst-case asymptotically bounded optimal in E on M i
9k; n0 8l0; n n > n0 ) V (l; kM; n; E)  V (l0; M; n; E)
where kM denotes a version of the machine M speeded up by a factor k (or with k times
more memory).
In English, this means that the program is basically along the right lines if it just needs a
faster (larger) machine to have worst-case behaviour as good as that of any other program
in all environments.
If a probability distribution is associated with the environment class E, then we can use
the expected value V (l; M; E) to dene an average-case notion of ABO:
Denition 14 Average-case asymptotic bounded optimality: an agent program l is timewise
(or spacewise) average-case asymptotically bounded optimal in E on M i
9k 8l0 V (l; kM; E)  V (l0 ; M; E)
For both the worst-case and average-case denitions of ABO, we would be happy with a
program that was ABO for a nontrivial environment on a nontrivial architecture M , unless
k were enormous.6 In the rest of the paper, we will use the worst-case denition of ABO.
Almost identical results can be obtained using the average-case denition.
The rst observation that can be made about ABO programs is that classically optimal
programs are a special case of ABO programs:7
6. The classical denitions allow for optimality up to a constant factor k in the runtime of the algorithms.
One might wonder why we chose to use the constant factor to expand the machine capabilities, rather
than to increase the time available to the program. In the context of ordinary complexity theory, the
two alternatives are exactly equivalent, but in the context of general time-dependent utilities, only the
former is appropriate. It would not be possible to simply \let l run k times longer," because the programs
we wish to consider control their own execution time, trading it o against solution quality. One could
imagine slowing down the entire environment by a factor of k, but this is merely a less realistic version
of what we propose.
7. This connection was suggested by Bart Selman.

600

Provably bounded-optimal agents

Theorem 8 A program is classically optimal for a given problem P if and only if it is
timewise worst-case ABO for the corresponding classical task environment class hEP ; U i.
This observation follows directly from Denitions 11, 12, and 13.
In summary, the notion of ABO will provide the same degree of theoretical robustness
and machine-independence for the study of bounded systems as asymptotic complexity does
for classical programs. Having set up a basic framework, we can now begin to exercise the
denitions.

6.3 Universal asymptotic bounded optimality

Asymptotic bounded optimality is dened with respect to a specic value function V . In
constructing real-time systems, we would prefer a certain degree of independence from the
temporal variation in the value function. We can achieve this by dening a family V of value
functions, diering only in their temporal variation. By this we mean that the value function
preserves the preference ordering of external actions over time, with all value functions in
the family having the same preference ordering.8
For example, in the xed-cost regime we can vary the time cost c to generate a family of
value functions; in the stochastic deadline case, we can vary the deadline distribution Pd to
generate another family. Also, since each of the three regimes uses the same quality measure
for actions, then the union of the three corresponding families is also a family. What we will
show is that a single program, which we call a universal program, can be asymptotically
bounded-optimal regardless of which value function is chosen within any particular family.
Denition 15 Universal asymptotic bounded optimality (UABO): An agent program l is
UABO in environment class E on M for the family of value functions V i l is ABO in E
on M for every Vi 2 V .
A UABO program must compete with the ABO programs for every individual value function
in the family. A UABO program is therefore a universal real-time solution for a given task.
Do UABO programs exist? If so, how can we construct them?
It turns out that we can use the scheduling construction from (Russell & Zilberstein,
1991) to design UABO programs. This construction was designed to reduce task environments with unknown interrupt times to the case of known deadlines, and the same insight
applies here. The construction requires the architecture M to provide program concatenation (e.g., the LISP prog construct), a conditional-return construct, and the null program
. The universal program lU has the form of a concatenation of individual programs of
increasing runtime, with an appropriate termination test after each. It can be written as
lU = [l0  l1    lj   ]
where each lj consists of a program and a termination test. The program part in lj is any
program in LM that is ABO in E for a value function Vj that corresponds to a xed deadline
at td = 2j , where  is a time increment smaller than the execution time of any non-null
program in LM .
8. The value function must therefore be separable (Russell & Wefald, 1989), since this preservation of rank
order allows a separate time cost to be dened. See chapter 9 of (Keeney & Raia, 1976) for a thorough
discussion of time-dependent utility.

601

Russell & Subramanian

q

l U on 4M

0.7

l opt on M

0.5
0.2

p(t)
t

0

10

Figure 6: Performance proles for lU running on 4M , and for lopt running on M
Before proceeding to a statement that lU is indeed UABO, let us look at an example.
Consider the simple, sequential machine architecture described earlier. Suppose we can
select rules from a three-rule set with r1 = (0:2; 2), r2 = (0:5; 5) and r3 = (0:7; 7). Since
the shortest runtime of these rules is 2 seconds, we let  = 1. Then we look at the optimal
programs l0 ; l1; l2; l3 ; : : : for the xed-deadline task environments with td = 1; 2; 4; 8; : : :.
These are:

l0 = ; l1 = r1; l2 = r1; l3 = r3 ; : : :
Hence the sequence of programs in lU is [; r1; r1 ; r3; : : :].
Now consider a task environment class with a value function Vi that species a stochastic
deadline uniformly distributed over the range [0: : : 10]. For this class, lopt = r1 r2 is a
bounded optimal sequence.9 It turns out that lU has higher utility than lopt provided it is
run on a machine that is four times faster. We can see this by plotting the two performance
proles: QU for lU on 4M and Qopt for lopt on M . QU dominates Qopt, as shown in Figure 6.
To establish that the lU construction yields UABO programs in general, we need to
dene a notion of worst-case performance prole. Let Q (t; l; M; n; E) be the minimum
value obtained by interrupting l at t, over all E in E of complexity n. We know that each
lj in lU satises the following:

8l0; n n > nj ) Vj(lj ; kj M; n; E)  Vj(l0 ; M; n; E)
for constants kj , nj . The aim is to prove that

8Vi 2 V 9k; n0 8l0; n n > n0 ) Vi(lU ; kM; n; E)  Vi(l0; M; n; E)
Given the denition of worst-case performance prole, it is fairly easy to show the following
lemma (the proof is essentially identical to the proof of Theorem 1 in Russell and Zilberstein,
1991):
9. Notice that, in our simple model, the output quality of a rule depends only on its execution time and
not on the input complexity. This also means that worst-case and average-case behaviour are the same.

602

Provably bounded-optimal agents

1
BO Sequence
ABO sequence

Average utility per second

0.8

0.6

0.4

0.2

0
5

10

15

20
25
Mean arrival time

30

35

40

i , as a function of mean
Figure 7: Throughput and accuracy improvement of lU over lopt
arrival time,  = 0.2, Poisson arrivals.

Lemma 6 If lU is a universal program in E for V , and li is ABO on M in E for Vi 2 V ,
then Q(t; lU ; kM; n; E) dominates Q(t; li ; M; n; E) for k  4 maxj kj , n > maxj nj .
This lemma establishes that, for a small constant penalty, we can ignore the specic realtime nature of the task environment in constructing bounded optimal programs. However,
we still need to deal with the issue of termination. It is not possible in general for lU
to terminate at an appropriate time without access to information concerning the timedependence of the utility function. For example, in a xed-time-cost task environment, the
appropriate termination time depends on the value of the time cost c.
For the general case with deterministic time-dependence, we can help out lU by supplying, for each Vi , an \aspiration level" Qi (ti; li ; M; n; E), where ti is the time at which
li acts. lU terminates when it has completed an lj such that qj  Qi (ti ; li; M; n; E). By
construction, this will happen no later than ti because of Lemma 6.

Theorem 9 In task environments with deterministic time-dependence, an lU with a suitable
aspiration level is UABO in E on M .
With deadline heralds, the termination test is somewhat simpler and does not require any
additional input to lU .
Theorem 10 In a task environment with stochastic deadlines, lU is UABO in E on M if
it terminates when the herald arrives.
Returning to the mail-sorting example, it is fairly easy to see that lU (which consists of
a sequence of networks, like the optimal programs for the stochastic deadline case) will be
ABO in the xed-deadline regime. It is not so obvious that it is also ABO in any particular
603

Russell & Subramanian

stochastic deadline case | recall that both regimes can be considered as a single family.
We have programmed a constructor function for universal programs, and applied it to the
mail-sorter environment class. Varying the letter arrival distribution gives us dierent value
functions Vi 2 V . Figure 7 shows that lU (on 4M ) has higher throughput and accuracy
i across the entire range of arrival distributions.
than lopt
Given the existence of UABO programs, it is possible to consider the behaviour of compositions thereof. The simplest form of composition is functional composition, in which
the output of one program is used as input by another. More complex, nested compositional structures can be entertained, including loops and conditionals (Zilberstein, 1993).
The main issue in constructing UABO compositions is how to allocate time among the
components. Provided that we can solve the time allocation problem when we know the
total runtime allowed, we can use the same construction technique as used above to generate composite UABO programs, where optimality is among all possible compositions of
the components. Zilberstein and Russell (1993), show that the allocation problem can be
solved in linear time in the size of the composite system, provided the composition is a tree
of bounded degree.

7. Conclusions And Further Work
We examined three possible formal bases for articial intelligence, and concluded that
bounded optimality provides the most appropriate goal in constructing intelligent systems.
We also noted that similar notions have arisen in philosophy and game theory for more or
less the same reason: the mismatch between classically optimal actions and what we have
called feasible behaviours|those that can be generated by an agent program running on a
computing device of nite speed and size.
We showed that with careful specication of the task environment and the computing
device one can design provably bounded-optimal agents. We exhibited only very simple
agents, and it is likely that bounded optimality in the strict sense is a dicult goal to
achieve when a larger space of agent programs is considered. More relaxed notions such
as asymptotic bounded optimality (ABO) may provide more theoretically robust tools for
further progress. In particular, ABO promises to yield useful results on composite agent
designs, allowing us to separate the problem of designing complex ABO agents into a discrete
structural problem and a continuous temporal optimization problem that is tractable in
many cases. Hence, we have reason to be optimistic that articial intelligence can be
usefully characterized as the study of bounded optimality. We may speculate that provided
the computing device is neither too small (so that small changes in speed or size cause
signicant changes in the optimal program design) nor too powerful (so that classically
optimal decisions can be computed feasibly), ABO designs should be stable over reasonably
wide variations in machine speed and size and in environmental complexity. The details of
the optimal designs may be rather arcane, and learning processes will play a large part in
their discovery; we expect that the focus of this type of research will be more on questions
of convergence to optimality for various structural classes than on the end result itself.
Perhaps the most important implication, beyond the conceptual foundations of the eld
itself, is that research on bounded optimality applies, by design, to the practice of articial
intelligence in a way that idealized, innite-resource models may not. We have given, by
604

Provably bounded-optimal agents

way of illustrating this denition, a bounded optimal agent: the design of a simple system
consisting of sequences of decision procedures that is provably better than any other program
in its class. A theorem that exhibits a bounded optimal design translates, by denition,
into an agent whose actual behaviour is desirable.
There appear to be plenty of worthwhile directions in which to continue the exploration
of bounded optimality. From a foundational point of view, one of the most interesting
questions is how the concept applies to agents that can incorporate a learning component.
(Note that in section 5, the learning algorithm was external to the agent.) In such a
case, there will not necessarily be a largely stable bounded optimal conguration if the
agent program is not large enough; instead, the agent will have to adapt to a shorter-term
horizon and rewrite itself as it becomes obsolete.
With results on the preservation of ABO under composition, we can start to examine
much more interesting architectures than the simple production system studied above. For
example, we can look at optimal search algorithms, where the algorithm is constrained to
apply a metalevel decision procedure at each step to decide which node to expand, if any
(Russell & Wefald, 1989). We can also extend the work on asymptotic bounded optimality
to provide a utility-based analogue to \big-O" notation for describing the performance of
agent designs, including those that are suboptimal.
In the context of computational learning theory, it is obvious that the stationarity
requirement on the environment, which is necessary to satisfy the preconditions of PAC
results, is too restrictive. The fact that the agent learns may have some eect on the
distribution of future episodes, and little is known about learning in such cases (Aldous &
Vazirani, 1990). We could also relax the deterministic and episodic requirement to allow
non-immediate rewards, thereby making connections to current research on reinforcement
learning.
The computation scheduling problem we examined is interesting in itself, and does not
appear to have been studied in the operations research or combinatorial optimization literature. Scheduling algorithms usually deal with physical rather than computational tasks,
hence the objective function usually involves summation of outputs rather than picking the
best. We would like to resolve the formal question of its tractability in the general case, and
also to look at cases in which the solution qualities of individual processes are interdependent
(such as when one can use the results of another). Practical extensions include computation
scheduling for parallel machines or multiple agents, and scheduling combinations of computational and physical (e.g., job-shop and ow-shop) processes, where objective functions are
a combination of summation and maximization. The latter extension broadens the scope
of applications considerably. An industrial process, such as designing and manufacturing a
car, consists of both computational steps (design, logistics, factory scheduling, inspection
etc.) and physical processes (stamping, assembling, painting etc.). One can easily imagine
many other applications in real-time nancial, industrial, and military contexts.
It may turn out that bounded optimality is found wanting as a theoretical framework. If
this is the case, we hope that it is refuted in an interesting way, so that a better framework
can be created in the process.
605

Russell & Subramanian

Appendix: Additional Proofs
This appendix contains formal proofs for three subsidiary lemmata in the main body of the
paper.

Lemma 3 There exists an optimal sequence that is sorted in increasing order of q's.
Proof: Suppose this is not the case, and s is an optimal sequence. Then there must be

two adjacent rules i, i + 1 where qi > qi+1 (see Figure 8). Removal of rule i + 1 yields a
sequence s0 such that Qs (t)  Qs (t), from Lemma 1 and the fact that ti+2  ti+1 + ti+2 . By
Lemma 2, s0 must also be optimal. We can repeat this removal process until s0 is ordered
by qi , proving the theorem by reductio ad absurdum.2
0

Lemma 4 For every sequence s = s1 : : : sm sorted in increasing order of quality, and single
step z with qz  qsm , V (sz )  V (s).
Proof: We calculate V (sz ) , V (s) using Equation 5 and show that it is non-negative:
V (sz ) , V (s) = qz [1 , Pd ((Pmj=1 tj ) + tz )] , qm[1 , Pd ((Pmj=1 tj ) + tz )]
P
= (qz , qm )[1 , Pd (( mj=1 tj ) + tz )]

which is non-negative since qz  qm .2
q

t i+2

qi+2
qi
qi-1
qi+1

t

ti

t i+1

Figure 8: Proof for ordering by qi; lower dotted line indicates original prole; upper dotted
line indicates prole after removal of rule i + 1.

Lemma 5 There exists an optimal sequence whose rules are in nondecreasing order of ti .
Proof: Suppose this is not the case, and s is an optimal sequence. Then there must be
two adjacent rules i, i + 1 where qi  qi+1 and ti > ti+1 (see Figure 9). Removal of rule i
yields a sequence s0 such that Qs (t)  Qs (t), from Lemma 1. By Lemma 2, s0 must also be
0

optimal. We can repeat this removal process until s0 is ordered by ti, proving the theorem
by reductio ad absurdum.2
606

Provably bounded-optimal agents

q

qi+1
qi

t i+1

qi-1

t

t i+1

ti

Figure 9: Proof for ordering by ti ; dotted line indicates prole after removal of rule i.

Acknowledgements

We would like to acknowledge stimulating discussions with Michael Fehling, Michael Genesereth, Russ Greiner, Eric Horvitz, Henry Kautz, Daphne Koller, and Bart Selman on the
subject of bounded optimality; with Dorit Hochbaum, Nimrod Megiddo, and Kevin Glazebrook on the subject of dynamic programming for scheduling problems; and with Nick
Littlestone and Michael Kearns on the subject of agnostic learning. We would also like to
thank the reviewers for their many constructive suggestions. Many of the early ideas on
which this work is based arose in discussions with the late Eric Wefald. Thanks also to
Ron Parr for his work on the uniform-distribution case, Rhonda Righter for extending the
results to the exponential distribution, and Patrick Zieske for help in implementing the dynamic programming algorithm. The rst author was supported by NSF grants IRI-8903146,
IRI-9211512 and IRI-9058427, by a visiting fellowship from the SERC while on sabbatical
in the UK, and by the NEC Research Institute. The second author was supported by NSF
grant IRI-8902721.

References

Agre, P., & Chapman, D. (1987). Pengi: An implementation of a theory of activity. In
Proc. 6th National Conference on Articial Intelligence, Seattle, WA. Morghan Kaufmann.
Aldous, D., & Vazirani, U. (1990). A markovian extension of valiant's learning model. In
Proc. 31st Annual Symposium on Foundations of Computer Science, St. Louis, MO.
IEEE Comput. Soc. Press.
Binder, J. (1994). On the complexity of deliberation scheduling with stochastic deadlines..
Boser, B. E., Sackinger, E., Bromley, J., & LeCun, Y. (1992). Hardware requirements for
neural network pattern classiers | a case study and implementation. IEEE Micro,
12, 32{40.
Brandt, R. (1953). In search of a credible form of rule utilitarianism. In Nakhnikian, G., &
Castaneda, H. (Eds.), Morality and the Language of Conduct.
607

Russell & Subramanian

Breese, J. S., & Fehling, M. R. (1990). Control of problem-solving: Principles and architecture. In Shachter, R. D., Levitt, T., Kanal, L., & Lemmer, J. (Eds.), Uncertainty in
Articial Intelligence 4. North Holland: Amsterdam.
Brooks, R. A. (1986). A robust, layered control system for a mobile robot. IEEE Journal
of Robotics and Automation, 2, 14{23.
Cherniak, C. (1986). Minimal rationality. MIT Press: Cambridge.
Dean, T., & Boddy, M. (1988). An analysis of time-dependent planning. In Proc. of AAAI88, pp. 49{54.
Dean, T. L., & Wellman, M. P. (1991). Planning and control. Morgan Kaufmann: San
Mateo, CA.
Dennett, D. (1986). The moral rst aid manual. Tanner lectures on human values, University
of Michigan.
Doyle, J. (1983). What is rational psychology? toward a modern mental philosophy. AI
Magazine, 4, 50{53.
Doyle, J. (1988). Articial intelligence and rational self-government. Tech. rep.. Technical
report CMU-CS-88-124.
Doyle, J., & Patil, R. (1991). Two theses of knowledge representation: language restrictions, taxonomic classication, and the utility of representation services. Articial
intelligence, 48, 261{297.
Etzioni, O. (1989). Tractable decision-analytic control. In Proc. of 1st International Conference on Knowledge Representation and Reasoning, pp. 114{125.
Fehling, M., & Russell, S. J. (1989). Proceedings of the AAAI Spring Symposium on Limited
Rationality. AAAI.
Genesereth, M. R., & Nilsson, N. J. (1987). Logical Foundations of Articial Intelligence.
Morgan Kaufmann: Mateo, CA.
Good, I. J. (1971). Twenty-seven principles of rationality. In In Godambe, V. P., & Sprott,
D. A. (Eds.), Foundations of Statistical Inference, pp. 108{141. Holt, Rinehart, Winston.: Toronto.
Hansson, O., & Mayer, A. (1989). Heuristic search as evidential reasoning. In Proceedings
of the Fifth Workshop on Uncertainty in Articial Intelligence, Windsor, Ontario.
Horvitz, E. J. (1988). Reasoning about beliefs and actions under computational resource
constraints. In Levitt, T., Lemmer, J., & Kanal, L. (Eds.), Uncertainty in Articial
Intelligence 3. North Holland: Amsterdam.
Kearns, M., Schapire, R., & Sellie, L. (1992). Toward ecient agnostic learning. In Proc. 5th
Ann. Workshop on Computational Learning Theory, Pittsburgh, PA. Morgan Kaufmann.
608

Provably bounded-optimal agents

Keeney, R., & Raia, H. (1976). Decisions with multiple objectives: Preferences and value
tradeos. Wiley: New York.
Levesque, H., & Brachman, R. (1987). Expressiveness and tractability in knowledge representation and reasoning. Computational Intelligence, 3, 78{93.
Luby, M., Sinclair, A., & Zuckerman, D. (1993). Optimal speedup of las vegas algorithms.
Information Processing Letters, 47, 173{80.
McCarthy, J. (1958). Programs with common sense. In Proceedings of the Symposium on
the Mechanization of Thought Processes, Teddington, England: HMSO.
Newell, A. (1981). The knowledge level. AI Magazine, 2, 1{20.
Neyman, A. (1985). Bounded complexity justies cooperation in the nitely repeated prisoners' dilemma. Economics Letters, 19, 227{229.
Papadimitriou, C., & Yannakakis, M. (1994). On complexity as bounded rationality. In
Proc. ACM Symposium on the Theory of Computation.
Ramsey, F. P. (1931). Truth and probability. In Braithwaite, R. (Ed.), The foundations of
mathematics and other logical essays. Harcourt Brace Jovanovich: New York.
Russell, S. J., & Wefald, E. H. (1989a). On optimal game tree search using rational metareasoning. In Proc. IJCAI-89.
Russell, S. J., & Wefald, E. H. (1989b). Principles of metareasoning. In Proc. KR-89.
Russell, S. J., & Wefald, E. H. (1991). Do the right thing: Studies in limited rationality.
MIT Press: Cambridge, MA.
Russell, S. J., & Zilberstein, S. (1991). Composing real-time systems. In Proc. IJCAI-91,
Sydney.
Sackinger, E., Boser, B. E., Bromley, J., & LeCun, Y. (1992). Application of the anna
neural network chip to high-speed character recognition. IEEE Transactions on Neural
Networks, 3, 498{505.
Simon, H. A. (1976). On how to decide what to do. In Models of bounded rationality,
Volume 2.
Simon, H. A. (1982). Models of bounded rationality, Volume 2. MIT Press: Cambridge.
von Neumann, J., & Morgenstern, O. (1947). Theory of games and economic behavior.
Princeton University Press: Princeton.
Zilberstein, S. (1993). Operational Rationality Through Compilation of Anytime Algorithms.
Ph.D. thesis, Computer Science Division, University of California, Berkeley.
Zilberstein, S., & Russell, S. (1993). Optimal composition of real-time systems. Submitted
to Articial Intelligence.
609

Journal of Articial Intelligence Research 2 (1995) 287-318

Submitted 9/94; published 1/95

Truncating Temporal Dierences:
On the Ecient Implementation of TD()
for Reinforcement Learning
Paweยช Cichosz

Institute of Electronics Fundamentals, Warsaw University of Technology
Nowowiejska 15/19, 00-665 Warsaw, Poland

cichosz@ipe.pw.edu.pl

Abstract

Temporal dierence (TD) methods constitute a class of methods for learning predictions
in multi-step prediction problems, parameterized by a recency factor . Currently the most
important application of these methods is to temporal credit assignment in reinforcement
learning. Well known reinforcement learning algorithms, such as AHC or Q-learning, may
be viewed as instances of TD learning. This paper examines the issues of the ecient
and general implementation of TD() for arbitrary , for use with reinforcement learning
algorithms optimizing the discounted sum of rewards. The traditional approach, based on
eligibility traces , is argued to suer from both ineciency and lack of generality. The TTD
(Truncated Temporal Dierences ) procedure is proposed as an alternative, that indeed
only approximates TD(), but requires very little computation per action and can be used
with arbitrary function representation methods. The idea from which it is derived is fairly
simple and not new, but probably unexplored so far. Encouraging experimental results are
presented, suggesting that using  > 0 with the TTD procedure allows one to obtain a
signicant learning speedup at essentially the same cost as usual TD(0) learning.

1. Introduction
Reinforcement learning (RL, e.g., Sutton, 1984; Watkins, 1989; Barto, 1992; Sutton, Barto,
& Williams, 1991; Lin, 1992, 1993; Cichosz, 1994) is a machine learning paradigm that relies
on evaluative training information. At each step of discrete time a learning agent observes
the current state of its environment and executes an action . Then it receives a reinforcement value, also called a payo or a reward (punishment), and a state transition takes
place. Reinforcement values provide a relative measure of the quality of actions executed
by the agent. Both state transitions and rewards may be stochastic, and the agent does not
know either transition probabilities or expected reinforcement values for any state-action
combinations. The objective of learning is to identify a decision policy (i.e., a state-action
mapping) that maximizes the reinforcement values received by the agent in the long term .
A commonly assumed formal model of a reinforcement learning task is a Markovian decision
problem (MDP, e.g., Ross, 1983). The Markov property means that state transitions and
reinforcement values always depend solely on the current state and the current action: there
is no dependence on previous states, actions, or rewards, i.e., the state information supplied
to the agent is sucient for making optimal decisions.
All the information the agent has about the external world and its task is contained
in a series of environment states and reinforcement values. It is never told what actions
to execute in particular states, or what actions (if any) would be better than those which
c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Cichosz

it actually performs. It must learn an optimal policy by observing the consequences of its
actions. The abstract formulation and generality of the reinforcement learning paradigm
make it widely applicable, especially in such domains as game-playing (Tesauro, 1992),
automatic control (Sutton et al., 1991), and robotics (Lin, 1993). To formulate a particular
task as a reinforcement learning task, one just has to design appropriate state and action
representation, and a reinforcement mechanism specifying the goal of the task. The main
limitation of RL applications is that it is by nature a trial-and-error learning method, and
it is hardly applicable in domains where making errors costs much.
A commonly studied performance measure to be maximized by an RL agent is the
expected total discounted sum of reinforcement:

E

"1
X

t=0

 trt

#

;

(1)

where rt denotes the reinforcement value received at step t, and 0    1 is a discount
factor , which adjusts the relative signicance of long-term rewards versus short-term ones.
To maximize the sum for any positive  , the agent must take into account the delayed
consequences of its actions: reinforcement values may be received several steps after the
actions that contributed to them were performed. This is referred to as learning with delayed
reinforcement (Sutton, 1984; Watkins, 1989). Other reinforcement learning performance
measures have also been considered (Heger, 1994; Schwartz, 1993; Singh, 1994), but in this
work we limit ourselves exclusively to the performance measure specied by Equation 1.
The key problem that must be solved in order to learn an optimal policy under the
conditions of delayed reinforcement is known as the temporal credit assignment problem
(Sutton, 1984). It is the problem of assigning credit or blame for the overall outcomes
of a learning system (i.e., long-term reinforcement values) to each of its individual actions,
possibly taken several steps before the outcomes could be observed. Discussing reinforcement
learning algorithms, we will concentrate on temporal credit assignment and ignore the issues
of structural credit assignment (Sutton, 1984), the other aspect of credit assignment in RL
systems.

1.1 Temporal Dierence Methods

The temporal credit assignment problem in reinforcement learning is typically solved using
algorithms based on the methods of temporal dierences (TD). They have been introduced
by Sutton (1988) as a class of methods for learning predictions in multi-step prediction
problems. In such problems prediction correctness is not revealed at once, but after more
than one step since the prediction was made, though some partial information relevant to
its correctness is revealed at each step. This information is available and observed as the
current state of a prediction problem, and the corresponding prediction is computed as a
value of a function of states.
Consider a multi-step prediction problem where at each step it is necessary to learn a
prediction of some nal outcome. It could be for example predicting the outcome of a game
of chess in subsequent board situations, predicting the weather on Sunday on each day of
the week, or forecasting some economic indicators. The traditional approach to learning
such predictions would be to wait until the outcome occurs, keeping track of all predictions
288

Truncating Temporal Differences

computed at intermediate steps, and then, for each of them, to use the dierence between
the actual outcome and the predicted value as the training error. It is supervised learning,
where directed training information is obtained by comparing the outcome with predictions
produced at each step. Each of the predictions is modied so as to make it closer to the
outcome.
Temporal dierence learning makes it unnecessary to always wait for the outcome. At
each step the dierence between two successive predictions is used as the training error.
Each prediction is modied so as to make it closer to the next one. In fact, TD is a class
of methods referred to as TD(), where 0    1 is called a recency factor . Using  > 0
allows one to incorporate prediction dierences from more time steps, to hopefully speed
up learning.
Temporal credit assignment in reinforcement learning may be viewed as a prediction
problem. The outcome to predict in each state is simply the total discounted reinforcement
that will be received starting from that state and following the current policy. Such predictions can be used for modifying the policy so as to optimize the performance measure given
by Equation 1. Example reinforcement learning algorithms that implement this idea, called
TD-based algorithms , will be presented in Section 2.2.

1.2 Paper Overview

Much of the research concerning TD-based reinforcement learning algorithms has concentrated on the simplest TD(0) case. However, experimental results obtained with TD( > 0)
indicate that it often allows one to obtain a signicant learning speedup (Sutton, 1988;
Lin, 1993; Tesauro, 1992). It has been also suggested (e.g., Peng & Williams, 1994) that
TD( > 0) should perform better in non-Markovian environments than TD(0) (i.e., it should
be less sensitive to the potential violations of the Markov property). It is thus important
to develop ecient and general implementation techniques that would allow TD-based RL
algorithms to use arbitrary . This has been the motivation of this work.
The remainder of this paper is organized as follows. In Section 2 a formal denition of
TD methods is presented and their application to reinforcement learning is discussed. Three
example RL algorithms are briey described: AHC (Sutton, 1984), Q-learning (Watkins,
1989; Watkins & Dayan, 1992), and advantage updating (Baird, 1993). Section 3 presents
the traditional approach to TD() implementation, based on so called eligibility traces,
which is criticized for ineciency and lack of generality. In Section 4 the analysis of the
eects of the TD algorithm leads to the formulation of the TTD (Truncated Temporal
Dierences ) procedure. The two remaining sections are devoted to experimental results
and concluding discussion.

2. Denition of TD()

When Sutton (1988) introduced TD methods, he assumed they would use parameter estimation techniques for prediction representation. According to his original formulation,
states of a prediction problem are represented by vectors of real-valued features, and corresponding predictions are computed by the use of a set of modiable parameters (weights).
Under such representation learning consists in adjusting the weights appropriately on the
basis of observed state sequences and outcomes. Below we present an alternative formula289

Cichosz

tion, adopted from Dayan (1992), that simplies the analysis of the eects of the TD()
algorithm. In this formulation states may be elements of an arbitrary nite state space, and
predictions are values of some function of states. Transforming Sutton's original denition
of TD() to this alternative form is straightforward.
When discussing either the generic or RL-oriented form of TD methods, we consequently ignore the issues of function representation. It is only assumed that TD predictions or functions maintained by reinforcement learning algorithms are represented by a
method that allows adjusting function values using some error values, controlled by a learning rate parameter. Whenever we write that the value of an n-argument function ' for
arguments p0; p1; : : :; pn,1 should be updated using an error value of , we mean that
'(p0; p1; : : :; pn,1) should be moved towards '(p0; p1; : : :; pn,1) + , to a degree controlled
by some learning rate factor  . The general form of this abstract update operation is written
as
update ('; p0 ; p1; : : :; pn,1; ):
(2)
Under this convention, a learning algorithm is dened by the rule it uses for computing
error values.

2.1 Basic Formulation

Let x0; x1; : : :; xm,1 be a sequence of m states of a multi-step prediction problem. Each
state xt can be observed at time step t, and at step m, after passing the whole sequence, a
real-valued outcome z can be observed. The learning system is required to produce a corresponding sequence of predictions P (x0 ); P (x1); : : :; P (xm,1 ), each of which is an estimate
of z .
Following Dayan (1992), let us dene for each state x:
(

x (t) =

1 if xt = x
0 otherwise:

Then the TD() prediction error for each state x determined at step t is given by:

x(t) = (P (xt+1) , P (xt ))

t
X
t,k 

k=0

x (k);

(3)

where 0    1 and P (xm ) = z by denition, and the total prediction error for state x
determined after the whole observed sequence accordingly is:

x =

mX
,1
t=0

x(t) =

mX
,1 (
t=0

(P (xt+1 ) , P (xt

t
X
)) t,k 
k=0

)

x (k )

:

(4)

Thus, learning at each step is driven by the dierence between two temporally successive
predictions. When  > 0, the prediction dierence at time t aects not only P (xt ), but also
predictions from previous time steps, to an exponentially decaying degree.1
1. Alternatively, learning the prediction at step t relies not only on the prediction dierence from that
step, but also on future prediction dierences. This equivalent formulation will play a signicant role in
Section 4.

290

Truncating Temporal Differences

There are two possibilities of using such dened errors for learning. The rst is to compute total errors x for all states x, by accumulating the x (t) errors computed at each time
step t, and to use them after passing the whole state sequence to update predictions P (x).
It corresponds to batch learning mode. The second possibility, called incremental or on-line
learning, often more attractive in practice, is to update predictions at each step t using
current error values x (t). It is then necessary to modify appropriately Equation 3, so as
to take into account that predictions are changed at each step:

x (t) = (Pt(xt+1 ) , Pt (xt ))

t
X
t,k 

k=0

x (k);

(5)

where Pt (x) designates the prediction for state x available at step t.
Sutton (1988) proved the convergence of batch TD(0) for a linear representation, with
states represented as linearly independent vectors, under the assumption that state sequences are generated by an absorbing Markov process .2 Dayan (1992) extended his proof
to arbitrary .3

2.2 TD() for Reinforcement Learning

So far, this paper has presented TD as a general class of prediction methods for multi-step
prediction problems. The most important application of these methods, however, is to reinforcement learning. As a matter of fact, TD methods were formulated by Sutton (1988) as
a generalization of techniques he had previously used only in the context of temporal credit
assignment in reinforcement learning (Sutton, 1984).
As already stated above, the most straightforward way to formulate temporal credit
assignment as a prediction problem is to predict at each time step t the discounted sum of
future reinforcement
1
X
zt =  k rt+k ;
k=0

called the TD return for time t. The corresponding prediction is designated by U (xt ) and
called the predicted utility of state xt . TD returns obviously depend on the policy being
followed; we therefore assume that U values represent predicted state utilities with respect
to the current policy. For perfectly accurate predictions we would have:
U (xt) = zt = rt + zt+1 = rt + U (xt+1):
Thus, for inaccurate predictions, the mismatch or TD error is rt + U (xt+1) , U (xt). The
resulting RL-oriented TD() equations take form:

x(t) = (rt + Ut(xt+1 ) , Ut (xt))

t
X

()t,k x (k)

(6)

k=0
2. An absorbing Markov process is dened by a set of terminal states XT , a set of non-terminal states XN ,
and the set of transition probabilities Pxy for all x 2 XN and y 2 XN [ XT . The absorbing property
means that any cycles among non-terminal states cannot last indenitely long, i.e., for any starting
non-terminal state a terminal state will eventually be reached (all sequences eventually terminate).
3. Recently stronger theoretical results were proved by Dayan and Sejnowski (1994) and Jaakkola, Jordan,
and Singh (1993).

291

Cichosz

and

x =

1
X
t=0

x (t) =

1
X
t=0

(

(rt + Ut(xt+1) , Ut (xt ))

t
X
k=0

)

()t,kx (k) :

(7)

Note the following additional dierences between these equations and Equations 3 and 4:

 time step subscripts are used with U values to emphasize on-line learning mode,
 the discount applied in the sum in Equation 6 includes  as well as  for reasons that
may be unclear now, but will be made clear in Section 4.1,

 the summation in Equation 7 extends to innity, because the predicted nal outcome
is not, in general, available after any nite number of steps.

TD-based reinforcement learning algorithms may be viewed as more or less direct implementations of the general rule described by Equation 6. To see this, we will consider
three algorithms: well known AHC (Sutton, 1984) and Q-learning (Watkins, 1989; Watkins
& Dayan, 1992), and a recent development of Baird (1993) called advantage updating . All
the algorithms rely on learning certain real-valued functions dened over the state or state
and action space of a task. The  superscript used with any of the described functions
designates its optimal values (i.e., corresponding to an optimal policy). Simplied versions
of the algorithms, corresponding to TD(0), will be presented and related to Equation 6.
The presentation below is limited solely to function update rules | for a more elaborated
description of the algorithms the reader should consult the original publications of their
developers or, for AHC and Q-learning, Lin (1993) or Cichosz (1994). They are all closely
related to dynamic programming methods (Barto, Sutton, & Watkins, 1990; Watkins, 1989;
Baird, 1993), but these relations, though theoretically and practically important and fruitful, are not essential for the subject of this paper and will not be discussed.
2.2.1 The AHC Algorithm

The variation of the AHC algorithm described here is adopted from Sutton (1990). Two
functions are maintained: an evaluation function V and a policy function f . The evaluation
function evaluates each environment state and is essentially the same as what was called
above the U function, i.e., V (x) is intended to be an estimate of the discounted sum of
future reinforcement values received starting from state x and following the current policy.
The policy function assigns to each state-action pair (x; a) a real number representing
the relative merit of performing action a in state x, called the action merit . The actual
policy is determined from action merits using some, usually stochastic, action selection
mechanism, e.g., according to a Boltzmann distribution (as described in Section 5). The
optimal evaluation of state x, V  (x), is the expected total discounted reinforcement that
will be received starting from state x and following an optimal policy.
Both the functions are updated at each step t, after executing action at in state xt,
according to the following rules:

update(V; xt ; rt + Vt(xt+1 ) , Vt(xt));
update (f; xt; at; rt + Vt(xt+1) , Vt (xt)).
292

Truncating Temporal Differences

The update rule for the V -function directly corresponds to Equation 6 for  = 0. The update
rule for the policy function increases or decreases the action merit of an action depending
on whether its long-term consequences appear to be better or worse than expected. We
present this, a simplied form of AHC corresponding to TD(0), because this paper proposes
an alternative way of using TD( > 0) to that implemented by the original AHC algorithm
presented by Sutton (1984).
2.2.2 The Q-Learning Algorithm

Q-learning learns a single function of states and actions, called a Q-function . To each
state-action pair (x; a) it assigns a Q-value or action utility Q(x; a), which is an estimate of
the discounted sum of future reinforcement values received starting from state x by executing
action a and then following a greedy policy with respect to the current Q-function (i.e.,
performing in each state actions with maximum Q-values). The current policy is implicitly
dened by Q-values. When the optimal Q-function is learned, then a greedy policy with
respect to action utilities is an optimal policy.
The update rule for the Q-function is:
update(Q; xt; at; rt +  maxa Qt (xt+1; a) , Qt (xt; at)).
To show its correspondence to the TD(0) version of Equation 6, we simply assume that
predicted state utilities are represented by Q-values so that Qt (xt; at) corresponds to Ut (xt)
and maxa Qt (xt+1 ; a) corresponds to Ut (xt+1).
2.2.3 The Advantage Updating Algorithm

In advantage updating two functions are maintained: an evaluation function V and an
advantage function A. The evaluation function has essentially the same interpretation as its
counterpart in AHC, though it is learned in a dierent way. The advantage function assigns
to each state-action pair (x; a) a real number A(x; a) representing the degree to which the
expected discounted sum of future reinforcement is increased by performing action a in
state x, relative to the action currently considered best in that state. The optimal action
advantages are negative for all suboptimal actions and equal 0 for optimal actions, and can
be related to the optimal Q-values by:
A (x; a) = Q(x; a) , max
Q(x; a0):
a
0

Similarly as action utilities, action advantages implicitly dene a policy.
The evaluation and advantage functions are updated at step t by applying the following
rules:
update(A; xt; at; maxa At(xt; a) , At(xt; at) + rt + Vt(xt+1) , Vt (xt));
update (V; xt; 1 [maxa At+1 (xt) , maxa At(xt )]).
The update rule for the advantage function is somewhat more complex that the AHC or
Q-learning rules, but it still contains a term that directly corresponds to the TD(0) form of
Equation 6, by replacing V with U .
Actually, what has been presented above is a simplied version of advantage updating.
The original algorithm diers in two details:
293

Cichosz

 the time step duration t is explicitly included in the update rules, while in this
presentation we assumed t = 1,
 besides learning updates , described above, so called normalizing updates are performed.

3. Eligibility Traces

It is obvious that the direct implementation of the computation described by Equation 6 is
not too tempting. It requires maintaining x (t) values for each state
x and past time step t.
P
Note, however, that one only needs to maintain the whole sums tk=0 ()t,kx (k) for all x
and only one (current) t, which is much easier due to a simple trick. Substituting

ex (t) =

t
X

()t,k x (k);

k=0

we can dene the following recursive update rule:

ex(0) =
ex(t) =

(
(

1 if x0 = x
0 otherwise;
ex(t , 1) + 1 if xt = x
ex(t , 1)
otherwise:

(8)

The quantities ex (t) dened this way are called activity or eligibility traces (Barto,
Sutton, & Anderson, 1983; Sutton, 1984; Watkins, 1989). Whenever a state is visited, its
activity becomes high and then gradually decays until it is visited again. The update to
the predicted utility of each state x resulting from visiting state xt at time t may be then
written as
x (t) = (rt + Ut(xt+1) , Ut (xt ))ex(t);
(9)
which is a direct transformation of Equation 6.
This technique (with minor dierences) was already used in the early works of Barto
et al. (1983) and Sutton (1984), before the actual formulation of TD(). It is especially
suitable for use with parameter estimation function representation methods, such as connectionist networks. Instead of having one ex value for each state x one then has one ei
value for each weight wi . That is how eligibility traces were actually used by Barto et al.
(1983) and Sutton (1984), inspired by an earlier work of Klopf (1982). Note that in the case
of the AHC algorithm, dierent  values may be used for maintaining traces used by the
evaluation and policy functions.
Unfortunately, the technique of eligibility traces is not general enough to be easy to implement with an arbitrary function representation method. It is not clear, for example, how
it could be used with such an important class of function approximators as memory-based
(or instance-based) function approximators (Moore & Atkeson, 1992). Applied with a pure
tabular representation, it has signicant drawbacks. First, it requires additional memory locations, one per state. Second, and even more painful, is that it requires modifying both U (x)
and ex for all x at each time step. This operation dominates the computational complexity
294

Truncating Temporal Differences

of TD-based reinforcement learning algorithms, and makes using TD( > 0) much more expensive than TD(0). The eligibility traces implementation of TD() is thus, for large state
spaces, absolutely impractical on serial computers, unless an appropriate function approximator is used that allows updating function values and eligibility traces for many states
concurrently (such as a multi-layer perceptron). But even when such an approximator is
used, there are still signicant computational (both memory and time) additional costs of
using TD() for  > 0 versus TD(0). Another drawback of this approach will be revealed
in Section 4.1.

4. Truncating Temporal Dierences

This section departs from an alternative formulation of TD() for reinforcement learning.
Then we follow with relating the TD() training errors used in this alternative formulation
to TD() returns. Finally, we propose approximating TD() returns with truncated TD()
returns, and we show how they can be computed and used for on-line reinforcement learning.

4.1 TD Errors and TD Returns

Let us take a closer look at Equation 7. Consider the eects of experiencing a sequence of
states x0 ; x1; : : :; xk ; : : : and corresponding reinforcement values r0; r1; : : :; rk ; : : :. For the
sake of simplicity, assume for a while that all states in the sequence are dierent (though it
is of course impossible for nite state spaces). Applying Equation 7 to state xt under this
assumption we have:

xt = rt +h Ut(xt+1) , Ut(xt ) +
i
 rt+1 + Ut+1(xt+2) , Ut+1(xt+1) +
h
i
()2 rt+2 + Ut+2(xt+3 ) , Ut+2 (xt+2) + : : :
1
X

=

k=0

h

i

()k rt+k + Ut+k (xt+k+1 ) , Ut+k (xt+k ) :

If a state occurs several times in the sequence, each visit to that state yields a similar update.
This simple observation opens a way to an alternative (though equivalent) formulation of
TD(), oering novel implementation possibilities.
Let
0t = rt + Ut(xt+1 ) , Ut(xt)
(10)
be the TD (0) error at time step t. We dene the TD () error at time t using TD(0) errors
as follows:

t =

1
X

h

i

()k rt+k + Ut+k (xt+k+1 ) , Ut+k (xt+k ) =

k=0

1
X

()k 0t+k :

k=0

(11)

Now, we can express the overall TD() error for state x, x , in terms of t errors:

x =

1
X
t=0

tx (t):

295

(12)

Cichosz

In fact, from Equation 7 we have:

x =

1
X
t=0

0t

t
X

k=0

()t,k x (k) =

1 X
t
X

()t,k0t x (k):

(13)

t=0 k=0

Swapping the order of the two summations we get:

x =

1 X
1
X

k=0 t=k

()t,k0t x (k):

Finally, by exchanging k and t with each other, we receive:

x =

1X
1
X
t=0 k=t

()k,t0k x (t) =

1 X
1
X

t=0 k=0

()k 0t+k x (t) =

(14)
1
X
t=0

t x(t):

(15)

Note the following important dierence between x (t) (Equation 6) and t : the former
is computed at each time step t for all x and the latter is computed at each step t only
for xt. Accordingly, at step t the error value x (t) is used for adjusting U (x) for all x
and t is only used for adjusting U (xt). This is crucial for the learning procedure proposed
in Section 4.2. While applying such dened t errors on-line makes changes to predicted
state utilities at individual steps clearly dierent than those described by Equation 6, the
overall eects of experiencing the whole state sequence (i.e., the sums of all individual error
values for each state) are equivalent, as shown above.
Having expressed TD() in terms of t errors, we can gain more insight into its operation and the role of . Some denitions will be helpful. Recall that the TD return for time t
is dened as
1
X
zt =  k rt+k :
k=0

The m-step truncated TD return (Watkins, 1989; Barto et al., 1990) is received by taking
into account only the rst m terms of the above sum, i.e.,
mX
,1
[
m
]
zt =  k rt+k :
k=0
m
Note, however, that the rejected terms  rt+m +  m+1rt+m+1 + : : : can be approximated by
 mUt+m,1 (xt+m ). The corrected m-step truncated TD return (Watkins, 1989; Barto et al.,

1990) is thus:

zt(m) =

mX
,1
k=0

 k rt+k +  mUt+m,1 (xt+m ):

Equation 11 may be rewritten in the following form:

t

=
=

1
X

h

i

()k rt+k +  (1 , )Ut+k (xt+k+1 ) + Ut+k (xt+k+1 ) , Ut+k (xt+k )

k=0
1
X

h

i

()k rt+k +  (1 , )Ut+k (xt+k+1 ) , Ut (xt) +

k=0
1
X

h

i

()k Ut+k,1 (xt+k ) , Ut+k (xt+k ) :

k=1

296

(16)

Truncating Temporal Differences

Note that for  = 1 it yields:

1t =

1
X
 kr

k=0

t+k , Ut (xt) +

= zt , Ut (xt

1 h
X
k U

k=1

t+k,1 (xt+k ) , Ut+k (xt+k )

1 h
X
) + k U
k=1

i

i

t+k,1 (xt+k ) , Ut+k (xt+k )

:

If we relax for a moment our assumption about on-line learning mode and leave out time
subscripts from U values, the last term disappears and we simply have:

1t = zt , U (xt ):
Similarly for general , if we dene the TD () return (Watkins, 1989) for time t as a
weighted average of corrected truncated TD returns:
1
1
h
i
X
X
(
k
+1)

k
zt = (1 , )  zt = ()k rt+k +  (1 , )Ut+k (xt+k+1 )
k=0
k=0

(17)

and again omit time subscripts, we will receive:

t = zt , U (xt):

(18)

The last equation brings more light on the exact nature of the computation performed
by TD(). The error at time step t is the dierence between the TD() return for that step
and the predicted utility of the current state, that is, learning with that error value will
bring the predicted utility closer to the return. For  = 1 the quantity zt is the usual TD
return for time t, i.e., the discounted sum of all future reinforcement values.4 For  < 1 the
term rt+k is replaced by rt+k +  (1 , )Ut+k (xt+k+1 ), that is, the actual immediate reward
is augmented with the predicted future reward.
The denition of the TD() return (Equation 17) may be written recursively as

zt = rt +  (zt+1 + (1 , )Ut(xt+1 )):

(19)

This probably best explains the role of  in TD() learning. It determines how the return
used for improving predictions is obtained. When  = 1, it is exactly the actual observed
return, the discounted sum of all rewards. For  = 0 it is the 1-step corrected truncated
return, i.e., the sum of the immediate reward and the discounted predicted utility of the
successor state. Using 0 <  < 1 allows to smoothly interpolate between these two extremes,
relying partially on actual returns and partially on predictions.
Equation 18 holds true only for batch learning mode, but in fact TD methods have been
originally formulated for batch learning. The incremental version, more practically useful,
4. This observation corresponds to the equivalence of \generic" TD() for  = 1 to supervised learning
shown by Sutton (1988). To receive such a result it was necessary to discount prediction dierences with
 instead of  alone in Equation 6, though Sutton presenting the RL-oriented form of TD did not make
this modication.

297

Cichosz

introduces an additional term. Let Dt designate that term. By comparing Equations 16
and 17 we get:

Dt = t , (zt , Ut(xt)) =

1
X

k=1

h

i

()k Ut+k,1 (xt+k ) , Ut+k (xt+k ) :

(20)

The magnitude of this discrepancy term, and consequently its inuence on the learning
process, obviously depends on the learning rate value. To examine it further, suppose a
learning rate  is used when learning U on the basis of t errors. Let the corresponding
learning rule be:
Ut+1(xt) := Ut(xt) + t:
Then we have
Ut+1 (xt ) , Ut(xt) = (zt , Ut (xt )) + Dt
=  (z  , Ut (xt )) + 



1
X

h

()k Ut+k,1 (xt+k ) , Ut+k (xt+k )

k=1
1
X
(z  , Ut (xt )) ,  2 ()kt+k,1;
k=1

i

(21)

with equality if and only if xt+k = xt+k,1 for all k. A similar result may be obtained for the
eligibility traces implementation, with learning driven by x (t) errors dened by Equation 9.
We would then have:

Ut+1 (xt) , Ut(xt) = (z , Ut(xt )) ,  2

1
X

k=1

()k0t+k,1 ext+k (t + k , 1):

(22)

This eect may be considered another drawback of the eligibility traces implementation of
TD(), apart from its ineciency and lack of generality. Though for small learning rates
the eect of Dt is negligible, it may be still harmful in some cases, especially for large 
and .5

4.2 The TTD Procedure

We have shown that TD errors t or zt , Ut (xt ) can be used almost equivalently for TD()
learning, yielding the same overall results as the eligibility traces implementation, which has,
however, important drawbacks in practice. Nevertheless, it is impossible to use either TD()
errors t or TD() returns zt for on-line learning, since they are not available. At step t
the knowledge of both rt+k and xt+k is required for all k = 1; 2; : : :, and there is no way to
implement this in practice. Recall, however, the denition of the truncated TD return. Why
not dene the truncated TD() error and the truncated TD() return? The appropriate
denitions are:
mX
,1
;m
=
()k0t+k
(23)
t
k=0
5. Sutton (1984) presented the technique of eligibility traces as an implementation of the recency and
frequency heuristics . In this context, the phenomenon examined above may be considered a harmful
eect of the frequency heuristic. Sutton discussed an example nite-state task where this heuristic might
be misleading (Sutton, 1984, page 171).

298

Truncating Temporal Differences

and

zt;m =
=

mX
,2

h

i

h

i

h

i

()k rt+k +  (1 , )Ut+k (xt+k+1 ) + ()m,1 rt+m,1 + Ut+m,1 (xt+m )

k=0
mX
,1

()k rt+k +  (1 , )Ut+k (xt+k+1 ) + ()mUt+m,1 (xt+m ):

(24)

k=0
We call ;m
t the m-step truncated TD() error, or simply the TTD (; m) error at time
step t, and zt;m the m-step truncated TD() return, or the TTD (; m) return for time t.
Note that zt;m dened by Equation 24 is corrected , i.e., it is not obtained by simply truncating Equation 17. The correction term ()mUt+m,1 (xt+m ) results in multiplying the
last prediction Ut+m,1 (xt+m ) by  alone instead of  (1 , ), which is virtually equivalent
to using  = 0 for that step. It is done in order to include in zt;m all the available infor-

mation about the expected returns for further time steps (t + m; t + m + 1; : : :) contained
in Ut+m,1 (xt+m ). Without this correction for large  this information would be almost
completely lost.
So dened, m-step truncated TD() errors or returns, can be used for on-line learning
by keeping track of the last m visited states, and updating at each step the predicted
utility of the least recent state of those m states. This idea leads to what we call the TTD
Procedure (Truncated Temporal Dierences ), which can be a good approximation of TD()
for suciently large m. The procedure is parameterized by  and m values. An m-element
experience buer is maintained, containing records hxt,k ; at,k ; rt,k ; Ut,k (xt,k+1 )i for all
k = 0; 1; : : :; m , 1, where t is the current time step. At each step t by writing x[k] , a[k] ,
r[k], and u[k] we refer to the corresponding elements of the buer, storing xt,k , at,k , rt,k ,
and Ut,k (xt,k+1 ).6 References to U are not subscripted with time steps, since all of them
concern the values available at the current time step | in a practical implementation this
directly corresponds to restoring a function value from some function approximator or a
look-up table. Under this notational convention, the operation of the TTD(; m) procedure
is presented in Figure 1. It uses TTD(; m) returns for learning. An alternative version, using
TTD(; m) errors instead (based on Equation 11), is also possible and straightforward to
formulate, but there is no reason to use a \weaker" version (subject to the harmful eects
described by Equations 20 and 21) when a \stronger" one is available at the same cost.
At the beginning of learning, before the rst m steps are made, no learning can take
place. During these initial steps the operation of the TTD procedure reduces to updating
appropriately the contents of the experience buer. This obvious technical detail was left
out in Figure 1 for the sake of simplicity.
The TTD(; m) return value z is computed in step 5 by the repeated application of
Equation 19. The computational cost of such propagating the return in time is acceptable
in practice for reasonable values of m. For some function representation methods, such
as neural networks, the overall time complexity is dominated by the costs of retrieving a
function value and learning performed in steps 4 and 6, and the cost of computing z is
negligible. One advantage of such implementation is that it allows to use adaptive  values:
in step 5 one can use k depending on whether a[k,1] was or was not a non-policy action, or
6. This naturally means that the buer's indices are shifted appropriately on each time tick.

299

Cichosz

At each time step t:
1. observe current state xt ; x[0] := xt ;
2. select an action at for state xt ; a[0] := at ;
3. perform action at ; observe new state xt+1 and immediate reinforcement rt;
4. r[0] := rt; u[0] := U (xt+1 );
5. for k = 0; 1; : : :; m , 1 do
if k = 0 then z := r[k] + u[k]
else z := r[k] +  (z + (1 , )u[k]);
6. update (U; x[m,1] ; a[m,1]; z , U (x[m,1]));
7. shift the indices of the experience buer.
Figure 1: The TTD(; m) procedure.
\how much" non-policy it was. This renement to the TD() algorithm was suggested by
Watkins (1989) or recently Sutton and Singh (1994). Later we will see how the TTD return
computation can be performed in a fully incremental way, using constant time at each step
for arbitrary m.
Note that the function update carried out in step 6 at time t applies to the state and
action from time t , m + 1, i.e., m , 1 time steps earlier. This delay between an experience
event and learning might be found a potential weakness of the presented approach, especially
for large m. Note, however, that as a baseline in computing the error value the current utility
U (x[m,1] ) = Ut(xt,m+1) is used. This is an important point, because it guarantees that
learning will have the desired eect of moving the utility (whatever value it currently has)
towards the corresponding TTD return. If the error used in step 6 were z , Ut,m (xt,m+1 )
instead of z , Ut (xt,m+1 ), then applying it to learning at time t would be problematic.
Anyway, it seems that m should not be too large.
The TTD procedure is not an exact implementation of TD methods for two reasons.
First, it only approximates TD() returns with TTD(; m) returns. Second, it introduces
the aforementioned delay between experience and learning. I believe, however, that it is
possible to give strict conditions under which the convergence properties of TD() hold
true for the TTD implementation.
4.2.1 Choice of m

The reasonable choice of m obviously depends on . For  = 0 the best possible is m = 1
and for  = 1 and  = 1 no nite value of m is large enough to accurately approximate
TD(). Fortunately, this does not seem to be very painful. It is rather unlikely that in any
application one wanted to use the combination of  = 1 and  = 1, the more so as existing
300

Truncating Temporal Differences

previous empirical results with TD() indicate that  = 1 is usually not the optimal value
to use, and it is at best comparable with other, smaller values (Sutton, 1984; Tesauro, 1992;
Lin, 1993). Similar conclusions follow from the discussion of the choice of  presented by
Watkins (1989) or Lin (1993). For  < 1 or  < 1 we would probably like to have such a
value of m that the discount ()m is a small number. One possible denition of `small'
here could be, e.g., `much less than '. This is obviously a completely informal criterion.
Table 1 illustrates the practical eects of this heuristic. On the other hand, for too large m,
the delay between experience and learning introduced by the TTD procedure might become
signicant and cause some problems. Some of the experiments described in Section 5 have
been designed in order to test dierent values of m for xed 0 <  < 1.


0:99 0:975 0:95 0:9 0:8 0:6
minfm j ()m < 101 g 231 92
46 23 12 6
Table 1: Choosing m: an illustration.
4.2.2 Reset Operation

Until now, we have assumed that the learning process, once started, continues innitely
long. This is not true for episodic tasks (Sutton, 1984) and for many real-world tasks,
where learning must usually stop some time. This imposes the necessity of designing a
special mechanism for the TTD procedure, that will be called the reset operation . The reset
operation would be invoked after the end of each episode in episodic tasks, or after the
overall end of learning.
There is not very much to be done. The only problem that must be dealt with is that the
experience buer contains the record of the last m steps for which learning has not taken
place yet, and there will be no further steps that would make learning for these remaining
steps possible. The implementation of the reset operation that we nd the most natural
and coherent with the TTD procedure is then to simulate m additional ctious steps, so
that learning takes place for all the real steps left in the buer, and their TTD returns
remain unaected by the simulated ctious steps. The corresponding algorithm, presented
in Figure 2, is formulated as a replacement of the original algorithm from Figure 1 for the
nal time step. At the nal step, when there is no successor state, the ctious successor
state utility is assumed to be 0. This corresponds to assigning 0 to u[0] . The actual reset
operation is performed in step 5.
4.2.3 Incremental TTD

As stated above, the cost of iteratively computing the TTD(; m) return is relatively small
for reasonable m, and with some function representation methods, for which restoring and
updating function values is computationally expensive, may be really negligible. We also
argued that reasonable values of m should not be too large. On the other hand, such iterative
return computation is easy to understand and reects well the idea of TTD. That is why
301

Cichosz

At the nal time step t:
1. observe current state xt ; x[0] := xt ;
2. select an action at for state xt ; a[0] := at ;
3. perform action at ; observe immediate reinforcement rt ;
4. r[0] := rt; u[0] := 0;
5. for k0 = 0; 1; : : :; m , 1 do
(a) for k = k0; k0 + 1; : : :; m , 1 do
if k = k0 then z := r[k] + u[k]
else z := r[k] +  (z + (1 , )u[k]);
(b) update (U; x[m,1] ; a[m,1]; z , U (x[m,1]));
(c) shift the indices of the experience buer.
Figure 2: The reset operation for the TTD(; m) procedure.
we presented the TTD procedure in that form. It is possible, however, to compute the
TTD(; m) return in a fully incremental manner, using constant time for arbitrary m.
To see this, note that the denition of the TTD(; m) return (Equation 24) may be
rewritten in the following form:

zt;m =
=

mX
,1

()krt+k +

mX
,2

()k (1 , )Ut+k (xt+k+1 ) + ()m,1Ut+m,1 (xt+m )

k=0
k=0
;m
;m
St + Tt + Wt;m;

where

St;m =
Tt;m =
Wt;m

mX
,1
k=0
mX
,2
k=0

()krt+k ;
()k (1 , )Ut+k (xt+k+1 );

= ()m,1Ut+m,1(xt+m ):

Wt;m can be directly computed in constant time for any m. It is not dicult to convince

oneself that:

1 ;m
m
St;m
+1 =  St , rt + () rt+m ;
1 hT ;m ,  (1 , )U (x ) + (1 , )W ;mi :
Tt;m
=
t t+1
t
+1
 t
h

i

302

(25)
(26)

Truncating Temporal Differences

The above two equations dene the algorithm for computing incrementally St;m and Tt;m,
and consequently computing zt;m in constant time for arbitrary m, with a very small computational expense. This algorithm is strictly mathematically equivalent to the algorithm
presented in Figure 1.7 Modifying appropriately the TTD procedure is straightforward and
will not be discussed. A drawback of this modication is that it probably does not allow
the learner to use dierent (adaptive)  values at each step, i.e., it may not be possible to
combine it with the renements suggested by Watkins (1989) or Sutton and Singh (1994).
Despite this, such implementation might be benecial if one wanted to use really large m.
4.2.4 TTD-Based Implementations of RL Algorithms

To implement particular TD-based reinforcement learning algorithms on the basis of the
TTD procedure, one just has to substitute appropriate function values for U , and dene
the updating operation of step 6 in Figure 1 and step 5b in Figure 2. Specically, for the
three algorithms outlined in Section 2.2 one should:

 for AHC:
1. replace U (xt+1) with V (xt+1 ) in step 4 (Figure 1);
2. implement step 6 (Figure 1) and step 5b (Figure 2) as:
v := V (x[m,1] );
update(V; x[m,1] ; z , v );
update (f; x[m,1] ; a[m,1]; z , v );

 for Q-learning:
1. replace U (xt+1) with maxa Q(xt+1; a) in step 4 (Figure 1);
2. implement step 6 (Figure 1) and step 5b (Figure 2) as:
update(Q; x[m,1] ; a[m,1]; z , Q(x[m,1] ; a[m,1]));

 for advantage updating:
1. replace U (xt+1) with V (xt+1 ) in step 4 (Figure 1);

2. implement step 6 (Figure 1) and step 5b (Figure 2) as:
Amax := maxa A(x[m,1] ; a);
update(A; x[m,1]; a[m,1]; Amax , A(x[m,1] ; at) + z , V (x[m,1]));
update (V; x[m,1]; 1 [maxa A(x[m,1]) , Amax]).

4.3 Related Work

The simple idea of truncating temporal dierences that is implemented by the TTD procedure is not new. It was probably rst suggested by Watkins (1989). This paper owes much
to his work. But, to the best of my knowledge, this idea has never been explicitly and
7. But it is not necessarily numerically equivalent, which may sometimes cause problems in practical
implementations.

303

Cichosz

exactly specied, implemented, and tested. In this sense the TTD procedure is an original
development.
Lin (1993) used a very similar implementation of TD(), but only for what he called
experience replay , and not for actual on-line reinforcement learning. In his approach a sequence of past experiences is replayed occasionally, and during replay for each experience
the TD() return (truncated to the length of the replayed sequence) is computed by applying Equation 19, and a corresponding function update is performed. Such a learning
method is by some means more computationally expensive than the TTD procedure (especially implemented in a fully incremental manner, as suggested above), since it requires
updating predictions sequentially for all replayed experiences, besides \regular" TD(0) updates performed at each step (while TTD always requires only one update per time step),
and it does not allow the learner to take full advantage of TD( > 0), which is applied only
occasionally.
Peng and Williams (1994) presented an alternative way of combining Q-learning and
TD(), dierent than discussed in Section 2.2. Their motivation was to better estimate TD
returns by the use of TD errors. Toward that end, they used the standard Q-learning error

rt +  max
a Qt (xt+1 ; a) , Qt (xt; at )
for one-step updates and a modied error

rt +  max
a Qt (xt+1 ; a) , max
a Qt (xt ; a);
propagated using eligibility traces, thereafter. The TTD procedure achieves a similar objective in a more straightforward way, by the use of truncated TD() returns.
Other related work is that of Pendrith (1994). He applied the idea of eligibility traces in
a non-standard way to estimate TD returns. His approach is more computationally ecient
that the classical eligibility traces technique (it requires one prediction update per time
step) and is free of the potentially harmful eect described by Equation 22. The method
seems to be roughly equivalent to the TTD procedure with  = 1 and large m, though it is
probably much more implementationally complex.

5. Demonstrations

The demonstrations presented in this section use the AHC variant of the TTD procedure.
The reason is that the AHC algorithm is the simplest of the three described algorithms and
its update rule for the evaluation function most directly corresponds to TD(). Future work
will investigate the TTD procedure for the two other algorithms.
A tabular representation of the evaluation and policy functions is used. The abstract
function update operation described by Equation 2 is implemented in a standard way as

'(p0; p1; : : :; pn,1 ) := '(p0; p1; : : :; pn,1 ) + :

(27)

Actions to execute at each step are selected using a simple stochastic selection mechanism based on a Boltzmann distribution. According to this mechanism, action a is selected
304

Truncating Temporal Differences

in state x with probability

Prob(x; a) = Pexp(f (x; a )=T ) ;
a exp(f (x; a)=T )

(28)

where the temperature T > 0 adjusts the amount of randomness.

5.1 The Car Parking Problem

This section presents experimental results for a learning control problem with a relatively
large state space and hard temporal credit assignment. We call this problem the car parking
problem, though it does not attempt to simulate any real-world problem at all. Using words
such as `car', `garage', or `parking' is just a convention that simplies problem description
and the interpretation of results. The primary purpose of the experiments is neither just
to solve the problem nor to provide evidence of the usefulness of the tested algorithm
for any particular practical problem. We use this example problem in order to illustrate
the performance of the AHC algorithm implemented within the TTD framework and to
empirically evaluate the eects of dierent values of the TTD parameters  and m.
The car parking problem is illustrated in Figure 3. A car, represented as a rectangle,
is initially located somewhere inside a bounded area, called the driving area. A garage is
a rectangular area of a size somewhat larger than the car. All important dimensions and
distances are shown in the gure. The agent | the driver of the car | is required to park
it in the garage, so that the car is entirely inside. The task is episodic, though it is neither
a time-until-success nor time-until-failure task (in Sutton's (1984) terminology), but rather
a combination of both. Each episode nishes either when the car enters the garage or when
it hits a wall (of the garage or of the driving area). After an episode the car is reset to its
initial position.
5.1.1 State Representation

The state representation consists of three variables: the rectangular coordinates of the center
of the car, x and y , and the angle  between the car's axis and the x axis of the coordinate
system. The orientation of the system is shown in the gure. The initial location and
orientation of the car is xed and described by x = 6:15 m, y = 10:47 m, and  = 3:7 rad.
It was chosen so as to make the task neither too easy nor too dicult.
5.1.2 Action Representation

The admissible actions are `drive straight on', `turn left', and `turn right'. The action of
driving straight on has the eect of moving the car forward along its axis, i.e., without
changing . The actions of turning left and right are equivalent to moving along an arc with
a xed radius. The distance of each move is determined by a constant car velocity v and
simulation time step  . Exact motion equations and other details are given in Appendix A.
5.1.3 Reinforcement Mechanism

The design of the reinforcement function is fairly straightforward. The agent receives a
reinforcement value of 1 (a reward) whenever it successfully parks the car in the garage,
305

Cichosz

y0

x

xG

x1

0

x0

yG

l

w
ฮธ
y1

0

1

2

3

m

y

Figure 3: The car parking problem. The scale of all dimensions is preserved: w = 2 m,
l = 4 m, x0 = ,1:5 m, xG = 1:5 m, x1 = 8:5 m, y0 = ,3 m, yG = 3 m, y1 = 13 m.
and a reinforcement value of ,1 (a punishment) whenever it hits a wall. At all other time
steps the reinforcement is 0. That is, non-zero reinforcements are received only at the last
step of each episode. This involves a relatively hard temporal credit assignment problem,
providing a good experimental framework for testing the eciency of the TTD procedure.
The problem is hard not only because of reinforcement delay, but also because punishments
are much more frequent than rewards: it is much easier to hit a wall than to park the car
correctly.
With such a reinforcement mechanism as presented above, an optimal policy for any
0 <  < 1 is a policy that allows to park the car in the garage in the smallest possible
number of steps.
306

Truncating Temporal Differences

5.1.4 Function Representation

The car parking problem has a continuous state space. It is articially discretized | divided
into a nite number of disjoint regions by quantizing the three state variables, and then a
function value for each region is stored in a look-up table. The quantization thresholds are:

 for x: ,0:5, 0:0, 0:5, 1:0, 2:0, 3:0, 4:0, 6:0 m,
 for y: 0:5, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 8:0, 10:0 m,
21
29 3 31
 for : 19
20  ,  , 20  , : : :, 20  , 2  , 20  rad.
This yields 9  10  14 = 1260 regions. Of course many of them will never be visited. The

threshold values were chosen so as to make the resulting discrete state space of a moderate
size. The quantization is dense near the garage, and becomes more sparse as the distance
from the garage increases.
5.1.5 Experimental Design and Results

Our experiments with applying the TTD procedure to the car parking problem are divided
into two studies, testing the eects of the two TTD parameters  and m. The parameter
settings for all experiments are presented in Table 2. The symbols  and  are used to
designate the learning rates for the evaluation and policy functions, respectively. The initial
values of the functions were all set to 0, since we assumed that no knowledge is available
about expected reinforcement levels.
Study TTD Parameters
Number 
m
0
0:3
0:5
1
0:7
25
0:8
0:9
1
5
10
2
0:9
15
20

Learning Rates


0:7
0:5
0:5
0:5
0:5
0:25
0:25
0:25
0:25
0:25
0:25



0:7
0:5
0:5
0:5
0:5
0:25
0:25
0:25
0:25
0:25
0:25

Table 2: Parameter settings for the experiments with the car parking problem.
As stated above, the experiments were designed to test the eects of the two TTD
parameters. The other parameters were assigned values according to following principles:

 the discount factor  was xed and equal 0:95 in all experiments,
307

Cichosz

 the temperature value was also xed and set to 0:02, which seemed to be equally good

for all experiments,
 the learning rates  and  were roughly optimized in each experiment.8
Each experiment continued for 250 episodes, the number selected so as to allow all or
almost all runs of all experiments to converge. The results presented for all experiments
are averaged over 25 individual runs, each diering only in the initial seed of the random
number generator. This number was chosen as a reasonable compromise between the reliability of results and computational costs. The results are presented as plots of the average
reinforcement value per time step for the previous 5 consecutive episodes versus the episode
number.
Study 1: Eects of . The objective of this study was to examine the eects of various
 values on learning speed and quality, with m set to 25. The value m = 25 was found to be
large enough for all the tested  values (perhaps except  = 1).9 Smaller m values might be
used for small  (in particular, m = 1 for  = 0), but it was kept constant for consistency.
Reinf/Step
0.04
0.02
0
-0.02
-0.04
-0.06
-0.08
0
50

 = 0:0
 = 0:3
 = 0:5
 = 0:7
100 150
Episode

200

Reinf/Step
0.04
0.02
0
-0.02
-0.04
-0.06
-0.08
250
0
50

 = 0:7
 = 0:8
 = 0:9
 = 1:0
100 150
Episode

200

250

Figure 4: The car parking problem, learning curves for study 1.
The learning curves for this study are presented in Figure 4. The observations can be
briey summarized as follows:
  = 0 gives the worst performance of all (not all of 25 runs managed to converge
within 250 episodes),
 increasing  improves learning speed,
  values above or equal 0:7 are all similarly eective, greatly outperforming  = 0 and
clearly better than  = 0:5,
8. The optimization procedure in most cases was as follows: some rather large value was tested in a few
runs; if it did not give any eects of overtraining and premature convergence, it was accepted; otherwise
a (usually twice) smaller value was tried, etc.
9. Note that for  = 0:9, m = 25, and  = 0:95 we have ()m  0:02  0:855 = .

308

Truncating Temporal Differences

 using large  caused the necessity of reducing the learning rates (cf. Table 2) to ensure

convergence.
The main result is that using large  with the TTD procedure (including 1) always
signicantly improved performance. It is not quite consistent with the empirical results of
Sutton (1988), who found the performance of TD() the best for intermediate , and the
worst for  = 1. Lin (1993), who used  > 0 for his experience replay experiments, reported
 close to 1 as the most successful, similarly as this work. He speculated that the dierence
between his results and Sutton's might have been caused by switching occasionally (for
non-policy actions) to  = 0 in his studies.10 Our results, obtained for  held xed all the
time11 , suggest that this is not a good explanation. It seems more likely that the optimal 
value simply strongly depends on the particular problem. Another point is that neither our
TTD(1; 25) nor Lin's implementation is exactly equivalent to TD(1).
Study 2: Eects of m. This study was designed to investigate the eects of using several
dierent m values for a xed and relatively large  value. The best (approximately)  from
study 1 was used, that is 0:9. The smallest tested m value is 5, which we nd to be rather
a small value.12
Reinf/Step
0.04
0.02
0
-0.02
-0.04
-0.06
-0.08
0
50

m= 5
m = 10
m = 15
m = 20
m = 25
100 150
Episode

200

250

Figure 5: The car parking problem, learning curves for study 2.
The learning curves for this study are presented in Figure 5. The results for m = 25
were taken from study 1 for comparison. The observations can be summarized as follows:
 m = 5 is the worst and m = 25 is the best,
 the dierences between intermediate m values do not seem to be very statistically
signicant,
10. As a matter of fact, non-policy actions were not replayed at all in Lin's experience replay experiments.
11. Except for using  = 0 for the most recent time step covered by the TTD return, as it follows from its
denition (Equation 24).
12. For  = 0:95,  = 0:9, and m = 5 we have ()m  0:457, which is by all means comparable with
 = 0:855.

309

Cichosz

 even the smallest m = 5 gives the performance level much better than that obtained in
study 1 for small , i.e., even relatively small m values allow us to have the advantages
of large , though larger m values are generally better than small ones,
The last observation is probably the most important. It is also very optimistic. It suggests
that, at least in some problems, the TTD procedure with  > 0 allows to obtain a signicant
learning speed improvement over traditional TD(0)-based algorithms with practically no
additional costs, because for small m both space and time complexity induced by TTD is
always negligible.

5.2 The Cart-Pole Balancing Problem
The experiments of this section have one basic purpose: to verify the eectiveness of the
TTD procedure by applying its AHC implementation to a realistic and complex problem,
with a long reinforcement delay, for which there exist many previous results for comparison.
The cart-pole balancing problem, a classical benchmark of control specialists, is just such
a problem. In particular, we would like to see whether it is possible to obtain performance
(learning speed and the quality of the nal policy) not worse than that reported by Barto
et al. (1983) and Sutton (1984) using the eligibility traces implementation.
Figure 6 shows the cart-pole system. The cart is allowed to move along a one-dimensional
bounded track. The pole can move only in the vertical plane of the cart and the track. The
controller applies either a left or right force of xed magnitude to the cart at each time
step. The task is episodic: each episode nishes when a failure occurs, i.e., the pole falls or
the cart hits an edge of the track. The objective is to delay the failure as long as possible.
The problem was realistically simulated by numerically solving a system of dierential
equations, describing the cart-pole system. These equations and other simulation details
are given in Appendix B. All parameters of the simulated cart-pole system are exactly the
same as used by Barto et al. (1983).
5.2.1 State Representation

The state of the cart-pole system is described by four state variables:

 x | the position of the cart on the track,
 x_ | the velocity of the cart,
  | the angle of the pole with the vertical,
 _ | the angular velocity of the pole.
5.2.2 Action Representation

At each step the agent controlling the cart-pole system chooses one of the two possible
actions of applying a left or right force to the cart. The force magnitude is xed and
equal 10 N.
310

Truncating Temporal Differences

ฮธ

2l

F

x
d

Figure 6: The cart-pole system. F is the force applied to the cart's center, l is a half of the
pole length, and d is a half of the length of the track.
5.2.3 Reinforcement Mechanism

The agent receives non-zero reinforcement values (namely ,1) only at the end of each
episode, i.e., after a failure. A failure occurs whenever jj > 0:21 rad (the pole begins to
fall) or jxj > 2:4 m (the cart hits an edge of the track). Even at the beginning of learning,
with a very poor policy, an episode may continue for hundreds of time steps, and there may
be many steps between a bad action and the resulting failure. This makes the temporal
credit assignment problem in the cart-pole task extremely hard.
5.2.4 Function Representation

As in the case of the car parking problem, we deal with the continuous state space of the
cart-pole system by dividing it into disjoint regions, called boxes after Mitchie and Chambers
(1968). The quantization thresholds are the same as used by Barto et al. (1983), i.e.:

 for x: ,0:8, 0:8 m,
 for x_ : ,0:5, 0:5 m/s,
 for : ,0:105, ,0:0175, 0, 0:0175, 0:105 rad,
 for _: ,0:8727, 0:8727 rad/s,
which yields 3  3  6  3 = 162 boxes. For each box there is a memory location, storing a
function value for that box.
311

Cichosz

5.2.5 Experimental Design and Results

Computational expense prevented such extensive experimental studies as for the car parking
problem. Only one experiment was carried out, intended to be a replication of the experiment presented by Barto et al. (1983). The values of the TTD parameters that seemed the
best from the previous experiments were used, that is  = 0:9 and m = 25. The discount
factor  was set to 0:95. The learning rates for the evaluation and policy functions were
roughly optimized by a small number of preliminary runs and equal  = 0:1 and  = 0:05,
respectively. The temperature of the Boltzmann distribution action selection mechanism
was set to 0:0001, so as to give nearly-deterministic action selection. The initial values of
the evaluation and policy functions were set to 0. We did not attempt to strictly replicate
the same learning parameter values as in the work of Barto et al. (1983), since they used not
only a dierent TD() implementation13 , but also a dierent policy representation (based
on the fact that there are only two actions, while our representation is general), action
selection mechanism (for the same reasons), and function learning rule.
The experiment consisted of 10 runs, diering only in the initial seed of the random
number generator, and the presented results are averaged over those 10 runs. Each run continued for 100 episodes. Some of individual runs were terminated after 500; 000 time steps,
before completing 100 episodes. To produce reliable averages for all 100 episodes, ctious
remaining episodes were added to such runs, with the duration assigned according to the
following principle, used in the experiments of Barto et al. (1983). If the duration of the
last, interrupted episode was less than the duration of the immediately preceding (complete) episode, the ctious episodes were assigned the duration of that preceding episode.
Otherwise, the ctious episodes were assigned the duration of the last (incomplete) episode.
This prevented any short interrupted episodes from producing unreliably low averages. The
results are presented in Figure 7 as plots of the average duration (the number of time steps)
of the previous 5 consecutive episodes versus the episode number, in linear and logarithmic
scale.
We can observe that TTD-based AHC achieved a similar (slightly better, to be exact)
performance level, both as to learning speed and the quality of the nal policy (i.e., the
balancing periods), to that reported by Barto et al. (1983). The nal balancing periods lasted
above 130; 000 steps, on the average. It was obtained without using 162 additional memory
locations for storing eligibility traces, and without the expensive computation necessary to
update all of them at each time step, as well as all evaluation and policy function values.

5.3 Computational Savings

The experiments presented above illustrate the computational savings possible with the
TTD procedure over conventional eligibility traces. A direct implementation of eligibility
traces requires computation proportional to the number of states, i.e., to 1260 in the car
parking task and to 162 in the cart-pole task | potentially many more in larger tasks.
Even the straightforward iterative version of TTD may be then benecial, as it requires
computation proportional to m, which may be reasonably assumed to be many times less
13. It was the eligibility traces implementation, but eligibility traces were updated by applying a somewhat
dierent update rule than specied by Equation 8. In particular, they were discounted with  alone
instead of . Moreover, two dierent  values were used for the evaluation and policy functions.

312

Truncating Temporal Differences

Episode Duration
140000
120000
100000
80000
60000
40000
20000
0
0
20

(a)

40 60
Episode

80

Episode Duration
100000
10000
1000
100
10
1
100
0
20

(b)

40 60
Episode

80

100

Figure 7: The cart-pole balancing problem, learning curve in (a) linear and (b) logarithmic
scale.
than the size of the state space. Of course, the incremental version of TTD, which requires
always very small computation independent of m, is much more ecient.
In many practical implementations, to improve eciency, eligibility traces and predictions are updated only for relatively few recently visited states. Traces are maintained only
for the n most recently visited states, and the eligibility traces of all other states are assumed
to be 0.14 But even for this \ecient" version of eligibility traces, the savings oered by
TTD are considerable. For a good approximation to innite traces in such tasks as considered here, n should be at least as large as m. For conventional eligibility traces, there will be
always a concern for keeping n low, by reducing  , , or the accuracy of the approximation.
The same problem occurs for iterative TTD,15 but for incremental TTD, on the other hand,
none of these are at issue. The same small computation is needed independent of m.

6. Conclusion
We have informally derived the TTD procedure from the analysis of the updates introduced
by TD methods to the predicted utilities of states, and shown that they can be approximated by the use of truncated TD() returns. Truncating temporal dierences allows easy
and ecient implementation. It is possible to compute TTD returns incrementally in constant time, irrespective of the value of m (the truncation period), so that the computational
expense of using TD-based reinforcement learning algorithms with  > 0 is negligible (cf.
Equations 25 and 26). It cannot be achieved with the eligibility traces implementation.
The latter, even for such function representation methods to which it is particularly well
14. This modication cannot be applied when a parameter estimation function representation technique is
used (e.g., a multi-layer perceptron), where traces are maintained for weights rather than for states.
15. The relative computational expense of iterative TTD and the \ecient" version of eligibility traces
depends on the cost of the function update operation, which is always performed only for one state by
the former, and for n states by the latter.

313

Cichosz

suited (e.g., neural networks), is always associated with signicant memory and time costs.
The TTD procedure is probably the most computationally ecient (although approximate)
on-line implementation of TD(). It is also general, equally good for any function representation method that might be used.
An important question concerning the TTD procedure is whether its computational
eciency is not obtained at the cost of reduced learning eciency. Having low computational costs per control action may not be attractive if the number of actions necessary to
converge becomes large. As for now, no theoretically grounded answer to this important
question has been provided, though it is not unlikely that such an answer will eventually
be found. Nevertheless, some informal consideration may suggest that the TTD-based implementation of TD methods not only does not have to perform worse than the classical
eligibility traces implementation, but it can even have some advantages. As it follows from
Equations 20, 21, and 22, using TD(0) errors for on-line TD() learning, as in the eligibility
traces implementation, introduces an additional discrepancy term, whose inuence on the
learning process is proportional to the square of the learning rate. That term, though often
negligible, may be still harmful in certain cases, especially in tasks where the agent is likely
to stay in the same states for long periods. The TTD procedure, based on truncated TD()
returns, is free of this drawback.
Another argument supporting the TTD procedure is associated with using large  values,
in particular 1. For an exact TD() implementation, such as that provided by eligibility
traces, it means that learning relies solely on actually observed outcomes, without any regard
to currently available predictions. It may be benecial at the early stages of learning, when
predictions are almost completely inaccurate, but in general it is rather risky | actual
outcomes may be noisy and therefore sometimes misleading. The TTD procedure never
relies on them entirely, even for  = 1, since it uses m-step TTD returns for some nite m,
corrected by always using  = 0 for discounting the predicted utility of the most recent step
covered by the return (cf. Equation 17). This deviation of the TTD procedure from TD()
may turn out to be advantageous.
The TTD procedure using TTD returns for learning is only suitable for the implementation of TD methods applied to reinforcement learning. This is because in RL a part of the
predicted outcome is available at each step, as the current reinforcement value. However,
it is straightforward to formulate another version of the TTD procedure, using truncated
TD() errors instead of truncated TD() returns, that would cover the whole scope of
applications of generic TD methods.
The experimental results obtained for the TTD procedure seem very promising. The results presented in Section 5.1 show that using large  with the TTD procedure can give a signicant performance improvement over simple TD(0) learning, even for relatively small m.
While it does not say anything about the relative performance of TTD and the eligibility
traces implementation of TD(), it at least suggests that the TTD procedure can be useful.
The best results have been obtained for the largest  values, including 1. This observation,
contradicting to the results reported by Sutton (1988), may be a positive consequence of
the TTD procedure's deviation from TD() discussed above.
The experiments with the cart-pole balancing problem supplied empirical evidence that
for a learning control problem with a very long reinforcement delay the TTD procedure can
equal or outperform the eligibility traces implementation of TD(), even for a value of m
314

Truncating Temporal Differences

many times less than the average duration of an episode. This performance level is obtained
with the TTD procedure at a much lower computational (both memory and time) expense.
To summarize, our informal consideration and empirical results suggest that the TTD
procedure may have the following advantages:

 the possibility of the implementation of reinforcement learning algorithms that may
be viewed as instantiations of TD(), using  > 0 for faster learning,
 computational eciency: low memory requirements (for reasonable m) and little computation per time step,

 generality, compatibility with various function representation methods,
 good approximation of TD() for  < 1 (or for  = 1 and  < 1),
 good practical performance, even for relatively small m.
There seems to be one important drawback: lack of theoretical analysis and a convergence proof. We do not know either what parameter values assure convergence or what
values make it impossible. In particular, no estimate is available of the potential harmful
eects of using too large m. Both the advantages and drawbacks cause that the TTD procedure is an interesting and promising subject for further work. This work should concentrate,
on one hand, on examining the theoretical properties of this technique, and, on the other
hand, on empirical studies investigating the performance of various TD-based reinforcement
learning algorithms implemented within the TTD framework on a variety of problems, in
particular in stochastic domains.

Appendix A. Car Parking Problem Details

The motion of the car in the experiments of Section 5.1 is simulated by applying at each
time step the following equations:
1. if r 6= 0 then
(a) (t +  ) = (t) +  vr ;
(b) x(t +  ) = x(t) , r sin (t) + r sin (t +  );
(c) y (t +  ) = y (t) + r cos (t) , r sin (t +  );
2. if r = 0 then
(a) (t +  ) = (t);
(b) x(t +  ) = x(t) + v cos (t);
(c) y (t +  ) = y (t) + v sin (t);
where r is the turn radius, v is the car's velocity, and  is the simulation time step. In the
experiments r = ,5 m was used for the `turn left' action, r = 5 m for `turn right', and r = 0
for `drive straight on'. The velocity was constant and set to 1 m/s, and the simulation time
315

Cichosz

step  = 0:5 s was used. With these parameter settings, the shortest possible path from the
car's initial location (x = 6:15 m, y = 10:47 m,  = 3:7 rad) to the garage requires 21 steps.
At each step, after determining the current x, y , and  values, the coordinates of the
car's corners are computed. Then the test for intersection of each side of the car with the
lines delimiting the driving area and the garage is performed to determine whether a failure
occurred. If the result is negative, the test is performed for each corner of the car whether
it is inside the garage, to determine if a success occurred.

Appendix B. Cart-Pole Balancing Problem Details

The dynamics of the cart-pole system are described by the following equations of motion:
h
i
F (t) + mpl _2(t) sin (t) ,  cos (t) , c sgn x_ (t)
x(t) =
mc + mp


where

2
(t)+c sgn x_ (t)
g sin (t) + cos (t) ,F (t),mp l_ (mt)sin
c +mp
h
(t) =
2 (t) i
l 43 , mmp cos
c +mp



, mp_p(lt)

= 9:8 m/s2 | acceleration due to gravity,
= 1:0 kg | mass of the cart,
= 0:1 kg | mass of the pole,
= 0:5 m
| half of the pole length,
= 0:0005 | friction coecient of the cart on the track,
= 0:000002 | friction coecient of the pole on the cart,
= 10:0 N | force applied to the center of the cart at time t.
The equations were simulated using Euler's method with simulation time step  = 0:02 s.

g
mc
mp
l
c
p
F (t)

Acknowledgements
I wish to thank the anonymous reviewers of this paper for many insightful comments. I was
unable to follow all their suggestions, but they contributed much to improving the paper's
clarity. Thanks also to Rich Sutton, whose assistance during the preparation of the nal
version of this paper was invaluable.
This research was partially supported by the Polish Committee for Scientic Research
under Grant 8 S503 019 05.

References

Baird, III, L. C. (1993). Advantage updating. Tech. rep. WL-TR-93-1146, Wright Laboratory, Wright-Patterson Air Force Base.
Barto, A. G. (1992). Reinforcement learning and adaptive critic methods. In White, D. A.,
& Sofge, D. A. (Eds.), Handbook of Intelligent Control, pp. 469{491. Van Nostrand
Reinhold, New York.
316

Truncating Temporal Differences

Barto, A. G., Sutton, R. S., & Anderson, C. (1983). Neuronlike adaptive elements that can
solve dicult learning control problems. IEEE Transactions on Systems, Man, and
Cybernetics, 13, 835{846.
Barto, A. G., Sutton, R. S., & Watkins, C. J. C. H. (1990). Learning and sequential
decision making. In Gabriel, M., & Moore, J. (Eds.), Learning and Computational
Neuroscience. The MIT Press.
Cichosz, P. (1994). Reinforcement learning algorithms based on the methods of temporal
dierences. Master's thesis, Institute of Computer Science, Warsaw University of
Technology.
Dayan, P. (1992). The convergence of TD() for general . Machine Learning, 8, 341{362.
Dayan, P., & Sejnowski, T. (1994). TD() converges with probability 1. Machine Learning,
14, 295{301.
Heger, M. (1994). Consideration of risk in reinforcement learning. In Proceedings of the
Eleventh International Conference on Machine Learning (ML-94). Morgan Kaufmann.
Jaakkola, T., Jordan, M. I., & Singh, S. P. (1993). On the convergence of stochastic iterative
dynamic programming algorithms. Tech. rep. 9307, MIT Computational Cognitive
Science. Submitted to Neural Computation .
Klopf, A. H. (1982). The Hedonistic Neuron: A Theory of Memory, Learning, and Intelligence. Washington D.C.: Hempisphere.
Lin, L.-J. (1992). Self-improving, reactive agents based on reinforcement learning, planning
and teaching. Machine Learning, 8, 293{321.
Lin, L.-J. (1993). Reinforcement Learning for Robots Using Neural Networks. Ph.D. thesis,
School of Computer Science, Carnegie-Mellon University.
Mitchie, D., & Chambers, R. A. (1968). BOXES: An experiment in adaptive control.
Machine Intelligence, 2, 137{152.
Moore, A. W., & Atkeson, C. G. (1992). An investigation of memory-based function approximators for learning control. Tech. rep., MIT Articial Intelligence Laboratory.
Pendrith, M. (1994). On reinforcement learning of control actions in noisy and
non-markovian domains. Tech. rep. UNSW-CSE-TR-9410, School of Computer Science and Engineering, The University of New South Wales, Australia.
Peng, J., & Williams, R. J. (1994). Incremental multi-step Q-learning. In Proceedings of the
Eleventh International Conference on Machine Learning (ML-94). Morgan Kaufmann.
Ross, S. (1983). Introduction to Stochastic Dynamic Programming. Academic Press, New
York.
317

Cichosz

Schwartz, A. (1993). A reinforcement learning method for maximizing undiscounted rewards. In Proceedings of the Tenth International Conference on Machine Learning
(ML-93). Morgan Kaufmann.
Singh, S. P. (1994). Reinforcement learning algorithms for average-payo markovian decision
processes. In Proceedings of the Twelfth National Conference on Articial Intelligence
(AAAI-94).
Sutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. Ph.D. thesis,
Department of Computer and Information Science, University of Massachusetts.
Sutton, R. S. (1988). Learning to predict by the methods of temporal dierences. Machine
Learning, 3, 9{44.
Sutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based
on approximating dynamic programming. In Proceedings of the Seventh International
Conference on Machine Learning (ML-90). Morgan Kaufmann.
Sutton, R. S., Barto, A. G., & Williams, R. J. (1991). Reinforcement learning is direct
adaptive optimal control. In Proceedings of the American Control Conference, pp.
2143{2146. Boston, MA.
Sutton, R. S., & Singh, S. P. (1994). On step-size and bias in temporal-dierence learning.
In Proceedings of the Eighth Yale Workshop on Adaptive and Learning Systems, pp.
91{96. Center for Systems Science, Yale University.
Tesauro, G. (1992). Practical issues in temporal dierence learning. Machine Learning, 8,
257{277.
Watkins, C. J. C. H. (1989). Learning from Delayed Rewards. Ph.D. thesis, King's College,
Cambridge.
Watkins, C. J. C. H., & Dayan, P. (1992). Technical note: Q-learning. Machine Learning,
8, 279{292.

318

Journal of Articial Intelligence Research 2 (1994) 89-110

Submitted 3/94; published 8/94

Pattern Matching and Discourse Processing in Information
Extraction from Japanese Text
Tsuyoshi Kitani

tkitani@cs.cmu.edu

Yoshio Eriguchi
Masami Hara

eriguchi@rd.nttdata.jp
masami@rd.nttdata.jp

Center for Machine Translation
Carnegie Mellon University
Pittsburgh, PA 15213 USA
Development Headquarters
NTT Data Communications Systems Corp.
66-2 Horikawa-cho, Saiwai-ku, Kawasaki-shi, Kanagawa 210 JAPAN

Abstract
Information extraction is the task of automatically picking up information of interest
from an unconstrained text. Information of interest is usually extracted in two steps.
First, sentence level processing locates relevant pieces of information scattered throughout
the text; second, discourse processing merges coreferential information to generate the
output. In the rst step, pieces of information are locally identied without recognizing
any relationships among them. A key word search or simple pattern search can achieve this
purpose. The second step requires deeper knowledge in order to understand relationships
among separately identied pieces of information. Previous information extraction systems
focused on the rst step, partly because they were not required to link up each piece
of information with other pieces. To link the extracted pieces of information and map
them onto a structured output format, complex discourse processing is essential. This
paper reports on a Japanese information extraction system that merges information using
a pattern matcher and discourse processor. Evaluation results show a high level of system
performance which approaches human performance.

1. Introduction
In recent information extraction systems, most individual pieces of information to be extracted directly from a text are usually identied by key word search or simple pattern
search in the preprocessing stage (Lehnert et al., 1993; Weischedel et al., 1993; Cowie et al.,
1993; Jacobs et al., 1993). Among the systems presented at the Fifth Message Understanding Conference (muc-5), however, the main architectures ranged from pattern matching to
full or fragment parsing (Onyshkevych, 1993). Full or fragment parsing systems, in which
several knowledge sources such as syntax, semantics, and domain knowledge are combined
at run-time, are generally so complicated that changing a part of the system tends to aect
the other components. In past information extraction research, this interference has slowed
development (Jacobs, 1993; Hobbs et al., 1992). A pattern matcher, which identies only
patterns of interest, is more appropriate for information extraction from texts in narrow
domains, since this task does not require full understanding of the text.

c 1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Kitani, Eriguchi, & Hara

textract, the information extraction system described here, uses a pattern matcher
similar to sri's fastus pattern matcher (Hobbs et al., 1992). The matcher is implemented
as a nite-state automaton. Unlike other pattern matchers, textract's matcher deals
with word matching problems caused by the word segmentation ambiguities often found in
Japanese compound words.
The goal of the pattern matcher is to identify the concepts represented by words and
phrases in the text. The pattern matcher rst performs a simple key-word-based concept
search, locating individual words associated with concepts. The second step is a template
pattern search which locates phrasal patterns involving critical pieces of information identied by the preprocessor. The template pattern search identies relationships between
matched objects in the dened pattern as well as recognizing the concept behind the relationship. One typical concept is the relationship of \economic activity" which companies
can participate in with each other.
It is usually dicult to determine the relationships among pieces of information which
have been identied in separate sentences. These relationships are often stated implicitly,
and even if the text explicitly mentions them the descriptions are often located far enough
apart to make detection dicult. Although the importance of discourse processing for information extraction has been emphasized in the Message Understanding Conferences (Lehnert
& Sundheim, 1991; Hirschman, 1992), no system presented has satisfactorily addressed the
issue.
The discourse processor in textract is able to correlate individual pieces of information
throughout the text. textract merges concepts which the pattern matcher has identied
separately (and usually in dierent sentences) when the concepts involve the same companies. textract can unify multiple references to the same company even when the company
name is missing, abbreviated, or pronominalized. Furthermore, the processor segments the
discourse to isolate portions of text relevant to a particular conceptual relationship. The
discourse segmentation lessens the chance of merging unrelated information (Kitani, 1994).
This paper analyzes some evaluation results for textract's discourse module and describes the tipster/muc-5 evaluation results in order to assess overall system performance.

2. tipster information extraction task
The goal of the tipster project sponsored by arpa is to capture information of interest
from English and Japanese newspaper articles about corporate joint ventures and microelectronics. A system must ll a generic template with information extracted from the text
by a fully automated process. The template is composed of several objects, each containing several slots. Slots may contain pointers to related objects (Tipster, 1992). Extracted
information is to be stored in an object-oriented database.
In the joint ventures domain, the task is to extract information concerning joint venture
relationships which organizations form and dissolve. The template structure represents
these relationships with tie-up-relationship objects, which contain pointers to organization
entity objects representing the organizations involved. Entity objects contain pointers to
other objects such as person and facility objects, as shown in Figure 1.
In the microelectronics domain, extraction focuses on layering, lithography, etching, and
packaging processes in semiconductor manufacturing for microchip fabrication. The entities
90

Pattern Matching and Discourse Processing

TEMPLATE
doc. no.
doc. date
doc. source
content (*)

ENTITY
name
aliases
location
nationality
type
entity rel. (+)
person(*)
facility (*)

TIE-UP-REL.
tie-up status
entity (+)
created
entity (*)
activity (*)

ACTIVITY
industry (*)

ENTITY-REL.
entity1 (+)
entity2 (*)
rel. of ent2
to ent1
PERSON
name
personโs entity
position
FACILITY
name
location
type
INDUSTRY
industry type
product /
service

denotes instantiations of multiple objects
(*) points to zero or more objects, (+) points to one or more objects

Figure 1: Object-oriented template structure of the joint ventures domain
extracted include manufacturer, distributor, and user, in addition to detailed manufacturing
information such as materials used and microchip specications such as wafer size and device
speed. The microelectronics template structure is similar to that of the joint ventures but
has fewer objects and slots.
Both of these extraction tasks must identify not only individual entities but also certain
relationships among them. Often, however, a particular piece of extracted information
describes only part of a relationship. This partial information must be merged with other
pieces of information referring to the same entities. For merging to produce correct results,
therefore, correct identication of entity references is crucial.

3. Problem denition
This section rst describes word matching problems caused by the word segmentation ambiguities. Diculties of reference resolution of company names are then explained. Issues
of discourse segmentation and concept merging are also discussed using an example text.

3.1 Word segmentation
Japanese word segmentation in the preprocessor gives rise to a subsequent under-matching
problem. When a key word in the text is not found in the word segmentor's lexicon, the
segmentor tends to divide it into separate words. With our current lexicon, for example, the
91

Kitani, Eriguchi, & Hara

compound noun \ ยชยนF " (teikei-kaisyo), consisting of two words, \ ยชยน " (teikei: joint
venture) and \ F " (kaisyo: dissolve), is segmented into the two individual nouns. Thus a
key word search for \ ยชยนF " (teikei-kaisyo) does not succeed in the segmented sentence.
On the other hand, the pattern matching process allows, by default, partial matching
between a key word and a word in the text. \ ยชยน " (teikei) and \ [Z ยชยน" (gyoum-teikei),
both meaning \a joint venture", can both be matched against the single key word \ ยชยน "
(teikei). This exibility creates an over-matching problem. For example, the key word
\ JS " (silicon) matches \ f)ร JS" (nisanka-silicon: silicon dioxide), although
they are dierent materials to be reported in the microelectronics domain. These segmentation diculties for compound nouns also cause major problems in word-based Japanese
information retrieval systems (Fujii & Croft, 1993).

3.2 Company name references

In the corporate joint ventures domain, output templates mostly describe relationships
among companies (as described in Section 2). Information of interest is therefore found in
sentences which mention companies or their activities. It is essential for the extractor to
identify topic companies|the main concern of the sentences they appear in|in order to
correlate other information identied in the sentence. There are three problems which make
it dicult to identify topic companies.
1. Missing subject
Topic companies are usually the subject of a sentence. Japanese sentences frequently
omit subjects, however|even in formal newspaper articles. The veniex system which
nec presented at muc-5 can identify the company implied by a missing subject if
there is an explicit reference to it in the immediately preceding sentence (Doi et al.,
1993; Muraki et al., 1993). It is not clear whether veniex can resolve the missing
reference when the explicit reference appears in a sentence further separated from the
subjectless sentence.
2. Company name abbreviations
As is also seen in English, company names are often abbreviated in a Japanese text after their rst appearance. A variety of ways to abbreviate company names in Japanese
is given in (Karasawa, 1993). The following examples show some typical abbreviations
of Japanese company names:
(a) a partial word
\ AK' 9S$" ! \ 9S$ "
(Mercedes-Benz)
(Benz)
(b) an English abbreviation
\ o%รยรย*NTT+ " ! \ NTT "
(Nippon Telegraph and Telephone)
(c) the rst Katakana character + \ ย "
\ AJS7L ย" ! \ ย "
(American Express Corp.)
92

Pattern Matching and Discourse Processing
(d) the rst character of each primitive segment
\o %  ย " ! \ o "1
(Japan Airlines)
(e) some randomly selected characters
\ยo %รผ ร" ! \ ยoร "
(Shin-nihon Steel)
Locating company name abbreviations is dicult, since many are not identied as
companies by either a morphological analyzer or the name recognizer in the preprocessor. Another problem is that the variety of ways of abbreviating names makes it
dicult to unify multiple references to one company.
Almost all muc-5 systems include a string matching mechanism to identify company name abbreviations. These abbreviations are specied in an aliases slot in the
company entity object. To the authors' knowledge, none of the systems other than
textract can detect company name abbreviations of type (d) or (e) above without
using a pre-dened abbreviation table.
3. Company name pronouns
Company name pronouns are often used in formal texts. Frequently used expressions
include \ !ย " (ryosya: both companies), \ $ย " (dosya: the company), and \ r
ย " (jisya: the company itself). As shown in the following examples, resolving the
references is particularly important for full understanding of a text. Direct English
translation follows the Japanese sentences.
(a) \ Xย/Yย(ยชยน $ย .รผWR rย 6IS)'รฝยตK "

*Yย+

*Xย+

\X Corp. has tied up with Y Corp. and sells products of the company by its own
brand name."
(Y Corp.) (X Corp.)
(b) \ Xย/.ยย'/รรขยฑ $ย .ยp/NยJ "

*Xย+

\X Corp. is the biggest company in this eld. The president of the company is
Mr. Suzuki."
(X Corp.)
Reference resolution for \ $ย " (dosya: the company) is implemented in veniex (Doi
et al., 1993). veniex resolves the pronominal reference in the same way as it identies
missing company references. The crl/brandeis diderot system presented at muc5 simply chooses the nearest company name as the referent of \dosya". This algorithm
was later improved by Wakao using corpus-based heuristic knowledge (Wakao, 1994).
These systems do not handle pronominalized company names other than \dosya".
The three problems described in this section often cause individual information to be
correlated with the wrong company or tie-up-relationship object. To avoid this error, the
topic companies must be tracked from the context, since they can be used to determine
which company objects an information fragment should be assigned to. Abbreviated and
pronominalized company names must be unied as references to the same company.
1. \ o% " (nihon: Japan) and \ ย " (koukuu: airlines) are the primitive segments in this example.
93

Kitani, Eriguchi, & Hara

3.3 Discourse segmentation and concept merging

In the joint ventures domain, a tie-up-relationship object contains pointers to other objects
such as economic activities (as shown in Figure 1). When a company is involved in multiple tie-ups, merging information into a tie-up relationship according to topic companies
sometimes yields incorrect results. Consider the following example:
"X Corp. has tied up with Y Corp. X will start selling products
in Japan next month. Last year X started a similar joint venture
with Z Inc."

Obviously, the sale in the second sentence is related to the tie-up relationship of X and
Y. However, since the topic company, which is the subject of a sentence, is X in all three
sentences, the sale could also be related to the X and Z tie-up relationship. This incorrect
merging can be avoided by separating the text into two blocks: the rst two sentences
describe the X and Y tie-up, and the last sentence describes the X and Z tie-up. Thus,
discourse segmentation is necessary to identify portions of text containing related pieces
of information. The crl/brandeis diderot system segments the joint ventures text into
two types of text structures (Cowie et al., 1993). It is not known how well their discourse
segmentation performed, however.
Once the text is segmented, concepts or identied pieces of information can be merged
within the same discourse segment. For example, the expected income from a joint venture
is often stated in a sentence which does not explicitly mention the participating companies;
they appear in the previous sentence. In this case, the joint venture concept identifying the
companies and the income concept identifying the expected income must be merged so that
the latter will be linked to the correct entity objects.
4. The solution

This section describes details of textract's pattern matcher and discourse processor as
well as the system architecture.

4.1

textract architecture
textract is an information extraction system developed for the tipster Japanese do-

mains of corporate joint ventures and microelectronics (Jacobs, 1993; Jacobs et al., 1993).
As shown in Figure 2, the textract joint ventures system comprises four major components: preprocessor, pattern matcher, discourse processor, and template generator. Because
of its shorter development time, the textract microelectronics system has a simpler conguration than the joint ventures system. It does not include the template pattern search
in the pattern matcher, or the discourse segmentation and concept merging in the discourse
processor, as also shown in Figure 2.
In the preprocessor, a Japanese segmentor called majesty segments Japanese text into
primitive words tagged with their parts of speech (Kitani, 1991). Next, the name recognizer
identies proper names and monetary, numeric, and temporal expressions. majesty tags
proper names which appear in its lexicon; the name recognizer identies additional proper
names by locating name designators such as \ ย " (sya, corresponding to \Inc." or \Corp.")
94

Pattern Matching and Discourse Processing

Pattern matcher
Preprocessor

concept
search

template
pattern
search

- concept
- concept
identification
identification
- information
merging within
a sentence

- morphological
analysis
- name
recognition

Discourse processor
company
discourse
concept
name
segmentamerging
unification
tion
- company
name
reference
resolution

- information
- text
segmentation merging
within a text

joint ventures
system only

Template
generator
- output
generation

Figure 2: textract system architecture
for company names. The recognizer extends the name string forward and backward from
the designator until it meets search stop conditions (Kitani & Mitamura, 1993). The name
segments are grouped into units which are meaningful to the pattern matching process
(Kitani & Mitamura, 1994). Most strings to be extracted directly from the text are identied
by majesty and the name recognizer.
Details of the pattern matcher and discourse processor are given in the following sections. The template generator assembles the extracted information and creates the output
described in Section 2.

4.2 Pattern matcher

The following subsections describe the concept search and the template pattern search in
the pattern matcher which identify concepts in the sentence. Whereas the former simply
searches for key words, the latter searches for phrasal patterns within a sentence. The
template pattern search also identies relationships between matched objects in the dened
pattern. In the course of textract development, key words and template patterns were
obtained manually by a system developer using a kwic (Key Word In Context) tool and
referring to a word frequency list obtained from the corpus.
4.2.1 Concept search

Key words representing the same concept are grouped into a list and used to recognize the
concept in a sentence. The list is written in a simple format: (concept-name word1 word2
...). For example, key words for recognizing a dissolved joint venture concept can be written
in the following way:
95

Kitani, Eriguchi, & Hara
(DISSOLVED

ยชยนF รฏ Fn)

or
(DISSOLVED dissolve terminate cancel).

The concept search module recognizes a concept when it locates one of the associated
words in a sentence. This simple procedure sometimes yields incorrect concepts. For example, the concept \dissolved" is erroneously identied from an expression such as \cancel a
hotel reservation". Key-word-based concept search is most successful when processing text
in a narrow domain in which words are used with restricted meanings.
The under-matching problem occurs when a compound noun in the key word list of a
concept fails to match the text because the instance of the compound in the text has been
segmented into separate primitive words. To avoid the problem, adjacent nouns in the text
are automatically concatenated during the concept search process, generating compound
nouns at run-time. The over-matching problem, on the other hand, arises when a key word
successfully matches part of a compound noun which as a whole is not associated with
the concept. Over-matching can be prevented by anchoring the beginning and/or end of a
key word pattern to word boundaries (with the symbol \>" at the beginning and \<" at
the end). For example, \> JS <" (silicon) must be matched against a single complete
word in the text. Since this problem is rare, its solution is not automatic: system developers
attach anchors to key words which are likely to over-match.
4.2.2 Template pattern search
textract's pattern matcher is implemented as a nite-state recognizer. This choice of

implementation is based on the assumption that a nite-state grammar can eciently handle
many of the inputs that a context-free grammar covers (Pereira, 1990). The pattern matcher
is similar to the pattern recognizer used in the muc-4 fastus system developed at sri
(Hobbs et al., 1992).
Patterns for the textract template pattern matcher are dened with rules similar
to regular expressions. Each pattern denition species the concept associated with the
pattern. (For the joint ventures domain, textract uses eighteen concepts.)
In the matcher, state transitions are driven by segmented words or grouped units from
the preprocessor. The matcher identies all possible patterns of interest in the text that
match dened patterns, recognizing the concepts associated with the patterns. For some
inputs, the matcher must skip words that are not explicitly dened in the pattern.
Figure 3 shows denitions of equivalent Japanese and English patterns for recognizing
the concept *joint-venture*. This English pattern is used to capture expressions such
as \XYZ Corp. created a joint venture with PQR Inc." The notation \@string" represents
a variable matching an arbitrary string. Variables whose names begin with \@cname"
are called company-name variables and are used where a company name is expected to
appear. In the denitions shown, a string matched by \@cname partner subj" is likely
to contain at least one company name referring to a joint venture partner and functioning
as the subject in a sentence.
The pattern \ /#:strict:P" matches the grammatical particles \ / " (wa ) and \  "
(ga ), which serve as subject case markers. The symbol \strict" species a full string match
(the default in case of template pattern search), whereas \loose" allows a partial string
96

Pattern Matching and Discourse Processing
(a)

(JointVenture1 6
@CNAME_PARTNER_SUBJ
/#:strict:P
@CNAME_PARTNER_WITH
(:strict:P
@SKIP
ยชยน:loose:VN)

(b)

(JointVenture1 3
@CNAME_PARTNER_SUBJ
create::V
a joint venture::NP
with::P
@CNAME_PARTNER_WITH)

Figure 3: A matching pattern for (a) Japanese and (b) English

match. Partial string matching is useful for matching a dened pattern to compound words.
The verbal nominal pattern \ ยชยน: loose:VN" matches compound words such as \ ร[ ยชยน"
(kigyo-teikei: a corporate joint venture) as well as \ ยชยน " (teikei: a joint venture).
The rst eld in a pattern is the pattern name, which refers to the concept associated
with the pattern. The second eld is a number indexing a eld in the pattern. This eld's
contents are used to decide quickly whether or not to search within a given string. The
matcher only applies the entire pattern to a string when the string contains the text in
the indexed eld. For eciency, therefore, this eld should contain the least frequent word
in the entire pattern (in this case, \ ยชยน " (teikei) for Japanese and \a joint venture" for
English).
The order of noun phrases is relatively unconstrained in a Japanese sentence. Case
markers, usually attached to the ends of noun phrases, provide a strong clue for identifying
the case role of each phrase (subject, object, etc.). Thus pattern matching driven mainly
by case markers recognizes the case roles well without parsing the sentence.
Approximately 150 patterns are used to extract various concepts in the Japanese joint
ventures domain. Several patterns usually match a single sentence. Moreover, since patterns
are often dened with case markers such as \ / " (wa), \  " (ga), and \ ( " (to), a single
pattern can match a sentence in more than one way when several of the same case markers
appear in the sentence. The template generator accepts only the best matched pattern,
which is chosen by applying the following three heuristic rules in the order shown:
1. select patterns that include the largest number of matched company-name variables
containing at least one company name;
2. select patterns that consume the fewest input segments (the shortest string match);
and
3. select patterns that include the largest number of variables and dened words.
These heuristic rules were obtained from an examination of matched patterns reported
by the system. To obtain more reliable heuristics, a large-scale statistical evaluation must
be performed. Heuristics for a similar problem of pattern selection in English are discussed
in (Rau & Jacobs, 1991). Their system chooses the pattern which consumes the most input
segments (the longest string match), as opposed to textract's choice of the shortest string
match in its second heuristic rule.2
2. In Rau and Jacobs' system, the third heuristic rule seems to be applied before the second rule. In this
case, there should be little dierence in performance between the heuristic rules of the two systems.
97

Kitani, Eriguchi, & Hara
Another important feature of the pattern matcher is that rules can be grouped according
to their concept. The rule name \JointVenture1" in Figure 3, for example, represents
the concept *joint-venture*. Using this grouping, the best matched pattern can be
selected from matched patterns of a particular concept group instead of from all the matched
patterns. This feature enables the discourse and template generation processes to narrow
their search for the best information to ll a particular slot.
4.3 Discourse processor

The following subsections describe the algorithm of company name reference resolution
throughout the discourse. Discourse segmentation and concept merging processes are also
discussed.
4.3.1 Identifying topic companies

Since no syntactic analysis is performed in textract, topic companies are simply identied wherever a subject case marker such as \  " (ga), \ / " (wa), or \ B " (mo) follows
company names. If no topic companies are found in a sentence, the previous sentence's
topic companies are inherited (even if the current sentence contains a non-company subject). This is based on the supposition that a sentence which introduces new companies
usually mentions them explicitly in its subject.
4.3.2 Abbreviation detection and unification

Company name abbreviations have the following observed characteristics:


majesty tags most abbreviations as \unknown", \company", \person", or \place";



a company name precedes its abbreviations;
an abbreviation is composed of two or more characters from the company name, in
their original order;





the characters need not be consecutive within the company name; and
English word abbreviations must be identical with an English word appearing in the
company name.

Thus the following are regarded as abbreviations: \unknown", \company", \person",
and \place" segments composed of two or more characters which also appear in company
names previously identied in the text. When comparing possible abbreviations against
known company names, the length of the longest common subsequence or LCS (Wagner &
Fischer, 1974) is computed to determine the maximum number of characters appearing in
the same order in both strings.3
To unify multiple references to the same company, a unique number is assigned to
the source and abbreviated companies. Repeated company names which contain strings
appearing earlier in the text are treated as abbreviations (and thus given unique numbers)
3. For example, the LCS of \abacbba" and \bcda" is \bca".
98

Pattern Matching and Discourse Processing
1. Step 1: Initialization to assign each entity in C a unique number.
for i in C do (1  i  cmax)
C [i; \id"] i
done
2. Step 2: Search abbreviations and give unique numbers
for i in C do (1  i  cmax)
if C [i; \id"] 6= i then
# already recognized as an abbreviation
continue i loop
LENSRC length of C [i; \string"]
for j in C do (i + 1  j  cmax)
if C [j; \id"] 6= j then
# already recognized as an abbreviation
continue j loop
LEN length of C [j; \string"]
LCS length of the LCS of C [i; \string"] and C [j; \string"]
if LCS  2 then do
if C [i; \eg"] = \YES" and LENSRC = LCS = LEN then
C[j; \id"] C [i; \id"] # an English word abbreviation
else if C [i; \eg"] = \NO" and LCS = LEN then
# an abbreviation
C[j; \id"] C [i; \id"]
done
done
done
Figure 4: Algorithm to unify multiple references to the same company
by the algorithm described in Figure 4. In the pseudocode shown, all identied company
names are stored in an associative array named C . \Unknown", \company", \person", and
\place" segments are also stored in the array as possible abbreviations. Company names are
sorted in ascending order of their starting position in the text and numbered from 1 to cmax
(Step 1). A company name string which is indexed i can be addressed by C [i; \string"]. A
ag C [i; \eg"] records whether the company name is an English word abbreviation or not.
Step 2 compares each company name in the array C with all names higher in the array
(and thus later in the text). When the LCS of a pair of earlier and later company names
is equal to the length of the later company name, the later company name is recognized as
an abbreviation of the earlier company name. Then, the \id" of the later company name is
replaced with that of the earlier company name. The LCS must be two or more characters,
and if the abbreviation is an English word, the LCS must be equal to the length of the
earlier company name.
At the end of execution, a number is given in C [i; \id"]. If C [i; \id"] was changed during
execution, C [i; \string"] was recognized as a company name abbreviation.
99

Kitani, Eriguchi, & Hara
4.3.3 Anaphora resolution of company name pronouns

The approach for reference resolution described in this section is based on heuristics obtained
by corpus analysis rather than linguistic theories. Three company name pronouns are the
target of reference resolution: \ !ย " (ryosya), \ $ย " (dosya), and \ rย " (jisya), meaning
\both companies", \the company", and \the company itself". They are three of the most
frequent company name pronouns appearing in our corpus provided by arpa for the tipster
information extraction project. \Ryosya", \dosya", and \jisya" appeared 456, 277, and 129
times, respectively, in 1100 newspaper articles containing an average of 481 characters per
article.
The following heuristics, derived from analysis of pronoun reference in the corpus, were
used for reference resolution:






\ryosya" almost always referred to the \current" tie-up company, with one exception
in a hundred occurrences;
about ninety percent of \dosya" occurrences referred to the topic company when there
was only one possible referent in the same sentence, but:
when more than two companies, including the topic company, preceded \dosya" in
the same sentence, about seventy-ve percent of the pronoun occurrences referred to
the nearest company, not necessarily the topic company; and
about eighty percent of \jisya" occurrences referred to the topic company.

Two additional heuristic rules were discovered but not implemented in textract:
 about four percent of \jisya" occurrences referred to more than one company; and
 about eight percent of \jisya" occurrences referred to entities which are general expressions about a company such as \ ย " (kaisya: a company).
As a result of the discourse processing described above, every company name, including
abbreviations and pronominal references, is given a unique number.
4.3.4 Discourse segmentation and concept merging
In the 150 articles of the tipster/muc-5 joint ventures test set, multiple tie-up relationships

appeared in thirty-one articles which included ninety individual tie-up relationships. The
two typical discourse models representing the discourse structures of tie-up relationships
are shown in Figure 5.




Type-I: tie-ups are described sequentially
Descriptions of tie-ups appear sequentially in this model. One tie-up is not mentioned
again after a new tie-up has been described.
Type-II: a main tie-up reappears after other tie-ups are mentioned
A major dierence from the Type-I model is that a description of a main tie-up
reappears in the text after other tie-up relationships have been introduced. Non-main
tie-ups are usually mentioned briey.
100

Pattern Matching and Discourse Processing

tie-up-1

tie-up-1

tie-up-2

tie-up-2
.
.non-main tie-ups
.

tie-up-3
.
.
.

tie-up-n

tie-up-n

tie-up-1

Type-I

Type-II

Figure 5: Discourse structure of tie-up relationships
Eleven Type-I structures and thirteen Type-II structures appeared in the thirty-one articles. Seven of the articles contained complicated discourse structures regarding the tie-up
relationships.
The two types of text structure described above are similar to the ones implemented in
the crl/brandeis diderot joint ventures system. The dierence is only in the Type-II
structure: diderot processes all tie-up relationships which reappear in the text, not just
the reappearing main tie-up focused on by textract.
textract's discourse processor divides the text when a dierent tie-up relationship is
identied by the template pattern search. A dierent tie-up relationship is recognized when
the numbers assigned to the joint venture companies are not identical to those appearing in
the previous tie-up relationships. diderot segments the discourse if any other related pieces
of information such as date and entity location are dierent between the tie-up relationships.
Such strict merging is preferable when the pieces of information in comparison are correctly
identied. The merging conditions of discourse segments should be chosen according to the
accuracy of identication of the information to be compared.
After the discourse is segmented, identied concepts and extracted words and phrases
are merged. Figure 6 shows the merging process for the following text passage which actually
appeared in the tipster/muc-5 test set (a direct English translation follows):
\ รรรผย/8oรฝ;.DยAAKย.ยย.o%ยU'.$ร

รฝยตRKยชยนยฑยRรS ยย.รฝยต'KH+*K56ย'+/!ย
รญรตZ&ยรยRK(Bย4 "

\On the eighth (of this month), Tanabe Pharmaceuticals made a joint
venture contract with a German pharmaceutical maker, Merck and Co.
Inc., to develop and sell its new medicine in Japan. They also agreed that
both companies would invest equally to establish a joint venture company
in ve or six years when they start selling new medicine."
101

Kitani, Eriguchi, & Hara

First sentence:
"On the eighth (of this month),
Tanabe Pharmaceuticals made
a joint venture contract with a
German pharmaceutical maker,
Merck and Co. Inc., to develop
and sell its new medicine in
Japan."

Second sentence:
"They also agreed that both
companies would invest equally
to establish a joint venture
company in five or six years when
they start selling new medicine."

"Tanabe Pharmaceuticals"
Template
pattern
search

*ESTABLISH*
"a joint venture
"both
company"
companies"

*ECONOMICACTIVITY*
"Merck and Co. Inc."

"Tanabe Pharmaceuticals"
Discourse
processor

"both
companies"

*ECONOMICACTIVITY*

*ESTABLISH*

"a joint venture
company"

"Merck and Co. Inc."

Figure 6: Example of concept merging
The two company names in the rst sentence, \ รรรผย " (tanabe seiyaku: Tanabe
Pharmaceuticals) and \ AKย " (ei meruku sya: Merck and Co. Inc.), are identied
by either majesty or the name recognizer during preprocessing. Next, the template pattern
search locates in the rst sentence the \economic activity" pattern shown in Figure 7 (a).
The *economic-activity* concept relating the two companies has now been recognized.
The template pattern search also recognizes the *establish* concept in the second sentence
by the template pattern shown in Figure 7 (b).
After sentence-level processing, discourse processing recognizes that \ !ย " (ryosya:
both companies) in the second sentence refers to Tanabe Pharmaceuticals and Merck in
the rst sentence because they are the current tie-up companies. Since the second sentence
does not introduce a new tie-up relationship, both sentences are in the same discourse
segment. Concepts separately identied in the two sentences can now be merged because
the subjects of the two sentences are the same. The *establish* concept is therefore joined
to the *economic-activity* concept.
(a)

(EconomicActivityE 6
@CNAME_PARTNER_SUBJ
:strict:P
@CNAME_PARTNER_SUBJ
:strict:P
@SKIP
:loose:VN)

(b)

(Establish3 6
@CNAME_PARTNER_SUBJ
:strict:P
@CNAME_CREATED_OBJ
:strict:P
@SKIP
:loose:VN)

/#
R


/#
.
$ร

Figure 7: Economic activity pattern (a) and establish pattern (b)
102

Pattern Matching and Discourse Processing

5. Performance evaluation
This section shows some evaluation results for textract's discourse module. muc-5 evaluation metrics and overall textract performance are also discussed.

5.1 Unique identication of company name abbreviations
A hundred joint ventures newspaper articles used for the tipster 18-month evaluation
were chosen as a blind test set for this evaluation. The evaluation measures were recall,
the percentage of correct answers extracted compared to possible answers, and precision,
the percentage of correct answers extracted compared to actual answers. majesty and
the name recognizer identied company names in the evaluation set with recall of seventyve percent and precision of ninety-ve percent when partial matches between expected
and recognized strings were allowed, and with recall of sixty-nine percent and precision of
eighty-seven percent in an exact matching condition.
Company names that appeared in a form dierent from their rst appearance in an
article were considered to be company name abbreviations. Among 318 abbreviations, the
recall and precision of abbreviation detection were sixty-seven and eighty-nine percent, respectively. Most importantly, detected abbreviations were unied correctly with the source
companies as long as the source companies were identied correctly by majesty and the
name recognizer.
The evaluation results clearly show that company name abbreviations were accurately
detected and unied with the source companies as long as company names were correctly
identied by the preceding processes. It is possible, however, that the simple string matching
algorithm currently used could erroneously unify similar company names, which are often
seen among family companies.

5.2 Anaphora resolution of company name pronouns
The accuracy of reference resolution for \ryosya", \dosya", and \jisya" is shown in Table
1. The numbers in parentheses were obtained by restricting attention to pronouns which
referred to companies identied correctly by the preceding processes. Since companies
referred to by \ryosya" (both companies) were usually \current" tie-up companies in the
joint ventures domain, reference resolution accuracy depended on the accuracy with which
tie-up relationships were identied.
company name pronouns

number of
references
\ !ย " (ryosya: both companies) 101 (93)
\ $ย " (dosya: the company)
100 (90)
\ rย " (jisya: the company itself) 60 (53)

resolution
accuracy
64% (70%)
78% (87%)
78% (89%)

Table 1: Accuracy of reference resolutions
103

Kitani, Eriguchi, & Hara
A major cause of incorrect references of \dosya" was the failure to locate topic companies. The simple mechanism of searching for topic companies using case markers did not
work well. A typical problem can be seen in the following example: \ ยชยน'/Xย " (A
joint venture partner is X Corp.). Here X Corp is a topic company, but the subject \ Xย "
(X Corp.) is not followed by a subject case marker. Other errors can be attributed to the
fact that \dosya" did not always refer to a topic company as discussed in the heuristic rules
of \dosya" reference resolution.
Regarding \jisya" resolutions, ve instances which should have referred to multiple
companies were bound to a single company. Since multiple companies are usually listed
using conjunctions such as \ ( " (to: and) and \ " (comma), they can be identied easily
if a simple phrase analysis is performed.
It became clear from this evaluation that resolving \dosya" references to a non-topic
company required intensive text understanding. Forty-seven percent of the occurrences of
\dosya" and \jisya" were bound to topic companies inherited from a previous sentence. This
result strongly supported the importance of keeping track of topic companies throughout
the discourse.

5.3 Discourse segmentation
Thirty-one of the 150 tipster/muc-5 evaluation test articles included ninety multiple tieup relationships. textract's discourse processor segmented the thirty-one articles into
seventy-one individual tie-up relationship blocks. Only thirty-eight of the blocks were correctly segmented. Main tie-up relationships which reappeared in Type-II discourse structures were not detected well, which caused the structures to be incorrectly recognized as
Type-I. This error was caused by the fact that the joint venture relationships were usually mentioned implicitly when they reappeared in the text. For example, a noun phrase,
\ ยฎ.ยชยน/ " (the joint venture this time), which was not detected by the template patterns used, brought the focus back to the main tie-up. As a result, textract identied eight
percent fewer tie-up relationships than the possible number expected in the tipster/muc-5
evaluation. This merging error must have aected system performance since the information in the reappearing main tie-up segment would not have been correctly linked to the
earlier main tie-up segment.
This preliminary study suggested that recognizing segmentation points in the text should
be regarded as crucial for performance. The template pattern matching alone was not good
enough to recognize the segmentation points. The discourse processor simply segmented
the text when it found a new tie-up relationship. The discourse models, currently unused at
run-time in textract, could be used to help infer the discourse structure when the system
is not sure whether to merge or separate discourse segments. Reference resolution of denite
and indenite noun phrases must also be solved for accurate discourse segmentation in future
research.
The accuracy of discourse segmentation might be improved by checking the dierence or
identity of date and entity location, as well as entity name, when deciding whether or not to
merge a tie-up relationship. textract did not take date and location objects into account
in making segmentation decisions, because textract's identication of these objects was
not considered reliable enough. For example, the date object was identied with recall of
104

Pattern Matching and Discourse Processing
only twenty-seven percent and precision of fty-nine percent. On the other hand, entities
were identied with over eighty percent accuracy in both recall and precision. To avoid
incorrect discourse segmentation, therefore, textract's merging conditions included only
entity names as reliable information.

5.4 Overall textract performance

250 newspaper articles, 150 about Japanese corporate joint ventures and 100 about Japanese
microelectronics, were provided by arpa for use in the tipster/muc-5 system evaluation.
Six joint ventures systems and ve microelectronics systems, including textract developed at cmu as an optional system of ge-cmu shogun, were presented in the Japanese
system evaluation at muc-5. A scoring program automatically compared the system output
with answer templates created by human analysts. When a human decision was necessary,
analysts instructed the scoring program whether the two strings in comparison were completely matched, partially matched, or unmatched. Finally, the scoring program calculated
an overall score combined from all the newspaper article scores. Although various evaluation metrics were measured in the evaluation (Chinchor & Sundheim, 1993), only the
following error-based and recall-precision-based metrics are discussed in this paper. The
basic scoring categories used are: correct (COR), partially correct (PAR), incorrect (INC),
missing (MIS), and spurious (SPU), counted as the number of pieces of information in the
system output compared to the possible information.
(1) Error-based metrics
 Error per response ll (ERR):
wrong = INC + P AR=2 + MIS + SPU
total COR + PAR + INC + MIS + SP U


Undergeneration (UND):

MIS
MIS =
possible COR + PAR + INC + MIS


Overgeneration (OVG):

SPU =
SPU
actual COR + P AR + INC + SP U


Substitution (SUB):

INC + P AR=2
COR + P AR + INC

105

Kitani, Eriguchi, & Hara

domain
ERR UND OVG SUB REC PRE P&R
textract (JJV)
50
32
23
12
60
68 63.8
System A (JJV)
54
36
27
12
57
64 60.1
System B (JJV)
63
51
23
12
42
67 52.1
textract (JME) 59
43
28
12
51
63 56.4
System A (JME)
58
30
38
14
60
53 56.3
System B (JME)
65
54
24
12
40
66 50.4
Table 2: Scores of textract and two other top-ranking ocial systems in tipster/muc-5
(2) Recall-precision-based metrics


Recall (REC):



Precision (PRE):



P&R F-measure (P&R):

COR + PAR=2
possible
COR + PAR=2
actual

2  REC  P RE
REC + PRE
The error per response ll (ERR) was the ocial measure of muc-5 system performance.
Secondary evaluation metrics were undergeneration (UND), overgeneration (OVG), and
substitution (SUB). The recall, precision, and F-measure metrics were used as unocial
metrics for muc-5.
Table 2 shows scores of textract and two other top-ranking ocial systems taken
from the tipster/muc-5 system evaluation results.4 textract processed only Japanese
domains of corporate joint ventures (JJV) and microelectronics (JME), whereas the two
other systems processed both English and Japanese text. textract performed as well as
the top-ranking systems in the two Japanese domains.
The human performance of four well-trained analysts was reported to be about eighty
percent in both recall and precision in the English microelectronics domain (Will, 1993).
This is about thirty percent better than the best tipster/muc-5 systems' performance
in P&R F-measure in the same language domain. In the Japanese joint ventures domain,
textract scored recall of seventy-ve percent and precision of eighty-one percent with
a core template comprising only essential objects. This result suggests that the current
technology could be used to support human extraction work if the task is well-constrained.
4. The textract scores submitted to muc-5 were unocial. It was scored ocially after the conference.
Table 2 shows textract's ocial scores.
106

Pattern Matching and Discourse Processing
Running on a SUN SPARCstation IPX, textract processed a joint ventures article in
about sixty seconds and a microelectronics article in about twenty-four seconds on average.
The human analysts took about fteen minutes to complete an English microelectronics
template and about sixty minutes for a Japanese joint ventures template (Will, 1993).
Thus a human-machine integrated system would be the best solution for fast, high quality,
information extraction.
Some tipster/muc-5 systems processed both Japanese and English domains. These
systems generally performed better in the Japanese domains than in the corresponding English domains. One likely reason is that the structure of Japanese articles is fairly standard,
particularly in the Japanese joint ventures domain, and can be readily analyzed into the
two discourse structure types described in this paper. Another possible reason is a characteristic of writing style: expressions which need to be identied tend to appear in the rst
few sentences in a form suitable for pattern matching.
The textract Japanese microelectronics system copied the preprocessor, the concept
search of the pattern matcher, and the company name unication of the discourse processor
used in the textract Japanese joint ventures system. The microelectronics system was
developed in only three weeks by one person who replaced joint ventures concepts and key
words with representative microelectronics concepts and key words. The lower performance
of the textract microelectronics system compared to the joint ventures system is largely
due to the short development time. It is also probably due to the less homogeneous discourse
structure and writing style of the microelectronics articles.
6. Conclusions and future research

This paper has described the importance of discourse processing in three aspects of information extraction: identifying key information throughout the text, i.e. topic companies and
company name references in the tipster/muc-5 domains; segmenting the text to select relevant portions of interest; and merging concepts identied by the sentence level processing.
The basic performance of the system depends on the preprocessor, however, since many
pieces of identied information are put directly into slots or are otherwise used to ll slots
during later processing. textract's pattern matcher solves the matching problem caused
by the segmentation ambiguities often found in Japanese compound words. The pattern
matching system based on a nite-state automaton is simple and runs fast. These factors
are essential for rapid system development and performance improvement.
To improve system performance with the pattern matching architecture, an increase
in the number of patterns is unavoidable. Since matching a large number of patterns is
a lengthy process, an ecient pattern matcher is required to shorten the running time.
Tomita's new generalized LR parser, known to be one of the fastest parsers for practical
purposes, skips unnecessary words during parsing (Bates & Lavie, 1991). The parser is
under evaluation to investigate if it is appropriate for information extraction from Japanese
text (Eriguchi & Kitani, 1993). Pattern matching alone, however, will not be able to improve
the system performance to human levels in a complicated information extraction task such
as tipster/muc-5, even if the task is well-dened and suitable for pattern matching. More
eorts should be made in discourse processing such as discourse segmentation and reference
resolution for denite and indenite noun phrases.
107

Kitani, Eriguchi, & Hara
The research discussed in this paper is based on an application-oriented, domain-specic,
and language-specic approach relying on patterns and heuristic rules collected from a
particular corpus. It is obvious that the patterns and heuristic rules described in this paper
do not cover a wide range of applications, domains, or languages. The empirical approach
described here is worth investigating even for an entirely new task, however, since it can
achieve a high level of system performance in a relatively short development time. While
linguistic theory-based systems tend to become complex and dicult to maintain, especially
if they incorporate full text parsing, the simplicity of an empirically-based, pattern-oriented
system such as textract keeps the development time short and the evaluation cycle quick.
Corpus analysis is a key element in this corpus-based paradigm. It is estimated that
corpus analysis took about half of the development time for textract. Statistically-based
corpus analysis tools are necessary to obtain better performance in a shorter development
time. Such tools could help developers not only extract important patterns and heuristic
rules from the corpus, but also monitor the system performance during the evaluationimprovement cycle.

Acknowledgements

The authors wish to express their appreciation to Jaime Carbonell, who provided the opportunity to pursue this research at the Center for Machine Translation, Carnegie Mellon
University. Thanks are also due to Teruko Mitamura and Michael Mauldin for their many
helpful suggestions.
References

Bates, J., & Lavie, A. (1991). Recognizing Substrings of LR(k) Languages in Linear Time.
Tech. rep. CMU-CS-91-188, Carnegie Mellon University, School of Computer Science.
Chinchor, N., & Sundheim, B. (1993). MUC-5 Evaluation Metrics. In Proceedings of the
Fifth Message Understanding Conference (MUC-5), pp. 69{78.
Cowie, J., Guthrie, L., et al. (1993). CRL/BRANDEIS: Description of the Diderot System
as Used for MUC-5. In Proceedings of the Fifth Message Understanding Conference
(MUC-5), pp. 161{179.
Doi, S., Ando, S., & Muraki, K. (1993). Context Analysis in Information Extraction System
Based on Keywords and Text Structure. In Proceedings of the Forty-seventh Annual
Conference of IPSJ (in Japanese).
Eriguchi, Y., & Kitani, T. (1993). A Preliminary Study of Using Tomita's Generalized LR
Parser for Information Extraction. Unpublished paper, Center for Machine Translation, Carnegie Mellon University.
Fujii, H., & Croft, B. (1993). A Comparison of Indexing Techniques for Japanese Text Retrieval. In Proceedings of the Sixteenth Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval, pp. 237{246.
Hirschman, L. (1992). An Adjunct Test for Discourse Processing in MUC-4. In Proceedings
of the Fourth Message Understanding Conference (MUC-4), pp. 67{77.
108

Pattern Matching and Discourse Processing
Hobbs, J., Appelt, D., et al. (1992). FASTUS: A System for Extracting Information from
Natural-Language Text. Tech. rep. 519, SRI International.
Jacobs, P. (1993). TIPSTER/SHOGUN 18-Month Progress Report. In Notebook of TIPSTER 18-Month Meeting.
Jacobs, P., Krupka, G., et al. (1993). GE-CMU: Description of the Shogun System Used
for MUC-5. In Proceedings of the Fifth Message Understanding Conference (MUC-5),
pp. 109{120.
Karasawa, I. (1993). Detection of Company Name Abbreviations in Japanese Texts. Unpublished paper, Center for Machine Translation, Carnegie Mellon University.
Kitani, T. (1991). An OCR Post-processing Method for Handwritten Japanese Documents.
In Proceedings of Natural Language Processing Pacic Rim Symposium, pp. 38{45.
Kitani, T. (1994). Merging Information by Discourse Processing for Information Extraction.
In Proceedings of the Tenth IEEE Conference on Articial Intelligence for Applications, pp. 412{418.
Kitani, T., & Mitamura, T. (1993). A Japanese Preprocessor for Syntactic and Semantic
Parsing. In Proceedings of the Ninth IEEE Conference on Articial Intelligence for
Applications, pp. 86{92.
Kitani, T., & Mitamura, T. (1994). An Accurate Morphological Analysis and Proper Name
Identication for Japanese Text Processing. Journal of Information Processing Society
of Japan, 35 (3), 404{413.
Lehnert, W., McCarthy, J., et al. (1993). UMASS/HUGHES: Description of the CIRCUS
System Used for MUC-5. In Proceedings of the Fifth Message Understanding Conference (MUC-5), pp. 277{291.
Lehnert, W., & Sundheim, B. (1991). A Performance Evaluation of Text-Analysis Technologies. AI Magazine, Fall, 81{94.
Muraki, K., Doi, S., & Ando, S. (1993). NEC: Description of the VENIEX System as Used
for MUC-5. In Proceedings of the Fifth Message Understanding Conference (MUC-5),
pp. 147{159.
Onyshkevych, B. (1993). Technology Perspective. In Notebook of the Fifth Message Understanding Conference (MUC-5).
Pereira, F. (1990). Finite-State Approximations of Grammars. In Proceedings of DARPA
Speech and Natural Language Workshop, pp. 20{25.
Rau, L., & Jacobs, P. (1991). Creating Segmented Databases from Free Text for Text
Retrieval. In Proceedings of Fourteenth Annual International ACM/SIGIR Conference
on Research and Development in Information Retrieval, pp. 337{346.
Tipster (1992). Joint Venture Template Fill Rules. In Plenary Session Notebook of the
TIPSTER 12-Month Meeting.
109

Kitani, Eriguchi, & Hara
Wagner, R., & Fischer, M. (1974). The String-to-String Correction Problem. Journal of
ACM, 21 (1), 168{173.
Wakao, T. (1994). Reference Resolution Using Semantic Patterns in Japanese Newspaper
Articles. In Proceedings of COLING 94, pp. 1133{1137.
Weischedel, R., Ayuso, D., et al. (1993). BBN: Description of the PLUM System as Used
for MUC-5. In Proceedings of the Fifth Message Understanding Conference (MUC-5),
pp. 93{107.
Will, C. (1993). Comparing Human and Machine Performance for Natural Language Information Extraction: Results for English Microelectronics from the MUC-5 Evaluation.
In Proceedings of the Fifth Message Understanding Conference (MUC-5), pp. 53{67.

110

Journal of Artificial Intelligence Research 2 (1995) 369-409

Submitted 10/94; published 3/95

Cost-Sensitive Classification: Empirical Evaluation
of a Hybrid Genetic Decision Tree Induction Algorithm
Peter D. Turney
Knowledge Systems Laboratory, Institute for Information Technology
National Research Council Canada, Ottawa, Ontario, Canada, K1A 0R6.

TURNEY@AI.IIT.NRC.CA

Abstract
This paper introduces ICET, a new algorithm for cost-sensitive classification. ICET
uses a genetic algorithm to evolve a population of biases for a decision tree induction algorithm. The fitness function of the genetic algorithm is the average cost of classification
when using the decision tree, including both the costs of tests (features, measurements) and
the costs of classification errors. ICET is compared here with three other algorithms for
cost-sensitive classification โ EG2, CS-ID3, and IDX โ and also with C4.5, which classifies without regard to cost. The five algorithms are evaluated empirically on five realworld medical datasets. Three sets of experiments are performed. The first set examines the
baseline performance of the five algorithms on the five datasets and establishes that ICET
performs significantly better than its competitors. The second set tests the robustness of
ICET under a variety of conditions and shows that ICET maintains its advantage. The third
set looks at ICETโs search in bias space and discovers a way to improve the search.

1. Introduction
The prototypical example of the problem of cost-sensitive classification is medical diagnosis, where a doctor would like to balance the costs of various possible medical tests with the
expected benefits of the tests for the patient. There are several aspects to this problem: When
does the benefit of a test, in terms of more accurate diagnosis, justify the cost of the test?
When is it time to stop testing and make a commitment to a particular diagnosis? How much
time should be spent pondering these issues? Does an extensive examination of the various
possible sequences of tests yield a significant improvement over a simpler, heuristic choice
of tests? These are some of the questions investigated here.
The words โcostโ, โexpenseโ, and โbenefitโ are used in this paper in the broadest sense,
to include factors such as quality of life, in addition to economic or monetary cost. Cost is
domain-specific and is quantified in arbitrary units. It is assumed here that the costs of tests
are measured in the same units as the benefits of correct classification. Benefit is treated as
negative cost.
This paper introduces a new algorithm for cost-sensitive classification, called ICET
(Inexpensive Classification with Expensive Tests โ pronounced โiced teaโ). ICET uses a
genetic algorithm (Grefenstette, 1986) to evolve a population of biases for a decision tree
induction algorithm (a modified version of C4.5, Quinlan, 1992). The fitness function of the
genetic algorithm is the average cost of classification when using the decision tree, including
both the costs of tests (features, measurements) and the costs of classification errors. ICET
has the following features: (1) It is sensitive to test costs. (2) It is sensitive to classification
error costs. (3) It combines a greedy search heuristic with a genetic search algorithm. (4) It
can handle conditional costs, where the cost of one test is conditional on whether a second

ยฉ 1995 National Research Council Canada. All rights reserved. Published by permission.

T URNEY

test has been selected yet. (5) It distinguishes tests with immediate results from tests with
delayed results.
The problem of cost-sensitive classification arises frequently. It is a problem in medical
diagnosis (Nรบรฑez, 1988, 1991), robotics (Tan & Schlimmer, 1989, 1990; Tan, 1993), industrial production processes (Verdenius, 1991), communication network troubleshooting
(Lirov & Yue, 1991), machinery diagnosis (where the main cost is skilled labor), automated
testing of electronic equipment (where the main cost is time), and many other areas.
There are several machine learning algorithms that consider the costs of tests, such as
EG2 (Nรบรฑez, 1988, 1991), CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993), and IDX
(Norton, 1989). There are also several algorithms that consider the costs of classification
errors (Breiman et al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974; Gordon &
Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, in press; Knoll et al.,
1994). However, there is very little work that considers both costs together.
There are good reasons for considering both the costs of tests and the costs of classification errors. An agent cannot rationally determine whether a test should be performed without
knowing the costs of correct and incorrect classification. An agent must balance the cost of
each test with the contribution of the test to accurate classification. The agent must also consider when further testing is not economically justified. It often happens that the benefits of
further testing are not worth the costs of the tests. This means that a cost must be assigned to
both the tests and the classification errors.
Another limitation of many existing cost-sensitive classification algorithms (EG2, CSID3) is that they use greedy heuristics, which select at each step whatever test contributes
most to accuracy and least to cost. A more sophisticated approach would evaluate the interactions among tests in a sequence of tests. A test that appears useful considered in isolation,
using a greedy heuristic, may not appear as useful when considered in combination with
other tests. Past work has demonstrated that more sophisticated algorithms can have superior
performance (Tcheng et al., 1989; Ragavan & Rendell, 1993; Norton, 1989; Schaffer, 1993;
Rymon, 1993; Seshu, 1989; Provost, 1994; Provost & Buchanan, in press).
Section 2 discusses why a decision tree is the natural form of knowledge representation
for classification with expensive tests and how we measure the average cost of classification
of a decision tree. Section 3 introduces the five algorithms that we examine here, C4.5
(Quinlan, 1992), EG2 (Nรบรฑez, 1991), CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993),
IDX (Norton, 1989), and ICET. The five algorithms are evaluated empirically on five realworld medical datasets. The datasets are discussed in detail in Appendix A. Section 4 presents three sets of experiments. The first set (Section 4.1) of experiments examines the baseline performance of the five algorithms on the five datasets and establishes that ICET
performs significantly better than its competitors for the given datasets. The second set (Section 4.2) tests the robustness of ICET under a variety of conditions and shows that ICET
maintains its advantage. The third set (Section 4.3) looks at ICETโs search in bias space and
discovers a way to improve the search. We then discuss related work and future work in Section 5. We end with a summary of what we have learned with this research and a statement of
the general motivation for this type of research.

2. Cost-Sensitive Classification
This section first explains why a decision tree is the natural form of knowledge representation for classification with expensive tests. It then discusses how we measure the average
cost of classification of a decision tree. Our method for measuring average cost handles
370

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

aspects of the problem that are typically ignored. The method can be applied to any standard
classification decision tree, regardless of how the tree is generated. We end with a discussion
of the relation between cost and accuracy.
2.1

Decision Trees and Cost-Sensitive Classification

The decision trees used in decision theory (Pearl, 1988) are somewhat different from the
classification decision trees that are typically used in machine learning (Quinlan, 1992).
When we refer to decision trees in this paper, we mean the standard classification decision
trees of machine learning. The claims we make here about classification decision trees also
apply to decision theoretical decision trees, with some modification. A full discussion of
decision theoretical decision trees is outside of the scope of this paper.
The decision to do a test must be based on both the cost of tests and the cost of classification errors. If a test costs $10 and the maximum penalty for a classification error is $5, then
there is clearly no point in doing the test. On the other hand, if the penalty for a classification
error is $10,000, the test may be quite worthwhile, even if its information content is relatively low. Past work with algorithms that are sensitive to test costs (Nรบรฑez, 1988, 1991;
Tan, 1993; Norton, 1989) has overlooked the importance of also considering the cost of classification errors.
When tests are inexpensive, relative to the cost of classification errors, it may be rational
to do all tests (i.e., measure all features; determine the values of all attributes) that seem possibly relevant. In this kind of situation, it is convenient to separate the selection of tests from
the process of making a classification. First we can decide on the set of tests that are relevant, then we can focus on the problem of learning to classify a case, using the results of
these tests. This is a common approach to classification in the machine learning literature.
Often a paper focuses on the problem of learning to classify a case, without any mention of
the decisions involved in selecting the set of relevant tests.1
When tests are expensive, relative to the cost of classification errors, it may be suboptimal to separate the selection of tests from the process of making a classification. We may be
able to achieve much lower costs by interleaving the two. First we choose a test, then we
examine the test result. The result of the test gives us information, which we can use to influence our choice for the next test. At some point, we decide that the cost of further tests is not
justified, so we stop testing and make a classification.
When the selection of tests is interleaved with classification in this way, a decision tree is
the natural form of representation. The root of the decision tree represents the first test that
we choose. The next level of the decision tree represents the next test that we choose. The
decision tree explicitly shows how the outcome of the first test determines the choice of the
second test. A leaf represents the point at which we decide to stop testing and make a classification.
Decision theory can be used to define what constitutes an optimal decision tree, given (1)
the costs of the tests, (2) the costs of classification errors, (3) the conditional probabilities of
test results, given sequences of prior test results, and (4) the conditional probabilities of
classes, given sequences of test results. However, searching for an optimal tree is infeasible
(Pearl, 1988). ICET was designed to find a good (but not necessarily optimal) tree, where
โgoodโ is defined as โbetter than the competitionโ (i.e., IDX, CS-ID3, and EG2).
1. Not all papers are like this. Decision tree induction algorithms such as C4.5 (Quinlan, 1992) automatically
select relevant tests. Aha and Bankert (1994), among others, have used sequential test selection procedures in
conjunction with a supervised learning algorithm.

371

T URNEY

2.2

Calculating the Average Cost of Classification

In this section, we describe how we calculate the average cost of classification for a decision
tree, given a set of testing data. The method described here is applied uniformly to the decision trees generated by the five algorithms examined here (EG2, CS-ID3, IDX, C4.5, and
ICET). The method assumes only a standard classification decision tree (such as generated
by C4.5); it makes no assumptions about how the tree is generated. The purpose of the
method is to give a plausible estimate of the average cost that can be expected in a real-world
application of the decision tree.
We assume that the dataset has been split into a training set and a testing set. The
expected cost of classification is estimated by the average cost of classification for the testing set. The average cost of classification is calculated by dividing the total cost for the
whole testing set by the number of cases in the testing set. The total cost includes both the
costs of tests and the costs of classification errors. In the simplest case, we assume that we
can specify test costs simply by listing each test, paired with its corresponding cost. More
complex cases will be considered later in this section. We assume that we can specify the
costs of classification errors using a classification cost matrix.
Suppose there are c distinct classes. A classification cost matrix is a c ร c matrix, where
the element C i, j is the cost of guessing that a case belongs in class i, when it actually
belongs in class j. We do not need to assume any constraints on this matrix, except that costs
are finite, real values. We allow negative costs, which can be interpreted as benefits. However, in the experiments reported here, we have restricted our attention to classification cost
matrices in which the diagonal elements are zero (we assume that correct classification has
no cost) and the off-diagonal elements are positive numbers. 2
To calculate the cost of a particular case, we follow its path down the decision tree. We
add up the cost of each test that is chosen (i.e., each test that occurs in the path from the root
to the leaf). If the same test appears twice, we only charge for the first occurrence of the test.
For example, one node in a path may say โpatient age is less than 10 yearsโ and another node
may say โpatient age is more than 5 yearsโ, but we only charge once for the cost of determining the patientโs age. The leaf of the tree specifies the treeโs guess for the class of the case.
Given the actual class of the case, we use the cost matrix to determine the cost of the treeโs
guess. This cost is added to the costs of the tests, to determine the total cost of classification
for the case.
This is the core of our method for calculating the average cost of classification of a decision tree. There are two additional elements to the method, for handling conditional test
costs and delayed test results.
We allow the cost of a test to be conditional on the choice of prior tests. Specifically, we
consider the case where a group of tests shares a common cost. For example, a set of blood
tests shares the common cost of collecting blood from the patient. This common cost is
charged only once, when the decision is made to do the first blood test. There is no charge
for collecting blood for the second blood test, since we may use the blood that was collected
for the first blood test. Thus the cost of a test in this group is conditional on whether another
member of the group has already been chosen.
Common costs appear frequently in testing. For example, in diagnosis of an aircraft
engine, a group of tests may share the common cost of removing the engine from the plane
2. This restriction seems reasonable as a starting point for exploring cost-sensitive classification. In future work,
we will investigate the effects of weakening the restriction.

372

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

and installing it in a test cell. In semiconductor manufacturing, a group of tests may share the
common cost of reserving a region on the silicon wafer for a special test structure. In image
recognition, a group of image processing algorithms may share a common preprocessing
algorithm. These examples show that a realistic assessment of the cost of using a decision
tree will frequently need to make allowances for conditional test costs.
It often happens that the result of a test is not available immediately. For example, a
medical doctor typically sends a blood test to a laboratory and gets the result the next day.
We allow a test to be labelled either โimmediateโ or โdelayedโ. If a test is delayed, we cannot
use its outcome to influence the choice of the next test. For example, if blood tests are
delayed, then we cannot allow the outcome of one blood test to play a role in the decision to
do a second blood test. We must make a commitment to doing (or not doing) the second
blood test before we know the results of the first blood test.
Delayed tests are relatively common. For example, many medical tests must be shipped
to a laboratory for analysis. In gas turbine engine diagnosis, the main fuel control is frequently shipped to a specialized company for diagnosis or repair. In any classification problem that requires multiple experts, one of the experts might not be immediately available.
We handle immediate tests in a decision tree as described above. We handle delayed tests
as follows. We follow the path of a case from the root of the decision tree to the appropriate
leaf. If we encounter a node, anywhere along this path, that is a delayed test, we are then
committed to performing all of the tests in the subtree that is rooted at this node. Since we
cannot make the decision to do tests below this node conditional on the outcome of the test at
this node, we must pledge to pay for all the tests that we might possibly need to perform,
from this point onwards in the decision tree.
Our method for handling delayed tests may seem a bit puzzling at first. The difficulty is
that a decision tree combines a method for selecting tests with a method for classifying
cases. When tests are delayed, we are forced to proceed in two phases. In the first phase, we
select tests. In the second phase, we collect test results and classify the case. For example, a
doctor collects blood from a patient and sends the blood to a laboratory. The doctor must tell
the laboratory what tests are to be done on the blood. The next day, the doctor gets the results
of the tests from the laboratory and then decides on the diagnosis of the patient. A decision
tree does not naturally handle a situation like this, where the selection of tests is isolated
from the classification of cases. In our method, in the first phase, the doctor uses the decision
tree to select the tests. As long as the tests are immediate, there is no problem. As soon as the
first delayed test is encountered, the doctor must select all the tests that might possibly be
needed in the second phase.3 That is, the doctor must select all the tests in the subtree rooted
at the first delayed test. In the second phase, when the test results arrive the next day, the
doctor will have all the information required to go from the root of the tree to a leaf, to make
a classification. The doctor must pay for all of the tests in the subtree, even though only the
tests along one branch of the subtree will actually be used. The doctor does not know in
advance which branch will actually be used, at the time when it is necessary to order the
blood tests. The laboratory that does the blood tests will naturally want the doctor to pay for
all the tests that were ordered, even if they are not all used in making the diagnosis.
In general, it makes sense to do all of the desired immediate tests before we do any of the
desired delayed tests, since the outcome of an immediate test can be used to influence the
decision to do a delayed test, but not vice versa. For example, a medical doctor will question
3. This is a simplification of the situation in the real world. A more realistic treatment of delayed tests is one of
the areas for future work (Section 5.2).

373

T URNEY

a patient (questions are immediate tests) before deciding what blood tests to order (blood
tests are delayed tests).4
When all of the tests are delayed (as they are in the BUPA data in Appendix A.1), we
must decide in advance (before we see any test results) what tests are to be performed. For a
given decision tree, the total cost of tests will be the same for all cases. In situations of this
type, the problem of minimizing cost simplifies to the problem of choosing the best subset of
the set of available tests (Aha and Bankert, 1994). The sequential order of the tests is no
longer important for reducing cost.
Let us consider a simple example to illustrate the method. Table 1 shows the test costs
for four tests. Two of the tests are immediate and two are delayed. The two delayed tests
share a common cost of $2.00. There are two classes, 0 and 1. Table 2 shows the classification cost matrix. Figure 1 shows a decision tree. Table 3 traces the path through the tree for a
particular case and shows how the cost is calculated. The first step is to do the test at the root
of the tree (test alpha). In the second step, we encounter a delayed test (delta), so we must
calculate the cost of the entire subtree rooted at this node. Note that epsilon only costs $8.00,
since we have already selected delta, and delta and epsilon have a common cost. In the third
step, we do test epsilon, but we do not need to pay, since we already paid in the second step.
In the fourth step, we guess the class of the case. Unfortunately, we guess incorrectly, so we
pay a penalty of $50.00.
Table 1: Test costs for a simple example.
Test

Group

Cost

Delayed

1

alpha

$5.00

no

2

beta

$10.00

no

3

delta

A

$7.00 if first test in group A,
$5.00 otherwise

yes

4

epsilon

A

$10.00 if first test in group A,
$8.00 otherwise

yes

Table 2: Classification costs for a simple example.
Actual Class

Guess Class

Cost

0

0

$0.00

0

1

$50.00

1

0

$50.00

1

1

$0.00

4. In the real world, there are many factors that can influence the sequence of tests, such as the length of the delay
and the probability that the delayed test will be needed. When we ignore these many factors and pay attention
only to the simplified model presented here, it makes sense to do all of the desired immediate tests before we
do any of the desired delayed tests. We do not know to what extent this actually occurs in the real world. One
complication is that medical doctors in most industrialized countries are not directly affected by the cost of the
tests they select. In fact, fear of law suits gives them incentive to order unnecessary tests.

374

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

alpha < 3
T

F

beta > 6
T

delta = 2
F

0

T

F

1
beta < 5
T

epsilon < 4
F

1

T

0

0

F

1

Figure 1: Decision tree for a simple example.

Table 3: Calculating the cost for a particular case.
Step

Action

Result

Cost

1

do alpha

alpha = 6

$5.00

2

do delta

delta = 3

$7.00 + $10.00 + $8.00 = $25.00

3

do epsilon

epsilon = 2

already paid, in step #2

4

guess class = 0

actual class = 1

$50.00

total cost

$80.00

In summary, this section presents a method for estimating the average cost of using a
given decision tree. The decision tree can be any standard classification decision tree; no
special assumptions are made about the tree; it does not matter how the tree was generated.
The method requires (1) a decision tree (Figure 1), (2) information on the calculation of test
costs (Table 1), (3) a classification cost matrix (Table 2), and (4) a set of testing data (Table
3). The method is (i) sensitive to the cost of tests, (ii) sensitive to the cost of classification
errors, (iii) capable of handling conditional test costs, and (iv) capable of handling delayed
tests. In the experiments reported in Section 4, this method is applied uniformly to all five
algorithms.
2.3

Cost and Accuracy

Our method for calculating cost does not explicitly deal with accuracy; however, we can
handle accuracy as a special case. If the test cost is set to $0.00 for all tests and the classification cost matrix is set to a positive constant value k when the guess class i does not equal
the actual class j, but it is set to $0.00 when i equals j, then the average total cost of using the
decision tree is pk , where p โ [0,1] is the frequency of errors on the testing dataset and
375

T URNEY

100 ( 1 โ p ) is the percentage accuracy on the testing dataset. Thus there is a linear relationship between average total cost and percentage accuracy, in this situation.
More generally, let C be a classification cost matrix that has cost x on the diagonal,
C i, i = x , and cost y off the diagonal, ( i โ j ) โ ( C i, j = y ) , where x is less than y, x < y . We
will call this type of classification cost matrix a simple classification cost matrix. A cost
matrix that is not simple will be called a complex classification cost matrix.5 When we have
a simple cost matrix and test costs are zero (equivalently, test costs are ignored), minimizing
cost is exactly equivalent to maximizing accuracy.
It follows from this that an algorithm that is sensitive to misclassification error costs but
ignores test costs (Breiman et al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974;
Gordon & Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, in press;
Knoll et al., 1994) will only be interesting when we have a complex cost matrix. If we have
a simple cost matrix, an algorithm such as CART (Breiman et al., 1984) that is sensitive to
misclassification error cost has no advantage over an algorithm such as C4.5 (Quinlan, 1992)
that maximizes accuracy (assuming other differences between these two algorithms are negligible). Most of the experiments in this paper use a simple cost matrix (the only exception is
Section 4.2.3). Therefore we focus on comparison of ICET with algorithms that are sensitive
to test cost (IDX, CS-ID3, and EG2). In future work, we will examine complex cost matrices
and compare ICET with algorithms that are sensitive to misclassification error cost.
It is difficult to find information on the costs of misclassification errors in medical practice, but it seems likely that a complex cost matrix is more appropriate than a simple cost
matrix for most medical applications. This paper focuses on simple cost matrices because, as
a research strategy, it seems wise to start with the simple cases before we attempt the complex cases.
Provost (Provost, 1994; Provost & Buchanan, in press) combines accuracy and classification error cost using the following formula:
score = A โ accuracy โ B โ cost

(1)

In this formula, A and B are arbitrary weights that the user can set for a particular application. Both โaccuracyโ and โcostโ, as defined by Provost (Provost, 1994; Provost & Buchanan, in press), can be represented using classification cost matrices. We can represent
โaccuracyโ using any simple cost matrix. In interesting applications, โcostโ will be represented by a complex cost matrix. Thus โscoreโ is a weighted sum of two classification cost
matrices, which means that โscoreโ is itself a classification cost matrix. This shows that
equation (1) can be handled as a special case of the method presented here. There is no loss
of information in this translation of Provostโs formula into a cost matrix. This does not mean
that all criteria can be represented as costs. An example of a criterion that cannot be represented as a cost is stability (Turney, in press).

3. Algorithms
This section discusses the algorithms used in this paper: C4.5 (Quinlan, 1992), EG2 (Nรบรฑez,
1991), CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993), IDX (Norton, 1989), and ICET.

5. We will occasionally say โsimple cost matrixโ or โcomplex cost matrixโ. This should not cause confusion,
since test costs are not represented with a matrix.

376

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

3.1

C4.5

C4.5 (Quinlan, 1992) builds a decision tree using the standard TDIDT (top-down induction
of decision trees) approach, recursively partitioning the data into smaller subsets, based on
the value of an attribute. At each step in the construction of the decision tree, C4.5 selects
the attribute that maximizes the information gain ratio. The induced decision tree is pruned
using pessimistic error estimation (Quinlan, 1992). There are several parameters that can be
adjusted to alter the behavior of C4.5. In our experiments with C4.5, we used the default settings for all parameters. We used the C4.5 source code that is distributed with (Quinlan,
1992).
3.2

EG2

EG2 (Nรบรฑez, 1991) is a TDIDT algorithm that uses the Information Cost Function (ICF)
(Nรบรฑez, 1991) for selection of attributes. ICF selects attributes based on both their information gain and their cost. We implemented EG2 by modifying the C4.5 source code so that
ICF was used instead of information gain ratio.
ICF for the i-th attribute, ICF i , is defined as follows:6
โI i

2 โ1
ICF i = ------------------------ฯ
( Ci + 1)

where 0 โค ฯ โค 1

(2)

In this equation, โI i is the information gain associated with the i-th attribute at a given stage
in the construction of the decision tree and C i is the cost of measuring the i-th attribute. C4.5
selects the attribute that maximizes the information gain ratio, which is a function of the
information gain โI i . We modified C4.5 so that it selects the attribute that maximizes ICF i .
The parameter ฯ adjusts the strength of the bias towards lower cost attributes. When
ฯ = 0 , cost is ignored and selection by ICF i is equivalent to selection by โI i . When
ฯ = 1 , ICF i is strongly biased by cost. Ideally, ฯ would be selected in a way that is sensitive to classification error cost (this is done in ICET โ see Section 3.5). Nรบรฑez (1991) does
not suggest a principled way of setting ฯ . In our experiments with EG2, ฯ was set to 1. In
other words, we used the following selection measure:
โI i

2 โ1
----------------Ci + 1

(3)

In addition to its sensitivity to the cost of tests, EG2 generalizes attributes by using an ISA
tree (a generalization hierarchy). We did not implement this aspect of EG2, since it was not
relevant for the experiments reported here.
3.3

CS-ID3

CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993) is a TDIDT algorithm that selects the
attribute that maximizes the following heuristic function:
2

( โI i )
--------------Ci

(4)

6. This is the inverse of ICF, as defined by Nรบรฑez (1991). Nรบรฑez minimizes his criterion. To facilitate comparison
with the other algorithms, we use equation (2). This criterion is intended to be maximized.

377

T URNEY

We implemented CS-ID3 by modifying C4.5 so that it selects the attribute that maximizes
(4).
CS-ID3 uses a lazy evaluation strategy. It only constructs the part of the decision tree
that classifies the current case. We did not implement this aspect of CS-ID3, since it was not
relevant for the experiments reported here.
3.4

IDX

IDX (Norton, 1989) is a TDIDT algorithm that selects the attribute that maximizes the following heuristic function:
โI i
------Ci

(5)

We implemented IDX by modifying C4.5 so that it selects the attribute that maximizes (5).
C4.5 uses a greedy search strategy that chooses at each step the attribute with the highest
information gain ratio. IDX uses a lookahead strategy that looks n tests ahead, where n is a
parameter that may be set by the user. We did not implement this aspect of IDX. The lookahead strategy would perhaps make IDX more competitive with ICET, but it would also complicate comparison of the heuristic function (5) with the heuristics (3) and (4) used by EG2
and CS-ID3.
3.5

ICET

ICET is a hybrid of a genetic algorithm and a decision tree induction algorithm. The genetic
algorithm evolves a population of biases for the decision tree induction algorithm. The
genetic algorithm we use is GENESIS (Grefenstette, 1986).7 The decision tree induction
algorithm is C4.5 (Quinlan, 1992), modified to use ICF. That is, the decision tree induction
algorithm is EG2, implemented as described in Section 3.2.
ICET uses a two-tiered search strategy. On the bottom tier, EG2 performs a greedy
search through the space of decision trees, using the standard TDIDT strategy. On the top
tier, GENESIS performs a genetic search through a space of biases. The biases are used to
modify the behavior of EG2. In other words, GENESIS controls EG2โs preference for one
type of decision tree over another.
ICET does not use EG2 the way it was designed to be used. The n costs, C i , used in
EG2โs attribute selection function, are treated by ICET as bias parameters, not as costs. That
is, ICET manipulates the bias of EG2 by adjusting the parameters, C i . In ICET, the values of
the bias parameters, C i , have no direct connection to the actual costs of the tests.
Genetic algorithms are inspired by biological evolution. The individuals that are evolved
by GENESIS are strings of bits. GENESIS begins with a population of randomly generated
individuals (bit strings) and then it measures the โfitnessโ of each individual. In ICET, an
individual (a bit string) represents a bias for EG2. An individual is evaluated by running EG2
on the data, using the bias of the given individual. The โfitnessโ of the individual is the average cost of classification of the decision tree that is generated by EG2. In the next generation, the population is replaced with new individuals. The new individuals are generated
from the previous generation, using mutation and crossover (sex). The fittest individuals in
the first generation have the most offspring in the second generation. After a fixed number of
7. We used GENESIS Version 5.0, which is available at URL ftp://ftp.aic.nrl.navy.mil/pub/galist/src/ga/genesis.tar.Z or ftp://alife.santafe.edu/pub/USER-AREA/EC/GA/src/gensis-5.0.tar.gz.

378

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

generations, ICET halts and its output is the decision tree determined by the fittest individual. Figure 2 gives a sketch of the ICET algorithm.

GENESIS
fittest

genetic algorithm

decision tree
population of
biases
EG2

data

fitness

decision tree

classifier

function
EG2
decision tree

classifier
EG2

decision tree

classifier

Figure 2: A sketch of the ICET algorithm.
GENESIS has several parameters that can be used to alter its performance. The parameters we used are listed in Table 4. These are essentially the default parameter settings
(Grefenstette, 1986). We used a population size of 50 individuals and 1,000 trials, which
results in 20 generations. An individual in the population consists of a string of n + 2 numbers, where n is the number of attributes (tests) in the given dataset. The n + 2 numbers are
represented in binary format, using a Gray code.8 This binary string is used as a bias for
EG2. The first n numbers in the string are treated as if they were the n costs, C i , used in ICF
(equation (2)). The first n numbers range from 1 to 10,000 and are coded with 12 binary digits each. The last two numbers in the string are used to set ฯ and CF. The parameter ฯ is
used in ICF. The parameter CF is used in C4.5 to control the level of pruning of the decision
tree. The last two numbers are coded with 8 binary digits each. ฯ ranges from 0 (cost is
ignored) to 1 (maximum sensitivity to cost) and CF ranges from 1 (high pruning) to 100 (low
pruning). Thus an individual is a string of 12n + 16 bits.
Each trial of an individual consists of running EG2 (implemented as a modification to
C4.5) on a given training dataset, using the numbers specified in the binary string to set C i
( i = 1, โฆ, n ), ฯ , and CF. The training dataset is randomly split into two equal-sized subsets
( ยฑ 1 for odd-sized training sets), a sub-training set and a sub-testing set. A different random
split is used for each trial, so the outcome of a trial is stochastic. We cannot assume that
identical individuals yield identical outcomes, so every individual must be evaluated. This
means that there will be duplicate individuals in the population, with slightly different fitness
scores. The measure of fitness of an individual is the average cost of classification on the
sub-testing set, using the decision tree that was generated on the sub-training set. The aver8. A Gray code is a binary code that is designed to avoid โHamming cliffsโ. In the standard binary code, 7 is represented as 0111 and 8 is represented as 1000. These numbers are adjacent, yet the Hamming distance from
0111 to 1000 is large. In a Gray code, adjacent numbers are represented with binary codes that have small
Hamming distances. This tends to improve the performance of a genetic algorithm (Grefenstette, 1986).

379

T URNEY

Table 4: Parameter settings for GENESIS.
Parameter

Setting

Experiments

1

Total Trials

1000

Population Size

50

Structure Length

12n + 16

Crossover Rate

0.6

Mutation Rate

0.001

Generation Gap

1.0

Scaling Window

5

Report Interval

100

Structures Saved

1

Max Gens w/o Eval

2

Dump Interval

0

Dumps Saved

0

Options

acefgl

Random Seed

123456789

Rank Min

0.75

age cost is measured as described in Section 2.2. After 1,000 trials, the most fit (lowest cost)
individual is then used as a bias for EG2 with the whole training set as input. The resulting
decision tree is the output of ICET for the given training dataset.9
The n costs (bias parameters), C i , used in ICF, are not directly related to the true costs of
the attributes. The 50 individuals in the first generation are generated randomly, so the initial
values of C i have no relation to the true costs. After 20 generations, the values of C i may
have some relation to the true costs, but it will not be a simple relationship. These values of
C i are more appropriately thought of as biases than costs. Thus GENESIS is searching
through a bias space for biases for C4.5 that result in decision trees with low average cost.
The biases C i range from 1 to 10,000. When a bias C i is greater than 9,000, the i-th
attribute is ignored. That is, the i-th attribute is not available for C4.5 to include in the decision tree, even if it might maximize ICF i . This threshold of 9,000 was arbitrarily chosen.
There was no attempt to optimize this value by experimentation.
We chose to use EG2 in ICET, rather than IDX or CS-ID3, because EG2 has the parameter ฯ , which gives GENESIS greater control over the bias of EG2. ICF i is partly based on
the data (via the information gain, โI i ) and it is partly based on the bias (via the โpseudo9. The 50/50 partition of sub-training and sub-testing sets could mean that ICET may not work well on small
datasets. The smallest dataset of the five we examine here is the Hepatitis dataset, which has 155 cases. The
training sets had 103 cases and the testing sets had 52 cases. The sub-training and sub-testing sets had 51 or 52
cases. We can see from Figure 3 that ICET performed slightly better than the other algorithms on this dataset
(the difference is not significant).

380

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

costโ, C i ). The exact mix of data and bias can be controlled by varying ฯ . Otherwise, there
is no reason to prefer EG2 to IDX or CS-ID3, which could easily be used instead of EG2.
The treatment of delayed tests and conditional test costs is not โhard-wiredโ into EG2. It
is built into the fitness function used by GENESIS, the average cost of classification (measured as described in Section 2). This makes it relatively simple to extend ICET to handle
other pragmatic constraints on the decision trees.
In effect, GENESIS โliesโ to EG2 about the costs of the tests. How can lies improve the
performance of EG2? EG2 is a hill-climbing algorithm that can get trapped at a local optimum. It is a greedy algorithm that looks only one test ahead as it builds a decision tree.
Because it looks only one step ahead, EG2 suffers from the horizon effect. This term is taken
from the literature on chess playing programs. Suppose that a chess playing program has a
fixed three-move lookahead depth and it finds that it will loose its queen in three moves, if it
follows a certain branch of the game tree. There may be an alternate branch where the program first sacrifices a pawn and then loses its queen in four moves. Because the loss of the
queen is over its three-move horizon, the program may foolishly decide to sacrifice its pawn.
One move later, it is again faced with the loss of its queen. Analogously, EG2 may try to
avoid a certain expensive test by selecting a less expensive test. One test later, it is again
faced with the more expensive test. After it has exhausted all the cheaper tests, it may be
forced to do the expensive test, in spite of its efforts to avoid the test. GENESIS can prevent
this short-sighted behavior by telling lies to EG2. GENESIS can exaggerate the cost of the
cheap tests or it can understate the cost of the expensive test. Based on past trials, GENESIS
can find the lies that yield the best performance from EG2.
In ICET, learning (local search in EG2) and evolution (in GENESIS) interact. A common
form of hybrid genetic algorithm uses local search to improve the individuals in a population
(Schaffer et al., 1992). The improvements are then coded into the strings that represent the
individuals. This is a form of Lamarckian evolution. In ICET, the improvements due to EG2
are not coded into the strings. However, the improvements can accelerate evolution by altering the fitness landscape. This phenomenon (and other phenomena that result from this form
of hybrid) is known as the Baldwin effect (Baldwin, 1896; Morgan, 1896; Waddington, 1942;
Maynard Smith, 1987; Hinton & Nowlan, 1987; Ackley & Littman, 1991; Whitley & Gruau,
1993; Whitley et al., 1994; Anderson, in press). The Baldwin effect may explain much of the
success of ICET.

4. Experiments
This section describes experiments that were performed on five datasets, taken from the Irvine collection (Murphy & Aha, 1994). The five datasets are described in detail in
Appendix A. All five datasets involve medical problems. The test costs are based on information from the Ontario Ministry of Health (1992). The main purpose of the experiments is
to gain insight into the behavior of ICET. The other cost-sensitive algorithms, EG2, CS-ID3,
and IDX, are included mainly as benchmarks for evaluating ICET. C4.5 is also included as a
benchmark, to illustrate the behavior of an algorithm that makes no use of cost information.
The main conclusion of these experiments is that ICET performs significantly better than its
competitors, under a wide range of conditions. With access to the Irvine collection and the
information in Appendix A, it should be possible for other researchers to duplicate the
results reported here.
Medical datasets frequently have missing values.10 We conjecture that many missing values in medical datasets are missing because the doctor involved in generating the dataset
381

T URNEY

decided that a particular test was not economically justified for a particular patient. Thus
there may be information content in the fact that a certain value is missing. There may be
many reasons for missing values other than the cost of the tests. For example, perhaps the
doctor forgot to order the test or perhaps the patient failed to show up for the test. However,
it seems likely that there is often information content in the fact that a value is missing. For
our experiments, this information content should be hidden from the learning algorithms,
since using it (at least in the testing sets) would be a form of cheating. Two of the five
datasets we selected had some missing data. To avoid accusations of cheating, we decided to
preprocess the datasets so that the data presented to the algorithms had no missing values.
This preprocessing is described in Appendices A.2 and A.3.
Note that ICET is capable of handling missing values without preprocessing โ it inherits this ability from its C4.5 component. We preprocessed the data only to avoid accusations
of cheating, not because ICET requires preprocessed data.
For the experiments, each dataset was randomly split into 10 pairs of training and testing
sets. Each training set consisted of two thirds of the dataset and each testing set consisted of
the remaining one third. The same 10 pairs were used in all experiments, in order to facilitate
comparison of results across experiments.
There are three groups of experiments. The first group of experiments examines the baseline performance of the algorithms. The second group considers how robust ICET is under a
variety of conditions. The final group looks at how ICET searches bias space.
4.1

Baseline Performance

This section examines the baseline performance of the algorithms. In Section 4.1.1, we look
at the average cost of classification of the five algorithms on the five datasets. Averaged
across the five datasets, ICET has the lowest average cost. In Section 4.1.2, we study test
expenditures and error rates as functions of the penalty for misclassification errors. Of the
five algorithms studied here, only ICET adjusts its test expenditures and error rates as functions of the penalty for misclassification errors. The other four algorithms ignore the penalty
for misclassification errors. ICET behaves as one would expect, increasing test expenditures
and decreasing error rates as the penalty for misclassification errors rises. In Section 4.1.3,
we examine the execution time of the algorithms. ICET requires 23 minutes on average on a
single-processor Sparc 10. Since ICET is inherently parallel, there is significant room for
speed increase on a parallel machine.
4.1.1

AVERAGE COST OF CLASSIFICATION

The experiment presented here establishes the baseline performance of the five algorithms.
The hypothesis was that ICET will, on average, perform better than the other four algorithms. The classification cost matrix was set to a positive constant value k when the guess
class i does not equal the actual class j, but it was set to $0.00 when i equals j. We experimented with seven settings for k, $10, $50, $100, $500, $1000, $5000, and $10000.
Initially, we used the average cost of classification as the performance measure, but we
found that there are three problems with using the average cost of classification to compare
the five algorithms. First, the differences in costs among the algorithms become relatively

10. A survey of 54 datasets from the Irvine collection (URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/
SUMMARY-TABLE) indicates that 85% of the medical datasets (17 out of 20) have missing values, while only
24% (8 out of 34) of the non-medical datasets have missing values.

382

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

small as the penalty for classification errors increases. This makes it difficult to see which
algorithm is best. Second, it is difficult to combine the results for the five datasets in a fair
manner.11 It is not fair to average the five datasets together, since their test costs have different scales (see Appendix A). The test costs in the Heart Disease dataset, for example, are
substantially larger than the test costs in the other four datasets. Third, it is difficult to combine average costs for different values of k in a fair manner, since more weight will be given
to the situations where k is large than to the situations where it is small.
To address these concerns, we decided to normalize the average cost of classification. We
normalized the average cost by dividing it by the standard cost. Let f i โ [0,1] be the frequency of class i in the given dataset. That is, f i is the fraction of the cases in the dataset that
belong in class i. We calculate f i using the entire dataset, not just the training set. Let C i, j be
the cost of guessing that a case belongs in class i, when it actually belongs in class j. Let T
be the total cost of doing all of the possible tests. The standard cost is defined as follows:
T + min (1 โ f i) โ max C i, j

(6)

i, j

i

We can decompose formula (6) into three components:
T

(7)

min (1 โ f i)

(8)

max C i, j

(9)

i

i, j

We may think of (7) as an upper bound on test expenditures, (8) as an upper bound on error
rate, and (9) as an upper bound on the penalty for errors. The standard cost is always less
than the maximum possible cost, which is given by the following formula:
T + max C i, j
i, j

(10)

The point is that (8) is not really an upper bound on error rate, since it is possible to be
wrong with every guess. However, our experiments suggest that the standard cost is better
for normalization, since it is a more realistic (tighter) upper bound on the average cost. In
our experiments, the average cost never went above the standard cost, although it occasionally came very close.
Figure 3 shows the result of using formula (6) to normalize the average cost of classification. In the plots, the x axis is the value of k and the y axis is the average cost of classification
as a percentage of the standard cost of classification. We see that, on average (the sixth plot
in Figure 3), ICET has the lowest classification cost. The one dataset where ICET does not
perform particularly well is the Heart Disease dataset (we discuss this later, in Sections 4.3.2
and 4.3.3).
To come up with a single number that characterizes the performance of each algorithm,
we averaged the numbers in the sixth plot in Figure 3.12 We calculated 95% confidence
regions for the averages, using the standard deviations across the 10 random splits of the
11. We want to combine the results in order to summarize the performance of the algorithms on the five datasets.
This is analogous to comparing students by calculating the GPA (Grade Point Average), where students are to
courses as algorithms are to datasets.
12. Like the GPA, all datasets (courses) have the same weight. However, unlike the GPA, all algorithms (students) are applied to the same datasets (have taken the same courses). Thus our approach is perhaps more fair
to the algorithms than GPA is to students.

383

T URNEY

BUPA Liver Disease

Heart Disease
100
Average % Standard Cost

Average % Standard Cost

100
80
60
40
20
0
10

80
60
40
20
0

100

1000

10000

10

Cost of Misclassification Error

Hepatitis Prognosis

Pima Indians Diabetes
Average % Standard Cost

Average % Standard Cost

60
40
20
0

80
60
40
20
0

100

1000

10000

10

Cost of Misclassification Error

100

1000

10000

Cost of Misclassification Error

Thyroid Disease

Average of Five Datasets
100
Average % Standard Cost

100
Average % Standard Cost

10000

100

80

80
60
40
20
0
10

1000

Cost of Misclassification Error

100

10

100

80
60
40
20
0

100

1000

10000

10

Cost of Misclassification Error

100

1000

10000

Cost of Misclassification Error

ICET:
EG2:
CS-ID3:
IDX:
C4.5:

Figure 3: Average cost of classification as a percentage of the standard cost of
classification for the baseline experiment.
datasets. The result is shown in Table 5.
Table 5 shows the averages for the first three misclassification error costs alone ($10,
$50, and $100), in addition to showing the averages for all seven misclassification error costs
($10 to $10000). We have two averages (the two columns in Table 5), based on two groups of
data, to address the following argument: As the penalty for misclassification errors increases,
the cost of the tests becomes relatively insignificant. With very high misclassification error
cost, the test cost is effectively zero, so the task becomes simply to maximize accuracy. As
384

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Table 5: Average percentage of standard cost for the baseline experiment.
Algorithm

Average Classification Cost as Percentage of Standard
ยฑ 95% Confidence
Misclassification Error Costs
from 10.00 to 10,000.00

Misclassification Error Costs
from 10.00 to 100.00

ICET

49 ยฑ 7

29 ยฑ 7

EG2

58 ยฑ 5

43 ยฑ 3

CS-ID3

61 ยฑ 6

49 ยฑ 4

IDX

58 ยฑ 5

43 ยฑ 3

C4.5

77 ยฑ 5

82 ยฑ 4

we see in Figure 3, the gap between C4.5 (which maximizes accuracy) and the other algorithms becomes smaller as the cost of misclassification error increases. Therefore the benefit
of sensitivity to test cost decreases as the cost of misclassification error increases. It could be
argued that one would only bother with an algorithm that is sensitive to test cost when tests
are relatively expensive, compared to the cost of misclassification errors. Thus the most realistic measure of performance is to examine the average cost of classification when the cost of
tests is the same order of magnitude as the cost of misclassification errors ($10 to $100).
This is why Table 5 shows both averages.
Our conclusion, based on Table 5, is that ICET performs significantly better than the
other four algorithms when the cost of tests is the same order of magnitude as the cost of
misclassification errors ($10, $50, and $100). When the cost of misclassification errors dominates the test costs, ICET still performs better than the competition, but the difference is less
significant. The other three cost-sensitive algorithms (EG2, CS-ID3, and IDX) perform significantly better than C4.5 (which ignores cost). The performance of EG2 and IDX is indistinguishable, but CS-ID3 appears to be consistently more costly than EG2 and IDX.
4.1.2

TEST EXPENDITURES AND ERROR RATES AS FUNCTIONS OF THE PENALTY FOR ERRORS

We argued in Section 2 that expenditures on tests should be conditional on the penalty for
misclassification errors. Therefore ICET is designed to be sensitive to both the cost of tests
and the cost of classification errors. This leads us to the hypothesis that ICET tends to spend
more on tests as the penalty for misclassification errors increases. We also expect that the
error rate of ICET should decrease as test expenditures increase. These two hypotheses are
confirmed in Figure 4. In the plots, the x axis is the value of k and the y axis is (1) the average expenditure on tests, expressed as a percentage of the maximum possible expenditure on
tests, T , and (2) the average percent error rate. On average (the sixth plot in Figure 4), test
expenditures rise and error rate falls as the penalty for classification errors increases. There
are some minor deviations from this trend, since ICET can only guess at the value of a test
(in terms of reduced error rate), based on what it sees in the training dataset. The testing
dataset may not always support that guess. Note that plots for the other four algorithms, corresponding to the plots for ICET in Figure 4, would be straight horizontal lines, since all four
algorithms ignore the cost of misclassification error. They generate the same decision trees
for every possible misclassification error cost.

385

T URNEY

80

60

60
40
40
20

20
0

0
1000

10000

30
40
20
20

10

Average % Maximum Test Expenditures

30
20
20
10
0

Average % Error Rate

Average % Maximum Test Expenditures

40

0
1000

10000

80

60

40
40
20
20

0
10

0
100

1000

10000

Cost of Misclassification Error

Average of Five Datasets

80

Average % Maximum Test Expenditures

Thyroid Disease
10
8

60

6
40
4
20

2

0

Average % Error Rate

Average % Maximum Test Expenditures

10000

60

Cost of Misclassification Error

0
100

1000

Pima Indians Diabetes
40

10

0
100

Cost of Misclassification Error

Hepatitis Prognosis
50

100

10

0

Cost of Misclassification Error

10

40

60

Average % Error Rate

100

50

1000

10000

Cost of Misclassification Error

80

50
40

60

30
40
20
20

10

0
10

Average % Error Rate

10

80

Average % Error Rate

Average % Maximum Test Expenditures

Heart Disease
80

Average % Error Rate

Average % Maximum Test Expenditures

BUPA Liver Disease
100

0
100

1000

10000

Cost of Misclassification Error

% Test Expenditures:
% Error Rate:

Figure 4: Average test expenditures and average error rate
as a function of misclassification error cost.
4.1.3

EXECUTION TIME

In essence, ICET works by invoking C4.5 1000 times (Section 3.5). Fortunately, Quinlanโs
(1992) implementation of C4.5 is quite fast. Table 6 shows the run-times for the algorithms,
using a single-processor Sun Sparc 10. One full experiment takes about one week (roughly
23 minutes for an average run, multiplied by 5 datasets, multiplied by 10 random splits, multiplied by 7 misclassification error costs equals about one week). Since genetic algorithms
can easily be executed in parallel, there is substantial room for speed increase with a parallel
machine. Each generation consists of 50 individuals, which could be evaluated in parallel,
reducing the average run-time to about half a minute.
4.2

Robustness of ICET

This group of experiments considers how robust ICET is under a variety of conditions. Each
section considers a different variation on the operating environment of ICET. The ICET
386

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Table 6: Elapsed run-time for the five algorithms.
Algorithm

Average Elapsed Run-Time for Each Dataset โ Minutes:Seconds
BUPA

Heart

Hepatitis

Pima

Thyroid

Average

ICET

15:43

13:14

10:29

28:19

45:25

22:38

EG2

0:1

0:1

0:1

0:3

0:3

0:2

CS-ID3

0:1

0:1

0:1

0:3

0:3

0:2

IDX

0:1

0:1

0:1

0:3

0:3

0:2

C4.5

0:2

0:1

0:1

0:4

0:3

0:2

algorithm itself is not modified. In Section 4.2.1, we alter the environment by labelling all
tests as immediate. In Section 4.2.2, we do not recognize shared costs, so there is no discount
for a group of tests with a common cost. In Section 4.2.3, we experiment with complex classification cost matrices, where different types of errors have different costs. In Section 4.2.4,
we examine what happens when ICET is trained with a certain penalty for misclassification
errors, then tested with a different penalty. In all four experiments, we find that ICET continues to perform well.
4.2.1

ALL TESTS IMMEDIATE

A critic might object that the previous experiments do not show that ICET is superior to the
other algorithms due to its sensitivity to both test costs and classification error costs. Perhaps
ICET is superior simply because it can handle delayed tests, while the other algorithms treat
all tests as immediate.13 That is, the method of estimating the average classification cost
(Section 2.2) is biased in favor of ICET (since ICET uses the method in its fitness function)
and against the other algorithms. In this experiment, we labelled all tests as immediate. Otherwise, nothing changed from the baseline experiments. Table 7 summarizes the results of
the experiment. ICET still performs well, although its advantage over the other algorithms
has decreased slightly. Sensitivity to delayed tests is part of the explanation of ICETโs performance, but it is not the whole story.
4.2.2

NO GROUP DISCOUNTS

Another hypothesis is that ICET is superior simply because it can handle groups of tests that
share a common cost. In this experiment, we eliminated group discounts for tests that share a
common cost. That is, test costs were not conditional on prior tests. Otherwise, nothing
changed from the baseline experiments. Table 8 summarizes the results of the experiment.
ICET maintains its advantage over the other algorithms.
4.2.3

COMPLEX CLASSIFICATION COST MATRICES

So far, we have only used simple classification cost matrices, where the penalty for a classification error is the same for all types of error. This assumption is not inherent in ICET. Each
13. While the other algorithms cannot currently handle delayed tests, it should be possible to alter them in some
way, so that they can handle delayed tests. This comment also extends to groups of tests that share a common
cost. ICET might be viewed as an alteration of EG2 that enables EG2 to handle delayed tests and common
costs.

387

T URNEY

Table 7: Average percentage of standard cost for the no-delay experiment.
Algorithm

Average Classification Cost as Percentage of Standard
ยฑ 95% Confidence
Misclassification Error Costs
from 10.00 to 10,000.00

Misclassification Error Costs
from 10.00 to 100.00

ICET

47 ยฑ 6

28 ยฑ 4

EG2

54 ยฑ 4

36 ยฑ 2

CS-ID3

54 ยฑ 5

39 ยฑ 3

IDX

54 ยฑ 4

36 ยฑ 2

C4.5

64 ยฑ 6

59 ยฑ 4

Table 8: Average percentage of standard cost for the no-discount experiment.
Algorithm

Average Classification Cost as Percentage of Standard
ยฑ 95% Confidence
Misclassification Error Costs
from 10.00 to 10,000.00

Misclassification Error Costs
from 10.00 to 100.00

ICET

46 ยฑ 6

25 ยฑ 5

EG2

56 ยฑ 5

42 ยฑ 3

CS-ID3

59 ยฑ 5

48 ยฑ 4

IDX

56 ยฑ 5

42 ยฑ 3

C4.5

75 ยฑ 5

80 ยฑ 4

element in the classification cost matrix can have a different value. In this experiment, we
explore ICETโs behavior when the classification cost matrix is complex.
We use the term โpositive errorโ to refer to a false positive diagnosis, which occurs when
a patient is diagnosed as being sick, but the patient is actually healthy. Conversely, the term
โnegative errorโ refers to a false negative diagnosis, which occurs when a patient is diagnosed as being healthy, but is actually sick. The term โpositive error costโ is the cost that is
assigned to positive errors, while โnegative error costโ is the cost that is assigned to negative
errors. See Appendix A for examples. We were interested in ICETโs behavior as the ratio of
negative to positive error cost was varied. Table 9 shows the ratios that we examined.
Figure 5 shows the performance of the five algorithms at each ratio.
Our hypothesis was that the difference in performance between ICET and the other algorithms would increase as we move away from the middles of the plots, where the ratio is 1.0,
since the other algorithms have no mechanism to deal with complex classification cost; they
were designed under the implicit assumption of simple classification cost matrices. In fact,
Figure 5 shows that the difference tends to decrease as we move away from the middles.
This is most pronounced on the right-hand sides of the plots. When the ratio is 8.0 (the
extreme right-hand sides of the plots), there is no advantage to using ICET. When the ratio is
0.125 (the extreme left-hand sides of the plots), there is still some advantage to using ICET.
388

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

BUPA Liver Disease

Heart Disease
100
Average % Standard Cost

Average % Standard Cost

100
80
60
40
20
0
0.1

80
60
40
20
0

1.0

10.0

0.1

Ratio of Negative to Positive Error Cost

Hepatitis Prognosis
Average % Standard Cost

Average % Standard Cost

100

80
60
40
20
0

80
60
40
20
0

1.0

10.0

0.1

Ratio of Negative to Positive Error Cost

Thyroid Disease

10.0

Average of Five Datasets
100
Average % Standard Cost

Average % Standard Cost

1.0
Ratio of Negative to Positive Error Cost

100
80
60
40
20
0
0.1

10.0

Pima Indians Diabetes

100

0.1

1.0
Ratio of Negative to Positive Error Cost

80
60
40
20
0

1.0

10.0

0.1

Ratio of Negative to Positive Error Cost

1.0

10.0

Ratio of Negative to Positive Error Cost

ICET:
EG2:
CS-ID3:
IDX:
C4.5:

Figure 5: Average cost of classification as a percentage of the standard cost of
classification, with complex classification cost matrices.
The interpretation of these plots is complicated by the fact that the gap between the algorithms tends to decrease as the penalty for classification errors increases (as we can see in
Figure 3 โ in retrospect, we should have held the sum of the negative error cost and the positive error cost at a constant value, as we varied their ratio). However, there is clearly an
asymmetry in the plots, which we expected to be symmetrical about a vertical line centered
on 1.0 on the x axis. The plots are close to symmetrical for the other algorithms, but they are
asymmetrical for ICET. This is also apparent in Table 10, which focuses on a comparison of
the performance of ICET and EG2, averaged across all five datasets (see the sixth plot in
Figure 5). This suggests that it is more difficult to reduce negative errors (on the right-hand
sides of the plots, negative errors have more weight) than it is to reduce positive errors (on
389

T URNEY

Table 9: Actual error costs for each ratio of negative to positive error cost.
Ratio of Negative to
Positive Error Cost

Negative
Error Cost

Positive
Error Cost

0.125

50

400

0.25

50

200

0.5

50

100

1.0

50

50

2.0

100

50

4.0

200

50

8.0

400

50

Table 10: Comparison of ICET and EG2
with various ratios of negative to positive error cost.

Algorithm

Average Classification Cost as Percentage of Standard
ยฑ 95% Confidence, as the Ratio of
Negative to Positive Error Cost is Varied
0.125

0.25

0.5

1.0

2.0

4.0

8.0

ICET

25 ยฑ 10

25 ยฑ 8

29 ยฑ 6

29 ยฑ 4

34 ยฑ 6

39 ยฑ 6

39 ยฑ 6

EG2

39 ยฑ 5

40 ยฑ 4

41 ยฑ 4

44 ยฑ 3

42 ยฑ 3

41 ยฑ 4

40 ยฑ 5

ICET/EG2 (as %)

64

63

71

66

81

95

98

the left-hand sides, positive errors have more weight). That is, it is easier to avoid false positive diagnoses (a patient is diagnosed as being sick, but the patient is actually healthy) than
it is to avoid false negative diagnoses (a patient is diagnosed as being healthy, but is actually
sick). This is unfortunate, since false negative diagnoses usually carry a heavier penalty, in
real-life. Preliminary investigation suggests that false negative diagnoses are harder to avoid
because the โsickโ class is usually less frequent than the โhealthyโ class, which makes the
โsickโ class harder to learn.
4.2.4

POORLY ESTIMATED CLASSIFICATION COST

We believe that it is an advantage of ICET that it is sensitive to both test costs and classification error costs. However, it might be argued that it is difficult to calculate the cost of classification errors in many real-world applications. Thus it is possible that an algorithm that
ignores the cost of classification errors (e.g., EG2, CS-ID3, IDX) may be more robust and
useful than an algorithm that is sensitive to classification errors (e.g., ICET). To address this
possibility, we examine what happens when ICET is trained with a certain penalty for classification errors, then tested with a different penalty.
Our hypothesis was that ICET would be robust to reasonable differences between the
penalty during training and the penalty during testing. Table 11 shows what happens when
ICET is trained with a penalty of $100 for classification errors, then tested with penalties of
390

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Table 11: Performance when training set classification error cost is $100.

Algorithm

Average Classification Cost as Percentage of
Standard ยฑ 95% Confidence, for Testing Set
Classification Error Cost of:
$50

$100

$500

ICET

33 ยฑ 10

41 ยฑ 10

62 ยฑ 9

EG2

44 ยฑ 3

49 ยฑ 4

63 ยฑ 6

CS-ID3

49 ยฑ 5

54 ยฑ 6

65 ยฑ 7

IDX

43 ยฑ 3

49 ยฑ 4

63 ยฑ 6

C4.5

82 ยฑ 5

82 ยฑ 5

78 ยฑ 7

$50, $100, and $500. We see that ICET has the best performance of the five algorithms,
although its edge is quite slight in the case where the penalty is $500 during testing.
We also examined what happens (1) when ICET is trained with a penalty of $500 and
tested with penalties of $100, $500, and $1,000 and (2) when ICET is trained with a penalty
of $1,000 and tested with penalties of $500, $1,000, and $5,000. The results show essentially
the same pattern as in Table 11: ICET is relatively robust to differences between the training
and testing penalties, at least when the penalties have the same order of magnitude. This suggests that ICET is applicable even in those situations where the reliability of the estimate of
the cost of classification errors is dubious.
When the penalty for errors on the testing set is $100, ICET works best when the penalty
for errors on the training set is also $100. When the penalty for errors on the testing set is
$500, ICET works best when the penalty for errors on the training set is also $500. When the
penalty for errors on the testing set is $1,000, ICET works best when the penalty for errors
on the training set is $500. This suggests that there might be an advantage in some situations
to underestimating the penalty for errors during training. In other, words ICET may have a
tendency to overestimate the benefits of tests (this is likely due to overfitting the training
data).
4.3

Searching Bias Space

The final group of experiments analyzes ICETโs method for searching in bias space. Section
4.3.1 studies the roles of the mutation and crossover operators. It appears that crossover is
mildly beneficial, compared to pure mutation. Section 4.3.2 considers what happens when
ICET is constrained to search in a binary bias space, instead of a real bias space. This constraint actually improves the performance of ICET. We hypothesized that the improvement
was due to a hidden advantage of searching in binary bias space: When searching in binary
bias space, ICET has direct access to the true costs of the tests. However, this advantage can
be available when searching in real bias space, if the initial population of biases is seeded
with the true costs of the tests. Section 4.3.3 shows that this seeding improves the performance of ICET.
4.3.1

CROSSOVER VERSUS MUTATION

Past work has shown that a genetic algorithm with crossover performs better than a genetic
algorithm with mutation alone (Grefenstette et al., 1990; Wilson, 1987). This section
391

T URNEY

attempts to test the hypothesis that crossover improves the performance of ICET. To test this
hypothesis, it is not sufficient to merely set the crossover rate to zero. Since crossover has a
randomizing effect, similar to mutation, we must also increase the mutation rate, to compensate for the loss of crossover (Wilson, 1987; Spears, 1992).
It is very difficult to analytically calculate the increase in mutation rate that is required to
compensate for the loss of crossover (Spears, 1992). Therefore we experimentally tested
three different mutation settings.14 The results are summarized in Table 12. When the crossover rate was set to zero, the best mutation rate was 0.10. For misclassification error costs
from $10 to $10,000, the performance of ICET without crossover was not as good as the performance of ICET with crossover, but the difference is not statistically significant. However,
this comparison is not entirely fair to crossover, since we made no attempt to optimize the
crossover rate (we simply used the default value). The results suggest that crossover is
mildly beneficial, but do not prove that pure mutation is inferior.
Table 12: Average percentage of standard cost for mutation experiment.
Average Classification Cost as Percentage of
Standard ยฑ 95% Confidence

ICET
Crossover
Rate

Mutation
Rate

Misclassification
Error Costs
from 10.00 to 10,000.00

Misclassification
Error Costs
from 10.00 to 100.00

0.6

0.001

49 ยฑ 7

29 ยฑ 7

0.0

0.05

51 ยฑ 8

32 ยฑ 9

0.0

0.10

50 ยฑ 8

29 ยฑ 8

0.0

0.15

51 ยฑ 8

30 ยฑ 9

4.3.2

SEARCH IN BINARY SPACE

ICET searches for biases in a space of n + 2 real numbers. Inspired by Aha and Bankert
(1994), we decided to see what would happen when ICET was restricted to a space of n
binary numbers and 2 real numbers. We modified ICET so that EG2 was given the true cost
of each test, instead of a โpseudo-costโ or bias. For conditional test costs, we used the nodiscount cost (see Section 4.2.2). The n binary digits were used to exclude or include a test.
EG2 was not allowed to use excluded tests in the decision trees that it generated.
To be more precise, let B 1, โฆ, B n be n binary numbers and let C 1, โฆ, C n be n real numbers. For this experiment, we set C i to the true cost of the i-th test. In this experiment, GENESIS does not change C i . That is, C i is constant for a given test in a given dataset. Instead,
GENESIS manipulates the value of B i for each i. The binary number B i is used to determine
whether EG2 is allowed to use a test in its decision tree. If B i = 0 , then EG2 is not allowed
to use the i-th test (the i-th attribute). Otherwise, if B i = 1 , EG2 is allowed to use the i-th
test. EG2 uses the ICF equation as usual, with the true costs C i . Thus this modified version
of ICET is searching through a binary bias space instead of a real bias space.
Our hypothesis was that ICET would perform better when searching in real bias space
14. Each of these three experiments took one week on a Sparc 10, which is why we only tried three settings for
the mutation rate.

392

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

than when searching in binary bias space. Table 13 shows that this hypothesis was not confirmed. It appears to be better to search in binary bias space, rather than real bias space.
However, the differences are not statistically significant.
Table 13: Average percentage of standard cost for the binary search experiment.
Algorithm

Average Classification Cost as Percentage of
Standard ยฑ 95% Confidence
Misclassification
Error Costs
from 10.00 to 10,000.00

Misclassification
Error Costs
from 10.00 to 100.00

ICET โ Binary Space

48 ยฑ 6

26 ยฑ 5

ICET โ Real Space

49 ยฑ 7

29 ยฑ 7

EG2

58 ยฑ 5

43 ยฑ 3

CS-ID3

61 ยฑ 6

49 ยฑ 4

IDX

58 ยฑ 5

43 ยฑ 3

C4.5

77 ยฑ 5

82 ยฑ 4

When we searched in binary space, we set C i to the true cost of the i-th test. GENESIS
manipulated B i instead of C i . When we searched in real space, GENESIS set C i to whatever
value it found useful in its attempt to optimize fitness. We hypothesized that this gives an
advantage to binary space search over real space search. Binary space search has direct
access to the true costs of the tests, but real space search only learns about the true costs of
the tests indirectly, by the feedback it gets from the fitness function.
When we examined the experiment in detail, we found that ICET did well on the Heart
Disease dataset when it was searching in binary bias space, although it did poorly when it
was searching in real bias space (see Section 4.1.1). We hypothesized that ICET, when
searching in real space, suffered most from the lack of direct access to the true costs when it
was applied to the Heart Disease dataset. These hypotheses were tested by the next experiment.
4.3.3

SEEDED POPULATION

In this experiment, we returned to searching in real bias space, but we seeded the initial population of biases with the true test costs. This gave ICET direct access to the true test costs.
For conditional test costs, we used the no-discount cost (see Section 4.2.2). In the baseline
experiment (Section 4.1), the initial population consists of 50 randomly generated strings,
representing n + 2 real numbers. In this experiment, the initial population consists of 49 randomly generated strings and one manually generated string. In the manually generated
string, the first n numbers are the true test costs. The last two numbers were set to 1.0 (for
ฯ ) and 25 (for CF). This string is exactly the bias of EG2, as implemented here (Section
3.2).
Our hypotheses were (1) that ICET would perform better (on average) when the initial
population is seeded than when it is purely random, (2) that ICET would perform better (on
average) searching in real space with a seeded population than when searching in binary
space,15 and (3) that ICET would perform better on the Heart Disease dataset when the ini393

T URNEY

tial population is seeded than when it is purely random. Table 14 appears to support the first
two hypotheses. Figure 6 appears to support the third hypothesis. However, the results are
not statistically significant.16
Table 14: Average percentage of standard cost for the seeded population
experiment.
Algorithm

Average Classification Cost as Percentage of
Standard ยฑ 95% Confidence
Misclassification
Error Costs
from 10.00 to 10,000.00

Misclassification
Error Costs
from 10.00 to 100.00

ICET โ Seeded
Search in Real Space

46 ยฑ 6

25 ยฑ 5

ICET โ Unseeded
Search in Real Space

49 ยฑ 7

29 ยฑ 7

ICET โ Unseeded
Search in Binary Space

48 ยฑ 6

26 ยฑ 5

EG2

58 ยฑ 5

43 ยฑ 3

CS-ID3

61 ยฑ 6

49 ยฑ 4

IDX

58 ยฑ 5

43 ยฑ 3

C4.5

77 ยฑ 5

82 ยฑ 4

This experiment raises some interesting questions: Should seeding the population be
built into the ICET algorithm? Should we seed the whole population with the true costs, perturbed by some random noise? Perhaps this is the right approach, but we prefer to modify
ICF i (equation (2)), the device by which GENESIS controls the decision tree induction. We
could alter this equation so that it contains both the true costs and some bias parameters.17
This seems to make more sense than our current approach, which deprives EG2 of direct
access to the true costs. We discuss some other ideas for modifying the equation in
Section 5.2.
Incidentally, this experiment lets us answer the following question: Does the genetic
search in bias space do anything useful? If we start with the true costs of the tests and reasonable values for the parameters ฯ and CF, how much improvement do we get from the
genetic search? In this experiment, we seeded the population with an individual that represents exactly the bias of EG2 (the first n numbers are the true test costs and the last two numbers are 1.0 for ฯ and 25 for CF). Therefore we can determine the value of genetic search by
comparing EG2 with ICET. ICET starts with the bias of EG2 (as a seed in the first genera15. Note that it does not make sense to seed the binary space search, since it already has direct access to the true
costs.
16. We would need to go from the current 10 trials (10 random splits of the data) to about 40 trials to make the
results significant. The experiments reported here took a total of 63 days of continuous computation on a Sun
Sparc 10, so 40 trials would require about six more months.
17. This idea was suggested in conversation by K. De Jong.

394

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Heart Disease
100

80

80

Average % Standard Cost

Average % Standard Cost

BUPA Liver Disease
100

60

40

20
0
10

60

40

20
0

100

1000

10000

10

Cost of Misclassification Error

80

80

60

40

20
0

40

20
0

100

1000

10000

10

100

1000

10000

Cost of Misclassification Error

Thyroid Disease

Average of Five Datasets

100

100

80

80

Average % Standard Cost

Average % Standard Cost

10000

60

Cost of Misclassification Error

60

40

20
0
10

1000

Pima Indians Diabetes
100
Average % Standard Cost

Average % Standard Cost

Hepatitis Prognosis
100

10

100

Cost of Misclassification Error

60

40

20
0

100

1000

10000

10

Cost of Misclassification Error

100

1000

10000

Cost of Misclassification Error

ICET:
EG2:
CS-ID3:
IDX:
C4.5:

Figure 6: Average cost of classification as a percentage of the standard cost of
classification for the seeded population experiment.
tion) and attempts to improve the bias. The score of EG2 in Table 14 shows the value of the
bias built into EG2. The score of ICET in Table 14 shows how genetic search in bias space
can improve the built-in bias of EG2. When the cost of misclassification errors has the same
order of magnitude as the test costs ($10 to $100), EG2 averages 43% of the standard cost,
while ICET averages 25% of the standard cost. When the cost of misclassification errors
ranges from $10 to $10,000, EG2 averages 58% of the standard cost, while ICET averages
46% of the standard cost. Both of these differences are significant with more than 95% confidence. This makes it clear that genetic search is adding value.
395

T URNEY

5. Discussion
This section compares ICET to related work and outlines some possibilities for future work.
5.1

Related Work

There are several other algorithms that are sensitive to test costs (Nรบรฑez, 1988, 1991; Tan &
Schlimmer, 1989, 1990; Tan, 1993; Norton, 1989). As we have discussed, the main limitation of these algorithms is that they do not consider the cost of classification errors. We cannot rationally determine whether a test should be performed until we know both the cost of
the test and the cost of classification errors.
There are also several algorithms that are sensitive to classification error costs (Breiman
et al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974; Gordon & Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, in press; Knoll et al., 1994). None of
these algorithms consider the cost of tests. Therefore they all focus on complex classification
cost matrices, since, when tests have no cost and the classification error matrix is simple, the
problem reduces to maximizing accuracy.
The FIS system (Pipitone et al., 1991) attempts to find a decision tree that minimizes the
average total cost of the tests required to achieve a certain level of accuracy. This approach
could be implemented in ICET by altering the fitness function. The main distinction between
FIS (Pipitone et al., 1991) and ICET is that FIS does not learn from data. The information
gain of a test is estimated using a qualitative causal model, instead of training cases. Qualitative causal models are elicited from domain experts, using a special knowledge acquisition
tool. When training data are available, ICET can be used to avoid the need for knowledge
acquisition. Otherwise, ICET is not applicable and the FIS approach is suitable.
Another feature of ICET is that it does not perform purely greedy search. Several other
authors have proposed non-greedy classification algorithms (Tcheng et al., 1989; Ragavan &
Rendell, 1993; Norton, 1989; Schaffer, 1993; Rymon, 1993; Seshu, 1989). In general, these
results show that there can be an advantage to more sophisticated search procedures. ICET is
different from these algorithms in that it uses a genetic algorithm and it is applied to minimizing both test costs and classification error costs.
ICET uses a two-tiered search strategy. At the bottom tier, EG2 performs a greedy search
through the space of classifiers. On the second tier, GENESIS performs a non-greedy search
through a space of biases. The idea of a two-tiered search strategy (where the first tier is
search in classifier space and the second tier is search in bias space) also appears in (Provost,
1994; Provost & Buchanan, in press; Aha & Bankert, 1994; Schaffer, 1993). Our work goes
beyond Aha and Bankert (1994) by considering search in a real bias space, rather than search
in a binary space. Our work fits in the general framework of Provost and Buchanan (in
press), but differs in many details. For example, their method of calculating cost is a special
case of ours (Section 2.3).
Other researchers have applied genetic algorithms to classification problems. For example, Frey and Slate (1991) applied a genetic algorithm (in particular, a learning classifier system (LCS)) to letter recognition. However, Fogarty (1992) obtained higher accuracy using a
simple nearest neighbor algorithm. More recent applications of genetic algorithms to classification have been more successful (De Jong et al., 1993). However, the work described here
is the first application of genetic algorithms to the problem of cost-sensitive classification.
We mentioned in Section 2.1 that decision theory may be used to define the optimal solution to the problem of cost-sensitive classification. However, searching for the optimal solution is computationally infeasible (Pearl, 1988). We attempted to take a decision theoretic
396

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

approach to this problem by implementing the AO* algorithm (Pearl, 1984) and designing a
heuristic evaluation function to speed up the AO* search (Lirov & Yue, 1991). We were
unable to make this approach execute fast enough to be practical.
We also attempted to apply genetic programming (Koza, 1993) to the problem of costsensitive classification. Again, we were unable to make this approach execute fast enough to
be practical, although it was faster than the AO* approach.
The cost-sensitive classification problem, as we have treated it here, is essentially a
problem in reinforcement learning (Sutton, 1992; Karakoulas, in preparation). The average
cost of classification, measured as described in Section 2.2, is a reward/punishment signal
that could be optimized using reinforcement learning techniques. This is something that
might be explored as an alternative approach.
5.2

Future Work

This paper discusses two types of costs, the cost of tests and the cost of misclassification
errors. These two costs have been treated together in decision theory, but ICET is the first
machine learning system that handles both costs together. The experiments in this paper have
compared ICET to other machine learning systems that can handle test costs (Nรบรฑez, 1988,
1991; Tan & Schlimmer, 1989, 1990; Tan, 1993; Norton, 1989), but we have not compared
ICET to other machine learning systems that can handle classification error costs (Breiman
et al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974; Gordon & Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, in press; Knoll et al., 1994). In future
work, we plan to address this omission. A proper treatment of this issue would make this
paper too long.
The absence of comparison with machine learning systems that can handle classification
error costs has no impact on most of the experiments reported here. The experiments in this
paper focussed on simple classification cost matrices (except for Section 4.2.3). When the
classification cost matrix is simple and the cost of tests is ignored, minimizing cost is exactly
equivalent to maximizing accuracy (see Section 2.3). Therefore, C4.5 (which is designed to
maximize accuracy) is a suitable surrogate for any of the systems that can handle classification error costs.
We also did not experiment with setting the test costs to zero. However, the behavior of
ICET when the penalty for misclassification errors is very high (the extreme right-hand sides
of the plots in Figure 3) is necessarily the same as its behavior when the cost of tests is very
low, since ICET is sensitive to the relative differences between test costs and error costs, not
the absolute costs. Therefore (given the behavior we can observe in the extreme right-hand
sides of the plots in Figure 3) we can expect that the performance of ICET will tend to converge with the performance of the other algorithms as the cost of tests approaches zero.
One natural addition to ICET would be the ability to output an โI donโt knowโ class. This
is easily handled by the GENESIS component, by extending the classification cost matrix so
that a cost is assigned to classifying a case as โunknownโ. We need to also make a small
modification to the EG2 component, so that it can generate decision trees with leaves
labelled โunknownโ. One way to do this would be to introduce a parameter that defines a
confidence threshold. Whenever the confidence in a certain leaf drops below the confidence
threshold, that leaf would be labelled โunknownโ. This confidence parameter would be made
accessible to the GENESIS component, so that it could be tuned to minimize average classification cost.
The mechanism in ICET for handling conditional test costs has some limitations. As it is
397

T URNEY

currently implemented, it does not handle the cost of attributes that are calculated from other
attributes. For example, in the Thyroid dataset (Appendix A.5), the FTI test is calculated
based on the results of the TT4 and T4U tests. If the FTI test is selected, we must pay for the
TT4 and T4U tests. If the TT4 and T4U tests have already been selected, the FTI test is free
(since the calculation is trivial). The ability to deal with calculated test results could be
added to ICET with relatively little effort.
ICET, as currently implemented, only handles two classes of test results: tests with
โimmediateโ results and tests with โdelayedโ results. Clearly there can be a continuous range
of delays, from seconds to years. We have chosen to treat delays as distinct from test costs,
but it could be argued that a delay is simply another type of test cost. For example, we could
say that a group of blood tests shares the common cost of a one-day wait for results. The cost
of one of the blood tests is conditional on whether we are prepared to commit ourselves to
doing one or more of the other tests in the group, before we see the results of the first test.
One difficultly with this approach to handling delays is the problem of assigning a cost to the
delay. How much does it cost to bring a patient in for two blood samples, instead of one? Do
we include the disruption to the patientโs life in our estimate of the cost? To avoid these
questions, we have not treated delays as another type of test cost, but our approach does not
readily handle a continuous range of delays.
The cost of a test can be a function of several things: (1) It can be a function of the prior
tests that have been selected. (2) It can be a function of the actual class of the case. (3) It can
be a function of other aspects of the case, where information about these other aspects may
be available through other tests. (4) It can be a function of the test result. This list seems
comprehensive, but there may be some possibilities we have overlooked. Let us consider
each of these four possibilities.
First, the cost of a test can be a function of the prior tests that have been selected. ICET
handles a special case of this, where a group of tests shares a common cost. As it is currently
implemented, ICET does not handle the general case. However, we could easily add this
capability to ICET by modifying the fitness function.
Second, the cost of a test can be a function of the actual class of the case. For example, a
test for heart disease might involve heavy exercise (Appendix A.2). If the patient actually
has heart disease, the exercise might trigger a heart attack. This risk should be included in
the cost of this particular test. Thus the cost of this test should vary, depending on whether
the patient actually has heart disease. We have not implemented this, although it could easily
be added to ICET by modifying the fitness function.
Third, the cost of a test can be a function of the results of other tests. For example, drawing blood from a newborn is more costly than drawing blood from an adult. To assign a cost
to a blood test, we need to know the age of the patient. The age of the patient can be represented as the result of another test โ the โpatient-ageโ test. This is slightly more complex
than the preceding cases, because we must now insure that the blood test is always accompanied with the patient-age test. We have not implemented this, although it could be added to
ICET.
Fourth, the cost of a test can be a function of the test result. For example, injecting a
radio-opaque die for an X-ray might cause an allergic reaction in the patient. This risk should
be added to the cost of the test. This makes the cost of the test a function of one of the possible outcomes of the test. In a situation like this, it may be wise to precede the injection of the
die with a screening test for allergies. This could be as simple as asking a question to the
patient. This question may have no relevance at all for determining the correct diagnosis of
398

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

the patient, but it may serve to reduce the average cost of classification. This case is similar
to the third case, above. Again, we have not implemented this, although it could be added to
ICET.
Attribute selection in EG2, CS-ID3, and IDX shares a common form. We may view
n
attribute selection as a function from โ to { 1, โฆ, n } , which takes as input n information
gain values โI 1, โฆ, โI n (one for each attribute) and generates as output the index of one of
the attributes. We may view C 1, โฆ, C n and ฯ as parameters in the attribute selection function. These parameters may be used to control the bias of the attribute selection procedure. In
this view, ICET uses GENESIS to tune the parameters of EG2โs attribute selection function.
In the future, we would like to investigate more general attribute selection functions. For
n
example, we might use a neural network to implement a function from โ to { 1, โฆ, n } .
GENESIS would then be used to tune the weights in the neural network.18 The attribute
selection function might also benefit from the addition of an input that specifies the depth of
the decision tree at the current node, where the information gain values are measured. This
would enable the bias for a test to vary, depending on how many tests have already been
selected.
Another area for future work is to explore the parameter settings that control GENESIS
(Table 4). There are many parameters that could be adjusted in GENESIS. We think it is significant that ICET works well with the default parameter settings in GENESIS, since it
shows that ICET is robust with respect to the parameters. However, it might be possible to
substantially improve the performance of ICET by tuning some of these parameters. A recent
trend in genetic algorithm research is to let the genetic algorithm adjust some of its own
parameters, such as mutation rate and crossover rate (Whitley et al., 1993). Another possibility is to stop breeding when the fitness levels stop improving, instead of stopping after a
fixed number of generations. Provost and Buchanan (in press) use a goodness measure as a
stopping condition for the bias space search.

6. Conclusions
The central problem investigated here is the problem of minimizing the cost of classification
when the tests are expensive. We argued that this requires assigning a cost to classification
errors. We also argued that a decision tree is the natural form of knowledge representation
for this type of problem. We then presented a general method for calculating the average cost
of classification for a decision tree, given a decision tree, information on the calculation of
test costs, a classification cost matrix, and a set of testing data. This method is applicable to
standard classification decision trees, without regard to how the decision tree is generated.
The method is sensitive to test costs, sensitive to classification error costs, capable of handling conditional test costs, and capable of handling delayed tests.
We introduced ICET, a hybrid genetic decision tree induction algorithm. ICET uses a
genetic algorithm to evolve a population of biases for a decision tree induction algorithm.
Each individual in the population represents one set of biases. The fitness of an individual is
determined by using it to generate a decision tree with a training dataset, then calculating the
average cost of classification for the decision tree with a testing dataset.
We analyzed the behavior of ICET in a series of experiments, using five real-world medical datasets. Three groups of experiments were performed. The first group looked at the
baseline performance of the five algorithms on the five datasets. ICET was found to have sig18. This idea was suggested in conversation by M. Brooks.

399

T URNEY

nificantly lower costs than the other algorithms. Although it executes more slowly, an average time of 23 minutes (for a typical dataset) is acceptable for many applications, and there
is the possibility of much greater speed on a parallel machine. The second group of experiments studied the robustness of ICET under a variety of modifications to its input. The
results show that ICET is robust. The third group of experiments examined ICETโs search in
bias space. We discovered that the search could be improved by seeding the initial population of biases.
In general, our research is concerned with pragmatic constraints on classification problems (Provost & Buchanan, in press). We believe that many real-world classification problems involve more than merely maximizing accuracy (Turney, in press). The results
presented here indicate that, in certain applications, a decision tree that merely maximizes
accuracy (e.g., trees generated by C4.5) may be far from the performance that is possible
with an algorithm that considers such realistic constraints as test costs, classification error
costs, conditional test costs, and delayed test results. These are just a few of the pragmatic
constraints that are faced in real-world classification problems.

Appendix A. Five Medical Datasets
This appendix presents the test costs for five medical datasets, taken from the Irvine collection (Murphy & Aha, 1994). The costs are based on information from the Ontario Ministry of
Health (1992). Although none of the medical data were gathered in Ontario, it is reasonable
to assume that other areas have similar relative test costs. For our purposes, the relative costs
are important, not the absolute costs.
A.1

BUPA Liver Disorders

The BUPA Liver Disorders dataset was created by BUPA Medical Research Ltd. and it was
donated to the Irvine collection by Richard Forsyth.19 Table 15 shows the test costs for the
BUPA Liver Disorders dataset. The tests in group A are blood tests that are thought to be
sensitive to liver disorders that might arise from excessive alcohol consumption. These tests
share the common cost of $2.10 for collecting blood. The target concept was defined using
the sixth column: Class 0 was defined as โdrinks < 3โ and class 1 was defined as โdrinks โฅ
3โ. Table 16 shows the general form of the classification cost matrix that was used in the
experiments in Section 4. For most of the experiments, the classification error cost equals the
positive error cost equals the negative error cost. The exception is in Section 4.2.3, for the
experiments with complex classification cost matrices. The terms โpositive error costโ and
โnegative error costโ are explained in Section 4.2.3. There are 345 cases in this dataset, with
no missing values. Column seven was originally used to split the data into training and testing sets. We did not use this column, since we required ten different random splits of the
data. In our ten random splits, the ten training sets all had 230 cases and the ten testing sets
all had 115 cases.
A.2

Heart Disease

The Heart Disease dataset was donated to the Irvine collection by David Aha.20 The princi19. The BUPA Liver Disorders dataset has the URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/liverdisorders/bupa.data.
20. The Heart Disease dataset has the URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/heart-disease/
cleve.mod.

400

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Table 15: Test costs for the BUPA Liver Disorders dataset.
Test

Description

Group

Cost

Delayed

1

mcv

mean corpuscular volume

A

$7.27 if first test in group A,
$5.17 otherwise

yes

2

alkphos

alkaline phosphotase

A

$7.27 if first test in group A,
$5.17 otherwise

yes

3

sgpt

alamine aminotransferase

A

$7.27 if first test in group A,
$5.17 otherwise

yes

4

sgot

aspartate aminotransferase

A

$7.27 if first test in group A,
$5.17 otherwise

yes

5

gammagt

gamma-glutamyl transpeptidase

A

$9.86 if first test in group A,
$7.76 otherwise

yes

6

drinks

number of half-pint equivalents of
alcoholic beverages drunk per day

diagnostic class: โdrinks < 3โ
or โdrinks โฅ 3โ

-

7

selector

field used to split data into two sets

not used

-

Table 16: Classification costs for the BUPA Liver Disorders dataset.
Actual Class

Guess Class

Cost

0 (drinks < 3)

0 (drinks < 3)

$0.00

0 (drinks < 3)

1 (drinks โฅ 3)

Positive Error Cost

1 (drinks โฅ 3)

0 (drinks < 3)

Negative Error Cost

1 (drinks โฅ 3)

1 (drinks โฅ 3)

$0.00

pal medical investigator was Robert Detrano, of the Cleveland Clinic Foundation. Table 17
shows the test costs for the Heart Disease dataset. A nominal cost of $1.00 was assigned to
the first four tests. The tests in group A are blood tests that are thought to be relevant for
heart disease. These tests share the common cost of $2.10 for collecting blood. The tests in
groups B and C involve measurements of the heart during exercise. A nominal cost of $1.00
was assigned for tests after the first test in each of these groups. The class variable has the
values โbuffโ (healthy) and โsickโ. There was a fifteenth column, which specified the class
variable as โHโ (healthy), โS1โ, โS2โ, โS3โ, or โS4โ (four different types of โsickโ), but we
deleted this column. Table 18 shows the classification cost matrix. There are 303 cases in
this dataset. We deleted all cases for which there were missing values. This reduced the
dataset to 296 cases. In our ten random splits, the training sets had 197 cases and the testing
sets had 99 cases.
A.3

Hepatitis Prognosis

The Hepatitis Prognosis dataset was donated by Gail Gong.21 Table 19 shows the test costs
for the Hepatitis dataset. Unlike the other four datasets, this dataset deals with prognosis, not
21. The Hepatitis Prognosis dataset has the URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/hepatitis/
hepatitis.data.

401

T URNEY

Table 17: Test costs for the Heart Disease dataset.
Test

Description

1

age

2

Group

Cost

Delayed

age in years

$1.00

no

sex

patientโs gender

$1.00

no

3

cp

chest pain type

$1.00

no

4

trestbps

resting blood pressure

$1.00

no

5

chol

serum cholesterol

A

$7.27 if first test in group A,
$5.17 otherwise

yes

6

fbs

fasting blood sugar

A

$5.20 if first test in group A,
$3.10 otherwise

yes

7

restecg

resting electrocardiograph

$15.50

yes

8

thalach

maximum heart rate
achieved

B

$102.90 if first test in group B,
$1.00 otherwise

yes

9

exang

exercise induced angina

C

$87.30 if first test in group C,
$1.00 otherwise

yes

10

oldpeak

ST depression induced by
exercise relative to rest

C

$87.30 if first test in group C,
$1.00 otherwise

yes

11

slope

slope of peak exercise ST
segment

C

$87.30 if first test in group C,
$1.00 otherwise

yes

12

ca

number of major vessels
coloured by fluoroscopy

$100.90

yes

13

thal

3 = normal; 6 = fixed defect;
7 = reversible defect

$102.90 if first test in group B,
$1.00 otherwise

yes

14

num

diagnosis of heart disease

diagnostic class

-

B

Table 18: Classification costs for the Heart Disease dataset.
Actual Class

Guess Class

Cost

buff

buff

$0.00

buff

sick

Positive Error Cost

sick

buff

Negative Error Cost

sick

sick

$0.00

diagnosis. With prognosis, the diagnosis is known, and the problem is to determine the likely
outcome of the disease. The tests that were assigned a nominal cost of $1.00 either involve
asking a question to the patient or performing a basic physical examination on the patient.
The tests in group A share the cost of $2.10 for collecting blood. Note that, although performing a histological examination of the liver costs $81.64, asking the patient whether a
histology was performed only costs $1.00. Thus the prognosis can exploit the information
conveyed by a decision (to perform a histological examination) that was made during the
diagnosis. The class variable has the values 1 (die) and 2 (live). Table 20 shows the classification costs. The dataset contains 155 cases, with many missing values. In our ten random
402

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Table 19: Test costs for the Hepatitis Prognosis dataset.
Test

Description

1

class

2

Group

Cost

Delayed

prognosis of hepatitis

prognostic class: live or die

-

age

age in years

$1.00

no

3

sex

gender

$1.00

no

4

steroid

patient on steroids

$1.00

no

5

antiviral

patient on antiviral

$1.00

no

6

fatigue

patient reports fatigue

$1.00

no

7

malaise

patient reports malaise

$1.00

no

8

anorexia

patient anorexic

$1.00

no

9

liver big

liver big on physical exam

$1.00

no

10

liver firm

liver firm on physical exam

$1.00

no

11

spleen palpable

spleen palpable on physical

$1.00

no

12

spiders

spider veins visible

$1.00

no

13

ascites

ascites visible

$1.00

no

14

varices

varices visible

$1.00

no

15

bilirubin

bilirubin โ blood test

A

$7.27 if first test in group A,
$5.17 otherwise

yes

16

alk phosphate

alkaline phosphotase

A

$7.27 if first test in group A,
$5.17 otherwise

yes

17

sgot

aspartate aminotransferase

A

$7.27 if first test in group A,
$5.17 otherwise

yes

18

albumin

albumin โ blood test

A

$7.27 if first test in group A,
$5.17 otherwise

yes

19

protime

protime โ blood test

A

$8.30 if first test in group A,
$6.20 otherwise

yes

20

histology

was histology performed?

$1.00

no

Table 20: Classification costs for the Hepatitis Prognosis dataset.
Actual Class

Guess Class

Cost

1 (die)

1 (die)

$0.00

1 (die)

2 (live)

Negative Error Cost

2 (live)

1 (die)

Positive Error Cost

2 (live)

2 (live)

$0.00

splits, the training sets had 103 cases and the testing sets had 52 cases. We filled in the missing values, using a simple single nearest neighbor algorithm (Aha et al., 1991). The missing
values were filled in using the whole dataset, before the dataset was split into training and
testing sets. For the nearest neighbor algorithm, the data were normalized so that the mini403

T URNEY

mum value of a feature was 0 and the maximum value was 1. The distance measure used was
the sum of the absolute values of the differences. The difference between two values was
defined to be 1 if one or both of the two values was missing.
A.4

Pima Indians Diabetes

The Pima Indians Diabetes dataset was donated by Vincent Sigillito. 22 The data were collected by the National Institute of Diabetes and Digestive and Kidney Diseases. Table 21
shows the test costs for the Pima Indians Diabetes dataset. The tests in group A share the
cost of $2.10 for collecting blood. The remaining tests were assigned a nominal cost of
$1.00. All of the patients were females at least 21 years old of Pima Indian heritage. The
class variable has the values 0 (healthy) and 1 (diabetes). Table 22 shows classification costs.
The dataset includes 768 cases, with no missing values. In our ten random splits, the training
sets had 512 cases and the testing sets had 256 cases.
Table 21: Test costs for the Pima Indians Diabetes dataset.
Test

Description

Group

Cost

Delayed

1

times pregnant

number of times pregnant

$1.00

no

2

glucose tol

glucose tolerance test

$17.61 if first test in group A,
$15.51 otherwise

yes

3

diastolic bp

diastolic blood pressure

$1.00

no

4

triceps

triceps skin fold thickness

$1.00

no

5

insulin

serum insulin test

$22.78 if first test in group A,
$20.68 otherwise

yes

6

mass index

body mass index

$1.00

no

7

pedigree

diabetes pedigree function

$1.00

no

8

age

age in years

$1.00

no

9

class

diagnostic class

diagnostic class

-

A

A

Table 22: Classification costs for the Pima Indians Diabetes dataset.

A.5

Actual Class

Guess Class

Cost

0 (healthy)

0 (healthy)

$0.00

0 (healthy)

1 (diabetes)

Positive Error Cost

1 (diabetes)

0 (healthy)

Negative Error Cost

1 (diabetes)

1 (diabetes)

$0.00

Thyroid Disease

The Thyroid Disease dataset was created by the Garavan Institute, Sydney, Australia. The
file was donated by Randolf Werner, obtained from Daimler-Benz. Daimler-Benz obtained
22. The Pima Indians Diabetes dataset has the URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/pimaindians-diabetes/pima-indians-diabetes.data.

404

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

the data from J.R. Quinlan.23 Table 23 shows the test costs for the Thyroid Disease dataset.
A nominal cost of $1.00 was assigned to the first 16 tests. The tests in group A share the cost
of $2.10 for collecting blood. The FTI test involves a calculation based on the results of the
TT4 and T4U tests. This complicates the calculation of the costs of these three tests, so we
chose not to use the FTI test in our experiments. The class variable has the values 1
(hypothyroid), 2 (hyperthyroid), and 3 (normal). Table 24 shows the classification costs.
There are 3772 cases in this dataset, with no missing values. In our ten random splits, the
training sets had 2515 cases and the testing sets had 1257 cases.
Table 23: Test costs for the Thyroid Disease dataset.
Test

Description

1

age

2

Group

Cost

Delayed

age in years

$1.00

no

sex

gender

$1.00

no

3

on thyroxine

patient on thyroxine

$1.00

no

4

query thyroxine

maybe on thyroxine

$1.00

no

5

on antithyroid

on antithyroid medication

$1.00

no

6

sick

patient reports malaise

$1.00

no

7

pregnant

patient pregnant

$1.00

no

8

thyroid surgery

history of thyroid surgery

$1.00

no

9

I131 treatment

patient on I131 treatment

$1.00

no

10

query hypothyroid

maybe hypothyroid

$1.00

no

11

query hyperthyroid

maybe hyperthyroid

$1.00

no

12

lithium

patient on lithium

$1.00

no

13

goitre

patient has goitre

$1.00

no

14

tumour

patient has tumour

$1.00

no

15

hypopituitary

patient hypopituitary

$1.00

no

16

psych

psychological symptoms

$1.00

no

17

TSH

TSH value, if measured

A

$22.78 if first test in group A, yes
$20.68 otherwise

18

T3

T3 value, if measured

A

$11.41 if first test in group A, yes
$9.31 otherwise

19

TT4

TT4 value, if measured

A

$14.51 if first test in group A, yes
$12.41 otherwise

20

T4U

T4U value, if measured

A

$11.41 if first test in group A, yes
$9.31 otherwise

21

FTI

FTI โ calculated from
TT4 and T4U

not used

-

22

class

diagnostic class

diagnostic class

-

23. The Thyroid Disease dataset has the URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/thyroid-disease/ann-train.data.

405

T URNEY

Table 24: Classification costs for the Thyroid Disease dataset.
Actual Class

Guess Class

Cost

1 (hypothyroid)

1 (hypothyroid)

$0.00

1 (hypothyroid)

2 (hyperthyroid)

Minimum(Negative Error Cost, Positive Error Cost)

1 (hypothyroid)

3 (normal)

Negative Error Cost

2 (hyperthyroid)

1 (hypothyroid)

Minimum(Negative Error Cost, Positive Error Cost)

2 (hyperthyroid)

2 (hyperthyroid)

$0.00

2 (hyperthyroid)

3 (normal)

Negative Error Cost

3 (normal)

1 (hypothyroid)

Positive Error Cost

3 (normal)

2 (hyperthyroid)

Positive Error Cost

3 (normal)

3 (normal)

$0.00

Acknowledgments
Thanks to Dr. Louise Linney for her help with interpretation of the Ontario Ministry of
Healthโs Schedule of Benefits. Thanks to Martin Brooks, Grigoris Karakoulas, Cullen Schaffer, Diana Gordon, Tim Niblett, Steven Minton, and three anonymous referees of JAIR for
their very helpful comments on earlier versions of this paper. This work was presented in
informal talks at the University of Ottawa and the Naval Research Laboratory. Thanks to
both audiences for their feedback.

References
Ackley, D., & Littman, M. (1991). Interactions between learning and evolution. In Proceedings of the Second Conference on Artificial Life, C. Langton, C. Taylor, D. Farmer, and S.
Rasmussen, editors. California: Addison-Wesley.
Aha, D.W., Kibler, D., & Albert, M.K. (1991). Instance-based learning algorithms, Machine
Learning, 6, 37-66.
Aha, D.W., & Bankert, R.L. (1994). Feature selection for case-based classification of cloud
types: An empirical comparison. Case-Based Reasoning: Papers from the 1994 Workshop, edited by D.W. Aha, Technical Report WS-94-07, pp. 106-112. Menlo Park, CA:
AAAI Press.
Anderson, R.W. (in press). Learning and evolution: A quantitative genetics approach. Journal of Theoretical Biology.
Baldwin, J.M. (1896). A new factor in evolution. American Naturalist, 30, 441-451.
Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification and regression
trees. California: Wadsworth.
De Jong, K.A., Spears, W.M., & Gordon, D.F. (1993). Using genetic algorithms for concept
learning. Machine Learning, 13, 161-188.

406

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Fogarty, T.C. (1992). Technical note: First nearest neighbor classification on Frey and Slateโs
letter recognition problem. Machine Learning, 9, 387-388.
Frey, P.W., & Slate, D.J., (1991). Letter recognition using Holland-style adaptive classifiers.
Machine Learning, 6, 161-182.
Friedman, J.H., & Stuetzle, W. (1981). Projection pursuit regression. Journal of the American Statistics Association, 76, 817-823.
Gordon, D.F., & Perlis, D. (1989). Explicitly biased generalization. Computational Intelligence, 5, 67-81.
Grefenstette, J.J. (1986). Optimization of control parameters for genetic algorithms. IEEE
Transactions on Systems, Man, and Cybernetics, 16, 122-128.
Grefenstette, J.J., Ramsey, C.L., & Schultz, A.C. (1990). Learning sequential decision rules
using simulation models and competition. Machine Learning, 5, 355-381.
Hermans, J., Habbema, J.D.F., & Van der Burght, A.T. (1974). Cases of doubt in allocation
problems, k populations. Bulletin of the International Statistics Institute, 45, 523-529.
Hinton, G.E., & Nowlan, S.J. (1987). How learning can guide evolution. Complex Systems,
1, 495-502.
Karakoulas, G. (in preparation). A Q-learning approach to cost-effective classification. Technical Report, Knowledge Systems Laboratory, National Research Council Canada. Also
submitted to the Twelfth International Conference on Machine Learning, ML-95.
Knoll, U., Nakhaeizadeh, G., & Tausend, B. (1994). Cost-sensitive pruning of decision trees.
Proceedings of the Eight European Conference on Machine Learning, ECML-94, pp.
383-386. Berlin, Germany: Springer-Verlag.
Koza, J.R. (1992). Genetic Programming: On the programming of computers by means of
natural selection. Cambridge, MA: MIT Press.
Lirov, Y., & Yue, O.-C. (1991). Automated network troubleshooting knowledge acquisition.
Journal of Applied Intelligence, 1, 121-132.
Maynard Smith, J. (1987). When learning guides evolution. Nature, 329, 761-762.
Morgan, C.L. (1896). On modification and variation. Science, 4, 733-740.
Murphy, P.M., & Aha, D.W. (1994). UCI Repository of Machine Learning Databases. University of California at Irvine, Department of Information and Computer Science.
Norton, S.W. (1989). Generating better decision trees. Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, IJCAI-89, pp. 800-805. Detroit, Michigan.
Nรบรฑez, M. (1988). Economic induction: A case study. Proceedings of the Third European
Working Session on Learning, EWSL-88, pp. 139-145. California: Morgan Kaufmann.

407

T URNEY

Nรบรฑez, M. (1991). The use of background knowledge in decision tree induction. Machine
Learning, 6, 231-250.
Ontario Ministry of Health (1992). Schedule of benefits: Physician services under the health
insurance act, October 1, 1992. Ontario: Ministry of Health.
Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., & Brunk, C. (1994). Reducing misclassification costs: Knowledge-intensive approaches to learning from noisy data. Proceedings of the Eleventh International Conference on Machine Learning, ML-94, pp. 217225. New Brunswick, New Jersey.
Pearl, J. (1984). Heuristics: Intelligent search strategies for computer problem solving. Massachusetts: Addison-Wesley.
Pearl, J. (1988). Probabilistic reasoning in intelligent systems: Networks of plausible inference. California: Morgan Kaufmann.
Pipitone, F., De Jong, K.A., & Spears, W.M. (1991). An artificial intelligence approach to
analog systems diagnosis. In Testing and Diagnosis of Analog Circuits and Systems,
Ruey-wen Liu, editor. New York: Van Nostrand-Reinhold.
Provost, F.J. (1994). Goal-directed inductive learning: Trading off accuracy for reduced error
cost. AAAI Spring Symposium on Goal-Driven Learning.
Provost, F.J., & Buchanan, B.G. (in press). Inductive policy: The pragmatics of bias selection. Machine Learning.
Quinlan, J.R. (1992). C4.5: Programs for machine learning. California: Morgan Kaufmann.
Ragavan, H., & Rendell, L. (1993). Lookahead feature construction for learning hard concepts. Proceedings of the Tenth International Conference on Machine Learning, ML-93,
pp. 252-259. California: Morgan Kaufmann.
Rymon, R. (1993). An SE-tree based characterization of the induction problem. Proceedings
of the Tenth International Conference on Machine Learning, ML-93, pp. 268-275. California: Morgan Kaufmann.
Schaffer, C. (1993). Selecting a classification method by cross-validation. Machine Learning, 13, 135-143.
Schaffer, J.D., Whitley, D., & Eshelman, L.J. (1992). Combinations of genetic algorithms
and neural networks: A survey of the state of the art. In Combinations of Genetic Algorithms and Neural Networks, D. Whitley and J.D. Schaffer, editors. California: IEEE
Computer Society Press.
Seshu, R. (1989). Solving the parity problem. Proceedings of the Fourth European Working
Session on Learning, EWSL-89, pp. 263-271. California: Morgan Kaufmann.
Spears, W.M. (1992). Crossover or mutation? Foundations of Genetic Algorithms 2, FOGA92, edited by D. Whitley. California: Morgan Kaufmann.

408

C OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Sutton, R.S. (1992). Introduction: The challenge of reinforcement learning. Machine Learning, 8, 225-227.
Tan, M., & Schlimmer, J. (1989). Cost-sensitive concept learning of sensor use in approach
and recognition. Proceedings of the Sixth International Workshop on Machine Learning,
ML-89, pp. 392-395. Ithaca, New York.
Tan, M., & Schlimmer, J. (1990). CSL: A cost-sensitive learning system for sensing and
grasping objects. IEEE International Conference on Robotics and Automation. Cincinnati, Ohio.
Tan, M. (1993). Cost-sensitive learning of classification knowledge and its applications in
robotics. Machine Learning, 13, 7-33.
Tcheng, D., Lambert, B., Lu, S., Rendell, L. (1989). Building robust learning systems by
combining induction and optimization. Proceedings of the Eleventh International Joint
Conference on Artificial Intelligence, IJCAI-89, pp. 806-812. Detroit, Michigan.
Turney, P.D. (in press). Technical note: Bias and the quantification of stability. Machine
Learning.
Verdenius, F. (1991). A method for inductive cost optimization. Proceedings of the Fifth
European Working Session on Learning, EWSL-91, pp. 179-191. New York: SpringerVerlag.
Waddington, C.H. (1942). Canalization of development and the inheritance of acquired characters. Nature, 150, 563-565.
Whitley, D., Dominic, S., Das, R., & Anderson, C.W. (1993). Genetic reinforcement learning
for neurocontrol problems. Machine Learning, 13, 259-284.
Whitley, D., & Gruau, F. (1993). Adding learning to the cellular development of neural networks: Evolution and the Baldwin effect. Evolutionary Computation, 1, 213-233.
Whitley, D., Gordon, S., & Mathias, K. (1994). Lamarckian evolution, the Baldwin effect
and function optimization. Parallel Problem Solving from Nature โ PPSN III. Y. Davidor, H.P. Schwefel, and R. Manner, editors, pp. 6-15. Berlin: Springer-Verlag.
Wilson, S.W. (1987). Classifier systems and the animat problem. Machine Learning, 2, 199228.

409

Journal of Articial Intelligence Research 2 (1995) 263{286

Submitted 8/94; published 1/95

Solving Multiclass Learning Problems via
Error-Correcting Output Codes
Thomas G. Dietterich

tgd@cs.orst.edu

Department of Computer Science, 303 Dearborn Hall
Oregon State University
Corvallis, OR 97331 USA

Ghulum Bakiri

eb004@isa.cc.uob.bh

Department of Computer Science
University of Bahrain
Isa Town, Bahrain

Abstract

Multiclass learning problems involve nding a denition for an unknown function (x)
whose range is a discrete set containing
2 values (i.e., \classes"). The denition is
acquired by studying collections of training examples of the form hx (x )i. Existing approaches to multiclass learning problems include direct application of multiclass algorithms
such as the decision-tree algorithms C4.5 and CART, application of binary concept learning
algorithms to learn individual binary functions for each of the classes, and application
of binary concept learning algorithms with distributed output representations. This paper
compares these three approaches to a new technique in which error-correcting codes are
employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide
range of multiclass learning tasks. We also demonstrate that this approach is robust with
respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overtting avoidance techniques such as
decision-tree pruning. Finally, we show that|like the other methods|the error-correcting
code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for
improving the performance of inductive learning programs on multiclass problems.
f

k >

k

i; f

i

k

1. Introduction

The task of learning from examples is to nd an approximate denition for an unknown
function f (x) given training examples of the form hx ; f (x )i. For cases in which f takes
only the values f0; 1g|binary functions|there are many algorithms available. For example,
the decision-tree methods, such as C4.5 (Quinlan, 1993) and CART (Breiman, Friedman,
Olshen, & Stone, 1984) can construct trees whose leaves are labeled with binary values.
Most articial neural network algorithms, such as the perceptron algorithm (Rosenblatt,
1958) and the error backpropagation (BP) algorithm (Rumelhart, Hinton, & Williams,
1986), are best suited to learning binary functions. Theoretical studies of learning have
focused almost entirely on learning binary functions (Valiant, 1984; Natarajan, 1991).
In many real-world learning tasks, however, the unknown function f often takes values
from a discrete set of \classes": fc1; : : : ; c g. For example, in medical diagnosis, the function
might map a description of a patient to one of k possible diseases. In digit recognition (e.g.,
i

i

k

c 1995 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

Dietterich & Bakiri
LeCun, Boser, Denker, Henderson, Howard, Hubbard, & Jackel, 1989), the function maps
each hand-printed digit to one of k = 10 classes. Phoneme recognition systems (e.g., Waibel,
Hanazawa, Hinton, Shikano, & Lang, 1989) typically classify a speech segment into one of
50 to 60 phonemes.
Decision-tree algorithms can be easily generalized to handle these \multiclass" learning
tasks. Each leaf of the decision tree can be labeled with one of the k classes, and internal
nodes can be selected to discriminate among these classes. We will call this the direct
multiclass approach.
Connectionist algorithms are more dicult to apply to multiclass problems. The standard approach is to learn k individual binary functions f1 ; : : : ; f , one for each class. To
assign a new case, x, to one of these classes, each of the f is evaluated on x, and x is
assigned the class j of the function f that returns the highest activation (Nilsson, 1965).
We will call this the one-per-class approach, since one binary function is learned for each
class.
An alternative approach explored by some researchers is to employ a distributed output
code. This approach was pioneered by Sejnowski and Rosenberg (1987) in their widelyknown NETtalk system. Each class is assigned a unique binary string of length n; we will
refer to these strings as \codewords." Then n binary functions are learned, one for each bit
position in these binary strings. During training for an example from class i, the desired
outputs of these n binary functions are specied by the codeword for class i. With articial
neural networks, these n functions can be implemented by the n output units of a single
network.
New values of x are classied by evaluating each of the n binary functions to generate an
n-bit string s. This string is then compared to each of the k codewords, and x is assigned to
the class whose codeword is closest, according to some distance measure, to the generated
string s.
As an example, consider Table 1, which shows a six-bit distributed code for a ten-class
digit-recognition problem. Notice that each row is distinct, so that each class has a unique
codeword. As in most applications of distributed output codes, the bit positions (columns)
have been chosen to be meaningful. Table 2 gives the meanings for the six columns. During
learning, one binary function will be learned for each column. Notice that each column is
also distinct and that each binary function to be learned is a disjunction of the original
classes. For example, f (x) = 1 if f (x) is 1, 4, or 5.
To classify a new hand-printed digit, x, the six functions f ; f ; f ; f ; f ; and f
are evaluated to obtain a six-bit string, such as 110001. Then the distance of this string
to each of the ten codewords is computed. The nearest codeword, according to Hamming
distance (which counts the number of bits that dier), is 110000, which corresponds to class
4. Hence, this predicts that f (x) = 4.
This process of mapping the output string to the nearest codeword is identical to the decoding step for error-correcting codes (Bose & Ray-Chaudhuri, 1960; Hocquenghem, 1959).
This suggests that there might be some advantage to employing error-correcting codes as
a distributed representation. Indeed, the idea of employing error-correcting, distributed
representations can be traced to early research in machine learning (Duda, Machanik, &
Singleton, 1963).
k

i

j

vl

vl

264

hl

dl

cc

ol

or

Error-Correcting Output Codes

Table 1: A distributed code for the digit recognition task.
Class
0
1
2
3
4
5
6
7
8
9

vl
0
1
0
0
1
1
0
0
0
0

Code Word
hl dl cc ol
0 0 1 0
0 0 0 0
1 1 0 1
0 0 0 1
1 0 0 0
1 0 0 1
0 1 1 0
0 1 0 0
0 0 1 0
0 1 1 0

or
0
0
0
0
0
0
1
0
0
0

Table 2: Meanings of the six columns for the code in Table 1.
Column position Abbreviation
Meaning
1
vl
contains vertical line
2
hl
contains horizontal line
3
dl
contains diagonal line
4
cc
contains closed curve
5
ol
contains curve open to left
6
or
contains curve open to right
Table 3: A 15-bit error-correcting output code for a ten-class problem.
Class
0
1
2
3
4
5
6
7
8
9

f0

1
0
1
0
1
0
1
0
1
0

f1

1
0
0
0
1
1
0
0
1
1

f2

0
1
0
1
1
0
1
0
0
1

f3

0
1
1
1
0
0
1
1
1
1

f4

0
1
0
0
1
1
1
1
0
0

f5

0
1
0
1
0
1
0
1
1
0

f6

1
0
0
1
1
0
0
1
1
0

Code Word
f7

0
1
1
1
1
1
0
0
0
0

265

f8

1
0
1
0
0
1
0
1
0
1

f9

0
1
1
0
0
1
1
0
1
0

f10

0
1
1
0
1
0
0
1
0
1

f11

1
0
0
0
0
0
1
1
0
0

f12

1
0
1
1
0
0
0
0
0
0

f13

0
1
0
0
0
0
0
0
1
1

f14

1
0
1
1
1
1
1
1
1
1

Dietterich & Bakiri
Table 3 shows a 15-bit error-correcting code for the digit-recognition task. Each class is
represented by a code word drawn from an error-correcting code. As with the distributed
encoding of Table 1, a separate boolean function is learned for each bit position of the errorcorrecting code. To classify a new example x, each of the learned functions f0 (x); : : :; f14(x)
is evaluated to produce a 15-bit string. This is then mapped to the nearest of the ten
codewords. This code can correct up to three errors out of the 15 bits.
This error-correcting code approach suggests that we view machine learning as a kind
of communications problem in which the identity of the correct output class for a new
example is being \transmitted" over a channel. The channel consists of the input features,
the training examples, and the learning algorithm. Because of errors introduced by the
nite training sample, poor choice of input features, and aws in the learning process,
the class information is corrupted. By encoding the class in an error-correcting code and
\transmitting" each bit separately (i.e., via a separate run of the learning algorithm), the
system may be able to recover from the errors.
This perspective further suggests that the one-per-class and \meaningful" distributed
output approaches will be inferior, because their output representations do not constitute
robust error-correcting codes. A measure of the quality of an error-correcting code is the
minimum Hamming distance between any pair of code words. If the minimum Hamming
distance is d, then the code can correct at least b ,2 1 c single bit errors. This is because each
single bit error moves us one unit away from the true codeword (in Hamming distance). If
we make only b ,2 1 c errors, the nearest codeword will still be the correct codeword. (The
code of Table 3 has minimum Hamming distance seven and hence it can correct errors in
any three bit positions.) The Hamming distance between any two codewords in the oneper-class code is two, so the one-per-class encoding of the k output classes cannot correct
any errors.
The minimum Hamming distance between pairs of codewords in a \meaningful" distributed representation tends to be very low. For example, in Table 1, the Hamming
distance between the codewords for classes 4 and 5 is only one. In these kinds of codes, new
columns are often introduced to discriminate between only two classes. Those two classes
will therefore dier only in one bit position, so the Hamming distance between their output
representations will be one. This is also true of the distributed representation developed by
Sejnowski and Rosenberg (1987) in the NETtalk task.
In this paper, we compare the performance of the error-correcting code approach to
the three existing approaches: the direct multiclass method (using decision trees), the
one-per-class method, and (in the NETtalk task only) the meaningful distributed output
representation approach. We show that error-correcting codes produce uniformly better
generalization performance across a variety of multiclass domains for both the C4.5 decisiontree learning algorithm and the backpropagation neural network learning algorithm. We
then report a series of experiments designed to assess the robustness of the error-correcting
code approach to various changes in the learning task: length of the code, size of the training
set, assignment of codewords to classes, and decision-tree pruning. Finally, we show that
the error-correcting code approach can produce reliable class probability estimates.
The paper concludes with a discussion of the open questions raised by these results.
Chief among these questions is the issue of why the errors being made in the dierent bit
positions of the output are somewhat independent of one another. Without this independ

d

266

Error-Correcting Output Codes
Table 4: Data sets employed in the study.
Name
glass
vowel
POS
soybean
audiologyS
ISOLET
letter
NETtalk

Number of Number of
Number of
Number of
Features
Classes
Training Examples Test Examples
9
6
214
10-fold xval
10
11
528
462
30
12
3,060
10-fold xval
35
19
307
376
69
24
200
26
617
26
6,238
1,559
16
26
16,000
4,000
203
54 phonemes
1000 words =
1000 words =
6 stresses
7,229 letters
7,242 letters

dence, the error-correcting output code method would fail. We address this question|for
the case of decision-tree algorithms|in a companion paper (Kong & Dietterich, 1995).

2. Methods
This section describes the data sets and learning algorithms employed in this study. It
also discusses the issues involved in the design of error-correcting codes and describes four
algorithms for code design. The section concludes with a brief description of the methods
applied to make classication decisions and evaluate performance on independent test sets.

2.1 Data Sets
Table 4 summarizes the data sets employed in the study. The glass, vowel, soybean, audiologyS, ISOLET, letter, and NETtalk data sets are available from the Irvine Repository of
machine learning databases (Murphy & Aha, 1994).1 The POS (part of speech) data set
was provided by C. Cardie (personal communication); an earlier version of the data set was
described by Cardie (1993). We did not use the entire NETtalk data set, which consists of
a dictionary of 20,003 words and their pronunciations. Instead, to make the experiments
feasible, we chose a training set of 1000 words and a disjoint test set of 1000 words at
random from the NETtalk dictionary. In this paper, we focus on the percentage of letters
pronounced correctly (rather than whole words). To pronounce a letter, both the phoneme
and stress of the letter must be determined. Although there are 54  6 syntactically possible
combinations of phonemes and stresses, only 140 of these appear in the training and test
sets we selected.
1. The repository refers to the soybean data set as \soybean-large", the \audiologyS" data set as \audiology.standardized", and the \letter" data set as \letter-recognition".

267

Dietterich & Bakiri

2.2 Learning Algorithms

We employed two general classes of learning methods: algorithms for learning decision trees
and algorithms for learning feed-forward networks of sigmoidal units (articial neural networks). For decision trees, we performed all of our experiments using C4.5, Release 1, which
is an older (but substantially identical) version of the program described in Quinlan (1993).
We have made several changes to C4.5 to support distributed output representations, but
these have not aected the tree-growing part of the algorithm. For pruning, the condence
factor was set to 0.25. C4.5 contains a facility for creating \soft thresholds" for continuous
features. We found experimentally that this improved the quality of the class probability
estimates produced by the algorithm in the \glass", \vowel", and \ISOLET" domains, so
the results reported for those domains were computed using soft thresholds.
For neural networks, we employed two implementations. In most domains, we used the
extremely fast backpropagation implementation provided by the CNAPS neurocomputer
(Adaptive Solutions, 1992). This performs simple gradient descent with a xed learning
rate. The gradient is updated after presenting each training example; no momentum term
was employed. A potential limitation of the CNAPS is that inputs are only represented
to eight bits of accuracy, and weights are only represented to 16 bits of accuracy. Weight
update arithmetic does not round, but instead performs jamming (i.e., forcing the lowest
order bit to 1 when low order bits are lost due to shifting or multiplication). On the
speech recognition, letter recognition, and vowel data sets, we employed the opt system
distributed by Oregon Graduate Institute (Barnard & Cole, 1989). This implements the
conjugate gradient algorithm and updates the gradient after each complete pass through
the training examples (known as per-epoch updating). No learning rate is required for this
approach.
Both the CNAPS and opt attempt to minimize the squared error between the computed
and desired outputs of the network. Many researchers have employed other error measures,
particularly cross-entropy (Hinton, 1989) and classication gure-of-merit (CFM, Hampshire II & Waibel, 1990). Many researchers also advocate using a softmax normalizing layer
at the outputs of the network (Bridle, 1990). While each of these congurations has good
theoretical support, Richard and Lippmann (1991) report that squared error works just as
well as these other measures in producing accurate posterior probability estimates. Furthermore, cross-entropy and CFM tend to overt more easily than squared error (Lippmann,
personal communication; Weigend, 1993). We chose to minimize squared error because this
is what the CNAPS and opt systems implement.
With either neural network algorithm, several parameters must be chosen by the user.
For the CNAPS, we must select the learning rate, the initial random seed, the number
of hidden units, and the stopping criteria. We selected these to optimize performance
on a validation set, following the methodology of Lang, Hinton, and Waibel (1990). The
training set is subdivided into a subtraining set and a validation set. While training on the
subtraining set, we observed generalization performance on the validation set to determine
the optimal settings of learning rate and network size and the best point at which to
stop training. The training set mean squared error at that stopping point is computed,
and training is then performed on the entire training set using the chosen parameters and
stopping at the indicated mean squared error. Finally, we measure network performance
on the test set.
268

Error-Correcting Output Codes
For most of the data sets, this procedure worked very well. However, for the letter
recognition data set, it was clearly choosing poor stopping points for the full training set.
To overcome this problem, we employed a slightly dierent procedure to determine the
stopping epoch. We trained on a series of progressively larger training sets (all of which
were subsets of the nal training set). Using a validation set, we determined the best
stopping epoch on each of these training sets. We then extrapolated from these training
sets to predict the best stopping epoch on the full training set.
For the \glass" and \POS" data sets, we employed ten-fold cross-validation to assess
generalization performance. We chose training parameters based on only one \fold" of the
ten-fold cross-validation. This creates some test set contamination, since examples in the
validation set data of one fold are in the test set data of other folds. However, we found
that there was little or no overtting, so the validation set had little eect on the choice of
parameters or stopping points.
The other data sets all come with designated test sets, which we employed to measure
generalization performance.

2.3 Error-Correcting Code Design

We dene an error-correcting code to be a matrix of binary values such as the matrix shown
in Table 3. The length of a code is the number of columns in the code. The number of
rows in the code is equal to the number of classes in the multiclass learning problem. A
\codeword" is a row in the code.
A good error-correcting output code for a k-class problem should satisfy two properties:
 Row separation. Each codeword should be well-separated in Hamming distance
from each of the other codewords.
 Column separation. Each bit-position function f should be uncorrelated with the
functions to be learned for the other bit positions f ; j 6= i: This can be achieved by
insisting that the Hamming distance between column i and each of the other columns
be large and that the Hamming distance between column i and the complement of
each of the other columns also be large.
i

j

The power of a code to correct errors is directly related to the row separation, as
discussed above. The purpose of the column separation condition is less obvious. If two
columns i and j are similar or identical, then when a deterministic learning algorithm
such as C4.5 is applied to learn f and f , it will make similar (correlated) mistakes. Errorcorrecting codes only succeed if the errors made in the individual bit positions are relatively
uncorrelated, so that the number of simultaneous errors in many bit positions is small. If
there are many simultaneous errors, the error-correcting code will not be able to correct
them (Peterson & Weldon, 1972).
The errors in columns i and j will also be highly correlated if the bits in those columns
are complementary. This is because algorithms such as C4.5 and backpropagation treat
a class and its complement symmetrically. C4.5 will construct identical decision trees if
the 0-class and 1-class are interchanged. The maximum Hamming distance between two
columns is attained when the columns are complements. Hence, the column separation
condition attempts to ensure that columns are neither identical nor complementary.
i

j

269

Dietterich & Bakiri

Table 5: All possible columns for a three-class problem. Note that the last four columns
are complements of the rst four and that the rst column does not discriminate
among any of the classes.
Class
c0
c1
c2

f0

0
0
0

f1

0
0
1

f2

0
1
0

Code Word
f3

f4

0
1
1

1
0
0

f5

1
0
1

f6

1
1
0

f7

1
1
1

Unless the number of classes is at least ve, it is dicult to satisfy both of these properties. For example, when the number of classes is three, there are only 23 = 8 possible
columns (see Table 5). Of these, half are complements of the other half. So this leaves us
with only four possible columns. One of these will be either all zeroes or all ones, which
will make it useless for discriminating among the rows. The result is that we are left with
only three possible columns, which is exactly what the one-per-class encoding provides.
In general, if there are k classes, there will be at most 2 ,1 , 1 usable columns after
removing complements and the all-zeros or all-ones column. For four classes, we get a
seven-column code with minimum inter-row Hamming distance 4. For ve classes, we get
a 15-column code, and so on.
We have employed four methods for constructing good error-correcting output codes
in this paper: (a) an exhaustive technique, (b) a method that selects columns from an
exhaustive code, (c) a method based on a randomized hill-climbing algorithm, and (d) BCH
codes. The choice of which method to use is based on the number of classes, k. Finding a
single method suitable for all values of k is an open research problem. We describe each of
our four methods in turn.
k

2.3.1 Exhaustive Codes

When 3  k  7, we construct a code of length 2 ,1 , 1 as follows. Row 1 is all ones. Row 2
consists of 2 ,2 zeroes followed by 2 ,2 , 1 ones. Row 3 consists of 2 ,3 zeroes, followed by
2 ,3 ones, followed by 2 ,3 zeroes, followed by 2 ,3 , 1 ones. In row i, there are alternating
runs of 2 , zeroes and ones. Table 6 shows the exhaustive code for a ve-class problem.
This code has inter-row Hamming distance 8; no columns are identical or complementary.
k

k

k

k

k

k

k

k

i

2.3.2 Column Selection from Exhaustive Codes

When 8  k  11, we construct an exhaustive code and then select a good subset of
its columns. We formulate this as a propositional satisability problem and apply the
GSAT algorithm (Selman, Levesque, & Mitchell, 1992) to attempt a solution. A solution
is required to include exactly L columns (the desired length of the code) while ensuring
that the Hamming distance between every two columns is between d and L , d, for some
chosen value of d. Each column is represented by a boolean variable. A pairwise mutual
270

Error-Correcting Output Codes

Row

1
1
0
0
0
0

1
2
3
4
5

2
1
0
0
0
1

Table 6: Exhaustive code for k=5.
Column
3 4 5 6 7 8 9 10 11 12
1 1 1 1 1 1 1 1 1 1
0 0 0 0 0 0 1 1 1 1
0 0 1 1 1 1 0 0 0 0
1 1 0 0 1 1 0 0 1 1
0 1 0 1 0 1 0 1 0 1

13
1
1
1
0
0

14
1
1
1
0
1

1

0

1

0

1

0

0

1

15
1
1
1
1
0

Figure 1: Hill-climbing algorithm for improving row and column separation. The two closest
rows and columns are indicated by lines. Where these lines intersect, the bits in
the code words are changed to improve separations as shown on the right.
exclusion constraint is placed between any two columns that violate the column separation
condition. To support these constraints, we extended GSAT to support mutual exclusion
and \m-of-n" constraints eciently.
2.3.3 Randomized Hill Climbing

For k > 11, we employed a random search algorithm that begins by drawing k random
strings of the desired length L. Any pair of such random strings will be separated by a
Hamming distance that is binomially distributed with mean L=2. Hence, such randomly
generated codes are generally quite good on average. To improve them, the algorithm
repeatedly nds the pair of rows closest together in Hamming distance and the pair of
columns that have the \most extreme" Hamming distance (i.e., either too close or too
far apart). The algorithm then computes the four codeword bits where these rows and
columns intersect and changes them to improve the row and column separations as shown
in Figure 1. When this hill climbing procedure reaches a local maximum, the algorithm
randomly chooses pairs of rows and columns and tries to improve their separations. This
combined hill-climbing/random-choice procedure is able to improve the minimum Hamming
distance separation quite substantially.
271

Dietterich & Bakiri
2.3.4 BCH Codes

For k > 11 we also applied the BCH algorithm to design codes (Bose & Ray-Chaudhuri,
1960; Hocquenghem, 1959). The BCH algorithm employs algebraic methods from Galois
eld theory to design nearly optimal error-correcting codes. However, there are three practical drawbacks to using this algorithm. First, published tables of the primitive polynomials
required by this algorithm only produce codes up to length 64, since this is the largest word
size employed in computer memories. Second, the codes do not always exhibit good column
separations. Third, the number of rows in these codes is always a power of two. If the number of classes k in our learning problem is not a power of two, we must shorten the code by
deleting rows (and possible columns) while maintaining good row and column separations.
We have experimented with various heuristic greedy algorithms for code shortening. For
most of the codes used in the NETtalk, ISOLET, and Letter Recognition domains, we have
used a combination of simple greedy algorithms and manual intervention to design good
shortened BCH codes.
In each of the data sets that we studied, we designed a series of error-correcting codes
of increasing lengths. We executed each learning algorithm for each of these codes. We
stopped lengthening the codes when performance appeared to be leveling o.

2.4 Making Classication Decisions
Each approach to solving multiclass problems|direct multiclass, one-per-class, and errorcorrecting output coding|assumes a method for classifying new examples. For the C4.5
direct multiclass approach, the C4.5 system computes a class probability estimate for each
new example. This estimates the probability that that example belongs to each of the
k classes. C4.5 then chooses the class having the highest probability as the class of the
example.
For the one-per-class approach, each decision tree or neural network output unit can
be viewed as computing the probability that the new example belongs to its corresponding
class. The class whose decision tree or output unit gives the highest probability estimate
is chosen as the predicted class. Ties are broken arbitrarily in favor of the class that comes
rst in the class ordering.
For the error-correcting output code approach, each decision tree or neural network
output unit can be viewed as computing the probability that its corresponding bit in the
codeword is one. Call these probability values B = hb1; b2; : : : ; b i, where n is the length of
the codewords in the error-correcting code. To classify a new example, we compute the L1
distance between this probability vector B and each of the codewords W (i = 1 : : :k) in
the error correcting code. The L1 distance between B and W is dened as
n

i

L1 (B; Wi)

=

Xj

i

L

j

=0

bj

, W j:
i;j

The class whose codeword has the smallest L1 distance to B is assigned as the class of the
new example. Ties are broken arbitrarily in favor of the class that comes rst in the class
ordering.
272

Performance relative to Multiclass

So
yb
ea
n
A
ud
io
lo
gy
IS
O
LE
T
Le
tte
r
N
ET
ta
lk

PO

S

el
ow
V

G

la

ss

Error-Correcting Output Codes

*

10

*

*
*

*

*

0

C4.5 Multiclass
*
*

-10

*

-20
-30

*

C4.5 one-per-class

C4.5 ECOC

Figure 2: Performance (in percentage points) of the one-per-class and ECOC methods relative to the direct multiclass method using C4.5. Asterisk indicates dierence is
signicant at the 0.05 level or better.

3. Results
We now present the results of our experiments. We begin with the results for decision trees.
Then, we consider neural networks. Finally, we report the results of a series of experiments
to assess the robustness of the error-correcting output code method.

3.1 Decision Trees
Figure 2 shows the performance of C4.5 in all eight domains. The horizontal line corresponds
to the performance of the standard multiclass decision-tree algorithm. The light bar shows
the performance of the one-per-class approach, and the dark bar shows the performance of
the ECOC approach with the longest error-correcting code tested. Performance is displayed
as the number of percentage points by which each pair of algorithms dier. An asterisk
indicates that the dierence is statistically signicant at the p < 0:05 level according to the
test for the dierence of two proportions (using the normal approximation to the binomial
distribution, see Snedecor & Cochran, 1989, p. 124).
From this gure, we can see that the one-per-class method performs signicantly worse
than the multiclass method in four of the eight domains and that its behavior is statistically
indistinguishable in the remaining four domains. Much more encouraging is the observation
that the error-correcting output code approach is signicantly superior to the multiclass
approach in six of the eight domains and indistinguishable in the remaining two.
273

Dietterich & Bakiri
In the NETtalk domain, we can also consider the performance of the meaningful distributed representation developed by Sejnowski and Rosenberg. This representation gave
66.7% correct classication as compared with 68.6% for the one-per-class conguration,
70.0% for the direct-multiclass conguration, and 74.3% for the ECOC conguration. The
dierences in each of these gures are statistically signicant at the 0.05 level or better
except that the one-per-class and direct-multiclass congurations are not statistically distinguishable.

3.2 Backpropagation

Figure 3 shows the results for backpropagation in ve of the most challenging domains.
The horizontal line corresponds to the performance of the one-per-class encoding for this
method. The bars show the number of percentage points by which the error-correcting
output coding representation outperforms the one-per-class representation. In four of the
ve domains, the ECOC encoding is superior; the dierences are statistically signicant in
the Vowel, NETtalk, and ISOLET domains.2
In the letter recognition domain, we encountered great diculty in successfully training
networks using the CNAPS machine, particularly for the ECOC conguration. Experiments
showed that the problem arose from the fact that the CNAPS implementation of backpropagation employs a xed learning rate. We therefore switched to the much slower opt
program, which chooses the learning rate adaptively via conjugate-gradient line searches.
This behaved better for both the one-per-class and ECOC congurations.
We also had some diculty training ISOLET in the ECOC conguration on large networks (182 units), even with the opt program. Some sets of initial random weights led to
local minima and poor performance on the validation set.
In the NETtalk task, we can again compare the performance of the Sejnowski-Rosenberg
distributed encoding to the one-per-class and ECOC encodings. The distributed encoding
yielded a performance of 71.5% correct, compared to 72.9% for the one-per-class encoding,
and 74.9% for the ECOC encoding. The dierence between the distributed encoding and the
one-per-class encoding is not statistically signicant. From these results and the previous
results for C4.5, we can conclude that the distributed encoding has no advantages over the
one-per-class and ECOC encoding in this domain.

3.3 Robustness

These results show that the ECOC approach performs as well as, and often better than,
the alternative approaches. However, there are several important questions that must be
answered before we can recommend the ECOC approach without reservation:

 Do the results hold for small samples? We have found that decision trees learned using

error-correcting codes are much larger than those learned using the one-per-class or
multiclass approaches. This suggests that with small sample sizes, the ECOC method
may not perform as well, since complex trees usually require more data to be learned
reliably. On the other hand, the experiments described above covered a wide range of

2. The dierence for ISOLET is only detectable using a test for paired dierences of proportions. See
Snedecor & Cochran (1989, p. 122.).

274

Performance relative to one-per-class

10

lk
ET
ta
N

te
Le
t

IS
O

r

LE
T

el
ow
V

G

la

ss

Error-Correcting Output Codes

*

5
*
*

0

Backprop one-per-class

Backprop ECOC

Figure 3: Performance of the ECOC method relative to the one-per-class using backpropagation. Asterisk indicates dierence is signicant at the 0.05 level or better.
training set sizes, which suggests that the results may not depend on having a large
training set.

 Do the results depend on the particular assignment of codewords to classes? The

codewords were assigned to the classes arbitrarily in the experiments reported above,
which suggests that the particular assignment may not be important. However, some
assignments might still be much better than others.

 Do the results depend on whether pruning techniques are applied to the decision-

tree algorithms? Pruning methods have been shown to improve the performance of
multiclass C4.5 in many domains.

 Can the ECOC approach provide class probability estimates? Both C4.5 and back-

propagation can be congured to provide estimates of the probability that a test
example belongs to each of the k possible classes. Can the ECOC approach do this
as well?

3.3.1 Small sample performance

As we have noted, we became concerned about the small sample performance of the ECOC
method when we noticed that the ECOC method always requires much larger decision trees
than the OPC method. Table 7 compares the sizes of the decision trees learned by C4.5
under the multiclass, one-per-class, and ECOC congurations for the letter recognition task
and the NETtalk task. For the OPC and ECOC congurations, the tables show the average
number of leaves in the trees learned for each bit position of the output representation. For
275

Dietterich & Bakiri

Table 7: Size of decision trees learned by C4.5 for the letter recognition task and the
NETtalk task.
Letter Recognition Leaves per bit Total leaves
Multiclass
2353
One-per-class
242
6292
207-bit ECOC
1606
332383
NETtalk
Multiclass
One-per-Class
159-bit ECOC

Leaves per bit
phoneme stress
61
901

600
911

Total leaves
phoneme stress
1425 1567
3320 3602
114469 29140

letter recognition, the trees learned for a 207-bit ECOC are more than six times larger
than those learned for the one-per-class representation. For the phoneme classication part
of NETtalk, the ECOC trees are 14 times larger than the OPC trees. Another way to
compare the sizes of the trees is to consider the total number of leaves in the trees. The
tables clearly show that the multiclass approach requires much less memory (many fewer
total leaves) than either the OPC or the ECOC approaches.
With backpropagation, it is more dicult to determine the amount of \network resources" that are consumed in training the network. One approach is to compare the
number of hidden units that give the best generalization performance. In the ISOLET task,
for example, the one-per-class encoding attains peak validation set performance with a 78hidden-unit network, whereas the 30-bit error-correcting encoding attained peak validation
set performance with a 156-hidden-unit network. In the letter recognition task, peak performance for the one-per-class encoding was obtained with a network of 120-hidden units
compared to 200 hidden units for a 62-bit error-correcting output code.
From the decision tree and neural network sizes, we can see that, in general, the errorcorrecting output representation requires more complex hypotheses than the one-per-class
representation. From learning theory and statistics, we known that complex hypotheses
typically require more training data than simple ones. On this basis, one might expect that
the performance of the ECOC method would be very poor with small training sets. To test
this prediction, we measured performance as a function of training set size in two of the
larger domains: NETtalk and letter recognition.
Figure 4 presents learning curves for C4.5 on the NETtalk and letter recognition tasks,
which show accuracy for a series of progressively larger training sets. From the gure it is
clear that the 61-bit error-correcting code consistently outperforms the other two congurations by a nearly constant margin. Figure 5 shows corresponding results for backpropagation on the NETtalk and letter recognition tasks. On the NETtalk task, the results are
the same: sample size has no apparent inuence on the benets of error-correcting output coding. However, for the letter-recognition task, there appears to be an interaction.
276

Error-Correcting Output Codes

NETtalk

Letter Recognition

75

100
C4 61-bit ECOC

Percent Correct

70

C4 Multiclass
C4 One-per-class

90
C4 Multiclass

65

80

60

70

55

60

50

50

45

40

40

30

35
100

C4 62-bit ECOC

C4 One-per-class

20
100

1000

1000

Training Set Size

10000
Training Set Size

Figure 4: Accuracy of C4.5 in the multiclass, one-per-class, and error-correcting output
coding congurations for increasing training set sizes in the NETtalk and letter
recognition tasks. Note that the horizontal axis is plotted on a logarithmic scale.
NETtalk

Letter Recognition
100

75
90
70

CNAPS 61-bit ECOC

Percent Correct

80
65

opt OPC
opt 62-bit ECOC

70

CNAPS One-per-class

60

60

55

50

50

45
100

1000
Training Set Size (words)

40
100

1000
Training Set Size

10000

Figure 5: Accuracy of backpropagation in the one-per-class and error-correcting output
coding congurations for increasing training set sizes on the NETtalk and letter
recognition tasks.
Error-correcting output coding works best for small training sets, where there is a statistically signicant benet. With the largest training set|16,000 examples|the one-per-class
method very slightly outperforms the ECOC method.
From these experiments, we conclude that error-correcting output coding works very
well with small samples, despite the increased size of the decision trees and the increased
complexity of training neural networks. Indeed, with backpropagation on the letter recognition task, error-correcting output coding worked better for small samples than it did for
277

Dietterich & Bakiri

Table 8: Five random assignments of codewords to classes for the NETtalk task. Each
column shows the percentage of letters correctly classied by C4.5 decision trees.
Multiclass One-per-class
70.0
68.6

61-Bit Error-Correcting Code Replications
a
b
c
d
e
73.8 73.6 73.5 73.8
73.3

large ones. This eect suggests that ECOC works by reducing the variance of the learning
algorithm. For small samples, the variance is higher, so ECOC can provide more benet.
3.3.2 Assignment of Codewords to Classes

In all of the results reported thus far, the codewords in the error-correcting code have been
arbitrarily assigned to the classes of the learning task. We conducted a series of experiments
in the NETtalk domain with C4.5 to determine whether randomly reassigning the codewords
to the classes had any eect on the success of ECOC. Table 8 shows the results of ve
random assignments of codewords to classes. There is no statistically signicant variation
in the performance of the dierent random assignments. This is consistent with similar
experiments reported in Bakiri (1991).
3.3.3 Effect of Tree Pruning

Pruning of decision trees is an important technique for preventing overtting. However, the
merit of pruning varies from one domain to another. Figure 6 shows the change in performance due to pruning in each of the eight domains and for each of the three congurations
studied in this paper: multiclass, one-per-class, and error-correcting output coding.
From the gure, we see that in most cases pruning makes no statistically signicant
dierence in performance (aside from the POS task, where it decreases the performance of
all three congurations). Aside from POS, only one of the statistically signicant changes
involves the ECOC conguration, while two aect the one-per-class conguration, and one
aects the multiclass conguration. These data suggest that pruning only occasionally has
a major eect on any of these congurations. There is no evidence to suggest that pruning
aects one conguration more than another.
3.3.4 Class Probability Estimates

In many applications, it is important to have a classier that cannot only classify new cases
well but also estimate the probability that a new case belongs to each of the k classes.
For example, in medical diagnosis, a simple classier might classify a patient as \healthy"
because, given the input features, that is the most likely class. However, if there is a
non-zero probability that the patient has a life-threatening disease, the right choice for the
physician may still be to prescribe a therapy for that disease.
A more mundane example involves automated reading of handwritten postal codes on
envelopes. If the classier is very condent of its classication (i.e., because the estimated
278

Performance relative to no pruning

lk
ET
N

Le

tte

r

ta

T
LE

lo
gy

IS
O

ud

io

an
A

be
So
y

PO

S

el
ow
V

G

la

ss

Error-Correcting Output Codes

10
*

*
*

5

0
-2

No Pruning
**

*
*

C4.5 Multiclass

C4.5 one-per-class

C4.5 ECOC

Figure 6: Change in percentage points of the performance of C4.5 with and without pruning
in three congurations. Horizontal line indicates performance with no pruning.
Asterisk indicates that the dierence is signicant at the 0.05 level or better.

279

Dietterich & Bakiri
probabilities are very strong), then it can proceed to route the envelope. However, if it
is uncertain, then the envelope should be \rejected", and sent to a human being who can
attempt to read the postal code and process the envelope (Wilkinson, Geist, Janet, et al.,
1992).
One way to assess the quality of the class probability estimates of a classier is to
compute a \rejection curve". When the learning algorithm classies a new case, we require
it to also output a \condence" level. Then we plot a curve showing the percentage of
correctly classied test cases whose condence level exceeds a given value. A rejection curve
that increases smoothly demonstrates that the condence level produced by the algorithm
can be transformed into an accurate probability measure.
For one-per-class neural networks, many researchers have found that the dierence in
activity between the class with the highest activity and the class with the second-highest
activity is a good measure of condence (e.g., LeCun et al., 1989). If this dierence is large,
then the chosen class is clearly much better than the others. If the dierence is small, then
the chosen class is nearly tied with another class. This same measure can be applied to the
class probability estimates produced by C4.5.
An analogous measure of condence for error-correcting output codes can be computed
from the L1 distance between the vector B of output probabilities for each bit and the
codewords of each of the classes. Specically, we employ the dierence between the L1
distance to the second-nearest codeword and the L1 distance to the nearest codeword as
our condence measure. If this dierence is large, an algorithm can be quite condent of
its classication decision. If the dierence is small, the algorithm is not condent.
Figure 7 compares the rejection curves for various congurations of C4.5 and backpropagation on the NETtalk task. These curves are constructed by rst running all of the test
examples through the learned decision trees and computing the predicted class of each example and the condence value for that prediction. To generate each point along the curve,
a value is chosen for a parameter , which denes the minimum required condence. The
classied test examples are then processed to determine the percentage of test examples
whose condence level is less than  (these are \rejected") and the percentage of the remaining examples that are correctly classied. The value of  is progressively incremented
(starting at 0) until all test examples are rejected.
The lower left portion of the curve shows the performance of the algorithm when  is
small, so only the least condent cases are rejected. The upper right portion of the curve
shows the performance when  is large, so only the most condent cases are classied.
Good class probability estimates produce a curve that rises smoothly and monotonically.
A at or decreasing region in a rejection curve reveals cases where the condence estimate
of the learning algorithm is unrelated or inversely related to the actual performance of the
algorithm.
The rejection curves often terminate prior to rejecting 100% of the examples. This occurs
when the nal increment in  causes all examples to be rejected. This gives some idea of the
number of examples for which the algorithm was highly condent of its classications. If
the curve terminates early, this shows that there were very few examples that the algorithm
could condently classify.
In Figure 7, we see that|with the exception of the Multiclass conguration|the rejection curves for all of the various congurations of C4.5 increase fairly smoothly, so all of
280

Error-Correcting Output Codes
C4.5

Backpropagation

100

100
61-bit ECOC

61-bit ECOC

159-bit ECOC

95

95
OPC

OPC

159-bit ECOC

Percent Correct

90
90

Multiclass

Distributed

Distributed

85
85
80
80
75
75

70

65

70
0

10

20

30

40
50
60
Percent Rejected

70

80

90

100

0

10

20

30

40
50
60
Percent Rejected

70

80

Figure 7: Rejection curves for various congurations of C4.5 and backpropagation on the
NETtalk task. The \Distributed" curve plots the behavior of the SejnowskiRosenberg distributed representation.
them are producing acceptable condence estimates. The two error-correcting congurations have smooth curves that remain above all of the other congurations. This shows that
the performance advantage of error-correcting output coding is maintained at all condence
levels|ECOC improves classication decisions on all examples, not just the borderline ones.
Similar behavior is seen in the rejection curves for backpropagation. Again all congurations of backpropagation give fairly smooth rejection curves. However, note that the
159-bit code actually decreases at high rejection rates. By contrast, the 61-bit code gives a
monotonic curve that eventually reaches 100%. We have seen this behavior in several of the
cases we have studied: extremely long error-correcting codes are usually the best method
at low rejection rates, but at high rejection rates, codes of \intermediate" length (typically
60-80 bits) behave better. We have no explanation for this behavior.
Figure 8 compares the rejection curves for various congurations of C4.5 and backpropagation on the ISOLET task. Here we see that the ECOC approach is markedly superior
to either the one-per-class or multiclass approaches. This gure illustrates another phenomenon we have frequently observed: the curve for multiclass C4.5 becomes quite at and
terminates very early, and the one-per-class curve eventually surpasses it. This suggests that
there may be opportunities to improve the class probability estimates produced by C4.5
on multiclass trees. (Note that we employed \softened thresholds" in these experiments.)
In the backpropagation rejection curves, the ECOC approach consistently outperforms the
one-per-class approach until both are very close to 100% correct. Note that both congurations of backpropagation can condently classify more than 50% of the test examples with
100% accuracy.
From these graphs, it is clear that the error-correcting approach (with codes of intermediate length) can provide condence estimates that are at least as good as those provided
by the standard approaches to multiclass problems.
281

90

100

Dietterich & Bakiri
C4.5

Backpropagation

100

101
107-bit ECOC

98
45-bit ECOC

100

96

30-bit ECOC

Multiclass
Percent Correct

94

99

92
90

98

One Per Class

88
97

86
84

96

One Per Class
82
80

95
0

10

20

30

40
50
60
Percent Rejected

70

80

90

100

0

5

10

15

20
25
Percent Rejected

30

35

Figure 8: Rejection curves for various congurations of C4.5 and backpropagation on the
ISOLET task.

4. Conclusions

In this paper, we experimentally compared four approaches to multiclass learning problems:
multiclass decision trees, the one-per-class (OPC) approach, the meaningful distributed
output approach, and the error-correcting output coding (ECOC) approach. The results
clearly show that the ECOC approach is superior to the other three approaches. The
improvements provided by the ECOC approach can be quite substantial: improvements on
the order of ten percentage points were observed in several domains. Statistically signicant
improvements were observed in six of eight domains with decision trees and three of ve
domains with backpropagation.
The improvements were also robust:
 ECOC improves both decision trees and neural networks;
 ECOC provides improvements even with very small sample sizes; and
 The improvements do not depend on the particular assignment of codewords to classes.
The error-correcting approach can also provide estimates of the condence of classication decisions that are at least as accurate as those provided by existing methods.
There are some additional costs to employing error-correcting output codes. Decision
trees learned using ECOC are generally much larger and more complex than trees constructed using the one-per-class or multiclass approaches. Neural networks learned using
ECOC often require more hidden units and longer and more careful training to obtain
the improved performance (see Section 3.2). These factors may argue against using errorcorrecting output coding in some domains. For example, in domains where it is important
for humans to understand and interpret the induced decision trees, ECOC methods are not
appropriate, because they produce such complex trees. In domains where training must
be rapid and completely autonomous, ECOC methods with backpropagation cannot be
recommended, because of the potential for encountering diculties during training.
282

40

45

Error-Correcting Output Codes
Finally, we found that error-correcting codes of intermediate length tend to give better
condence estimates than very long error-correcting codes, even though the very long codes
give the best generalization performance.
There are many open problems that require further research. First and foremost, it is
important to obtain a deeper understanding of why the ECOC method works. If we assume
that each of the learned hypotheses makes classication errors independently, then coding
theory provides the explanation: individual errors can be corrected because the codewords
are \far apart" in the output space. However, because each of the hypotheses is learned
using the same algorithm on the same training data, we would expect that the errors made
by individual hypotheses would be highly correlated, and such errors cannot be corrected by
an error-correcting code. So the key open problem is to understand why the classication
errors at dierent bit positions are fairly independent. How does the error-correcting output
code result in this independence?
A closely related open problem concerns the relationship between the ECOC approach
and various \ensemble", \committee", and \boosting" methods (Perrone & Cooper, 1993;
Schapire, 1990; Freund, 1992). These methods construct multiple hypotheses which then
\vote" to determine the classication of an example. An error-correcting code can also
be viewed as a very compact form of voting in which a certain number of incorrect votes
can be corrected. An interesting dierence between standard ensemble methods and the
ECOC approach is that in the ensemble methods, each hypothesis is attempting to predict
the same function, whereas in the ECOC approach, each hypothesis predicts a dierent
function. This may reduce the correlations between the hypotheses and make them more
eective \voters." Much more work is needed to explore this relationship.
Another open question concerns the relationship between the ECOC approach and the
exible discriminant analysis technique of Hastie, Tibshirani, and Buja (In Press). Their
method rst employs the one-per-class approach (e.g., with neural networks) and then
applies a kind of discriminant analysis to the outputs. This discriminant analysis maps the
outputs into a k , 1 dimensional space such that each class has a dened \center point". New
cases are classied by mapping them into this space and then nding the nearest \center
point" and its class. These center points are similar to our codewords but in a continuous
space of dimension k , 1. It may be that the ECOC method is a kind of randomized,
higher-dimensional variant of this approach.
Finally, the ECOC approach shows promise of scaling neural networks to very large
classication problems (with hundreds or thousands of classes) much better than the oneper-class method. This is because a good error-correcting code can have a length n that is
much less than the total number of classes, whereas the one-per-class approach requires that
there be one output unit for each class. Networks with thousands of output units would
be expensive and dicult to train. Future studies should test the scaling ability of these
dierent approaches to such large classication tasks.

Acknowledgements
The authors thank the anonymous reviewers for their valuable suggestions which improved
the presentation of the paper. The authors also thank Prasad Tadepalli for proof-reading the
283

Dietterich & Bakiri
nal manuscript. The authors gratefully acknowledge the support of the National Science
Foundation under grants numbered IRI-8667316, CDA-9216172, and IRI-9204129. Bakiri
also thanks Bahrain University for its support of his doctoral research.

References

Adaptive Solutions (1992). CNAPS back-propagation guide. Tech. rep. 801-20030-04, Adaptive Solutions, Inc., Beaverton, OR.
Bakiri, G. (1991). Converting English text to speech: A machine learning approach. Tech.
rep. 91-30-2, Department of Computer Science, Oregon State University, Corvallis,
OR.
Barnard, E., & Cole, R. A. (1989). A neural-net training program based on conjugategradient optimization. Tech. rep. CSE 89-014, Oregon Graduate Institute, Beaverton,
OR.
Bose, R. C., & Ray-Chaudhuri, D. K. (1960). On a class of error-correcting binary group
codes. Information and Control, 3, 68{79.
Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classication and
Regression Trees. Wadsworth International Group.
Bridle, J. S. (1990). Training stochastic model recognition algorithms as networks can
lead to maximum mutual information estimation of parameters. In Touretzky, D. S.
(Ed.), Neural Information Processing Systems, Vol. 2, pp. 211{217 San Francisco, CA.
Morgan Kaufmann.
Cardie, C. (1993). Using decision trees to improve case-based learning. In Proceedings of
the Tenth International Conference on Machine Learning, pp. 17{24 San Francisco,
CA. Morgan Kaufmann.
Duda, R. O., Machanik, J. W., & Singleton, R. C. (1963). Function modeling experiments.
Tech. rep. 3605, Stanford Research Institute.
Freund, Y. (1992). An improved boosting algorithm and its implications on learning complexity. In Proc. 5th Annu. Workshop on Comput. Learning Theory, pp. 391{398.
ACM Press, New York, NY.
Hampshire II, J. B., & Waibel, A. H. (1990). A novel objective function for improved
phoneme recognition using time-delay neural networks. IEEE Transactions on Neural
Networks, 1 (2), 216{228.
Hastie, T., Tibshirani, R., & Buja, A. (In Press). Flexible discriminant analysis by optimal
scoring. Journal of the American Statistical Association.
Hinton, G. (1989). Connectionist learning procedures. Articial Intelligence, 40, 185{234.
Hocquenghem, A. (1959). Codes corecteurs d'erreurs. Chires, 2, 147{156.
284

Error-Correcting Output Codes
Kong, E. B., & Dietterich, T. G. (1995). Why error-correcting output coding works with
decision trees. Tech. rep., Department of Computer Science, Oregon State University,
Corvallis, OR.
Lang, K. J., Hinton, G. E., & Waibel, A. (1990). A time-delay neural network architecture
for isolated word recognition. Neural Networks, 3 (1), 23{43.
LeCun, Y., Boser, B., Denker, J. S., Henderson, B., Howard, R. E., Hubbard, W., & Jackel,
L. D. (1989). Backpropagation applied to handwritten zip code recognition. Neural
Computation, 1 (4), 541{551.
Murphy, P., & Aha, D. (1994). UCI repository of machine learning databases [machinereadable data repository]. Tech. rep., University of California, Irvine.
Natarajan, B. K. (1991). Machine Learning: A Theoretical Approach. Morgan Kaufmann,
San Mateo, CA.
Nilsson, N. J. (1965). Learning Machines. McGraw-Hill, New York.
Perrone, M. P., & Cooper, L. N. (1993). When networks disagree: Ensemble methods for
hybrid neural networks. In Mammone, R. J. (Ed.), Neural networks for speech and
image processing. Chapman and Hall.
Peterson, W. W., & Weldon, Jr., E. J. (1972). Error-Correcting Codes. MIT Press, Cambridge, MA.
Quinlan, J. R. (1993). C4.5: Programs for Empirical Learning. Morgan Kaufmann, San
Francisco, CA.
Richard, M. D., & Lippmann, R. P. (1991). Neural network classiers estimate bayesian a
posteriori probabilities. Neural Computation, 3 (4), 461{483.
Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and
organization in the brain. Psychological Review, 65 (6), 386{408.
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning internal representations by error propagation. In Parallel Distributed Processing { Explorations in the
Microstructure of Cognition, chap. 8, pp. 318{362. MIT Press.
Schapire, R. E. (1990). The strength of weak learnability. Machine Learning, 5 (2), 197{227.
Sejnowski, T. J., & Rosenberg, C. R. (1987). Parallel networks that learn to pronounce
english text. Journal of Complex Systems, 1 (1), 145{168.
Selman, B., Levesque, H., & Mitchell, D. (1992). A new method for solving hard satisability
problems. In Proceedings of AAAI-92, pp. 440{446. AAAI/MIT Press.
Snedecor, G. W., & Cochran, W. G. (1989). Statistical Methods. Iowa State University
Press, Ames, IA. Eighth Edition.
Valiant, L. G. (1984). A theory of the learnable. Commun. ACM, 27 (11), 1134{1142.
285

Dietterich & Bakiri
Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., & Lang, K. (1989). Phoneme recognition using time-delay networks. IEEE Transactions on Acoustics, Speech, and Signal
Processing, 37 (3), 328{339.
Weigend, A. (1993). Measuring the eective number of dimensions during backpropagation
training. In Proceedings of the 1993 Connectionist Models Summer School, pp. 335{
342. Morgan Kaufmann, San Francisco, CA.
Wilkinson, R. A., Geist, J., Janet, S., et al. (1992). The rst census optical character recognition systems conference. Tech. rep. NISTIR 4912, National Institute of Standards
and Technology.

286

	
		
	

	!"$#%$&('*),+--.0/!+++21+3'-

435678:9;<-.=>5
#&	8?-0;-.

@BADCFE0GHAIAIJALKNMPOIJE0QSRTQUGWVXAIJALK

Y?Z:[]\_^a`UY_bac2deb
w:xy{z}|e~ยยยtยยย}ยยยยzยยยย~yH3ยo~
ย ยย3ย23ยFยยยย,ยย<ย<ย~ย
ย ย3|	ย0ยยยย3yWย"ยยยย	3ย

f3gahjiag_kmlafon paqarsn gtlun s,v

ยยยยยยZย^ยย{^"ย?ยย^ยย?Zยย_`,ยยก

ยข(ยฃ"f3paยคยฅ_kWs,ยค_n ยฅaยค}l!paยฆas%ยฃยฆ:n gtlun s,v

ยย!ยง|eย~ยยยยaยจ?ยoยฉยย}ยยยoยฉยชยยย}ยงยฌยซยญยยย}ยยยฉ	yยฎ3ยo~
ยฏ ยยฐoยย%xย
ย ยยย ยฑ0ยยชยฒ	ยณjยดeยด	ยดยยยยย2ยeย

ยตยยถยธยทeยนยบยป_ยผ!ยน
ยฝ7ยพยฟรรยร!รร!รรuยฟรรoรรรรoร!รรร:ร*ร%รรรรร3ร:รeรร7รร	รuร_ร,รรรรยฟยรรรยธร7ยพยฟรรรรXรร0รร$รยฟรรรUร7ยพรร$รยธรรยฎรร	ร3รjร_ยฟรร_ร	ยฟรร	ร3ร
รยฌรeรjรร!รรรรeร$ยพยฟรรร	รUยฟรรIรรรร3รjรoยฟรรรeรรรร3รjร?ร7ยพรeรรUร}ร3ยพ}รยรoยฟยรeร7ยฟรร7ร	รรรรยฎร}รยรรยฟ,รยรยรรรยฌรoรรร7รรรรยฌรยพรmรยร	รรoรยร
รรรยรยยฟยรรรร$รยธรยพรรmร$ร2ร	ร<ร2รรรยฟยรรยฟรรรยฎรรuร	รยรยฟรร	รร7รรรรยรoรรoรรยฟรรeรLรรรรรรร$รรรยรUรรร(ร$ยพรรรรยพ}รยร7ร%รeรยธรยฌรรยรรeร
ร}รยรรรรรaรร,รร$ร:รยรยร_ร,รรรยฟรรรUร7ยพยฟรรรรยธรaรยรรรรยฟยรรรmร$รjร$รรรรร0รoรUรร,รรรร3รรรร!รยธรรรรร$รรoรร0รIรยร}รยฌร	ร3รยฟรขรก}ร0รยชยฟยรรร
ร$ร0รร$ร	ร}รยรรรรWรยฟรรรรeรXรฃ*รร:ร3ร	รรยรtร<ร	รรยฟรรรยฌรรยญรยรรร	ร	ร$ยฟรรยพรรยฟร3รรรรรยฌร7ยฟรรยพยญรยฌรร,รยร"รรรeรรรคร%ร	ร7ร$ยฟยรรรรรรWรร,รรร$รร
รยรยรรoร$รรรรร7ยฟยรยธรยรร}รยรรรoรรรรรยฟยรoร$ร2ร	ร<ร2รรรรยรeร
รรร:รรรรยพ}รยรยฟรรฅร7รยพรรยธรรeรรร7รร"รรฆ!รรยยฟรรร7รรรรยรoรรoรรยฟรรeรmรร$รรรรร$รร0รaรยร}รยฌรยพรรยร$ยพ}รรยรoยฟยรยฌรรรeรร_ร}รยรรรรร
รรยรรรยรรยพรรuร	รร$ยฟรขรกtรรร$ยฟรร	รยฌรง%รร$ร0รจร0ร<รยฟรร	ร}รฉร}รร$ร"รรรรชร}ร:ร3รรร$ยฟรร0รUร	รรรaยฟยรWรยร(ร<รซLรยฟรรรoรยรรรรeร	รยฟรร$ยพรรยฟ,รรรLรรรรรร0ร
รฌuรญรรฎoรฏ ยนยบรฐ7รฑ{รฒยธยผ!ยนรณรฐ รฏ
รดรตaรถaรถ"รท!รธรนรรบรทรตรรป"รผ"รฝรพรบรทรรตaรฟ0รธรน,รผ	ยรท
Iรถ,รน!รบ}รฟรผรพรผaรทยญรฟ3รนยรทรนรทรรผรธรท !"aรฟรทรต$#%&
รบรทรต'aรนรฟรนรรทรรฟ(aรท)รท*#รน+รรทรต$,*-7รทรรต

รฝ!รท.!0/รนรรธรท
รครน21}รผaรท,รน	รฝ#รนรธ3รท*aรน

รถ"รท!รธรธ4,รน

รทรต$5ยรท%
Iรน	รธ

รทยธรบรทรรตaรฟ67,รทรผ"รธ98รน,:#!,;$#รฟ50/%<7,รทรผ2Uรทรฟ1aรธ รธรชรต"รธรต!0>=5, ?รชรท(รน+/รรนรฟ"รธรน+/รนรฟ0@!รธ4AB!oรฟ5รนรฟรธ7รธWรท
รบรทรตaรฟยธรธ<รตaรฟรฟรทรตaรผ"รฝ%,รผ$#oรฟรนUรตaรผ$1!รผaรทCรชรผ28รน,:#!,;aรน
DoรถIรทaรน!รบ}รฟรผ!รทรฟ aรนรฟ3รนWรบรทรตoรฟรนE,รผ	<F=5,G-7รทรตaรฟ

#รท&C%รธ รทรครถ;ยรผ รบรรทรตaรฟ  eรบยญรทรต$ยฌรทaรนรฟรน3 รนHรนยรฟรผ,รผ$#รรนรผaรทรต$#2I0รธ"รทรต$ยฌรบรทรตaรฟยฌรธรตaรฟรฟรทรตaรผ"รฝ%,รผ$#}รธ
รทIรนรผ!Jรน3!#รท&CK,
 รนM"
 รท
/ รนรรนNรค

 รถรน9% รธ6 รธรถ"รนB0O+
 รธรนรรทa
 รนP รท,รทCQ รผ$#R
# รนรผaรนรฟ5CXรธรน+7,รผ$#!SETยฌรผUV%WX!Y[Z รทรถยรนรฟ\
L a
7,รผ$#3,รผ.oรผ*WX^]_a`7bX!cAWX^Y>%รธรฟ3รบ,รผ$#AรทBรน+/รรนMA#/รนรผR#รท&0I,T UรนB รถ"รท,รผ,รผ7
IรนaรนM#รรนรผ&
%รธd รผRร
 รธรถยรนBย รปeยฎ
 รธ5 รน, L a
 รนfJ
# รนรผ&O+ย
 รผ"
 รน รต,รบยญรฝ!รน	รธย
 รฟ<ย
 รน	รฝR}
 รบร
 รฝ!รนBรธgรทรรผรรถaรฟรทhยรน	รฝ!รตaรฟ3รน&;Bรรฝ!รน+รนรฟ\

A, รผaรน	รธa
 รนยฎรผaรนh"7, รทรผ2 รทA"
 รน35
1 รนรผ$
 รธ3 รตaรผ!7, รทรผรรทG0 รธ% รธ7 รทรรฟรบยญรทJX
 รธ753 รน	รธ+,Gm
T รผรรนรผ/, รฟรทรรผ$I

 รนรผ&Q รธ
5ร
1 รนรผ(3 รท6!0
/ รนWรธรทรค

 รน9iBW+jU]_ab`}
 รฝ!รน+ รนรฟ
H รผ, รผ$#!J รทรรฟ7รน+
/ รนรฟรบLรธ5 รนo
 รผ"รฝ7, รทรผA5J
1 รนรผA}
 รบ3a
 รน 
# รนรผa
 รน
รผaรน$H
 รธ7 รนMB! O"รนรรฟรนBa
 รน	รฝ, Zk% รธ"#
/ รนรผlยญ
 รธรน+ยฌ
 รท7
 รถ"รท}รธ3รธ4รน	"
 รน+!0/, รทรฟยรธยฌรทOa
 รนรรนรผ/% รฟ3รทรผ$I

 รนรผ
รทรผ,รบ]รทรผaรนรครทm Bl% รธ9a
 รนn% รต!0O"รน+!0/%, รทรฟรรท a
 รน รธรถ"รนBร รปe รนรผ/, รฟรทรผ$รค

 รนรผ&,	]
o รนยญรธe
 รบ*!PZp!
 รธ
รถ!o
 รฟ;C!,รผ รทรรฟ
D7, รทรผ รทรผRa
 รน""
 รน+!0/, รทรฟรชรท@a
 รนยฌรนรผ&/%, รฟรทรผ$I

 รนรผ,GZDรขq รธ
# รท&C!%รธE#
/ รนรผ
 รธOร
 รธรต$"
 รธ<รน+(
 รทJ@a
 รน
รธ75J
 รน	รธ+,U
r รนB, รผ$#no
 รผ!รบ รทsBa
 รน	รธรนรรธ753 รน	รธE% รธ ย
 รทรผ"รธ4% รฝ!รนรฟรน	รฝ*I
 รธรต!+ย
 รน	รธ3รธ+,
ZRq รธ3
# รท&0%รธ+{
 รทMย รทรตaรฟ0รธรน{
 รผaรทI
 รผaรนย รน	รธรธBo
 รฟ7,รบt, รน+/,รนu"
De
 รบย
 รนa
 รน+ร
 รธรน2!3 รทรฟIรทรผaรนรรท
aรนรรถ"รท!รธรธ4,รน"รน+!0/%รทรรฟ0รธยฌรทOaรนรรนรผ/,รฟรทรผ$
รครนรผ&BaรนรฟรนIรฝ!รท}รน	รธยฌรผaรทยฌรนรธMยญรธรนvรรตaรนรผ!ยรนIรทm%7,รทรผ"รธ"!
Uรทรต%รฝw,รน
 รฝxZy รทlย
 รธรต!+ย รน	รธรธF,{รพ
z รทรฟ3รนรทCร
/ รนรฟ?
 รน+
/ รนรผ'6 รทรฟIรน+
/ รนรฟ3รบ]รถ"รท}รธ3รธ4รน]รธรถยรนBร รปe2"
 รน+!0/%, รทรฟยญรทJ6a
 รน
รนรผ/, รฟรทรผ$รค

 รนรผ&"a
 รนรฟรนรรน รธ0 รธ6 รธรนร
v รตaรนรผ!ย รนรรทm7, รทรผ"รธ!,รน
 รฝaรธ"Z| รท	0 รธ
# รท&0IN<"
De
 รบรรธ77O"รนAa
 รน
+
 รธรน!JZ}+o
 รผaรผaรทOB, รน+
/ รนE<ย รธ
# รท0I,s
~ รทรฟ?รนNรค

 รถรนย
 รทรรผ"รธ4% รฝ!รนรฟmo
 รผรรนรผ&/%, รฟรทรผ$I

 รนรผA7(
 รทรชรถ"รท}รธรธg<,รน
"รน+!0/%, รทรฟ0รธM + o
 รผ"รฝl ' ,AยI3
De
 รบ"
 รนRa
 รน+
 รธรน	!Pa
 รนIรทรรผรบ'7, รทรผยย!",รน
 รฝaรธ3Zย รทR0 รธ9
# รท&0
 +--3.:ยa"$##$ย	8:	8EยU3Oย_6ยธOยร5
#&	#Iย	

o%&##>ย8ย
ยยฎ

ยย$ยย$ย*ย$ย$ยxยOย%ย$ย$ยย$ย!ย$ยย!ย

ย $
ย ย+ยยยย$ยMย+ยยยยยก%ย$ยขย+ย&ยยฃ>ยกยคยค<ยกย6ยฅ"ยฆ"ยงยจยคยยฉยช$ยฅ"ยยกDยฉยฅย5ยฉยย"ยฃ>ยยกยขยซย ยยยฌยยยย$ยMยญ%ยก&ยฉ0ยคยยฅย$ยกยfยฉ%ยฌยยย+ยCยฉJยฎยค<ย3ยยฃ
ยย$ยPย+ยยยยยก%ย$ยขย+ย&ย"ยฎ!ย+ย!ยฉ0ยยยฅยฉยฌ+ยฌยกย5ยช%ยย$ยญRยยกยฆ6ยฏยฐ
ยฑ ย+ยย+ยยBย$ยยค<ยยฅBยฅ+ยจย+ย%ย+ยPยยMยย$ยOยฌ+ยฉ%ยฅ7ยยยMยยย;ยฌBยMยฒRยณยดยฅ^ยต&ย$ยกยยคยยชยญยEยกยฃ$ยย$ยย+ย&ย%ยยยกย$ยขย+ยย[ย;ยฅ[ย$ยกย@ยฌยก%ยขยถยคย+ยยยจ
ยย6ยขRยฉยท2ยฅ7ยกยข	ย+ย7ยยขยยฅยฎ!ย3ยถ!ยก&ยฅBยฅ4ยยฎยค<ยHยฃยธยกยยฒ|ยยกDยฉ%ยฌยยย+ยย9ย<ยยฅ ยญยกยฉ0ยคIยฐยนยบ$ยถ$ยถ!ยก&ยฅย(ยBย!ยฉย ยยMยฌย!ยฉJย$ยญยMยย$ยMยฉJยฎ!ยกยย
ยยป!ยฉยขยถยคย.ยฅ7ยก*ยย!ยฉJย(ยBย$ย+ยย2ยยป&ยยฅ7ยยฅยฉยtยฉยฌย7ยยกยtยผRยBย!ยฉยยจ[ยยฃMย5ยฉยตย+ย'ยฎ&ยท'ยฒDยจsยค<ยยฉ%ยช$ยฅ3ยยtยย$ยยฌ+ยฉยฅย2ยย!ยฉยAยย$ย
ย+ยยยยยกย$ยข	ย+ย&ยRยฎ!ย+ย!ยฉ0ยยยฅRยฉยฌ+ยฌยกย5ยช%ยย$ยญ'ยยกยฆ ยง ยจยBยกยฉlยฅ7ย5ยฉJยย2ย ยยยฌยยฝยยฅRยกยฎ!ยฅ7ย+ยยยฉยฎยคยทtยช%ยยฟยพย+ยย+ยย	ยฃยธยยก%ยขรยย$ย
ยฅ7ย5ยฉJยย6ยBย!ยฉย ยยยฅ7ยบยคย5ยฅmยฃยธยยก%ยขรยBย$ยMยฅยฉยขยMยฉยฌยย<ยก%ย*รgยผ0รOย5ยฉยต%ย+ยDย ย$ย+ยยยย$ยPย+ยยยยยกย$ยข	ย+ย&ย ยฎ^ย+ย!ยฉ0ยยยฅ ยฉยฌ+ยฌยกยยช%ย<ย$ยญ
ยยกยฆ"ยฏยฐAร4ยยฉยช$ยช%ยย7ยยกยยจGยฅ7ยบ$ยถ$ยถ!ยก&ยฅยยย!ยฉยPยย$ย+ยBยยยป&ยยฅ7ยยฅPยฉยยฉยฌย7ยยกยlรยยย!ยฉยยจ^ยย'ยฎ!ยกยยlยฌ+ยฉยฅยยฅ+ยจยย+ย%ย+ย5ยฅ7ยยฅPยผยณยดยฅ
ยยพยยฌย5ยฅ+ยฐGรIย	ยยยยฅOยฌ+ยฉยฅ7ยยจ%ยฒรยฌ+ยฉยยฉ0ยคย ยฉ0ยทhยฅยฉยฌBยยย+ยยยย5ยฅdยญยก&ยฉ0ยคIยจยฎยทPยฃ>ยกยคยคยกยยย$ยญMยย$ยยฃ>ยกยคยคยกยยย$ยญPรeรรร^รร!ย5ยฅ7ยEย5ยฉยตย
ยผยจ!ยฉย!ยชยจ^ยฉยฌ+ยฌยกย5ยช%ยย$ยญRยยกยย$ยPยยยฅยบยค<ยย<ย$ยญ2ยฅ7ย5ยฉJยยMยชยยฌBยยชย(ยย$ย+ยย$ย+ย6ยBย$ยPย+ย&ย%ยยยกย$ยขย+ยย6ยฎ^ย+ย!ยฉ0ยยยฅ ยฉยฌ+ยฌยกยยช%ย<ย$ยญ
ยยกMยฆ ยง ยกยOยฉยฌ+ยฌยก%ย5ยช%ยย$ยญ(ยBยกMยฆ6ยฏ0ยฐรย$ย+ยยจย5ยฉยตย ยฉยฌย7ยยกยRรMยBยกPยญย+ยOยฎ!ยฉยฌBยตAยยกPยย$ยยยยย7ยยฉ0ยค@ยฅ7ย5ยฉJยยยฐ@ร[ยย!ยฉ0ยคยคยทยจ!ยฉยถ$ยถยคยท
ยย$ยMยฉยถ$ยถยคยยฌ+ยฉยฎยคยDยฅ7ยร%ยบ$ย+ย!ยฌยMยกJยฃdยฉยฌยย<ยก%ย!ยฅEยฃยธยก%ย ยยยย$ย+ย6ยBย$ยMยฆ ยง ยกยยย$ยAยฆ6ยฏ"ยฌ+ยฉยฅ7ยยฐ
ร4ยยญย+ย$ย+ย5ยฉCยคKยจ%ยฒ}ยขDยฉ0ยทMยถ^ย+ย7ยฃ>ยกยยข|ยฅ7ยกยข	ย6ยฉยฌย7ยยกย!ยฅGยย!ยฉJยยยยชยบ!ยฌย6ยBย$ย ย&ยบ$ยขAยฎ^ย+ยOยกยฃยถ!ยก&ยฅยฅgย<ยฎยคย"ยฎ!ย+ย!ยฉ0ย%ย<ยก%ย5ยฅยกยฃ
ยย$ยย+ยยยยยก%ย$ยขย+ย&ยEร>ยIยฐ:ยยฐยจFยย!ยฌยยยฉยฅ7ยEยย$ยOยตย$ยกยยคยยชยญยยย!ยฉย[ยฒtย!ยฉยฅยก%ยPยย$ยOย+ยยยยยก%ย$ยขย+ย&ยFร5ยจยยยยฟยคย ยฉ0ยยกยยช%ยย$ยญ
ยฉยฌย7ยยกย!ยฅ9ยย!ยฉยPยขDยฉ0ยทRยคยยฉยชwยยกRยฃIยฉ0ยยค<ยบ$ยBยAยย{ยฉยยทยกJยฃmยย$ยDยฅย7ยยคยฟยคยถ!ยก&ยฅBยฅ4ยยฎยค<ยRยฎ!ย+ย!ยฉ0ย%ยยกย5ยฅPยกJยฃmยย$ยย+ยย%ย<ยBยกย$ยขย+ยยยจ
ยฉยฌ+ยฌยกยยช%ย<ย$ยญ'ยยก*ยฒDยณรยฅMยตย$ยกยยคยยชยญยยฐรยฒรยขDยฉ0ยทlย+ยย+ยยยบ!ยฉ0ยคยคยท'ยคยยฉยยรย+ย$ยกยบ$ยญยรยฉJยฎ!ยกยบ$ยยBย$ย2ยฎ!ย+ย!ยฉ0ยยยกยRยกยฃPยย$ย
ย+ยยยยยกย$ยข	ย+ย&ย"ยยกDยฌBย$ยก&ยกยฅ7ยMยย$ยAยฉยถ$ยถยคยยฌ+ยฉยฎยคยDยฉยฌย7ยยกยยยย!ยฉยยคยยฉยช$ยฅยยกDยฅ7ยบ!ยฌ+ยฌยยฅBยฅ+ยฐ รยยยฅ"ยถ$ยยก$ยฌยยฅยฅยยฅยยยฃ>ย+ยยยยช
ยยกDยฉ%ยฅ รรรร!ร^รรรรGร&รaรรMรรรรร!รaร&รยฐ
รQยย;ยฅEยถ!ยฉยถ^ย+ย6ยช%ยยฅยฌยบ!ยฅยฅ7ยยฅQยย$ย"ยฃยธยยฉยขย+ยยกยยต3ยกยฃsรGยคยฉย$ยยย$ยญRย ยยยคยMรยยฉยยยย$ยญ!ยจ$ยยยยฟยคยAยฌยกย!ยฌย+ย&ยBย5ยฉย7ยย$ยญAยกย
ยย$ยร>ร7รร0ร>ร&ร+รaรรร>รกAยกJยฃOร!ย!ยช%ยย$ยญยฉ2ยฅยฉยย;ยฅgยฃKยฉยฌยBยกยยทยถยคยฉยรร>ยIยฐ:ยยฐยจยฉยย ยฉยทยยยกยฉยฌBยยย+ยยDยBย$ยยญยก&ยฉ0ยคยย+ยญ&ยฉJย5ยช%ยคยยฅยฅPยกยฃ
ย ยยยฌBยDยถ^ยก&ยฅยฅ4ยยฎยคยPยฎ!ย+ย!ยฉ0ย%ย<ยก%ยOยกยฃยย$ย ย+ยย%ย<ยBยกย$ยขย+ยยยยฅEยย$ย6ยฉยฌยยบ!ยฉCยค!ยกย$ยร5ยจยก%ยmยฌBย$ยยฌยต%ยย$ยญ(ยBย!ยฉยOยฉPยญยยย+ย	ยถยค;ยฉJย
ยยฅ6ยฅBยฉย7ยยฅ4ยฃIยฉยฌยยกยยทยฐรย$ย3ย$ยยปhยPยฅ7ยยฌย7ยยกย*ยชยร!ย$ยยฅPยฉAยฎ!ยฉยฅ4ยยฌ"ยฃ>ย5ยฉยข	ย+ยmยก%ยยตDยย$ย+ยย(รsยค;ยฉJย$ยย<ย$ยญ2ยยยยฟยคยรยยฉยBยย<ย$ยญ
ยฌ+ยฉยยยฎ!ย(ยฅยยบ!ยช%ยยยชยฐ รIยwยนยยฌย7ยยกยlรขAยย(ยช%ยยฅยฌยบ!ยฅยฅยย$ยAยฌยกยขยถ$ยบ$ย5ยฉยย<ยก%ย!ยฉ0ยคsยฉยฅ7ยถ^ยยฌย5ยฅยmยMยฅยยบ!ยชยทRยย.ยBยย;ยฅยถ!ยฉยถ^ย+ยยฐ
รIย	ยถ!ยฉยย7ยยฌยบยคยฉยยจยย ยช%ย;ยฅย7ยย$ยญยบยยฅ7ย2ยฎ!ย+ย7ยย+ย+ยยย$ยย+ยยขDยฉ0ยยย7ยท&ยถ^ยยฅยกยฃยย+ยถ$ยยยฅ7ย+ยย5ยฉยย<ยก%ยยจ&ยฉย!ยชAยฎ^ย+ย7ยmย+ย+ย	ยย$ยย+ย
ยขDยฉ0ยย*ยฌยกยขยถ$ยบ$ย5ยฉยย<ยก%ย!ยฉ0ยคsยฌ+ยฉยย+ยญ%ยกย7ยยยฅ+ยฐร4ยยนยยฌยย<ยก%ย!ยฅ รฃรค$รฅAยยMยฌBยค;ยฉ%ยฅยฅ4ยยฃยธยทรsยคยฉย$ยยย$ยญ2ย ยยยค<ยรยยฉJยยยย$ยญ2ยฎ!ยฉยฅ7ยยช
ยกยยBย$ยยฅ7ยfยฌยก%ยขยถ$ยบ$ย5ยฉย7ยยกย!ยฉCยครฆยฌ+ยฉยย+ยญ%ยกย7ยยยฅOยฉย!ยชHยย+ยถ$ยยยฅ7ย+ยย5ยฉย7ยยกยRย7ยท&ยถ^ยยฅ+ยฐGรIยยยนยยฌย7ยยกย2รงPยย6ยช%ยยฅยฌยบ!ยฅยฅ ยฅย+ยย+ย5ยฉ0ยค
ยยป$ยย+ย!ยฅ4ยยกย!ยฅPยBยกDยกยบ$ยPยฎ!ยฉ%ยฅ4ยยฌ3ยฃยธยยฉยขย+ยยกยยตยฐ"รIย'ยนยยฌย7ยยกยxรจยยAยถ$ยบ$ยPยกยบ$ย"ยฃ>ย5ยฉยขย+ยยกยยต2ยฉย!ยช*ยBยยฅ7ยบยคย5ยฅยยlยย$ย
ยถ!ย+ยยฅ7ยถ!ยยฌย7ยยยAยกยฃยยยคยฉยยยชRยยกยBยตeยฐ
รฉ@รชรซRรฌOรญ'รฎ2รฏรฐCรฑ4รฒรณรด%รฏeรตรถรญ$รทDรธรด%รน
รบ 
ยก ย!ยฅgย;ยชย+ยRยย$ยยยฃยธยกยคยคยกยยย$ยญxยยปNยฉJยขยถยคยยฅRย ยยยฌยรปย!ยฉ0ยย2ยขยกยย<ยยฉยยยชยฝยกยบ$ยnยฅยยบ!ยชยทยฐ}รย$ย2ร!ยยฅ7ยDยยป!ยฉยขยถยคยยย;ยฅ
ยง
ย5ยฉยต%ย+ยxยฃ>ยยกยขรผยฉwยขยยช%ยยฌ+ยฉ0ยค(ยชยกยขDยฉCย<ยยฐ รบ ยก%ย!ยฅ4ยยชย+ยnยฉwยย5ยฉยบ$ยขRยฉรฝIยฌ+ยฉยย*ยฅ7ยทhยฅยย+ยขยจย ย$ย+ยยยยย$ย+ยยlยฉยยยยขDยฉยยท
ยกยฎ!ยฅ7ย+ยBยCยฉย7ยยกย!ยฅRยBย!ยฉยnยฌ+ยฉJยรยฎ^ย*ยขDยฉยชยยก%ยรพยฉwยถ!ยฉย7ยย+ยยยณยดยฅ2ยฅ7ยยฉยยยฐ}รฟPยฌย7ยยกย!ยฅRย5ยฉJยตย+ยtยฎยทtยย$ย*ยชยกhยฌยBยกย2ยขDยฉ0ยท
ยฌBย!ยฉย$ยญยRยย$ยยฅ7ยยกยฎ!ยฅย+ยยCยฉยย<ยก%ย!ยฅ+ยฐ.รNยกยMยยป!ยฉยขยถยคยยจยBย$ยDยชยกhยฌยBยกยMยขDยฉ0ยท*ยฎ!ย2ยฉยฎยคย2ยยกยยกยฎ!ยฅ7ย+ยยย	ย ย$ย+ยย$ย+ยAยย$ย
ยถ!ยฉย7ยย+ยยยณยดยฅ[ยฎยคยก&ยก$ยช3ยถ$ยยยฅยฅ7ยบ$ยยEยยฅ[ยย<ยญ%ย(ยก%ยยคยกยMยจยฉย!ยช3ย ย$ย+ยย$ย+ยGยย$ยOยถ!ยฉยย<ย+ยยย!ยฉยฅ[ยยยญยAยกยยคยกยยย+ยข	ยถ!ย+ย5ยฉยBยบ$ยยยฐ
ยฉยฅ7ยยช	ยกยยย$ย"ยกยฎ!ยฅ7ย+ยBยCยฉย7ยยกย!ยฅdยขDยฉยชยยจยย$ย6ยชยก$ยฌยยกยEยขDยฉ0ยทAย$ย+ยยชยยก3ย5ยฉยตย"ยฉยDยฉยฌย7ยยกยยจย ยยยฌBยDยขRยฉยท3ยยRยยบ$ยย
ยคยยฉยชRยยกAย$ย+ยรปยกยฎ!ยฅ7ย+ยยยฉย7ยยกย!ยฅ+ยฐ
ยฉ%ยฅ7ยยชRยกยRยย$ยยฅ7ยPย$ย+ยรปยกยฎ!ยฅ7ย+ยยยฉย7ยยกย!ยฅ+ยจNยย$ยfยชยก$ยฌยยกยยขDยฉ0ยทย$ย+ยยชRยBยกHยฌBย$ยกยก&ยฅ7ย
ยฉ2ยฅ7ยบ$ยฎ!ยฅ7ยร%ยบ$ย+ยยMยฉยฌย7ยยกยยจ@ยฉJย!ยช.ยฅยก2ยกยยฐMรย$ย+ยย3ยยฅMยฉ	ยคยยฅ7ยPยกยฃยถ!ยก&ยฅBยฅ4ยยฎยค<ย	ย
ย gยบ$ยย<ยยฅ9ยย!ยฉยPยย$ยAยถ!ยฉยย<ย+ยยPยขAยยญยย
ยฅ7ยบยพย+ยยฃยธยยก%ยขยจ$ยฎ$ยบ$ยPยย$ย3ยยปNยฉยฌย"ย!ยฉยยบ$ยBย(ยกJยฃsยย$ยยฉยฌยBยบ!ยฉ0ยคย
ย gยบ$ยBยท2ยยฅ"ย$ยกย"ยต&ย$ยกย ย*ยฉยถ$ย7ยยกย7ยIยฐ ยฑ ยฉJยยบ$ย5ยฉ0ยคยคยทยจยย$ย
ยยพยยฌย5ยฅ"ยกยฃยย$ยDยฉยฌย7ยยกยwย5ยฉยตย+ย*ยฎยทยย$ยDยชยก$ยฌยยกยPยขRยฉยทยชย+ยถ^ย+ย!ยช'ยกย*ยย$ยDยฉยฌยBยบ!ยฉ0ยค@ย
ย 7ยบ$ยยทlยกยฃยย$ยAยถ!ยฉย7ยย+ยยยฐ
รย$ย6ยชยก$ยฌยยกยEย$ย+ยยช$ยฅEยยกMยชย+ยยยฅ7ยMยฉ"ยถยคยฉยRยย!ยฉยEยยยคยคeย5ยฉยต%ย ยย$ย ยถ!ยฉยย<ย+ยยยฃ>ยยกยข ยยยฅdยยยย7ยยฉ0ยค@ยกยฎ!ยฅ7ย+ยBยCยฉยฎยคยPยฅ7ย5ยฉยย
ยยกMยฉ9ยญยก&ยฉ0ยค!ยฅ7ย5ยฉยBยPร>ยIยฐ:ยยฐยจ
ยฉ ยถ$ย&ยท$ยฅ4ยยฌ+ยฉ0ยคยคยทDยฅ7ย5ยฉJยฎยค<ยPยฅ7ย5ยฉJย
ย ร5ยฐรย$ย"ยชยกhยฌยยก%ยmยฌ+ยฉยยก%ยฎ!ยฅ7ย+ยยย ยBย$ย ยถ!ยฉย7ยย+ยยmยฉยยยฉยฌBย

	
 "!$#$#&%'()**()'(	'+,-.0/!21'31141567!2()8941*!2:0;<(6#$#2(.!2,=
1'>7!2(	@?A6/"!$#$#B%C!2/,7!$DE/+.!2F4(	7!G#H!2,I'3#41567!2()J17!2K>	
LL)M

NOQPSRTHOHOHUVOWYXZHU<R[8\[@TH]HOHUVOW

^_a`cbedf`<b d`<g h@iejbkmlchjanbJoj@p	d	qCjr_@sHdCd6tHhujvp	dsj,le`cbwsHnxzy`5{$h@{<idtHhj@p	dsjElhbA|v`<n_@bHg hbedCrhtj,|v`c_vn}
kesHn`<bH~:dtHhh	ยHhp	sHd`<_@bย_oIdtHhJ^el9jb{+ยยhbp	h@i.ยยhJ~@hdj bjd6sHn)j,lIqย`<dsjd`<_@bYยtHhnh ย&lVjabHbe`cbH~:ยยte`Gl<h
ย hjn6be`cbH~z`9qbHhp	hq6qjnx@{
ย sHnยqยhp	_@bkยh	ยยjgz^elchย`Vqd	jย@hbยoยn6_@gยjQd6n)jbq^ย_@nd)jdย`c_vbยke_@g-jE`cb{ ย hdยยยยยyย*ย)ยz}rhยj
kv`<nhp	dhkQ~@n	j^HtiCยtHhnh:d6tHh-|@hn6d`9p	hq kehbH_@dhzl<_Bpjdย`c_vbqJ`<bยjยtH_eqd`ยl<hยhbe|@`<n_@bHgzhbAd{ยยutHh:hke~@hq
kehbH_@dh:qj,ohn_@sHdhqmon_@gย_vbHhlc_Hpjd`<_@bยd6_ยjbH_@d6tHhniCdtjdjnhd)jย@hbQd_8rh:_@bHhยกยj,xQy5dยย_vยกยuj,x
n_@sHd6hqIjnh0kehqp	n`<rhk8j@qIjm^j,`<nย_ao"_vbHhยกยuj,xJn_@sHdhq	}	{ ย bHhยกยj,xn_@sHdhq&`<b:dte`9qC^jndย`Vp	sel9jn0ke_@g-j,`<b
_Hpp	sHnยขj@qยj nhqยselcd0_oCdtHhยฃqยdnsp	dsHnh_ao&dtHhJhbA|v`<n_@bHg hbedjbk:_oCdtHhJ|@hte`9p6lchsqhk8rex-d6tHhยฃj~@hbed{
ยค bp	hnd)j,`<bed>xQ`<bยฅdte`9qzdn)jbq^ย_@nd)jad`<_@bยke_@g-j,`<bยฆjn`9qhqยงon_@gยจdtHhยฉojvp	d-dtjad dtHhnhยฉ`Vqยง`<bp	_@g ^el<hdh
`<beoย_vng-jd`<_@bยjar_@sHd0dtHhhbkeยก^_a`cbed_oIq_@g hn6_@sHdhq*_@nย`c~a`cbjad`<bH~ on_@gยชq_@gzh^jnd`9p	sel9jnยlc_Hpjd`<_@bq
y`5{$h@{<iยซdtHh g-ja^Y`9q^jndย`VjElGl<xยsHbHยebH_ยb.}){zยฌ5bยq_@g h pj@qhqmdte`9qm`cbp	_vg ^el<hdh `<beo_@ng-jad`<_@bยญp	_@bp	hnbq
_@bel<xยjqg-jElGlCbesHgยฃrยhnยฎ_aoClc_Hpjd`<_@bqjabk:n_@sHd6hqยtHhn6h+dtHhbesHgยฃrยhnยฎ_aoI^_Aqq>`crel<h hbkeยก^ย_`<bAd	q0_oยj
n_@sHd6h*`9qIj,l9q_qg-j,lยl5{2ยฏยฐ0b-j~@hbedIg _|v`<bH~ยฃj,l<_@bH~JdtHhqhn_vsHdhqCยebH_ยยฎqfdtHh0^_Aqq>`crel<hj,l<dhnbjdย`c|vhqCo_@n
dtHh+qd6nsp	dsHnh0_oSdtHhhbe|v`cn6_@bHg hbedยฎjbkpjb`9kehbAd`ยox:dtHh0l<_Bpjad`<_@bqยฑ`<dยฎjnnย`c|vhqยjad{Cย*tHh_@rHwhp	d`<|@h
_oCdtHh+j~@hbed*`9q*d_znhj@p6tยj~`<|@hb:d)jn6~@hdยฑlc_Hpjd`<_@bYqd)jn6d`<bH~oยn6_@gยชj~`<|@hb:`<be`<d`9j,lfl<_Bpjdย`c_vb{
ยutHhยงjar_|@hh	ยยjag ^el<hq+jnhd)jaย@hb8oยn_vgnhjElcยก7lย`ยohยqย`<dsjd`<_@bq{ย*tHhxยjnhdxe^e`VpjElยqย`<dsjd`<_@bq0_o
r_vsHbkehksHbp	hnd)j,`<bed>x@{uยฒ`cgยง`Gl9jn0qย`<dsjd`<_@bqยฑ_Hpp	sHnยยยtHhbHh|@hn*ยยh0tj,|@hd_J_@^hn	jdhjg-j@p6te`<bHhdtjd
ย*_@nยHq*`<bQ_@bHh_ouqยh|@hn)j,lI_@^Hdย`c_vbq{ยฌ5bยgjbAx8_oยd6tH_Aqh pj@qhqiยซdtHh ^ย_Aqqย`<rel<h-_vrqhn|Ejad`<_@bq+pjbYrยh
qd)jadhk.iCjbkYdtHh:qhd_o^_Aq6qย`<relch:hbe|v`cn6_@bHg hbedยฃrยhtj,|@`<_@n)qzpjbยrhlย`9qdhk.ยณยฑdtHh:j@p	dsj,l*rhtj,|v`c_vni
tH_ย*h|@hnig-j,x0rhยฑsHbHยAbH_ยbj^Hn`<_@n`5{fยฌlยl<sHg`<bjd`<bH~+n6hqsel<d)qn6h~Ajn)kv`<bH~0dtHhqhIh	ยjg ^el<hqfjnhยด`cgz^elG`<hk
rex-_@sHn0qdskex@{Iยตยh|@hndtHh	l<hqqยถiย*h0ยทn)qdยtj|vhยฎd_keh	ยทbHh+_@sHnrjvqย`9p0on)jg hย*_@nย.{
ยธยนvยบยยปยผ<ยฝยผVยพยปรยฟรรรCร

ยฐ0bยฆรร@ร,รร7รรรร,ร9รร@รยรรรรยรรรรรรรร
ยรy>ร ย6ร-ย	ร	รย)ร.ร8}+p	_@bqย`9qd)qยง_omjYqhd_o
รAรรรรร,ร)ร,รAรรยรYรรร@รรร-รziยฎjยqhd:_aoรBรร	รรรรรยรYร@ร,รร9ร@รยร-รiยฎjbรร7รร7รร9ร@รยรรร@รร:ร	รยรกรขรยจjbkรฃร@รรฃร@รรรคeร@ร
รรร@รHรร7รร9ร@รmรฅรรคeรร,รร9รvร-ร.รยรฆ	รยฆรง:รรฉรจรชรรd6tjdkehdhng`<bHhquoย_@n*hj@p6tยqd)jadh+ร+รก8รรซjbk:j@p	dย`c_vbยรฌ-รกยฉร
dtHhbHh	ยHdยขqยd)jdhร,รญยยร"รYy>รeย)รฌย}){
รฎ @
j qยhk_@b-d6tHh+jr_|@h+keh	ยทbe`<d`<_@bYย*hยขpjabยkeh	ยทbHh+ยtjdj ย&l9jbHbe`<bH~-ยte`ยl<h ย hjnbe`<bH~-qยxBqd6hgรข`9q{
ยตย_@d`9p	hยฎd6tjdIย*hj@qq_Hp6`9jdh0dtHh*`<beoย_vng-j,ldhngรฐรฏ	rhtj,|v`<_@n)รฑJย*`<dtdtHhยฎd6hngรฒรฏ	d6n)jbqย`<d`<_@bzoยsHbp	dย`c_vbรฑHi
ยtHhnhd6tHhยฃj@p	dsjEl"rยhtj,|@`<_@nย`9qdtHh+jvp	dsj,ldn)jbq>`cdย`c_vb:osHbp	d`<_@b{
ยธยนvยบยยปยผ<ยฝยผVยพยปรยฟรร7ยฟยร

ยฐรดรณ*รยร@รรยร9รeรยญรตfรถeร9รยรYรทร	ร@ร)รยร9รeรรธรรร,รรรรรดรนรยยyรยฆยรบf}ยฃp	_vbqย`9qd)q_oยฃjbยฆj~vhbAdยก
hbe|@`<n_@bHgzhbAd:qxHqdhgยร
ยรชyร ยรย)ร	รย	ร.รY})i*jbkยjรธqhd _oรHรร)รรรรยรยรรร@รยรรร7รร9รvร:รฅรคAรยรรร7ร@รHรรบรย
รป ย0รผย,รฝ<รฝ<รฝ<ย	ยรพรฟei&jElGlยฎqtjan`<bH~ยd6tHh qjag h-qhd_o_@rqhn6|Ejrel<hยqd)jadhq+รziSยยtHhnh-d6tHh j@p	d6sj,lยdn	jbqย`<d`<_@b
osHbp	d`<_@b:`9qย_@bHh_oCdtHhqh^_eqqย`<rel<hยฃd6n)jbqย`<d`<_@bยฉoยsHbp	d`<_@bqยถ{
ยตย_@d`9p	hdtjd0ย*hsqhkYdtHhdhngรดรAรรรรร,ร)ร,รAรรยรzรรรร@รรยถรn)jdtHhnd6tjb wsqd+qd)jadhq{ยฐ0bY_@rqhn|jrel<h
q d)jadh_oujbQj~@hbed0`VqmยtjddtHh j~vhbAd^ยhn)p	h	`<|@hqjd+j:~`<|@hbรธ^_`<bed y5h@{$~ย{<iย`<d)q^HtexBq>`VpjElClc_Hpjd`<_@b.}
n)jd6tHhndtjb0`<d)qfp	_@g ^el<hdh*qd)jdhC_aoBยebH_ย*l<hke~@h@{  hยj@q6qsHg hCdtjdfjbยฃj~vhbAdpjbj,l<ยuj,xHqkv`9qd`<bH~@se`9qt
rhdย*hhbยkv` hnhbAdย_@rqhn|jrel<hยฃqd)jadhq{Cย*tHh+p	_@g ^el<hdh+qยd)jdh0_oCยAbH_ย*l<hke~@h+pjb:rยh+keh	ยทbHhk8rj@qhk
_@bยdtHhte`9qd_@n6xยญ_oj@p	d`<_@bq jbkQ_vrqhn|Ejarelchยqยd)jdhqJ_oยฎd6tHhยj~@hbed{ยย*te`9q te`9qd6_@nxY`9q jbย_vn)kehnhk
qh vsHhbp	h-_aou_vrqhn|Ejarelch:qd)jadhq0dtHh-j~vhbAd|v`9qย`<dhkยญjabkยj@p	d`<_@bqยข`cdJ^hno_@ngzhk.
{ H_vnh	ยยjgz^elch@iยซ`ยo
jbยj~vhbAdยฑ^hno_@ng hk:jabยj@p	d`<_@bยรฌdtjadIl<hk oยn_vg jab-_@rqยhn|Ejrel<hqd)jd	
h รผยd_ jb:_@rqยhn|Ejrel<hqd)jdh



!#"$&%&'$$)(*+'-,.'$/0,1%2'-43*56-7859!+':";<'-="6>0";)?38?@A"B.'-59C.'-D@E	830"B?.'->AF

:"B?G"B33$'->6"B.'-59H%&#3*6"BIJ"B(*598)8#!+(*@?K56LM8>?A"'-N?5EO'-+-5EPQ"B?.'-!+'->EM%<'$-G8G8!R(*?56L
3*59.'-('$$':.':SLT5E?<6"B>AU>A	?5EV'-V(*598,8@,	(7	"+!#"$

WW9X

>598A"6

Y[Z

jk*l
t6}

m*noHpJqrm[o

ย

ย

ยCrnt1o{u{rs4n

t=t6}{vs=ย{n

u0ยwqTro{ยdrNvยsJn

m^sยยยBvnยEtm*noยnm*t6ย

t#ย{n

ยw}
t6}

t6}

ย6|Hu*xOt6}

ยย4ยงKu*ยyr~drยltB}

sKt6u1t6}

r4ย6rm[o{rย	s}

r#n{ย

o{rt6rย6ยJvTn

ร

u[ย{qoยซn

rroยn

u[t	ยEu[n{xยsr4u[ย

ร

rNvt9sยqu

rย

ร
ร

rwmzยu*~[rRยCuรo{rEq

ยsrรdย

ย6rรdย{vย6r	t6}

ยUยsru*xym*ย

rEvn<lUยขยฃ[ยค[ยฅ

u*ยGs

ย6t6}

rย	odvs6ยEยs6svu[n

ยฝยus;rns6ย6}

rEvnยฏยจ

rยGยEu[ยCย1u[n
r[l<ย

ย

t	u[n{q|

ยCrntยr}m8~dvu[ย9sSvtwยEudnsvo{rย9sSvยs

ยm8qรstEm*t6r=vs=n

ย6rsrn{t9m*t;vTudnsl)mdst6}

vsVxย

no

ยCrntยOย}{vsSย

m*ย1rn{t9m8qm*no1s;u[ย1rRrEยฑ
u*ยยฏm*ย{qr	t6u1o{rEรn

ย6u[ย{qrยยชvsVxย

ย6t6}

u[t=n

r	u[n

rยErs6sBm*ยvq|ยย6rย

r4t6}

n

rn

}

u*qt6ร[lOยขยฃdยฃ

ยข*ยฆ9ย

t6rnsvu[nsSu*xvtOยvqq)ยยr#odvยsBยEยs6sroJvTnยปร[rยEtvu[nยฟ

rUยm[svยยย

ย6u[ย{qrยรvnยรVqm*n

rยRodvs6ยEยsBsrovn1t6}

ยNย#}{vqTr=ร&rm*ย6n{vn

rxu*qqu*ยvn

n{vn

ยยฐย#}{vqrยร&rm*ย6n{vn

ยยฏs|

rยm*ย[rn{t#รทNl[vsUms;ย

t9m0vTn

r4ย6rย

ยxยยBu[ย

รชEรซ[lย

no{rยUm*n|Nยu{s6svย{qr4t6ย9mznsvtvu[nNxยย

ย=ยrยu*xMm[ยEtvu[nsSt6}m*tymzย6r#rEยฑ

ย6rs;rnt9mztvu[nHu*xOt6}

s6m*tvsxAm[ยEt6udย6|ยย{qm*n

vยsยปt6}

r4รVqm*n

n{vn

rยEย

t6ro=vnยmยEu[ย

ยNย#}{vqTr1ร&rmzย6n{vn

rย6rExu[ย6rยmยย{qยmznยฐvnยฏย#}{vย6}รt6}

ยCrnt=ยr}m8~dvu[ยlรvnยu[ยEo{rย4t6uยซย[ยm*ย9m*n{t6rr=t6}

st6rยยlKย#}

ยsrtGuzxรธt6}

}m[stBu4รnoHmJย

rย6rยรฅ

ยBuรยErs6sl&t6u

รข

r=st9m*tBrsyรจCย

ย

s|

s@vTtBยm*tvu[nsUย#}

ย6u[ยrยUยEu[ย=ย{vTnmztvu[nHu*xVqTrmzย6n{vn

u1qu[n

ยm*noHm[ยEtvn

	

ย1ย

u[ย=vm8qq|

st6rยยปยฆ9ย

rยm*ย[rn{t1qrm*ย6nsยปrn

rHm[ยB}{vTr~drย1rntu*x#t6}

rย6rยvt9sUย[um8qVvsUn

rยm*ยdrnt

nยEt;vTudnNvTnยซรงOยMรรย{qยmzn

ย9sr#uzx)vt+vs+ยu*q|{n

u[ย

ย[}ยฏm*ยยu[ย

tHt6}

r

rm*ยdrntย s	ย[um0qยทย#udtvยEr

rย9m8qAlOmยซย{qm*nยยJvTยd}t=ยrN~[rย6|ยยEu[ยCย{qTrEยฑ<ยยซรGnbm*ย[rn{t4ย=vย[}{t1m*ย6ยv~[r[lรvnยt6}

ยยย

ro&lยvxRยr

rUยus6s@vTย{qrยr}m8~[vu[ยEsย

nยEtvu[nยซxยยBu[ยรบvTtEs#}{vst6udย6|uzxSstEm*t6rsUยกvn^รจ=ยฆym*noHmdยEtvu[ns	ยกยทvTnยซรฉ1ยฆRtBum*n

stEm*ย6tvn
r#n{ย

ยย

ยยsrยEtvu[n1m*noยvTn{~[rst;vTย{m*t6ro

vT~drnยนmJย[um8q<รถ<lmยปรปQยdรผรฝรป.รพQยdรฟรผรต[ย	รน&ย:ยdย1vsUm1ย{qm*n^t6}mzt#ย[ยm*ย9mznt6rrswt6}m*tUt6}

ยvqqMย6rm[ยB}ยm=st9m*tBr#vnNรถ

t6}m*t8l&vnbย[rn

n{vn

ย

ย*v~[rn1m*n{|4ยยus6svย{qrGยยr}m8~[vu[ยOu*x

u*qo&l<m*noยs@vTยJv:qm*ยย6rsย{qt9sUยm*n^ยr=u[ย

ร&rtยรกรฃรขรคยก@รฅยฏรฆBรงVยฆยยยrยmbรVqm*n

vsยm8qqTro^ยยผรฟรฝ.ยยยรผ{vxMt6}
no{roยยกยทvTnHtB}

ยBrsrntBro

rsUodvs6ยEยs6sroยปvnยรKvยsBยEย6rt6r

r	m*ย[rn{tytBuยm[ยB}{vr~[rUvt9s+ย[um8q&u[n{q|1vnยmUxย9m[ยEt;vTudnยยกAr[ยรยยlยฃdร{ร1ยฆMuzxMt6}

ร

ยฆGm*noยนvs=ย=ยย6}ยยCu[ย6r

}m*ยยซlRยขยฃ[ยคdยฃ{ยฆGm*noยนvTnยยu[ย6ยยปvTnยรUยณ	t6}m*t	vnยEu[ยBยu[ย9mzt6rs

rnt4srยEt;vTudnsย	ร*vย=vqm*ยยo{rEรn{vtvu[nsย}

m[ยEtvu[nยยกvn^รฉ1ยฆ9ย

vt9sKqrm*ย6n{vn

rย6rUvtUยn

r

ยยm*noยซu[ยsrย6~{ยธ

ย6rswยvt6}^u[t6}

ยฝOยงUm8qยrย6nยผยจรยฉยซusrslยขยฃdยค*ยฌ

ru[ยBrtvยUย1uรo{rEqsยยก@ยฉยซusrs#ยจยชยรrn

รรรน)ยย[ย=xu[ยGm*n^m*ย[rn{t+vยsUmxย

ร

rEvn<lยขยฃ[ยค[ยฅ

t6u[ยNm*t9m*ยธ.qvย[rยs;t6ย6ยยEt6ย

ยก@รจCรฆ6รฉรฆEรชEรซรฆ9รฌ2รญยยฆ9lยm*noHรง^รขยฎรฎรฏGรฐ*รฆ8รฑรฑรฑรฆEรฏรฒ[รณย+รยรด[รต8ย[ยdรถxu[ย#tB}

rn{~[vย6u[n

ยEt6u[ยl*ยK}{v:qr

st6rยNs	ยก@ยงUm8qยrยBnยนยจรยฉยซusrslMยขยฃdยค*ยฌยฝยงUm8qยrย6n<l

rHm*ย[rn{tย-sUqu

ยฆ	ยก@ย#m*ยNm[o{ย[rยจรรยu[n

ยr4m*ยBrGn

รNรdรyร<รรรยร<รรร)รรUร

ยudย

ro^ย{|ยm[ยEtvn

ย{q:vยBvtq|[ยยบยถย

tBu[ยm*t9mยนยก@ยus;rns6ย6}

t6roยs|

ย6u[ย{qrยรvยsSt6uKรno1mGsBm*tvsxAm[ยEt6u[ย6|	ย{qm*nCt6}m*tRm[ยB}{vr~[rsOmUย[um8q

rrn{~[vย6u[n

vnยsย

rGo{u

r

ยEt6u[ย8ย-s4m[ยEt;vTudns	ย*v~[rnยt6}

ย1r=t6}mzt4m*nยm*ย[rn{tรm[ยEtEs	m[sKvxyvtยvยsmCรn{vt6rยธAst9m*tBr1ยm[ยB}{vn

ยm8qGst9m*t6rยปยก@ยusrnsBย6}

v~[rnHtB}{vยswย1uรo{rEqAl

ย}

ย6rsrn{t6roยซrEยฑ

ย=ยrยu*x<ยusBsvย{qTru[ยsrย6~*m*tvu[ns+m*no1ยยus6svย{qr	rn{~[vย6u[n

nยErย6t9m0vTn{t|tBuยEu[n{t6ย6u*qยธt6}

t6}

ย

ย{q:vยBvtq|[ย	ย}{vs#ยzvT~drsGsยยยBvnยEt4m*noยปยsrExย{qMย6rย

ยw}

rxAm[ยEt9swvtUqTrmzย6n

udt	ยr=ย6rย

rยsvt6ยmzt6roยผm*ย

ro{u

ยm*tvu[nsl4m*no
ย1rn{tยKยณAn^t6}

r=m*ย[rn{tย-s#o{rยBvsvu[nsUยvqqOยr4ยmdsroHu[nยปvt9s#}{vstBu[ย6|Nu*xRu[ยsrย6~*m*tvu[nsUm*noHm[ยEtvu[nsKย#}{vยB}

~drntCร[|รstBrยs=ยก@ร

ย

rยซqTu

rrn~dvย6u[n

r=m*ย[rn{tยm8|1ย6rm[ยB}ยm1st9mzt6r	ย#}

ยCrntl&ยmdsroยซu[nยt6}

ยEu[ย1ย{qrEยฑbt6}m*nยvt9s=u[ยsrยB~0m*ย{qrยst9mzt6r[ยยย}
rEยฑ

ย

ย6r[ย

ยsrsยu*xRvtยรยro{u[n<ยรt4m[s6sย
t6}m*t+t6}

t6rsyvTn^t6}

rNrEยด<rยEt9sUu*x#t6}

st6uHt6}

noยฒvnยผt6}

รn{vt6r[ย#ย}

r1m[ยEt6ยm0qMย6u[ย

r#ยยus6svย{qrGudยsrย6~0mztvu[nsOu*x&t6}

ยlยขยฃ[ยค[ยฟ{ยฆm*noHย{n

t9m8vn

ย6rsrn{t9m*tvu[nย}m[s=m8qย6rm[o{|

nยEtvu[nbยEu[ย6ย6rs;ยu[no

rsr1ยEu[ย1ย{qrEยฑยนstEm*t6rsKn

r

rEvn<lยขยฃ[ยคdยฅ{ยฆ2mzno	vn=ยu[ย6ย	u[n=ย6rmdsu[n{vn

r#u[ยs;rย6~0m*ย{qr	st9m*tBrsOm*ย6rt6}

r=rn{~[vย6u[n

u*ยยt6}m*tt6}

ย^t6}{vsยmdยEtvu[n<l

ยฆ9ย

u*ยqro{ย[rvTnยodvยs;t6ยvย

ยw}

rsยxยu*qqu*ยvn

r^t6ย9m*ns;ยu[ย6tEm*tvu[nยฏo{u[ยm8vnยฐrEยฑm*ย1ย{qrยฒmzย6r^t6}

ยพ

ยขยฃ[ยคdยค{ยฆVq:vt6rยEm*t6ย

rnHยyr4ยmznยs6m8|1t6}m*t

rm*ย[rn{t4ยvqqGย{n

rstEm*t6r=vt=ย6rm[ยB}

t6u[ยNm*t9mUยก@ยwusrns6ยB}

u*x	t6}{vs=t6u[ย{vยยซยm*nbยrยปxยu[ย
m*rEqย{qvn

u[t1ย*ยยยw}

u[t#ย*ltB}

rHu[ยsrย6~*m*ย{qrยst9m*t6rNjk*ยยย}{vs=rnm*ย{qrsCยst6u^u[ย

nยEtvu[nยยEu[ย6ย6rsยยu[no

tUt6}

g6hi

ย1rn{t#ยยr}m~dvu[ยvsKn

r	ยm*tvrn{tยOยณnยยudt6}HrEยฑยถmzย1ย{qrs#t6}

ยEu[ย1ย{qrEยฑยซxยทm[ยEtEs	m*ยu[ย

ef

ย^t6u^ยvnbt6}

u0ยwqTro{ยdr1ยก@ยงUm8qยrย6n^ยจยชยฉยซusrsl<ยข8ยฃ[ยค*ยฌ

m[ยEt6ยm0q)vn*ยตย

c[_

ย6rs;rnt9mztvu[nยuzxKmzย[rntEsยยย}{vs=t|{ยrNu*x#ย6rย

rยญudยsrย6~0mzย{qTrbst9mzt6rs=vnยฎt6}

ยm	ยm*ย6r#o{u[ยm0vTn<ldt6}

_

ยCrnt=ยr}m8~dvu[ยvยsn

rEvn<ย-sRs@vTtBยm*t6ro=m*ย

rmdยEt6ยm8qytBย9m*nsvtvu[n^xย

vn

`baRcd_

u[t	vย1ย{qvroยย|ยtB}

ย9m0qyย6rย

r=m[ยEt6ยm8qMtBย9m*nsvtvu[nยซxย

t6ย9mzย

rNrn{~[vย6u[n

_

r	rn~dvย6u[n

udt4ยr}m8~[rHm[ยยEu[ยEodvTn

ยrrnCยsroยvTnNยusrns6ยB}
m*ยยu[ย

Z^Z

sKt6u1jkwv:xymznoNu[n{q|1vxOt6}

rHm*ย[rn{tยยEย[ย9ยยย9ย=tB}m*t4t6}

rn{~[vย6u[n

\*]

r

ยEudย

ยdrย4m[ย6}{vr~*m*ย{qr[ยยยงKrnยEr[l<t6}

ย9sr1u*x

r1m*ยdrnt

}mdsrsยOรยชs6m*tvsxAm[ยEt6udย6|1ย{qm*nยยm*nHยยr




!#"%$'&($)+*#,-*.)/$01"2,3"456	7	$'$8&9$'7	$:$*019;$)/<#$:"2,*,	,=401"2*>6	$)&("6	9?*A@*B"74>CD*E5;4#F,=$'7	!G*EF/H%$,=6I*>6	$
*>5)*>5.*06="45.6	48F$@$'7JCK471LM$):"5.6	9*E6N,=6I*E6	$OQP(9/"2,R"2,N*8<$'5$'7*BHS7	$'@7	$,=$'5/6I*>6="454>C045)#"6="45*BH
@/H2*>5,'OTU5+$VW01"$'5X6:@/H2*>5&("YHYHD619$'7	$CK4#7	$M047	7	$,=@Z45)[6	4\*\)/$01"2,3"45]6	71$'$^4ECD@Z4>H_X54#L:"2*BH-)/$'@6	9SO
` 46="20$a94>&($'!$'7a/6	9*>6619$b,3"c'$b4>Cd*>5.$VW01"$'5/6-@/H2*>5Le*B_W,=6J"fHYHFZ$b$g@45$'5/6="2*BHhO
iQjMkmlenSoqpdrtsusv3nSwduyxUz{sErR|Q}
~ :
5 6	9$R@7	$'!#"%4#,Q,=$06="45:&($N)/$ย5$)M*ยF*,3"20tCK7*>LM$'&(47	ยU4>Cยย{H2*>55/"5<8&9/"YH%$ยy$*E7	5/"5<aX*>5)8&9*E6*
,	*>6J",ยC*06147	_@/H2*>5ACK47d*E5ย*E<$'5X6Qยย",ยO ~ 5M6	9/"2,d,J$06="45e*>5)8"5M619$(CK4EHfH4>&("5<b4#5$,Q&($(&N4#/H)^Hf"ย$U6	4
045,3"2)/$'7U6	9$04LM@/H$gX"6=_.4>CSย5)#"5<.,=019\*b@/H2*>5470	9$01ย#"%5<A6	9*>6*:<>"!$'5.@/H2*>5"2,,	*E6="2,3C*#06	47	_O
~ 547)/$'76	4.)#",10,	,U6	9$"2,	,J$^4ECN04LM@/H$gX"6=_\&($5$'$)6	4.)#"2,	0,	,U47ULM$*,=71$,4>Cd04LM@/H$g/"6ย_a
*>5)6	9$:6=_X@Z$b4>C(7	$'@7	$,=$'5/6I*>6J"%4#5,4>CNยยH2*>55/"5<\&9/"YH$Wยย$*>7	5/"5<\,=_,=6	$'Le,ย&($^&(4/H2)ยHY"ย$M6	4AH4X4ย
*>6O

ยก

ยSยยยยย8ยยยยย\ยAยยยBยย'ยZยย'ยยยยยยยย
$&("YHYHd)#",J6="5</"2,=9F$'6=&($'$'5\6	97	$'$F*#,3"20bย{H2*>55/"5<.&9/"YH$^ยย$*>7	5/"5<\,=_,=6	$'Lยข7	$'@7	$,=$'5/6I*>6J"%4#5,'ยฃ
ยค OUยฅ ยZยยยยBยยฆdยAยยงย{ยBยยงยยยยย'ยยยยSยSยยจ(ยฉ 46	9.6	9$U5/L^FZ$'74>Cย*E<$'5X6Bยชยซ,R4F,=$'7	!>*>F/H$b,=6I*>6	$,ยยฌย"hOยญ$Oaยยฎยฐยฏeยฎยฒยฑ
*>5):6	9$5/L:F$'7(4>CS@4/,	,3"F/H$6	7I*>5,3"6="45ยCK506="45,RLe*B_8F$U$gยง@Z45$'5X6J"*GH%ยณU"56	9$-,3"c'$U4>CS6	9$
*06	*BHS7	$'@7	$,J$'5X6I*E6="45SO
ยด OUยตeยถ ยยยยท	ยธยนยยยบยยยยย'ยยMยยยBยย'ยZยย'ยยยยSยSยยจ P(9$(5/L:F$'7Q4>Cย*><#$'5X6ยชยฒ,y4F,=$'7	!>*>F/H$-,=6I*E6	$,yL:"<9/6
F$M$g@45$'5/6="2*BH("5?619$e,3"c'$e4ECD619$e,=_,=6	$'Lยป7	$'@7	$,J$'5X6I*E6="45Sa{F6b6	9$M5/L:F$'7:4>C(@4X,	,ย"%F/H$
6	7I*>5,3"6="45ACย506="45,R"2,*>6(LM4X,J6d@4EH%_/54L:"2*BHย"%56	9*>6(,3"c'$ONP(9/"2,R"2,(*8LM4X,=6*>@@Z$*BHY"5<e6=_/@$
4>C7	$'@7	$,J$'5X6I*E6="45.Cย47,=_,=6	$'Le,(&D"%619.F45)/$)50$'7	6I*G"%5/6=_\ยฌ=ยผ-*BH@Z$'7	5;ยฝยฟยพd*>7I)#"ha ยครรรยค ยฑIO
P(9$.6	7I*>L*>รh0'*>7	$e,J_ยง,=61$'LรLA$'5X6="45$)ร"5รร$06="45 ยด "2,M*>5]$g*>LM@/H$\4ECร*,=_ยง,J6	$'Lร&("6	9ร*
ร*,ย"%รยLA4ยง)/$'7I*E6	$\7	$'@71$,=$'5X6*>6="45SO ~ 5q,=0	9ร*?,J_ยง,=61$'Lร&($.,=*BHYH_ร9*B!$;*,=$'6M4>C^*E6	4L:"20
4F,=$'7	!>*>6="45,ยยฌh$Oยญ<OaX&ย9$'6	9$'7(6	9$F/H4X4).@7	$,1,=7	$U"2,(9/"<9.47RH4>&ยฑIOQP(9$5/L^FZ$'74>Cย*E6	4L:"20
4F,=$'7	!>*>6="45,t"2,RHY"%5$*E7("5.6	9$-@714F/H$'L\ยชยซ,R"5@6aF6(6	9$U5/L^FZ$'74>C@Z4X,	,3"F/H$b4#F,=$'7	!G*E6="45,
ยฌย"hOยญ$OaB4F,=$'7	!>*>F/H$D,J6I*>6	$,y&ย9/"019M*>7	$Q6	@/H$,{4>Cย*E6	4L:"20Q4#F,=$'7	!G*E6="45,ยฑZ",ย$g@45$'5/6="2*BHhOdP(9$tHf"2,=6
4>CS@4X,	,ย"%F/H$U"5>ร=7="$,(6	9*>6Q6	9$U@*>6="$'5/6dL:"<9/6d9*B!$D",R,=*GHfH_@4>H_/54L:"2*BHy"5e619$-@7	4#F/H%$'Lยชยซ,
"%5@6BOMยผ$'50$aย&N$:<$'6b*.ร#*,3"รยLM4)/$'7I*>61$M7	$'@7	$,=$'5/6I*>6J"%4#5;4ECD*.ย{H2*>55/"5<&9/"YH%$.ยy$*>715/"%5<
,=_ยง,J6	$'L\O
ร O ยน
ยธ ยยยบยยยยย'ย.ยAยยงย{ยBยยงยยยยย'ยยยยSยSยยจยฉ 4619M6	9$5/L:F$'7(4>C*><$'5/6ยชยซ,{4F,J$'7	!G*>F/H$b,=6*>6	$,Q*>5)A6	9$
5XL:F$'7ย4>C@4/,	,3"F/H$b6	7I*E5,3"6="45CK506J"%4#5,R"2,(@4>H_/54L:"2*BHS"56	9$-71$'@7	$,=$'5/6I*>6="45\,3"c'$OdPD9/",
6ย_/@$84>Cd7	$'@7	$,J$'5X6I*E6="45",ยH$,	,U<#$'5$'7I*BHd6	9*>5*eร#*,3"รยLM4)/$'7I*>61$^71$'@7	$,=$'5/6I*>6="45SayF6U"6ย",
,=6="YHYHย$g@7	$,	,3"!$*>5)M04LA@/H%$'61$H%_545รย6	7J"%!#"2*BHย*#,Q&($&("YHYHH2*>6	$'7()#"2,	0,	,'O(P(9$6	7*>5,=@4#7	6I*>6="45
)/4Le*B"5ร$g*>LM@/H$4>Cb6	9$\@71$'!"4,,=$06="45ร",LA4ยง)/$'7I*E6	$H_]7	$'@7	$,=$'5/6	$)]F/_]*+<7*>@9รยHY"%ย#$
,=6	7	06	71$aS"5ร0'*,J$,b&ย9$'7	$e619$'7	$\*>7	$e*E6^LA4X,=6b@Z4>H_X54#L:"2*BHYH%_รLe*>5/_+*BH6	$'7	5*>6="!$,รCย47:6	9$
*06	*BHย,J6	7	06	7	$84>Cd6	9*>6U<7I*E@9+ยฌh$Oยญ<ZOa/"5+*M@*>716="20/H*E7b*>@@/HY"0'*E6="45SaS6	9$'71$ย*E7	$^*045,=6I*E5X6
5XL:F$'7ย4>Cย@Z4X,	,3"F/"YHY"6="$,6	4e*^H%4#<X*>7="6	9L:"20ร5XL:F$'7ย4>Cย7146	$,ยฑIO
รยรรbร:รyรยญรยญร(รรยรbรยรBร:รยรยรยรรร=ร'รGรกรขร=รข'รยรfรฃJร-รฃ	รขBรค\ร>รก	รยซรฅ'รขBรกIรRรfรฃJร-รyรยซรยรรยรร3รยซรรยรhรฃ	รขBรคGรฃ1รยรครNร	รฃ1รขร:รฆYรร รIรยฒรงร>รก1รยซรฅ'รขรกรdรfรฃJรรรฃ1รขรค
ร=ร'รGรกรขร=รข'รยรfรฃJรรยซรขรยรBรdรฃ	รจ=รยรGรฃJรรยรยรBรยร=รยรยรข'รhรฃ1รยรยซรกรขbรยรยซรฉ=รร รช
รซรซIรฌ

รญรฎรฏ>รฐรฎรฎรฑรฒ]รณdรด#รฑรฑรดรฑรตรถรท	รธรน

รบ3รป.รผ	รฝรพรฟ	รพ%รป/รพ'รฟ	
รผ1รฝZรพ'รฟ(รพ=รพรผ	รฝรพรผ	รพ'รฟ/รพ'รฟ>รผ	รพ=รผ	รพhรฟ	รพ !#"$%&')(*/รพ'รฟ>รผ	รพ
=รผ	รพ,+รผรฟ	รพ
ยรพ'รฟ^รผ-.//รพ'รฟ0>รผ1รพeรฟ	รพรฟ1รพ =รพ'รปXรผ>รผ)รป1hรฟ	รพ #"2$&%')(*/รพ'รฟ0>รผ	รพรฟ	รพรฟ	รพ =รพ'รป/รผ0>รผ!3#รป4+56
5
798:>รปรปรป;,รฝ<8รพ>=ยรพ >รฟ	รปรป;?!=รผ1รพ@"

A#BDCFEHGI JDKMLHN#OQP2R4SGS J)NUTVGWXLGSYZN4[JYI
\ )]รพ'รป^M7_8Eรปรป%รป;`รฝ<8รพa=yรพ Eรฟ	รปรป;b=รผ	รพ@cQรผ1รฝรพ'รฟ	รพ@>รฟ	รพรผ	รฝรฟ	รพ'รพdeรป^f%รผ>รผ)รปe8gf>รผ	รพ;&รฟรพ 
	รผ รฝ>รผh(รพfรป':/รพ'รฟ "
i "j TS [eGKSGklWY4mn รฝรพ f-o&%รป;ยรฝรพ'รผ	รฝรพ'รฟp	8:>รปH
qรฟ{รผ	รฝรพ;รพ'รป/รผU2->รผ:'
r%fรผรฟ	:2f%sรผ0>รผ)รปe8<8)
รฝ>รฟ04c>รป,d Aรผ0oรพรพtรปรพ'รป/รผ:e8รผ!3Aรพ%"
รบhรปQ798:>รปรปรป;,รฝ<8รพ=yรพ >รฟ1รป%รป;M=รผ	รพ(รผ1รฝ>รผ
re8<89%รป/รผ.รผ	รฝ:f>รผ1รพ;%รฟ%cรพ]#รพ'รป;รผ1รฝรพรฟ%u8รพv

รฟ	รพรฟ	รพ =รพ'รป/รผรป;:รผ	รฝรพg8:>รป@Eรป]รพ'รฟ!w
*&%รป;Aรผ	รฝ>รผ(รผ	รฝ:h8<%รพ x8:>รปd:>รผ:'
r%fรผ#รฟHhf%sรผ0(
รผ)รปe8<8),รปXรผ1รฟ0%fรผ0u8รพd*r"ยญรพ%")cZรพรผ	รฝรพ'รฟรผ	รฝรพy%fรพbรปรพ'รพ /รพ a
qรฟUรฟ	รพรฟ	รพ Jรพ'รปXรผ0Eรผ)รป,:Uรพtรปรพ'รป/รผ:e82รฟ
รผ	รฝรพ]รพ'รฟ<zVf>รผ)รป/รฟfรพ (รผ0o#รพ (รพt{รปรพ'รป/รผ:e8รผ)Mรพ+0"

| "	}~h!ย J*T4YQยh[ GK6SGklWYVm	ย >รผ:'
r%fรผ#รฟ8:>รป,
*รฟรผ1รฝรพ;รพ'รป/รผยยรฝ%Jรฝรฟ	รผรฟ	รพรฟ1รพ =รพ'รปXรผ>รผ)รป
*r"ยญรพ%")c{8)Xรป%ย82%รปรผ	รฝรพ:รฟ	รพรฟ1รพ =รพ'รปXรผ>รผ)รป@6
dรผ	รฝรพรฟ&u8%รพx+0cVEรปaf1รฝรพ f-o%รป;,รฝรพ'รผ	รฝรพ'รฟs8Eรป
>รผ:'
r%fรผรฟ-f>รป@uZรพ>f>รฟ1รฟรพ d%รผรพย?f-รพ'รป/รผ8),รป@{8)Xรป%ย8ยรผ)Mรพ%"
ย =รผ1รพ{รผ	รฝ>รผU
ย8w8รป/รผรผ	รฝ:pfEรผ	รพ;%รฟd yffyu:รผUรผ	รฟ:e8V>รป8รพ'รฟ	รฟ-รฟlรฟ-fรพ ยcรปรฝ:f1รฝ
>รปdรปXรผ	รพ8<8<);รพ'รป/รผ/รพ ');รปรพ'รฟยย"ยฐรพ%")c:รฝยd>รป4+ย;%;#รพ =รผ0h%Mรพ8:>รปx
q#รฟย!8)]%รป;,yรฟ%u8รพ@"hย(รฝรพ
;%;รพ =รผ1รพ `8Eรป`Aรฟ	รพรฟ	รพ Jรพ'รปXรผ	รพ ยEรปย]รพ'รฟ<zรพ 4"]รบย
gรผs
ยw8:Aรผ	รฝรพ,]รพ'รฟ<zVf>รผ!3#รปยรฟfรพ -c(รผ	รฝรพ'รป
รผ	รฝรพbรพt%fรผh
re<8)รฟ	รพHUรฟ1รพรฟ	รผ1รพ 4cV>รปยรผ	รฝรพ>/รพ ย3;#รปรพ'รฟeeรผ	รฟ-.รผ;รพ'รปรพ'รฟ0Eรผ	รพ>:รปรพย8:>รป#"gยDรฝ
รผ	รฟ:e8l>รปAรพ'รฟ	รฟรฟhรฟfรพ ย:8รผย:fe8l683รผ!3#รป,
*รฟ/รพ ');รป,รฟ%u8รพ" ย /รพ ');รปรพ'รฟยย;)]รพ'รป@
รพ f-<zVfyรฟ&u8%รพ/c#>รป/e=รพ:รฝรพ'รฟรพtรพ'รฟรพ'รปfรพsรป.!;%;รพ =รผรป;,8:>รป#"Hย(รฝรพy8Eรป.Jรฝ%8:
uรพ\รฟ1รพรฟ	รพ =รพ'รป/รผ	รพ ย>รปย]รพ'รฟ!wzรพ รรพย?f-รพ'รป/รผ8)%"ยนรบ*
bรผ	รฝรพ/8Eรป^:Aรปรผ,>รผ:'
r%fรผรฟ-%cdรผ	รฝรพรพย?f-รพ'รปXรผ
]รพ'รฟ<zVf>รผ)รป/รฟfรพ -h8)f>รผ	รพ ยรผ	รฝรพ
re<8)รฟ	รพ >รปdรป
*รฟยรผ	รฝรพy/รพ ');รปรพ'รฟ c{รฝd ,f1รฝยยJรพbรผ
;รพ'รปรพ'รฟ0>รผ	รพhUรปรพ^8:>รป#c#รพ'รผ0f"2ย(รฝ:2รฟย%f1รฝy%lEรผ9zรฟ0=รผ2%/รพ(รพt8<f-รผยรปMรผ	รฝรพ ย รบ#8<รผ	รพ'รฟ0>รผ-รฟ	รพ%c
uยรผ	รฝรพ>Jรพyรปe8_6รพ'รฟ	
ยยMf n Eรฟ	รผ	รฝ>รป@ย	eรพ 	ยยMf n Eรฟ	รผ	รฝยFย	eรพ c i ย&ย%ย +2รฝรพ'รฟ	รพX%รผย
รฟ	รพ
Kรพ'รฟ1รฟ	รพ รผ?&Uรผ	รฝรพย,:!%รฟ2รฟ%;รฟ06@"ย(รฝ:รฟ%f	รฝ/:รป/รพ'รพ +รผ1รฝรพ>#รปรพ>Jรพ /รปad>รปย
รฟ0%fรผ:fe8'รผ>รผ)รปยcgf1รฝย%Mรผ1รฝรพa#รปรพ dMรพ'รปXรผ!3#รปรพ ยรปรรผ	รฝรพMรฟ	รพ]&)%,=รพ fรผ)รป#" ย sรฟ	รพ
/รพ'รผ0e<8รพ @/รพรปJรผ	รฟ0>รผ)รป,6
ยรผ	รฝEรผp:/รพ d>รปs
*รฟ	รผ	รฝรพ'รฟg&:fย3#รป.fEรป,uรพ
*%รป?%รปMย@=รพ ย
ยSรพ'รปรปรพ'รปรฝ8รผย%c i ย%ย&ยย ย รฝรฝยยยยSรพ'รปรปรพ'รปรฝ8รผย%c i ย%ย6ย +0"
ยรพ'รปfรพ%cรป@=รผ	รพddรผ1รฝ>รผย
ยe8<84รปXรผ-^รผ1รฝf>รผ	รพ;%รฟ-%cย]>รฟ)%h8:>รปf>รปuZรพรผ	รฟรพ sรปa>รปdย#(D8<รปรพ
/รพ ');รปยรฟfรพ cp{รฟ	รผ	รพ `ubMf%รผ1รพ'รฟ)ย'รพ ]รพย?f-รพ'รป/รผ]รพ'รฟ<zVf>รผ)รปยรฟ-fรพ รฟ	รพ%cยรฝ:f1รฝ
รฝ%รพ
*8<83รฟ	รพ 8รผ0ยรปM>รผ:'
r%fรผ#รฟ8:>รป#"
ย "	} T ย JยTVYMยย[eGKSGk9WYVmย >รผ:'
r%fรผ#รฟy8Eรปd
qรฟRรผ	รฝรพ;รพ'รป/รผยยยนรฝ%hH8)/รป%y:e8Sรฟ	รพรฟ	รพ Jรพ'รป(
รผ0>รผ)รป#cรผ1รฝ>รผh:รป#รผรป8).รพย?f-รพ'รปXรผ!83/]รพ'รฟ<zVu8รพ%c4uรผf>รป,uรพ%fรผ8w8)@f%sรผ	รพ Mย83;&รฟรผ	รฝ(
y:fe8<83{+ย%รป/8)/รป%y:e8Qรผ)Mรพ%"
A#B)AยEHGI JDK@ยกsYIR#W)S I
ยข+รพyh%8:x8w)oรพAรผ?f-8:%-'<
qM798:>รปรปรป;@รฝ<8รพ=yรพ Eรฟ	รปรป;a=รผ	รพu%=รพ /รปรผ	รฝรพyu]รพf>รผ1รพ;%รฟรพ "
ย(รฝรพX
q8<8)hรป;.รฟ	รพ 8รผ0	>รฟ	รพy')8รพยf#รฟ8<8:>รฟรพ 	
Qรฟ	รพ 8รผ0	รฟ]รพ du@ย/ย=รพ >รปMยยรพ'รปรปรพ'รปรฝ8รผยHรป
>รปรผ1รฝรพ'รฟยf#รปXรผ	รพtรผยSรพ'รปรปรพ'รปรฝ8รผยMยยย/ยJรพ c i ย%ยฃ%ยย ย@=รพ ยvยยรพ'รปรปรพ'รปรฝ8รผย%c i ย%ยi%ย ยSรพ'รปรปรพ'รปรฝ8รผย%c
i ย%ยยคi +p>รปรผ	รฝรพรฟgรฟ-ย
9:h%yรผ	รผ1รพ d
ยรฟ%ยขรผ	รฝรพu,
Qรผ	รฝ:6รพ'รฟ "
ยฅ ยฅ0ยฆ

ยงยจ`ยฉlยช ยซยจยจยฌยจยญMยฎยฏยฌ)ยช ยฐ/ยฑยฐ%ยซยฒยจยฌยจยญ

ยณ&ยด	ยตยถ)ยท%ยธยนยบยป&ยธยนยธยผ0ยบeยฝยผยธยพยผยธ ยฟยธยนร0ยบรยถ)ร%ยนyรรVยบ	ร9ยฝ:ยบยนยนยถ)ยนยปyรรยถ<ยฝ)ยธgร4ยธ ยบยผ-ยนยถ3ยนยปยฟ!รยฟร-ยธร@รeรยนร&ยถ)ยนยป>ยบ	ยฟยบรยถ:ยฟ'รrยบ%รร
รร%ยผรdยพยฝ:ยบยน,ยถ:ยฟรhรยร{รรhร2ร*รยบยผ0ร4ยดhรhรยธHยพยผร%รยฝ)ยธรvยผยธรยบeยถ)ยนยฟ	รhรยร{รรยรยร*รยบยผ0รxยธยท%ยธยน,ยถ<รpรhยธ>รร%ยนยฟยยถรยธยผ
ร%ยนยฝ)รยธร?ร-ยถ)ยธยนรgยพยฝ:ยบยนยฟยยดยรhรยธยฟยยถ3รยธHรร2รรยธยผยธยฝ:ยบร-ยธ รdยพยฝยบ6ยน@รยบeรdรยธยธรยพร&ยนยธยนยรยถ:ยบeยฝrยด
รยคยดhร*รgรhยธรรMยนร%รHยผยธ ยฟรยผยถ:รรyร%รยผ0ยฟ!ยธยฝ3ยท&ยธ ยฟร-ร@ยธร?ร-ยถ)ยธยนร>ยพยฝ:ยบยนยฟยร2รรยธยนรรยนร&ยถ3ยนยปยยบ@ยฟ-ยบรยถ:ยฟ'รrยบ%รรร%ยผรMยพยฝ:ยบยนรยถ)ยน
ร รยบ%ยฟยยถ3ร*รsรรยธยผ0ยบ6รยธยฟรยฟรยธรยฟยยถ:ยฟ	รhรยร{รรยรยร*รยบยผ0ร4ยดhรhรยธ>ยฟยยถ3รยธHรร2รรยธยผยธยฝ:ยบรยธ รxยพยฝยบ6ยนaรdยบ รsรยธยธรยพร&ร
ยนยธยนยรยถ:ยบeยฝrยด
รยคยดรUยถ3ยนร&ยถ)ยนยป,ยบยนdยธร?ร-ยถ)ยธยนยรยฟยบรยถ:ยฟ'รrยบ%รรร&ยผรyยพยฝยบ6ยน,ยถ)ยน ร รยบ&ยฟ'ยถ)ร*รรรยธยผยบรยธร9ยฝ:ยบยนยนยถ)ยนยปdรรยถ<ยฝ)ยธ>ร4ยธ ยบยผ-ยนยถ3ยนยป,ยฟรยฟร
รยธรยฟ2รยยถ3ร-รร%ยนยฝ)รร%ยนยธ	ยพรยยฟ-ยฟ'ยถ)รยฝ3ยธร-ยผ0ยบยนยฟ'ยถ)รยถ)ร%ยนsรqรยนรร!ยถ3ร&ยน@ร*ยถrยดรยธ%ยดร%ยพยฝ:ยบยนยนยถ)ยนยปรhยถ)รร,รร%รยพยฝ)ยธรยธยถ)ยนรqร%ยผ-รยบร
รยถ)ร%ยน4ร2ยถ:ยฟรก	ร2ร*รยบยผ0ร4ยดยรrยน@ร-รยถยฟ	รยบ%ยฟยธ%รยถ3รยยถยฟยธยนร%รยป%ร/รรรร%ยนยฟ'ยถ:รยธยผ	ยพยฝยบ6ยนยฟรร2ยพร6ยฝ3รยนร%รyยถ:ยบeยฝpยฟ'ยถ)รยธ%ยด
ร รยธbยบรรยท%ยธMยผยธ ยฟ!รยฝ3รยฟ,ยปยถ)ยท%ยธbยฟ!ยธยท%ยธยผ0ยบeยฝยผยธ ยฟ!รยผยถ:รรยถ)ร%ยนยฟ@ยบ&ยฟ,รรยรรยบ6ร,รยยธรรhยถ<ยฝwยฝyรยธbยบ6รยฝ3ยธ`รรยร%รรยบeยถ)ยนยยถ)ยน
ย
ร%รยผยฟร-รรรVรขรhยธรยบยนQยนร&รรร%ยพยธร-รยบร	รยนร&ยถ)ยนยปaยบ@ยฟ-ยบรยถ:ยฟ'รrยบ%รรร%ยผร,ยพยฝ:ยบยน#รUยธยถ)รรยธยผyยธร?ร-ยถ)ยธยนยรHร%ยผ	ยถ)ยนยธร?ร-ยถ)ยธยนยร ร
รhยถ<ยฝ<ยฝ_ร{ยธdรrยธยท%ยธยน,รรฃ#รDยฝ<ยถ)ยนยธรรยผ0ยบ%รร0ยบ6รยฝ3ยธยป6ยถ3ยท&ยธยน.ยบ6ยผรยถ)รยผ0ยบยผรdยผยธยพยผยธ ยฟ!ยธยนยร0ยบ6รยถ)ร%ยนยฟhรรยร_ยฝยบ6ยนยนยถ3ยนยป,รรยถwยฝ)ยธร4ยธ ยบยผ-ยนยถ3ยนยป
ยฟรยฟรยธรยฟยยดbร'ยน^ยบ%รร&ยถ)รยถ)ร%ยน#รhรhยธ@รยบยนยยนร%รyรร%ยพ{ยธ@รรยบรร_ยฝยบ6ยนยนยถ3ยนยป`รรยถ<ยฝ)ยธQร4ยธ ยบยผยนยถ)ยนยปรยถ3ยน ร รยบ&ยฟ'ยถ)ร*รรรยธยผยบรยธ
ยผยธยพยผยธ ยฟ!ยธยนยร0ยบ6รยถ)ร%ยนยฟhรhยถ<ยฝ<ยฝ_ร{ยธร%ยนรDยฝ<ยถ)ยนยธyรยผ0ยบ%รรยบรยฝ)ยธ%ยด2รคQยธยผยธรยบยถ3ยน/รรรยยธยท&ยธยผยรยยถ3ร-ร@ยฟยธยท%ยธยผ0ยบeยฝ9รยบ%ยฟ'ยถ:ร ร รยธ ยฟรยถ)ร%ยนยฟรข
ยณ&ยด	ยตยถ)ยท%ยธยนยยบ ร รยบ%ยฟ'ยถ)ร*รรรยธยผ0ยบรยธdยผยธยพยผยธ ยฟยธยนร0ยบร!ยถ3ร&ยนQรร5ยบ/ร9ยฝ:ยบยนยนยถ)ยนยปQรรยถ<ยฝ)ยธ@ร4ยธ ยบยผยนยถ)ยนยปQยฟรยฟรยธร@รUยถ:ยฟHรรยธ
ยพยผร%รยฝ)ยธรยร6ร#รยนร&ยถ)ยนยป@ยบยน,ยธร?ร-ยถ)ยธยนยรยฟยบร!ยถยฟยรยยบ%รร-ร%ยผรยพยฝ:ยบยน/รรฃ#รDยฝwยถ)ยนยธHรยผ0ยบ%รร0ยบ6รยฝ3ยธรฅ
รยคยด	ยตยถ)ยท%ยธยนยบHรรรยธยผ0ยบรยธยผยธยพยผยธ ยฟยธยนร0ยบรยถ)ร%ยนsรรlยบร9ยฝ:ยบยนยนยถ)ยนยปรรยถwยฝ)ยธร4ยธ ยบ6ยผยนยถ)ยนยปยยฟ!รยฟร-ยธร@รยถ:ยฟยรรยธยพยผร&รยฝ3ยธร
รร#รยนร&ยถ)ยนยป@ยบยน,ยธยถ)รรยธยผ	ยธร?ร-ยถ)ยธยนยร	ร%ยผgยบยผ-รยถ3ร-ยผ0ยบยผรยฟ-ยบรยถ:ยฟ'รrยบ%รรร%ยผรsยพยฝยบ6ยน@รยผ0ยบ&รร0ยบรยฝ)ยธรrยธยถ)รรยธยผรรฃ#รDยฝwยถ)ยนยธHร%ยผ
ร%ยนรDยฝ<ยถ3ยนยธร-รฅ
รค ยธaรยยถwยฝ<ยฝรยผยธ ยบ6ร,รรรยธยผยบรยธMยผยธยพยผยธ ยฟยธยนร0ยบร!ยถ3ร&ยนยฟdรยผ0ยฟรeยดรฆรยผ@ยผยธ ยฟรยฝ)ร0ยฟxยผยธยปยยบยผร&ยถ3ยนยป ร รยบ&ยฟ'ยถ)ร*รรรยธยผยบรยธ
ร
ยผยธยพยผยธ ยฟ!ยธยนยร0ยบ6รยถ)ร%ยนยฟ9รhยถ<ยฝ<ยฝรยธgยบยฟ'ยถ)รยพยฝ)ยธรsรร&ยถ<รVรยบรยถ)ร%ยนsรรรงยบยผยธ ยฟรยฝ)รpยผ-ยธยปยยบยผ0ร&ยถ)ยนยปรรรยธยผ0ยบรยธยยผยธยพยผยธ ยฟยธยนร0ยบร!ยถ3ร&ยนยฟยด
รคQยธ	รhร%รยฝ:รsยฝwยถ)รจ%ยธยนรร1รรยฟรรร^รรรรรยธยพยผร&รยฝ3ยธรยรร2ร9ยฝ:ยบยนยนยถ)ยนยปรรยถ<ยฝ)ยธ>ร4ยธ ยบ6ยผยนยถ)ยนยปรยธยท%ยธยนsยถ)ยน,รรรยธยผยบรยธ
ยผยธยพยผยธ ยฟ!ยธยนยร0ยบ6รยถ)ร%ยนยฟรยถ:ยฟhยนร%ยนร*ร-ยผยถ)ยท%ยถ:ยบeยฝrยด
รยร%ยนยฟ'ยถ:รยธยผยบยนaยบ6ยป%ยธยนยร	รฉยรรร,รรยยธ ยฟ	ยนร%ร	รยบeยท%ยธ>รร&รยพยฝ)ยธรยธHยถ3ยนร*ร%ยผรdยบรยถ)ร%ยน@ร&ยนaร-รยธยธยนยท%ยถ)ยผร%ยนรsยธยนยรร{ยธร
รยบeยท%ยถ)ร%ยผeร{ยถยยดรชยธ%ยด)ร#รรยธยผ-ยธรยบeร,รยธdรร%ยผยธyรรยบ6ยนaร&ยนยธ>ยพ{รยยฟยฟ'ยถ)รยฝ)ยธ,รยธรยบeยท&ยถ)ร%ยผรรhรรยธyยธยนยยท&ยถ)ยผร%ยนรยธยนร ยดyรฉdรซรฌยฟ	ยพยฝ:ยบยน#ร
ยถ)ยนยฟรยธ ยบ%รsรร#รยธยถ)ยนยปยบyยฟยธ ร รยธยนรยธรรlยบ%รรยถ)ร%ยนยฟยร%รยธ รร&รยธ ยฟยบรยธ ร-ยถ:ยฟ'ยถ)ร%ยน,ร-ยผยธยธ%ยด2รฉรซรญยฟ2ยบ%รรยถ)ร%ยน#ร&ยถ)ยนรรยถ:ยฟรยบ&ยฟยธ%รยถ:ยฟhยบ
ร*รยนรรยถ)ร%ยน#รยนร%รยร%ยนยฝ)รรร#รรยธ	ร%รยฟยธยผ-ยทยบรยฝ)ยธยฟร0ยบ6รยธ%ร%รรรยบeยฝ:ยฟรyรร#รรยธ	ยพยบ%ยฟรhรยถ:ยฟรร%ยผ-ร>ร6รlรฉยด9รrยนdรรยธ	ยธรยคยบ6รยพยฝ)ยธ
รยธยนรยถ)ร%ยนยธ รHยถ)ยนรรยธยยถ3ยนรยผรรรรรยถ)ร%ยน#ร&รรยธยผยธรฉยรยบ&ยฟ9รร	รยผ0ยฟร2ร0ยบ6รจ%ยธpรรยธ	ยบ%รรยถ)ร%ยนรฎยรรร&ยถ:ยฟรยถ)ยนยป%รยถ:ยฟรdรยธรยบeยท%ยถ)ร%ยผ
รฏ	รฐ ร*ยผร%รยรยธรยบeยท%ยถ)ร%ยผ รฏgรฑ รรฉรซรญยฟ2ยพยฝ:ยบยน,รยบ%ยฟhยบร&ยถ<รฃ#ยธยผยธยนรยรยผยบยนรรdร*ร%ยผhรรยธรยบ%ยฟยธรรยธ	ยธยนยท%ยถ)ยผร%ยนรsยธยนยรร{ยธรยบeยท%ยธ ยฟ
ยบ%รรร%ยผร&ยถ3ยนยปdรร รฏ	รฐ ยบยนรsรqร&ยผรรยธรยบ%ยฟยธ	ยถ)ร	รยธรยบeยท%ยธ ยฟยบ&รรร%ยผ0ร&ยถ)ยนยปร-ร รฏ	รฑ ยด
รกร%รยธร-รยบร ร&ยถ)ยนยรยผ-รรรร-ยถ)ยนยป,รฉรซรญยฟhรยธรร%ยผ-รยบ%ยฟ	ยบyยพยบยผ0ยบรsยธรยธยผhยถ)ยน,ยถ)ร0ยฟยพยฝ:ยบยน,รฒรณรรยถ:ยฟhยถ:ยฟยธ ยฟยฟยธยนรยถ:ยบeยฝ<ยฝ3ร/รรยธ
ร&ยถ<รฃ#ยธยผยธยนรยธร{ยธรยรhยธยธยน@ยบยฟ!ยธ ร รยธยนรยธyยบยนร,ยบรยธ ร-ยถ:ยฟ'ยถ)ร%ยน@ร-ยผยธยธยบ%ยฟhรฉdรซรฌยฟ2ยพยฝ:ยบยนxรฒรดรยบeรรยบรยฟยธyยบยนยธรยพร&ยนยธยนยรยถ:ยบeยฝ
รยฝ)รรร*รยพdยถ)ยน,รรยธ5ยฟยยถ3รยธร6รรงร-รยบรpยพยฝ:ยบยน#รยบ6ยนรรยบeรyรยบรจ&ยธยถ)ยนรยผ0ยบ%รร0ยบ6รยฝ3ยธ	รรยธ	ร0ยบ%ยฟรจsรรlรยธยท%ยถ:ยฟ'ยถ)ยนยปdร%ยผpยท%ยธยผ!ยถwร*ร&ยถ3ยนยป
ยบHยพยฝ:ยบยน#รยธยท%ยธยนdยถ)ยนรรรยธยผ0ยบร-ยธgยผยธยพยผ-ยธ ยฟยธยนยรยบรยถ)ร%ยนยฟยด2รhรยถ:ยฟhรรยฝ:รยฟhยธยท&ยธยนรรยธยนdรhยธ5รร&ยนยฟ'ยถ:รยธยผยธร?ร-ยถ)ยธยนยรยพยฝยบ6ยนยฟรต
รถ ยธยนรยธ%รVร_ยฝยบ6ยนยนยถ3ยนยปdรรยถ<ยฝ)ยธร4ยธ ยบยผยนยถ)ยนยปdยธยท%ยธยน,ยถ)ยน@รsรรยธยผ0ยบ6รยธยฟรยฟรยธรยฟยยถ:ยฟgรร&รยพยฝ)ยธรยธยฝ)ร,ยนร%ยนร*รยผยถ)ยท&ยถยบยฝยยด

	
pรฝ'รฝ

รท2รธdรน,รบรปรผgรฝ'รพpรฟ

รrยนรรรยถ:ยฟยฟยธ รรยถ)ร%ยน`รhยธยฟรรร รรยบรยปยถ)ยท%ยธยนยยบdรรรยธยผ0ยบรยธdรrยผยธ ยฟ!ยพ#ยด ร รยบ%ยฟยยถ3ร*รsรรยธยผ0ยบ6รยธรgยผยธยพยผ-ยธ ยฟยธยนยรยบรยถ)ร%ยนQรรยบ
ร9ยฝ:ยบยนยนยถ)ยนยป`รรยถ<ยฝ)ยธ@ร4ยธ ยบยผยนยถ)ยนยปbยฟรยฟรยธร/ร2รรยธยนยธยท%ยธยผyรรยธยผยธdยถ:ยฟyยบMยฟยบรยถ:ยฟ'รrยบ%รรร&ยผรMยพยฝยบ6ยนยรrยผยธ ยฟยพ#ยด2ยบยน`ยธร?ร-ยถ)ยธยนยร
ยฟยบร!ยถยฟยรยยบ%รร-ร%ยผร@ยพยฝ:ยบยน4ร	ร*ร%ยผ>ยบ6ยนbยบยป%ยธยนร ร#ร-รยธยผยธyยถ:ยฟyยบ@ยฟยบร!ยถยฟยรยยบ%รร-ร%ยผร@ยพยฝ:ยบยน1รrยผยธ ยฟ!ยพ#ยดlยบยนQยธร,ร-ยถ3ยธยนรยฟยบรยถ:ยฟ'รrยบ%รรร&ยผร



 !"$#%'&$(
	) *,+-. /'0$+1/32+-54$687'6)796:;6-</96=?>@-5)$A1*@B	-ACD>,+E*F:;)$+2G6H+-$=I2+-54$6J29062'K!6=D>L-I)$A1*@B	-ACD>,+E*M/;>LCN6O
P :RQR68CN6-</S>@A!-6=H/'0	>,:T>,:U+V-A-W/97;>LX>,+E*MYZ+!2G/36X!6-5Y[A7RCNA=	67+/96J:;B\:S/'6C]:OM^68)7'AX68/90687'6:;_	*L/
YA78CNA=	67+/'6N:;B:;/'6C]:`Ha+-$=/'06-b:S0AcQdQe0<Bf>@/g>,:J+))	*h>,2+4	*L65Y[A!78/'06D7;>,2'067D2GA-</96Gi\/8AYkj_$+!:l>LW
CNA=	67+/'6J:SB\:;/96C]:O
m 068)7'A<A1YnAYFA_7kA1opWq*h>L-6J/97+2G/+4	>h*h>L/;B]7'6:S_	*@/QR>h*h*rYA*h*LAQsY[79ACt/'06YA*h*LAcQu>@-v?*L6CNC]+:`O
wFxzy{y}|~rยzยยยpยEยrยยย'ยDย5ยยยยLยqยยยยยยGยยยยhยยVยยยJย!ยย<ยqยยEย,ย	ย]ยNย!ยยย\ยยย ยqยยยIยDยย!ยย;ยยยDยRยยย!ย$ย$ยqย<ย
ย ย<ยqยhย8ยaยย!ยยzย,ย	ยVยยยEยยยยยขยก ย ยqยqยยคยฃgยยยยยยยhยJย'ย`ย<ยยEย,ยย9ยNยฅยยฆLยยฆhยFยย;ยยยยqยย,ยยยยง	ย$ยยยqยยยจยยฉยnย!ย$ย8ยช8ย<ยยยยยEยย`ยซ
ย<ย`ยยยVยยยยยยยฆ5ยฌยญย<ยยยจย ยqย<ยย;ยNย`ยฎcยยฏยยยqยJย5ยยยยยฏย,ยยยยย!ยยgยยยhยยIยยฐ$ยยยDยย`ย<ย,ยEยยqย<ยgยbยqย}ยqย	ยยRยยยยยยVย ย ย<ยEย;ย
ยqย<ยNยhยย	ยย`ยยยzยยยqยยฑยSยยฒย ยฐ ยยฏยยณย'ย!ยง<ยzยยยIยยVยฃUยดGยชยฆ
ยตJยถEยทยท$ยธGยRยน YF/'06ยณ+1v6-</Rยบย)$67;YA7'C]:e+E*LA-v]+-	B])$+1/'0ยAY ย CNA7968/'0$+- ยช +2G/;>LA-$:RQu>@/90A_/k*L6+79-	>@-v
+-	B</90	>@-v5ยป>Oยผ6OยฏH\+c*@A!-v ยช +2G/;>LA-$:T/'068+v6-	/k=	A<6:T-A/Rv6/k+-	BN-6Qยฝ>L-	Y[A79C]+/;>LA-I+4$A!_/k/'068+2G/'_$+E*
4$60$+EX!>LA7.H/'06-ยพ>@/8CD_$:;/X!>ยฏ:ยฟ>@/D+5)$+7'/;>,2G_	*,+7VA4$:;67'X+4	*L6]:;/+/96J/;QR>,2G6HQR>L/'0A_/8v6/9/;>L-vย+-<Bร-6Q
>L-	Y[A!7'C]+/;>LA-ย+14$A_/k/906J+2G/'_$+E*a4$60$+EX!>LA7H+-$=5/'067'6GYA7'68QR682+-}:;07S>@-K ย 4	BI=	7'A))	>L-vยฑ+!2G/;>LA-$:
Qk0	>,290}/9A<AKI)	*,+2G6?4$6/;QR66-}/906:;6JX!>,:l>L/:O8^6N2+-ร)$67;YA7'Cร/90	>ยฏ:)79A\2G6:':e_-	/;>h* /'067'6DQR>h*h* 4$6D-A
:;6j!_6-$2G6JAY ยช +2G/;>LA-$:T>L-รQk0	>,2'0ร-AD*L6+7'-	>L-vIA\22G_7G:O
m 06*@6+17'-	>L-v]AYF/'06J+v!6-</T>,:RCNA-A!/'A-	>,2รFQk06-6X67e>L/R*L6+7'-6=ร:;ACN6/'0	>L-v]+14$A_/k/90686-<X!>LW
7'A-C?6-</84z60$+X!>LA7HzY_/'_7'6V>L-	Y[A!7'C]+/;>LA-{2+-5รยฟ_$:;/8C5+K6J/90	>ยฏ:gK<-AQR*L6=	v6DCNA!7'6N2GA-$2G7'6/'6ODร>L-$2G6
/'06V-<_CD4$67AYF)$A<:9:l>L4	*@6D4z60$+X!>LA7:u>,: ยฃ H$QR68v6/k/90$+/U+?K<-AQR*L6=	v6g>@-$2G796+:;6ยณ2+1-ยA22G_7ยฒ+/eCNA<:;/
ยฃ /S>@C?6:O
ร ACD4	>@-	>L-v5/'06J+4zAcX!68A4$:;67'X+/;>LA-$:T*L6+=:e/'AN/'06J=	6:ยฟ>@796=I7'6:;_	*L/O
wFxzy{y}|~rqรkย ยaยย8ยรย9ยยยยยยLยqยยยยยยGยfยยยhยยยkยยยรยรยDยยยEย;ยยยย3ยhยยzย$ยqย<ย ย ย<ยqยhยยaยยยGย$ยqย<ย
ยยยยยยรยก ย ยqยqยยฑยฃ?ย\ย1ยยย[ย`ยhย]ย'ย`ย<ยยEย,ยย9ยGยuย!ย$ย ย ย	ยย;ยIยqย<ย5ยยย!ย<ยย`ยยยยญยยยqยย,ยยรยยฏยยคยSยยคยhยEย<ยยqย5ยชยยฆรยฌaย<ยยยจย
ยqย<ยEยยฟยVยยฎยLยยqยยฒย5ย;ย;ยยย;ย`ยยยEย$ยยยยqยยIย;ยkยรยฅlย,ยfยยยยฏรยTย\ยยhยย$ยยDยqยยย,ย5ยฃ5ยย$ย8ยชZยฉยณยยง	ยยIยqย<ยย ยEยยGย ยยย,ย	ยDยqย<ยยFย
ยยฏย8ยยยยยยฏยqยยยยEยยยGยNยย!ยยพย9ยDยย!ยยGย,ยGย5ยยง<ยRยqยยยยqยDยย\ยยhยย$ย!ยยณยqยยaยqย]ยฃfยย$ยVยชยยฆ
ยตJยถEยทยท$ยธGย}m 06ย2GA-$29>,:;67'6)7'6:;6-	/+/;>LA- ย ยฐ AY ย 2GA-$:l>,:;/G:DAYยฒ+/G+4	*L6H Qk06796ย6+2906-</97'BAY8/'06
/+4	*L6J2GA797'6:;)$A!-$=:R/'AN+D=!>ยฏ:S/;>L-$2G/UA4$:S67'Xc+4	*L6V0	>ยฏ:S/'A7'BNA1Yร+-?>L-</967+2G/;>LA-IAYM/'06J+v6-	/RยบdQR>L/'0I/'06
6-	X>L7'A-C?6-</Hz+-$=ย2GA-	/+E>L-$:k+1-ย+2G/;>LA-I/'AN4z6J/+1K6-]4	BIยบยขYA7k/'0$+/k:;)z629>hรย2Nยปl)$+7'/;>,+E*.3:'2G6-$+17;>LA$O
m 06ย-	_Cยณ4z67IAYยณ=!>,:;/;>L-$2G/I6-	/'7;>L6:?>L-ร/906ย/+4	*L6{ยป ย ยฐ .N2+-ร4z6ย*h>LCD>L/'6=ร/'A>L-$29*L_$=	6bA!-	*@Bยฝ/'06
)	*,+_$:l>L4	*L6N=!>ยฏ:S/;>L-$2G/k0	>,:;/'A7;>L6:ยปZ>ZOร6OLH	/'06U0	>,:;/9A7;>L6:RQk0	>,290ย2+-]4z68v6-67+/96=z.rYA7R/'068:;B:;/'6C ยก +-$=
/'068)	*,+- ย O m 068-	_Cยณ4z67kAY :;_$290ย=!>ยฏ:S/;>L-$2G/U0	>,:;/'A!7;>L6:T>,:k4zA_-$=	6=I4	B ยฃ ยป/'068-	_CD4$67kAYM)$A<:':ยฟ>@4	*L6
4$60$+EX!>LA7:eAYr/'06V6-<X!>L7'A-CN6-	/. /;>LCN6: ยช ยปl/'06J=!>hop6796-</U:S/+v6:T>L-+N:;)$629>hรย2g>L-</967+2G/;>LA-.O
ยน -]A!7=	67 /'AVX67;>hYBยณ/90$+/3+g)	*ยฏ+1- ย ยฐ Qk0	>,2'05>,:T7'6)7'6:;6-	/'6=ยค>L-]/'0$+1/ C]+--67ร>,:3:9+/;>,:lY+2G/'A7'BH!A-6
-66=:e/'ADvADAcX!67u+c*ย*M)$A<:':ยฟ>@4	*L6D4$60$+EX!>LA7:k+-$=?YA7k6+!2'0IA-68A1Yr/'06Cร29062'K5/'0$+/ ย ยฐ *L6+=:e/'ADยบ]รร:
vA<+c*ZO
P :k+-5>LCNC?6=!>ยฏ+1/'6ยณ2GA79A*h*ยฏ+17'B]QR68A4/G+E>L-ย/'06YA*h*LAcQu>@-vzร
รDร x	ยทยยถEx$yร~rLรeย ร ย,ยzยยqย<ยย{ยยยยยฏย,ยยยEยยยยfยยยhยยยยยรยย$ยยDยEยยย;ยยยยRยhยย$ยzย,ย	ย ย ย<ยqยhยยaยยยGย$ยqย<ย
ยยยยยยรยLย8ย;รkยซ;ยยยqย$ยDยย;ยยEยย<ยยhยcยฆ
ร A-$:l>,=	67]+-ร+v!6-</DQk0AQu>ยฏ:S06:D/'A7'6+2900	>,:]=	6:;/S>@-$+1/;>LA->L-/'06I0A<:S/;>h*@6ร6-<X!>L7'A-CN6-	/NAY
ร62G/;>LA-sรO ยน -ร)7;>L-$29>L)	*L6HU/9067'6ICD>Lv0	/ยณ4z6ย6Gi)$A-6-	/;>,+E*h*LB<WC5+-<Bยฝ0	>,:;/'A7;>L6:DA1YยฒA4$:S67'Xc+/S>@A!-$:D/'06
รรร

รkร{รกrรขรฃรรรคยฏร$รฅรฆ8รงรคLรขรจรรฉ$รจรฃรชรรคยฏร$รฅ
รซ รฌรญรฎ	รฏIรฐ]รซEรฑรญรฎ$รฒGรณรดรฎ	รฏ'รญรตรถtรทeรญรธรญรต'รฏ'รนรญGรบLรญรป'รป`รผ8รณรดรตIรต'รญรป;รด	รบLรฏรป'รซEรฑ\รป5รฏ'รน$รซ1รฏ5รฝ@รฏ5รฝ,รปfรญรฎรณรดรฌรนรพรฏ'รณรรฒGรณ!รฎ$รปlรฝ,รฟ	รญรตยรณรฎ	รบLรฑ
$รณ1รบ@รฑ	รฎรณรฐDรฝ,รซEรบhรบLรฑรฐ]รซรฎ	รฑรณ รฏ'รนรญรฐรรฝLรฎรณรตรฟ	รญรตรฏ'รณIรป$รญรฒ9รฝรฑ}รฏ9รนรญยณรซรต'รณรต;รฝ,รซรฏ'รญ		รบ,รซรฎaรถ
Rรน	รฝ,รปeรฝ,รปรฐNรณ<รป;รฏรนรญGรบ[รด	รบ
รณรตkรฏ'รนรญJรฟ	รญรปlรฝLรฌรฎรญรตaรป;รนรญRรฝhรบhรบ$รญยณรซ	รบ@รญVรฏ'รณNรต'รญรต9รญรป;รญรฎ<รฏeรนรญรตยฒรป;รดรฌ!รฌรญรป;รฏ'รญรฟIรป;รณ1รบ@รดรฏSรฝ@รณ!รฎยรฝLรฎรซDรต'รญGรบ,รซรฏ;รฝLรธรญGรบLรฑ}รฒGรณ!รฎ$รฒ9รฝยฏรปSรญ
kรซEรฑ
รถ 3รฏ9รนรญยณรป;รดรฌรฌ!รญรป;รฏ'รญรฟยรปSรณรบLรดรฏ;รฝLรณรฎรรฝยฏรปรฎรณ!รฏUรป'รซรฏ;รฝ,รป รซรฒGรฏ'รณ!รต'รฑรผ$รฏ9รน	รฝยฏรป รซรฒGรฏ uรฝยรบhรบ zรญยณรญ Iรฒ9รฝ@รญรฎ	รฏ;รบLรฑ}รฟ	รญรฏ9รญรฒGรฏ'รญรฟรผaรซรฎ$รฟ
$รญรต9รน$รซ $รปRรฒรซรฎ $รญรต'รญ $รซEรฝLรต'รญรฟรถ 
Rรนรญ รต9รณ 	รบLรญรฐ รณ ยรฎ$รซEรธรฝLรฌ<รซ1รฏ;รฝLรณรฎNรฝLรฎIรซ8รนรณ<รปSรฏ;รฝhรบ@รญรญรฎ	รธรฝLรต'รณรฎรฐ?รญรฎ<รฏ 3รญeรฐNรญรฎ<รฏSรฝ@รณ!รฎรญรฟ
รซ zรณcรธ!รญรฝยฏรปeรซรฒGรฏ'รด$รซEรบhรบLรฑยรป;รณรบLรธรญรฟfรฏ'รน	รฝ,รป kรซEรฑรถ
รทeรณรฏ;รฝ,รฒGรญDรฏ'รน$รซรฏ !รญรฐNรฐ5#รซ "$รถ $gรฝยฏ&รป %รด	รฝLรฏ'รญ5รป'รซรฏ;รฝ,รป รซรฒGรฏ'รณรต9#รฑ [รณรตรฐNรณรฟ	รญรตรซรฏ'รญNรป;รฑรป;รฏ'รญรฐ5รปรถ 'kรณ RรญรธรญรตรผยจรฝL(
รฎ %รด$รซ!รปlรฝ 
รฐNรณรฟ	รญรตรซรฏ'รญรป;รฑรป;รฏ'รญรฐ5รปNรฏ'รน	รฝ,)รป !รญรฐNรฐ]รซ รฝ,รปIรฎรณ!รฏIรด$รป;รญ [รด	รบรผ8รปlรฝLรฎ$รฒGรญ รฝ@รฎรรฏ'รน$รซรฏIรฒรซรป;+รญ *]รฐยครฝ@รฌ!รน<,รฏ zรญ}รญ -$รณ!รฎรญรฎ<รฏ;รฝ,รซEรบ
รฝLรฎsรฏ'รนรญรซรฒGรฏ'รด$รซEรบ8รต'รญ รต'รญรป;รญรฎ	รฏรซรฏSรฝ@รณ!รฎรรปlรฝ .รญ/
รถ 'eรณ 3รญรธ!รญรตรผ รฏ9รน0รญ รต9รณ $รญรต'รฏSรฝ@รญรป5รณ รฏรซEรฝLรฎรญ1
รฟ <2
รฑ !รญรฐNรฐ]3
รซ "$รถ $รฒรซรฎ
$รญรรต'รญรฌ<รซEรฝLรฎรญ1
รฟ 	รฑรรฒGรณรฎ$รปlรฝ,รฟ	รญรต;รฝLรฎรฌรญ ยฑรฒ9รฝLรญรฎ<,รฏ 	รบ,รซรฎ$รป`5
รถ 4รณรตNรฐ?รณ<รป;รฏ รตรซ!รฒGรฏ;รฝ,รฒรซE6รบ รด7รต $รณ<รป;รญรป`รผ 3รญยรฟ	รณ{รฎรณรฏ?รบLรณ<รป;รญ
รฌรญรฎรญรตรซcรบยรฝLรฏ;,รฑ 	รฑ]รต'รญรปSรฏ'รต;รฝ,รฒGรฏ;รฝLรฎรฌNรณรดรตUรซ1รฏ'รฏ'รญรฎ	รฏ;รฝLรณรฎIรฏ'รณDรญ ยฑรฒ9รฝLรญรฎ<8รฏ 	รบ,รซรฎ$รปรผรปlรฝLรฎ$รฒGรญยณ	รซ 	รบ,รซรฎรฎรญ8รต Rรฝhรบhรบnรฎรณ!รฏ $รญDรซ 	รบLรญJรฏ'รณ
รญ -รญรฒGรดรฏ'รญ]รญ -$รณ!รฎรญรฎ<รฏ;รฝ,รซEรบhรบLรฑ{รฐ]รซรฎ	รฑยพรซ!รฒGรฏ;รฝLรณรฎ$รปรฝLรฎ{รฏ'รนรญ]รฒGรณรดรตรปSรญNรณ U,
รซ 	รบ,รซรฎa)รถ 9kรฝLรธรญรฎ{รฏ'รน$รซรฏ !pรญรฐNรฐ],รซ "z;รถ :ยรฟ	รณ	รญรป
รนรณรบ,#รฟ รณรต %!รด$รซรปlรฝ รฐNรณรฟ	รญรตรซรฏ9รญ8รต'รญ รต'รญรป;รญรฎ	รฏรซรฏ;รฝLรณรฎ$รป`รผ Rรญ8รฌรญ=รฏ <
>	?A@BDC@FEHGJIGLKNM OQP=RS(T)U=VTWOXY[Z\R=]T^R`_baTScSFOQSed#fhgeOQaRijRT]kScOlSd`WnmW^R=Y&oApbSF\OlSdqTS(RsrtX
u OQR=Sc^LWT^OvWlwT u ^Z]km8xAayTS0OvWZz XaOlScR^]T u ^T{aR}|

uรน	รฝยฏรปMรต9รญรป;รด	รบLรฏFรฝ,
รป %!รด	รฝLรฏ'รญUรป'รซรฏSรฝยฏ~รป ZรซรฒGรฏ9รณรต'รฑรผรปlรฝLรฎ$รฒG6รญ %!รด$รซรปlรฝรฐNรณรฟ	รญรตรซรฏ'รญkรป;รฑรป;รฏ'รญรฐ5รปFรซรต'รญkรซรต;รฝ,รฒ'รน5รฒGรณรฎ<รฏ9รญ-\รฏรถh4รณรตFรญ-
รซรฐ	รบLรญรผรป;รณรฐ?รญUรซรตรฒ9รน	รฝLรฏ'รญรฒGรฏ'รดรต'รญรปRรป;รด$รฒ9รนยฑรซ!รปFรฏ'รนรญkรณ!รฎรญรป รฟ!รฝ,รป'รฒGรด$รป'รป;รญรฟ#	รฑqTรต'รณ	รณยรป รซรฎ$รฟDรน	รฝ,รปRรฒGรณรบhรบLรญรซรฌรดรญรป8ย รต9รณ<รณยรปรผ
$=ยยยยpรฒรซ1
รฎ $รญรฏ'รต'รญรซรฏ9รญรฟยณรซ!รป %!รด$รซรปlรฝ รฐNรณรฟ	รญรตรซรฏ'รญUรปSรฑ\รป;รฏ9รญรฐ]รปรถL
RรนรญรฑVรฝLรฎ$รฒ9รบ@รด$รฟ	รญJรซ8zรณรบLรฑ<รฎรณ!รฐDรฝ,รซEรบaรฎ<รดรฐ	$รญรตTรณpรปSรญรฎ
รป;รณรตGรปAรผ kรน	รฝ,รฒ'รน{รฒGรณรต'รต'รญรป zรณรฎ$รฟรฏ'รณยรซ1รฎ}รญ -$รณ!รฎรญรฎ<รฏ;รฝ,รซEรบRรฎ<รดรฐ	zรญรตJรณ$รณ<รป'รปยฟรฝย	รบLรญ5รณ$รป;รญรต'รธรซรฏ;รฝLรณรฎ$รปรผMรซรฎ$รฟรซรต9รญยณรฏ9รญรป;รฏ'รญรฟ
รซรฌ<รซcรฝ@รฎ$รปSรฏUรซVรบhรฝ,รป;รฏUรณ J$รณ<รป9รปlรฝ 	รบ@รญDรญรฎ	รธรฝLรต'รณ!รฎรฐNรญรฎ<8รฏ $รญรน$รซEรธ!รฝLรณรต8รป ยรฝรถยผรญรถยฏรผรฏ'รนรญJรซ รต'รณ รต;รฝ,รซรฏ'รญJรป;รญรฎ$รป;รณ!ยรต รญ ยaรญรฒGรฏ'รณรตkรฐNรญรฒ9รน 
รซรฎ	รฝ,รป;รฐรรฝ,รปIรฒ9รนรญยรฒ ยรญย
รฟ รณรตIรซ รบยรฝ,รป;รฏIรณ Jรญรฎ<รธ!รฝLรต'รณรฎรฐNรญรฎ	,รฏ $รญรน$รซEรธ!รฝ@รณ!รตรป ยย
รถ ย8`รป 3รญรฐNรญรฎ	รฏ;รฝLรณรฎรญ1
รฟ $รญ รณรต'รญ8รผ %รด$รซ!รปlรฝ 
รฐNรณรฟ	รญรตรซรฏ'รญรป;รฑรป;รฏ'รญรฐ]รป5รฒGรณรต'รต'รญยรป $รณรฎ$รฟยฝรฏ'รณรฒGรณรฐ 	รบLรญ - รป;รฑรป;รฏ'รญรฐ]`รป kรนรญรต'รญรฏ'รนรญรฎ	รด[รฐ zรญรตIรณ $รณ	รป'รปlรฝ 	รบLย
รญ Rรณรต;รบ,รฟรป
รฟ	รญรป'รฒGรต;รฝ 	รฝLรฎรฌรฏ'รนรญNรญรฎ	รธรฝLรต'รณ!รฎรฐNรญรฎ<รฏgรฝ,รป8รญ ยฑรฒ9รฝLรญรฎ	รฏ;รบLรฑรญรฎ<รดรฐ?รญรตรซ 	รบLรญqรถ 
RรนรญรปSรญ]รฒGรณรฎ$รป;รฏ;รฝLรฏ'รดรฏ9รญ]รซ5รต;รฝ,รฒ'รน{รซรฎ$รฟรซ zรญรซEรบ 
รฝLรฎ)รฌ ZรซรฐยครฝยรบLรฑ{รณ Uรป;รฑรป;รฏ'รญรฐ]ยรป ย~'Uรซcยรบ zรญรต'ย
รฎ ยยย รซรตรฟ!รฝรผ $=ยยย$ย)รถ ยkรดรตDรต'รญรป;รด	รบLรฏรปDรป;รนรณ Jยรผ [รณ!รตJรญ -$รซรฐ 	รบLรญรผFรฏ'รน$รซรฏVรฏ'รนรญ
รฏ'รตรซ1รดรฐ]รซ รฒรซรต'รญรป;รฑ\รปSรฏ'รญรฐtรฟ!รฝยฏรป9รฒGรด$รป'รป;รญรฟ?รฝLรฎIรฏ'รน8รญ รต'รญรธรฝLรณรด$รปeรป;รญรฒGรฏ;รฝLรณรฎยรฒรซ#รฎ $รญ รด	รฝhรบLรฏUรซรปRรซรฎ5รญ -ยzรญรต'รฏkรป;รฑรป;รฏ'รญรฐรรฏ'รน$รซรฏ
รฟ	รญรธ!รฝยฏรปSรญรปRรฏ'รนรญรฎรญ -\รฏkรซรฒGรฏ;รฝLรณรฎ5รฏ'&รณ $รญ $รญยรต [รณรต9รฐNรญรฟ $รซ!รป;รญรฟNรณ!รฎ]รฏ'รนรญรน	รฝ,รป;รฏ'รณรต9รฑNรณ rรณ $รปSรญรต'รธcรซรฏSรฝ@รณ!รฎ$รป 	รฑNรฏ'รนรญ8รฟ	รณรฒGรฏ'รณรตรถ

Rรน
รญ รต9รณ 	รบLรญรฐรรณ kรฒGรณรฐDรฝLรฎรฌยรด ยuรฝ@รฏ9รนรฏ'รนรญ 	รบ,รซรฎรฐ5รซรฑรรฎรณรฏ $รญ5รฏ'รต;รฝLรธ!รฝ,รซEรบjรผ รดรฏJรณ!รดรต8รต'รญรป;รด	รบLรฏรปJรปSรน}รณ dรฏ9รน$รซรฏJรซ
รฒGรณรฎ$รฒ9รฝ,รป;รญkรต9รญ รต'รญรป;รญรฎ	รฏรซรฏ;รฝLรณรฎDรณ ย8รซ 	รบ,รซรฎ eรน	รฝยฏรฒ9รนDรฝ,รปMรญ ยฑรฒ9รฝLรญรฎ	รฏ;รบLรฑยณรธ!รญรต;รฝ ยยรซ 	รบLรญUรฟ	รณ<รญรปรรญ -<รฝ,รป;Lรฏ eรนรญรฎรญรธรญรต รซรฎDรญ ยฑรฒ9รฝLรญรฎ<รฏ
รป'รซรฏSรฝยฏ~รป ZรซรฒGรฏ9รณรต'#รฑ 	รบยฏรซ1รฎ}รญ -	รฝ,รป;รฏรปรถ 
Rรนรญรต9รญ [รณรต9รญรผ$รฏ'รนรญDรญ ยaรณรต'รฏeรณ  รฌรญรฎรญรตรซรฏSรฝ@รฎรฌ5รฏ'รนรญNรซ รต'รณ รต;รฝ,รซรฏ'รญ 	รบ,รซรฎรณ ยjqรบhรฝLรฎรญVรฝยฏรป
Rรณรต'รฏ'รน kรน	รฝhรบLรญรถ
ยJย#ยยยLยย8ยยยยกยฃยข}ยยยคยฅยฆDยงยคยฆAยจยยฉยยsยคยช

รฎ?รฏ'รน	รฝ,รป รป;รญรฒGรฏ;รฝLรณรฎ`3รญkรป;รนรณ{รฏ'รน$รซรฏMรฝLรฏMรฝ,รปFรฎรณรฏMรบhรฝยรญGรบLรฑNรฏ'รน$รซ1รฏFรฏ'รนรญรต'รญTรฝ,รป รซรฌรญรฎรญรตรซcรบยรซEรบLรฌรณรต;รฝLรฏ'รนรฐdรฏ'รณJรฒGรณ!รฐNรญRรดRรฝLรฏ'รน
รซIรป'รซรฏSรฝยฏรป~ZรซรฒGรฏ9รณรต'รฑย	รบ,รซรฎ3รณรตJรซรฎ	รฑยรฐNรณรฟ	รญรตรซรฏ'รญ#ยซMรบ,รซรฎรฎ	รฝLรฎรฌ+kรน	รฝhรบ@รญ,!รญรซรต9รฎ	รฝ@รฎรฌรป;รฑรป;รฏ'รญรฐรรผaรปlรฝLรฎ$รฒGรญยฌ;รด$รป;รฏยณรฟ	รญรฒ9รฝ,รฟ!รฝLรฎรฌ
kรนรญรฏ'รนรญรต5รป;รด$รฒ'รนรรซยฃ	รบยฏรซ1รฎรรญ-	รฝยฏรปSรฏรปVรฝ,รป]รท8ยซLรน$รซ1รตรฟรถยฎยญรญ,รต'รณรธรญ5รฏ'รนรญIรต'รญรป;รด	รบLรฏ`[รณรตDรฏ'รนรญ,$รซ!รปlรฝ,รฒ#รตรซรฐNรญRรณรตยยรณ
ยฏ รญรฒGรฏ;รฝLรณ5
รฎ :1
รถ ยรรปlรฝLรฐDรฝhรบ,รซรตIรต'รญรป;รด	รบLรฏ5รนรณรบ,รฟรปNรต'รญรฌ	รซรตรฟ!รฝLรฎรฌbรญ Iรฒ9รฝ@รญรฎ	รฏยรป'รซรฏSรฝยฏ~รป ZรซรฒGรฏ9รณรต'ย
รฑ 	รบ,รซรฎ$รปยฐ
รถ 
Rรน	รฝ,รป uรฝยรบhรบ8รฝLรฐ 	รบLรฑ
รปlรฝLรฐDรฝhรบ,รซรต8รต'รญรป;รด	รบLรฏยฑรป รณรตรฏ'รนรญยณรฒรซ!รป;รญJรณ  รญ Iรฒ9รฝ@รญรฎ	รฏJรป'รซรฏSรฝยฏ~รป ZรซรฒGรฏ9รณรต'#รฑ 	รบยฏรซ1รฎ$รปeรฝ@ย
รฎ %!รด$รซรปlรฝ รฐNรณรฟ	รญรตรซรฏ'รญ ยซnรบยฏรซ1รฎรฎ	รฝ@รฎ,รฌ eรน	รฝยรบLรญ
!รญรซรต9รฎ	รฝ@รฎรฌIรป;รฑรป;รฏ'รญรฐ]รป`รผ\รซ1รฎ$#
รฟ รณรตRรฏ'รนรญJรญ -รฏ'รญรฎ$รฟ	รญ#รฟ รตรซรฐ?รญ 3รณ!ยรต ย\รปRรฟ!รฝ,รป'รฒGรด$รป'รปSรญรฟ5รฝLรฎยรฏ'รนยฒรญ [รณ1รบยรบLรณ RรฝLรฎรฌIรป;รญรฒGรฏ;รฝLรณรฎaรถ

uรน	รฝยฏรปfรต'รญรป;รด	รบLรฏยรฏ9รณรฌรญรฏ'รนรญ#
รต RรฝLรฏ'รนรพรฏ'รนรญ รต'รญรป;รด	รบLรฏรป5รณ รฏรซcรฝ@รฎรญรฟ รฝLรฎรพรฏ'รน+รญ รต'รญรธ!รฝLรณรด$รปรป;รญรฒGรฏ;รฝLรณรฎ รฒGรณ`รฐ 	รบ@รญรฏ9รญรฏ'รนรญ
รฒ9รบ,รซรป'รปlรฝ ยยรฒรซรฏSรฝ@รณ!รฎ}รณ ยณยซMรบ,รซรฎรฎ	รฝLรฎ,รฌ kรน	รฝhรบLรญ !รญรซรต'รฎ	รฝLรฎรฌIรฟ!รฝยฏรป9รฒGรด$รป'รป;รญรฟfรฝLรฎ ยฏ รญรฒGรฏ;รฝLรณ+รฎ ยดรถ


>	?A@BDC@FEยถยตDIยทKยธM OQP=RS,TqY	Z=\R=]T^R_aTSFScOlSd	fhgeOlaRยฒiยนRT]SFOQSed&WmWn^RYoh\R u Ol\OlSdfhgeR^QgeR=]^QgeR]R
RยบOW^QWTSยยปT]n{ยผOl^]T]mZ]&Rr u OlRSF^ยพยฝ	WnT^OvWQwnT u ^Z]m8xDaTS&wnZ]^QgeR	T=dR=SF^ยยฟรOWTS)ร8_XgeT]\xA]~Z{aR=Y|
ร=รkร

รรรรร+รรรtรรรรรรรFรรยรFร

รรรAรFรรยฐร รรรรรรรeร5รรรรกรขรครฃรฆรฅรง7รจ[รฉรชlรtรซรฌ8รฅรรรง,ร}รรงรlรรญรชร=รฎยรฏรฐ=รฑรฒรฒรฒรฑรฏ=รณtรรFรด5รตรฅรFรฎรlรฎรถรรรท2รฅรฃยรต7รชlรรฉFรฎร=รฎ
รธ รฐ รฑ=รฒรฒรฒvรฑ รธรน รฌรบร+รตรฅรFรฎรถยรงยรฉFรตรถรฌhรรรผรปcรฅรชรeรรฅรจ	รlรรช6รถยรยรจ`รรฌbร+รจ`รฅยรดรรงkรรถยรยรฝhรชlรรรรรรทtรบ รพรรชรยรฟAร=รรงยรรรรทtรฎรยรฎยรถยรรจ
 รฌรฎรฉFรต7รพtรถยรพFรรถรถยรพรรง7ร,ร eรlรฎรถkรฎ#รยรฎยรรถรlรฎรฃsรรตรถยรฅรงยรยรปรชlรรยรฃรฆรฅ
รง  รร  รรฃ[รรFรดยฎรฅรรชรยฎรรฃรถยรพรรงยร,รรlรฎรถkรฎ#รร
รรฎยรฎ~รยรทรรจรรeรถยฑรถยรฅqรฏรฐ=รฑรฒรฒรฒรฑรฏรณรถยรพFรรถ8รฎยรรถรlรฎ Fร=รฎ 

รซ 	รรยรFรตร	รฎยรรถรlรฎ Dรรญรyรชรรถรยรฅรฃร	รรรรกรข รฃรฆรฅรง7รจ[รฉรชlรยฒรl
รฎ 8รฝLรรพFรรงkรดAรฌ
รถยรพรlรฎยฑรรจรปรชรร=รฎรถ7รพFรรถ6รดร=รต7รlรดรรรท+รบ รพรรถยรพรรง8รถยรพรรงยร	ร รvรฎยรถkรฎร,รฎยรรถรlรฎรฃยรรตรถยรฅรงยรqรปรชlรรjรฌยรรรร)รฃรฆรฅรงรจรฅรดรรงkรรถ7ร[รฎรรฎร
รถยรรจqรฎยผรฌรlรฎ8ร
ร 8รฝLรรพFรรงรด,รปรงยรฅรญรชร
รจ 	 รฉรง8รงยร=รดรฉFรตรถรรฅร+รบรรชรชรพรฅรชlรด รฃรฆรฅรงยฑรถยรพรรตรรฎรรฅรฃยณร  รต7รรรรถรฎยรรถรlรฎรฃยรรตรถยรฅรงยร
รปรชlรรFรฎ8รรฎรบรรชรช 	
 รฌ รl
รขรพรยรฎรรถรฅรฃรฅรญFรฎรรงยร}รรญรชยรยรฎรถรรถยร=
รฎ รฌ รรยรถยรพรยรฎรรฎรถยรรจ
รฎ รฑ รฐ=รฑรฒรฒรฒรฑ รณ  รน Aรฐ 	ยรขรพร,รปFรฅeรฎยรฎ~รยรญรชร
รญFรรพFรรรรฅรงkรฎ	รฅรฃ8รถยรพร,รรeรรรงยรฅรรจรรรถqรรง7
ร ! รฐ " $# รฑรฒรฒรฒรฑ %รณ " $# &รฑ รฐ " #รฐ รฑ=รฒvรฒรฒ&รฑ รณ " #รฐ ')( รถยรพรรง7ร รรง7+
ร *,1รปcรฅeรฎยรฎรรญรชรยรญcรร
รพFรรรรฅรงรฎ -&		รขรพร	รรรรถรlรรชรฎรถkรรถยร&รl.
รฎ  รฐ 	รขรพรqรฎรรถรฅรฃรปFรฅeรฎยรฎ~รยรญรชร,รรตรถรรฅรFรฎ6รฃรฆรฅ/
รง  รv0
รฎ 21 รฑ 13&รฑ 4 รฐ รฑ=รฒรฒรฒv&รฑ 46576	98;รฎ
รทรฅeร}รชDรlรฎยฑรถยรฅ	รงยร=รรต7รพ,รถยรพร[รฎยรถkรรถย/
ร  รณ  รน Aรฐ 	
รขรพร รฎรถรรถย9
ร ยรlรฎ;
ร :<>=?@BADCFEHG%<JIรฌhรบ รพรรงยรยรรร(รรตรถรรฅรยฎรถยรพFร
รถ รรถkLร Kร=รฎ6รฃรฆรง7รฅM
รจ 	รงยร=รฎรฉรชรถkรฎ	รญFร7รต K(รN
รถ 
( รบ รพรlรต7รพ,รlรฎ รร)รฉรFรฎรฉFรตรตร=รฎยรฎรฃรฉรชรฎรถkรรถยLร -	
Oยรงยรฅรจ รรeร,รฎรถkรรถ7.
ร P~R
รฌ QTSUH3รฑ=รฒvรฒรฒ&รฑ ,V=รฌยรรยรญFรฅรถยรพยรถยรพร	รตรรฎร=รฎรฌยรบ รพรรงย.
ร  รถkLร Kร=รฎรถยรพร[รรตรถยรยรฅW
ร 13รรFรด รถยรพร
รรรรรงยรฅรรจ`รรeรถLรญcรรพFร=รร=รฎLรรตรตรฅรงkรดรรรทรถ7X
รฅ PY" $# รฌรรFรด&รบ รพรรง7Z
ร 2รถk[ร Kร=รฎjรถ7รพรรรตรถรรฅร 21 รรFรด&รถยรพรรรeรรรงยรฅรรจรรรถ
รฃรฅรชรชยรฅรบรฎ8รถ7รพรรญFรรพFรรรรฅ.
รง PY" #รฐ รฌAรถยรพร	รงยร=รฎรฉรชรถรรรทยรฎรถkรรถยรรl/
รฎ  รณ  รน Aรฐ ( 98;รฎรทรฅeร\รช -&	Oยรฅรงรรชรชรฅรถยรพรรง8รญFรรพFรรรรฅรงรฎรฌ
รZ
รฃ  รถkLร Kร=รฎรถยรพรรรตรถรรฅรFรฎ 21 รฅรง 13ยฒรฃรฆรง7รฅรจรครฎยรถkรรถย
ร P~รฌcรถยรพร	รงยร=รฎรฉรชรถรรรทยรฎรถkรรถยรรl/
รฎ P]ALรฐ 	 ( รขJLร Kรรรทqรถ7รพรยรรตรถรรฅรFรฎ
4 รฐ รฑ=รฒรฒรฒv&รฑ 465Lรชร=รรดรฎยฑรถยรฅ	รถยรพร	รฎรถkรรถย/
ร B-&	
Oยรฅรงbรรeรqรต7รชlรรฉFรฎยร _รธ ^ รฌรบรรถยรพ,ร=รรต7รพ รรฎ7รฎรรทรรจรรรถ ( รถยรฅรถยรพรรรรงรlรรญรชร=รฎรจรรรถรรฅรร=รด`รร _รธ ^ -LรถยรพFรรถ รฎยรรถรlรฎ Fร=รฎ
ยฑ
รฌ
รฎ 4 รฐ รฑ=รฒรฒรฒv&รฑ 465 ( รtรต7รชlรรฉFรฎร+รบรรถยรพรร3ร}รรงรlรรญรชร=รฎ,รพFรa
รฎ `(รฎยรรถรlรฎรฃรฆรรรรท
_รธ ^ รบbรยรรฎ7รฎรฅยรต7รlรรถ7รยรฅรร+รฅรฃ	รถยรพรยรรตรถรรฅรF
รรฎยรฎ~รยรทรรจรรeรถรฎรถยรฅ)รรถkรฎรรรงรlรรญรชร=รฎ -&	cbsรฃรถยรพรรฅรญFรฎรรงยร}รรญรชยรยรฎยรถkรรถยร	รl
รฎ รณ  ^ รฌLรรFd
รด  รถLร Kร=รฎ8รถยรพร,รรตรถรรฅ;
ร 4Heรฌ
รบ รพรlรต7รพรlรฎLรรฎ7รฎรฅยรต7รlรรถ7ร=รดรบรยรถ7รพ[รรรรฎยรฎ~รยรทรรจรรeรถhรถยรพFรรถLรรฎยรฎ~รยรทรFรฎ 2 ( 3L-Fรถ7รฅรรรงรlรรญรชรLรฏ fรฌรรFรดยฒรถยรพรรรeรรรงยรฅรรจรรรถ
รญFรรพFรรร=รฎ&รรตรตรฅรงkรดรรรท)รถย
รฅ
รฎ  ( รพรรFรตh
ร 98ยพรฎ รทรฅรรชLรlรฎรรฅรถรรต7รพรรร}รรญรชร
f " #g
รฐ ( f " $# -kรฌยรถยรพรรงยร=รฎรฉรชรถรรรท+รฎรถkรรถยร&รl.
รรรeรจ`รฅรงยLร -&icรถkLร Kรรรท,รถยรพรรรตรถรรฅW
ร 4 e รฃรฆรงยรฅรจรรถยรพรรฎรถkรรถย
ร  รณ  ^ รฌAรฉรFรดรรงรฅรถยรพรรง8รปFรฅรฎยรฎรรญรชรqรญcรรพFร=รรรฅรงkรฎรฌcรชร=รรดรฎ
รถยรฅqรฎยรถkรรถย/
ร รณ  ^ A%รฐ 	
j3รรฎยรพรฅ}รบtรรฅรบtรถยรพFรรถjรรฃJรซ รvรฎรฎยรรถรlรฎ Dรรญรชยรยฑรถยรพรรรถยรพรรงยรยฑร eรlรฎรถkรฎLรรฎ7รรถรlรฎรฃยรรตรถยรฅรงยรรปรชlรร`รฃรฅรง  รยร  	LรฟAรรถ
kZl 3รฑรฒรฒรฒรฑ ,VnmM 2 Bรฑ 3L รญFรรรยรรฎยรฎรรทรรจรรรถรถยรฅรรรงรlรรญรชร=รฎ8รฏ รฐ รฑรฒรฒรฒรฑรฏ รณ รฌรถยรพFรรถรฎยรรถรlรฎ Fร=รฎ 

รซ 	FjยรรตรฅรFรฎรถ7รงยรฉFรตรถ
ร`รปรชvร
ร oFp,รฃรฅn
รง ยธรรฎbรฃรฆรฅรชรชรฅรบรฎ l รร+รถยรพN
ร Qq>r รฎรถ7รรปjรฌFรถยรพร	รรทรรรถ รถkLร Kร=รฎ รถยรพร	รรตรถรรฅร 21 รฅรง 13[รดรรปcรรFรดรยรรท)รฅร
รถยรพร	ร}ร}รชยรฉร`รฅรฃ k ( Qs-&	รขรพรรjรฌAรรยรฎรถยรW
รป ,tcuAรฌFรถยรพรรรทรรeรถ8รถkLร Kร=รฎ8รรตรถรรฅW
ร 46eรฌFรถยรพFรรถรตรฅรงยรงยร=รฎรปcรฅรFรดรฎ8รถยรฅ#รถยรพร
รงยร=รฎรถ7รงรlรตรถรรฅร,รฅรฃ k รถยรฅ	รถยรพรรรรงรlรรญรชร=รฎยฑรถยรพFรรถ รรปรปFร=รรงรร _รธ ^ 	Fbยรถรlรฎร=รรฎยรรถยรฅรฎรรรถยรพFรn
รถ oFp`รชร=รรดรฎรถยรฅรฎรฉFรตรตร=รฎยรฎ
รงยรรทeรรงkรดรชร=รฎยรฎรฅรฃLรถยรพรรรตรถยรฉFร}รชJรรeรรรงยรฅรรจรรรถ รญcรรพFร=รรรฅรง	
 ร	รถยรพรรฅรถยรพรรงhรพFรรFรดAรฌรทรยรรรยร8รฎยรรถยรvรฎ~รฃsรรตรถ7รฅรงยร8รปรชvร9
ร oยรฃรฆรฅF
รง qรฌรบรbรฎรพรฅรบยฎรถยรพรรงยรร รvรฎยรถkรฎLรร[รรฎยรฎรรทรรจรรรถ
kwv รถยรพFรรถ รฎยรรถยรvDรฎ Fร=รฎ 

รซ 	 kwv รlรฎ รตรฅรFรฎรถยรงยรฉFรตรถยร=รด,รรตรตรฅรงkรดรรรทรถยรฅ	รถยรพx
ร Fรงkรฎn
รถ ,+รฎรถยรรปFรฎรฅy
รฃ o ( รฃรฅรงรถยรพรรญcรรพFรรรรฅรงkรฎ
รถยรพFรรถhรดรvรดยรรฅรถjรงยร=รรต7รพ[รฎยรฉFรตรตร=รฎยรฎhรรรถ -!zNรบ รพรlรต7รพ	รจ[รฉFรฎยรถjรญFรรรรถยรพรรง 21 รฅรง 13	 k v รฎยรรถรlรฎ Fร=รฎJรซรฌรฅรถยรพรรงยรบรlรฎรรถยรพรรงยร
รบรฅรฉรชlรดรญcรร	รต7รชvรรฉFรฎร _รธ ^ รฌรฎรฉFรต7รพqรถยรพFรรถbรรรqรรฎยรฎ~รยรทรรจรรeรถรถยรพFรรถbรฎยรรถยรvDรฎ Fร=รฎ _รธ ^ รฌยรตรฅรeรถยรงรรดรlรตรถkรฎรถยรพร6รรฎยรฎรรทรรจรรรถ
รฅรฃ k v 8;รฎรรรชรฉรรถยรฅ	รฅรรรฅรฃJรถยรพร8ร}รรงรlรรญรชร=รฎ8Lรฏ fรฌรบ รพรlรตยรพ,รบรฅรฉรชlรด,รตรรฉFรฎรยฑรฃยรรรชยรฉรง7รรฌรฅร#รถยรพร ( ,t9u{- q>r รฎรถ7รรปjรฌรฃรฅรง
รรรถยรพรรง8รญFรรพFรรรรฅ/
รง f " $# รฅn
รง f " #รฐ 	

|~}ย~ยยwยยยTยยยยยยยยTยยย~ยยRยยย!ย9ยVยย
รขรพร	รปรงยรรรรฅรฉFรฎรฎร=รตรถยรยรฅรFรฎ8รรeรถ7รงยรฅยรดรฉFรตร=รดยฎรรFรด รรeรร=รฎรถรรทeรรถยร=รด3ร#รทรรรรงkรรชhรฃรฆรงรรจรรบรฅรง7K,รฅรฃ รฝhรชlรรรรรรทยรบยฑรพรyรชร
รฟAร=รรง7รรยร{
รท 	Lรก5รจq[ร ย~รฅรง รฃรฆร=รรถยรฉรงยร8รฅรฃjรถยรพรรจ`รฅยรดรรชJรดรlรฎยรตรฉFรฎยรฎร=รด รร,รถยรพรรปรง7รรรรฅรฉFรฎรฎร=รตรถรรฅรFรฎbรvรฎรถยรพFรรถbรถ7รพร6รรทรรeรถ
รดรฅร=รฎรรฅรถBร ยjร=รตรถรถยรพร	รรรรรงยรฅรรจรรeรถ&รญFรรพFรรรยรฅรง 	รขรพรlรฎ8รlรฎ.ยรฉรรถยรรFรรถ7รฉรงkรรชLรรยรจqรรร0รรปรปรชyรlรตรรถยรยรฅรFรฎ	bร
รจqรรรtรตรรฎร=รฎ	รบร,รจqรรยรบรlรฎรพ1รถยรฅ(รตรฅรFรฎ~รvรดรรง,ร+รปFรรง7รถรlรตรฉรชvรรงqรฎรรถรฅรฃรปFรฅรฎยรฎรรญรชร0รบรฅรงยรชvรดรฎ ( ยร 	 ร 	รฌรญFรรพFรรรรฅรงรฎรฌ

ย&ยย

ยnยยยยย!ย!ย!ยยยwยcยก/ยข!ย]ยยฃaยคwยฃย!ยฅ!ย!ยยยwย

ยฆ7ยง&ยจ[ยฉwยชยซ]ยฆยฌยซ]ยญยฉ)ยฎยฐยฏ!ยฉwยฑยฆยฌยซ]ยญยฉwยชยณยฒ&ยดยจLยฉwยต)ยฆ7ยถ!ยทยง_ยทยซยธยชhยฉ!ยญ;ยง7ยทยจ%ยชยฌยญยฉยนยฆ_ยญยนยจยช7ยชยฌยฏ!ยบยทaยฆ7ยถ!ยทยปยนยบhยจยปยนยฑ_ยถwยจLยฉ!ยผยทยดxยผLยซ]ยฝยทยฉยพยฆ7ยถwยจLยฆยจ
ยฟ ยญ6ยช7ยชยซ]ร6ร]ยทร
ยญยงsรยยต)ยช ยฟ ยทยฑ_ยซ>รwยทยชaยจaยฎยฐยฏ6ร>ร/ยฆ7ยง&ยจLยฉwยชDยซรยฆsยซรยญ%ยฉ;ยฎ\ยฏ!ยฉwยฑยฆยฌยซ]ยญยฉรรยพรยฉรยซ]ยฉHยฆ7ยทยง_ยทยชยฌยฆยฌยซ]ยฉ!ยผรยทร!ยฆ7ยทยฉwยชยซ]ยญยฉยพยง7ยทยชsยฏ6รรยฆยชยฎ\ยง7ยญยบ
ยง7ยทรยธยจBร6ยซ]ยฉ!ยผWยฆ7ยถ6ยซยธยชรยฎยฐยทยจLยฆ_ยฏ!ยง7ยทรร!ยญ%ยงNยทรwยจLยบ ยฟ ร]ยทยดFยซ]ยฉยยฆ7ยถ!ยทhยฆ7ยง&ยจLยฉwยช ยฟ ยญยง7ยฆ&ยจ[ยฆยฌยซ]ยญยฉรยต6ยญยบ9ยจยซรยฉยนยต6ยทยช_ยฑยงยฌยซ]รwยทยตdยซ]ยฉยพรยทยฑยฆยฌยซ]ยญยฉยนร!ยด
ยญยฉ!ยทยบ9ยจBยปยร
ยซยธยชยฌยถ;ยฆ7ยญรยฑยญ%ยฉwยชยซยธยต6ยทยง+ยจcยฑยจยชยฌยทaรnยถ!ยทยง7ยทยบยญLยฝ%ยซ]ยฉ!ยผรยจBร]ยญยฉ!ยผรยจ ยฟ ยจ[ยง7ยฆยฌยซยธยฑยฏ6รยธยจLยงยง7ยญยฏ!ยฆ_ยท ยฟ ยง7ยทยฝยทยฉ6ยฆ&ยชยฎ\ยฏ!ยฆ7ยฏ!ยง7ยท
ยบยญLยฝยทยบยทยฉ6ยฆ&ยชnยจรรยญ%ยฉ!ยผยญยฆ7ยถ!ยทยงรยง7ยญยฏ!ยฆ7ยทยชร/ร
ยถ6ยซยธยชxยซยธยช/ยต6ยฏ!ยทยฆ7ยญhยฆ7ยถ!ยท/ยฎยยจยฑยฆรยฆ7ยถwยจLยฆรยบยญLยฝยทยบยทยฉ6ยฆ&ยชยจรรยญ%ยฉ!ยผ+ยชยฌยญ%ยบยท.ยง7ยญ%ยฏ!ยฆ7ยทยช
ยบ9ยจBยปยง7ยทยฝ%ยทยจBรรยฆ7ยถ!ยท.ยจLยผยทยฉ6ยฆรรยชรยทรHยซยธยชยฌยฆ7ยทยฉwยฑยทยฆ7ยญยจLยฉยทยฉ!ยทยบยป+ยจ[ยฉwยตhร
ยซ>รJร ยฟ ยง7ยทยฝยทยฉ6ยฆnยฆ7ยถ!ยท.ยจLยผยทยฉ6ยฆรรยชรยบยญLยฝยทยบยทยฉ6ยฆnยจBร]ยญยฉ!ยผ
ยชยฌยญยบgยทยง7ยญยฏ!ยฆ7ยทยชรยฆ_ยถwยจLยฆ.ยจLยง7ยทยฏ!ยฉwยต6ยทยงยฆ7ยถ!ยทยทยฉ!ยทยบยปRรรยช.ยฑยญยฉ6ยฆ7ยง7ยญ[รรgรยฉ!ยญ%ยฆ7ยถ!ยทยง/ยซ]ยฉ6ยฆ7ยทยง7ยทยชยฌยฆยฌยซ]ยฉ!ยผaยทรรยฆ7ยทยฉwยชDยซรยญ%ยฉรร
ยทร
ยญยฏ6รยธยต
ร>ยซ]รยท.ยฆ7ยญhยฑยญยฉwยชยซยธยต6ยทยงnยซยธยชxยฆ7ยถ!ยท.ยฑยจยชยฌยท/ยญ[ยฎรยจยบยฏ6ร]ยฆยฌยซ]รยยจLยผยทยฉ6ยฆยชยฌยป!ยชยฌยฆ7ยทยบรยซรยฉwยชsยฆ7ยทยจยตยญ[ยฎรยจยชDยซรยฉ!ยผ[รรยทรยยจ[ยผยทยฉHยฆรยญยฉ!ยทร
ร ยญยฆ_ยถยญLยฎTยฆ7ยถ!ยทยจLรwยญLยฝยทยทรรยฆ_ยทยฉwยชยซ]ยญยฉwยช/ยจLยง7ยทยชยฌยฆ7ยงยฌยซยธยฑยฆรยผยทยฉ!ยทยง&ยจBร>ยซ]รยจLยฆยฌยซ]ยญยฉwยชรยญLยฎTยญยฏ!ยงรรwยจยชยซยธยฑรยฎยฐยง&ยจ[ยบยทร
ยญยง7รRรnร
ยถ!ยทยง7ยทร
ยฎ\ยญยง7ยทยดรยญ%ยฏ!ยง/ยญยฉ!รYร>ยซรยฉ!ยทgยซ]ยฉHยฆ_ยง&ยจยฑยฆ&ยจLร6ยซ>ร>ยซ]ยฆยฌยปWยง7ยทยชยฌยฏ6ร]ยฆ&ยชรยถ!ยญLรยธยตcยซ]ยฉWยฆ7ยถ!ยทgยทรรยฆ7ยทยฉwยต6ยทยตUยฎ\ยง&ยจLยบยทร
ยญยง7ร!ยชรยจยชรร
ยทรJรยรhรnยญLร
ยทยฝยทยงยด
ร ยฏ!ยทยชยฌยฆยฌยซ]ยญยฉwยชยยง7ยทยผHยจLยงยต%ยซรยฉ!ยผยญLรรรYร>ยซ]ยฉ!ยท/ยฆ7ยง&ยจยฑยฆยจLร6ยซ>รJยซ]ยฆยฌยปยชยฌยถ!ยญยฏ6รยธยตhรwยท/ยฑยจLยง7ยทยฎ\ยฏ6ร>ร]ยป+ยฑยญยฉwยชDยซยยต6ยทยง_ยทยตRรTรWยทรร
ยซ>ร>รยต6ยทรwยฉ!ยท/ยฆ7ยถ!ยทยชยฌยท
ยทร!ยฆ7ยทยฉwยต6ยทยตhยฎ\ยง&ยจLยบยทร
ยญยง_รรยช
ยจLยฉwยตgยซ]ยฉ6ยฝยทยชยฌยฆยฌยซ]ยผHยจLยฆ_ยท/ยฆ7ยถ!ยท.ยญLรรรYร>ยซ]ยฉ!ยท.ยฆ7ยงยจยฑยฆ&ยจLร6ยซ>ร>ยซ]ยฆDยปยญ[ยฎยฆ7ยถ!ยท.ยง_ยทรยยจ[ยฆ7ยทยต ยฟ ยง7ยญร6ร]ยทยบ9ยชร
รรร\รรขรกhรฃ~รคRรฅRรฆWรงยธรจaรฉ.รช{รซRรฅHรฌรงยรญรฏรฎLรฐ
รกhรช%รฑZรครรง]รฒรงยรญรรคยพรรร\รFรณ รยฉยรดรถรตรท\รดBรธwรนรดรนcรบZรป>รผรธ{รธwรฝYรธHรพaรฟHรฝYรปJรดรรด&รผรธwรฝYรธHรพ	
ยณรท\รด

!"#ยฒ
ยฑยญยฉwยชยซยธยชยฌยฆยชรยญLยฎZยจ9ยชsยทยฆ/ยญLยฎ%$'&รด!(รผ)&รป>รด*รท\รผรท\รด	%gยดรฏยจhยชยฌยทยฆ/ยญLยฎ,+-$.รฝ/&รป>รดรผ0รท\รฝ1$รธ-2hยดรยจLยฉรรฝYรธwรฝYรท\รฝYรผรปรผBรพรดรธ{รท	3 4ยณรท\รผ%รท\รด
657ยดรยจยชsยทยฆ/ยญLยฎXรดBรธ8(รฝ19$รธ8รดBรธwรท%&_รด	Hรผ:(รฝ1$;<ยด~ยจLยฉ;รฝYรธwรฝYรท\รฝยธรผรป~รดBรธ8(Bรฝ=9$รธ>NรดBรธwรท2&_รด	Hรผ(Bรฝ=$:?57ยดยจ[ยฉwยตWยจ
รพรป@$'&_รผรปFรท/ยฌรผรธ-รฝยธรท\รฝ1$รธ*A	B6รธ80รท\รฝ1$รธC"#!DFEGHEIJKHEIยดรยฆ_ยถwยจLยฆ.ยต6ยทยฆ7ยทยง7ยบยซ]ยฉ!ยทยชXยฎยฐยญยงรยทยจยฑ_ยถWยชยฌยฆ&ยจLยฆ7ยท5Lgยด
รwยทยถwยจBยฝ%ยซ]ยญยง?M5CgยดwยจLยฉwยตaยจยฑยฆยฌยซ]ยญยฉCN569ยด ยฆ7ยถ!ยท/ยฉ!ยทรรยฆรยชยฌยฆ&ยจLยฆ7ยท.ยจ[ยฉwยตhรwยทยถwยจBยฝยซ]ยญยง4
O=O>ยฒPQ"#	).N ยฒ&ร

R ยญยฆยฌยซยธยฑยทnยฆ_ยถwยจLยฆFยซ]ยฉ9ยทร!ยฆ7ยทยฉwยต6ยทยตSFรยธยจLยฉ!ยฉ6ยซ]ยฉ!ยผรnยถ6ยซ>ร]ยท?T ยทยจLยง7ยฉ6ยซ]ยฉ!ยผยชยฌยปรยชsยฆ7ยทยบ9ยชยด%ยฆ7ยถ!ยทnยผLร]ยญรwยจBรRยฆ7ยง&ยจLยฉwยชDยซรยฆsยซรยญ%ยฉยฎยฐยฏ!ยฉwยฑยฆsยซรยญ%ยฉ
ยบ9ยจBยป9ยฑ_ยถwยจLยฉ!ยผยทยฆ7ยถ!ยท/รwยทยถwยจBยฝ%ยซรยญ%ยงU\ยซยร ยทรยยดHยฆ_ยถ!ยทNยจยฑยฆ7ยฏwยจร ยฆ_ยง&ยจLยฉwยชยซ]ยฆยฌยซ]ยญยฉhยฎ\ยฏ!ยฉwยฑยฆยฌยซ]ยญยฉRยฒ
ยญLยฎyยฆ_ยถ!ยท/ยทยฉHยฝ%ยซ]ยง7ยญยฉ!ยบยทยฉ6ยฆร
ร ยถ!ยทยต6ยทรwยฉ6ยซ]ยฆยฌยซ]ยญยฉยญLยฎยจรยผยญHยจBรรฏยจ[ยฉwยตยญLยฎยจ.ยช7ยจ[ยฆยฌยซยธยชยฎยจ%ยฑยฆ7ยญยง7ยป ยฟ รยยจ[ยฉ9ร
ยซ>ร>รรยง7ยทยบ9ยจBยซ]ยฉ9ยจ%ยชFยซ]ยฉ9ยฆ_ยถ!ยทnรwยจยชยซยธยฑ ยฎยฐยง&ยจ[ยบยทร
ยญยง7รRร
V ยญยง7ยทWยช ยฟ ยทยฑ_ยซJรรฏยฑยจรJร]ยปยดร
ยทUยจ%ยช7ยชยฌยฏ!ยบยทcยฆ7ยถwยจLยฆยฆ_ยถ!ยทUยจ[ยผยทยฉHยฆยต6ยญ6ยทยชhยฉ!ยญยฆhยซ]ยฉ6ยซ]ยฆยฌยซยธยจBร>ร]ยป ร6ยฉ!ยญLร ยฆ7ยถ!ยทaยซยยต6ยทยฉ6ยฆยฌยซ]ยฆยฌยปยพยญLยฎWLยด
ร!ยฏ!ยฆร
ยซยธยชยฌยถ!ยทยชยฆ7ยญยต6ยทยฝ%ยซยธยชยฌยทยจ ยฟ รยธยจLยฉรยฆ_ยถwยจLยฆ.ร
ยซ>ร>รXยชยฌยฏwยฑยฑยทยทยตdยง7ยทยผHยจ[ยง&ยต%ร]ยทยช7ยชยญLยฎยฆ7ยถ!ยทยซยธยต6ยทยฉ6ยฆยฌยซ]ยฆยฌยปรยญLยฎ%  รร ยถ!ยท+ยจLยผ%ยทยฉHยฆ
ยถ!ยญLร
ยทยฝยทยงร6ยฉ!ยญLรยช"XรFร
ยถ!ยทยชยฌยทnยจยช7ยชsยฏ!ยบ ยฟ ยฆยฌยซ]ยญยฉwยชFร
ยซ>ร>รรฏยฑยจ ยฟ ยฆ7ยฏ!ยง7ยท2Syรยยจ[ยฉ!ยฉ6ยซรยฉ!ยผ/รxยถ6ยซJร]ยท%T ยทยจLยง7ยฉ6ยซ]ยฉ!ยผ/ยซ]ยฉยฆ7ยถ!ยท
ยทรรยฆ_ยทยฉwยต6ยทยต
ยฎ\ยง&ยจLยบยทร
ยญยง7รRรnรรขยบgยญรยต6ยทยง&ยจ[ยฆ7ยทยยง7ยทยช ยฟ ร ร ยฏwยจ%ยชยซ]ร\ยบยญรยต6ยทยงยจLยฆ7ยทLยฒ
ยทรรยฆ7ยทยฉwยต6ยทยตYSFรยธยจLยฉ!ยฉ6ยซ]ยฉ!ยผรnยถ6ยซ>ร]ยทTRยทยจ[ยง7ยฉ6ยซ]ยฉ!ยผยชยฌยปรยชsยฆ7ยทยบ
ยซยธยช/ยจSyรยธยจLยฉ!ยฉ6ยซ]ยฉ!ยผรnยถ6ยซ>รรยทTRยทยจLยง7ยฉ6ยซ]ยฉ!ยผยชsยปรยชยฌยฆ_ยทยบ ยซ]ยฉWรnยถ6ยซยธยฑ7ยถcยฆ7ยถ!ยทยฉHยฏ!ยบร{ยทยง/ยญLยฎTยทร]ยทยบยทยฉHยฆยชxยซรยฉG ยซยธยช ยฟ ยญLร]ยปHยฉ!ยญยบ0ยซยยจรยด
ยจLยฉwยตhยฆ7ยถ!ยทรยฉ6ยฏ!ยบNร{ยทยงnยญLยฎยทร]ยทยบยทยฉ6ยฆ&ยชยยซ]ยฉI ยซยยช ยฟ ยญLร]ยป6ยฉ!ยญยบยซยธยจBรPยยง_ยทยช ยฟ ร6ยทร ยฟ ยญยฉ!ยทยฉHยฆsยซยยจรยฐยฒยยซ]ยฉยฆ7ยถ!ยท.ยชยซ]รยท/ยญLยฎยฆ_ยถ!ยท.ยจยฑยฆ7ยฏwยจBร
ยง7ยท ยฟ ยง7ยทยชsยทยฉHยฆ&ยจ[ยฆยฌยซ]ยญยฉรรTร
ยถ!ยทรยบยทยจLยฉ6ยซ]ยฉ!ยผยญLยฎFยฆ7ยถ!ยทยชยฌยท/ยต6ยทรwยฉ6ยซรยฆsยซรยญ%ยฉwยช
ยซยธยชnยจยชรยซ]ยฉยฆ7ยถ!ยท/รwยจยชDยซยยฑ<SyรยธยจLยฉ!ยฉ6ยซ]ยฉ!ยผรnยถ6ยซ>ร]ยทZTRยทยจLยง_ยฉ6ยซรยฉ!ยผ
ยฎ\ยง&ยจLยบยทร
ยญยง7รRร
[ ยฉ6ยฎ\ยญยง7ยฆ7ยฏ!ยฉwยจLยฆ_ยทรรยปยด\TRยทยบยบ9ยจ]wร_^hยต6ยญHยทยชยฉ!ยญยฆยถ!ยญLรยธยตยยทยฝยทยฉdยฎ\ยญยง.ยบยญ!ยต6ยทยง&ยจLยฆ_ยท9ยทร!ยฆ7ยทยฉwยต6ยทยต7SFรยธยจLยฉ!ยฉ6ยซ]ยฉ!ยผWรxยถ6ยซJร]ยท
TRยทยจLยง_ยฉ6ยซรยฉ!ยผยพยชยฌยป!ยชยฌยฆ7ยทยบ9ยชรรขรxยญLรZยทยฝ%ยทยงยดZยจ%ยชhรZยทaยบยทยฉ6ยฆยฌยซ]ยญยฉ!ยทยต)ยซ]ยฉ รยทยฑยฆยฌยซ]ยญยฉ`]wยด
ยฆ7ยถ!ยท ยฟ ยง7ยญ ยฟ ยทยง7ยฆยฌยซ]ยทยช ยญร!ยฆ&ยจBยซ]ยฉ!ยทยต)รHยป
TRยทยบยบhยจ*]wร_^/ยฑยจLยฉaรwยท/ยง7ยทยผHยจยซรยฉ!ยทยต รHยปยฑยญยฉwยชยซยธยต6ยทยงยฌยซ]ยฉ!ยผยทa+ยฑ_ยซ]ยทยฉ6ยฆ ยฟ รยธยจLยฉwยชรถรTร!ยญยงxยบยญHยชยฌยฆ ยฟ ยง&ยจยฑยฆsยซยยฑยจร ยฟ ยฏ!ยง ยฟ ยญ6ยชยฌยทยชยด!ร
ยท
ยต6ยญยฉ!ยญยฆ~ร]ยญHยชsยท/ยผยทยฉ!ยทยง&ยจBร>ยซ]ยฆยฌยปรHยปgยง7ยทยชยฌยฆ7ยงยฌยซยธยฑยฆยฌยซ]ยฉ!ยผยญยฏ!ยงnยจLยฆ7ยฆ_ยทยฉHยฆยฌยซ]ยญยฉgยฆ7ยญ.ยทa+ยฑ_ยซ]ยทยฉ6ยฆ ยฟ รยธยจLยฉwยชยด!ยชยซ]ยฉwยฑยท.ยจ ยฟ รยธยจLยฉ!ยฉ!ยทยงnร
ยซ>ร>รรยฉ!ยญยฆ
รwยทยจLร6ร]ยทยฆ7ยญaยทรรยทยฑยฏ!ยฆ_ยทยทร ยฟ ยญยฉ!ยทยฉ6ยฆยฌยซยธยจBร>รรยปยยบ9ยจLยฉ6ยปWยจยฑยฆยฌยซ]ยญยฉwยชXยซรยฉยยฆ7ยถ!ยท9ยฑยญ%ยฏ!ยง&ยชยฌยทยญLยฎnยจ ยฟ รยธยจLยฉรรhร/ยช/ร
ยทยบยทยฉHยฆsยซรยญ%ยฉ!ยทยต
รwยทยฎ\ยญยง7ยทยดรร6รรยญLรnร\ยฏ ยฟ ยซ]ยฉ;ยฆ7ยถ!ยทยชยซ]รยทยญLยฎXยช_ยจLยฆยฌยซยธยชยฎยยจยฑยฆ7ยญยง7ยป ยฟ รยธยจLยฉwยชยบ9ยจBยปรยชยฌยฆsยซJร>รnรwยท ยฟ ยญHยช7ยชยซ]ร6ร]ยทยดยยทยฝยทยฉยยซ>ยฎร
ยท9ยง7ยทยชsยฆ7ยงยฌยซยธยฑยฆ
ยญยฏ!ยง&ยชsยทรรยฝ%ยทยชยฆ7ยญcยทa+ยฑ_ยซ]ยทยฉHยฆ ยฟ รยธยจLยฉwยชยญยฉ6ร]ยปรยรWยทยบ9ยจ[รยท9ยฉ!ยญยยจยช7ยชยฌยฏ!ยบ ยฟ ยฆsยซรยญ%ยฉwยชยจLรwยญ%ยฏ!ยฆNยฆ_ยถ!ยทยชยซ]รยทยญLยฎXยฆ_ยถ!ยทยง7ยทรยธยจLยฆ7ยทยต
ยต6ยทยฑ_ยซยธยชยซ]ยญยฉcยฆ7ยง7ยทยทร
ร ยญยง7ยฆ7ยฏ!ยฉwยจ[ยฆ7ยทร]ยปยด8TRยทยบยบ9ยจb]wรรรยต6ยญ6ยทยชnยถ!ยญ[รยยต ยฎ\ยญยงnยทร!ยฆ7ยทยฉwยต6ยทยตCSyรยธยจLยฉ!ยฉ6ยซ]ยฉ!ยผรnยถ6ยซ>รรยทTRยทยจ[ยง7ยฉ6ยซ]ยฉ!ยผยชยฌยปรยชsยฆ7ยทยบ9ยชร
ร
ยถ!ยท
ยงยฟ 7ยญ6ยญLยฎ!ยญLยฎwยฆ7ยถ6ยซยธยชRร]ยทยบยบhยจ
ยฎ\ยญยงรยฆ7ยถ!ยทยยทรรยฆ_ยทยฉwยต6ยทยต/ยฎ\ยง&ยจLยบgยทรZยญ%ยง7รnยซยธยชFยชยซ]ยบยซ>รยธยจLยง~ยฆ7ยญ
ยซ]ยฆ&ยช ยฟ ยง7ยญ6ยญLยฎ!ยซ]ยฉ.ยฆ7ยถ!ยทยรwยจยชยซยธยฑรยฎยฐยง&ยจ[ยบยทร
ยญยง7รRร
c ยญยบร6ยซ]ยฉ6ยซ]ยฉ!ยผ9ยฆ_ยถ!ยท.ยจLรwยญLยฝยท/ร
ยท/ยผยทยฆ!D
d Rรซ รช6รญรฏรฎBรชwรฆ รรร\ร!รณfe รฝ=(Bรดรธรผg!B6รผ.รฝ_h9*$รนรด
Dรผ%รท\รดNรดรถรตรท\รดBรธwรนรดรนรบ
รป>รผรธwรธ{รฝยธรธ6รพรฟHรฝยธรป>รดi รดรผรธwรฝYรธHรพ4	
ยณรท\รด
Uj)kZรธ{รนรฝYรธHรพ
รผรธcรด/lm0BรฝยธรดBรธwรทnยณรผรท\รฝo1Aยณรผ0Bรท/$M+รฏรป>รผรธcรฝ_U$qp2hDรป>รฝYรธwรด.รท/ยฌรผ0Bรท\รผ'&รป>รด.r
sts

uv-w.x-vIv-y-z7{P|:y-y-|y-}8~-;ย8ย

Xย ย1ยยยยยXย_ย!ยoย;ยย8ย>ย8ย6ยยยยย	ย	ย>ยGย
ยMย-ยย;ย-ย	ยย_ย'ยย	ย;ย!ย9ย9ย_ย-ยUยยก-ย;ย	ย8ยยขย_ยยWยoย,ยฃย:ย8ยฃย	ย;ย-ย!ยคยฅยฆยยงยยZยย-ยยฆยฃ	ยจ:ย9ยยฉยฅ2ย-ย	ย;ย2ย;ย-ย	ยยPย=ยnยชยย;ย\ย;ย8ยจ.ย*ยย-ย%ยจ.ย:ย	ย'ย
ย_ยย;ย-ย2ย9ยซ-ย9ย;ย	ยชCยฌยญ-ยยย!ยจย9ยยฉย.ยฎ8ยยก-ยฏ8ย)ยยขย_ย9ย_ยยXยฐ)ยฅยฑย\ยฅยฉย@ยฒ@ยฒยณยจย;ยqยด-ยชยPย;ย8ยจยตยยย;ย-ย	ยยยฆยจยตย;ยยฉยยฅยฉย%ยจยตยย	ย'ยยXย;ย8ยจยตยยยย	ย-ย	ยยจ.ย;ย
ยจยฃย9ย_ยย8ยยถยฌยธยท
ยMยYยยงย)ย;ย	ย;ย!ยqย9ย_ย-ยIยฎยนย!ยจยตย;ยด-ย;ยย.ยฎMย;ย-ยยช*ยด)ยฒ_ย9ย_ยบยปยจ.ยย	ย)ยยฃ	ยจย9ยย=ย4ย;ย8ยจ.ย*ยจ.ยยผยจยตยย	ย'ย*ยช*ย_ยย)ย*ย-ยยUยฝ>ยยฎยพยจยตยช*ย@ยฒยฟย=ยจ.ย
ยฅยฉย_ย;ยยย-ย2ยย'ยจ
ยฒยจยตย8ยค*ย;ย-ยยฉย_ย)ย_ย9ย=ยจ
ยฒยย9ยยจ.ยยยฑยยตยฎย;ย-ย2ย:ย;ย-ย	ยยฑยจ.ย:ย	ย'ย!ยฌร2ย	ย8ยฃยยฐ-รรยฒoยจยตย-ย)ยยงย-ย*ยฅ2ย)ย@ยฒ_ย<รยย!ยจ.ย;ย)ย_ย-ย*ย;ยยฎ/ย	ยย
ย-ย.ยฅ7ย;ยUยย-ย%ยฃ	ยจย9ย\ย_ยยฅรย)ยoยฃย6ยจ.ยยจ.ยย	ย)ย,ย;ย9ย_ย!ยnย;ยUยจยฃย)ยยงย	ร:ย2ย_ยยnยย'ยจ
ยฒ-ยฅ2ย)ย@ยฒ_ยMยฒยงย!ยจยตย;ย)ย_ย-ยWยจยตยฝ8ยยด-ย,ย;ย-ยรยฝ8ย	ย8ยจ
รย_ยย
ย.ยฎ,ย;ย-ย<ย	ย)รย_ย;ยย-ยชbย	ย'ย!ยฐ>ยจ.ย8ยคCยจ.ยฝ>ยยด-ย2ย;ย-ย<ย:ย'ยจ
ยฒ=ย2ยจ.ย8ยคbย_ย)ย_ย9ย=ยจ
ยฒPย9ยยจ.ย;ย!ยยฆย.ยฎรย:ย;ย-ย	ย2ยจ.ยย	ย)ยย	ยฌ
ร :ย รยฑยXย_ย!ยoรXยรยXย1ย8ร ยHร*ร'ร@ร/รoรร!ร:ร!ร>รPรยฉร@รร8ร>ร=ร)ร*รร'ร=ร@ร4รXรร:รร>ร=ร)ร*ร	ร!ร	ร/ร!รรย=ย%ยจ*ยยด-ยฏ)ยฒยงย
ร8รรร`รกรขUรฃ
รครข4รฅ!รครฆรครง รจ รฃ รครง รจ รฅ รครฉรครช รจ รครซยรMรฌยฅ2ย-ย	ยยยฆรขiรญ)ย=ย,ยจ%ย9ย	ยย.ยฎยรฎ'รฏรฐรรฐร
รรฑ
ร'รฏ	ร@รยฉร	ร/รร/รยถร8รฒ	รฎรยฉร!รร
ร8ร'รณยฐ!รฆรดย=ย,ยจMย9ย	ย
รญ รขiรญ-ย=ย,ย;ย-ย4ร=ร>ร=ร/ร1รร)ร	ร/รร/ร<รฎ9รฒ%ร!ร:ร!ร>รรถรณยฐรฉรย=ย\ยจ<ย9ย	ย,ย.ยฎรร
ร8รฑ
ร=ร9รฎร>รZร
ร8รยรฏร	ร)รรฑ!ร1รฎรรรฐยฐ
ย.ยฎ>รตรถรฎ.รรรฐร/รฏ	ร@ร2ร:รท!ร/ร1รฎร-ร	ยฐ'รง รจ<
รธ
รช รจ รธ รฉรนย=ย%ยจ.ยรบร=ร>ร=ร/ร1รรยร!ร>รฑ!ร1ร9รฎร8ร*ร!ร>รPรฏ;รยถร'รรฑ
ร=รฎรยฐ8ยจ.ย8ยครซ ร*รป รข รฃรรผ รข รฅMรผ รฉ รผ รฆ รฅMรฝ รข รฃรรผ รข รฅMรผ รฉFย=ยMยจ
รร@รฎ'รฏรรยร/ร9รร-ร	ร1ร/ร=รฎร4รฒ	ร'ร>รท!ร/ร1รฎรยย8ยจ.ย2ยค)ย	ย;ย	ย;ยช*ย_ย-ย!ยยฆยฎยนยยรย!ยจยฃ;ย6ยฏ8ยจรพยยงยMย.ยฎPย9ยยจ.ยย!ยยฑรง	รฃ รธ รขUรฃ.ยฐรฟรง	รฅ รธ รข4รฅ.ยฐ)ยฝ8ย	ย8ยจ
รย_ยย
รช รธ รฉยฐยฉยจ.ย8ยครบยจ9ย.ย_ย'ยยจยฃย9ย_ยย7ยยตยฎ%ย;ย-ยCยจ.ยย	ย)ยยbรก)รฃ!รค'รฅยถรฌ รธ รฆ รฅ ยฐ,ย;ย-ย6ย-ยยก-ยZย:ยฝ8ย9ย	ย;รรพยจยตยฝ)ยฒยงยIย9ยยจ.ยย!ยUยยตยฎ%ย;ย-ย
ยจ.ยย	ย)ยยยฉยจ.ย8ยคย;ย-ย<ย-ยยกรถยรย	ย'ร:ย_ย;ยย-ยชย	ย)ย%ยฝ>ย	ย8ยจ
รย_ยย รป รกรง รฃ  รครง รฅ  รครช  รฌ,รQรซ ร รกรง รฃ รครง รฅ รครช.รค  รฃ รค  รฅ รฌยฌ



ยจยฃยmยจ.ยย	ย)ยZย8ยจ:ย4ย_ยยย.ยฅ2ยรบยย'ยจรพยฒยพยฐยฉยจ.ย8ยคยผยยงยย*ยฏ)ยฒ=ยจ.ย7ย=ยยจGยค)ย!ยฃย=ยยขย_ยยรย;ย;ย	ย6ย;ย8ยจ.ย*ย;ยยฎ/ย	ยย*ย:ย)ยฒยงยซ7ยยGย;ย8ยจ.ย
ยจ.ยย	ย)ย	รฟยยฆยยฝ8ย9ย	ย;ร.ยจ.ยฝ)ยฒ_ยZย9ยยจยตย;ย!ย	
ยฌ 
ยฉย-ยZยค)ย 8ย)ย_ย9ย_ยย8ยMย.ยฎPยชbยรถยค)ย	ยยจยตย;ยUยจ.ย8
ยค :ยด8ยจยยขย_ยบ/ยชย-ยค)ย	ยยจ.ยยUย;ย	ยฏ-ย;ย!ยqย	ย'ยยจยตย9ย_ยย8ย
ยจ.ย;ยยฉย9ย;ยยจรพยยงย:ย'ย9ยฎ/ยย;ยฅ2ยจ.ยยคMยย	ย-ย	ยยจ
ยฒ@ย !ยจ.ย9ย_ยย8ยnย.ยฎย;ย-ยย_ยPยค)ย 8ย)ย_ย9ย_ยย8ยnยฎ/ยยยยยก-ย;ย	ย8ยค)ย!ยครรยฒ=ยจ.ย-ย)ย_ย-ยUยฅ2ย)ย@ยฒ_ย%รยย!ยจ.ยย)ยยงย-ย
ย9ยซ-ย9ย;ย	ยชยยถ
ยฌ ยปย7ยจยค-ยค:ย_ย9ย_ยยXยฐ\ยฅยฑยยจย;ย9ยด-ยชยย;ย8ยจ.ย4ย!ยจยฃยLยจ.ยย	ย)ยZยฃ	ยจ.ย7ย9ยยจยตย;ย<ย_ยรบยย-ยยยตยฎยฆยฏ>ย.ยฒ_ยซ'ย-ย:ยช*ย=ยจ
ยฒ@ยฒยงยซ7ยชยจ.ย'ยซ
ย_ย)ย_ย9ย=ยจ
ยฒXยยฝ8ย9ย	ย;ร.ยจ.ยฝ)ยฒ_ย?ย9ยยจ.ย;ย!ย	ยฐยจ.ย8ยคWยชยจ
ยซZย8ยจ
รยยฉยย-ยรย.ยฎXยฏ8ย.ยฒ_ยซ)ย-ยยช*ย=ยจ
ยฒ@ยฒ_ยซยชยจยตย'ยซ*ยย'ยจรพยฒoย#ย_ย\ยช*ย_ยย'ย\ยฝ8ยMย;	ย :ยด)ยยงยย!ยค
ย;ย*ยจยฃ;ย)ย_ย	รย
ยฌ 2ย	รย	ยย;ย-ยยฒ_ย!ย;ย	ยฐ-ย!ยจยฃย6ยจ.ยย	ย)ยยฑยชยจ!ยซ*ย-ย
ย 'ย-ย.ยฅรยฅ2ย8ยจ.ย\ย;ย-ย<ยยก8ยจยฃย,ย_ย)ย_ย9ย=ยจ
ยฒรย9ยยจ.ย;ยMย.ยฎXย;ย-ย%ย:ย;ย-ย	ย
ยจ.ยย	ย)ยMย=ย	ยฐยยจยตย8ยคIยฅ2ย8ยจ.ย<ย;ย-ยbยยก ยจยฃย<ย:ย'ยจ
ยฒPย.ยฎยฉย;ย-ยยย;ย-ย	ยUยจยตยย	ย'ยMย=ย	
ยฌ Yยยจ.ย;ย4ย_ย'ยย	ย;ย!ย9ย;ย!ยค ย_ยLย;ยจ.ย9ย=ยยขยฎยปยจยฃย;ย:ย;ยซ
ร*ร'ร@ร/รoรร!ร:ร!ร>รยรตร@รร รรฐยฌPยญ ยย;ยชยจรพยฒยฟยฒ_ยซยฐ-ยฅยฉย<ย8ยจ
รย รป
ร ย:รยฑยXย_ย!ยoรXยรยXย ย
ร  ย_รย	ย ยจ*ยช*ยด)ยฒ_ย9ย_ยบยปยจ.ยย	ย)ย%รรยฒoยจยตย-ย)ยยงย-ย6ยฅรย)ยยฟยฒ_ย*รยย!ยจ.ย;ย)ย_ย-ยCย9ยซ-ย9ย;ย	ยชCยฐ-ยจ6ร*ร)ร@ร/ร_ร9ร!รร
ร8รยรตยรยฟร:ร
ย=ย2ยจ4ยฏ8ยจ
ย_ยยฉย.ยฎรย9ย	ยย\ย.ยฎยฏ)ยฒoยจยตย8ย	ยฐ-ยย-ย<ย9ย	ย\ยฎยนย:ยยฑย!ยจ:ยฃ;ย6ยจ.ยย	ย)ย!ยฌ,รยย	ย! รญ ยค)ย	ย-ยย;ย<ย;ย-ย<ย9ย	ยยฉย.ยฎยยฏ)ยฒ=ยจ.ย8ยยฑยฎยนยย2ยจ.ย:ย	ย'ย
รณ;ยฌย`ยช*ยด)ยฒ_ย9ย_ยบยปยจ.ยย	ย)ยยฉยฏ)ยฒ=ยจ.ยย=ยMรรฐร:ร/ร_ร1รฒรฐร:รท!ร/รฎรรยฉย@ยฎXยฎ/ยยยฉย!ยจยฃย ยจ.ย:ย	ย'ยยฉรณ,ยจ.ย8ยค*ยฎ/ยย\ย!ยจยฃ;ยยฏ8ย)ย;ยยขย_ยฝ)ยฒ_ยUยย'ยจรพยฒ#"ย.ยฎรยจ.ย:ย	ย'ย
รณ;ยฐ:ย;ย-ย	ย;ยยฉย=ยยฉยจMยฏ)ยฒ=ยจ.ยbยยง
ย $%!_รญ9ยฐ)ย;ย8ยจ.ยPยจยฃย)ย_ย	รย!&
ย "Wย9ยยจ.ย;ย9ย_ย-ยMยฎ/ย;ยยช ยจยตย'ยซ4ยฏ8ย'ย;ยยยงยฝ)ยฒ_ยMย_ย)ย_ย9ย=ยจ
ยฒยย9ยยจ.ยยยฐย;ย	ย'ยจยตยยค:ยฒ_ย!ย;ย
ย.ยฎยยย-ย%ยฏ)ยฒ=ยจ.ยรก/ย_ย6ย;ย-ย<ยฃยย;ย;ย!ย9ยฏ>ยย8ยค:ย_ย-'
ย $%!)(
รฌnยจยตย8ยค*ย_ย)ย_ย9ย=ยจ
ยฒรย9ยยจ.ย;ย2ย.ยฎย;ย-ย%ย:ย;ย-ย	ย2ยจ.ยย	ย)ย!ยฐ'ยจ.ย8ยคbย;ย	ย'ยจยตยยค:ยฒ_ย!ย;ย
ย.ยฎ2ย;ย-ย*ย_ย)ย_ย9ย=ยจ
ยฒ2ยฝ8ย	ย8ยจ
ร:ย_ยย*ย.ยฎ2ย;ย-ยย	ย)รย_ย;ยย-ยชbย	ย'ย!ยฌยMยQ+ร *mรท!ร1ร!ร>รยฉรรฐร:ร/ร_ร1รฒรฐร:รท!ร/รฎรรIรZร)ร@ร/ร_ร9ร!รร
ร8รnรตร@รรยoย4ยจ
ย;ยจ.ยqยoยยฎยพยจยฃยยย;ยซ6ยช*ยด)ยฒ_ย9ย_ยบยปยจ.ยย	ย)ย<ยฏ)ยฒoยจยตย ยย8ยจ.ย?ยฃย:ย8ยยขย=ย9ยยMย.ยฎPยฏ)ยฒ=ยจ.ย8ยiยฅ2ย)ย=ยฃ;ยLยจยตย;ยZยค)ย!ยฃย=ยยขย_ยยGยย;ย	ย!ยMย.ยฎPยฏ8ยยตยฒยงยซ)ย-ยยช*ย=ยจ
ยฒ
ยค)ย	ยฏ-ย;ยXยฌ


ยฆย-ยZยจ.ยฝ>ยรพร:ยUยค)ย 8ย)ยยงยqยยงย:ยLยฃ	ยจ.ยฏ-ย;ยด-ย;ย!ยยฆย_ย'ยยด)ยยงยqยยงร:ยยยขย_ย;ยด8ยจ.ย9ย_ยย8ยรย.ยฎPรยฒ=ยจ.ย-ย)ย_ย-ยCยฅรย)ยยฟยฒ_ยรยย!ยจ.ยย)ยยงย-ยย_ยIยช*ยด)ยฒ_ย9ย_ยบ
.ยจ ยย	ย)ยZยค)ย:ยชยจ
ย_ย8ย	ยฌGย<ย;ย9ยด-ยชbยยฎ/ยยUยยก8ยจ.ยชยฏ)ยฒ_ย6ย;ย8ยจยตยZยย-ย	ย;ย ยจยตย;ยย9ยฅยฉย6ยฎ/ยยยฃย!ย4ย;ย8ยจยตยUย8ยจ
รยย;ยCยชย.รยbย_ย7ย;ย-ย
ย-ย'ย9ยqยยฟยฒ_ยIย	ย)รย_ย;ยย-ยชbย	ย'ยย.ยฎ,ย!ยฃย9ย_ยย.--ยฌ/
ยฆย-ย	ยซmย9ยยจ.ย;ยbยชย.รย_ย-ยGยย10.ย32Lยฐ\ยจ.ย8ยค7ย-ย	ย!ยครดย;ยGยย!ยจยฃ;ย7ยย-ยยยงย
ยค)ย!ย9ย9ย_ย8ยจ.ยqยยงย:ย8ยUยฝ)5
ยซ 4
ร 2รบ6ยฌ 2ย	รย	ยย;ย-ยยฒ_ย!ย;ย	ยฐ,ย;ย-ย	ยซรบยฃ	ยจ.ยGย-ยย4ยฝ8ยCย9ยด-ย;ยยจ.ยฝ8ยยด-ย*ย;ย-ยbยยก ยจยฃย4ย_ย)ย_ย9ย=ยจ
ยฒยฉยฒ_ยรถยฃ	ยจ.ยqยยงย:ย
ย.ยฎ,ย!ยจยฃยCยย;ย-ย	ยMยจ.ย8ยค6ยจ.ยฝ8ย:ยด-ย%ย!ยจยฃย6ยย;ย-ย	7ย รฟย2ยค)ย!ย9ย9ย_ย8ยจ.ยqยยงย:ยX
ยฌ Qย8ยจยตยยฑยย-ยZยฃยยชยชยจ.ย8ยค)ย	ย%ยจ.ย;ยย	ยชยฏ-ยยยฉย;ยยค)ย*ย_ย
ย;ย8ยจ.ย2ยฃ	ยจยqยยฐย=ย\ย;ยยค)ย	ร:ยoยqยUยจUยชยจย9ย;ย	ย;ยบ/ยฏ)ยฒ=ยจ.ย6ยย8ยจ.ยยฑย9ย-ย:ยด)ยฒoยคยฝ>ย<ยย)ยรถยคWยฎยนย:ยยฆยจรพยฒยฟยฒXยย)ยจ
ยฒ=ย	ยฐ:ยยงย)ย_ย9ย=ยจ
ยฒ#ยฒยงย-ยฃ	ยจ.ย9ย_ยย8ยยถยฐ8ยจ.ย8ยค
ย	ย)รย_ย;ยย-ยชbย	ย'ย<ยฝ>ย	ย8ยจ!ร:ย_ยยย	
ยฌ 
ยฉย)ย=ยMยชยจย9ย;ย	ยยบ/ยฏ)ยฒoยจยตยCย=ยiย;ย-ยZย;ยจยตย9ย=ยยขยฎยพยจ:ยฃย;ยย;ยซ6ยช*ยด)ยฒ_ย9ย_ยบยปยจ.ยย	ย)ย<ยฏ)ยฒ=ยจ.ยIยฅยฑย4ยฒ_ย)8ย 6ยฎ/ยย!ยฌ
รยย9ย=ยฃยUย;ย8ยจยตย2ยชย.รย	ยชย	ย)ยย\ย.ยฎ,ยย-ยZยจ.ย:ย	ย'ย2ยชยจ!ยซ7ยจ 9Xย!ยฃย2ย;ย-ยUยฝ>ย	ย8ยจ
รย_ยยMย.ยฎ,ย;ย-ยUย9ยซ-ย9ย;ย	ยช ยจ.ย8ยคย;ย-ยUยย!ย9ยด)ยฒ_ยย
:7;=<?>	@6ACBEDGFH>	DGDIBEJKMLK	A3@GNHDG>COEPGDRQ7JSOEAUTJ@RLSKWVXFJK	DGPYLKWP?KW>	Z\[%NH@?JST=L]^NHKWPGDH;
_^`	`

acbedRf	ghbhbhijb#kml3nhif	oqp#o8ghrhbhijb#k
s%tus8vSwhxWy{z}|8xW~vยWย	ยยsยยxWยยxW~vยWย6ยยvยยยยยx	z8ยยยยvs$ยHxWxcvSw#z%v\ยยยยMยยยยz%ycยSยxW~#z}yHยsยยshยWยยhyยย~vSwhxcvSy^z}ยhย'z%ยYยWz%ySx
xย#z%ยยย!ยxยz%~#ยยย~qย'z%~!ย's8vwhxWyc~#z%vSยhy^zยยยยยยยHvxWย'ยWย
ย x'~hs%ยยยยwhsยยvSw#z}v$s8ยhys%ย6ยIย)ย~hxvSy^z8ยv^z}ย!ยยย)ยvHย5yx	ยHย!ยvMยWz}~5ยยกxxยยvSxW~#ย!x	ยevsยขvwhx'ยย!ยvHยยYz%|8xW~!v
ยWz8ยHx$zยยยxยยยYย ย x3ยย)ย)ยยฃย#ยยx$vSwhxUtGs%ย)ยsยยคยย~h|vHยs$ยxWยยยz8ยWย
ยฅยยฆยกยงeยงยขยจยชยฉ6ยซIยฌ#ยญยฏยฎcยฐIยฑ	ยฒ7ยณยขยดยต	ยถ!ยด%ยทWยฐยธHยนยบ	ยป8ยฒ7ยผยดยยฝGยฒยยนยถยพ)ยฝGยฐjยธยด	ยฟยยฒ	ยณ#ยฝ&ร{ยพ)ยด8ยณยกยณ#ยฐIยณยฟร?รยฐIยพ)ยฒ$ร6ยฒยด8ยผยณ#ยฐIยณยฟยทWร	ยทWยฝGยฒ	ยนร\ร?รยฒ7ยผHยฒ
ยฒยด8รWรรยด	ยฟยยฒ	ยณยกยฝ$ร!ยด%ยทยชยบ8ยณ#ยพ)รยชยบ8ยณยกยฒยฟ8ยบ7ยด8ยพรร'ยฐ รยขยด8ยณ.ยฒ+รรร	ยฐIยฒ	ยณยกยฝ$ยทWยด8ยฝGยฐjยทยรWยด8ร7ยฝGยบ8ยผ^รeยนยถยพ)ยฝGยฐjยธยด	ยฟยยฒ	ยณยกยฝUร=ยพ)ยด8ยณmรWยบ8ยผยดยรWรยฐIยฒ	ยฑ7ยฐยยณ!ยฟ
ยฝIรยฒCยทรยฒ$ยฟยยบ	ยด8ยพรยท$ยฒCรยฐjยทรยฝIยทร\ยฝIรยฒ	ยณรยฝIรยฒ	ยผHยฒยยฒCรยฐjยทรยฝIยทยทรยถ!รWรยด8ยณmยฒ+รรร	ยฐIยฒ	ยณ#ยฝยทWยด8ยฝGยฐjยทยรWยด8ร	ยฝGยบยยผ^รยนยถยพ)ยฝGยฐjยธยด	ยฟยยฒ	ยณ#ยฝยร=ยพ)ยด8ยณรยฝIรยด8ยฝcรยด8ยณ
ร ยฒยยฒ7ยณ#รยบ	ยป8ยฒยปยยฐIยณยรยยบ8ยพ)ร%ยณ#ยบ8ยนยฐIยด8ยพuยทยรยยด8รยฒWรยยด8ยณยกยป ร ยฒยฑ	ยฒ	ยผยฐ ร{ยฒยป'ยฐIยณรhยบ8ยพ)รยณยกยบ8ยนยฐยยดยยพ6ยฝGยฐยยนยฒ%ร
ร$ร7รuร#รยญ ยย~vSw!ยยยยWz8ยHxUx	z8ยSwz%|8xW~!v\ร!~hs%ยยvSwhxU|8sz7ยus%t6vSwhxsยvSwhxWycz%|8xW~!v	รz%~#ยยwhxW~#ยxUยvยยยยยยx	z}ycvSw#z%v
ยvcยย|8w!vcยx	z%y~s8~!ยยt+z8ยvยcz%ย#sยยhvcvSwhx3ย#s!ยSยยยย!ยx$ย~!ยvHยยz7ย\ยHv^z%vx	ย{z}~#ยย#xWw#z7ยยยs8y^ยWย
ร ยย8xW~รvSw#z%vยvwhxWySxยยยยs8~!ยยรzmยยกs%ยย~hsยยยยz7ยร~!ยhยย#xWyยs}tรยยกsยSยยยย!ยxqยย~!ยvHยยz7ย$ยHv^z}vSx	ยยz%~#ยexW~!ย8ยySsย~hย
ยยxW~!vcย#xWw#z7ยยยs8y^ยWร#z}~#ย|%ยย8xW~vSwhx3ยยกs%ยย~hsยยยยz7ยยฃยยกs8ยh~#ยsย~vSwhx$ย!xWยhvSwqs%tRvSwhxย!ยยz%~#ยWรhvSwhxWyxยz%ySxUs8~!ยย
ย#s}ยยย!~hs8ยยยz7ย)ยยmย'z%~!ยรยยx	ร8ยhxW~#ยx	ยXs%tยs8ย#ยHxWySย%z%vHยs8~#ยรรYx	z8ยws%tยฃยXw!ยjยwms%tยย#s%ยย!~hs8ยยยz7ย?ยยxW~h|ยvSwuร{s}tยฃx	z8ยw
z%|8xW~!vUvSw#z%v3z%ySx$s}tยยย~!vSxWySx	ยยv$รzยยย~5รuxWยยย'zยรกยกยEรข!ร^ยUรฃcxW~#ยx8รuยxยยWz}~ยขxW~#ยshย!x8รuย~mย#s}ยยย!~hs8ยยยz7ย{ยยย#z8ยx8ร
zย!x	ยยjยยยsย~ยชv^z%ย!ยxtรคsยycx	z8ยwรฅz}|8xW~vXยยxW~vยยยsย~!ยย~h|qs8~!ยยvSwhx	ยHxยยHx	รยยhxW~#ยx	ยWร6z%~#ยmยwhx	ยSรuรhย~mย#s}ยยย!~hs8ยยยz7ย
vHยยยx8รhยXwhxWvSwhxWycยvย!xWvxWySยย~hx	ยzยยSz%vHยยยยtYz8ยvSs8yยยยย!ยยvยยยยYz}|8xW~vUย!ยยz%~6ย
ยฅยยฆยกยงeยงยขยจยชยฉ6ยซรฆยยญ.ยฎcยฐIยฑ	ยฒ7ยณยยด3ยต7ยถยด}ยทรยฐjยธยนยบ	ยปยยฒ	ยผHยด8ยฝGยฒUยนยยถ!ยพ)ยฝGยฐยธHยด	ยฟ8ยฒ7ยณ#ยฝยกร{ยพ)ยด8ยณยกยณ#ยฐIยณยฟXร?รยฐยยพ)ยฒร6ยฒยด8ยผยณ#ยฐIยณยฟยทWร7ยทรยฝGยฒ7ยน.รงร=ร?รยฒ7ยผHยฒ
ยฒยด8รWรยด	ยฟ8ยฒ7ยณ#ยฝยกรยด%ยท\รจยรยยบ}ยท^ยทWยฐ ร ยพ)ยฒ\ยฟ8ยบ7ยด8ยพรยทรรฉยร?รยฒ	ยผHยฒ\รจmยฐjยท?รยยบ8ยพ)ร%ยณ#ยบ8ยนยฐIยด8ยพIยพยร ร ยบ8ยถยณยกยป8ยฒยป3ยฐยยณยฝIรยฒUยด8ร	ยฝGยถ!ยด8ยพhยผHยฒHร=ยผHยฒCยทรยฒ7ยณ#ยฝGยด8ยฝGยฐIยบ8ยณ
ยทWยฐรชWยฒHรซ%ร\ยฝIรยฒ	ยผHยฒยฒWร%ยฐยทWยฝIยท$ยดยต7ยถยด%ยทWยฐjยธยนยบ	ยป8ยฒ7ยผHยด8ยฝGยฒ'ยนยถ!ยพ)ยฝGยฐยธHยด	ยฟ8ยฒ7ยณ#ยฝรยพยยดยยณ#ยณ#ยฐIยณยฟยร?รยฐIยพยยฒMรรฌยฒยด8ยผยณ#ยฐIยณยฟยยทWร	ยทWยฝGยฒ	ยนรญรงRรฎUรฉYร?รยฒ7ยผHยฒ
ยต	ยถ!ยด%ยทWยฐยธHยนยบ	ยป8ยฒ7ยผยดยยฝGยฒยชยผHยฒGรWยฒ	ยผยทรยฝGยบรยฝIรยฒmยด8ร7ยฝGยถยดยยพยผHยฒHรuยผยฒCยทรยฒ7ยณ#ยฝGยด8ยฝGยฐIยบ8ยณรฏยทรยฐjรชรยฒqยบHรยขยฝIร!ยฒยขยบยยผ^ยฐ)ยฟ8ยฐIยณ#ยด8ยพยคยทรร7ยทWยฝGยฒ	ยนรฐรงยรซร$รยยฐIยฝIรรยด
ยถ!ยณ#ยฐIยต	ยถ!ยฒcยฟ8ยบ	ยดยยพWรWยบ8ยผcยฒยด8รWร'ยด7ยฟ8ยฒ	ยณยกยฝIร6ยทWยถรCรMยฝIร!ยด8ยฝยยฝIรยฒ	ยผHยฒ3ยฒCรยฐjยทรยฝIยทcยด8ยณยชรฉยยฒGรรร7ยฐยยฒ7ยณ#ยฝรรซ3ยทรยดยยฝGยฐยทIรรยดยร	ยฝGยบ8ยผร$ยนยถ!ยพยยฝGยฐjยธยด7ยฟ8ยฒ	ยณยกยฝhรuยพยยดยยณ
ยฐIยณรง รฎ ยฐ ร$ยด8ยณยกยปยบ8ยณ#ยพ)รยฐ ร$ยฝIรยฒ	ยผHยฒยยฒCรยฐjยทรยฝIยท3ยด8ยณรรฉYยฒ+รรร	ยฐIยฒ	ยณยกยฝรรซยยทWยด8ยฝGยฐjยทยรWยด8ร	ยฝGยบยยผ^รยยนยถ!ยพ)ยฝGยฐยธHยด	ยฟ8ยฒ7ยณ#ยฝยร=ยพ)ยด8ยณmยฐยยณqรงcร
ร$ร7รuร#รยญรง รฎ ยย)ยยย#ยยกxยhย!ยยยvz8ยutGs%ย)ยs%ยยWยยรฑwhxs8ย#ยHxWySย%z%ย!ยxยHv^z}vSx	ยus%t=z%|ยxW~vยรฒยกย~ รง รฎ ยย)ย)ย#ย#xvSwhxcยWz%yvSx	ยยยยz%~
ยhySshย!ย#ยvs}tRvSwhx$sยย#ยHxWySยz}ย!ยยxยHv^z%vx	ย\s}t&z%|ยxW~vรฒยย~ รง ยยvSwqvSwhx$ยHxWvcs%t\ยHv^z}vSx	ยWรณ
รดรตCรถรท!รธ	รถHรนHรบ^รป%รผWรต	รฝ7รธ%รพรฝรนรฟ7รบ Sรบ^รป%รผWรต7รฝ	รธ%รพรฝรน  รบ  รป%รท  รน  ย'รฑwhxMยย~!ยvHยยz7ยยHvz%vSxs%tยคz}|8xW~v$รฒยคย~ รง รฎ ยย)ย)ยcย#xยv^z}ร8xW~ยชvSs
ย#xvwhx'ย#z7ยyยยs8~#ยยยยยHvยยย~h|รs%tยยvย3ย~!ยvHยยz7ยรยยv^z%vSxย~ รง z%~#ย รตWรถHรทรธ7รถ รน รยฃz}~#ยmยv^ย|8szย\ยยยv^z%รยxW~ยชvSsยยกxvSwhx
ยHxWvยs}tรยHvz%vSx	ยรย~รยcw!ยยย	
w  รปรท  รน ยยยยzยชยsยยยย#s8~hxW~!v	ย1รฑwhxxW~!ย8ยySs8~hยยxW~vยย~ รง รฎ ยย)ย)ยรยยกxยขzยชยWz%yvSx	ยยยยz%~
ยhySshย!ย#ยvs}tยฃvSwhx3ยยกxWw#z7ย8ยs8y^ยยคย
~ 
 ยยvSwqvยsยยHxWv^ย 3z%~#
ย %รhยcwhxWySx  รน w#z8ย รจ ยยยยยHvHย~#ยv3xยxWยยxW~vยWรณ
รด  รฎ รนรฟ รบ Sรบ  รฎ รน   ย
รฝ   รป%รท  รน ร ยcw!ยยยSwqwhx$ยย#ยHvxยยx	ยยhvx
 |8xW~!vcรฒยยย)ย)ยยฃw#z7ย8x$zยยยยjยยvHย~h|8ย!ยยยHwhx	ยmz8ยvHยs8~6ร#ยWz7ย)ยx	ย รป%รผWรต	รฝ	รธ%รพ!
ย~ ยยvยยยย~!ยvHยยz7ยยยยv^z%vSx8ย.รฑwhxยชยHvz%vSxvSy^z}~#ยยยvHยs8~ tรคยh~#ยvHยs8~1ยย)ย)ย$ย#xmz8ยMยย~ รง รยhยhvยcwhxW~/รฒย#xWyยtรคs8yย'ย
รป%รผWรต	รฝ7รธรพ!รฝ   รป%รท  รน ยv^
ย ร~hxWย ยsยยยย#s8~hxW~!v  ย~รvSwhxยWz%ySvx	ยยยยz%~5ยhysยย!ย#ยvรHz%~#ยยชsย~!ยยยรยvWร3ยย)ย)ยรยSw#z}~h|8x 
vSwhx$ยw#z%~h|8xยย)ยยย?ย#x$vs รป%รผWรต	รฝ	รธ%รพ!รฝ รน  ย)t{z%~#ยs8~!ยยย)t\vSwhx3ยhySs x	ยvยยยsย~s%tยฃvwhxUยย~!ยvHยยz7ย\ยยกxWw#z7ย8ยs8ycsย
~  รน ยjย
รบ Sรบ  รน   z%ySxUvSwhx3ย#sยยยยย!ยยx|8sz7ยยย&tGs8y{z%|ยxW~vcรฒยย~ รง รยvSwhxW~vSwhx
 รฎ รน  ยยยย~z8ยhยยยvHยs8~6ร#z8ยSยยยhยยxvw#z%v รด  รนยรฟ 
รง
รฎ
vSy^z}~#ยยยvHยs8~tGยh~#ยvHยs8~ย~ ยย)ย)ย{ยSw#z%~h|ยx3vSwhx$~hxWยยs8ยยยยกs8~hxW~vUs%tยvSwhx$sยย#ยHxWySยz}ย!ยยxยHv^z%vx3vS!s  รปรท  รน ย)t
z%~#ยยs8~!ยยยย)tRvwhx~hxWย.ยs8ยยยยกs8~hxW~!v{s}tRvSwhxUxW~ยยยySs8~hยยxW~!vยjย{ยย~ยยv^z%vS"x  รฎ รน  รhz%~#ยzยHv^z%vxยSz%vHยยยยtGยยยย~h|  รน 
w#z8ยยยกxWxW~ySx	zยยSwhx	ยuย
รฑยคwhx\z%ย#s%ย8xยvSyz%~#ยยtGs8ySย'z%vยยยsย~UtรคySsยย รง vSs รง รฎ ยz%ร8x	ยuvSwhx?ยยย!xW~!vHยvHย$s%t#z%~$z%|8xW~!v	ยEยยก|8s!z7ยยzcยs8ยยย#s8~hxW~!v
s%t6vSwhxย~!ยvHยยz7ย)ยยยh~hร~hs%ยc~ยย#xWw#z7ย8ยs8y7ย\รฃcs%ยxWย8xWy	ร8z}|8xW~v\รฒ?z%~#ย~hss8vSwhxWyz%|8xW~!vยยย)ยยย6s8ย#ยยxWySย8xยv^ย&|8sz7ย
z7tGvSxWyยยv^$ย ##y^ยHvcz8ยvHยs8~6ย?ยยv\ยยยx	z8ยHยvSsยHxWxcvSw#z}v\vSwhx3z%ย#s%ย8xXvSy^z%~#ยยtGs8ySยz%vHยs8~ร8xWxWย#ยvSwhxยHยhยHvSxWยยร8ย#zยยยยย
ยยshย!xWy^z%vSx8รz%~#ยยชvw#z%vvSwhxWySxxยยยยHv^ยยzยขยz%vHยยยยtYz8ยvSs8ySยmยย!ยvHยยYz%|8xW~!vย!ยjz}~eยย~ รง ย)t$z%~#ยรs8~!ยยยชย)t3vSwhxWySx
xย!ยjยยv^ยยยย#ยSwzย!ยยz%~ ย~ รง รฎ รhยcwhxWySxUย~ รง รฎ x	z8ยwz%|8xW~!v{w#zยยs8~!ยยs8~hx$ยยกsยSยยยย!ยx$|ยsz7ยYย
%&'

()+*-,+).)+/+0	132 /+/+2/+465+7986:

;=<>!?@BA@CA+DFE9G+HI-?J<LK HNMBHO>P>QIR"S"HTDHOEU
V!WYXZ\[X6]_^a`cbYdfegihjklnmol-pOgCqsr!tujvwl xyjr!ozcxyg{qwl| jkJx~}zclk6kJgยk|ย~ยgยzcjNยajl vkJgยk|TpOยpยxyjrย ยk6uยq
gik|Flk.jyยยยgยjk6xยr!ozcxyg{qwl| jkJx3pOlxyg{pยยOlยxytvยยย\zclk.gCptsยqwzcgยkJj!xyvwl ยxylยยzยj-ย
ย G+HยI-?J<-KHยย+ยย<<-ยยRsG+<-SยR=E9G6I-Eย$MยI-A+A@CA+DQSG@cMCHยยยHI-ย9A@CA+D!@ยR"<-ยaยiMc@CA+HTE9ยIยกยขEIย?MBHN@CA>!ยฃMCEs@CยยคI-DHOAE
Oยก IRsHR"Rsยฃ6ยกยGIRยฅE9G+Hย< A+HRยฆHR9ยกยขยs@C?6HยฆยงI-?6<-KHยจ=ยฉ@BK HOAQE9G+HRยชE9ย9ยฃ6ยกยขE9ยฃ+ย9H<-ยยซE9G+HTI-?6<-KHMCHO>n>QIRยยฌ@CE"@ยR=HIRsยญ
E9<ย+ยย<LK HR@C>!@cMยI-ยยย9HRsยฃMCERยยy<ยยซ< E9G+HOยยซยกยข<AE9Hยขยฎ+ERaSG+HOย9H3E9G+HOยยHย@ยRยI"ย6<ยMBยญA+<>!@ยIMcMCยญ!?6<ยฃ+A6ยฆHยฆยฏยฃ+A6ยกยขHOย9EI@CAEwยญ
I-?J<ยฃ+EIT>!ยฃMCEs@CยยคI-DHOAEยฐRยชยญยฑRsEยHO>ยจยยฒ+<ย=HยขยฎยณI->PยMBHยฌยฑ@ยย~SHNS"<ยฃMยยฆ!Mc@CยดHTE9<ยถยต6A6ยฆIT>!ยฃMCEs@CยยคI-DHOAE"ยM{IยASG+HOย9H
ยกยขยIRยชGยยคI@cMCยฃ+ย9HR=<-ยยIยDHOAEยขRa>!@CDGE3<+ยกOยกยขยฃ+ยยทยธ@BAPE9G6I-EยยกOIRsHE9G+H=ยยคI-ยฃMCEsยญยฏIยDHOAE~>!@CDGE3A+<E3I ยก9G@CHOKH"@CERยฅD<IMยคยฌ
?+ยฃ+ES"HNย9Hยนยฃ@Cย9HE9G6I-E"E9G+HT<EยG+HOยยฐIยDHOAE"S"@cMcM3RsEs@cMcMยซ?6HI-?MCHE9<nIยก9G@CHOKHN@CERยฐD<IMyยบยฌE9G+HOAS"HยกOI-ARsG+<-S
E9G6I-E"E9G@ยRย+ย9<?MCHO>ยป@{R<-ยaยiMc@CA+HE9ยI ยกยขEI-?MCHยฌ+ยฃ6R@CA+DFE9G+HI-?6<-KHTEยHยก9G+A@ยยน ยฃ+HROยจ
ยผยซยฝnยพ.ยฟ6รรJรยยฟJรรรรรaร ร
ร IยยsMCยญQS"<ย9ยดยฏ@BAยงE9G+HI-ย9HI<-ยยยMยI-A+A@CA+DQSIR"ยฆHOK< E9HยฆFE9<nK-I-ยs@C<ยฃ6RยกOIRยชHR"<-ยยซยMยI-A+A@CA+DS"@CE9Gยกยข<>PยMBHOEยH
C@ Aยร< ย9>QI-Es@C<AFยทwRยชHOHยยทรMcMCHOAaยฌ+รHOA6ยฆ MCHOยยฌ+ร ย IยE9Hยฌ+รรรรยฑยบ+ยร< ยย>QI-AยญTย6I-ย6HOยRยฅ<A!E9G6I-E~E9<ย@ยยกยบยจ~รTR~ย9HRsHI-ยยกยG
@CAรE9G@ยRTI-ย9HIFย+ยย<Dย9HR9RยชHยฆ@CAรK-I-ยs@C<ยฃ6RTยฆ @Cย9HยกยขEs@C<A6Rยยฌ$RยชHOKHOยIM~@CA6ยฆHOย6HOA6ยฆHOAEยS"<ยยยดยฑR<?6RsHOย9K Hยฆ.E9G6I-ENE9G+H
IR9Rยชยฃ+>nย+Es@C<AE9G6I-EI!ยMยI-A+A+HOยNG6IRยกยข< >nยMCHOE9HT@CAยy<ย9>QI-Eยช@B< AF@{Rยฐยฃ+A+ย9HIMc@ยRsEs@ยยกยถยร<ย">QIยAยญR@CE9ยฃ6I-Es@C<A6Rยร+E9G+H
Rsยฃ+?+ยยคI-ยยHInE9G6IยEEยย9HI-ER"EยG6I-EยIRsยJHยกยขE<-ย3ยMยI-A+A@CA+DF@ยRยฃ6Rยชยฃ6IMcMBยญยงย9HยขยyHOย9ย9HยฆFE9<QI R"ยMยI-A+A@CA+D@CAยฃ+A6ยกยขHOย9EยขI@CA
E9HOย9ยยช@BEย<ยs@CHROยจ
ร ยฎยณIย>nยMCHR<ยยยย9HRยชHI-ยยกยGร@CA	E9G@ยRnRsยฃ+?+ยยคIยย9HIร@BA6ยกยMCยฃ6ยฆH.S"<ย9ยดรยกยข< A6ยกยขHOย9A@CA+DรยดA+<-S"MCHยฆDHIยA6ยฆ	IยกยขEs@C<A
ยทwรยง<<ย9Hยฌ~รรรร+รaรNIMCย6HOยยAaยฌรรรรยบยฌJS"<ย9ยด<Aรยกยข<A6ยฆ @CEs@C<A6ILMยฐI-A6ยฆยงย9HIยกยขEยช@BK HnยMยI-A6R!ยทwรNHIยAรรรรรHยขMcMC>QI-Aaยฌ
รรรยณร-ยบยฐIยA6ยฆS"<ยยยด<Aยง@BAE9HOยsMCHIK @BA+D.ยMยI-A+A@CA+DรI-A6ยฆยงHยขยฎยฑHยกยขยฃ+Eยช@B< A	ยทยครN>ย?+ยย<RsยyรยคA+D HOยRs<AรรรรE9HOHยขMยคยฌ=รรรรยบยขยจ
ย G+HTย9HI ยกยขEs@CKHI-ย+ย+ย9<I ยก9GF@ยR"ย+ย9< ย6<RsHยฆยงIR"I!E9<<-MY@CAE9G+Hยกยข<AE9ย9<ยM\<-ยยย9<?J<ER=<ย6HOยยขI-Es@CA+D!@CAยฃ+A6ยกยขHOย9EยขI@CA
HOAK@Cย9<A+>PHOAERยยฌยI-A6ยฆร@BAรE9G+HยฆHR@CDAร<-ยย9HIMCยiMc@cยรHรยกยข< AE9ยย<-MยI-ยยกยG@CE9HยกยขE9ยฃ+ย9HRFEยG6I-EnS"<ยฃMยยฆรก?6HรI-?MCHรE9<
ย9HIยกยขE@CAรIFR9I-Es@ยRยยคIยกยขE9<ยยยญQ>QIยA+A+HOยยฌYD-@CKHOA.ยฃ+A+ย+ย9Hยฆ @ยยกยขE9Hยฆ.HOKHOAERยทwรข=ย9<<ยด+ROยฌaรร รรฃยบยจ ย G+H@BAE9HOยsMCHIK @BA+D
<-ย3ยMยI-A+A@CA+DรI-A6ยฆยงHยขยฎยฑHยกยขยฃ+Eยช@B< Aร>QIยญRs<>nHOEยช@B>PHR?JHQInยฃ6RยชHยขยรยฃMIMCE9HOยยA6I-Es@CKH!E9<ยกยข<A6ยฆ @CEs@C<A6IMยMยI-A+A@CA+D6ยจ
ร<LS"HOKHOยยฌ@CA>QI-AยญFย9HIMc@ยRsEs@ยยกnยฆ<>QIL@BA6RE9G+HOย9HN@ยRTI!A+HOHยฆยงE9<Fยกยข<A6R@ยยฆHOยI!SG+<-MCH<ยMยI-ยยDHTย6<ยยEs@C<A<-ย
I!ยMยI-A?JHยขยร<ยยHยฆHยกย@{ยฆ @CA+D<AIยAIยกยขEs@C<Aaยจ ย G@ยR=@ยR"EยG+HยกOIRsHN@CAE9G+HTEยยI-A6RsยJ<ย9EI-Eยช@B< Aรคยฆ<>FI@CAI-A6ยฆFE9G+H
E9ยIยยฃ+>QI-ยยคยกOI-ย9H!ยฆ<>QI@CA.S"Hยยฆ @ยR9ยกยขยฃ6R9RยชHยฆYยจรฅHOK HOย9E9G+HยขMCHR9ROยฌรฆSH!RsHOH!E9G+HT@CAE9HOยsMCHIK@CA+Dยง<-ย3HยขยฎยฑHยกยขยฃ+Eยช@B< AรS"@CE9G
ย~MยI-A+A@CA+DSG@cMCHยยยHI-ย9A@CA+DรคIยฏย+ย9<>!@ยR@CA+Dยฆ @BยยHยกยขEs@C<Aยงยร<ย=ยyยฃ+E9ยฃ+ย9Hย9HRsHI-ยยกยGaยจ
รง HRsHI-ยยกยGร@CAรจE9G+Hยฆ @BยยHยกยขEs@C<Aย<-ยยยกยข<A6ยฆ @CEs@C<A6ILMยฐยMยI-A6RPยฆHIMยRSยฐ@BEยG	ยM{IยA6RT@CA	SG@ยยกยGรEยG+HQ<ยฃ+Eยขยกยข<>nH
<-ย"E9G+HQI-DHOAEรฉรชRNIยกยขEs@C<A.>QIยญ.IยaHยกยขETE9G+H!A+HยขยฎยฑE!IยกยขEs@C<AรEIยยดHOA?ยญ.E9G+HnI-DHOAEยจ ย G+HO<ย9HOEs@ยยกOIM"S< ย9ยด<A
E9G@ยR@ยR9Rsยฃ+H!@ยRN>QIL@BAMCยญรยฆHOK<EยHยฆEย<รคIRยชย6HยกยขERN<-ย"ย9HIRs< A@BA+DIย?6<ยฃ+ENยดA+<-S"MCHยฆDHFI-A6ยฆ.IยกยขEs@C<A_ยทwรยง<<ย9Hยฌ
รรร ร+รยรNIMCย6HOย9Aaยฌ!รร รร+รTรยง<ย9DHOA6RsEยHOย9AaยฌTรรรรซ ยบยฌIยA6ยฆยEย<รE9G+HยงMC<D-@ยยกOIMTยy<ย9>!ยฃMยI-Es@C<Aรก<-ยยยกยข<A6ยฆ @CEs@C<A6IM
ยMยI-A6Rรยท รง <RsHOA6Rยยก9G+Hยข@CAaยฌTรรรยณร-ยบยจ	รยJHยกย@cยต\ยก.>nHยกยG6I-A@ยRs>QR!E9<รจยกยข<A6RsE9ย9ยฃ6ยกยขEFยกยข<A6ยฆ @CEs@C<A6IMยยMยI-A6Rยฏ@CAยSG@ยยกยG
<?6RsHOยยKLI-?MCHFHOKHOAEยขRTI-A6ยฆ.E9HRsEยขRTI-ย9HnHยขยฎ+ยMc@{ยกย@CEsMCยญ	ยฆHยกยMยI-ย9HยฆรจI-ย9HQยฆ @ยR9ยกยขยฃ6R9RยชHยฆรIRNS"HยขMcMยยทรรHยขMcMB>FI-Aaยฌ"รรรรยบยขยจ
ย G+HRsHNIR~S"HยขMยM\I R~E9G+H">n<ย9HยกยMยIR9R@ยยกOIMYS"<ย9ยดT<AQยกยข< A6ยฆ @BEยช@B< A6IM\ยMยI-A6RยฐยทยครรI-ย9ยยHOAaยฌ+รรรซLรฃยบยขยฌLI-A6ยฆS"<ย9ยดE9G6I-E
ยy<-McMB<-S"HยฆIยA6ยฆHยขยฎ+E9HOA6ยฆHยฆร@BE@CAK-I-ยs@C<ยฃ6RNยฆ @BยยHยกยขEs@C<A6Rยทwย~HO<Eยรรฌร>!@CE9Gaยฌ~รรรรญ+ร ร E9รฎยข@C<A@ยคยฌaรยI-A+ยด+ROยฌ+รรฏHยขM{ยฆYยฌ
รNยI-ยJHOยยฌยYHRsGaยฌรรรรก@cMยMc@ยI->QRยช<AaยฌYรรรรญยบYG6IKH"A+<Eยยกยข<A6ยกยขHOAEยยI-E9Hยฆ<AnD HOA+HOยIM6ยกยข<>nย+ยฃ+EI-Eยช@B< A6IM\IRsยJHยกยขER
<-ย$ย$MยI-A+A@CA+DQSG@cMCHยยยHI-ย9A@CA+D6ยจรฐยฃ+ยS"<ยยยดQยฆ<HRยฐA+<Eยกยข<A6ยกยขHOAEยยI-E9HN<ARsยJHยกย@ยยต\ยกยถ>nHยกยG6I-A@ยRs>QR=ยy<ย"E9G+H
ยกยข<A6RsEยย9ยฃ6ยกยขEs@C<AF<-ยยซยกยข<A6ยฆ @CEs@C<A6IMยซยMยI-A6ROร รง IยE9G+HOยยฌ @CEยฐยกยข< A6ยกยขHOAE9ยยขI-E9HRย< AQDHOA+HOยยขIMยยกยข<>Pย+ยฃ+EI-Es@C<A6IMaIRsยJHยกยขER
<-ยยยกยข< A6ยฆ @BEยช@B< A6IMยยMยI-A+A@CA+D6ยจรฑร<>nHย9HยกยขHOAEnS"<ย9ยดรG6I RnIMยRs<ร?JHOHOAรฒยกยข<A6ยกยขHOย9A+HยฆรจSยฐ@BEยGรฒยกยข<>nย+ยฃ+EยขI-Es@C<A6IM
IRsยJHยกยขER"<-ยยยกยข<A6ยฆ @CEs@C<A6IMยซยMยI-A6ROยฌยณ?+ยฃ+Eยยกยข<A6ยกยขHOAE9ยI-E9HยฆF< AรคRsHOK HOยIMaA6I-E9ยฃ+ยILMยย+ยยยฃ+A@BA+DPย9ยฃMCHRE9G6IยEยฐยกOIยAQ?JH
ยฃ6RsHยฆรค@BAยงE9G+Hยกยข<A6RsEยย9ยฃ6ยกยขEs@C<A<ยย$ยกยข<A6ยฆ @CEs@C<A6IMยยMยI-A6RTยทยยฉยHOA+HRsHOย9HOEยGรfรฅ<ยฃ+ย9?6I-ยดG6RsGaยฌaรร รรณยบยจ
รดรตOรถ

รทรธรจรนยซรบรป+รธ+รธ+รผ{รธ6รฝ.รพTรฟ+รผCรบรป+รธ+รผ{รธ6รฝ

	


!#"%$&$(')*+',-'.0/$%'12*'$%
3+#'/546#.'$%'17+#
'89$%':;'<
7#'/5589$=>46#?@'<+##'6A'B:;4	AC4D+E<F#$<>G$9/+$%'1HI	/B#'J$%7$%

K#L#$?LM$%7+#
>,N$J$%'17+#
'89$%'JOK1P*L#$Q46
A<MR3SUTJ$?0
$>8G#'#V'$%
'$<W46#
/$%'$%
3XN893)*+'NM$35,9YZA''#'/[4	#$]\;$
'#'/:(0'<^AK,_`YaA''#'/b4c!#$
\;$
'*'/@L$<F'$%7$%
d893)*+'a%$%/+
#$	'<@
$%
$$%'1#'51M$%S
e '$%
g//+$$<(0
1h,i
jE0''*'/h#'='$%
3#'9$%'7+*
'89$%'3%:4	1)$DA%LA
f

+$<k#$66$$<<+AK#'=
$$%'1)*:Aj
$,_$%

$<f	;'#7$%
E0'ZOl1$%
3m:nopqRS
er'#7$%
3DA'QAh'$s*'Q4	At$G
$#'J,f$G/$%'($%7$%
C1P*L#$5$%7$%'=,	$
$%'7#
'8s$%'1&E)$u$<Q$vA##S	

$#3fA0E0$9/$%'$%
3XDE+$f,	)$%8Gc#'J4	A
$/+$%'1HI9#'9%0']L$>$u$<w$v!A##[#'b0't$xy#$%'1@8G''$%
:z#'w
3<$%
9C$%'L#$
+8GA67$%
)!u{%0#'BS-|
$%
89
$:1$%8G}6<('},~M*'=$0L7$E+$z8G(LM$
#'1
33L#$h$%7$%'@,-$(/$%'		+89#$%$#',_
8G)*+'5'5$$%'7#
'8s$%'1fL$%7+*+
S
	$%
(89$%4	0
$A$<>46
ยFA('$%
'$<ย46#JA''#'/>
$f4	$%
$9$9/$%+/
3
A6'ย'X4c'>OPYg<+#8=#
#>ยยย-''ยยA%:jnopoยย>%<$%
89+	ยยย7+A%:;nopยยR3Sd|ย
6$vย8s*$:
'$f8G9L$f#'$%
$$<=#'5u'<+#'/Gh
+$	#$<+#'/=,_
8U'$#G='$%
646#+2+%$6
'G0

A$c8GBSdย2Ez46
ยh8G(LM$	7#$%46$<@g(M$Ad%$6,B$	/$%'$%
,i
89$%46
ย,
YZA''#'/=4	#$ย\ย$
'#'/SZTJ
ยh'9$	<$K#/'s,{PE%X{
g+
#$%'1$%
zOยL$#3%:+''<+#$%
3R
g%$%g'=Lย$;#'='$6,{$%7$%
3X11K#L*$f
#$%'3#'Z'<&Z#B#'9c
$<$%$%
8=#'$<

#$%'3#'>Oย
3ย'B:MnopยRg8GGXE)9L$h7#$%46$<c9$A-%$0,a
2,i
3089$%46
ย;S
	
z46
ยfA6'$%
'$<=46#s$	ยBยย#'$ย'<ย'ยย*'$f
3+3L*=,BYZA''#'/=4	*$\;$
'ย
#'/SUย6A5
$A$s#5t46
ย]'$%
'$<[46#W$>
330L!#],9<+ยย$%
$%'15$@,=E0'ย
'#'/Oยg
ย:;ย0B:ยUlL
308G'A'B:Znooยยยz+E0'<$%
:gnooยR3S-ย6Ac4D+
ย98G#'#.+'$%'1
$<
'w'ยย*'$C
330L!#t,NJK#'/#$%ยย/+$%'15A''#'/]42*b89#$%$#',_
8G)*+'BSย	
@4D+
ย
'$%'
3$='t/$%'$%
3Xc+893#'XยM$3=,A''#'/t46#ย#'89#$%$5#',_
8G0#'B:
'KA<$%
3@A>8=##ยย/$%'GK##'m:D'<t<+A$9LMt'ยย#'$.0'<QยBยย!#'$
330L!#S
ย $%6fยBยย!#'$5<$K#/'?'<
3L#P:Z#/J'PE<$%
$<?'J
3#7$h#'QOPย>ย
ยD
1ยยย$%:gnoยoR3:MfLM$%$%'ย#89'$%/#$$<#'Q$=
$$%'1h$
3&OยL($%$FOPย1$ย
ยj$%''$%'#":dnooยยMl8ยยยกยj$%''$%'#":dnooยRRS
ย $$
3?+'J#',_$%
$%'$5,	u'#$%ยย8G3ยO ย #7$@ยยขl1#
$:	no+pq1:-nopoR89$='
 /$%'66
#$zh#',i$%
f$

$,a'5+8G'BSZย6$(/$%'zA6/#7$%'&*8ย*$<.+%$

Q$.08G'B:D0'<tA@$v$$<wQ/1#'w$%'/w#',i
8G#'wย<$<$C$.8s*$%$


$@,ย$8G0'BSJยz?'
3):j*'w$@,_
389$%46
ยJ<+A)$<Q#']A=0$%
:z$
/$%''$%$<+'*>/1X*'#',_
8G)*+'Jf46A<>$#>#'?
$+#'/5$9/0*7+$%'J/1ยS(ย2$%
$,i+
$:
#'54	}E2
LL#Gh891)-'
ย%)$:$8@'=A},ย#
#y8s!A%$<;:g#$
'*'/
#3=89#$%$

$=A9+893#'X!#>#',_$K#L#$S?ยc4D$%7+$%
:ZL$#'/J'#>#'$%
$$<C#'t
M$!u{/ย:g'$58GQL$L#$.+L3#']$5'$$
C#',i
8G#'B:6'<t%$%$<ย*'w
/1X~S>ยฃK']<<+##'B:z4D+
ย>'ย+893#'XD#$
'*'/Q89$h0N$G/#7$%']08G'E
,_!#t''$$<;:Z$%'L#$5
$*'/J'J3$N0,2$y8G+'ย0'<>$*8ย*'0#'/?$s'$%$<
,-7A<+#'/3$D,i
+8r4	A>$%
3$	
$h'	
$L#$Sย2Ef)89#'5ยค
0	$
+8G'sA6,_#''$$<@ยคยฅ8G97$%
@46$dL$&,~A$&#'8G'9
$X*ยย,_$ย0!A%)*+'%S
ย2$D0
g,{+
g4D+
ย(4	AG<+A$z8=##ยย/$%'gA'ZAg
$E0$<=fA$a#'G<+A
#L$<
efยฃOPยz'<ยยยฆ$%
:jnoppR6'<G$N89#$v#P,-8=##ยย/$%'E0''*'/COPยj$%''$%'#"Gย
ย1$%:fnop+oR3ยz4D$F#'17$)#/1$$8930#'ย<+xy#]0G
A$@<$C'$%
3#'P
'$%
'#'/@$N+#7##$c,}0'<<+##'-/$%'%OPR3S
ยง3ยจยฉ

ยชยซยฌยญยซ>ยซยฎยฏtยฐ-ยฑ+ยฎยฎยฑยฎยฒยณยดยตยถ

ยทยธ2ยน~ยบยปfยบยธ	ยปยผยฝEยบ0ยพยผยฟยปยผ%รยปยผยธยผ%รยพ3ยบยพร#รรยธfยบยปยผ=รรรรยผ%ยปรยผยฟ;ร;ยพรยผhร9รยฟยผยฝaร6ยผ(รยปยผยธยผ%ร1ยพfรAยธfยฟ+รรยยผ%ยปยผ%ร1ยพfยน_ยปรร
ร ยฝAยบยธยธKรAร%ยบยฝ	ยปยผ%รยปยผยธยผ%ร1ยพยบยพร#รรยธfร#รQยพรยผ5ยธรร#ยปร#ยพNร0ยนยร1รcร6รPร2รรdรKยพรAยธ=ยบรยฝAยบยธยธPรEร%ยบXยฝcรcรAยธรยปยผ%ยพยผGรgร+ยผ%ร1ยพsรรยธยพยผ%รGยธ
ร9รยฟยผยฝรPรcยบรGยบยฟรยผ@รUรJรรรยบรรDรรรรร3ร9ร2รยผ9รยผ%รยผ%ยป3ยบยฝ	รรรรยผรยพร#รรQรยผ%ยพร6ยผ%ยผ%ร?รยฝAยบรรร#รรJยบรยฟQรรรร
ยพยปร0ยฝ2ยพรยผ%รยปรQรยบยธ=รยผ%ยผ%ร[ยฟ+รAยธรรยธยธยผยฟยร#ร]รยปยผ%ร+ร#รรยธsรDร+ยปรtรPรfยผยบร]รรรJยผยฝยฝ#รGยบรBรรรรยรร3รJรKรbยบยฟยฟ+ร#ยพร#รรBร
รjยผ%รรยผ%รรรยฝ#ยพรก?ยบรยฟ[รขร1ยธยผยธ9ยธ)รรXรยขยบCยปยผยฟรรยพร#รร[ยน_ยปรรรฃรร+ร1ยพยปรยฝ#ร_ยพรยผ%รยปยผ%ยพรAรร9รยฟยผยฝAยธ5ยบยธ9ยพรยผ.ร+รยผยธ9ร6ยผ
ยฟ+รAยธรรยธยธsยพรJยพรยผ5รsรยปยผรยฝAยบยธยธKรAร%ยบยฝ=ร1ร	ร6รร6รรยยฝร#รยผ>ยปยผ%รยปยผยธยผ%รยพ3ยบยพ)ร*ร+รยธ@รPรjยผ%รรยผ%รรรยฝ#ยพรกJรยขรขรยธยผยธ%รfรรรรรร
ร6รยผ%ร>ยธรรรVรรรยยบ9ยพรรรEร%ยบXยฝ	ร1ร	ร6รPร2รรยยฝร*ร+ยผGยปยผ%รยปยผยธยผ%ร1ยพยบยพร#รร.ร%ยบ0ร.รMยผ9ยปยผยฟรรยผยฟยพร5ยบ5รครยบ+ยธKร#ร_ร9รยฟยผ%ยปยบยพยผ
ยปยผ%รยปยผยธ)ยผ%ร1ยพ3ยบ0ยพร#รรBร.รฅcรร6ยผ%รยผ%ยปรZยพรยผ5รรร1ยพยปรยฝ#ร_ยพรยผ%รยปยผ%ยพ)รEรsยปยผ%รยปยผยธ)ยผ%ร1ยพ3ยบ0ยพร#รรยธhรDยผ5รรรยธKรAยฟยผ%ยปยผยฟtยบ0ยปยผyรร+รรยผ%รร
ยพรยบยฝยฝ#รJยฟ+รรยยผ%ยปยผ%ร1ยพ&ยนiยปร+รรฆรยฝAยบยธยธPรEร%ยบXยฝDรยฝAยบรรร#รร>ร9รยฟยผยฝAยธ%รgยฟรยผ9ยพร5ยพรยผhยน~ยบรยพยพรยบยพยพรยผ%ร.รsรยฟยผยฝ-ยผรงรยฝรEรร#ยพยฝ#ร
ยพรยผ@รร1ยธยธKร#รยฝ*ยผรรยธยผ%ยปรยบยพร#รรยธhรยนยบรยผ%รยพ3ยธhยบรยฟJยพรยผGยผรBยผรยพ3ยธhรยนยบรยพร#รรยธhรร#รยผ%ร]ยฟ+รรBยผ%ยปยผ%รยพ=ยผ%ร1ร+ร#ยปรรร9ยผ%รยพ
รยผ%รยบร+ร#รยป3ยธ%รยปยบยพรยผ%ยป-ยพรยบรGยปยผ%รยปยผยธยผ%รยพDร+ยผ%รยผ%ยป3ยบยฝ;ยน~ยบ+รยพ3ยธ6ยบรรรยพ	ยบร@ยผ%รรร#ยปรรรsยผ%ร1ยพรzร6รยผNรจรฉรช3รซรจรKรยปรฌNรญรฎรฏ_รซรจIร
ยธยพ3ยบ0ยพยผ	รยนdยบร5ยบรยผ%รยพรรcรรEรร@รAยธzยพรยผร+ยผ%รยผ%ยป3ยบยฝยยบ0รยผ%ร1ยพรฐรฑยธ-ยธยพยบยพยผยฟ+รAยธรรยธยธ)ยผยฟsร*ร5ยพรยผยทfรZยฝร*ยพยผ%ยป3ยบยพรยปยผ=รรรรรยบรร
รรร+รฒร3รMร6รยฝ!ยฝ	รรยพรMยผ9ยปยผ%รยปยผยธ)ยผ%ร1ยพยผยฟยยผรงรยฝรAรร#ยพยฝ#รQร*รCรรยป(ยปยผ%รยปยผยธยผ%ร1ยพยบยพร#รรJยบรยฟ>ร6รยฝยฝ	รยผ9รรรยฝ#ยพhร#ร9รยฝรEรร#ยพยฝ#ร
รยบยธยผยฟhรรhยพรยผ	ยบรยผ%รยพรฐรฑยธBยบ+รยพร#รรยธgยบรยฟhรรยธยผ%ยปรยบยพร#รรยธ%รgรฅcยผ%รรยผรยพรยผ6ร9ร1ยธยพgยบรรยปร+รยปรAยบยพยผDยธPร*รยร!ยฝAยบยปzร9รยฟยผยฝรยน
รรรXร2ยฝ*ยผยฟร+ยผยปยผ%รยปยผยธยผ%รยพ3ยบยพร#รร@ร#รยทร}รEยธ2ยพรยผ(ยธKร#ยพรยบยพยผยฟ5ยบ0รยพรรGยบยพยบ9รร6ร1ยธยผ%รยธรรยผร#รBรBรรร+รณร3รgรด	ร+ยพรAรยผยพรยบยพร
ร#ร5รยผ%รยผ%ยป3ยบยฝยรยพรยผรรร=รยผ%ยป	รยนjยฝ*รร%ยบยฝdยธยพ3ยบ0ยพยผยธ6ยบรyยบ0รยผ%ร1ยพ6ร=ร#รรยพ6ยปยผยบรร9รAยธ6ยผรงรร+รยผ%ร1ยพรAยบยฝjร*ร5ยพรยผร1รร=รยผ%ยป6รยน
ร#ยพ3ยธ6รรยธ)ยผ%ยปรXยบรยฝ#ยผ9ยธยพ3ยบ0ยพยผยธ%ร
รตgรถ@รทGรธBรนzรบรปยรผzรฝXรพKรธBรน-รฝ
ยทยรยธ)ยผยนiรยฝgรยฝAยบรรร#รรยธรยธ)ยพยผ%รรฟรยผ%ยผยฟยธcยพร@รยบรยผยพรยปยผ%ยผhยผยธยธยผ%รยพรAยบยฝaรยปรรยผ%ยปยพ)ร*ยผยธm
ร jร*ยปยธยพรยร*ยพยธรรรยฝAยฟ>ยธรรรยฝ#ร
ยบ=ร9ยผรรยบรรAยธรUยน_รยป	ยพรยผรยผ%รยผ%ยป3ยบยพร#รร5ร0ยนaรยฝAยบรยธ%ร2รยผรรรยฟ;รร#ยพยธ)รรรยฝAยฟยธรรรยฝ#รยบ9รรรรรAยธยผhร	ยบรยยนiรยปcยปยผ%รยปยผ%ร
ยธยผ%รยพร#รร5รยฝAยบรยธ%ร=ร6รร#ยป3ยฟ;ร;ร#ยพNยธ)รรรยฝAยฟJยธรรรยฝ#รJยบร>ยผ yรร#ยผ%ร1ยพhร9ยผรรยบ0รรEยธ)รยยนiรยปยพรยผNร+ยผ%ยปร {ร%ยบยพร#รรJรยน-รยฝAยบรยธ
รยป6ยน_รยป6ยพยผยธยพ)ร*รร@ร%ยบรยฟ+รAยฟยบยพยผ=รยฝAยบรยธ%ร
Kร ร?ยพรรEยธยรยบรยผ%ยป=ร6ยผyรร+รรยผ%ร1ยพยปยบยพยผ9รรQรยฝAยบรรร#รรJร#รtรรรยผ%ยปยพยบร#รtยพยผ%ยปยปร#ยพรยปรรdรcรยผ%ยปยผGยพรยผyยบร+ยผ%ร1ยพhรยบยธ
รรยฝ#รJรยบยปยพรAยบยฝ-ร#รยน_รยปรGยบยพ)ร*ร+ร?รรQยพรยผ9ยผ%รร+ร*ยปรรร9ยผ%รยพNรMยผ%รยบรร#รยปรรJยผ5ยธรรรยกยพรยบ0ยพfร*ยพhรAยธhร#รยพยป3ยบรยพ3ยบ0รยฝ*ยผ@ยพร
รรรยฝAยฟtยบ5รยธยผยน_รยฝDรยฝAยบรรร#รร?ยธรยธยพยผ%รยขยผ%ร+ยผ%รJยน_รยปร9รยฟยผ%ยป3ยบยพยผsยปยผ%รยปยผยธยผ%รยพ3ยบยพร#รรยธยร_รยร ยผร#รjยปยผ%รยปยผยธยผ%รยพ3ยบยพร#รรยธยร#ร
ร	รรAรรJยพรยผ9รรร=รยผ%ยปรยน6รรยธยผ%ยปรยบรยฝ#ยผyยธยพยบยพยผยธfยบรยฟ>รรยธยธKร#รยฝ#ยผGรMยผ%รยบร+ร#รยป3ยธcรAยธรMรยฝ#ร1รร+ร=รAยบยฝ-ร#รJยพรยผ@ยบรยพรยบยฝ
ยปยผ%รยปยผยธ)ยผ%ร1ยพ3ยบ0ยพร#รรGยธKร#รก%ยผร3ร-รฅcรร6ยผ%รยผ%ยปรร+รยป-รรยธKร#ยพร#รยผยปยผยธรยฝ#ยพ3ยธ6ยธรรร]ยพรยบยพ}ร#ยพgรAยธzรร1ยธยธPร*รยฝ#ยผรร#ร5ร9รยฟยผ%ยป3ยบยพยผยบรยฟ
รค+รยบยธKร#ร_ร9รยฟยผ%ยป3ยบยพยผhยปยผ%รยปยผยธยผ%รยพ3ยบยพร#รรยธยรยร	รยผ%ยปยผยพรยผรรร=รยผ%ยปfรยนdรรยธ)ยผ%ยปรXยบรยฝ#ยผ9ยธยพ3ยบ0ยพยผยธzร=ร#รร1ยพcรยผ(ยผรงรร+รยผ%รร
ยพรAยบยฝ_ร3ร+ยพร(ยธยบยพรAยธKยน_ร=ยพร
ยผ รยฟsยบร	
ยฟ ยป3ยฟhรยปร+รยผ%ยปยพร#ยผยธzร9ยผ%รยพร#รรยผยฟ9ยบรMรรยผรgรฅ	ยผ%รรยผรรรBรยยฝร#รยผยยฟยผยธKร#รรsรยผรรร9ยผยธ
ยพยป3ยบ+รยพ3ยบรยฝ#ยผรยบยธ6ยฟ+รAยธรรยธยธยผยฟ>ยบรยฟยฟยผ%ร9รรยธยพยป3ยบยพยผยฟยร#รยพรยผhรยบรยผ%ยปร

รด รยพรAรยผ=ยพรยบยพ2ร!ยน6ร6ยผยรร+รยธKรAยฟยผ%ยป(รครยบยธPร*ร_รsรยฟยผ%ยป3ยบ0ยพยผ9ยธรยธ)ยพยผ%รGยธfยบรยฟยผyรร#ยผ%ร1ยพรยฝAยบรยธ%ร;รcรรEรร>รAยธยบsร9ร1ยธยพ
c
รยบยพรยปยบยฝยยธKร#ยพรยบ0ยพร#รรBรรรยป6ยปยผยธรยฝ#ยพ3ยธDร*รsรยฝ*ร@ยพรยบยพ	รaยฝEยบ0รรร*รร@ร	รรยฝ#ยผ
;ยผยบยปรร#รรsรAยธDยบ+ยธ-ยผ5รร*ยผ%รยพยบยธzรยฝAยบรรร#รร
ร6ร#ยพรรรรsรยฝ*ยผ%ยพยผfร*รยน_รยปร@ยบยพร#รรB
ร zรยพรยบยปยผfรรBรยยฝร#รยผยพยป3ยบรยพ3ยบ0รยฝ*ยผยบรยฟ@รรรยยฝร#รยผร#รยพยป3ยบรยพยบรยฝ#ยผร-รฅ	รร6ยผ%รยผ%ยปร0ร#ร
ร9รยฟยผ%ยป3ยบยพยผ9ยธรยธยพยผ%ร@ยธ%ร;รยฝAยบรรร#รร.ร2ร*ยพร?รรร9รยฝ#ยผ%ยพยผ=ร#รยน_รยปรGยบ0ยพร#รร>รAยธ(รค+รร#ยพยผ9ยพยปร#ร+รAยบยฝรยยพรรEยธcรAยธ&ยพรยผGร%ยบยธยผ=รยน
รยป3ยบ0รรยธยผยบยป3รร>รยยทfรรรรฅcรร{รยปร0ยนiยพรM
ร 6ยฝยฝ#รGยบรBรgร
ร รร3ร+ร	รรยฝ#ยผ(ร#รยพรยบยพร%ยบยธ)ยผรDยผ(ยธ)รรXรWยพรยบยพรaยฝEยบ0รรร*รร
ร	รรยฝ#
ยผ 
;ยผยบ0ยปรร#รร=รAยธ	รดร}ร_รยบยป3ยฟ;รgรขรยปยผfรยผ%รยผ%ยป3ยบยฝยฝ#รรร6ยผfรรยพ3ยบร#ร5ยบNรร+ร9รยฝ#ยผ%ยพยผ(รยฝAยบยธยธKร {ร%ยบยพร#รรรยนaรaยฝEยบ0รรร*รร
ร	รรยฝ#
ยผ 
ยยผยบยปรร#รรNยธรยธยพยผ%ร@ยธBรยบยธ)ยผยฟ=รร9ยธยผ%รยผ%ยป3ยบXยฝยปยผ%รยปยผยธยผ%ร1ยพยบยพร#รร9ยพรรยผยธgยบรยฟ=รรร9รรยพยบยพร#รรยบยฝ{ร%ยบยพยผ%รรยปร#ยผยธ%ร
รยรยยบยฟยฟ+ร#ยพร#รรBรร6ยผ?ยฟ+รAยธรรยธยธยผรงยพยผ%รยธKร#รรยธ5ร0ยนยรZยฝAยบรรร#รร[ร	รรยฝ#
ยผ 
ยยผยบยปรร#รรร(ยธรรรVยบยธ5รaยฝAยบรรร#รรbรcรร!ยฝ#ยผ

;ยผยบยปรร*รรsร#รร=รยฝ#ยพร#รยยบรยผ%รยพยยฟร+รGยบร#รยธ%ร
ร2รยผ=ยนiยปยบร9ยผ%ร6รยปรรยนรZยฝAยบรรร#รรJร	รรยฝ#
ยผ 
;ยผยบยปรร#รรรAยธ=ยบ5รยผ%รยผ%ยป3ยบXยฝ-ยน_ยป3ยบร9ยผ%ร6รยปรร	รยผ%ยปยผ@รยฝAยบรรร#รร>ร#ร
รรรยผ%ยปยพ3ยบXร*รtยพยผ%ยปยปร#ยพรยปรQร%ยบร?รMยผยธยพรยฟ+ร#ยผยฟ;รtร6รยผsร*รยพยปรยฟรรยพร#รรtรยนfยพรรAยธhยนiยปยบร9ยผ%ร6รยปร;รgยบรยฟJยพรยผ5ยปยผยฝAยบยพยผยฟ
รยรMร1ยธKร#ยพร#รยผNยบ0รยฟ@รยผ%ร1ยบยพร#รยผร-ยปยผยธรยฝ#ยพ3ยธ%รยนยยบรรยฝร*ยพยบยพยผ=ยพรยบยพ	ยธยพรยฟรร



!"###$%'&)(+*#$,-/.'-0"#1##$%'&
2 30457698:<;>=7?@;>AB;'5DC0E
)
FHGJILKNMPO%QROS,T0G	UVKWUVX'Y[Z#T]\+Y^ZFHGO_Qa`@Y[Z'QRUVX#bVGcG	Y[Z#K0ZPdfeWK0M'gbhGci0S,GcIjGcbgc`9klKNbUVX#GS,b+X#GO,mPknMPOLoK0e	p
e	GcZPUgcq
r);#sV;'tN;'573#;>E
u X#K'` u q>vwq,`@xKNmyobVK[knU`az#qavwq,`y{}|~OOeยY[Z@`!z#qยยยยPยย#ยqยย@ยfยยยยยยยยย0ยHยNย'ย	ย+ย>ย0ยยยcย%ยยยยยย7ย0ยยยyยPยกnยยข
ย+ยยฃย0ย0ยขย_ยกยคยPยยcq u Q#QNS_gยKNZ#pnFHGgยฅO,Gcd]ยฆยงM#ยจPOยฉS_gยXPS,Z#ยชยฌยซ7K0e	m'Y[ZPd0q
u OยฉO,GcZ@`az#q,`PxGcZ'QNO,Gcb`az#q,`#{ยฎยญ!Y[UhG0` u q'ยยยฏ7Q#gcqยq#ยยยNย0ยฐPยqDยฑLยย0ย0ยยคยfย^ยย_ยRยฒLยย0ย>ย'ยยคยfย0qaยณ/K0bVยชfY^Zยตยด+Y[MPkneยตY[Z#Z
ยฆM#ยจPOS_gยX#Gcbgcq
u eยจ#bhKfgยpnยถ<Z#ยชNGcbgยK0Z@`#z#q,`#{ยธยท0UVGcGO<`aยท'q>ยยยนย0ยบ0ยบPยqaยถยฅZfUhGcbยO,GYiNS,Z#ยชยปยฆยผO_Y[Z#ZPS,Z#ยช'`'ยฏยยฝ#GoM#UยS,K0Z)Y[Z'Qยตยณ/K0ZPS,UVKNbยS,Z#ยช'q
ยถ<Z)ยฒLยขยยยนยพยยยNย_ยPย[ยยยย+ยยยฟยรรVรรร0ร^`fm#m@q>ยบ0รยนรยบ0ยบ9q
รรK0Z'Qa` u q!xJq,`!{รรYNgVgยGcb`รqยงยยย0ยบ0ยบยq/ยฑLยย0ย0ยยคยfย[ยยย_ยย+ย,ยcยกnยขยlรยยfยกnยยยย+ยขยกnย รLยพยนย_ยNยรย>ยกnยยยคยยฉยย0ยยนย'ยพยq u ยจPO,Gยฝ
ยฆM#ยจPOS_gยXPS,Z#ยช)ยซ7K0bVm>K0bY[UยS,K0Z@q
รรbVKPK0Tgย`0ร+q u qPยยย0ยบNรPยq u รjKNยจ#M'gยULร@Yยนd0GcbhGQ	ยซรK0ZfUhbVK[Oaยท0dgยUhGceยฎklK0b7Y+ยณ/K0ยจPSO,G+รjK0ยจ'KNUqaรVรรร7รรย0ยPยขย'ย0ย
ยยย+ยฑLยfรhย0ยกnยยคยพcยย0ย>ยยรยfยกnย0ยwย0ยกnยยคย0ย>`@ร0`@ยcย^รร0ร#q
รรd0O_Y[Z'QPGcbยน`ยญ+qjยยยNย0รPยqรยซ7K0eWmPOGยฝPS,UยdรรjGgยMPO,UgยklK0bRยท0GcbยS_YยนOร\ยฟGoKNe	m'KfghY[ยจPSOยฉS,UยdaqรยถยฅZBยฒLยขยยยนยพยยยNย_ยPย[ยยตยยย
ยยยฟย+รhรรร[ร0`Pm#m@qaย[ร0ยยนร'ยรร^ย'q
\ยฟGY[Z@`[ยญ+qcรq,`{รFHGOO,eยตY[Z@`0ยณqcยฆ!q^ยยย0ย#ย^ยq[ยฒLยย0ย>ย'ยยคยfยยย0ย'ยJย7ย0ย>ยกnยขรยNยยฉq[ยณ/K0bVยชfY^Zยด+Y[MPkneยตY[Z#ZwยฆM#ยจPOS_gยX#Gcbgcq
ยฏbhK[O<`fยดwq,`PรY[M@`P\wq,`0{รยท0M#ยจ#bY[X#eยตY[ZPS_Y[Z@`Nvq9ยยย0ยNรPยq>รZยตUhX#G+ยซ7K0e	mPO,GยฝPS,UรdwK[k!\ยฟK0eยYยนS,Z#pnยถ<Z'QPGcm'GcZ'QPGcZPU
ยฆยO_Y[Z#ZPS,Z#ยช'qรยถ<ZHยฒjยขยยยพยยย0ยยคยfย[ยยยย+ยยยฟยรรVรรร[รN`fm#m@qaร0ยบ#ยยรร0ยบ0ร#q
ยฏUhรSKNZPSร`ยรq%`yxยฟY[Z#T#gc`@ยท'q%`#FHGO_Qa`@\wq,`'\ยฟbY^m'Gcb`a\wq,`'รaGgรX@`@รq,`>{รFรSOOS%Y^eยตgยK0Z@`@ยณqDยยย0ย0รยq u Z u m#p
m#bVKfY0ohXUVK+ยฆยO_Y[Z#ZPS,Z#ยชJIjS,UVXยถยฅZ'oK0e	mPO,GcUVGยถยฅZPklK0bheยตY[UยS,K0Z@qPยถ<Zยยฒjยขรยยนยพยยย0ย_ยPย[ย7ยยยยกยคยfยยฟร0ยขยย	ยรย0ยcยcยยขยยย>ยพย
ย0ย/ยฒLยขย_ย>ยพยรกยyยยcย+ยยย+รขย'ย0รฃjยยยย0ยJยฑLยรยyยขยยcยcยย>ยกnย0ยกnย_ยNยย0ย>ย	ยฑjยย[ยcย0ย>ย_ยPย0`Pm#m@q@ย0ยรครรyยร0รค#q
รGcZ#GgรGcbVGcUVX@`@ยณq,`'{ยธรK0M#bhยจ'Y[TfX'gรX@`aยถqaร+q@ยยย0ย0รPยqยญรS,e	G	ยทfYยนiNSZ#ยชยญLSm'gยknK0b+ยฆbVKNยจPOGceรฅยท0K[O,iNSZ#ยช/IjS,UVX
ยถ<Z'oK0e	mPO,GcUVGJยถ<ZPknK0bVeยตY^UยS,K0Z@qยถ<ZยฌยฒLยขยยยพยยย0ยยคยfย^ยยยยยยฟยยรรVรVร[ร0q
xยฟYยนO,m'GcbVZ@`z9q,`L{รฆยณKPgยGgc`รงwqยยยย0ยบ[ย9ยqยดยฟZ#KรI~OGQPยชNGรจY[Z'QรฉoK0e	e	KNZรชTPZ#K[IjO,GQPยช0G)S,ZรซYQNS_gยUVbยS,ยจ#M#UVGQ
GcZfiNS,bVK0Z#e	GcZPUq7ยญDGoVX@q9bVGcm@q'รzWย0ยfร#ยN`fยถยรjยณรจq
xยฟYยนO,m'GcbVZ@`'z9qfรงwq>ยยย0ยบNยบPยqaรjGY0gยK0ZPS,Z#ยชยตY^ยจ'K0M#UjTPZ#KรI~OGQPยชNG0รฌ u ZยK[i0GcbViNS,GcIq@ยถยฅZรย@ยPยย0ยขยยยนยกnย_ยพย0ยfยรยยฅยยยพยกยคยยยย
ยฑLยย[ยยยNย'ยยคยfย+ยรhย0ยPยก7รขย>ย0รฃjยยยยNยรรญ~ยฒLยขยยยพยยย0ยยคยfย^ยยยยยกยคยfยยรฎ0ร0ร0รยยพย0ยcยcยยขยยยนย'ยพย`0m#m@qDยรyย[ยfq
xยฟYยนO,m'GcbVZ@`ยz#q'รงwq,`a{ยธv7Y[bQNS<`@ยณq'รงwq@ยยย0ย#ย[ยqยณ/KQPGO7ohX#GoVTNS,Z#ยชยigcqaUhX#GcK0bVGceรฏm#bVK[i0S,Z#ยช'รฌaY	eยY[ZPSklGgยUhK'q
ยถ<Z)ยฒLยขยยคย'ยพยรกยyยยcยยฟยยย+รขย'ย0รฃjยยยย0ยรยฑLยยยaยขรยยยยยยนย'ยกnย0ยกnยยคย0ยยย0ย>ย	ยฑjยย^ยยย0ย>ย_ยPยfรญรยฒjยขยยยพยยย0ยยคยfย[ยยยย+ยกยคยfยJรฐyยยพย0ย'ย
รย'ยกnยยนยขย>ย0ยกnย_ยNย'ย0ยยย7ย0ยยยยยยนยขยยย'ยพย`[m#m@qaร0ร0รครรร0ร[ย'q
ยณ)oยนยซLY[bVUhXfd0`ยz#q,`!{รฑxYยนd0Ggc`ยยฆยqยยยNร0ยPยqRยท0K0e	GยตยฆยงXPSยฉO,KfgรK0m#XPS_ocYยนOยฆbhK0ยจPO,GceยตgยฟknbVK0eรฅUVX#Gยยท0UY[Z'QPm>K[S,ZfUwK[k
u bVUรSยฉรฒyohS_YยนOยยถ<ZPUVGOOยฉS,ยช0GcZ'oG0qรรณ]ย0ยพยยfยยคย'ยJรย'ยกnยยนย_ยยย0ยยนย'ยพย`0รด#`#ยPร0รยนรรค0ยฐNร#q
รตรถรท

รธ0รน#รบ[รป#รน)รน#รผ#รฝรฟรพNรผ#รผ0รผ	

 "!$#&%(')+*,.-0/1&243507698(!;::%<:=?>@A'CB@DA=&DFEC:GG!$%:FB7H%<IKJ"LGMON PRQ
S NUTV.W0XYMOZVUV[N]\Z$X S Z^^&

_&`>a7bcd-0/1&2e50C>@!&'f:%:=F!4g&Aihj:lkm8<=&n!;:Fo"Gf%:7"BdD7Yp7K/1/&.q>@rjrs:t
:!4f%:!$8s

_=:'H:7uv-G/12w5G7hj:lkm8<=&i6KG:&%f%:'yxz@o"Gf%:'@!4:?6{8|!4:')7rs:}@Lf~ S Z0ZGNUX\4ยC~fย
MUยZย4ยMUย?W0XYMOZLGXTMONU~XT&Vย~NUXM,ย~XยZLfZ$X S Z"~X?J"L0MON P S N|T&V.W0XMOZ$V|V]N]\Z$X S Z4pp72ย+wย+2w3Y

_'f'YยยB7:::D;8<ยย
ย7-G/11/45GCbpY0!4f%:_%:ยEC:GG!$%:ยBdf%&IFEc'ย%<:=F!ย
_A8f%t
*K:f%ยIย
_G8ยrย:ยยRAย'fG%:7KoKยยG8|ย!4:7KยR-ย*')]50KJ"LGMON P S NUTVKW0XYMOZVUV]N[\&ZX S ZTXY
ย~ย,ย`ยMOZLFย{N(ยยNU~X*{8|'f#&%q%<:G

_'f'jยCยBd:::D48ย
ยj-G/11ยก5Gยฃยข@ยค7tsuY%:ยฅ>@!'f:%:=ยฆxzยงยขC:tsuY%:ย*{ยจยฉ%:GI.ยฃrย:
}yLf~ S Z0ZGNUX\;ยm~HยยชMUยZย4ยซMUย?WGXMOZLGXT&MON|~XYTVYย+~&N|XYMCย~X)ยยZ$LfZX S Zi~&XยฉJaL0MON P S N|TVYW0XYMOZVUV]N[\&ZX S Z
ยฌj!40!40!;ยญH!;:7ยh;-0/12ย+50oj:"oC8=H%<D%|ojpp!&D"@DRocAย!4f%|Kj'ย%=:";xยฎ6K!40'dยขCf%:G'
rs:ย}yLf~ S ZGZ0NUX\;ย~fยCMUยZ"^ยฏMUยWยฐRยฐยฐยยฑ.ยฒ4ยCย7ยณ~&Xยด7~ยXT&MON|~Xยฎยy~fยยตย~ย,ย`ยMOZL@ยฑ S NUZX S Z)pp7/ยกยถ$ย
/3ยถ
6K!4p!&%%H%<&A7bjlยทย!4::!4ยl!;ย%|'
ย-G/1215GqD&'f6K!4D'{ยธย%D&A!i
!4p7rย:ยJ"ยMO~ยTMOT4ยน
ยบยปT&X\ยT\&Zย?TXยง}yLf~0\&LfTยยยชNUX\ยณ"ยยผ4MUยยฅW0XYMOZLGXTMONU~XYTVjย~VUV[~$ยฝยN|ยยยพ}yLf~ S Z0ZGNUX\;ยย{pp7@ย/e$ย
ยยถe
6{7
ย.o7ยq%D7{{-0/11&ยถ50b&:&%<H%<&:!$8mยฌ,:8]%:!46{8|!4::%:=rย:ยฅ}yLf~ S ZGZ0&N|X\4ยi~fยยMUยZ
ย$ยยMvW0XMOZ$L0XYTMONU~XTV{ย~&XยZLfZX S Z"~X?JaWc}@V]TXXYN|X\ยตยฑ`ยฒ$ยยMOZ$ยยตยยpp77/2&1$ย`/1w
>C!4ย!=6{ยฟยธย:D!4Fยธ Y-0/12&1507B@Db&:48`4x,%|'Gi*v#:cqI'fย'K}@Lย~ S ZGZGN|X\4ย
~fยiMUยZWยฐRยฐยฐ@ยยฏยฏยฎ-0/450Y2/Gย+12ยฎ
>R%#'f$`>".uKYยqD!4p%7>".*C.-0/$12w50R,%<#&0'ย%fItsย@!'fรrs:xO:G4xยd%<:%ojAย!;0!Krย:
}yLf~ S Z0ZGNUX\;ยy~fย"MUยZi^ร4MUยWยฐRยฐยฐยฅยฑ`ยฒ4ย,ยยปยณd~Xยยด7~ยXTMONU~Xยฎยc~fยย~ย,ย`ยMOZL,ยฑ S NUZX S Z$4pp7Ywl2$ย+2w
>R%#'f$i>"CuKCรqD!4p%ยต>"c*,j-0/121+50 rs:xO:Gย4xnยd%:%ยojAย!40!ยฅEj'ย%:=รร,%:=
qรA:G'รrs:ยฟ}yLf~ S ZGZ0NUX\;ยย~fยMUยZย^ย4MUยยJยKรรยฑ`ยฒ4ย,ยยปยณa~Xยร7ยZG~LGยฒย~Hยยย~ย,ย`ยMONUX\,pp7
3//Gย3ยถe
>@'H:'DG%:7dqร.-G/12ร5G7ยย!$8ยปB@D&f%'C4xKhj:lkm8<=&"%:?ocr@!4:?>@gYf%|'@ร"ZรรรCZXYZLQ
TMON|~&XยยR~ย,ย`ยMONUX\&7ยซd-ยยก50ยก43รlย+ยกรw
>@'H:'DG%:7รqCรCยh"!;G8<g8]%:=uKC6{j-0/$12ย50รB@Dย'fI:D'ย%|'?4x&%<=;%<G!$8ยชย!&D%:'_k@%D
p4#l!4g8@p%|'f%|@p&pf%'ยฎrs:ย}yLf~ S ZGZ0NUX\;ย~fยCMUยZยตย~XยZLfZ$X S Zy~&Xยร7ยZ0~LfZ$MON S TV4J"ยยย+Z S MUย
~fย"รyZGT4ย~XNUX\รTร~&ยMRรiX~&รyV]ZG\Z$pp72ยก$ย+1&2
>@'H:'DG%:7,qK-0/$12/450ย6{8|!4:รqI:D'ย%|'รoร8<&=4%|!$8c6{0'fpGH%<#&?rs:ร}yLf~ S ZGZ0NUX\;ยยช~fย?MUยZยยฏMUย
W0XMOZ$L0XYTMON|~&XTVYย~N|XYMCย~&XยZLfZX S Z"~X?J"LGMON P S NUTVYW0XYMOZVUV[N]\Z$X S Z
qDpp0')+
ย&-0/12+w50EC:%#G'!$8ยป698|!4:'{xOK>@!Gf%#">@gY0'd%:ยฉE,:p&%|G0!4g8"*K:#%&::G'
rs:}yLf~ S ZGZ0&N|X\4ยc~fย~fยcJjJcJaWQHรยฎยฏpp77/e&ยก1$ย`/e;3ย
ร0รร

รCรยรยรรรรร(รรกรข"รฃรรรค_รฅรครรฆรร(รรก
รงรจรฉรจรช4รซ_รฌรญรฎรฌรฏรฑรฐdรฒรณรณรฒรณรจรฉ4รดรตรถรฌ.รทยรฎรธGรนรบรบ4รปรผGรฎ{รงรฉ+รฝรพ|รช$รด9รฟdรชOรฉ	รตfรพ
`รฝรพ|รช$รดรฒรณรตjรงรฉ+รฝรพรฒรตfรพรฒ	Uรด[รพรณรฒ
รฒ ยรพ รณ7รฎรฐdรฉยรช รฒรช @รพ
รณ fรฉ รณรชlรดยรฎ
 
รงรจรฉรจรช4รซ_รฌรญยชรฎยปรธ0รนรบรบ+รผ0รฎรฒรณรต!"fรพรฒรณรตรฒ#%$&รฉGรช4รซรซรพรณรฎKรฐdรฒรฝรจ7รฎรฒ7รฎ.รงรฐ!')(Cรงรน**+sรบรฌ  รฒ รตรฎรฉ
(รฉ,
รซ รตรฒ jรงรฝรพรฒรณรฝGรฒรฌ{รงรต0รช4-รณ Oรฉ .#0/,รณรพ 1รฒ .ยรพ)รต 2รฎ
รฐdรฒรณรณรฒรณรจรฉ4รดรตรถรฌKรทยรฎ{รธGรนรบรบรน4รผGรฎ4365879;:<6=?>	:)@BA):DCE:F<G=IH=I9;J<KH<GLM>N:OHCEJP<G9;<RQS9T<VU%W-X=I9YIZ	Q:<6=\[B]FCE=I:F^_Cยรฎ
$Kรจ7รฎ  รฎรตรจรฒ ยTรพ a
รฌ `ยรฒGรพรถรซยรช4รณb
รณ sGรณ fรตHรพ<cรต รตรฒรฌ de0รช4รฒGรดsรฎ
รฐdรฒรณรณรฒรณรจรฉ4รดรตรถรฌKรทยรฎ(รฌ`รฏยรท_รฉRfรฒรฌ.รญรฎ{รธ0รนรบPfรบรผ0รฎ_CรณK(รฉรฉรฒ0รช;รตfรพรฉรณ_รพ<รณยรช?รทรดรตfรพhgvรณรตfรพรต)2ยรท_รฉi#รฒGรดsรฎ_sรณkj	AdJY
7.:O:.LP9T<-QClJ)m4=;n-:poo=;n0q.<6=I:AO<GH=I9;J<6HX6riJ9;<G=s?J<DmE:FAd:F<G7O:!J<0Z!AO=I9 t	79;HX6q.<G=I:FXTX9Q:<67.:$รฎ
`ยรชcรฒรณ7รฌ  รฎvuยตรฎ  รฎ7รธ0รน$รบ-wyxรผ0รฎzjรฒรณรฒ0รช4รตfรพรณb(รฉ&รณG#&รพ<รตHรพ<รฉ&รณรช$รดN${รด|รช4รณG"รช4รณG#b$&รฉ0รช;รซpรฎ	sรณ{jNA)J7O:.:OL9;<RQC|J)m
=;nR:_[BW-^}^~:FA,s?J<DmE:FA):<G7O:!J<0Zq|H<6L,[v9T^~W-XH=I9TJP<%Jem!ยย:DnRHยF9TJAcย?3	L9;<BยW-AIQnรฎ
`ยรฒGรด]รดรซยรช4รณ7รฌรทยรฎ${รฎYรธ0รนรบรบ+รผ0รฎยยJA.^~W-XH=I9TJP<%JempยGA)HL:OJ)ย"C!9;<0jNXH<6<G9;<RQยย<6L:A0ยย<G7O:AO=IH9;<G=I]4รฎ${รพรตรซยรช4รณ7รฌ
รฟ.รฉรณG# รฉรณ7รฎ

ย.ยย

	
		
	

	!"$#%$&('*)+,,-/.1002433

@BADCFEHGJI

K

GDL*M/EHNOADCFEQP

_a`cbedgfihkjmlonqper

56789:;-/<=,->?7
#&	:*3/<,-

ASRHTIVUWI

XYC[Z;L;GD\^]

sutwv!xqyqz{tqy!|y}qtq~!;ยย"ยยยยqyย~eย~qvqย

ยiยSยยยiยย/ยย4ยยย^ยยยยยยยยยยย=ยยย^ยยwยoยยย=ยwย	ย=ยwยยยยHย4ย
ย*ยยยwยยยยก4ยยeยยฃยขยฅยคยงยฆ	ยจยงยยฉยค
f;nยยชยงrwยซยญยฌยฏยฎยฐh9ยฑab"ยฒยซ*rqlยณ

q}qยด	ยตy1tqยez{}qยด	ยi}qยถuyยยทยยยธยยนยยDย~qvqย

ยยบ*ยปยฝยผยฃยพยฟรย	ยoยยFยkยย/ยย4ยยยยยยทยยยย=ยยยทรยจยยคDรDย4ยยย{ยiยoร
ร ยยรยขยยก4ย/ย=ย9ย"ยผรรยจoรรยค
รยb"ยซยญยฌยทยณerยฐรanยยฒยฒrwl

ยถq}qยต1qยqyqz[~q|	ยยยนยytqรuyยด	yยฉรยทยyยยถuร

ยยทยก4ยฟยฃยยรยยย ร ยยยยwยยkรยฃยยร4ยยยยยกยย{ร"ย1ยยรยงย=ยยยยยยJยกร^ยยทยยพยย ร/ยกยยยย
ยบยฃยยร	ย6ยพรย=ยยรย"ยผรร4ย!รรยฉยค

รรรiรoรรรeร!ร
รกยฃรขรครฃ	รฅรฆรจรงรรฉรรฆรชรซ9รฌรครฅรญ1รฎ	รฅ{รฏรงoรฐรฅ^รฑ ยบรณรฒ รช	รฆรรด$รง	รขรครฆรขรครฆรฎ^รต!รถรฐรดรทรชoรถรญรฅ6รถ(รงoรฆรญHรฐ$รดรง	รด$รขรครฐรดรข รฒ รงoรฌuรธ%รง รฒ รด$รฐรน"รซ*รฅ รฒ รชoรฆรฐรขรครญรฅ6รถ(รง
รบ $รถ รขรครฆ รฒ รข รบ รฌรครฅรญรรปรผรฅรดรฝ1รชยรญรน รฒ รง	รฌรครฌรครฅรญรผรดรฝรฅ ย=ย4ยqย	ยก4ยฟDรพรฟรยกย/ยพรยย9ยฟJย=ย%ยยก/ย รนรรธ%รช	รถ รฒ รช	รป รบ รดรขรครฆรฎ รง{รญรฅรฎoรถรฅรฅยฃรชรธ"รฏ!รฅรฌรครขรครฅรธยรด$รฝรง	รด
รฐ$รช	รปรรฅ*รธรช	รถรป
รฌรรงยรฝรชoรฌรครญรฐ;รฎ	รขรครฃoรฅรฆFรฑ ยบ รธ"รซ*รฅยฃรง	รถรฅkรถรฅรงoรฐรชoรฆรขรครฆรฎ[รงoรฏยรช รด*รง[รซ*รช	รถ$รฌรรญ^รชoรถยทรฐ	รรฐรดรฅ6รป รฒ รชoรฆรฐรขรครฐ$รดรขรครฆรฎDรชรธ

 รขรรฆ1รญรขรครฃยรขรครญ รง	รฌรครฐรนรดรฝ1รฅรฆรจรซยทรฅ รฒ รง	รฆ รฒ รช	รฆรฐ$รขรครญรฅรถ*รง	รฌรครฌ รบ รช	รฐ$รฐรขรครฏรฌรครฅiรซ*รช	รถ$รฌรครญรฐรนรรช	รถeรต!รถรฐรดรทรชoรถรญรฅ6รถiรปรรชรรญรฅ6รฌรรฐ6รน	รซ9รขรครดรฝ^รญรชoรปรรงoรขรรฆ
 
 รดรฝรงoรดeรฐรง	รด$รขรครฐรธ [รฑ ยบ รนoรง	รฆรญ รฒ รช	รป รบ รด$รฅยทรด$รฝรฅ*รธ%รถรง รฒ รดรขรครชoรฆ{รชรธqรดรฝรฅ6รป รขรครฆ[รซ9รฝ1รข รฒ รฝรจรขรครฐeรดรถ รฅ  รฅยทรญ1รฅรตยรฆรฅ
รด$รฝรฅยฃรญรฅรฎ	รถ$รฅรฅ{รชยงรธยรฏ!รฅรฌรครขรรฅ=รธ รด$รช[รฏ!รฅ{รด$รฝรฅยฃรง	รฐ รรป รบ รดรชoรดรข รฒ รฃรง	รฌ รฅkรชรธยรด$รฝรขรครฐ*รธ%รถรง รฒ รดรขรครชoรฆรรง	รฐ 
 รฎ	รถ$รชรซ9รฐ9รฌรรงoรถรฎoรฅ  รฅiรฐ$รฝรชรซ
รด$รฝรง	รด*รซ9รฝรฅรฆรจรดรฝ1รฅ(รฃoรช รฒ รง	รฏ รฌรรงoรถ 
รฆรญรฅ6รถรฌ รรขรรฆ1
รฎ  รง	รฆรญรจรฑ ยบ
รฐรฅ6รฐ รฒ รชoรฆรฐรด$รง	รฆรรดรฐ9รงoรฆรญ
รฆ1รง	รถ  รบ รถรฅรญรข รฒ รงoรดรฅรฐkรช	รฆรฌ oรน
รซ*รฅ รฒ รงoรฆรรฆรงoรด รถ$รง	รฌรครฌ [รงoรฐรฐ$รช รฒ รขรรงoรดรฅยฃรง	รฆ ย=ยยย=ยก=ยย รซ9รขรครดรฝรผรฅรง รฒ รฝรผรซยทรชoรถรฌรครญ  รฐ 
 รฎ	รถ$รชรซ9รฐยทรฌรครงoรถรฎ	รฅ6รถรนรรดรฝรฅรถ$รฅ{รง	รถ$รฅiรปรผรง	
รฆ 
รปรผรช	รถรฅkรซยทรชoรถรฌรครญรฐ9รซ9รขรครดรฝรจรฝรขรครฎoรฝรฅรถ9รฅรฆรรดรถ$รช รบ   รฝรฅรถรฅ=รธ%รช	รถ$รฅรนqรซยทรฅ รฒ รงoรฆ
รฐรฅ รง ยฟร
ย 4ยยยฟDยยยฟDรพ ย=ยย%ยยกย1ย{รฒ รช	รป รบ รดรงoรดรขรครช	รฆ
รด$รช รฒ รช	รป รบ รด$รฅยทรด$รฝรฅ9รญรฅรฎoรถรฅรฅkรชรธqรฏยรฅรฌรครขรครฅรธ  รฝรขรครฐ;รถ$รฅรฐ รฌรครด;รขรครฐ;รขรครฆDรงยฃรฐรขรครปรรขรครฌรครง	รถuรฐ รบ รขรครถรขรครดeรดรช รบ รถรฅรฃรรขรครช รฐ;รซยทรชoรถรฉ{รขรครฆ รบ 
รฝ รรฐรข รฒ รฐ
รงoรฆรญรจรง	รถรด$รข รต รฒ รขรครง	รฌqรขรครฆรรดรฅรฌรครฌรครขรครฎ	รฅรฆ รฒ รฅรนยรฏ รด9รขรครฐ9รธ%รงoรถยทรปรผรช	รถรฅยฃรฎoรฅรฆรฅรถ$รง	รฌ ! รธ;#
รฅ " รง	รฌuรขรครฆยรด$รฅรถรฅ6รฐรดยฃรดรช^รด$รฝรฅ{รถรฅ6รฐ รฌรครดiรขรครด$รฐรฅรฌ รธ;รง	รถ$รฅ
รด$รฝรฅ[รฌรครขรครปรรขรครด$รง	รดรขรครชoรฆรฐยทรชoรฆรรขรครด$รฐ(รฐ รฒ รช รบ รฅ %$ รชoรฐรดยฃรขรครป รบ รช	รถรด$รง	รฆรรดรฌ oรนรดรฝรฅDรถรฅ6รฐรดรถ$รข รฒ รดรขรครช	รฆaรด$รช
รฆรง		รถ  รบ รถรฅรญ1รข รฒ รง	รดรฅ6รฐ[รฐ$รฅรฅรปรรฐ
รฆรฅ รฒ รฅรฐรฐ$รง		รถ  & รฌรครด$รฝรช รฎoรฝรรด$รฝรฅยฃรถรง	รฆ1รญรช	รปJรทรซ*รช	รถ$รฌรรญ1รฐ;รปรรฅรด$รฝรชรรญรรปรผรง	รฉ	รฅ6รฐ;รฐรฅรฆ1รฐรฅDรขรรฆรผรฎ	รฅรฆ1รฅรถรงoรฌ%รนรด$รฝรฅ รฒ รชoรฆรฆรฅ รฒ รดรขรครช	รฆรจรดรช
รปรผ
รง 'รขรครป
รปรณรฅรฆรรด$รถรช รบ Hรฐ$รฅรฅรปรผรฐiรดรชรจรญรขรครฐ$รง รบรบ รฅรง	รถkรขรครฆHรดรฝรฅDรฆรชoรฆรรท รฆรง		รถ  รฒ รง	รฐรฅ ( รฝรฅรฐ$รฅ^รช	รฏรฐ$รฅรถรฃรงoรดรขรครช	รฆ1รฐiรฐ รฎ	รฎ	รฅ6รฐรด
รฆ1)รฅ ' รบ รฅ รฒ รดรฅรญยรฌรครขรรปรผรขรครดรง	รด$รขรครช	รฆรฐeรดรช^รด$รฝรฅยฃรง รบรบ รฌรรข รฒ รงoรฏรขรครฌรครขร*รด Dรชรธeรปร+
รง 'ยฉรขรครป
รปDรทรฅรฆรรดรถรช รบ {รปรรฅรด$รฝรชรรญรฐ 

,-/.0

รร1&243iร!ร51

0

6879;:=<?>A@B&C9/CD@9AE&F7B@GIH;@BJEK:)LI:)E#@MON;PQ<REJST:)7MU@&<V9AWX7B#MYCE)<V79%C[Z;7\IE&C]H;CB#E^<?_\A`CB8:^\IZIa=@b_Edce:)\;_JS
C :!<R9AE#@B#9;Ce`fMU@b><?_J<V9I@gh7MU@]WiC_E:cj:^\;_#SOC:kCd``lH;C[E)<V@9jE:4P]<VE#SmSI@H;CE)<VE)<:]@GISA<VZA<VE&a^C[\I9;><?_@bnoc;_C9
Z;@9;CE#\IBCd``RL/@GIHIB#@b:#:)@b>p<V9YC%:^EC9;>ICB>Tq;B:)E#r*7B>A@Bs`R7D<_cjPtSA<u`V@7E#SI@B:+cj:)\;_JSYC:kvwAxy7WH;CE)<V@9AE:
E#S;CE@GISA<RZA<VEta^C\I9;><_@mS;Cdz@%SI@H;CE^<RE^<?:JnIcCB#@:)EC[E)<:)E)<_Cd`g/h\IHIH;7A:)@E#SI@CD@9AEPQC[9jE:QE#7Y\;:^@E#SA<:
<V9AWX7B#MYCE)<V79{EJ7|MYC}@O>A@b_J<:	<V79;:gy~I7BU@GoCMUHA`R@c]C|>A7I_E#7BMp<RDSjE/9I@@b>{EJ7>A@b_J<>A@ยPSI@E#SI@BmE#7
C>AM/<V9A<:)E#@BยC[9jE)<VZA<V7E)<_:mE#7ยC|H;CBJE)<_\A`?C[BYH;CE^<R@9AEOย&B)<_gยยย7{CHIHA`VLย:)EC9;>ICB>{E#7A7`:7[Wย>A@b_J<:	<V79
E#SI@7BJLOF=:)@@ยF=ย\;_@/ยยยCe<uยKCIcยbยยAยNW*7BC9m<V9AE#B#7I>A\;_E)<V79NcAE#SI@%CD@9AE]M/\;:)EยC:J:	<VD9OHIB#7Z;CZA<`<VE)<V@b:c
7Bยย)ยย#ยยย(ย)ยTยJยbยยย*ย*ยcE#7/zCB)<V7\;:]@z@9AE:g4~I7B]@G;CMHA`V@cIE#SI@>A7ย_EJ7BMYCdL/9I@@b>OE#7C:J:	<VD9ยCU>A@DB#@@
7W]Z;@`<V@WE#7ยC[9ย@z@9jE%:)\;_JSC:k+ย&B)<_/S;C:ยSI@H;CE)<VE)<:#nIg/ยยก@P]7\A`>ยขE#SI@B#@W*7B#@ย`<V}@YEJ@b_#SI9A<ยฃ\I@b:W*7B
_7MHI\IE^<R9ID/>A@DB#@@b:s7WZ;@`<V@W<V9YCHIB)<V9;_J<VHA`V@b>mMYC9I9I@Bbc\;:	<V9ID%Cd``lE#SI@>ICECCE&S;C9;>gsยค	9E#SA<:sH;CH;@B
P]@<V9jz@b:^E)<VDjCE#@E#SI@HIBJ7H;@B#E^<R@b:t7W&79I@H;CB#E^<?_\A`CBtW*7B#MYCd`<:)MยฅWX7BQ>A7[<R9IDmE#SA<:g
ยQSI@%M@E#SI7I>mP]@T_79;:=<?>A@Bdc;PSA<_JSยขPย@%_Ce`u`&E#SI@Uย^ยฆ[ยงlยยยจ/ยฉ	ยช8ยยยซยยย/ยจยbยฌยฎยญAยbยcยฏS;C:]7B^<RD[<R9;:EJS;CED7
Z;C_J}EJ7ยฑยฐ4@B#9I7\A`u`<TC9;>{ยยCHA`C_@|FยdvยฒwANgยกยคE/<:/@b:J:)@9jE^<?Ce`u`VLyC9ยCHIHA`<_CE)<V79y7W(PS;C[ES;C:/Zยฏ@@9
ยณ

+,,6-ยทuq"$##Iยด	:ยท	:4ยตยฃ68ยถe8i8ยท17
#&	#ยธ	

ร%&##*ยน:ยธ

ยบยปยยผAยฝIยพยฏยฟ&ร]รIรbรยพยปIรยฟsรรรQยผoรbรdยพยป

รรdรรVรbรOร#รIรรยฏรรยฎรlร+รรรlรรยร)รรรรร ร(รร#รรรรยร=รรรAรIรbรรรกdรขรฃIรกรครฅsรฆ]รIรรง;รร	รจรtรจรAรbรยรจรรฉรชAรจVร#ร%ร)ร#รซรeรจRรฌรjร)รญ*รฎรซ#รฏรรซรรฅ
รฐ รชIรฑIรฑ;รฎAร)ร%รฏ]ร%รรซ#รรจVรAร#รรซ#รbร)รJรbรUรจRรยขรร#รรร#รAรจVรIรฌYร/รAรรฌรซ#รรยรฎรญ&รง;รรรจVรรญยร#รฎรยรญ*รฎรซ#รฒ/รชAรร/รณยรฌรจVรดรรยรยรตAรIรฎeรฏQรRรbรAรฌร
รง;รร)รยขรถรทรฅyรธรIรรนรช;ร)รรญ*รชAร(รฏรbรยกรฎรญยรร#ร	รจVรฌรAรจVรIรฌรบร^รรฒYรรAร)รจรร/ร#รฎรAรรฌรซJรรbรรฎรญรง;รรรจVรรญTรญ*รฎรซ#รฒ/รชAรรรยรจรรJรฎ|รช;ร)ร
รOรฑIรซJรฎรง;รรงAรจรรจRร)รรรจร)รJรซ)รจVรงIรชIร)รจVรฎร{รฎรดรรซ%รยร)รรยรฎรญ8รIรรปรปรรผรรUรฝ8รรยซรรรปรนร=รพรdรVรฑ;รรซ#รร]รกbรขรขรฟAรครฅ รนรฎรซ#รรรฎร;รรซ#รรJรรRรร
ร)รชIรฑIรฑยฏรฎjร)ร]รญ*รฎรซsรIรฎeรฏ{รJร;รรKรฏ]รรรซ#ร]รซ#รbรร)รฎรAรจVรIรฌ%รรง;รฎรชIร รรจVร;รรจRรดรจรAรช;รdรXรรยรก
ยรฅ
รฝ8รรรuรยรจร&รรรฎรฒUรฑAรRรรJร
รAรbร#รรซ)รจVรฑIร)รจVรฎรยกรฎรญ8รฏรAรจร#รยฑรจRร;รรจVรดรจรAรช;รdรร/ร;รdรดร%รbรรJรยขรฎรญ8ร#รIร/รฑIรซ#รฎรฑยฏรรซ#ร)รจVรbรรฎรญ&รจVรAร#รรซ#รbร)รbรฅ Iรฎรซ#รฒmรdรรRรรรUรฏ]รฎรซ)รร
รจร =รช;ร)ร/รOรฒรฎIรAรรร&รฎรซรจVรAร#รรซ#รฑIรซ#รรร[ร)รจVรฎรรsรฎeรดรรซ%รฎรชIรซ ;รซร)ร *รฎรซรAรรซรรรIรฌรช;รรฌรรฅ oรฎรซ%ร ;รรฒรฑAรVรรยรจรญ(รฎรชIรซยร?ร[ร
รฌรช;รรฌรQรรฎร;ร	รจร)รร&รฎ[รญlร#รIร]รชIร;รรซ#ร/รฑIรซ#รbรรจรรรJรbร ร)ร ยฎร ยฎรยฎรปร
jรรร*รรAร ร*รรรรร;รmรทร ยร รรlรร#รIร]รงAรจVร;รรซ#ร
รฑIรซ#รbรรจรรร#ร #รdรยซรร *รร รท ;รsรร;รยขร#รIรYรรฎร;ร)รรรAร 8รรXรAรรJรIรรรOรฏ]รฎรซ)รร|รAรbรJรรซ)รจVรง;รbรยรฏรAรจร#ร{ร)รชIรง;ร)รรยรฎรญร#รIร
รจVร;รรจVรดรจ?รAรช;รeร?รรJรร)รจร ;รbร4รbรรJรOรฎรญร#รIรรชIร;รรซ#รรฑIรซJรbรรจ?รร[ร#รbรรIรฏรAรจรJรยร)รร]รฎรญKรฑ;รdรจVรซร รจร4รจVรOร#รIร #รbรรร *รร รท
รซ#รรรร)รจVรฎรร]รร;รยรฏรAรจรJรยรฎรญร#รIร
รจRร;รรจVรดรจรAรช;รdรรmรจร 4รร*รรฅ รจVรดรรยรยฑรฑIรซ)รจVรฎรซรฑIรซ#รฎรง;ร[รงAรจuรรจVร)รยรรจร)ร#รซ)รจVรงIรชIร)รจVรฎร
รฎรดรรซร#รIรYร^รรรฎรญยรฑยฏรฎjร#ร	รจVรงAรVรOรฏ]รฎรซ)รรIรรรJรIรpร[รฌรรjร%รร[ร|รฎรงIรรdรจVรรรนรAรรฌรซ#รรรฎ[รญยรง;รรรจVรรญรจRรยรณyรฌรจVรดรร{รถรท รงjร
รรฎร;รรจVร)รจVรฎรAรจVรIรฌmรฎรยขรถรทยร#รฎ/รฎรงIรรdรจVรยรยรฑ;รฎAร)ร#รรซ)รจVรฎรซ(รรจร)รJรซ)รจVรงIรชIร)รจVรฎรรlร[ร;รร#รIรรยรรฎรฒรฑIรชIร)รจVรIรฌร#รIรรฑIรซ#รฎรง;รรงAรจรuรจVร)ร
รฎรญรณยรรรรฎรซรรจRรIรฌรนร#รฎmร#รAรจรรIรรฏ รรจร)ร#รซ)รจVรงIรชIร)รจVรฎรรฅmรฆ]รIร/รซรร;รAรฎรฒ *รฏยรฎรซ)รรIรรฒรร#รIรฎIรยรช;ร^รbรร#รIร/รฑIรซ)รจVร;รJรจRรฑAรVรOรฎรญ
รจVร;รรจ fรรซJรร;รรYร#รฎOรJรIรฎjรฎjร^รpรUรฑ;รรซ#ร^รจ?รรชAรรรซยรฑIรซ)รจVรฎรซ%รรจร)ร#รซ)รจVรงIรชIร)รจVรฎรยกรฎeรดรรซ(ร#รIรร)รรรฎรญ]รฏยรฎรซ)รรIร (รdรร&ร#รIร/รฏ]รฎรซ)รรIร
รรซ#ร/รร[รตรรยขร#รฎYรงยฏรรbรฉรช;รdรรRรรนรรจVรตรรVรรฅ 	รรจ?รยรbรร)รรนร#รฎยร)รร/ร#ร;รรร#รIรรAรรฌรซJรรTรฎ[รญยรง;รรรจVรรญรจVรรณ รฌรจVรดรร{รถรท รจ?ร
ร#รIรรรนรฑIรซ#รbรJรจร)รรVรยร#รIรรญ*รซรรร)รจVรฎรOรฎ[รญ!รฏ]รฎรซ)รรIรรJรร)รจร	รญ*รรจVรIรฌOรถรท ร#ร;รรรdรร)รฎOร#รร^รจ?ร=รญXรOรณ]รฅ
รฆQรIรpร[รฑIรฑIรซ#รฎjรรJรร)รฎmรญรรซ%รAรbร#รรซ)รจVรงยฏรbร{รรฑIรฑAรรจRรbรยรฏรIรรIรรดรรซ%รฏ]รรรร#รช;รdรรVร|รตAรIรฎeรฏ รJรIรรฑIรซ#รbรJรจร)รOรAรฎรฒYรdรจVร
ร	รจ ร
jรชIรAรญ*รฎรซ#รJรชIร;รร#รรVรOร#รAรจร]รจรtรญรdรจVรซ)รVรOรชIร;รรฎรฒรฒรฎรรฅ 	รยรฒYรรAรยรรร)รbร+รIรIรฎรฏยรรดรรซbรAรจVร]รจรรซ#รbรร)รฎร;รรงAรVร%ร#รฎ
รง;รรรจVรรดรยร#ร;รร ยฅรจร Jรรรซ#รฌร Iรฅ |รรรซ#รtร#รjรช;ร รฑ;รรซ#ร)รจรรชAรรรซ)รVรUรจVรjร#รรซJรbร)ร#รbรpรจRรOรJรIร(รร)รAรฒรฑIร#รฎร^รจ?รQรง;รร;รdรดรจVรฎรซ]รฎรญ
ร#รAรจร รญXรซรรร)รจVรฎร Aร#ร;รรsรจรรAรฏ]ร(รรรตร]รฎรชIรซรAรรฌรซ#รรรฎรญรง;รรรจVรรญ!รJรฎ%รง;รร#รIรรร)รAรฒรฑIร#รฎร^รจ?รQรดeรdรVรชIรรฎรญร#รAรจร รญXรซรรร)รจVรฎร
รร
รฌรซ#รฎรฏ(ร รรรซ#รฌรรฅ
oรฎรซร ;รรฒรฑAรVรรร)รชIรฑIรฑยฏรฎjร)ร%รฏ]รยรฏQร[รjรรJรฎรซ#รbรร)รฎรยรรงยฏรฎรชIรรmรAรฎรฒYรeรจRรรนรฎรญ8รIรฎjร)รฑAรจVรรeรKรฑ;รร)รจVรรAรรรรร;รยขรถรท
รจรรJรIร%รรฎร =รชIร;รร^รจRรฎรยรฎรญ&ร#รIรรญ*รฎรรRรฎรฏ]รจVรIรฌUรญXรฎรชIรซ]รญXรฎรซ#รฒ/รชAร?รร


 
   "!
.(
1( 32



,+



 "- *
/

0

4

65

;: =<
C

9

 ?>
<


	
 

 
$#&%
'! )(*



+

 "- *

87

9

@ BA




"
87
DFEHGร ร)รยฎรยฎรยฎรปรIGรค&JK"!Aรlร[รXรรร	Gรค#รค]รL>รeรuรsรฑ;รร)รจVรรjรร]รฏ]รจVร#รOรIรรฑ;รร)รจVร)รจรรIรAรจVรงAรจRรM#รรชIร;รรจรร@Aรคร
DON ร^รยฎ รPยฎ รยฎรปeร	G รค
Q'"j! รรร*รรรIG รค
NRS รฟ'T รL+> รรฑIรฑIรซ#รฎj รจVรฒYร[ร#รรVรUT รฟWy
V รฎรญ;รฑ;รร^รจRรรAรรsร#ร;ร[รKรI รAรจVรงAรจRรB#รรชIร
รรจ?รรร;รdรดรรIรรฑ;รร^รจRร^รจ?รX@I
< รฏ]รรI รฑAร?รeรจRรOรJรAรจ?ร รญ*รฎรซ#รฒYรeรuรจร)รฒ รร;รpร#รIร(รซJรbรร)รฎรรฏ]ร(รJรbร0> ร[รฑIรฑIรซ#รฎj รจVรฒYรรJรรRร
T รฟWVUย
@ รซรรJรIรรซยรJร;รรY> ร; รรร)รVร1T รฟWVUย@ รจVร รฐ รbรร^รจRรฎรยรฃAรคร
DOQ;ยQ รทร'ย! ร)(* รรoรIG รค8Q;Q R S รฟZร รฃU
[ รL> รรฑIรฑIรซJรฎA รจVรฒYรร#รรVรOรฃ[WV รฎ[รญKรฑ;รร)รจVรรAรรร;รdรดรรงAรVรชIร/รรรbรA@ รคร
D3"A! รlร[รXรรรX8( รรXรรค]\^#&% ร*รuรยฏร4( รร*รรคร8>8&_ รซ)รจรtรจรรร#รAรจรรรนรฏรIรฎรI รAรจVรงAรจRรร&# รรชIร;รรจรรA@ รครฅ
` รร8รณ รง;รa ร^รยฎ รPยฎ รยฎรปeรX8( รรXรรค< ร#ร;รรยรจ?ร+รรฏยร4รฏรรjรยร#รฎรร#รรซ)รจVรง;รรรAรรฌรซ#รร]รฎรญ;รง;รรรจVรรญร#รฎร#รIรยร)รรร#รรฒรรAรb>)&_ รซ)รจร
ร;รร]รIรรฑ;ร[ร)รจVร)รจรI
@ รฅ รฐ รชIรฑIรฑ;รฎAร)ร/ร#รIรTรAรฎรฒmรdรจVรยร;รรร=รจc: รย
 รฅ8รฆ]รIรรรนรฏ]ร%รฏรรAรยรJรฎYรรฎร;ร	รจรAรรซ%รdรรKรฏ]รฎรซ)รรIรtรฏ]รจVร#ร
รAรฎรฒYรeรจRรFj
d รก	^%e ร)รช;รJรยรJร;รรร#รIร%ร)รรtรฎรญKรจVร;รรจVรดรจ?รAรช;รeรXร%ร#ร[ร)รจร	รญXรรจVรIรฌ0 ร)รยฎ รPยฎ รรปรจร(รmร)รชIรง;ร)รรtรฎรญ!รJรIรฎjร)ร
ร#รร^รจ?ร=รญXรรจVรIรฌ^"j
! รรร*รรAรรรฑIรฑIรซ#รฎA รจRรฒmรร#รรVรfT รฟV รฎรญยร#รIรยรจVร;รรจVรดรจรAรช;รdรรรJรร)รจร	รญ*รรจVรIรฌ^"A! รlรร*รรYรdรร)รฎยร#ร[ร)รจร	รญXร
 ร)รยฎ รยฎ รยฎรปรรรฑIรฑIรซ#รฎj รจVรฒYร[ร#รรVรยรฃ[WV รฎรญ]ร#รIรรจVร;รรจVรดรจรAรช;รdร*รTรJรร)รจร	รญ*ร|รทร'ย! ร)(* รรรรร;รยกรรJรIร%รจVรAร#รรซ#รฑIรซ#รรรร)รจVรฎร

รฎรญbรค]8
( รรXร รจร&รรรจVร;รรจVรดรจรAรช;รdรร#รร)รจร	รญ*รรจRรIรฌ."A! รlรร*รรรร;ร1#&% รXรรรฅB9 รยรจ?ร&ร^ร#รซรdรจVรฌรAร)รญ*รฎรซ#รฏรรซรtร#รฎ(ร)รIรฎรฏยร#ร;รรbรdรร
รI
 รฑ;รbรร#รbรร] ร^รยฎ รPยฎ รยฎรปeรX8( รรXรรค8รIรฎรรIรQรจRรยขรรฑIรฑIรซ#รฎA รจRรฒmรร#รรVร0T รฟWV รฎรญ&ร#รIรbร)ร/ร)ร#รซ#รช;รร#รชIรซJรbรรฅรนรฎรซ#รรฎรดรรซbรIรร
รฌรรรยรรรซJรฌรรร#รIร4รญ*รซรรร)รจVรฎรUรฎรญfร)รJรซ#รช;รร#รชIรซ#รbรยรจVรรฏรAรจรJร0
 ร)รยฎ รPยฎ รรปร4( รร*รรครIรฎรรIร]รรฎรAรดรรซ#รฌรbรsร#รฎร; รรร)รVรรฟgIT รฅ
รฐ รจVร;รรh
T รฟWย
V รฎรญ;ร#รIร]รฑ;รร^รจRรรAรรยร#ร;รรsรย รAรจVรงAรจVร# รรชIร;รรจรรร;รdรดร4รIรรฑ;รร)รจVร)รจร&รร;ร.&_ รซ)รจร4รI รAรจRรงAรจVรรi^ ร[รชIร;รรจ?รรร
ร%รAรรฌรซ#รรรฎรญรง;รรรจVรรญ!รฎรญKรฟIรฅ'
T ร#ร;รร&_ รซ^รจ?รQร;รร&รIรรฑ;รร^รจRร^รจ?ร]ร)รรรฒmร$) รช;ร)ร)รจ6l รรงAรVรรฅktj รฎร#ร]ร#ร;ร[รbรdรจVรYร#รAรจร รo รรฒUรฑAรRรร
ร#รIร/รจVรAรญ*รฎรซ#รฒYร[ร)รจVรฎรยขร#ร;รรl&
_ รซ^รจ?รpรจร%รOรJรAรจร?รยฑรจรรbร#ร^รรjร)รจรdรรVร|ร#รซJรbรร#รbร|รรtรจVรซ#รซJรรRรรดรรAรbรฅ|
A รUรฏยรฎรชAร?รยฑรฌรรร#รIร
ร#รรฒUรยรร;ร^รฏยรรซยรจรญรฏ]รOรรจ?รยรIรฎรTร;รdรดรUร#รIรmรจVรAรญXรฎรซJรฒYรร)รจVรฎรm#&% ร*รร;รX8
( รรXรรครฅn9 รรรร{รdรร)รฎยขรง;รยร)รIรฎรฏรยร#ร;รร
o8p

qrstHuZvxwyuz{tH|arst=}~rยยvยvยยBsยzuZย"ย
ยยย0ยWย8ยยXย8ย0ย"ย?ยยยย6ย;ยยlย;ยOยCย'ยย8ยยยย$ยยMย
ยPย"ยUยกยยWยขย8ยยยยฃlยXยยฅยคยฆgยงยจnยฉยฃ4ยชยซยย8ย
ยฃยยฌยฉ"ยยยยญ3ยฎยฏยยยย8ยXยฐยยยยฑ
ยยย1ยWย8ยยย8ยยฒย"ยhยยณยยยดย;ยยlย"ยยbย'ยย8ยMยยตยยยณยยยยPย"ย&ยถ^ยทยธ"ยยบยน$ย"ยPย
ยWยยMย
ยPย"ย?ยกยยยบยขย8ยยยยฃยยfยคZยฆgยงยฑBยยยยฒยปยยยWยฏยยกย.ย"ย
ยคยฆgยผยฒยฉ"ยยยnยคยฆgยงยจยญCยฝ?ยฃaยพยUยฃ	ยยยฉย6ยkยฃ	ย8ยยฑ$ยXยWยยยฃยยฌยฃaยยณยยก8ยฉ"ยฏยยฃ	ยยยย.ย
ยฉยตยยยWยยฐยฟPยพยย	ยยฌยยฃยฐย8ยยยย0ยXยยยฉ"ย
ยฃยCยgยย8ยMยย
ย
ยฉ"ยยย3ยทยธ"ยWยนHย"ยPย
ย,ยฉยฃยยยย;ยย^ย;ยยยWย8ยปยณย8ยยยWย8ยยบยยฑยพยWยยยกXย=ยยฌยฃ.ยยยฉยฃรยยยยฉ"ยWย;ย1ยยยยก8ยฉ"ยฏยยฃรย0ยยย8ยย4ยยฌยฃ.ยยnย8ยขยยฌยWย8ยยยกยnยย
ยยย.ยกยยยบยXย
ยฉ"ยรยญยรรรยaยพยยฏWยยฌยfยฃรยฏยยย;ร0ยยยยฃ	ยXย
ยฉ"ยยยยยยปยยWยฃ	ยยฏWยยฌยฉ"ยย.ยยยยฉ"ยaย	ยพยยปยยยปยณย8ยย	ย;ยยฃaยพย8ยยUยกยยXยยยยฌยฉ"ยยย
ยฏยWย;ยยฃยฃยยย8ยยlยพย8ยย?ยXยยฉยฃ	ยยยฒยยยยณยยยดย;ย8ยขย.ยยย8รยฒยพย8ยยlยกยยยยยกยยยย;ยรยฃรยยฐยaยพbยฉรยญ6ย
ร ยยบยฏยยฃ8ยฑMยฉ"ย.ย;ยยฉยฃ	ย.ย;ยyยยWยยฌยฃ4ยรZยฉยตยฐยปWย;ยยฑยยย1ยยฉ"ยยยWยยฐยฟPยพยย	ยยฌยยฃ.ยฐ4ย8ยยยย=ยยตยcยขยยฃยฉ"ยยยฃ	ยพย8ย
ยฃ.ยXยยยฉ"ยยย"ย6ย;ย"ยพ
ยPยยยฐรยยยยย8ยฏย	ยยฌยฃ	ย	ยยฌยกยฉยฃยฃ	ยฏยฐยปย	ย;ยยยยฃยฐ~ยฉยWยย;ย^ยฐ~ยฉ"ยWร0ยฃ	ย
ยฉยตยยยยฉ"ย
ย,ยฝhร?ยฃ	รยฃ	ยย8ยฐ~ยฃยIรBยยฉ"ยรยรยฑ&รรยผรรรBย"ย6ย;ยยกร$ยฑ
รรยผยตรยรWรยปWย;ย8ยยย;ยยยฉย;ยย8ยยฑ$รรยผรย
ยญยฝaยยCยฃ	ยฏยยกXย.ย;ยยบยยฏWย;ย	ย;ยขยยยยฃ	ยฏWย;ย
ยฃ&ย	รWยปWยยฌยก8ยฉยPรรรยย8ยยWย?ยพยยย8ยยกยยWยขย8ยยย8ยยยกย"ร
ยฝaยยย1ยพCยย8ย1ยพยlยWยยยฑWยยฌยฃยยย8ยยlยฉ.ยปยยฉยกย	ยยฌยก8ยฉยรยพCยฉรยย~ยกยยฐยปยฏยยlยWย8ยยย8ยยฃย"ยรยยณยย6ยcยยร
ร ยยfยฉ"ยยยฃ	ยพย8ยยฒยย=ยยย,รยย
ยฃ	ย1รยฏยยฃ	ยรยcยยรยยฌยฃยฒรยยฃ)ยฑhยฉยฃ4ยพยfยยยฌยฃยกยฏยยฃยฃยcยOยWย8ย
ยฉรยยดย?ย;ยmยIรยฉยก8ยกXยยบยฏยยฃ)ยฑ?รhยXยรยขยยฑ
ร ยฉย;ยปยย8ยยยฑ&รรรaย"ย6ยcย8ยยฑkรรร"รย
ยญ.ร/ย^ยยยยฉ"ยยปยยฉ"ยปยย8ยยฑยพย~ยฃ	ยย"ยพยยยยยฉ"ยยยยย
ยฉยตยยยWยยฐยฟPยพยย	ยยฌยยฃaยฐย8ยยยย,ยยฌยฃยย8ยฟ
ยฐ~ยฉ"ยXรรยฉ"ยWย;รfยฃรยฏยยก8ยกยยฃยฃ/ยPยฏWยรยฉ"ย?ยฃยฉ"ยรยยยฃIยรย;ยย~ยยยยWยยฃ/ยยฌยWย8ย
ยฉยตย
ยฉยยตยยยยยย,ยยยยฐยยยยยยWยยฌยก4ยIยWยยรยฉยตยฏWยcย)ยkยยยฉยฃ	ยยWย;ยย
ยรยcยยยฃรยยย8ยยยยฑaรรยผรยรยฉ"ยยยFยยยPย8ยย8ยยยกย0ยกXยยฌยฉยฃยฃ.ยXยยฉยฃ	ยยWย;ยยyยIรaรWยยฏยยยยฑรรยผรกWยยญ ร ยย1ยXยยฃ	ยฏWย;ย
ยฃ.ย"ยlย	รkยฉยก8ยกยWยฏยยฃ
ย8ยยฉยรยญ;ยฑCรรร"รยhยฃ	ยย"ยพรขยยยยฉ"ยยยย~ยยณย8ยยยฉยขย;ยยยพย1ยฃยฉยพยย;ยyยยยยฒยรZยฉยตยฐยปWย;ย0ยฉ"ยยย"ยขยยยยตยยยยฃ4รยฏWย;ยย1ยย8ยย8ยยฉย6ยcรยฑ
ยฉยฃยWย1ยฐ~ยฉ"ยWรfยยยย8ยlยปยยยปยย8ยย	ย;ยยฃยพkย4ยพkยยฏWยยยรยยยปยยยฒยย1ยยยฉยขย~ยฃยฉยตย	ยยฌยฃ/รยยย$ยญ ร ยWยฏยยฃ8ยฑ]ย;ยยฅยXยWยยยฃยปยยฉ"ยปยย8ยยพย~ยWย
ยยยCยฃ	ยปยณย8ยยย1ย	ย;ยฐยMรฃIยฏยยฃรย	ย6ยรย;ยย~ยยยaย
ยฉ"ยยยWยยฐยฟPยพยย	ยยฌยยฃยฉ"ยปยปยยยบยฉยกXยยฑWยยยCยWยยพยรยยยฌยฃยกยฏยยฃXยฃMย;ย
ยฃCยฃ	ยยXย8ยยยยยยฃยฉ"ยยย
ยพยยฉ"รWยยยฃยฃ	ยยฃ8รaยยยnยยยฉยWย8ย4ยยฌยฃยฒยยยPย8ยยยย3ยย3ยIรยฉยก8ยกยWยฏยยฃ4ย8ยยฉยรยญ;ยฑรรร"รZย?ยยยยฃ	ยฏยยกXยOยยยฌยฃยกยฏยยฃยฃ/ย;ยยxยฉยตยยยyยPยย
ยฉ"ย=ยรZยฉยตยฐ.ย;ยยยฉ"ย	ย;ยยyย"ยCยปยย8ยขย;ยยฏยยฃ.ยพยยร,ย;ยyยยย0ยฃ	ยปWย;ย	ย;ยย"ยCย
ยฉยตยยยWยยฐรคยพkยย	ยยฌยยฃย/ยฐยยบยฃ	ยยยย
ยฉ"ยWย;รyยรฅยฉ"ยยยยฉ"ยปยฑ
รรยจยคยฑรรยจยงWยlยฉ"ยยย3ยฃ	ยฏยยยฃ	ยรยฏย8ยยบยยฒยพยยรยณย
ยญรรฆCยฉยตยยย8ยยฑยพยยฒยยยกยฏยยฃ4ยยรยยยยฒยยฌยฉ"ยXยย8ย~ย	ยพย^รยฏยยฃรย	ย;ยยยยฃ~ยฉยฃ	รยย
ยฉ"ยยณยรยขยยญ ร ยยยฃ	ย~รยฏยยฃ	ย	ย;ยยยยฃยฐ~ยฉรfยฃรย8ย8ยฐรงรยฏWยcยXย.ยรยฉยตยฐ.ย6ยยดยยฌยฉ"ย.ยย1ยXยยฉยWย8ย
ยฃlยฉยพCยฉ"ยยย"ยCยยยยพยยร1ยยยฅยฉยฃ	รWยฐยปยฟ
ยยยรยยยก4ยปยยยยยฉยตยWยยดย6ย;ย	ย;ยยฃรจยยย.ยข"ยฉ"ย	ย;ยยฏยยฃย;ยย"ยยฌยก8ยฃ8ยญ^ยฎยยUยรยยฉ"ยฐยปWย;ยยฑBย;ยyยยย1ยกยยWยยรยUยยตยkรยยยฃ	ยยฟPยย
ยWย8ยยPยยยฐ.ยฏWยยฌยฉยฃ8ยฑ
ย;ยยยยฃยพยย6ย;ยฟPรยบยย"ยพCยyยยยยฉยตยUยฉยฒยPยยยฐ.ยฏWยยฌยฉ,ยพย;ยย=ยยfยกยยยยฃ	ยยฉ"ยยบยยย?ยPยฏยยยกย	ย;ยยรยฃ	รWยฐ.ยยย"ยยฌยฃยยยฉยฃ.ยฉ"ยยฅยฉยฃ	รยบยฐ4ยปยยย	ยยฌยก
ยปยยยยยฉยตยWยยดย6ย;ย	รยฅยยตยhยย;ยยย8ยยค0ยย~ร,ยIยฎยยฉยตย"ย;ยยฑkรรWรรรรรย;ย8ยยยฃ	รยgรฉรชรยฑรaยยยบยฉ"ยยฑ&รซยณย;ยยยยรฌรญรยgรฉรชรยฑร ร ยฉยยฌยฉ"ยย"ยขHยฑรรรรWยยญ
ยฎยฏยXยยย8ยยฐยยXยยฑ&ยพย0ยก8ยฉ"ยรยWยยกXยยฌยWยfยพยWยยยกXยOยXรhย
ยฉ"ยยยรฃ	ยยฉ"ยยฑaรรยผรกย
ยญ ร ย"ยพkย8ยขย8ยยฑ&ยยย0ยค"ยฟร4ยยฌยฉยพรขยรยฉย6ยยยฃรจย6ยlยยย
ยยฌยฉ"ยยยฏยยฉ"ยยaยcยยยกXย;ยฏยยWยยฃaยกยยยยฃ	ย
ยฉยตยยบย
ยฃMยยMย6ย&ยพkยย;ยยบยร~ยฉ"ยCยกยยยยย;ย	ย;ยยยยฉยรยปยXยยยยฉ"ยWย6ย6ยcยรยcยยฃย	ยฎยยฉ"ย"ย;ยยฑ$รรWร"รWย
ยฑยบยฉยตยยยยพย
ยย8ยยnยยณยยย^ยยยยฃรย.ยยยฉยตยยฏยยยฃย;ยยฅยย
ยWย8ย?ยย1ยยยฉยฃรยย^ยฉ"ยยยยฏย.ยWย8ยยย8ยยฃย"ยkยยณยยยดย;ยยaยยยaยยยXยฐUยฏWยยฌยฉยฃย;ยWยขย"ย;ยขยcยย
ยปยยฉ"ยยรยยยกยฏWยยฌยฉ"ยย;ยยยย;ยขยยฌยWยฏยยฉยยฌยฃ8ยฑBยกยยยยย;ย	ย;ยยยย,ยย1ยพCยยยฉ"ยยยฌยฃรWยย"ยพCยยญ
ร/ยยฅย	ยพยfยกยยฐยปยยฉ"ยWย;ยยyยปยยฉ"ยปยณย8ย
ยฃ4ยรhยย"ยขยยฑ ร ยฉย;ยปยย8ยยยฑรรรaย"ย6ยcย8ยยฑhรรรรกยฉยฑ&รรรรก"ย$ย
ยฑBยพยยกยยยยฃ/ยยฌยWย8ยยยย
รยฏยยฃ	ย	ย;ยย1ย"ยยพCยยยฉ"ยMยยยฉ"ยปยปยย8ยยยฃkยcย1ยXยยhยปยฏยยรยย
ยฃ	ยXยฟPยย
ยWย8ยCยก8ยฉยฃ	ยยรยพCยย8ยยยยย8ยยยยยฃbยยรจยฃรย
ยฉ"ย	ยยฌยฃ	ย	ยยฌยก8ยฉยยณย;ยWยยยXยฐ~ยฉ"ยฟ
ย	ย;ยย$ย$ย;ยยยXยยฉ"ยย8ย&ยWย8ย
ยฉย6ยรยญ&ร^ยhยฃ	ยย"ยพ=ยยยยฉ"ยยฉยฃiย;ยยยUยฉยฃBยยย8ยยMยยฌยฃยฉ"ย]ย;ยยฉยฃ	ย&ยยยCยWย;ยยยฉ"ยXรUยปยXยยยยยก8ยฉยตยยhยฃ	รWยฐ.ยยย"ย
ย;ย~ยXยยยยฌยฉ"ยยยฏยยฉ"ยยยฑWยยย8ย4ยยยยยWย;ร~ยWยยพยCยยยยย8ยยฉยฃ	รWยฐยปยยย	ยยฌยกCยกยยยยยcยรยcยยยยฉยยปยยยยยฉ"ยWย6ย6ย;ย	ย;ยยฃMย;ย~ยย8ยย8ยยฉย
ยIยฉยฃBยพCยฉยฃMยฉย;ยยยฉยWรยฃ	ยย"ยพCยยWรยฎยยฉ"ยยตยcยรฎย
รรWรรรยย
ยฑยยฏยkยฉย;ยฐยยบยฃรยkยฉย6ยHยXยยhรยฏยยฃรย	ย;ยยยยฃMยยยCยฐ.ย;ยยWยยพCยฉ"ยWย&ยยlยฉยฃ	ร
ยIยฃ	ยฏยยกXย1ยฉยฃยWยยกXยยฌยย;ยย~ยพยย8ยยย8ยยยยCย6ย;ยฐ.ย;ย	ย;ยยยฒยปยยยยยฉ"ยWย6ย6ย;ยIรยฒยรWยยยฃรย
ยฃยรฏยฉยตยยhยWย;ยยWย;รยฒยฏยยยWยยกXยยฌยยฉ"ยWย;ยยญ ร ย"ยพย8ยขย8ยยฑ
ย6ยรยพยยฒยยยฃ	ยยรยยยกย.ยยnยฉ,ยขยยก8ยฉยตยยฏWยยยฉยตยรยฅยพbยcยXยyยยWย;ร^ยฏยยยฉ"ยร=ยปยยยยยฌยก8ยฉ"ยย0ยฃ	รWยฐ.ยยย"ยยฌยฃ.ยฉ"ยยยyยกยยยยฃรย
ยฉ"ยWย
ยฃ8ยฑBยยย8ยyยฉยฃ
ย;ยยย0ยฉยฃaยยยรจยยยXยฐUยฏWยยฌยฉ4ยยnยพCยWยยฌยกยFยพkยยฉ"ยXยรจยกยยยยยcยรยcยยWยcยย,ยยฌยฃlยฃXยฉ"ย	ยยฌยฃ/รHยฉ"ยWย;ย.ย;ยยฅยฉ"ยXยWยcยXย
ยฉ"ย	ย6ย;ร,ยยยฉยตยยย.ยฐยยWยยยยฃ
ยIยฉ,รยฏยยฃ	ยรยcยย^ยพCยWยยฌยกยFยยฌยฃ.ยWยยกXยยยยฉยตยWยcยยฒย;ย=ยยยยฒยฏยยยฉ"ยร^ยก8ยฉยฃ	ย"ย
ยฑ]ยยย~ยฉยฃรรยบยฐยปยXยย	ยยฌยกยกยยยยย;ย	ย;ยยยยฉยCยปยยยยยฉ"ยWย6ยยดย;ย	ร
ยรWยยยฃรย
ยฃhยฉยตยยย1ยก8ยฉ"ย1ยยย.ยกยยฐยปยฏยยยรฎยรฐiยยกยรยcยขยยcรยญ
ร/ยyยยWยยฌยฃ.ยปยยฉ"ยปยณย8ยยฑยพย1ยกยยยยฃ/ยยฌยWย8ยยฒยยย~ยฐ.ยฏยยกXยyยฐยยยยฒยฏยยฃ	ยยPยฏWยรยก8ยฉยฃ	ย1ยพยย8ยย1ยยยยฒรยบยย"ยพย;ยยWยย1ยยยฉยฃรย1ยยยฉยฃ
ยฃ	ย
ยฉยตย	ยยฌยฃ	ย	ยยฌยก8ยฉยยฉยฃยพยย6ยkยฉยฃรยยยฃ	ยยฟPยย
ยWย8ยbยcยWยPยยยฐยฒยฉ"ย	ย;ยยยญaรรยรฎยยดย;ยยWย?ย"ยยยยยยยฃ	ยฏWย;ย
ยฃaย"ยCยXรhยย"ยขย?ย8ย?ยฉยรยญ;ยฑBรรรรกยฉZยฑ
รรรรก"ย$ย
ยฑยยยkยฐ4ยยบยฃ	ยCย"ย&ยยย?ยปยยฉ"ยปยณย8ยhยพย?ยยยฃรยย	ยยฌยกยhยฉ"ยยXย8ยยบย	ย;ยย1ยXยUยXยยUยก8ยฉยฃ	ย?ยพยย8ย0ยยย?รWยย"ยพย;ยยWยยlยยยฉยฃรยaยยยฃ
ยรยปยยยฃยฃ	ยยรฎย;ย^ยฉยฒยฏยยยฉ"ยรยฒยยฌยฉ"ยยยฏยยฉ"ยยยญlรฑCยฏย?ยฐยฒยฉ"รฃ	ยยCยยยฃ	ยฏWย;ยaย;ยWยขย"ย;ยขยยฃaยฃ	ยย"ยพย;ยย1ยยยยฉ"ย?ยฉยฃ	รWยฐยปยยย	ยยฌยกยกยยยยย;ยฟ
ย	ย;ยยยยฉยยยปยXยยยยฉ"ยWย6ย6ยcยรยcยยฃยก8ยฉ"ย4ย"ยยXย8ยUยยณยhยกยยฐยปยฏยXยยlยฏยยฃIยcยยยฒรฒรดรณยบยMรตยยยยฌยน$ย8ย'รตHย6ยhรถรรทCรธยธ"รนยรดรธ.ยยบรธรขย8ยนHรฒรดยรถรตยณยยIรบยฉรWยยยฃ8ยฑ
รรยจรยบรยณรยยยฉ"ยยยยfรmรFยยฉยขย8ยยฑ$รร"รยบรWยยญ
รปรผ

รฝCรพรฟ	
รพbรฟ

รพ
 "!$#"%&'#)(*!+,&-/.0%1324.5.6 #7789:	!+883!$;#)(=<?> %A@BC%A79D>E!F2G!+#"!H&-I#7(
Q[Z\^]&_5`
%&79587J2G> %&#)/!+9.5@&BG!LKNM&OPPP+OEKQRNSTU> %&V>E!W2C 	#7(YX
#)(%&#a> %&b@	-cJ7.bd-^7.
#7(!+587J2G> %&#)V!+9.e@&BG! :f%&.bEB49:f#7(d-^7.5BG%!g&-h#7(d-^7.iKYMg
j k PPP k KYQ j :flm( 75%>)(nKYo j 2C!
E24#7( UK o JUpNK o RSI> %&=<24 lq#)(Ur&l	B4sI@%!+I%!L8BG%>)24st>E!$#7"%24#"!LV#)(I878J7#+24
&-u.0%24bEB4 .b 3#E!!7%&#+2G!F-^924sI%>)(b%&#7.,RNvJNE1%&.b8B4:#)(h>E!+#)"%243#UwxKNMxyWzf{ |}KN~xyWz{ยwEยdยยย&ยยX
!7%9!#7(%&##)(m8787#+240&-#7(U.0%24t!7%A#+2G!F-c9J24se!+J.bU%&#7J.ย#7(%&#a>EJ3#"%ย2ย!	K~m%!L%[>E&ย+>E#
2G!L#Wlย2C>EI#)(U87J87#$2ยJt!7%A#+2G!F-c9J24sb%&#7.V!ย#7(%&#	>E3#E%24V@#7(=K M %&0K ~ %!a>EJ&ยW>E#"!xR	ยm24< =%
.bEB&-ยIยe:l	/> %&0EยI#7(U #77J8395&-#7(2G!L.bEB%J!#7(Y 3#)789eA-#)(U<>E#)h #$2ยs
#7(b8)8)#+24!I&-m#7(0J2ยย 7 #e%A#7.V! R5STV!+(&lย#7(%A#:%![ยยsJ7&lU!	BG%&)s:#7( 7V%&)e.0%&39
.b)*.bEBG!=l	24#7(ย(24s(q #7789'#7(%&ยl	24#7(ยB4&l	 V #7789Rย	( 7E-^7:g.b?EBG!ยl	24#7(ย(2ยsJ(
 #7789b.524%&#7RSY!+Y#7(2G!Uย ]&ย ย"ย ยu\ยย7Z\ยย^]&ย[ยย ย ยf]&_ ย ยu]Aย #75!+(&lD#7(%A#a	 s7 Y&-@EBย24E24*ยกsA2ย<J ยขยIยยฃ%> >EEJ2ยs5#75#7(IE%&.bยค^l	+BG!ย.b #7(H2C!g>)Bย!+EB49V7EBG%&#)0#75#7(/%J!7!F24s.b #
&-N8)8)#+24!	#7b%&#7J.V!L#7(%&#	(%!	.V%ย1324.5.ยฅ #7789V%&.bJsยฆ%ยBยB%!7!F24s.b #"!m>EJ!F2G!+#7 #ml	24#7(
#7([>E!$#7"%24#"!L24.b8!+=@9*ยIยbR
ย(e>E>E #7"%&#$2ยJ*8( .b J;)EBC%A#+24sV #77890#7V#)(["%&J.bยค^l	+BG!	.b #7(02G!Yl	EBยB4ยค
rยlgยyWยง%93!x:mยยจยฉX:hยยจยฉยช{"RTยซยฌ'8(9?!W2C> !x:	#7(ยฎยญEl	+BG!7ยฏ%A7V#)(=83!7!W2ย@B4>EยsE%&#+24!5&%ยข!+9!+#7 .ยฐ#+9382G> %BยB49D>E!F2G!+#+24sA-U.V%&98%&7#$2C>)B4!5Je.H&B4>EBย!x:h%&ยฑ#)(V.5#7%BยB49ยฑE1>)B4!F24<
878 7#+24!Hyยฌ5%&#7.V!ย{U> %&@:ยฒ-cIE1%&.b8B4:aยณJ%&#7.ยด!$#"%&#7! R=	(0>E77!+8J24s #7789
.b%!$7m2G!m%A#h#)(U(%&)#hA-ยฒ!+#"%&#$2C!$#+2G> %B.b>7(%A2C> !g%&0#7( 7.b9%&.52G> ! Ra	( )[%&7I!+@#+B4I@#
24.b8J7#"%&#ยฒJ2ยย 7 >E!ย@ #+l	  eJN<J24 lm8&24#h%&d#7(%A#N&-#)(h8(9!F2G>)2C!$#"! R	(	.V%245LBย2ย!ยฒ24
>7(A2C>Eg&-BG%&s%&sRS	lm%&#N#)UE187!7!!+.bL24#7EBยBย24s #h%As 3#ยตยถ!r&l	B4s:&lm(2G>)(52G!lm(39
l	a#"%Arย"!$#7ยค^" B4s&2G>	%!Ja!+#"%&7#$2ยsg8&24#R	(	.b3!+#!+8>)2ยยu>mJ2ยย 7 >EU>EJ>E 7!>E!+#"%A3#
!+9.5@&BG! RYSd *#)(!+5@> %&!+5#7(d.b3!+#g243#) 7!+#+24s*ยณJ!+#+24!g-^m!I%&+2G!+dlm( ยขl	[(%<
!+.Her&l	B4sV%&@#+ยทยธ%&Tl	2G!+(#)*%!7!F24s s7 !ยน&-m@EBย24E-U#7*!+#E%&#7 .b #"!I>E>E 724s&ยทยธ%
8%&7#$2C>EBG%&g24J24<2G%BWRI	(d8%&"%BยB4EB2ยยข8(9?!W2C> !gl	BG*%)!7!m8)8 7#$2ย!YA-ย%b!W2ยsABย58%&)#+2G>)Bย:
lm(2G>)(=2G!msJ  "%BยB49*>E!F2G 7,#75@dlhEBยBNJ#"!F2G[#7([!)>E8d&-ย!$#"%&#+2G!+#+2G> %B.b>)(%&2G> ! R
ยบ #7( glhJ7r=#7(%&#mE1%&.524!g#7(b>E>E#+24ยข@ #+l	  *E%&.ยฅl	+BG!Y%&= #77J8390-^7.
	8A2ย#	&-N<J24 l	ยทยธ>E.b8#+24s= s7 !L&-N@EBย2ยE--^ย-c).eBG%!ย24*%I8%A7#+2G>EBG%&	B4s&2G>)ยท'2G!m#)(%&#a&ยป%&+2G!%&ยฆยผ >Eย<!+r&%5y"ยยจJยฉยจ{"R	( 957!+#7+2G>E##7(gr3&l	B4sm@%J!+m#7[>E!W2C!$#a&-%[>E&ย+>E#+24b&>E!+#)"%243#E!f#7(%&#Ly^24[#"%A#+24f{(%<#7(ย-c).ยฝw ยพยyWz{)| ยฟayWzf{"wEยdรDรU%AV|4|}ยพยyWz{)|C| ย[รยร?:lm( 7ยยฟ
%&dยพ=%&7aยณJ%&3#$2ยย )ยคย-c7 ย-cJ7.5BC%J!243<ABย<J24sI%&79I87J2G> %&#)!B49:&l	24#7(dU>E!+#E%&3#!+93.5@&BG! R
รg#	B4952G!	.b3!+#L&-N#7(YE187!7!F24<I8ยl	 L&-ย"!+#)ยค^" LB4s&2G>Y#h%<&%2ยBC%A@Bยยน24=#7(E24U%&8873%J>7(:
@#	#7(U!+#E%&#+2G!+#+2G> %B2ย-^7.0%&#+240#7(%&#a> %&0@YE1?8)!7!+ยฆ2C!	ยณJ24#7mBย24.524#7fRmvE1%A.b8B4:24#2G!L#
8!7!F24@B4*#7,.V%&r0s  "%Bm%!7!+ )#+24!5%&@#b!+#E%&#+2G!+#+2G> %BL2ย 8  >ERDยป%&+2G!d%&ยผ >E&<!+rย%
!+(&lD#)(%&#a#7(I s7 Y&-@EBย24E-h> %&0@I>E.b8#70!W2ยs5.V%ย1324.5.ร 3#7)839d-^a#)(E2ยLBG%&s%AsR
ร (%!+#)+2my"ยยจยฉJยจ{m(%![%ยBC!$;!+(&lmยฎ!$>7('%=7!$Bย#:NA-ย%A+B49;ยณJ24<ย%B4 #V!7>E8R*รL#:%!Yl	b(%<
%B47%9'!+ssJ!+#7f:l	V@EBย24 <ยข#7(%A#I24#52G!ยน2ย.H87#E%&3#5#7,B4rยฎ%&#b%ย-ร%Ae$2C>)( dBG%&s%&sRรรm
BG%&s%&sJn%ยBยB4&lU!=%&)@2ย#)"%&79ยฑย"!$#7ยค^" =%!7!+ )#+24! :	-^BยBeรL3&B4%&ยB4s&2G>&:I%&7@24#7E%&79'8ABย9.52G%B
>E.5@24%&#+24!d&-U!+#"%A#+2G!+#+2G> %BhE187!7!W2ยJ! :%&.H7ร#7(!+=%&7V%ยBยBL-c%A#77!I#)(%&#e%&)V%>E#7%BยB49
!+E-^BN#7br&l	B4s ยค^7 87!$ 3#"%A#+24=8"%>E#+24#+24 E! Rhv7#7( 7.H7:#7(I"%&J.bยค^l	+BG!	.b #7(
.V%&rJ!m8 +-c>E#[!$ !+d2ยT#7(2G!Y+2G>7(!+ #7#$2ยsR[	(5s3%B&-h#7(2G!Y8%&8 Y2G!Y#7tJ2G!7>E&< Ilg( #7( I#7(
>E>E#+24ย#7ยข.V%124.e.ร 3#7)839ยฑ%BG!+(&BG! RยฑS*!+(&lร#7(%&#H.V%12ย.5.ยฐ #7789ยฎ>E#+243!
#7@*l	2GEB49'!+E-^Bยฌ:U>E&< +24s.V%A39ยฑ87@B4 .V!H#7(%&#b%&70-ยฌ%&bJ#"!F2G*#7(*!)>E8,&-ey+ยป%&+2G!bร
ยผ >E&<?!$rย%:ยยจยฉยจร ร (%!+#)+2ยฌ:ยยจยฉยจ?{"R
รร

รaรรรuรรยร'รรรรuรYรรรยฑรVรรรCรaรรยรรรร?รร&ร

รmรก'รข)รฃรค=รฅรข7รฃรค รฆbรฃรง&รกรจfรฉยรช4รขbรข7รซรฆ7รกรฌยฆรฅรซรขbรข7รฃรง&รข5รญ	รค*รฎ รง&รกรกรฅรข5รฏVรงAรฐรค=รข7รฃรชGรฌbรฎEรฅรกรกรครฎEรข+รช4รฅรกรรฑcรฅJรฆbรฅรซรฆ5รค รก3รข$รชยรฆ)รค
รฒGรง&รกรณรซรง&รณJรครดยฒรตรฅรฆaรฅJรกรคUรข7รฃรช4รกรณรฉรงรฌยรญ	รคmรฃรช4รก3รข7รครจHรครง&รฆ+รฒยรช4รค รฆรฉรข7รฃรค รฆ7รคIรง&รฆ7รคgรถรฆ7รฅรทรฒ4รค รฏVรฌยรชยรฑNรญ	รคmรข7รฆ7รธ5รข7รฅ[รฎEรฅรกรจJรช4รข+รช4รฅรก=รฅJรกVรง
รฐรกรฅยรญยรฒยรครจรณJรคIรทรงรฌ+รคYรข7รฃรง&รขLรช4รกรฎ)รฒยรซรจรครฌgรกรฅรกรน^รซรกรง&รฆ)รธVรถรฆ7รครจJรชGรฎ รง&รข)รครฌ รบรญ	รค/รฌ$รซรฌ+รถรครฎEรขmรข)รฃรง&รขaรฏVรงยรป3รช4รฏ5รซรฏรรค รก3รข7รฆ)รฅรถ3รธHรฃรงรฌ
รกรฅ*รฆ)รฅ&รฒ4รคbรญmรฃรง&รข"รฌ$รฅ3รค รผรค รฆYรช4รกยฑรข7รฃรชGรฌ[รฎ รงรฌ$รครดbรฝยฌรกยฑรงรจรจJรช4รข+รช4รฅรกรฉรญhรค0รฌ+รฃรฅ&รญยรข7รฃรง&รขIรข7รฃรค รฆ7รค0รง&รฆ7รคbรฌ+รซรทรข+รฒ4รค รข+รช4รครฌ5รข7รฃรงAรข[รง&รฆ+รชGรฌ+รค
รช4รก3รผJรฅ&รฒ4รผรช4รกรณ0รข7รฃรคIรช4รกรข7รค รฆ"รงรฎEรข+รช4รฅรก=รทรค รข+รญhรค รค รกยขรฌ+รข"รง&รข$รชCรฌ$รข+รชGรฎ รงรฒfรชยรกรฑ^รฅรฆ7รฏ0รง&รข+รช4รฅรก*รง&รกรจtรพรฆ"รฌ+รข7รน^รฅรฆEรจรค รฆยรฟJรซรง&รกรข+รชยรพuรฎ รง&รข+รช4รฅรก
รด รค
รฑ^รค รคEรฒhรข7รฃรง&รข5รง&รกยขรช4รฏbรถรฅรฆ7รข"รง&รกรขeรฎEรฅJรก3รข7รฆ$รชยรทรซรข$รชยรฅJรกยฎรฅ&รฑmรข7รฃรชGรฌdรถรง&รถรค รฆ[รฒยรช4รครฌdรช4รกยฑรถรฅ&รช4รกรข+รช4รกรณรฅรซรขeรฌ$รฅรฏbรค5รฒยรช4รฏ5รช4รข"รง&รข+รช4รฅรกรฌdรฅ&รฑ
รฏVรงรปรช4รฏ5รซรฏbรน^รค รก3รข)รฆ7รฅรถรธVรฏbรค รข)รฃรฅ?รจรฌ รด
 รฃรคmรฆ7รครฌ+รข	รฅ&รฑรข7รฃรชGรฌLรถรง&รถรค รฆLรชGรฌLรฅรฆ7รณ3รงAรกรช  รครจ0รงรฌยรฑ^รฅ&รฒยรฒยรฅ&รญUรฌxรดNรฝFรกVรข)รฃรคUรกรคEรปรขmรฌ+รครฎEรข+รช4รฅรกรฉรญ	รคUรจJรชGรฌ7รฎEรซรฌ7รฌ	รฅรซรฆยรฑcรฅJรฆ7รฏVรงรฒ
รฑ^รฆ"รง&รฏbรค รญ	รฅรฆ7
รฐ ยฌรครฌ7รฌ+รค รกรข+รชGรงรฒยรฒ4รธรฉ	รข7รฃรง&รขbรฅ&
รฑ 		รงรฎ รฎ7รฃรซรฌ 
รฉ 

รบ Uรงรฒ4รถรค รฆ7รก
รฉ 
"
รด Tรค*รจJรชCรฌ)รฎEรซรฌ7รฌbรข)รฃรค;รฌ+รธรก3รขEรงรป
รง&รกรจ,รฌ+รค รฏVรง&รกรข+รชGรฎ รฌ	รฅ&รฑhรฌ+รขEรง&รข+รชGรฌ+รข+รชGรฎ รงรฒยฒรงJรฌ7รฌ+รค รฆ7รข+รช4รฅรกรฌxรฉรชGรฌ7รฌ+รซรครฌยรชยรกรผรฅ&รฒ4รผJรช4รก
รณ  รง&รถรถรฆ7รฅรปรชยรฏ0รง&รข7รคEรฒ4รธVรครฟJรซรงรฒGรฌ รฉfรง&รกรจ,รจรคEรพรกรค
รข7รฃรคbรฆEรง&รกรจรฅรฏbรน^รญ	รฅรฆ+รฒGรจรฌYรฏbรค รข)รฃรฅ?รจยรฑ^รฅรฆ7รฏVรงรฒยรฒ4รธรดbรฝF
รก รครฎEรข+รช4รฅ
รก 0รญ	รคbรฌ+รข"รงAรข7รค5รข7รฃรคbรทรงรฌFรชGรฎ5รฆ)รครฌ+รซรฒ4รข"รฌIรข7รฃรงAรข/รฎEรฅรกรกรครฎEรข
รฏVรงรปรช4รฏ5รซรฏiรค รกรข7รฆ7รฅรถรธ0รข7รฅVรฆEรง&รกรจรฅรฏbรน^รญ	รฅรฆ+รฒGรจรฌ รฉรง&รกรจยรชย
รก รครฎEรข+รช4รฅ
รก Vรญ	รค5รจJรชCรฌ)รฎEรซรฌ7รฌYรฃรฅ&รญยรข7รฅHรซรฌ+รค[รข7รฃรครฌ$รคeรฆ)รครฌ+รซรฒ4รข"รฌ
รงรฌg!รค รครฎEรข+รช4รผรคeรฎEรฅรฏHรถรซรข"รง&รข+รช4รฅรกรงรฒรถรฆ7รฅรฎEรครจรซรฆ7รครฌ รดgรฝยฌ"
รก รครฎEรข+รช4รฅ
รก #bรญ	รคdรฆ7รค รข7รซรฆ7รก,รข7รฅbรข7รฃรคยนรชCรฌ)รฌ+รซรค5รฅ&รฑaรซรกรง&รฆ7รธ=รผรค รฆEรฌ+รซรฌ
รกรฅรกรน^รซรกรง&รฆ)รธVรถรฆ7รครจJรชGรฎ รง&รข)รครฌ รฉรง&รกรจ0รข7รฃรคIรฟรซรครฌ+รข$รชยรฅJรก=รฅ&รฑNรฃรฅ&รญยรญ	รชGรจรคEรฒยรธ=รง&รถรถรฒยรชGรฎ รง&รทรฒ4รค5รข7รฃรคUรถรฆ$รชยรกรฎ)รช4รถรฒ4รค5รฅ&รฑNรฏVรงยรป3รช4รฏ5รซรฏ
รค รกรข7รฆ7รฅรถรธbรชGรฌ $
รด รค[รฎEรฅรกรฎ)รฒ4รซรจรคdรชย
รก รครฎEรข+รช4รฅ&
รก %ยฆรญ	รช4รข7รฃ;รฌ+รฅJรฏbรค/รจJรชGรฌ7รฎEรซรฌ)รฌFรช4รฅรกรด

')(+*,.-/0213-547698;:<,56=13>?130;4@:A13,7B
รฝยฌรก5รข7รฃรชGรฌรฌ+รครฎEรข$รชยรฅJรกรฉ&รญhรคLรณ&รช4รผรคaรข)รฃรครฑ^รฅรฆ7รฏVรงรฒรจรคEรพรกรชยรข$รชยรฅJรกbรฅ&รฑรฅรซรฆรฒGรง&รกรณรซรง&รณรคmรง&รกรจยนรข7รฃรคaรฆEรง&รกรจรฅรฏbรน^รญ	รฅรฆ+รฒGรจรฌรฏbรค รข7รฃรฅรจfรด
รฏ 		รงรฎ รฎ)รฃรซรฌ	รค รขUรงรฒยฌรด4@
รฉ 
D5"รด
 รฃรคIรฏVรงAรข7รค รฆ+รชGรงรฒรชGรฌยรฒCรงAรฆ7รณรคEรฒ4รธVรขEรง&รฐรค รก0รฑ^รฆ7รฅC

EGF=HJILKGMNPO@QGRTSGO5R7M
รค*รง&รฆ)รค0รชยรกรข7รค รฆ7รครฌ$รข7รครจTรช4รกยธรง,รฑ^รฅรฆ7รฏVรงยรฒaรฒ4รฅรณ&รชGรฎ รงรฒmรฒGรง&รกรณJรซรง&รณรค=รข7รฃรง&รข0รงรฒยรฒยรฅ&รญUรฌยฆรซรฌ5รข7รฅยขรคEรป?รถรฆ)รครฌ7รฌ5รทรฅJรข7รฃยธรฌ+รขEรง&รข+รชGรฌ+รข+รชGรฎ รงรฒ
รช4รกรฑcรฅJรฆ7รฏVรง&รข+รช4รฅรกqรง&รกรจรรพรฆ"รฌ$รข7รน^รฅรฆ"รจรค รฆHรช4รกรฑcรฅรฆ)รฏVรง&รข+รช4รฅรกรดJTรค;รข)รฃรค รฆ7รคEรฑcรฅJรฆ7รครจรคEรพรกรคยฎรงยฎรฌ$รข"รง&รข+รชGรฌ+รข+รชGรฎ รงรฒYรฒGรง&รกรณรซรง&รณJรคU;VLรฉ
รญmรฃรชGรฎ)รฃ'รชGรฌbรงยขรผ&รง&รฆ+รชGรง&รกรขbรฅ&รฑ/รง,รฒGรง&รกรณJรซรง&รณรค*รจรครฌFรช4รณรกรครจรรท3รธ	รงรฎ รฎ7รฃรซรฌWX
YZ"รดรตรฅรฆbรข7รฃรค=รฆ)รค รฏVรงรช4รกรจรค รฆbรฅ&รฑIรข7รฃรค
รถรง&รถรค รฆรฉJรฒยรค \
รข [ยฎรทรค/รง/รพรกรชยรข)รคmรพรฆ"รฌ$รข7รน^รฅรฆ"รจรค รฆLรผรฅ?รฎ รงAรทรซรฒCรงAรฆ7รธรฉ3รฎEรฅรกรฌWรชCรฌ$รข+รช4รกรณbรฅ&รฑuรถรฆ)รครจJรชCรฎ รงAรข7รค/รงAรกรจVรฎEรฅรกรฌ$รข"รง&รกรขaรฌ+รธ3รฏ5รทรฅ&รฒGรฌ รฉ
รง&รกรจHรฒ4รค 
รข ]ยฅรทรคeรงbรฌ+รค รขgรฅ&รฑยฒรผ&รง&รฆ+รชGรง&รทรฒ4รครฌ _รด ^
รmรซรฆ5รฌ+รข"รง&รข$รชCรฌ$รข+รชGรฎ รงรฒaรฒGรง&รกรณรซรงAรณรคVรง&รซรณรฏHรค รก3รข"รฌdรฌ+รข"รง&รกรจรงAรฆ"รจ,รพรฆ"รฌ+รข)รน^รฅรฆ"รจรค รฆIรฒ4รฅรณAรชCรฎHรญ	รช4รข7รฃยฎรงtรฑcรฅJรฆ7รฏ รฅ&รฑUรฌ+รขEรง&รข+รชGรฌ+รข+รชGรฎ รงรฒ
รฟJรซรง&รก3รข$รชยรพรค รฆรด รตรฅรฆbรงยรฑcรฅรฆ)รฏeรซรฒG
รง `	ab"รฉรข7รฃรค0รข7รค รฆ7d
รฏ cec `fga@hcPc i0รชGรฌVk
รง j.lm!j5mAlonqp=mArtsvuwj.lsvx!xXp=mArfรดยขรฝยฌรขbรญ	รชยรฒยรฒUรทรค
รช4รก3รข)รค รฆ7รถรฆ7รค รข7รครจ0รงรฌรงIรฆEรง&รข+รช4รฅรกรงรฒรกรซรฏ5รทรค รฆaรทรค รข+รญhรค รค y
รก [รงAรกy
รจ 
รฉ&รข7รฃรง&รขLรฆ7รค รถรฆ7รครฌ+รค รกรข"รฌยรข7รฃรคmรถรฆ7รฅรถรฅรฆ7รข+รช4รฅรกHรฅ&รฑNรจรฅรฏVรงรช4รก
รคEรฒ4รค รฏbรค รกรข"รฌ[รฌ7รงAรข+รชGรฌFรฑcรธJรช4รกz
รณ `	ab"{
รด รคbรงรฎEรข7รซรงรฒยรฒ4รธยฎรงรฒยรฒ4รฅ&รญ รง&รกยฑรง&รฆ7รทรช4รข7รฆ"รง&รฆ)รธ;รฌ+รค รขIรฅ&รฑmรผ&รง&รฆ+รชGรง&รทรฒ4รครฌยนรชยรกTรข7รฃรคVรฌ+รซรทรฌ7รฎEรฆ$รชยรถรข
รง&รกรจรรช4รกยรข)รฃรค*รฑ^รฅรฆ7รฏ5รซรฒG"
รง `Iรด  รฃรซรฌ รฉgรฑcรฅJรฆ=รคEรปรง&รฏHรถรฒยรค+
รฉ cec}|2~p=ยย7	aTยย7hcec iยฎรจรครฌ7รฎEรฆ+รช4รทรครฌxรฉIรฑ^รฅรฆ=รงTรพรป?รคย
รจ ยfรฉYรข7รฃรค
รถรฆ7รฅรถรฅรฆ7รข+รช4รฅรก,รฅ&รฑhรจรฅรฏVรงยรชยรกยขรคEรฒ4รค รฏbรค รกรข"รฌYรข7รฃรง&รขYรง&รฆ7รคeรฎ)รฃรชยรฒGรจรฆ7รค รกรฅ&;
รฑ ยfย
รบ cPcq|2~p=ยย.gaยยย7hcPc ยยฆรจรครฌ7รฎEรฆ$รชยรทรครฌ รฉfรฑ^รฅรฆUรงยฆรพรปรครจ
aรฉfรข)รฃรคeรถรฆ)รฅรถรฅรฆ)รข+รช4รฅรกยขรฅ&รฑhรจรฅรฏVรงรช4รกยขรคEรฒ4รค รฏbรค รกรข"รฌIรญmรฃรฅรฌ+รคbรฎ7รฃรชยรฒGรจ,รชG9
รฌ aรบรง&รก
รจ cecq|2~p=ยย5	aTยย7Xcec iAย ย รจรครฌ)รฎEรฆ+รช4รทรครฌdรข7รฃรค
รถรฆ7รฅรถรฅรฆ7รข+รช4รฅรก=รฅAรฑยฒรถรงรช4รฆ"รฌgรฅ&รฑยฒรจรฅJรฏVรงรช4รก*รคEรฒ4รค รฏbรค รกรข"รฌgรข7รฃรง&รขmรง&รฆ7รคYรช4รก*รข)รฃรค[รฎ7รฃรชยรฒGรจ,รฆ7รคEรฒGรง&รข+รช4รฅรกรด_ย
TรคhรงรฒGรฌ+รฅ[รงรฒยรฒ4รฅ&รญ'รถรฆ7รฅรถรฅรฆ7รข+รช4รฅรก5รคEรปรถรฆ7รครฌ7รฌFรช4รฅรกรฌยรฅ&รฑรข7รฃรคLรฑcรฅJรฆ7ย
รฏ ยo`fga@hcยยga@oย i รฉ&รญmรฃรชGรฎ)รฃ5รญhรคmรฎ รงรฒย)
รฒ ย!mArGยDp}nqp=mArGย<
j.lm!j5mAlonqp=mArsvuwj.lsvx!xXp=mAr7xยรด รซรฎ)รฃ*รง&รก=รคEรป?รถรฆ)รครฌ7รฌFรช4รฅรก=รชGรฌLรช4รกรข7รค รกรจรครจ,รข7รฅbรจรค รกรฅรข7รคIรข7รฃรคIรถรฆ)รฅรถรฅรฆ)รข+รช4รฅรก=รฅ&รฑaรจรฅรฏVรงรช4รก
รคEรฒ4รค รฏbรค รกรข"รฌ	รฌ7รง&รข+รชGรฌFรฑ^รธรช4รก{
รณ `Tรฑcรฆ7รฅJรฏ รง&รฏbรฅJรกรณIรข7รฃรฅ3รฌ+รคYรคEรฒ4รค รฏbรค รก3รขEรฌhรฌ)รง&รข+รชGรฌFรฑ^รธรช4รกย
รณ ยรดaรตรช4รกรงรฒยรฒ4รธรฉรง&รกรธ5รฆ"รง&รข+รช4รฅรกรงยรฒuรกรซรฏeรทรค รฆ
รชGรฌ5รงรฒGรฌ+รฅ;รฎEรฅรกรฌWรชCรจรค รฆ)รครจยฑรข7รฅ=รทรค=รง=รถรฆ7รฅJรถรฅรฆ7รข$รชยรฅJรกยฎรคEรปรถรฆ7รครฌ7รฌFรช4รฅรกรฉรง&รกรจTรข7รฃรคVรฌ$รค รข[รฅ&รฑmรถรฆ7รฅรถรฅรฆ7รข+รช4รฅรกTรคEรป?รถรฆ7รครฌ)รฌFรช4รฅรกรฌยนรชCรฌ
รฎ)รฒ4รฅ3รฌ+รครจยรซรกรจรค รฆ/รงรจรจJรช4รข+รช4รฅรกยขรง&รกรจ=รฏ5รซรฒ4รข+รช4รถรฒยรชGรฎ รง&รข+รช4รฅรกรด

!ย ย.ยAยXย@ยqย_ยยยvยยย_ยยยยยqยก!ยขยคยฃ@ยฅ)ยฆwย=ย=ยงยยยฅ2ย=ยจยฉยฆwย)ยชWยซYยvยฅ	ยTยฌย!ยGยgย!ยฌvย3ยฆgย_ยฌยญeยงYยฌยgยqย_ยXยฌย=ยกvยยฎAยย_ยgยขยqย_ยฌYย	ยฅ2ย=ยจYยฅgย=ยฅ2ยhยฆwยฌยฎAยฅ2ยซยฅยยฏAยฌยฅgยซย_ยฌfย=ยฅgย=ยยย
ยยญ.ยYย=ยฅgยซvย_ยยฆย=ยฅgยgย
ยฐยฑยGยฒvย=ยqย_ยgยqย_ยก9ย=ยAยฅยฆwยณ!ย_ยฌยดYยขยคย=ยจยฅgย=ยฅยย=ย!ยยฉย!ย=ยqย_ย!ยฌยยฅยยตยYย=ยฅ	ย=ยqย_ย!ยฌ{ย=ยจย!ยงvย_ยซยฎยฉยฅยฃยยqย_ย=ย=ยฅgยฌยฃ@ย_ย=ยจย=ยฅgย=ย)ยwยญ.ยถXยฆwยqยยยฆยฎยฑย_ยฅgย2ย_ยฌ9ย=ยจYยฅ2ย=ยงยฎYย=ย	ยqย_ยYยgยข
ยฆยยทย_ยฌyยธยนยธeยบยปYยผยยฝ_ยพยฟ=ร<รรรรยธยนยธ ร=รwร รhรoยรรยทยยฃ@ยฅgยถXยฅgยgยขยคยฃTยจยฅgยฌ{ย=ยจยฅย_ยฌvย=ยฅgย=ยย=ยฅgย3ยฆยqย_ย!ยฌ{ย_ยย3ย_ยฅยฆwยgยข5ยฃGยฅยยwยญeย=ยฅ	ยฌ{ยฆยฎYยงย=ยฅยฌYยXย3ยฆwยqย_ย!ยฌLยฆwยฌยซ{ยซYย=ยXย
ย=ยจยฅ2ย=ยฅgยยยซYยฅ3ยยย_ยย_ย=ยฅgย=ยgย
รร

รรรZร5ร7ร2รยร5รรรร5รยรยทรรรรรรรรYรร
ร ร5รรขรกeรฃ+รค.รฅรฆwรงXรจAรZรงfรฉ<รกยรชbรvรฆwรvร.รซ!รยรฌ7รvรงgรญรรvรvรรฎรฅรฏ5รฆfรฐgรฑZรZรงXรจYรฒzรจAร.รฉyรงรณ.รจDรงรรฅDรดรต	รถยรจรซvรซรณZรฏ.รฐvรทยรธYรนรนรบZรปยทรกรรฐรงwรณ5รรฏ.รฐgร9รฅAรด

รผ!รฝรฝ7รพhรฟLรผ	
รผ	 รงรฅ+รซ!รฅรฃ+รค.รจAรฆwรรค5รฆรฅรค.รฅ<รฆรงgรกeรฅรyร!รฒ5รค5รฆรรฐรฐยรกeรฅร.รฐยรณ5รvรฆร9รจDรฆรรฌ.รฅรงwรณyรค5รณZรกeรฅยครฐhรฅรค5รณZรกรรซvรจรรจAร.รฉ
รค5รฆXรจรซ!รงhรกPรซvรจ ยรฆรรจ<รฐgรฅร.รฐยรญรณZรฑWร!รฒรรจรซ!รงรซ!รฅรฃ+รค.รจDรฆgรกรรฐgรฅร.รฐรซvรจAรyรฌ.รรกeร.รจAรค5รค5รฆรฅรค5รฆhรกPรจDรงร ;รฅร.รฐยรกรรฉZรvรฆ9รจ+รฐhรงXรจAรงรvรฃ+รvรZรงfรฐgรฏ.รซwรณ
รจ
รฐ รบ tรฅAรดยรค.รจAรงhรกรvรZรงXรฐ$รญยรกeรง"
รณ !รจAรฏ5ร.รฉ<รกรรซ!รรณ.รจ #<รรณ5รvรค.รจAรงgรกeรงgรกร%รฐ $&(' รด)รงรณZรกรรฐ;รฐgรง!รจAรงรvรฃ+รvรZรง;รจAรค5รค.รรจDรฆXรฐ$รกรy
รจ )Zร5รฅยฉ*
รญ รรฉ +<ร
รฌ.รจรฐgรรทDรกeรง2รกรรฐ;รจ eรฃ+รฅZรฐgรง;รซ!รvรฆรงXรจYรกeร eรฑ{รงรณ5รvรฆรfรจ<รฐ2รจรฐgรฏ5รฃ+รฃ รจAรฆwรฑ9รฅAรด),
รจ รรจA%รฆ +รยรค7รฅยค-รฅ @รฅAรดbรฉ5รจDรงX&รจ /.รฅรขรกeรง2รญรรฅ<รฏ PรฉLรฌ.รรญรฆรฅ&ร +
รงรฅ รกeรยครงรvรฆwรค5รฆรvรงfรงwรณ50
ร #Aรจ eรฏ5ร+รงรฅZ1
รฅ ยรกeรงรvรฆXรจ 2eรฑรทยทรงรฅ รฃ รรจAรzรงรณ.รจDรง ยฉรผ3	 4รบ  รฅAรดรรจ 2รค.รจAรงgรกeรvรZรงXรฐรญยรกeรง5
รณ !รจAรฏ5ร.รฉ<รกรรซ!ร
รณ.รจ #รรณ5รvรค.รจAรงhรกรงhรกPรฐ 765รฏ5รฆรงwรณ5รvรฆรฃ+รฅรฆรรทAรงwรณZรกPรฐยรกeรยครงรvรฆwรค5รฆรvรงXรจAรงgรกeรฅรWรญยรฅรฏ รรฉรกeรฃ+รค eรฑyรต	รจAรฃ รฅ&ร +รฅรงรณ5รvรฆรงรณZรกe&ร +ยครฐ!รป)รงwรณ.รจAรง2รงรณ5ร
รZรฏ5รฃยรฌ7รvรฆ)รฅDรด !รจAรฏ5ร.รฉ<รกรรซ!รรฉรค.รจAรงgรกeรvรยครง!รฐ@รกรรฐยทรจรฃ{รฏ eรงgรกeรค eรรรฅD9รด 8:#ร ;/ยรณZรกรรฐbรกรรฐยรฏ5ร ย<รก )=ร eรฑ{รงรฅรฌ.รยรฐgรฅรฃ+รvรงwรณZรก&ร +รญยรยทรกeรยครงรvร.>รฉ 
รฆ DGFรจ eรค.รvรฆรยรทTรธรนรน HZรป!รท
? รรงรณ5รvรฆร!รด=รฅรฆรรฏ.รฐgรรงรณ5รรจAรค5รค5รฆรฅยครจ<รซรณyรฉZรรฐรซ!รฆgรกeรฌ.รรฉ รกeรรต	รถยรจรซvรซรณZรฏ.รฐรvรงรจ @eรท@รธรน-รน A:B C-รฅ 2eรvE
รจAร.รฉWรซ!รฅรฃ+รค.รจAรฆwรรค5รฆรฅรค.รฅ<รฆรงgรกeรฅรWร!รฒรค5รฆรรฐwรฐยรกeรฅร.รฐยรฏ.รฐยรกe&ร +yรต=รกeร.รฐgรงwรรจรฉWรฅA/รด I รจAร.K
รฉ Jรป;รฅร5รรฅAรดยรจAร รกeร 8.รZรกรงwรรด3รจAรฃ{2รก eรฑ รฅAรด
รซ!รฅร5ร5รรซ!รงg<รก #รM
รฐ LNรรจAร.P
รฉ OQNXรทZรด=รฅ
รฆ RQI รธ SHTSUWVVVXรต =R@Y3รจDรค5รค5รฆรฅYรฒยครกeรฃ รจAรงw=ร รฑWร Z<รฏ.รจ [$+รฅ\
รฆ =R]Y3รจAรค5รค5รฆรฅYรฒZรกรฃWรจAรง=ร e

รฑ eรรฐรฐ
รงรณ.รจAร"รฅรฆร Zรฏ.รจ [$Z=รป ^65รฅรฆร!รฒรรจDรฃ+รค eรรท2รญยร รซvรจAรร!รฒรค5รฆรรฐwรฐ9รงwรณ5ร รฐgรง!รจAรงรvรฃ+รvรZ_
รง <รบ  รฅA7รด !รจAรฏ5ร.รฉ<รกรรซ!รรฉ"รค.รจAรงgรกeรvรZรงXรฐ
รณ.รจ #รรณ5รvรค.รจAรงhรกรงhรกP`รฐ $9รฌZรฑ+รงรณ5ร รฝ.รพรฟ!รฝรฟAรพ =-รฟ a\bvรฟA=รพ 0&ยd
รผ ce"gรฝ gรต fbรป hji รผ 9a gรต fbรป c=k Lml\&รบ V&(ยรณ5รรกeรZรงรฏZรกeรงgรกeรฅรyรฌ.รvรณZรกeร.รฉ
รงรณ5ร9รฐgรvรฃWรจAรยครงhรกPรซvรฐรรฅAรด$รจDรค5รค5รฆรฅYรฒยครกeรฃ รจAรงwรfร Z<รฏ.รจ ยรกeรง	รฑWรกรรฐ\รงรณ.รจAรงยรรจรซwรณzรซ!รฅรฃ+รค.รจAรฆhรกPรฐhรฅรzรฐgรณ5รฅรฏ รรฉyรฌ7รรกeรZรงรvรฆรค5รฆรvรงรรฉWรฏ.รฐ	รก&ร +
รฐgรฅรฃ ร รฐgรฃWรจ 2รรง-รฅ eรvรฆXรจDร.รซ!ร+รด3รจรซ!รงรฅรฆรงรฅรจรซvรซ!รฅรฏ5รZรงรด=รฅรฆ{รฃ+รรจรฐgรฏ5รฆwรvรฃ+รvรยครงรvรฆรฆรฅรฆYรทยรฐwรจAรฃ+รค ed
ร #AรจAรฆgรกรรจAรงgรกeรฅร.รฐยฑรท;รจAร.รฉ"รฐgรฅ
รฅnร oยรณ5รรจAรค5รค5รฆรฅรค5รฆhรกPรจDรงรzรงรฅ รvรฆ!รจAร.รซ!รzรญย2รก 2ยรฉ<รกยรชยรvรฆ+รด=รฅ5
รฆ #ยฉรจAรฆgรกeรฅรฏ.รฐ รคZรกeรรซ!รรฐWรฅAรดรกeรZรด}รฅ<รฆรฃ รจAรงgรกeรฅรยรทรฐhรฅรฅรฏ5p
รฆ รฅ +Aรกรรซ
รจ 2eรฅAรญfรฐ9รฉ<รกยรชยรvรฆรvรZรงยรฐgรฏ5รฌ.รฐรซ!รฆhรกรค5รง!รฐรฅรรงรณ5q
ร ยฑรจAรค5รค5รฆรฅYรฒยครกeรฃ รจDรง=ร eรฑ&ร Z<รฏ.รจ ร%รฐ $รฎรซ!รฅร5ร5รรซ!รงg<รก #รรฐ prJรด=รฅรฆรฃ{รฏ รรจyรฐgรฏ.รซรณ"รจรฐ
รธ v cs7j gรต fb`รป ht รผ  gรต f@รป c=k Lmw"รธรฐรจYรฑรฐLรงรณ.รจAรง+รฌ7รฅรงรณ cs7j gรต fbXรป hjt ร%รพ u gรต fbรป cXk รจAร.รฉ
cs7j gรต fb`รป ht q%รพ u gรต fbรป cXk Lml\
รด vรจDรค5รค5รฆรฅYรฒยครกeรฃ รจAรงw=ร &รฑ $รฃ รจYรฑ+รฌ.ร9รฉ<รกยรชยรvรฆรvรZรง
cs7j gรต fb`รป ht 
รผ  gรต f@รป c=k รจAรฆรรจAรค5รค5รฆรฅYรฒยครกeรฃ รจDรง=ร eรฑzรธรทZรฌ5รฏ5รงรงรณ5รร5รฅรงgรกeรฅรWรฅA*
รกeรyรรจรซwรณ รซvรจรฐhร xรรณ5ร9รจรซ!รงรฏ.รจ ยรซรณ5รฅAรกรรซ!รรฅAรด)รฐgรฏ5รฌ.รฐwรซ!รฆgรกeรค5รงยรด}รฅ<W
รฆ Lยรกรรฐรฏ5รZรกรฃ รค.รฅรฆรง!รจAรยครง 7FรฅAรญยร #รvรฆรทDรกeรง;รกรรฐ$รกeรฃ+รค.รฅ<รฆรงXรจAรZรง
รงรฅyรฏ.รฐgร+รฉ<รกยรชยรvรฆรvรZรงยรฐgรฏ5รฌ.รฐรซ!รฆhรกรค5รง!รฐรด=รฅรฆ9รฉ<รกยรชbรvรฆwรvรยครง{รจAรค5รค5รฆรฅYรฒยครกeรฃ รจDรงร+รซ!รฅรฃ+รค.รจAรฆgรกรรฐgรฅ<ร.รฐรฏ5ร รรฐwรฐรงรณ5ร+รงw-รฅ eรvรฆXรจAร.รซ!รรฐรด=รฅรฆ
รงรณ5ร9รฉ<รกยรชยรvรฆรvรZรงfรฃ+รรจ<รฐgรฏ5รฆรvรฃ+รvรZรงXรฐรจAรฆwm
ร )Zร5รฅAรญรyรงรฅ+รฌ7ร9รงรณ5ร9รฐรจDรฃ+ร 
รญ +A<รก #ร9รจ{รฆรรซ!รฏ5รฆ!รฐย<รก #รยรฉZ=ร 8.รZรกeรงgรกeรฅรรฅAรด2รงรณ5
ร รรจA&ร +รฏ.-รจ +<0
ร y7zQ
? ร9รซvรจAรyร5รฅAo
ร y z รกP,
รฐ ยGยยยรญรณ5รvรฆ
ร ยzรกรรฐยรงwรณ5ร9รฐgรvรงรฅAรดยรซ!รฅ<ร.รฐgรงXรจAรZรงรฐgรฑยครฃ{รฌ7-รฅ รรฐรกeร
{5|}~nย<ยยยยn~ยยยย	ย&ย ยรณ5ร9รฐgรvรง\รฅAรด 	vรพ=1ย รกe^
ร eรรจรฐgรงรฐgรvรงรงรณ.รจAรง
ย 7รรณ5ร9รฐgรvรงรฅAรด รฝ.รพรฟ!รฝรฟDXรพ =-รฟ aยรฝ.%รพ ย=ย=}-รฟ aยย รกPรฐ\รงรณ5
รต	รจรป รซ!รฅรยครง!รจYรกeร.รฐยรงรณ5รรฆXรจAรงhรกรฅ<ร.รจ )รZรฏ5รฃยรฌ7รvรฆXรฐvรท
รต3รฌ@รปyรซ!รฅรยครง!รจYรกeร.รฐ รฝ.รพรฟ!รฝ5รฟAรพ}รฟ-aย	vรพ=0ย รฅDรดยรงรณ5รรขรด}รฅ<รฆย
รฃ h<h ย\h<h ย รจAร.รฉ c ยMhยย c ยยรด}รฅ<รฆรด=รฅรฆรฃ{รฏ รรจ
รฐ ย"Sย1ยPy z รจAร.รฉ
รจ 8.รZรกeรงรยรฐgรvรง\รฅAยรด #ยฉรจDรฆgรกรรจAรฌ eร
ย
รฐ ยยยยยรฎรท.รจAร.รฉ
รต	รซYรป รกPรฐ\`รซ รฅZรฐgรรฉyรฏ5ร.รฉZรvรฆรจรฉ5รฉ<รกeรงgรกeรฅรรจAร.รฉWรฃ{รฏ eรงgรกeรค ยรกรรซvรจAรงhรกรฅ<nร 
ยรณ5ร9รฐgรvรง\รฅAรด)รด=รฅรฆรฃ{รฏ รรจรฐรกeP
ร y z รกรรฐรงwรณ5
ร รรจ<รฐgรงfรฐgรvรงรงwรณ.รจAรง
รต	รจรป รซ!รฅรยครง!รจYรกeร.รฐ รผ=รฟ-1	3boรฟDรพ1&ยรผย รฅAรด2รงรณ5รรขรด}รฅรฆwย
รฃ ย]รต ยgl-SVVV%S`ย	ยAรปXรทZรญรณ5รvรฆ1
ร ย รกรรฐfรจLรค5รฆรรฉ<รกรรซvรจAรงรยรฐhรฑยครฃ{รฌ.รฅ ยรกeร
รฑ ยค+รจAร.ยฅ
รฉ ยgl-SVVV%S`ย	ย รจDรฆรรงรvรฆรฃ รฐยฑรท
ย ยยขยกI"ยฃ9รฅAรด;รจAรฆgรกeรงg

รต3รฌ@รปyรซ!รฅรยครง!รจYรกeร.รฐ รฝ.รพรฟ!รฝรฟDรพX=รฟ-aPboรฟDรพ1&ยรผย รฅAรดfรงรณ5รWรด=รฅรฆยง
รฃ ยฆยจLN0ยฆ-ยฉรจAร.ยช
รฉ ยฆยซOQN0ยฆ-ยฉรรทรญรณ5รvรฆ^
ร ยฆรจDร.ย
รฉ ยฆ-ยฉยรจAรฆร
รค5รฆรฅรค.รฅ<รฆรงgรกeรฅรyร!รฒ5รค5รฆรรฐรฐยรกeรฅร.รฐรจAร.5
รฉ R2รกรรฐรจ{ร.รจAรงรฏ5รฆ!รจ ยรยครฏ5รฃ{รฌ.รvรฆYรท.รจAร.รฉ
รต	รซYรป รกPรฐ\`รซ รฅZรฐgรรฉyรฏ5ร.รฉZรvรฆรซ!รฅ-ร !gรฏ5ร.รซ!รงgรกeรฅรยรท.ร5ร +ZรจAรงgรกeรฅรยรท.รจAร._
รฉ 8.รฆXรฐg%รง Y=รฅ<รฆXรฉZรvW
รฆ Zรฏ.รจAรZรงg2รก 8GรซvรจAรงgรกeรฅnร 
รง y z รจ 2eรฅAรญfรฐรงรณ5ร{รฏ.รฐgร{รฅAรดยรZรฏ.รจยรกeรงgรฑรญรณ5รvรรซ!รฅรฃ รค.รจAรฆgรกeร&+yรงรvรฆรฃ รฐยฑรท@รฌ5รฏ5รงร5รฅรงรญรณ5รvรรซ!รฅรฃ รค.รจAรฆgรกeร&+
ยฌ รฅรงร{รงรณ.รจA"
รค5รฆรฅรค7รฅรฆรงgรกeรฅรyร!รฒ5รค5รฆรรฐรฐ	รกรฅ<ร.รฐ
รรณZรกPรฐ{รฉZ=ร 8.รZรกeรงgรกeรฅร รจ 2eรฅAรญfรฐ{รจAรฆรฌZรกeรงรฆXรจDรฆรฑร5รรฐgรงgรกeร&+รฅAรดmZ<รฏ.รจAรยครงhรก8.รvรฆ!รฐยรจDร.รฉรค5รฆรฅ<รค.รฅรฆรงhรกรฅ<รร!รฒ5รค5รฆรรฐรฐยรกeรฅร.รฐ^rรฐ
รฅรฌ.รฐgรv`รฆ #รรฉ รกeรรต	รถยรจรซvรซรณZรฏ.รฐvรท@รธรนรน<รบZรปXรทรงwรณ5ร9รฐgรฏ5รฌ.รฐรซ!รฆgรกeรค5"
รง f รกรรจรค5รฆรฅรค.รฅ<รฆรงgรกeรฅรyร!รฒ5รค5รฆรรฐรฐยรกeรฅร.รฐยรฌZรกeร.รฉ5รฐ\รงรณ5"
ร #ยฉรจDรฆgรกรรจAรฌ eร
fWรกeรรฎรงรณ5รร!รฒรค5รฆwรรฐรฐยรกeรฅn
ร Bรรกร.รฉZรvรรฉ@รท@รญยร9รซvรจAยข
ร #<รกeรvย
รญ hยh2ยญ<h<h k รจ<รฐรจ{ร5รvรญtรงgรฑZรค.รรฅA7รด Zรฏ.รจDรยครงg2รก 8GรซvรจAรงhรกรฅ<nร 
ยฎยฏ

ยฐ7ยฑ&ยฒ&ยณยยดTยตยทยถยชยด&ยธ&ยนยณยยบยฑ&ยฒ&ยณยยปdยฑ&ยผ&ยฝยยต7ยพ&ยตGยฟ(ยฒ&ร&ยธ4ยดTร-ร

รรร0ร&ร-รGร&รรรPร`ร^ร ร=ร:ร&รdร%ร&ร
รรรร5ร-ร9รรรยรรMร-รร`ร&ร\ร<รรรยร-รpรEรรรdรรร:รร2รรรรร
ร:ร=ร<ร-ร\รรร
ร9รรร"ร-รร%ร&ร
ร ร=ร:ร ร<รรร<รร:ร
ร-ร`ร1รรรรรรรรรรยรรร%รรร<รร รรร	รร%รWร-รร>ร5รร&ร5รgรร5ร	รร-ร%รก&ร%รรMร%ร:ร-ร1รร-รก:รรร
รข&ร%รร รรรร5ร
ร-ร%รdรรข&รข&ร%รรฃ9ร<รค
รdร-ร`รรฅร=รร
รข:ร-รรร[รรรร:ร1ร-ร:รรร=รร:รร<รรร<รร:รรWรข&ร%รรขยรร%รรร<รรรฆร=รฃ&รข&ร%รร%รgรรรร:รร5รรฆร1ร<ร9ร`รร%รข&ร%รร\ร`ร&รdร-รข&รข&ร%รรฃ รรร5ร-ร%ร
ร=รร&ร&รร=รรร<รงร1รจ1รฉรชยรจ-รซ>ร%ร1ร
รร-รยขร`ร:ร-รWรจยร[รรงรร%รยขร`ร<ร9รรร"ร%รdรจรซรฌร/รญKรร%ร"รข&ร`รร`รยรรร=รรรรTร<รร[รWร*รรร`ร รรรPรรรร
รรงรร%ร
รรรdรรร7ร`ร-ร<รรร-ร:ร=ร1ร@รร=ร%รรร"รรฆรยรรฌรร%รdรร2ร<รฎร
ร%ร ร[ร"รก:รgรรร&รPรPรฏ	รฐรฑ2รฒรณ%รด-รต>รถรฒpรทรฒ=รถรฏ	รฐ-รณยขรนรฆ
รธ รบรผรป	รนรฝรพ%รนรฟรพ   ร รน 
รช  ร
@ร ร%รก ร<รรร<รงร=ร<ร^รจ1รฉ รช รจ-รซTร2รxร%ร&รรงรรรรก&รร,ร-รxรจ1ร-ร:รยขรจรซยร-ร%รรร<ร%ร ร<ร รน รช ร-ร(รรร%รยขรร%ร&รร

ร 	รxร=รรก&รรรรร9รร&ร"รข&ร%รร&รค
ร<รร ร*รรร`รยขร%ร ร[รQร[รWร%ร:รรรรรรร&รรรร2ร<ร^ร*รร2รยร&ร
ร 9ร&ร-ร ร%ร&ร"รง-รร<รก&ร\ร-ร รน รชรร(รรฆร"รขยร9รรร%รขยรร&ร0รร[ร%ร=รก:ร`ร]ร<รร^ร-ร
ร%ร ร[ร*รยร`รรรก&ร\รก&ร รรร2รยร%ร&ร"ร&ร=รฃ&รEรรรร=รรร<รรnร
รร&รร%ร&รร^รร _ร=รก ร<รรรยทร-รรรยรรรรยขรWร&รร ร<ร ร%รร%รข&ร%รรรรรร&รoร=รร:รร<รรร<รร:รร0รข&ร%รรขยรร%รรร<รรยร=รฃ4รข&ร%รร`ร]ร<รร:รร รร&ร
รข&ร%รร ร<รร รยรยฅร%ร:ร-
ร  รรร&ร&รรยขร:รยร ร=ร:ร&รร รรยขรยชร=รร:รร<รรร<รร:รร0รข&ร%รร:รร รร2ร<รรรoรWร&รรoร`ร&รร%รรฆร-ร%ร
ร&รรฆรร%ร]ร<รร&รpรร9รรยร%รPร%ร&ร5รงรรรร[ร-ร ร<รร1ร<
ร  ร`ร:ร-ร\รรรก ร[รยชร%ร-รรรยรgรรฌ
ร  ร/รยรรร-รก:รรรKรรยขรร-ร&ร&รร
รร<รงรยร รKร9ร
รฎรร%ร:รยร ร&รรยรรรรร:ร&ร-รรยร รก:รร2รรรรรยร[ร1รก:รรรรยจรร-ร%ร&รร1ร%ร:รรยร-รข&รข&ร%รรฃ9ร<รdรร%รยขร รก:รรร<รรรยร%ร ร[ร
รข&ร%รร รรรร รยร
รรร]ร2ร<รรฆร-รงรรร=รร
รรยร]ร<ร
รข ร<รรฆร รรฆรรงร-ร[รร<ร&รqร=รร:รร<รรร<รร:รรWรข&ร%รร:ร-ร ร2รร<รรร<รรรฅร<รรฆร%ร&รยขรรรรdร-ร รรร[รร\รร<ร%รรรร%ร&รรร
&รรร<ร-รร<ร&
ร  รร<รข:รร%รn"
ร !#$# &% รTรร1รร-ร^ร=ร2ร<ร1ร<ร:ร-ร%ร5ร=รร:รร<รรร<รร:รร7รข&ร%รรขยรร%รรร<รรKร=รฃ4รข&ร`รร%ร]ร<รร:ร"รร<ร%รรรร%ร&รร
ร รรฆรงร<รรร<ร&รยรKรรรร-ร`รร
รร9ร1รรรก:ร`ร ร'
ร  )
ร + -,./+  0
รบ ( รร1ร-รยชร-ร&ร&ร%รรงร[ร-รรร<รรรรรฌร*
รบ ( ++ \ร
รร รก:รร>รร1ร&รรงรร0รร=ร`รก:รร2รรรKร	รร%1
ร รก&รรรร<รร9รรรรรข&ร%รร:รร รร2ร<รรร<รรร5รร ร[ร\ร-รข&รข&ร%ร รร%รรฆร-รร%รรร"ร=รร
รข ร<รร%ร=ร<ร
รร<ร%รยซร%ร&ร^รรรร-ร:ร&รรรรร<ร9ร%รร`รข&ร%รรร-รรร<รรยซร-ร\ร=รร:รร<รรร<รร:รร[รdรรรPร<รร&ร ร2
ร ++ 4รบ 3  
ร ร5
ร +678  รบ  ร ร<ร
รร ร	รรร=รร,ร%ร&ร0ร=รร รงรร รรร<รรยขร%ร:ร-ร,ร	รร%ร1รก ร[รรWรรรก:ร`รqร9
ร  :

ร
ร
;










=

<

ร
ร%ร\ร%ร`รก&รรรฌรรร-ร9ร
รบ (
(
ร  ?,รร%ร5ร%ร:ร-ร1รร^ร รรฆร&รร
ร%รรรร<รยรร-ร`รยขร0รก:ร`รยชรWร:ร-ร1ร:ร-รข&รขยรร:ร1ร<ร รรรก:ร%ร รรรรรรรQรรรPร<รร&รยรร0รรรรฅรยร
( >
ร=รร:ร]ร[รรร`รร9รmรร:รยขรร=ร2รรรค@ร ร=ร:ร&รร>รWร*ร รยรร=รร รงรร รรร<รรยขร%รรข&ร%รรรรร รรรร&ร\ร%รรรรรร:ร-ร ร<ร\ร%ร&รรยร=รร %
รรร^รก:รรรร ร`ร&ร ร`ร-ร
รqรรข&รข&ร%ร9รร`รยซรรรoร-ร รร-รรรร<รรKรงรรร]ร<รร ร-ร\ร%ร ร[รยฅรข:ร-รข:ร
ร A@ร%ร-รงรB
ร mรร<รขยรร%รn
ร C
Dร-ร2ร<รรE
ร !#$#$F % ร<รรฆร`ร&รรฅร=รร9ร%ร=รฃ&รร-รรpร[ร-ร&รรก:ร-รร1ร%ร:รรmรก:รรรร"รรข&รข&ร%รรฃ9ร<รdร-ร`ร\ร รก:รร2รรรรรH
ร GWร ร	รร%ร`รก&ร:ร-ร%ร=ร<รร
รร,ร%ร&ร"ร	ร-ร2ร<ร-รร<ร&รdร=รฃ:ร-ร
รข ร<ร
รรร&ร-รmรร&ร`ร รยร,ร:รร,รข&ร%รร ร<รรdรI
ร GWร ร2+ร ร1ร%ร&ร1รรรรร"ร	รรWร`ร%รก&ร\ร รก:รร2ร<รgรรยร2ร7รร
ร1รก ร<รรร<รข รรรยชร :
ร ++  ร%รรฆร`ร<รร-รdรร2J
ร รก&รรรร<รร รรร/รร^ร รPร&รร
รร&รรร<ร รรยชร รก ร<รงรรรรร รpรรฌรร`ร0รก ร[รPรรงรรรฆร2ร
++  ร[รWร&รร&รฎรร`ร:ร
KMLONQPSRUT8VWQXYWEZ[รร<รรรรdร=รร:รgรยร รรยขร`ร&ร\9ร&ร-รร<รร รรKร:รรรร[]_^ รบ `a7รฑcbd e % cfรฒรต7gihdjรตO e % kยชรฉ รฝ &% ร
รร ร[ร^ร`รร&ร5ร%ร:ร-รยขร`ร&รรฆร9รก&ร1รยรร^ร-ร9:
l รร<ร&รยรขยรร&รรก ร<ร:รยฅรรฌรร`รdรยขรยรรร<ร9ร รข&ร%รรข:รร%รรรรรรoร-รรฅรรร1รข:รร&รรก ร<ร:รร

 รรรรงรรร ร2ร7รรร<ร ร%รร%รข&ร%รร"ร=รร:รร<รรร<รร:รร7รข&ร`รรข:รร`รรร<รร:รmรรWร-ร:ร-รงร1ร-ร:รยขร1รก ร<รรร<รข ร<ร^รรก&รร&รร\รร&รรรรรKร%ร&ร
,
 &
ร รร*รรรร รร0ร:รรรร']_^ รซ รบ +ca7รฑcb& e % ,\fรฒรตgmh&j[รตU e % + k
รฉ รฝ 9n +cfรฒรต7gihdjรตO e % + k&ร:ร,ร รยร`รKร[รรรก ร<รงรรรรร ร
ร%ร.+oa7รฑpbqe % ,2fรฒรต7gihdjรตO e % + kMรฉ รฝ  ร
]_^ รซmr รก:รรร7ร%รร&ร/ร%ร:ร-ร/ร`ร&รWร9รก&ร1รยรร7ร-รsl:รร<ร&ร1รข:รร&รรก ร<ร:ร ร[รรรรdรร2ร@ร
ร-ร:รยร:รรMร<ร9รรร1ร%ร&ร] รข:ร9ร%รgรรร ร<รรร<ร
รข:รร`รร-ร ร % รรร ร	รร%ร5ร-รรร<รรยร%ร:ร-ร1ร%ร&ร5ร9รก&ร1ร:รรpร-รl:รรรร&รรรข:รร&รรก ร<ร:รรฅรยร
รรรdรรร>รณ%รฒรฑ2รดรฏYj รทรฒรฏ	รฐ\รฏY9
t รฒ,รต/h&u.%v รฒรณWรฐqwU4
x รฒรตgmh&[j รตsXy รz@ ร(ร[ร"
 รก รรร`ร*ร=รร:ร]ร[รรร%รร ร/รร<ร%ร']_^ รซ ร%ร:ร-ร/รร2ร4รขยรร&รรก ร<ร:ร"l:ร
@ รข&ร%ร-รงร[ร รร"ร%ร&รQร%รรรร ร9รก&ร1ร:รร(ร-ร&รข:รร&รรก ร<ร:รxรยร(รรรdรรร %|{ ร%ร ร[ร>ร[ร(ร&รร(ร=รร:ร]ร[รรร%รร รยร*รรร`ร}]_

^ ร
/
~ ร<รร-รรร<รร-ร%ร&ร

รข&ร%ร&ร=รร%ร/รร ร1รก ร<รรร<รข ร<รร<ร&ร
รรก&รรร=ร%ร ร%ร/ร-รdร-รข&รข&ร`รรฃ ร<รdร-ร%รร=รร&ร&รร=รรร<รงร"ร ร9รรQร&รร/รข&ร%รรรรร%รงรWร%ร&ร,ร<ร9ร`รร:ร รร
ร<ร9ร`รร%รข&ร%รรร-รรรรรรยขร-รxร`ร&รรรฌรร`ร0รก ร[รรร

ร ร รยรร=รฃ:ร-ร
รข ร<รdร รรpรร:รรร%รรร%รร"ร-รPรก&ร:ร รร]ร<รร-ร ร<รpรรร ร%รรรร=รรร<รรรฆร:รรรรรรรรฆร%ร&ร
รรรร5ร-ร9รรรยรรรร1ร:รรงร
*
ร`ร&ร9รรรร1ร	รรรรข&รข&ร%รรฃ9ร<รdร-ร`รรรก:รร2ร<รรรdร-ร:รpร%ร&รWรข&ร%ร&ร=รร%ร/รร ร1รก ร<รรร<รข ร<รร<ร&รรค	รรก&รWร%ร"ร=ร2ร<ร1ร<ร:ร-ร%ร\ร=รร:รร<รรร<รร:รร
รข&ร%รรขยรร%รรร<รร:รรยรรรWร=รฃ4รขยรร=J
ร  รฉ รฝM( ร`ร"ร
รร-ร1ร%ร:ร-
ร ยจร[ร/รร<ร%ร ร<รdรรรร
รWร%ร-ร<รรรร:ร=ร รนรฝ ร-ร ( ร
ร"ร%รรรก&ร1ร<ร&
ร +678  ย ร ร%ร ร[ร ร[รร%ร&ร"ร%ร-ร
ร"รรร%รรร<ร&ร1ร%ร:ร-_
ร + -,./+ ยซร[รรร<ร%ร ร<ร รนรฝ ++  ร-ร ( 8+ "ร
	Wร^ร`ร&ร\รร%ร&รรร:ร-ร:ร>รยร%ร&ร\ร=รฃ&รข&ร%รร%รgรรรรqร`ร:ร-รWร%รรรรก ร<รร,ร รยขร0รก ร<รรร<รข ร<รรรร&รKรรก&ร,รยH
ร + -,./+ ยรฉ 
รฝ ( ++ \ร
รร ร[ร5ร%รร4รยร%ร:ร-
ร + [,.s+  ร[รpรร<ร%ร ร<ร รน รฝ @ร&รร รน รฝ 8+ ย % ร-ร ( ++  รยชรEรpรร^ร%รรรผร-ร:ร-รงรรQร%ร&ร
รร ยnรร%รร:ร=ร\รยรรgรรรรKร%ร&ร"รรรรยร<ร ร%รร%รข&ร%รรรรรร<รร:รWรร-รKร:ร0รgรรรร รรยรรร9รร
ยQรรร-รก:รรร\ร-ร7ร%ร ร[ร,รข&ร%รร ร<รร^ร>รร\รร-ร&ร&รรร%ร%รรรmร=รร:รร<รรร<รร:รร7รข&ร%รรข:รร%รรรรรร:รmรร,ร-ร&ร&ร%รรงรยรรรร<รร:ร"ร-ร:ร
ร<ร:รรร%รรร5ร:รรงร\รร&ร รร5ร%ร&รร รรรข&รรรรรรฅรรรรรรรงร\ร=รฃ&รข&ร%รร%ร]ร<รร:รQร<รKร%ร&รร[ร-ร&รรก:ร-รรย
ร 	ร ร=รรก&ร=รรรร รร"ร&ร-ร ร:รรงร
ยย

ยย7ย&ยยsย"ยBยยย$ย$ยยOยUยยยยยยยยย$ย

ยAย\ยmย+ย$ย5ยAยย`ยยยยกยqย`ย.ยmยข&ยqยยคยฃ`ย_ยAย/ยiยยยยฅยmยยคยฆย_ยAยย}ยงยจยฉย$ยช&ยซ+ย`ย1ยยซยซยญยฌ/ยยฎยAยจ|ยmยAยยฆ-ยช&ยฏSยฐยฒยฑยยiย}ยง&ยซ+ย*ยณยดcยณยดHยต[ย}ยถBย$ยฌ&ยซยคยฆ
ยซย+ยท$ย[ยAยยธย.ยย+ยข&ย|ยย+ยข>ยAยย[ยฃย$ยขdยยฅย`ยขdยqย+ย$ยข/ย'ยฌ/ยยฎยยฆยยถMยย`ยขยนยถBยยกย/ย$ยฆยบยยป$ยฌ/ยยซย+ยqยฏยบยยญยข>ยAยยยกยซยคยmยขย$ยฌ/ยiย$ย$ยดยฝยผIยmย}ยยซ+ยฏ$ยพ
ย+ยขยยถBย$ยจqยซยคยฆย5ยถยย`ยจAยยฟ+ยฟรยฅร รQร|ยฟ+ยฟ ร:รร
ร ร ยพEยถBย\ยถยmยข&ยรรIร รรรยฎยฟรยฅร รรรรรรยAยยฆ&ย`ยขย$ยยฉย2ยAยยรรรยจ|ยยฅยฃยqย+ย$ยขยmร_ยยซยญย`ย5ย`ยขdย|ย
ยAยmยยฎย8ย รรยฏยฅย+ยขยรรยฅร รรรยAย/ยmย.ยรยซ8ยยฎยรยAยiยqยยคยรรรยฏยธรIร รQรยดรรยขยบยถรยยฅยจqยซยคยฆยHยถยย`ยจAยยฟ+ยฟรยฅร รQร`ยฟ+ยฟ ร รรร ยพBยถBย\ยถยยiยขdย9รรย$ยจAยHยฌ&ยซยคย$ย
ยmรยAยย5รรยยฅยจAยรรรIร รรรยฎยฟรยฅร รรรรรรรBรJรรย$ยจ*รรIร รรร|ยฟรยฅร รQร|รรยธร
รJรยบยAยยช/ย'ยAยจAยฌย$ยด2รBยย`ยจยฉย*ยmยจยฉย*ยรยขdยฌยHยช/ย`ยจHยmร
ยถยยฏ7ยรยmรยฒย$ยฃ`ยฃย$ย5ยง&ยซoยยคยqย&ย+ยขย\ยAย&ยยคย`ยด
รBยยยถยยฏ}ยถBยIย|ยmยท$ยBยยคยBยงsย`ยจAย/ยmยง/ยBยขยยฅยรยยฉยยยรย+ย}ยง&ยซ+ยยqยยพยชยฌย
ย+ยBย+ยขdยยฉยจAย7ยฆ&ยฌ/ยฃยย
ย.ย$ยฃยฉย&ย+ยขย`ยจAยฏ}ยAย/ยiย"ยถBยยซoยซOยชsยยยยซ+ยง&รรยฌ&ยซรยซยคยmยยฉย`ยจยด"รBยยยช/ย$ยรยยคยฃยยยคยฆ&ยย_ยยคย
ยAย_ย.ยiยท$ยยAยยBย+ยข&ยAย`ยจAยงยจAย`ยยmยqย+ย$ยข}ยmร"รยย5ย$ยจAย
ยยฑยง&ยซoยยคยฃยฉย+ยยพOยqย5ยAย/ยmยBยถBยยฃ`ยmยข\ยยซยยญยรยยญยข/ยiยAย}ยฃย$ยข/ยฆยฅย+ยqย+ย$ยข/ยยซ"ยงยจAย$ยง/ยยฅยจAยqย+ย$ยข/ยBยช&ยฏ.ยHยฌ&ยซ+ยqย+ยง&ยซยยคยฃ`ยmยqย+ย$ยข[ยmยข/ยฆ'ยท$ย`ย`ยงยกยAยจ|ย$ยฃยฉยท
ยmรEยยซยซรกยAยยยฃย$ยข/ยยฎยยป$ยฌย`ยข/ยฃยยIยmร"ยฆ&ยmย+ยขย*ยqยsยด
ยต-ยยmย+ย$ยHยqย`ย.ยmยข&ยqยยคยฃ`ยยยฉย.ยAยยรขยซยคยmยขย$ยฌ/ยmย$ยHรฃEรครฅยชdยฏ\ยงยจAยmยยฅยยคยฆยฅยยญยขย2ย5ยAยจ|ยiยข/ยรยซยคยmยqย+ย$ยข\รรยจAย$ยรฆรรย$ยจAยHยฌ&ยซยคย$ย
ย+ยข[รฃEรค
ยAยIรรย$ยจAยHยฌ&ยซยคย$ยUย+ยข\ยIยซยคยmยขย$ยฌ/ยiย$ยรฃEรงSยถยยdยqยยยฎย`ย.ยmยข&ยqยยคยฃ`ยUยยคย
ย}ย$ยจAยยยยฅยรยยซยญยฏ'ยฆ&ยยAยฃยจqย+ยช/ยยฆQยดBรBยยBยซยคยmยขย$ยฌ/ยiย$ยJรฃ
รงรย8ย
ยยAยqย`ยข&ยqยยคยยซยซ+ยฏ}ยAยย
ยซยคยmยขย$ยฌ/ยmยยฅยยmรรกรqรจยยซ+ยงsย`ยจAยขOยพ/รฉรชยฅรช ร ร|ยพยAย/ยmย"ยฌ/ยqยยรยAยจAยฌยBยยปยฅยฌ/ยยซยยญยqยฏHยจ|ยmยยฉยย`ยจ"ยAย/ยmยข}ยmยงยงยจAยยฑ&ยยญย'ยmยAย
ยยปยฅยฌ/ยยซยยญยqยฏยธยถยย`ยขยยฃย$ย}ยง/ยiยจqย+ยขยยงยจยฉย$ยง/ย$ยจยฉยqย+ย$ยขยยฑ7ยงยจยฉยยAยรย+ย$ยข/ย`ยด[รซยกย$ยจAย'ยงยจAยยฃยฉยยคยqยยซ+ยฏ$ยพBยAยย\ยฆ&ยรฌ/ยข&ย+ยqย+ย$ยขยยmร_รฃEรงรย8ย
ยยคยฆ&ย`ยขdยยฎย8ยฃ`ยรยซยฒยAย5ยAยยยฆ&ยรฌ/ยข&ย+ยqย+ย$ยขยmรรรฃ
รคยธยmย+ย$ย`ยข'ย+ยขรญIยรฌ/ยข&ย+ยqย+ย$ยข[ยณยด+รฉยฅยพdยยฑ/ยฃย`ยงยยAย/ยiยรฎ
รฏ

ยถรย_ยฌ/ยยฎย ร

ยmยข/ยฆรฐยบย+ยข/ยqยAยยยฅยฆ\ยmรBรBรรยmยข/ยฆร
รAยพ

รฏ

ยถรย_ยยซยซ+ยmยถยนยยฉยยJยqย`ยBยmรUยงยจAย$ยง/ยยฅยจAยqย+ย$ยข'ยยฑ7ยงยจAยยยฉยรย+ย$ยข/ยBยAย9ย+ยข/ยฃยฉยซ+ยฌ/ยฆ&ย;ยmยจAยช&ย+ยAยจยmยจAยฏ}ยจAยยรยซรยข&ยฌย;ยชsย`ยจ|ยIรรยขยยฅยOรฑqยฌ/ยqย
ยจ|ยmยqย+ย$ยข/ยยซOยข&ยฌยHยช/ย`ยจ|ยรยพ
รฏ

ยถรยยฆ&ย5ยขย$ยยยซยซยญยmยถยฃย$ยข/ยฆยฅย+ยqย+ย$ยข/ยยซ"ยงยจAย$ยงsย$ยจAยqย+ย$ยข\ยยฑยงยจAยยAยรย+ย$ยข/ยรฒยพ
รฏ

ยถรย}ย$ยยฉยqยฌย}ย}ยAย/ยiย;รฃ รง
ยรยยฅย`ยจรยยฉยย_ยจAยยยซยคย`ยด

ย/ย$ย9ย2ยqยงsยยฃยฉยยคยยซBรรณยmยรยoยซ+ยฏ[ยiรรยรยiยจqยยคยmยช&ยซ+ยยรดรฒร|ยพQรรย$ยจ_รต ร

รฉยฅรถ|ยณรถรทรท`รทยพQย+ยข&ยAย`ยจAยงยจAย`ยยฉยยฆ

รBยย9ยรยmยจqยยคยmยช&ยซ+ย9รด`ร"ยยคยยฌ/ยยฎยยฆ\ยAย}ยยฑยง&ยซยยคยฃยฉยยญยยฎยซยญยฏยกย+ยข&ยAย`ยจAยงยจAย`ยIยAยยยmยงยงยจAยยฑ&ย+ย.ยmยAย_ยยปยฅยฌ/ยยซย+ย ยฏยฃย$ยขยขยยฃยqย+ย$ยยรขรBรรยmยข/ยฆ
ร
รAยดEรธยข/ยฃยMยAย&ยยคยUยยคย"ยฆ&ย$ยขย$ยพ$ยถBยBยฃ`ยmยข}ยAยรรยยซยญยฏHยHยฌ&ยซ+ยqย+ยง&ยซ+ยฏ;ยยฅยฌย"ยAยยยฃย$ยข/ยฆยฅย+ยqย+ย$ยข/ยยซยคย`ยพย$ยUยฆ&ยยAยฃยจqย+ยช/ยยฆ'ยmยช/ยmย$ย$ยด"รซยกย$ยจAย
ยงยจAยยฃยฉยยคยqยยซ+ยฏ$ยพQย`ย$ย`ยจAยฏ5รรย$ยจAยHยฌ&ยซยคยHรนรบ2รฃ
รคยฃ`ยmยข\ยช/ยHย$ยAยqยยฃยฉยยคยmยAยยฆ'ยถBย+ยAยย9รรย$ยจAยHยฌ&ยซยคยHรนรกรปIรบ2รฃ รง ยยฅย
รรยiยซoยซ+ยmยถย`รฎ
รฏ

ย`ย$ย`ยจAยฏ'ยงยจAย$ยง/ยยฅยจAยqย+ย$ยข'รรย$ยจAยHยฌ&ยซยคย}รผ}ร ร รผmรฝsย+ยข2รนยยคยรขรรยจAยยฃยฌยจ|ย ยยญยยฅยยซยญยฏsรBยจAย`ยง&ยซยคย$ยฃยยฆ\ยช&ยฏ*รผ9รพ[รผmรฝOรฐรด ร ยพ
รฏ

ย`ย$ย`ยจAยฏยธยงยจAย$ยง/ยยฅยจAยqย+ย$ยขยธรรย$ยจAยHยฌ&ยซยคยรผรฅรBร;รผ รฝ ย+ยขยรน>ยยคยรรรยจAยยฃยฌยจ|ย ยยญยยฅยยซยญยฏsรHยจAย`ยง&ยซยคย$ยฃยยฆยช&ยฏยยฉยย2ยฃย$ยขmรฑ ยฌยข/ยฃยยฎยยญยยฅยข
ร รผ_รพ[รผmรฝOรฐรดรฒร รzรฟ\ร รผmรฝรพ[รผ}รฐรดรฒร รยพ
รฏ

รฌ/ยข/ยยซยซยญยฏ$ยพQยฃย$ยข/ยฆยฅย+ยqย+ย$ยข/ยยซ"ยงยจAย$ยงsย$ยจAยqย+ย$ยข\ยยฑยงยจAยยAย ยยญยยฅยข/ยยmยจยฉย_ยยซoย+ยHย+ยข/ยmยยฉยยฆ2ยช&ยฏ'ย;ยฌ&ยซ+ยqย+ยง&ยซ+ยฏยฅยยญยขยยกย$ยฌยยด

ร ย&ยยคยHยAยจยmยข/ยรยซยคยmยqย+ย$ยขยยซยซ+ยรยถIยยฌ/ย9ยAยยกย`ยHยช/ยยฆรฃ รค ย+ยขdยยฉยSรฃ
รงEยด[รยยdยฌ/ย`ยพzรรย$ยจHยAยย'ยจAย`ย.ยย+ยข/ยฆ&ย`ยจHยmรยAยย'ยง/ยmยงsย`ยจยพ
B
ยถBย_ยจAย`ยdยmยจยฆ\รฃEรคย$ยBย}ยqยฌยช&ยซยคยmยขยยฅยฌ/ยmย$ยยiรรรฃ รง ยดEรBย&ยยคยBย`ยHยช/ยยฆยฆยฅย+ยขย2ยย$ยmยยคยฆยBยAยย_ยงยจยฉย$ยช&ยซ+ย`ย ย`ยข/ยฃย$ยฌยข&ยAย`ยจAยยฆ*ย+ยข
ยฐUยฑ/ยmย}ยง&ยซ+ยยณยดcยณยพ&ยช/ยยฃ`ยmยฌ/ยqย9ยถยย`ยข\ยถBยยHยฌ&ยซ+ยqย+ยง&ยซ+ยฏ2ยAย}ยฃยฉยซ+ยยmยจยฃยยฅยข/ยฆยฅยยญยยฎยยญยยฅยข/ยยซยฒยงยจยฉย$ยง/ย$ยจยฉยqย+ย$ยข/ย
ยAยย_ยAยmยซ+ย`ยจ|ยmยข/ยฃยยMยmยจAย
ยยฑยง&ยซoยยคยฃยฉย+ยยพOยmยข/ยฆ\ยยฎย.ยmยจAยยยซยคยqย5ย;ยฌ&ยซ+ยqย+ยง&ยซ+ยยฆ[ย$ยBยmยงยงยจยฉย$ยงยจqยยคยmยAย$ยด
รยยยยqย`ย.ยmยข&ยqยยคยฃ`ยรรรยยฅยจEรฃ รง ยยคยBยป$ยฌ&ย+ยAย_ยqยAยจยย+ย$ยdยยฎรรย$ยจยฉยถยยiยจ|ยฆQยพรยiยข/ยฆHยยคยBยรย+ยHยยซยคยmยจBยAยยAย/ยiยUยยญยขรqรจยยซ+ยงsย`ยจAยขOยพQรฉรช$รช ร รยด
ยต[ยHยmย+ย$ย.ยยฎย`ย.ยmยข&ยqยยคยฃ`ยIยAย2รฃEรงรฅย+ยข[ยAย`ยจAย'ยยmร 	
 ยพQย$ยจIรฌ/ยข&ย+ยAยHรฌ/ยจ|ยยฎย รย$ยจ|ยฆ&ย`ยจ_ย}ยยฆ&ยยซยคย`
ยด ย$ยจยmยข&ยฏ2ยข/ยiยAยฌยจ|ยยซ
ยข&ยฌย;ยชsย`
ยจ ยธยพQยซ+ย`
ย )ยฃย$ยข/ยรยยคยqย9ยmรยยซยซEยถBย$ยจqยซยคยฆย_ยถBย+ยAยยธยฆ&ย$ย.ยย+
ยข dรฉยฅรถรทรทรทAรถ dยดHรBย&ยฌ/ย`ยพQย+
ยข HยพOยถBยHย/ยย$ย
ย$ยขย_ยถBย$ยจqยซยคยฆ5รรย$ยจBยย$ยฃยฉย\ยง/ยdยAย ยยญยช&ยซ+ยรขยยญยข&ยAย`ยจAยงยจยฉย`ย|ยmยqย+ย$ยข\ยmร"ยAยย_ยqยฏ&ย;ยชsยmยซยคย
ย+
ยข ยยmย$ย`ยจBยAยย_ยฆ&ย$ย.ยรยยญ
ยข dรฉ$รถรทรทรทqรถ &ยด
 ย`
ย =รป_ยฆ&ย`ยขย$ยA
ย !"Hยด
ยผMยmยถยพQยฃย$ยข/ยรยยคยฆ&ย`ยจ}ยqย$ย}ยHยถBย$ยจqยซยค$
ยฆ #
รบ  รป9ยmย$ย`ยจIยAยย}ยฆ&ย$ย.ยย+

ยข % ร dรฉ$รถรทรทรท รถ dยพรยยฎย$ย}ยHยรยยซ+ยฌ/ยmยยฎยยญยยฅยข
& (รฎ '*)+% รรย$ยจยยฉยยยรยiยจqยยคยmยช&ยซ+ยยMยยญ$
ยข '2ยพsยmยข/ยฆ2ยqยยฅย}ย_ยAยmยซ+ย`ยจ|ยmยข/ยฃย9ย$ยยฃยAย$ยจ . , ยดEยต[ยHยรย+ยHยฌ&ยซยญยยmยขย`ย$ยฌ/ยรยซ+ยฏยยฅยAยรย+ย$ยข
ยAย9ยย$ยฃAย5ยงยจAย$ยง/ยยฅยจAยqย+ย$ยข'ยยฑ7ยงยจAยยยฉยรย+ย$ยข*รผ9ย_ยจAยยรยซรยข&ยฌย;ยชsย`"
ยจ /p
รผ 0214365 785	9 ยmยข/ยฆHยAย9ยย$ยฃAย5รรย$ยจAยHยฌ&ยซยค>
ย =;ย_ยยฉยจAยฌยAย}ยmยยซ+ยฌย

:<;

?A@

BCEDEFHGJILKGEMENOFHP>CEDEFQRCESETUIVEIXWYDEZEM[GJ\]
^`_bacedfhgjikfhlaYam"njopq`pst8r uvYwxm8gja!myEacEfzl|{2}~kgfhgYmy[a|cEfzย8fยkย8_baj_bmsยย}df`lmsยยi8{bfaf{bยgja}ยkยE}dยยยgjmย^`f
msย_baยa|cEfยยcEfdfsvยยยยยik}daj_2l~8{2}dhยยยA}dj_2}ย8{bfhg}d|f"_bย(a|fdiEdfafhยย~kgย_bยEยยqยยkacEf"a|m{bfd}ยklfยย}dj_2}ย8{bfhg>ยย
}dfย_bย(afd|iEdfafhย"~kgย_ยยEย"a|cEfam{bfd}ยklfhg!tยยha|cEfiEdfhยย_2l}afhgย}ยkยยlmsยkgja}ย(ag!}dfย_ยย8afdiEd|fafhย"~kgย_bยEยยoยย
acEfยยยm8m{bfh}ยยlmsยEยEfhla_ยยยfhg}ยkย-a|cEf>ยkdgjaยกยขmsdย8fdยฃs~k}ย(aj_ยคยkfdg6}dfยย8fยkยEfhย-_bยยacEfยgja}ยkยE}dยยyยฅ}ยgjc8_bmsย!ย
}ยkย$^6cEfย$_bย(afd|iEdfaj_bยEยxiEdmยikmsda_ยmยยfยฆ[iEd|fhggย_bmsยkgยYacEfยd|fh}O{ย(~Eยยยfdgยยง}ยยEยย_ยa_ยmยย!ยยงย~8{baj_bi8{ยค_2l}aj_bmsย!ย
}ยkยยจL}dfeย_bยsfย$acEf_bdยฉga}ยkยE}dยยยยfh}ย8_bยEยkv6ยยa>dfยR}O_bยkg6am_bย(a|fdiEdfa>iEdmsikmยdaj_bmsยxa|fdยRgvยยช`fhl}A{	{
ack}a`^`f>f{	_bย_bยk}a|flmsยkยย_baj_bmsยk}O{YiEdmsikmยdaj_bmsย-afdยRgยย8ยยย~8{ยa_ยi8{bยย_bยEยRms~EaOย[gma|ck}a^`f>ยEffhยยซamย8fh}O{
msย8{bย^`_bac~Eยklmยยkยย_ยa_ยmยยk}O{ยฉiEd|msikmsd|aj_bmsยa|fdยRgvยยยขyยยฌย_UgยญacEfRiEd|msikmsd|aj_bmsยfยฆ[iEd|fhggย_bmsยLยฎbยฎ ยฏยยฎbยฎ ยฐยฑ	ยฒ|ยณยตยดยตยดยตยด ยณ ยฐยฑยคยถ
nยขyยขmsd`ยทยยธ>ยนยทjยบยนยผยปhยปhยป8ยนยทยยฝsuย8acEfย
ยพ ยฌยฟ2ร2ร ยณ ร8ยณยคร ร รYร ร ยฝ รร nยร8ยธhpOยปhยปhยป|pรยยฝuรxร ยฝรร njopq รยพ ร ย ยฒรร ร8ยธhpOยปhยปhยป|p ร ย ยถ ร รยยฝhยฟยpstยr uzยฎ ร ยฏ"ร รร ยป
ร
ยฎยตร ยฎ รรร

ร 8c ~kgยร_ยคyยยฎยตร$ยฎ รรร ยYacEfRiEd|msikmsd|aj_bmsยfยฆEiEdfhggย_ยmยยLยฎbยฎ ยฏยยฎbยฎ ยฐยฑยคยฒยณยตยดยตยดยตยด ยณ ยฐยฑ ยถ ย8fยEmsa|fhgยa|cEfยyยขd}slaj_bmsยmyยacEf ร ยฝ
ร ยกยขa~Ei8{bfhgย_bยยซร ยฝ ack}ag}aj_2gยyยขยยยฏ"vยงรJmsdfยฆk}ยยi8{bfsย ยพ ยฎbยฎ4รยรsรรรยครJn ร p|รกkuยฎbยฎ ยฐOยฟ ร4ร ยณ ร[ยณ	ร<ร ร _2ga|cEf`yรd}slaj_bmsยยmyยย8msยR}O_bย
f{bfยยfย8agรยack}aย}d|fยlc8_ยค{2ย8dfย$myqยnยรกkuv
รข gย_bยEยยซms~Ed"fยยkfhยEยย_bยEย$myยรฃรครฅ_bย(amxรฃยรฆ`ยย^`fยEm^รงck}OยsfยgjfยR}ย8aj_2lg6yรmยdยรฃรค`vยรJmsd"รจรฉรรฃรค`ยย^`f
g}Oย$ack}aยnยopq`pย8rt uยญยฎ ร รจรช_ยครซLnjopq`pst8r uยฎ ร รจยงรฌv-ยยae_2ggmsยยfaj_bยยfhgรญ~kgjfyร~8{`_bยms~Ed"yยข~Ea~Edf-dfhgj~8{bageam
_bยklmsdiยmsd}af6ik}daj_2l~8{2}dย}O{b~EfhgYyยขmsdยacEfยa|m{bfd}ยklfhgร_bย(a|mยacEfยyยขmsdย~8{2}>รจ รฌ v ร c(~kgรย{bfaรจ ยพ tEr ยฟEdfiEdfhgjfย8a
acEfeyยขmsdย~8{2}ยack}adfhg~8{ยag6yรdmยยรรจ รฌ _ยคyzfh}sl|c$ยA}dj_2}ย8{bfยยรยย_2g>dfi8{2}slfhย$^`_bacย_bag>ยA}A{ย~Ef-}sllmsdยย_bยEยยซam
!rt ยEack}a_2gยยtยvยตรฎ
ร ย8i8_2l}O{ยค{ยยยซ^`fย}d|fย_bย8afdfhgjafhยย_bยยl|{bm(gjfhยยgjfย(a|fยklfhgยEack}aย_2gย8yยขmsdย~8{2}sg`^รฏ_ยa|cยซยEmyรd|ff"ยA}dj_2}ย8{bfhgv
ยยยack}a"l}sgjfsยย_ba>_2g"ยEmsa>ck}dยยamxgjcEm^รงack}a>acEfยA}O{b~k}a_ยmยยi8{2}hยEg"ยEmยซd|m{bfsv ร c8~kgยย_ยคyรฏรจ_2g"l|{bm(gjfhยยย
^`fย^ยd_ยa|fRnjopยtEr uยฉยฎ ร รจd}acEfda|ck}ยnยopq`pst8r uยฎ ร รจvยรร_bยk}O{ยค{bยsยย_ยคyรฐ"รฑรฒ}ยkยยซรจรณ}dfl|{bm(gjfhยยyยขmsdย~8{2}sgย
^`f"^ยdj_bafรฐ"รฑรดยฎ ร รจ_	yยnjopยtEr uรฏยฎ ร รฐ"รฑรต_bยยi8{ยค_bfhgnยopยtJr uรฏยฎ ร รจv
รถHรท4รถรดรธ-รน8รบHรปOรน8รน[รผรฝยรพรฟYรนUรน(รพ
 gY^zf`fยฆEi8{2}O_bยEfhย_bยยacEfย_bย8admEย8~klaj_bmsย!ย8^zf`ย_bยsfยgjfย-}ย(a_UlgYa|mยย8fยsdffhgYmyยยkf{ยค_bfyยงย(ยlmsยkgย_2ย8fdj_bยEยย}O{ยค{
^`msdj{2ยEgยmy gย_ f ร ameยkfยfhยฃย~k}O{ยค{bย{ยค_ sf{bยsยklmsยkยย_baj_bmsย8_bยEยยmsยยซรฐ"รฑยย8}ยkยacEfยRl|cEfhl s_bยEยacEfยiEdmยยk}ย8_ยค{	_bajย
m
y 	mยsfd`acEf"d|fhgj~8{baj_bยEยยiEdmsยk}ย8_ยค{ยค_bajยยยย_2gjadj_bยE~Eaj_bmsย!vยยยยยซa|cEf"iEdfยs_bms~kg>gjfhlaj_bmsย!ยJ^zf"ย8fยkยEfhยยซ^6ck}a`_ba
ยยfh}ยkg yรmยdz}ยgfย(afยklf>รจxa|mยยkf"ร g}a_Ugยยkfhยe_bยx}>^zmยdj{2ยยmy!gย_ f ร ~kgย_ยยEยย}รญam{bfd}ยklf>ยsfhlamsde!rt 
v ย_bยsfย
ร
 nยรจยuยamยยf"acEf"ย(~Eยยยfdยmyยง^`msdj{2ยEgย_bย  g~klcยซack}anjopยtkr uzยฎ รจv

}
k
ย

ย
!
t
r
(
ย
`
^
ย
f
8
ย

f
k
ย
E
ย

f






<

ยค
ร

ร

ร
ร
_bยklf^zfย}df"a} ย_bยEยย}O{ยค{ยง^`msd{UยEgรฏamยยkfefhยฃs~k}A{	{bยย{	_ sf{bยsยยacEfย8fยsdffemy ยยf{ยค_ยfyย_b
ย 	ย_ยยยfยรฐ"รฑX^`_bac
dfhgjiยfhlaa|
m   }ยkยรฅrt _2g
<รยคร ร ร n 	"!xรฐ"รฑยu
 d ร ร n 	ยยฎรรฐ"รฑ"u
ยป
ร
ร
<รยคร ร nรฐ"รฑยu

ยยขy#รยคร ร ร nรฐ"รฑยu ร%$ ยEac8_2gยย8fยsd|ffยmyยยkf{ยค_bfy_2g6ยEmsa`^zf{ยค{bยกยย8fยkยEfhยยv
ร cEfl}dfyยข~8{dfh}sย8fd>ยR}Oยยซck}OยsfยยEmยaj_2lfhย}ยiยmsafย8aj_2}O{iEdmsย8{bfย ^`_bac$ac8_2g"ย8fยkย8_baj_bmsย!v  adj_2laj{bย
gjiยfh} s_bยEยkยย^`fยgjcEms~8{2ยยซ^6dj_ba&
f   n 'ยuยd}acEfdยa|ck}
ย   ยkgย_bยklfacEfยgjfa6my ^`msd{UยEgรฏ~Eยkย8fdยฉlmsยkgย_Uย8fd|ยก
}aj_bmsยยซl|{bfh}dj{bยxย8fikfยkยEg6msย-acEf"ยsmEl}ยE~8{2}dยsv (6fยklfsยEacEf"ย8~Eยยkfd`myยง^`msd{UยEgz_ย
ย   }O{2gjmยย8fikfยkยEg
ร
ร


ร
ร
ร
msย-acEf>ยsm[l}ยE~8{U}dยsv c8~kgย8ยkmsa|c<รยคร n 	ยuย}ยkย)<รยคร n 	*!ยรฐ"รฑeuย8fikfยkยยซmยยRacEf"l|cEm_2lf

+-,/.1032546257892;:50=<#4>09?@257A4B2509CD4EF8GAH4:6I ยฑ <68JLKM4NDE5EF892OND03GM8PCQ@ND2;ND:R?03E257-ND:SE5498:50=G257M82TU4V8PCWCD09TU4 XNDE5EF892OND03GM8PC
GZYA<;KM4E5:[NDG]\AE503\0=E52OND03G&4_^A\AE54:5:OND03GA:`NDG]aUbU,
ced

f6gihkjlnmRo#pqArslsgt@m`uwvhxqAqylsg

z{&|>}~xzs9ย9ยย/ยยย3ยยยsย`ย9ยkยยยkยZย/ยZย/ยkยZย/ย3ยยยZยZยย/ย3ย3ยยยzsยย=ยยLย5{|ย6ยย|VยRย9ยยZยยย9ยยZ9ยยยยย3zsย/ยPย=ยยkยยย
ยฎ ยฑ ยRยณ#ยดยตยyยฃยถยคFยฅ ยฆ5ยงยจยฉZยชยทยฌยญ ยฏยฐL
ยฎ ยฑ ยRยณ=}
ยย/ยยยย9ย/ยย#{5zs]ยyยยย1{5zs9ยยkยยsย#ยยzยsยZ6ยยยยยยกziยZยยขยkยยยย|Vย@ยฃ*ยคFยฅ ยWยฆ5ยงยจยฉยซยชยยฌยญยฏยฐยฒ
ยฐ

ยฑ
6
ยฟ
ร
ร
]
ร
ร
ยธ ยkยยL9ยAยยkยยAย[{59zsยยบยนBยkยยยยยปยยยL{5zยยยzMยน>ย&ยย/ยย&ย9ยย*ยkยZยผs9ยZย*z{6ยข/ย3ยยยย3{ยฒยฝยพ ยฏ ยฎ
ยณ>ยยยยฒยย/ยkยZย/ยZย/ยkยZยkยz{

ยฑ
ร
zsยVยยzยย3ย&z{
ยszยZยยขยkยย9ยsยiยยยBย9zยsยAยยย
9zยsย6ยZย>ยyยF}ย@รAรsรยกรยขUยณ=}
ยธ ยkยkยยZยyยยยยยsยBยน#ย)รeยzยนรยย3ยย9ยยZ)รรยzsยย
รร ย3ร/ยsย3ยยยs}รร6ยยยVยน#ยรรeยzยนwยยย9ย/ยยยถรรยยยยยยย9ยผยกยAยรยย/ย
ย9ย/ยยรร
รร ยยรยZยยยถยyยยยย} ยธ ยkย/ยZย6ยน#ยยยน#zsยkยยรยยทยรsยยปย9zรย3ยรsยzยกย ยฌsรรยฉ9ร3รรยจรรร9รyยชDร5ร5ร ยย ยฟ ยผยยยยกยZย ร]ร ย9z
ยข/ยรยยยย ยฏMร ร ยยยย ยฐ ร]ร ยฝยพ ยฏยฐ ยฎ ยฑ ยฟ6รรร]ร ยณ=}ยตรBzsยยย3ยรย9ย/ยย*ย9ยยzs3ยkยZยถz{&ยยยยยน#zยยยยยยยยย3ย)zยsยZย%
รร ยย/ย%ร
ยฎ
ยยยยย/zsยฎ ย=ยยkยA}รย5{ย9ยย)ยยยยยยรยยยยย ยฏAร ร ยยย/ยAย9ยAยรยยsยยyย6ย9ยยZย%ยน#ยยยนSzยกยkยยยรยผeยMยยย%ยzsยยkยยยยผยยขkยยย/ย ยยยยผ
ยฎ
ยยย9zyรkยยยถยย9ย6ยAรยกย/ยyยยยยยsยiย ยยย/ย3ย]ยยย6ยฎ 9ยAยยkยย#ยน#zsยkยยยขnยVยAรยกยkยยMยyยยZยkย6ย9z&ย9ยAยยยยยผยยยย9zyรeยยยถยยย6ยAรsย/ยMยยทยยย
ยsย#ย3ร/ยsย3ย6ยAรยกย/ยyยยยยยs}
ยธ ยkยยย]ยkย3รก/ยkยยยzsย@ย[ยzยนSยZยยกยZAยยย>ยzsย&ยPยkรขรฃยยยZยeยAรคUยยย&ยยยยยย&ย*ยAย)ยzยกยVย3รkยยยA}]รฅรยzsยข/ยPยZ9ยsยAยยยข/zยsย
ย9ย/ยย)ยฝR ยฏยฐ ยฎ ยฑ ยฟ6รรร]ร ยณ&ยย*ยzsย)ยyยยน6ยAยยยนSย3ยยยรฆFยkย3รก/ยยAยU}รงยFย%ย/ยยยย3ยkยยยAย#ยย)ยยถยyยยยข/ยยย9ยยรยZยsยPยยย/ยย{5zs
ย3ยZ9ย=ยMยยย%ยยyยยยAย*z{@รร ย6ยฝR ยฏยฐ ยฎ ยฑ ยฟ6รรร]ร ยณยฒยยย*ยzยกยยถยน#ย3ยยยรฆFยkย3รก/ยยAย%{5zsยถยยขkยยย=ยยยยยยยย9ยผยกยรย}%ยFย%zs=ยkยZ*ย9z
ยkยAยyย&ยน#ยย9ย%ย9ยkยย*ย9zยกยขkยยยZยรจz{ยยน#ย3ยยยยรฆFยkย3รก/ยยAยkยยAยยZย]ยน#ยรยkย3รก/ยย ร]ร ย9zยยข/ย ร-รฉMร-รชรฌรซOรญรฎsยชยชรรฏยรฐ=ยจรช/ยญ3รยญZรซ5รZรชUรซ ยย{
{5zsรฃยMยยทย&ยยkรขรฃยยยZยkยยยรฑยย*ยyยยย%
รร ยย/ยยยยkรข)ยยยยZยkยยยรยย9ยผsยยรรย>ยฃ ยงยจยฉยซยชยยฌยญ ยฏยฐ ยฎ ยฑ9ร]ร ยณรฒรดรณ}รตร>ยzsยยผรzยกย9ยยZ
ย9ยkยยยผeยZยยZยยกยZยeย9ย/ยMยxย3zsย/ย ยยยPย9ยZย/ย3ยLยยยยkยยทยยAยยพย9ย/ยย`ย9ยย ร]ร ยยRย9ยยยย_รกรฌยยขkยย;ยยรก/ยkยย9ย6ยkzsยยถยyยย/ย`z{รฌย9ยขkยย9=ยยยยย
ยย9ยผsย*ย_ยรถZยs}รฃ~xzs]ย3ร/ยยยkยยsยRย ร]ร ยย=ยยยยยผรย9ย/ยยยถย3ย9ยยZยยถย9ยย3ร/ยsย3ยยยยรทยถยkzsยยถยMยยยยย3ยยZยยZยkย=ย9ย*ยยยฒยzsย
ยZยsยZยkย9ย/ยyยยยยย3zsย/ย_ยยย9ยZยkยA}~xzs6ย9ยย]ยZยยถยyยย/ยkยZVz{1ย9ยย&ย/ยย/ยZAยยน#ย&ยsยยยยย]ย9ย/ยย6ยyยยย1รkยzยน#ยยAยkยผsย&ยข/ยยกยยAย
ย9ยยยZยsยZยkย9ย/ยyยยยยรตย3zsย/ย_ยยย9ยZยkยA}รย_ยรตย=ยsย3ยPยยย3ยsยBยน#ยย3รย/ยAย3ยรยZยsยZยkย9ย/ยyย&ย3zsย/ย_ยยย9ยZย/ย3ย%ย9zยยข/ยรยzยย/ย=ยkยZ
ย9zรยยยAย9รยย9ย/ยยยย3zsย/ย_ยยย9ยZย/ย3ยs}ยรฅยปยยkzรยzsยย3รย/ยAย3ยยถยรธรeยzยน#ยยAยkยผsยรยข/ยsยย)ย9zยยkยยsย3ยยขnzsยย/ยยzsยยย9ยย
ยkzsยยถยMยยยยย_ยรถZยsยRย3ร/ย3ยZยยยน6ยยZยรยยยยถยขnzsยย/ยยปยย&ยAยsยยกยยยยยยยยย/ยยZยeยA})~xzs&ย9ยzkยยยถยย/ยยยยย_รก/ยAยยยน#ยย9ยยย9ยkยย
ยยeยยkยยยPยยzยกย@ยRยยยยยyยยzยยnzeย9ย_ยยขkยยย9zรรก/ย/ยยป{5zs9ยยถยMยย3zsย/ยยกยยยzsย/ยยZย/ยยยยยผรยZยsยZยkย9ย/ยyยVย3zsย/ย ยยยPย9ยZย/ย3ยs}ย~zs
ยย/ยย=ยย/ย3ยsยยยยย*ย/zkย9ย_ยยขkยยรย9zรยยzยนรนย9ย/ยยยยย){5zยยยยzยน#ยยยผยย3zsย/ยยกยยยzsย/ยรย9ยยยkรข)ยยยยZยkย)ย9zยยผsย/ย=ยยeย9ยZย
ย9ย/ยย ร]ร ยย#ยZยsยZยkย9ย/ยyยยยย)ย3zsย/ย_ยยย9ยZยkยAย ยฑ ยkยณ ร]ร ยkzkยAย#ยzsย#ย/ยย]ยยeยยzsยรฆ5ยย/ย9ยยยAยยกยยยZยย9ยAยZยkยย/ยยย/ยยกยยยผ
ยAรยกย/ยyยยยยยยยข/ยZยยน#ยZยZยย9ยZ9ยยถย`ยย/ย ยฑ ยขUยณ ร]ร ยยย3zsย/ย ยยยPย9ยZยeย`{5zs ยญยซยจรบร ยkzsยยถยyยยย_ยรถZย#ยน6ยยZยยถยyยยย/ยยย9zyรkยยย*ยย9ย
ย3zsยย/ยยยzsย/ย>ย9ย&ยZยkยยยยกย3ยAยยขkยย3ร/ยsย3ยVย3zยกยย/ยยยzsย/ย-}]รปยย/ย3ยยน#ยยย3zsย/ย3ยZยkย9=ยย9ย&zsยรยย/ย9ย*ยยยยผsย/ยยผsยAย
ยยย9ยkยยBย/ยย/ยZyยย9ยkยย6ยAยยkยยVย3zยsยZ=ย#ยzkยย6ยZยsยยAย#z{`ยยeยยZ9ยAยยA}
รผ ยsยZยรยย{ ร]ร ยยยฒยZยsยZยeยย/ยyยยยยยยย3zsย/ย_ยยย9ยZยkยAย@ยยยยยยยยยยยยยถยyยยยzsย]ย3รkยยยA}ยถ~xzs]ย3ร/ยยยkยยsย[ยยยLยยถยyยยขnย
ย9ยย]ยZยsยย>ย9ย/ยยยฝยพ ยฏยฐ ยฎ ยฑยฟ6รรร]ร ยณRzeย9ยยยยยยยยย9ยAย#ยข/ยZยยน#ยZยZย)รฝรพ)ร3รฟ
ยย/ย*
รฝ *ร3รฟ`{ zs#ยzsยย  ยsย#รรนยผsยZย3ย`ยย9ยผsยs}`ย_ย
ย9ยkยย>ยZยsยยsยx{ zs>ยยeย*ย/ย9ยPยยย3ยkยยร@รร ยxย9ยย]ยยยยยยยยsยBรยบยผยก9zยนVย#ยน#ยยยยย
ยzยกยVย3รkยยยA
} BzMยน#ยZยsยZyยsยย]ยยZยZยยถย>ยsย;ยย{
ย9ยย>ยยยยยย ยยกย)
รร ยผs9zยนVย#ยยยถยMยยทย1ยยzยกยkยยยUยxยยย9ยkยย6ยZยยกยยsยยข/ย&รฝ;ย/ย_ยย/ย3ยLย9ยย]zeย9ยยยยยยยยยzsย/ย]ยยขnzsยยVรฝยปยผszย9zยถรณ}
รฅรย&ยyยszยยรยย/ย9ย)ยzsยขkยยZยยถย6ยขkย)ย3zsย/ย_ยยkยZยยยผ)ย9ยย ยชรรOรบรยญ3รญ  ยย/ย ยชรรรบยรรชyร ยk=ยยยยZ6ย9ย/ยย)ย9ยยยฒยยยยยยยยy}6~zs
ยยkยรฃยPยZ
ย 
	 ยยยย]ยยkรก/ยยย z
{ Sยiยยยk
{ Sยยกยย6ยยย&ยผs9ยAยย9ยAยย;ยzMยน#ยZ6ยขnzsยย/ยรz
{ S} ยธ ยย ยชรรรบรรOรชAร z{ย
ยยAรยกยยZย/ย3ย]ยยBย9ยย>ยยยยยยยย]z{
ย9ยย>ยยkรก/ยยยยถยZรคnย9ย/ยย;ยยZย
ยยยยฐ ยยยยk{ ยฐ ยด ยฐ ยยยยย ยยk{ รฟ ย  รฒยร
ร]
ร 
ร]ร

ยธ ยย#ยยยยรงยยk{1ย3รkยยย=ย`{5zs#ยยeยยยAรยกยยZย/ย3ย>ยข/zsยย/ยkยAย{59zsยรยขnย3ยยzยน&ยkยZยsยZยยย{@ย9ยย6ยยยยยย6ยkzeยAย;ยzsยy} ยธ ยย ยชรรรบ
ยญ3รญ  ยยยBยkย3รก/ยยAยยย/ยyยzsยผยกzsย/ย_ยยsยยน6ยยZ9ย&ยPย
ย รยkยZยzsย9ยAย#ยยย6ยยAยsยยBยยย/ยZ>ยข/zsยย/ย*z
{ S}Rย {@ยยยยย ยฐ ร]ร ยฐ
ยฐ
ยฐ
ยฐ
ยฐ
ยฐ
ยฐ
ยkzkยAยย3รeยยยyยย9ยยZยยยยยย ร]ร
ยดรยยยยยยยk{ ร]ร
ยดรงยยยยรฑยPยย ร]ร
}รรปยย/ย3ยsย;{ zsยยkย @รร ย;ย9 ยย
ยยAรยกยยZย/ย3ยยฝยพ ยฏยฐ ยฎ ยฑ ยฟ6รรร]ร ยณ>ยยยยyย ยนยyยยLยข/zsยย/ยkยAย { 9zยกย ยยข/zยsย)ยย/ยยยข/ย3ยzยน& ย;ย9ยย*ยยยยยรจยยยยยย/ยยปยยยย ยยk{
ยyยยน6ยyยiยUย3รkยยยA} ยธ ยeย/ย-ยAยน#ยSยkzBยzsย@ย/ยyยsยRย9z6ยน#zs9ยVยยขnzsยย1ยยยRย9zsยขkยยZยยตz{iยzsยย3รkยยย9ยZย/ย3ย;{5zs@ย/ย9ยPยยย3ยkยย
ยMยMยยยยAยBz{&@รร }Rรฅรย&ยZยยรยzยนรฑยยAยยZยeยBย9ยย]รก/ย/ยyย[{ zsย z{
zsย>ยkย3รก/ยkยยยPยยzยกย@}

!#"%$!'&)(#*,+

ย{

ยย ยยย ยย ยยฐ ยยยยk{ ยฝยพ ยฏยฐ ยฎ ยฑ ยฟ6รยทร]ร ยณรฑยย/ย
ร ร
]
ยฏMยฎ ร ร ยฎ

-.

ยทย ยย ยย ยยยฐ ย%ยยย ยฝยพ ยฏยฐ ยฎ ยฑ ยฟ6รยทร]ร ยณ
ยฏMยฎ ร ร ยฎ
ร ร
]

/103234)57698:53;3<=4)>?03234A@B03C3D%61E36GF23H3;I57JLK
MONPRQTSVUWYXZP\[L]O^[L_RS\S`aO[=bdc!PeQ3Sf]gPeQ3SihjlknmojVjpZqBrejsutvjvq\txwzy{kntY|Ljfwz}~c,_oWยPePRSf]zยย_ยยยยlyยu}~ยยยc7W%X
^ยSVยO]3S^T[ยXPRQ3SยยVNยยNย]ยbยWยยยWยP=ย,NยPRQ3Sf_RWYXZS\ยย_ยยยยlyยu}~ยย^ยNSXย]3NยPSVUยW%XoPย
ย S\ยebยNยXZSยPRQยWYXยXZSยVPoWยN]gยWยPeQg[ยยvSf9_eSfยB[L_Rย3XNย]ยNยa3_ย^ยSVยO]ยW#PZW#Nย]!ยยยW#_ยXZP?]3NยPRSPRQO[LP=c3SfยยSf]ยaOXlWย]3ย
PRQยWYXย^ยSVยO]ยW#PZW#Nย]!c1PeQ3Sf_RSB[L_RSยยB[L]ยยกยขยf[XZSXยQ3Sf_RSPRQ3Sยฃ^ยSfยย_RSfSNLยMOSVbยW#SVย\^ยNSXย]3NยPยSVUWYXZPยยยคNLSfยยSf_c
[ยXXoNยยSNLยยฅNa3_ยbY[LPRSf_SVUO[Lยยฆยb#SXXZQ3NL\cยW#]:ยB[L]ยยก
XlWยPeaO[LPZW#Nย]OXยPRQ3Sยฃ]3N]3SVUWYXZPRSf]OยVSTNLย[z^ยSfยย_RSfSยฃNLย
MOSVbยW#SVยiยf[L]ยงMยจSga3]O^ยSf_VXZPRNยNI^W#]ยPRaยW#PZW#ยยSVb#ยกGยvยvNย_ยWย]OXoPย[L]OยVSยcXZSfSzยฉUO[Lยยฆยb#STยชOยยฌยซA[L]O^PRQ3SzXZa3MOXZS`a3Sf]P
^WYXRยVaOXRXยญW#Nย],ยVย ย STยVNยaยbY^,c[=b#PRSf_R]O[LPZW#ยยยฐ SVb#ยกยc?QO[=ยยSTPย[LยยSf]PRQ3Sz^ยSfยย_RSfSTNLย\MยจSVbยฎW#SVยBยฐ PeNยฏMยจSgPeQ3SยW#]ยPRSf_RยL[=b
^ยSVยO]3S^ยMยกbยW#ยAยฑยฐ ยฒ ยณยฐ bยฎW#ยยดWย]ยยfยต ยฒ ย ยย_ ยฑยต ยlyยu}~ยย[L]O^bยW#ยAยฑยฐ ยฒ ยณยฐ bยฎW#ยยงXZa3ยฆ ยต ยฒ ย ยย_ ยฑยต ยlyยu}~ยยยc3ยฆ3_RNLยWY^ยS^
S[ยยeQBNnยยถPRQ3SfยยทSVUยWYXZPย1ยธQยWYXNยaยbY^QO[=ยยS?MOSfSf]ย[ยยฆOSf_oยยนSยVPZb#ยกยฃ_RS[XZNย]O[LMยb#SยRQ3NLWYยVSยยยจยNXZPNLยยถPRQ3S_eSXZaยb#PยX
SยXZPย[LPeSยฃNaยb%^AยNzPRQ3_RNยa3ยQ:W#PRQยบยยSf_eยกzbยW#PRPZb#SgยeQO[L]3ยยSยปWยยยฅSยฃQO[ย^ยดPย[LยยSf]:PRQยWYX^ยSVยO]ยW#PZW#Nย]!ยยฝยผa3_
^ยSVยO]ยW#PZW#Nย]zXยญW#ยยฆยbยWยยOSX?PRQ3S\SVU3ยฆONXlWยPoWยN]gXยญbยW#ยยQยPZb#ยกยย
ยยW#]O[=bยb#ยกยc3S?_RSfยB[L_RยยPRQO[LPยW#PยB[=ยกBXZSfSfยยพa3]3_RS[XZNย]O[LMยb#SPRNยPย[LยยSยฟbยWยยยWยPVXWยยรS?ย]3NLยบPeQ3S\^ยNยยB[=W#]
XยญW#รfS\N_QO[=ยยS\[ยMONยa3]O^ยNย]ยฃPRQ3S\^ยNยB[=W#]gXยญW#รfSยยรยbยS[n_Zb#ยกยc3Wยย1Sย]3NLรรร[L]O^ยดร ร c3PRQ3Sf]ยปW#PยXZSfSfยBXยยNย_RS
_RS[ยXoNย]O[LMยb#SPRNยaOXoSยยย_ ยฑยต ยฐ _ย[LPeQ3Sf_ยถPRQO[n]ยยย_ยยยฝ[ยX!Na3_ร^ยSfยย_eSfSNnย)MOSVbยW#SVยRยยรd]O^ยSfS^,c3[ยXXZQ3NL]ยW#]BยZร[ยfยRQยaOX
SfPย[bรย%cร=รยรLยช3ยยc,ย[L]ยกTNLยฐ ยPRQ3SยW#ยยฆON_RPย[L]ยPยฆ3_RNยยฆยจSf_RPZW#SXยPRQO[LPQ3Nnb%^ยขยvNย_\PRQ3Sยฃ^ยSfยย_RSfSยNLยMOSVbยW#SVยย^ยSVยO]3S^
Mยยกยฏยย_ยยรQ3NLbY^ยvNย_ยย_ ยฑยต cยยนN_ย[bยฎbยยeQ3NLWYยVSXยNLยยฅรร[L]O^ยฝร ร ยยฏยธQ3SยฃยVNย]3]3SยVPZW#Nย]PRNยยB[UW#ยยa3ยรSf]ยPR_RNยยฆยยก
PRQO[LPSยยB[LยSW#]ยฏPeQยW%X?ยฆO[nยฆOSf_\Q3Nnb%^3XรNย]ยb#ยกz[LPPRQ3SยbยWยยยWยP=cยถM3a3PยMOSยf[LaOXoSยฃยl[ยXยฟNยa3_\ยฆ3_eNNLยdX?XZQ3NLยPRQ3S
ยVNย]ยยยSf_RยยSf]OยVSWYXย_V[LยฆยWY^,cยPRQ3S^ยSfยย_RSfSNLย,MOSVbยW#SVยรยย_VยยZyยu}~\ยยจWYXยPZยกยฆยWYยf[=bยb#ยกB[ยฟยยSf_RยกยยยNN3^ย[Lยฆ3ยฆ3_RN=UยWยย[LPZW#Nย]
PRNBยย_ ยฑยต ยฐ ยlyยยฎ}~\ยยcยSfยยSf]ยปยยนNย_ยN3^ยSf_ย[LPeSVbยยกbY[L_eยยS\รร[L]O^ยปยN3^ยSf_ย[LPRSVb#ยกยฃXZยB[=bยbร ร ย
รยรรzร7รยถรยรยจรOรร,รยรร7รvรdร3รยร)รรGรOรยรกยรยร!รขยรฃ
รค!รฅvรฆยฝรงLรจOรฉรช=รซ!รฌยรญรฏรฎLรฉรฐ%รซ!รจรฉfรซ:รฑgรฒ3รณรฐvรฑzรญรฑรตรดยจรจOรฉรช=รซ)รถรฏรท
ยธQ3SยปW%^ยS[NLยยB[=UยW#ยยW#รVWย]3ยรjรธw)รนxmRpVรบOรป:QO[ยXยยฆยb%[=ยกยS^[L]:W#ยยฆยจNย_RPย[n]P_eNLb#SยฃW#]ยบยB[n]ยกยOSVbY^3XfcW#]Oยeb#aO^W#]3ย
PRQ3SXZPRaO^ยยกTNLย1ยฆ3_eNยMO[LMยWยbยW%XoPZWYยยNI^ยSVbYXยฅยยนNย_ยฟW#]ยยยนSf_e_ZW#]3ยยข^ยSfย_RSfSX?NLยMยจSVbยWยSVยยยlรผย[=ยก]3SXรธcยรรยรฝยรพยรฟยQO[n]3]3Nย
]
ย S[=ยยSf_cยร=รLยชรยยยยยรd]PRQ3SBXยญW#ยยฆยb#SXZPยXZSfPRPZW#]3ยOcยSยf[L]TยยW#SfยพSf]Pe_RNยยฆยยกย[ยX\[_RS[b vยL[=b#a3S^ยยนa3]OยVPoWยN]zNย]
ยO]ยW#PRSยฆ3_RNMO[LMยWยbยฎW#PZยกBXZยฆO[ยVSXfยรvย WYX1[?ยO]ยW#PRS?XZSfP1[L]O^ WYX1[?ยฆ3_eNยMO[LMยWยbยWยPZยกยยS[ยXZa3_RSNย
] รcLPRQ3SSf]ยPR_RNยยฆยยก
ย





ย
%
W
?
X
ย
^
V
S
O
ย
3
]

S
ย
^
R
P
ย
N
O
M
S













ย




ย
#
b
]



ย



ยฟ
ย
d
ย


ย
S
ย
P
n
[
ย
ย
S



#
b
]





ย

ย
ย
ย
	
ยผ]3S?XZPย[L]O^3[L_V^ย[nยฆ3ยฆยbยฎWYยf[LPoWยN]BNLย,Sf]Pe_RNยยฆยยกWYXยPRQ3SยยยนNLbยb#NLW#]3ยOยรฟยa3ยฆ3ยฆยจNXZSSยย]3NL:PRQ3SXZยฆO[ยยVS cM3a3P
QO[=ยยS\N]ยbยยกยยฆO[L_RPZWY[=bยWย]ยยvNย_Rย[LPZW#Nย]z[LMONa3P ยcยจSVUIยฆ3_RSXeXZS^ยปWย]PRQ3SยvNย_RยยทNnยยVNย]OXZPR_ย[Wย]ยPยXfย?ย3Nย_ยฟSVU7[LยยยฆยbยSยc
SยยW#ยยQยPยQO[ยSB[ยยVNย]OXoPR_ย[=W#]ยPยXZaOยeQ
[X ย =ย  ย !Lย "$ร #ยซ3
ย %ยฟbยPeQ3Nยa3ยยQ:PRQ3Sf_RSยยB[=ยกgMยจSBย[L]ยก
ยS[ยXoa3_RS&X ยฝPRQO[LPยฃ[L_RSzยVNย]OXlW%XoPRSf]PW#PRQยงQO[LPSTย]3NL\cPRQ3SยรบOmVtx(w 'ftuรบ)sยjTpZ*
q )&+$,-t )./) jรธw)รนxmRpVรบOรป
XZa3ยยยSXZPยXยPRQO[nPBSย[ย^ยNยยฆ3PPRQO[L0
P 21ยปQยWYยRQยงQO[ยXยPRQ3Sยปb%[n_RยยSXZPSf]Pe_RNยยฆยยก
[LยยNย]3ย
[bยฎbPRQ3STยVNย]OXยญWYXZPRSf]ยP
ยฆONยXRXยญW#MยWยbยฎW#PZW#SXf4
ย 3?XยญW#]3ยzPRQ3Sย[nยฆ3ยฆ3_RNยยฆ3_ZWY[LPRSยฃ^ยSVยO]ยW#PZW#Nย]OXfcW#PBยf[L]:MยจSยXZQ3NL]:PRQO[LPยPRQ3Sf_eSWYX[gXoSf]OXZSW#]
QยWYยeQzPRQยWYX  1 W#]OยVNย_eยฆONย_ย[nPRSX?PRQ3
S 5eb#S[ยXZ7P 6ยฃ[^3^WยPoWยN]O[=bยW#]ยยยนNย_eยB[LPZW#Nย]AยRรฟยQO[L]3]3N8
] ย S[=ยยSf_cรรLยชรยยVย
ย3N_1SVUO[Lยยฆยb#SยcยWยยยถSQO[=ยยS?]3NยยVN]OXZPR_ย[=W#]ยPยXยN
] ยcยPRQ3Sf
]  1 Wยbยb!MOSPRQ3SยยS[ยXZa3_RS?PRQO[LP[ยXRXยญW#ยย]OXS`aO[=b
ยฆ3_RNยMO[nMยWยฎbยW#PZยกzPRNย[=bยb1SVb#SfยSf]ยPยXยNL9ย ร&ย :Nยa3ยยQยb#ยกzXZยฆOS[nยยW#]3ยO;c  1 [ยXRXlWยย]OX?ยฆ3_RNยMO[LMยWยbยW#PZW#SX[ยX?S`aO[=bยbยยกA[ยX
ยฆONยXRXยญW#Mยb#SยยnWยยSf]ยPRQ3S\ยVNย]OXoPR_ย[=W#]ยPยXfย
รค!รฅ-<>=รช=รซ!รฑ@?dรซ,รชรฑzรญ2A%รฒCBยรฉfรซ:รฎLรซ!รจDBfรฉรช=รฒOรฐรรจOรฉEB
F W#ยยS?ยB[=UยW#ยยa3ยยพSf]ยPR_RNยยฆยยกยcยPeQ3Sย_ย[n]O^ยNย&ย vNย_ZbY^3XยยยSfPRQ3NI^ยWYX[=bYXZN\aOXZS^PeN\^ยSfPRSf_RยยW#]3S\^ยSfยย_RSfSXNLย!MยจSG
bยW#SVยรยรWรย Sยย#cยฆ3_eNยMO[LMยWยbยWยPoWยSX ยร_eSVb%[nPZW#ยยSPRN[ยย]3NLb#S^ยยยSMO[ยXoSยDย %XยญWY^ยSยยน_eNยย9PeQยW%XรธcLWYXPRQ3Sf_RS[L]ยยกยยVNย]3]3SยVPoWยN]
MOSfPZSfSf]ยPRQ3SPZNยW%^ยS[IX Hยยผย!ยVNยa3_ยXZSยcLPRQ3Sf_eS1WYXPRQ3S_ย[nPRQ3Sf_ยถPe_ZW#ยยWY[=b3NยMOXZSf_RยL[LPZW#Nย]ยPRQO[LP_ย[L]O^ยN&
ย vNย_ZbY^3X
ยVNย]OXยญWY^ยSf_ยX[รa3]ยWยฎยvNย_Rยยพยฆ3_RNMO[LMยWยbยฎW#PZยกB^WYXZPR_oWยM3a3PoWยN]TยdNLยยSf_ยPRQ3SยXZSfPNLย,Nย_ZbY^3X1XR[nPZWYXยญยยนยกW#]3ย}~ยยVc[L]O^ยW#PยW%X

JLK

MONQPSRUTWVXZYU[E\]T]NU^DV;_a`9P[E[bT]N
cZdIefeghSiUj$cOikmlCn$kkmlUdOoUiSpfqj]rmsutvpxwykmryp{zUoUkyp{j]ij$|]dGr}n$iS~&wydGk}lCn]wkmlUdOlSp{ย]lUdEwykยยCj/wยwยp{zSedยdGiSkmrmjvย/~]ยยยoUk
p{iยkmlSpxwwydEยIkyp{j]iยcZdยwยlUjLcยn$iUj]kmlUdGrbยdGi/kยprยdIe~ยtvpfยDdGrmdGi/k&n$iCtยsoCยยl tSdGdGยWdGrEย}ยIj]iUiUdEยIkยpjviยzCdGkycZdGdGi
r7n$iCtSjvs&gcZj]ryextUwZn$iCt&kยlUdยUryp{iCยยp{ยSe{dj$q;snbยSpsoUsยdGiSkmrmj]ยS~]ยยZlSpxwยยIjviUiUdEยIkyp{j]iยlUj$extUwยยCยmย$ยbยยย]ยIย&ย-ย/ยvย
ย}ย0ยยยยขยกยฃย-ยIยยคEย}ย-ยSย&ยฅbยฆ(ย$ยOยงfย7ยbยจ]ยยฉยย$ยกGยยกยฃยยย-ยSย]ยZยย}ยชSยกGยGยกยซยยฆ(ยงยญยฌยชSยฆ(ยย7ยฌยCยmยIย$ยยค7ย]ยยยขยกยซย$ยฆยฎยยยค7ยยฆCยกGยย$ยฆ(ย-ยกEยฏZยฐยikmlSpxw
ยGn]wydcZdยฑยGniยยIj]iCwยฒpยณtSdGrยดยUrmj]zCn$zSpfefp{kยฒ~ยtvpxwykmryp{zUoUkyp{j]iCwยขย(n$iCtยตp{iยยCn$rmkypxยIoSexn$rZkmlUdยsnLย/p{soUs&gdGiSkmrmj]ยS~&tvpยณwยg
kmryp{zUoUkyp{j]iDยยถj$|]dGrยkmlUd&wydGkjq9n$kmjvswGยยยทOkยj]swn$rmdj$q9ยIjvoUr7wyd&|]dGrย~ยธtvpfยDdGrmdGi/kยนqยrยj]sยบยWj/wmwยp{zSe{dcยjvryextUwGยป
qj]rp{iCwyk7n$iCยId]ยยดkmlUdGrmd*n$rmdยj]iSe{~ยยผCiSp{kmdIe{~sn$iS~j$qkยlUdGsยพยฝp{iCtSdGยCdGiCtSdGiSkj$qยkmlUd*tSj]snbp{i4wยp{ยฟGdยธรรร7ย
ร oUrยkmlUdGrms&j]rยd]ย$kmlUdOsnbยSp{soUs&gdGi/kยrmj]ยS~ยซtvpxwykmryp{zUoUkyp{j]iCw9cยdOยIj]iCwยฒpยณtSdGrZcZpfefe(ky~SยSpxยGnbefe~0iUj]k}zWdOoUiSpfqยj]rยsย
ร dG|]dGrmkmlUdIe{dEwmwGยยsnbยSp{soUsรdGiSkmrmjvย/~รp{iยkmlSpxwiUdGcuwyยCn]ยIdยGn$iยkmdIefeOoCw&ne{j]kn$zCj]oUkkmlUdยtSdGย]rmdGdEwรj$q
zCdIefp{dIqยฑtSdIยผCiUdEt*zS~rIn$iCtSj]sรcZj]ryextUwGยยยฐยi*ยCn$rmkypxยIoSexn$rEยยถkmlSpxwยยIj]iUiUdEยIkyp{j]icZpfefe9nLeรe{j$cรoCwkmjoCwyd&snLย/p{g
soUsยบdGiSkmrmjvย/~n]wยn0kmjSj$e;qยj]rยยIj]s&ยUoUkยpiUยtSdGยvrmdGdEwj$qยzWdIefpdIqmยรรdzCdIefp{dG|]dkยlCn$kkยlUdrmdEwykยrypxยIkyp{j]ikmj
oUiCn$rm~ยยUrยdEtvpยณยGnkmdEwยดpxwOiUdEยIdEwmwยn$rm~qj]rOkmlUdยIj]iUiUdEยIkyp{j]i*cZdยซn$rmdn$zCjvoUkkmj0sn$h]d]ยยยฐ
iCtSdGdEtยฎย;n]wยe{j]iUยยn]w
kmlUdh/iUj$cZe{dEtSย]dzCn]wydsn$hvdEwoCwydยj$qnรzSp{iCn$rm~&ยUrยdEtvpยณยGnkmdwy~SsยซzWj$eยฝยj]r}oUiCn$rย~qยoUiCยIkยpjviยwy~SszCj$eร7ยScZd
wyoCwyยWdEยIkkยlCn$kZkmlUdGrmdpxwOiUj0oCwydIqยoSeยIj]iUiUdEยIkyp{j]izWdGkycยdGdGiยkยlUdยkยฒcZj&n$ยUยUrmjSn]ยmlUdEwยดn$kOnbefeรยปยฎwydGdร]dEยIkyp{j]i*ร
qj]rwyj]s0dยฑtvpxwmยIoCwmwยฒpjviDย
ร dGkยตรยรรรzCdยkmlUdwยoUzSeยณniUย]oCn$ย]dยj$qยร}รรcOlUdGrmdยjviSe~ยoUiCn$rm~ยยUrmdEtvpxยGn$kmdwย~/szCjeยณw0n$iCtยยIj]iCwyk7ni/k
wy~SszCj$exwZn$ยUยCdEnr}p{iqj]rmsoSexn]wGยปvp{iยCnrmkypxยIoSexn$rEยUcZdยn]wmwyoUs&dยดkmlCn$kZdEร]oCnLeรp{ky~&zWdGkยฒcZdGdGiยkmdGrยsw}tSjSdEwยiUj]k
jUยGยIoUrp{i&qj]rmsoSexn]wยถp{iร รรZยรรยฝyรZdEยGnbefe(kmlCnk;piร ร ย]cZdnLeรe{j$cยdEรvoCnbefpky~&zWdGkycยdGdGi0kmdGrmswGย$zUoUkZtvpxwmnbefe{jLc
dEรvoCnbefpky~รzCdGkycZdGdGiยยUrmj]ยWj]rmkyp{j]iรdIยQยUrยdEwmwยp{j]iCwGยfร ร dGkร}รรรzCd*kmlUd*ยIj]rmrmdEwยยCj]iCtvp{iUยยwyoUzSexn$iUย]oCnย]dj$q
ร ร ยZยฐยikmlSpxwwyoUzCwydEยIkyp{j]iDยยฎcZdwylUj$cรkmlCn$kOkยlUddIยUยUrmdEwmwยp{|]dรยCj$cZdGrj$q}n0h/iUj$cZe{dEtSย]dรzCn]wyd&รยรรp{i*kmlUd
exn$iUย]oCn$ยvd9รยรรpxw;ร]oSp{kmdยefp{sp{kmdEtยฎย}ยฐยiรqรn]ยIkbยLwyoCยยl&nยรยรยยGn$idEwmwยdGi/kypxnbefe{~j]iSe{~ยSexn]ยIdOยIj]iCwykยr7nbp{i/kIwDj]iรkmlUd
ยUrmj]ยWj]rmkyp{j]iCwยถj$qยฎkmlUdยn$kยj]swGยยถยฐqCcZdZkmlUdGikmlSp{iUhj$qCkmlUdEwยdยn]w;ยIj]iCwykmrInbp{i/k7wยถj]ikmlUdยรIยUrmj]zCn$zSpfefp{kyp{dEwยj$qCkmlUd
n$kmjvswmรUยvkmlUdGicZdlCnE|vdยkmlUdp{iUย]rmdEtvp{dGi/kIwiUdEยIdEwmwmn$rm~kmj&n$ยUยSe{~ยsnbยSp{soUsudGi/kยrmj]ยS~]ยยฐยiร]dEยIkyp{j]i*รUยรร
cZdOwylUj$cยkmlCn$k;kmlUdGrmdยpxw}nยwykmrยj]iUยยฑยIjviUiUdEยIkyp{j]i&zCdGkycZdGdGi&kmlUdZsnLย/p{soUs&gdGiSkmrmj]ยS~ยซtvpxwykmrยpzUoUkยpjvi&qj]oUiCt
kmlSpxwยดc9nb~niCtkmlUdtSdGย]rmdGdรj$qรzWdIefpdIqZย]dGiUdGr7n$kยdEtz/~r7n$iCtSj]s0gcยjvryextUwZs&dGkmlUjUtยฎย
ยยถjwydGdcOlCn$kยIj]iCwยkmr7nbp{iSk7wยnqj]rmsoSexnยยSexn]ยIdEwรj]ikยlUd&ยUrmj]zCn$zSpfefp{kyp{dEwj$qn$kmjvswGยWp{kยpxwยoCwยdIqยoSeOkmj
ยIj]iS|]dGrmkOkยlUdqยj]rยsยซoSexnkยjยตn0ยIdGrmk7nbp{iยธยGniUj]iSpxยGnbeqj]rmsรยยทยwOnรยผCrIwykwykmdGยยkยjtSj$p{iUยkยlSpยณwยขยUcZdqยj]rยsnbefp{ยฟGd
kmlUdtSdIยผCiSp{kyp{j]iรj$qรn$kmjvs>ย$p{|]dGip{ikยlUdOp{iSkmrmjUtSoCยIkyp{j]iDย ร dGkยรกรฃรข4รคLรฅ ร$รฆEรงEรงbรงmรฆ รฅยถรจ]รฉยIj]iCwยฒpยณwยkยjqkmlUdยoUiCn$rm~
ยUrmdEtvpxยGn$kmdwy~SsยซzWj$exwZp{iรkmlUdย|]jUยGn$zUoSexn$rm~รรชย
รซรฌvรญยรฎDรฏ{รฐEรฏยณรฑDรฎรฒDรณรดยฎรต ยทi ยvยย$รถรธรทyย$ยLยยขยรกยนรนยซpxwยยIj]i$รบyoUiCยIkyp{j]ijq}kmlUdรqยj]rยsรปรฅรbรผ ยฝยฒรฝยฎร;รพ รงbรงEรง รพยรฅรจ รผ ยฝยฒรฝ2ร7ยWcOlUdGrmd
dEn]ยยlยธรฅ รฟ รผ pxwยดdIp{kmlUdGrรฅ รฟ jvr รฅ รฟ ยร$p{iCยIdkmlUdร|Ln$rypxn$zSe{d&รฝ pxwZp{rmrmdIe{dG|$n$i/kยkยj&j]oUrยยIj]iCยIdGrmiCwGยWcZdky~/ยSpxยGnbefe{~
wyoUยUยUrmdEwยwZp{kn$iCtรtSdEwmยIryp{zCdn$iยธnkmj]sun]wOnยIj]i$รบyoUiCยIkyp{j]i*j$qkmlUdยนqยjvrmsuรฅ รยรผ รพ รงEรงEรง รพรฅ รจ รผ ย
ร j]kmdkยlCn$k&kmlUdGrmdn$rยd  9
รข  รจ n$kยj]swj$|]dGr&รก@n$iCt kmlCn$kkmlUรจ dG~ยn$rmdรsoUkmoCnbefe{~ยdIยCยยeoCwยฒp|vdn$iCt
dIยUlCn$oCwykyp{|]d]ยยยZlUrmjvoUย]lUj]oUkkmlSpxwยCn$ยCdGrEยยฎcZdoCwyd
	@kmjยtSdGiUj]kmd
 n$iCt รEรฆbรงEรงbรงmรฆ รkmjยtSdGiUj]kmdkmlUd
n$kmjvswยjL|vdGrยรกยSefpxwykmdEtp{i*wyj]s&dยผUยUdEtยj]rItSdGrEย
ยณรฌ8รฒยถรณ (รต ยZlUdGrmdยซnrm
d 	 
รข n$kmjvswZj$|]dGrOรกaรข รค$รฅ รbรฆ รฅ Gรฉ ! ร รข4รฅ ร รพ"รฅ $ย #ยฑรข รฅ ร $รพ รฅ$ย
%&
รข รฅ ร รพ"รฅ $'ย  ร (
รข รฅ ร )รพ รฅ$ย
ย9lUd ยvยย$รถยยยค ยCยmยIยQยยยฃย-ยย$ยฆ ยยยขย7รถ+
ยก *,*- ร ยฝยฒรฝยฎ.ร *,* / รฆEรงbรงEรงmรฆ *,*-ยยฝยฒรฝยฎ.ร *,* /cZpfefeยซยSexnb~ nยwยp{ย]iSpfยผ(ยGn$i/kรrmj$e{dรp{i
j]oUr&kยdEยmlUiSpxยGnbeยซtSdG|]dIe{j]ยUs0dGi/kEยรยฐ
kkmoUrmiCw0j]oUk&kยlCn$kร รร pxwยn*rIn$kmlUdGr&cZdEn$hรexn$iUย]oCn$ย]0d *nรqj]rmsoSexn
รย2
ร 1*ร}รรยtSjSdEwZefp{kmkye{ds&j]rmdยkยlCn$iยธยIj]iCwยkmr7nbp{ikยlUdยUrmj]ยWj]rmkyp{j]iยjqรkmlUdn$kmj]swGยยฐ
i*j]kยlUdGrOcZj]r7tUwGยSqj]r
354'687:9;7=<?>@9;A#B;CD>EB<?>EFHGJI@KLINMO99;7=P;MRQSB;PUTE>EFVD77XWOB;7YFOZ57YZ8B;I[B;CO7:T\>@P;7]^CO7Y9;7B;C57?_`a<!7YFHBbSINFOPU7YcHMd>\Q-bSBG.e VOM5B
B;CO7U7XWOB;9f>[TYIN<!gRQS7XWHbSBhGINVOP;TYMO9;7YP<?>EFHGJIEKB;CO7U7YP;P;7YFHBbi>\QbSZO7E>EPY4
jOj

klmnLoqpsrtouv5nLwJlmnyxzl{|}p~pยmยuยoqยDย
ยDย ยยYย'ย@ยyยยยย[ยยยHยDยย'ย'ยยยDยยOย0ย ย,ยdยdยยยHย ยยยยยยยEยก
ย ย}ยยขย,ยaยย ย}ย@ยaยEยย
ย0ย ย,ยยฃยEย0ยฃยคย0ยEยYย,ย0ยยNยฅยฃยEยOยEยXย,ย0ย'ย
ยDยEยยขยEยยOยYยยยย'ยNย0ย'ยยย,ยYย,ย0ย'ยdย#ยฃยEย0ยฃยคย0ยEยYย,ย0ย'ยยฆยDย#ยDยEย0ยกzยRยงzยจ[ยยยยกยขย0ยEยzยNย0ยกยขยฃ ยยยNยฅtยYยยฉย ย.ย0ยNย\ย}ยยชยกzย0ย@ย ยยยยHย@ยย,ย
ยซยฌยญ^ยฎ ยฃย@ย0ยฃ'ย0ย@ยYย,ย0ย'ย:ยdยยยHยUยEยยฃ ย,ยOยHยฏยย'ย.ยYยEยฐ;ย0ยNย ยHย!ยยย'ยDยยฉย\ยiยLยHยยฑยYย,ย0ยยฏ ยยOยYยEยOยยขยฃยEย0ยฃ'ยยยEยYย,ย0ย'ยHยฏ ยDย'ยยยNย0ย'ยยย,ยYย,ย0ย'ย5ย
ยฃยEย0ยฃยคย0ยEยYย,ย0ย'ย ยฎ ย ยยฉยOย:ยย0ยย0ยยยชยNยฅยยฃยEยOย@ยXย,ย0ย#ยฃยคยDย!ยHย5ยงUยฒfยณXย!ย ย ยOยย0ยยยยNยยยยฉย0ยHย ย,ยHย'ยNย0ยฏ ยยDย[ยHย0ยHยOยดDยต ยยdยยถยยยOย ยทยย
ยHยDย)ยDย;ยEยHย)ยธยคย8ยNยฅยฃยEยOยEยYยOยยนยยบยDยยกยย0ยEยยYย'ยHย@ย,ย'ยNยYย,ย)ยยปยยEยยJย;ย ยยปยยผยฃยคยdย[ยHย?ยยฑย"ยEยยยhยDยยทยย'ยDยท0ยJยhย?ย'ย\ยOยยงยปยฝ
ยพ ย,ย0ยHย$ยDย ย)ยยยยฏยฉย@ยยJย'ย.ยYย#ยYย@ยHยฃ)ยEยDยยถยยฑย.ยยยDยฃยฃ ย,ยยย,ยยทzยกzยdยฅยฉย,ยก
ยยกยฟยHย ยEยEย0ยฃ ย
ยhย?ย@ยย'ย\ย ยซ ยฌยญ[ร ยรยhย0ย@ยตzยDย
ยNยฅยฃยEยOยEยXย,ยยยยยYยยDย'ย)ยEยHยฃ ยhย0ยNยยย5ยยปย"ยฃย@ย0ยฃ'ย0ย@ยYย,ย0ย$ยEยHย@ยกzย[ยธ ย$ยDยEย0ยก
ยhยยฆยฃยEย0ยฃ'ยยยEยYย,ย0ย$ย@ยHยEยกzยHยง:ยณfยยhย#ย5ยhยYยรย'ยYยNย;ย ย
ยEยรยกzยDยต0ยยยDยDยYย,ย0ย'ย
ยยยEยยHยยXย,ยกยยฃ ยยปยยปยLยHยDยYย,ย0ย'ยยขยEยยยยยฟยEย'ยDย8ยยถยiยยปย#ยยNย,ยฃtย'ยยฆย,ยรร0ยOยNย\ยยยยยtร'ยง)รaย)ยNย0ยก
ยธ ย,ยย
ยEยยOยYยJยYยEยHยฃ'ยยยฑย'ย
ยEยOยยย ยยย@ย?ยEย'ยDย[ยย(ยธยคย?ยEย.ยDย'ย=ยยย0ย@ยกยยOยยฆยยย ยEย
ยยYยฃ'ยOย@ยhย5ยรNรDรLรDรยครยรNร0ร@รรรยฑร.ร(ย?ย ยhย@ยzย[ย[ยยDย
ย ยOยEยNยYย,ยธยคย0ยง
รรรยร!รร,รOร}รรรร,รยคร[ร ยร0ร;รDร
ร;ร8ร;รHรNรยรรDรdรRร!รก&ยhย!ยยฆยฃ'ยDย,ย ยย0ยก
ยhย5ย"ยDย0ยHยUย@ยHยEยกzย:ยDยยEยยยยย0ย@ยกรฃรข,รข-รคยฆยฒ=รฅรฆยฝ\รข,รข รง ยฏ
ย?ยยHยEยรรคยยhยยยDยรจยDยEยยยกรฉยDย0ยHยรกรยงรร0ย'ยEยรจยยฑยรจยDยEย0ยกยชย}ยยชยEยHยEยกรชรย}ยรฌรซยรDรญNร;รรhรDรยฟยยปย?ยHย0ยHยEยaยNย ยNรฎยนย@ย,ยHยยฉย
ยDย[ยEยย
ยฃ'ยยฑยยย ยย0ยก
ยhย5ยUรUยhยยฃ'ยยฉย=ยยย\ยยยยย0ยง
รรรยร!รร,รOร}รรรรยปรฏUร
ร ยฒ=ย@ย,ยยฉยYยOยยคยฝ#ยYยHย ยEยHย'ยNยยรฐ(รฑ ยซรฒยญ ยhยJย,ยรรNรDรรDร'ร;ร.รยร0รHรDรNรรณยยปยย,ยยฆย}ยยย)ยยยhยรด=ยย'ยNย\ยยยยยรจยDย
ยNย0ยDรดYยย'ยNยYย,ย0ย'ยHยฏยคย?ยยHยEยยOย0ย@ย$ยNย0ยDรดYยย'ยNย?ยhยย0ยยยDยUยEยยJย;ยDยยปยยยDย[ย,ยยท'รต
รถyรfรทยครธ+รนยฏยคยฒfรfรทรบรรน:รปร?รผtรXรทiรฝRรพ=ยฝNยฏยฉยยย?ยฒfรXรทรบรรน:รป
รฟUยฒfร!รผ รfรท,รฝRรพ=ยฝEยฝNยฏ0ย?ยยHยEยJรยDย'ยยขรfรทยDยEยยDยEย0ยก
ยhย[ย@ยHยEยกzย[ยDย'ย
ร รท ยhย?ยฃยคยยฉยXย,ยYย,ย0ย0ยฏ
รถ  รฅ#รคยถรพ=ยฒYรฅยฝUย0ยรฟ  รฅJรค[รพYยฒ=รฅรฆยฝ!ยYย0ยกยยยDยEยยยกรฉรค[รพEยฏย0ย
รถtรค[รพYยฒ 5ยฝยยยยย#ยYย0ยกยยยDยEยยยกรฉรค[รพยDย'ย)ยYยยยกยย8ยNย0ย'ยYย.ยยฑยยฉย Dยง
 ยย@ยEยยHยEยกยย0ย@ย0ยฏ0ยยยย}ยยรด=ยย'ยNย?ยHยDยยย0ยยNยยยยฉย.ยdยยยยขยธ'ย0ยEยยรคยถรพEยฒ 5ยฝยDย'ยย
รค Oยฒ 5ยฝย;ย0	ย 
รธ 
 8ยยยยNย0ยDรดYยย'ยNย.ยHยฏยยย0ยยHยDย

ย,ย8ยNย0ย ย.ย5ย,ย$ยธยคย0ยEยaรค[รพYยฒ 5ยฝ?ยDย'ยรฟ รฅJรคยถรพEยฒ=รฅยฝ.ยงยฆยฒ ย0ยEย8ย@ย'ยDย#ย@ยยOยYยยhย0ยYย8ยNยยย'ยยยยย\ยยยยย'ยยDยEยยยXย,ยกยยฃ ย,ย$ยก
ย,ย ย,ยกzย5ย
ยNย0ย'ยXยhยYย@ยHย'ยNย)ยEยOย0ย ย,ยEยHยกยขยHยยฉย.ยRยงยปยฝ
ร 5ร  รร 
ร  รDรHร#รHรDรNรรยปร)รร ยซรฒยญ รhรญ8ร รhรDร0รยปรHรร!ร;รรรJรรรยฑร.รรiร รhรyร.รDรรDร'ร;รNร0รDรHรDร.ร "!รDรEร$#
 ร 
รDรDรHร %!ร &ยฉรRร\รยรรญ8รDรรจ(ร '#ร.ร5รรhรDร[รซ'รEร5ร.ร )* ร\รยขร &ยฉร0ร %,+ยฑรhรDรHรaรรรรDรN
ร ยรยป.
ร -ยรฑ ยซ รฒยญ %[ร.รDรยครญรรร รOรรญ8รDรaร  รhรDร0รยปรHรLร
รHรDร.
ร รยป0
ร )
/- รhรaรNรDรLรยฑร'ร;ร.ร0ร5รHรDรN.
ร 
ยจ[ยยJยฃยEย ยDย"ยDยยEย ยhย[ยEยยHยยยEยHยก$ยฏยฉยยฑย'ยยยDยUย5ยยปยรฆย@ยยHย0ยEยHยกzยยผยยย)ย@ย ย}ย:ยฃ'ยDยฃยคยHยOยฏ'ยHยDยรยธ'ยย;ย0ยย'ยยขย,ย)ยEยยรฌยยฑยฃยฃ'ยHย'ยยยยปยฅรฆยง
รยย8ยEยHยกzยยฑยEยตzย@ย'ยDย#ย@ยยย,ยHยยท0ยEยยDยUยEยยย;ย0ยEยก
ย ยhย 
/- ยhยJย=ย ยฃ ยhยHย5ยยปย,ย$ยNยฅยฃ'ย0ยยHย ยYยhย5ยUย,ยยEยยย,ยHยยท0ยEยยDย1-ยง
ร0ย'ย@ยย
ยธ ย,ยDย?ยยฃย\ยHยHยกzย:ย,ยยยHยEยHย ย?ย,ยยยฑยยฉย)ยEย@ยยHยกยย8ย ยNย'ยยOย ยยยรยEยHยEยกzยยถยDยยผยDยEยยยกzยHยง
ยจยถยยHย0ยEยH3
ยก 25ยง 4รยhยzยยยท0ยHยยHย.ย5ยยป7ย 6OยDยYย,ย0ยยD
ย 8Uยhย5ย,9
ยก 4;ยง : 7ยง <ยขยยย ยฒ=Jย5ย,ยฃ'ยHย@ยยฏ><@?A?0รน ยฝNยงรยณXยOยฏ:ย,ยรยEยยEยยฏ:ยhยรย
ยท0ยHยยHย.ยdยi7ย 6OยDย\ยยยยยยยDย'ย?ย[ยNยยปย,ยฐ;ยตยฉยยDย?ย
ยEยOยYย ย,ยUย?ย ยhย@ยยยEย5ยยย^ยEย'ยDย"ยยฑยยฉยJย'ย.ย\ยEยฐ;ย0ย.ย ยHย^ยยยยยEยก
ย ย}ยย[ย,ยEยยย0ย ย,ย8ยย'ยDยEย
ยฃยEยOยยยhยHยDยEยOยรฌยhยJยOย0ย ย,ยdยdยยยHย ย
ยEย)ย0ยย
ย[ย,ยEยย0ย ย,ยaย ยHยฃยEยยฐ;ย0ยยzยยย'ยDย ยYยยปย'ยHยยยOยYยYย,ยยท'
ยง B[ย0ยยท0ย ย,ยaยYยฃยคยOยDยต0ย,ยยท'ยฏ
ยEย ยhยยถย}ยยธ'ยOยHยยฑย'ยYยย;ย0ย#ยยยยย'ยDย ยYยยปย'ยOยรย;ย0ยEยก
ย ยhยยยYย'ย@ยย0ย  Cรฅ - รท ยฏ'ยYยยธ ย;ย0ยEยก
ย ยhย0ย[ย.ยdยยยตยย,ยยทยนยยฑยธ'ย0ยย#ยยขยdยยฑยYยhยDยธ ย,ย
D)ย0ยEยยHยJยEย'ยDยaรฅยHยDยยธ'ย
ยกยยDย0ยOย ย0ยย.ยXยhย ย
ยEยยยยEยNย0ยฃยคย
ยDยยEยยยยย'ยDย ยYยยปย'ยHยOยง8ยจ[ย ยhยยhยJยฃ'ย ยEยXย,ยธ ย,ยยยธ'ยOยHยDย'ย\ย
ยยรยยปย,ยEยHย.ย5ย!ยYยยธ ย;ย0ยEยก
ย ยhย)ยHยDยย.ย5ย,ยต$ยDยธยคย0ยย8รฅaยDย'0
ย D)ยEยยยท0ยHยEยยHยOF
ยง E?ยยยฃยEย ยDย!ย'ย\ยOยJยEยยยยEยDยกยยยฆย}ย ยOยรยDย'ย
ยNยฅยEยHย'ยย:ย,ย[ยEย
ยฃยEย0ยฃ'ยยยEยYย,ย0ย)ยYย.ยDยEยHยกยขยHยยฉย.ยRยง"ยณXย)ยฃ'ยDยEยYยhยNย ยhยDยOยฏยยยย?ยYยยDย#ย:ยEย'ยยฑยUย;ย0ย?ยDยยฉย.-ยรฑ ยซ ยฌยญ ยEยยHยEยยhย?ยDย
ยOยยย ยยยDย5ย,ยHยยฉH
ย -
G ย?ย ยhย@ย$ย'ย0ยยถยยยยยOยYยEยOยรย0ย'ยDย ยYยยปย'ยHย.ยย0ย[ยยOยYยEยOย ยฃยEย0ยฃ'ยยยEยYย,ย0ย'ยHยง
ย0ยYยhยNย0ยฏยยDย[ยHย0ยHยOยฏยEย'ยDย8ยYย'ย@ยtย$ย@ยOยYย ย,ยยชย ย ยOยยยย0ย8ยยยฑย}ยyยยย'ยNยzย[ยzย5ยยปย,ยDย
ยHย0ยHย ยรยXย,ยยทDย,ย)ยธ ย,ย'ยDยEย


ยฃยEยOยยยhยHยDยEยรย,ยยEยยรยhยDยยท0ย'ยDยท0ย0ยง ย0ยรยNยฅ'ยDยกยยฃ ย,ย0ยฏ?ยEยย ยยยยยEยก
ย ย}J
ย ID รฅ KยยฒYMรฅ LNDยคยฝ8ย@ย,ยOยDยYย,ยรยยHยOยยรยยOยYย@ยOย
ยยย'ยDยยฉย\ยiยLยHยยฑยYย,ย0ยtยธ'ยOยHยDย'ย\O
ย Kยยฒ=Pรฅ LNDยคยฝ?ย.ย5ย,ยตยยยขยDยธ'ย0ยย
ยธยคย0ยEยtรฅรยDย'Q
ย D ยยฑย'ยรจยYยยก
ย'ยYย
ยEยHยกzยdยยยtยยถยยย@ย ยยย ยEยย
RTS

UCVXWZY\[^]1_a`\b@cA[AV\dM]fehgWib@bj[AV

kNlmAn^opm*qsr^mAtNuwvAxy{z|t~};oยยk$ยยยย}7tNuยrZ}7zy*ยNยยn\ยNo@ยย}ยly*tNo@kยPo@yAlยuJyยย\ยย}ยtย}ยmยzyjยยยZon\tNuwm*qsz\o@kยt~}7z\ยยยNo@yjย;ย7ย
ยZmZo@k1yAย\ยoย\n\ยNo@kNkย}7ยAoCn^m*ยยoยjย	ยauZ}ยkskยu\mTยkMtNuy{t	tNu\oยยoly{zproaz\m

ยly*z\mยzZ}ยlyTย\qยmAยยย

ย;}7ยAoยยau\omAยNoยยย\ย5ยกยขqยmAยaย~}ยlยu\oยยฃยยy{z\ยAxy*ยAo@kยยฃยauZ}ยk1}ยkNkยx\o}ยkยฃmAz\om*qMtNu\oย

ยCtยu\omAยNoยยvยxZ}ยtยo

yj}7zยคยNo@yAkยmAzk1ยCuZยpยaoCยNo@kยtNย~}ยlt

tNu\oยฅ>ยฆยงtNm yFx\zy*ยยยยจยยy{z\ยAxy*ยAo}7zยtยuZ}ยkยขny*n^oย@ยaยฉNยชAoo.ยชAo@lt~}7mAzOยกยซqยฌmAยaqยฌx\ยNtNu\oยยย}ยkNlxkNk}ยmยzMย;ยญ
ยฎ }7ยAozOy*zZย.qยmAยยยxZยยy"}7zOly*z\mAzZ}ยlyjยfqยฌmAยNยยยaoly{zยค}7ยpยpo@ยย}ยy*tNoย7ยยยZoย~}7ยAoFqยยNmยยยฏ}7t@ย\}7zยy k~ยZzZtยyAlt~}ยl
ย

y*z\z\oยjยypk~otCm{qยฐlmAzkยtNยยyj}7zZtยkยฃmAzยคtNu\oยn^m|kNkย}7rZย7on\ยNmAn^mAยNt~}7mAzkam{qยฐy*tยmAย

kย

ยฑยจยฒยยณยยดMยต7ยถ@ยตยยทMยดยนยธMยบ7ยปMยผยฝ otFยฅ>ยฆยพr^o>}7zOly{z\mAzZ}ยlyjย	qยฌmAยNย0ย1ยwolmยzk~tNยNxltyqยmAยยยxZยยypยฟ,ยฉNยฅ>ยฆยญ1}7zOtยu\oยยy{z\ร
ยAxy*ยยosm*qรยNo@yjยlยยยmZk~o@ยFoยยย\kaยฉยฌ}รยรoAยยย@m*ยAoยftNu\oaยAm\ly*r\xZยยy*ยNยpร*ร\ร@รAร$รยรAรaรAยญyAkPqยฌm*ย;ย7m*ยkย*ยCu\oยNoaรรร*ร@ร@รjรNรNร^ร
y*ยNoqยฌยNo@k~uยคย*y*ย~}ยy*rZย7o@kFยฉยย}ยk~t~}7zltยขqยยยmAยยฏtNu\o>tNm*ย7oยยy*zloFยTy*ยย}ยy{rZยยo@kร$ร@ยญยร
รรยยo>ยยonZยยyยloยo@yAlยuยคmXllx\ยยยNozloยm*q1tNu\oqยฌmAยNยxZยยypรรยฉรjยญ1rZยยคร\ร1รรร\ย
รรยยo0ยNonZยยyAloยo@yAlยuรm\llx\ยNยNozloยm*qpร|รร ร ยฉรรยญrZยHร ร
ร ร|รร ร ยฉ~รรยญ1r|ยยคร ร	ร

รรรy*zยยยNonZยยyAloยo@yAlยuรm\llx\ยNยNozloยm*q

รiย

รรยยo>ยยonZยยyยloยo@yAlยuยคmXllx\ยยยNozloยm*qร7รรร ร ยฉรรยญรร7ร รกFr|ยยจร ร ย
รข

mAt~}ยloCtNuy*taยฟfยฉNยฅ>ยฆFยญ	uyAkยฐtยamt~ย|n^o@k1m*qMยTy{ย~}ยy*rZย7o@kร1tNu\oCz\oยยยTy*ยย}ยy{rZยยo@kร ร tNuy*t1ยaofรฃxkยt1}7z|tยยNmXยZxlo@ยรย
y*zย.tNu\o>tNm*ย7oยยy{zlo>ยTy*ย~}ยy*rZย7o@kaร$รยยยฐรครzยคmAยยยZoยatNmoย;}7ย}7zy*tNoFtNu\oรฅยZon^ozยZozlomAz

tยu\oCยยy*tNtยoย@ยZยยom*qยฌtNoz

lmAzkย}ยยZoยtNu\oqยฌmAยNยxZยยypยฟfยฉNยฅ>ยฆpรฆย\
รงรจ รฉยฌยญfqยฌmAยk~mยยpotยm*ย7oยยy*zloยยยo@ltNmAยpM
รงรจ ย
ยฎ

ยฑยจยฒยยณยยดMยต7ยถ@ยตยยทMยดยนยธMยบรชMยผ
ร
ยAo@ltNmยยยkยข}7zรฏ
ร

}7ยAozy.qยฌmAยNยxZยยy.รซm*ยAoย>tNu\oยTy*ยย}ยy{rZยยo@kยซรร*รjร@ร@รยรNรรFยรย7otรฌรญAรฎ~รฆ รซMรฉMr^optNu\opk~ot>m*q
ร
ร
ร
รรรฒ ร รiร ร
ร{รรณkNy*tย}ยkqยยย}7z\ยยจรซยฐยpรดimAยNย yTยรตย7ยAยร}รตq>ยฉรถ ร ร@รjร@ร~รยรถ ร ยญรฅรฐHรฏ
ย

รรย
รงร รฐรฑรฆ5ร\ร@รรฉ

tNu\ozQยฉรถ|รjร@รjร@รNรรถAรยซยญsรฐยรฌรญAรฎ~รฆ รซMรฉi}รตรทHยฉรรธ รนรยรบยญยร ร
ยฑยจยฒยยณยยดMยต7ยถ@ยตยยทMยดยนยธMยบ7รผfยผ

รซยฐยZยCu\oยยoรบรป}ยkyยTyTยยxy{t~}7mAzOk~xlยuยtยuy*tCรบpยฉยร ร ยญ ร

ยau\oยครฝรญAรฎ5รพ\รฟยรญQรฝ	
.รญยฅ>ยฆ
ยนM
รงรจ ยMยZoz\mAtยo@ย  รฆ;ยฅ>ยฆรฉรยi}ยkยยZoz\o@ยยtยm

รถ ร ย
r^optNu\o

lยย7m|k~x\ยNoFm*qaรฌรญAรฎยรฆ ยฟfยฉNยฅ>ยฆรฆร\
รงรจ รฉยฌยญรรฉรย
รคยฌqFยฅ>ยฆย}ยkFz\mAt>}7zHly*z\mAzZ}ยlyjยaqยฌmAยNยยยรยao
ยNo@k~n^o@lt~}7ยAoย7ยAยรยCu\oยNo

ยฅ>
 ยฆ

ยZoz\oยยฟfยฉยยฅ>ยฆยยญรฅy{zย  รฆ;ยฅ>ยฆรฉstNm0ro

}ยkatยu\o>qยฌmAยNยxZยยyF}7zOly*z\mAzZ}ยlyjยfqยฌmAยNยยo@vยxZ}7ยTyjย7ozZt>tNmยจยฅ>ยฆยพmยr\tยyj}7z\o@ย0r|ยยจtNu\o

n\ยNm\lo@ยZx\ยNoy{n\no@y*ย~}7z\ย.}7zยtNu\o>n\ยNmZm*q1m*qsยau\omAยNoย

ยiย ยก\ย

!#"%$'&)(ยยฒJยธPยบ+*รยผยฝ ot!,ยroร-	ร*ร.-0/$ร|ย\ยa}7tNuยคtยu\oยy*tNmAย

ยฅ>ยฆ
ยau\oยly*z\mยzZ}ยlyTย	qยฌmAยNยxZยยy

ยฅ>
 ยฆ

ร7 ร8- ร ยฉรรยญ#9

k1oย\no@ltNo@ยรย ยฅ>
 ยฆ

kmAยยยZoยNo@ยยคyAkย}ยz21fยy*ยpnZย7oยiย43\ย65ยฃmAzkย}ยยZoย

ย;:- ร ยฉรรยญ<9=- / ยฉรรยญ>: รก@? รยฐรยร

o@vยxZ}7ยTyjย7ozZt>tNmยคยฅ>ยฆรป}ยkรBA

ร ร|รร!C*ยฉรรยญ<9
H

ยฟfยฉยฅ>
 ยฆยญ>y*zย  รฆยฅ>
 ยฆรฉยฌย

lmAzk~tยยยyj}7zk1r^mAtNuwร7รรร

ร ร|รร!D*ยฉรรยญ#9

ย^ร7รรร

C ยฉรรยญยร7ร รก>y*zยOรยรรร

k~oo>tNuy{tร7รรรรยฉ~รรยญยร7ร รกยจยฉยฌ}รยรoAย7ยZรรรjยญ1}ยk>ยฉy{n\n\ยNmjย|}7ย
   รฆ;ยฅ>ยฆรฉ รKJ ยฉรรรjร@รjร@รNรยร D ยญsรฐยรฏ

ร ยฉรรยญยร7ร รกFE

รFGHรรยร

D ยฉรรยญยรยร รกยยฉ(}(ย oAย7ยAร C

y*zยร D ยญ	tNm>r^oรฅร\ยfยwoyjยยk~m

y*tยoยยย^ยญsy*tCยpmZk~t>รITย\ย1ยau\oยNoqยฌmAยNoAร
D

รAรรร8G

รITยยขรรจ ร ITย\รยร Cยขร

ร Dร

รL>ร

MON;P#QRTSUBU0VXWSTVYVXWQ[Z\4].^_`baYcedfS8gXQVihkjml npoRqc.rsgt4gXVXg!cTdfSuUBU0v8wVXxyU4QgYVXWzSeV{Se|XQVXWsQ!UBt4}~t4VcedfS8gXQxsQrsRQ8cTd
v8wVXxyU4Qgpt4rยh@N
ยON;ย0c.VXQ6VXWzSeV)WsQ|XQ6ย%QiST|XQ6ย.t4Qย%t4rย[ย)ย'Seg0S~d+c.|X}ixOUยS~t4rFย<ยยยxsrยsQ|0VXWQ6VX|STrgUยSTVt4c.r@ยsQยยzrQยFQuSe|UBt4Q|ยยย%Q)ยscVXWOt4g
VXWs|Xc.xsย.Wsc.xsV6VXWQ6yzSeyzQ|)ย%t4VXWsc.xsVpd+xs|XVXWQq|pRqc.}Y}YQqrVN

ยzย

ยiยยย<ยยยยยยยยย<ย8ยยยยยกยยขยฃยคยiยฅยยงยฆ)ยยจยยฉยยยชยซ

ยฌ#ยญ+ยฌยฏยฎยฑยฐ<ยฒยดยณยตpยถยทยณยฒยยถ;ยธsยนยบยธsยปยคยต#ยถยฝยผยพยฐ<ยฒยยถ%ยต#ยฟ2ยฒ;ยถ%ยต#ยถ
รรร+รTรยดรร	รรTร2รYร	รรร2ร

รรยรร#รYรยกรรรร	รTรรรeรรรTรยกรรร;รรeรรร.รรรยครร[รTรรรร+รรรร<รรรsรsรรeร;รรขรกร#รรรรeรรฃรรค~รยคร

รTรร8รฅXรร	ร.รร+ร	ร=รรฅfรTรร[รรรรงรฆยกรร+รรจรTรรรรยรฅbรรงร+รรฉรชรรTร	รฆรฌรซYรค)ร+รรจรรฎรญ
รฏ

ยฒรงรฐ{ยถ#ยป+ยธsยปยคยต#ยถรฑยฌ#ยญXรฒยรณpรด8รต

ร+ร	รร2ร[รYร	รuรยครรร

รรถรKรรรYรFรรร.รท;รรรนรธยพรqรยงรกiร=รบ

ร

รeรรนรปยร

ร>รผยครผBรซยรOรqรฝยทรกeรผยครผ รพยรรผ+รผBรซ8รฟรqรฝยทรกรผ+รผ รพยรรsรรTรzรผ+รผ รซรFรรฝ%รกรผ+รผ รพรงรก
รรรรTรรขรTรร[รรร+รรsร8รรฅ6รTรร[รรTรรงร;ร	รTรuร รรงร;รยรรeร8ร รรรTรรTรรeรรTรsร=รร	รรยร


	

รรรeรร

ร	รTรรรeรรรTรsร

รYร+รTรรถร

 
	

ร

รรนรรร.รท;รรรฃรTรร






ร 

รTร;รรFรธยพรqรยงรก



รร

รยดร@รTรรยกรTร;รรรTรร[รรงรsร.รTร	รรธ6รรยงรก



รรฅ{รรรร=รฆรฃรรรร.ร{ร

รร%รsรรsรsรeรTร;รรขรกรรTรรรรฃรTรรYรรรรTรTร	รรรFรรฅ#ร
รรฅ รยกรTรรeร;ร6ร	รร6รTรรขรป;รยร



รTรรรป;ร@รTรร@รรรรTรTร	รรรรรรฅ{รธ6รรยงรก %รTร;รร

รร<รรsรรsรTรeร;รFรก





รยครOร%ร รฅ{รธ6รรยงรก

ร)รYร{รรTรรรปยร	รร6รTรFรรรร[รรTรรYรรรรTรTร	รรร

ร	รรTรรขรฉ	รยรรรฃร	รuรยรฆรฃรรeร	รรร~ร+ร;รรงรรรรTร	ร~รรฅ#รรzรรฆรรรยรรขรYร	รรรรiร

ร'รฅร	ร.รร%รTรรรTรยกรรeรรนรร=รฆยกรรยรรรฆรฃร	รTร@รYร	รรรร8รรsรรFรยรร+รยรร

รรฅ{รรร+รฉ	ร

รTรรรTร8รรTรยรร;รeร
รรยรTรeร	รยรรรTร;รร

   !
 	#"$	%

รรรรร	รฆรฃรรรรงร#รfรรงร'&	รรรรรsรรฃร(*),+,-รรกYร;ร	รรขรรร ร รsร
ร+รsรยทรร[รsรeรรรยรuรยครzรยร+รรTรรยกร.ร	รรรTรรรร[รรฅYรTรรรฃรร
. รรขร รรฆ รฆยกร
รร;ร/ร;รรร รท;รsร!ร	รรYร+รยรeรรTรsรรYร+ร2รTรรFรฆยกร0.รร+รฆ@รรฆ21XรรยรeรTร	รรรรฃร;รร+รรรรqร.รก~รรฅ4367
5 8:9<;>=

ร
5
7
L
?ยร	ร=รรยรA@CB รบ
ร รรEDGF IH:JK
8 @ = ร 9<; รกยกรรรรร	รeรรจรeรรรจรรรรฆ@รป;รร=รรฅ[รYร	รuรยครร=ร รรฅยฑรqรNM รPO
รร;รeรยดรTร;รร รqรร,Q  รกรนรผ 
 รรบ ร ร+รรRDGF IH:JK L 57 8 ร = ร 9<; รก
9<; รร;ร รร;รTรรTร;รรFรธยพรqรยงรกFรA@>%รฅXร	รรนรรรรรรฑ
รรปรปรTรรรงรรรTรGD2F SHTJUK 7L 5 8NV รX
 W = ร 9<; รก
รฅ{ร.ร	รรรuร2D2F SH:JK L 57 8 ร  = ร 9<; รกiรร!รรsร.รsรTรTรรuรยร+รE
M รรTรรรรรร+รsรTรFรร ร
>Y
ร.ร	รฆรฃรยร	รรรรรรYรรฅรรจ
 รรeรFรฆรนรรร+รร+รรร+รsร8รรฅ6(Z[O
รzรYรร	รรร	ร รฅ6รTรรรTร@รรTร[รรรรยกรฆ รยฉรรร.รร8ร	รTรรรeรรรTรsรรรYร+รTรEร
]\
รรยรzรยรรรYร[รรรรรsรรร+รฆยกรรeรFรTรร.ร+รรยรรฆ@รปยรร_รง
^ รรร+รTรรนร	รร.รร.รรTร.ร+รยกร;รqร รรฉรรTรรรขรรยรTรeร	รยรยฑรฅbรร;ร.รร+ร	ร#รญ
รYรยกรรร

ร+รฉ	รร	รTร=รร รรTรรยกรรงรTรรร[รยรร+รยรรยฑรรรร

ร.รรงรฆรฃรรรร+รรฉยดรรรรฉ	รTรรsรรขรรฅยรปยร.รยร+ร.รฅ

รรรร

` ยฒยยฟยฟรจยบยดยฌ#ยญXรฒยรฒ;รดbadc   Se KS	KSf g[h  	 %Ubi รญj Olkmj O " 
J	 F nKS	 	oHpqK%	r   ,Hs
Ufto",H
gh  	oK'u รwvยดรญj Oxkxj y K hz c 	 c "$	{ g U <
 รรบ ร {6 gtDGF IH:JK L 57 8 ร = ร <
ย ย {
9 ; ร}|~ร "U
J ร
9 ; รกยย
	c  
L4ยย
Lย4ย
ย[5 ยย D2F  SH:JK L 57 8 ร = ร 9<; รก ย i รwOยดรกยv<รยO รกยย
ยI5 ย ร
ร i รยO
รกZ u รยO
รกTรกยย
Y

รฅiร.ร	รรรuร	รร+ร!รฅbรร ร+รรยร~รฅXรTร	รฆรฌรeรรFร+รรฆรฃรฆยกร

รร	รรTรรร.ร+รฆ@ร+รรฉร+รรรฆรฃร	รTร=รYร	รรรรรร	รTรรรeรรรTรsร

รTร;รรYรTรรรร+รsร8รรรยรuรFรรยรTรeร	รยร
รYร+รTร

รTรรรฆ

รTร;รร

รeรรรร รsร

ย.

รรรรsรรยรฆยกร ยร+รฆ@รรฆรฌร;รร	ร

รรรยรuรรถรรรรTรTร	รรรยดรรยฑรฅbรรeรTรรร

0.
รรรรYรร!รsรTรรรรรรรร ร+ร=รTรรรนร.ร	ร;ร.รรรรTรรรร+ร	ร=รรรรร	รฆรฃรรร	ร
]

ยยทรรฆรฃรฆยกรยย ,( ( รยคร@ร	ร.รeร;รร ร รรรฅรร+รร+รยดรsร	รรรรTรรถรรeรzรรงร
รร@รฅXรร ร รรYร+รรฉยดรยร+รฆรฃรรร+รรร.ยรรฆ

2
รTรรFรฆยกรzร ร รรรรsร


รฅXรTร	รฆยฏรฆยกร รร+รฆรนรรฆ

ยย

ยยคยฒ'ยฌpยญXรฒzย;รดqย

ยบ%ยฟ'ยผ

GยA Vยย Wรฃรร;ร 9<;

รรร;รยรร

ร
รบ

รรรรTรรรTรรรจรรTร	รฆยกรรฃรรTรรจรซ


ร

ยฝรบ
ย

รฟ

รร;ร

 V
รซ

Pย



ยย ย

*(ย2ร

Cย ย

ร	รร.ร	ร.รรงร รรฉ@รTรรขรTรร8ร;รร+รรร{รeรFรรรรรTร=รTรรรรฃร.ร	รeรTรsรร;รรงร;ร
ร;ร6รeรรยร;รรeรร+รร+ร	รยกรรฅยทร

ร ร ร ร;รuรTรรรTรsร

 	 hzรรรยดร[ร;รร	ร

รร ร ร
รฟ

รรร ร

รรรฉ	รรรรรรรร.ร	ร;รยรรรรรรร	รฆ



ร รกYรญ

ร ร

ย (Wiร

?ยร	รรชรรยรAOรYร;รรTรร+รร+ร	รรรTรร2ร{รรงรรรร@ร+รยฝร L
? ร	ร~รย

. รรฆรฃรรร+ร	รยรeรรยรฉ	ร.รรร@ร+รยp
? ร+รฉ	รรTร'F
( รรรรยร

รร;รร+รรร>รย
  รยยยZ[ร
O รรwOยย2รย รกZ[O รก
รร8รรรรฆรนรปยรร


Eร  รรรนรqร รฆ รรร ร รTรรรฃรรรรฆรนรปยรร[รรฅรรรยฉร8รรฅ รeรรยรยรqร รรฉรรTรรยกรรรรร	ร.รรร+ร	รยดรรฅ
รรsร
รTรรจรeรรยรรรร=รรรรรTรAร
ย ร.ร+รรฆรฃรรยร.รรฃรTรรรรยรฅXร ย Y
 รรร;ร.ร	ร~รTรร=รรรรฆรนรปยรร@รรฅ รร;รeรรYร	รรรร รยคร
ย L ร รยกรL>
ยข
ย   ยข ยยฃLยฅยค ยข ?pร+รฉ	รรTรย-ยกรuรรzร8รYรTรรt^รงร;รร ร+รรรร+ร	ร@รปยรร;รsรรงร+ร	ร8รรฅiรTรรรรYรฅXรร;ร.รร+ร	รรรฅXร	รYรรรTรฉ	รรขรzรร+รรsร8รรฅ
ยยก
ย
ย[ย 
O
ร!รยคร รsร	รรรรTรยกรรรFรTรร@ร	รรรรฆรฃรรTร	รuรยคร8ร.ร	ร;ร.รรรรTรรรuร รรงรรจรรTรรงรร;รร
  รยยรsยฆร[ยรsยฆรรก



รรฅYร{รรงรรรร[ร.ร	รTรeรsรร;ร	ร;รรงร+รรฉ2รTร

ยงยยจ

ยฉ6ยชยซยญยฌzยฎยฐยฏยฑยฒzยณ*ยด,ยฎ,ยชzยตdยฏยถยธยทยนยซยบยณ*ยณ0ยฎ,ยช

#worlds4

||P(x)|| x

ยป

0

0.25

0.5

0.75

1

ยผXยฝยฟยพ,รzรรnร,รรรรรwยฝยฟรwยฝยฟร,รรร]รPร'ร,รSรร,รร$ยฝNรzยพGรร2รรยรรรร

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

ยผXยฝยฟยพ,รzรรtรยบร]รร,รรรSรยรรร[รรwยฝยฟร,รรzรzรSรzร,รGรSรzร,รรoร,ร6รร,รwรรรzรรUรรรยยฝยฟรSรยรก}ร
รข รรครSรUรรฅร*ร#รwยฝยฟรRรรรEรรzรรฆรรงร,รรร,รwยฝรร0รรรรฆรรzรร*รร#ยฝNรzยพPยฝยฟรยรรรยญยฝยฃรbรรจรzรรร*รรยยฝยฟร,รยรรยยฝยฟรzยพรชรฉ,รwยฝยฟรwร:ยฝNรzยพยฐรซpรยรรzรzรรร*รจzรฌ
รฃ
ยฝยฟรRรรwยฝยฟร,รdรญ>ร6รยญยฝรรรรรฎร,รร#รSรร[รGรรรรnรรzรรฆรยร,รรรร,รwยฝรร0ร<รฏรฐยฝรรยรUรzรzรร0รจยยฝยฟรRรรรรรNรฑรฒรฏnรณรตรดรทรถIรณ2
 รธรบรนzรณtรรป
 รฉ,รรญ<ร0รoรรS ร
รwรzรผรwร#ยฝNรรรzรwยฝยฟรzยพnร%ร,ร>รรzรยรรzรรSรqรยร$รรร,รwยฝรร0รรรSรญ!รรtรSรUรรฝร*ร#รwยฝยฟรRรรรรฆรพรรฟ 'ร,ร>รถ รฟ รธ  
รฟ 
	 รธ  	ยฃ
รฟ  รธ  	ยฃ
รฟ  รญ

ร6รยญยฝรรรรยรร*รยญรรร*ร>รร2รถ 
ร ยนรzร'รSรยญรรร,รยญรฑRรรรSรรรปยฝยฟรยรรzร<ยพ$รSรzรSร[ร0ร รSร,ร#รtรรwยฝรรwร*ร4ร%รร$ร รรzร'รร#ร<รร4รฉ,ร#ยฝNรรรฌ
รฟ 	  
ร:ยฝยฟรzยพรซpร>รรzรzรร0รจยยฝยฟรRรUรwยฝยฟร,ร2ยฝยฟรEรรยรรร0รยฟร,ยพ,ร$รร]ร6ร0รฑ,รร  ร2ร,รร<รSรรรรoรยญรdร*รwรwยฝยฟรRรรร>ยฝรร6รยญร$รzร>ยฝNรรฆรรzร<รzรรยญรร รร
!รSร2รn
ร zรยฟร,รยฝยฟรยรรรzรtรรzรยฐรSรร$ยฝTรจdร:ร



"!"#%$
&(')$"*"+,#%-"!"#/.0"1"23&4"&657!"8"*9$
:<;

=?>A@CB<DEF>HGJIKL">M>ON9PQGSR">CRTKFU3BVWX>CP>CRWX>CR@O>YGSR[Z\K]U_^>AEKL">`>CRXKaGSPXbScdKL">MRTD"^MeQ>CafG<IghGSa]ViW"E
BSEEFG9@UiB<K>AWjghU_KLkPQG<U_RTKOElG<ImLXU_nSLo>CRXKaGSPXb)E]gB<^PpBVqVGSKL">CalgGra]ViW"E0BSEsZtnSaG<guEvViB<anS>SwyxzLXU3E
@OGSR@O>CRXKaB<KFU{GrR|P"L">CR"GS^s>CR"GSR}c~gh>OVV{ยยยXR"G<gR/U_R)KL">sย>OV3W/GJImE]KB<KFU3EFK]Ui@CB,VP"LXb9EยU3@CEยc7IยGra^0EยKL">leBSEยUiE
IยGSaGSD"al^0B,U_Roa>AE]DXV_KsU{RoKLXUiElE]>A@OK]U_GSR}wยยKYBSEE]>CaKOEKLBJKMU_KUiElPGXEEยU_eXV_>ยKG|@OGS^sP"D"K>ยWX>CnSa>C>AEG<I
e>OVU_>OIBS@C@OGSaWrU_R"nMKG`aOB<RWXGS^ยghGSaFV3W"EยgLXUV_>U{nrR"GSa]U_R"nB,VV%e"D"KKL"GTE]>ghGSa]ViW"EยgL"GTE]>>CRTKaGSPXbfUiE?R">AB<a
^0B,NXU_^MD"^ยwxhL">fR">ON"KuKL">CGSa>C^ย>AEE]>CRXK]UiB,VV_bYIยGSa^0B,VU_ยC>AEยKLXUiEhP"L">CR"GS^>CR"GSR}w
ยMยdยXย%ย,ยยยย}ยยยXยยย}ย<ยfยกSยขยขยฃOยคOยฅkยฆCยงยยจCยฉ%ยชยยขยฌยซยยฃยญยกSยขยข~ยฏdยฎ ยฐ ยชยฑTยจ?ยฒCยSยขยขqยJยณ~ยงiยฉ"ยดยยงiยฃfยชยยค"ยจยตยยถ}ยจ,ยชยทยนยธยจ`ยชยฑXยจhยบ9ยJยงiยฉdยชยฃuยณ~ยงยยชยฑ
ยด<ยยจOยกSยชยยจCยฃCยชยจยยฉ%ยชยยOยบยซยปยงiยฉ/ยผยพ"ยฝ ยฟรfรuร ยก<ยฉdรยยขยจ,ยชhรรรรร รรรยธยจYยก<ยฉยซยยOยบ"ยจCยฉ)ยฃCยจAยชยฆOย<ยฉdยชยยก<ยงiยฉQยงiยฉ"ยดยยทMยตjร}ยฑTยจยยฉ0ยฒCย<ย0ยกrยขiยข
รMรยร?ร ยก<ยฉdรยฒรย<ย VU_^Yร ร[ร VU{^oE]D"PรรFVU{^jU_RXIร ยณยจยยฑTยก<รยจ
ยณย<ยCยขqรJยฃ ร ยฝยพ ยฟ ร ร ร รยรก รfรMร
ยณย<ยCยขร<ยฃ ยพร ยฝ ยฟ ร ร ร รfร`รรฃรข
ร
รครฅ>a>C^0BJaยfKLB<K}KLXU3EรฆUiE~รงrDXU_K>BuWrUรจรฉ@ODXV_KKL">CGSa>C^ยwรฆรคย>hLBAรชr>WrUiE@ODEE]>AWรซgLTb`รฌรญ>C^^0Bรฎ"w_รฏSรฏยV_>CKE
DEรฆV_GTGrยHB<K ยญยAรrยจAยขยฌยฃย]ยฒ รfร gL"GXE]>>CRXKaGSPXbfUiE ร R">ABJa ร ^0B,NXU_^HD"^ยปwร=?D"K~KL">hKL">CGSa>C^6K>OVqViEยDE7KGV_GTGSย
B<KKL">f^0B,NXU{^MD"^ยย>CRXKaGrPTblPG<U_RXKEยG<I ยผ ยพ ยฝ ยฟรfรfร c"gLXUi@L[g>`WX>OยR">AWยปDEยU_R"nYB ร E]GรซIรฐB<aยD"R"^GSK]U_รชBJK>AW ร
E]bXRTKOBS@OK]Ui@fP"aG"@O>AWXD"a>MB<P"PXVU{>AWรฑKG รfร whยยKuE]>C>C^0Eยa>ABSEFGSRB<eXV_>`KGs>ON9PQ>A@OKfKLB<K ยผยพ9ยฝ ยฟรfรร E]L"GSDXViWยปK>OVV
DE ยฃCย<ยญยจAยชยฑSยงยฉ9ยด B<eGSD"K^G"WX>OViEG<I รfร wm=?D"K^0B<ยrU_R"nYKLXUiEf@OGSR"R">A@OK]U_GSRยP"a>A@UiE]>Sc}BJRWYU_R[PB<aKFU3@ODXViB<a
E]L"G<ghU_R"n[L"G<gยKL">l^0B,NXU_^HD"^sยย>CRTKaGSPTb[PQG<U_RTKEยG<I ยผยพ9ยฝ ยฟรfรfร a>OV3BJK>0KGย^G"WX>OViE`GJI รfร ghU_KL/R">AB<aย
^0B,NXU_^MD"^รฒ>CRTKaGSPTbScQUiEยWrUqรจY@ODXV{K,wรณG<gh>CรชS>CaAcQgh>รซWX>OIย>CaHBVqVWX>CKOB,UV3EvG<IKL">HP"aGTG<IhG<IKLB<Kua>AE]DXV_K`KG
KL">`B<P"PQ>CRWrUNรญw
ยยR[nS>CR">CaB,Vc}xhL">CGSa>C^รรฎ"w_รฏAรฎM^lBAbยEF>C>C^รฒKG0eQ>G<I~VU_^MU_K>AWรฅDE]>OIยDXV_R">AEEยรดuยXR"G<ghU_R"nYKLB<Kgh>`GSRXV_b
LB,รชS>[KG/V_GTGrยpB<KYghGSa]ViW"ElR">AB<aยปKL">[^0B,NXU_^HD"^sยย>CRTKaGSPTbรตPG<U_RXKยWXGX>AEYR"GSKยEFD"eE]KB<RXK]UiB,VV_bpa>AWXD@O>
KL">MRXD"^HeQ>Ca`G<IhghGSa]ViW"Egh>HR">C>AWรฅKGY@OGSREยUiWX>CaAw ร ยRWX>C>AWdcรฆKL">gL"G<V_>PQG<U_RTKยG<IKL">0@OGrR@O>CRTKaOB<K]U_GSR
P"L">CR"GS^>CR"GrRยUiEยKLB<KfB,V_^GTE]KB,VVรถghGSa]ViW"EยLB,รชS>fLXU_nSL[>CRTKaGSPTbSw รรซรท >CรชS>CaKL">OV_>AEEยc}BSEhKL">ยa>AE]KG<IKLXUiE
PB<PQ>CamE]L"G<guEยcTKLXU3Eza>AE]DXV_Km@CBJRYe>HรงrDXU_K>`DEF>OIยDXVรgยL">CRย@OGS^MeXU_R">AWYghU_KLยปKL">IยG<VV{G<ghU_R"nlK]gGMa>AEFDXV{KOECw
xhL">?ยaE]K~GJI%KL">AE]>EB,b"E}KLB<K7UIรญB,VVKL">ghGSaFV3W"E7R">ABJa~KL">h^0B,NXU{^MD"^ยย>CRXKaGrPTbfPQG<U_RTKEยLB,รชS>Bf@O>CaKB,U_R
P"aGSPQ>CaK]bSc"KL">CRYgh>`E]L"GSDXViWรธLBAรชr>`WX>CnSa>C>`GJIรe>OVU_>OI`รฏfKLB<KhKLXUiEยP"aGSP>CaKยbsUiEhKaD">Sw
qV U_^ ร~ร a ยพf
ร ยฝ ร ร9ร รfรHร~ร รzVqU_รf^ ร รยร
รรfร

รน ยdยAยdรบiรบ3รปย,รผpยรฆยยยSรฝยย}ย<ยuยกrยขiยข
ยฃยคOยฅkยฆCยงยยจCยฉdยชยยขยซยยฃยญยกSยขยขรฆยฏdยฎ ยฐ ยชยฑTยจ~ยฒCยSยขยขqยJยณ~ยงiยฉ"ยดยยงiยฃuยชยOยค"ยจยตยยถ}ยจAยช~ยทยยธยจfยชยฑTยจhยบ"ย<ยงยฉ%ยชยฃยณ~ยงยยชยฑ
ยด<ยยจOยกSยชยยจCยฃCยช7ยจCยฉ%ยชยยOยบQยซHยงยฉยยผ ยพ ยฝ ยฟรfรร ยฐ ยขqยจ,ยช7รยร)ร ร ร ยธยจfยก<ยฉยยรยบ9ยจCยฉยปยฃรยจ,ยช~ยฆยJยฉ%ยชยยก<ยงยฉยงยฉ9ยดMยท ยฐ ยก<ยฉ%รยขยจ,ยช ร ยฟ ร ร รยรรพ ยธยจ
ยก<ยฉยยก<ยฃOยฃรยจยยรยชยงยย<ยฉยยชยฑTยกSยช?ยฑTยrยขqรJยฃ~ยฒรย<ย`ยจยรยจยยยซlยณย<ยCยขqรMรฟ ยฃOยค"ยฆCยฑYยชยฑTยกSยช ร รฟ ร ร ร0ยตsร}ยฑTยจยยฉ
ร a ร ยฝยพ ร ร ยฟ ร ร ร รfร`ร~ร รฏ
รข
 รปdย7รบ3ยรฑยรฆยยยรย
	 GSafKL">ยTR"G<ghV_>AWXnS>leBSE]> ยชยยค"ยจ U_
R 7NB<^PXV_>Yรฎ"w_รฏ"cQU_KvU3Eย>ABSEFbยKGยE]>C>0KLB<KfKL">
^0B,NXU_^MD"^ยย>CRTKaGSPXbPG<U_RXKhUiE ร ร  ร w 	 UqNยE]GS^s>mB<aeXU{KaB<a
b   
w ~V_>AB<a]V_bScQKL">Ca>ยUiEE]GS^>GSPQ>CR

รข

รข


E]>CK ร B<aGSD"RWรฑKLXUiEPG<U_RXKHE]D@L[KLB<KfKL">BSEEF>CaK]U_GSR ร ร ร_ร  ร  ร ร_ร Yร ยฟ 
Cร 
 ร L"G<ViW"EIยGSa
รข
รข 
>CรชS>CablghGSa]ViWsU_R ร whxhL">Ca>OIยGSa>ScXgh>`@CB<Rย@OGSR@V_DWX>MKLB<K
ร Sa ร ยฝยพ ร ร3ร ร ร ร_ร Hร ยฟ 
Cร
 ร ร ยชยOยค9ยจ ร~ร รฏ
รข !  รข
รข
" Ehgh>`E]L"G<goU_R ร =hBS@C@LTDEh>CKfB,Vw_c}$รฏ #%#'& ร crIยGSa^MDXViBSE ร gzU{KLยWX>CnSa>C>ยG<I~e>OVU_>OIvรฏย@CB<RY>AEE]>CRXK]UiB,VV_b
e>sKa>AB<K>AW)(]DE]KVU_ยS>0GrKL">CafยTR"G<ghV_>AWXnS>รซU{R รfร wMxhLB<KUiECcรฆKL">0WX>CnSa>C>AEG<IeQ>OVqU_>OIfa>OViB<K]U_รชS>KG รfร
B<RW รfร รกยร ghUVqVhe>MUiWX>CRXK]Ui@CB,V ร >CรชS>CRยปUqI รfร BJRW รfร รกรธร B<a>MR"GSKV_GSnJU3@CBVqV_bย>AรงrDXU_รชB,V_>CRXK ร +
w *ยปGSa>
IยGSa^0B,VV_bdรด

,.-

/!0214357698;:<=%5%0>?6@BAC1D<<$5%0
EGFIH4JLK$HNMPO?QSR4TVUXWY;Z%[\[^]4_N`ba\cdZ$egfih;jklk'mnporqds9t'vw u Wxyz{bnp|}j~'ยLย
eยยiยยยยยยย\eยยiยย`ย_ยยยeยยiยยiย4ยย%ย
ยยยยย ยdq\ย'ยG~'ยNย!q\ย'ยยยGย2ยย~+ย!ย
eยยiย ย s9t vย u Wย!yยขz{n9| eยยยฃย w ย s9t vย u Wย!yz{ยฅยคxlnยงยฆ
ย ยก w
!
ย!ยก
ยจ K$JIJNยฉยUยซยชDยฌ%t[ยยฌ%ย+ย4eia\c^a\ยa`ยญ`\h;ยฎ;aยฏt^a\ย7aZ'ccยญ]aยยt^ยฌยยฌยฐยยยฑt^ยฌlยPWY;Z%[\[ยญ]ย_N`Ga\cยZ$egfih jk%k'mDnp]a\t^a%fยYยฒZl`ยณยยด[
ยt^ยฌ%ยตNZยฐยต4ยยขeยยยด`ยcยยยด[bt^aZ%`ยถยฌ%ย4ยiยยทยธ`ย]ยฌ'ยฎยน`c^]NZยฐch4ยSยฌ%t!Z'ยยยบยยปยผZ'ยNยฝยฟร?ยพ ร
s9t ย uv Wย!yz{pn9|ยs9t vย u Wยย!yรรยยคยธxlns9t vย u Wยx2yยขz{pnยฏรยซsรt vย u Wย!yรรรรรยคยฏรรxlnรsรt vย u W^รรxyz{Gnยงยฆ
YยบยZ%`ยญ`ย_ย+ยcยยiยฌ%ย?h s9t'vย u Wยx2yยขz{dnGc^a\ยNยฝ`
c^ยฌยjรยฎ!]a\ยยยฎยฒaรcยงZ'ร%aXeยยยฃยรยยฃcย`\h`ยยฌc^]aยรNtย`ยcยc^a\t^ยรc^a\ยNยฝ`
c^ยฌ
s9t'vย u Wย!yz{ยคdxlnยงfCร!ยยc^]aยฌ%c^]a\t;]NZ'ยNยฝIhs9t'vย u W^รรxyz{Gn9]NZ%`รeยยiยGยicรรf9Ya[\Z'_N`ยapsรt'vย u Wย!yรรรรรยค)รรx%nรยr`
ยตNยฌl_ยNยฝ4aยฝIhNยฎ;ap[ยยฌ%ยN[ยญei_Nยฝ4aGc^]NZ'c!cยญ]ap`ยa[ยยฌ%ยNยฝยยtยญยฌ2ยฝ4_N[ยcยนZ.er`ยถยฌ+c^a\ยNยฝ`c^ยฌ+รfVร;]at^a`ยถ_4eยฃcยSยฌ'eยeยฃยฌ'ยฎยน`รf
ร `9ยฎ;aยน`ยถ]NZ$eยeร`ยa\a;ยiยยc^]aยaยร2c;`ยa[ยcยยiยฌ%ย?h4c^]aยน[ยยฌ%ยGยต4ยiยNZ'cยยiยฌ%ย
ยฌ'ย9รVยฌ%tยญยฌ'eยerZยฐt^ยบ+รfij\mpZ'ยNยฝ)ร;]a\ยฌ%t^a\ยรรfijร
ยยด`ยนรl_4ยic^apย7ยฌ'ยฎยฒa\tยถยยฑ_4egf
ร9ร
รกรข?รฃยรครฅ9รฆ'รงgรจ9รฉรซรชรฌรฉรรญlรฌNรฌNรฎ
รขIรฏGรฐ!รฌรฑSรงgรฌDรฏ
ร ยฃe cยญ]ยฌ%_ยท%]pcยญ]aV[ยยฌ%ยN[ยa\ยยcยญtยงZ'cยยiยฌ%ยbย]a\ยยฌ%ย+a\ยยฌ%ยรฒยr`รยiยยcยญa\t^a`ยcยยiยยทNh$ยicยง`?Zยฐยย4eยขยยด[\Z'cยถยยฃยฌlยdcยญยฌยนZ%[ยc^_NZ$eยeiยบp[ยยฌ%ย+ย_cยถยยฃยยท
ยฝ4a\ยท%t^a\a`ยฌ'ยVยต7aยeยขยiaยย;ยZ$ยบยยยฌ%c!ยตNaGยฌ%ยต4รณlยยฃยฌl_N`\f รด'ยiยN[ยadยฎ;abรยยยฌ'ยฎรตc^]NZยฐcยนZ$eiย+ยฌย`ยcZ$eยeรยฎ;ยฌ%tยeยดยฝ`ยฎ;ยยeยeV]NZรณla]4ยยฃยทl]
a\ย4c^t^ยฌ%ย4ยบ%hNZ
ยฝlยยฃtยญa[ยcรZ'ยย4eยยยด[\Z'cยยiยฌ%ยรถยฌ'ยรรC]a\ยฌ%t^a\ย}รfijรGยฝ4ยฌ4a`!ยยฌ%c`ย_ยตN`ยcยZ'ยยcยถยrZ.eยขeiยบรt^aยฝ4_N[ยapc^]abยย_ยGยต7a\tยนยฌ'ย
ยฎ;ยฌ%tยeยดยฝ` ยฎ;a+ยG_N`ยc[ยยฌ%ยN`ยณยยดยฝ4a\tf+รท9a\c$hรZ%` ยฎ;a+`ย]ยฌ'ยฎรธยiยรนc^]4ยยด`p`ยa[ยcยถยยฃยฌlย?hรcยญ]aร[ยยฌlยN[ยa\ยยc^tยZ'cยยiยฌ%ยรนc^]a\ยฌlt^a\ยรบ[\Z'ย
ยSยฌ%t^ยรปc^]apยตNZ%`ยr` ยฌยฐยCZ)ยtยงZ%[ยcยยยด[\Z$e9c^a[ยญ]ย4ยrรl_aGยSยฌ%t[ยยฌ%ย+ย_cยยiยยทยฏยฝ4a\ยท%t^a\a` ยฌยฐยVยตNaยeยยiaยย!ยiยรย
Z'ยยยบยฏ[\Zl`ยa`\f รผรนa
ยตNa\ยทยฐยยฃยยยฃยรฝรด%a[ยcยยiยฌ%ยmNfrjยตยยบรยt^a`ยa\ย4cยยiยยทยc^]aยiย4c^_4ยicยยiยฌ%ยN`_ยNยฝ4a\tยถeยฃยบlยiยยทยฏc^]4ยยด` c^a[ยญ]ย4ยยดร%_a%fรฒรพgยรฝรด%a[ยcยยiยฌ%ยmNfรฟ
ยฎ;aGยต_4ยยขeยดยฝรถยฌ%ยรนc^]a`ยถapยiย4c^_4ยicยยiยฌ%ยN`bยตยยบรยt^a`ยa\ย4cยยiยยทยฏt^a`ยถ_4eยฃcย` ยSยฌ%tpZ
t^a`ยc^tยถยr[ยcยญaยฝรน[ยญeยดZ%`^` ยฌ'ยยยฑยฌ%tยญยd_4eยดZ%` ร cยญ]ยฌย`ยa
รl_a\tยยia`
ยฎ!]4ยยด[^]รตZ't^aร%_NZยฐยยcยยยรNa\t ยยSt^a\aรยSยฌ%t^ยG_4eยดZ%`+ยฌ'รณ%a\tยZรถ_ยNZ't^ยบรฝeยดZ'ยยท%_NZ'ยทlaรยฎCยยฃcยญ]ยZรฝ`ยณยiยยท'eiaยซ[ยยฌ%ยN`ยcยงZยฐยยc
`ยยบ4ยGยตNยฌ'egf+รพยณย `ยถย4ยยฃcยญa+ยฌ'ย!c^]4ยยด`t^a`ยถc^tยยยด[ยcยยiยฌ%ย?hยZ'ย4ยบยฏยฌ'ย;c^]aGยยด`^`ย_a`GZ'tยถยr`ยยฃยยทXยiยยซc^]a)ยท%a\ยa\tยงZ$eยฒ[\Z%`ยถa[\Z'ยรนยต7a
`ยa\a\ย]a\t^a%
f รยฌ%t^a\ยฌ'รณ%a\t$hNZ%`;ยฎ;ad`ยถ]ยฌ.ยฎรยยฃยรฝรด%a[ยcยยiยฌ%ยmNfรhc^]4ยยด` t^a`ยcยญtยยยด[ยc^aยฝยฏ`ย_ยต4eยดZ'ยยทl_NZ'ยท%aยยด` tยยยด[ยญ]a\ยยฌ%_ยท%]
c^ยฌ+Z$eยeiยฌ'ยฎรต_N`;c^ยฌGa\ยGยตNaยฝXcยฎ;ยฌbยฎยฒaยeยe Sร4ยยฌ.ยฎยยฏยt^ยฌ%ย7ยฌย`ยณยicยยiยฌ%ยNZ$e9Z'ยยt^ยฌยZ%[ยญ]a`c^]NZ'c;ยZ'รla_N`ยaยฌ'ยรยZ.รยยiยG_ย
a\ย4c^t^ยฌ%ย4ยบ 
` 	7ยยถย ^~ ยฑย  ย 
ยยขย 
W  ยยขeยด`^`ยถยฌ%ย?h!jk lร4nCZยฐยNยฝc^]a
ยZ$ร4ยยฃยG_
ย Sa\ย4c^t^ยฌlยยยบรaยร2c^a\ยN`ยยฃยฌlย
ร  ยยeยด`^`ยยฌ%ย 

ยฌ'
ย g`ยa\ย
Z'ยยcยถยr[\`)W  a ?ยa\!
t
saZ'tยถe h;jklk%ร4nยนยฝ4_a
c^"
ยฌ ยนยฌ'eยดยฝ` #\ยรยrยฝ4c$$
h รยฌ%t^tยยยด`\hsaZ'tยe!Wยงjklk%ร4nW`ยa\a
Z$eยด`ยยฌ
W  ยฌ'eยดยฝ` #\ยGยยดยฝ4c%
h รยฌ%tยญtยยยด`\&h saZ'tยegh9jk%k%ร2n^nยงf รพgยรด%a[ยcยถยยฃยฌlยรนmNfmNh7ยฎ;a+[ยยฌ%ยN`ยณยยดยฝ4a\tpยฎ!]a\cยญ]a\tc^]aGt^a`ย_4eicยง` ยSยฌ%t
c^]abt^a`ยc^tยยยด[ยc^aยฝยธeยดZ'ยยท%_NZ'ยท%a+[\Z'ยรยตNaGaยรc^a\ยNยฝ4aยฝIf!รผรนaG`ย]ยฌ'ยฎรตc^]NZ'c c^]a\ยบยฏ[\Z'ย?h7ยต_c`ยa\รณ%a\tยงZ$e9ยฝl(ย 'ย[ย_4eยฃcGZ'ยNยฝ
`ย_ยตcยeiaรฒยr`ยญ`ย_a`ยนZยฐtยยยด`ยa%f
QSR EรFLH"*NH,+IH2K.-0/2143K.-53\H*06
ร eยฃcยญ]ยฌ%_ยท%]ยc^]atยงZยฐยNยฝ4ยฌ%ยSยฎ;ยฌ%tยeยดยฝ`ย+a\c^]ยฌยฝ)ยยด`!ยฝ4aยรNยaยฝยยต4ยบ[ยยฌ%_ย4cยยiยยท+ยฎ;ยฌ%tยeยดยฝ`\hยฎ;a[\Z'ยยฏ`ยยฌ%ย)a\cยยiย+a`รรNยNยฝ
ย+ยฌ%tยญaรยฝlยit^a[ยcGยฎ!Z$ยบ2`c^ยฌยฏ[\Z$eยด[ย_4eยดZ'c^a
c^]aยฝ4a\ยทlt^a\a`ยฌ'ย;ยตNaยeยยiaยยยนยicbยบ%ยiaยeยดยฝ`\f
รพgยWยYยฒZl[\[^]4_N`a\cpZ$egfihjk%k'mDnยฒยฎ;a
ยt^a`ยa\ย4cZ+ย4_ยGยตNa\tยฌ'ยV`ย_N[ยญ]รcยญa[^]ย4ยยดรl_a`\hIย+ยฌย`ยc ยฌ'ยVยฎ!]4ยยด[ยญ]รนZ'ยย4eiยบยฏยฌ%ย4eiยบ
ยiยรณ%a\t^ยบยฏ`ยย7a[ยญยrZ.eยฒ[\Z%`ยa`\fรฒร!ยa
ยฌ'ย?c^]aยน`ยยฃย)ย4eยฃa`ยถcCZยฐยNยฝ+ย+ยฌ4`ยcยiยยc^_4ยicยยiรณ%a ยยด`รc^]a!ยSยฌ'eยeiยฌ'ยฎ;ยiยยทdรณla\tยง`ยณยiยฌ%ย+ยฌ'ย?ยฎ!]NZยฐc9ย]4ยยขeiยฌย`ยถยฌ%ย]a\tยง`;]NZ$รณ%a!cยญa\t^ย+aยฝ
ย ยย ย  ย ยยq ย ย ย 7ย  ย 9W 8;aยยยด[^]a\ย4ยตNZ%[ยญ]?h!jk'm4k4nยงfรรด%_ยยNยฌย`ยถaยc^]NZ'cpZ$eยe!ยฎ;a+รยยยฌ'ยฎรZ'ยตNยฌ%_c+Z'ยยยฃยNยฝlยiรณlยยดยฝ4_NZ$;

e :dยr`
`ยยฌ%ย)apZ%`^`ยa\t^cยถยยฃยฌl=
ย < 9W :$?n >ยฐยiยยฏยฌ%c^]a\t;ยฎ;ยฌ%tยงยฝ`รhNz{รธ]NZ%`;c^]a ยSยฌ%t^@
ย < 9W :$n7ยคz

{ AShZ'ยNยฝ
c^]ap[ยยฌ%ยN`ยcยZ'ยย
c :ยฝ4ยฌ4a`
ยยฌ%cยนZยฐยยNaZ't ยiยรนz{ A f ร eยด`ยยฌยฏ`ย_ยย7ยฌย`ยabc^]NZ'cz{+h7c^ยฌ%ยท%a\c^]a\tยฎ;ยic^]รZGยNZยฐt^cยยยด[ย_4eยดZ'tc^ยฌ'eia\tยงZ'ยN[ยaร ยพ hยiย+ย4eยยia`
c^]NZ'
c B\ย9W Cรnยงy < 9W CIn BDGยยด`ยiย`ยถยฌ%ย+aยiย4c^a\t^รณ'Z$
e EGFย H%Igf;รพยณcp`ยa\a\ย`t^aZ%`ยยฌlยNZ'ยต4eiadcยญยฌยธZ'tยญยท%_apc^]NZยฐJ
c :รยr``ยถ]ยฌ%_4eยดยฝ
ยตNaGc^tยญaZ'c^aยฝยฏZl`ยนL
Z Kยcยยบ4ย4ยยด[\Z$Ne M+aยeia\ย+a\ย4cd`^Zยฐcยยยด`ยณยยฑยบlยiยO
ยท < 9W CInยงhDยตNa[\Z'_N`ยaGยต4ยบรZ%`ยญ`ย_ย+ยcยยiยฌ%ยรถz{ [ยยฌ%ย4cยงZ$ยiยN` ยยฌ
)

PRQ

S$T5U5V&WYX[Z\W5]5^.V&_`T5U5Vba=T5c5deX$f5XhgiU5j5]kWYlm

noprqtsu=vwnqoLxy5ztz{Rxwno5zOqw|5{4s~}2nNx{ยยย|5{4s{prqts{ย}2{ยuยnz|w
|5qย0{ยwqOy0x{ยw|5{ยxw?vwnNxwยneย4x
ยtns{Rยwยยยยย
vo0ยยยqo0ย~ยy0ย{
w|0vw2ยยsยย
ยย ย9ยยย9ย.ย4ยย
ย
ย$ยLยGยยยย%ยยขยก ย2|nNxยnNxยฃno0ย{4{RยOw|5{`ย4vx{ย5vxยw~|5{pqยยคยqยฅ}ยnยo5zยw|5{4qts{4u
x|5q}x4
รย ย5รยย
ย
รNร
ยฆยยง7ยจยฉ&ยช.ยจ0ยซยญยฌ%ยฎยฏยฐ ย9ยฑ vย4ย~|y0xยฃ{4wยvยฅยยฒeย&ยณ.ยดยดยต ยยยถยธยท.ยนย
ยยปยบ~ยท
ยผยฝ.ยพ&ยฟรรยคยทรRรยท!ยบ~ยผร4ยท
ยฟรJยนรยท$ร4ยฟรรhรย.7
ยผยพ7ร=ยผรรรรยทยยนรยผยน,ร4ยฟรยยผรร7รร?รรรรรrยทรยพ&ยนรรยร?รยผรรยนยฟรยคยทรรยยผรยพ&รยท
รยฅยทรRยนยฟรร?รยร ร ร
ย
ยรยยขร ร ย%ย รรรรยยยยฅร ร ย?ย ร`ยร ร ย?ร

ยLยย$ยยย%ยรยก
ยร

ยยฃs ย

ร ยผรขรขkยทยผรรยรNยพbย
ย ร รรยพรฃยยยร ร ยรยฟรรยพรฃร`ยร ร ยรยนรยทรยพ
ร รยยพ&ยฟรร?ยฟรยพ0ร4ยนยผยพ&ยนยรยพรยรก

ย9ยยยRรค
รย ยยย
ยยJยรฅยGยยยย%ยรฆรร ร

ยนรยทรtยท9รรรยยทยท!ยฟยรยยบยท.รGรยทร
ยท4รง.รNร4ยนร!ยผยนยยผรNร รจรฉ

ยย|nexรชs{RxyยwRยYnoLยquยรซnยo0vรwnqoรฌ}2nw|รw|5{!s~{Rxyยw?xqรpรญw|5{ย5s{4รฎtnยqty0x
x{Rยwnqoย,ย5sqรฎnNย{Rxรชy0x}2nw|"v
รฎ{4sยยย0q}2{4spyยwqqรยยฒยรฏ2qy5z|ยยรฌxย,{Rvรฐno5z0ย,}2{
ย5sqย,qx{
wqy0xย{
w|5{`prqรย(ยq}2no5zOxws?vw~{4zย&รฑยย2|5{
รซ0vtxยขnNย
ยqo0ย{4ows?vwยnยqtoLย5|5{4o5qu{4o5qoรx~vRย5x`w|0vwuqxw
}2qsยยeย5xยvs~{รฎ{4sยLxยขnuยnยคยNvsnoรฒvรย{4sw?v.no\x{4o0x{Oรณ
x
x|5q}oยnoรรด$qsqรย(ยNvs~ย=รต5ยณ4ยต,ย}รถ{`ย4vo=y0xย{w|nNxยwq
รท0o0ยยxqu{`vxx{4swยnยqto0x$w~|0vw$vs{ยรธรv.ยuqxw2ย{4sw?v.noยยkรน
wsy5{ ย nรรบ{eยR}2nw|ย{4zs~{4{รถqรp0รซ0{ยยคn{pรถยณ ย {4รฎ{4on(p7w|5{4ยยvรs{$o5qwรคยqznNย4v.ยยคยยยnuยยยคn{Rยรซย

ย
ย

รญยย|5{4qs{4uhรต5ยณRรป

w|5{4oรผw{ยยคยNxOy0xยw|0vรw=}2{Lย4voรผws{Rvwยw|5{Rx{"o5{4}รฝvx~x{4swnqo0xOvxรnยคpยw|5{4ยรผvs{รnoรผpรvยw=รฐo5q}oรผ}2nw|
ย{4sw?vยฅnยowยiรพรฟ|5{4oยw|5{Rx{o5{4}รผvxx{4swยnยqto0xยxw?vw{xwvwnNxwnNย4v.ยรญรธรฐo5q}2ย{Rยz{Rรน5ยtw|5{4ยยย4voยรฎยฅvxwยยยยno0ยs{Rvxย{
qy5sqtย5ย0qsw~y5onยwยnย{Rxรชwq=vรย5ยยยยรฌยtns{Rยwรชnopr{4s~{4o0ย{รถย2|5{;prqรย(ยq}2no5zย{Yvuยยยย{;nยคย(ยy0xw~s?vw{Rxรชw|nNx2nNย{Rv5

 ยซ
	eยจรฃยฌiยฎ%ยฐ

รดยqo0xยขnNย{4svOรฎ{4sยLxยขnuยย{Oรฐo5q}2ย{Rยz{=รซ0vtx{=qรฎ{4s!v

รฎq5ย4vรซ5yยNvsย

ยqow?v.nono5z"w|5{

xยขno5zย{ยy5o0vsยยย5s{RยtnNย4vw{รฑ
ย
ย
ย2|5{4s{!vรs{w}2qvwquยx
w|nNx ย
ย

ยยeย 
ร

ย ร ย?ยย
ยก รต ย?ยก
ร

vo0ยรชqรฎ{4s=ย}2nw|





ร ร nex`ย~ย{Rvsยย

znรฎ{4o

#ยย ยยคย
ย
ย7ร

 ย%$




ย&$  ย$ย'

hvo0ย  ร"! ย$ย2|5{!xยqยy5wnqoรฌxย0vย{
qp
ร

$
รฑ

รณรxwsv.nz|wยprqs~}ยvรs?ยยquย5y5wvwnqo!x|5q}x7w~|0vwRยรprqs ร
*
ย0qรnยow 1 ร ร ย ยก 
รต ) ร ย 3ยก 24 ร ย 



ยก รต)



(

ร   ยก

ยก,+ ย4w~|nexรค|0vxvยy5on.-ty5{2u=v/nuยy5u0{4ows~qยย


 

5 q}!ย0ยqo0x9neย{4s
w~|5{-ty5{4sย ย9ย.ย  65qsvยฅย(ย798
รช
ยYยย{4w;: ย 7 ย รซ,{ยw~|5{
pqsuยyยNv ยย  ย ร ย?ยย ย\ยยย ยก รต)


ร
<
ย
4
ร
ย
ย
ยก
ย
ย
7

รต
)
=
)
7

2
ย

|
N
n
=
x

x
ร
v

w
N
n
ยข
x
0
รท
R
{

x
~
w
5
|
รฃ
{

ย

q
0
o
t
ย
nwnqoรผqpยรด$qsqรย(ยNvs~ยรรต5ยณ4ยต0ยยxqLnwยprqรย(ยq}xยw|0vw
ร 
ร 

ยย ย : ย 7 ย9ยย
ยย$ร ยณ ยก?> xยขno5z ยย|5{4qs{4uยปรต5eยณRรป5ย}2{รฐo5q} w|0vw2pqsยยยคnuA@ ย 4ยยคnยu nop ย ยยคnuรฟxy5ยBย
ยยs ย
ยยคnu ย @ ยยฃs ยย
C

CD

ย 

ยยคnu ย @ ยยs ยย ย 
C

ย9ย.ย4ยย
ย
ย$ร

ย9ย.ยย(ย
ยรร

CD

: ย 7 ยยยก

ยฑ y5w2o5qยฅ}รผ}2{
ย4vo=y0xย{!ยtnยs~{Rยw2nopr{4s{4o0ย{ ย 5รชqw{`w|0vwย|5{4s{ย5qy5s!รธรฐo5q}2ย{Rยz{Rรนยvรซ0qy5w ย nNx2รฎยฅvยy5qty0x4ย
nรรบ{ย%รธ ยนร?ร5ยท4ย9ย.ย รน5 ย รพ

{ยยqo0ย~ยy0ย{ยw|0vwRยtnยคpยw|5{4s{
nNxvรoยยยยคnยuรnยw!vรwยvยฅย(ยรย,w|5{4oรฌo5{Rย{Rx~xvsnยคยย
ยย ย 
ยยs ยย

F q0ยpqsv.ยยคยG7 8


ย9ย.ย4ยย
ยรร

: ย 7 ยยฒยรถยLยยคย

ยก รต)



ร  ย
4

7 ย4ย


ยก รต)

ร  ย )E7 ยรยก

ย
ยยs ย ยย

ย 

ย9ย.ยยย
ย
ย$ย

ยยคย



ยก รต)

ร  ยH4

7 ย4ย


ยก รต)

ร  ย )I7 ยรยก

F no0ย{
w|nNxinNxยwsy5{2pqs2v.ยยคยJ74ยw|5{qtoยยยยย0qxx9nยรซย{
รฎv.ยy5{pqsรถยยฃsย ยย ย  ย9ย.ยย(ย
ย
ย nNx ยก รตK) ร ย}|nNย~|nNxยw|5{


ย nรรบ{eย ยย  ย ร ยยย ยJยผtยน w|5{ยu=v/nยuยy5uL0 {4owsqtยยยย0qnowR5รชqw{w|0vwรชnwnNx
v.ยNxqOย~ย{Rvs
}|0vw

ร
ยก รต5
|0vย5ย,{4o0xvx ร ร w{4o0ย5xw~q ร รฑ$ยยs ย ย  ยย.ยยย
ย
ย nNx


รฎยฅvยฅยยy5{ยqp $

MON

PQSRUT?VXWZY[?\^]OVOQ?_W
`bacRd\^\/VOQ

ecfUg.hBijlknmLoUpqi<rUismLtvulhxwzy{knw&i^hlw&f?im|k/gqu}hxwzisolh~tn?tOu?iยoltยhzhยgยยUpqihxwzyknwzisยOยยtOyHยtvmLo?ย?wxgqu?ยยrUisยvyzisi^h
tnlยXipยgqizยยBgqy{hxw
wzf?im|k/jUgqmย?mยยisuยwzy&tOoยย9oXtngquยwhtnlw&f?ichยolkOยiยS
ยย ย3ย9ย9ย kny&i<ยtOmLo?ย?w&i^rkvhHkcยย?ulยwยgยtvu
ยย ยยยยก ย9ย9ยข ยkOhzhxย?mยฃgยu?ยยคwzf?ip3gยmยฃgยwijยgยฅhxwh ย g3<u?tvw^ย~wzf?i
tnLย ย ย;ef?isuยw&f?i^hxiLknyzi}ยlhxi^rยwztยtOmLo?ย?w&i|ยZy ยย
p3gqmยฆhxย?oยknulrยp3gqmIgquU9tnKยKyzยง ยยยยก ย9ย;ยข kny&i;ยtOmLo?ย?wzi^rAgยulhยwzi^kOr ยข ยยBgqulk/p3pqยOย~ยจi;ยtOmLo?ย?wzi9w&f?i9p3gqm}gqwtn
wzfUgยฅho?yztOยlknยUg3p3gqwยยkvhLย ย ยOtUi^hwztLยฉsisyztXย
ยชuUยtOyzwzย?ulknw&ipยยOยnw&fUg.hZhยwzy{knwzisยvย9flkOhHk9hยisyxgqtOยlh
oltOw&isuยwxgยฅk/p?o?yztOยUpqismยKยซยฌi<ย&pqi^knyxpqยLยsknu?u?tOwยtvmLo?ย?wzi
ยZy ย ยย ยxยยก ย9ย;ยข hxisolkny{knw&ipยยยtOyi^kvยzfLtยญJw&f?igquUยฎluUgqwzipqย|mยknuยย}wztยญpยisyknulยiยฏvi^ยwztOy{hย ย knulrwzf?isuLw{knยฐviwzf?i
p3gqm}gqw;kOhย ย ยvtยi^hwzt|ยฑ?ยยยซยฌim}gqยOfยw f?tOoXi;wzt|ยtvmLo?ย?wzi;w&fUg.h o?y&tOยlknยUg3p3gยwxยยkOhknuยคijSoUp3gยฅย&gqw9ยย?ulยwxgqtOutn
ย ย ย~knulrwzf?isuยยtOmLo?ย?wzi}wzf?iยฒp3gยmยฃgยw/ยย?tvygqulhxw{kยญulยiOย?gquยฌยณ
jlknmLoUpqi}ยดlย,ยตLยKy ย ยย ยยยถยยยท ยข ยก ย9ย9ยข ยจkOhยยtOย?ulrยคwzt
ยliยฑ?ยธ,ยนยบ ย^ยป ยZknulrยฌhยtgqwg.hi^kOhยยยผw&tยผhxisiยยจflknwflkno?olisulh}kOh ย^ยปยยฝ
ยฑdยAยพยย?w}wzf?isyziยฃg.hu?tยyzi^kOhยtOuยฌwzt
ยlip3gqisยฏOiLw&flknwยยKy ย ยย ยยยยก ย9ย9ยข gยฅhsยXgquยผยvisu?isy{k/p%ยยฟkยญui^kOhรg3pqยยฌยzflkยญy{kOยwzisyxgqยฉ^knยUpqiยย?ulยwxgqtOuยtn;ย ย ยรยยgยwgยฅh u?tOw^ย
wzf?isuEยtvmLo?ย?wxgqu?ยยwzf?i}p3gqm}gqw|kOhรย ย ยOtUi^h9w&tยฑยsknuยฌยXiArvg3รAยยUpqwLtvy gยmoltยhzhยgยยUpqiOยยคยซยฌiLยจtOยUpยฅrยคp3gqยฐOiwzt
ยฎlulrkยจk^ย}wzt}k/ยฏOtngยฅrLw&fUg.hcijSoUp3gยฅย&gqwp3gqm}gqwxgqu?ย|o?y&tSยi^hzhk/pqwztOยOiswzf?isy/ยHรรwwzย?yzulhยtOย?wwzflknwยwzfUgยฅhยgยฅhZgqulrUisi^r
oltUhzhรgqยUpqigquhxtOmLiย&gqy{ยย?m|hxwknulยi^hsยef?im|kgยuyzi^รOยUgqyzismisuยwKg.hKwzflknwZwzf?im|k/jUgยm}ย?mLยยisuUwzyztvoยยoltngquUw{h
tnย ยย ย3ย9ย9ย ยtOuยยฏvisyzยOiw&twzf?im|kjยgqm}ย?mLยยisuUwzyztOoUย;oXtngquยw{h
tยญย ร ย ย3ย9ย9ย ย ย ยdtOyBยย?wzย?y&i<y&iยisyzisulยiOยnu?tvwxgยฅยi
ย9ย ยยย
wzflknwย ร ย ย3ย9ยย gยฅh;w&f?iAย&pqtยhxย?y&i|tnwzf?ihxtnpqย?wxgqtOuIhxolkOยiยtnwzf?i|ยtvulhxwzy{k/gquUw{h9tvย?w{k/gqu?i^rยยyztOm
yzisoUpยฅkOย&gqu?ยk/p3pZtSยsยย?yzy&isulยi^h tn<รรยยยยครรkยญulrk/p3pZtSยsยย?yzy&isulยi^h tnรยรยยยยฌร9ย ยข รรuยผmยknuยยhxยlย&fยผยskOhยi^hsย
ยจiยsknuยฌยtOmo?ย?wziยKy ย ยยยยก ย9ย;ยข rvgqyzi^ยwxpqยgquยฌw&isyzm|htnw&f?imยk/jยgqm}ย?mLยยisuUwzyztOoUยoltยญgยuUw{htnย ร ย ยยย9ยย ย
ยจgqwzf?tOย?ww{knยฐvgยu?ยยฃpยgqm}gqw{h9kยญwckpยp%ย
ร h wzf?i}ยtnp3pqtnยจgqu?ยยผijlknmLoUpqi|hยf?tยจ s
h ย~wzfUgยฅhwยยUoli}tnยtOuUwxgquUยUgยwxยยฌrUtUi^h9u?tOw9f?tnpยฅrgquยฌยOisu?isy{k/p%รwzf?i
m|k/jUgqm}ย?mLยยisuยw&yztOoUย|oXtngquยw{hctn<ย ยย ย3ย9ย ย rUtLu?tOwu?i^ยi^hzhzknyxg3pqยยผยtvuยยฏOisy&ยOiw&tLwzf?tยhxi9tยญ<ย ร ย ย3ย9ย ย ย
รรร~รร
ร.รยผร
รqร~ร9รtvulhรgยฅrUisywzf?i;ยฐUu?tnยจpqi^rUยOi9ยlkOhxi
ย9ย
ร

ยรยก.ยกรยถยยร ยข ยกqยก ร ร ยป ยฑ?ยธ,ยนร

ยกqยกรยถยยร ยข ยกqยก ร รรยฑ?ยธรยด ยข~รก

ยกqยกรยถยยร ยข ยก.ยก ร|ร
รข รฃ ยฑ?ยธรยดยธ

ร%wยฒgยฅh9i^kOhxยยคwzthxisiLw&flknw}ย ร ย ย3ย9ยย gยฅhรคxยlhxw}รฅ ย ยฑ?ยธรฆยน?รง{ยฑ?ยธยรจ ยขzรฉ ร;ef?iLoXtngquยw ย ยฑ?ยธรยดlรงยฑ?ยธ,รช ยข gยฅh;rvgยฅhzkpยpqtnยจi^rยฌยUยยwzf?i
ย ยยรย ร ร
hxi^ยtOulrรซยtOunรคxย?ulยw^ยรฌtnยจ;ย
ยtOulhรgยฅrUisyยย ยย ย3ย9ยย ยtOyยยรฎ
ย รญ ยฑU
ย
ย รฃ ย
wzf?isuรฏย ยย ยยย9ยย gqulrUisi^rIrUtUi^h
u?tOwยtvuยw{kgยuยคoltngquUw{hยจf?isyzi9รฐ ยป gยฅhu?i^kny ยฑ?ยธรยดlรฑUwzf?im|k/jUgยm}ย?mLยยisuUwzyztvoยยยoltngquUwtยญGwzfUgยฅh hxolkOยiยฒg.hi^kOhรg3pqย
hxisisuยฌw&tยliยยฑ?ยธ,ยนยฒยบ ย ยป ยยรฒtnยจisยฏOisy^ยXg3 ย ร รญรณย รฃLwzf?isuยฌwzf?isy&iLยจg3p3pcยXiLoltยญgยuUw{hยgยuรฏย ยย ย3ย9ยย ยจf?isyziLรฐ ยป g.h
knyztvย?ulrEยฑ?ยธรยดlรฑBยtOygqulhxw{kยญulยiOยZwzf?tยhxiยยจf?isyziยฑ?ยธรยด}ยบ ย รฃรด รฐ ยป รรตยฑ?ยธรยด}ยบ ย ร ยรทรถngqulยiwzf?i^hxiยoltยญgยuUw{hflk/ยฏOi
kยคfUgqยOf?isy}isuยwzy&tOoยยรทwzflknuรซwzf?i|oXtngquยwh9gquIwzf?iยยฏvg.ย&gquUgqwxยItnยฑ?ยธ,ยน?ย
wzf?iยtOyzmLisyยจg3pยp9rUtOm}gqulknwziOยยefUยlhsย
wzf?ihxisw;tยญm|k/jUgqmย?mยยisuยwzy&tOoยยยoXtngquยw{htn9ย ยย ย3ย9ย9ย rUtUi^hu?tOwยtvuยยฏOisy&ยOi|w&tยผkhยgยu?ยยญpยiยจip3pqย%rUiยฎlu?i^r
ย ยecfUg.h u?tvulยtOuยยฏvisyzยOisulยi
hxisw^ยยซรธflknwcgยw9ยtOuUยฏOisyzยOi^hwzt ย gย<kยญuยยUwzfUgqu?ย ยข rUisolisulr?h9tvuf?tnยจรนย ย ยvtยi^hwzt ยฑ?
flkOhยtvulhxi^รOย?isulยi^hยtOyrUisยvyzisi^htยญGยXipยgqizยร%wgยฅhu?tOwflkny{rยwzt|hยf?tยจรณยZy ย ยย ยยยถยยยท ยข ยก ย9ย9ยข ยskยญuยli;igqwzf?isy
ยฑ?ยธ,ยนcยบ ย ยป tOyยฑ?ยธรยดยบ ย ร/ยSrUisoXisulrvgqu?ยtOu|w&f?i9o?yzi^ย&gยฅhxi9yzipยฅknwxgqtOulhxfUgqoยยliswxยจisisu ย ยป ย ย รnย?knulr ย รฃOย
ร%wยtnp3pqtnยจh
wzflknwยKy ย ยยยถยยยท ยข ยก ย9ย9ยข rUtUi^hu?tOwijUgยฅhxw^ย
ยย ยยยยก ย9ยยข ย tOyZtn
ยซ ih&k^ยwzflknwk9rUisยOy&isitn~ยlip3gqiKยKy ย ยยยยก ย9ย9ยข g.hKu?tOwรบzรปยรผรฝUรพsรฟ
g3Jw&f?iยlisflk/ยฏvgยtvytnยZy ย
ยฌ
ย rUisolisulr?htOuยรป	รนย ยOtยi^hยwzt ยฑ?
ย ย
รรu
p3gqmIgquUHยZy ยง ยย ยxยยก ยข knulrยฃpยgqmรธhxย?oยยKy ยง ยย ยยยยก ยขzยข kOhย ย ยvtยi^hยwzt ยฑ}
ย
tOwzf?isy ยจtOyxpยฅr?hsย~u?tOu?y&tOย?ยlhxwzu?i^hzh rUi^hzยyxgqยXi^hhยgยw&ยlknwxgqtOulh ยจf?isuยฌยZy ย ยxยยก ย9ย;ยข rUtยi^h u?tvwijUgยฅhxw9ยli^ยsknยlhยi
tnhxisulhรgqwxgqยฏvgยwxยwzt}wzf?i9ijlkOยwย&f?tngยฅยi;tยญHwztnpqisy{knulยi^h ยZยซยฌi;hxflk/p3pGhยisikuยย?m}ยXisytn
tOwzf?isyijlknmLoUpqi^htn
u?tOu?yztvย?ยlhxwzu?i^hzhยgquรp.kยญwzisyยhยi^ยwxgqtOulhsย
รรwZm}gqยOfUw<hxisismbwzflknwZwzf?iu?tvwxgqtOuLtnyztOย?ยlhยwzu?i^hzhBgยฅhknuLknyzwxg3%kOยwtn~tOย?y<kยญo?o?yztยkOย&fยรรu|olkny&wxgยฅยยUp.kยญy^ย
gqwGhยisism|hw&tยrUisoXisulr;tvu;wzf?iK kOยw
wzflknwBtOย?ypยฅknu?ยvยlknยOiflkvhwzf?iยijSo?y&i^hzhรgqยฏOioltnยจisywzt9hzk/ยw&flknwwzf?iยwยยจt
wztnpqisy{kยญulยi^h
yziso?yzi^hxisuUwGk9rvg 
isyzisuUwGrUisยvyzisitnยฟkno?o?yzt/jUgยmยknwxgqtOuยOhรgqmLoUpqย}ยยย9ยlhยgยu?ย;rvg 
isyzisuUwhxย?ยlhzยyxgqo?w{h




"!#$%&')(*+,-	.

/1032546870:9:;=<?>1@A3ACBED	FHGJI=A	KL7NMO<PD	<#D	GGQ1RJDTS1@U>1RVQ1A8GQWA6XA8<Y>Z;[<\HD	GGQWRBJ;=F#D	>1A
<RT>cF#D	dTA5>1@A6XA

A]_^EDI;[>X`H>W@ED	>baJRJA6

a_;&6Z>X;=<ESC>X;=RT<E68ef5A3D	QWAbghRT^<EaU>1RV\TA8>i>W@A3D	<E6Xf5A8Qbjkmln;=<H>1@A:ACBED	FHGJI=AoD	gER	4TATeJ6p;=<ESCA

>1@A8<q=qrs/utvKLq=q w:
x0
9yjkzyQ1AD{I|I=`"f5RT^JI}a"gEAP>1@A?<A8\YD	>X;=RT<"R	~q&qrs/utvKLq=q wย032?jkzE7ยยAPf5RT^JI}aยDยQ1\T^A
>1@ED	>

>1@AยD	<E6Xf5A8Qยjkmls;}6

a_;ยยA8Q1A8<Y>

;=<E6X>CD	<ESCA6

<RT>oDT6:QWADT6XRT<ED	gJI=AยDT65;=>oFย;=\T@J>oD	>:ยEQC6X>ย6ZA8A8Fy7oยT^GGhRY6XAHRT<AsR	~>1@As>uf5R

R	~bjkzU;=<>1@AยGQ1A84T;=RT^E6

ACBED	FHGJI=Aย@EDTagEA8A8<"6pI;[\_@Y>XI=`ยa_;ยยA8Q1A8<Y>ยh~ยRTQnACBD	FUGJI[ATe

6X^GGhRY6XA?fbAU@EDTaย^E6ZAaยjkยlTยTยUQLD	>1@A8Qs>1@ED	<ยjkzP;=<ย>W@AยยEQL6X>sR	~:>1@A8Fy7PMp<ย>1@J;}6ยS8DT6XATe*>1@A#6XASCR_<Ea
SCRT<	ยX^<ESC>*;}6iA616XA8<J>X;}DII=`H4{D_SC^RT^E68eTD	<EaHS8Dย<ยghAc;[\_<RTQ1Aav7ย5@A5F#D{BY;=Fย^FHยยA8<J>1Q1RTGJ`VGhR	;=<Y>*;=<Pยย{
ย ยยoย

ย

;}6*<R	fยjkmlTยTยeD	<Easf5A;=<EaJA8AaHaJA8QZ;[4_A3D3aJA8\_Q1A8A5R	~ghACI|;=AC~ยR	~ยjkmlTยTยb;[<?rs/uยKL7ยยย@Y^E68eTDยQ1gJ;=>1QLD	QX;I=`ย6ZF#DII
SW@ED	<\TA6ย>1R:>1@Ac<Y^FยgEA8QC6v;=<ย>1@AR_QX;=\	;=<EDI,dJ<R	f5I=AaJ\TAgED_6XAbS8D	<ยS8D	^E6XAcI}D	Q1\TASW@ED	<\TA6ย;=<ยRT^QยaJA8\_Q1A8A6
R	~ยghACI;[AC~175ยก^>5>1@A6ZA3<J^FยgEA8QL6:DยQ1AยD{I[FURY6X>bD{I[f:D`6i>1@AoQ1A6Z^JI[>5R	~iD	GGQ1RBY;=F#Dย>1A:RTgE6XA8QW4{D	>X;=RT<E6ยขย>1@J;}6
;}6cQ1ACยฃEASC>1Aa?gJ`HRT^Q:aJASW;}6p;=RT<P>1Rย^E6XAoD	GGQ1RBY;=F#Dย>1A3A]_^EDI;=>X`HQLD	>1@A8Q5>1@EDย<#A]_^EDI;=>u`Uf:@A8<PQ1AC~ยA8Q1QX;=<\
>1RH>W@A8Fy7iMO>

aJRYA65<R_>36XA8A8FยคQ1ADT6XRT<EDยgJI[Ao>WRHgEDT6XAVDTSC>Z;[R_<E65RT<DยaJA8\TQWA8AVR	~igEACI;=AC~b>1@EDย>3S8D	<ySW@ED	<\TA

6XRaJQLDT6X>X;}S8DII=`ยฅ;[<>W@AH~ODTSCAHR	~o6XF#DIIยSW@ED	<\TA6ยฆ;=<>1@AHFUADT6X^Q1A8FHA8<J>ยR	~3aD	>LD7ยง:RT>WAH>1@ED	>eย;~3f5A
dJ<R{fยจ>1@ED	>s>1@A#>Xf5R?;=<E6X>LD	<ESCA6R	~3jkzaJRheย;=<"~ODTSC>e*aJA8<RT>1A?ACBDTSC>ZI[`">1@A#6WD	FHA?<Y^FยgEA8Qeยf5APS8D	<
Q1A8GQ1A6ZA8<Y>5>1@J;}6cgY`U^E6p;=<\H>1@A361DยFHAยDยGGQ1RBY;=F#D	>WA:A]T^ED{I|;=>X`PSCRT<<ASC>X;=4TA

;=<PgERT>W@ยฉa_;}6ยชยX^<ESC>L687iMp<P>1@J;}6

S8DT6XATe,;[>ย;&6nADT6X`?>1R#6ZA8Ao>1@ED	>:f5AoaJR#\TA8>5>1@AยD	<E6Xf5A8Q:jkml7
ยซ

SWI=RY6XA

I=RJRTd#D	>c>1@A

ACBD	FUGJI[AV6Z@R{f

6i>1@ED	>c>1@A

<RT<Q1RTg^E6X>W<A6165D	QX;}6XA6cgEAS8D	^E6ZAoR	~ย>1@A3<A8\JD	>1Aa

GQ1RTGhRTQ1>X;=RT<"ACBGQ1A616p;=RT<ยฌq=qrs/utยKLq=q w":
x0
9ยฉjkzE7?MO<EaJA8Aaveif5A#S8D	<6X@R	fยญ>W@ED	>o;~3f5A#6X>CD	Q1>of5;=>1@D
;=<ยS8D	<RT<J;}S8DIi~ยRTQ1Fยค>1@ED	>VaJRJA6

<RT>VSCRT<J>LD;=<<A8\YD	>WAayGQWRTGERTQW>X;=RT<ACB,GQ1A6W6p;=RT<E6o>1@A8<ยeh;=<ยD?GQWASW;&6ZA

6XA8<E6XATe5>W@Ay6XA8>HR	~oF#DBJ;=Fย^FUยยA8<Y>1QWRTGY`"GERย;[<J>L6UR	~Vย
F#DBJ;=Fย^FHยยA8<Y>WQ1RTGJ`PGER	;=<J>L6

ยoย

R	~:ย

ย ย ยยoย

ย 7
ยซ

ยฎย

ย[ยoย3ย

<ยD	Q1\_^FHA8<Y>

<ASCA616WD	QX;I=`ยSCRT<Y4_A8Q1\TA6ย>1Rยฅ>1@Ay6XA8>HR	~

S8D	<gEAยF#DTaJAย>1@ED	>nfbAH6X@R_^JI&aACI;=Fย;=<ED	>1A

<A8\YD	>WAaยGQWRTGERTQW>X;=RT<ยฌACBGQ1A616p;=RT<E6ยฉ~ยQ1R_Fยฏ>1@AI}D	<\T^EDย\TAยDI=>1RT\_A8>1@A8Q7)Mp>P;}6PRT<A>1@J;=<\ยฐ>1RยฑDยQ1\T^A
>1@ED	>

6XRTFHA8>X;=FHA6nf5Ao@ED4TAV6X>LD	>Z;&6Z>X;}S8DIย4{D{I[^A6

f:D	<J>o>1RyF#DยdTAsI[R_\	;}S8DI3DT616XA8Q1>Z;[R_<E6
>1R>1@J;=<dยR	~3S8DT6ZA6

fn@RY6XAVDTS8SC^QLD_SC`PfbAVDยQ1AV^<E6X^QWAยD	ghRT^>eE6XRU>1@ED	>:f5A

I=A616ย6X>1QX;=<\TA8<J>V>W@ED	<ยACBEDTSC>V<J^FHA8QX;}S8DI5A]T^EDI;=>X`T7PMO>ยฆ;}6V@EDยQLaJA8Q

;=<f:@J;}S1@>W@AHRTGGERJ6p;=>1A?;}6V>WQ1^ATeiD	<EaยD{I|I5f5A#dJ<R	f);}6s>1@ED	>ย6ZRTFHAP6X>LD	>X;}6X>Z;&Sย;&6

ยฒ <RT>?A84TA8<ยฌD	GGQ1RBJ;=F#D	>1ACI=`ยณยA]_^EDIo>1Rยฑ6ZRTFHAy4	DI=^AT7ยญยดnR{f5A84TA8Qef5AaJR<R_>#ACI;=Fย;=<ED	>1Aย<A8\JD	>1Aa
GQ1RTGhRTQ1>X;=RT<ยฐACB,GQWA616p;=RT<E6~ยQ1R_Fยต>1@API}D	<\_^ED	\TATe:6p;=<ESCAf5;=>1@RT^>?>1@A8FยถfbAPf5RT^JI}aยท<RT>#ghAD	gJI=A>1R
GQ1R	4TAHD	<ยD	<EDI=RT\_^Aย>WRyย5@A8RTQ1A8Fยธl7ยยน7ย/uย5@A8`D	QX;}6XAยf:@A8<ยf5Aย>1Q1`>1RUยฃD	>1>1A8<<A6X>1AaยบGQ1RTGER_Q1>X;=RT<
ACBGQ1A616p;=RT<E68eh~ยRTQ

ACBDยFHGJI=AT7KPMO<E6X>1AD_aveEf5AV@ED4TAยฆ;}aJA8<Y>X;ยEAa"DHf5AD	d_A8Q3SCRT<Ea_;=>X;=RT<ย>1@EDย>5;}6o6X^JยปยฉSW;=A8<Y>

>1RoGQ1A84_A8<Y>cGQ1RTgJI=A8F#656X^ESW@#DT6i>W@ED	>6XA8A8<U;[<Pยผ*BED	FHGJI=A:zh7ml7cยฝยพCยพรยฟยขรรยชรยรTรยขร,ร	ยพCรยรยชรยชรรยรยชรH6p;=FHGJI=`H>1A6X>L6N>1@ED	>
<A8\YD	>Z;[R_<E6:D	Q1Ao<RT>ย;[<J>1A8QLD_SC>X;=<\#f5;=>1@P>W@AoF#DBJ;[Fย^FHยยA8<J>1Q1R_GY`PSCRTFHG^>LDย>X;=RT<?;=<Dย@ED	QWFย~ย^JI*fยD`T7
A8>ร*รi/ ยoยยยยชj ร ย KvgEAn>1@A5Q1A6X^JI=>R	~vQ1A8GJI}DTSW;=<\VADTSW@#6X>WQX;}SC>*;[<A]_^EDI;=>X`ย;=<#ร*/ ยoยยยยชj ร ย K

ร?ร_รbรยร=รร&รยรยฐรยรรEร5ร

f5;=>1@ยฅ;=>L6of5AD	dTA8<Aaยบ4TA8QL6p;=RT<ย7PรRTQ1Aย~ยRTQ1F?DII[`Te*f5AHQ1A8GJI}DTSCA?ADTSW@ย6X^gJ~ยRTQ1Fย^JI}DR	~b>1@AU~ยRTQ1Fรรยรยญj
f5;=>1@รoรjevD	<EaAD_S1@ย6X^gJ~ยRTQ1Fย^JI}DHRย~b>1@As~ยRTQ1FยครยรรjHfย;[>W@รoรรj7ย/uร5AS8D{I|I>W@ED	>3>W@A6XAHD	Q1Aย>1@A
RT<JI=`ยSCRT<E6X>1QCD;=<Y>L6nGERJ616p;=gJI=AH;=<ยร*/ ยoยย j ร ย KLeย6p;=<ESCA#DII>1R	I=A8QLDย<ESCAH4{D	QZ;&DยgJI[A6ยฆรก8รข3D	Q1AHDT6W6p;=\T<Aaยj7K
ย

ร ยย
v

ยยoย

ย

gEA

รฃvรTร ย ร
ร

/ ยoยยย j ร ย K ย eJf:@A8Q1AsfbAo^E6ZA

;}6yยฟยขยพLยพ8ยฟ8รvรยชรยร_ร}รยรHรร	ยพCรยรยชรยชร{ยฟรฅ;~V>1@Ay6XA8>L6?ย

รย ย

ยยoย

รคยธ>1R#aJA8<R_>1Ao>1@AยSWI=RY6Z^Q1AoR	~iรคยฑ7*ยยAV6WD`?>1@ED	>
ย

ยย

D	<Eaรฆย

ร

A8>

ยoย

ยยoย3ย

@ED4TA?>1@A61D	FHA?F#DBJ;=Fย^FUยยA8<Y>1QWRTGY`

ยoย

~ยQ1RTFรฏยผยBDยFHGJI=AVzE7ml7bย5@AVSCR_<E6X>1QLD;=<J>

GERย;[<J>L687

รงnรจยรฉvรชยบรซ*รฌ&รร*รยชรญรยฆรฎ

RT<E6p;}aJA8QoD	\YD;=<y>W@AodY<R	f5I=AaJ\TAsgEDT6XA

~ยRTQ1Fย^JI}DHร*/ ยoยยยยชj ร ย K*;}6o/XD~ย>WA8Qย6u;[FUGJI|;ยS8D	>Z;[R_<vKLรฐ

/Oรฑรฒnรณรดjkยl:รตHรฑvรฒ

MO>C6

ยฒ f5AD	dTA8<Aaยณย4_A8QL6p;=RT<?;}63ร

/Oรฑ

รฒ

ร

รณรดjkzKvรถP/Oรฑรฒ

รยjkz3รต?รฑรฒ

รยjkzKLk

/ ยoยยย j ร ย KLรฐ

รณรดjkยl:รตHรฑ

รฒ

รณรดjkzKvรถP/Oรฑ

รธรน

รฒ

รยjkz3รต?รฑ

รฒ

รยjkzKLรท

  
	 

รบ:รป,รผJรฝรพhรฟ

TรพTรป

ยรฟ

ยรผ

รพTรป

!"$#%&')(*+*-,/.10/24357698/.:0;2<35>=@?A#@)BCEF&D G!HJILK 2NM$OP.:0'Q-.1R)SUTWV R X
. 0ZY 3576[\)-$]C_^ F D G`HJIJK 2aC F D G`HJIbK@c M$Od35>=@Qe357fSg[h?ji!(@eU*\*gk,+l@meJ@'%mn$`op)-)(*
q'rq]#q\st)(h*-,mllu,(h*vwB*-Jx(,&y!nz$|{@mg HJI }(,m*-g)(*g'`!~l@,hd!*!%$m?
 *-Jยt,`,(z-#!*Jg,ย)B-)(h*g'
l@,hย*g%$*d/g#ยยe*,\zm#@vย(h*-)J*@**-|q&rhq]#q\s
)(*--,mllu,(h*e,ยC ย D G!HJIยK e,m(%m)-z$J*-,]*-,hgJ,ยC F D G`HJIbK ?
ย|ย'ย1ย
ย:ยยยยยยยpยยย
ยยhยยยยvยeยhย\ยbยยยhยmย HJIยย ยย)ยeยยกยwยข:ย ย ยmยฃยฃยฅยคEยฆยจยงย ย ย ยยฉ ยยยยข1ยช|ยฃ`ยย
ยซยยฌยยยยยhยbย)ยยยญยงgยฎย\ยยฏ ย ย]ยhย]ยฐ
ย)ยข1ยยยฑ-ยงeยฆ@ยคยฆยง ย ยข:ยยยยงgยฎ C FD G`HJIยKEยฒ ยยข1ยช]ยยยmยhยbยmยฃยฅยยกยง]ยงgยฎ CE^ FD G`HJIยKยฅยณhยด]ยต ยย)ยขbยฎยกยงยฑJยmยฃยยฃmยถยท 3 ยยข1ยช]ยmยฃยฃยvยeยธยยน ย ย)ยข1ยtยฃยฅยค
ยeยjย$ยฃยฃEยtยงmยฃ`ย)ยฑ-ยยข:ยนeย ยฉ ยeยนยtยงยฑeย/ยป ยบ ยฒPยผ ยhยwยฑ-ยยพยฝยยvยeยธยยน ย ย)ยข1ยtยฃยฅยค+ยeยjย$ยฃยฃยยฟ]ย\ยยครยชmยยฆยจยwยข:ยช/ยงยขรยถ ยณร ย ยฉ ยwยฑvยค/ย\ยยฏ ย ย]ยhย]ยฐ
ย)ยข1ยยยฑ-ยงeยฆ@ยคJยฆยง ย ยข1ย_ยงgยฎ C ย D G`HJIยKEย ย ยผย ยยย ย ยข+ยถ]ยงgยฎJยยกยงยยjย9ย\ยยฏ ย ย]ยยรยฐgย)ยข1ยยยฑยงยกยฆ@ยค]ยฆยจยง ย ยข:ย ย ยขรยซ ยด
ยรยยรaรZร:รยยรย]รยย:ยรรรยยรย:ร1รรรยรยรร
ยยรรยรรร1ร@ร

ร ]
 (,ยg,W,ร*-,Ue,mq\l#*ร-รZOdรร HJI Sยร,m|Ue)*v'(/-g*-ge*-nรย$-b,ย_ร@eg*-st,mvn)}ยt,m-s
q]#m|รร(@n/x(,nzm\{@m HJI ?ZA]q\,hg*|d!z$(รร:)ย(h*gรg*-ge*g,m(รJ*-@*J*j"$#)-รร
g,m#n/{u\"$#@(*g`ร@)-sยยร)UOtร@vg*st,mvn))Sk)(h*-)(@e],%m)b*-]%m,){#-+ร c M&ร'[hรกu*-h#@wB*bJ
"$#)-\{@,m#*Jย(z(@n$%$ยn#@&รB1ร? ร รb*-_m-รe*-)-g*ge*g%mmB*k#ยยeE*-,Jerยจl-
q(+'sยร`ยtUerยq\l)?รข~,m-),%m)'Bu!*ย|ยzm(`ร:)(*gรย)|*@(ร*-](z$#@zme,m(@dยn)n
{ยรย}(@nZรฃ)(@e,%ยจgxUOeรครฅmรฆmรฅSe?
Aybยร,`,(zยneร@(*g,m(/el@bneร@(ร*|ย$-,ยยp(h*-)g*?
รงZร$รจkยpยยยยยpยรฉยรยยรช&ย  ยt,m-q]#E ย)ยeย)ย)ยข:ย ย ยmยฃยยฃยฅยครยฆuยฑยงยกยฆยจยงย ย ย ย ยงยข1ยmยฃ รย*1
"m#@(*g`ร@)-sยยt-)kย(@nJl-,mlu,m-*g,m(s
ยt-)\ยt,m-q]#Z(*-ร(zm#@z$ยรซEรฌEO-M&รญ
0Q'555Qvรญยญรฎm[mS9Odg,+*@*Bยญ(l@-*ge#B
*]@m9(,รe,m(@g*vย(h*
gq]{@,eSkย(@nU@m,$(!Z,m(ยยร-)J%g{\รฏp?
ร |-'\*-@*ยรEOgร'Sร ย ย ยbยฆ:ยฃ`ยรรฐwยยจยwยฑvยคbยฎ)ยงยฑ HJI รย X
รฑรEOdรฏ1S
g)(h*ย&รUl-,$l@,hย*g,m(@&รB
รฑ HJI b,ยรฒ*|ยt,m-qรดรณbOdร'S1รต HJIJรถ B1})-|รณbOdรฏรSb-g)(*g'`!/l-,$l@,hย*g,m(@&ร(@n HJIJรถ n,
(,m*q\)(h*!,$(รร?
A#@)Bรทg#@g*]mb(A),m-)qรธ=@?รคmBยญkZg#ll@,h*-@ย*JรณยOgร'Syg#qรqgรน)]'`k*-@ย*JJx(,(ร{u,m#*
ร?EรบP(+*ยJe*g,m(pB@bยt,ยจe#@},m(/e,mq\l#*g(zZ*-รn)zm)|,ย{@e`eยยร ร OdรEOdร'S)ร HJI Sยt,mย9ยt,m-q]#
รEOdร'SUยรยq\lร"$#)-Zยร,$ HJI ?
รป ,m*-~*-@*ย(รผg)(h*ย&รรl-,mlu,hย*g,m(@'bยt,m-q]#รรฝOdรฏรS|Z"$#!%')(h*~*-,รn$ยรทg#(@e*g,m(ยพ,ย
*-,$q)?รพ,mer@q\lmBu,&%$)k*J%m,ยจ)ย{#ยย-/Mรญร0QeรญรRw[hB*-bยt,m-q]#\รญ
0'Odรฏ1S18รญ
R'Odรฏ1Sรยy"m#%&&!)(*
*-,Jรฟ 0 Ogรฏ1S8รฟ R Odรฏ1S'8k
รฟ Odรฏ1S
OP)-E*-*-,mqZp-,mvn)nร$1
( 
r@q\l6? Se?รรพ,$รฒ(9-g)(*g'`
l-,mlu,hย*g,m(@'uยร,$-q]#ยยรฝBh*exm
 \OPรฝS
{@b*-|Oย#(ย"$#S_g)*_,ยยยญย*-,mqg#@Z*-@*รฝbE"m#%&&!)(*
*-	
, 

รฟ &Ogรฏ1Sv?
ยบ T~
ยบ *-,
รบรยpk}%m)ยพJ*-#l .+
V Wm9l-,m{@ย{ร`*gm-d!z$(q\)(h*E*-,|*L*,mq)Bย)(รerยจ*-)(@n .U
Zl-,m{@ย{ร`*g $-ยzm(q\)(*J,m(ร'`-g)(*g'`!รl-,ml@,ย*g,m(@'_ยt,m-q]#mb#@ย(z/*-bn)(*g`ร:)*g,m(
,ย_(U-g)(*g'`+l-,$l@,hย*g,m(@&pยร,$-q]#ยj*-+\g)*,ยยร*,mq X
รงZร$รจkยpยยยยยpยรฉยรย p
ย  )*Jรฝร{@รย(+-g)(*g'`/l-,ml@,ย*g,m(@'
ยร,mqร#? ร jneร@(\jยร#(@e*!,$( "! $# X
V &%(' )ร$Eยร,ยร,ย X

 ! $# O .pยบ S2 *
. 5


+,-

./

0213465+798:5;<=46>?134A@	1BC72D7FEG3H;I5+JK
LMONQPSR-RTPUWVTXZY=[\[^]`_N-Ma_bMWR$XVTXMaUbY=[bcdMaN-efW[ZYaRQgihkjmlYUbno?hkjml6pPqnWPsrbUPtVvuPhw_bYN-VXYx[ylzcdfUb{sVTXMaU}|Q~ +ย ยยยยย
ยย&ย(ย ย VvMยbPaย
| ~ OยยWย hkย l
| ~ +ย ยยย hTยย lQย |"~ ยยยwhTย ย lยย
ย
ยMaV-PV-ubYVV-uWXZRcyfUb{sVX^MOU}XZRqfUbnWPsrbUPSnยpquPUย|"~ ยWยwhTยย l2ยยยย
ย RยV-uPQcdM[\[MxpX^UยNvPSRTfW[V2RTuMptRยยXยcยg`XZRQYtR$Xe_W[PtยOfPN-]?cdMaNiยยยกhwMcV-uPicyMON-eยกo?hkยข=lยฃยยtยคlยฅยV-uPU
Y=[\[qV-ubYVยฆe	YV-V-PNsRX^U:{sMae_fVX^UยAยงQN-ยจยฉhkgqยชยซยยl?XZR|Q~ +ย ยยยdhTยย lqcdMaNยฌV-f_W[PSR`ย ย Mcqe	Y=ยญWXefeยฎPUWV-N-MO_ย]aย
ยฏuWfbRยzXU:Y}RTPUbRTPaยยpยฐPยฉYN-PMaUW[]ยฑfbR$XUยยฑยย ยค V-MยnWPV-PN-eยฒX^UPยฉV-uP	R_bYa{sPMยณaPNpquWXZ{vuยฑpPe	Y=ยญWXeXยดP
PUWV-N-Ma_W]aยqยตtY=ยณOXUยยถnWPsrbUPSnยV-uWXZRRT_bYa{sPaยยทpPยฌ{YUยcyM{sfbRMaUยoยกYUbnยgAXUยฑnWPV-PN-eXUWXUย}V-uPยธnWPยON-PPยฌMc
ยbPs[\XPsc-ย
ยนยบzยปWยผ6ยฝ=ยปbยพรยฟรรGร`รbร=รaรIรรร gihkยข=ltร รร}ร รZร 6ร ร\รรยรIรยรยฅรยฆร,รรร ยย	ร`ร รรรaรZร ร ย ยฅร รsรรร ร รรzรdรรยฉร ร รOรZรยซร ร ร	ร
ร ร	รรรยรยฉรรSรqรTร ร รร รรร ร รร รร6รรร-รsรยทร	รร ร ร6รรร ร ร:รกqรฃIรข รค\ยย?รฅ รร6รฆ |"~ ยยยdhSรง ย lรจรฉย ร,รร ร	รaรรร รงรซ
ย รช รรรรรยรยร}ร,รร
[\Xeยถรฌ รชรฎรญ [\XeยRf_รฏT[\Xe:XUWcSรฐยฉรฑ รรยรรรฒxร
[\X^e รฌ ยง"N รฃ รข hkgihkยข=l,ยชยซยย?l รชรถรต XUWc |"~ ย ยWยdhSรง ย lยฅรฏsRTf_ |Q~ +ย ยยยdhSรง ย lรปรบ ย
qรณ รด ยจ รณ
รทSรข รธรน
รทรข รธIรน
ยฏuP2cdM[\[MpXUย?XRiYUยฆXeePSnOXZYV-PqยfVGXe_ยทMaN-VยฅYรUยVQ{sMaN-M[\[ZYN-]ยฆMc6VvuWXRGVvuPMaN-PeยยGรผwV2YOR-RTPN-VยฅRยV-ubYV=ยX\c
V-uPtR_bYa{sP รก รฃ รข รค\ยย?รฅbubYaRiYfUWXZยOfPe	Y=ยญWX^efeรฝdPUWV-N-MO_ย]ยฆ_bMXUWVSยWV-uPUXVยฅR"ยณxY=[fPfUWXZยOfPs[^]}nWPV-PN-eยฒX^UPSR
V-uP_N-MOยbYยWX\[ยXVT]รพยง"N รฃยจ รข hkgihkยข=l,ยชยยยtlยฅย
รฟ zยผ ยฝSยผbยฝรยฟGร
	zรยฆรbร=รaรIรรร,ร gihkยข=l?ร รยฌรยฉร รรร รzรยรรยรIรยรยฅรรรร 
ย ยร ร รรรยฌรaรZร ร ย รsรsรรร ร รร6รdรยซรร ร รaรรรร ร ร รง ย
ร รยฌรรรยรยฆรยร ร รยรIร ร รร รZร ร รร รรzรรร-รsรbรยฆรIร ร ร6ร ร รรฎรกqรฃรข รค\ยยtรฅ รรzรฆ Q| ~ ยWยdhSรง ย l2รจรย ร2รรรWรร
ยงQN รฃยจ รข hTgihkยข=l,ยชยซยย?l2ยย| ~ ย ยWย hSรง ย l ย
 PยฌYN-P?XUยVvPN-PSRTV-PSn`XUรพยง"N ยจ hkgihkยข=l,ยชยยยlยฅยpquWXZ{vuยePSYUbRV-ubYVpPยฌYN-P?XUยVvPN-PSRTV-PSnยถX^UยV-uP?[\XeXVtMc
ยงQN รฃยจ รข hTgihkยข=l,ยชยซยยltYaRยถร ย ย ยWย ยaf__bMยRTP ยยรฉXZRPSRvRTPUยVXYx[ย[]ยฑ_ยทMยR$XVTXยณaPaยยฏuPUยยยยW]ยVvuPN-PSRTfW[VยฅR?McV-uP
_N-PยณOXMafbRqRTPS{sVTXMaUยYรUbnV-uP{sMaUWVTXUยfWXVT]ยฉMc | ~ ย ยยย ยOXViXRPUMafยauยฉV-Mยฆ[^MWM }nOXN-PS{sVT[]ยถYรVยฐVvuPte	YxยญยXefeรฝ
PUWV-N-Ma_W]_bMรX^UWVยฅRMรc รก  รข รค\ยย?รฅwย ยMaN-PcyMaNve	Y=[\[]aยยย]}{sMaeยWXUWXUยยถยฏuPMON-P
e bย ยฆpXV-uยยงQNvMa_bMยRkX^VX^MO
U bย ย
pPยฌ{YUยRTuMpยฌย
ยนยบzยปWยผ6ยฝ=ยปbยพรยฟร ร?รbร=รaรIรรร gihTยข=lยฐร รรร รZร ร6ร\รรยรIรยรยฅรqรรร ยย	ร  ร รรรWรยฌร$รIรOรยฅร?รก รข รค\ยยtรฅ รWรร ร`รWร ร รรร
ร รร รรร ร รร รร6รรร-รsรยทรรIร ร ร6ร รง ย ร ยย ร รยฌรรsร,รยร6ร ร รaรZรยซร?รIรรร ร ร ร รฒรร2รรzรฆ | ~ ยWย h=รง ย lยฐรจรย ร2รรรยรร
ยงQN-ยจยฉhTgihkยข=l,ยชยซยย?l2ยย| ~ ย ยWย hSรง ย l ย
 PtยยทPs[ยXPยณaPยฆV-ubYViV-uWXZRV-uPMaN-Pe pX\[\[V-fN-UยฉMafVV-Mยธ{sMยณaPNqY [^MOVยฐMรc {YaRTPSRiV-ubYViMI{{sfNX^U}_NsYa{sVTXZ{sPaย
ย RQMafNiPsยญ+Ye`_W[^PSRiYUbn`V-uPtnOXZR-{sfbR-R$XMaU`XU	VvuPqUPsยญIVRTPS{sVTXMaU}RTuMpยฌยapPMcdV-PU	nWMยฆยaPVยฐR$Xe_W[PยafPNX^PSR
YUb
n ยUMp[PSnWยaPยbYaRTPSRV-ubYVYN-PยฆPSR-RTPUWVTXZY=[\[]ย_bMWR$XVTXยณaPa
ย 2MaUb{sPN-UWXUย}V-uPYaR-RTfe_VX^MOUรพMรcยฐYfUWXZยafP
e	Y=ยญWXefeรฝdPUยVvN-Ma_W]}_bMXUWVSยzUMaV-PยฆV-ubYV?V-uPPUยV-NvMa_ย]ยฉcdfUb{sVTXMaUยXZR{sMaUWยณaPsยญ YUbnรฎRTMยฉV-uWXZRYaR-Rfe_VTXMaU
XZRYfV-Mae	YรVTXZ{Y=[\[^]ยฑR-YรVTXZR$rbPSnรฎX\c รก  รข รค^ยยtรฅQXZRY รsรรbรฒรร RT_bYa{sPa
ย PS{Y=[\[qV-ubYVYยRT_bYa{sP รก XZR{sMaUWยณaPsยญยX\c
cdMaN Yx[ย[ยย รฏsยย ยค รช รก ยยYUbnรฎY=[\![ รช รคยรฏ "รฅwยWXV?XZRY=[ZRTM}V-uP{YaRTPV-ubYร#
V "ย
ย $ %h "'&(QlTยย ยค รช รก ยยฆยฏuPRT_bYa{sP

รขรก รค\ยย?รฅGXRRfN-Ps[] {sMOUยยณaPsยญ}X\cXV?XZRnWPsrbUPSnรฎfbR$XUยรพYยฉ{sMa*U )TfUb{sVTXMaUยฑMcQ[\XUPSYNยธ{sMOUbRTV-NยฅY=XUWVยฅR+
ย ยuWX\[PXV
XZRq{v[PSYNT[]}_bMWR-R$XยW[PยฌV-M{sN-PSYVv+
P ยUMp[PSnWยaPยbYaRPSRpquPN-P รก รข รค\ยย?รฅzubYaRefW[VTX_W[^Pยฆe	Y=ยญWXeยธfe`รฝdPUยV-NvMa_ย]
,-,

.0/21436578:96;-<5/6=>7@?BA1C;-;5/

DFEHGJI4KMLONPEQ@R%SCTHUVD4WXRYZFL[GXI6\^]_GLa`bZ6IFc%KbGXEIFL%d%Y
e:ROR%S2DR-c%KfKhgFT*KLbZFcigkj
I6E*e:WXR-]4\R0lFT_LbR-LT*QbGLbR:QMT*QiR%WJmnGXI
D6QMTc%KoGcpTqWrT*D6D4WsGcpTHKbGXEIFLpt:u@RpQhgFT*DFL@Khg6R:UVE
LoKQhR-LbKhQbGc%KbGXvR0TLiLbZ6UVD6KbGXEIVUwT]4R:l
mKhg4GLKhg6RpEQiRpUxGL!Khg6R
LbRpRpUkGXI6\*WXmwGXI6I6E6c%Z6EZFLyQhR-z_Z4GJQiRpUVRpI
KyKhgFT*K0{!| }
~Nย dfยยย6tย:g4GLyTLhLbZ6UD6KbGXEIwGL0El4v_GJE_ZFL[WXmI6R-c%R-LhLhT*Qhm
PEQKhg6R:Khg6RpEQhRpUBKhEyg6E*W]ย
eGJKig6EZ6KGXK-Y_Khg6ROPยZ6IFc%KbGXEIw{ | ย6ย }4~ GLfL[GXUVD4WXmVI6E_Kf]4R%ยFI6R-]tfย0I4PEQhKiZ6IFT*KhR%WXmY
e:RfLbg6E*eยGXIwยR-c%KbGXEIยยFtยย0KhgFT*K>Kig4GLยQhR-z_Z4GXQhRpUVRpI4K>GLpYGXI+PยTc%K-Y*T0LbRpv_RpQhRfEI6RยGXIDFT*QhKbGc%Z4WT*Q-YGXK@D6QhRpvRpI4KML
Khg6R0Kig6RpEQhRpUยPQhEUยlR%GXI6\ยT*D6D4WsGXR-]wKhE+UVE
LbKOR%SCTHUVD4WXR-Lย]4RpQoGJv_R-]kPQhEUย]4R%PยT*Z4WXK:QhR-TLbEI4GXI6\FY_ZFL[GXI6\kEZ6Q
LbKMTHKbGLbKbGcpTW>GXI4KhRpQhD6QhRpKMTHKbGXEIE*Pf]4R%PยT*Z4WXKML#Nยย:Tcpchg4ZFLยRpK'TWยtXYย-ยย*ยCdMt
ย RยciWJE4LbRยKhg4GL0LoZ6lFLbR-c%KbGXEIยe:GXKhgยT*IR%SFT*UVD4WXRยEHPยKhg6R+Kig6RpEQhRpUGXIยT_c%KbGXEI>t
ยยยก>ยขยฃยฅยค@ยฆยงยยจ@ยฉยช6ยซยฌ#ยญ RpK+Khg6R+WT*I6\ZFTH\R^c%EIFLยGLoK+E*Pfยฎยยฏยยฐ_ยฑ#ยฒoยณ2ยดยตaยถยตaยถยทยธยบยน2ยด*ยป4ยผrยฝHยถยยพ%ยฒยธpยฟ0รรยป6ยฒpรfรยฒ%ยฝรรT*IF]Khg6R
c%EIFLbK%T*I
Kยรfร%ยถยยพ4tย:g6RpQhRยT*QiR0R%GX\g
KyT*KhEUยLยGXIKhg4GLWT*I6\_ZFT*\Rt ย RyZFLbRยรยร>ร ร ร>ร ร ร>ร ร KhE^]4RpI6E_KhR+Khg6R+T*KhEU
รyร ร Nยรdร รyร ร Nยรรd-ร รyร ร Nยรd%Y-e0g6RpQiR รyร ร GL!R%GJKig6RpQfรรNย]4RpI6EKbGXI6\kยฑ+ยฒbยณ6ยดยตaยถยยตaยถaยท*drE_Q รรNย]4RpI6EKbGXI6\kรยฑ+ยฒbยณ2ยด_ยตaยถยยตaยถaยทdMY
ร ร ร GLรรEQ รรNPEQ+ยน6ยด*ยป
ยผยฝ*ยถยพMยฒVT*IF]ยฅรยน2ยดHยป
ยผยฝ*ยถยยพ%ยฒ6YQhR-LbDFR-c%KoGJv_R%WJmdMY>T*IF] ร ร ร GL+รEQ รรNPยE_Q+ยฟ0รยป6ยฒpรOรยฒMยฝ
T*IF]รรยฟ0รรยป2ยฒยบรfรยฒ%ยฝ>Y6QhR-LbDR-c%KbGXvR%WXmFd%t
ร EIFL[G]4RpQ0Khg6Rj
I6E*e:WXR-]4\R+lFTLoRVร+ยฟ#รกยบรขXรฃ
รค
รฅ n
ร Nhยฑ#ยฒoยณ2ยดยตaยถยตaยถaยทqNbรd@รฆรงยน6ยด*ยป4ยผrยฝ*ยถยพMยฒ_Nยรรdhdรจร
รฉ ยฑ+ยฒbยณ6ยดยตaยถยยตaยถaยท*NยรdMรชรยน6ยด*ยป4ยผrยฝ*ยถยพ%ยฒNยรรd รฉ%รซยรฌ ร ย6รญรฎร
รชXรชรยฟ0รรยป6ยฒpรfรยฒ%ยฝ6NยรรdรฏรชXรช รซ รฌ ร ย6รญรฐรฑร
ยน6ยด*ยป
ยผยฝ*ยถยพMยฒ_Nhรfร%ยถยพ*dMรญ
รฒ Pรจe:R'E_QM]4RpQ:Khg6R#THKhEUยL:TLOร0รณyรดpรต'Yร รณyรด รต Y ร รณ รด2รต รY ร รณ รด รต Y2ร รณรดpรต Y ร 
รณ รด รต Y ร รณ รดrรต รY ร รณ รด รต Y4Khg6RpI
GXK:GL0I6E_K0gFT*QM]wKhEwLbg6E*eรถKigFT*K'รท@Nhร+ยฟยรกpรขรฃ*dGLpรค
รธ ร
Fรธ รน
Nยรธ
Nยรธ
Nยรธ
Nยรธ
Nยรธ

ยฏ
รรบ รธ
ร รบ รธ
รรบ รธ
รรบ รธ
ร รบ รธ

รd

รd
ร:รบ rรธ รพ รบ รธยบd
ร รบ rรธ รพ รบ รธยบd
:
ร รบ รธ รพ รบ รธ รฟd

ยฏ

ย

ร
NยยCรญรฎ รบรฝรผqร diNยรธ !
ร รบ รธ O
ร รบ รธ รพ รบ Fรธ รฟ*d
NยยCรญรฎ รผ ร diNยรธ ร รบ รธ ร รบ รธ รพ รบ รธ รฟ d
NยยCรญรฐรฑ รบรรผqร d
NยยCรญรฐ
รฑ  รผqร d
ย6รญ

ร

ย
รป


รป


ย

ร
ร
ร
ร

ย EkยFIF]Khg6RยLoDFTc%R	
  ร+ยฟ รกยบรขXรฃ  eยRยLยGJUD4WJmยLoRpK รผ ร ยฏ รผ ร ยฏรถย6tย:g6RpIGJKGLyz_Z4GJKiR^LbKhQ%TGX\g
KoPยEQieTHQM]kKhE
ย
ยFIF]Kig6R+UยTS4GJUkZ6URpI4KhQhE_D
mwDFE*GXI4K0GXIรKhg4GL0LbDFTc%RY6eยg4Gcig>YKMT*j_GJI6\ยยฏรถรฐ ร รฟ Y_GLpรค
ย
ย


N[ย ร ยธiย ร ยธiย ร ยธhย*รนqยธiยรพ-ยธiยqรฟ*ยธhย ยธhย df
ยฏ 
ยธ  ยธMย6ยธ%ย6ยธ
ยธ

ยธ
ยธ 
รญ
รฑ รบ  รฑ รบ 
ย6Nbรฑ รบ d ยCNยรฑ รบ ยd ย6Nbรฑ รบ d ยCNยรฑ รบ ยd 
ยyL[GXI6\(ย Y6e:RยcpT*Iยc%EUVD6Z6KiR+vqT*QbGXEZFLyTLbm4UVD6KhE_KbGcyD6QhElFT*l4GsWsGXKbGXR-LyvRpQhmwR-TL[GsWXmtCEQ0R%SFT*UVD4WXRY
u!QNhยฑ#ยฒoยณ2ยดยตaยถยตaยถaยทqNiรfร%ยถยยพ*dpรชรร+ยฟ#รกpรขรฃd

{| รขXรฃ!#"%$&"'$)( +ย * !-,./-$10aรข ~Nย d
ย รOรบ ย ร
ย รOรบ ย รOรบ ย รพ รบ ย*รฟ
3รพ 25ร 4 รบ รพ325ร 4
ยฏรถย6รญรฎ6ยธ
3รพ 25ร 4 รบ 3รพ 25ร 4 รบ รน 6a3รพ ร 25487 ยธ รน6รพ3ร 25487

ยฏ
ยฏ
ยฏ

TLkR%S6DFR-c%KiR-]t ย*GXUkGsWT*QbWXmYye:RยcpT*IยLbg6E*eรKhgFT*KVu!QNhยฟ0รยป6ยฒpรOรยฒMยฝNhรOรMยถยพ*dรฏรชรร+ยฟ+รกpรขXรฃ_dยยฏ ย6รญรฐรฑยT*IF] KhgFT*K
uQ  Niยฟรรยป6ยฒpรOรHยฒ%ยฝNhรfร%ยถยพ*dรจรรยฑ#ยฒoยณ2ยดยตaยถยตaยถaยทqNiรfร%ยถยยพ*dpรชรร+ยฟ รกpรขรฃ d:ยฏ ย6รญรฐ6t:9ยEKhRKhgFT*KyKhg6R+ยFQMLoK'Kbe:EVT*IFLbe:RpQMLyTWLbE

;<

=	>@?@AB5CEDFB@G@HIAJ>@?@ALKM>@N@OPC	Q@CSRT?@U@GVB5WX
Y[Z\)\&Z]^Y`_3Zbadc3e@fMgbh1_3ficjh1klY[f_3fkmifon@_ph1kmiqh1nl\&fsrutve@fZ_qfadwmx1y{z|}]:elh~i3eFem{n@nmfkmยjc3Zยยยfย{n@nl\ยh~iยl\1f
h1kยc3elh~ยibยpfxFtve@foc3elh1_gย{kmยp]vf_ย#e@Z8]ยยcqemcoย:ยยยVยยย	ยยยFkmgยยยยpย@ยยยย[ยยย~ยย_3foยยfh1k@ยยc3_qfc3fgLย
h1kmglfnmfkmglfklcx:ยยc:h~ยยpnยfiqhP8\iยpfยZYaยZ_3fยยfk@f_8\ยh1kmglfnmfkmglfkmifยn@e@fk@Zafk@Zkยกc3emc:n@nl\)h1fย
c3Z_kmglZaยข]vZ_p\~g@ยยฃmยpffยruยคviiqeยฆยฅmยvfcI\-x1|}yยงยงwm|@tve@fZb_3faยจ@x'ยฉVยชzx
ยซยญยฌ1ยฎยขยฏjยฐยฑ}ยฒยยณmยฒTยดยยต~ยดยยถยทยดยยธยกยนยญยฐIยฑยนTยฑ}ยถยดPยทยด1ยฑยปยบยณmยตยตPยฑmยผยดยยธ
ย-kocqelhPยvยpfic#h&Zbko]fiZkmยยh~glf_:cp]vZยยฝ8{_ph~kยฆcย	Z{Yยยยพ#ยฟlร3ยยฆรรย`ยยยยยรยยย[รรยmยพ3ยฟย@ยฟยย[ยยย[ยฟรยย}ย)ยฟร{ย`รรTรยย	cqe@f:Y[Z\)\1Z8]รh&k@ย
gbh~ย3iยฅmย3ยยh1Zkoยpe@Z]รย|ยยZc3eikfยยh)\1รยยfรin@c3ยฅ@_3fgjยlรยZยฅ@_TY[_aยf]Zb_3รxTtve@fvfaยmfg@gbh1k@ย]f:gbh~ย3iยฅmย3ย
ยฅmยpfยvยยh1anl\1fรรbยฅ@f_ph1fยvc3e@_qZยฅ@ยe@Zยฅ@c|lI\)\1Z8]รh&k@ยยฅmยvcqZรn@nยfI\ยc3Zยc3e@f_3fยpยฅl\1cยรZYยc3e@fรn@_qfยฝh1Zยฅmย:ย#ficph1Zkยx
ร h)\~ย3ยpZkรryยงรbรlzMiZkmยยh~glf_3fgรc3e@fLn@_3Zยl\1faรZY]:emcรiZbยฅl\Pgรยmfsh1klY`f_q_3fgE{ยmZยฅ@cยกc3e@fรn@_qZยmร
ยlh)\)h&cpรFZYยif_3c8h&kFn@_qZnmZยฆยuh&c#h&Zbkmยยh1ยฝfkรยpZafoiZkmยpcq_Ih1kยฆcยxFร@Z_fรmanl\1f|v]vfMah1ยelcรlk@Z]รc3emc
ร _ยr1รvยร1รยยยพ3ยmzรยขร@ร)ยชokmgLc3emc ร _r3ยยIย~ย)ยฟรกzรขรฃร@รยยฉ@|รคkmgรยยfh1klc3f_3fยpcqfgsh&k ร _r1รvยร1รรย~ยพ3ยยรฅรยยยยย)ยฟรกยzx
รฆ Zยฅ@ยbel\&รยย#nmfรbh1k@ยm| ร h)\PยqยpZkรงย#ยฅ@ยยfยpcยยiZan@ยฅ@cph1k@ยLc3elh~ยยlรยiZkmยยh~glf_ph1k@ยยI\)\รn@_3Zbยmยlh)\ยh1cpรรงgbh~ยpcq_ph1ร
ย@ยฅ@cph1ZkmยiZkmยยh~ยpcqfkยฆcj]vh1c3eรc3e@fMiZbkmยpc3_Ih1klcย|ยkmgsc3e@fkยiZban@ยฅ@cph1k@ยยc3e@fย_k@ยfZ{Yยฝ88\&ยฅ@fยjยh1ยฝfkLc3Z
ร _ยr1รvยร1รยยยพ3ยรฅยกยยยยย)ยฟรกยz:ยยฆรรจc3e@fยpfMgbh~ยpcq_ph1ย@ยฅ@cph1ZkmยxMร5Z_3aM8\ย\1ร|Tยpยฅ@n@nmZยฆย#fZรญ ยฅ@_\~k@ยยฅmยbfiZbkmยยh~ยpcยยZ{Yรรฉ
n@_ph1ah1cph1ยฝfon@_3ZbnmZยฆยยh1cph1Zkย|ยปรชmรซรฌรIรร3รฌยรช@รญxรรฎรZkmยยh~glf_c3e@fยย#fcรฏรZYรรฐรฒรฑรณยฉ cq_3ยฅ@c3eยยqยยh1ยk@afklcยjc3e@fยpf
n@_3ZnยZยฆยยh1cph1Zkmยxรตรดรfยย{h&ยฝbfรยpfaM{kยฆcph~iยรตc3Zยn@_3Zยmยlh)\)h~ยpcph~iยpc{c3fafklcยvZยฝf_:c3elh~ยร\~k@ยยฅmยfรถh1kรcqf_3aMยvZY
n@_3Zbยmยlh)\ยh1cpรรgbh~ยpc3_ph1ย@ยฅ@cph1ZkยกรทรZยฝf_:cqe@fรยpfcรรฏ^ruยpffยruรm{ยh1kย|mรธร8\&nยf_3kย|ยรนรปรบรจfยh~g@glZm|TyยงยงbรlzTY`Z_glfร
cIh)\~ยzxรรผh&kmifยfbi3eยc3_3ยฅ@c3eยย3ยยh1ยk@afklcรครฝรรพยรฏยglfc3f_qah1k@fยvc3e@fc3_3ยฅ@c3eยฝI\1ยฅ@fยZYยfยฝf_3รn@_qZnmZยฆยuh&c#h&ZbkmI\
Y[Z_3aยฅl\~jรฟ|@]vfยikยglfc3f_qah1k@fยc3e@fยn@_3Zbยmยlh)\ยh1cpรยZ{Y fยฝf_qรยยpยฅmiqeยY[Z_3aยฅl\~ 
ร _ยr[รฟ	zรครฑ

รทTr[รฝ:zร

 	


รฎรค\1f_p\1ร|T]vfikรglfc3f_3ah1k@fย]:e@fc3e@f_ยon@_qZยmยlh)\)h&cpรรgbh~ยpc3_ph1ย@ยฅ@cph1ZkLรทยย3c#hPยmfยยoยpfc ZYn@_qZยmร
ยlh)\)hPย#cph~iiZkmยpc3_Ih1kยฆcยยxtve@fยpckmg@{_gยk@Zcph1ZkรจZY n@_qZยmยlh)\)hPย#cph~in@_3ZnยZยฆยยh1cph1ZkmI\Th1klY`f_3fkmifย]Zbยฅl\Pgรจย3Iร
c3emc Eร รฑ ร _r[รฟรz	
รพ  รซ รฌ 5hยY ร _ @r[รฟ	zยปhPยร]vh1c3elh1koc3e@fร_k@ยf รซ รฌmY[Z_vfยฝf_3รยgbhPย#c3_ph1ย@ยฅ@cph1Zkoรทยกc3emc
ย3c#hPย mfยvc3e@fรiZbkmยpc3_Ih1klcย h1
k ยx
f mklh1cph1Zkย|5c3e@fรiZkmยpcq_Ih1kยฆcยTc3emcรคZk@f:ikMglf_#h&ยฝbf
 klY[Z_3c3ยฅ@kmcqf\&ร|b]:elh)\1fรc3elh~ย hPยรยฝf_3รkmc3ยฅ@_I\5gl
Y[_3Zaยขh1cร_qfยcpรยฆnlh~iI\)\1รยรbยฅlh&cqfร]vfร}x	ร5Z_:c3emcv_3fbยpZkย| ร h)\PยqยpZkรยpยฅ@ยยfย#c3fgoยpc3_3fk@ยbc3e@fklh1k@ยMc3elh~ยรตk@Zร
cph1ZkZ{Yh1klY[f_3fkmifยยlร{n@nl\&รbh1k@ยc3e@f:n@_#h&kmiqh1nl\1fยZ{YaMIรlh1aยฅ@aรฃfklc3_3Zbnยฆร  _{c3e@f_รคc3emkMiZbkmยยh~glf_ph1k@ยMI\)\
gbh~ยpc3_ph1ย@ยฅ@cph1ZkmยรทLย3cph~ยยY[รh1k@
ย ย|@]vfยiZkmยยh~glf_ยZkl\1รoc3e@fรgbh~ยpcq_ph1ย@ยฅ@cph1Zk}ruยzvรท ยc3emcรตemยฝbfยc3e@fยย_qfc3fยpc
fklc3_3ZnlรรaZk@ยoc3e@ZlยpfMย3c#hPยuY`รbh1k@ยรcqe@fMiZkmยpc3_Ih1kยฆcยยxMรยยย]vfk@Z]ยขยpe@Z]ย|ยZk@fh&aยnl\ยh~ic#h&ZbkFZY:Zยฅ@_
_3fยpยฅl\1cยยปh~ยTc3em{cรคc3e@f	_{kmglZaร[]vZ_p\~g@ยTafc3e@Z@gjn@_3Zยฝh~glfยรคn@_ph1kmiqh1nl\1fgยaZcph1ยฝcph1ZkjY`Zb_ยญc3elh~ยยปh1klc3_3Z@glยฅmiร
cph1ZkยกZY	aM8รยฆh1aยฅ@afklc3_3Zbnยฆรoc3Zยn@_3Zยm{ยlhย\)h~ยpcph~iยn@_3ZnยZยฆยยh1cph1ZkmI\รค_3fยpZbklh&k@ยยxรยยkรจY icI|mc3e@fiZk@k@fic#h&Zbk
ยmfcp]vffkn@_3Zยmยlh)\)h~ยpcph~iรตn@_3ZnmZlยยh1cph1ZkmI\m_3fย#Zklh1k@ยยkmgj_kmglZaS]vZ_p\~g@ยรคยpe@Zbยฅl\Pgjk@Z]FยmfรY-Ih1_p\1รiq\&f{_ 
ย "ยญรซรฌIรรIร3$รฌ "ยปรญยฆx
! tve@fยn@_ph1ah1cph1ยฝfn@_3ZnยZยฆยยh1cph1ZkmยรรชmรซรฌIรรIร3รฌ-รช@รญiZ_3_3fย#nmZkmgยc3Zยc3e@fยยฅ@km_3รยn@_3fgbh~ic3f#
! รFn@_3ZnmZlยยh1cph1ZkmI\lY`Z_qaรยฅl\~รรฟoZยฝf_mรช รซ รฌรIรร3รฌยรช รญ iZb_3_3fยpnยZkmg@ยTยฅ@klh~รยฅ@f\1รc3Zรkjfย3ยpfklcph~I\)\1รยn@_3ZnยZร
ยยh1cph1ZkmI\}Y`Zb_3aยฅl\P&
 % 
 ย Y`Z\)\1Z]รย  ]vfร_qfnl\PbifรfiqeยZViiยฅ@_3_qfkmifยZYยญc3e@fn@_3ZnยZยฆยยh1cph1ZkmI\ยญยpรlaยmZ\
รช ' ]รh&cq
(
e ")'3r * zx
c รฃZ{Y n@_3Zbยmยlh)\ยh~ยpc#hPisiZkmยpc3_8h&klcยiZb_3_3fยpnยZkmg@ยcqZยยกรlk@Z8]ร\&fglยbfยยmยp,
f +ย.ย -/012รง
! tve@fยยpf
iZkmยpcklc3รยY[_3ff:รlk@Z]v\1fglยfยยmยpfยiZklcIh1klh1k@ยMZkl\1รn@_qZnmZ_qcph1ZkofรVn@_qfย3ยยh1Zkmยx	tรe@fยiZ_3_3fยpnยZk@ร
glfkmifยh~ยรbยรคY[Z\)\1Z]รย 
354

687:9(;=<?>A@1B=C5DE<E7=F >HGJIK9C5CL<E7

MONQP=RSETUVT(WXWZY\[^]_:P=R]5``aWZSEbcSVd#Ye=]dfSgRhjiARlk/mn&UoP=P]5UVR\WZb=pqWZbsrOWt`R]lP(XtUEu]5vwTx[yYe=]
P=RSEPSgRY\WZSEbq]_:P=R]5``aWZSEb{zZz |}k~)n$zZz xยยVWZhWXtUVR\XZ[EยยUยuSEbvgWZY\WZSEbULX1P=RSETUVT(WXWZY\[ย]_=P=R]5``WยSgb
iARk/mKz mHยtnHWt`ยR]lP(XtUEu]5vยT([ยย|}k~ยn$z | }xย k\~)nยย(ย
MยยยUEueยuSEhยPUoR\Wt`\SEbยuSEb=b=]5uYWยยg]ยcWt`R]lP(XtUEu]5vยTx[ยย1ย)d/SERย`SEhย]8ยยxUVbvย]5UEueยcยKWยYeยยยย
kย1e=]ยPUoRY\Wtuย(XtUVRยue=SoWยu]5`ยd/SER8Ye=]ยUoP=P=RSL_xWZhยUVY]ย]5ยกgยULXWยY\[quSEb=b=]5uY\WZยE]5`v(Sb=SEY.hUVYY]lR
WยbยขYe(Wt`8uSEb(Y]_=Y5ยn
ย1e=]SEYe=]lR]XZ]lhย]lb(Y$`#YeUVYยulUVbUVP=P?]5UVR#WZbยUP=RSEP?SERY\WZSEbยขd/SERhย(XtUยขk`\ยueUE`#R$UVY\WZSEbUยฃXยคb(ย=hยยฅ
T]lR$`ยขUVbvยฆUoR\WZYe=hย]lY\WtulULXยuSEb=b=]5uY\WZยE]5`nยR]lhยUยฃWยbยงย=bueUVb=pg]5v)ยยฉยจ=SERย]_UVhยP(XZ]EยยชYe=]qd/SERhย(XtU
iARkZยซ1ยฌยzZยญยยฎtยฏยฐ?n8ยฑ{ยฒยณยดยย1SEย(XtvuSERR]5`\PSgbvYSยYe=]ยP=RSEPSERY\WZSEbยd/SERhย(XtUยยยฃยตยยถยทยฌ=k\~)n$zยนยธ1ยฎtยฏยฐ)k\~)n$ยยยบย
ยฒ=ยณยดxย
ยป

ย1e=]lR]Wt`AU8SEb=]lยฅ/YSEยฅ/SEb=]8uSERR]5`PSEbv(]lbu]8T?]lY\ยย]l]lbYRย=YeยผUE``aWZpEb=hย]lb(Y$`AUVbvUVYSEh`lยฝ)Ye=]ยYRย=Ye
ย ร
UE``aWZpEb=hย]lb(YAยพ^uSERR]5`\P?SEbv=`YSYe=]UVYSEhยยฟยงย{ร ร)

ยณ5ยณLยณ ร ร ร ย ย8e=]lR]ร ย ย Wt`ยรยย Wd ยพ8ktร:ยรnAยยฆรรยฏร:ร
SEYe=]lRย1Wt`\]Eย^ร)]lYยพ รVร ยณ5ยณ5ยณ ร ยพรรรT?]ยYe=]ยYRย=YeรUE``aWZpEb=hย]lb(Y$`ยuSERR]5`\PSEbvgWZb=pyYSqYe=]

UVbvsรรร ย
UVYSEhย`1ยฟ
ยป

ร ร ยณLยณ5ยณ ร ยฟ
ร

ยR]5`\P]5uYWยยg]Xย[Eย

ย1e=]lR]Wt`ยUรSEb=]lยฅ/YSEยฅ/Sgb=]ยuSgRR]5`\P?SEbv(]lbu]ยT]lY\ย1]l]lbP=RSETUoT(WรXWZY\[ยvgWt`\YR\WZT=ย=Y\WZSEb`ยชSVยE]lRYe=]`\]lY
ร
ร
ร
SVdAYRย=YeUE``aWZpEb=hย]lb(Y$`.UobvยPSoWยb(Y$`KWZbยร
ยยยจSER8]5UEueP?SVWZbxY&รq
ร ร ร
ย(XZ]lYร ร ร v(]lb=SEY]ยชYe=]
ร
ร
uSERR]5`\P?SEbvgWZb=pยP=RSETUVT(WXWยY\[vgWt`\YRWยT=ย=YWยSgbยSVยE]lR
ยย8e=]lR]ยร ร ร k/ยพ ย\nยย
ยย

รยรรqรรVรyรกรรข/รฃxรค)รฅ&รฆ

XZ]5UVR\XZ[Eย(ยพ รงยz ยcm,WรรจยยฟKรง

รยรฉ

kร| } n$ยย1e=]lR]d/SER]Eย(d/SER8ULXXยร ร ยยย]eULยE]

รชรฌรซ รญ/รฎlรฏ ร
k\ร nAยรฐiARรฑEรณ รฒ k/mยn$ยณ
ยKe=]dfSoXรXZSVย1WZb=pqR]5`\ย(XZYยv(]lhยSEb`\YR$UVY]5`#Ye=]ยY\WZpEexYuSEb=b=]5uY\WZSEbyT]lY\ย1]l]lbยP=RSETUVT(WXWย`Y\WtuP=RSEPS(`aWZยฅ
Y\WZSEbULXHR]5UE`\SEb(WZb=pยย`Wยb=phยUL_(WZhย=hQ]lbxYRSEP([รดUVbvRUVbv(SEhQย1SER\Xtv=`lย
รตรถ ร(รทรธรLรรรนรกรรข/รฃgรกยผรฅJรบ
ร

ร5รยชrรปยญรcรผรพรฝรฟ$รรธรฝLรรยฎfรฟOรฟ^รฝ$รฟ	lรรยฏรผoยฎ
)ร

รฟรรxรยรฟoยฏ

iARk/mKz m ย ncย

 ร ร (รlยฏรqยฎยรผsร?ยฎร:รยฏรฟxยญรผxยญยฎ/ยถยนยฎfรรยฌcยฐoยฎ
lรรยฏ$ยฎรยญร=รรยฎ/รฟ{ร!^รฟ"ยรผ#Lยฎ
ร
lรผEรรยฎ
ยฌยฃยฎ'&&r(*)ยรฟVยฏรรฟ+ยฃรยฏ,ยรฟoยฏยผรผEยถรยถ(mรพรผรธยฐยชm ย ,ยยฎ .iARรฑ.-lk/m ย n0/cยฒ,Aรxร$

iARk/mKz mรยยn

iAR1ยkร|}k2Lnยz |} ย k2Ln ร43
ยธ

QรฟVยฏ

ร$รธรรยฏรฟ%ยฌ

ย 5
r /nHย{iARรฑ.-lk/mKz m ย nยณ

ยKe=]lSER]lh6?ย8796ยWt`UVby]5UE`\[ยuSgRSVXXtUVR[SVd.ย1e=]lSgR]lh:6ย87;7Eยยขย Sque=]5u<YeUVYYe=]ยP=R]5uSEbvgWZY\WZSEb`
SVdYe=]XtUVYY]lRยYe=]lSER]lh
UoP=P(Xย[Eย1b=SgY]ยYeUVYYe=]quSEb`\YR$UยฃWยb(Y$`&WZbcrQUVR]XWZb=]5UVR5ย8Uobvc`\SqYe=]q`\PUEu]
= >ร  3 ยธ ย  0
r ?รeUE`Uย=b(Wtยกgย=]ยhUL_xWZhย=hยยฅ/]lb(YRSEP([P?SVWZbxYยข@ ร ยBAรb d UEuYLย?WยY#Wt`&]5UE`\[YS`\e=SVยOYeUVY#ร C ร Wย`
ร
Ye=]qkรย=b(Wtยกgย=]VnยhยUL_(WZhย=hยยฅ/]lbxYRSEP([P=RSETUVT(WXWZY\[รvgWt`\YRWยT=ย=YWยSgbcSVยE]lR
`UVY\Wt`ad/[EWZb=pYe=]รดuSEb`YR$ULWZb(Y$`
rยDAรbรฐUEv=vgWZY\WZSEb ย8T?]5ulUVย`\]qYe=]lR]qUVR]b=Syb=]lpxUVY]5vyP=RSEPSgRY\WZSEbc]_=P=R]5``WยSgb`ยWยbรฐrยYe=]ยd/SERhย(XtU

3
ยธ

E

ยc| } ย k2Ln ร43

ยธ

ย 0
r ?Wt`8u]lRYULWZb(Xย[ยข]5``\]lb(Y\WtULXXZ[ยPSx`aWZY\WZยE]Eย

Sx`YยผUVP=P(XWtulUVY\WZSEb`SVd#P=RSETUVT(WXWt`\Y\WtuยขP=RSEPS(`aWZY\WZSEbULX.R]5Ug`\SEb(WZb=pยuSEb`aWtv(]lRย`aWZhยP(XZ]ยuSEb`YR$ULWZb(Y$`

SVd8Ye=]ยdfSgRhยฉย`\]5v

WZbยYe=]ยYe=]lSgR]lhยAUVbvy`\Sย`\ยue^UVP=P(XWยulUoY\WZSEb`ยulUVbyT]ยEWZ]lย1]5v^UE`&ยE]lR[`\P]5uWtULX

ulUE`\]5`ยชSVd#Ye=]ยRUVbv(SEhยยฅ/ย1SER$v=`ยชUVP=P=RSxUgue ยFAรbydรUEuY5ยHYe(Wt`ยชYe=]lSER]lh
ย1e=]ยuSgb=b=]5uY\WZSEbยT?]lY\ยย]l]lbyuSEย=bxYWยb=pHGย1SER\Xtv=`JIยUVbv

Wt`]5``]lbxY\WtULXXZ[^Uยg]lR[SVXtv,SEb=]Eย
WZb^Uย`\PUEu]ยv(]%Kb=]5v

Ye=]ย]lb(YRSEP([qhยUL_(Wยhย=h

UE`UยขuSEbLย=buYWยSgbSVdยXWZb=]5UVRยuSEb`\YR$ULWZbxY`ยWย`&ยE]lR[qย1]XXยยฅM<(b=SVย8b ยNAaYยeUE`#T?]l]lbย]_=Y]lb`aWZยE]XZ[ย`YยvgWZ]5v

WZbยYe=]*K]XยvSodยYe=]lRhยS=v([xbUVhยWยul`ย`\Y$UVRY\WZb=pยย1WZYeYe=]B7POVYeยu]lb(Yย=R[ยยยSgRJ<SVd

R

E

Uยฃ_:ย1]XXรUVbv4QยWZT=T`lย

]5u]lb(Y\XZ[Eย=Ye(Wt`AY\[(P]8SodยR]5UE`\SEb(WZb=pยeUg`AT]l]lbยUVP=P(XWZ]5vYSP=RSET(XZ]lhย`รฌWZbยUVbยNSAยuSEb(Y]_=YATx[ยiAUVRWย`UVbv

TPU

VXWZYZ[]\^`_H\ZaZb[]cdWZYZ[fegWZhZi?^XjZ^lkmYZnZa'\op

qsr9t	u%vw'xy{z}|~P;ย.ยยzt	ยย;ย	z;xยJยยm|~P;ย.ยยย5ยZrdย5v;ยJyBvยยยszยย
xXzt	ยNqsr9t	u%vwZxy{zยย
xยย	zยJยย
u%ยย
zยย8ยยยJr%ย8r9ย
w{ztยsยยrPu9zย	xrdยJยZr9ยNzย
xvยยJrPzยยย8ย9rSยยZrยtZrPu%rPxJxยย8ยยBvยยz;ยv;ยZยย8tZย(zdยMv;ยJยยzย'tZv.ยย8v;tNvยXย9zยZยZยJvยยยกยยzยย8v;t	ยขZยฃ
zย8ยJยZv;ยZย.ย4ยJยZr(ยZยJrPuย
xrBยr9ยzยยย
xยvยยคยJยZr%ย8ยยzยZยZยJvz.uJยยฅย.ยยยฆr9ย5ยMยJv;ยยงv;ยZย%x9ย
ยยจv4ยJยZrBย	rPxย(vย5v;ยZยยytZvย5ย8rPยย;r;ยฃยจยNvxยยvย0ยJยZrBย5v;ยJyFv;tยฉยZยJv;ย	zยยยยยย
xยย
uยยZยJv;ยยvxยย8ยย8v;t	zยXยJrPz.xv;tZย
ย8tZย"zt	ยยชzยยยXยMv;ยJยgzยsยZยJrPxr9tยzยยยกv.t	xยvย5ยJยZrNr9tยJยJv;ยย	ยซย0v.ยย
ยZxSu%v;tZtZrPu%ยยยกv.tยฌ|Mย8tยญย	zยJยย
u%ยย
zยPยฃmยJยZvxrBvย
|ยszยย?xยยฎยฏqsr9t	u%v{wZxyzZยฃ5~P;ย;Zยฐmย;ย	z.xยJยยยฑยฃ5~P;ย;'ยJย0ย	zw;rยฒยยย8ยBย8ยJrPยfยJยZr9ยgxr%ยยกw.rPxยยJvยฅu%v;tยณยZt	u%ยย8v;t	xยฒvยยยยดย8tZย
rPzย(u%v;t	xยJยzย8tยx9ยยยตยยZย(ย}v;ยJrBย;r9tZr9ยzยsย
ztZย.ย	zย;rNยย8w;rPxยย	x(zยย;ยrPzย(ยrPzยXvยยz;ยZย.ย8ยย8v;t	zย0r%ยZยZยJrPxJxยยกw.r
ย	vย5r9ยPย4ยถZv.ย(r%ย	zยNยย8r;ยฃยจยยกยยฒย
xBยท;ยย8ยJrยยJrPz;xv;t	zยย8rNยJvFยยธztยยยJยZr4zยยยยยย8ยยยชยJvยฅr%ยZยZยJrPxJxยฒยJย	zยยยZยv;ย	r9ยJยยยกrPx
zยJrF|zยZยZยvPยย8ยgzยJr%ย8ยยยยนxยzยย
xยย
u9zยยย8ยยญย8t	ยr9ย	r9t	ยr9tยPยDยถZv;ยBr%ย	zยNยย8r;ยฃ5ย5rgยยzPยยชย5ย
xยยฌยJvยญz;xxr9ยJยBยJย	zย
ยบ5ยป
ยผJยฝยพ|ยฟยพยNzt	ยรร	รPรรยรรd|ยฟยพยNzยJrยยกt	ยr9ยยr9t	ยr9tยยชยZยJv;ย	r9ยยย8rPxยยยDxJzย;ย8tZย`ร8รรยบ5ยปยผJยฝ	|ยฟยพยยพรยฌร	รร
รยรรd|ยฟยพย9ร8ร รDร
ร8รรยบ5ยปยผJยฝ	|ยฟยพยร8ร รยรร8รร	รร
รยรร*|ยฟยพยรร?ร รยยรsย8rPzยย8ย;ยฃ	xย	uJยยฅu%v;t	xยJยzย8tยx5zยJrdtZv;ยยยยดย8tZrPzยย0ร*r9w;r9ยJยJยZr%ย8rPxJx$ยฃv.ยZยSย5ยZr9ย
v;ยJr9ยรร	ย8~;~ยu%vw;r9ยx5xย	uย"u9z;xrPxยzt	ยยยBย	uJย4ย}v;ยJr;ย
ร w;r9ยxยยกv.tยฅvยXยZยJv.ย	zยยยยยดย
xยย?uรยZยJv;ย	vxยย8ยย8v;t	zยsยJrPz;xv;tย8tZยยย	z;xdzย
xvNย	r9r9tยฉย	xrPยFยJv}ยZยJvw;ย
ยr(ยZยv;ย	zย
ยยยยยย?xยย
uNxr9ยgztยย
u9xยธยv;ยdยr%ยรzยย8ยยยJrPz.xv;tย8tZยยฅ|ยmrPzยยยฑยฃ~P;ย.ยยXร*r9ยJrยนz{ย?xv	ยฃ	ยยZr(u%v;tZtZrPu%ยย8v;tยฉยJvNยzt	ยv;ย
ย5v;ยย
ยZx*ย
xยvย5ย8tยJr9ยJrPxยPยยฒรยฑtfย	zยJยย?u%ยย
zยยฃยพย8ยยยMvยยย8v{ยdx*ยยJv.ย:รXv;ยJvยยย
zยJยยฉร	ย?~PรNยย	zยยยJยZrBยJrPu%r9tย(ย5v;ยJy4vย
ร vย?ยZxย9ยBย
ยยPยฃรFv;ยJยย
x9ยฃยพzt	ยยฉยmrPzยย5|~P;.รย0u9ztFย	rBr9ยBย	rPยZยrPยFย8tยญยJยZrBยzt	ยv;ยNยMย5v;ยย
ยZxยธยยzยNr9ย5v;ยJyยพย
รยฑtFยJยZrยยJrPxย*vยยคยยย?x*xยZย	xrPu%ยย8v;tยฃยย5rยr%ย'ยย
zย8tยฉยJยZr%ย8ยรzยZยZยJvz;uยรzt	ย4ยJยZrยr9ยBย	rPยZย.ย8tZย	ย
รยv;t	xยย
ยr9ยXz*ย
ztZย;ย	zย;rยu%v;t	xย?xยย8tZย(vยยพยZยJv;ย	vxยย8ยย8v;t	zยZยMv;ยJยBยย
z;xvw;r9ยmยJยZr5ยZยJv;ยยvxยย8ยย8v;t	zยยพw{zยย?zยยยกrPx
ร	รรรPรPรรยรZร ยฃmzt	ยยญยr%ยยฑzยย8ยยนยยยยกrPxยฒvย5ยJยZrBยMv;ยJย:รรขรกรครฃรฅ|ยฑยJrPz;ยยฌย$รNรฆรงxยฒzยJrNยยยย?u9z{ยยดย8ยรจรฃBรฆรxJยขยยฃยจยยยZr9ยJrgร
zt	ยNรฃรฉzยJr*ยZยJv;ย	vxยย8ยย8v;t	zย	ยMv;ยJยBยย
z;x9ย ร ย.ย
xยJยย8ยZยZยย8v;tยรช4ย
xยxJzย
ยNยv(รซ%รฌยฑรญ9รฎ;รฏยปรญ
รฐ%รฑgzยยr%ยรzยยยกย5ยJยย8rยรlรก:รฃ
ยยย(รชm|รฃNรรฒรBยยฅรณรด~Bรตยฌรซ9ยรถรยฑtรทz.ยZย.ยยกยยยกv.tยฌยJvรจยr%ยยฑzยย8ยยยJยย8rPx9ยฃ5ยJยZrยยMยzยNr9ย5v;ยJyfzย
xvยญย	r9ยยBย8ยxNยยZrยฅย	xr4vย
ยgzยr9ยย
zยย8ยNยยยย
u9zยย8v;tยฉย8t"z}ยJยย8r;ยฃยพz;xยย8tยญรรนรธรบรฃBย ร ย.ย
xยJยยยกยZยZยยยกv.t"รชรปย?xdxJzย
ยยJvgxJzยย?xยยยฅxย	uย"zรยJยย8r
ยยยSรชm|รฃNรรฒรรยNรผรฅ~.ย รรพรฝ รฎยผJรฎรฟNรPรฏMร$ยผยป Pร%ยฝ รฝ ยผJ
ร 
รฎ รยปรรยปMรฏรฑ"ยฝยปรญรรฏยผ%ยป Zรฏยป
ร H|ยX

ย 	ยฒย*ย
xยนzยฉu%vยยย8rPu%ยย8v;
t P
รช gvย
ยZยJv;ย	zยยยดยยย8ยย"ย.ย
xยJยยยกยZยZยยยกv.t	xdv{w.r9
ย ยยฃZย	zย%zยNr9ยJr9ยย8ย9rPยยยยฅรซ9ย ร ยX
ย 	Pรช    รซยยฑxzยย
x 	rPxยzNxr9
ย  vย
ยJยย8rPxยยยยยยv.ย(r9w;r9ยJยยญรซ9ยฃmรช  รซยยฑxJzยย
x 	rPxยฒr9w;r9ยJยยญยr%ยยฑzยย8ยBยJยย8
r  ! zt	ยfxJzยย?"x 	rPx(r9w.r9ยJยยฉtZv;tZยยฑยr%ยรzยยยกย
ยJยย8#
r $ ยฉย ร xr9%
ย รขvยยธยr%ยยฑzยย8ยยยJยย8rPx(รซยMr9tยz{ยยดย
xยฒรรรก รฃรนยยยยยv;ยdr9w;r9ยJยยฉยX
ย 	 ยย	zยรรซยยฑxJzยย
x 	rP&
x "ยฃ
ยยย8ย (') รช  |รฃNรรฒรBยยรผ`~;ย
ร xNxยZvยยtยชย8t | ร r%ยฆtZr9ยรยฎรดยmrPzยยยฑยฃย~P.;รยยฃXรซยMr9tยz{ยยดย8ยNr9tยยย	vxJxrPxJxrPxNzยฉtยZยBย	r9ยNvยรยJrPz.xv;t	zยย8r
ยZยJv;ยยr9ยJยย8rPx*ยยยย
u9zยยย8ยยฅz;xJxvZuย
zยJrPย4ยยธยยกยยยฅยr%ยรzยย8ยdยJrPz;xv.tยยกtZยยยฃย8t	uยยกย	ย.ย8tZยยฅzBยZยr%ยr9ยJr9t	u%rยยv.ยยยNv;ยJrยxยยr9ย
uย*]u5ย8tยMv;ยJยgzยย8v;tยsรยvย5r9w;r9ยPยฃยJยZr9ยrยธzยJrSzdtยZยยนยยr9ยXvยยrPxยย8ยzยย8rdยZยJv;ย	r9ยยย8rPxXยย	zยmย8ย0ยvrPxmtZv;ยXย	zw;r;ย
ร ยNv;tZยยv;ยJยZr9ยdยJยย8tZยx9ยฃยย8ยJยJr%ย8r9w{ztยยยยกtยMv;ยJยยzยย8v;t4ย
xdtZv;ย*ย8ย;tZv;ยJrPยยพยร|Jย;r9rN"| +5z;u9uยย	xdr9ยรzยยฑย8ยฃs~;รZยsยMv;ย
zt4r%ยZยJr9t	xยย8w;rBย.ย?xu%ย	xJxยย8v;tยฉvยsยJยย
xยย
xJxยZr;ยยย
ยยจv(v.ยZยzย8tรz.ยZย.ยยกยยยกv.t	zยยยrPxยย8ยzยย8rยยZยJv;ยยr9ยJยย8rPx9ยฃ	รซยยฑxr9ยgztยย
u9xsย
xยr%ยZยJr9t	ยrPย}ย8tยฉ| ร vย
ยZxย9ยรย?ยย5r9ย5zยยฑย8ยฃ
P~ ;.รยSยยยญzt zยZยยยดย
u9zยยยกv.tยฌvยSยยZrgยZยย8t	uย8ยย8rยฅvยdยgzยยยกยBยZย:r9tยJยJv;ยย;ยยฅรยt	xยJrPz;ยยชvยSu%v;t	xยย
ยr9ยย8tZยHzยยย
ย	vxJxยย8ยย8r ยยย
	Bรฆรงx9ยฃยz;x4zย	vw;r;ยฃdย5rรปu%v.t	xยย
ยr9ย"v.tยยกยDยJยZrfยXย
	-,Zรช/. 0 132  xย	uJยรฉยJย	zยPยฃยธยv.ย4rPz;uJยรฉรซ9ยฃ

รช/. 0 1 ย	z;x*ยJยZrBยgzยย8ยยนยZยรr9tยJยJv;ยยยฅzยNv;tZย4ย.ย
xยJยย8ยZยZยย8v;t	xยยJย	zยSรซยยฑxJzยย
xยยMย"zยยยXยJยZrยฒยJยย8rPx*ย8t "ยย|Jย;r9r
| ร vย
ยZxย9ยBย
ยยยr9ยSzยยฑย8ยฃยพ~P.;รยยMv;ย5ยZยJrPuย
xrBยr	tยยกยยยกv.t	xยzt	ยNยrPuJยZtย
u9zยXยr9ยzยยย
x9ยยยยนร*v;ยJrdยJย	zยยฃ'xยยกt	u%rยฒยJยZr
u%v;t	xยยzย8tย%xย	xrPยยฒยJvยยr 	tZr5/
รช . 0 1 zยJr5zยยยยยย8tZrPzยPยฃ.ยJยZr9ยJrยย
xยจยยกt	ยr9rPย}zยยZtย
ยท.ยZrSxย	uย(ย	vยยกtยsvย	ยgz{ยย8ยBยZย
r9tยJยJv;ยย;ย ร ยJยย8r"ร รก รฃรฅย
xgz5
t 476Xรฌ รฝ รย
รฎ รญ%ยป 9รย9
ร 8
ร ยรญรร :;Z;ร 8รยฒv%
ย  ยยยยยยย8ย (') รช . 0 1 |รฃNรรฒรBย4รผ ~;ย
ย5ยZrยฒtZv;ยย8v;t4vยX9
ร <sยMยย
zย	xยย8ยย8rgu%v;t	xrPยท.ยZr9t	u%rยฒย
xยzt	zย8ยย9rPย4ย8tยฉยr9ยzยยยย8tf| ร vย
ยZxย9ยรย?ยยdr9ยSzยยฑย8ยฃยจ~P;;รย%ยฃ
ยยยZr9ยJrยฒย8ยdย
xยxยZvยยtยฉยJv}ยยกtZยZr9ยยยกยBzยยยXยJยZr(tย
u%rNยZยv;ย	r9ยJยยยกrPxยvย0รซยMr9tยzยยย8ยNr9tยN|xย	uยยญz;x*ยJยZrBยZยJr%ยr9ยr9t	u%r
ยMv;ยยยNv;ยJrNxยยrPu*ย ]uBย8tยMv;ยJยgzยยยกv.tยพยยฃยพยยยยยย8rรxย	u9u%rPxxยยยยยย8ยยชยยกย.tZv;ยย8tZย4ย8ยJยJr%ย8r9wztยยย8tยv;ยยgzยย8v;t#
ย <ยยท;ย	zยยย8ย
ย8ยNย	v.ยJยztยย8ย;ยฃZzย8ย;v;ยย8ยJยZยgxยzยJrdยZยJvw;ย
ยrPยรยv.ย0u%v;ยNยZยZยยยกtZยBยJยZrd=
ร < ยMยย
zย	xยย8ยย8rยนu%v;t	xrPยท.ยZr9t	u%rPx5vยยzยฒxr9ย
vยsยJยย8rPx5ย8tยฉu%r9ยJยzย8tยฅu9z;xrPx$ย
>?

@BADCFEGIHKJMLNOPGPAQ/HSRTUCVNNWGPA

XBYZ%[]\W^_[`Y[#a(b;cFdZePfFghZibjkYFldmj3n;\cpoqb#Yqjkbrsdetjvuexwydiuq\d%dub#\ffZe\znu9e{)|}el~rjk;[ย_ยrFd
b;d3\WlยยKยยzยPยFยยn;\c9oIb#b;[`oqbrrFbr7_c

ePYZ){(Zm\[ยb;wยezZย#_cย\$jvdZm\W_ยPuFdk{(ePZwB\Zmrย[\ccb;Zยยย

b#j_[#fFlg

dZm\ยcqjl~\db3\#rFb{\YFldBZYFlb3ย`e{Kdub){(ePZ[ยยยยยยย_ยcFde\ยยqZmjkda(ePZrFb;ZrFb{ย\ยYFlยd)ZYFlb
ยย%ยBย;ยยยกUยขยฃ;ยค|"ยฅยmยฆ ยฃ;ยงM|"ยฅยจยยฉยขยชยยซยฌ%ยPยญ
\Pj_c$ePYZb\Zkl*_b;ZdZ\cqjl~\dk_ePc$e{KยฎM_*l~jjkePc/ยฏยฐj\ffZie\Pniu/ยยฑยฎePdb%diuq\dMdub){ยฒezZ[`YFlย\zjMduq\dB\Zk_~jkb%YcqrFb;Z
duF_~j%diZm\cqjl~\dk_ePcยณ\Wl*l
Yqjkb#dubji\[#b\ffZie^F_[\db`bยดzYq\Wl*_d"g

nePccbndv_ยยตzbhยซ

ยฌ ย#ยถMubยZb\PjkePc7_~jยduq\d

dubh\ffZie\Pniuยe{ย|}el~rjk;[ย_ยrFdยb;d\WlยยBยยzยPยFยYqjvbj`dub$j\[#bhยทย{(ePZ#\Wl*lrFb{ย\ยYFlยdยZYFlbj;ยยณย

bhn;\c

j_[`_*l~\ZklgยณdZm\cqjl~\dibย\s|cePcarFb{\YFld;ยZiYFlยb]ย$e{Mdub`{(ePZ[ยธยยบยนยปย_cdie=\]ยqZjkda(ePZmrFb;Z3nezcqjkdZm\W_cFd
Yqj_cย]YcF_ยยตzb;Zmj\WlยฑยดzYq\cFdk_*ยยผn;\dk_ePc/ยฝ
ย ย

ย

ย;ย(ยก/ยพ

ยฅ&|ยฃ ยง

|"ยฅยจยKยนยธยฃ ยค

|"ยฅยยยฟ

cqrFb;ZduF_~jMdiZm\cqjl~\dk_ePc/ยVwยb3n;\ยc$fZeยตPb%dub){(el*lยewM_cย]dub;ePZb;[7ย
ร

ร`รรFรยผรWรqร-รรร(รVรยจร]ร/รWรยฑร7รร]รhรรรqร;ร(รรร
รรรรรPรรรร/รร~รร$รรรร]รรรกรรqร;ร*รPรรรยฒรยรยรขPร;ร;ร;รกรรรรขhรรiรรฃร;รครฅยฉรยรกร
ร;รร/รฆรงรvรฅ3รขPร(รฅ;รmรยรmรร;ร*ร%รกรจรรฉรรชรmรค

ยยยรงย

รฒ

รพ cรฟfq\Zdk_~nYFl~\ZยduF_~j`dub;ezZb;[

รรร%รร9รซhรฌ
รญยรฎยผร*รรจFรmรร;ร*ร`รmรยรqร;รmรฏรชรจDรรชรยผรร%รkรฅBรฆรฐร รฑ

ZรณรตรดFยฃ ยค

| ร ยรรถ ยฃ ยง | ร ย/รทรนรธ
ย ยยฉรผ
ยรฝยPยฟ
รถ
ย;รบรป
รถ
รถ
รถ
_ย[ยfFlรฉ_bj]duq\ยdย\xlรฉlBdubhnez[#fYdm\dk_ePcq\xlUdibnucF_~ยดzYbj\cqrยณZibjkYFldmj

rFbjnZk_oIbr$_cยณ|})el~rjk;[`_~rFdb;d)\Wlยย/ยยPยzยFย
n;\ZZg]eยตPb;ZBde#diuF_ยj)jkfIbni_ย\xl
[#b;duerย

รพ dร\xlยjve

jkuewjยจduq\d/Zm\ยcqrFeP[#a(wMePZkl~r)fZeยตz_ยrFbjS\MfZk_cqni_fFlbr

n;\Pjvb3e{Kdub%Zm\cqrFez[#a(wMePZkl~rj

 Yqjkdk_*ยยผn;\dv_ยezc3{(ePZ/diub
\ffZe\Pniu

|})el~rjk;[`_~rFd%b;d3\WlยยSยยPยPยFยfZbjkb;cFd`|ePcb3wuF_ยniu9_~j%ยดzYF_ยdibrz_ยจb;Zib;cd{ยฒZieP[ยdiub
_c

 Yqjkdk_*ยยผn;\dv_ยezc=ยย_ยยตzb;c

|})el~rjk;[`_~rFd)b;d\WlยยยยPยPยDยร_dmjvblรฉ{Wยmย

รรร*รยรยผร	
ยรยรรDรยร
รพ cPbndk_ePcqย]wMb#ZbjvdZk_~ndbrยณ\ddb;cFdk_ePc9de=j"_ย[ยfFlยb$ยดzYb;Zk_bj;ย9XBYZ`[\W_cยณZbjkYFldยKยถMub;ePZb;[qยยPยzย
cb;brFbr$ezdub;ZB\PjjkY[#fdv_ยezcqjB\PjwMblรฉlยฝKbjijkb;cdv_ย\xlรfqej"_ยdv_ยยตz_dkgPยdub%b^F_ยjvdb;cqnb3e{K\3YcF_~ยดzYb3[\x^_[`Y[#a
b;cFdZePfFgfIe_cd
!

 ยx\cqr&dub
ZibยดPYF_Zb;[#b;cFdKduq\d"$# %&(| ย('!ยย/ย
!

boqbl*_b;ยตPbBduq\ยd/duF_~j/diub;ePZb;[_ยj

_cยjkfF_db`e{K_dmjl*_[`_dm\dk_ePcqjรชยยฑ\zjrFb;[#ezcqjkdZm\dibr$oFghdub#rz_~jnYqjj_ePcs_ยc)Pbndk_ePc*qย+Vย

Yqjkb{(YFl

ยฎb;ยตPb;Zdiublยbjij;ย

duF_~jZbjkYFld3\Wl*lexw)jBYqjde#dm\ยยPb3\PrFยตx\ยcdm\ยยPb%e{
ePcFlgh\#jk[]\Wl*lร{(Zm\ยP[ยb;cdBeย{ยฑePYZ)Zk_~niu$l~\cยPYq\ยzbPย-,ย\c
wMbยยqcqr=\][ยePZb3ยzb;cb;Zm\WlKdub;ePZb;[/.0B{(db;Z3\Wl*lย/diubยoq\zj_~nยnezcqnb;cdZ\dk_ePc9ZbjkYFld`|"ยถMub;ezZb;[1+ยย+Dย
uel~rj)wM_du9bjjkb;cFdk_~\Wl*lg9ceZbjvdZk_~ndk_ePcqj;ย

รพ c9duF_~j%jkbndk_ePc9wMbยjkuew

ย

b^db;cqrยยถUub;ePZb;[2qยยPย]j_ยPcF_*ยยผn;\cFdklgPย*3BewMb;ยตPb;Zย

duq\dM_d)_~j_cqrFb;br9fqejij_oFlยb]de

dub;Zb$\Zb$jkb;Zk_ePYqj&l*_ย[ย_ยd\dk_ePcqj#\cqrยณjkYodklb;dk_bj;ย

b)_*l*lYqjkdZm\dibยdiubjkb%fZePoFlb;[jog][#b\cqjUe{ยฑb^q\[#fFlbj;ยI\cqr$dub;chjvdm\db3\c$b^db;cqrFbr
XBYZ#\didb;[#fd3dies\zrrFZbjjยdubjkb]fZePoFlb;[j]|kjke7{ย\Z`\Pj&_~j`fqejij_oFlยbย&lยb\zrj`de
fFl*_~n;\dbrsยqcq\Wl
ZbjkYFldย
dub$dub;ezZb;[

ZbjkYFldย

\$Zm\ยdub;Z#neP[#a

รพ c9{\PndยdiubยfZiePoFlb;[j%wMbยrz_~jnYqjijย\ยZb#\PjM_cFdb;Zbjkdv_ยcย9\cqr7_[#fIePZdm\cFd%\Pj

wMbh\PndiYq\Wl*lยgรฟยย_ยยตzbPยฝ$dub;gรฟublf

b;cFdZePfFgPย`XM{UnezYZmjkbPยb;ยตPb;Zg

YqjยYcqrFb;Zmjkdm\ยcqrรฟ[#ePZib$e{

_~jjkYb`wMbยrz_~jnYqjij)_c

de7[\W^F_ย[`Y[รฐb;cFdZePfFgยผยฏยฐj)[\W_c

diub]lรฉ_[`_dmj]eย{

[\x^_[`Y[

duF_~j%jkYoqjkbndv_ยezc9_~j%Zbl~\dk_ยตPblg9[`_cePZ3neP[ยfq\Zbr

|k\ffq\Zb;cFd;ยZibjkdZk_~ndk_ePc/ยKwuF_ยniuรฟnePcqnb;Zcqj`dubยYqjkbeย{cePca(Ycq\Zg

fZbrz_~n;\dbjรชย54ePZ)dubยZb\PrFb;Z%wBueย_~jlยbjij%nePcqnb;Zcbr

\oIePYd)dub`ePdub;ZยVlbjjkb;ZยI_~jjkYbj)wMb`Zb;[\ยZย

duq\d_d_ยjfqeFjj_oFlbยdieยjvยP_f=rz_Zbndvlยg7de6Pbndk_ePc7Vย
ยpb3ยqZjkdยnePcqj"_ยrFb;Z`dubยZbjkdZk_~ndk_ePcqj&wMbยfFl~\PnbrpePc

diub/8:9ยยK\cqr

jvuexwydiubrz_;tnYFldk_bj`duq\d

\Zk_~jkbU_รฉ{Swยb)rFZePf#dub;[7ยรยpbjkdm\ZidKwM_dudiubBZbjkdZv_ยndv_ยezcde3\ยj_cยlb%[\W^F_[ยY[ยa(b;cdZiePfgยfqe_cFdย$0%j

<=

>@?BABCEDGFIHJDBKBLMCEN-?BABCOP?BQBRSF@TBFVUABWBKDGXY

Z\[B]_^a`cbd^a]ebfZ\gahZjik`lb6Z\[B]e`lg\]emonqpr[B]e`lg\]emtsGukvswyxj[B`z_x|{lZ\[B]}]ebZ\g\`c~fย`ยyhMยkmย`fxjZย]eยl]eg\ยzr`lgjยยยยiยxยbB]hg
mPhMยikmยยBmยu(ยยยBZrikZ:ย`f]xยbB`lZrยย`ยยk`zยZ\[dhZ_hยยยยZ\[B]5mPhยยfikmยยBmยยย]ebZ\g\`l~P~ย`ikbfZax_hgย]5xjยBg\g\`lยBbdย]ยยยf
xยikmยiยยhg@bยBmยยd]egยxย`ยEzr`lgยยSยBx|uยpย[fยdxe{MikbยZย[B]}~Bg\]xj]ebd^a]}`ยยEmย`lgย](Zย[dhbย`lbB]rmPhMยiยmยยBmยยย]ebZ\g\`c~fย~d`ikbZ{
zr]ยยยhl^a]6Z\[B]6~Bg\`lยยk]emย`ยrยกdbdยcikbBยขยฃZย[B]Pg\]aยยhZยiยยc]ยikmย~ย`lg\Zยhbd^a]l{ย`lgยz(]aikยขl[ZjikbBยขd{$`ยย_]hl^ย[ยฃmPhยยfikmยยBmยย
]ebZ\g\`l~P~ย`ikbfZMu@ยค:x}Zย[B]-ยยฅ`ยยk`zrikbBยข/]aยGhยmย~ยk]:iยยยยdxยZ\gยhZ\]x|{dZ\[iยxยzr]aiยยขc[fZjikbBยขยฆiยx}`ยยZ\]ebยxj]ebdxqiยZยiยยc]ยงZย`ยZ\[B]
Z\`ยk]egยhยbd^a]:ยยhMยkยB]xeurยจB`lgrZ\[iยxยg\]hlxย`lbยฉ{bB`lbBยยยBbiยยชlยB]ย]ebZ\g\`l~ยmPhMยikmPhย`ยยZ\]ebยยk]hlย6Zย`ยงbB`cbBg\`lยBยdxjZ\bB]xยxeu

ยซยยฌยฉยญยฏยฎยฐยฑSยฒยณยดยยตfยถยฏยท

ยธ ยB~B~d`fxj]ยฆยนJยบยยปยยผrยฝaยพMยฟย{ยhbdย/^a`lbdxยiยย]eg-Z\[B]5รbB`zrยk]ยยขl]:ยdhlxj]

ร:ร

ยบยnรรeยผยnqรwรรaร5ร}รยรBรswยฏร/nรรeยผยnjรยฏwรรaรยร}รrรBรรlwaร

ยค:x\xjยBmย]:zr]:z}hbZ(Zย`P^a`lmย~BยBZ\]5รยgยรยฆnqยผยnqยพMwรร ร:ร

ยปlnยร

hbdยรร

รค ร ร ร:ร_ร

ร ยฝยร

`lZ\]:Z\[dhZ-ร
รฅ

ร

รBรsยรยฃร ร

ร_ร

iSx

`lgrร

ร:รก

รBรร}รขรฃร ร ยฟfยฝ

รBรsย`lgrร

ร:รก

รBรรcยฟfร

iยx
ยปlnยร

รฅ

รรร

ร w@รยร

wยuรยbรZ\[iยx_^ehcxj]l{ยฏร}รB
ร ร ร:ร_ร

รค ร ร ร:ร_ร

ร ยฝยร

ร w@รยร

รVร
ร

ร_ร

[dhcxrZjz(`ยmPhMยikmยงยBmยฆยย]ebfZ\gย`l~f6~d`ยiยbZยx

`zย^a`lbdxยiยย]eg}Z\[B]:mPhยยfikmยยBmยยย]ebZ\g\`l~P~ย`ikbfZaxr`ย@ร

รร

ร

nqรBรsBยฝaรBรรlwyhยbdยรฃnqรBรยรfยฝยรBรรฆsw\u
ร ร:ร-ร

ร รง u$รยZriยxrbB`cZ}[dhgยยยฆZ\`Pxj[B`z


ยยฅ`cgยรร
รง รจ

Z\[dhZยiย(รรยรจรฉรร{dZ\[B]ebรชZ\[iยx:xj~dhl^a]ย[dhlx:hยฆยBbiSยชcยB]ยmPhยยfikmยยBmยยย]ebZ\g\`l~ย~ย`ikbfZ{njรBรs-รJรรMยฝยรBรร-รข*รรMwยu
รยbZ\[iยxย^ehlxj]l{@รยg ร ร ร

nqยผยnqยพMwรร ร:ร

w5ยบรBรsยรรซร ร u*รฌ}bZ\[B]6`lZ\[B]egย[dhbdยยฏ{iย_ร ร/รญ

ร ร {ยZ\[B]ebยฃZย[B]ยยBbiยยชlยB]

mPhMยikmยยBmยยย]ebfZยg\`l~ย~d`ikbZr`ยyZ\[iยx}xj~dhc^a]}iยx-nqรGรร$รร ร ยฝยรGรsrรข/ร ร wย{ยiยb/zย[iS^ย[^ehcxj]รฎรยg ร ร ร
ยบรฐร ร {rZ\[B]ebJZย[B]xj~dhl^a]รชร}รB
ร ร ร:ร_ร

รBรรยรรฏร ร uรยย5ร ร

xjmยmย]eZ\gย/z(]ย`lยBZยhMikbรZ\[dhZ:ร$g ร ร ร

nqยผยnqยพMweร ร:ร

ยธ `d{ยยยh~B~Bg\`l~BgjiยhZย]aยยรฃ^\[B``fxยikbBยขยh6xj]ยชcยB]ebd^a]

wrยบIรBรรฑBu

]aikZ\[B]eg:รBรsB{BรBรรฑG{f`cgยรGรรfu$pr[ยdx}ร$gยรยฆnqยผยnjยพMwรร ร:ร

wrย`f]xrbB`cZ}]aยfiยxjZMu

รยZยiSxรฒbB`lZยยciSxยฅรณqยBbd^aZjik`lbdxยฆรดBรตeรถ6รทรรตยZ\[dhZย^ehยdxj]/Z\[B]/~Bgย`lยยk]emย[B]eg\]
ร:ร:รธ

ยบรnรรeยผยnqรwรร ร

ร

รBรsw	รยnรรeยผยnjรยฏwรร ร
ร

w@ยบ

ร รง {@zr]ย^ehbmPhรl]/Zย[B]ยhlxjmย~BZ\`lZjiย^6ยhMยkยB]`ยยรฎZ\[iยxรฒยยฅgahl^aZjik`lb
B

`ย:Z\`ยk]egยhbd^a]รยl]^aZ\`lgaxย^a`lbยl]eg\ยขikbBยข*Z\`

ยBhZยhยยdhlxj]

nqยผยnqยพMwรร ร:ร

[dhcxยZjzr`รฃmPhMยiยmยยBmยยย]ebZ\g\`c~f~d`ikbZยxe{}hbdย)ยf

h/ยBbiยยชcยB]/mPhMยikmยงยBmยฆยย]ebfZ\gย`l~fรฃ~ย`ikbfZย`ยยรฎร

ร

รค ร ร ร:ร_รธรรฆรผ

ร

ร

iยรฎzr]/^a`lbdxยiยย]egยikbdxjZย]hlยZ\[B]

รBรรนwย{Z\[B]ebรฃZ\[B]eg\]:iยxยbB`รบยciรปรบ^aยยkZjlu:pr[B]eg\]

nqรBรรนBยฝaรBรรพรฝBw รผ

iSx

hbdยรZ\[B]Phlxjmย~BZ\`cZjiย^ย~Bg\`cยdhยiยยikZj

ร$gaรยnjยผยnqยพMwรร ร:ร-รธ w$ยบรฏรBรรฆรนB{Bhlxrzr]:zr`lยยยย/z}hbZuรฟ

รยbรยikยขl[Z_`ย@Zย[iSxย]aยdhmย~ยk]6nqhbdย/m6hbfยxqiยmรฒiยยยhg:`cbB]x}zr]ยง^ehยb^a`lbdxjZยg\ยd^aZewย{Bzr]ย^a`lbfZยiยbยB]ยZ\`PhlxยxjยBmย]
Z\[dhZ-Z\[B]eg\]ยiยx:h6xยikbBยขยk]ยmPhMยikmยงยBmยฆยย]ebfZ\gย`l~f/~ย`ikbfZMu:ยครฎxยzr]ยhg\ยขlยB]ยย]hgjยik]eg{ยฉzr]5]aยB~d]^aZ:Zย[iSx-Zย`Pยย]
Z\g\ยB]-ikbรZq~iย^ehMยy~Bgยhc^aZjiย^ehMยยh~B~ยiย^ehZjik`lbdxe{ยฉxย`ยงZย[B]5g\]xjZยgjiย^aZjik`lbยย`f]xยbB`lZ_xย]e]emยl]eg\/xj]egjik`lยdxeu

  

]ยbB`ztZ\ยBg\bรช`lยBgยงhZยZ\]ebfZยiย`cb*Z\`ยZย[B]Pg\]ยชcยikg\]emย]ebZยZ\[dhZ

ยnMรง wยงรจtรGuPยค:xยzr]ย[dhMยl]PhMยkg\]hlย

`lยdxj]egยยl]ยยฏ{$Z\[iยxยxj]e]em6x5Zย`ยย]ยhb`lยfยcik`lยdxยg\]xjZ\gยiS^aZยiย`cbJZ\`รฃmPhรl]l{$^a`lbdxยiยย]egjikbBยขZ\[dhZยZ\[B]ยฆยยฅยBbd^aZยiย`cb

  	
  



nMรง wriSx-bB`cZ5ย]aยกdbB]ยรฃ`lZ\[B]eg\zยiSxย]lu

}`zr]eยl]eg{ยZ\[iยx:ยciรปรบ^aยยkZjยiยxยhl^aZ\ยdhMยยkh6mPhยbiยยย]xjZยhยZjik`lbรฃ`ย(h

mยยd^\[Jย]e]e~d]egย~Bg\`cยยย]emรuยยค:x:Z\[B]ยฆยย`ยยย`zrikbBยขรฃ]aยdhmย~ยk]รบxย[B`ยz-xe{$hbf*h~B~Bgย`fhl^ย[*Z\[dhZยรณqยdxjZยยdxj]xยZ\[B]
mPhMยikmยยBmยยย]ebfZยg\`l~P~ย`ikbfZ}`ยย(ร

 

ยซยยฌยฉยญยฏยฎยฐยฑSยฒยณยดยยต

dยท

รค ร ร ร:ร:ร

   

zriยยยbB]^a]xยx\hgjiยk/ยยhMiยยฉikbxj`cmย]5^ehlxj]xrzย[B]eg\]

nMรง w$ยบรฏรBu

`lbdxยiยย]eg}Z\[B]ยรfbB`zrยk]ยยขl]:ยdhcxj]

 
  !"  #
$ %& '#( *),+ .-"
/0213546387:9;46<>=6?#9@=%AB=ABC!9EDBC6F:G#FHC6CABIDB3:=6FKJLFHMC6=646N.JL=O3QPR9@7KG'DB3LCOF;S27!NDB=ABG'DB3:7:9EP'AB7!N.75T(3LM'=646FUG'VG#F;ABM'=6CWI'VXN.CABMY
Z NR9;[.4\9;=ABJKJ8FUM.C6=64\9EABM'=6CO4\9;=6?.3L4,=6?R9;M]['ABCB^QNM.JL=ABFHM_0
ร:ร

ยบยneรkร (รต

ยฉnqรwยรSร ร

_ร

รw

/nรร

Bnqรยฏweร (รต

`*a

ยฉnqรยฏwยร ร

_ร

รw

(รต

n

@รตaรต

Bwaร

b:cdefhgOiKjk.l*f*cm,gonqprdsk.kf*c
t*uvv2wxLy]zKy:z:{#|}5}@w~Uw_ยยvu}@y]ยOยHยยย@ย!ยยย*ย,ย!ยUย.ยยย'ยยยยยย#ยยย,ย*ย,ย!ยUย.ยยย@ยUยWยยyย~'{#|ยy.{*xQยยยยยย~Uw*|2~;ยยu2ยy
ยก6ย@w*ยยฃยขKยคy'w_ย@y'ยยฃยฅhยยยฆ]}@ยค2{#}X}@ยคย(xXยy'ยง*ย@y'yยจw#ยกOยฉ2yUยยยยyUยกยย(x]ยชยซh{*x:zKy$zยw_uยยฌยยญyUยฎvhy.~U}.ย>ยฏยฐw#zKy'ยฑ*y'ย.ยซzKy~'{||w*}
ย@y.{*~;ยคยฒ}@ยคย(x]~Uw*|2~;ยยu2xQยยw*|ยณu2xQยย|ยงยฒยขKยคy'w*ย@y'ยยดยฅ2ยยฌยฆ*ยฆ]w*ย]{#|ย}@ยคยย|ยงยตยยยยยถ*yยจยย}.ยยทsw*ย]~Uw*|2xQย(ยy'ย}@ยคyยจยยธ{Rยฎยยยยจuยยยน
y'|}@ย@w*vยยบvhw#ยย|}ยตw#ยกยจยป!ยฝ#ยผ ยพยยฟ]ร>ร ยรยขKยคyร~Uww*ยHย_ยย|2{#};y.xยร*ร#ยซK~Uw*ย@ย@y.xLvhw*|2ย_ยย|ยงยบ};wรย!ยยยธรรยยยยย#ยยยรยซK{#|2ยรร*รยซ
~Uw*ย@ย;y.xLv2w*|2ย_ยย|ยง]}@w$รOย!ยย,รKยยยยย#ยยยรยซ*{#ย;y!ยฉ2w_}@ยคยชยoยฏ:y'|2~Uy*ยซรOร รhร(ร'รQรHรร#ร6ยรsร ยOรรยชยซ*xLw:};ยค2{#}รยขrยคy'w*ย@y'ยรยฅ2ยยยฆ*ยฆ
ยwy.x:|w*}:{#vvยยย*ย
ร u}.ยซh{*x:zKyx@{Rยยฌยยซh}@ยคy$vย@w*ยฉยยy'ยรย(x:ยยw_ย@y]ยก6u|2ย{#ยยy'|}H{ย\ย:ยขKยคyยร|ยก6w*ย@ยยต{#}Lยยw*|รzKy]|y'y.ยยย\}@ยค2{}:}@ยคy
vย@w*vhw*ย@}Lยยw*|&wยก,ร2ย*ยย|ยงยvhy'|ยง*uยย|2xKย(xKร'y'ย;wยOย(x:xQยยยยvยยย&|w*}Kvย@y.xLy'|}5ยยยก{ยยยWzKyXยถ|w#zรย(xK}@ยคy]ยยธ{Rยฎยยยยจuยยยน
y'|}@ย@w*vยยณv2wยร|}ยณรร ยยณยยณy&~'{#|รw*ยฉ}U{ยย|ร}@ยคyยx;{#ยยyรxLv2{*~Uyยฒยป ยฝยผ ยพรยฟ]ร>ร ย8{#|2ยร}@ยคu2x$}@ยคyรx@{#ยยyยธยยธ{Rยฎยยยยจuยยยน
y'|}@ย@w*vยยบvhw#ยย|}ย]ยกรย;w*ยรขรก_uยร};yรย_ยยรฃ,y'ย@y'|}ยตยถ|wRzrยรy.ยยง_yรยฉ2{*xLy.xยรรคQ|รv2{#ย@}Eยยฌ~Uuย({#ยยซ:~Uw*|2xQย(ยy'ย ยฟ]ร]รฅ z:ยคย(~;ยค
xQยยยยvยยยยบ{*x;xLy'ย@}Hx}@ยค2{#}ยจยรฆยยยรงยยย'ยย#ยย(ย%ย8รจยHยยย รฉรซรช ร ยชยOรยฒยยยยย#ยยยWย*ย%ย!ยHยยยยHยยธยขKยคย(x$|y'zรฌยถ|w#zKยยy.ยยง*yยตยฉ2{*xLy
}@yUยยย(x]u2xยฐ|w*}@ยคยย|ยง&z:ยค2{#}UxLwy'ยฑ_y'ยย{#ยฉhw*u}]}@ยคy$ยก6ยH{*~U}Lยยw*|รซw#ยกOร2ย*ยย|ยงรซv2y'|ยง*uยย|2x'ยซo{#|2ยยญยร|รซยก\{*~U}ยฐยร}Xย(xXy.{*xEยร}@w
xLยคw#zร}@ยค2{#}]ยOย ย ย@ย!ยยย*ย,ย!ยUย.ยยย'ย ยฟ]ร รฅ ย:รรญยชรฎBรฏย ร u}>w#ยกย~Uw_uยHxLy$ยย}:ย(xยฐยยยยv2wx@xQยยยฉยยyยธ}@w&ย_ย(xL}Lยย|ยง*uย(xLยคร}@ยคย(x
~'{*xLyรฐยกรย@w_ยยด}@ยคyยvย@y'ยฑ_ยยw*u2x$w*|y]รฑLu2xL}ยฉยรซยรww*ยถ_ยย|ยงรฒ{}&รรณร ยยรค\}]ยก6w#ยยยยw#z>x]}@ยค2{}]|wรย@y.xLuยย}$ยย|ยณ}@ยคy&xLvยยยLยย}w#ยก
ยขKยคy'w*ย;y'ยรดยฅ2ยยยฆ*ยฆ$ย\zยฐยคยยฌ~;ยคยรฑLu2xL}:u2xEy.xK}@ยคy]ยฑR{Rยรuyยจw#ยก]รร ยย~'{|&ยฉ2y~Uw_ยยvย@y'ยคy'|2xQยยยฑ*y*ย
ยขrยคy]yUยฎs{#ยยvยรyxEยคwRzXxK}@ยค2{#}K}@ยคy]vยคยยยยwxLw*vยคย&ยฉ2y'ยคยย|2ยยฒยขKยคy'w*ย;y'ยรดยฅ2ยยยฆ*ยฆX~'{#||w*}:ยฉhy]yUยฎ};y'|2ยy.ย&ยฑ*y'ย@ย
ยก\{#ย.ยซยรงยก,{#}W{ยยย\รต%ยย},ย(x%ยย|y'ยฑ*ยย}H{ยฉยรyยฐ}@ยค2{#}o}@ยคy'ย@y!zrยรงยยย2ยฉhyยvย;w*ยฉยยy'ยยธx,zยฐยคy'|รฐรร รถรรยร2ร ยOรรยชsย ร u},ยย}oย(x%|2{#}@uยH{ย}@w
{*xLยถ$z:ยคy'}@ยคy'ย5}@ยคy'ย@yKย(x!{$ย_ยยรฃรทy'ย;y'|}K{#vvย@w{*~;ยคย{ยย}@w*ยง*y'};ยคy'ยOยย|ยz:ยคย(~;ยคยธ}@ยคย(xย@y.xL}@ยLย(~U}Lยยw*|ยต~'{#|ยยฉ2yXย@yUย({ยฎy.ยย
ยขKยค2{#}ย(x'ยซ_ยยฌxรยย}ยvhwx@xQยยยฉยยy]}@wยจ~Uw*|2xL}@ย@u2~U}K{]}@y.~;ยค|ย(รก*uyยฐยก6w*ยย~Uw*ยยvu}Lยย|ยงรฐยy'ยง_ย@y'y.x5w#ยกรณยฉhyUยรงยยyUยกOยย|ยต}@ยคwxLy>~'{_xLy.x
z:ยคy'ย@yยร ร รถร รรญยชรธรรนยxยฐzKyยจยยy'|}Eยรw_|y.ย&ยย|รt*y.~U}Lยยw*|ยณยฅhยยยฆ*ยซzKyยจยยจยยยง*ยค}>ยคw*vhyยจ}@wยยw&};ยคยยฌxXยฉยร~Uw*ยยvu}Eยร|ยง
ยOย รบย ยผ ยLรป:ย ยฟ]ร ย]{*x]{ยตยก6u|2~U}Lยยw*|รw#ยกยธรผ ร {#|2ยยฒ}@ยคy'|ร}H{#ยถ_ยย|ยงร}@ยคyรฐยรงยยยยจยย}ยธ{_xรซรผ ร ยง*wy.x]};wรยชยยตรค\|ยณยง*y'|y'ยU{ย\ยซO}@ยคย(x
xLy'y'ยยธxรฐยฑ*y'ย@ยรยค2{#ยHยย ร u}.ยซOยย|}@y'ย@y.xL}Eยร|ยงยรย*ยซยฐ}@ยคyร~Uw*ยยvu}U{#}Lยยw*|2{ย:}@y.~;ยค|ย(รก*uyรซw#ยกย;รฝ>w#ย(ยxLร'ยยจย(ย}ยตy'}ย{ย\ยยยซ
ยฆ.รพ*รพ_ยชย>ยwy.x$u2xLyยต}@ยคย(x$}Lยvhyยธw#ยก:v2{#ยU{#ยยy'}@ยLย(~ย{|2{ยยยxQย(x'ยซ5ยy'ยยw*|2xL}@ยU{#}Lยย|ยงยฒ}@ยค2{#}};ยคยร|ยงxยรฐยรยง_ยค}$|w*}ยฉhy
xLwยยฉ2{_ยยตยก6w*ย:ยฑR{ยLยยw*u2x:ย;y.xL}@ยLย(~U}@y.ยรซ~'{*xLy.x'ยKรนX|w*}@ยคy'ย>xEw*uยH~Uywยก!ยคw*v2yยยฌxยฐ}@wยย;y'ยยy'ยยจยฉ2y'ยX}@ยค2{#}:ยยธ{Rยฎยยยยจuย
y'|}@ย@w*vย&ย(x'ยซ%ยก6w*ย]u2x'ยซ,ยยy'ย@yUยยยยณw*|yย}@ww#ยOยก6w*ย~Uw*ยยvu}Eยร|ยงยฒยH{|2ยw*ยยยน6zKw*ยLย(ยx]ยy'ยง*ย@y'y.xw#ยก:ยฉ2yUยยยยyUยกEยรซยขKยคy'ย@y
ยยธ{ยยยฉhyw*}@ยคy'ย:{#vvย;w{*~;ยคy.x!};ยค2{#}Kยฉยv2{*x@xKy'|}@ย@w_vยยy'|}Lยยย@yUยยย*ย5รค\|รv2{ย@}Lย(~Uuย({#ย.ยซ2xLw_ยยy>wยกW}@ยคy]}@ยคy'w*ย;y'ยยธx
zKy]ยง#ยยยฑ*yXยย|ยฒย ร {*~'~;ยคu2xKy'}:{ย\ยยยซ%ยฆ.รพ*รพ#ยฅยO~'{#|&ยฉ2yยจxLy'y'|ร{*xKยw#ยย|ยงยต}@ยคย(x'รฟ}@ยคy.xLy]};ยคy'w*ย@y'ยยธx5zKยยยยยรwยกร}@y'|&{#vvยยย
y'ยฑ*y'|ยญยรงยก!ร ร รถร รรยชsย
รนX|w*}@ยคy'ยK{*x@xLuยยv}Lยยw*|ยธยยต{*ยy:}@ยคย@w_uยง*ยคw*u}:t*y.~U}Eยรw_|ยธยฅ2ย Xย(x5}@ยค2{#}!};ยคy:ยถ|w#zKยยy.ยยง*yXยฉ2{*xLyXยค2{*x5{xLvhy'ยน
~;ย({ย2ยก6w*ย@ยรซยซ#|2{#ยยyUยยย Xย ย*ร ยฟ]ร:รฅ ยซ*zยฐยคy'ย@y ยยยฌxy.x@xLy'|}Lย({ยยยยยยvย@w*vhwxQยย}Lยยw*|2{ย,{#|2ย ยฟ]ร]รฅ ยwy.xO|w_}ย~Uw*|}H{ยย|
{#|ยรw~'~Uuย@ย@y'|2~Uy.xw#ยก #ยยจยขKยคyยยยw_ย@yยจยง*y'|y'ยH{ย!}@ยคy'w_ย@y'ยยดzKyรฐxE}H{#}@y$ย({#}@y'ย]ย@yUย({ยฎy.x]};ยคยยฌx$xLw*ยยy'zยฐยค2{#}.ยซ{*x
ยก6w#ยยยรw#z>xย
รนqยถ|w#zKยยy.ยยง*yยจยฉ2{*xEy ยฟ]ร ยยฌx]x;{ย(ยร};wยธยฉhy รฆย ยยยยOย6ย @ย ย .ย!ย 'ยย Uยยธรปรยยยก
ยย}ยจยค2{*x$}@ยคyยจยก6w*ย@ย รร ยฟ]ร]รฅ ยซoz:ยคy'ย;y รฌ~Uw*|}H{ยย|2x$|yUยย}@ยคy'ยยธรก_u2{#|}Lย 2y'ยHx$|w*ยยจvย@w*v2w_ย@}Lยยw*|2x'ยซO{#|2ย ยฟ]ร>รฅ
~Uw*|}H{ยย|2xK|w*|y$w#ยกO}@ยคy~Uw*|2xL}H{|}:xLยยยจยฉ2w#ย(xX{#vv2y.{#ยEยร|ยงยยย|รรปรw*ย5ยย| ]ย
รค\}KxLยคw*uย(ยยยฉ2y]~;ยรy.{ย!}@ยค2{#}ยยยก%{$รก*uy'ย;ยรฐรป5ย ยยยฌx5xQยยยยvยยyXยกรw_ย ยฟ]ร ย8{*xO{*x@xEuยยy.ย$ยย|ยธvย;y'ยฑ*ยยw*u2xKxLuยฉ2xLy.~U}Eยรw_|ยHยซ
}@ยคy'|รซ}@ยคyxLy'v2{#ยH{ยฉยรงยยยย}Lยร~Uw*|2ย_ยย}Lยยw*|รซย(x:x@{#}Lย(x 2y.ยย
รน]xO};ยคyKยกรwยรงยยw#zKยย|ยงยจyUยฎs{ยยvยยyยxLยคw#z>xยซ#ยยยก,zยyXยw|w*}K{*x@xLuยยyXxLy'v2{#ยH{#ยฉยยยยยย}Lย*ยซzKy>~'{#|ยy.{*x8ยรงยยยยย;u|ยยย|}@w
|w*|ย@w_ยฉu2xL}:ยฉ2y'ยค2{ยฑ_ยยw*ย.รต
w_|2xQย(ยy'ยO}@ยคyOยก6w#ยยยยwRzrยร|ยง]ยถ|w#zKยยy.ยยง*yKยฉ2{*xLy ยฟ]ร w#ยฑ*y'ย,}@ยคyKยฑ*w~'{#ยฉuย({#ย@ย รร
รต
ยรฆยยย $ย8รจรทยHยยย รฉ$รช ร ยชรฎ ยฐร $ย ย@ย รซยรฆยยย $ย8รจรทยHยยย รฉ]รช ร ยชsรฎ ยฐรรร $ย ย@ยUรฎ










! #"%$'&($)


	
4

5

+*,& -."

0/

132

-&

76

8

:9

;

.6

KJ

<>=?A@CBD	EFGIH

:N

UT

5N



AV

:N

UT

N

MLONIPQSR



WYX[Z]\U^(_(`+\Ua0bdc+eUfSg^ihQjS`_lkk`h-_lgnmoeUcpc^q_lf-r+eUg0b'eUr_;\Uch>cj-sg;^cte:uvrm^xw-fhlyA\U^;z{0^|'_(c^eUc^;}-j-eU~Q_#\U^;f-rxrhIhQfS^xh(uvrmYeUc
uhQ`qX

ยย

ยยยยยยย[ยยยยยยยยยยยSย[ยยยยยยยย5ยยยยยยยยยยยยยยยยย%ยยย'ย

ยoยยยยยกยยยขdยฃ8ย;ยค-ยฅvยฆ'ยง0ยฆ'ยจGยฉยคยซยชIยยฃ(ยฌยญยง(ยคย#ยฅvยคยฎ0ยฃ8ยฃlยขยยฃ(ยฌยยคยซยฏยฐยยค-ยง(ยฑMยฒยดยณยตSยถQยทยนยธIยฌยยคยซย;ยฅvยฆdยฎ0ยคยปยบยยฝ'
ยผ ยพ ยoยรยฟรยฎ0ยขdยvยยย#ยฃQย!ยข'รยฆ
ยฐยยGยรยฏdยฐยยค!ยฅ]ยข'ยยยฃ
ยณรยรUรยรQรยร7รdยถQร]ยชยฌGยรยฎ(ยฌรยรยKยฆSยฉรย;ยข,ยฃ(ยฌยยคร5ยฆSรGยรKยฐยรรยค-ยยฃlยง(ยขdยฅGยฑEยฅ]ยข'ยยยฃSยท8รiยขdยฃ(ยฌยรยยรรยฐยยvยฎ0ยฃQยยดยข'รoยoย
ยฆ'ยง(ยครยฎ0ยขdยvยยย#ยฃ(ยค-ยยฃ!ยชยรยฃlยฌรยฃlยฌยยคEร
ยฆSรยรKยฐยรรยค-ยGยฃ(ยง(ยขdยฅGยฑยยฅvยขรยรยGยฃรoย;ยขยยชIยคEรรยรรยฌยฃ
ยค0ร%ยฅ]ยคยฎ0ยฃ8ยฃ(ยฌvยฆรยฃ5ยฃ(ยฌยยค,ยฅยยง(ยคย#ยค-ยvยฎ0ยค
ยข'รรยฃ(ยฌยยค,ยฎ0ยขdย'ร;ยฐยยvยฎ0ยฃQย5ยฒยดยณ;ยตSยถรยฆ'ยvรรรยฒยดยณยตSยถยดยรยยญยฃ(ยฌยยค,รยรย+รยฐยยvยฎ0ยฃ0ย
ยชยขยฐGยฉรยปยยยขdยฃ!ยฆSรยคยฎ0ยฃยฃ(ยฌยยคEรGยค-รยง(ยค-ยค,ยข'รรยจ]ยค0ยฉ7ยรยค0ร(ยท
ยธIยฌvยฆ'ยฃยกยรย-รย7รKยรยฃ,ยชIยค-ยง(ยค,ยฅvยขGย(ย.ยยจGยฉยคยยฃ(ยขยยรdยยยขdยงlยครยขdยงEรยรย(ยฎ0ยขยฐยยยฃ3ยฃ(ยฌยยคยซยงlยข'ยฉยครยข'รยฃ(ยฌยยครยฃ(ยขรยฉรยค-ยง0ยฆ'ยvยฎ0ยคย-รoยชIยครยชIยขdยฐGยฉรร
ยค0รยยฅvยคยฎ0ยฃ8รxยง0รยณ;ยฒยดยณยตSยถรรยoยยดยถ!รรกรยรรยยทMรขยข'ยชIยค-รฃdยค-ยงรIยฃlยฌGยยรยรย
ยยยขdยฃ
ยฃ(ยฌยยครยฎ-ยฆdย;ยคdยทยรคiยขdยvย.ยรรGยค-ยง!ยฃ(ยฌยยค8ยจ]ยค-ยฌvยฆSรฃdยยขdยง!ยข'ร
รxยง รฅ ร ยผ

ร รฆ ยทรชรฉรซร
ย

ยณ;ยฒยดยณยตSยถรรยoยยดยถรรยขdยงรงยน
รฆ
รจ

ยณรยรรoรฐ

รงรฌ รQรยร7รรรฑ

รงรฌ ยถQยท
รฒยข'ยชรรxยฎ0ยขdยvยยรGยค-ยงย;ยขรยค5รณ

ยณยฒยดยณยตSยถรรยoยยดยถoรรชรxยง รฅ ร ยผ

ร8ย;ยฐGรดรตยฎlยยค-ยGยฃ;ยฉยฑรถย#ร5ยฆSยฉ7ยฉย;ยข3ยฃ(ยฌvยฆ'ยฃ
รจ



รคยยขdยงlยข'ยฉ7ยฉยฆรยง(ยฑEรยยทรน-รบvรAยชIยครGยครGยฐvยฎ0ยค5ยฃ(ยฌvยฆรยฃรรxยง รฅ ร ยผ
ยขdยง(ยค-ร4รยยทรนรพยรรรฟยง รฅ ร ยผ

ยณ(ยณรรร:ยฒยดยณรปAยถ(รร รผ

รยรรรฐ
รจ

รยณรรร:ยฒยดยณ;รปAยถ#รร รผ

ยณยฒยดยณยตSยถร0ยoย

ยณ;ยฒยดยณยตSยถรรยoยoยถoรรชรxยง รฅ ร ยผ



ยฃ(ยฌยยคยขdยฃ(ยฌยยค-ยงiยฌvยฆ'ยvรAรGย7ร
ย7ร

ร

รจ



รยรรoรฐ

รงรฌ

รงรฎ

Eยณ-รร:ยฒยดยณรปtยถlรร รผ











รยคยยยข'ยชรชยฃ(ยฐยยง(ยรยขdยฐยยง
ยฃ(ยข5ยฏยฐยยค-ยง;ยยคย>ยข'รยฃ(ยฌยยค

รฌ

ร รฆ รGยชIยคยฎ-ยฆรย5ร5ยฆ
G

รงรฌ รQรยร7รรฑ
รยรUรรรฐ

รงรฌ

รxยง รฅ ร ยผ

pยยงQย;ยฃoย;ยฐยยฅยยฅ]ยขย;ยค

,+

รท

รงรฎ

รงรฌ ยท

/



รร:ยฒยดยณรปtยถQรร รผ

ยยยข'ยชIยฉยครGรdยครธยจvยฆdย;ยค
ยoยรฝร

ย;ยฃ0ยฆ'ยฃ;ยรย;ยฃ;ยรยฎ-ยฆSยฉยยGรยขdยง(ร5ยฆรยฃ;ยยขdยยท

)

รยรUร
ยฆ'ยvร,ยฃ(ยฌยยคยฏยฐยยค-ยง(ยฑ
รฌ

-

รยยทรนรพยรvรรฟยง รฅ ร ยผ

. 


ยณ รยoยยดยถยรยรxยง รฅ ร ยผ

-

ยณ รยoย

รงรฎ ยถ(ยถiยณยฆdย(ย;ยฐยรรยรยยรยดยฃ(ยฌยยคIยฉ7ยรKยยฃยค0รGยย#ยฃQย0ยถQยทxยธIยฌยยคIยฉรยฆ'ยฃlยฃ(ยค-ยงยยค0รยยฅยยง(ยคย(ย.ยยขdยรธยรยIยฎlยฉรยคยฆรยง;ยฉยฑ5รยยท

.

รท

รงรฎ รยฃ(ยฌยยค-ยยoย

ยพ รฆ ยฟรร ร
รง

ยณ รยoยรยถรGยค-ยฅ]ยค-ยvรยยยขย!ยฌยยขOยช

,

ยธยฌยยคoยง(ยคยฆSยฉยฅยยง(ยขยจGยฉรยค-ร
ยฐยยฃ;ย7ยฉ7ยยฃยฑKยข'รvยฃlยฌยยค

ยพ รฆ ยฟnรxย;ยข3ยฃ(ยฌvยฆ'ยฃ
รง
รงรฆ

ยฌยยค-ยงlยคoยรยยฃlยฌยยค

รรฟยง รฅ ร ยผ

ยณ ร ยoยoยถ

ร

.

ยณlยณรรร:ยฒยดยณรปtยถ#รร รผ
รจ

3ยณรรร:ยฒยดยณรปtยถQรร รผ
รจ

ย5ยฃlยฌยยคยขdยฃ(ยฌยยค-ยงiยฌvยฆ'ยvรAร

รนdยท,ยธIยฌGยฐvย-รยฃ(ยฌยยคยฉ7ยรKยยฃ;ยยยรยยจvยค-ยฌvยฆSรฃdยยขdยงKยข'ร

รฆ รยย;ยขยฃlยฌvยฆ'ยฃรรxยง(ร
ยณ รยoยoยถxยรยยยยขยยยง(ยขdยจยยฐvย;ยฃยท
รย

รdยขยคยยฃ(ยข

0

0

ย;ยค-ร5ยฆรยยฃ;ยรยฎ-ย>ยข'รxยฅยยง(ยขdยฅvยขยง(ยฃ;ยยขdย8ยค0รยยฅยยง(ยคย(ยยรยขยvยIยยรยฏdยฐยยค-ยง#ยรยคยYยท

2143657198;:=<4>?365@<
4C
D
E
-H I
KJLJM

ยฎ0ยขdยยยยยคยฎ0ยฃ#ยรรฃยคยยยยค0รยยฅยยง(ยคย(ยยรยยรรย#ยฃQยฆ'ยฃ;ยรย;ยฃ;ยรยฎ-ยฆSยฉGยยGรรยขยง(ร5ยฆ'ยฃ;ยยขdยยดยยKยฃ(ยฌยยค

ยจvยคiรnยฆSยยง;ยฉยฑoยฐยยvยฎ0ยขdยGยฃ(ยง(ยข'รฃdยค-ยงQย.ยรยฆSยฉnรSยยฃQยpยง(ยข'ยฉยคiยย

ยญร

รฅ ยผ ยพ ยoยรยฟ'ยรยiยณรยรUรยรฐ

ยฐยรรธรยค-ยยฃ(ยงlยขdยฅยฑยฅvยขรยรยGยฃยข'รยบ

รคiยขdยง(ยข'ยฉ7ยฉรยฆ'ยง(ยฑ8รยยทรน-รบvรGยยฃรยข'ยฉ7ยฉรยข'ยชรย>ยฃ(ยฌvยฆ'ยฃoรxยง รฅ ร ยผ

ยยง(ยขdร

รง รฎ ยถtร0ยoยoยถIรยรนdยทยธIยฌยยค-ยง(ยค0รยขdยง(ยคdร]ยจGยฑ8ยธIยฌยยค-ยขdยง(ยค-ร

รยรUรรฟรฐ
ย7ร

รฎ

ยฉยฃ(ยฌยยขยฐยรdยฌ8ยชIยค>ยรยGยฃ(ยค-ยvร3ยฃ(ยข

ยฏยฐยยค-ยง;ยยคยยฃlยฌvยฆ'ยฃIยยรฃยข'ยฉรฃdยค

รยรUรยยทAรฉnยฃpยยpยคยฆdย;ยฑoยฃlยขรย;ยค-ยคยยฃlยฌvยฆ'ยฃยฃ(ยฌยยคiยฐยยGยยฏยฐยยคIร5ยฆSรGยร

รงรฌ ยถQยท




iยณรปtยถยรย>ยคย(ย;ยค-ยGยฃ;ยรยฆSยฉ7ยฉยฑ8ยฅยยง(ยขdยฅ]ยขย.ยยฃ;ยยขdยvยฆSยฉnยท

(


รคยยขdยvยยรGยค-ยงยดยฃ(ยฌยยค

รร:ยฒยดยณ;รปAยถQรร รผ



dยคยฎ0ยฃ#ยรยขยยซรบvยท ยรยชIยคยง(ยคย#ยฃ(ยง;ยรยฎ0ยฃ(ยคร

ยธIยฌยยครยข'ยฉ7ยฉยขOยชยรยยร
ยค0รvยฆ'รยฅGยฉยคoย7ยฉ7ยฉยฐvย;ยฃ(ยงQยฆ'ยฃlยคยยฃ(ยฌยยครรย7รดรตยฎ0ยฐGยฉยฃ;ยยคย-ยท

!"$#&%'
*
,+

dยคยฃ(ยฌยยครยฆย;ยฑรรธยฅยยฃ(ยขdยฃ;ยรยฎIรฃ'ยฆSยฉยฐยยค





iยณ;ยตSยถQรGยชยฌยยค-ยง(ยค

ยคยฆdย;ยคยดยฃ(ยฌGยรยยงlยคย;ยฃ(ยง;ยรยฎ0ยฃ;ยยขdยรยยชIยคKรGยขยยยขdยฃIยยGยฃ(ยค-ยvร3ยฃ(ยข5ยฆSยฉ7ยฉยข'ยช

ยGยฉยฑ

รยฆSยยรยรxยง(ร
ยณยฒยดยณ;ยตSยถรรยoยoยถยรยiยยยขdยฃiยง(ยขdยจยยฐvย;ยฃยท

ยฆ'ยฃlยฃ(ยค-ยยฃ#ยรยขยรยฃ(ยข!ยง(ยคย;ยฃlยง;ยรยฎ0ยฃ;ยยขdยvยยดยขdยรยฃ(ยฌยยค5ยฏยฐยยค-ยง(ยฑdยท!รฉ.ย

รรยขdยงlร

ยณยฒยดยณยตSยถรร ยoยoยถยรรรยยท

ย

ยณยฒยดยณยตSยถรร ยoยoยถQยทiรคxยฉยคยฆ'ยง;ยฉยฑdรvยจGยฑ5ยฆ'ยฅยยฅยยงlยขdยฅยยง;ยรยฆ'ยฃ(ยค0ยฉยฑ

ยฎlยฌยยขยขยยรยยรยฆยดย;ยคยฏdยฐยยค-ยvยฎ0ยคยข'รยฃ(ยข'ยฉยค-ยงQยฆรยvยฎ0ยครฃdยคยฎ0ยฃ(ยขdยง0ยยยฎ0ยขdยGรฃdยค-ยง(ร'ยยยรยดยฃ(ยข
ยข'รยฃ(ยฌGยรยรฟรรยง0ยฆdยฎ0ยฃ;ยยขdย5ยฆ'ยGยฑยข'รรยรGรยรUรยร'ยขยงรรนdรGยขdยงiยยยขdยฃยยค0รGยรย;ยฃยฆ'ยฃยฆSยฉ7ยฉnยท

ย;ยคยฎ0ยขยvรEรยรย+ร;ยฐยยvยฎ0ยฃรAยชIยค

รยรUรGยถ(ยถรรฝรนdรยฆ'ยvร,ยยยขdยฃรรยรUรยยท

	

รรยขยงรรฟยง รฅ ร ยผ

รGยขยชIยครdยค-ยฃIยฃ(ยฌยยคoยค0ร%ยฅ]ยคยฎ0ยฃ(ยคร!รฃ'ยฆSยฉยฐยยครยข'รxรยรUร

รงรฌ ยท!รยยฑ

รง รฎ ยถ(ยถoยณยฆย(ย;ยฐยรKยยยร8ยฃlยฌยยคKยฉ7ยรรรยรยฃ

รฅ
รงรฎ รdยชIยครdยค-ยฃยยฃlยฌยยครย;ยฑGรรยค-ยฃlยง;ยรยฎยจ]ยค-ยฌvยฆรฃยยขdยงรยยช>ยฌยยค-ยง(ยครรรฟยง ร ยผ
รท

รงรฌ

ยณ;ยฒยดยณยตSยถtรlยฒยดยณยตSยถ

รฐรรณรธรท

รงรฎ

รง รฎ ยถtร0ยoยoยถรรฝรนdยทยธIยฌยยค-ยง(ยค0รยขdยง(ยคdรAยจGยฑรยธIยฌยยค-ร


ยค0รGยย#ยฃQย0ยถQยทรยยฐยยฃoย.ยยvยฎ0ยคKยฃ(ยฌยยคKยยยค-ยชIยฉยฑEยฆdรยรGยคร3ยค0รยยฅยยง(ยคย(ย.ยยขdย3ยรยยรยvยฎ0ยขยvย.ยรย;ยฃ(ยค-ยGยฃรยชIยยฃ(ยฌ3ยฃ(ยฌยยค
ยขdยจยยฃQยฆOยรยรยฃ(ยฌvยฆ'ยฃรรxยง รฅ ร ยผ

รฅ ยผ ยพ ยoยoยฟ>ยย

รงรฌรจรญรงรฎ รยฃ(ยฌยยค-ยรฏยฃ(ยฌยยค,ร5ยฆSรGยรรKยฐยรรยค-ยGยฃ(ยง(ยขยฅยฑยฅvยข'ยยGยฃ8ยข'รยบ

xยชยคiรKยรdยฌGยฃ

รGยงQยฆSยชรร'ย;ยฐvยฎlยฌ

 BA

รยฌGย7ยฉยคKยฃ(ยฌยยค

;

ยยยข'ยชIยฉยครGรdยคIยจvยฆdย#ยคย#ยฌยยขdยฐGยฉรร

ยฆdย


ยย

รvยฆ'รยฅGยฉยคiรบvยท dรยรรยย

รKยฐvยฎ(ยฌรยฉรยคยlยยยฎlยฉยคยฆ'ยงยทxยธIยฌยยคIรยขdยง(ร
ยฆSยฉยย#ยค-ร5ยฆ'ยGยฃ;ยรยฎ-ยxยชIยคIยฌvยฆรฃยครรGยค vยยยครยงlยคยฏdยฐGยยง(ยคยรฟยฃ(ยฌvยฆ'ยฃxยชIยคยฎ0ยขdยvย.ยรรGยค-ยงยฆSยฉ7ยฉvยฅvยขย(ยยรยจGยฉยค

(

ยฃ(ยข'ยฉยค-ยงQยฆรยvยฎ0ยคยxรยขdยงยฆoยฅยยงlยขdยฅvยขdยงlยฃ;ยยขdย5ยค0รยยฅยยง(ยคย(ยยรยขย
ยรย
ยฐvย;ยฐvยฆSยฉยยงlยคย;ยฐGยฉยฃยท

Iรยย;ยข

ยยฃIยรยยยยยขยฃย#ยฐยยง(ยฅยยง;ยรย.ยยยรยฃ(ยฌvยฆ'ยฃiยยยขdยยยง(ยขdยจยยฐvย#ยฃ(ยยยคย(ยรฟยรยIยฃ(ยฌยยค

GF

ยยยคKรKยรdยฌGยฃรยฆ'ยง(รdยฐยยคยดยฃ(ยฌvยฆ'ยฃยฃ(ยฌยยคKยฃ(ยข'ยฉยค-ยงQยฆ'ยvยฎ0ยคย>ยยยซยฏยฐยยค-ยง;ยยคยรย;ยฌยยขยฐGยฉรCยจvยคยฆSยฉ7ยฉยขOยชIยคร3ยฃ(ยข!รGยค-ยฅvยค-ยvร

รยขdยงlยครยฎlยฉรยขGย;ยค0ยฉยฑ8ยขdย!ยฃ(ยขรยฉรยค-ยง0ยฆ'ยvยฎ0ยคยยขรร

ยค0รยยฅยยง(ยคย(ย.ยยขdยvยยย,ยฃ(ยฌยยค

ยฃ(ยฌGยรย>ยยยฃlยฐGยรยฃ#ยรยขยรยฆdยยรยoรGยขdยยยคยดยรยยปยณ ยข'ยฉ7ยฉยค-ยง

ยยยข'ยชIยฉยครGรdยคยดยจvยฆdย;ยคdยทiรฉnยฃIยรย>ยฅvยขยlย.ยยจGยฉรยคKยฃ(ยขรรรยขdยงlร5ยฆSยฉ7ย -ยค

รGยคยฆSยฉ7ยยยร,ยชIยยฃ(ยฌรยฅยยง(ยขdยฅ]ยขdยง(ยฃ;ยยขdย,ยค0รยยฅยยง(ยคย(ย.ยยขdยvย

รขยฆSยฉยฅvยค-ยงlยรยรน

GยถQรยยฃ(ยข
ร'ยรฃdยคยฆ'ย,ยฆSยฉยฃ(ยค-ยง(ยvยฆ'ยฃ;ยรฃdยคย;ยค-ร
ยฆ'ยยฃ#ยยฎ-ย>รยขdยง

OC

รยข%ยฎ0ยฐvยiรยขdยงยฃlยฌยยคoยง(ยคย;ยฃยข'รxยฃ(ยฌยยครย;ยคยฎ0ยฃ#ยรยขย8ยขdย

LN

ยฃ(ยขGยข!รnยฆ'ยงยฆ vยค0ยฉรรรยฌยยค-ยง(ยค

BC

*

vยง0ย;ยฃ(รยขdยงQรGยค-ยงoยฏยฐยยค-ยง;ยยคย-รvยค-รฃยค-ยEยฃlยฌยยขย;ยครยฃlยฌvยฆ'ยฃIยยรฃยข'ยฉรฃdยคKยฅยยง(ยครยร

ยฎ-ยฆ'ยฃ(ยคยยข'รรฟยฆรยง(ยจGยยฃ(ยงQยฆ'ยง(ยฑ!ยฆ'ยง;ยยฃ;ยฑ!ยฆ'ยvร!ยคยฏยฐvยฆSยฉ7ยยฃยฑรยณยฆSยฉยฃ(ยฌยยขdยฐยรยฌ!ยชยครย#ยฃ;ย7ยฉ ยฉxยยยค-ยคร3ยฃ(ยขยง(ยคย#ยฃ(ยง;ยรยฎ0ยฃยฃ(ยฌยยค
ยฃ(ยข
ยฃ(ยฌยยคKยฐยยvยฆ'ยง(ยฑ
ยฉรยฆ'ยยรdยฐvยฆรรdยค

รฌ

Aยค-ยฃ

ร

รปยฒยดยณรปAยถQยทรฉ.ยฃxยรยiยคยฆdย;ยฑKยฃ(ยข

SC

ยยยข'ยชIยฉยครGรdยคยดยจvยฆdย;ยค

ยถQยทoรข>ยขOยชIยค-รฃdยค-ยงSรvยฆdย>ยฃ(ยฌยยคยดรรยข'ยฉ7ยฉยข'ยชIยยยร!ยค0รยยฆรรยฅGยฉยค5ย;ยฌยยข'ยชรย-รAยฏdยฐvยฆรยยฃ;ย vยค-ยงQยยฃlยขยข!ยฎ-ยฆ'ย

ยฎ-ยฆ'ยฐvย;ยคoยฅยยงlยขdยจGยฉยค-ร5ย-ยท

รร

iยงQยฆ'ยฃlยฌยยค-ยงรยชIยค

vยงQย#ยฃ(รยขdยงQรGยค-ยงรยฏยฐยยค-ยง;ยยคย-ยท

รฉ.ย!รnยฆdยฎ0ยฃร]ยขdยฐยยงรdยขGยฆSยฉยรยยฃ(ยข5ยฆSยฉ7ยฉยข'ยชรชยฆ'ยง(ยจGยยฃ(ยงQยฆรยง(ยฑ

BPRQ
!"$#7T=',U WV YX 9Z
 Y]

9C

ยยยซยฏยฐยยค-ยง;ยยคยยดยฃ(ยฌvยฆ'ยฃoยข'รยฃ(ยค-ย,ร'ยรฃdยคยoรรธยขdยง(ยคKยง(ยคยฆdย;ยขยvยฆ'ยจGยฉยคยจvยค-ยฌvยฆSรฃยรยขยงยท

รคยยขdยvยยรGยค-ยง0ยฆ'ยฃ;ยยขdยvยยดยข'รยฃ(ยฌGยรยยฆSยฉยฃ(ยค-ยง(ยvยฆรยฃ;ยรฃdยค5ย;ยค-ร5ยฆรยยฃ;ยรยฎ-ยยดยชIยขdยฐGยฉรร,ยฉยคยฆdรยยฐvย

OยฒIรQยต oยฆ'ยvร
ยฎ0ยขdยvย.ยรรGยค-ยงoยoย
ย#ยค-ยคยฃ(ยฌvยฆ'ยฃIยบ

ยฝ ยผ ยพ ยoย

รฌ ยฟAรMยบ

_K`

\[
ร
รฌ

ยฝ ยผ ยพ ยoย

YX

^Z

[รปoรยฒยดยณ;รปAยถQร]ยoย

รฎ ยฟAร

รฎ

ร

*

รร:ยฒยดยณรปtยถQรร รผ

รฌ

รยรยยฆ'ยvร

dยณ;รยรรน'ยถ รOยฆรยvรKยฃ(ยฌยยค-ยง(ยค0รยขdยง(ยคยฃlยฌยยคยฐยยGยรยฏdยฐยยค

acb&d=egf@hi0jgkKlLfLbgmhnpoqdrkKk9fLb
s2t9u=vGs(wgsBxzy|{~}^ยยLย=ยยยย6vG{=}cvG{ยยยยL}^ยvยยOยย ยยย-ยgยKย	ยยยยยย6ย0y|ยLy|Kยgยยยยย-ยcยยEย,ยก9ยvยWยข^ยฃGyKt6.ยฃGย!ยrยยยcยgy|^yKtLย
ย4ยBย.ยcยยEยยฅยคยฆยvยย,tMยข4}ยwยt9ยฃSยฃGย)ยLยRยงยจย2ย.y|y}^ยgyWยฃt	}ย}ยy|0ยฉยชtLยข4}Kย=ยLยยย.y|^ยLyE}ยยยt6}c}^ยgyEยยซtLย.}s2t6ยฌ.ยL.vG}.ยBย6ยฉsBยgยญ=y4ยฃย
ย6ยฉ0ยEยยคEt6ยยLwg{ยยญยฎยย tLยข4}ยwยt9ยฃSยฃGย!ย^t6}.vยยยฏยฉzยDยฐ~ยฑWยฒ*ย-ยฑย4ยRยง0ยgy|ยyยณvยWtLยข4}ยwยtยซยฃยดยฃGยยL{=ยฃGยDtOยยฏvG{gยต6ยฃGyย0ยL.ยฃยยญยถtLยยย.ยgยข^vยt6}ยyKยญ
ย0vG}ยยยทย-ยgย9ย6ยยt6}Wยcย=vยยข^ย\ยฐ~ยฑ,ยฒ*ย.ยฑยvยยฉยชt9ยฃยย.yLยยงqย=vยยณy4urt	sBย=ยฃGyvยยยณยy4ยฃยt6}ยyKยญยถ}ยยยธuยt6sBย=ยฃGy(ยนยยGย6ยบ~ย@ยยyKยข|t	wยย.y(vG}
vSยฃSยฃยปwยยยผ}ยยt6}ยyKยWt6{gยL}^ยgy|,ยข|tLย.yWvG{Dยย=vยข^ย)ยฝยฟLยพ ร ยEย,รยข|t6{g{gยL},ย.w=รยข4yE}ยย2ยญ=y|}ยy|ยsรvยป{gy(ยญ=y|ยตLยy|yKย0ย	ยฉรย@y4ยฃยดvGy4ยฉยย
ร {ร}ยยgyรยข|tLย.yยย6ยฉ}^ยgy)ร~{gย6ย0ยฃGyKยญ=ยตLyยยยtLยยผyรยEยยก6ย}ยยgyยs2t9u=vยปs(wgsBxzy|{=}ยยยMย~ยยยย6vG{=})ย-ยgย9ย6ยEvยยรMw=vยป}^y
s(vยยยฏยฃGyKtLยญMvG{gยต!t	ยยยLwg}W}ยยgy({ยt6}ยwgยy*ย6ยฉR{gyKt6ยย=ยย0ยL.ยฃยยญgย|ยWร)y*sรwยยยผ}ยฅt9ยLย6vยยญร}ยย=vยยEย.ยLย}cย	ยฉยฅร|ยญMvยยยยข4ยL{=}.vG{~w=vG}.ย&ร
ยcยgy|{ยฎรย{ยยญMvยป{gยต\}^ยgyDยญ=y|ยตLยy|yDย	ยฉยฅย@y4ยฃยดvGy4ยฉBย6ยฉtยถยฉzยLยs(w=ยฃยtย}ยยยt6}(vG{=ยLย6ยฃGยLyKยรรยยย.}ยxzยMยยญ=y|2รMwยt6{~}ยผvยดรยy|4ย|ยYยง0ยgy
{gยL}.vGยL{Oย6ยฉรย.}ยt	ย=vยดยฃSvG}.ย2ยญ=y4รย{gyKยญOยยy4ยฃGย6ยvยยvG{=}ยy|{ยยญ=yKยญ}ยยรยญ=yKt9ยฃย0vG}ยยO}ยย=vยยRยg^ยLย=ยฃGy|sDยRยงยจยรยญ=y4รย{gyWvG}KยLย0y0รย4ย.}
{gy|yKยญ}^ยgyWยฉ?ย6ยฃSยฃGย6ย0vG{gยต2{gยM}.vGยL{Dย6ยฉtOรยรGร9รรรMร|ร|ร|ร4ร;ร7ร$รzร6รร
รOรMรยรรกGรขKรกรฃรรครรฅ$รฆยรฆRรงรรจ ร4รยปร9รรLรยฆรรฉรยฆรยรร7ร$รzร6รรซรช.ร6รฌยซรยฆรBรญEรฎOvยยBtยยข4ยL{6ยฌ-wg{ยยข4}ยผvยปยM{ย6ยฉยฅรฏรฐยฉzยLยs(w=ยฃยtLย|รฑรยฉ?ยL(yKtLยข^ย
t6}ยยMsรณรฒ0รดย6ยLy|รญ2ย=vG}vยป{ยยข^ยฃGwยยญ=yKย*y4uยtLยข4}.ยฃGยยL{gy(ย6ยฉ0ยฐ~ยฑ,รฒqรดKย-ยฑรตยqt6{ยยญรถยฐ~ยฑ,รฒqรด6ย-ยฑยยยcรทrยLWรธ)ย รน)รบ(รป(ย@}ยยgyBร4รยปร9ร
รLรยฆรรฉรยฆรยรร7ร$รzร6รยรผ	รยร|รKรยฆร?รผLรzร4รWรฝรzร$รพEรธรย ย6ย.vG}ย}ยy|{Bรฟย.รธย ยยย9vยย}ยยยt	}รยยฏv |ycยญ=yKยยยข4.vGยg}.vGยL{ยcย=vยยขยย(vG{ยยข^ยฃGwยยญ=yKย0รถยฐ~ยฑWรฒ ยย-ยฑย
vSยฉRรธ ย ยBt6{ยยญยฐ~ยฑ,รฒ -ย.ยฑยvSยฉRรธ \ยgย
ยงqยgyรยg^ยLย=ยฃGy|s2ยE}ยยยt	},ย0y(ยct6{~}W}ยยDt9ยLย6vยยญยถยgยข|ยข4wgEยcยgy|{)}ยยgy|ยy*vยยtOsOt9u~vGs(wgsBxzy|{=}ยยยLย=ยDย@ย6vG{~}ยถย ย
ย0vG}ยย)ยยฏv |y2ยญ=yKยยยข4ยผvยปยg}ยผvยปยM{รรฟยKยgย ยcย.wยยข^ยย}ยยยt6}vยป{ยทtB{gy4vGยตLย=ยยร ยL^ยgย~ยgยญยย6ยฉ(ยยย ยsBย~ย.}Wย6ยฉR}ยยgy(ย0ยL.ยฃยยญgยEยยt	}.vยยยฏยฉ?ยMvG{gยต
ยEยYt6ยyctLยยย.ยgยข^vยt6}ยyKยญ*ย0vG}ยย(ยL}ยยgy|Rย-v |yWยญ=yKยยยข4.vGยg}.vGยL{ยย|ย {~}^w=vยป}ยผvยปยMy4ยฃยปยLยr}ยยgy0ยgยยLย=ยฃGy|s ย0vG}ยย(}ยย=vยยรvย}^ยยt6}}ยยgy
ยข4ย=ยLยยญMvG{ยt6}ยyKยย6ยฉ0ยรย t9ยฃGยL{gy0ยต6vGยLy0wยย s(vยยยฏยฃGyKtLยญMvG{gยตEvG{=ยฉ?ยMยs2t6}.vGยL{Ot6ยยยLwg}}ยยgyc{ยt	}ยwgยyRย	ยฉ7ย0ยL.ยฃยยญgย{gyKt6Eยย ยLt6{ยยญ
ย.ยt6ยยยMwg}ยฅยญ=y|ยตL^y|yKยcย6ยฉRย@y4ยฃSvยปy4ยฉยย ยทร)yยต	vยปยMyรtย.w=รยยข^vGy|{~}ยข4ยM{ยยญMvยป}ยผvยปยM{)ยcย=vยยขยย)ยข|t6{ยถยยy(wยย.yKยญร}ยยยt9ยLย	vยญร}ยย=vยย
ยgยยLย=ยฃGy|s vG{D}ยยgyยข4ยL{=}ยy4ug}cย6ยฉยLwgc}ยยgy|ยMยy|s2ย|ยยง0ย=vยยWยข4ยL{ยยญMvG}.vGยL{vยยy รตyKยข4}ยผvยปยMyรt6{ยยญOwยย.yKยs2tLยข^ย=vG{gy|ยยยถยzvG{
ยยt6ย}ยผvยข4w=ยฃยt69ย}ยยgyรt6ย=vSยฃSvG}.ยD}ยย(รย{ยยญยย.ย	ยฃยปwg}ยผvยปยM{ยยผยยtLยข4yKย4ย0}ยยยt6}0vยยW{gy|yKยญ=yKยญยถ}ยยOwยย.y}ยยgy*s2t9u=vGsรwgsxzy|{~}ย^ยLย~ย
t6ยgยgยย=tLยขยยOvG{ยt6{~ยยข|tLย.yLย
รOรMรยรรกGรขKรกรฃรรครรฅ$รฆ รง y|}ยย ย ยยyts2t9u=vGs(wgsBxzy|{~}^ยยLย=ย)ยยย6vG{=}(ย6ยฉEยฝ ยพ ร ยEย,รยชยร yยยยt9ยย}ยยยt6}ยถยยถย vยร|รผ |ร
ยยชย0vG}ยย^yKย.ยยyKยข4}W}ยยยEย t6{ยยญย ยvSยฉยBย vยย{gยL},ยข4ยL{=}ยt9vG{gyKยญOvG{ ยฝ ยพ ร ยEย Dรถรร รฟรยKยgย ยยชรzยร)yยยt9ย2}^ยยt6}EยEยรณรผ6รร
)ย รผ6รยรEร|รzรผ ยดร รฉร	รqรฟ qvSยฉยฉzยL0y|ยLy|ยยBs2tยซu~vGs(wgsBxzy|{=}ยยยLย=ย(ยยย6vG{=}Bย2ย รนยยฝ ยพ ยEยWรยยyยยtKยMy,}ยยยt	}ยรฟย9ยrย ยRย รฟ
t6{ยยญO}ยยยt	}BยBย vยยcย^t9ยฉ?yEยqvยป}^ยDยyKย.ย@yKยข4},}^ย2ยEย t6{ยยญยฎย ย
ยงqยgyE{gy4u&}c^yKย.w=ยฃG}cvยย}ยยgyEรLy|ยOยgยยLยยy|^}-ยOย6ยฉRย.}ยt6ย=vSยฃSvG}.ย}ยยยt6}0ย0yE{gy|yKยญย
ร=รฃ 9ร รครรฅ$รฆ=รครง EยEย/รผ6รร)ย ยDย รผ6รยร(ร|รzรผ ยดร รฉร	รยฅรฟ (ร$รพ~รยฆรDย ย ยพ ย.รฟ ยซยยดยEยยยรซย~ร
wgE}^ยgy|ยLยy|s2ยย0vSยฃSยฃRwยย.y(}ยยgyBtLยยย.wgsBยg}ยผvยปยM{!}^ยยt6},}^ยgy|ยy(y4u~vยย.}ยยEยยผยLsByรรฟ ย.wยยข^ยย}ยยยt6}Kยrยฉ?ยMยฅt9ยฃSยฃยย.w=ยฉzx
ร7ยข^vGy|{~}ยผยฃยปยยย.s2t9ยฃSยฃย ยยEย t6{ยยญ!ย t6^yรย.}ยt	ย=ยฃยปyยณยฉ?ยM,รฟ ย0ร)y*{gยL}ยyE}ยยยt6}W}ยย=vยยWยญ=ย~yKย{gยL}vยปsย=ยฃยปยยถ}ยยยt6ยฟL},ร รฟ vย
{gyKยข4yKยยยยt	.vSยฃยปยยถ}ยยgyยยฏv |yรยญ=yKย^ยข4.vGยg}.vGยL{)tLยยย.ยgยข^vยt6}ยyKยญOย0vG}ยยยถ}ยยgyEs2t9u=vGsรwgsxzy|{~}ย^ยLย~ยOยยย	vยป{=}|ย-ยรฉยRย6ยฉRยฝRยพ ยEย,รzย
ร!รครฅ$รฆ รง ยL{ยยยฏvยยญ=y|}^ยgyรร={gย6ย0ยฃGyKยญ=ยตLyBยยtLยยผy2ยEยยคยฅvยป{ยทยธuยt6sBย=ยฃGyBยนยย rยLย7t	{ยยญD^yKยข|t9ยฃSยฃย}ยยยt6}Oยยย ย
ย-ยgย9ย6ยvยยc}ยยgy*s2t9u=vGsรwgsxzy|{~}ย^ยLย~ยOยยย	vยป{=}Wย6ยฉยยฝ ยฟยพ ร ยEย ยค รzยcยง0ยgy(ร ยยฏv |yรยญ=yKยยยข4ยผvยปยg}ยผvยปยM{รฟรยKยgย ยvยยณรถยฐ~ยฑWรฒ ยก ย-ยฑรตย
ยฐ~ยฑWรฒ,ยค9ย-ยฑรตยยยรยย6ย0y|ยLy|ร}^ยgycs2t9u=vGsรwgsxzy|{~}ย^ยLย~ย*ยยย6vG{=}Rย6ยฉยฝ ยพ ยEยยคยรrยฉ?ยLยณย ยvยยtLยข4}ยwยt9ยฃSยฃGยย Kยก9ยKย Kยกยฆยยย
ย.ยB}^ยยt6}0}ยยgyรt6ยgยg^ยLยg.vยt6}ยyรฟ qยฉzยL,ย.wยยข^ยDt Oย vยยEยฐ~ยฑWรฒWยก9ย-ยฑย Dยฐ~ยฑWรฒWยค|ย-ยฑรตยยย














	
















 













! #"



%$ 

&

 







'













(*)+,".-0/

#1

32

54













6

7

,

89#:<;=?>A@CBD=E	E5F*>AGHAIJ;>@CBLKC;,@CMN=?@OLKCGPQE5;RTS,GVUW@CMX5S#SCGKC@>Y=?HLHZG[@#=?K\X5SC;,X5HT@CML;,>GHQ@C;6]L@^GVU=_R`=]QX5RaBLRabc;HQ@CKCG[OQFTODG?X5HQ@
m 9#r%GKC;sOLKC;>JX5SC;JE5F[tuUG[K,SCBQv`>JX5;HQ@\E5F%SCR`=E	Em =VHZwx=aR`=Y]QX5RBLRabc;HQ@CKCGOQF%ODGVX5HQ@pm G?Uzd0D
GVU!d0D
ef gihkj_l UG[K`np
m o
q
ef gihkj_l
n
y
0
h
}
j

|

~

t { ;>G[HIJ;A>@CBZKC;ย@CMD=V@sย^K ย ef g ยยlยCยWย yL
m ยย hkj ย0ยยย{ ML;KC; ย
X5S=?H<GON;HSC;@k@CMD=V@s>GHQ@6=X5HZS<y m PLBZ@
{ X5@CM
ย
HLG`G@CML;KsR`=Y]QX5RBLRabc;HQ@CKCGOQF%ODGVX5HQ@aGVU!d ef gihkj_l 9ยยU#@CMQX5S0X5S_X5HLwL;;w@CML;a>Y=?SC;tW@CML;H%@CML;aR`=V>JMQX5HL;KCFยGVU#SC@6=?PQX	E	X5@\F
@CMN=?@

{ ;ย=?KC;ย=?PNGBL@s@CGX5HQ@CKCGQwLBL>;TX5SsBZHLHZ;A>;SCS6=?KCF[tS\X5HZ>A;X5@kMZGVE5wZSsX5Hย=YE	E!>V=?SC;Ss@CMN=?@
MN=ย;PD;;HpBZHN=?PQE5;T@CGTOLKCGVย;@CMX5S9

ยQย

{

;HZ;;AwยX5@9sยkG {

;ย;AKt {

;

ยยยย!ยzยยย}ยยยZย!ยยกยยยยยขยฃยยคยฅยยยฆยยจยง0ยยฉยยzยชDยซ

ยฌ%ยญTยฎTยฏ<ยฐยฑDยฎยยญYยฒยฑNยฎยยณยดVยฒยฏ<ยต?ยฏLยญยดVยตยถcยทยดยถยฑWยฐ#ยญ*ยฑWยธยดยนยบยถยฐยฏLยปยฝยผDยพ#ยฑDยฟWยฏ<ยผDยฐ#ยป&ยถยฐรรWยฏLยทยดYยถยฑuยฐรร#รรยยญยธรร&ยทยฏ%รCยฑWยต`ยฑuยธยตpยฐยฏรยด
ยตVยฏLยญยธรยนยดยกยฑWยฐรยทยฑuรรรยธยดยถยฐร&ยปรยฏQรuยตVยฏQยฏLยญ`ยฑรรยยพ^ยฏยนรยถยฏรVรยกร6ยฐรยฑuยต[ยปรยฏQยตpยด?ยฑยฃยญยด[ยผDยด?ยฏ%ยดVยฒรยถcยญ`ยตVยฏLยญYยธรยนยดZยณ#ยฎTยฏ%ยฐยฏQยฏLยปรยฑWยฐยฏยยผuยปยปuยถยดYยถยฑuยฐ#ยผZยน
ยทยฑWยฐ#ยทยฏQรยดLรยกรTยฏLยทQยผZยนยบยนsยดVยฒ#ยผDยด*ยถยฐยรWยฏLยทยดยถยฑWยฐรร#ร5รยฎTยฏยยฏรรยตVยฏLยญ?ยญยฏLยปยฝยผDยฐรยฏLยญVยญYยฏQยฐยดยถcยผZยนยบยนรรรยต?ยฑWร#ยฑยญAยถยดYยถยฑuยฐ#ยผZยนkรCยฑWยตVรยธรยนcยผรรaรAร,ร
ยผWยญรยผรยปuยถcยญ\รยธยฐ#ยทยดยถยฑWยฐรยฑรรยยผDยด?ยฑWรยฃยญQรรรDยถยฐ#ยทยฏรยฎTยฏรยฎยถยยญYยฒรยด?ยฑรยผZยนcยญยฑ}ยทยฑWยฐ#ยญJยถcยปรยฏQยตรCยฑWยตVรยธรยนcยผWยญรรรยธ#ยญJยถยฐร}รรยฑWยตVยฏรยดVยฒ#ยผDยฐ
ยฑWยฐยฏรยทยฑWยฐ#ยญยดยผDยฐยดยกยผDยฐ#ยปรยฐยฑWยฐรกCยธยฐ#ยผDยตVรรรยตVยฏLยปuยถcยทQยผDยดVยฏLยญQยณ_ยฎยยฏยยฐยฏQยฏLยปยฝยผรยตยถcยทVยฒยฏQยต<ยทยฑWยฐ#ยทยฏQรยด%ยด?ยฒ#ยผDยฐรยผDยดVยฑWรยฃยญรpรขTยฒรยถcยญ*ยถcยญยกยดVยฒยฏ
รรยฑWยดYยถยฟDยผDยดยถยฑWยฐรยพ#ยฏQยฒรยถยฐ#ยปรยดVยฒยฏ<ยปรยฏรฃ#ยฐรยถยดยถยฑWยฐยฝยฑDรรครฅDรฆยกรง!รจยบรฉLรชCรฉรซWรฉรฌรญรครฎ[รฏiรง!รช\รฏCรฅDรฐ#รฌQร

ยฏQยดpรฟยจยพ^ยฏ<ยญยฑWรรยฏ<ยญยฏQยดTยฑDร0ยฟNยผDยตยถcยผDยพรยนยฏLยญยกยผDยฐ#ยปยทยฑWยฐ#ยญยดยผDยฐยดยญQร0ยฌ

รฑรรฒuรณยรดรตรถLรตยรทรดรนรธkรบ\รปรผ,รฝ`รพ



ยฑDยฟWยฏQยต



รยผDยฐ#ยปรรฟรยถcยญpยผรยฐยธยฐ


	



( )$* ! !!" % +, )- !!!.# %
0/21

ยฑWยตTยฏQยฟWยฏQยตVรรรยตVยฏLยปuยถcยทQยผDยดVยฏ

ยฏรzยผWยทยดYยนรรยฑWยฐยฏยยฑDร

 
3



รค[รฅรรฆpรง,รจรรฉZรชCรฉรซWรฉQรฌQรคQรฎรฏ5รง,รช\รฏ รฅDรฐ


 !! !"#$&%'

Wยธ#ยผDยฐรยดยถยบรฃ#ยฏLยปยฝยทยฑWยฐDรยธยฐ#ยทยดยถยฑWยฐรยฑDร0รCยฑWยตVรยธรยนcยผWยญ`ยญยธ#ยท?ยฒยดVยฒ#ยผDยด

<ยฑDรยยผDยตYยถยดร

ยร

รยฑWยต

รยผDยฐ#ยป3ร ยฑuยต`ยฏQยฟWยฏQยตVร

ยร

รรฟยฃยณ



ยทยฑWยฐรยด[ยผZยถยฐ#ยญ

รTยผWยญ`ยผรยทยฑWยฐDรยธยฐ#ยทยดLร

ยถcยญ`ยทยฑWยฐ#ยญAยถยยญYยดVยฏQยฐยดZร

ยฑWร3รรยนยฏQยด?ยฏxยปรยฏLยญVยทยตYยถรยดYยถยฑuยฐ#ยญยยญAยถร3รรยนรรยฏรยดVยฏQยฐ#ยปรยด?ยฒยฏ`ยตVยฑDยนยฏยกยฑDรkยผDยดVยฑWรยฃยญ_ยถยฐรยดVยฒยฏ%ยทยฑWยฐยด?ยฏรยดaยฑDร!ยฏLยญVยญYยฏQยฐยดยถcยผZยนยบยนรรรยตVยฑWร#ยฑรยญJยถรก
ยดยถยฑWยฐ#ยผZยนรCยฑWยตVรยธรยนcยผWยญ0ยดVยฑ%ยด?ยฒยฏ`รรยฑWยตVยฏTรWยฏQยฐยฏQยตยผZยน!ยญยฏQยดVยดยถยฐร#ร0ยฌ%ยญ_ยถยฐยฃยดVยฒยฏ`ยทQยผWยญYยฏ`ยฑDร

ยผรยดVยฑWรยฃยญQยณยถยบรยฎยยฏรฃร

ยฑWยต[ยปรยฏQยตYยถยฐรยฑรรยดVยฒยฏรยทยฑWยฐDรAยธยฐ#ยทยดยญ*ยถยฐรยผรยทยฑWรรรรยนยฏQยดVยฏยฃยปรยฏLยญ?ยทยตยถรยดยถยฑWยฐยณยดVยฒยฏQยฐรยทยฑWร3รรยนยฏQยด?ยฏ

3


ยผZยนยบยนรยฝยฏรzยท?ยนยธ#ยญJยถยฟWยฏยฃยผรยฐ#ยปรยฏรยฒ#ยผDยธ#ยญยดYยถยฟuยฏWร

54

sยนยฏLยผDยตยนรWยณkยผ

ยทยฑWยฐรยด[ยผZยถยฐยฏLยปรยถยฐรรฟรยณยผDยฐ#ยปยฝยฎ*ยฒรยถยยท?ยฒยฝยถcยญยกยถcยญ

รCยฑWยตVรยธรยนcยผ

64

ยญยธ#ยท?ยฒรยดVยฒ#ยผรยด


64 87 94
:;=<->,?&@"A 


B4

	

ยฑWยตยญYยธ#ยทVยฒรยผ*รCยฑWยตVรยธรยนcยผ

ยณZยนยฏQยด

ยฑWยตยกยดVยฒยฏรยธยตVร#ยฑยญYยฏLยญยกยฑDรยยดVยฒยฏยตVยฏQรรยผZยถยฐรยถยฐรยฝยปuยถcยญVยทยธ#ยญVยญJยถยฑWยฐ}รJยฏรzยทยฏQรยด%ยฎTยถยดVยฒรยถยฐรรยต?ยฑยฑDร6ยญร[ยณ^ยฎTยฏรยผDยตVยฏ%ยถยฐรยดVยฏQยตVยฏLยญยด?ยฏLยป



ยทQยผDยฐรยฟuยถยฏQยฎรยผยยปรยฏLยญVยทยตยถรยดยถยฑWยฐ

	
IHKJLEGM

DC

ยฑDยฟWยฏQยตรฟรยผWยญaยปรยฏLยญVยทยตยถยพรยถยฐรยยด?ยฒยฏpยปuยถ

FEG

ยฑWยธยตxยทยฑuยฐ#ยญยดVยตVยธ#ยทยดยถยฑWยฐยณ,ยฎ*ยฒยฏQยฐรยทยฑWยฐ#ยญJยถcยปรยฏQยตยถยฐรรยผ

N

ยตVยฏLยญร^ยฏLยทยด%ยดVยฑยผ

H

uยธยฏQยตVรรรTยณ^ยฎยยฏยปรยฏรฃ#ยฐยฏรยดVยฒยฏยญยฏQยดxรฟ

EGOM

%ร0ร6ยฐรร#ยผรยตVยดยถcยทยธรยนcยผDยตLยณยดVยฒรยถcยญ*รรยฏLยผDยฐ#ยญTยด?ยฒ#ยผDยด
ยฌรยทยฑWร3รรยนยฏQยด?ยฏxยปรยฏLยญVยทยตYยถรยดYยถยฑuยฐ

Q SR T UR

ยดVยฒยฏ

รAยผDยฐ#ยป

รรฐ

Dรฎ รง Dรฎรญรช

 /



ยฏQยต?ยฏQยฐยดaรยตVยฑWร#ยฏQยต?ยดยถยฏLยญยฑรร!ยดVยฒยฏpยทยฑuยฐ#ยญยด[ยผDยฐรยด[ยญ_ยถยฐ&รฟรร

ยฑDรยยดVยฒยฏยรCยฑWยตVร

kยฎ*ยฒรยถยยท?ยฒยฝยถcยญ%ยญยฏQร#ยผDยตยผDยพรยนยฏรยฎTยถยดVยฒ

ยดVยฑ&ยทยฑuยฐยด[ยผNยถยฐยฝรยตVยฏLยท?ยถcยญยฏยนรยฝยดVยฒยฑยญYยฏยยทยฑWยฐ#ยญยดยผDยฐยดยญaยถยฐ.ร




P



ยถยฐยดVยฑยยดVยฒยตVยฏQยฏ`ร#ยผรยตVยด[ยญ

ยดVยฒ#ยผDยดยยถยฐรยฟWยฑDยนยฟWยฏยธยฐ#ยผDยตVรยรยตVยฏLยปuยถcยทQยผDยดVยฏLยญ

VPQUR WT XR

ร ยฑuยตยฃยฏLยผWยท?ยฒยฑรร<ยดVยฒยฏรยทยฑWยฐ#ยญยด[ยผDยฐรยดยญรรยพ^ยฑDยนcยญร[ยณTยดVยฒยฏ}รฉ



ยถยฐรยฟWยฑDยนยฟuยถยฐร



Wรจiรฏ รช รรง DรฎQรช

ยฏ Wยธ#ยผZยนยบยถยดรรยผDยฐ#ยปรยด?ยฒยธ#ยญยปรยฏQยดVยฏQยตVร

Z9Q SR #T UR

ยดVยฒยฏรยทยฑWยฐ#ยญยดยผDยฐยดยญยผDยตVยฏยฝยฏ Wยธ#ยผNยน<ยดVยฑ}ยฏLยผWยท?ยฒรยฑuยดVยฒยฏQยตQร[ยณยกยผDยฐ#ยปรนยดVยฒยฏ}รฐ!รฅรรฐ

รรฐ

รรฎ ยฝรง Dรฎรญรช

\[ /

ยถยฐยฏLยญ

NY

ยฎ`ยฒรยถcยทVยฒ}ยฑDร

ยฎ`ยฒรยถcยทVยฒรยทยฑWยฐ#ยญJยถcยญยด[ยญรยฑDร

ยถยฐรยฟWยฑDยนยฟWยถยฐรรยฐยฑWยฐรกCยธยฐ#ยผDยตVรยฝรยตVยฏLยปuยถcยทQยผDยดVยฏLยญ3รAยผรยฐ#ยปรยด?ยฒยธ#ยญ%ยปรยฏQยดVยฏQยต?รยถยฐยฏLยญ<ยด?ยฒยฏรยฐยฑWยฐรกCยธยฐ#ยผDยตVร



รยตVยฑWร^ยฏQยตVยดยถยฏLยญยกยฑWยดVยฒยฏQยต%ยด?ยฒ#ยผDยฐรยฏ uยธ#ยผZยนยบยถยดAรยฝยฑDรยด?ยฒยฏ
ยผยทยฑWรรรรยนยฏQยดVยฏรยปรยฏLยญVยทยตยถรยดยถยฑWยฐ

	

ยผDยฐ#ยป&ยถยฐ

,ยฎTยถยบยนยบยนsรรยฏQยฐยดYยถยฑuยฐยฐยฑ&ยทยฑWยฐ#ยญYยด[ยผDยฐรยดยถยฐยฝรฟยฃร

รยฎ`ยฒรยถcยท?ยฒยทยฑWยฐ#ยญJยถcยญยด[ยญยฑรรxยดVยฒยฑรยญยฏ&ยทยฑuยฐDรAยธยฐ#ยทยด[ยญ3ยฑDร

ยดVยฒรยธ#ยญยปรยฏQยดVยฏQยตVรยถยฐยฏLยญยผDยฐรยผDยดVยฑuร

ยทยฑuยฐDรAยธยฐ#ยทยด[ยญยยฑDร

ยฑuยตpยผยฃยญYยฏQยดpยฑDรยยทยฑuยฐ#ยญยด[ยผDยฐรยด[ยญ`รฟรยณยฎTยฏ

ยฑDยฟWยฏQยตยผ<ยญYยฏQยดยฑDรkยทยฑWยฐ#ยญยดยผDยฐยดยญsรฟยจยทQยผDยฐรยพ#ยฏ%ยปรยฏLยทยฑWรรร^ยฑยญยฏLยป

ยฎ`ยฒรยถcยท?ยฒยทยฑWยฐ#ยญJยถcยญยด[ยญยยฑDรpยด?ยฒยฑยญยฏยทยฑWยฐDรAยธยฐ#ยทยดยญยฑDร

ยดVยฒยฑยญYยฏ

ยณยฎ`ยฒยฏQยตVยฏรฟยจยถcยญ*ยดVยฒยฏ

ร

ยฑWยฐรยนรยถยฐรยทยฑWรรรรยนยฏQยดVยฏยปรยฏLยญVยทยตยถรยดยถยฑWยฐ#ยญยยฑNยฟuยฏQยตpยผDยฐรยฏQร3รยดAรรยญYยฏQยดpยฑDรยฟDยผDยตยถcยผDยพรยนยฏLยญQร

ร6ยฐ

3ร รร!ยพ^ยฏpยผ%ยญยฏQยดsยฑDร!ยทยฑWร3รรยนยฏQยด?ยฏ

ยยถยยญ*ยฏ uยธรยถยฟDยผZยนยฏQยฐยดยยดVยฑยดVยฒยฏยยปuยถcยญ\รยธยฐ#ยทยดยถยฑWยฐ

ยญยฏQยด`ยฑรรยทยฑWยฐ#ยญYยด[ยผDยฐรยด[ยญTยผDยฐ#ยป3ร ยต?ยฏQยฏ<ยฟNยผรยตยถcยผDยพรยนยฏLยญTยถยฐ

	

รยฎ`ยฒยฑยญยฏยรCยตVยฏQยฏยฟNยผDยตYยถยยผรยพรยนยฏLยญยยผDยฐ#ยปรยทยฑuยฐ#ยญยด[ยผDยฐรยด[ยญยกยผDยตVยฏ

uยธ#ยผDยฐรยดยถยบรฃ#ยฏQยตVรกpยผDยฐ#ยปยฝรยต?ยฑWร#ยฑWยต?ยดยถยฑWยฐรก\ร ยต?ยฏQยฏWยณ,ยถcยญยกยฏ WยธรยถยฟDยผZยนยฏQยฐยดยดVยฑรยญยฑuรรยฏรยปuยถยยญYรก

รยธยฐ#ยทยดยถยฑWยฐรยฑDร!ยทยฑWร3รรยนยฏQยด?ยฏpยปรยฏLยญVยทยตยถรยดยถยฑWยฐ#ยญaยฑDยฟWยฏQยตsรฟยฃร
ยปรยฏLยญVยทยตยถรยดยถยฑWยฐ#ยญยกยฑDยฟWยฏQยตยกรฟ

ยญยฑWร3ยฏpยผDยตVยพรยถยดVยต[ยผรยตVร

ยปรยฏLยญ?ยทยตยถรยดยถยฑWยฐ#ยญ<ยผDยต?ยฏ<รยธยดVยธรก



ยทยฑuยฐ#ยญยด[ยผDยฐรยด[ยญร[รTยฌ%ยญยกยฎTยฏยยญยธรuรWยฏLยญยดVยฏLยป,ยณ,ยดVยฒยฏยธยฐ#ยผDยต?รร#ยผDยตVยดยกยฑDรยญยธ#ยท?ยฒ

^] _
gf

ยฏรยดVยฏQยฐ#ยปยญยยดVยฒยฏยฐยฑWยดยถยฑWยฐยฝยฑDร

F`ba ced

ยฑuยต`ยดVยฒรยถcยญ`รยธยต?ร#ยฑยญยฏWยณ^ยฎTยฏ<ยผZยนcยญยฑรยฏรยดVยฏQยฐ#ยป

รCรCยฑWยตpยผDยฐรยผรยดVยฑWร

QยผDยดVยฑWร

ยดVยฑยดVยฒยฏรยทQยผWยญยฏยฑDรรยธรยนยดยถรรยนยฏ&ยทยฑuยฐ#ยญยด[ยผDยฐรยด[ยญQร

h`ia ; d

ยรยยผDยฐ#ยปรยปรยฏรฃ#ยฐยฏ

,ร ยฑuยตpยผรยปรยฏLยญVยทยตยถรยดยถยฑWยฐ



ร

ร6ยฐรยดVยธรยถยดยถยฟWยฏยนรWยณยฎTยฏยผDยตVยฏ<ยด?ยตVยฏLยผDยดยถยฐรรยฏLยผWยทVยฒยฝยฑDรยด?ยฒยฏ%ยถยฐ#ยปuยถยฟWยถcยปรยธ#ยผZยน ยญยผWยญยถยฐ#ยปรยฏQร#ยฏQยฐ#ยปรยฏQยฐรยดLยณsยญยฑรยดVยฒ#ยผDยด`ยดVยฒยฏรยตVยฑuยพ#ยผDยพรยถยบยนรยถยดร

6j /

ยดVยฒ#ยผDยดยกยทยฑWยฐ#ยญยด[ยผDยฐรยด

kfmln
6fml9

ยญVยผDยดยถcยญJรฃ#ยฏLยญ*ยผDยดVยฑWร

6j /

ยดVยฒยฏ%รยตVยฑuยพ#ยผDยพรยถยบยนรยถยดรรยด?ยฒ#ยผDยด

%ยผDยฐ#ยป

ยญVยผDยดยถcยญJรฃ#ยฏLยญ

jPo

ยดVยฒ#ยผDยด`ยทยฑWยฐ#ยญยดยผDยฐยด

Ofmlqp

pยญVยผรยดยถcยญJรฃ#ยฏLยญ

OjPo

%ยผDยฐ#ยปรยดVยฒยฏ<รยต?ยฑWยพ#ยผDยพรยถยบยนยบยถยดรยดVยฒ#ยผรยด

ยกยถยยญยรAยธ#ยญยด*ยดVยฒยฏ<รยตVยฑยปรยธ#ยทยด`ยฑDร

f=lqp

pยญ?ยผDยดยถcยญJรฃ#ยฏLยญ

Nร

r 	


sfmln )j / =Js! !!tJf=lqu )j.v
5j / ! !!2#j.v
w"xzy{| }2~#| Wย0qยqย).| ย }2ยย0bย-~#qWย0ยzย0ยpยยย ย2}"ย.ยqย6~.ยSยqย ย6ยqย=~.ยยย2ยPย-ย2ยยย0ยWยyOย-~#ยbยยยยย.ย=ยPย0ยยtยยยยยยย0|}ยย0ยย ย)=ย =ย ย p "ย bยยยยยยยยย.
ย-~#ยqย^ยยยWย ย"ยก#ยขยย"| ย^ยฃยยยWย ย"ยก"ยขยยย0ยยย0ยย0|}2~|Wย0qยqย2|Pย2y
รฑรรฒuรณยรดรตรถLรตยรทรดรนรธkรบ\รป

ยฏ uยธรยถยฟDยผZยนยฏQยฐยดยด?ยฑ

ยฑWยตยผรยทยฑuรรรรยนยฏQยดVยฏรยปรยฏLยญVยทยตยถรยดยถยฑWยฐ

Tรฝ

ร

ร

ยกร

ยฎTยถยดVยฒยฑuยธยดยฃยฟDยผDยตยถcยผDยพรยนยฏLยญยฎ`ยฒยฑรยญยฏรยธยฐ#ยผDยต?รร#ยผDยตVยด

%รยฃรCรCยฑWยตรยปuยถcยญยดยถยฐ#ยทยดรยทยฑWยฐ#ยญยดยผDยฐยดยญ

ยคยฅ

ยถยยญ

<รยผDยฐ#ยปยรCยฑWยตยฃยผยฝร^ยฑDยถยฐยด

ยฆ6ยงXยจtยฉUยชยฌยซbยญ=ยฎUยฏยฐยฑยชยฑยงUยฒยณยซยยดยถยตmยจeยฏยฏ ยชยฑยง
ยธยท ยนBยบยปยผยพยฝ=ยฟ^รtยฟรรUยฟ


รbร รยรqร ยธยยท รbรรร ร ยธUรqร$ร
รnรยร
รรรยฑร ยฟ ร"รร-ร6ร ร รยรยร*ร รtยฟPรยฟPรร ร ร รtรรร ร รBร ร ยผ ร"ร ยฟรUร ร-รก รร ร-รก"ร=ร-รข รBรฃ
รคร ยฝ=ยฟ5รฅรฆยฟPร ร ร ร รUยฟรยยผiยฝรงยฟรจ ร รยฟรฉ ร ยฟPรร ร ร รกhร รUร รก"รยพร รจ รร"ร รtยฟ ร ร6ยฝ ร ร"รรชรขรซรยฑรก รฅรฌรtร ร รรฎรญ ร.รร-ร^ร ร ร ร ร ร ยฟ
ร ร รUรฏqรUร ร-รก รรฐร รก ยฟร ร รจ ร-ร ยฟ ร รtรฅรฑ ร ร ร รฃรณรฒ6ร รก รจ ร รฅ5รUร ร#ร-ร ร ร ร ร ร'ร รก"ร รจยฟรtร รก ยฟ รขqรยฑรก ร รรจ รรดรขรซรยฑรก รฅรฌรtร ร ร ร ร ยฟ ร ร"ร ยฟ
รฅ ร รฉ ร รฅรUรฅ5รฏqยฟPร ร.รก"ร รtร ร รUร รก"รtร รจ ร รtยฟ ร รจ รก ร รฑยฟร ร รฑ ร-รต ยฟรจ ร รฅรฑ ร รUยฟรhยฝ ร ร"ร5ร"ร ยฟ ร ยฟรจ ร ร รรถ รUยฟ ร รรทรข(ร.รธOรก"ร-รต ยฟรนยฟ ร8ร ร9รฃ&ยผ
รบรปยฑรปรผ รฑยร#รฃbรฝ ร ยฟ ร ยฟร ร-ร"ร ยฟ รก ยฝ=ยฟ รก ยฟร ร ยฟร ร ร ร.รธOรก"ร-รต ยฟยฟ ร6ร ร9รฃ&ยผ รบรปยฑรปยฑรผ รฑยร ร"ร รจ ร รฅรฆรUร ร ยฟ ร ร รtรฅ5ร ร"รยฑร ร รจรจ ร รร ร ร ร ร ร ร ร
ร รก"ร รฑ ร รฑ ร ร ร ร ร ยฟ ร ยฝ ร ยฟPรsรจ ร รร ร ร ร ร ร ร รUรพ ร ร ร ร รก ร ร รฏ รรก รtยฟ รก^รฟ ร ร ยฝ=ร&ยฟรtรพยฑยฟNรฑ ร ร ยฟยรฃFรฝ ร ยฟNรฑ ร รยร รจ ร รtยฟ ร ร ร
ร"รร-ร รจ ร ร ยฟ รร ร ร รขqร รDร ร ยฝ 	ร  รฝ ร รจ ร รฅรฆรUร ร 
ยฟ 
 รก รฆร รญ   ร#ยผยยฝ=ยฟ^ยฟรฉ ร รฅ ร รUยฟ ร.ร ยฟรฌรฑยฌยฟ รร รต ร รยฑรกรฎร-รข รญ ร รรชรร ร ร ยฟ
รฅ ร รtยฟร ร ร-รข   รฃ  ยฟรงร รรทรก"ร ร ร ร ร ร ร"ร ยฟ=รฅ ร รtยฟร ร ร-รข   ร ร ร.ร^ร รร ร ร ยฟOรจ ร รDร&ยฟรจ ร ร ร ร รรทรข รจ.ร ร ร"ร ยฟ รรนร รรจ ร5ร"รร-ร
รญ รฑยฌยฟ รรรต ยฟ ร  !^ร รยฟ ร รจ ร ร รร ร รต ร รtร ร ร(รจ.ร ร ร"ร รฃ#"8ร ร"ร รร ยฝรงยฟรรฅ5ยฟ ร ร ร"รรรทรรงร ร&รฅ ร ร ร8ร รDรSยฝ รยฑรก รร รยร ร ร"ร ยฟ
ร &')( +*, รขรซรรก8ร"ร ยฟ ร ร รtรฅ5ร ร"รยฑร ร รจmร รก"ร รฑ ร รฑ ร ร ร ร ร ร-รข รญ
รจ.ร ร ร"ร=ร ร-ร ร*ร รข ร รญ รยฑรกรงร ร&รฅ ร ร ร ร ร รUยฟ^ร %ร $ ร รฃยยฟยฑรฃ&ยผ ร"ร ยฟ รก ยฟ รร 
ยฝ ร ยฟPร5ยฝ=ยฟ รก ยฟ ร ร.รก ร รจ ร8ร-ร"ร ยฟPร ร ร ร ร ร"ร รฅ ร รtยฟร รยร ร ร ร)ร รUรพรทรรยฟรจ.ร ร ร"ร .รฃ -ยร รยฑรก รtยฟ รกbร"ร รจ ร รฅ5รUร ร ยฟ 
 รก รฆร รญ  / รยณยฝ=ยฟ
ร"ร ยฟ รก ยฟ รขqรยฑรก ยฟ ร รtยฟPร ร ร รข ร ร"ร ยฟ5รจ.ร ร ร"ร ยฟ ร ยผยณรจ ร รฅ5รUร ร ยฟ ร"ร ยฟ รก ยฟร ร-ร ร รต ยฟรฆยฝรงยฟ ร รพ รtรร-รข ยฟ ร รจ ร รจ.ร ร ร"ร ร ยฝ ร ร รจ ร ร*ร รก ยฟ รถ ร ร รก ยฟร
รฑยฟรจ ร ร ร ยฟ ร"ร ยฟรจ.ร ร ร"ร ยฟ ร ร-รก ยฟร รยฑร รUยฟรจยฟ ร"ร ร-รก ร ร&ร ร-รข ยฟ รถ ร ร ร รก ยฟร รรทร ร รต ยฟ รย1ร 0 ยฟ-ร#ยผ ร รร ร"ร ยฟPรBรtยฟรจ ร รtยฟ รขรซรยฑรก ยฟ ร รจ ร รจ.ร ร ร"ร
ยฝ ร ยฟ ร"ร ยฟ รกรร"ร ยฟ ร ร รยพรฅ5ร ร.รยฑร ร รจ6ร รก.ร รฑ ร รฑ ร ร ร ร ร ร-รข รญ รร 0 ยฟ รก.ร5รยฑรก6ร รUยฟยฑรฃ
- ร=ร ร รก ร ร ร ร ร=ร"รร-ร รฅรฌรรจ รร"ร ยฟ ร ร รฅ5ยฟ ร รtยฟ ร ร รจ ร ร ร ร รยพรUยฟ ร"ร ยฝ รยฑรก"รฟ ร ร ร"ร รร รขรซรกร รฅ5ยฟPยฝ รยฑรก"รฟ 2รฃ -ยร ร"ร รร รจ ร ร ยฟยฑยผ
ร"ร ยฟ^รจ.ร ร ร"ร ยฟ ร ร-รก ยฟรtยฟรรUยฟรร รยร รUรพNรจ ร รฅ5รtร&ยฟ ร ยฟรฌรtยฟ ร รจ รก ร ร ร ร ร ร ร ร รร ร"ร ยฟ ร รUร รก.ร ร รก ร ร-ร ยฟ รย1ร 0 ยฟรฌรtยฟ ร รจ รก ร ร ร ร ร ร4325 รฃ
รฝ ร ยฟ6รฅ ร ร รNร +ร 6 ยฟ รก ยฟPรรจยฟ รร ร.รร-ร ยผ รก#ร-ร.ร ยฟ รกbร"รร ร5ยฟรฉ ร รฅ ร ร ร รUรพ ร รDรยฝ รรก รร ร รจ ร ร รยรร ร ยฟPร ร ยฝ ร ร"ร5ร"ร ยฟ รฟ ร ร ยฝmรรยฟรtรพยฟ
รฑ ร ร ยฟยฑยผยฝ=ยฟรงร ร ยฝรฐรจ ร รรจยฟPร ร"รก#รรทร ยฟ ร ร ร"รUร ร ยฟรงยฝ รยฑรก ร*ร รยร ร ร"ร ยฟ รต ร รจ ร ร ร ร ร ร-รขยร"ร ยฟ=รฅ ร รฉ ร รฅรฌรUรฅรฆรฏqยฟPร ร"รก.ร รยพรรฎร ร ร ร ร ร ยผ ร ร
ร ร ร ร ร รUยฟร ร ร ร"ร ยฟ=ร รก ยฟ รต ร ร ร รรนร ยฟรจ ร ร ร รยณรฃ.- รbร ร รก ร ร ร ร รbร.รร-รbร"ร ยฟ รก ยฟ ร ร"รก ร รจ ร ร ร ร ร"รร"ร ยฟ ร ยฟ6ยฝ รยฑรก รร ร ร 6 ยฟรจ ร ร รต ยฟ รก ร
รข ยฟPยฝ ร ร รยฟรจ ร ร ร-รขยร"ร รร รจ ร รฅ5รUร ร#ร-ร ร ร ร ร รร รก"ร รจยฟรtร รก ยฟยฑ.รฃ -9ร รขnร รจ ร ยผ ร"ร ยฟ ร รtร&รhร +ร 6 ยฟ รก ยฟPรรจยฟ รรยร รNรจ ร รฅ5รUร ร ร รUรพ ร"ร ยฟ
รก ยฟร ร-ร ร รต ยฟยฝ=ยฟ ร รพ รtร6ร-รข,ร.ร ยฟ'ร +ร 6 ยฟ รก ยฟPร ร รจ.ร ร ร"ร ยฟ ร รฃรนรฝ ร รร ร ร ร ร ร ร ยฟPรBรจ ร รรฑยฟ^ร ร รUยฟร ร)ร รUรพรฆรฅ ร รฉ ร รฅรUรฅkยฟPร ร"รก"ร รยพรยฑยผ
ร รยร รUรพ ร"ร ยฟ ร"รtร ร ร รtยฟ ร รจ รก ร รฑยฟร ร ร87ยฑยฟรจ ร ร ร ร49ยฌรฃ;:Uรฃ

<>=@?ACBD?EGFHJILKMON2PRQ รญTS P *U>LV*WJYX[ZT*@\ PRQ  ร^]`_>a S P * Pcbb	P  Q d*eJ!fg b  Q ih P
jR@, P \Rk P Sl* b	P i8X Zร ,.meon	mp bqb	P fL*)*rS	 P ,. Q ms Pcb f P n QQ Bรญt NuPDQ[v S PwQ m PwbPDQ xYn bQ * QJb
*fef P *UJgk`Jรช
 รญyYJ ]{z b  Q mr* Q  a n Q *i b C P x Q m P nU b	Q * QJb i v}| *UC\p PRQ#~/ร  S P
Q m P >g+*pยยxย ยยยยeย>ย8ร ย ย a t8ย bb r PยQ mr* QยQ m P  P`P	ย  b	QJb * b ยย P \ Pcb nc!f Q o 3 5 b gncm Q mr* QJย 
*eJรน ยpยท ยยย ย  *C\ ย ยท * PยbQ *S	 P  3 5 ย *UC\ Q mr* QQ m Pยb fg*en Pยย[ยeย ย Oย m* b *woย	 P * ย J>r '
P  Q )ffLi Q ย ยท tWย2m P 
 ร รญ 3 5 _ ร\ร ร ร รยร ร eย ยท ร
ร ยยยยยกยฃยขยฅยคยงrยฆ ยจ 
 รก)

 รก  ร รญ ^รbรยย
ร ยยยยยกยฃยขยฅยคยงLยฆ ยจ ร ร รยร ร eย ยท ร
ย
  Q m P \ P C>JC* Q  b fg b  Q ih P t
7 ร รรจยฟOรฑ รยฑร"ร รญ ร รรW3 5 O_ ร ร-รก ยฟรนร รก ร ร รฏ รรก รtยฟ รกยรขรซรรก รฅรtร ร ร ร รร3 5 O_ ร รร ร รก ยฟรจ รร ยฟร&ร ร-รขSร.ร ยฟ รก ยฟ รถ ร ร รก ยฟร รขqรยฑรก รฅ
ร ร ร"รธOรก.รยรต ยฟ6ยฟ ร6ร ร9รฃ&ยผ รบรปยฑรปยฑรผ รฑยรยผ ร"ร ยฟPรย
 รกรฆ
 ร รญ 3 5 รฌ_ รร รร ยฟ ร ร"ร ยฟ รก ย รยฑรก รบ ยผ ร รรยฝ=ยฟ'รจ ร รNร ร ยฟ ร.ร ยฟ ร ร&รพ รรก ร ร"ร รฅ
ร-รข=ร"รธOรก.รยรต ยฟ6ยฟ รOร ร9รฃ&ยผ รบ รปยฑรปยฑรผ รฑยร ร"ร รจ ร รฅ5รUร ร ยฟ ร"ร รร ร ร รฅ ร ร ยผ ร ร ร"ร ยฟ ร ร รฅรฆยฟ^รฑ ร รUรร ร ร ร ร ร ร รUยฟร ร"ร ยฟ รก ยฟยฑรฃ
รฒ6รUยฟรจ รยฑรก"ร รDร ร-รก ร ร-รข6ร"ร ยฟ ร รฑ ร-รต ยฟ รร ร.รร-รร"ร ยฟ รขqรยฑรก รฅรtร ร ~/ร  รUร รร ร ยฝ ร ร.ร ร รก"ร รฑ ร รฑ ร ร ร ร ร รบ รพ ร รต ยฟPร ร รยพร
รฟ ร ร ยฝmรรยฟรtรพยฟOรฑ ร ร ยฟ ร-รขSร.ร ยฟ รขqรยฑรก รฅยฝ=ยฟ ร-รก ยฟ ร ร ร ยฟ รก ยฟ ร ร ยฟร ร รยณรฃ=รฝ ร รร รจ รยฑรก"รก ยฟ ร ร ร รร ร ร"ร^ร รtยฟ รขnร รtร ร6ร ร.ร รUรฅ5รUรฏ
ร ร ร ร ร-รข r%dยc P C* Pcb ยผ ร ร รก"ร รยฟ รก.ร ร ร-รขรซร ยฟPร รจ ร ร รยร รtยฟ รก ยฟร ร"ร รฑยฟBรtยฟ รยร รก#ร รฑtรรยฟ ร ร ร รรtรรจ ร ร รต ยฟ รก ยฟ ร ร ร ร ร รUรพ
ร ร ร ร Pยฟ รฅ ร รฃ

ยฉRยฉ

ยช[ยซLยฌLยญCยฎยฃยฏยฑยฐยฒยฎLยณLยดDยญCยตOยซLยฌLยญยทยถYยซLยธLยนยยฏ[ยบLยฏยผยปยฝยฌLยพLยณgยฎยฃยฟร
รTรร+ร1รร)รรiรOร)รLร	รeรlร	รรรรรRรOร)ร	รLร)รRรxร	รรรYรร1รeรร+รCร	รรรยรยฅร	รLร	รรDร+ร1รRรรxร1รeร8รร/รรรLร	รeร)ร	รรรร1รeรeรยฃร1รยรxรxร+ร+ร
รรeรรรrรLรร	รlรeรร#ร)รRรxร)รxรiรรxร1รeรรcรรรรรLร	ร)รรiร#รLรรeรLรRรรรxร1รeรYรlรรร[รxรeรรรกรร2ร)รLรRรxรร	รรYรข%รร1รรrรxร	รLรRรWร)รqรรรeรร
รรฃLร)ร	รrรDรรDร1ร)รLรeรLรeรpรครรรDรฅeรยรLรยฅรรขร	ร	ร8รรขร1ร>ร)ร>รรรpรwรlรยรRรUรรฆรรร	รรร/รรeรรยฅร1รxร1รeรรรร1รeรร+รCร	รรรxร1รงรWรeร)ร
รeร	รLร	รรรจรuรlรรรยร)รLรรeรLรRร#ร)รรรรครรรDรฅeรรxรรร)รRร@รยฝร8รOร1รRรDรฅeรOร1รรฉรeรรรยรยฅรร	รยรLร)รeรขร1ร	รรชรครกรLร	ร)รLร	รรฉรxรรlรร
รxร	รรUรรรรeรรยฅร1รxร1รeรรOรรฃรiรxรร	รรรซร/รรeรLรรxรeรLรlรLรยรรrรรรรรรฌรยรยฅรรVรCร	รUรrรOร)รRรxร)รรรยรรรรยรยฅรรครรรDรฅeรรYรeรร}รยรรกร)รรรรร
รDร+ร1รรคร1รLรYรeรร1รงยรLรรรlรงYรLร)รRรยฅรiร	รรlรRรร1รรlรLร>รญรฎรฏร#รรรiรรiร)รxรLร}รiรร)รLรqรรรLรขLรฐรฌรRรรรกรรร)รLรยรLรรฃgรรฉรรรRรรxร1รeร2ร
รฑ รฒรณยรดยฃรตรถ2รท[รธรบรน[รท#รป@รผรตรพรฝ#รผeรดรธรรฟ Lรป Uรด
รรLรรรรรรeร oรครeรxรiรLรยฝรร	รlรLรgรยรYร ยฅรRร.รรeรรร1ร	ร)รรฉรxร	รรรรรdรยฅรร)รLรรรdรร+รLรiรรLรeรรรยฅร รฌรรร@รDร1รรร	รRร@รรoรeร
ร	รฅeร	รยรxรiรlรLร	รรiรรLรยฅรรรeรRร ร#รซร`ร)รLรรeร)รLร	รรรUรร@รLรeรLรรรLรร+รiร	รรxร1รeรpรรรYรรจรฃrร1ร>รLรรชร	รร)ร)รeรรงรiร#ร+ร1ร>ร1ร)รRร
ร)ร>รLรรร)รง rรLรรคร1รRรรeรรขรeรรรRร	ร รฌรรร)รรiรร)รRรxร)รรรยรรรรยรยฅรยรRร)รxร	รรxรiรDร รTรร+ร1รยรครรรรLรeรรรDรฅeร รยร)รLร	รยฅร)ร	รรชร)ร
ร)รรiรOร 2รRรร รยรรร	รRร@รuร1รOรยรOรLรยฅรรฉร	รฅeร	ร8รlร1รRรรรครรUรร)รLร>รครeรรยฅร1รLรYรUร/รxรร)ร8รWร)รLร	รeร)ร	ร{รครeรรiร รขร ร@รคร
รรeรรฐxรRรร)รLร)รยร)รรร#ร1รรiร	ร
ร	ร)รรDร1รร1รงรLรeรLร>รรร)รLรยรlรRร)รLรรiรยฅรLรRรยรครยรรRรฅยฅรยรรxรRร`ร1รpร)รรiรรรUรร	รยร	รรpรขร>รeร	รLร	รรDร+รยร	รRรยทรร1รeรร+ร
รiร	รรรxร1รงeรwรซรLรYรยฅร รรร1รรฌรง`รiรรlรรรRร@รeรรร>รคร>รรRรฅยฅรรฏรwรขร1รรรlรง4รยฅรรรยรยฅรLร	รqรรxร1รxรงยรLร)รRรยฅรiร	รร)รeรยฝรครรxร	ร>รLร
รรรDร1รeรยฅรLรยร)ร>ร)รLรยรLรeรxร1รeรยรร[รร)รeรwร/รUรรยรLรYร	รรLรยฅรรยร	รรจรรoรeร)รรชรlรLร	รeร)ร	รร ร eรRรรxร1รeร Lร >รรร รยร`ร)รLร
รLร)รรรรร.รรLร	รeร)ร	ร Lร ยฃรeรคร รยฅรiร)รรรlรรครรง>ร1รรขรRรรeรWรRร.ร1รร%รrร)รร1รขร1ร>ร)รยรยฅร	ร/รรรยรwรUรรLรRรxร)รRรยรยฅรรรรxร+รร	รร
รรร รLร)รeรรยฅร)รxร1รeรรรกรครLร	รรครรรDรฅeรยรLรยฅร oรLรรร)รงยรLร)รRรยฅรiร	รร)รRรcร .รฅeร	รpรรeรรรiรร	รxร1รLรยรYรร)รlร	รร#รeร4ร>รWรeร)ร
ร1รrรlรรยรรรยรฅยฅรwรยร	รฅยฅรร รยฝร)รLรwรLร)รeรขร1ร	รYรยรxร	ร	ร รdรยฅร)ร>รiรLรรขร1รeร รยฒรยรLรรร)รง`รiรรLรeรรUรeรeร.รร)รeรYรรUร)รรรxรรoรร/รข%ร
ร	รรรxรรกร)รLร	รงรร)รรฉรร1รรร1รรรRร)รรxร1รLรxร1รeรรรlรรร[รxรLรรYรUรxร1ร	รรฉร	รฅeร	รlรงrร)รร1รLรยร)รรร.ร>ร1รeรร[รข%ร รLรรครรรขรeรLรร
รรeรYรรจรยรWรรยร	รWร	รrร#ร1ร ร}รรLรรร ร [รLรรรeรรรiรร	รรOรiรรLรeรรUรeรรคร1ร)รYรยรร1รLรร1รรฉรขร1รรรlรงรLร)รRรยฅรiร	รร)ร
ร
ร8รeรรรยรLรรรรจรฅยฅร	รร)รรiร#รiรรLรeรรรยฅรร1รรlร1รรรรฏรรจรVรยฝรรร1ร)รยรยฅรรรLรร รครกรLร	ร)รรครร)รร1ร ยรร
รรยฅรรLรรiรยฅร1รLรร+ร
ร)รLร	ร)รรฏรยรยรรpรRรรeร>รoร)รeร 8รlร ร รsร)รรiรOรiรรLรeรรรยฅรeร2ร)รLร	ร)รรรlร>ร1รรรรยรlรรยรงsรYรรรง4รLรlรeรร	ร)รรรยรRร}ร)รรร
รYรDรงรข%รยร)ร)รLรOรeร#รรDรiรxรqรรข%รeรLรรรรeรYรรจรยรยรร1ร	รร	รรRร ยฃรeรรรฃรรรร1รeรLร)รLรยรยฅร)รxร	ร)รxร1รeรร รlรLรรLรgรร รรeร
รLรร1รeรrรข%รeรร `รร)ร>รรฃLรLร)รRร)รร1รขร1รwร1ร8ร)รLร>รiรรLรeรรUรeรยรdรeรรRรยฅร)ร ร>รรรร	ร@ร1ร8รยฅรรร	รร)รยรรร)รรรยรรรยรยฅร8ร)รLร
รรeรYรรจรยรwรร1ร	รร	รรรรeร	รรeรรยฅร1รLร>ร)รยรlรLรรฉรLร)รยฅรร	ร)รxร1รRร#ร)รLร	รงYรlรรxรiรรoรงeรeรครรฉรครeรรiร>รLร	รRรwรlรqรรรรLรรกรยรรรร1ร)รร1รง
รYรรรงยรรร)รรรยรรรยรยฅรร	ร LรLร)ร)รLร	รlรรeร)รeรยฃร1ร ร	รรpรข%รรฏรรรLรรจรครกรร)รรร รxรงรรยร	รรจรVร1รง oรร รeร1รยฃร1รรฆรรจรยรWรrรxรรDร+รรรeรรรLรร
รร/รxร ยรlรยร	รรxร1รง8รeรlรRรรรฉรร1ร	ร รฉรRรยฅร)รpรLรLรรรฏรlรรxรiรรรRรรยรยฅร 2ร	ร)ร	รrรยรรร	รรฉรร#รรรxร oรยฅรรร	รรฉรLรlรeรร	ร)รรรยรRรcรยรรรร	ร
ร1รยรรrรxร#รeรรรLรรcรrรDร+ร2ร)รLรOรLรgรรRรรUร)ร 	รยฅร 2ร	ร)ร	รร LรLรxรqรยรรรlรxร1รxร1รeรYรUรรรรeรYรDร1รwรร1ร	รร	รrรร.ร1รร)รร}รรรยรlร
รรLรqรข%ร	รรร cรร)รeรYร ยรYร ยฅรRรร+ร1ร)รxร1รpรxร	รรxรeร ร>รiร>รฅeร	ร)รงsรรรรยทร)ร8รxร	รยรLรรคยร)รLรยรขรeรรiรwรLร)รรร รรร)รรร
ร	รeรงยรครยรรDรฅeรรรรรRร@รรร/รรรLรรYรรxร1รร1รLรร>รรLรรร.รขrรงwร+รiรxรxร1รLรรlรLรยรรLรqรข%ร	รรฉรUร[รรยร	รWร	รrรรรกรคร1ร)ร`รฅรจรรxร1รeรร
รLร)รeร%ร	ร)รxร1รRร	ร@ร	รรยรรร)รร1รขร1รงรขรรรรรรdรร.รLร	ร)รeร
รรรLรรรยฅร รรร1รxรง>รรLรรรยฅร1รLรqรรรรรรจรยรยฅรeรLรร)รร	รrรlร)รeรรงOรยร>ร)รLรรLร)รRรรร	รรรรรCรร1รeรLร	ร รรxร1รxรงยรLร)รRรยฅรiร	รร)รRร
รiรยรxรLรLรรยฅร)ร)รRรTรขรง^ร)รRรรรรยรรWรdร)รยฅร Oร)รรฅeรร	รยรDรร1รยร eร ร รTร)รรiรยรรร%ร	รยรคร4รรDรฅeรpรxรLรรครTร)รรร
รYรDรฃร1ร>รLร{ร	รrร)รlรeรrรงpร	รรpรขรYรwรรรรรdรรร)รrรUรรoรeรยรรeรรLรLรรรยรLรรร	รยฅร)ร	รRรรรรขรร+ร1รรร1ร รร	ร)รรรจรยรยทร	รeรxรRร	ร%ร+ร
ร)รLร>รญรฎ รยรรฅeรร1รฅeรRรรกรeรร1รงwรLรรร)รงwรLร)รRรยฅรiร	รร)รRร	ร ร Oร)รรฅeรร	รรฉรรจร รยรCร eร ยฝรครยรxรLรรคยรlรรรร)รLร	ร)รยร	รรยรข%ร
รeร	รLร	รรDร[รรeรรLรLรรUรxร1รeรรDร.ร)รRร)รLรรiรยฅรLรร)รรรeรWรLรLร)รรร	รeร)ร	รRรOรร[รข%รรVร1รรรฉรยฅรรรqรคร>รรDรฅeร>รLรeร oรLรรร)รง
รLร)รRรยฅรiร	รร)รยรรรงrร>รขรUรยรรร1รยร)รLรยรญรฎรรรรรLรรฉรLร)รยฅรขรยร	ร รร2รรรยฅร1รLรYรร	รeร)ร	รRร#รรยฝรขรร+ร1รร.รยรยรlรรยรร	รeรรรรiรรร1รeรร1รง
รLรรรRรlรiรLรรขร1รeรรรรiรร)รRรรรรยรรครeรรกรLร)รรฅeร	ร`รคร1ร)รLรeรLรยรxรรUรxรiรxรxรiร	รDร[รeร)รxร	ร)รรรยรยฅรรรกรยรpร)รLรยรiรรLรeรรUรeรeร2รรร ร1ร
รรeรรรLรรiรLรยรoรeรYรยฅรร1ร)รรครRร รรรLรขรยรUรLรeรรรeรRรWรรOรรรxร oรeรรร	รยร1รeรUรยรร ยฃรeรยรยรรรรรรรรeร#ร1รยร`รiรรLรยฅรรรeร
รคร1ร)รLรeรLรwรRรยฅรรDร+รยรxรงTรรรยฒรคร1ร)ร รeรร1รง^รร	รLร)ร oรxรคร รยฅรรรรxร+รร	รYรLรRรxรรรยรLร%ร eร ร	รฅยฅร	รยฒร+รqร)รLร	รlรยรiรYรรรeรร
รeร	รLร	รรรจรVร1ร	รRรWรฅeร	รรร1รeรWรรรYรDรฃร1รqรLร ร	รrรlร)รeรรงeรDรยร.รครรVร+ร@รรยรlรLร	ร[รขรรกรรฃgร)รlร	รรร1รงqรlรRรxร)รxรiรร)รRร}ร1รYรรLรร+รiร	รรxร1รeร
รeรรครรVร+ร.รขรรรxรร1รRร)รรยฅร/รWรรeรรLรLรรรรรยรยฅรรDรร)รรรร
  






	





















 

!





"

#

%$

&

'




)(

+*,.-0/213

4

8-

#

913 ,

:

+;

7=

<

-

<

+:

?

;



2=

#=>



@;

B;

5*,6-0/71





7

A=



7



2

C7D

FEGEG H I

 7D

JEGEG ?

#

KML



7

#

N.:

8

OFP

Q%RHS?T#UVWYX#ZF[GUGR#\0V^]`_aSbZFZJUGR

cd&egf0h)i?jklJmf0h
nYo?prqYs3tsu'v%o3tGqYo3txw&y.zY{,|gt}6{Gvy2o#v7~3q6yAq'nYo#u4ย3vq6y)pยqay7{ยuFq6yAtย?ยprq6oยtย{Gย#ย#uFยy6pย{Gย9ย3u'y6zYu'u'ย9|gtJย#ย
pย|ย~#|!u'ยยy2v7{Gs?ยยtย3w&y2o#uvAtย3w?{x|ยzY{Gv6ยrw#qYts#s#v7{ยtGย2o&ย{Gv
tยqยpยยGย?pยMย'tย?y4ยยvAtยยG|u'ย?yย{ยยย{G~#vYยrtย#ยG~3tยยGuGย
{Gย#uยยยtv4v6prย2o#u'v+y7o3tยยy2o3tyยย{xย3qยprw?u'v7uFw>ยยยยยtvยpยq4tย3w>ยu'ย3ย{ยHqยยยt>ยAยกFยขxยฃGยข?ยค{Gv,ยฅGo3tGq6y7v6p)ยAยกFยขGยฃGยขHยคA%nYo#u
q6uFย{Gย3wยprqYy7{gq6~#ยxยGuFq6y%y7o3tยyaqย~3ย7otv2uFq6~?ยยy%prqยฆ~#ย?ยยงpยยGuยยย>y7{{Gย#yAtยpยจย&ย{Gv%y2o#u4ยย~?ยย^ยยtยย#ยG~3tยGuG
nao#uยtGยy+y7o3tยy+zยuยo3tJยGut9ย{Gย#ย#uFยy6pย{Gยยฉย3u'y6zYu'u'ย|gtJย?pย|@~#|ยชu'ยยy2v7{Gs?ยยtย3w>vAtย3w?{x|8zย{xv6ยrw#qยฆpยq
qยpยยGย?pยMย'tย?yF
ยซb{Gv%{Gย#u,y7o?pยย#ย3ยbpยจy+tJยยย{z
q%~3qยฆy7{~#yยpยงยpยยฌ'ugtJยยy7o#uy2{ย{ยrqยฆy7o3ty%o3tJยGu+ยu'u'ยw?u'ยGuยย{Gs3uFw>ย{Gv
ย{G|s#~#yยpยจย#ย|gtยยยpย|ย~#|ยญu'ยยy7v2{Gsยยuยฎยย2pยu'ย?y6ยยยยฏย6q6u'uยย2ยฐ
{ยrw?|gtย0ยYยกFยขxยฃ?ยฑGยคยtยย3wy7o#uย~#v7y7o#u'vv2uยยu'v7u'ย3ยuFq
y7o#u'v7upยยยฒยคAย?tย3w,|gtJยy7o?~3qยณยยจuFtxwยy7{+uยฎยย2pยu'ย?yยtJยยยG{Gvยpยจy2o#|gqยดยย{xvยตย{G|s#~#y6pยย#ย@w?u'ยGv7u'uFq^{ยยMย3uยpยuย0ย{Gvtยฆยยtยv7ยGu
ย2ยrtGq7qY{ยยยยยย#{zYยยuFw?ยGu+ย3txq6uFq'ยถยยยtGw#wxpยy6pย{Gย0ย3|&tJยยpย|ย~#|ยทu'ยยy2v7{Gs?ยยprqYยยย#{z%ย9y2{@o3tJยGu4|gtย?ยgty2y7vAtGยy6pยยGu
s#v7{Gsu'v7y6pยuFq&ย.ยธxtFย?ย#uFq'ย%ยกJยข?ยฑยยฃ?ยคAยบยน%~#vv7uFq6~?ยยyq6o#{z
q,y7o#uFq6u&s#v7{Gsu'v7y6pยuFqtv7u9q6o3tv2uFwยฉยยยy2o#ugvAtยย3w?{G|ย
zY{Gv6ยrw#qts#s#v2{ยtGย2opยยยฏy2o#uยw?{G|gtJpยยยปz%o#u'v7u>y7o#uFq6u9y6zY{ยts#s#v2{ยtGย2o#uFqtยGv7u'uGยฉยถยย3w?u'uFwยฒย%tGqq6o#{z%ยpยย
ย.ยผYtGย'ย2oย~3qยu'y@tJยยย)ยกFยขGยขยฝbยคAยยฒy7o#uvAtยย3w?{G|ยzY{Gv6ยrw#q+ts#s#v7{?tGย7oo3tGq+|gtยยยย>{ย%y7o#uFq6us#v7{xs3u'v7y6pยuFqยย{Gv+y7o#u
ย~?ยยงยYยย#{Gย#ย~#ย3tยv7ย3ยค^ยrtย#ยG~3tยยGuG
ยน%ยy7o#u
{xy7o#u'vo3tย3wยฒย#t+ย?~#|ยย3u'v{ยยยณs#v2{Gs3u'v7yยpยจuFq){ย0|gtยยยpย|ย~#|ยพu'ย?y7v7{Gs?ยGยยqย~3ย7ogtxq^pยyAqw?u'su'ย3w?u'ย3ยu
{Gยy7o#ugย2o#{prยu{ย)ยrtย#ยG~3tยGu&tย3w>pยyAqยฆpยย3tย?pยpยจy6ยยฉy7{9o3tย3wxยยugย't~3q2tJยv7uFtGq6{xย?pยจย#ยts#s#v7{Gs#vยpยtยy7uยยยGย0o3tJยGu
ย3u'u'ยIq6u'ยGu'v7uยยยยฟยv6pยy6prย2pยยฌ'uFw`ย.ย^uFtv6ยยยกJยขGยฃGยฃ#ร,ยฐ
{ยยยw#qยยฌ'|ยprw?yยu'yยtJยยยยยกFยขGยขxร?ยคAรร%{xyยq6~#v7s#v6prqยpยย#ยยยยGย,y7o#uFq6u
ยv6pยy6prย2prq6|gqtยs#s?ยยจยy7{ยvAtยย3w?{G|รzY{Gv6ยrw#q,tGq+zYuยย9ร!wxpยq2ย~3q7qยpย{Gยยฏ{ยยยy7o#uFq6u9ยv6pยy6prย2prq6|gq'ย)tย3wzยฆo#u'y7o#u'v
y7o#u'ย&v7uFtJยยยยยq6o#{x~?ยยwรย3u,ยGpยu'zYuFwยtGqยฆq6o#{Gv7yAย{x|ยpยย#ยยqY{ยy7o#u+vAtย3w?{x|ยzY{Gv6ยrw#qY|u'y7o#{#wยฒย?prqยฆย3u'ยG{Gย3wรy7o#u
q7ย{Gsu{ยy7o?prqยs3ts3u'vFรยดy7o#u,pยยยy7u'v2uFq6y7uFwv7uFtxw?u'vq6o#{G~?ยrwย{Gย3q6~?ยยyย.ยผYtGย'ย7o?~3q4u'y+tJยยยยกFยขxยขยฝ3ยยดยฅGuFยy6pย{GยยปยฑGยค
ย{Gv
t,|{Gv7u+y2o#{Gv7{G~#ยGoยwxprq7ย~3q2qยpย{Gย{ยยy7o#uFq6uยprq7q6~#uFq4tย3w9tGw#wxpยy6pย{Gย3tJยv7uยu'v7u'ย3ยuFq'
รu9ย3uยpยu'ยGuy7o3tyย{G~#v{Gย3qยu'v7ยยtyยpยจ{xย3qv7u'ย?tvAwxpยย#ยy7o#u&ยpย|ยpยyAq{ยยy2o#uยย{Gย#ย#uFยy6pย{Gยยฏยu'y6zยu'u'ยรy7o#u
vAtย3w?{x|ยzY{Gv6ยrw#q,|u'y7o#{#wรtย3wยฉ|&tJยยpย|ย~#|ยญu'ยยy2v7{Gs?ยยบtยv7uยtยยยqย{ยบq.pยจยxย?pยงยMย'tยยยyFยฉnYo#uยรx~#uFq6y6pย{Gยยป{ย%o#{z
zYprw?uยยย|gtJย?pยจ|ย~#|8u'ย?y7v7{Gs?ยยts#s?ยpยuFqยpยq+รx~?pยy7u,pย|s3{Gv2yAtย?yFรtJย?pยจ|ย~#|รu'ยยy2v7{Gs?ย9o3tGq4ย3u'u'ยย?tJpยย?pยจย#ย
s#v7{G|Bpยจย#u'ย3ยu&tGq
t&|uFtย3q%{ยยยw?uFtJยpยจย#ย>zYpยy7o~#ย3ยu'v7yAtJpยย?y6ยยย3{xy7o>pยจยร4ยถยtยย3w9{Gy7o#u'vtยv7uFtGq'4ร%{zYu'ยGu'vFย
y7o#uwxpยงยฎ9ย~?ยยจyยpยจuFqร{ย~3qยpยย#ยยฉy7o#uย|u'y2o#{Hwร{Gย3ยu>zยu9|{ยยxuยy7{ย#{xย#ย~#ย3tv7ยยปs#v7uFwxprย'ty7uFq9qยu'u'|รย#{Gy&y7{
o3tJยGuยย3u'u'ยยย~?ยยยยรts#s#v2uFย2pยtยy7uFwยฒ&ยถยv7u'y2v7{ยq6suFยyFย0y7o?prqยpยqยย#{Gy+y7o3ty+o3tยvAwy7{9uย#s?ยrtJpยย0ร^pยยรtJยย|{?q6ytJยย
ts#s?ยprย'ty6pย{Gย3qYzยฆo#u'v7u+|gtJย?pย|@~#|`u'ยยy2v7{Gs?ยo3tGqยu'u'ย9~3q6uFw>ย.tย3wBz%o#u'v7u4pยyAqYts#s?ยprย'ty6pย{Gยยย'tย&ย3u+ยuFq6y
}6~3q6y6pย3uFwรpยจยy7u'v2|gq%{ยยy7o#uvAtยย3w?{G|ยzY{Gv6ยrw#qยฆ|u'y7o#{#wยคยy7o#u,ยยย#{zYยยuFw?ยGuยย3tGqยuprq4w?uFq7ยv6pยย3uFwยpยจยy7u'v7|gq
{ย~#ย3tv7ย&s#v7uFwxprย'ty7uFqยย{GvFยbuFรG~?pยยยtยยยจu'ย?y6ยยยGยยฒ~#ย3tv7ย&ย~#ย3ยy6pย{Gย3qยฆzYpยy7oยt,ย3ย?pยy7uยvAtย#ยxuยคAยซ#{xvยuย3t|s?ยยuGยbpยย
s#o?ยHqยprย'qยts#s?ยprย'ty6pย{Gย3qยzYugtยv7u,pยจย?y7u'v7uFqยy7uFwpยยยปq6~3ย2oรs#v7uFwxprย'ty2uFq@txq+รG~3tย?y7~#|ยญq6yAty7u&ย.qยu'uยย.ร4u'ย?ย?pยยGo
ร

ร4u'ย?ย?pยจยxo0ยยยกFยขxยฃGร?ยค7ยคA5ยฅpย|ยpยยtยv6ยยยGยยฒร
ยถ%ts#s?ยprย'ty6pย{Gย3q+tย3w&uย#s3u'v2y
q6ย#q6y7u'|gqYy6ย?s?prย'tJยยยจย>~3q6u+{Gย?ยยย9~#ย3tv7ย

s#v7uFwxprย'ty7uFqqย~3ย7oยtGq0q6ย?|s#y7{G|&q0tย3wwxprq6uFtxq6uFq)ย7รo#u'uFq6u'|&tย0ยยยกJยขGยฃGร?ยคAยฒรuยq6~3q6suFยyยy2o3ty0y7o?prqยณpยq^ย#{xyยtย
tGย'ย2prw?u'ย?yFย#tย3wy2o3tyยw?u'u's&s#v7{Gย?ยยu'|gq)zYpยยยtv6prq6uยฆpยจย&|{Gv2u%ยGu'ย#u'vAtJย0ย'tGq6uFq')nYo?prqs{ยq6uFq)t@ย2o3tJยยยu'ย#ยGu+y7{
s#v7{Gs{Gย#u'ย?yAq){ย0|gtJย?pยจ|ย~#|`u'ยยy7v2{Gsยยqยpยย3ยuGย#u'ยxu'ยpยย{Gย#u+tGย'ยu's#yAq)y7o#uยฆ|gtJย?pยจ|ย~#|ยu'ย?y7v7{xsยยs#vยpยจย3ย2pยs?ยยuGย
y7o#uYwxpยq2ย~3q7qยpย{Gยtย3{ยGuqย~#ยGยGuFq6yAqยณy7o3tyยฒpยy^|gtJยยqยpย|s?ยยย,ย3upยย3ts#s?ยprย'tย?ยยuยฆpยจยยtYยrtv2ยGuย2ยrtGq7qยด{ยยpยย?y7u'v7uFq6yยpยจย#ย
uย3t|s?ยยuFq'

รรร%ร hร5mร

ร

dรรGffยณรยlรร7f0รร ร i?รmf0hรรdยร

รกยรขยฒรฃ?รคMรฅJรฃ3รฆรจรง0รฉรซรชยฒรฌรรญรฎยรฏรฑรฐAรฒยฆรณรตรดรฐรถยรทHรธรน&รบรซรปยรผรฝรพรฟรบ+รฏรฑรทยรบรซรฎยรนGรธรฏรฑรปรด&รนยฆรณรตรดรฐรถยรทHรธรน&รบรซรปAรนรปยฒรดรป3รบ	รนGรธJรณรตรดรฐรถ
ยรดรฐ7รฏ
รดรฎรฏ'รฐ?รฏ'รฐ7รฏยรบ@รนรปรรฏ
รฏรซรบรซรฎยรฏ3รฐ7รดAรฏรท?รฐยรฏยรนรบรซรฎยรฏรฑรปรรน4รณรตรดยรฐAรถยรท#รธยงรน
รณ'รดรฐAรถยรท#รธรน%9
$ รบrรปรนรปMรดยรป3รบ	AรนGรธJรณ'รดรฐรถ


&'

รผ

รฝรพ

Aรดยรป!"รซรฐAรท#ยรนรปรรฏ'รท?รบrรฎรนGรธรฏ'รป

()#*#+,.-0/1,#2#3+4)#*#+65)#7#89-:#-<;=*#>#2?,.@BA

CEDFGF!HIJLKNMPO#QBRSO#QTRVUWQXKYZK[UP\^]K_^`UWabBc!Med	QaWf<gihkjmlnUWQobBcpKqrs\^]TbT_tK"csUud	QaWfors_vbw\tck["bxc#Qcs\v["b_
d	QaWfkyLJKpz!aMPUoaWK"c!bBfK

]TbBaP\vbB{s_^KM|\}dNc#K[KMWM~bBaW`uMPQยUWO!bBUib_}_ย]TbxaP\vbB{s_^KMir!MPKยL\^cSgยbBaWKkยย\vMPUย\tc![U

ย \ยyยKy^ยc#QoUยRuQEqยr!bBcsUP\}z!K"aM"ย\^c![~_tr!ยย\^c#ยpย#aWQยยQaWUP\^QcpKย?ย#a~KMWMe\^Qc!M"sK"]K"au{s\^c!ยpUWO#KยMWbBfiKย]BbBaP\vbB{s_^KEMP`sfiย
{!Qx_ยยy
JLK

c#Kย?UpUWabBc!MยdยQa~fยg%\tcsUWQ1bBcยKqยrs\^]Tb_^K"csUoยยยยXdยQยaWfors_9bยgยVhยjmnu
ย uRยO#K"aWKยb%ยbBUod	QaWfors_vb

\vMpQc#KkRO#K"aWKยc#Qยqยr!bBcsUP\}z!K"aM

ย \tc![~_^r!ยย\^c#ยVย#aWQยยQaWUP\^QcVqr!bBcsUP\}z!K"aMยiO!b]K

R\tU~Os\tcยUWO#K\^akMW[Qย!Kยb

[Qc!MPUbBcยUmQa]BbBaP\vbB{s_^KยQUWO#K"amUWO!bBciU~O#Kย]BbBaP\vbB{s_^K ย MยยUWO#Kยqr!bxcยUP\}z!K"am\^UMPK_}dย{s\tc!ย#My

ยยยก

QUWKยUWO!bxUย\^c

UWOs\vM

UWabxc!MedยQยaWfbBUP\^QcpRuKEยsQic#QUยa~Kqrs\^aWKXUWO!bBUยgo{!Ko[~_^QยMPKยGymยขย_vMPQ!.Q{!MPK"aW]KยU~O!bBUuยbBUWc#KM~Mm\^fiยs_}\tKMwUWO!bBU
UWO#K"aWKEbxaWKยc#Qic#KMPUWKย%qr!bxcยUP\}z!K"aMy}ย
JLKยฃยsKz!c#KoUWO#KEU~abBc!Med	QaWfbxUP\^Qc

{ย`p\^c!ยsr![UP\^QcLQckUWO#KoMPUWaWr![U~r#aWKEQBdg#yยคuO#K"aWKยฃbxaWKEUWO#a~K"KEKbMP`

MPUWK"ย!Mยฅ
ยฆ6ยง dยgE\vMbBc

r#c!qยr!bBcยUย\ยจz!Kยยฉd	QaWfors_vbM"sUWO#K"c%gยXยชยg#y

ยฆ

ย gBยซยฌigBยซยยซยยยXยชยgBย ยซ
ยฆ

ยWยญ gBยซ^ยยยXยช

ยฌgBย ยซยยซ

ยญยย gยsยy

ยข_ยจ_ยUWO!bBUaWK"fb\^c!M\9MU~Q[Qc!Me\vยsK"aEqยr!bBcยUย\ยจz!Kยยฉd	QaWfors_vbMuQBdยUWO#KwdยQa~fยฏยฎยยฐยgBยซ	ยฒยฑ^ยฑ gTยซ	ยฑ^ยฑยณ !QaoยตgBยซยฑ gTยซยยซvยต?ยณ y ยง U
ยด
ยด
UWr#aWc!MmQr#UuUWO!bxUUWO#KEMWbxfiKยUWabxc!MedยQยaWfbBUP\^QcRยถQยaWยท?Mยธ\^ckb_}_ยฒUWO#aWK"Kย["bMPKM"y=JK\}_}_^r!MPUWabxUWKยUWO#KUWabBc!Med	QaWย
fbBUย\tQยci{ย`X_^QsQยท\^c#ยibBUmUWO#Kย["bMPKRยO#K"aWKg\9MmQBdZUWO#KdยQa~fยนยฑ^ยฑ g ยซ ยฑ^ยฑยยณ yยบm`oUWO#Ku\^c!ยsr![UP\^]KยOs`ยยยQUWO#KMe\vM"sRuK
ยด
["bBcbยMWMPr#fiKUWO!bBUยg ยซ \vMยปยbBUyยยผ#QamUWO#Kยย#r#aWยยQยMPKMmQBdZUWOs\vMmย#aWQsQBdยยRuKยยsKz!c#KNbpยฝWยxยพยฟ	รud	QaWfors_vbยUWQX{!KยbBc
bBUWQยfo\v[d	QaWfors_vb

ย \ยyยKy9!Qc#KoQBdU~O#KEd	QaWfยฏร

ย \ยyยKy^Qc#KuQBdGUWO#KmdยQa~fรยฎยยฐรยยyยรGK"Uยร
bBcs`]BbBaP\vbB{s_^KX\^cรร
รยฐ yรGK"U
ร

_^KbBaP_^`

ร

ร

nBรรรรWร รZร{!Kยb_}_!{!bMe\v[ยMPr#{sd	QaWfors_vbM=QBd!g ยซ UWO!bBUยยsQยc#QUยfiK"csUP\^Qc

{ยKยฃbo]BbBaP\vbB{s_^KoQaย[Qc!MยUbBcsUยMP`sfยฃ{ยQB_ยฒc#QU\tcยยฉ
รยฐ UWO!bBUu\vMfiK"cยUย\tQยc#Kยp\^cยg ยซ y

fยฃr!MยUยQ?["[r#a\^cMPQfKE{!bMe\v[oMPr#{sd	QaWfors_vbpQBdgBยซvGMWb`

\^U\vMKbMP`kU~QรMPK"KoUWO!bxUยร ยซ
ร ร

ยยร ย~ยbย#aWQยยQaWUP\^Qc%d	QaWfors_vb#ยQaNbpqยr!bBcยUย\ยจz!Kยยฉd	QaWfors_vb

nBรรรรWร รGรรยy

["bBc#c#QUยfiK"csUP\^QcbBcs`

nBรรรร~ร รGรรic#QUยfK"cยUP\^QcbBcs`

ร ร

ยง cยQUWO#K"aRuQaย#M"ยc#QUQcs_^`ยยsQ

รZยซ	yยยบm`kU~O#Kย\^c!ยsr![UP\^]KpOย`sย!QU~O#KMe\vM"

]TbBaP\vbB{s_^Ko\^cรย
รยฐ bBc!ยยMPQย!{s`ย[Qc!MยUWaWr![UP\^QcZย\^U\vM\^c

UWO#K"`รbT_9MยQร[QcsUb\^cยbT_ยจ_ยQ#["[r#aWaWK"c![KMQBdยUWO#KpQUWO#K"ao]BbBaP\vbB{s_^KMbxc!ยร[Qc!MยUbBcsUM"y

]TbxaP\vbB{s_^Ko\^c0ยฐร
ร G{#r#U
ยยยก

QยUP\v[K

UWO!bBUoUWOs\vM

bBaWยยr#fiK"cยUdยb\}_vMw\ยจdUWO#Ki_vbBc#ยยr!bBยK[QcsUb\^c!MXbBcย`ยOs\^ยO#ยยbBaย\tUP`ย#a~Kยย\9["bxUWKM"=\^c![~_^r!ยย\^c#ยรKqยr!b_}\^Uย`y%ยผ#Qa
UWO#K"cg ยซ

fo\^ยOsUย\^c![~_^r!ยsKkMPr#{sdยQยaWfors_9bยMยQBduUWO#Kod	QaWfรร

Qr#UMย\9ยsKยรยฐ
ยก
ร ยซnmร

Ru\^UWO

QBRE_tK"Uร
รรร ร

ร รยซ

ย ยฐ รWร ยยถQaEยฐยยช

ร ZROs\9[~O1["bBcยfo\}ยร]TbBaย\9bx{s_tKM

U~O#QยMPK\^cรยฐร
ร y}ย
n รรรรWร รยรยรZ{!Kยb_}_#UWO#KEร"bBUWQfpMWรuQB]K"aZร
RยO#K"aWKยร ยซ
ร

\9MK\^UWO#K"aร
ร

Qยa

ยญ ร

ยก
y

ร

n รรรรWร ร ร yยยคuO!bBUร\9MBRยถKย[Qc!Mย\9ยsK"abT_ยจ_sd	QaWfors_vbM

QBRร[Qยc!Me\vยsK"aยUWO#Koยย\vMรยr#c![Uย\tQยcZยฅ

ร ร
ร
ร l

ย ร
n

ร

ยฑ^ยฑ g ยซ ยฑ^ยฑยยณ ย ร
ยด
ร

ยคuOs\vM\vMยMPr#a~K_t`ยKqยrs\^]Tb_^K"csUยฃU~Qยยฑ^ยฑ g ยซ ยฑ^ยฑ ยณ G{!K["bBr!MPKpMPQfiKiร
for!MPUย{!KUWaWr#KyEรQBRยถK"]ยK"a#\}duRยถKibM~MPr#fiK
ร
ยด
\vMยU~aWr#KZRuKi["bBcMe\^fiยs_}\}dย`รกยฑ9ยฑ g ยซ ยฑ^ยฑยณ {ย`ยaWK"ยs_vb[~\^c#ยยbT_ยจ_uUWO#Koร
MPr#{sd	QaWfors_vbM{ย`
ร
ร
ยด
ยรขรฃ?รคmQaZรฅ"ยรฆรงยพ"รค"ยb["[Qaยย\tc#ยXUWQEร
y ยPยก QU~KยถU~O!bBUยUWOs\vM=\vMub_}_^QTRuKยoQยcs_t`o{ยK["bBr!MPKUWO#Kยร
ยsQEc#QยUfiK"csUP\^Qc
ร
ร
ร ยฑ^ยฑ g ยซ ยฑ^ยฑยยณ ยยฒ[Qc!Me\vยsK"abx{s_t`y
bBcs`ย]TbBaย\9bx{s_tK\^c6ยฐG
ร ยyยยคuO#KuaWKMPrs_^U=\vM=UWO!bBU=RuKย["bBciMe\^fiยs_}\ยจd	`oKb[~O|ยย\vMรPr#c![U ย ร
ร
ยด
n รรรรPร ร ร รxยUWO#K"aWKยR\ยจ_}_{ยKEc#Q [Qc!MPUbBcsUM
ร
ยฉ
c

d
ย
b

[

U
#

!
{

K
"
[
x
b
!
r
P
M
o
K
B
Q
ยธ
d
ย
Q
#
r
ย
a
#
ย
W
a
"
K
ย
]
^
\

Q
!
r

M

Q
!
{
P
M
"
K
W
a
B
]
B
b
P
U
^
\

Q
ย
c
x
b
!
{

Q
#
r
U
ร
ยง
UWO!bBUEbpย!bxaWUP\v[rs_vbBaยฃร

Qai]BbBaP\vbB{s_^KMQr#UMe\vยsKVย
รยฐ
_^KdยUpRu\^UWOs\^cยUWO#K

ย#a~Qย!Qa~UP\^Qcรกqยr!bBcsUP\}z!K"ayยยคuOs\vMp[Qfiยs_^K"UWKMUWOs\vMiMPUWK"ย1QBd

UWO#Kw\tc!ยsr![Uย\tQยcZyEรจB\^c![KiU~O#KEQUWO#K"aยqยr!bBcยUย\ยจz!K"aMย["bBc%{!KoUWaWKbxUWKย
aWKMPrs_^Uy

รฉรช

Me\^fo\}_vbBaP_^`ZUWOs\vMย#aWQB]KMUWO#KยยbBU~c#KMWM

รซยรฌ?รญsรฎ#รฏยรฐยรฑuรฒ#รณรดรฏรฌ#รตZรฐ=รถรธรทรญ.รณรณรฏรฌ

รนeรบยรป#รผBรฝVรพWรฟ^ รปu
 รบWรผ
#
	 รผBรฝ#
	 รผBรฝB
 รบยรผรพxรป!รฟยรบWรพxรปยรผยรพiรฟ รบWรผBรป#รผรป!ยรผรพ#"%$&('()
'!รผPรฟ*,+.-%/012Bรบ"43Gรฟ"รบ5*768+.-:0;
9 !รฟ รบ	#รฟยรผรพ%รฟ <=?>@^รฟ"รปยรบpรบWรผ8*ยรผ=#รบ^รป#รฟ ACBD^รป(Eรบ	#รฟ
รบWรพx
 รปB
 รบ
^ รผรปiรผ@F
$ รฟ  รบ
^ รผรป#GH"JI&"LKM
> รฟ"รพB5#
' รพWรผ&ย
' รผรพWรบ
^รผรปรผ&'Bรพ
Pรผยรป^รป* 6 Nmรผ@Zรบ	#รฟO	รผรพQPSRTPVUXWY=รฝZ	#รฟ"รพWรฟ[P
 รป\P U B
B
 รพ~รฟZ!
' รผ@Js
B รป#รผ&5u
 รผ@
> รฟ"รพMB
 รบ:#
 รป รผรป=^ รบ
^ รผรป]L#
' รพWรผ&ย
' รผรพWรบ
^ รผรป=
" รนeรปV& รบ ^=P U SJ'JB2#
' รพWรผ( รบรผ@
B
 รบZ#
 รป รผยรป=t รบ_t รผยรป:#
' รพWรผ&ย
' รผรพWรบ
^ รผรปย` รฝZ#
	 รฟ"รพWรฟXรบ#
	 รฟยรฟ'#รบ
B#
' รพWรผ( รบZ
 รบ@
a รฟ"รป รบ~รผ!
 รฟI@b"S
c รผรบWรฟdP รผpรบ	B
 รบ
^ รป รฟpรฝยถรฟ^ รฟ B
 รพWรฟ A
 รฝZ B# รผยรป=t รบ_t รผยรปZ#
' รพWรผ&!
' รผยรพWรบ
^ รผรปeB#5^ รบ
J'JB&^ รป(E4BfP U ^gXZP U%hji รบ#
	 รฟ"รปAP รผePk^
 รปP รผรบ#
B
	 รฟ[ย รผยรพ5NlPmRTP U W Y N[@#
 รบWรผ=Bรบ
XJB รบWรพ#
 รฟ&"o
n รฟdB
 รป%รบ#
	 รฟ"รพWรฟk	 รผรพWรฟยรพWรฟ'& รฟXรบ#
	 รฟd รผ&'x
 รพ
P รผรป
BT`VPVU hpi bFqD`VPRrPUsWtYFu#PVUev i b"D$@J5XB
 รพ
JB&ย
^ รฝuรฟB
 รปรรพ~รฟ'N= รฟw%
 รป#รฟECB
 รบWรฟ A รผ&'x
 รพ
P รผรปTB,B
 รป
รฟkx(#
' รพWรฟ ^ รผรป%รผ@ยธ
 รบ#
	 รฟ[ย รผรพzyF`VPSRTP U WY
b{uP U v i "
|#
	 รฟ รป#รฟk?
x รบย รบWรฟ'4
 รบWรผยรพWรฟ"รฝยรพ
^ รบWรฟ#]s
 รบ#
	 รฟB
 รบ5#
 รป รผรป=^ รบ
^ รผรปe#
' รพWรผ&ย
' รผรพWรบ
^ รผรป2^ รป1รบWรฟ"รพoรผ@}x
 รบWรผ&5
' รพWรผ&ย
#
' รผรพWรบ
^ รผรปtV
" รนยรปx
 รปCBT
	#
' รพ~รผ&!
' รผรพ~รบ
^ รผรป~J~ * U ~N~ย u
^ รบ#
	 รฟยย รผยรพ5Nย* U NA
ย รผsรผ@^ รฟ B
 รป1 รผ&5t รปx
 รบ
^ รผรปยรผ@
ย
ย2`ยยยYยbF	 รผรพZ#
' รพ~รฟ =Nx
 รบWรฟ eยย+ยยjB
 รป8ยY:+.!
ยย "m|	C.
^ รบ#
	 รฟe	 รผรพ55*@Ugย
 รฟ <=J>]^ รฟ"รปsรบยรบWรผ=ยย
#
 รป รบ
^ รผรป
ย
ย
ย
ยOย `ยย 0 `ยยYยtb:u,ยย ย%uwย
`ยยYJยObbย
^ รฝZ#
	 รฟ"รพ~รฟ รฟ &	Tย Y B
 รปTB
 รบWรผ& รผ]ย
> รฟ"รพยยx
 รปQยย hzย ยYยย ย ยยยยยYJยZย&"
ย
|S#
	 รฟ P รฟ5=ยยย#
 รป รบkยB
 รพ~รฟe#รบX?B รฟkxJJ
> รฟB
 รปp
 รบ#
	 รฟdP รฟBรปยรบ_N
 รบWรพWรฟ x
 รบS=Nย รบ
^ รป รบย>@B
 รพ
@^ รฟ e&Sย
 รฟkt รป(E
^รปs
 รฟ!
' รฟ"รปs
 รฟ"รปsรบ ^{P รผ
ย
ย
ย
~N~ * U ~J~ย hย
~J~ยย Y `ยยยb~J~ ย
ย
ย
ย Y 0
9
n รฟ5!

' รฟ"รพ_ย รผรพยนรบ	
 รพWรฟ'& รฟรฟ"รปยรบย รผรพรฟ &	##
' รพWรผ=!
' รผรพWรบ_t รผยรปยรฟkx(#
' รพWรฟ ยt รผยรป{"dย(#
 รพWรบ#
	 รฟ"รพiรผรพWรฟ&^gB
 รปB รบWรฟ"รพยP U ^ รป
 รป1รฟkx(#
B
' รพWรฟ ^ รผรป1รผ7ย
 รบ#
	 รฟ	 รผรพยPRrP U W Y รฝSXXย!
 รฟf##
' รพ~รผย รบiรผ7
1
	 รฟkx(#
' รพWรฟ ^ รผรป^SB
 รปTP รผยรฝSXXยย
 รฟ
' รผ^ รบ
J
!
> รฟ&"
c รฟk#

x รบ #
^ รฝuรฟe5P รบZ'(#
 รบeXย'(#
 รพWรฟ!
ยก รพP รบ	) รผยรพs
 รฟ"รพS	 รผรพ5&%^ รปยรบ#
	 รฟยรพ
JE&s
	 รบZ	 รผรพ#"M
n รฟe!
ยก รพย รบยรพWรฟ"รฝยรพ
^ รบWรฟ2i
* รบWรผ
'(
	X#
 รป#รฟECB
 รบ
^ รผรป!^ รปsรฝx
 รพ(L=gVB
 รพF&{ย
' รผCJ^ รฟ&^Cย รผยรบ	x
 รบยฒรผรปJB5B
 รบWรผ&5%
(ย รผยรพ5N=LB
 รปX
 รฟkxP รบWรฟ"รปsรบ

	รผรพ5&SB
 รพWรฟยรป#รฟECx
 รบWรฟ g":
c รฟk?
x รบ ย
^ รป#รผรบWรฟรบ	B
 รบZ^ รป รฟe*eMB
 รบ s
^ รฟ &p
	 รฟkxCP รบWรฟ"รปsรบ
L
(	 รผรพ52ย รบ:	
> รฟ
รบ#
	 รฟย รผรพzยขCยZ*@Uยฃ#
^ รฝZ#
	 รฟ"รพ~รฟ*@Uยคe<=B
 รปยรบ_s!
ยก รฟ"รพ)ยย รพWรฟ"รฟ2	 รผรพ5o
 รฝZ		wรฟ"รปยรบ
^ รผรป
 รป#รผ รผรปP รบB
 รปsรบZx
 รป รผรปJB
รบ#
	 รฟ5>]x
 รพ
@^ รฟยฅย!"dย
ยฆ รฟ"รป รฟ&^!* U dm
ย รผยรผ7t รฟ x
 รป, รผ&5^ รปB
 รบ
^ รผรปรผ@mย2`
ยgbS	 รผรพe#
' รพWรฟ =B
 รบWรฟ 2ยยง+4ย"eยจ[EC^ รป{^
รบ#
	 รฟย รผรพ5* U 
 รฟ <&J>@^ รฟ"รปยรบXรบWรผ=ยย
#
 รป รบ
^ รผรปรผ@:B
 รบWรผ=uรผ@ย
 รบ#
	 รฟe	 รผรพ ย[ยฉ{ยช@ยซLยฌJยญยฎ ย`ยยยb^P รผยขCยO* U N
รฟ <=?>@^ รฟ"รปยรบXรบWรผ ย ยฉ{ยช@ยซLยฌJยญยฎ ยขCย[ย`ยยgb"O
n รฟoรพWรฟ'& รฟยขCยZ* U k
B รบ	N
 รฟkx(#
' รพWรฟ ยt รผยรป{"dย!^ รปX?B&ร
^ รฝuรฟd5P รบes
 รฟ 
รฝS^ รบ	ยย
 รผรพ&
 รผ@
 รบ#
	 รฟย รผยรพpย2`ยยฏu
b รผรพeyLย2`ยยฏb%	 รผรพ}ยยฐ+fย"Z|	NO
 รฟ &
BgยฑS
n รฟx
 รปw@EC]t รป%รพWรฟ'& รฟ
	รผรพ55o
* รผ@ยธ
 รบ#
	 รฟ[ย รผรพยฒย2`
ยฏ
b รผรพeyLย2`ยยฏb%Cp
B รบ#
	 รฟd=ยย
#
 รป รบ
^ รผรป ย ยฉ{ยช@ยซLยฌJยญยฎ ย`ยยฏb"
|#
	 รฟ:!
' รฟ"รป^ รบ
JBรบWรฟZP รบ~รฟ'[N=
 รบ~รผย รผรป
> รฟ"รพWรบ{*%^ รปยรบ~รผ}=ยย
#
 รป รบ
J
> รฟuรป#รผรพ]&ย รผยรพ#"L|	Nร
 รฟ P รฟ"รปsรบ
XJBd#
 รพ
^ รป(EC
รบ	^ รป(EC}t รปsรบWรผfB
 รป#รผรปSย รผยรพ#"
c รผรบWรฟoรบ	B
 รบ5^ รป รฟรฝuรฟs
 รฟ ^ รบXรฝS^ รบ	f	 รผรพ5&ย
 รผ@u
 รบ#
	 รฟ5	 รผรพยyLย2`ยยฏbO^ รป
รบ#
	 รฟe#
' รพWรฟ>=^ รผ&[P รบWรฟ'{#
^ รฝuรฟds
 รผiรป#รผรบO	 ย
> รฟยรบWรผs
 รฟ ยฒ
 รฝS^ รบ	f รผรป@ยย#
 รป รบku
 รผ@ย
 รบ#
	 รฟ[ย รผยรพzyLยYย`ยยฏbk"
|#
	 รฟe!
ยก รป:P รบWรฟ'8ย
 รบ~รผยฅ#
	 รฟ %
a รบ	B
 รบยรฝuรฟ5s
 รผรป#รผรบ[	
> รฟย Y `ยยฏbx
 รป รฟk^ รบ#
	 รฟ"รพ5yFยขCย[ย Y `ยยย
b รผรพeย ย `ยยฏb:	 รผรพ
 รผ&รฟZยณ#hยถ
P
ยด ยต &S รผรป@ย
#
 รป รบย
 รผ7MP รผ=iรฟd=ยยย#
 รป รบ
" รนยทยธ
 รฝuรฟds
 รผ#
^ รฝuรฟdJ'JB รพWรฟรผ]ย
> รฟยรบ	x
 รบย=ยยย#
 รป รบ"

ยธfยน:ยนZยบยคยป:ยผMยฝยฃยพรยฟfร5รร&รFรgรรยร_ร!ร4รFยบรร@ยฝVร{ยปQรFรยร
รFรยครAรwร#ร!รยฃรรรรร{รCรtรรeรร รยรร{รร@รรFรkรCรgรก รยรยฃร@รรข8ยฑ@รฃ รครฆรฅยรฃ รครรง@รรจรยรฉ:รlรรชรยรkรรซรกรยฃรฌรฎรญZรฏยร7รรยฃรยรรฐ@ร%รฏยร&รฌรฎรญ@รร7รรยฃรง&รฌ

รkรCรgรก รยรยฃร@รรdรฑยคย
รฒ4ยฑรฃ รครณรฅรณรฃ รดยรร(รกร4รยรCรง=รยรต{รรชร7รรถeรทp+,-%/0 รง7รรจรธ ย +Tรน5รบ2รตZร รรป\รฉ:ร@รรชรฌXรจ@ร รผรฝ5รพ รธ
ย รฟ `รถeรทdb5hร
ยด i รต
รยรCรtร
รฝ  ยฌ ยฎ

รฝ ยฌ  ยฎ

 R1รปรฉ:ร@รรฌXรจ@ร รผรฝdรพ รธ
 ย
`ยรขย`ยรคDb ]รฑ!`ยรคDbb 
ย รฟ `รถeรท2b:Rยถรขg`
รค4bยทรฒ`ยรคDb
+  รฝ Bรบ
ยฃB=tรป(Eยรถeรทร
	kรบ	xรบM`Qb h รธ ย ^#รฝuรฟย5PรบZ'xรพWรบ
^รบ
^รผรป

รบ	#รฟesรผ&^รป@iรผรป(EXรบ	#รฟeBรบWรผ&F=รผรพ=^รป(EiรบWรผXรบ	#รฟZ'#รพWรผ&'ยรผรพWรบ
^รผรปM^รป8รธ ย ^Bรปรบ	#รฟ"รปยฅ	#รผsรผCPรฟeBรปยฅ=JEรป()
iรฟ"รปsรบS	รผรพuรบ	#รฟdรผรปPรบxรปยรบM^รปkรบ	#รฟ[Nx
 รป(E&@
E รฟd
(Hยย รฟ  รบยรบ~รผiรบ#
	 รฟd รผรปP รบ~รพ^ รปยรบkFJ'ยรผCPรฟ ยCB8รถeรท":ย!^รปX?B&^

	kรm|รรผ	#รผยรผPรฟdXรฝยถรผยรพ




!#"$%&'(*)+,-./1023%4!576

8:9;8:<>=?@;ABC?EDGF1HI8:<J=LKM@;<N(@;<JOMPQA<R7STPUS8VCKXW:R7=8N:YZK\[=?8:S8IR7ST8]R7<JP+<@C<^_A<R7SPQUST8VCK.W:R`=8NaKM<
=?8G9;@W:R7bAJOXR7SPc8GHdANL=eWT?@f@fNg8
RIVJ8:<@;=hR7=gKi@C<[_@;S=?8:HQj
k AUUZ@fNL8m$
l npoqmr7stttsTmuGvhY2R7<VwOi8:=dxzy{n|m!y}x~[_@;S
znยย;stttLshย*jยย?8+<JAHdb8:Sย@7[eUR7S=LKM^
=LKM@;<N@`[ย=?8GVJ@;H+RKM<ยKM<f=T@+R7=@;HยN2KXNdย

ย
8R;WT?QNLAW?UR`S=LKM=LKM@;<QWย@;HIUJOM8:=8ยOMPVJ8:=8:SHdKM<8Na=?8
ยยยยยยยยยยย ย ยยยยย7ย
VJ8:<@;=hR`=LKM@;<[_@;Se=?8ยA<R7SPยUS8VCKXW:R7=8N:jaยE8GHdANL=GROXNL@+NLUZ8WTKย[_PQ=T?8]VJ8:<@;=hR`=LKM@;<Ne@`[ย=?8dWย@;<NL=hR`<f=
NLPJHdb@7OXN:jยย?8:S8GR7S8(R7=ยHI@fNL=x*ย ยยceRP4N2@`[ยWT?@J@fNยKM<B
=?8Ng8;jยe<ย=?8e@C=?8:S?R7<VYCc8eยf<@7cย=?8:S8

KXNยR7=2OM8R;NL=ย@;<8ยHI@VJ8ยOยoย*s;Z
lย v@7[2DGFยNLAWT?d=?R7=2ยกยoยยขvยnยmย
l Y;NL@G=T?8:S8=?8:S8eR7=ยOi8RCNL=ย@;<8eW?@`K.Wย8;j2ยฃย<
[qR;Wย=Y;=T?8:S8KXNR7=ยคOM8R;NL=@C<8ec@;SLOXVIยยฆยฅยยงQยจ

NLAWT?ย=?R7=oยยฆยฅ_s;Z
lย vยฉ nยชDGFยซ[ยฌ@CSย8R;W?ยญ@7[ย=?8Iย
ย
ย
ยยยยยยยยยยย ย ย ย ย
ceRP4Nย@7[ยUR7S=LKM=LKM@;<JKM<B=?8G8ยOM8:HI8:<J=hNa@7[ย=?8]VJ@;HยRKM<EoR7<Vย8RCW?>NLAW?c@;SLOXVย ยฅ KXNยฎK.Ng@;HI@;SU?JKXWย=@
ยยขvhjeยฏยKM<RO\OiPc8GH]ANg=ยฐWT?@f@JNL8G=?8]VJ8:<@C=hR7=LKM@;<@7[=?8
<@C<^_A<R7SPUS8VCKXW:R7=8Nยฑjeยฒe@7c8:9;8:SYZmE
l VJ@J8N
<@;=
Wย@;<Ng=ShRKM<>=?JKXN
WT?@7KXWย8+R7<VYยbfP>R;NNLAHIU=gKi@C<ยYย<8ยKM=?8:SIVJ@f8NยDGFIjdย?8:S8ย[_@;S8d=?8I<JAHdb8:SG@7[
rr
NLAWT?ยณWT?@7KXWย8NKXNeNg@;HI8([_A<Wย=LKM@;<ยณยดยตoxยถvยca?JK.WT?KXNยฎKM<VJ8:U8:<VJ8:<J=
@7[m2
l j
ยE8
Wย@C<WTOiAVJ8d=?R`=ยท
x

ร mZ
l ร_oDGFยv
xdrstttsยx
uaยนยชยบยซยปIยผยฝ7ยพ:ยฟ\ร7รย

ร ร
ยบ

ยดoLxEvยธ

x

ย ยย ยธ

ยดoxยถvx

xdrstttTshxยueยน

t

ยฃq=aS8:H+RKM<Na=@d8NL=LKMH+R7=8
ยธ

x

xรร
n

xdrstttshxยueยน

xdrรรxdรรtttqxยu]ร

t

k =LKMSLO\Ki<BZรรN{R`UUS@รfKMH+R7=gKi@C<ย[ยฌ@;Sa=?8([}RCWย=@;SLKXROXN:Yca?JK.WT?ยณNTRPN=?R`=

ยย@I@;b=hRKM<@;ASaS8NLAJOM=YZc8(ANL8

รEรJnยร

ร7ยกยรรรยร
ร7รJร
ohยรรรรohย7รร>vTvht

ยฃq=ยฎ[ยฌ@7O\OM@7c{N=?R`=e8ยรfKXNL=GWย@;<NL=ยR7<f=ยNรasรยชรยร]NgAW?=?R7=
รIรยรGร7รJร

รEร
ยบ

รeรยชรยรยฐร`รJร
ยบ

[_@;S{RO\OยรEjยร(NยKM<B+=?8Ng8
b@CA<VN:YR;Nc8ยO\OยR;Nยฎ=?8([}RCWย=e=?R7={x y

ร

u

ร
x

u

x

ยQร
ร ย
ร

u
yXรr ร ยยร

x*ร

u
x y ยยร ยบ
yXรr 

xdrรรxdรรtttยx
udรยยบ

ยบ
ร(x
u
ร

x*Yfc8GB;8:=ยท
x

ยยณร
ร ย
ร

u
yXรr ร ยยร

t
u
x y ยยร
yXรr Q

ร @รcยY4Wย@C<NยKXVJ8:S{=?8ย8ยร4UST8NNยKM@;<ยณWย@CHIHI@;<ย=@IbZ@;=?b@CA<VN:ยท
x
ร ย

ร
ย
ร

u
yXรr ร ยยร

n

u
Xy รr x y ยยร

ร
n

n
n
รฎยรฎยรฏรฐXรฑยตรฒรรณยรดรตรณ_รถ(รฑ_รทรธhรดLรนรบรฒยรปMรถ{รฑ_รผ7รตรฑยตรฒรรฝ(รป.รตTรพLรฑ

รฟ



รก

u

x ย
u
Xy รr x y ยยร
x

ยยร

x yรครฃ
yXรraรข z
u
รก
ร ยยT
ร รฅยรฆรรง.ยยรจ}ยยรMรฉ
yXรr
ร ร ย+รช
รก

ย

รยรซ;ย`รฌ รTรฅยรฆรรง รฌ รXรฉ nยร ยรญeรง รฌ รฉ t
ร

!"

	

# รผรดLรน_รด%$รฒรรณ2รฑ_รผรด&รฝรรตTรน_รถeรปMรนqรต')(รดLรฝ:รฑยรทTรป+*รตรฝ!,.-0/1324 56 ,รดLรฝรทยรฑ_รดรณ2รฑ_รผรดรตTรนรบรฒรรฑรบรถ{รทTรปรฑ_รผรด7รน_รด8,ยฑรฒรรพรตรฑ_รดยรณ_รถ9(%:7รท<; 5 รฏ
=?>

@BADCFEGIHJLKMON?G?APQHSRUTVC	MOM!G?A

WYXZ?[\0]!^3_`\<ab]c\
d egYhi
f
j g
ย ย e<ยยยh%vยgยย ยDย dfe8gยh
sutwvyx.z%{c|9}~c ย ยยยb
sOt<ย
i kยl9nLoBprq
jBk g kml9nLoBprq
q
n
ย aF^ยยam^ยยB\aXยยFXOยย^3ย<XOยmย<XOยยยFยย\!ย
WยX._X0ยD\ ย ]c_F\ย\<Zยยย<Zcย?XยขยกLaX9Zย<X9ยฃยฅยคย3ยฆOยคยmยกยงZYยFZยจ\aF^ยฉยuยชQ^3\ย^ยยยซยbยX0ยฌยFยB\<Z`ab]!ย?Xยข]_ยญ]!ย3\<X9ย_b]c\^3ย?X
ย<X9ยย<XOยยX9_ยฎ\)]\^3Z?_yZcยฌยฏ\<aXยยZcย3ย\^3Z?_ยฐยยb]ย0Xยยฑ ย ย ยย ย ยยฒยกยงZ ย ]cย0ยย.\<aF^ยยยณX9_bยfยช ย Xยจab]!ย?Xยด\<aXยยฌZcยย3Z ย ^ย_ยต
q
ยFX0ยถb_F^3\^3Z?_Qย
ยท ยธยบยผยปQยฝ3ยพOยฝยฉยฟQยปyรยขรรยงรยฏรfX9\ยซร ย ย ยย ย%รยฒรOร eรรhร+ร
ยน
q
ย^3ยฃยซ^3\Zcยฌ6\<aXOยXยซยยb]?ย0XOย9ยwรZ?n ย<ยฃยน]!ยยยร?ยช
ร ย ร ย ยย ยfรรร ยย
q

รยร
nยร

e8รยย ร ย ยฏ
h รร

ยยยร ย.รfX9\ยซร ย ร ย ยย ย [IX.\<aX
q

รยซรยฎgยรยOร3รรDร+gรรยgยรรร ยย
n

ย รยฎร
ร ร ย ย ยย ย O รยฉรร ย^3ยฃ ร ย ย
n ร ย
q
n
nBร
ยกVaXรยฌรกZcยย3Z ย ^3_ยตยข\aX9Z?ย<X9ยฃรขXOย\0]c[Fย^ยฉยยaXOยร]ยซ\ย^ยยตaยฎ\รย0Z__XOย0\^3Z?_`[IX9\ ย 9X X9_ยยฑ ย ย ยย ย ]c_bยmร ย ร ย ยย ย ย
q
q
รฃยซรค ยธFยฟ+รฅ!ยธbรฆรงรยนรrรจbร
รฉรช0รซยรฌ {| รช }r}fg

รชรญ ~ รf
ย รฎ z%รฏยรฐ รชcรฑ รฏ ร ย ย ยย ยQรฒ ยฑ ย ย ยย ย ร
q
q
n
รฉ<รณยรซยรฌ {| รช }r}ยง)รด0รตยรถ9รทรฏ รญ ร}รธยซ0รน รช }ย} รfย รฎ z%รฏยรฐ รชcรฑ รฏ ร ย ร ย ยย ยfร ยฑ ย ย ยย ย ร
q
q
ย รรฟร e8ร h ยฌรกZยยฏยZ?ยฃ X ร
รยร
รบยรฅ!ยฟfยฟbรป0ร.รผ]cย<\ e ] h ^ยยร^3ยฃ.ยฃ.XOย^ย]c\X รยซรฝ ยฌ ย ย ร ร ย ย ยย ย ยชยง\<aX9_ ยรพ
ยยbยa
q
\<ab]c\ e8รยย ร ย h ร ร ยย ย รฝ \ยซ^ยย.]ยยยฃ Zยฎย\ย^3ยฃ.ยฃ.XOย^ยn ]c\<Xยนยฌย<Z?ยฃ \<aX`ยFX0ยถb_F^3\^3Z?_bย \<ab]c\ ร e8ร h ยฃยซยbย\.n ย<]\^ยยยยฌรกร
e<ยย ยยรFย ย h ยชยZ ร e8ร h%ร +{?} ย e<ยย ยยรย ย h ย ย ยกVaXร^ย_bยย3ยbยย^3Z?_ยร ย ย ยย ยQรฒ ยฑ ย ย ยย ย _Z ย ยฌZcยย3Z ย ย9ย
q
q
_X`ย^3ย<XOย0\ย^ยZ_ยZcยฌรยb]cย<\ e [ h ยฌZcยย3Z ย ย^3ยฃ.ยฃ XOย^ยฉ]\<X0ย3รยยฌรกย<Zn ยฃ ยb]ย<\ e ] h ย LXOย9]!ยยร\<ab]\ ร ย ย ยย ยรฒ
q
ยฑ ย ย ยย ย ]c_bย \<ab]c\%\aXรยIZc^3_ยฎ\)ย ^3_mร ย ร ย ยย ย ]ย<XLย^3ยฃยซ^3\)ยLZcยฌ6]ยยยX ?ยX9_bย0XZcยฌQยIZc^3_ยฎ\)ย ^3_`ร ย ย ยn ย ย ย c^3_bย0X
q ย
q
q
ยฑ ย ยย ย ^ยฉยรยย3ZยฎยXOยfยชF^3\ ยฌรกZย ย3Z ย ยL\ab]c\รร ย ร ย ยย ย6รฒ ยฑ ย ย ยย ย ย
n
q
q
q
ร	Z?ยL\<aXZ?ยยIZยฎยย^3\<X^3_bยย3ยbยย^3Z?_Qยชf\<aXยยต?X9_X9ย)]!ย6ย\ย)]c\<X9ยต?ร Zcยฌ \aXยย<ZยฎZยฌ6^ยย \<Z.ยaZ ย \<aXรยฌZcยยยZ ย ^3_ยต ร



 







   



	



e ^ hยนรฝ ยฌ ร ย ^ยย.ยยย ย ^3X9_ยฎ\ย3รยยยฃยข]!ยยยช\<aX9_ยยฌZ?ยยฏ]!ยย ยย ร ยฑ ย ย ยย ย ยช\<aX9ย<X ^ยฉย ยZ?ยฃ.XยขยยX ?ยX9_bย0XmZcยฌรยbZc^3_F\)ย
q
ย ยย
ย ยย
ย!รOร!ร
+{?} ย e<ยย ยรย ย h ย ยยbยa`\<ab]\OยชFยฌZ?ยร]!ยย g รygยร ยช\aXยฏย0ZFZ?ย
ย ย n ย ยย n
n
n
ร ยย
ย ย
ย^ย_b]\<XOยLZcยฌ ย ย
] ย<Xย]!ยยQ^3_F\<X9ยต?X9ยBยฃยซยFย3\^3ยFย3XOยรZcยฌBยฆ g ]c_bย ย^ยยฃ
c
n
n ร ย
nBร
e ^^ h ^ ยฌ ย ร +{} ย eยย ยยรย ย h ย ]c_bยยด]!ยย%^3\)ยยย0ZFZ?ย)ย^3_b]c\XOยย]cย<Xย^3_F\<X9ยต?X9ยยยฃยซยFย3\^3ยFย3XOยยZcยฌ ยฆ g ยชf\<aX9_ ย ร
ร ย ย ยย ย ย
q
n
ยกLaF^ยยรยยยXO]ยย3ร`ยย ย0XOย \<Zยซยย<Zcย?X\<ab]c\ ย ย ร ร ย ร ย ยย ย ย
q
WยXร[bX9ยตc^3_ ย ^ย\a \<aXรยย<ZFZcยฌยZcยฌ e ^^ h ยช ย aF^ยฉยa ^ยย ย\<ย)]^ยยตaยฎ\ยฌZ?ย ย ]cย0ยfย
?ยยยbZFยXรฟ\<aXรยIZc^3_ยฎ\
ยรร e
ย
ย
ย
ย
gยย
gยยOร!รOร<ย k gยh ^ยยย^3_ +{} eยย รยฎย h ย ยYWYX`ย0Z_bย\<ย<ยbย0\ยข] ย Zยยยย ร
ร ร
ยยbยa
\<ab]c\ ร e8รรh ร ย ]?ย ยฌZcยย3Z ย ย9ย.ยกLaX.ยFX9_Z?\)]\^3Z?_YZcยฌB]c\<Z?ยฃ
^ยฉย \<aX.ยX9\ยZยฌยผX0ย3X9ยฃ.X9_F\)ย ร ยฆ ยOรOn ร!รย ร ยช
ร ยช]c_bย`ยZ Z?_Qย รฝ \Bย<X9ยฃยข]^ย_bย \<ZยขยaZFZยฎยX
\<aXยซยFX9_Z?\)]c\^3Z?_mZยฌ ]c\Z?ยฃ
^ยฉย \<aXยยX9\ ร
ยฆ ย!รOร!ร<ย
\<aX`ยFX9_Z?\0]c\^3Z?_bย Zcยฌร\<aXยดย0Z?_bย\)]c_F\)ย e ยย^3_bย0Xย\<aXยจยFX9_Z?\0]c\^3Z?_ยZcยฌ\<aXmยย<XOย^ยย9]c\<XOย Zcยฌย]cย^3\ร ยตย<XO]c\<X9ย
\<ab]c_ ยฆย^ยฉย ^3ย<ย<X0ย3X9ยc]c_ยฎ\ h ย.W ^3\<aZ?ย\ ยยZFย<ยZcยฌLยต?X9_X9ย)]!ย^3\ร ย Xยนย9]c_ ]?ย<ยยยยฃ.X ยย ^ยฉย ^3_ยญย9]c_Z_F^ยฉย9]ย%ยฌZ?ย<ยฃ`ย
eรฝ ยฌย_Z?\Oยช ย X`ย0Z_bยย^ยยFX9ย ยย ย h ยกLaFยbย9ยช ยย ^ยยยข]ยย^ยฉย 8ย_bย0\^3Z?_ Zcยฌ ย0Z_ 8ย_bย0\^3Z?_bยuยชBย<]!ร
ย c^3_bย0X
ย ร f{?} ย e<ยย ยยรFย ย h ย ยช ย Xยนยฃยซยbย\ยab]!ย?X ย ร f{?} ย e ยยรย ย h ย ยฌรกZยยฏยZ?ยฃ X fยmWYXยนยbยX
\<ZยดยFX0ยถb_Xm\<aX
e !h ยฌรกZยรยZ?ยฃ.Xยซ]c\<Z?ยฃ
ยย<Z?ยIX9ย<\^3XOยรZcยฌ\<aXยฏย0Z_bย\)]c_F\)ย9ย รฝ ยฌ
ย0Z?_F\)]!^3_bย
<ยชb\aX9_ ย Xยยฃยข] ?X
ย<]\^ยยยยฌรกร



 



! " 

!



$#  # 
 !

! 4 

%# 
(&

,+







!



 
#
*)

#

.4


31
!
	3 1
8&:9 <;
@A

#

'&
#
*) 

65
=&29

0/213 1 4
73 1
?> ;



B	CEDEFHGJILK,GEMENOFHPQCEDEFSR7CETEUVI	WEIYXZDE[EM\GJ] ^
_:`acbedgfhcfij fklmonQhpqrsf<rsdtqvuJaxwEkyr{ze|}6ijt~8~<ย%ivj7ยdgq ยยEqยfยfiEhยqยยE`ยยยยEaยยยzย|}ยยdtqfยjOrsq~QqEd
j fdgยcrยยยยdgq ยยEqยfยยยhยqยfrยdgqrยqEยยfiEhยยdtq~<fยj?qยf6ย kfiEhยqย2hcย7j ยthยยc~j frV~ยz.m_2`ยz.dtย6~dtยยhยj ยlrsfยยj?ยm
j fdgยยย2rsfiยยJ`ยย0ยEaยย$fe~<iEdtยยยกยขqEd ยยฃlh6ย%ยshj ย2fij fQยคยยฅSยฆgยจJยง ยฉ ~%j f<rย~ยชph~2|}ยซkEj qยกยข~<dย~j frV~ยph~eยฌ8ยญยa*bยฎdtfh
fij fQrsq,firย~ยยdtq~<fยยfrยdgqยrsf8rย~ยฏrsยยยฐdgยfยj qf6fij?f8ยยhยข~<fยj ยf%hยกย:rยf%iยฑยฒ ยง rsq0ยณยดtยต$ยถยธยทZยคยฌ8ยญยยถยชยจEยง ยนยยฉ$ยน kยยj fiEhยย
fij qยบยย~<fยฏrsq,fiEh7ย%ยsdย~<Eยhย~<ยฐjtยhยยปeยฝ ยผ ยถ{ยฌ8ยญ ยนยชยพ dgfiEhยยย2rย~<htkZfiEhยขยฐd rsqfcยยdgยVยกยฟqEdtf6qEhยh~~%j ย<r{ยsmร~j?f<rย~ยชz.m
ยทZยคยฌ8ยญยยถยชยจยง ยนยยฉ a
ร hยqEd ย4ยdtq~ยชrยยกhยย8ยdtqยกgrsf<rsdtqยยคยr ยฉ a*ร2irย~ยrย~Q~<EยยฐEย<rย~ยชrsqEย ยsmยกgr{รรยยsfQfdcยฐEยd รth ยพ fiEhยยฐEย%dยd zrยqรtd ยsรth~
fhย%iEqrยรtEh~ยzยยdtยยjยซยยยghยlEยยjOrยยQยthยdtยยhยf%ยmta*รeEยZย<dtlยย2dtยยยกยlyhยย%hยVj?f<rsรthยsm7hjt~m6r{z	ยณยดtยต$ยถยธยทZยคยฌ8ยญยยถยชยจEยง ยนยยฉ$ยน ย2hยยh
j qdtยฐyhยqยฟ~hยfa6รยฎqz.dtย%fEqj fhยsmtkyrsfQrย~QqEdtfaยรeqยfiEhcdtfiEhยยQij qยกkyrsf8ย2dtยยยกยlhยijOรthยh~~hยqยf<rยjOย{ยsmยย{rยยgh
j qยdtยฐhยqย~<hยfยฎr{z	ยยhcยdtยยยกยยhยยฐยยjtยhcfiEh6dEยยยEยยhยqยh~Qd zยรรrsqยฟยทยยคยฌ8ยญยยถ$ยจยยง ยนรยฉ lmยร8a2ย$fQfEยq~ยฎdtEfQfij fk
zยdtยedtEยeยฐEEย%ยฐdย~<h~:iEhยยhtkf%irV~:ยhยยฐยยjtยhยยยhยqferย~ยฎยฐdย~~ยrยlยshta
ร hยfรยทZร*ยค%ยฌ8ยญยยถยชยจEยง ยนยยฉ lhf%iEho~j ยรhojt~ยยทยยคยฌ8ยญยยถ$ยจEยง ยนยยฉ hรยhยยฐEf7f%ij f7hยรghยยm0ยคยชEqEqEhยยยj fhยก ยฉ ยdtq ย<Eqยf7d z
fiEhรz.dgยยรยค$รcร ยจ `.รยชร ยฉ rย~6ย%hยยฐยVjgยhยกSlยm,ยคยชรยร ยจ `.รยชร ยฉ aยคยbยฎdtf<rยยhยfij?f6firย~ยฏrย~ch~~<hยqf<rยjOย{ยsmvfiEhรdtยฐEยฐdย~ยrยf%h
fยยj?q~ยชz.dgยย7j f<rsdtqcfd8f%iEhedtqEhe~<hยกรยeiEhยqรยกhpqrsqEยch~~<hยqf<rยjOยยฐdย~ยชrsf<rsรgrsfยmรrยqยnQhpqrsf<rsdtqยขรaรa ยฉ8ร rsqjOย{ยยmtk
ยshยfcยป	ร ยฝ ยผ ยถ{ยฌ8ยญ ยน lh ยณHยดtยตยชยถxยท ร ยคยฌ8ยญยยถ$ยจยยง ยนรยฉ$ยน aยยยชf8fEยq~QdtEfQfij fOkEzยdtย8jOย{ยย~<รรย%rshยqยfยยmย~<ยยขjOย{ย6ยจ ยง kZยปยร ยฝ ยผ ยถ{ยฌ8ยญ ยน*ร
ยป ยฝ ยผ ยถ{ยฌ8ยญ ยน aยร2irย~8ยh~<ยsfkยeirยยiรยยhยยยj lyhย:jt~ ร hยยรย7jรQaxรEkย:rรย{ย2lh7~<fj fhยกj qยกยฐEยd รthยกยบยยj fhยยOa ร dtย
qEd ย4ย2h8~<h8fiEhQยshยยยย7jรfdยยdtqf<rsqยEhยfiEh6ยฐEย%dยd zZd zยf%iEh8ย7jOrsqยh~ยยfOa
รยdtq~ยชrยยกhยย6~<dtยยhรยS
ยง ร,ยป ยฝ ยผ ยถ{ยฌ8ยญ ยน a8ย$f6~<รยยh~Qfdร~iEdยซยยfij fQzยdtย8jOย{ยยรยขยLยรfiEhยยhchรยrย~<f~6รยรย~<ย%i
fij fยz.dgยยjOย{ยรกรยฑย"ร6ร ktf%iEhยยhยhรrย~<fย~2j8ยฐyd rsqยfQยyยง รขรฃรยยณยดtยต<ยถxยทZรZยคยฌ8ยญยยถยชยจEยง ยนรยฉ$ยน ~<ย%iยfij?fยjOย{ยfiEh8ยdยdtยยกgrยqj?fh~
d zยยง รข j ยhยrsqยfhยยghยย*ยยยsf<rsยฐยsh~ยd zรฅรค รฆยร=j qยกc~<ย%iยf%ij f:รงรจยยยง รฉcยยง รข รงร0รa*ยค ร dtยZfiEhยqcยยheยยj qcfยj?ยthย~<ยยขjOย{ยยhยย
j qยกยข~<ย7jOย{ยshยยeรรชx~Zfdยยย%hj fhยjย~<hรtEhยqยhยยยง รข ยdtqยรghยยย rsqEย6fdcยรฅยง a ยฉ8รซ hยqยhtk\ยยhยfeรยย0ยEa*รยm ร hยยยย7jcรQaxรEk
ย2hยยยj qยpqยกc~<dtยรheยยง ร รยฟยณHยดtยตยชยถxยท ร ยคยฌ8ยญยยถ$ยจยยง ยนรยฉ$ยน ~ยiยfij feรงยยยง รฉยยยง ร รงร0รgรฆยซรฌEaZร	m6ยกhpqrsf<rsdtqkhยรthยย%mยยdtq ย<Eqยf
rsqoยท ร ยค%ยฌ8ยญยยถยชยจEยง ยนยยฉ rย~ยฎd zยf%iEhQz.dtย%ย(รญ ร ยคZยฒ8ยง ยฉ*ร ยEkรญ ร ยคZยฒ8ยง ยฉ ย0ยEkEรญgยครฅยฒยยง ยฉ ร ยจ `รรญ ร ยคZยฒ8ยง ยฉ kEdtยeรญgยครฅยฒ6ยง ยฉ ย ยจ `รรญ ร ยครฅยฒ8ยง ยฉ kJยeiEhยยh
รญOรrย~8jยขยฐydย~ยชrsf<rsรthยยฐyd ยsmยqEdtยรrVjยซยรacยยชยtqEdtยhรz.dtยQfiEhcยยdtยรhยqยf8fiEhยยdtq~fยยjOrsqfย~Qd z	fiEhczยdtยย'รญOรรฎยครฅยฒ8ยง ยฉeร ยEk
j qยกยยdtq~ยrVยกhยยยฎfiEh8ยhยย7jOrsqrsqEยยยdtq~<fยยjยซrยqfย~ยfij fยฎยยง ร ~j frV~ยph~ยa*ร2iEh~<h6ยdtq~<f%ยยjOrsqยf~ยjยซยรยrยqรtd ยsรth6~fย<rยยf
rsqEhรtjยซยรrsf<rsh~ยkZj qยกยf%iEh8zยEqยf<rsdtq~ยฎrsqยรtd?ยยรghยกยคยรญยj?qยกยรญ ร ยฉ j?ยhยยdtqf<rsqEdt~ยaeร2i~ยkEfiEhยย%h6hรrV~fย~ย~dtยยh
รOรยรฏยร~<ยiยfij f2zยdtยยjยซยรยยยฒ ยง zยdtยeยeirยย%iรรงรจยยง ร รฉรฐยฒ ยง รงyร0รOรยkEfiEh~<hcยdtq~<fยยjยซrยqfย~Qj ยh6jOยย~<dย~j frV~ยphยกยlmรฑยฒ ยง a
bยฎdยซย,ยdtq~ยชrยยกhยย	jQยdtq ย<Eqยf*d z\f%iEh	zยdtยยยรญ ร ยคZยฒยฏยง ยฉ	ร ยยฎfij frย~*~j?f<rย~ยชphยกยlยmcยยง ร aยรฒ rsqยhยรญ ร rย~Zยฐdย~ยชrsf<rsรthtkgfirย~
ij ยฐEยฐyhยq~ยฎrรzej qยกยdgqยยmยขr{z2fiEh8zยd ย{ยsdยซย:rยqEยยdgqยกgrยfrยdgqoiEd?ยVยกE~รดรณยz.dtยQhยรthยยmยddtยยยกgrsqj f%h ยฒ `	fij?fยjtยfjOย{ยsm
j ยฐEยฐyhj ยย~Qrsqvรญ ร kย2hยijOรthcย `ร ร ยEacยยชqยยฐj ยf<rยยยยj ยkr{z,ยฒ ยง j qยกรยยง ร ijรghยf%iEhร~%j ยยh7ยddtยยยกgrsqj fh~ยฏย2rsfi
รยซjยซยยEhยยEkfiEhยqvรญOรsยคZยฒยยง ยฉQร ยJacย$f8zยd ย{ยsd ยย~8fij?fQz.dtย6jยซยรยSยฒ ยง kr{zยรงรจยยง ร รฉยฑยฒ ยง รงZรYรOรยj?qยกยยยง ร j qยกรตยฒ ยง ijOรthcfiEh
~j ยรh6ยdยdtยยกgrยqj?fh~2ย2rsfiยรยซjOยsEhยยJkEfiEhยqรถยฒ ยง jOยย~<dร~%j f<rย~ยชph~eยทZร*ยค%ยฌ8ยญยยถยชยจEยง ยนยยฉ a
ร hoqEd ยรทยdtq~fยยfยยง รข fij?f~j f<rย~ยชph~ยบfiEhยยhรgrsยhยยยhยqfย~ยa ร hยfยรธยรนยlhSfiEhยrsqยกhรLd zยfij f
ยdtยยยฐydtqEhยqf	d zZยยง ร ย2rsfiยfiEhยยยj ยยth~f*รยซjOยsEhta ร heยกhpqEhยยงยyรข0lmยยdtq~ยชrยยกhยย<rsqEยยhjgยicd zyrยf~	ยdtยยยฐydtqEhยqfย~
ย `รข kgz.dgย6รค8ร,รธ2ร0รบSรณ
ย
ย `ร ร ย
รธ ร รธ รน j qยกยขย `ร ย0ย
ย `รข รรผรพ รปรฝ รยย `ร รฆยร
ย `รข รฉ } ` ยคยชย }รข รฉรย}ร ยฉ รธ ร รธยรน
รฝรฟ
ย$fQrย~ยฏhjt~<mยfdยรthยยrรzยmf%ij fยf%iEhรยdgยยยฐdtqEhยqfย~ยฏd zeยยง รขรฑ~<Eย'fdรคta ยฎยรย2fiEhยยdtยยยฐydtqEhยqยf~ยฎrยqSยยง ร kdgfiEhยย
fij qยfiEh8รธยรน รชรจfikj ยhQrsqยยhjg~<hยกยlยmj fยฎยยdย~<f6รค?รฆยรva2ร2iEhcยdtยยยฐdgqEhยqยfQย `รข rย~ยยกhยยhjg~<hยกยlยmj fยฎยยdย~<f
รบยรฆยรSa ร hQย2r{ย{ยย~<iEd ย0f%ij feยยง รข ijt~ยfiEhQย<rsยtiยf2ยฐEยdgยฐhยยf<rsh~ยzยdtยejOย{ยรรฐย0ร ร kยยยฎiEhยยh ร ร rย~2~<ย%iยfij f
รค รฆยรยรยร4ยรrยqยค$ยE` ยฆรtรฆ รฌEยฆยร ร ยฉ รฆยซรฌtรบSa6ร2iEhยz$jtยf8fij f8รบยรฆยรยรยร4ยJ` ยtj?ยยj qfhยh~Qfij f8ยยง รข rV~ยฎrsq ยzยdtย
jOย{ยยรรฐย0รยรยซaยร2iEhQzรjgยfefij feรฌtรบยรฆยรยรยร"รtรฆ รฌ6ยgj ยยj qfhยh~ยfij fQยยง รข rย~eย:rยf%irยqรgรฆยซรฌยd z	ยยง ร kj?qยกยขiEhยqยh
ย2rsfirsq,รยขd zeยZยง aยขรฒ rยqยhรฌgรบยรฆยร ร รYรOรยkrsf8zยd ย{ยsd ยย~Qfij f7รงรจยยง ร รฉ ยยง รข รงZรYรรยaยขรฒ rsqยhยยยง รข rย~6ยdtq~<f%ยยfhยก








 

	
















!#"%$&'(()*,+.-/0&&(

13254768(9:9<;76(=<1?>A@1B49DC36FE:9HG5=<2I2(J<KMLON76P139CQ6(CST!
R UV,W
J39oMXLAJ39Kqprs2(N7KMLA1?LA2(N
{ 1vN2 W

JB9uED6LAN7CQ132}|J32F8(9D~q9uE:EH6ยยpcย V,W

4LANI1B9Kย96PJ?>zLO9uJ V

134LCยJ39oMXLAJ39Cย1322F>C

|J39C?9uN1t6F1wLO2MNLAN
L x

9=<2MN7=B>OX7K951B476F1QT!
R Y[Z]\_^(`?acbedgf3hiQa j
R 
k lmknV

f L ltV 6FN7KH49uN7=<9v1349S9uNI1wLOJB91349u2(J39uE

LA1%LCยK9<ย7N76Fย>A9LANย1B49ย>ย6PNย(X76Fย(92 x
x 2(J3EQX>6Dยฒ

R ยต ร
R l
f Lmpยฉ9(p VรรFf Tg

รIร

6MCยX7C?9KยLAN1349H|J322 x#ย X7Cw1ยยPLO8M9uN*pยยC
W

6(C

Cup
9
W

x J32(Eย6>Aย(9uยJt6L=QยM9u2(E:9u13J3@(pยย9vย76MC?9Q2(XJยK9<ย7NLA1?LA2(N7Cย2(Nย1349

f s2=34N76Pย V rย2C?139 Vย]ย

6ย7JtC?13ยญ2(J<K9uJ

4L=B4

V N2 Wyx 2F>z>A2 W

2F@ Vยย(ยย(l pqยยC?Xย7C?9u1#ยย2 xย ยยย LC#C36ยLยKย132ย!9ยuยกuยขQยฃยฅยคยงยฆ `ยฉยจ ยกยยช<ยซ3ยฆFยฃยฌ

J396>Aยญm=B>A2IC?9Kยฎย79<>KCupยฐยฏ/476F1sLยC V ยยฑLCยC?9uEQLAยญm6>Aย(9uยJt6ยLย=ยL x

\ ยก ` ยกtยฌรยฅยฃ ^Fร}ร

ยกuยขQยข:ยฆ

ร#ร!รยรรรรHรกAรข,รฃ

ยบรรv
R รFl

f C?9u9

x J39u9S8ย6FJ?L6Fย>A9CS6FJ39

1349uJB9ยLยC

f?ยณ7ยดยตยถยถยถ?ยตtยณ
lยฐW
ยณ!ยดFยตยถยถยถ?ยตtยณ
W 42IC?9v2MN>O@}N2(Nยญ
ย
ย
ยด3ยฟ
>A2(ยFL=u6>gC?@EQย72F>C%6FJ39G V7ยMVยทvV,ยธVqยน
6FN7K5ยบ V C?X7=B4H13476F1 ย ยยผยป ยบยฝยฒ fmT ยด ยตยถยถยถBยต3T
l Lzยพ
fnT ยด ยตยถยถยถ3ยต3T
lยZ ยSp
ย
ย
ย
x XN7=<1wLO2MNยฎรยรFรรรรร V(W 49uJB9รรร
ย ยยร 6FN7Kรรร
ย ย ย V LCยฐCB6LK:132ย!9ยCw9uEQLAยญm6>Aย(9uยJt6L=ยL x LA1tC#ยMJt6F|4
R l
f Tq

42IC?9

6PN7K

LยCQC?9uELOยญm6ย>OยM9uยJt6L=Fpรยฏ/49DED6ยLONย1322F>

9:X7C?9รLCv1B49
W

x 2P>ร>A2 W

LANยยฑร#รยซtรFยก

f s2=34N76PยD9u16ย>รp V*ยย(ยยV |*p7รFร l3l ร

\ รรคMรค ^ ยuยก:รยฅรฅIยฆ(รsยรฆยฃยฅยยยฆHยuยกuยขQยฃยค?ยฆ `ยฉยจ ยกFยช<ยซ3ยฆFยฃยฌQยรงยกรsยฃ ร5ย ย

ยฆยยฌ ^Fร รยฅยฃ ร ร ^ รย<รญยยuยกuยขQยฃยค?ยฆ `ยฉยจ ยกFยช<ยซ3ยฆFยฃยฌยรฎ<ร ร ยฌรยฅยฃ ^Fร

รยร a G ยตยuk

ร

ย

ยย

ยฆ ร_รจ

R
Tย
Z

ย<รยฌรฅยรยฅรฅIยฆ(รยร

f G l

ย

ย:รฉรซรช*รฅยก ร
ยบรฏT R

รยฅรฅIยกยซ3ยก:ยกuรฌยฃยuรยฅย

ยฆ ร_รจ

ร

fnรฐ3lZ

ยรฑรฎ ^ ยซ

ยฆ `ยฅ`รฐ%Zยf G ยตยuk รฉ
รฒ

XJยย7JtC?1SX7C?9D2 x

1349}rยXJ38(9รซรณ(9<>O9=<1wLO2MNย~q9uE:ED6ยฎLยCยLANย1349

x 2F>z>O2 W

LANย VรW

4L=34รดC36@Cย1B476F1 V LANรด6

=<9uJ31t6ยLONQC?9uN7C?9 V C?9uELOยญm6ย>OยM9uยJt6L=

x XN7=<1?LA2(N7C,ย79u4768(9รต<NL=<9<>O@รถยN96FJรท>zLOELO1<Cupยยฏ%49s1ยง@|79s2 x |49uN2(E:9uN2MN
ยด
9

L
?
C
}
4
3
1
5
2

6
M
8
F
2

L
ร
K
ย
L
ย
C
z
L
z
>
A
>
7
X
?
C
3
1
t
J
P
6
3
1

9
K}ยI@ ยณ CnLAN
W
W
W 4Lย=B45LC=<2(N1?LANX2(X7C6P1ยG V ยX1476(C/LANย7NLA139<>A@รEH6FNI@
รธ
>A2=u6ย>gED6;LAED6:6FN7KรซEQLANLOEH6:N96FJยGp

ยฝรHรก%รฃ \ รรค(รค ^ ยรงยก}รยฅรฅIยฆ(รยร a G ยตยuk ร ย ย ยฃย5ยฆยฌ ^Fร รยฅยฃ ร ร ^ รยtรญยuยกuยขQยฃยฅยคยงยฆ `ยฉยจ ยกFยช<ยซ3ยฆFยฃยฌยรฎtร ร ยฌรยฅยฃ ^Pร
ย<รยฌรฅรยฅรฅIยฆ(ร fmT*l
Gยยฃ รฎ T	
Gยยฆ รqรจ  f G l ยบ
G!รฉยฑรช*รฅยก ร
รยฅรฅIยกยซwยก5ยกรฌยฃยฅยรงรยฅยรย ^ ยข:ยก
 
GยยtรยฌuรฅรยฅรฅIยฆMรรดยฃย
ยuรยฅยซtยฃยฌร ` ยฃ ร ยฌยซwยก<ยฆFย<ยฃ รยจ ยฃ ร
รยฅรฅIยกSยฃ ร รยกยซtรFยฆ `*a G ยต 
 k รฉ
รนvรบรปqรป<
 รฃ รณ(X||!2IC?9 V ย@ W 6@Q2 x =<2(NI1BJt6(KML=<1?LA2(N V 13476F1รCB6F1?LCnย79Cs1349ย4I@|72(1B49C?9Cs2 x 1349|JB2(|72ICยงLO1wLO2MN
ยX11349uJB9ยLยC/N2

 C?X7=B4ย13476P1ร
 LยCยฐLON7=<JB96(CnLANยHLAN51349ยLANI139uJB8ย6> a G ยต 
 k pgยย9vK9<ย7N9ย6S|72FLAN1 T LAN a G ยตยk
132ย!9ยชBยฆ รจ L x*x 2(JยC?2ME:9 T U Za G ยต3T*lqW 94768(9 fmT U l
 fmT*l p,~q9u1ยฐย]ย!91349ยC?9u1ย2 x 6>z>_1349ยย76(KQ|!2FLANI1<Cup
รณFLAN7=<9:
 LCC?9uELOยญm6ย>OยM9uยJt6L=QC?2SLยCยย V CnLAN7=<9 T U Z ยยฑLzยพ
รนvรบรปqรผ,รป_รฝรพยรฟรพยรป


รณFLAN7=<9 V



s9=u6FX7C?9S2 x

ร

a G ยตยuk

1B49ย=<2(N1?LANIXLA1?@52 x

ย@ร6(C3CwXE:|1?LA2(N
2F8(9uJย1349QJt6FNย(9

V

ร





fmT*l

fmT U l3l3ltยถ

V }LCN2(1ยLAN7=<J396(CยงLONย}LAN6FN@5LAN139uJ38F6>

6FJ3ยLA13J<6FJ?Lz>A@ร=B>A2IC?9Q1325GD6PN7KรCw2ยฎG

1349K9<ย7NLA1?LA2(Nย2 x

T U ยนรดTรทl



ย@ร6(C3CwXE:|1?LA2(N

C?9uEQLAยญm6>Aย(9uยJ<6L=ร=<XJB8(9ยร

5f 

T U fBf G

ร




9:=u6FNยย7N7Kยย76(K

kmV!W

|72FLAN1tC

ยSps@51349HrยXJ38(9รรณ(9<>O9=<1wLO2MN~q9uE:ED6 V 1349uJ39LC65=<2(N1?LANX2(X7C
Z
ร

C?X7=34รด1B476F1:ร

ยย

V 1349vJ<6FNย(92 x

Z 

V!f G ยต Fk ร]ยvp%รณFLAN7=<9QG

ย

ร

V LA1

f G l

V Lmpยฉ9(p V ร
x 2F>z>A2 W

ยบ

Gย6FN7Kรดร


 

ย

9=u6>z> V 1349J39C?X>A1
W

รฐยZรฏf G ยตยuk p

ยบยฝG 1349uJ39 x 2(J39

 fl  GpรรณFLAN7=<9!}LยCS65=<2(N1?LANIX2(X7C x XN7=<1wLO2MN V
a G ยต$F
 k prย2MN7CnLK9uJv1349QEQLANLAEQXEร|72PLON1LAN1B49vLAN139uJ38F6>

|J32F8(9~q9uE:ED6Hยpcยp

x 2(JD6ย>ร>

a G ยต Fk!x 2MJC?2(E:9

f<ยFl

%}2(J39|JB9=BLยCw9<>O@ V >A9u1 T ย79Q1349LANย7EQXEย2 x 1349ยCw9u1 รT U
 fmT*l ยบ("qp%รณFLAN7=<9"  G W 92Mย1t6LANย13476P1 T) G:6PN7K51349uJ39 x 2(J39 T}Z
E:96FN7Cย13476F1ย1349uJ39ยLC6:|!2FLANI1 T U ยนยฑTยx 2(J W 4L=34* fmT U l+
 fmT*l<VW
"56FN7K T p

ย

fmรฐ3lDZ

:Zยa G ยตยk p#ย@
 GQ6FN7K5C?2 V
LA1ย6(=B4LA9u8(9Cv6HEH6;ILAEQXE#"
 G

f<a G ยตยkltV LC
Cย1B476F1ร

6(=B4LA9u8(9Kqp

ยย9v=u6FN5N2 W

aG ยต

$

49uJ39Q134LCยED6;LAEQXE
W

Zยa G ยต Fk

&
ร

ยSpยยฏ%4X7C VT

fnT U l

'" ร 7 =B>A96FJw>O@

LยC

ยบ

LยCยย76(KqpยsX1ย13476F1

4L=34=<2(N13Jt6MKMLย=<1<C1349Q=342PLย=<9Q2 x

9N9u9KHLC6(C

x 2F>z>A2 W

Cup

,-/.021436587:9<;=1?>@+ACBD>CEF.GH,JILKLMON<GP>QRACBSTQDB>@LAC1 BR7VU?AWQRACXSZYR>CET[LACU=\>JST9VSZ]^STBSTQD_`Aa?1P>$U:ETAU:bc>d:eP>$1:B6SZaPAC\6YF]F\RACAW]F7O\RXe:Ef>
ST1gBR;=AWEf>$1=[LeP>$[OA7$]h\RA>JEh9<ET7LQRAC_4aPA<ET_=QC.Dij7$k8AClLAC\CGP>Qm7OU`QRAn\RlLAC_4ST1365D7:9;=1?>@4ACBj>JEF.TG^,JILKOMLN<G?Q6ST1=9CABR;`AVBR;=AC7O\Rbo7]
\RA>CE9<ET7OQRAC_4aPA<ET_`Q>_`XSTBRQWA<EZSTXST1P>$B6ST7O17$]d:eP>$1:B6SZaPAC\RQ36p>\RQR@LSFG^,IOq/,JN<G?BR;=AB2kD7_=A<aP1/STB6ST7O1`Qr>$\RAVAnd:e/STlO>JETAn1:BC.
s:t

V

uvwx8y{z}|~yย`x8ย4vwxยvยยยzยzยยWwยhy{ย?ย

ยVยยยยยยยยย6ย8ย4ยย?ยยHย6ยDยLยOย(ย/ยยขยก/ยฃ8ยคRยยฅยOยฆ!ยHย6ยยจDยง ยฉVยชยซjยญยฌ ยฎยฏgยฐ4ยฑDยฒ ยช+ยญยฌ ยฎยฏgยฐgยฑยณ
ยดยต`ยถDยถยทLยยนยธVยบFยป=ยผ?ยฝCยบFยพ ยช ยซWยญ ยฌ ยฎยฏgยฐ ยฑ*ยฟ ยช ยญ ยฌ ยฎยฏgยฐ ยฑ<รรรรรร ยฝ ร?ร ยป)รรยปยยฝยป ร ยป:ยฝLรCยป*รรรร$ยบFรร<ร ร รรรยปรร ร รร<ร2ร^ยป:ยฝ	ยฏg
ร ยฐร
ยผ~ร:ยผ?ร ร ร^ร2ร:ยผ`ยบgร ร ยฝรรยป=รรร^ร ร ยผ`ยบFยป:รรร ร ร ยฏgยฐ!รยร ยปยร ร รร<ร2ร^ยป:ยฝรยป=ยผรรรรรรร2ร6รCรรรLร ร ร ยฏg
ร ยฐ รCยป ร ยผรยฝOยผ?รยปLยบFยพ รรกร ยป:ร
รขยรฃ ยปรยผรคร ร ร?รnรรรLรJร ร ร'ร$รยผ?ร!ร2ร ร รยป ร รรรยปยรรร2ร6รCรรรLรOร&รFร ยฏg
ร ยฐรฅร	รฆ รรงร$ยบFยป=ยผ?ยฝCยบFยพรรCร^รจรฉรLยป=ร&ร ร รCร ร รรชรรยผ?ร
รซ ยรย ยฎTรฌWรญ รข ยฎ ยจ ยง ยฑRรฎ<ยฑยฟ ยช ยซWยญ ยฌ ยฎ รข ยฑDยฒ รซ ยHย ยฎTรฌ ยซ รญ รข ยฎ ยจ ยง ยฑRรฎยฑRรWรฏ ร$รCรร!ยป ร รฃ ยพร+ยผ`ยพ ร รjร ร รรร$ยฝOยผHรรร2รLรCร ร ร ร รรยผ?รร ร ยฝยผรยฝ รฃ รFรยฝOยผ?ยฝCรยบFยพ
รCรยผPยบfยบgยจ ยง ร ร$รยป:ยฝยปยปLรฐ^ร2รCรOร+ร ร ร!ยปรฑร
ยง รฒ รซ ยHย ยฎTรฌรณรญ รข ยฎ ยจ ยง ยฑRรฎยฑ ร+ร^ร2ร$รรยรรด:รCยป ร ยผรยฝOยผ?รยป=รรตรถรRยฝ ร รรรรยปรJยป:ร รซ ยHย ยฎTรฌ ยซ รญ รข ยฎ ยจ ยง ยฑRรฎยฑร
ร ร ยป รFร ร2ร4ร ร ร+รFร*รFรOรgร$ยบ ร รCรยฝยป รgรท*ร ยฝยป4ร ร ยฝรยผ`ยบยบรยพ ร รยปรยผ`ยพรงรรยผ?รgรฑยง ร2รgรธรรนย:ยกCรบย?ยยHยคRยกLรปรงรRยฝ ร ร รซ ยHย ยฎรฌ ยซ รญ รข ยฎ ยจ ยง ยฑRรฎยฑ รร
รรยป:ยฝยป4ร2รoร ร รฑยง รผDรฒ รซ ยHย ยฎTรฌ ยซ รญ รข ยฎ ยจ ยง ยฑรฝรฎยฑ รCรรร*รรยผ?รรพรฟรฑ
ยง ยงรฑรผ รพ  รธ ร
ร ยปยร ร ร ร ร รร<ร2ร^ยป:ยฝยร$ร ร รCยปยนยจ ยง ยผ?รร รร ร รCยป รยร รFรรรLร*รFร รซ ยHย ยฎTรฌรณรญ รข ยฎ ยจ ยง ยฑรฝรฎยฑ รรยผรรยยผ?ยฝยปรCยป ร ยผ?ยฝOยผ?ร$ยป=ร	รRยฝ ร ร
รซ ยรย ยฎTรฌ ยซ รญ รข ยฎ ยจ ยง ยฑRรฎ<ยฑ 

	 ยฒ
รรญ ยจยง  รฑยง  รธ รฎ ยจยง   ยง  รธ    รฑร
ยง รฒ รซ ยHย ยฎรฌWรญ รข ยฎ ยจ ยง ยฑRรฎยฑ ร2ร+รธรCยป ร ยผ?ยฝLยผ?รยป=ร&รRยฝ ร ร รซ ยHย ยฎTรฌ ยซ รญ รข ยฎ ยจ ยง ยฑรฝรฎยฑ
ยธVยบFยป=ยผ?ยฝCยบFยพ 	 ร2รรงรCยป:รรยผ`ยบHยป รฃ ยฝLยผ`ร2ร ร ยพ(ยผHรรCรร ร รCร ร ร ร รรยป:ยฝยป)ยผ?ยฝยป รยร รFรรรLรรFร 	 ร ร ยฝยผ?ยฝ รฃ รFรยฝOยผรยฝCรยบรยพ(รJรยผ`ยบยบ
ร ร ยบFยป:ยฝOยผรรรLยป ร ยป=รLร ร ยฝLรยจ ยง ร รFรรLยป 	 ร2รgยผ รฃ ร รรร^ยป=รรCร รฃ รCยป:ร ร ร  !#"%$'&($  รญ ร+รยป:ยฝยป*) ร2รgรรยป!ร^รร รฃ ยป:ยฝ ร ร
ร ร ยบFยป:ยฝOยผรรรLยป ร ยผPยบรรยป=ร รFร	ยจ ยง รฎOร รยปรฅร:ยผ?ร*รรCยปรรยป 
 ร ยบ=+ ยผ?ร ร-,รร ยปLรFยป:ยฝOรJรยฝOยผHรร ร รยป ร ยฝ$ยป:ร ร ร ร ร รร$ยบFรร^ยปรรยผ?ร4รร^ร2ร
รCยป:ร ร ร รยร รFรรรOรoรยผHร4ยผ?รรยผHร:รLรรร^ยบ2ยผ?รCร ร ร รร รรร^ร4ร+ร ร รCยป/. ยฝOรJร ร ร ร รยร รยป:รรรoร2ร {ยง รร ร^รร ร ร$รยป:ยฝยป4ร2ร ยผ รยร รFรรร
รญ  ยง  01ยง  รธ รผ รฎ รFร 	 ร2 ยพรรยป4ยธรยฝ ร ยป  ยปLยบFยป=รLรCร ร ร ร ยป:ร!รยผ ร รรยป:ยฝยปร2รVยผgร ร ร^รCรFรรร ร รรVรCยป:รรยผ`ยบHยป รฃ ยฝOยผ`ร2รcรยขรรรLรJร ร ร
3  ยฎ 4 ยฑ'5  ! "%$'&($  รCรร$รรงรรยผ?ร 3 รญ^ รฎVยฒรรญ  ยง  01ยง  รธ รผ รฎ ยผรรร 3 รญ76 รฎ รฒ 	 ร ร ยฝ 6 รฒ รญ8 94 ยฑร
 รFรรLยป 3 รยรoรCยป:รร ยผ`ยบH ยป รฃ ยฝOยผ`ร2ร ร รFรร2ร+รCยป:รร ยผ`ยบH ยป รฃ ยฝLยผ`ร2รoรรรรงยป=ยผรรร ร รรFรOรร ร^ร ยฝOรรรFรยผ?รยป=ร ร: ยพ ร ยป:ร!รยผ 4ร<;ร
รรยป:ยฝยป4ร2ร4ร ร ร!ยป>=   รCรรรรงร$รยผ?ร 3 ร2ร รCร$ยฝCร2รLรCยบFยพรรรรLยฝ$ยป=ยผHร<รFร?!
 รFร*ยป=ยผHรร ร รWรรรLร:. ยฝOรCร() ร ร^ร ยฝOรรรFรยผ?ร$ยป=ร ร?ร ยป:ยฝ
3
รรยป!ร ร รยผPรรร ยฎ  = ยฑร1 ร รรร รCยป!รรยผ?ร รญ = รฎ+ยฒรรญ ยจยง  รฑยง  รธ รฎLรA+
@ ร ร ร ร ร รร<ร2ร^ยป:ยฝรรยป!ร ร รรCร$ยฝOยผ`รFรรรLรoรรร รฌWรญ รข ยฎ ยจ ยง ยฑRรฎ
รรยผ?รรยผ ร ยป!รรยป&ร ร ยฝรCB รญ 0 ยง รฎ  ยจED B รผ รญ 0 ยง รฎOรรงร รยป=รCยปรงร ร รรCรยฝOยผPรรร^รOรยผ?ยฝยปรงยผ`ยบยบcรยผ?รJรยร8. ยป=ร รฃ ยพรฑยง ยผ?รร)ร$รยป:ยพ ยผ`ยบยบ
รFร รรร ยบ ร ยป รJรยฝ ร ร?รถ
 รรรยป=รรรยผ`ยบรFรCรFยป=ร ร( ยพรรยป ร ร รรรCรFร^ร^รFรnยพ ร รรรยป รร ยบรยพ^ร ร รร2ยผ`ยบ2ร(gB ยผรรรFB รผ ร รรยป:ยฝ$ยป ยปLรฐ^ร2รCรOรร ร ร!ยป
G(  รCรร$รรงรรยผ?ร ร ร ร ยฝ+ยผ`ยบยบรฑยง รผ รJรรร*รรยผ?รรพรฟรฑFยง รฑยง รผ รพ  G ร รฑยง รผ ยผPยบยร ร รยผ?รCร2ร7. ยป=รoรรยป=รCยปร ร รรCรยฝOยผ`รFร^รOร ร
@oร ร ร รฃ ยพ(ร$รยป ร ร ร^รCรFรรร^รFรCยพ ร ร 3 ร ร$รยป:ยฝยปยยปLรฐ^รยรJรOรรยผ รยร รFรรรH= รผ รฒ รญ8  = รฎ รCร^รจรงร$รรยป:ร^รCยบFยพ}ร$ยบ ร รCยปรคร ร =
รCรร$ร ร$รยผ?รgรร 3 รญ = รผ รฎยฒ รญ ยจ ยง รผ  รฑยง รผ  รธ รผ รฎOร รรยป:รรรพรฟรฑ
ยง
รฑยง รผ รพ  ร รรร รญ รธ IG รฎOรH รFรรLยป 3 รญ = รฎยฒรชรญ ยจยง  รฑยง  รธ รฎ รฒ 	 ยผ?รร
รพรฟรฑ(
ยง รฑ ยง รผ รพ  รธ ร รFรร ร ยบยบ ร ร รWรรยผ?รรฑ ยง รผKรฒ J รซ ยHย ยฎTรฌ ยซ รญ รข ยฎ ยจ ยง ยฑRรฎยฑรmร ยป+ร ร รร$ยบFรร^ยป ร$รยป ร ยฝ รรร ร รฃ ยพ!รCร ร รรFร?g
 รรยผรรVรร^ร2รmรยร
รฃ
รซ
รข
รFร รร รร<ร ยบFยป รrร รยผรรWรยร ร รยป รCร ร รรรยผ?รรฑยง รผDรฒ ยHย ยฎTรฌ ยซ รญ ยฎ ยจ ยง ยฑรฝรฎยฑรjร รยป ร ร รรCรยฝLยผ`รFรรรOรVยผ รร ยป=ยผ?ยฝCรFร?รถ
 รรร รฌ ยซ รญ รข ยฎ ยจ ยง ยฑRรฎ
ร:ยผ?ร รฃ ยป ร ร+รรยปร ร ยบยบ ร รcรรร?*
 ร ร ยฝรร  B รผ รญ 0 ยง รฎยฒL ร B รผ รญ 0 ยง รฎ  { ร B รญ 0 ยง รฎ  ยจED B รผ รญ 0 ยง รฎLรร ยฝ*B รญ 0 ยง รฎ รยจED B รผ รญ 0 ยง รฎOร
ร+รยป:ยฝยปB รผ ร2รยผ รยร ร<รFรCร ร ยป รร ยบFยพ^ร ร รร2ยผ`ยบ รH รรรรLยป 3 รญ = รผ รฎ รฒ 	 ร รยป*^M ร ร รรรรยผ?รรฑยง รผรฒ รซ ยHย ยฎTรฌWรญ รข ยฎ ยจ^ยง รผ ยฑRรฎยฑรร รยป
ร ร รรCร$ยฝOยผ`รFรรรLร ร รrรรยป ร ร ยฝ$รNB รผ รญ 0 ยง รฎgยฒO ยผ?รรPB รผ รญ 0 ยง รฎ   ยผ?ยฝยปร2ร^ยป:รรรJรยร:ยผPยบรFร รฌรณรญ รข ยฎ ยจ ยง รผ ยฑRรฎ ยผรรรรงรFร รฌ ยซ รญ รข ยฎ ยจ ยง ยฑRรฎOร
ยผ?รร!ยผรยฝยปรรยป:ยฝยปLร ร ยฝยป+รยผ?รCร2ร7. ยป=ร รฃ ยพ!รฑยง รผ ร: รFรรLยป รพรฟรฑยง รผQ รฑยง รพ  G รHร รยฝรรร2รรLรรรnร ร ร!รFรรรยป ร ยฝยป ร ร ร รร ร ยผ?ยฝLยผ-H ยฝOยผ ร ร
รFร ร ยบรFยป=ร!รรยผรรรรยปรงร ร รรCรยฝLยผ`รFรรรOร ร ร+รรยป&ร ร ยฝรCB รญ 0 ยง รฎ รยจED B รผ รญ 0 ยง รฎ ยผ?ยฝยปรงยผ`ยบ2ร ร รยผ?รJรยร8. ยป=ร รฃ ยพรฑยง รผ รPR รFรยผ`ยบยบรยพ ร
ร ร รร<ร2ร^ยป:ยฝยผ!ร ร รรCรยฝOยผ`รFร^ร ร รรรยป4ร ร ยฝรSB รญ 0 ยง รฎ  ยจED B รผ รญ 0 ยง รฎLรrร รยปร ร ยฝยฝยป=ร รร รรรรFร?ร
 ร ร รรCร$ยฝOยผ`รFรรรcรรร รฌWรญ รข ยฎ ยจ ยง รผ ยฑRรฎ
ร2รAB รญ 0 ยง รฎAT ยจ D รผ B รผ รญ 0 ยง รฎOรU รFรรLยป*รฑยง รผ รยผรรCร2ร7. ยป=รgรร^ร2ร ยบยยผรรรยป:ยฝรฅร ร รรCรยฝOยผ`รFร^ร ร รยป*^M ร ร รรรรยผ?รAB รญ รฑยยง รผ รฎAT ยจ D รผ B รผ รญ รฑยง รผ รฎOร
 รรร ร ร ร ยฝยป=ร:ยผ`ยบยบrรรยผรรgรrยป ร ยฝ ร?ร ยป=ร)รรยผ?ร 3 ร2ร รรรรLยฝ$ยป=ยผHร<รFร? ร?ร ยป:ยฝ ยฎ  = ยฑ รFรยรรยป. ยฝOรCร>) ร ร^ร ยฝOรรรFรยผ?ร$ยป=ร ร
รฆ ร ร ยผ?ยฝ$รCร2รLร^ยบยยผรยฝ ร ยจ D รผ  ยจED รV ยพรงรรยปร^ยปE. ร^รFรCร ร ร ร รร:ยผ?ร ร ร^ร2ร:ยผ`ยบWร ร ยฝร ร B รผ รญ รฑยง รผ รฎ   ร ร ร ร$รยผ?ร รยปร ร รร$ยบFรร^ยป
B รญ รฑยง รผ รฎ T ยจ D รผ B รผ รญ รฑยง รผ รฎ  ยจ D B รผ รญ รฑยง รผ รฎLร W ยป:รรLยปรรยปร ร รรCรยฝLยผ`รFรรรOร ร รรณร$ร^รยรoรCยพ ร ยปรฅยผ?ยฝ$ยปยผ`ยบ2ร ร ร$ยผ?รCร2ร7. ยป=ร รฃ ยพ*รฑยง รผ รrร ร^รยร
ร ร รร$ยบFรร^ยป=รรรยป ร ยฝ ร^ร ร+รรยผ?รรฑยง รผ+รฒ รซ ยHย ยฎTรฌ ยซ รญยฏgยฐรฅยฎ ยจ ยง ยฑรฝรฎยฑร ร$รรรรร^ยป:ยฝCร ร รFร? ยผ*ร ร รรร$ยฝOยผHรรร2รLรCร ร ร~ยผ?รร ร ยฝ รPร รFร?
รรยปgยฝยป=รJร^ยบรร ร

ร ยปยผ?ยฝยป/.รยผ`ยบยบFยพรงยฝยป=ยผHร^ยพร ร!ร ยฝ ร?ร ยป ร รยป ร ยฝยป:รYX ร 4 X ร

Z\[^]`_Aa2bcEd9egfh9ai2cEdkjl/mn9cEeaobcEp q-cEd9adkmerfd>mn9asfdkt-dkfmavuEa8bmcEiVx w mnzy{mKy{bm|zy}j<jlAy\qqza\y{ifd*~ยEยยย ย?ยgยยยxEw ยยยยย ]
ย9ย

ย(ยยยยย?ย'ย2ยvย?ย9ยยยยย?ยยsยยย%ยยย9ยยยย
ย*ยยขยกยยฃยคยก`ยฅ ยฆrยงยจยฆrยฉ ยชยซ-ยฌFยญยยฎgยฎ2ยฏEยฐIยฑยฒ^ยณยตยด^ยถยทยฎยนยธUยฏEยบยปยญยผยฎยยฎ/ยพยขยฝ ยฟ ยทgรยด/รkยซยยฎgยฎยยซ-ร2ยณยยถ?รHยณยยฏยทgยฌIยฐ?ยดzรรรยด9ยท/รรร{ยดUยทgรยด*ร?ยซ-ยณยยถยขยทgยฏ
ร2ยณยทgรรร-ยฌ\ยดEยญยยทยดkยฏkยทoยด^ยถยทgยฌ\ยซEร`ยธยณgยถรร(ร?ร รยร>รVร ยญ-ยถยขรUยฎยยด9ยท:รรรร ร/รรร{ยด*ยญ-ยถ`ยธUยซEรยยด^ยถรยฏkยด9ยทvยฒIยซรยถยทยญ-ยณgยถ`ยณgยถยรUร*รรรรยดkยถรQยซ-ยฌ
ยญยยฎgยฎ`ร*รร:รกยญ-ยถยขร/รQยซ-ยฌvรขยรฃรคUรฅ>รPรฆEรขยรฃยรครจรงรฉ?รชKรซ}รขยรฃยรครฌรฃรญยรฎzรฏรoยด1รยญ-รฐzยด
รขรฒ(รฑรฃรณ>รค รด รฅ2รต2รถ รรฒ>ร รท รยรธ ร>รรบรน2รป รฒ%รขรฑรฃรณ>รค รด รฅ1รผ รoยซ-รoยฌkยซ-ยฎรฑรรยฌkยฏยฎยร-รรฒ ร ยฏ รร ร ร ร รร รท ร#ร รฝ ร>รAร>ร*รนรฟรน รพ
รฒ รท
รผ
ยคยฃยขยฃ Eยฉ H

รค  -รช?รชยรขย!รฃ 9"รง -รญ #$รข F%
ร รรญ #Pร 	 &ยผรงvรฃ'
รญ 
ยพ ยฝ 	  รงรค zรขรฑรข kรญ ยรฉ รรง  รถ k
รถ
รถ
รง(kรค k)รญ *-+รฎ , kรค *./0{$รข 1 $รข 2รรง)รฉ 34059รง 6รรง 879686ยป:รง A;
ร 05ย)รญ (รฃรญ`รง zรขยรค รง <รขย=รข -รฎ
<7 รถ >รข #??
รง 8(รง รฃยรง7@รฎ 2 ร>ร ?A- รถ Vรช รถ 10{รฃย:รง E!รข 2BC/รฎ รถ 05$รฃ ยDรญ -รฎ รง}รฉ 0E7 รถ >รข #?รง 68* รถ #รฃยรญร%
ร kรญ #?รง
-F<ยH
รง GJILK รพ
MN 	 &6&k)รญ  รถ ยผOรช 2D-+
รฎ *รช รรฃย)รญ Iรง#รฃรญ"ร PQ 	 รรฃยQ
รญ Aรช รถ )-รฎ 	 2 รง 87vรฃรญ D,5Rรฃยรง 6kรญ 05
-
รฎ NOSUTVNXW รท TYN รน รงรฉ 06Z รท @รฎ  รถ รง)รฉ 340{$รฃ kOรญ }!รข 2ร>รข  รถ DG รน รท  รน [ รถ 2Hรช -รฃ)รญ  \ยฝ ^
ร ]ยปรรฒ ร รยร>รVร 7* รถ 
ยฝ รP
ร ยผ*รง k)รญ  รถ ย)รช 2E8(รค :รง *NOS'-รญ # รท 	 รน  รถ /รฃย<รง v$รข 1ยรง ยรญ >รช -รฃ)รญ A\ยฝ '
ร ] รรฒ ร รยร>ร/ร 7v$รฃ ร\ยฝ รรร
\`
_ a
-
รญ #Dk)รญ  รถ ย)รช 2E8:!รข 1ยผรง N W 
ร d ร Qf-รฃรญ 05-ยรฃยรง*รงรช 05รฃย&รง 0{$รข :รง 1#B+Dk)รญ  รถ ย)รช 2
b  รถ รช  รถ  รท  รน Bc05ยรญ`รง7>รฃ #) รถ รรง}รช 05ร ร ร รยร>รVe
รฎรฉ?รญ 05$รฃ ยU
รญ (g9"รง ย^
รญ  รค hRยรฃยรค*รฉ?i
รค [jzรขยรฉ รรฃU
รญ ยรฃยรง รงรช k05l:!รข  ยรฃยรง 	 EN S `f-รฃรญ 05-ยรฃยรงรงรช 05-#))9รง
รญ *รฃ
รญ 0{รขรฉ #)m-)รญ 2 รช -รฃ)รญ 7v$รฃ `k)รญ  รถ ยผOรช 2nN รท 9รง - รถ -zรขรฑรข#รฃยรญ ร ร ร รน B7Uรครบรฉ`:รง h[ENOSYToNp
?2Z รถ ki
รค qBr]*ร ร รยร>ร>ร รยร(ร?ร รยร>รVร D รถ Esรฎ  รถ B @รฎ  รถ -)รญ 2tGnBe kOรญ  รถ ยOรช 2
-รฎ -)รญ 2 Cรช -รฃOรญ  รฃรญ
ร d ร รฃย*รง 8#รค รฒ รง N)Sr
] รรฒ ร รยร>ร/
b  รถ รช  รถ  รท 	 รน Bย$รข NW 	  รง ยรค u[jzรขยรฉ 1รฃ'
รญ >รฃ)รญ  รถ [8รข รท NOS6รซ N รน รท @รฎ  รถ 5Rv-รครชย!รข  รท N)S,wmN รน x  รน -รญ #
รข$yDยฝ 	 u-)รญ 2FCรช -รฃOรญ %รฃยรญ "ร +?2u05ยผOรญ รฃรญยรฉย$รฃ z2D-cรฎ kOรญ  รถ ยOรช 2รฎรฉ?รญ 05$รฃ ยeรญ B รถ <5Rยรฃยรง Iรง(:รง ยรค u{"|U}
รงรฉ 06Z8/sรฎ  รถ รขยรข(\ยฝ 7v$รฃ รจรธ<\ยฝ d y ยฝ ~รธ T{)B~7ย 1[kยย รท \ยฝ uรน ย N W ?10-รฉ`:รง ร รฃย<รง ยผรช kZ
รญ 70-eรญ B 	 2
05ยรญ`รง7>รฃ #) รถ รฃรญ - รง}
รค รขย$รข  รถ {>รฃยรฎoรญ 1059รง\6รง  รถ 2Bยรง\รงรฉ?รค <68รบรธ \ยฝ d y ยฝ vรธ TU{1รฃรค รชยรขย$รฃ 9รง1\ยฝ รรร ย?2EAรง 105ยผรญ #
รช  รถ -eรฎ  รถ ka

รค Bh รถ :รฃย+รง /รง 1ยยผรฉ kรญ 05รรฎ`รช -รฃ)รญ Iรง2\ยฝ รฒ ร ]*รรฒ ร รยร>ร>ร รงรฉ 0,sรขรฑรฃรค รฒ(รณ>รด \ยฝ รฒ รป y ยฝ 
.รญH
รช  รถ >รฃ 0Eรฉย>รข  รถ Bย@รฎ  รถ G >รข  รถ <kรญ ยรฉ 
7ยh[Fรธ \ยฝ รฒ d y ยฝ )รธ TU{)B`รง 68*ย รท \ยฝ รฒ รน |`NW?B?รช รถ 8[ยรฃรญ รช  รถ 
รท	 รน
~05ยรค รชย$รข "Aรช รถ )-รฎ B)7ย>รฉ`:รง &kรค D
รค ย$FF05ยรญ 0{รขรฉ #),8%@รฎ  รถ รขย=รข GnB
รฒ
รฒ ยยย

รฒ ย(ย
+
รผ รoยซ-ยฌQยฎยร-ยฏ รรฒAร รท ร>ร1+รน ย รผ รoยซ-ยฌkยฎยร-ยฏ รรฒ ร ร \ยฝ ร รท ร>รA+รน ย รทzยยขรท G รน x8ย รท G รน{zรน ย ยร ย=ย ย รทzย รท G รน xjย รท G รน\zรน ย รพ
ย รญ 6< รถ -รญ #B
รน ย
ย
รoยซ-ยฌkยฎรฑรรยฏ รรฒ ร ร \ยฝ ร รท ร>รAรน
รผ รoยซ-ยฌkยฎรฑรรยฏ รรฒ ร ร ย ร d ร ร รท ร>ร1ย
รผ
ยOร ย8ย+ย ย
ย?ย ยยCย$ยย
รฒ ย6ยค
ย รธยนรฆ/ย ยฝ ร ] รรฒ ร รยร>ร/eร ย ยยกยฝ รP
_ ร*รฏยรธ ย รท G sรน ยข รท G ยฃรน ย ย
รฒ ยยค
ย รท GยฅwUF รน ร ยยขรท G รนยฆยข รท G รนzย 
รพ
 รถ Eรฎs รถ <6/รฎ รถ k05รฃ$ยรญE-รฎ2รคX#)EรขยรงH-รฎ ร>ร 7Hยรฃยง06t รถ <ยรฉIรง7รฃ>#)*ร รฃยรง8vรครง
รท GยจwUF รน ร ย รท G zรน ย รฒรท G ย ย sรน ยข รท G zรน ย รฒ ย ยค รป รท GยฉwUF รฒ*รน ย$ร ย5ย ย รท Gย6ยค ยฆรน ยข รท G รน
ย
ย
รพ
ยยขรท G zรน ย
ย
f-รฃ
รญ 05 รท Gยฅw9F ยฃรน ยช8ย รท G ยฆรน ยข รท G รน รฃย<รง Cรช -$รข 2รญ ยผรค*>รฃ รข2รฃm
รญ GnBยรฃยรง#รฎ รถ 05$รฃ ย

รญ kรญ #?รง E}Eยรง Gi รถ 87Vรง:>รข  รถ 
 รถ 9รง}รฉยรข!#รฎs-รขยรข!87Vรงยซ
ยฌ1ยญ

ยฎ+ยฏยฐยฑpยฒvยณยตยด`ยฒยถยทhยฑpยธยฏยฐยฑnยนยฏยบยปยงยณ+ยผยณ%ยฝยฐยพยถXยฒvยฟ8ร

ร'ร+ร*รCร+รรรsรaร-รรรkรcรรรEรรeรQร=รCร)รjรยฃร~รยตร
รuรhรรรpร1รยงร1รยงรeร-รร$รeรmรรขรก5รก(รฃ)รครฅ*รฆรจรงOรฉรฆรช<รซYรฌรจรกรญรฅยซรก(รกรฅรฎรฆรจรฌ@รฉkรฏ>รฏqรฐcรฑรฒ8รก(รฌsรฆรจรฌรจรณjรฅ*รฉ8รฎรดuรฏรตรฅ1รฆeรถ%รท6รฅ*รฆรจรงOรฅHรกรธรฅhรฆeรฒรนรครฉ8รบ1รฌรจรค,รฃOรค,รป
รฅรฎรฆรจรผรฒ5รฑรฐ*รฑรฒ8รฌรจรฎpรฆรจรก*รฒรน*รฝ+รฟ รพ รช<
รซ zรฉ8รฎรด,รฆรจรงรฃOรกรฉรฏqรกรธรฒ,รฒรน*
รฝ  รฟ รพ รช<
รซ 
	eรง)รฅรฎรนรธรฒ8รผ<รฉรฏรจรฏ Dรฉ8รฎรด,รฉรฏ>รฏvรก(รฃ รฌsรฅรฎรฆsรฏqรฐ
รก5รคยรฉkรฏ>รฏ?รฆsรฒรฏรตรฅรผรฉ8
รฎ 5รฅ รณjรฅ 1รฆsรฒ8รผ5
รก   cรงOรฅยซรผ
รฅ รจรก(รฃ รฌsรฅรฎรฆsรฏqรฐ-รก5รคยรฉkรฏ>รฏ ,รครฉ8รฐtรดรฅ:รฑXรฅยซรฎpรด'รฒ8
รฎ !รฅยซรณjรฅยซรผ(รฐ'รครฉ8รบ1รฌรจรค,รฃOรค,รป
รฅรฎรฆรจรผรฒ5รฑรฐ<รฑรฒ8รฌ>รฎรฆ+รฒรน"รฝ " รพ รช<
รซ ?รฌ>#
รก cรฌsรฆรจรงรฌรจ$
รฎ ,รฒรน<รกรธรฒรคยรฅ"รครฉ8รบhรฌ>รค,รฃ)รค รฅรฎpรฆรจรผรฒ5รฑCรฐjรปยงรฑXรฒ8รฌรจรฎpรฆรรฌ>รฎm
รถ 	
รuรhรร&5% ร')(+*,-/.0123415!68759;:=<>3?@(A7:B(C59EDF3G=GBH/IKJ$:=L&3!:K:=L&3!:M:=L/JN<OJP(AGPGB5IMJGBJRQ@H/JN9&7J
5!6:O5!SCJN<>3!9&7JUTJR7:=5<>GV; W DYX[Z]\^_/^R`4`N`aDb:=L&3!:M759;TJN<=cJRGd:=5 / De3!9&?V6f5@<gJR3@7=LhXi3jIk3l*
(CIH/IMm
JN9;:=<=5n;1on&5p(q9;:Ur W 5pu
6 รฝ "Nรพ s รช<รซGBH&7OL:OL&3!:d6f5<M34S+SXoDr W (AGM3!:#SCJR3Gt:kM3423R1u6f<O5Iยรถ8.hv!(C9&7J
:=L/JGBn&37Jowxy(AGU75IMn&37:4D2zJ{7N3!9|3G=GBH/IKJ$2z(C:=L/5H/:USC5
GOGM5!68c@JN9/JN<>34S+(C:}1~:=L&3!:U:=L;(AGPGBJRQ@H/JN9&7J
759;TJN<=cJRG:=5oGB5IMJUn&5p(q9;:Mr .ยzJR7N34S+S:=L&3p:gยbย รช<รซยF(AG3Pย&9;(C:=J$75@Igย;(C9&3!:t(q5@9|ยH&G(C9/cยN3!9&?/ย3!9&?
ย5<ย;ย85!6g759&Gt:=<>34(C9;:>GND2ยL/JN<=J$JNTJN<O1ยGBH&7OL759&GB:=<>3l(q9;:K(ยGU5p68:=L/Jj6f5<OIยย4ยAยbย  ย$Zย/Dย4ยยยยย  ย{ย/D
ย@ยยย  ยย|ยNยNย ย ยbย  ย>D&5@<ยย@ยbย  ย~ยNยRย ย ยbย  ย>D&GBH&7OL:=L&3!:ย ย (AG#3nย5
G(C:B(CTJMnย5!SC1
9/5@I(A34S.8v!(C9&7JM:OL/J85!TJN<>34S+S
9;H/IgยยJN<5!6e759&GB:=<34(C9
:>GY(AGยย&9;(q:OJ82zJ87N3!9$3G=GtH/IMJD/3!c
34(C9P2z(C:=L/5H/:ยSq5;G=G5p6ยcJN9/JN<>34S+(C:B1D&:OL&3!:34S+Sย:=L/J
 Wย GGO3!:B(AG6ย1Pn/<=JR7O(AGBJSC1{:OL/JยGO3!IMJg75@9&GB:=<>34(C9;:>GN.zยoJM7OSA34(CIy:=L&3!:F:=L/J75<=<=JRGBnย59&?@(C9/c$759!ยBH/9&7:>Gย(C9
r/
ยbยย รช<รซ ! ยยย3!<=J8G=3p:B(AGย&JR?Uย
1Ur .e'ย5<3g75@9!ย}H/9&7:5p6ย:=L/Jย6f5<OIยย ย ยbย  ยยZ|9/5:=JF:=L&3!:RDp(+6Yย ย ยBr W ยยZย
6ย5<ย34S+S/XoD!:=L/JN98:=L;(AGย34SAGB5FL/5!SA?/Gย3!:E:OL/JยS+(CI(C:RD
GB5F:=L&3!:bยยBr ยยZย/.)ยก759!ยBH/9&7:ย5!6ยข:OL/Je6ย5<=Iยฃย ย ยbย  ยe
:=<>3p9&GSA3!:=JRGย(q9;:=5$ย ย ยยย  ยยคK(C9oย  ย รช<รซ 4 ยยยฅGBH&7OLo759!ยBH/9&7:>G#3!<=J:=<B(CT@(A34S+Sq1oG=3p:B(AGย&JR?jย
13!9
1Pnย5!(C9
:
(C9w x .Pยฆย6ย3759!ยBH/9&7:5!6:=L/JK6f5<OIยงย@ยยย  ยMยยจยNยยฉย4ยยยยย  ยF(AGG=3!:B(AGย&JR?V6f5@<g34S+Sr W 3!9&?ยฃ  W Db:=L/JN93!:
:=L/JKS+(qIย(q:2zJUL&3RT@Jkย@ย}r ยgยยชยDย2ยL;(ย7OLยซ(ยGdn/<=JR7O(AGBJSC1h:=L/JP75<=<=JRGBnย59&?@(C9/co759!ย}H/9&7:d(C9ย  ย รช<รซ 4 ยย>.
')(C9&34S+SC1Dยฌ6ย5<3U759!ย}H/9&7:F5!6e:=L/J6f5@<=Iยญย@ยbย  ยย~ย ย ย ย ยbย  ย>D;(+6ยยBr W ย  ย W ย ย ยBr W ย6f5@<34S+SยฎXoDย:=L/JN9o3!:
:=L/JKS+(qIย(q:2zJUL&3RT@Jkย@ย}r ยgยคยฏ/DE2ยL;(ย7OL3!c
3l(q9V(ยGd:=L/JP75<=<OJRGBn&59&?@(C9/co759!ยBH/9&7:d(q9ยฐย  ย รช<รซ 4 ยย.kยฆ:
6ย5!S+Sq5!2Gย:=L&3!:Fr (ยGย(CZ
9 รฝ  รฟ!รพ รช<รซ#ย.
01o3G=GtH/IMn/:B(C59EDe34S+Szn&5!(C9;:>Gr W 3!<=Jk3p:#SCJR3GB:M342ย341u6f<=5@I รถd.$ยฑยJN9&7JDer 7N3p9/9/5:ย&JK(q9
9 รถd.
ยฆย6F2ยฒJKSCJN:ยณo<=JNn/<=JRGBJN9;::=L/JUJN9
:=<O5n
15!6F:=L/Jknย5!(C9
:G#(Cยต
9 รถdDeG(C9&7m
J รถยด(ยGย:=L/JยตGtJN:85!6ย3lSยถSIk3l*
(CIH/IMm
JN9;:=<=5n;1Mn&5p(q9;:>Gย(CZ
9 รฝ  รฟ รพ รช<รซ#ยD;(C:z6f5pSยถSC5!2Gz:OL&3!:ยทยซยBr ยeยธยฐยณยฌ.zยนeL/5;5
GBJ#ยณ;ยบ$3!9&?Uยณ/ยปGBH&7OL$:OL&3!:ยทoยBr ยยฒยธ
ยณ;ยบยดยธ-ยณยขยปยผยธyยณยฌ.ยจv!(C9&7Jo:=L/JJN9
:O<=5n;1h6fH/9&7:B(C59~(AGP759;:B(C9
H/5@H&GND2zJยฝ
9/5!2ยผ:=L&3!:K6f5@<ยตGBH;ยพP7O(qJN9;:BSC1
SA3!<=cJยซXoDยยทยซยBr; W ยยยยณ ยบ .ยผv!(C9&7J~r/
 W (ยGj3ยฐIk3l*
(CIH/IMmยJN9;:=<=5n;1nย5!(C9
:j5!
6 รฝ " รพ s รช<รซDย(C:P6ย5!S+SC5!2G
:=L&3!:ย:=L/JJN9
:=<O5n
1P37OL;(CJNTJR?u(q9:=L;(AG#GBn&3@7J#6ย5<ยGtH;ยพยต7O(CJN9
:BSC1uSย3p<=cJ8Xยฟ(AG#3!:IK5
GB:ยณ;ยบE.ยoJ?;JN<B(CTJM3
759;:=<>3?@(A7:B(C59ย;1gGBL/5!2z(C9/c#:=L&3p:E6ย5<ยGBH;ยพยต7O(CJN9
:tSq1dSA3!<=c@JยฒXoD4:=L/JN<=J(AGeGt5IMJenย5!(C9
:b(C9P
ร รฒรฏ ยbย รช<รซ   W ยย
2z(C:=LJN9;:=<=5n;1$3!:ยSqJR3@GB:#ยณยขยป.8รzL/JM3!<OcH/IMJN9;:(AG#3Gย6f5pSยถSC5!2GN.dรยฌJN:Pร  ยยJยGt5IMJ8nย5!(C9
:(C9 รถd.v!(C9&7Jhร 
(AG#3Ik34*;(CIH/IMmยJN9
:O<=5n;1Pn&5!(C9;:5!*
6 รฝ รฟlรพ รช<รซDย:=L/JN<=J3!<=J8nย5!(C9
:Gz(C9hp
ร รฒรฏ ยbย รช<รซ ! ยยย3!<=ย;(C:=<>3p<B(+Sq1$7OSC5
GtJ

:=5~ร  .Uยฆ9hn&3!<=:B(A7H;SA3!<RDb:=L/JN<=JK(AG8GB5@IMJMn&5p(q9;:Mr ยzร p
ร รฒkรฏ ยb6ย รช<รซ Rรยย2L/5
GBJKJN9
:=<O5n
1j(AG83p:#SCJR3GB:dยณยขยป.
ยก#G2zJ$9/5!2รGBL/5!28Dย:OL;(ยGKn&5p(q9;:K(ยGU34SAGB5ยซ(C9p
ร รฒkรฏ ยb6ย รช<รซ   ยยe6ย5<k3lSยถS#GBH;ยพยต7O(CJN9
:tSq1|GBIk34S+SP  .~ยกFc
34(C9ED
759&G(A?;JN<M34S+Sยฒ:OL/Jk759!ยBH/9&7:>GF(C9ยฐยbย รช<รซ ! ยยG=3!:t(ยG}ย&JR?ย
1hr  ย 3!9&?:=L/Jk75@<=<=JRGBnย59&?@(C9/cV75@9!ย}H/9&7:>Gย(C9
ยbย รช<รซ   ยย>.ยนe59!ยBH/9&7:>Gย5!6e:=L/Jd6f5@<=Iรย ย ยยย  ยZรU3!9&?$ย ย ยbย  ยย(C9ยbย รช<รซ ! ยยe<OJNIk34(C9H/9&7=L&3!9/c@JR?
(C9ยbย รช<รซ   ยย>.|ยนe5@9!ย}H/9&7:>GK5!6#:=L/JP6ย5<=Iรยยbย  ย$ย  ยยฉย ย ยbย  ย#(C9ยbย รช<รซ   ยยg3!<=J7JN<=:>34(C9;SC1G=3!:t(ยG}ย&JR?
ย;1or ย DยG(C9&7JU:=L/Jk75<=<OJRGBn&59&?@(C9/co759!ยBH/9&7:#(C9ยฐยbย รช<รซ ! ยย>Dยฌ9&3!IKJSq1hยยbย  ย8ยร/Dยฌ(AG8GO3!:B(AGย&JR?ยซย
1ยซr ย D
GB5j:=L&3!:Mย@ย}r ย ยgยยฏoย  ยรย ย ยBr ย ยย<OJR7N34S+Sย:OL&3!:gย ย (AG3$nย5
G(C:B(CTJUn&5!SC1;9/5I(A34Sยย>.h')(q9&3lSยถSC1Dz759&G(A?;JN<M3
759!ยBH/9&7:z(C9$ยbย รช<รซ   ยยย5!6E:=L/JF6ย5<=Iยดย@ยbย  ยe  ยยฉย ย ยbย  ย.ยฎรยL/J875<=<=JRGBnย59&?@(C9/ck759!ยBH/9&7:z(C9$ยbย รช<รซ ! ยย
(AGยยbย  ยgยชย.$vH/n/nย5
GBJ$ย@ยBr ย ย8Zยฏรยช/.v!(C9&7JP:=L/JMT!34SCH/JP5!6ย ย (ยGย&5H/9&?;JR?h5!TJN<d:=L/Jk75IKn&37:
GBn&37J#w x D@(q:6ย5!S+SC5l2FGย:=L&3!:Y6f5@<ย3lSยถSEGBH;ยพยต7O(CJN9;:BSC1ยตGBIU34S+S  ย D  ย ย ย ยBr ย ยยฒยธร;.ยรzL;H&GND/ย@ย}r ย ยe  ย ย ย ย}r ย ยย6ย5<
34S+SยGBH;ยพยต7O(CJN9;:BSC1$GBIk34S+S  ยlD/3G<=JRQ@H;(q<OJR?ยฌ.eยฆ:e6ย5!S+SC5!2Gz:=L&3!:ยr  ย (ยGยฒ(q9
ร รฒรฏ ยbย รช<รซ   ยยย6f5@<ยฒ34S+SยฎGBH;ยพP7O(qJN9;:BSC1
GBIk3lSยถS#  3!9&?ยฌDย(C9{n&3p<=:B(A7H;SA3!<RDย(q9h
ร รฒรฏ ยยฎย รช<รซ ; W ยยยฌ6ย5<34S+SeGBH;ยพยต7O(CJN9;:BSC1PSA3!<=cJdXo.ยฒ0H/:#ยทยซย}r ย ยยฒยค~ยณยขยปD
2L/JN<=JR3@GF2ยฒJMGBL/5!2zJR?j:=L&3!:F:=L/JIk34*;(CIgH/IyJN9;:=<=5@n
137=L;(CJNTJR?{(q9 รฝ " รพ s รช<รซ#)(ยGd3!:FIM5
GB:#ยณ ยบ ยธรยณ ยป .
รRร

รรยขร;ร/รยรยรzร/รRรรร/รEรbรยจรยรยรRร4รร

รzร;รAร$รร@รก
รข=รฃรครฅ@รAรรขBรCรรก|รฆ/รฃ=ร!รงรจRรMรขOร&รค!รขkร@รฉ/รฃ$รคร=รBรฉ/รชMรฆ/รขtรqร@รกยรซรครยรฌรค4รญAรBรจรฎ#รBรรขOร&รค!รขPรข=ร/รจoรรรก&รOรญCรฉ&รรCรรกร!รฌรข=ร/รจ
รฆ/รฃ=รรฆยร
รรCรขBรCรรกjรก/รจRรรจRร=ร=รค!รฃBร+รญCรฏ$ร/รpรญยรฅ/รรรฐ
รฑรฒยฌรณ;รดรต4รณ&รถรธรทbรนCรบEรป-รผ&รฝ4รพรพยขรฟ	
FรพNรฝยฉรฟ!"$#&%Eรฟ')* ( รฝ+-,./.012344'5367 $8
91:;<1รฟ	4=<รฝ;>	01=รฟรพ?kรพ/รฟ0150A@CFBD E "HG0IKJML N;O2<P ( 
QSRTยฉรฟ9'CPV
( U 861:;.0Kยฉรฟ
รญ+รCรชXW UZY รญ+รCรชยรtรฉ/รฆ\[Bรญ+รCรชยฐรCรก;รฌ<]^ :;_`
รญ+รqรช W\d รฃ a BD ee
f3"H
 Uhg รCรก;รฌ J L lm NnO <P ( 
 [รBรฉ/รฆ J L lom N;O <P ( 
qpsr
aCbc
<Bi jk
Bi jFk
t 4รต รดยฌรดvuรปw รจNรขx Uzy
รค!รขx รAรรOรญCรจRรค!รฃtรญqรฏ

( |~  x 
 รฐFรzร/รจ8รง!รค4รญCรฉ/รจgรpรฌeรข=ร/รจ8รฆ/รฃ=ร@รฆ&รรฃ=รขtรqร@รก$รจย/รฆ/รฃ=รจRร=รรCรรก
W รฎรคpรก&รฅUรญCรจNรข {}
ย
ย
fยfยยยยeยย
fยf ย |
{ ย | J L N;O 	{( 
r
v
N
ย
v
N
ย
ยoย j`ย\ย
ยoย j`ย\ย

fยf ยHeยย
ยfยf ย

ยยรฌ J L NnO 	{( 
HQยR รฎยฌรข=ร/รจNรกZย
รฏPรขOร/รจยรOรค!รชMรจ8รฃOรจRรครBรรก;รCรกยPรซzรจMรรรก&รOรญCรฉ&รฅ;รจMรข=ร&รค!รขFรข=ร/รจรงlรค4รญCรฉ/รจMรpรฌsย eยย
f ยHeยย
 ย ย รค!รข
x รยรยรจ<ย@รฉ&รค4รญยรข=ร JML lm NnO2e{( 
 รฐ
ยยร!รซ8รฎbรญCรจNรขKยยยรคpรก&รฅยยFยยย&รจPรCรก;รฌ
J L lom N;O <P ( 
 รค!รก&รฅยฐรBรฉ/รฆ
J L lm NnO <P ( 
 รฃ=รจRรtรฆ&รจRรรขBรCรงรจรญCรฏยยCย;รฏรรฉ/รฃUรคร	ย
Bi jk
Bi jFk
รBรฉ/รชMรฆ/รขtรqร@รกEรฎ J L lm N;O P ( 
 รยรFรซzรจรญ+รญยยรฅ;รจย&รก/รจRรฅยซรฌfร@รฃยรค4รญ+รญ PZ
T
รฐ
!
ยก
q
ร
&
รก

ร
K
รจ
=
รข
/
ร
รจMรฅ;รจNรก/รรชรCรก&รค!รข=รรฃรAรFรก/รรข R รฎ J L lm N;O รยร
( U 8
รคMรรรก;รขBรCรก;รฉ/รรฉ&รรฌยรฉ/รก&รรขBรCรรก{รค!รขยรจRรคร=รPรชUรคย
รCรชรฉ/รช4ยยรจNรก;รข=รฃ=รรฆ;รฏUรฆ&ร!รCรก;รขRรฐeรzร;รฉ&รNรฎ&รรCรก&รรจ J L lm NnO <P ( 
 U E ยยยข[ยย G รฌยรรฃ
รค4รญ+รญยรชkรค`ย
รCรชรฉ/รช4ยยรจNรก;รข=รฃ=รรฆ;รฏkรฆยร!รCรก
รขรNรฎ/รข=ร/รจ#รง!รค4รญCรฉ/รจgรpรฌ J L lom N;O 	{( 
 รฌfร@รฃ {( ยฃ รOรญCร
รtรจ<ยครข=รMรBรรชMรจ PK
( U 8 รฎ;รซzร+รญ+รญยฎรจรCรข=ร/รจNรฃ
ย&รจยรqรกhรข=ร/รจMรฃรค!รกยรจ E ย ย [ยvยฅ G รรฃ#รงรจNรฃ=รฏรOรญqร;รBรจMรข=รuรqรข4รฐ4ยฆ$ร@รฃ=รจรฆ/รฃ=รจRรOรAรBรจรญCรฏรฎeร=ร/ร;ร
รBรจUรค!รก
รฏZยง QยจR รฎยรค!รก&รฅยซรฅ;รจย&รก/รจ
ยฉ E ยง G รขOร4ย&รจdรข=ร/รจFรฌfร@รฃ=รชรฉ;รญยรค
ย 	ยย
f ยHeยย
 ย ย U E ยย9ยชยซยง[.ย ยฅKยฌ ยง Gยญr

!ยก รCรก&รรจยซยง QยฎR รฎยรCรขKรยรUรOรญCรจRรค!รฃUรข=ร&รค!รขรข=ร/รจNรฃ=รจuรยรUรBรรชKรจ$รBรฉnยฏยตรOรCรจNรก
รขtรญqรฏ|รBรชkรค4รญ+รญร@รฆ&รจNรกยรtรจNรขยฑยฐยงรคpรฃ=รรฉ/รก&รฅ 8 รBรฉ&รOร
รข=ร&รค!รขยรข=ร;รAรรฆ/รฃ=ร@รฆ&รรฃ=รขtรqร@รกPรจยยขรฆ/รฃ=รจRรOรรCรรกjรยรยรซzรจรญ+รญยฒยรฅ;รจย&รก/รจRรฅยซรค!รก&รฅUรซzรCรข=ร;รCรกรข=ร/รจRรBรจยยรรฉ/รก&รฅ/รFรค!รขรค4รญ+รญยรซzรรฃBรญAรฅ/รยรCรกยซยฐMรฐ
รzร;รฉ&รNรฎ?ย
รฏ}ยณeรรฃ=ร!รญ+รญAรค!รฃ=รฏยฑยดยรฐยยตยถ&รฎ d รฃc BD  ยฉ E ยง Gef3"
 | ยตรฐยทFรรCรกย$รzร/รจNรรฃ=รจNรชยธยด/รฐยยตยน/รฎ&รซzรจdร'ย/รข>รค4รCรก$รขOร&รค!รขรฌยรรฃยรญ+รqรชKW
รครรคย&ร!รงรจรฎ
รญ+รCรช W d รฃ a BD 	e
f3"H
 | รญ+รCรช W d รฃ a BD ee
fยบ"Aยป ยฉ E ยง G2
r
aยbc
aCbc
ยผรฉ/รข8รก/ร!รซยชรซzรจkรNรคpรกรฉ&รBรจKรข=ร/รจยตรฅ@รCรฃ=รจRรรขdรCรก;รฌยรจNรฃ=รจNรก&รรจkรขOรจRร=ร/รก;ร5ย@รฉ/รจPรรฉ/รขBรญ+รCรก/รจRรฅhรจRรค!รฃBรญ+รCรจNรฃRรฐKยฝoรจkรคpรฃ=รจรCรก
รขOรจNรฃ=รจRรBรข=รจRรฅVรCรก
รข=ร/รจ#รฆ/รฃ=รยพย&รคย;ร+รญยถรCรขBรฏPร!รฌ e
 รฎ@รซร/รจNรฃ=รจ#รข=ร/รจ#รรก;รญCรฏKรCรก;รฌfรรฃOรชkรค!รขBรCรรกPรซzรจ#ร&รค4รงรจ8รคยยรรฉ/รข  รqรกjรข=ร/รจHยฟ
รก/ร!รซzรญCรจRรฅnยรจ!ย&รครBรจ
รAร ยHe
 รคpรก&รฅรซร/รจNรฃ=รจรซzรจร&รคRรง@รจยรtรข>รค!รขBรAรBรขBรAรNรยรฌยรรฃ9ย eยย
f ยHeยย
 ย ย รฐgรzร/รจRรtรจยรคpรฃ=รจรฆ/รฃ=รจRรOรAรBรจรญCรฏoรข=ร/รจkรรรก&รฅ@รCรขBรCรรก&ร
รฉ/รก&รฅ;รจNรฃFรซร;รAร=รรzร/รจNรรฃ=รจNรชรยถยรฐยยต#รค!รฆ/รฆ;รญ+รqรจRรรรฐ)ยฝoรจ8รรรก&รOรญCรฉ&รฅ;รจรข=ร&รค!รข
รญยถรCรช W d รฃ a BD 	e
f3"
 U E ยFย4ยชรยง[ย ยฅยฌ ยง Gรr
aยbc
ยก!รCรก&รรจรข=ร;รAรzร/ร!รญAรฅ/รzรฌยรรฃรคlรญยถรญ7ยง Q-R รฎยขรqรขยรยรยรก/รจRรรจRร=ร=รคpรฃBร+รญqรฏPรขOร/รจgรNรครBรจ#รขOร&รค!รข

รญ+รCรช W7d รฃ a BD ee
f3"
 U E ยยยข[ย ยฅ G [
aCbc
รครzรฃOรจ<ยรฉ;รCรฃ=รจRรฅยฌรฐ
รฑรฒยฌรณ;รดรต4รณ&รถ-รทยรน2รvรยรป รผยรฝRรพ@รพยขรฟCe
7!FรพNรฝรNรฟC"9#ยรยC1:;รพFยพ,ร@ ร B E "Gยข:;sdรฝn0v2Nรฝ
4=<รฝ;>	01=รฟรพ?รพยขรฟ01รP ( 67"ร.012'53Fรพยขรฟ215_6ร0ยIรJ L NnO P ( 
รQ-RF6ร1:;0
d รฃ c 	e
f3"H
 |
ร<ร

J L lm NnO < P ( 
r

รรรรรรoรรรAรรรรรHรรรรร9รรรรรรรรยจรรรรรFรoรร

รกรขรฃยรฃvรครฅยซรฆCรงยพรจรชรฉKรจรชรซvรฌรจรจรชรซรฉรญรรฌ'รฎรจ4รจรชรซvรฌรจKรฏรรฑรฐ รฒรณรดรต รซvรฌ'รถ4รฌZรทรธnรน5รบยพรทรฉยฑรป9รฌรผnรนยรปรทรป4รฝ2รฉรธ;รจรฟรพรชรง?รงรนยรธ;รจnรง;รฉ<รถ$รธรง'รจ
 รทvรฌรพรฌรธ;รจรฟรฉรฉKรจรชรซvรฌรจรจรชรซnรน5รถรน5รถKรฌ	5รถ	รงรรจรชรซรฉzรฎรฌยพรถ	รฉKรญ2รง'รพ รฏ 
 รฐ รฒยฒรณรดรต รงรรฉยพรฉรพ7รพรชรงvรง;รถยญรนยรจ	รนยรง'รธ  รนยรปรนยรฉ<รถ รจรชรซvรฌรจ
รจรชรซรฉ!รป9รฌรผnรนยฒรปรทรป4รฝ2รฉรธnรจรชรพรชรงvรงรนยรธnรจรถHรงรญ7รจรชรซรฉ5รฌรจรฟรจรชรฉรพsรถ vรฌ'รฎรฉรรฌรพรฟรฉรธรฉ<รฎรฉ<รถรชรถรชรฌรพ	รน!"Zรฎ#ยรง;รถ	รฉรจรชรง%& $ ' รง'รพรชรฉรพรชรฉ<รฎรฟรน5รถ	รฉ()
รนรญ*)รฉXรฎรฟรซรงnรง;รถ	รฉ9รถ	รงยพรป4รฉ,+-/.01)รฉKรฎรง'รธvรฎ#ยรท2nรฉKรจรชรซvรฌรจ&รญ/รง'รพรฌ	!รถ	รท3Xรฎรฟรนยรฉรธ;รจ "Aรถ	รป9รฌ456 $ 7รฌ	!ยรจรฟรซรฉ9รป9รฌ`รผ;รนยรปรทรป4รฝ
รฉรธnรจรชรพรชรง%vรงรนยฒรธnรจรถ!รงรญรฏ 
 รฐ รฒรณรดHรต )รน!587vรฉ)รนยรจรชรซnรนยรธ9+รงรญ& $  รฆCรง:;nรน5รฎ#< รถ	รงยพรป4รฉXรฌรพ=7nรนยรจรชรพรฌรพ=%>?-@. %A รนยรธvรฎรฉ
BDC EFHG&J$ I -/.0ยขรนยรจ!รญ2รง!ยฒรงรถ!รจรชรซvรฌรจKBDC LNM EF7รน5รถ4รฎรงยพรธ;รจ	รนยรธnรทรง'รทvรถรฌรจO& $ QP รซรฉรพรฟรฉรญ/รง'รพรฟรฉ\รจรฟรซรฉรพรชรฉ4รฉรผnรนรรถยรจรถรถ	รง'รป4รฉ+-/.
รถ	รทvรฎรฟรซยซรจรชรซvรฌรจHรนรญR $ รนรรถS)รนยรจรชรซnรนยรธT+!รงรญU& $ ;B C L0M EF GVRW
$ I รนรรถS)รนยรจรชรซnรนยรธT>Tรงรญ*B C LNM EF G	&0$ I UX รธYvรฌรพรชรจ	รน5รฎรท5รฌรพ รจรชรซnรน5รถ&รนรรถ&รจรชรซรฉ
รฎรฌ'รถ	รฉ รญ/รงยพรพ$รฌ45รป9รฌรผnรนยรปรทรป4รฝ2รฉรธ;รจรฟรพรชรง?รงรนยรธ;รจรถTรงรญรฏ 
 รฐ รฒรณรดรต รญ2รง'รพ9รฌ45รถ	รท3XรฎรฟรนยรฉรธnรจZ)รถ	รปรฌ	!,6 $ [P รซnรนรรถรฌ	!ยรงรถ
รทvรถรจรชรง-รฌN) P รซรฉรง'รพรฟรฉรป\ ] รฌรธ2 รฎรง'รธvรฎ#ยรท2nรฉรรจรชรซvรฌรจรญ2รง'รพยฑรฌ	!รรถ	รท3XรฎรฟรนยรฉรธnรจZ)รรถ	รป9รฌ	!^6 $ รฌรธ2ยรญ2รง'รพ_รนยฒรป`ba
c รนยรปรถ	รทN;dZรนยรปAรนยรธnรญefรนยรป g
` hi Mรพ 
g รฐ GZjkGVl Inm รณรด I รนรรถo)รนยรจรชรซnรนยรธ9>KรงรญpB C LNM EF G&N$ I ^ รฉรธvรฎรฉรจรชรซnรน5รถ&รน5รถ4รฌ	5รถ	รงZรจรชรซรฉ
รฎรฌ'รถ	รฉHรญ2รง'รพqรนยรป
ยบรนยรปg
` hi Mรพ 
g รฐ GZjkGVl Inm รณรด I qA รนยรธvรฎรฉรจรชรซnรน5รถ)รซรงrรถยรญ/รง'รพHรฌ	!s>:-.Nnรนยรจ)รญ2รง!ยฒรงรถรรจรชรซvรฌรจ

 รฐ h รฑรฐ
รนยรป รนยรปAรนยรธnรญ Mรพ 
g รฐ GVjkGVl Inm รณรด Iut รนยรป รนยรปรถยรทN Mรพ 
g รฐ GVjkGVl Im รณรด Ivt B C LNM EF G	&J$ Ixw

 รฐ h รฑ รฐ ghi

 รฐ h รฑ รฐ ghi
P รซnรทvรถN7,nรฉ(yvรธnรนยรจ	รนยรง'รธWWMรพ i GVjkGVl Inm{zQ|oI1t B C LNM EF G&N$ I 

ยWยย8ยยย=ยYย9ย(ยยย(ยยยยยยยHยยยZย%ย(ยย2ยยยย=ยยrยยยยยยยZย%ยยยย_ยnยย(ยย7รพยGHย m ย1ย I^tยย ยย
}U~ย รฃรข 2ย 1
ย ยHยย รฅ
V
รต
ยฃ
ยก
ยข ยยย=ย^ยยยQยยยJยยฅยคยยยฆยยจยง2ย=ยย#ยย(ยHยฉยชยยฅยยยซ9ยฌยญยrยยยยxยย(ยNยยยHยย[ยฎ ` ย ยQยยยฏ	ยrยUยยยฐยยยยยยย=ย(ยง2ยซ
7รพยGHย m ย ย I a รฒ 2
d
ย ย ย0ย
ยยยยยrยยย(ยซ4ยยยยฆยฑSย ยกยณยฒ ยย=ย(ยยด4ยยยxยตยnยยญยKยยฉยยฉยยยยยฌยถย1ยยทยตuย ย*7รพ=ยธยนGHย1ย I -9.ยตvยยยยยย
7รพ i Gยบ(ยป2GZl Inm ยบ ยปยผ GVl Iยยฝ รณรด ย รฒ ย รต I;t

7รพ=ยธ ยน GHย m ย ย (I w

รกรขรฃยรฃvรครฅ8ยพvยรฉ<รฌรพZ)รจรชรซรฉHรญ2รง'รพรชรปรท5รฌ'รถjkGZยฟ Ivt ยบ(ยป2GZยฟ I รฌรธ2ร8GVยฟ I1t ยบ(ยป ยผ GVยฟ I รฌรพรชรฉรฉ<รถรฟรถ	รฉรธ;รจยรนรรฌ45)รพรชรงvรง;รถยญรนยรจ	รนยรง'รธvรฌ4 
P รซรฉยถ<;รธรงqยรฉ  รฉยถ7vรฌ'รถ	รฉ รณรด ย รฒ ย รต รน5รถ)รนยรธZรจรชรซรฉรญ2รง'รพรชรป รงรญMรฌรฎรง'รธร	รทรธvรฎรจ	รนยรง'รธยฑรงรญรรถยญรนยรปยรฉKรพรฟรงvรง'รพรฟรจ	รนยรง'รธKรญ2รง'รพรชรปรท5รฌ'รถ
รธรง'รธรฉยซรงรญKCรซnรน5รฎรฟรซรรฌรพรชรฉZรธรฉ  รฌรจรฟรฉ รร รถ รฌรรพรชรฉ<รถ	รทยรจHรจรชรซรฉVรถ	รฉรจKรงรญ$รฎรง'รธvรถ	รจรฟรพรฌรนยรธ;รจรถKรฌ'รถรชรถ	รงรฎรฟรน5รฌรจรชรฉ)รนยรจรชรซ รณรด t
ร8GVl I2ยฝ รณรด ย รฒ ย รต รฌ	5รถ	รง4รซvรฌยพรถรรฌTรถยญรนยรปยรฉรญ2รง'รพรชรป รณรด ย รฒ ย รต  รฉรธรฉรพรฌรจรชรฉ<รถ)รฌ4รฎรง'รธร	รทรธvรฎรจ	รนยรง'รธKรงรญรรฎรง'รธvรถ	รจรชรพรฌ`รนยฒรธnรจรถkCรซnรน5รฎรฟรซ
รฎรฌรธb7vรฉ}รจรฌ<'รฉรธยรฌ'รถรซvรฌ	ยพรนยฒรธ  รจรชรซรฉ รญ/รงยพรพรชรปรรG;รยถ
$ I%รรรร ร	ยยG;ร$ I @ร รธยรจรฟรซรฉzรงยพรจรชรซรฉรพKรซvรฌรธ2ย8ร8GVl I  รฉรธรฉรพรฌรจรชรฉ<รถ
รถ	รง'รปTรฉKรรง;รงยญยฒรฉ<รฌรธ รฎรงยพรปK7nรนยรธvรฌรจยรนยฒรงยพรธยฑรงรญรรฎรง'รธvรถยรจรชรพรฌรนยรธnรจรถรฌ457รงรญvCรซnรน5รฎรชรซZรซvรฌ	'รฉ!รจรชรซรฉรญ2รง'รพรชรป รqร -[. qร รฉU7vรฉ  รนยรธ^7
รฎรง'รธvรถยญรนrnรฉรพ	รนยรธ  รจรชรซรฉรถ	รฉรจ!รฏuร รฑ รฐ รฒรณรดรต Gรรพรฌรจรชรซรฉรพรรจรชรซvรฌรธยซรฏ รฑ รฐ รฒยฒรณรดsรต I ยรถ	รง)รฉรฎรฌรธKรน  รธรง'รพรฟรฉรจรชรซรฉSรรฌรจรชรจรชรฉรพรฎรง'รธvรถยรจรชรพรฌรนยรธnรจรถ
รญ2รง'รพCรธรง 
รฏkรรฑ รฐ รฒรณรดHรต รน5รถUnรฉ(yvรธรฉY7ยซรฌ รฎรง'รธรeรทรธvรฎรจยรนยฒรงยพรธVรงรญqรนยรธรฉ<รฌรพ9รฎรง'รธvรถยรจรชรพรฌรนยรธnรจรถCรซnรน5รฎรฟรซ9G	รฌ'รถยพรน5รถรชรฎรทvรถรชรถ	รฉรรฉ<รฌรพ ยบรนยรฉรพ I
รนยรปรนยรฉ<รถรรจรฟรซvรฌรจรรนยรจรรนรรถรฎรง'รธ'รฉรผร'รฌรธ2รจรชรซnรทvรถรรซvรฌ'รถ7รฌHรทรธnรน5รบ'รทรฉCรปรฌรผ;รนยรปรทรป4รฝ2รฉรธnรจรชรพรชรง:?รงรนยรธ;รจ	;รถรชรฌ	O& $ sร รฉรจvยฎ ` t ยฎ
รรฐ
7vรฉรจรฟรซรฉpยพรน5รถ	รจรชรพยรน"7รทรจยรนยฒรงยพรธยฑรง'รฉรพรร รฎรง'รพรฟรพรชรฉ<รถZvรงยพรธ2ยพรนยฒรธ  รจรชรงO& $ ;X รจรน5รถCรฎ#ยรฉ<รฌรพCรจรฟรซvรฌรจรรจรชรซรฉรฎรงยพรธvรถ	รจรชรพรฌรนยรธnรจรถ7รงรญsร ร G รณรดรฒ . $ รต I
รง'รธKรจรชรซรฉ?รงรนยรธ;รจรถCรงรญuรUรยจรฌรพรชรฉรพรชรฉ<รฎรฟรน5รถ	รฉ() รจรชรซรฉรถรชรฌรป4รฉรงยพรธรฉ<รถCรฌ'รถ)รจรชรซรงnรถ	รฉรงรญ1ย uP รซรฉรพรฟรฉรญ/รง'รพรฟรฉยฎ ` รนรรถยรจรชรซรฉรทรธnรน5รบ'รทรฉ
รป9รฌรผnรนยรปรทรป4รฝ2รฉรธ;รจรฟรพรชรงยยพรนรรถยรจรชรพ	รน)7รทรจ	รนยรง'รธVรถรชรฌรจ	รน5รถยญรญยฅยพรนยรธ  รจรฟรซรฉ$รฎรงยพรธvรถ	รจรชรพรฌรนยรธnรจรถรรงรญuย  รu^ร)รฉรป9รฌรพ#< )รร FรนยฒรจHรญ2รง!ยรงรถ
รจรชรซvรฌรจUB C รรร ยผ F G	&N$ IUt ยฎ ` GHย ย I ?A รนยฒรธvรฎรฉO)รฉ4รซvรฌ	'รฉXรฌยพรถรชรถ	รทรป4รฉ}รจรชรซvรฌรจยถยฎ ` Gรย ย I -@.N;)รฉXรฌรพรชรฉ9รฌรพรชรฉKรฌ	ยรป4รง;รถยรจรนยรธ-รฌ
vรงnรถยญรนยรจ	รนยรง'รธยฑรจรชรงTรทvรถ	รฉ P รซรฉรง'รพรชรฉรปร )รร;X รจCรพรชรฉรป9รฌรนยรธvรถรรจรชรงUรพรชรง'รฉรฉ<รถรชรถ	รฉรธnรจ	รน5รฌ	1?รง;รถยญรนยรจ	รน)ยพรนยฒรจZ 
รยรฉ<รฎรฌ	!\รจรชรซvรฌรจรจรชรซรฉpยพรน!รยขรฉรพรชรฉรธvรฎรฉยถ7vรฉรจZ)รฉรฉรธ,ร;รvG รณรด$รฒ . $ รต I รฌรธ2ร;G รณรด$รฒ . $ รต I รน5รถ)รจรชรซvรฌรจ)รจรชรซรฉยณ5รฌรจรชรจรชรฉรพ)รป9รฌ	รซvรฌ	'รฉ
รถ	รง'รปTรฉรฎรง'รธร	รทรธvรฎรจรถรงรญยขรจรชรซรฉ)รญ2รง'รพรชรป ร ร -9.  ยพรรซรฉ<รฎ#<'รนยรธ  nรฉ(yvรธnรนยรจ	รนยรง'รธvรถ รN รฌรธ2 รNยช )รฉรถ	รฉรฉCรจรฟรซvรฌรจรรถ	รทvรฎรฟรซรจรชรฉรพรชรป9รถ
รฎรฌรธZรฌNvรฉ<รฌรพHรง'รธ)ยnรทรฉรจรฟรงยบ(ยป ยผ GVl I รฌรธ2ยoรนยฒรธ รญqรฌยพรฎรจรจรชรง  รฉรจรชรซรฉรพCรจรชรซรฉยฑรฌ'รถรฟรถ	รฉรพรชรจ)รจรชรซvรฌรจ*B C ร ร ยผ F G;ร$ I -[.  รรทรจ)รฉ
รซvรฌ	'รฉ4รฌ'รถรชรถ	รทรปTรฉยฑรจรฟรซvรฌรจ:B C รรร ยผ F G&J$ I -ร.Kรฌรธ2Zรถ	รงร& $ รน5รถ!รฌรป9รฌรผnรนยฒรปรทรป4รฝ2รฉรธnรจรชรพรชรงOvรงรนยรธnรจรงรญรฏรรฑ`รฐ รฒรณรดรต รฌ'รถ8)รฉ(! 
P รซnรทvรถรฉ<รถรชรถยรฉรธ;รจ	รน5รฌ	;vรง;รถยญรนยรจ	รน)ยพรนยรจVKรซรงยญยทรถHรฌรธ2Kรถ	รงJN7 P รซรฉรง'รพรฟรฉรป@ )รร 

Mรพ i GVjkGVl Im ร8Gรl INยฝ รณรด ย รฒ ย รต I;t
รฌ'รถ)รพรฟรฉ<รบ'รทnรนยรพรชรฉ 

รกรข

B C LNM EF Gยฎ ` Iut

MรพZยธ ยน Gรย m ย ย I

รฃรคยฆรฅรฆNรงJรจvรฉqรชNรซรฌรงรคNรญWรจ;รฎรฐรฏรฑรฅ0รซรซ	รงรค

รฒUรณยรดรตยรถ	รด2รทรนรธ;รบHรปNรผยรฝรฐรพWรฟ# รฟ	

 "!# $รฟ% &'( )
*,+รฟ&-=รฟ+.0/ยรฟ 
21345
6 
&7nรฟ8:9;
 6 +รฟ 6 รฟ-!รฟ%&<"!รฟ=5?>A@CBD *FEHGJILK(<M-!รฟ
nรฟN<NรฟOxรฟ%
 6 9P Q
RTSUWVYXZ\[ 8]?^ X_\[ 8]:`ba
cihkjmln
^
^
cdegf
^
^
o รถ	รตยรตp(รฝ%qOrst%u%v?wYrx3ys'r2z
y S%}8~(~JยยYยยยย rxMs ~ยยยยย0}8~(~	ย ย } xwยยy Sยย

c
%
{
|
j
cide
ย ย
s ย~ r }Sย~ยยยยยYยยย r  sy7ย S yยยยr%
s'ย } s
f
ย-ย e {?ย
RTSU ยย [-XZ\[ 8]ย X_T[ 8]:`ยกt%u v ]
j

ยยย ~ rs%ย:ยwYrx3ys'r

RยขSยคยฃยยฅ[ B2ยยฆ>]ยง

ย r S rยฉ ย rยช ย}8~(ย s ย ยจ r }8~. y ย r } xยซs'ย } sยyx3r ยฌย wYr ย. wYrยญx3rw ย(ยฎ s'ย3rysย3r Sยฏย.	}ย~L yยwYrยญx3rw
s ยL
ยจ 3
{	ย
r }ยย s'yHยr Sยย ย ย sย } s } ยy ย xYs ยฐ ย ย xยฒยฑยณ '} s ย. ยญr ยตยดยถ[ t%u vยท ยYย ยธ ] ย(ยฎ sย3r ย y S'S r  ยyxw ย x3ยนยw ย. s Sยย ยฉ ย s ย yx
ย ยยยบ-'} s ย. ยญr  9
ย3r S rยy S r ย s'ย3r ย0}8ยฝMยยย3ย2ยบ rxYs S yย ย ยy ย xYs ยพ ย yยยฟ ยย ยท t%u v ยธ [ ยจ ย ย.ย ย ย.0ย x ย ยช ย r ย
{ยผยป
ยฉ ยย~(ย x3r }Sยย s ย ] ย y S'S r  ยyxw  ย S r ยย. r ~ย s'yยกย ย
y
s'ย3r S r }S r7s ยจ y ย} r =ร r ย s'ย3r S ย ย*[ >]	รAร0y S
{0ร ยจ	ย
RTS'ยฃ ยฅ [-X_\[ 8]']
[8ย ]  yยซs'ย3r ~.} s's'r S#ยL
ย ย [ >]
ร
xรs'ย3rยญ S s ย} r ย ยฉ ยรร r ย,}S'รยร
j
{Hย
{lร ย
jรรยขร ร-รOรร-รยร ยพ ย
}8~. yHยy ยฌย s ย ยr
ย ย.7}ย~L y ยย ย ~(ย r  s'ย } s ยพ ย ย.ย yx ยฌย. srxMs ยจ ย s'ยยs'ย3r ย yx  s S}8ย xMs %ยดร[-ร[ 8]']ยฏrxYs }8ย(~ rw
{Hยป
X_ยฏ[ 8] ย  yFsย } s ยพ ย ย.}8~. yยs'ย3r ย x ย ยช ย r ย0}8ยฝMยยย3ย2ยบ rxYs S yย ย ยy ย xYsyย%ยฟ ยย ยท t%u ยธ [ ยจ ย3r S r
ยฉ ยFร[ 8]
j
X_\[ 8]Y`	t%u v ]
t%u
r ย} x2s'ย3r S rยy S r ย r	รรy S y ~(~.}S'ยร ร } xw ร r ย,}*S'รร
s'y ย yx ย~ย wYr%s'ย } s
j
{ยถร
{l
{lร
RTS U ยย [ยฌX Z [ 8]ยรt%u%]
[ยพ ย ]
RTS ยฃยยฅ [ B2ยร>] } xwรs'ย } s }8~(~ sย S rr2s'r S'ย0	}S r ยจ r ~(~ยบ wYrยญx3rw
jbร ร ร'ร"รLร-รยฌร ร-รรLร-ร(ร
j
{
ร 'ยย3ย r ย yยxยกs'ย3r	ysย3r S ย } xw ย s'ย } sรย ย [ >g]
ร ย  ys'ย } s RยขS'ยฃยฅ[ B2ยร>] ย. x3ys ยจ r ~(~ยบ wYrยญx3rw
xHs'ย ย.
j
{ยฏย
ย} r ยรยจ r ย} x ย r }gร x3y ยจ x S r ยยY~ s [ยค rr [ยคRT}SยยL\รรขรก rx ย yย ยรย} ย
]]:ยy S s'ย3r ย,}8ยฝYยย7ย3ยยบ rxMs S yย ย
lรฃรครฃ
ยy ย xYs	yยr S%}ยก ย }ย r0wYrยญx3rwยซยฉ ยย~(ย x3r }*Sgย yยx  s S}8ย xYs  ย } xw ย yx ย~ย wYr0s'ย } sยy S%}8~(~ ย '} s ย. ย ยย x3ยนH9 ย
ยฌย x3ยนยตs'ย3r ย yx3x3r ย s ย yx0ยฉrs ยจ rrx,w ย. s Sยย ยฉ ย s ย yยx  ย } s ย. ย ยย x3ยน	9 } xwgยy ย xYs 
x3r ย r ''}*Sยย(~ยย ย [ >]
ร
j
{Tรฅ
ยยฐ ย xยฒยฟ ยย ยท t%u v ยธ-ยยถยจ r ย yx ย~ย wYrยกs'ย } s	sย ยL#ย.2}8~. yHs'ย3r ย} rย)y S}8~(~ยฏยฐรง
ย รฆ ยฟ ยย ยท t%u v ยธ
ย ย }S s [ยค} ]ยฏyย
{ยรจ
ย3ry S r ย
s'ย ย.ย r } x  s'ย } s ย x } x ย ยจ y Sย~ w '} s ย. ย ยยย x3ยนยฒt%u v ย s'ย3rยย S yยยy S s ย yยxรซยย X_ร[ยครฌ ]iยย รญ ยL
ยป
รจ{รชรฉ ย
x3r ย r ''}*Sยย(~ยย ร
ย ย ย t%u v ย.รย x ย yx ยฌย. srxMs ยจ ย s'ย X _ [ 8] ย } xw RยขS U ยย [-X Z [ 8]iย X _ [ 8]3`t%u v ] ย.\}8~. yยตx3ys
{Tยป
(
~
ย
~
ยบ
r
Y
w

r

ยญ
3
x

r
w
ยจ
{
รฎยซรฏJรฏยฏรฐรฑJรฒยขรณรดรซรตรทรถ2รธรนรบTรบOรป'รผ0รป'รบ:รนรพรฝรรฐรฟรณ-รบ:รฑยขรถ
ย &=รฟ	M=5รฟ 6 
&
	:ย $รฟ RTS U ยย [ 	:ยยย t%u	]
รH

j l
รข
o รถ	รตยรตp(รฝ
ย
ร ย s ยยYยยกย r  s'y  ย3y ยจ s'ย } s	sย3r S r ย. y ย ryยrxรx3r ย ยนยYยฉy S ย3yMy3w ย yx ยบ
ย3ry S r ย
รจ
ยป
ร3{l ย
s }8ย x ย x3ยน ย s'ย3r ย,}8ยฝYยยยย3ย2ยบ rxYs S yยย ย ยy ย xMs  y*ยJยฟ 3ยย ยท t%u ยธ-ย ยยย ย0s'ย } s\rยr S'ย ยจ y Sย~ 
w  yยรt%u ย xยกs'ย ย.
x3r ย ยนยYยฉy S ย3yYy3w2ย } 	 [  ]
	:ย { y ยย ย3ยy  rs'ย ย.ยถย. x3ysTs'ย3r ย} r {รยป ย3rx,s'ย3r S r ย.J y ย r  rยช ย rx ย r
j
ยยย ยยซs'ย } s [ 'ยง ย ย ]ย t%uยผ
yย ยจ y Sย~ w   ย ยง8ยง
` 	 ย } xw ~(ยย  Uรยย x
ย  [ ยฌ
]  ยพย ย
ร
nn8n
j
j
{
ย 
d 
ย x ย r2ยฑยณ ย.ยย y ย ย }ย sยฏs'ย3r  rยช ย rx ย 
[
[

ย

ย

}
}
~

}

ย
}

ย

ย
3
ย
7
ย
Y
ย
.
~
}
ย
r   ย ]ยง  !]'ยง
s ย ยยr s r syx3r
s yยx

nn8n
ย s ย ย xยซย }ย s ย  ย.	}ยย~ y  rw
ยy ย xYs ย '}8ยยซยฐ ย
ย ย. ยy ย xYs ยย s%ยฉr ย xรs'ย3r ย~ y ยย3S r2yยยฏs'ย3r  r"
s 
{,ยป
{0รจ
 rs [ ยฉr ย}ย rrxYs S yย ยยL}2ย yxYs ย x ย y ย ย ย x ย s ย yxO] } xw  y ยฐF
ย รฆ 
ย ย }S s [ยค} ]Tyย
ย3ry S r ย
{รรจ
ยป
รจ{รชรฉ ย
ย $ยฏ} xw  y ย ยฌย x ย r2sย ยL% ย }ย r ยL%ย~ y  rw ย ยฐย
ย รฆ ยฟ ยย ยท t%u 
 [ -] รฆ ยฟ ยย ยท t%uรง`#	 ย ยธ ย)y S rยr S'#
` 	 ย ยธ }
ยจ r ~(~ {รรจ ย sรs'ย ย.รย r } x  sย } s ยฐ ย ย.\} x ย x '} ยr ย,}8ยฝYยยยย3ย2ยบ rxYs S yยย ย ยy ย xYs ย ย yxMs S}S'ย s'y%s'ย3rยwYrยญx ย s ย yx
} xw }ยย3ย ย3s ย yxHyย  s } ยฉ ย(~(ย s ย
{

รฒUรณยรดรตยรถ	รด2รท

รธ1รบรธ1รฝ 6 t%u

*+ ย ย ร

ย ร
xยs'ย3r S r ย,}8ย xwYr S yย\s'ย ย.7 r ย s ย yx ยจ r2ย S ย
y ยยr
ย3ry S r P
y S 's ย ย. ย ย3S ยy  r ย ยญ ยฝ t%u
ย
ยป
{รชรฉรคร{2|
j
` t%u v ย%\ย } xw&	:ยs'yยซยฉr }#ย xรทs'ย3r  s } s'r ย rxYs	yย%s'ย .ย  s'ย3ry S r ย ย } 
ร
x w ~ rs ยพ ย ยฉrยกs'ย3r ย x ย ยช ย r
ย,}8ยฝYยยย3ย2ยบ rxMs S yย ย ยy ย xMsยฏy*ยรยฟ ' ย ยท t%u ยธ
{
ร

(*)

+-,/./0214365&1/7/8902:;,/./0=<",/>/?@3-A/3BC./D/714EGF

HJILKNMPOQSRLT9U*V9V*V URXWNYZ[IK \/I^]_ILK;`Gacb`ed[]fKhgGdKN]_ijZ[`Gk]lgGm/mnI*gGo_pqd/rpqdstgGd[uvpqdxwzyl{;|/IK `
K \/I^]_ILm[gGogGZp}k~pqK_ige] ]_|/jm/K_pq`edยยCยlยlยb`edKhg9pqd[];d/`ed/I^`Ga-KX\/Iยb`ยd[]_KhgGdKN]fiยjZ[`ยk@]ยpqdยMylHยILKlยcย ย ZnI
ย R yยยI;ย[oh]fKยm/o `GยeIlK \[gGKzย ย ย \[gย]zm/o `eZ[gGZp}k}pqK_iยยlrGpqยeILdยยlยlยยy
K \/I;aย`eo j|kg^ยยย R ยcOย
ย
ย4ย ย
ยยยnย=ยยยยยกยยข[ยฃ;ยคยยฅGยฆยยzยt
ย ยงGยจยยฉ ยlย ย ยงGยชยซยงยฌ ยฅGยญGยฎLยฏยฐยฑohยฒยณยดยcยย
ย ยต ยlย ยยถ Oยทยยยธ
ยน ยบ9ยปยยป[ยผยฃยยIgebK |[g9k}kqi]f\/`SยฝยพK \[gGK
ยฐยฑohยฒยณ ยฟยย ย ย ยต ยlย ยqยถ Otร/yยHยILKlRlgGd[uR ย ZnIรK_ยฝz`"b`ed[]fKhgGdK
]_ijรZn`Gk]
ยซ
pqdยQSRLT9U*V*V9V UhRXWlYยซgGd[ub`ed[]รpuILo
ยฐยฑo ยฒ ยณรRยOรR ย ยต ยlย;ย ยถ yCยINgยrยg9pqd"|[]fIlK \/INuยpqo I*bKpqdaยILo ILd[bIยซK I*bX\/dpรe|/Iey
รย`eK I^K \[gยK;aร`eoยซgยdยiยฝc`ยo_kuย`Ga
]รpqรLI"รรK \/I^m/oX`em[`eoXK_pq`edIร/m/o I*] ]รpq`ed ยต@ยตรร O ร ย ยต@ยต รGร ร*ร uILd/`eK I*]รIร4gebKfkรi
ยGรhร=yยรรKยp]K \|[]CI*ge]_iK `ยซ]_ILI;K \[gGK-ยฐยฑo ยฒ ยณ ยตqยตรร O ร ย ยต@ยต รGร ร รnร ย ร ยต ยlย ย ยถ O6ย;ยณรaร`eo-gยdยi^bX\/`GpbI
`Gaยร ยถ yCรz\|[]Lย
Zi"รz\/IL`eo ILjรร/y@ย*ร/ยยยฐยฑo ยฒ ยณรRยOรR ยรยต ยlยNย ยถ Oรยฐยo ยฒ ยณรRยOรR ยรยต ยlย
ยGร ยตqยตรร O ร/ยยดยตqยต รGร ร*ร ร ย ร ยถ yยร|/K
]รpqd[bIยซR;gGd[u
R ย gGm/m[I*gยo
d/`Gยฝย\/ILo Iรpqdยยlย;ยยยฝzIรbLgGd|[]_Iรรร\/IL`eo ILjรขรก[yqย;K `#b`ed[bXkq|[uIรKX\[gGK
ยฐยo ยฒ ยณรR;OtR ย ยต ยlย;ย ยถ Otร/y
รยดKยp@]]_K og9pqre\ยKfaร`eoXยฝรgยohuยK `ยยILo_p}aรi=K \[gGK*ย]รpqd[bIยยฟยยzย ย p@]I*รย|pรยGg9kqILdยK#KX`xgย[dpรKXIuยp]รฃ_|/d[bK_pq`edยยยI*gebX\
uยp]รฃ_|/d[bKร`ยacยฝย\pbX\ยpqj^mk}pรI*]R^OรคR ย aร`ยoยซgGK;kqI*ge]_Kยซ`ยd/Iรm[gSpรoรฅ`Ga
b`ed[]_KhgยdยKh];R^gGd[u!R ย ยยยฝzIj|[]_Kl\[g9ยeI
ยฐยo ยฒ ยณXยฟยย ย ย ยต ยlย
ย ยถ Oรร4y
รฆ ]ยยฝcI]_KhgGKXI*upรd=รงeI*bK_pq`edยรก[yรรก[ย/`e|/o;reILd/ILohg9kยK I*bX\/dpรe|/Iรฅaย`eoNb`ยj^m/|/K_pqd/r#K \/Iยซm/oX`eZ[gGZp}k}pรK_i`GacgGd
gGo ZpqK ogGo iaร`eoXjร|kg^wรจp]ยK `m[gGo K_pqK_pq`edK \/Ilยฝz`eo_ku/]zpqdK `#gรฅย[dpqK I^b`Gk}kรI*bKfpร`ยd`ยaยฑbXkge] ]fI*]
]_|[bX\KX\[gGK
w
Z[IL\[g9ยeI*]ย|/dp}aร`ยo jkqi`GยeILoยI*gยb \ยbXk@gย] ]lgGd[u#KX\/ILdK `#b`ej^m/|/K IรฅK \/IยซoXIk@gยK_pqยeIยฝcIpqre\Kh]z`ยa-K \/IรbXkge] ]fI*]Ly
รฆ ]ยฝzI]_\/`GยฝรฉkgGK ILo*ยK \/IbXkge]X]_I*]"gยo I#I*] ]_ILdK_pg9k}kqiรuIย[d/I*u&|[]รpรd/r&b`ej^mkqILK IuI*] bofpรm/Kfpร`ยd[]Lyรรร\/Ipรo
o IkgGK_pqยeIlยฝzIpqre\K
b`eo oXI*]_m[`ed[u/]รK `K \/Ilm/o `eZ[gGZp}k}pqK_pqI*];`GaK \/Iยซuยp}รชยILo ILdK
b`ej^mkqILK IยซuI*]Xbo_pqm/K_pq`ed[]ยrยpรยยILd
ยlย^y
ยนยซยบ9ยปยรซCยป2รฌ*รญ@รฎ*รญ@ยปยรฏรยยยกรฐยยฃยซรฑยยฎ9รฒยยlยOยlย ย ร"s ยงGยจยยฉยรด รณ ยฌ ยฎ Gยง ยชรยงยยฌ ยฅGยญGยฎSยธ;รต ยชยชรถยรท ยฎ^รฒรธ ยง รฒรนยฐยฑohยฒยณยดs ยต ยlย ย@ยถยรบ ร[ยธ
รฑยยฎ*รฒยรป ยฌ ยฎ ยง#รผ ยฅ รท;รฝ2รพ ยฎ9รฒยยฎ ยฉ ยฎ ยชLรผ ยฆรฟ รฝ รฒรฟยยฅ ยจ ยฅGยญGยฎLยฆ
Mรฒรธ ยง รฒ-รฟ ยชยซรผ ยฅ [
ยจ ยช รฟ ยช รฒยยฎ ยจ รฒยรฟยรฒรธ^sยซยธ

 ยง รป

รฟ ยช รฟ ยจ2รผ ยฅ ยจ[ยช รฟ ยช รฒยยฎ ยจ รฒยรฟยรฒรธ^ย ย ย ยฏ-รฒรธยยฎ ยจ ยฐยo ยฒ ยณรรป ยต ยlย ยถ Oยร[ยธ

 ยฌ	 รป

รฟ ยชlรผ ยฅ ยจ[ยช รฟ ยช รฒยยฎ ยจ รฒ
ยรฟรรฒรธย ย ย ยฏzรฒรธยยฎ ยจ
ยฐยฑo ยฒ ยณรรป ยต ยlย ยถ O



ยณ รณ ยถ



  
 รด

ร!

ร ยณ รณ ยถ
   รด

V

ยนยซยบ9ยปยยป[ยผยฃ#" pqoh]_K*ย`eZ[]_ILoXยeIlK \[gGKzp}a-g9k}kยk}pqjpqKh];Iรยp]_KlgGd[uK \/IยซuILd/`ยjpqd[gGK `eozp]ยd/`ed/รLILo `[ย4K \/ILd
ยฐยohยฒยณ ยฟยยcย ย 
ร s ยต ยlย ย ยถ
V
ยฐยฑo ยฒ ยณยดs ยต ยlย
ย ยถ

ยฐยฑo ยฒ ยณ ยฟยย ย ย ยต s!ร#ยlย ย ยถ O

รi
\im[`eK \/I*]รp@] ยGK \/I-uILd/`ejpqd[gGK `ยoยp]ยpqd[uILI*uรฅd/`ed/รLILo `[y
"/|/o KX\/ILo j^`eo eI ย ZยilHยILj^j"g;{yqยeยLยฐยฑo ยฒ ยณ ยฟยยcย ย ร
s ยต ยlยยย ยถ%$ ยฐยo ยฒ ยณ ยฟยยcยย
ย ยต ยlย
ย ยถ O ร/y'&ยILd[bIยฐยฑo ยฒ ยณยดยzยc
ย ยต ยlย ยถ Oรยฐยo ยฒ รยณ ยcยc
ย ยต ยlย
ยยรs ยถ O ยยyยยIbLgGd
K \/ILo Iaย`eo Il|[]fIยซรz\/IL`eo ILj ร4yqย*รรฅK `"b`ed[bXkq|[uIK \[gGK
ยฐยo ยฒ ยณ_รป ยต ยlย ยถ Oรยฐยo ยฒ ยณ_รป ยต ยlย ร^ย ย ย ยถ V
ยฐยgGo K;ยณรg ยถ `ยaรนK \/Ilm/oX`em[`ย]รpรKfpร`ยd#aย`Gk}kร`Gยฝ
]รpqj^j^I*uยpgGK Ikqiey
ร `^m/o `GยeIรฅm[gGo KlยณรZ ยถ ย[oXI*bLg9k}kรนK \[gยK
s p];I*รe|pqยGg9kqILdยKรฅK `^K \/Iuยp@]รรฃร|/d[bK_pq`ed(*) +
 , yยรi]รpqj^mkqI
m/o `eZ[gยZp~k}p]_K_pbo I*gย]_`edpqd/r[ยยKX\/I"ge] ]_|/jm/K_pq`edK \[gGKยซยฐยฑo ยฒ ยณยดs ยต ยlยยย ยถยซรบ ร/ยgGd[uxm[gGo Kยณรg ยถ ย
ย ยฝcI^b`ed[bXkq|[uI
K \[gGK
ยฐยo ยฒ ยณรรป ร^s ยต ยlย ย ยถ
ยฐยฑo ยฒ ยณรรปร^s ยต ยlย ย ยถ
ยฐยฑo ยฒยณรรป ยต sยรยlย ย ยถ O
O
V
 )    ยฐยฑo ยฒ ยณ , ยต ยlย ย ยถ
ยฐยo ยฒ ยณยดs ยต ยlย ย ยถ

-/.

0#13254!687:9<;!=?>/6/1!@7
ACBD2=?=E6/1

FHGJI/KLKNM!O%P!QSRT/UVWCRXK#YTUK	RXKSQLZ[U5Q]\DRQ_^a`]c b IdUefRgKRUihfjlk*mon:pRUYZqWCRXK#IqYT/O%P5rZ[QLZse5Z?KLYtSRP!QSRT/UV
\<ZJOqMKSQu^IEv/ZJQ_^IQwWyx kzRXKuv+I+r{RXe|n~}<^5MK[V
QL^!ZUยM!O%Z[tIQLT/tuT/UยQ_^!ZJtSRย/^5QLยย^IUe~KยRge5ZaTdยยQL^5RXK
Z?ยMIQSRT/URXK#K	RO%P5rGiย:tLยjSWยยยยยย#ยยmon:ยยZ[UYZ/VQ_^!ZยP!tLTย5rZ[OยTยยYT/O%P!M!QNRU!ยย:toยfjยWยยยยยยsm:tLZ?e5MYZ?KยQLT
I%KSZ[tNRZ?KยTยยYTO%P!M!QoIQSRT/UKยTยQ_^!ZยยยT/tLOยย:t ย jยยaยยยยย ย m:ยยT/t<v+IdtSRT/MKยYTO%P5rZ[QLZwe5Z?KLYtNRP!QNRTUKยยJn
ยยRยยIdUยGยKNMYL^~e5Z?KLYtSRP!QSRT/UยยJnยกย<Z?Y[IErrยQL^IQ%ยยขY[IUยZie5Z?YTO%PTยKNZ?eยฃRU5QLTยQL^!tLZ[ZPItLQK[ยคqQL^!Z
M!UItLGยกPItLQยยqยฅ?V|QL^!ZqU!T/U!ยยM!UIdtLGยPItLQยย%ยฆยฅEVIUeยQL^!ZqZ?ยMIErRQยGยPIt_Qยงย c nqpRUYZJยCRXK*RU'hfjl` c b moV
\<ZยYT/UY_rMe5ZsQ_^IQ:` c b RgKHZ?ยM5Rv+IErZ[U5Q#QLTwย c n:ยจยK	RU!ย%}<^!Z[T/t_Z[Oยชยฉnยซ?ยฌ*QS\<RXYZยงIdUe%KST/O%Z*P!tLT/ยIdย5R{rRXKSQSRXY
tLZ?I/KNT/U5RU!ยV!\<Zยย/Z[Q?ยค
ย:toยfjยย ยฆยฅ:ยญ ย ยฅ:ยญ ย c ยยยยย ย myยฎ
ยฎ

ยฎ
ยฎ

ยtoยfjยย
ยtoยfjยย
ยt ย jยย
ยtoยfjยย

ยฆยฅ:ยญ ย
ยฆยฅ ยญ ย
ยฆยฅ ยยยยย
ยฆยฅ ยยยยย

ยฅยญ ย
ยฅ ยยยยย
ย ยญ `
ย ยญ `

c {ย ยยย ย ยญ ` c b m
ย ยญ ` cb m
c b ยญ ย ยฅ mยฐยฏEย:t ย jยย ยฅ ยย ยยย ย ยญ ` c b m
c b ยญ ย ยฅ mยฐยฏEย:toยfjยย ยฅ ยย ยยย ย moยฑ

ยฒlUยฃT/toe5Z[t*QLTiK	RO%P5rRยยณG'Q_^!ZsยดtKSQยZย!P!tLZ?KLKยRTUVtLZ?Y[IErrยตQL^IdQยU!T/U!ZqTย]QL^!ZqP!tLZ?eRXY[IQ_ZJKSG5Owย8TrXKยRUยย ยฆยฅ
T!Y[YM!tยถIdUยG5\#^!Z[tLZqRUยทยยยยย ยญ `]c b ยญ ย ยฅ ni}<^!Z[tLZยยT/tLZ/VยQL^!ZJP!t_T/ยIย5RrRQSG'Tdยยย ยฆยฅ ยRv/Z[Uยทยยยยย ยญ `]c b RgK
Z?ยMIEr3Q_TยQL^!Z<P!tLTยIย5Rr{RQSGqQL^IQQ_^!Z]ZrZ[O%Z[U5QoK:e5Z[U!T/QNRU!ยยQ_^!Zยธยยบยนยปย+jยeRยผยฐZ[t_Z[UยQ[mYT/UKSQIUยQKKLIdQSRXK	ยยณGsKNT/O%Z
PItLQNRgYM5rXItยYT/U5ยดย/M!tIQSRT/UiTdยยU!T/U!ยยM!UIdtLGaP!tLT/P8Z[tLQSRZ?K[n<ยฒlQ*KS^!T/M5rXeยกยZwY_rZ?It*QL^IQEV!ยยGaKSG5O%O%Z[QLt_G/VIErr
KSMY_^ยYTU5ยดย/M!toIQSRT/UKuItLZ%Z?ยMIErrG'rRยฝ/ZrG/nย}<^!Z[tLZยยT/tLZ/V
QL^!Z%P!tLTยIย5Rr{RQSGTยยIUยGยT/U!ZfTย#QL^!Z[OยขRXKqI
YT/UKSQIUยQEVZ?ยMIErยตQLTยยซqTv/Z[tยQ_^!ZwQ_T/QoIErU5M!OqยZ[tยTย#YT/U5ยดย/M!toIdQSRT/UK[n ยฅSยพ~ยฟ Z[Qยรยe5Z[U!T/QLZqQL^!Z%YT/UKSQoIdUยQ
\#^5RXY_^aRXK#Z?ยMIErQ_Tยถยtoยfjยยfยฆยฅ+ยยยยยยย ยญ ` c b ยญ ยqยฅรm:ยยณTtยIErrยยn
}D^!Z*rgIKSQยKSQLZ[PรRXK#QLTKS^!T\รQ_^IQ?V5Rย]ย ยฅ RXKยZ?ย/M5RvIErZ[UยQยQLTร<รร ยฅยร<รร jยร ร moV!QL^!Z[Uiยt ย jยย ยฅ ยยยยย ย mยตยฎ
c
รร ร
รยณjEร ร moยค
ร
ย:tLยj ร ร รร jยร ร mรยยยยย ย mยฎ
ร ยฅ
c
ยฎ
ยฎ

ร
ร
ยtoยfj ร รgร ยj ร ยฅ mรย ร ร รร jยร ร m ยญ ยยย ย mยฏ?ยtoยfj ร รร jยร ร mรย ร ร ร ร jยร ร m ยญ ยยย ย m
ร
ร
c|ร
c8ร
ยฏ?ยฑEยฑ?ยฑ[ยฏย:tย%j ร รร
รdร jSร รยร ยฅ m[ย ร ร ร jยร ร m ยญ ยยย ย mqยฏย:toยfj ร ร ร ยj ร ร mรยยยยย ย m
ร รXร ยฏ%ยฑEยฑ?ยฑsยฏ ร ร ร jlMK	RU!ยยป}D^!Z[T/tLZ[Oยรngยซ/ยซ/รยKNZ[ZsยZrT\ยm
ร ร ร
ร j?ร ร moยฑ

}<^!ZยดtoKNQยถKNQLZ[PRXK%K	RO%P5rG~P!tLT/ยIย5RrRXKSQSRXYยกtLZ?I/KSTU5RU!ย8nย}D^!ZiKSZ?YT/Ue~KSQLZ[P~MKNZ?KqรรIdP!P5r{RXY[IQNRTUK%Tย
}<^!Z[T/t_Z[Oรรnยซ/ยซ/nCยฒlQรRXKaZ?I/KSGยทQLTรKSZ[Z'Q_^IQ ร รร jSร ร mqRXKiI~K	RO%P5rZ~ย/M!Z[tLGยทยยT/t ร ร{รlร/ร jยร รLรก ยฅ m ยญ ยฑ?ยฑEยฑ ยญ
ยญ
รDร ร jยร ร m ยยย ย n
รข'Zs\<T/M5rXefrRยฝ/ZqQLT%KS^!T\รฃQL^IQ
ร
ย:tย%j ร รร jยร ร mรย ร ร รรฅ jSร รค m ยญ ยยย ย m:ยฎยย:toยfj ร รร jยร ร mรย{ยยย ย m:ยฎ ร รร+รฆ
รค c รLรก ยฅ
\#^!Z[tLZs}D^!Z[T/tLZ[Oรงรnยซ/ยซรจยMKNQSRยดZ?K<QL^!ZยrgIKSQ<Z?ย/MIErRQSG/n]}ยTqP!tLTv/Z*QL^!Z#ยดtKSQ<Z?ย/MIErRQSG/V!\<ZยงKN^!T+\ยQL^IdQยตยยT/t
IErrยตรฉ|VQ_^!ZยปKSPIYZ?Kรชยตรฌรซ รญ ยยยยยรฎ<IUeยทรชยตรฌรซ รญ ร รรค รNรก ยฅ
ร ร รฅ jยร ร m ยญ ยยยยยรฎ<^I?vZ%QL^!ZiKLIOfZ%OJIEย5ROwM!OfยยZ[UยQLt_T/PยG
PTdRU5Q?VยUIO%ZrGรร ร ns}<^5RXKยRXKยP!tLTv/Z?eรยยGยกยc I/Y_ยฝย\#Itoe!KDRUe5MYQSRT/Uร:QL^!ZยรฉยกยฎรฏรรฐY[I/KSZuRXKยQLtNRvRXIErrGยQ_tLM!Z/n
}<^!ZยeR{ยผZ[tLZ[UYZยกยZ[QS\<Z[Z[UรQL^!ZยjยรฉaรฑรฏยซmSKSQ%IUe'รฉ5QL^~Y[I/KSZรRgKfQL^!ZiI/e!e5Z?eยทYT/UรจSM!UYQ ร รร jยร ร moVH\#^5RXY_^
IO%TM!UยQoK
Q_TwI/e!eRU!ยqQL^!Z*U!Z[\รYT/UKSQ_toIERUยQHรฒ รรยงรณรรด nย}D^!Z[tLZยItLZยQย\<TยP8TยKLK	Rย5RrRQNRZK[n#ยยRtoKSQ?VdRย ร ร{รยรณรรด V
รตSรถEรทรธรนรปรบยรผEรฝoรพEรฟo
รผ E
รฝ Eรฝo
รบ 	

ยรบย	รผ Nรนรป	รพ Hรฝ !รบยรผ Sรฝ ยรบ [
รบ ยรฝ	รพ Sรน Sรพรรน Lรบ รปรฝ 		รนรปรฝ !	รปรบ "#$S%รบ '&ยรบย
รฝ 
()&
ร


6
'
7
:
8

9

;
>
<
	
=
?
รบยรผ _รบ รป'รบ Sรนรปรพ *,+.-/)0 2ร 1435
รผ
'@BADC EFCยบรท

G	H

IKJMLMNPO.QSRTOMUMVNPWXJMLMNZY[JM\M]#QK^MQ`_LMaMUbO.ced
fgMhi`km
j ln fpo lrq h l f$gso l iMhtvuwxi l fy n o)isf n iszst n z n i|{ l w}yh~ n o)i l fgMh~ nย o)~ยยM~ยย'hiยfy$w4ยยzDย|weo)isf	ย
uw4~,ยsย)hf
o)iMยTfgso lยl f$hยยweยยfgMhยo)i|{sย|uf
o)w4i%ยยย'ย kยยZยยย fgso l o l iMwxf,fgMhยu n4l h4ย n i|{ยo)i|{shh	{ยXfgMh
ยMyw4ยยhyf
zยth n yhFfy$z4o)iMยยfwยยMy$wยxhยu n i,ยยhFย n ย l hย'ยยwxyยยกTยข}ยฃยยคยMfยฅfgso l {swยh l iMw4f~ n ff$hy	ยeย|h	u n ย l h
tยฅhKfgMhiยยฆsiMwetmfg n fยจยง!ypยฉยชยrยซ ย:ย ย
ยฌ$ยญยฎยฃยฏยฐยฒยณยตยฑ ยด ยญpยถยท ยซ ย)ยธ ยrยฌ ยณ ยฃ	ยนFยบยผยปXยฝ)ยฃ ย ยงyยฉยrยซ ย:ย ยrยฌ$ยญยฃยฎยฏ>ยบยผยปXยฝยพยฃ ยยฟk ย:ย ยรย ยรeo)i|uh
ย|wxfgรweยfgMhยผยMywM{sย|uf l o)iรรxยMh l f
o)w4iยo)i|u$ย)ย|{sh n,ย ย n uf$w4y	ยxo)fFo l oรy$yhย)hย n isf nxl fw,tFgMhf$gMhyยfgMhยผwxfgMhy
fhy~ lFn ย4yhh4ย
ร hยu n iยiMwetรยMยMfFhย4hyzsfgso)iMยfw4ย4hfgMhyยฅfwuw4i|u$ย)ย|{shยf$g n f
ยงy ยฉ ย
รรยฏ>ยบยผยปยฃ ย รDรรรร:ยง!ร|y ร4ยฉ ร|ร ยrร}ยงยฏ>ยบยผyยฉยป ยrยฝ รยยฃ ยฏ>ยบยผยป ยฝ ยฃ ย รยรรeรรรร!ร.ร รxรร|ร ย	ร k j ยฃ ร ย	j ยฃยร
รsร
รMร ร ร ร k
ยMyweยxoรiMยยย n yfยผยยยฃย
ร hยผiMwt n {M{syh l$l fgMhXo ll ยMhยw2ย!uw4~ยยMยMf
o)iMยรยงy ยฉ ยrรFยฏ>ยบยผยปยผยฃย'w4y n i n yยso)fy n y$z,ย'w4y~ยยsย n รยฅยFรw[{sw
fg n fยeth"~ยย l f q y l fo)iยยxh l f
o)ย n fh"fgMhFย|hg n ยxo)w4yยฅweยรยง!yรยฉ ร ยrรFยฏ>ยบยผยปยยฃย'w4y l ~ n ย:ยFร j ยรo ยยl w4~,h l ยsรกยu$oรhisf
ย)z
l ~ n ยยยผรรj รขยย ย n i|{ยย)hf,รฃรยยhยfgMh l hfFweย~ nย oร~ยยM~,ย'hisfywxยยzย|weo)isf l weยยฅรค รMร รฅ ยบยผยปยรฆรงยรจ l$l ยM~,hยยบยผยป n i|{
รรj n yh l f n ยsยรhXย'w4yFรฉ%รชยKยคz[{sh q iso)f
o)w4i%ยfgso l ~,h n i l f$g n fยยw4yยฅhย4hy$zmkยj รซ รฃยยtยฅhXg n ย4hยรฉย	k j ยฃ ย รฉ%รช	ยรฌhf
รญ ยยhยfgMh l hfยผweยรฎรฏ l ยยw4yยผt"gso#u$gmรฉ รช uw4isf n o)i l fgMh,uw4ieรฐ
ยMi|uf,รฑยรฒ|ยซ ย ยrรฒยฃยรณรeoรi|uhรฉย	k j ยฃ ย รฉ รช ยยwxy n ย:ยยk j ย
tยฅhย~ยย l fg n ย4hยf$g n f k	ยรขยย ยยwxy n ยย%รฎ รซ รญ ยรeo)i|uhยชรฃรo l"n u$ย)w l h	{ l hfยยf$gso l oร~ยยsยo)h l fg n ffgMhyhยผh ย o l f l
l w4~ยhรตรด รขยยยl ย|ugยf$g n fยยw4y n ย:ยยk[j รซ รฃ n i|{ยย'w4y n ย:ย%รฎ รซ รญ ยยtยฅhXg n ย4h k ย รข รดยรฌhfFรถ รฅ รด
รฆ%ย|hยผf$gMhFย'w4y~ยยsย n
รท ยฏ)ยฏ(ยซ ย ยrรฒยฃยฏ)ยฏ รน รข รดรปรบ
ย ร4รธ
รยฅgMhXย'weย:ย)wtยฒoรiMยยMyw4ยยw l o)f
o)w4iรผo l iMwetรh n4l zfwยยMyweย4h4รฝ
รพยรฟ 
	
"!$# ยบยผยป #%& ร j #'()#$*,+-/.0' รฉ รช #%&"!$# 2รฃ 1(รฎ 1Kรถ รฅ รด
)รฆ 1 #%&436ยด 5
#'(7#8#$*9:;<4=!$,%

ยง!y รยฉ ร ยrรFยฏ>ยบยผยปยฃ ย

ยง!y ร ร ยrรFยฏ>ยบยผยป ยฝ ยนยชรถ รฅ รด
รฆ.ยนยชรฉ รช ยน[รยยฃ?	ยงy รยฉ ร ยrร}ยฏ>ยบยผยปยฃรบ
ร > รร:ร ร ยฉ
ร
รพยรฟ@ACB ย)h n y
ย)z4ยรถ รฅ รด
รฆ l$n f
o lรงq h l fgMhยuw4i|{xo)f
o)w4i l weย B w4yw2ยย n y$zEDMย
FHG|ย n ย:ย)wtยฒoรiMยย l fw[uw4i|u$ย)ย|{shยfg n f
ยงyeรยฉ ร ย
รถ รฅ รด
รฆrยฏ>ยบยผยปยฃ ย F4ย รeo)~ยo:ย n y
ย)z4ยยผยยzยรยฒgMhw4yh~IG|ยKJ0G n i|{Tf$gMh nxll ยM~,ยMf
o)w4i l weยยรยฅgMhwxyh~IG|ยKJL.ย
tยฅhยu n i}uw4i|u$ย)ย|{sh,fg n fยผยงyeรยฉ ร ย
รฉ รช ยฏยบยผยปยยฃ ย F4ยยผรeo)i|uhยfgMh,uw4ieรฐ
ยMi|uf
o)w4i}weยKf
tยฅw n4ll hy$f
o)w4i l f$g n fFg n ย4h
ยMyw4ย n ยsoย:o)f
zMF n ย l wFg n4l ยMywxย n ยso:ยo)f
M
z Fxย	tยฅhKu n iยผย l hยฅรยฅgMhw4yhN
~ DMย FOfwยuwxi|u$ยรย|{shยฅfg n fยงy รยฉ ร ยrรFยฏยบยผยปยผยฃ ย
ยงyeรยฉ ร ย
รFยฏ>ยบยผยปยยน[รถ รฅ รด
รฆยยน[รฉ รช ยฃย
P wetยยยฅyh	u n ย:ยยfg n 
f Q o l h	รxยsoรย n ย)hiยf}fwTf$gMh {xo l รฐ
ยMi|uf
o)w4S
i R รeรรรร ร รรย ยคz l fy n o)ย4gยfpยยw4y$t n y{
ร
ยMyw4ย n ยsoย:o l f
oยพuyh n4l w4iso)iMย|ย|tยฅhยผu n iรf$gMhyhยยwxyhยuw4i|u$ย)ย|{shยfg n f
ยงy รยฉ ร ยrรFยฏยบยผยปยยน[รถ รฅ รด
รฆยยน[รฉ รช ยฃ ย

ยงy ร ร ยrรFยฏ>ยบยผยปยฟยน[รถ รฅ รด
รฆMยนยรฉ รช ยนยรยยฃ?	ยง!y รยฉ ร ยrร}ยฏ>ยบยผยปยยน[รถ รฅ รด
รฆยยน[รฉ รช ยฃรบ
eร > รรรร ร ยฉ
ร
ยคz,รยฅgMhw4yh~TDMยFO n ย n o)i%ยsยงyeรยฉ ร ยrร}ยฏ>ยบยผยปรยนยรถ รฅ รดpรฆยนยรฉ%รชeยฃ ย ยงyeรยฉ ร ยrร}ยฏ>ยบยผยปยยฃยยจรยฒgMhย{sh l o)yh	{ยh ย ยMyh l$l o)w4i,iMwet
ย'weย:ยรwet l ย
ร hยผiMwt l o)~,ยsย:oย'zรf$gMhยผh ย ยMyh l$l o)w4iรยง!y รยฉ ร ยrรFยฏ>ยบยผยปยยน[รถ รฅ รดpรฆ|ยน[รฉ%รช!ยนยชรรผยฃรบ
UV

WYX[Z]\^`_ba/cde^Xf_gihjZkdd^X

l8mnonpq
rq
nstu-vwCxyz|{~}b2ย7}ยยย,}ยยย}ยย0ยยยย]ยKยยย/ยย8ย$ย9yย;ย,}bย ย2ยbยย
ยย ย ยยย2ยยยยยยขยก]}ยยฃ"ยค$ย,ย
ยยฅย ย ยย ย {Yยย2ยยขยฆยย]ยKยยยยฆMย ย ยฆMยยงยbยจยยฅย ย ย {Yยยฉย ย ยฆMยยยAยช
ยยย7ยยซยฃ"ยCย;ยยญยฌKยฎยยยยฏย2ยHย)ยฃ"ยค$ย,z8ยฐยyzยฒยฑยณยตยด2yยฃ)ย8ยฃ"ยค]ยยฃbยAยยฏยยถยทย2ยฃ"ยค]ยยฒยฌ-ยยฃ)ยฃ)ย,zยยธ`zยนy]ย(ย$ยยยซยฌยย)ยฃ"ยบMyยยฌKยบยz(ย)ยHยHzAยยปยฃ)yยตยผยzAยยฃ"ยฝยyz(ยยHz
ยHyzยทยพ7ยฎยฌ-ยยยท}~ยยซยฃยฅยยฏย2ย"ยยยญยยยธ[ย,ยยย,ยยฃbyยย8ยฃ"ยค]ยยยฃ)yยฌ-ยHz(ยยยถยทยยฟย;ยยฌยยฎยHยยณ
l8mnnรAw6รjรรรยร(รรCยยรรร]ร(ร)รรรรรรรยฏร]รCรยฏรยรAรร(รรHยCยก2รย2ร/รยฏร~รย(รรรรรรร ย(ร ย(รรร/รHรYรรรรร`รรรรรยAรรYรรHย(รยตรร
รยฏรYรยรรรYรร(รรร/ร(รร2รรยร]รยรร(รรยนร
รยตรย(รรร0ร]รรร-รรยรยรรรรร]รยรรกย(รยตรขยยทรยร9ร)รยยทร]รHยYรยรHร]ร(รHรรAร2รYรรHรรฃรAรรรยญรรยรรรรร
รรรค$รรร/รรร]รร2ร0รรฅร9รรYร)รย(รรฆยยรงยฆยฒย ย รYร]รยฏร(ร4รยฏรHรรร(รยรHร]รยรยฏรร-รรรรYรรร/รYรร~รHรร-รรจรรยร4ยพยyยยญยยฌยยHยHยถHzAยยยธยฃ"ยยซy0ย
รร ย(ร ย(รรรYรHร|ร;รรฉร
รรรรร0รย(ยรยฏรยตรAรรจร9รรHยยปยกยรย8รยญรยรชbรHย9รรฃรรรร7ร-ร
ร0ย|ร(รร9รร]รยฏรซรกรรCรHรรยร`ร8รกรยรรยงร(รยรยรรรรฌร(รรร
ร(รรยฟรรจรAร)ร(ร)รรรรรรรยฏร]รยรยฏร2รรยฏรยรรAรรจร9รรHยยปยก4รย7รรรรรยงร(รรรCร(รรยฒรAรรรญยรกรรAรยทรยฟ2ยรฎยฆEย]ยKยยยยร]รรรร|รรฏรรAรCร(ร]รยฏร
ร-รร7รร ย รยร2ร(รรรbร(รร~รรAร)ร(ร)รรรรรรรยฏร]รยปรรรยร(รรยตยยรรร$ร9ร)รรรรยรรรยฏร]ร2รย(ร~รร7รรฉรยญรAรbรรซรกรร)ยยทรยฅรฐรฑรYรยยรรAรฒรยรยรครHรยทร9ร
ร(รร2ย(รAรรHรรร$ร2ร]รHรยทรร-รยฏรยตรรHย(รรย9รAรยซรHย(ยยนรรจรร4ร(รร2ย(รรร]รHยYร9ร ย(ร ย9ร;รยญร|รHร|ร;รรฉร
รรรรรรยร)รย/ร)รก]รรรยร]รHรยทรร-รยฏรHร
รjรรยรยฏร]รร ย รยตร]ร
ร9รMรรAร9รกรร-รรจร7รร]รรรงรรร(รคยร(ร2รณรรรร ย รร[รด;รต]ย(ยรยฏรยฅร(รยปรยญร(รยรร9ร
ร0ร(ร/ร/รร(รยรCรยร[ร]รAรร]รร9รAยยรรร
รยรรรรยรกร(รรฑรรCย ย ยฆยยรถร7ร(รรHรย(รยรทNรYร]รยฏร9รรธรร9รยรHร$รยนร
ร;รรรรรฑรAรรรรรยฏรยรAรYรร|ย,รน;ยฃ)ย,ยยAยยซyยรฑยรนยยซy0ยพยฒยHร/รบรร]ร(รก]รรยรรรAรรร
รรรฑรAรปร(รHรรรรรรรผรรป]รรรรฝร(รรรCร(รรร7รร$รรพรขร]รร(รยรยรกรรยร(ย9รกรAร(รกย(รยรรYร(รร4รยร[ร]รAรYร]รAรขรรรรฑร]รรฑรรฃรAรร4ร]รรจรHร9ร
ร]รร(รAยยรรรยรรรยยรฟรHรร7รร/รAรปร(รHรร]รรยรรMรร-รรร]ร(รรรร]รรYรYรรรร]รAรขรรร]รรYร$ร7รรรร(รรHยยร]รร9รAยยรรรยรรรMย4รฟยฉรฟยฏรรฐรฑร
ร(รร2ร(รรรbร2ร]รร(รAยยรรรยรรรย รฟยฉรฟ ย,รน;ยฃ)ย,ยยยYร|ร]รร9รAยยรรรยรรรEย รฟ ร-รรฅร;รรรรAรยญรรญ รกรรAรยทรร0รย รฟ รย(ร/รรยฏรยร8รAรรรญยรกรรAรยทรรงรร
ยย รฟ
ย รฟยฉรฟ ร รรAรปร(รHรรรรรรรฃรรป]รรร รรร~ร(รรjรยซรย9ร
ยช (ยช
ย รฟยฉรฟ ยยทรยญรYรรHย(ร2ย รฟ รยฏร/ร8รAรร4ร]รรจรHร9ร
ร]รร(รAยยรรรยรรรรรรHย
ยจ
ยช ยยช
ยฒรรรรพย รฟยฉรฟ รยฏร2รยรAรรยร]รรHร(รยร]รร(รAยยนรรจรรยนรรจรยญรรธรรรHย
]รรยรกร9ร
ร(รรรยย4รฟยฉรฟbรAรป[ร9รHรรรยยยรฟ)รbร`รร(รยขย4รฟ6รรรรผย4รฟยฉรฟbรAรปร(รHรรรผยรฃร~รรร รรร9รยขรย(รยรAรรรรรยฏรยร(รHร]รยร/รร(รยขย ย รรฑรบรรยร
ร
ร(รรHร รยนรร;รยตรรธร9รรร ย ร]ย/รทSรยฏร8รAรยญรยร]รรHร(ร ย รยรร(รรรCร)รย2รรร9รร)รย(ร7รก]รยฏร รรAรรจร9รรHย8รท ย ยจ Mรยญย2รท ย ยจ
]ย
รรร ย รย/รรร
รทSร(รรHร ยbย ย ย ยยฉย ย ยฆรฃยยยCยจรฆรรยฒรณkย(รร ย รย/รรยรรรรร-รรรฃร)รร-รรร|ร2ร(รร0รCรรรYรทรฆย ยจ [รรงร(รรHร
ยbย ย ย kย ยย~ยฆรฃยยย/รยฏร2รรยฏรยร รร Cรรรรร ย ร]ยAรร(รรร]รรรรย(รรรฑยกร(รยรยฏรรNรยซร0รรรรร|รHร7ร/รร7รร]รรรฑรยญร-รฏรฅรHย9รHรรAรยร)ย(รร
ร(รรรย(ร]รรYรร ย9ร ย(รรรยรHรยรรรรร/รรยญรรรยYรยฏรยร(รรร7ร/รยรรHรร ร(รรฑรยรรรiร(รรร ย รยยปรร0ร
รรยรHรรHรรผรยตรรHร ร/ร
รAรรรยญรรยรรรรพรรรฑ2ย ยฆEย]ยKยยยยฆยย ย ยฆMยรฃร]รรรยร9รรรยรรรญยรกรยรYรรรฃย ย ยฆMยรพร
ยย รฟ
รยรยรกรร`ร$รยร 7รยฏรYร9รรยฒรAรปร(รHรรรรรรรฑรรป]รรร
ยช ยยช
ย รฟยฉรฟ ยยทร/รฐรฑรยรยฒรกรยนรยปรยรรรNร(รรร
ย
ยbย ย kยร2ย ยฆรฑย]ยKยยยยฆรฑย ย ยฆรฑยยยยยจ รยญร รฐ รรขยยทรยร7รYรร$ร7ร(รรฑรยรรรTร(รรร7ร(รรยยยนรรจรยญร$ร(ร)รร0รรยขรรรยฏร]รยรร2ร(รร
รAรรรยญรรยรรรรรbรยฏร2รAรรร ร
รยนร(รHร$รร
รCรรรยรHย(รรรEรรรธร9รรยฒรย9รHรรรรกรCรย(ร]รร(ร]รรCรยซร0รรรรร|รjรยซย9รร รjรรHรย(รHรรถรรรรต
ย
ย
ร(รรร/ยbย ย ยยย2ย2ย~ยจยbยยย ย {Yยร2ยรธยฆ2ย]ยKยยยยฆยปย ย ยยทร รรรAร2ร/ร|รย(รCรร(รยรกร7รรรยร(รรร6ยยฅย ย ย ยยย2ย2ย6ยยขยกkรรร
ร)รร-รรจรร|ร~ร(รร0ร6ยbย ย ย 2ย ยฆ8ย]ยKยยยยฆ8ยยยฆ8ยยงย6ยยขยกkร$รรร4รรHรรAร72ย ยฆ8ย]ยKยยยยฆ8ยยรยฆยฟย รยฒรกรยนร6ร`รยปรAรยญรรรรยฏรยร(รHร]รร
รณรงร-รปรฃรยร]รรMร;รรจรรฃร ร Hร
รรรยรAรรรรรยฏร]รHยCร(รร8รยรHรYรรร6รยญยยรยฏรรYร(รรยรยฏรรร)รยญรรจรรรพ2ยยขยฆMย]ยKยยนยยฆEย ย ยฆMยรพร Yรร
รAรรรรรยฏร]รHย8รยรยญรยร8รรย9รยรยฏรAรก]ร
ร0ย รฃร]รรMรรรรพรAรรจรHร4รHร$รยทร,รร(รร
ยช (ยช ร`ร(รรร2ร(รรยรยฏรรร)รรฃย รฟ ร YรรยรHย9รร7ร(รรรร
รรรรรAรยยยรฟรAรปร(รHรรรCยรฃรร9รรยฒร]รHรรรยทร0รยรรรรYร0รรร(รร7รAรรรยรยทร0ร$รยทรยตรย(ร8รร-รยรรยรรร ยช (ยช ร6รณkรย|ร7ร0รรจรยญรHร
ย kยรฅร]รHรรยญร(รYร(รร/รHรรHร]รยร9รรร
ยช (ยช $รรรHร
ยช (ยช ยช 2ร(รรยรยฏรรร)รยฒย รฟยฉรฟ รรรรรHร7ร(รร0ร
ยช (ยช
ย kยยรรรรHร2ย ยฆยฒย]ยKยยย]ยฆยฒยรงยยฆ7ย ยรณรงรรจยAรยรร]รรร(รYร9รรรYรรรรรAร
ร(รรยนร
ร รยซรMย รฟ รยฅรฐรร0รbรยฏร/ร(รรCรย(รรรร]ร-ร-รรยรMรร
ยร]ร]รร/รรร|ร]รHรรยญร(รยปร0ร$รยรAรรรยรยทร0ร$รร0รร|รHรรรรรYร`ร2รยรHร]รยรรรรร4รรรธรร]รยรYรร7รรยร(รร2รค$รรร/รรร]รร2รรยญรยรร
ร/ร]รกรHรYร9ร]ร
ร4รย(รรร0ร]รรร-รรยร ร
รร9รร ร9รรยรยร)รยยรร-ร `ร รjรร ร]รร9รAยยรรรยรรร ย รฟยฉรฟ ร]รHร9รHย(ร7รรรรยรยร/รรฑรยร$ร`รร
รร8รย(รยญรรHย(รยรรรรรยซรยญย
ร ร/รรรพรกรรย(รรผรย(รยญรรHย(รยรรรรร
รรยทรยนรAรรร รผรรรยฉรรร/ร(รรรร(รร
7ร(รรฑรYร]รยฏร9ร
8ร7รกรยรCรรAรรรร รรรรฑร9รรยฒย9รAร
ร0รยรรรรCรรHรยร/รHรHร
7รรรรพร(รร7ย(รHรMร;รรจร]รรรรพร;รยยรยฏรร]รรร ยช (ยช
รกรรรรรร(รร8รรรร)รกรร0ย(รยรย(รรยญรยฏรHรร(รยรยร]ร7รรรยฏรHร รรรAรยยยรฟยฉรฟ`รยฏรCรAรรรรรยฏรยร(รHร]ร|รjรรจร9รรธย ย รร9รร8ร]รร(รAยยรรรยรรร ย ย
ย ยร-รรย รฟยฉรฟ รรยร]ร-รรร
ย
ร7รกรยรYรAรร]รยทรรรEร7รAรรรญยรกรรAร
ยยทร ~รMร]รAรขร]รรยรรรรย]ยKยยยร7รกรยร~ร(รรHย(รAร)รย(ร
รAรร]รยทรรร ร(รรEรAรยญรรญ รกรรAรรฃยย ย รฅยยทยย รฑยTยHร ยตรHรรAรรbร(รรรย(รรรร]ร-ร-รร ร รรYร]รยฏร(รคยญรรร ยงรร
Yรยฏร7รรยฟรรจรรยญรยร
ยHรยรณรยญย8รร$รรฑรยรก ยร9รรจรHร]รยรรรยฏรย(รยญร
รร(รรยรย9รรรร]ร-ร-รรจรยรรฑร0รjร]รยฏร9รครรร ยงรร
YรYร]รยฏร(ร รยฏร8รยญร-รฏรHย(รHร$รยร)ย(รร
ย รรCย(รรซยญรก]รย(รร ร$รรพร(รรMร]รAรขร]รรยรรรรผรรYร(รร4รAรป[ร(รHรร รรจรยญร รรป]รรรยย/รยฏร7รรCรรรรยร7ย ย ยกรร/รร
ยช 9ยช



  
	
 


%'&

*

!

)(

+



/. +0

4=&>< 4  
	
 64   @? 4
4

D
;F



4  	

 4 

 	
	   

T

, 
	
 

- 

*

%3

A? 4

'45 
	
 647
24  
	
 64  64

C4

	

#"$


21

98
:4 

	 4;
24  
	
 64 
=B

E GF
IH@J
K
L 
	
 6
*
H J   AM
 H J 
NHOJ  P 'Q
R4 SH@J
:0
K4 UHOJ
6VW
XZY

[2\^]^_`bacd`^e^f	_g\^]^_ih:\^j^kla2m^ano]^p^e`bq7r
s^tuwvx;vy{z|y~}ยS}ยx;}:ย5ย	ย
ย	ย
ยย6ย7ย
ย6ยKx	zยยยudยx7}ยyยยยยยR}ยย^ย=tยZย:x	y~ยy~ย^ยSย6uwย7ยยย^ยย6}ยยu;ยCยยยNยย@ย7y~ยwยZยย}ยx7}ยย>ylย
y~ยSx7}uwยยย@ย)x7ยยiย5ย7ย	ย
ย	ยยย6ย7ย:ยx;}ยyยยยยยiย ย ยoyยย+ยwยZtRยย:x	z|z%v^ย^}Evuwย^ยยย
ยSx	ยก%x	ยยtuยขยยคยฃ^ยฅiยฆยยงbuwt+}ยyยย
}ยช9u=ยซ ย^u7zยยยoยก@ย+ย^ยZย
ยK}ย^ยยxwยยยยย^ยยs^}ยy~uwยK}ยยx7}9}ย^ยยย^uwย^ยจยย^ยx;tยฉs^tย
ยยขyยยZx7}ย
ย)x;tยยย^uw}9ยยยZย}ยy~uwย^ย
ยy~ยi}ย^ย
ยฅ|ยฌDยญOยylยยฎyยยC}ย^ยEยZxwยยEvย
ยZx7ยยยย+}ย^ย+}uw}6x	zยฏยย^ยEvยZt9u7ย2suยยยy~vz~ย:ยก%x	ยO}uDยยย^uu5ยยย+}ย^ย)s^tยuwsยZt}y/ย
ย
u7ย2ย=ยฆยxwยยฐ}ย^ยZ=tย6zยx7}ย9}u!ย5ย	ย
ย	ย
ยย6ย7ยZยฌAyยยOy/ยยยZsยZยยยZย5}Lu7ย2ยฑRยฅoยฒKย+ยZx7ย=}ย^ยZtย6ยยuwtย)ย6uยขยยยz/ยยย+}ยx;}%}ย^ย
s^tuwvx;vy{z|y~}ยRu;ยยฎยณยดยฆยยbยฌEยฆยตยยuwt+ยยยยถDยยy~ยZย}ยz~Kzยx7tยยขยDยฑยทยฌยยธย7y~ยwยZยR}ยยx7}ยย5ย	ย
ย	ย
ยย6ย7ยยยx7}ยyยยยยยKย'ยยธyยย+vuยขย^ยยย
ย
x	ยก%x
ยนยยtยuwยยคยฃvRยยuwยยยยนยบKy~ยยยZsยZยยยZย}:u;ยยฎยฑiยฅยทยป7y~ยย6ย=}ย^ย:s^tยuwsยZt}y/ย
ยLu7ย9x7ยRย6z~ยZยยยZย}ยยKx7ยย>y~}ย
tย6zยx7}ยy~uwยยท}uDยย7ย
ย	ย
ยยยย;ยEยZx7ยยฉvย!ยย^u5ยยZย'y~ยยยZsยZยยยZย5}ยz~iu7ยยผ}ยย^ยEs^tยuwsยZt}y/ย
ยCu7ยOx=ยยขy|ยฝ$ยZtยZย}9ย6z~ยZยยยZย5}
ย ย ย^}ย^ย9ยยขy|ยฝ$ยZtยZย5}ยฎยZยwยZย5}6ย%ยณ+ยฆยยbยฌย6ยณ+ยฆยย ย ยฌย
ย	ย
ย2x;tยยพxz{zยธy/ยยยZsยZยยยZย5}	ยฅ%ยญ@ย^ยZtย6ยยuwtยwย}ย^ย9s^tuwvx;vy{z|y~}ย=}ยx7}
}ย^ยZtย!yยยยย^uRย ยuwย:xy/ยยย6z~ยZยยยZย5}=x7}:xz{z}ยx7}
ยA}ยuwยwยZ}ย^ยZtยยกOy/}ยยSย5ย7ย	ย
ย
ยยย6ย7ยย@ยx7}ylยยยฟย
ยยยยยNรyยย:x7}ยยยu5ยย}
ยฆรLรรยบยฌรรร ยฅdยญ@ยyยย+vuwย^ยย^ยย}ยย^ย=s^tuwvx7vy|z|y~}ยdu;ย}ย^ย=ย6ร^}ยZยยยy~uwยรxร5y~uwยยvย6y~ย^ยยทยยตxzlยยwย2tย6zยx7}y/ยยขย'}u
ยฟ^ร^ย
ย'ยย
ย	ย
ย
ยยยย;ยรยฅยฐยญ@ย^ยZtย)x7tย=ร ร ย^ร ยก%x	^ย2u;ยร}ย^ย
ยย)ยย^uu5ยยy~ย^ย+รยย6z~ยZยยยZยย }ยZยยยu+}ย^ย)s^tuยขvx7vy|z{y~}ย=u7ยA}ย^ย
x	รy~uwยรvย6y/ย^ยยดยยตx	zยยยย9x7ย5ยกยฎย^ยZtย@y~ยDxCยยu^ยย6zyยย@x7}Aยยuยย} ร ร ย ร ยฆรoร=ยบยฌ รร ยฅAยญ@ยyยยยฐ}ยZยย^ยA}ยuEยฃ)xยขย2ยฑรยwuย
ย
}uy~ยยฟยy~}ยwยฅรยญ@ย^ยZtย6ยยuwยช9tยwยซรยร}ร:ย^รย=รรรย6รรร:}ยยZร$ยรรยยy~รuwยรx	รy~uwยยรรย7ย
ย
ย	ยยยรย$ยฆยยยยร ร5รยรย$ยยยN~ยฌ)ยxยขยExยขยย5ยยs^}uw}ยyยย
s^tuwvx;vy{z|y~}ยKร9ย7y~ยwยZย
ยย^xwย%ยย
ยยy~tย
ยยฅ
ยงยธy~ยx	z|z~wยยกยผย)x;tยy~ยรx+su5ยยy~}ยy~uwย'}ยuยs^tu7ยwย9ยญ@ย^ยZuwtยZยรรยฅรรwรก^ยฅ
ยซ ยยช9ยซ ย ร  รฒรฎยฉรด7รฝรฎ ZรฎZรฝรฐGรผยรดยขรปย
รป 	
รข+รฃรครฅรฆ	รครง รจoรฉGรชรซ$รฌ รญรฏรฎ	รฐ%รฑรณรฒรฎยฉรดยรตรทรถ7รธ6รน+รบรป|รดiรผGรฝSรพ2รฟรรด7รฝ iรป|รฎ
รฐ ยช9
รฎ 	รฝรถ %รป|รฎ wรฎKรฒรด รทรฎยนรผยรฝdรพ รฟย wรผ dรผ Zรฎ  รด7รธรด5รฒZรป|รฎ Aรผยรฐ iรธรฎ   รฎ 
รฐ9รฐยรถยฉรฑ ยรญ$รฎ
รฐ -รฒรฎรฐ 5!รฎ รทรฎ	รฐ)รถยรต
 รถ รผยรฐGรผ 7
6รถ7รฝ"Zรฐยรด7รฝรฐ+รด # รฎรด7รธ6รผGรฝ$รผGรฝ>รฑ รถ7รธยรผGรฝ &% Zรถ'รฐ5รดwรฐ ยช9ยซ ย 6รถ7รฝรฐยรด;รผยรฝ')รฝรถ;รฝรฎ!รถยรต:รฐ5รฎ6รถ7รฝ"Zรฐยรด7รฝรฐEรผGรฝ)('รด7รฝ*
รป|รฎ
,รฐ +./#- รฒรฎ!รฐ 5รฎ9รตZรถ7รธ6รน+รบรป|รด 02143 165879;:!< : ย !=>6รบ5รนยรฎ!รฐ 5รดwรฐ9รฐ 5รฎรรธรฎ=รฎ ?
รผ Zรฐ ยด@รด Bรผ A
Cรฎ wรฎDรรธรผ  รฐGรผยรถ7รฝ รยธร 6$รบ 
รฐ 5รดwรฐ EoรตZรถ7รธ:รดwรปG2
รป GIF Hยฃ E ยช9ยซ รด7รฝ JFG รด7รธCรฎ Zรฐยรด5รฒZรป|รฎยฎรตรทรถ;รธ ร ร Eรด7*รฝ ยฉรฐ5รดwรฐรฐ 5Cรฎ   #รด 6Lรฎ KNOM ร|ยช9ยซ9ร 5รด :รดรบรฝรผ PZรบ^รฎ
รนยรด ?
รผGรน+รบ5;รน QยรฎZรฝรฐGรธรถ  	  รถ7รผGรฝRรฐ S F UTV5รฎรรฝ
W t Xยยฆ 
รฑ Y ยช9ยซ ยฌ [Z\ 7]_^a`cbed"gif 7h ]_W ^ajt`cX!bed"ยฆ รฑf Y รkร m ร ยฆ ยยนSbF ยฌ lยฌ knm \po ยฆ SbF ยฌ
gqh \po
Z \
รผ รต)รฐ 5Lรฎ wรฎรรฝรถ7รน+รผGรฝรดwรฐยรถ7รธ9รผ   รถ รผยรฐGรผ 7rรฎ 
s รฆ	รฅ"รฅ t62
รฌ u ยยย^ยยย+ยก@y~}ย^uwย^}z~u5ยยu7ย2ยยขยZย^ยZtx	z|y~}ย}ยx7}  ยยยZย}ยy~uwยยx	z|z2}ย^ยยย6uwยยย}x7ย}ยยย+vu7zยยยฎy~ย รฑ ย
ยยuย}ยยx7w} vยยฆ iร + /- Nยฌ xIvยยฆ  ยฌยฅ yยฐ W tuwsuยยy~}ยy~uw{ย z+}ยฅ |bย
W t ~X M ยฆ 
รฑ Y ยช9ยซ ยฌ  7 ]_^a` W t ~X M ยฆ รฑ Y ยช9ยซยร:รรรรยร^ร=ร ร ร ย=Vยฌ ย W t ~X M ยฆยย Y ยช9ยซ ยฌย
h
\
ย uw}ย%}ยx;}ยผยก@ยยZx;ย^ย^uw}@ย
xwยยy|z~:}xยwย@z|y~ย+y~}ย@u7ย W t ~ X M ยฆ 
รฑ Y ยช9ยซKร)รรรรยร7ร)ร ร ร ยยนยฌ2xwย G!F ยwuย
ยA}u ยฃF ยvย
ยZx7ยยย
}ยyยยAย6ร^s^tย
W ยยยy/uยขย:ยยZsยZยย^ยยฐuwย รรรรร x7ยยL}ย^ย%ย7x	z~ย^ยu;ย ร ยยย
ยยยยZsยZยย^ยยฐuwย+}ย^ยยยย^u7yยย6ย%u7ย G$F ยฅ ย%u7ยก@ยZยwยZt
ย
x7s^sz~ยขy/ย^ย tuwsu5ยยy~}ยy~uwยย z+ยฅNรยยก@ย9ยwยZ}
W t ~X M ยฆ 
รฑ Y ยช9ยซ ยฌ  7r ]p^ย` W t Xยยฆ รฑ Y ร ร ร ย=pยฌ ย W t ~X M ยฆยย Y ยช9ยซ ยฌย
\

h

ยฒKยKยZx7ยรย^u7ยก }6xยwยยฉ}ย^ย=z|y~ย+y~}Kxwย GรF ยยขu5ย
ย!W }u ยฃ^F ยฅร Y ยญยธยช9udยซ ยud}ยyยยZยยกยผยยยยย W tuยขsu5ยยy~}ยy~uwยยz+ยฅรร^ยฅยญ@ย^ย
ย5suw7r}]_ย^^a`"ย
ยยb#ย
d ย+f u;ย}ย^ย!F }ย^ยZuwtยZยยy~ยยsz~i}ยx7W } tXยยฆ ย ยฌLHรยฃiยฆยตยยuwtLuw}ย^ยZtยกOylยยwยA}ย^ย'ยยZย^uwย+y~ยx7}ยuwt
ย ยZยZtยu^ยฌยฅ x;t}@ยฆยxยฌ$u7ย$}ย^ยยฎs^tuwsuยยy~}ยy~uwย:}ยย6z{zยยยฐยยAยก@ยยZx;ย+y~ยwย^uwtย%}ยย^u5ยยย
g h k m \po ยฆ Sยฌ$ยก@uwยzยยยvย
Z \
ย6uwยยsz~ยZ}ย)ยย
ยยย6tยy~s^}ยy~uwยย%}ยยx7}%x7tยยฎy/ยย6uยขยยยyยยย}ยZย}%ยก@y~}ย + /- ยฅAยฒยทยยพยZx7ย=ย^u7ยก x7s^sz~!sx7t}ยฆรvยฌA}u+ยwยZ}@}ย^ย
ยย
ยยy~tย
ยยนtย
ยยยz~}
ยฅ
ยยย

ยย$ยยยqย'ยยwยqยยย#ย#ยqยVยpยยยยยcยยยย#ย

ยยย#ยยNยกcยขยฃ6ยค"ยฅnยฆ_ยค"ยงยจยค"ยยชยฉ#ยซ
ยฌยญ;ยฎยฏjยญ)ยฐ#ยญยฏjยฑCยฒ#ยฏยฎยณjยญยดยถยตยยทVยณjยธCยนยฏjยธยดยญยยบjยบ4ยธeยฏยยป2ยฏjยญยฒ#ยธ#ยฏยผยฑยฝ,ยฏjยตqยพ;ยฟ"ยญยท6ร*ยธยดยณjรqยญรร2ยญร"ยฎยฏjยณjยพUยญรiยณยธยดnรยยฎยณjรqยญยพCยฎยณ4รรยบยฎยณ
ร ยณยฎรยยดยธ#ยฏรรรรยรBยฐ#ยญยฏยบรรBยณlยฑ#รยดยถยธeยฏรยร8ยบยชรBรยยฐrยฎยทBยต"ยฎรยยทBยญ)รqยญยทBรLรwรBยณjร;ยณjรqยญwรqยฏjยธยยธยด"ยธยดVยนยฏjยธeร"ยธiยบรรBยณ4รBยธ#รLยฝ2ร}รqรpยฌยญwรwยธ#ยตยยทร>ยทยรaรeยญ
ยณjยธLยณยผร"ยฎรqรรร'ยฎรยรBยญยพรยฝwยฎ#รรยผรยยต"ยบรqรwรBยณjร!รรqยธ#ยพรรwยญรยบ4ยณยฎยฏjยณjยญยรCรwยธ#ยฏjรeรBรqยฒLยธ#รยณjรยรยบRยฒ#ยญรqยญยฏยฎยทรยฎยฏjยญยยฎ;ยธยดยฏjยญยยบ4ยญยยฎยฏรjรVร
ยฎร"รIร!ยธiยบ4รqยญCรNยฎยฏรeรRยดยถยธ#ยฏ;ยต"ยบ4ยญยดยตยยทรรยธ#ยพUยพLยญรiยณยบรยธeรรยฎยรqยฏยผยญยฐ#รBยธ#ยต"ยบCรยยฏยฎยดยณ;ยธยดยยณjรยรยบ;ร"ยฎร'ยญยฏยรรรรรqยฏยผยญยทรรBยพ;รBร"ยฎยฏยผยฑ
ยฐ#ยญยฏยบlรaยธeรรยธยด"ยณjรยรยบร"ยฎร'ยญยฏNยฎรqร"ยญยยฎยฏยผยญยร)รBรร,รกjรขยรฃrรคยชรฅยรฆรงรรจjรฉNรฉ,รฉรช"รซรฌ2รญ$รขรฎรฏรฐiรฌ[รขรฑCรฒรณรขรดรฏรฃwรฏรฑยรตNรขรฌ2รญ"รฐqรฆรถรกรช*รฃรฏรถรฑ*รฃรถร
ร ยธ#ยพLยญ,ยธยด"ยณjรยรยบpยฏjยญยยบ4ยญยยฎยฏรjรรรยฎ#ยบVร'ยญยฏ4ยดยธ#ยฏjยพLยญยร>รรยรยยทaยญRรรรqยฎยพ[ยป2ยฏjยธยฐ#ยญNยฎร"รรร)ยฎรqรqรqยญรท2ยธยทยยทaยญยฏ,รwยญยฏjยญ.ยฎยณ ร ยณยฎรยยดยธ#ยฏร
รRรยรaยฐeยญยฏยบรรBยณ4ยฑรยฎร"รCยฎยณwยณjรqยญ2รธlยฝwรรนรยทBยพยฎeรยยญรยรบwยญยยบ4ยญยยฎยฏรยผรยรปNยญรยยณjยญยฏยรรผwรยรยบwยฏjยญยยบรฝยญยยฎยฏรยผรรยฎ#ยบ,ยบ4ร"ยธ#ร"ยบรฝยธ#ยฏjยญยรรพรaร!ร"ยฎยฏjยณ
รยยฑยยณjรqยญรรรBยฏรรcยธ#ยฏรยญU
รฟ รรยญ;ยธยด ร รยผรBยญรiยณ4รยยฟรCรบwยญยยบ4ยญยยฎยฏรj
ร 6ร)ร ร 
รป ร'ยตqร"รยยญยฏ;รปNยธ#รยยณjยฏยฎ#รยณ2
ร 	
4
รป qร
รยยฑรพยฎรLรธlยฝwร ยป2ยฏยฎ#รยยต"ยฎยณยผยญยรcยญยทยยทaยธรยยบรฝรยรaรVร'ยฎร"รLรยยฑLยฎรรRรยรBยฐ#ยญยฏยบรรBยณ4ยฑLยธยดรป.ยฎยทยรยยดยธ#ยฏjรยรยฎ;ยนยฏjยญยยบรรรยยญรยยณ }ยบ,ยนpยธiยบ4ยณรยยธqรยณjยธ#ยฏยฎrยท
รqยญยทยยทBยธรยยบ4รยรBรVร

ยยคjยคeยค"ยNยqยค'ยซ
ยฝwยฎ#รรยผรiยต"ยบร'รwรร.รถ4รญ'รกรฝรถรฎDรถรฑรฆรฏรฑ$รด!รฑ#"$wรถ%รฎDรขรฑ'รฏรฑqรด'&รฏยถรฆรงร,รกjรข	()(รฏ+*}รฏรฎรฆรฏยถรฃ-,)รฑ*รข.&*รรถ"ยรดeรถยรร!รธlรผ ยนnยฏjยญยยบjยบร
รป.ยฎยพ;รqยฏ4รรยยฒ#ยญ#รqรยยฎ#ยบjยบร

ยฝwยฎ#รรยผรiยต"ยบร'รwรBร*ยปยยฏยผยธrยฐeยญ#รiร;ร/qร8ร0ยยฎrยทaร'ยญยฏjรVร#/qร1;รBร2 รท2ยธยทรยทBยญยฏยร*ร;ร#%3.รรcยฏjยธ#ยพรยบ4ยณยฎยณ4รยบ4ยณ4รรยฎยทVรยรqยธrรยยทaยญยรยยฒeยญ
ร"ยฎ#ยบ4ยญยยบ2ยณjยธยรยยญยฒ#ยฏยผยญยญยยบ)ยธยดร"ยญยทยรBยญยดjรรรผยชยญยรยผรVรVยฏjยญรVร4eร#รqร*รธlยฝwรรร;ร2ยฐrยฎรยยทยฎรยยทBยญรiยฑยฎรqยธeรiยฑยยพLยธ#ยต"ยบRยดยณjรยยดยฏjยธ#ยพ
ยทaยธeยฒ#ยธiยบร ยตยรยฎยณjยญยฏ4ยทBยธยยธ"ร}รยฎ 5รqยตq#
ร 5ยร"ยฎ#รรยผรiยต"ยบ*ยธeยฏVยฐ#รยฎ2ยฌยจยฌยจยฌยฎยณVรยยณjยณj4
ร 6755ยทBยธ#ยฒ#ยธiยบร ยตiรยฎยณยผยญยฏ4ยทBยธiยธ"ร
รยฎqรjรIรqยฏjยญยทยรB$
ยพ 
รaร"ยฎยฏjยฑ)ยฐ#ยญยฏยบรรBยธ#ร;ยธยดqยณjรยรยบpรwยธ#ยฏjร)ยฎรqร"ยญยยฎยฏjยญยร>รaรCร,รกรฝรขรฃr9
รค 8Vรง#รฏรกDรฆรถรถรฑ*รฆรง)รจjรฑรฆรถรก#
รฑ #รฆรฏยถรข:รฑ *;$รขรฏรฑ*รฆ_รต,รขรฑ <รถรกjรถรฑ*รฃรถ
รข>
รฑ =2รกDรฆรฏ ?.รฃ@รฏ *"รจยผรฑรฆรถ *A*
รฏaรดeรถรฑ*รฃ!
รถ Bร)รจ ;*C
รต =)E
รจ D F.GH#4
ร Iqร#ร"ยฎยฒeยญยยบร 
I3Jqร 
qร
ยฝ,ยธ$รยผรqร"ยฎร*ร	/qร8รqรปNยธiยบรฝยณjยญ#ร#รรรBร2 รบwยธยฑ#รiรรร%3KรMLOรถยN รขรฌPรถN รฆรกรฏรถQ=R* รดรถSN (รกรฏ+TรฐqรถUรถN รถ*A*ยรถยรรยธยท6ร32ยธยดV=ยรช*รถรกรฏยถรถรฎ
Wรข <RX{3รข "#รถรกรฑยรช"รฐย%รก Yรถรซรฎ)รฏรฑZX[#รฆรงยรถรฌ$#รฆรฏรฃรฎร ร รqยฏ4รBรqยฒ#ยญยฏ)รยญยฏ4ยทยฎยฒ'รVยฝNยญยฏรฝยทรรBร\0Rยญร8รยยญยทBร'ยญยฏjยฒ"ร
รป.ยฎยฏยผร"ยฎรVรrรบ>ร%3#รรeรฒVรขรดรฏยถรฃ*3]Vรขรฐiรฑ#"#รฆรฏรขรฑ"รฎwรขW<Nร,รกjรข	()(รฏ+*}รฏรฆรซ#ร#รรยรBยฐ#ยญยฏยบรรBยณlยฑรยธยด*รปNรยรรยฎยฒ#ยธ)ยนยฏjยญยยบjยบร#รปNรยรรยฎยฒeยธ"ร
รป.ยฎยฏยผร"ยฎรVรNรบ)ร^%eรร_8Vรงiรถ รตNรขรฑรฆรฏรฑ"รฐยรฐยรฌ
รปNรยรรยฎยฒ#ยธ"ร

รข`<รจjรฑ:"รฐqรฃยรฆรฏAYrรถaXยรถยรฆรงiรข3"รฎร รRรยรaยฐeยญยฏยบรรBยณ4ยฑ ยธยดรรปNรยรรยฎยฒ#ยธ ยนnยฏjยญยยบjยบร

รปNรqยญยญยยบ4ยญยพCยฎรVรNยนpร,รปยร^%Ibรร ยพLยญยณjรqยธqรยธยด)รยธ#ยพLรqยตqยณ4รBรqยฒยฒ#ยญรqยญยฏยฎยทยร7cยญยร ยฝwยฎยยฑeยญยยบรรยฎร รqยฏยผยธ#ร"ยฎรยรยยทยรaยณ4ยฑIยฐยฎยทd
ยตqยญยยบRยดยถยธ#ยฏ2ยญ eqร"ยญยฏjยณรยบ4ยฑqยบ4ยณjยญยพCยบร2รธ6รIร,รกรฝรขรฃrรครรฉNรฏBรดรงiรฆรง!รจjรฑรฆรถรกรฑ##รฆรฏยถรขรฑ:*#;qรขรฏรฑรฆ)รต,รขรฑ<รถรกjรถรฑ*รฃรถรรขรฑ\=2รกDรฆรฏ ?.รฃรฏ@*
รจjรฑรฆ3รถ *f*
รฏBรด#รถรฑ*รฃ'
รถ Bร)รจ ;
รต =)E
รจ D g.GH#รqรqรV4
ร SJbqร
ร2ยญรยรยรaยฒeรVรรรท;รnยปรรBร42 ร2ยญรยรยรaยฒeรVร9/cร ร รh#รรLรฉ,รฑรฆรกjรขรญ"รซ!รฏรฑiwรถ*j#รฆรฏรขรฑ รฆรข!รจjรฑ*รฃรขรฌยรญ#*รรถรฆรถ$,)รฑ*รข.&*ยรถ%"ยรดeรถยร
รป.ยฎยพ;รqยฏ4รรยยฒ#ยญรรRรยรaยฐeยญยฏยบรรBยณ4ยฑยยนยฏjยญยยบjยบร"รปwยฎยพ;รqยฏ4รรยยฒ#ยญ#ร"รรร}รท;ร
ร"ยฎยฒรBรVร"รบ)ร#%3KS
รpยนยฏjยธeร"ยฎรยรยยทรรBยณ4รBยญยยบ2ยธ#รยฟ"รยรBยณjยญรยพLยธqรยยญยทยบร;qรขรฐยรกรฑ#*_รข`<)รช"รซรฌE(jรข*
รฏรฃ)รฒVรขรดรฏยถรฃรlkbm#.ร"รSJ$รqร
ร"ยฎยฒรBรVร"รบ)รBร02ยฎยทBร"ยญยฏjรVร/cร1รร8รb2 ร!ยญยฒรรqรยยธ"รlnรรl%ร*รยจยทBยธ#ยฒร8รรยดยถยธeยฏ.ยฏยผยญยยฎ#ยบ4ยธ#รยรBรqยฒยฎร'ยธ#ยตqยณwรqยฏjยธ#ร"ยฎรยรยยทยรBยณ4รBยญยยบร
รจjรฑ <รขรกo
รฌ eรฆรฏยถรข\
รฑ รฑ#"ยรต,รขรฌ2รญ"รฐ$รฆ+eรฆรฏยถรขรฑ*รCgยรฅl%.5SbรlKS3J:qร
ยปยยญpVรqยญยฏยร0รรBร2 ยนpยญยยฎยฏรฝยท ร/cร%3ร ร ยดยฏยฎยพLยญรwยธ#ยฏjร ยดยธ#ยฏLยฏjยญยยฎ#ยบรฝยธ#รยรBรqยฒIรwรBยณjร รยยญยด6ยฎยตยยทBยณยบร รธ6ร รท2ยฑiรqยตqยฏยผยฒ"ร
/ยฏยรB#ร 0รlร qรBร#r*ยธ#ยตยร6รVรบ)รBร42 รป.ยฎยฏ4ยทยบ4ยธeรVรVยป;ร4sq,รqยบรjรV,)รฑรข.&*ยรถ%"รด#รถ!wรถ4รญ"รกjรถรฎรถรฑ*รฆ+#รฆรฏรขรฑ\รฑ:"at)รถ+<รถ%รฎรฏ(*ยรถ
.รถ รฎDรขรฑ"รฏรฑ$รด#รeรqรVร .iSร Jb
qรยรทRยทaยตยรwยญยฏยร)รยฎ#รยยญยพ;รรรยนยฏjยญยยบjยบร"ร2ยธ#ยฏรยยฏjยญยรยผรยยณยรnยญยณjรqยญยฏรฝยท8ยฎร"รqยบร

uv

wxyz:{}|~i{ยย3z:ย-xyzยExยยย|ย|ยยyยยb{}ย.ย
ยยdยยWยlยย)ยย#ยยbยยย#ยsยย#ย7ย%ยlย%ยยยยกย%ย^ยขยคยฃยฅยฆ.ยงAยจbยฉ.ยช-ยงAยจ[ยซRยฌ.ยจlยญoยฌยจ:ยฌยฎ+ยฌ.ยจlยง@ยฏยฐยขยคยฃยฅ.ยชยฌ.ยจยงAยจbยฉย9ยZยฑยยฒย	ยณ.ยยตยดRยณ.ยถยท@ยธaยณ.ยย4ย
ยน	ยณ.ย>ยยบยณ.ยป)ยยฑยlยผยฝ!ยlยน	ยณ.ย[ยพ}ย%ยณ.ยยฟยฒยfย)ยฟยฑย
ยรdยยยWรยรรรยรรยร!ยdย	ยด-ยฑย	ยณ.ย4ย	ร!ยร%ยdยยlยdยฑยยฑย4รรรยรรรยยยรยdยรรยณ3รfยณ.ยยฑ.ร:ยรรยยฝรยlย%ย3ยรยย%ยlรยณ.ยยยRยณ.ยยaยยยย)ยย
ยฑ.ยทCย)ยยณ3รjยdรยณ.ยยjรjยdยปsรยบยฑ.ยทMยท@ยฑยยฒยธรยถรfยณยยคย7ยZยป)รยRย)ยยWยปยฒยWยfยฟยป)ยยaรย)ยยยfยฟยณ.ยป)ย!ยฟยณ3รfยฟยถร7ยถยรยรRยงร)ยฃรร%ยจ#ยฃยฎAยงAรSยฅยlร	ยMย.ยกรbรย}ย
ยQยฑรยยยธaยณ.ย4ย4ยนยlยฝ!ย4ย%ย3ยยยกย%ยย9รรยฟยฒยdยย	ยปRยธรยยป)รยฑbยยรกยท+ยฑยรขยฟยณSรยยฟยถรfยณ.ยป`ย7ยยZยธEยณ3รฃยdยธรยถยธรคยยยป)ย)ยฑรร>ยยfยWยป)ยWยdยยถยปWยdยฑยยย
ย[ยณยWยป)ยยรรยยยป)รยยรฅยfยย#ยZรsรPย^ยยผยนEร-ยรยณ.ยยฒยป)ยธ$ยยยปย
ยQยฑรยยย`รยธ!ยfยยปยยUยยยย>ยฑย)ยWยfยยรฆยdย	ร_รฆยยณ.ยWรย	รงย}ย%ยยยรจย%ยยฝรฉยธEยณ3รฃย7ยธ!ยถยธยยยยป)ย)ยฑรรรยณรรย)ยฑ	ยณยฟยฒร$ยป)ยฑRยยฑยยธรยฑยยฑรช
ยป)ยฑยยfยฟRย)ยยณยWยฑยยdยยยรรฅยรซร)ยฌยฏ.รฌMยซรขยฅยฎAยง+ยฌ.ยจ#ยฅรญรฎ^ยฌ.ยจรฏยฃร)ยฃยจ#ยฏ%ยฃ'ยฌ.ยจ[รฐ-รยฎAยง รฑยคยฏยง+ยฅรญ#รฒยฒยจ:ยฎ+ยฃรญAรญรณยง7ยฉยฃยจ#ยฏ%ยฃรรด@รฐ-รฐQรฐยฐรฒรรต รถ.รทรธย
รร4ยร.รน	ร3รรรบรย
ยQยฑรยยย`รยธ!ยfยยปยยUยยยย>ยฑย)ยWยfยยรฆยdย	ร_รฆยยณ.ยWรย	รงย}ย%ยยยรปย%ยยฝรฉยธEยณ3รฃย7ยธ!ยถยธยยยยป)ย)ยฑรรรยณรรย)ยฑ	ยณยฟยฒร$ยป)ยฑRยยฑยยธรยฑยยฑรช
ยป)ยฑยยfยฟ$ยยฒยยณยWยฑยยdยยยรผรฒ)รฝรฝ^รฝรฟรพร`ยฅยจยชยฅยฏยฎAยง+ยฌ.ยจยช$ยฌWรฏaรซยคยฅยฎ+ยฎ+ยฃรยจ รฐ-ยจ#ยฅรญ  ยชยงAยชoยฅยจ:
ยฆ ยบยฅยฏ ยงAยจ:ยฃ รฒ)ยจ:ยฎ+ยฃ3รญfรญรณยงdยฉยฃรยจ:ยฏยฃย
 ยWรปย%ยรรรจ3รรรปรย
ยQยยณ.ยย
	sยยณย4ยยยlย%ยยยรปย%ย^ยผยฑยธรรร7ยรฃยdยปWรEยฑ.ยทhยป)รยย%ยWยปWร	ยฑย%ยยยยป)รยยฑย)รaยฑ.ยทยณ3รdยธ$ยฑ	ยWยปยณ3รjรยWยป)ย)ยถยฟยป)ยถยยฒยยย^รฒ)ยจรฏยฌ.ร
ยญoยฅยฎAยง@ยฌ.ยจ\ยฅ.ยจ#ยฆ>รฎ^ยฌ.ยจ:ยฎAร)ยฌรญjย  ร	ยยยรจ3รรรจ.รนย

ยQยยฒยฑSรยย	ยฝ!ย#รงยdย-ยณ3รdรยย)ย4ย4รง}ยร!ยdยร ยด-ยฑ.รjร7ยย3ย4ร!ย#ย%ยยยรย%ยรยณยยยฑยธยฑย`รยยย ยณ.ยยยตยธEยณSรฃ	ยdยธ!ยถยธ ยยยป)ย)ยฑร	รย
รยยบรซร)ยฌ3ยฏSรฌ ยฎ ยตรฒ)รฝ^รฝ
รฝ  
ยญ Vรฌยฌ.ยจ4ยฌ%ยฉยง@ยฏยฐยงfยจรฎยฌ.ยญยฎ+ยฃร:ยฏยง+ยฃยจ#ยฏ%ยฃ3ยรร4ยรร3รbรปรปย
ยQยยฒยฑSรยยยฝ!ยCรงยยยQยณ3รdรlยย)ย4ยhรงย#รรยยยCร ยด-ยฑ.รjรdยยยhร!ยย%ยยยรปยณย%ยRยฝRยWรยธ$รยป)ยฑยปWยfยฟ!ยฟยฑยยยdยปWยdยฑยยณ3รรย)ยฑยยณ.ยยjรjยdยปWยdยย
ยป)รยRยยฑยรช+ยถยยณ.ย)รยตยฟยณยWยย^รยยWยยณ.ย%ยฟยฒร>ยยฒยรยฑย)ยป-รรงEยรบร.รนยร!ยย
ยQยยฒยฑSรยยยฝ!ย4รงยdย"Qยณ3รdรlยย)ย4ยCรงย#ร!ยdย4ร ยด-ยฑร รdยยยhร!ยMย%ยยยรป.ย#ย%ยRยฝRยWรยธ$รยป)ยฑยปWยfยฟ!ยฟยฑยยยdยปWยdยฑยยณ3รรย)ยฑยยณ.ยยjรjยdยปWยdยย
ยป)รย>ยถยยณ.ยยฒร ยฟยณยWยยPรยยWยยณย%ยฟ)รรฉย)ยรlยฑย)ยปยตรรงยรบรรปยร#ยUยPรMยฑiยณ.รรยยณ.ยรยdย$รฒsรฐ&%ยฌร%ยจ#ยฅรญQยฌยจ
รฎยฌ.
ยญ ยฎAยงfยจยฉย

-ยณ3รdรยย)ย4ยรง}ยร!ยย%ยยยรจย%ยlยฝQยยตยณ.ยยณ3รdรยรฅยfย^ยฑ.ยท'ย%ยWยป)รช+ยฑยยยยhรdยฑย.ยfยฟย^ยฑ.ยทCรย)ยฑยยณ.ยยjร ยdยปWรยCรฐรขร ยฎAยง รฑยคยฏรยง@ยฅรญ}รฒ)ยจ:ยฎ+ยฃ3รญfรญรณยงdยฉยฃรยจ:ยฏยฃย
(*) ยรปยยรbรปรบรจ}ย
รงยณ3รยยยยSย ยSรยฐย.ย%ยยรบbยกย%ย3รยยท+ยฑย)ยธEยณ.ยป`ย7ยฑย ยป)รยยฑยยฒรรขยณ.ยย'ยWยป%ยณ.ยป`ยยย`ยปWยfยฟยณ3ร	ยธ$ยยฟยฒรยณ.ยยfยฟยยlรซ+  ยชยง@ยฏยฅรญ.ยขยฃ3, ยง@ยฃ.- ย  รท ) ยรน}ย%ย
รรรจ3รbรรปรจย

รงยณ3รยยยยยย.รRยย%ยยยกSยbย%ย0/รฉรยย)ยยยฑ1ยคยยWยป%ยณ.ยย'ยฑย'ยธEยณ3รฃยdยธรยถยธยยยยป)ย)ยฑร	ร32SยรยEย#ยรยdยยย	รRยร!ยยยSร ร4ยWยdยยถยย
ยUยยยsยยยรยjย%ยR4รพ 	5
ยฃ ยบยฅ 6ยงA7
ยญ 	ยญ รฝยจ#ยฎAร)ยฌ  8 ยฌ.ร%ยญ$ยฅรญรณยงAยช%ยญ$ยMรร4ยยคย3รบ3ร:ยยย}ย4ย>รWร รฆhย)ยย)ยรยยผยณ.ยธ!ยยWยfยยยย
ย[ยณย)ยย

รงยณ3รยยยย:ย ยรRย#ยยยยรยย+9ยZยป)รย ยยณ.ยปWยdยฑยยณ3รdย ยฑยท9ยธEยณSรฃ	ยdยธ!ยถยธ$รช+ยยยป)ย)ยฑรรEยธรยยป)รยฑbยยรยรซ^ร`ยฌ3ยฏSรฌMรฒ)รฝ^รฝรฝย:รทMยsยย%ย
ยรปย3รbยรบรย
รงยณ3รยยยย4ยย#รRยMย%ยยยรปbย%ย-ยผยฑยยฟยยยป)ย%ยณ.ยป`ย7ยฑย>ยฑ.ยทยคยยfยWยป)ย`ย7ยยถยป`ย7ยฑยย ยณ.ยป-ยย	ยปยฒย)ยฑรรยตยธEยณ3รฃย7ยธaยณยรรฅยOรรกยฑ	ยWยยรย%ยณ.ยยป)รย
รRย:ร!ยVยsยย#ยjยย4รฝรฌ-รพ94รฌ %ยฅ  ยจ:ยฃร.ยช ;รขรซยคยฅ ยฃรยชรยฌ.ยจ\รซ^ร)ยฌ	ร)ยฅรยง+รญรยง+ยฎ < :ยฎ+ยฅยฎAยงAยชยฎAยง@ยฏรยช < ยฅ.ยจ:ยฆ=#ยฎ+ยฅยฎAยงfยชยฎAยง+ยฏ%ยฅรญVรซ+  ยชยง@ยฏรยช ย
รร4ยรปยรบ3รรปรปรยยด ร7>ยถ ยยยร-ยฑยยย)ยยฟ)รยป@ย ? ยยป)รยยWรfยณ.ยยยย
ยด-ยรยยยย:รง}ยยยยยยรย.ยยCรฐ รพร)ยฃ%ยฅยฎAยงfยชยฃ ยฌ.ยจยบรซ^ร`ยฌร)ยฅ	ร ยง@รญรณยง+ยฎ  ย9ยยบยณยฟยธoย รjรfยณ.ย4ย#ย#ยฑยยยฑย4ย

ACB

DFE*G>H@I3J:KML@NCO0I0E@PJ"QSRTGUNCN
I0E
VWXYX[Z\C]^7_[]>`badceXgf3Z\ih]j@_>k7_3lnmCo0o0p*qn_'rsX[W0tuwvyxzW0\Fcf@f@\iW
{|u[}~cยiZF\iZCc0ยยW0h>u[h@t_"ยยhยยyZยZXย]ย_[]@ยuwviย]
ย _[]+`ยย0ยFc\iยW0ย@C]:ยb_Ml!ยย@ย_Yqn]Tย+ยiยCยย7ยยยยwยiยยiย'ยzยกยยยฃยข0ยยzยย'ยข0ยคยฅ+ยยCยฆยกยiยก.ยยฃยยก5ยยยงยยnยยยฃย.ยยฉยจยฃยคYยก.ยชยซย#ยฆ
ยฌ1ยยฃยยญFยคYยกnย
ยฎ0ยกyยฏยฐยก#ยจ3ยยยก.ยชยฑยก.ยยฃยzยข0ยยzยย5ยขยยฃยยฏMยกnยขยชยย3ยwย@ยฎยณยฒยดยฌ1ยฏยถยต ยท
ยธยน0]0f@f_3mCยบ0ยป
ยผยฃm
ยฝยพ_CยฟรW0\it>chยณV1cย>xร}=ch@h]
ย|chยยฟciZW] ย r7]3ย|chรรU\nchvยuwยivW_
Vร>ย@ย@\it]j\
_[]a_ยF_3lnmCoยร0ยป>qn_"รMย@Zร\iZxzZ\iZhvZ7vยXรcยยiย_Mยย0ยรยคYยยชยยจยยร~ย#ยฆรร'ยยzยกย'ยnยก
]ร0ร4l!ยป>qn]@ยป>ร
ยพยผ*ยป0o>ร>_
รcf>Xwc0vZ0]Mร"_ย_รย'_lnmCร0pยร>qn_รยชnยชยขยFยย0ยรยคYยยชยยจยยยรร.ร*ยกรยชnร>ยยซยคYยก.ยชยณย+ยiย|รยยข|รยzยครยรย@.รยก ยชยฑ_รย:h@tXYuwย#ยรi\ncยhยยดXwc#u[W0hรuรย
ย+ย0ยzยคYยยชยฑยยฑยจย0ยzยnยข0ยค3รยชnยชยขร~ยยยยยย>รiยข|รยฑยรยครยzยยรยก.ยชยฑ]>^Wร0Z\dร:ย@ย>XYuwvc#u[W0hย.]ยฃยyZยรk:W0\iร']mCoยยบ@m0_
ร'ยvZ0]'ย1_^7_[]@`SยFc
uYรรc@]a_lmCo0ยบ>ร0q_FรFยขรยณยกยช7ยขยยฃยรขรก1ยกnย.ยwยชยรยยยยช_ยuYXgZร0]ยyZยรk:W0\ยรยฃ_
ยTuรฃXwยiยยW0h]ยฃย_'lnm
o0ร0ยฝ>qn_"ร:\iWยยcย>uYXรฃuwย#ยuรvX[W0tuwv_Mรคยยย รฅMยยzยข0ยค3ยiยยฃยzยก
ยคwยครย[ยฎ0ยกย'ยnยก
]>ยธ*รฆ]4ร|mยผ*ร*ร|_
ร:c\#uwย]3j@_ย_[]`รจรง:ZhvWeร@ย#รc@]@r7_'lnmCo0รยo>qn_รฉFhยiย@Zรcยf@f>Xรฃuwvcย>uYXYuw#รWx:}~c
{>ug}7ย@}รชZh>i\iWยf|ร=iWu[h@Z{Uc0v
\iZCc0ย#W0h>u[h@t_Tยiยยฃยzยก.ยnย'ยข0ยยรยยยยฃยข0ยค3รซ@ยร|ยยยฃยขยยคย#ยฆdรคTยจ0ยจยiยรฌCยรยณยข0ยzยก7ยฏMยกnยขยชยย3ยwย@ยฎ0]'รญ0]mยผ@ยปยพ_
ร"ZCc\#Xย]:j@_รฎlnmCo0ร0ร*qn_=ย+ยiย|รยยข|รยzยครยwยชยยzยยณยฏMยกnยขยชยย3ยwย@ยฎ=ยwยรยยยยฃยzยกCยคยครยgยฎยยกย'ย+ร3รยชยฑยzยก.รรฏยช_~ยฟรW0\ยt|chV1cย>xz}~ch@h]ย|ch
ร@\nchvยuwยivW3] ย c
XYuYxย_
ร"ZCc\#Xย]jU_lnm
o0ร0o>qn_:รรฎ\iW0ยcย>uYXYuwย##uwvยณย#Z}~cยh|#uwvยxzW0\Fh@Wยh@}ยณW0h@W0iWยh>uรv\iZCc0ย#Wยh>ugh@t3รฐ+rรฑย#ย@\ยร0Zร0_+ยยดhรย\nc0vยย@รฒ
}~ch]|ย_|j@_[]>ร'Zร0ZCยiรณยย@Z0]@aร_>j@_[]>`รดยMZu[iZ\
]*ย_@l!ยย@ย_Yqn]>ยยยย
ยeย3รต:ยยยชยฑย4ยiยยฃยzยก.ยnย'ยข0ยยzยยยฃยข0ยค'ยฅยยCยฆยกยiยกย'ยnยก
ยยยงยยnยยยฃย.ยยฉยจยฃยคYยก.ยชยซย#ยฆรยฌ1ยยฃยยญFยคYยกยCยฎ0ยกรยฏยฐยก#ยจ3ยยยก.ยชยฑยก.ยยฃยzยข0ยยzยยsยขยยฃยรยฏยฐยกยขยชยยยย*ยฎรถยฒยยฌ1ยฏรทยต รฆ0ยทยยน0]Mf@f_+ยบ0รยยบ
ยผ*ยบ@mCยฝU_
ยMZf@\#u[h|ยZCยรขughรยฏยฐยกยข0ยยย*ยฎยยชFยยยงรธย'ยnยก.ยยฑยzยขยย=ยฏยฐยกยขยชยฑยยยยย*ยฎ0]>รน7_@ย0ยc
xzZ\Fchยยณj@_@ร"ZCc\ยXlยZCย@ย_Yqn]>ยฟรW0\it|ch
V1cย>xร}=ch@h]'ย|chยรU\nchvยuwยivW] ย c
XYuรฃxi_[]m
o0o0ร@]>f@f_ยฝ0oยo
ยผร|mCรU_
ร"WXYX[W*vยร']j@_ร:_4lnmCoยรยพ@qn_"ร@W0ย@hย@cยugWยhยxรWย\dยยu[\iZCvFu[h>xzZ\iZhvZ0_รขยย|ยกยยnร=ยขยยฃยรขรกรบยกยยยชยรยย']รป>รผC]'p0p@mยผ*pยยบ0ยฝ@_
ยMZuwvยย@Zh|ยc0vยย]a_lmCoยพ|o>q_รย4ย|ยกnยยยnร~ยยยฆ1ย+ยiย|รยยข|รยzยครยรยร0_:รฝFh>u[ร0Z\ยยดu[!ร5Wยx ย ceXรฃuYxzW0\ih>uwc=ร:\iZCยiย.]*ยZ\iรยZXgZร0_
ย0ยch@h@W0h] ย _[]|`$ยรZCcCรยZ\C]>ยรพ_3lnm
oยพ|o>qn_ย4ย|ยก1รฟยข0ยย|ยกรยณยข0ยยzยยข0ยคMยย|ยกยยรยณย#ยฆยณยฅ+ยร7ร7ร|ย3ยรยยข0ยยzยยยฃ_"รฝFh>u[ร0Z\ยรฒ
ยยดu[!ร=Wx:ยzXYXYu[h@Wuwย1ร:\iZCยiย._
ย0ยc0ย#ย\#uย]ร:_lmCo0ร0o>q_ยฐ^Zxยcย>X[1\iZCc0ย#Wยh>ugh@tรขu[h ย#Z}~cยh|#uwvh@Z!ยMW0\ยร*ยรฐMc xรW0\ย}~c
XYu  c#u[W0hWx+\iZCvW0tยh>ugยugWยh
chยรขu[h@ย@Z\#u[nchvZ0_Fรคยยย รฅMยยzยข0ยค3ยiยยฃยzยก
ยคwยครย[ยฎ0ยก.ยยฃยยกC]@รญ|ยท l!ยป>qn]3p0ร0ยบ
ยผ*ยปยยบ0ยบ@_
ย0f>u[Zt0ZX[ยc
X[iZ\C]'^7_|j@_3lnm
o0ร0ยฝ>qn_'ร:\ยW0ยcย>uYXYuรยย#uwv1\iZCc0ยยW0h>u[h@tรu[h5f@\iZCยยuwv#u[ร0Z1Z{@fZ\ยTยยร*ย#ยZ}~ย_ยยดhยซV1cย}~c
Xย]
ร:_eย_[]`ยงร'Z}รข}ยณZ\C]0j@_0รM_l#ย+ย@ย_Yq]รรธ4ยยฃยยกยยzยขยยยฃยรยwยรฏรครบยยฑยย รฅยฐย.ยรยข0ยคยยiย'ยzยกCยคยคยฉย[ยฎ0ยก.ยยฃยยกC]f@f_ยพ@รยผ@ยฝ0ร@_ยFWย\iiย@รฒ
aFWXYXwchย']3rd}=ย#iZ\nย@c}ร_
ร"c\nย#รยuย]|r7_@lmCo0ยบ@mq_รคbรก1ยกยยยชnยzยย5รฟรยก
ยย|ยCยTยฆยฑยยรยฐยคYยก.ร ยก.ยยฃยzยขยรรบรคยค ยฎยยกeรยฑยยยข7ยขย'ย5รFยกยรยณยกCยยร5l!phย7ZCยยu[#u[W0h'qn_
รฝFh>u[ร'_@Wx ย c
XYuYxรW0\ยh>uรc=ร:\ยZCยiย_



