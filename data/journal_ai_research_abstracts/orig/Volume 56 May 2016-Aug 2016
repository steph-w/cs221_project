<cite>Franz  Baader, Meghyn  Bienvenu, Carsten  Lutz and Frank  Wolter (2016) "Query and Predicate Emptiness in Ontology-Based Data Access", Volume 56, pages 1-59</cite>
<p class="media"><a href="/media/4866/live-4866-9290-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4866'>doi:10.1613/jair.4866</a></p>
<p>In ontology-based data access (OBDA), database querying is enriched with an ontology that provides domain knowledge and additional vocabulary for query formulation. We identify query emptiness and predicate emptiness as two central reasoning services in this context. Query emptiness asks whether a given query has an empty answer over all databases formulated in a given vocabulary. Predicate emptiness is defined analogously, but quantifies universally over all queries that contain a given predicate. In this paper, we determine the computational complexity of query emptiness and predicate emptiness in the EL, DL-Lite, and ALC-families of description logics, investigate the connection to ontology modules, and perform a practical case study to evaluate the new reasoning services.</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="Query and Predicate Emptiness in Ontology-Based Data Access">
<meta name="citation_author" content="Baader, Franz">
<meta name="citation_author" content="Bienvenu, Meghyn">
<meta name="citation_author" content="Lutz, Carsten">
<meta name="citation_author" content="Wolter, Frank">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1">
<meta name="citation_lastpage" content="59">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4866/live-4866-9290-jair.pdf">

<cite>Nasrin  Taghizadeh and Hesham  Faili (2016) "Automatic Wordnet Development for Low-Resource Languages using Cross-Lingual WSD", Volume 56, pages 61-87</cite>
<p class="media"><a href="/media/4968/live-4968-9312-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4968'>doi:10.1613/jair.4968</a></p>
<p>&#8206;Wordnets are an effective resource for natural language processing and information retrieval&#8206;, &#8206;especially for semantic processing and meaning related tasks&#8206;. &#8206;So far&#8206;, &#8206;wordnets have been constructed for many languages&#8206;. &#8206;However&#8206;, &#8206;the automatic development of wordnets for low-resource languages has not been well studied&#8206;. &#8206;In this paper&#8206;, &#8206;an Expectation-Maximization algorithm is used to create high quality and large scale wordnets for poor-resource languages&#8206;. &#8206;The proposed method benefits from possessing cross-lingual word sense disambiguation and develops a wordnet by only using a bi-lingual dictionary and a mono-lingual corpus&#8206;. &#8206;The proposed method has been executed with Persian language and the resulting wordnet has been evaluated through several experiments&#8206;. &#8206;The results show that the induced wordnet has a precision score of 90% and a recall score of 35%&#8206;.</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="Automatic Wordnet Development for Low-Resource Languages using Cross-Lingual WSD">
<meta name="citation_author" content="Taghizadeh, Nasrin">
<meta name="citation_author" content="Faili, Hesham">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="61">
<meta name="citation_lastpage" content="87">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4968/live-4968-9312-jair.pdf">

<cite>Daniel  Damir Harabor, Alban  Grastien, Dindar  &#214;z and Vural  Aksakalli (2016) "Optimal Any-Angle Pathfinding In Practice", Volume 56, pages 89-118</cite>
<p class="media"><a href="/media/5007/live-5007-9321-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5007'>doi:10.1613/jair.5007</a></p>
<p>Any-angle pathfinding is a fundamental problem in robotics and computer games. The goal is to find a shortest path between a pair of points on a grid map such that the path is not artificially constrained to the points of the grid. Prior research has focused on approximate online solutions. A number of exact methods exist but they all require super-linear space and pre-processing time.  In this study, we describe Anya: a new and optimal any-angle pathfinding algorithm. Where other works find approximate any-angle paths by searching over individual points from the grid, Anya finds optimal paths by searching over sets of states represented as intervals. Each interval is identified on-the-fly. From each interval Anya selects a single representative point that it uses to compute an admissible cost estimate for the entire set.  Anya always returns an optimal path if one exists.  Moreover it does so without any offline pre-processing or the introduction of additional memory overheads.  In a range of empirical comparisons we show that Anya is competitive with several recent (sub-optimal) online and pre-processing based techniques and is up to an order of magnitude faster than the most common benchmark algorithm, a grid-based implementation of A*.<br />
</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="Optimal Any-Angle Pathfinding In Practice">
<meta name="citation_author" content="Harabor, Daniel Damir">
<meta name="citation_author" content="Grastien, Alban">
<meta name="citation_author" content="&#214;z, Dindar">
<meta name="citation_author" content="Aksakalli, Vural">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="89">
<meta name="citation_lastpage" content="118">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5007/live-5007-9321-jair.pdf">

<cite>Javad  Azimi, Xiaoli  Fern and Alan  Fern (2016) "Budgeted Optimization with Constrained Experiments", Volume 56, pages 119-152</cite>
<p class="media"><a href="/media/4896/live-4896-9331-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4896'>doi:10.1613/jair.4896</a></p>
<p>Motivated by a real-world problem, we study a novel budgeted optimization problem where the goal is to optimize an unknown function f(.) given a budget by requesting a sequence of samples from the function. In our setting, however, evaluating the function at precisely specified points is not practically possible due to prohibitive costs. Instead, we can only request constrained experiments. A constrained experiment, denoted by Q, specifies a subset of the input space for the experimenter to sample the function from. The outcome of Q includes a sampled experiment x, and its function output f(x). Importantly, as the constraints of Q become looser, the cost of fulfilling the request decreases, but the uncertainty about the location x increases. Our goal is to manage this trade-off by selecting a set of constrained experiments that best optimize f(.) within the budget.  We study this problem in two different settings, the non-sequential (or batch) setting where a set of constrained experiments is selected at once, and the sequential setting where experiments are selected one at a time. We evaluate our proposed methods for both settings using synthetic and real functions. The experimental results demonstrate the efficacy of the proposed methods.</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="Budgeted Optimization with Constrained Experiments">
<meta name="citation_author" content="Azimi, Javad">
<meta name="citation_author" content="Fern, Xiaoli">
<meta name="citation_author" content="Fern, Alan">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="119">
<meta name="citation_lastpage" content="152">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4896/live-4896-9331-jair.pdf">

<cite>Kenji  Kawaguchi, Yu  Maruyama and Xiaoyu  Zheng (2016) "Global Continuous Optimization with Error Bound and Fast Convergence", Volume 56, pages 153-195</cite>
<p class="media"><a href="/media/4742/live-4742-9353-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4742'>doi:10.1613/jair.4742</a></p>
<p>This paper considers global optimization with a black-box unknown objective function that can be non-convex and non-differentiable. Such a difficult optimization problem arises in many real-world applications, such as parameter tuning in machine learning, engineering design problem, and planning with a complex physics simulator. This paper proposes a new global optimization algorithm, called Locally Oriented Global Optimization (LOGO), to aim for both fast convergence in practice and finite-time error bound in theory. The advantage and usage of the new algorithm are illustrated via theoretical analysis and an experiment conducted with 11 benchmark test functions. Further, we modify the LOGO algorithm to specifically solve a planning problem via policy search with continuous state/action space and long time horizon while maintaining its finite-time error bound. We apply the proposed planning method to accident management of a nuclear power plant. The result of the application study demonstrates the practical utility of our method.</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="Global Continuous Optimization with Error Bound and Fast Convergence">
<meta name="citation_author" content="Kawaguchi, Kenji">
<meta name="citation_author" content="Maruyama, Yu">
<meta name="citation_author" content="Zheng, Xiaoyu">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="153">
<meta name="citation_lastpage" content="195">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4742/live-4742-9353-jair.pdf">

<cite>Diana  Grooters and Henry  Prakken (2016) "Two Aspects of Relevance in Structured Argumentation: Minimality and Paraconsistency", Volume 56, pages 197-245</cite>
<p class="media"><a href="/media/5058/live-5058-9355-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5058'>doi:10.1613/jair.5058</a></p>
<p>This paper studies two issues concerning relevance in structured argumentation in the context of the ASPIC+ framework, arising from the combined use of strict and defeasible inference rules. One issue arises if the strict inference rules correspond to classical logic.  A longstanding problem is how the trivialising effect of the classical Ex Falso principle can be avoided while satisfying consistency and closure postulates. In this paper, this problem is solved by disallowing chaining of strict rules, resulting in a variant of the ASPIC+ framework called ASPIC*, and then disallowing the application of strict rules to inconsistent sets of formulas. Thus in effect Rescher & Manor's paraconsistent notion of weak consequence is embedded in ASPIC*.<br />
<br />
Another issue is minimality of arguments. If arguments can apply defeasible inference rules, then they cannot be required to have subset-minimal premises, since defeasible rules based on more information may well make an argument stronger. In this paper instead minimality is required of applications of strict rules throughout an argument. It is shown that under some plausible assumptions this does not affect the set of conclusions.  In addition, circular arguments are in the new ASPIC* framework excluded in a way that satisfies closure and consistency postulates and that generates finitary argumentation frameworks if the knowledge base and set of defeasible rules are finite. For  the latter result the exclusion of chaining of strict rules is essential.<br />
<br />
Finally, the combined results of this paper are shown to be a proper extension of classical-logic argumentation with preferences and defeasible rules.</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="Two Aspects of Relevance in Structured Argumentation: Minimality and Paraconsistency">
<meta name="citation_author" content="Grooters, Diana">
<meta name="citation_author" content="Prakken, Henry">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="197">
<meta name="citation_lastpage" content="245">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5058/live-5058-9355-jair.pdf">

<cite>Zenglin  Xu,  Shandian  Zhe, Yuan  Qi and Peng  Yu (2016) "Association Discovery and Diagnosis of Alzheimer’s Disease with Bayesian Multiview Learning", Volume 56, pages 247-268</cite>
<p class="media"><a href="/media/4956/live-4956-9370-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4956'>doi:10.1613/jair.4956</a></p>
<p>The analysis and diagnosis of Alzheimer’s disease (AD) can be based on genetic variations, e.g., single nucleotide polymorphisms (SNPs) and phenotypic traits, e.g., Magnetic Resonance Imaging (MRI) features. We consider two important and related tasks: i) to select genetic and phenotypical markers for AD diagnosis and ii) to identify associations between genetic and phenotypical data. While previous studies treat these two tasks separately, they are tightly coupled because underlying associations between genetic variations and phenotypical features contain the biological basis for a disease. Here we present a new sparse Bayesian approach for joint association study and disease diagnosis. In this approach, common latent features are extracted from different data sources based on sparse projection matrices and used to predict multiple disease severity levels; in return, the disease status can guide the discovery of relationships between data sources. The sparse projection matrices not only reveal interactions between data sources but also select groups of biomarkers related to the disease. Moreover, to take advantage of the linkage disequilibrium (LD) measuring the non-random association of alleles, we incorporate a graph Laplacian type of prior in the model. To learn the model from data, we develop an efficient variational inference algorithm. Analysis on an imaging genetics dataset for the study of Alzheimer’s Disease (AD) indicates that our model identifies biologically meaningful associations between genetic variations and MRI features, and achieves significantly higher accuracy for predicting ordinal AD stages than the competing methods.</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="Association Discovery and Diagnosis of Alzheimer’s Disease with Bayesian Multiview Learning">
<meta name="citation_author" content="Xu, Zenglin">
<meta name="citation_author" content="Zhe,  Shandian">
<meta name="citation_author" content="Qi, Yuan">
<meta name="citation_author" content="Yu, Peng">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="247">
<meta name="citation_lastpage" content="268">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4956/live-4956-9370-jair.pdf">

<cite>Maximilian  Fickert, Joerg  Hoffmann and Marcel  Steinmetz (2016) "Combining the Delete Relaxation with Critical-Path Heuristics: A Direct Characterization", Volume 56, pages 269-327</cite>
<p class="media"><a href="/media/5057/live-5057-9386-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/5057/live-5057-9387-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5057'>doi:10.1613/jair.5057</a></p>
<p>Recent work has shown how to improve delete relaxation heuristics by computing relaxed plans, i.e., the hFF heuristic, in a compiled planning task PiC which represents a given set C of fact conjunctions explicitly. While this compilation view of such partial delete relaxation is simple and elegant, its meaning with respect to the original planning task is opaque, and the size of PiC grows exponentially in |C|. We herein provide a direct characterization, without compilation, making explicit how the approach arises from a combination of the delete-relaxation with critical-path heuristics. Designing equations characterizing a novel view on h+ on the one hand, and a generalized version hC of hm on the other hand, we show that h+(PiC) can be characterized in terms of a combined hcplus equation. This naturally generalizes the standard delete-relaxation framework: understanding that framework as a relaxation over singleton facts as atomic subgoals, one can refine the relaxation by using the conjunctions C as atomic subgoals instead. Thanks to this explicit view, we identify the precise source of complexity in hFF(PiC), namely maximization of sets of supported atomic subgoals during relaxed plan extraction, which is easy for singleton-fact subgoals but is NP-complete in the general case. Approximating that problem greedily, we obtain a polynomial-time hCFF version of hFF(PiC), superseding the PiC compilation, and superseding the modified PiCce compilation which achieves the same complexity reduction but at an information loss. Experiments on IPC benchmarks show that these theoretical advantages can translate into empirical ones.</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="Combining the Delete Relaxation with Critical-Path Heuristics: A Direct Characterization">
<meta name="citation_author" content="Fickert, Maximilian">
<meta name="citation_author" content="Hoffmann, Joerg">
<meta name="citation_author" content="Steinmetz, Marcel">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="269">
<meta name="citation_lastpage" content="327">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5057/live-5057-9386-jair.pdf">

<cite>Zhiqiang  Zhuang, Zhe  Wang, Kewen  Wang and Guilin  Qi (2016) "DL-Lite Contraction and Revision", Volume 56, pages 329-378</cite>
<p class="media"><a href="/media/5050/live-5050-9389-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5050'>doi:10.1613/jair.5050</a></p>
<p>Two essential tasks in managing description logic knowledge bases are eliminating problematic axioms and incorporating newly formed ones. Such elimination and incorporation are formalised as the operations of contraction and revision in belief change. In this paper, we deal with contraction and revision for the DL-Lite family through a model-theoretic approach. Standard description logic semantics yields an infinite number of models for DL-Lite knowledge bases, thus it is difficult to develop algorithms for contraction and revision that involve DL models.  The key to our approach is the introduction of an alternative semantics called type semantics which can replace the standard semantics in characterising the standard inference tasks of DL-Lite. Type semantics has several advantages over the standard one. It is more succinct and importantly, with a finite signature, the semantics always yields a finite number of models. We then define model-based contraction and revision functions for DL-Lite knowledge bases under type semantics and provide representation theorems for them. Finally, the finiteness and succinctness of type semantics allow us to develop tractable algorithms for instantiating the functions.</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="DL-Lite Contraction and Revision">
<meta name="citation_author" content="Zhuang, Zhiqiang">
<meta name="citation_author" content="Wang, Zhe">
<meta name="citation_author" content="Wang, Kewen">
<meta name="citation_author" content="Qi, Guilin">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="329">
<meta name="citation_lastpage" content="378">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5050/live-5050-9389-jair.pdf">

<cite>Petr  Savick&#253; and Petr  Ku&#269;era (2016) "Generating Models of a Matched Formula With a Polynomial Delay", Volume 56, pages 379-402</cite>
<p class="media"><a href="/media/4989/live-4989-9392-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4989/live-4989-9391-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4989'>doi:10.1613/jair.4989</a></p>
<p>A matched formula is a CNF formula whose incidence graph admits a matching which matches a distinct variable to every clause. Such a formula is always satisfiable. Matched formulas are used, for example, in the area of parametrized complexity. We prove that the problem of counting the number of the models (satisfying assignments) of a matched formula is #P-complete. On the other hand, we define a class of formulas generalizing the matched formulas and prove that for a formula in this class one can choose in polynomial time a variable suitable for splitting the tree for the search of the models of the formula. As a consequence, the models of a formula from this class, in particular of any matched formula, can be generated sequentially with a delay polynomial in the size of the input. On the other hand, we prove that this task cannot be performed efficiently for linearly satisfiable formulas, which is a generalization of matched formulas containing the class considered above. <br />
</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="Generating Models of a Matched Formula With a Polynomial Delay">
<meta name="citation_author" content="Savick&#253;, Petr">
<meta name="citation_author" content="Ku&#269;era, Petr">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="379">
<meta name="citation_lastpage" content="402">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4989/live-4989-9392-jair.pdf">

<cite>Xiaowang  Zhang, Jan  Van den Bussche and Fran&#231;ois  Picalausa (2016) "On the Satisfiability Problem for SPARQL Patterns", Volume 56, pages 403-428</cite>
<p class="media"><a href="/media/5028/live-5028-9408-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5028'>doi:10.1613/jair.5028</a></p>
<p>The satisfiability problem for SPARQL 1.0 patterns is undecidable in general, since the relational algebra can be emulated using such patterns. The goal of this paper is to delineate the boundary of decidability of satisfiability in terms of the constraints allowed in filter conditions. The classes of constraints considered are bound-constraints, negated bound- constraints, equalities, nonequalities, constant-equalities, and constant-nonequalities. The main result of the paper can be summarized by saying that, as soon as inconsistent filter conditions can be formed, satisfiability is undecidable. The key insight in each case is to find a way to emulate the set difference operation. Undecidability can then be obtained from a known undecidability result for the algebra of binary relations with union, composition, and set difference. When no inconsistent filter conditions can be formed, satisfiability is decidable by syntactic checks on bound variables and on the use of literals. Although the problem is shown to be NP-complete, it is experimentally shown that the checks can be implemented efficiently in practice. The paper also points out that satisfiability for the so-called &lsquo;well-designed&rsquo;  patterns can be decided by a check on bound variables and a check for inconsistent filter conditions.</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="On the Satisfiability Problem for SPARQL Patterns">
<meta name="citation_author" content="Zhang, Xiaowang">
<meta name="citation_author" content="Van den Bussche, Jan">
<meta name="citation_author" content="Picalausa, Fran&#231;ois">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="403">
<meta name="citation_lastpage" content="428">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5028/live-5028-9408-jair.pdf">

<cite>Xujin  Chen, Xiaodong  Hu, Tie-Yan  Liu, Weidong  Ma, Tao  Qin, Pingzhong  Tang, Changjun  Wang and Bo  Zheng (2016) "Efficient Mechanism Design for Online Scheduling", Volume 56, pages 429-461</cite>
<p class="media"><a href="/media/5100/live-5100-9425-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5100'>doi:10.1613/jair.5100</a></p>
<p>This paper concerns the mechanism design for online scheduling in a strategic setting. In this setting, each job is owned by a self-interested agent who may misreport the release time, deadline, length, and value of her job, while we need to determine not only the schedule of the jobs, but also the payment of each agent. We focus on the design of incentive compatible (IC) mechanisms, and study the maximization of social welfare (i.e., the aggregated value of completed jobs) by competitive analysis. We first derive two lower bounds on the competitive ratio of any deterministic IC mechanism to characterize the landscape of our research. We then propose a deterministic IC mechanism and show that such a simple mechanism works very well for both the preemption-restart model and the preemption-resume model. We show the mechanism can achieve the optimal competitive ratio of 5 for equal-length jobs and a near optimal competitive ratio (within a constant factor) for unequal-length jobs.<br />
</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="Efficient Mechanism Design for Online Scheduling">
<meta name="citation_author" content="Chen, Xujin">
<meta name="citation_author" content="Hu, Xiaodong">
<meta name="citation_author" content="Liu, Tie-Yan">
<meta name="citation_author" content="Ma, Weidong">
<meta name="citation_author" content="Qin, Tao">
<meta name="citation_author" content="Tang, Pingzhong">
<meta name="citation_author" content="Wang, Changjun">
<meta name="citation_author" content="Zheng, Bo">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="429">
<meta name="citation_lastpage" content="461">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5100/live-5100-9425-jair.pdf">

<cite>Thomas  Eiter, Michael  Fink and Daria  Stepanova (2016) "Computing Repairs of Inconsistent DL-Programs over EL Ontologies", Volume 56, pages 463-515</cite>
<p class="media"><a href="/media/5047/live-5047-9427-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/5047/live-5047-9428-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5047'>doi:10.1613/jair.5047</a></p>
<p>Description Logic (DL) ontologies and non-monotonic rules are two prominent Knowledge Representation (KR) formalisms with complementary features that are essential for various applications. Nonmonotonic Description Logic (DL) programs combine these formalisms thus providing support for rule-based reasoning on top of DL ontologies using a well-defined query interface represented by so-called DL-atoms. Unfortunately, interaction of the rules and the ontology may incur inconsistencies such that a DL-program lacks answer sets (i.e., models), and thus yields no information. This issue is addressed by recently defined repair answer sets, for computing which an effective practical algorithm was proposed for DL-Lite A ontologies that reduces a repair computation to constraint matching based on so-called support sets. However, the algorithm exploits particular features of DL-Lite A and can not be readily applied to repairing DL-programs over other prominent DLs like EL. compared to DL-Lite A , in EL support sets may neither be small nor only few support sets might exist, and completeness of the algorithm may need to be given up when the support information is bounded. We thus provide an approach for computing repairs for DL-programs over EL ontologies based on partial (incomplete) support families. The latter are constructed using datalog query rewriting techniques as well as ontology approximation based on logical difference between EL-terminologies. We show how the maximal size and number of support sets for a given DL-atom can be estimated by analyzing the properties of a support hypergraph, which characterizes a relevant set of TBox axioms needed for query derivation. We present a declarative implementation of the repair approach and experimentally evaluate it on a set of benchmark problems; the promising results witness practical feasibility of our repair approach. </p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="Computing Repairs of Inconsistent DL-Programs over EL Ontologies">
<meta name="citation_author" content="Eiter, Thomas">
<meta name="citation_author" content="Fink, Michael">
<meta name="citation_author" content="Stepanova, Daria">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="463">
<meta name="citation_lastpage" content="515">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5047/live-5047-9427-jair.pdf">

<cite>Matteo  Venanzi, John  Guiver, Pushmeet  Kohli and Nicholas  R. Jennings (2016) "Time-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems", Volume 56, pages 517-545</cite>
<p class="media"><a href="/media/5175/live-5175-9434-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5175'>doi:10.1613/jair.5175</a></p>
<p>Many aspects of the design of efficient crowdsourcing processes, such as defining worker’s bonuses, fair prices and time limits of the tasks, involve knowledge of the likely duration of the task at hand. In this work we introduce a new time–sensitive Bayesian aggregation method that simultaneously estimates a task’s duration and obtains reliable aggregations of crowdsourced judgments. Our method, called BCCTime, uses latent variables to represent the uncertainty about the workers’ completion time, the tasks’ duration and the workers’ accuracy. To relate the quality of a judgment to the time a worker spends on a task, our model assumes that each task is completed within a latent time window within which all workers with a propensity to genuinely attempt the labelling task (i.e., no spammers) are expected to submit their judgments. In contrast, workers with a lower propensity to valid labelling, such as spammers, bots or lazy labellers, are assumed to perform tasks considerably faster or slower than the time required by normal workers. Specifically, we use efficient message-passing Bayesian inference to learn approximate posterior probabilities of (i) the confusion matrix of each worker, (ii) the propensity to valid labelling of each worker, (iii) the unbiased duration of each task and (iv) the true label of each task. Using two real- world public datasets for entity linking tasks, we show that BCCTime produces up to 11% more accurate classifications and up to 100% more informative estimates of a task’s duration compared to state–of–the–art methods.</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="Time-Sensitive Bayesian Information Aggregation for Crowdsourcing Systems">
<meta name="citation_author" content="Venanzi, Matteo">
<meta name="citation_author" content="Guiver, John">
<meta name="citation_author" content="Kohli, Pushmeet">
<meta name="citation_author" content="Jennings, Nicholas R.">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="517">
<meta name="citation_lastpage" content="545">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5175/live-5175-9434-jair.pdf">

<cite>Carlos  Hern&#225;ndez, Jorge  A. Baier and Roberto  As&#237;n (2016) "Time-Bounded Best-First Search for Reversible and Non-reversible Search Graphs", Volume 56, pages 547-571</cite>
<p class="media"><a href="/media/5073/live-5073-9453-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5073'>doi:10.1613/jair.5073</a></p>
<p>Time-Bounded A* is a real-time, single-agent, deterministic search algorithm that expands states of a graph in the same order as A* does, but that unlike A* interleaves search and action execution.  Known to outperform state-of-the-art real-time search algorithms based on Korf's Learning Real-Time A* (LRTA*) in some benchmarks, it has not been studied in detail and is sometimes not considered as a ``true'' real-time search algorithm since it fails in non-reversible problems even it the goal is still reachable from the current state.  In this paper we propose and study Time-Bounded Best-First Search (TB(BFS)) a straightforward generalization of the time-bounded approach to any best-first search algorithm. Furthermore, we propose Restarting Time-Bounded Weighted A* (TB_R(WA*)), an algorithm that deals more adequately with non-reversible search graphs, eliminating  ``backtracking moves'' and incorporating search restarts and heuristic learning. In non-reversible problems we prove that TB(BFS) terminates and we deduce cost bounds for the solutions returned by Time-Bounded Weighted A* (TB(WA*)), an instance of TB(BFS). Furthermore, we prove TB_R(WA*), under reasonable conditions, terminates.  We evaluate TB(WA) in both grid pathfinding and the 15-puzzle. In addition, we evaluate TB_R(WA*) on the racetrack problem. We compare our algorithms to LSS-LRTWA*, a variant of LRTA* that can exploit lookahead search and a weighted heuristic. A general observation is that the performance of both TB(WA*) and TB_R(WA*) improves as the weight parameter is increased. In addition, our time-bounded algorithms almost always outperform LSS-LRTWA* by a significant margin.<br />
</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="Time-Bounded Best-First Search for Reversible and Non-reversible Search Graphs">
<meta name="citation_author" content="Hern&#225;ndez, Carlos">
<meta name="citation_author" content="Baier, Jorge A.">
<meta name="citation_author" content="As&#237;n, Roberto">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="547">
<meta name="citation_lastpage" content="571">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5073/live-5073-9453-jair.pdf">

<cite>Haris  Aziz, Casey  Cahan, Charles  Gretton, Philip  Kilby, Nicholas  Mattei and Toby  Walsh (2016) "A Study of Proxies for Shapley Allocations of Transport Costs", Volume 56, pages 573-611</cite>
<p class="media"><a href="/media/5021/live-5021-9468-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5021'>doi:10.1613/jair.5021</a></p>
<p>We survey existing rules of thumb, propose novel methods,  and comprehensively evaluate a number of solutions to the problem of calculating the cost to serve each location in a single-vehicle transport setting. Cost to serve analysis has applications both strategically and operationally in transportation settings. The problem is formally modeled as the traveling salesperson game (TSG), a cooperative transferable utility game in which agents correspond to locations in a traveling salesperson problem (TSP). The total cost to serve all locations in the TSP is the length of an optimal tour. An allocation divides the total cost among individual locations, thus providing the cost to serve each of them. As one of the most important normative division schemes in cooperative games, the Shapley value gives a principled and fair allocation for a broad variety of games including the TSG. We consider a number of direct and sampling-based procedures for calculating the Shapley value, and  prove that approximating the Shapley value of the TSG within a constant factor is NP-hard. Treating the Shapley value as an ideal baseline allocation, we survey six proxies for it that are each relatively easy to compute. Some of these proxies are rules of thumb and some are procedures international delivery companies use(d) as cost allocation methods. We perform an experimental evaluation using synthetic Euclidean games as well as games derived from real-world tours calculated for scenarios involving fast-moving goods; where deliveries are made on a road network every day. We explore several computationally tractable allocation techniques that are good proxies for the Shapley value in problem instances of a size and complexity that is commercially relevant.</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="A Study of Proxies for Shapley Allocations of Transport Costs">
<meta name="citation_author" content="Aziz, Haris">
<meta name="citation_author" content="Cahan, Casey">
<meta name="citation_author" content="Gretton, Charles">
<meta name="citation_author" content="Kilby, Philip">
<meta name="citation_author" content="Mattei, Nicholas">
<meta name="citation_author" content="Walsh, Toby">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="573">
<meta name="citation_lastpage" content="611">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5021/live-5021-9468-jair.pdf">

<cite>Cristhian  Ariel D. Deagustini, Maria Vanina  Martinez, Marcelo  A. Falappa and Guillermo  R. Simari (2016) "Datalog+- Ontology Consolidation", Volume 56, pages 613-656</cite>
<p class="media"><a href="/media/5131/live-5131-9495-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5131'>doi:10.1613/jair.5131</a></p>
<p>Knowledge bases in the form of ontologies are receiving increasing attention as they allow to clearly represent both the available knowledge, which includes the knowledge in itself and the constraints imposed to it by the domain or the users. In particular, Datalog&#177; ontologies are attractive because of their property of decidability and the possibility of dealing with the massive amounts of data in real world environments; however, as it is the case with many other ontological languages, their application in collaborative environments often lead to inconsistency related issues. In this paper we introduce the notion of incoherence regarding Datalog&#177; ontologies, in terms of satisfiability of sets of constraints, and show how under specific conditions incoherence leads to inconsistent Datalog&#177; ontologies. The main contribution of this work is a novel approach to restore both consistency and coherence in Datalog&#177; ontologies. The proposed approach is based on kernel contraction and restoration is performed by the application of incision functions that select formulas to delete. Nevertheless, instead of working  over minimal incoherent/inconsistent sets encountered in the ontologies, our operators produce incisions over non-minimal structures called clusters. We present a construction for consolidation operators, along with the properties expected to be satisfied by them. Finally, we establish the relation between the construction and the properties by means of a representation theorem. Although this proposal is presented for Datalog&#177; ontologies consolidation, these operators can be applied to other types of ontological languages, such as Description Logics, making  them apt to be used in collaborative environments like the Semantic Web.</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="Datalog+- Ontology Consolidation">
<meta name="citation_author" content="Deagustini, Cristhian Ariel D.">
<meta name="citation_author" content="Martinez, Maria Vanina">
<meta name="citation_author" content="Falappa, Marcelo A.">
<meta name="citation_author" content="Simari, Guillermo R.">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="613">
<meta name="citation_lastpage" content="656">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5131/live-5131-9495-jair.pdf">

<cite>Isabel  Cenamor, Tom&#225;s  de la Rosa and Fernando  Fern&#225;ndez (2016) "The IBaCoP Planning System: Instance-Based Configured Portfolios", Volume 56, pages 657-691</cite>
<p class="media"><a href="/media/5080/live-5080-9497-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/5080/live-5080-9498-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5080'>doi:10.1613/jair.5080</a></p>
<p>Sequential planning portfolios are very powerful in exploiting the complementary strength of different automated planners. The main challenge of a portfolio planner is to define which base planners to run, to assign the running time for each planner and to decide in what order they should be carried out to optimize a planning metric. Portfolio configurations are usually derived empirically from training benchmarks and remain fixed for an evaluation phase. In this work, we create a per-instance configurable portfolio, which is able to adapt itself to every planning task. The proposed system pre-selects a group of candidate planners using a Pareto-dominance filtering approach and then it decides which planners to include and the time assigned according to predictive models. These models estimate whether a base planner will be able to solve the given problem and, if so, how long it will take. We define different portfolio strategies to combine the knowledge generated by the models. The experimental evaluation shows that the resulting portfolios provide an improvement when compared with non-informed strategies. One of the proposed portfolios was the winner of the Sequential Satisficing Track of the International Planning Competition held in 2014.</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="The IBaCoP Planning System: Instance-Based Configured Portfolios">
<meta name="citation_author" content="Cenamor, Isabel">
<meta name="citation_author" content="de la Rosa, Tom&#225;s">
<meta name="citation_author" content="Fern&#225;ndez, Fernando">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="657">
<meta name="citation_lastpage" content="691">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5080/live-5080-9497-jair.pdf">

<cite>Heshan  Du and Natasha  Alechina (2016) "Qualitative Spatial Logics for Buffered Geometries", Volume 56, pages 693-745</cite>
<p class="media"><a href="/media/5140/live-5140-9501-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.5140'>doi:10.1613/jair.5140</a></p>
<p>This paper describes a series of new qualitative spatial logics for checking consistency of sameAs and partOf matches between spatial objects from different geospatial datasets, especially from crowd-sourced datasets. Since geometries in crowd-sourced data are usually not very accurate or precise, we buffer geometries by a margin of error or a level of tolerance, and define spatial relations for buffered geometries. The spatial logics formalize the notions of `buffered equal' (intuitively corresponding to `possibly sameAs'), `buffered part of' (`possibly partOf'), `near' (`possibly connected') and `far' (`definitely disconnected'). A sound and complete axiomatisation of each logic is provided with respect to models based on metric spaces. For each of the logics, the satisfiability problem is shown to be NP-complete. Finally, we briefly describe how the logics are used in a system for generating and debugging matches between spatial objects, and report positive experimental evaluation results for the system.</p>
<a href="/vol/vol56.html">Click here to return to Volume 56 contents list</a>
<meta name="citation_title" content="Qualitative Spatial Logics for Buffered Geometries">
<meta name="citation_author" content="Du, Heshan">
<meta name="citation_author" content="Alechina, Natasha">
<meta name="citation_publication_date" content="2016">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="693">
<meta name="citation_lastpage" content="745">
<meta name="citation_volume" content="56">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/5140/live-5140-9501-jair.pdf">
