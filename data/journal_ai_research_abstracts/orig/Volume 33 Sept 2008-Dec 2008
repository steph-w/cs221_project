<cite>S.  Esmeir and S.  Markovitch (2008) "Anytime Induction of Low-cost, Low-error Classifiers: a Sampling-based Approach", Volume 33, pages 1-31</cite>
<p class="media"><a href="/media/2602/live-2602-4041-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2602/live-2602-4040-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2602'>doi:10.1613/jair.2602</a></p>
<p>Machine learning techniques are gaining prevalence in the production of a wide range of classifiers for complex real-world applications with nonuniform testing and misclassification costs. The increasing complexity of these applications poses a real challenge to resource management during learning and classification. In this work we introduce ACT (anytime cost-sensitive tree learner), a novel framework for operating in such complex environments. ACT is an anytime algorithm that allows learning time to be increased in return for lower classification costs. It builds a tree top-down and exploits additional time resources to obtain better estimations for the utility of the different candidate splits. Using sampling techniques, ACT approximates the cost of the subtree under each candidate split and favors the one with a minimal cost. As a stochastic algorithm, ACT is expected to be able to escape local minima, into which greedy methods may be trapped. Experiments with a variety of datasets were conducted to compare ACT to the state-of-the-art cost-sensitive tree learners. The results show that for the majority of domains ACT produces significantly less costly trees. ACT also exhibits good anytime behavior with diminishing returns.</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="Anytime Induction of Low-cost, Low-error Classifiers: a Sampling-based Approach">
<meta name="citation_author" content="Esmeir, S.">
<meta name="citation_author" content="Markovitch, S.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1">
<meta name="citation_lastpage" content="31">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2602/live-2602-4041-jair.pdf">

<cite>B.  Lubin, A.  I. Juda, R.  Cavallo, S.  Lahaie, J.  Shneidman and D.  C. Parkes (2008) "ICE: An Expressive Iterative Combinatorial Exchange", Volume 33, pages 33-77</cite>
<p class="media"><a href="/media/2440/live-2440-4051-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2440/live-2440-4050-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2440'>doi:10.1613/jair.2440</a></p>
<p>We present the design and analysis of the first fully expressive, iterative combinatorial exchange (ICE). The exchange incorporates a tree-based bidding language (TBBL) that is concise and expressive for CEs. Bidders specify lower and upper bounds in TBBL on their value for different trades and refine these bounds across rounds. These bounds allow price discovery and useful preference elicitation in early rounds, and allow termination with an efficient trade despite partial information on bidder valuations. All computation in the exchange is carefully optimized to exploit the structure of the bid-trees and to avoid enumerating trades. A proxied interpretation of a revealed-preference activity rule, coupled with simple linear prices, ensures progress across rounds. The exchange is fully implemented, and we give results demonstrating several aspects of its scalability and economic properties with simulated bidding strategies.</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="ICE: An Expressive Iterative Combinatorial Exchange">
<meta name="citation_author" content="Lubin, B.">
<meta name="citation_author" content="Juda, A. I.">
<meta name="citation_author" content="Cavallo, R.">
<meta name="citation_author" content="Lahaie, S.">
<meta name="citation_author" content="Shneidman, J.">
<meta name="citation_author" content="Parkes, D. C.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="33">
<meta name="citation_lastpage" content="77">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2440/live-2440-4051-jair.pdf">

<cite>D.  Martinez, O.  Lopez de Lacalle and E.  Agirre (2008) "On the Use of Automatically Acquired Examples   for All-Nouns Word Sense Disambiguation ", Volume 33, pages 79-107</cite>
<p class="media"><a href="/media/2395/live-2395-4056-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2395/live-2395-4055-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2395'>doi:10.1613/jair.2395</a></p>
<p>This article focuses on Word Sense Disambiguation (WSD), which is a Natural Language Processing task that is thought to be important for many Language Technology applications, such as Information Retrieval, Information Extraction, or Machine Translation. One of the main issues preventing the deployment of WSD technology is the lack of training examples for Machine Learning systems, also known as the Knowledge Acquisition Bottleneck. A method which has been shown to work for small samples of words is the automatic acquisition of examples. We have previously shown that one of the most promising example acquisition methods scales up and produces a freely available database of 150 million examples from Web snippets for all polysemous nouns in WordNet. This paper focuses on the issues that arise when using those examples, all alone or in addition to manually tagged examples, to train a supervised WSD system for all nouns. The extensive evaluation on both lexical-sample and all-words Senseval benchmarks shows that we are able to improve over commonly used baselines and to achieve top-rank performance. The good use of the prior distributions from the senses proved to be a crucial factor.</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="On the Use of Automatically Acquired Examples   for All-Nouns Word Sense Disambiguation ">
<meta name="citation_author" content="Martinez, D.">
<meta name="citation_author" content="Lopez de Lacalle, O.">
<meta name="citation_author" content="Agirre, E.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="79">
<meta name="citation_lastpage" content="107">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2395/live-2395-4056-jair.pdf">

<cite>Y.  Gal and A.  Pfeffer (2008) "Networks of Influence Diagrams: A Formalism for Representing  Agents'  Beliefs and  Decision-Making Processes", Volume 33, pages 109-147</cite>
<p class="media"><a href="/media/2503/live-2503-4064-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2503/live-2503-4063-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2503'>doi:10.1613/jair.2503</a></p>
<p>This paper presents Networks of Influence Diagrams (NID), a compact, natural and highly expressive language for reasoning about agents' beliefs and decision-making processes.  NIDs are graphical structures in which agents' mental models are represented as nodes in a network; a mental model for an agent may itself use descriptions of the mental models of other agents. NIDs are demonstrated by examples, showing how they can be used to describe conflicting and cyclic belief structures, and certain forms of bounded rationality.  In an opponent modeling domain, NIDs were able to outperform other computational agents whose strategies were not known in advance.  NIDs are equivalent in representation to Bayesian games  but they are more compact and structured than this formalism. In particular, the equilibrium definition for NIDs makes an explicit distinction between agents' optimal strategies, and how they actually behave in reality.</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="Networks of Influence Diagrams: A Formalism for Representing  Agents'  Beliefs and  Decision-Making Processes">
<meta name="citation_author" content="Gal, Y.">
<meta name="citation_author" content="Pfeffer, A.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="109">
<meta name="citation_lastpage" content="147">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2503/live-2503-4064-jair.pdf">

<cite>R.  Meir, A.  D.  Procaccia, J.  S.  Rosenschein and Aviv  Zohar (2008) "Complexity of Strategic Behavior in Multi-Winner Elections", Volume 33, pages 149-178</cite>
<p class="media"><a href="/media/2566/live-2566-4071-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2566/live-2566-4070-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2566'>doi:10.1613/jair.2566</a></p>
<p>Although recent years have seen a surge of interest in the computational aspects of social choice, no specific attention has previously been devoted to elections with multiple winners, e.g., elections of an assembly or committee. In this paper, we characterize the worst-case complexity of manipulation and control in the context of four prominent multi-winner voting systems, under different formulations of the strategic agent&#226;€™s goal.</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="Complexity of Strategic Behavior in Multi-Winner Elections">
<meta name="citation_author" content="Meir, R.">
<meta name="citation_author" content="Procaccia, A. D. ">
<meta name="citation_author" content="Rosenschein, J. S. ">
<meta name="citation_author" content="Zohar, Aviv">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="149">
<meta name="citation_lastpage" content="178">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2566/live-2566-4071-jair.pdf">

<cite>T.  De Laet, J.  De Schutter and H.  Bruyninckx (2008) "A Rigorously Bayesian Beam Model and an Adaptive Full Scan Model for Range Finders in Dynamic Environments", Volume 33, pages 179-222</cite>
<p class="media"><a href="/media/2540/live-2540-4104-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2540/live-2540-4105-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2540'>doi:10.1613/jair.2540</a></p>
<p>This paper proposes and experimentally validates a Bayesian network model of a range finder adapted to dynamic environments. All modeling assumptions are rigorously explained, and all model parameters have a physical interpretation. This approach results in a transparent and intuitive model. With respect to the state of the art beam model this paper: (i) proposes a different functional form for the probability of range measurements caused by unmodeled objects, (ii) intuitively explains the discontinuity encountered in te state of the art beam model, and (iii) reduces the number of model parameters, while maintaining the same representational power for experimental data. The proposed beam model is called RBBM, short for Rigorously Bayesian Beam Model. A maximum likelihood and a variational Bayesian estimator (both based on expectation-maximization) are proposed to learn the model parameters.<br />
<br />
Furthermore, the RBBM is extended to a full scan model in two steps: first, to a full scan model for static environments and next, to a full scan model for general, dynamic environments. The full scan model accounts for the dependency between beams and adapts to the local sample density when using a particle filter. In contrast to Gaussian-based state of the art models, the proposed full scan model uses a sample-based approximation. This sample-based approximation enables handling dynamic environments and capturing multi-modality, which occurs even in simple static environments.<br />
</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="A Rigorously Bayesian Beam Model and an Adaptive Full Scan Model for Range Finders in Dynamic Environments">
<meta name="citation_author" content="De Laet, T.">
<meta name="citation_author" content="De Schutter, J.">
<meta name="citation_author" content="Bruyninckx, H.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="179">
<meta name="citation_lastpage" content="222">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2540/live-2540-4104-jair.pdf">

<cite>T.  Grinshpoun and A.  Meisels (2008) "Completeness and Performance Of The APO Algorithm", Volume 33, pages 223-258</cite>
<p class="media"><a href="/media/2611/live-2611-4109-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2611/live-2611-4111-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2611'>doi:10.1613/jair.2611</a></p>
<p>Asynchronous Partial Overlay (APO) is a search algorithm that uses cooperative mediation to solve Distributed Constraint Satisfaction Problems (DisCSPs). The algorithm partitions the search into different subproblems of the DisCSP. The original proof of completeness of the APO algorithm is based on the growth of the size of the subproblems. The present paper demonstrates that this expected growth of subproblems does not occur in some situations, leading to a termination problem of the algorithm. The problematic parts in the APO algorithm that interfere with its completeness are identified and necessary modifications to the algorithm that fix these problematic parts are given. The resulting version of the algorithm, Complete Asynchronous Partial Overlay (CompAPO), ensures its completeness. Formal proofs for the soundness and completeness of CompAPO are given. A detailed performance evaluation of CompAPO comparing it to other DisCSP algorithms is presented, along with an extensive experimental evaluation of the algorithm’s unique behavior. Additionally, an optimization version of the algorithm, CompOptAPO, is presented, discussed, and evaluated.</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="Completeness and Performance Of The APO Algorithm">
<meta name="citation_author" content="Grinshpoun, T.">
<meta name="citation_author" content="Meisels, A.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="223">
<meta name="citation_lastpage" content="258">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2611/live-2611-4109-jair.pdf">

<cite>I.  Rezek, D.  S. Leslie, S.  Reece, S.  J. Roberts, A.  Rogers, R.  K. Dash and N.  R. Jennings (2008) "On Similarities between Inference in Game Theory and  Machine Learning", Volume 33, pages 259-283</cite>
<p class="media"><a href="/media/2523/live-2523-4114-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2523/live-2523-4113-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2523'>doi:10.1613/jair.2523</a></p>
<p>In this paper, we elucidate the equivalence between inference in game theory and machine learning. Our aim in so doing is to establish an equivalent vocabulary between the two domains so as to facilitate developments at the intersection of both &#64257;elds, and as proof of the usefulness of this approach, we use recent developments in each &#64257;eld to make useful improvements to the other. More speci&#64257;cally, we consider the analogies between smooth best responses in &#64257;ctitious play and Bayesian inference methods. Initially, we use these insights to develop and demonstrate an improved algorithm for learning in games based on probabilistic moderation. That is, by integrating over the distribution of opponent strategies (a Bayesian approach within machine learning) rather than taking a simple empirical average (the approach used in standard &#64257;ctitious play) we derive a novel moderated &#64257;ctitious play algorithm and show that it is more likely than standard &#64257;ctitious play to converge to a payoff-dominant but risk-dominated Nash equilibrium in a simple coordination game. Furthermore we consider the converse case, and show how insights from game theory can be used to derive two improved mean &#64257;eld variational learning algorithms. We &#64257;rst show that the standard update rule of mean &#64257;eld variational learning is analogous to a Cournot adjustment within game theory. By analogy with &#64257;ctitious play, we then suggest an improved update rule, and show that this results in &#64257;ctitious variational play, an improved mean &#64257;eld variational learning algorithm that exhibits better convergence in highly or strongly connected graphical models. Second, we use a recent advance in &#64257;ctitious play, namely dynamic &#64257;ctitious play, to derive a derivative action variational learning algorithm, that exhibits superior convergence properties on a canonical machine learning problem (clustering a mixture distribution).</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="On Similarities between Inference in Game Theory and  Machine Learning">
<meta name="citation_author" content="Rezek, I.">
<meta name="citation_author" content="Leslie, D. S.">
<meta name="citation_author" content="Reece, S.">
<meta name="citation_author" content="Roberts, S. J.">
<meta name="citation_author" content="Rogers, A.">
<meta name="citation_author" content="Dash, R. K.">
<meta name="citation_author" content="Jennings, N. R.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="259">
<meta name="citation_lastpage" content="283">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2523/live-2523-4114-jair.pdf">

<cite>A.  Kakas, P.  Mancarella, F.  Sadri, K.  Stathis and F.  Toni (2008) "Computational Logic Foundations of KGP Agents", Volume 33, pages 285-348</cite>
<p class="media"><a href="/media/2596/live-2596-4130-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2596/live-2596-4129-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2596'>doi:10.1613/jair.2596</a></p>
<p>This paper presents the computational logic foundations of a model of agency called the KGP (Knowledge, Goals and Plan model. This model allows the specification of heterogeneous agents that can interact with each other, and can exhibit both proactive and reactive behaviour allowing them to function in dynamic environments by adjusting their goals and plans when changes happen in such environments.  KGP provides a highly modular agent architecture that integrates a collection of reasoning and physical capabilities, synthesised within transitions that update the agent's state in response to reasoning, sensing and acting. Transitions are orchestrated by cycle theories that specify the order in which transitions are executed while taking into account the dynamic context and agent preferences, as well as selection operators for providing inputs to transitions.</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="Computational Logic Foundations of KGP Agents">
<meta name="citation_author" content="Kakas, A.">
<meta name="citation_author" content="Mancarella, P.">
<meta name="citation_author" content="Sadri, F.">
<meta name="citation_author" content="Stathis, K.">
<meta name="citation_author" content="Toni, F.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="285">
<meta name="citation_lastpage" content="348">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2596/live-2596-4130-jair.pdf">

<cite>E.  Amir and A.  Chang (2008) "Learning Partially Observable Deterministic Action Models", Volume 33, pages 349-402</cite>
<p class="media"><a href="/media/2575/live-2575-4156-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2575/live-2575-4157-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2575'>doi:10.1613/jair.2575</a></p>
<p>We present exact algorithms for identifying deterministic-actions' effects and preconditions in dynamic partially observable domains.  They apply when one does not know the action model(the way actions affect the world) of a domain and must learn it from partial observations over time. Such scenarios are common in real world applications. They are challenging for AI tasks because traditional domain structures that underly tractability (e.g., conditional independence) fail there (e.g., world features become correlated). Our work departs from traditional assumptions about partial observations and action models. In particular, it focuses on problems in which actions are deterministic of simple logical structure and observation models have all features observed with some frequency. We yield tractable algorithms for the modified problem for such domains. <br />
<br />
Our algorithms take sequences of partial observations over time as input, and output deterministic action models that could have lead to those observations. The algorithms output all or one of those models (depending on our choice), and are exact in that no model is misclassified given the observations. Our algorithms take polynomial time in the number of time steps and state features for some traditional action classes examined in the AI-planning literature, e.g., STRIPS actions.  In contrast, traditional approaches for HMMs and Reinforcement Learning are inexact and exponentially intractable for such domains. Our experiments verify the theoretical tractability guarantees, and show that we identify action models exactly. Several applications in planning, autonomous exploration, and adventure-game playing already use these results.  They are also promising for probabilistic settings, partially observable reinforcement learning, and diagnosis.<br />
</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="Learning Partially Observable Deterministic Action Models">
<meta name="citation_author" content="Amir, E.">
<meta name="citation_author" content="Chang, A.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="349">
<meta name="citation_lastpage" content="402">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2575/live-2575-4156-jair.pdf">

<cite>J.  Goldsmith, J.  Lang, M.  Truszczynski and N.  Wilson (2008) "The Computational Complexity of Dominance and Consistency in CP-Nets", Volume 33, pages 403-432</cite>
<p class="media"><a href="/media/2627/live-2627-4168-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2627/live-2627-4169-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2627'>doi:10.1613/jair.2627</a></p>
<p>We investigate the computational complexity of testing dominance and consistency in CP-nets. Previously, the complexity of dominance has been determined for restricted classes in which the dependency graph of the  CP-net is acyclic. However, there are preferences of interest that define cyclic dependency graphs; these are modeled with general CP-nets. In our main results, we show here that both dominance and consistency for general CP-nets are PSPACE-complete. We then consider the concept of strong dominance, dominance equivalence and dominance incomparability, and several notions of optimality, and identify the complexity of the corresponding decision problems. The reductions used in the proofs are from STRIPS planning, and thus reinforce the earlier established connections between both areas.<br />
</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="The Computational Complexity of Dominance and Consistency in CP-Nets">
<meta name="citation_author" content="Goldsmith, J.">
<meta name="citation_author" content="Lang, J.">
<meta name="citation_author" content="Truszczynski, M.">
<meta name="citation_author" content="Wilson, N.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="403">
<meta name="citation_lastpage" content="432">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2627/live-2627-4168-jair.pdf">

<cite>D.  Zhang and Y.  Zhang (2008) "An Ordinal Bargaining Solution with Fixed-Point Property", Volume 33, pages 433-464</cite>
<p class="media"><a href="/media/2656/live-2656-4178-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2656/live-2656-4179-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2656'>doi:10.1613/jair.2656</a></p>
<p>Shapley's impossibility result indicates that the two-person bargaining problem has no non-trivial ordinal solution with the traditional game-theoretic bargaining model. Although the result is no longer true for bargaining problems with more than two agents, none of the well known bargaining solutions are ordinal. Searching for meaningful ordinal solutions, especially for the bilateral bargaining problem, has been a challenging issue in bargaining theory for more than three decades. This paper proposes a logic-based ordinal solution to the bilateral bargaining problem. We argue that if a bargaining problem is modeled in terms of the logical relation of players' physical negotiation items, a meaningful bargaining solution can be constructed based on the ordinal structure of bargainers' preferences. We represent bargainers' demands in propositional logic and bargainers' preferences over their demands in total preorder. We show that the solution satisfies most desirable logical properties, such as individual rationality (logical version), consistency, collective rationality as well as a few typical game-theoretic properties, such as weak Pareto optimality and contraction invariance. In addition, if all players' demand sets are logically closed, the solution satisfies a fixed-point condition, which says that the outcome of a negotiation is the result of mutual belief revision. Finally, we define various decision problems in relation to our bargaining model and study their computational complexity.</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="An Ordinal Bargaining Solution with Fixed-Point Property">
<meta name="citation_author" content="Zhang, D.">
<meta name="citation_author" content="Zhang, Y.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="433">
<meta name="citation_lastpage" content="464">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2656/live-2656-4178-jair.pdf">

<cite>R.  Mateescu, R.  Dechter and R.  Marinescu (2008) "AND/OR Multi-Valued Decision Diagrams (AOMDDs) for Graphical Models", Volume 33, pages 465-519</cite>
<p class="media"><a href="/media/2605/live-2605-4194-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2605/live-2605-4193-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2605'>doi:10.1613/jair.2605</a></p>
<p>Inspired by the recently introduced framework of AND/OR search spaces for graphical models, we propose to augment Multi-Valued Decision Diagrams (MDD) with AND nodes, in order to capture function decomposition structure and to extend these compiled data structures to general weighted graphical models (e.g., probabilistic models). We present the AND/OR Multi-Valued Decision Diagram (AOMDD) which compiles a graphical model into a canonical form that supports polynomial (e.g., solution counting, belief updating) or constant time (e.g. equivalence of graphical models) queries. We provide two algorithms for compiling the AOMDD of a graphical model. The first is search-based, and works by applying reduction rules to the trace of the memory intensive AND/OR search algorithm. The second is inference-based and uses a Bucket Elimination schedule to combine the AOMDDs of the input functions via the the APPLY operator. For both algorithms, the compilation time and the size of the AOMDD are, in the worst case, exponential in the treewidth of the graphical model, rather than pathwidth as is known for ordered binary decision diagrams (OBDDs). We introduce the concept of semantic treewidth, which helps explain why the size of a decision diagram is often much smaller than the worst case bound. We provide an experimental evaluation that demonstrates the potential of AOMDDs.</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="AND/OR Multi-Valued Decision Diagrams (AOMDDs) for Graphical Models">
<meta name="citation_author" content="Mateescu, R.">
<meta name="citation_author" content="Dechter, R.">
<meta name="citation_author" content="Marinescu, R.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="465">
<meta name="citation_lastpage" content="519">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2605/live-2605-4194-jair.pdf">

<cite>S.  Abdallah and V.  Lesser (2008) "A Multiagent Reinforcement Learning Algorithm with Non-linear Dynamics", Volume 33, pages 521-549</cite>
<p class="media"><a href="/media/2628/live-2628-4198-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2628/live-2628-4197-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2628'>doi:10.1613/jair.2628</a></p>
<p>Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents' decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they received.<br />
<br />
We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does not observe other agents actions or rewards). Furthermore, WPL does not assume that agents know the underlying game or the corresponding Nash Equilibrium a priori. We experimentally show that our algorithm converges in benchmark two-player-two-action games. We also show that our algorithm converges in the challenging Shapley's game where previous MARL algorithms failed to converge without knowing the underlying game or the NE. Furthermore, we show that WPL outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and learning concurrently.<br />
<br />
An important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how the policies of multiple learning agents evolve over time as agents interact with one another. Such an analysis not only verifies whether agents using a given MARL algorithm will eventually converge, but also reveals the behavior of the MARL algorithm prior to convergence. We analyze our algorithm in two-player-two-action games and show that symbolically proving WPL's convergence is difficult, because of the non-linear nature of WPL's dynamics, unlike previous MARL algorithms that had either linear or piece-wise-linear dynamics. Instead, we numerically solve WPL's dynamics differential equations and compare the solution to the dynamics of previous MARL algorithms.<br />
</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="A Multiagent Reinforcement Learning Algorithm with Non-linear Dynamics">
<meta name="citation_author" content="Abdallah, S.">
<meta name="citation_author" content="Lesser, V.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="521">
<meta name="citation_lastpage" content="549">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2628/live-2628-4198-jair.pdf">

<cite>S.  de Jong, S.  Uyttendaele and K.  Tuyls (2008) "Learning to Reach Agreement in a Continuous Ultimatum Game", Volume 33, pages 551-574</cite>
<p class="media"><a href="/media/2685/live-2685-4205-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2685/live-2685-4204-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2685'>doi:10.1613/jair.2685</a></p>
<p>It is well-known that acting in an individually rational manner, according to the principles of classical game theory, may lead to sub-optimal solutions in a class of problems named social dilemmas. In contrast, humans generally do not have much difficulty with social dilemmas, as they are able to balance personal benefit and group benefit. As agents in multi-agent systems are regularly confronted with social dilemmas, for instance in tasks such as resource allocation, these agents may benefit from the inclusion of mechanisms thought to facilitate human fairness. Although many of such mechanisms have already been implemented in a multi-agent systems context, their application is usually limited to rather abstract social dilemmas with a discrete set of available strategies (usually two). Given that many real-world examples of social dilemmas are actually continuous in nature, we extend this previous work to more general dilemmas, in which agents operate in a continuous strategy space. The social dilemma under study here is the well-known Ultimatum Game, in which an optimal solution is achieved if agents agree on a common strategy. We investigate whether a scale-free interaction network facilitates agents to reach agreement, especially in the presence of fixed-strategy agents that represent a desired (e.g. human) outcome. Moreover, we study the influence of rewiring in the interaction network. The agents are equipped with continuous-action learning automata and play a large number of random pairwise games in order to establish a common strategy. From our experiments, we may conclude that results obtained in discrete-strategy games can be generalized to continuous-strategy games to a certain extent: a scale-free interaction network structure allows agents to achieve agreement on a common strategy, and rewiring in the interaction network greatly enhances the agents' ability to reach agreement. However, it also becomes clear that some alternative mechanisms, such as reputation and volunteering, have many subtleties involved and do not have convincing beneficial effects in the continuous case.</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="Learning to Reach Agreement in a Continuous Ultimatum Game">
<meta name="citation_author" content="de Jong, S.">
<meta name="citation_author" content="Uyttendaele, S.">
<meta name="citation_author" content="Tuyls, K.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="551">
<meta name="citation_lastpage" content="574">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2685/live-2685-4205-jair.pdf">

<cite>I.  Ashlagi, D.  Monderer and M.  Tennenholtz (2008) "On the Value of Correlation", Volume 33, pages 575-613</cite>
<p class="media"><a href="/media/2588/live-2588-4210-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2588/live-2588-4209-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2588'>doi:10.1613/jair.2588</a></p>
<p>Correlated equilibrium generalizes Nash equilibrium to allow correlation devices. Correlated equilibrium captures the idea that in many systems there exists a trusted administrator who can recommend behavior to a set of agents, but can not enforce such behavior. This makes this solution concept most appropriate to the study of multi-agent systems in AI.  Aumann showed an example of a game, and of a correlated equilibrium in this game in which the agents' welfare (expected sum of players' utilities) is greater than their welfare in all mixed-strategy equilibria. Following the idea initiated by the price of anarchy literature this suggests the study of two major measures for the value of correlation in a game with nonnegative payoffs:<br />
<br />
1. The ratio between the maximal welfare obtained in a correlated equilibrium to the maximal welfare obtained in a mixed-strategy equilibrium. We refer to this ratio as the mediation value.<br />
<br />
2. The ratio between the maximal welfare to the maximal welfare obtained in a correlated equilibrium. We refer to this ratio as the enforcement value.<br />
<br />
In this work we initiate the study of the mediation and enforcement values, providing several general results on the value of correlation as captured by these concepts. We also present a set of results for the more specialized case of congestion games, a class of games that received a lot of attention in the recent literature.</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="On the Value of Correlation">
<meta name="citation_author" content="Ashlagi, I.">
<meta name="citation_author" content="Monderer, D.">
<meta name="citation_author" content="Tennenholtz, M.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="575">
<meta name="citation_lastpage" content="613">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2588/live-2588-4210-jair.pdf">

<cite>P.  D. Turney (2008) "The Latent Relation Mapping Engine: Algorithm and Experiments", Volume 33, pages 615-655</cite>
<p class="media"><a href="/media/2693/live-2693-4212-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/2693/live-2693-4213-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.2693'>doi:10.1613/jair.2693</a>
<br/><a href="/media/2693/live-2693-4214-jair.txt">Appendix </a> - Data</p>
<p>Many AI researchers and cognitive scientists have argued that analogy is the core of cognition. The most influential work on computational modeling of analogy-making is Structure Mapping Theory (SMT) and its implementation in the Structure Mapping Engine (SME). A limitation of SME is the requirement for complex hand-coded representations. We introduce the Latent Relation Mapping Engine (LRME), which combines ideas from SME and Latent Relational Analysis (LRA) in order to remove the requirement for hand-coded representations. LRME builds analogical mappings between lists of words, using a large corpus of raw text to automatically discover the semantic relations among the words. We evaluate LRME on a set of twenty analogical mapping problems, ten based on scientific analogies and ten based on common metaphors. LRME achieves human-level performance on the twenty problems. We compare LRME with a variety of alternative approaches and find that they are not able to reach the same level of performance.</p>
<a href="/vol/vol33.html">Click here to return to Volume 33 contents list</a>
<meta name="citation_title" content="The Latent Relation Mapping Engine: Algorithm and Experiments">
<meta name="citation_author" content="Turney, P. D.">
<meta name="citation_publication_date" content="2008">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="615">
<meta name="citation_lastpage" content="655">
<meta name="citation_volume" content="33">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/2693/live-2693-4212-jair.pdf">
