<cite>M.  Zhang, X.  Xiao, D.  Xiong and Q.  Liu (2014) "Topic-Based Dissimilarity and Sensitivity Models for Translation Rule Selection", Volume 50, pages 1-30</cite>
<p class="media"><a href="/media/4265/live-4265-7881-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4265/live-4265-7880-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4265'>doi:10.1613/jair.4265</a></p>
<p>Translation rule selection is a task of selecting appropriate translation rules for an ambiguous source-language segment. As translation ambiguities are pervasive in statistical machine translation, we introduce two topic-based models for translation rule selection which incorporates global topic information into translation disambiguation. We associate each synchronous translation rule with source- and target-side topic distributions.With these topic distributions, we propose a topic dissimilarity model to select desirable (less dissimilar) rules by imposing penalties for rules with a large value of dissimilarity of their topic distributions to those of given documents. In order to encourage the use of non-topic specific translation rules, we also present a topic sensitivity model to balance translation rule selection between generic rules and topic-specific rules. Furthermore, we project target-side topic distributions onto the source-side topic model space so that we can benefit from topic information of both the source and target language. We integrate the proposed topic dissimilarity and sensitivity model into hierarchical phrase-based machine translation for synchronous translation rule selection. Experiments show that our topic-based translation rule selection model can substantially improve translation quality.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Topic-Based Dissimilarity and Sensitivity Models for Translation Rule Selection">
<meta name="citation_author" content="Zhang, M.">
<meta name="citation_author" content="Xiao, X.">
<meta name="citation_author" content="Xiong, D.">
<meta name="citation_author" content="Liu, Q.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="1">
<meta name="citation_lastpage" content="30">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4265/live-4265-7881-jair.pdf">

<cite>Y.  Wang, Y.  Zhang, Y.  Zhou and M.  Zhang (2014) "Knowledge Forgetting in Answer Set Programming", Volume 50, pages 31-70</cite>
<p class="media"><a href="/media/4297/live-4297-7899-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4297/live-4297-7901-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4297'>doi:10.1613/jair.4297</a></p>
<p>The ability of discarding or hiding irrelevant information has been<br />
recognized as an important feature for knowledge based systems, including answer set programming. The notion of strong equivalence in answer set programming plays an important role for different problems as it gives rise to a substitution principle and amounts to knowledge equivalence of logic programs. In this paper, we uniformly propose a semantic knowledge forgetting,  called HT- and FLP-forgetting, for logic programs under stable model and FLP-stable model semantics, respectively. Our proposed knowledge forgetting discards exactly the knowledge of a logic program which is relevant to forgotten variables. Thus it preserves strong equivalence in the sense that strongly equivalent logic programs will remain strongly equivalent after forgetting the same variables. We show that this semantic forgetting result is always expressible; and we prove a representation theorem stating that the HT- and FLP-forgetting can be precisely characterized by Zhang-Zhou's four forgetting postulates under the HT- and FLP-model semantics, respectively. We also reveal underlying connections between the proposed forgetting and the forgetting of propositional logic, and provide complexity results for decision problems in relation to the forgetting. An application of the proposed forgetting is also considered in a conflict solving scenario.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Knowledge Forgetting in Answer Set Programming">
<meta name="citation_author" content="Wang, Y.">
<meta name="citation_author" content="Zhang, Y.">
<meta name="citation_author" content="Zhou, Y.">
<meta name="citation_author" content="Zhang, M.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="31">
<meta name="citation_lastpage" content="70">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4297/live-4297-7899-jair.pdf">

<cite>A.  Fern, S.  Natarajan, K.  Judah and P.  Tadepalli (2014) "A Decision-Theoretic Model of Assistance", Volume 50, pages 71-104</cite>
<p class="media"><a href="/media/4213/live-4213-7905-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4213/live-4213-7908-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4213'>doi:10.1613/jair.4213</a></p>
<p>There is a growing interest in intelligent assistants for a variety of applications from sorting email to helping people with disabilities to do their daily chores. In this paper, we formulate the problem of intelligent assistance in a decision-theoretic framework, and present both theoretical and empirical results. We first introduce a class of POMDPs called hidden-goal MDPs (HGMDPs), which formalizes the problem of interactively assisting an agent whose goal is hidden and whose actions are observable. In spite of its restricted nature, we show that optimal action selection for HGMDPs is PSPACE-complete even for deterministic dynamics. We then introduce a more restricted model called helper action MDPs (HAMDPs), which are sufficient for modeling many real-world problems. We show classes of HAMDPs for which efficient algorithms are possible. More interestingly, for general HAMDPs we show that a simple myopic policy achieves a near optimal regret, compared to an oracle assistant that knows the agent's goal. We then introduce more sophisticated versions of this policy for the general case of HGMDPs that we combine with a novel approach for quickly learning about the agent being assisted. We evaluate our approach in two game-like computer environments where human subjects perform tasks, and in a real-world domain of providing assistance during folder navigation in a computer desktop environment. The results show that in all three domains the framework results in an assistant that substantially reduces user effort with only modest computation.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="A Decision-Theoretic Model of Assistance">
<meta name="citation_author" content="Fern, A.">
<meta name="citation_author" content="Natarajan, S.">
<meta name="citation_author" content="Judah, K.">
<meta name="citation_author" content="Tadepalli, P.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="71">
<meta name="citation_lastpage" content="104">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4213/live-4213-7905-jair.pdf">

<cite>B.  de Keijzer, T.  B. Klos and Y.  Zhang (2014) "Finding Optimal Solutions for Voting Game Design Problems", Volume 50, pages 105-140</cite>
<p class="media"><a href="/media/4109/live-4109-7914-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4109/live-4109-7913-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4109'>doi:10.1613/jair.4109</a></p>
<p>In many circumstances where multiple agents need to make a joint decision, voting is used to aggregate the agents' preferences. Each agent's vote carries a weight, and if the sum of the weights of the  agents in favor of some outcome is larger than or equal to a given quota, then this outcome is decided upon. The distribution of weights leads to a certain distribution of power. Several `power indices' have been proposed to measure such power. In the so-called inverse problem, we are given a target distribution of power, and are asked to come up with a game in the form of a quota, plus an assignment of weights to the players whose power distribution is as close as possible to the target distribution (according to some specied distance measure).<br />
<br />
Here we study solution approaches for the larger class of voting game design (VGD) problems, one of which is the inverse problem. In the general VGD problem, the goal is to find a voting game (with a given number of players) that optimizes some function over these games. In the inverse problem, for example, we look for a weighted voting game that minimizes the distance between the distribution of power among the players and a given target distribution of power (according to a given distance measure). Our goal is to find algorithms that solve voting game design problems exactly, and we approach this goal by enumerating all games in the class of games of interest. <br />
<br />
We first present a doubly exponential algorithm for enumerating the set of simple games. We then improve on this algorithm for the class of weighted voting games and obtain a quadratic exponential (i.e., 2^O(n^2)) algorithm for enumerating them. We show that this improved algorithm runs in output-polynomial time, making it the fastest possible enumeration algorithm up to a polynomial factor. Finally, we propose an exact anytime-algorithm that runs in exponential time for the power index weighted voting game design problem (the `inverse problem'). We implement this algorithm to find a weighted voting game with a normalized Banzhaf power distribution closest to a target power index, and perform experiments to obtain some insights about the set of weighted voting games. We remark that our algorithm is applicable to optimizing any exponential-time computable function, the distance of the normalized Banzhaf index to a target power index is merely taken as an example.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Finding Optimal Solutions for Voting Game Design Problems">
<meta name="citation_author" content="de Keijzer, B.">
<meta name="citation_author" content="Klos, T. B.">
<meta name="citation_author" content="Zhang, Y.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="105">
<meta name="citation_lastpage" content="140">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4109/live-4109-7914-jair.pdf">

<cite>M.  Goldenberg, A.  Felner, R.  Stern, G.  Sharon, N.  Sturtevant, R.  C. Holte and J.  Schaeffer (2014) "Enhanced Partial Expansion A*", Volume 50, pages 141-187</cite>
<p class="media"><a href="/media/4171/live-4171-7932-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4171/live-4171-7931-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4171'>doi:10.1613/jair.4171</a></p>
<p>When solving instances of problem domains that feature a large branching factor, A* may generate a large number of nodes whose cost is greater than the cost of the optimal solution. We designate such nodes as surplus. Generating surplus nodes and adding them to the OPEN list may dominate both time and memory of the search. A recently introduced variant of A* called Partial Expansion A* (PEA*) deals with the memory aspect of this problem. When expanding a node n, PEA* generates all of its children and puts into OPEN only the children with f = f (n). n is re-inserted in the OPEN list with the f -cost of the best discarded child. This guarantees that surplus nodes are not inserted into OPEN.<br />
<br />
In this paper, we present a novel variant of A* called Enhanced Partial Expansion A* (EPEA*) that advances the idea of PEA* to address the time aspect. Given a priori domain- and heuristic- specific knowledge, EPEA* generates only the nodes with f = f(n). Although EPEA* is not always applicable or practical, we study several variants of EPEA*, which make it applicable to a large number of domains and heuristics. In particular, the ideas of EPEA* are applicable to IDA* and to the domains where pattern databases are traditionally used. Experimental studies show significant improvements in run-time and memory performance for several standard benchmark applications. We provide several theoretical studies to facilitate an understanding of the new algorithm.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Enhanced Partial Expansion A*">
<meta name="citation_author" content="Goldenberg, M.">
<meta name="citation_author" content="Felner, A.">
<meta name="citation_author" content="Stern, R.">
<meta name="citation_author" content="Sharon, G.">
<meta name="citation_author" content="Sturtevant, N.">
<meta name="citation_author" content="Holte, R. C.">
<meta name="citation_author" content="Schaeffer, J.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="141">
<meta name="citation_lastpage" content="187">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4171/live-4171-7932-jair.pdf">

<cite>J.  De Bock and G.  de Cooman (2014) "An Efficient Algorithm for Estimating State Sequences in Imprecise Hidden Markov Models", Volume 50, pages 189-233</cite>
<p class="media"><a href="/media/4385/live-4385-7939-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4385/live-4385-7938-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4385'>doi:10.1613/jair.4385</a></p>
<p>We present an efficient exact algorithm for estimating state sequences from outputs or observations in imprecise hidden Markov models (iHMMs). The uncertainty linking one state to the next, and that linking a state to its output, is represented by a set of probability mass functions instead of a single such mass function. We consider as best estimates for state sequences the maximal sequences for the posterior joint state model conditioned on the observed output sequence, associated with a gain function that is the indicator of the state sequence. This corresponds to and generalises finding the state sequence with the highest posterior probability in (precise-probabilistic) HMMs, thereby making our algorithm a generalisation of the one by Viterbi. We argue that the computational complexity of our algorithm is at worst quadratic in the length of the iHMM, cubic in the number of states, and essentially linear in the number of maximal state sequences. An important feature of our imprecise approach is that there may be more than one maximal sequence, typically in those instances where its precise-probabilistic counterpart is sensitive to the choice of prior. For binary iHMMs, we investigate experimentally how the number of maximal state sequences depends on the model parameters. We also present an application in optical character recognition, demonstrating that our algorithm can be usefully applied to robustify the inferences made by its precise-probabilistic counterpart.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="An Efficient Algorithm for Estimating State Sequences in Imprecise Hidden Markov Models">
<meta name="citation_author" content="De Bock, J.">
<meta name="citation_author" content="de Cooman, G.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="189">
<meta name="citation_lastpage" content="233">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4385/live-4385-7939-jair.pdf">

<cite>N.  Rivera, L.  Illanes, J.  A. Baier and C.  Hernandez (2014) "Reconnection with the Ideal Tree: A New Approach to Real-Time Search", Volume 50, pages 235-264</cite>
<p class="media"><a href="/media/4292/live-4292-7951-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4292/live-4292-7956-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4292'>doi:10.1613/jair.4292</a></p>
<p>Many applications, ranging from video games to dynamic robotics, require solving single-agent, deterministic search problems in partially known environments under very tight time constraints. Real-Time Heuristic Search (RTHS) algorithms are specifically designed for those applications. As a subroutine, most of them invoke a standard, but bounded, search algorithm that searches for the goal. In this paper we present FRIT, a simple approach for single-agent deterministic search problems under tight constraints and partially known environments that unlike traditional RTHS does not search for the goal but rather searches for a path that connects the current state with a so-called ideal tree T . When the agent observes that an arc in the tree cannot be traversed in the actual environment, it removes such an arc from T and then carries out a reconnection search whose objective is to find a path between the current state and any node in T . The reconnection search is done using an algorithm that is passed as a parameter to FRIT. If such a parameter is an RTHS algorithm, then the resulting algorithm can be an RTHS algorithm. We show, in addition, that FRIT may be fed with a (bounded) complete blind-search algorithm. We evaluate our approach over grid pathfinding benchmarks including game maps and mazes. Our results show that FRIT, used with RTAA*, a standard RTHS algorithm, outperforms RTAA* significantly; by one order of magnitude under tight time constraints. In addition, FRIT(daRTAA*) substantially outperforms daRTAA*, a state-of-the-art RTHS algorithm, usually obtaining solutions 50% cheaper on average when performing the same search effort. Finally, FRIT(BFS), i.e., FRIT using breadth-first-search, obtains best-quality solutions when time is limited compared to Adaptive A* and Repeated A*. Finally we show that Bug2, a pathfinding-specific navigation algorithm, outperforms FRIT(BFS) when planning time is extremely limited, but when given more time, the situation reverses.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Reconnection with the Ideal Tree: A New Approach to Real-Time Search">
<meta name="citation_author" content="Rivera, N.">
<meta name="citation_author" content="Illanes, L.">
<meta name="citation_author" content="Baier, J. A.">
<meta name="citation_author" content="Hernandez, C.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="235">
<meta name="citation_lastpage" content="264">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4292/live-4292-7951-jair.pdf">

<cite>M.  Suda (2014) "Property Directed Reachability for Automated Planning", Volume 50, pages 265-319</cite>
<p class="media"><a href="/media/4231/live-4231-7958-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4231/live-4231-7957-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4231'>doi:10.1613/jair.4231</a></p>
<p>Property Directed Reachability (PDR) is a very promising recent method for deciding reachability in symbolically represented transition systems. While originally conceived as a model checking algorithm for hardware circuits, it has already been successfully applied in several other areas. This paper is the first investigation of PDR from the perspective of automated planning.<br />
 <br />
Similarly to the planning as satisfiability paradigm, PDR draws its strength from internally employing an efficient SAT-solver. We show that most standard encoding schemes of planning into SAT can be directly used to turn PDR into a planning algorithm. As a non-obvious alternative, we propose to replace the SAT-solver inside PDR by a planning-specific procedure implementing the same interface. This SAT-solver free variant is not only more efficient, but offers additional insights and opportunities for further improvements. An experimental comparison to the state of the art planners finds it highly competitive, solving most problems on several domains.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Property Directed Reachability for Automated Planning">
<meta name="citation_author" content="Suda, M.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="265">
<meta name="citation_lastpage" content="319">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4231/live-4231-7958-jair.pdf">

<cite>F.M.  Delle Fave, A.X.  Jiang, Z.  Yin, C.  Zhang, M.  Tambe, S.  Kraus and J.  P. Sullivan (2014) "Game-Theoretic Patrolling with Dynamic Execution Uncertainty and a Case Study on a Real Transit System", Volume 50, pages 321-367</cite>
<cite>AAMAS 2013 Best Paper Award Finalist</cite><p class="media"><a href="/media/4317/live-4317-7981-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4317/live-4317-7982-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4317'>doi:10.1613/jair.4317</a></p>
<p>Attacker-Defender Stackelberg security games (SSGs) have emerged as an important research area in multi-agent systems. However, existing SSGs models yield fixed, static, schedules which fail in dynamic domains where defenders face execution uncertainty, i.e., in domains where defenders may face unanticipated disruptions of their schedules. A concrete example is an application involving checking fares on trains, where a defender's schedule is frequently interrupted by fare evaders, making static schedules useless.<br />
<br />
To address this shortcoming, this paper provides four main contributions. First, we present a novel general Bayesian Stackelberg game model for security resource allocation in dynamic uncertain domains. In this new model, execution uncertainty is handled by using a Markov decision process (MDP) for generating defender policies. Second, we study the problem of computing a Stackelberg equilibrium for this game and exploit problem structure to reduce it to a polynomial-sized optimization problem. Shifting to evaluation, our third contribution shows, in simulation, that our MDP-based policies overcome the failures of previous SSG algorithms. In so doing, we can now build a complete system, that enables handling of schedule interruptions and, consequently, to conduct some of the first controlled experiments on SSGs in the field. Hence, as our final contribution, we present results from a real-world experiment on Metro trains in Los Angeles validating our MDP-based model, and most importantly, concretely measuring the benefits of SSGs for security resource allocation.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Game-Theoretic Patrolling with Dynamic Execution Uncertainty and a Case Study on a Real Transit System">
<meta name="citation_author" content="Delle Fave, F.M.">
<meta name="citation_author" content="Jiang, A.X.">
<meta name="citation_author" content="Yin, Z.">
<meta name="citation_author" content="Zhang, C.">
<meta name="citation_author" content="Tambe, M.">
<meta name="citation_author" content="Kraus, S.">
<meta name="citation_author" content="Sullivan, J. P.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="321">
<meta name="citation_lastpage" content="367">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4317/live-4317-7981-jair.pdf">

<cite>J.R.  Doppa, A.  Fern and P.  Tadepalli (2014) "HC-Search: A Learning Framework for Search-based Structured Prediction", Volume 50, pages 369-407</cite>
<cite>AAAI 2013 Outstanding Paper Award</cite><p class="media"><a href="/media/4212/live-4212-7985-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4212'>doi:10.1613/jair.4212</a></p>
<p>Structured prediction is the problem of learning a function that maps structured inputs to structured outputs. Prototypical examples of structured prediction include part-of-speech tagging and semantic segmentation of images. Inspired by the recent successes of search-based structured prediction, we introduce a new framework for structured prediction called HC-Search. Given a structured input, the framework uses a search procedure guided by a learned heuristic H to uncover high quality candidate outputs and then employs a separate learned cost function C to select a final prediction among those outputs. The overall loss of this prediction architecture decomposes into the loss due to H not leading to high quality outputs, and the loss due to C not selecting the best among the generated outputs. Guided by this decomposition, we minimize the overall loss in a greedy stage-wise manner by first training H to quickly uncover high quality outputs via imitation learning, and then training C to correctly rank the outputs generated via H according to their true losses. Importantly, this training procedure is sensitive to the particular loss function of interest and the time-bound allowed for predictions. Experiments on several benchmark domains show that our approach significantly outperforms several state-of-the-art methods.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="HC-Search: A Learning Framework for Search-based Structured Prediction">
<meta name="citation_author" content="Doppa, J.R.">
<meta name="citation_author" content="Fern, A.">
<meta name="citation_author" content="Tadepalli, P.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="369">
<meta name="citation_lastpage" content="407">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4212/live-4212-7985-jair.pdf">

<cite>R.  Bredereck, J.  Chen, S.  Hartung, S.  Kratsch, R.  Niedermeier, O.  Suchy and G.  J. Woeginger (2014) "A Multivariate Complexity Analysis of Lobbying in Multiple Referenda", Volume 50, pages 409-446</cite>
<p class="media"><a href="/media/4285/live-4285-8000-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4285/live-4285-7999-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4285'>doi:10.1613/jair.4285</a></p>
<p>Assume that each of n voters may or may not approve each of m issues. If an agent (the lobby) may influence up to k voters, then the central question of the NP-hard Lobbying problem is whether the lobby can choose the voters to be influenced so that as a result each issue gets a majority of approvals. This problem can be modeled as a simple matrix modification problem: Can one replace k rows of a binary n x m-matrix by k all-1 rows such that each column in the resulting matrix has a majority of 1s? Significantly extending on previous work that showed parameterized intractability (W[2]-completeness) with respect to the number k of modified rows, we study how natural parameters such as n, m, k, or the "maximum number of 1s missing for any column to have a majority of 1s" (referred to as "gap value g") govern the computational complexity of Lobbying. Among other results, we prove that Lobbying is fixed-parameter tractable for parameter m and provide a greedy logarithmic-factor approximation algorithm which solves Lobbying even optimally if m < 5. We also show empirically that this greedy algorithm performs well on general instances. As a further key result, we prove that Lobbying is LOGSNP-complete for constant values g>0, thus providing a first natural complete problem from voting for this complexity class of limited nondeterminism.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="A Multivariate Complexity Analysis of Lobbying in Multiple Referenda">
<meta name="citation_author" content="Bredereck, R.">
<meta name="citation_author" content="Chen, J.">
<meta name="citation_author" content="Hartung, S.">
<meta name="citation_author" content="Kratsch, S.">
<meta name="citation_author" content="Niedermeier, R.">
<meta name="citation_author" content="Suchy, O.">
<meta name="citation_author" content="Woeginger, G. J.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="409">
<meta name="citation_lastpage" content="446">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4285/live-4285-8000-jair.pdf">

<cite>M.  Cooper, F.  Maris and P.  R&#233;gnier (2014) "Monotone Temporal Planning: Tractability, Extensions and Applications", Volume 50, pages 447-485</cite>
<p class="media"><a href="/media/4358/live-4358-8004-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4358/live-4358-8008-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4358'>doi:10.1613/jair.4358</a></p>
<p>This paper describes a polynomially-solvable class of temporal planning problems. Polynomiality follows from two assumptions. Firstly, by supposing that each sub-goal fluent can be established by at most one action, we can quickly determine which actions are necessary in any plan. Secondly, the monotonicity of sub-goal fluents allows us to express planning as an instance of STP&#8800; (Simple Temporal Problem with difference constraints). This class includes temporally-expressive problems requiring the concurrent execution of actions, with potential applications in the chemical, pharmaceutical and construction industries. <br />
<br />
We also show that any (temporal) planning problem has a monotone relaxation which can lead to the polynomial-time detection of its unsolvability in certain cases. Indeed we show that our relaxation is orthogonal to relaxations based on the ignore-deletes approach used in classical planning since it preserves deletes and can also exploit temporal information.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Monotone Temporal Planning: Tractability, Extensions and Applications">
<meta name="citation_author" content="Cooper, M.">
<meta name="citation_author" content="Maris, F.">
<meta name="citation_author" content="R&#233;gnier, P.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="447">
<meta name="citation_lastpage" content="485">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4358/live-4358-8004-jair.pdf">

<cite>E.  Keyder, J.  Hoffmann and P.  Haslum (2014) "Improving Delete Relaxation Heuristics Through Explicitly Represented Conjunctions", Volume 50, pages 487-533</cite>
<cite>ICAPS 2012 Best Paper Award</cite><p class="media"><a href="/media/4277/live-4277-8010-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4277/live-4277-8009-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4277'>doi:10.1613/jair.4277</a></p>
<p>Heuristic functions based on the delete relaxation compute upper and lower bounds on the optimal delete-relaxation heuristic h+, and are of paramount importance in both optimal and satisficing planning. Here we introduce a principled and flexible technique for improving h+, by augmenting delete-relaxed planning tasks with a limited amount of delete information. This is done by introducing special fluents that explicitly represent conjunctions of fluents in the original planning task, rendering h+ the perfect heuristic h* in the limit. Previous work has introduced a method in which the growth of the task is potentially exponential in the number of conjunctions introduced. We formulate an alternative technique relying on conditional effects, limiting the growth of the task to be linear in this number. We show that this method still renders h+ the perfect heuristic h* in the limit. We propose techniques to find an informative set of conjunctions to be introduced in different settings, and analyze and extend existing methods for lower-bounding and upper-bounding h+ in the presence of conditional effects. We evaluate the resulting heuristic functions empirically on a set of IPC benchmarks, and show that they are sometimes much more informative than standard delete-relaxation heuristics.<br />
</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Improving Delete Relaxation Heuristics Through Explicitly Represented Conjunctions">
<meta name="citation_author" content="Keyder, E.">
<meta name="citation_author" content="Hoffmann, J.">
<meta name="citation_author" content="Haslum, P.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="487">
<meta name="citation_lastpage" content="533">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4277/live-4277-8010-jair.pdf">

<cite>D.  Terekhov, T.  T. Tran, D.  G. Down and J.C.  Beck (2014) "Integrating Queueing Theory and Scheduling for Dynamic Scheduling Problems", Volume 50, pages 535-572</cite>
<p class="media"><a href="/media/4278/live-4278-8057-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4278/live-4278-8058-jair.ps.gz">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4278'>doi:10.1613/jair.4278</a></p>
<p>Dynamic scheduling problems consist of both challenging combinatorics, as found in classical scheduling problems, and stochastics due to uncertainty about the arrival times, resource requirements, and processing times of jobs. To address these two challenges, we investigate the integration of queueing theory and scheduling. The former reasons about long-run stochastic system characteristics, whereas the latter typically deals with short-term combinatorics. We investigate two simple problems to isolate the core differences and potential synergies between the two approaches: a two-machine dynamic flowshop and a flexible queueing network. We show for the first time that stability, a fundamental characteristic in queueing theory, can be applied to approaches that periodically solve combinatorial scheduling problems.  We empirically demonstrate that for a dynamic flowshop, the use of combinatorial reasoning has little impact on schedule quality beyond queueing approaches. In contrast, for the more complicated flexible queueing network, a novel algorithm that combines long-term guidance from queueing theory with short-term combinatorial decision making outperforms all other tested approaches. To our knowledge, this is the first time that such a hybrid of queueing theory and scheduling techniques has been proposed and evaluated. </p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Integrating Queueing Theory and Scheduling for Dynamic Scheduling Problems">
<meta name="citation_author" content="Terekhov, D.">
<meta name="citation_author" content="Tran, T. T.">
<meta name="citation_author" content="Down, D. G.">
<meta name="citation_author" content="Beck, J.C.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="535">
<meta name="citation_lastpage" content="572">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4278/live-4278-8057-jair.pdf">

<cite>A.  Rey and J.  Rothe (2014) "False-Name Manipulation in Weighted Voting Games is Hard for Probabilistic Polynomial Time", Volume 50, pages 573-601</cite>
<p class="media"><a href="/media/4293/live-4293-8061-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4293/live-4293-8060-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4293'>doi:10.1613/jair.4293</a></p>
<p>False-name manipulation refers to the question of whether a player in a weighted voting game can increase her power by splitting into several players and distributing her weight among these false identities.  Relatedly, the beneficial merging problem asks whether a coalition of players can increase their power in a weighted voting game by merging their weights.  For the problems of whether merging or splitting players in weighted voting games is beneficial in terms of the Shapley--Shubik and the normalized Banzhaf index, merely NP-hardness lower bounds are known, leaving the question about their exact complexity open.  For the Shapley--Shubik and the probabilistic Banzhaf index, we raise these lower bounds to hardness for PP, "probabilistic polynomial time," a class considered to be by far a larger class than NP.  For both power indices, we provide matching upper bounds for beneficial merging and, whenever the new players' weights are given, also for beneficial splitting, thus resolving previous conjectures in the affirmative.  Relatedly, we consider the beneficial annexation problem, asking whether a single player can increase her power by taking over other players' weights.  It is known that annexation is never disadvantageous for the Shapley--Shubik index, and that beneficial annexation is NP-hard for the normalized Banzhaf index.  We show that annexation is never disadvantageous for the probabilistic Banzhaf index either, and for both the Shapley--Shubik index and the probabilistic Banzhaf index we show that it is NP-complete to decide whether annexing another player is advantageous.  Moreover, we propose a general framework for merging and splitting that can be applied to different classes and representations of games.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="False-Name Manipulation in Weighted Voting Games is Hard for Probabilistic Polynomial Time">
<meta name="citation_author" content="Rey, A.">
<meta name="citation_author" content="Rothe, J.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="573">
<meta name="citation_lastpage" content="601">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4293/live-4293-8061-jair.pdf">

<cite>D.  D. Maua, C.  P. de Campos, A.  Benavoli and A.  Antonucci (2014) "Probabilistic Inference in Credal Networks: New Complexity Results", Volume 50, pages 603-637</cite>
<cite>UAI 2013 Best Student Paper</cite><p class="media"><a href="/media/4355/live-4355-8068-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4355/live-4355-8067-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4355'>doi:10.1613/jair.4355</a></p>
<p>Credal networks are graph-based statistical models whose parameters take values in a set, instead of being sharply specified as in traditional statistical models (e.g., Bayesian networks). The computational complexity of inferences on such models depends on the irrelevance/independence concept adopted. In this paper, we study inferential complexity under the concepts of epistemic irrelevance and strong independence. We show that inferences under strong independence are NP-hard even in trees with binary variables except for a single ternary one. We prove that under epistemic irrelevance the polynomial-time complexity of inferences in credal trees is not likely to extend to more general models (e.g., singly connected topologies). These results clearly distinguish networks that admit efficient inferences and those where inferences are most likely hard, and settle several open questions regarding their computational complexity. We show that these results remain valid even if we disallow the use of zero probabilities. We also show that the computation of bounds on the probability of the future state in a hidden Markov model is the same whether we assume epistemic irrelevance or strong independence, and we prove a similar result for inference in naive Bayes structures. These inferential equivalences are important for practitioners, as hidden Markov models and naive Bayes structures are used in real applications of imprecise probability.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Probabilistic Inference in Credal Networks: New Complexity Results">
<meta name="citation_author" content="Maua, D. D.">
<meta name="citation_author" content="de Campos, C. P.">
<meta name="citation_author" content="Benavoli, A.">
<meta name="citation_author" content="Antonucci, A.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="603">
<meta name="citation_lastpage" content="637">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4355/live-4355-8068-jair.pdf">

<cite>A.  Gerevini, A.  Saetti and M.  Vallati (2014) "Planning through Automatic Portfolio Configuration: The PbP Approach", Volume 50, pages 639-696</cite>
<p class="media"><a href="/media/4359/live-4359-8072-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4359/live-4359-8071-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4359'>doi:10.1613/jair.4359</a></p>
<p>In the field of domain-independent planning, several powerful planners implementing different techniques have been developed. However, no one of these systems outperforms all others in every known benchmark domain. In this work, we propose a multi-planner approach that automatically configures a portfolio of planning techniques for each given domain. The configuration process for a given domain uses a set of training instances to: (i) compute and analyze some alternative sets of macro-actions for each planner in the portfolio identifying a (possibly empty) useful set, (ii) select a cluster of planners, each one with the identified useful set of macro-actions, that is expected to perform best, and (iii) derive some additional information for configuring the execution scheduling of the selected planners at planning time. The resulting planning system, called PbP (Portfolio- based Planner), has two variants focusing on speed and plan quality. Different versions of PbP entered and won the learning track of the sixth and seventh International Planning Competitions. In this paper, we experimentally analyze PbP considering planning speed and plan quality in depth. We provide a collection of results that help to understand PbP’s behavior, and demonstrate the effectiveness of our approach to configuring a portfolio of planners with macro-actions.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Planning through Automatic Portfolio Configuration: The PbP Approach">
<meta name="citation_author" content="Gerevini, A.">
<meta name="citation_author" content="Saetti, A.">
<meta name="citation_author" content="Vallati, M.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="639">
<meta name="citation_lastpage" content="696">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4359/live-4359-8072-jair.pdf">

<cite>D.  Bergman, A.  A. Cire and W.  van Hoeve (2014) "MDD Propagation for Sequence Constraints", Volume 50, pages 697-722</cite>
<p class="media"><a href="/media/4199/live-4199-8076-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4199/live-4199-8075-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4199'>doi:10.1613/jair.4199</a></p>
<p>We study propagation for the Sequence constraint in the context of constraint programming based on limited-width MDDs. Our first contribution is proving that establishing MDD-consistency for Sequence is NP-hard. Yet, we also show that this task is fixed parameter tractable with respect to the length of the sub-sequences. In addition, we propose a partial filtering algorithm that relies on a specific decomposition of the constraint and a novel extension of MDD filtering to node domains. We experimentally evaluate the performance of our proposed filtering algorithm, and demonstrate that the strength of the MDD propagation increases as the maximum width is increased. In particular, MDD propagation can outperform conventional domain propagation for Sequence by reducing the search tree size and solving time by several orders of magnitude. Similar improvements are observed with respect to the current best MDD approach that applies the decomposition of Sequence into Among constraints.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="MDD Propagation for Sequence Constraints">
<meta name="citation_author" content="Bergman, D.">
<meta name="citation_author" content="Cire, A. A.">
<meta name="citation_author" content="van Hoeve, W.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="697">
<meta name="citation_lastpage" content="722">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4199/live-4199-8076-jair.pdf">

<cite>S.  Kiritchenko, X.  Zhu and S.  M. Mohammad (2014) "Sentiment Analysis of Short Informal Texts", Volume 50, pages 723-762</cite>
<p class="media"><a href="/media/4272/live-4272-8102-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4272/live-4272-8105-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4272'>doi:10.1613/jair.4272</a></p>
<p>We describe a state-of-the-art sentiment analysis system that detects (a) the sentiment of short informal textual messages such as tweets and SMS (message-level task) and (b) the sentiment of a word or a phrase within a message (term-level task).  The system is based on a supervised statistical text classification approach leveraging a variety of surface-form, semantic, and sentiment features.  The sentiment features are primarily derived from novel high-coverage tweet-specific sentiment lexicons.  These lexicons are automatically generated from tweets with sentiment-word hashtags and from tweets with emoticons.  To adequately capture the sentiment of words in negated contexts, a separate sentiment lexicon is generated for negated words.<br />
<br />
The system ranked first in the SemEval-2013 shared task `Sentiment Analysis in Twitter' (Task 2), obtaining an F-score of 69.02 in the message-level task and 88.93 in the term-level task.  Post-competition improvements boost the performance to an F-score of 70.45 (message-level task) and 89.50 (term-level task).  The system also obtains state-of-the-art performance on two additional datasets: the SemEval-2013 SMS test set and a corpus of movie review excerpts.  The ablation experiments demonstrate that the use of the automatically generated lexicons results in performance gains of up to 6.5 absolute percentage points.<br />
</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Sentiment Analysis of Short Informal Texts">
<meta name="citation_author" content="Kiritchenko, S.">
<meta name="citation_author" content="Zhu, X.">
<meta name="citation_author" content="Mohammad, S. M.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="723">
<meta name="citation_lastpage" content="762">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4272/live-4272-8102-jair.pdf">

<cite>A.  M. S. Barreto, J.  Pineau and D.  Precup (2014) "Policy Iteration Based on Stochastic Factorization", Volume 50, pages 763-803</cite>
<p class="media"><a href="/media/4301/live-4301-8106-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4301/live-4301-8107-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4301'>doi:10.1613/jair.4301</a></p>
<p>When a transition probability matrix is represented as the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another transition matrix that retains some fundamental characteristics of the original. Since the derived matrix can be much smaller than its precursor, this property can be exploited to create a compact version of a Markov decision process (MDP), and hence to reduce the computational cost of dynamic programming. Building on this idea, this paper presents  an approximate policy iteration algorithm called policy iteration based on stochastic factorization, or PISF for short. In terms of computational complexity, PISF replaces standard policy iteration's cubic dependence on the size of the MDP with a function that grows only linearly with the number of states in the model. The proposed algorithm also enjoys nice theoretical properties: it always terminates after a finite number of iterations and returns a decision policy whose performance only depends on the quality of the stochastic factorization. In particular, if the approximation error in the factorization is sufficiently small,  PISF computes the optimal value function of the MDP. The paper also discusses practical ways of factoring an MDP and illustrates the usefulness of the proposed algorithm with an application involving a large-scale decision problem of real economical interest.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Policy Iteration Based on Stochastic Factorization">
<meta name="citation_author" content="Barreto, A. M. S.">
<meta name="citation_author" content="Pineau, J.">
<meta name="citation_author" content="Precup, D.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="763">
<meta name="citation_lastpage" content="803">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4301/live-4301-8106-jair.pdf">

<cite>U.  Thayasivam and P.  Doshi (2014) "Speeding Up Iterative Ontology Alignment using Block-Coordinate Descent", Volume 50, pages 805-845</cite>
<p class="media"><a href="/media/4366/live-4366-8122-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4366/live-4366-8123-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4366'>doi:10.1613/jair.4366</a>
<br/><a href="/media/4366/live-4366-8124-jair.xlsx">Appendix </a> - Test Results</p>
<p>In domains such as  biomedicine, ontologies are prominently utilized for annotating data.   Consequently, aligning ontologies facilitates integrating  data.   Several   algorithms  exist  for  automatically aligning  ontologies   with  diverse  levels   of  performance.   As alignment   applications  evolve   and  exhibit   online   run  time constraints, performing the alignment in a reasonable amount of time without  compromising the  quality  of the  alignment  is a  crucial challenge.  A  large class of alignment algorithms  is iterative and often consumes more time than others in delivering solutions of high quality. We present a novel and general approach for speeding up the multivariable  optimization process  utilized  by these  algorithms. Specifically,  we  use  the  technique of  block-coordinate  descent (BCD),  which exploits  the subdimensions  of the  alignment problem identified using a partitioning  scheme.  We integrate this approach into  multiple well-known  alignment  algorithms and  show that  the enhanced  algorithms  generate  similar  or improved  alignments  in significantly  less  time on  a  comprehensive  testbed of  ontology pairs.  Because  BCD does not  overly constrain how we  partition or order the  parts, we vary  the partitioning and ordering  schemes in order  to empirically  determine the  best schemes  for each  of the selected  algorithms.  As biomedicine  represents a  key application domain  for  ontologies,  we  introduce a  comprehensive  biomedical ontology testbed  for the community  in order to  evaluate alignment algorithms.  Because biomedical ontologies tend to be large, default iterative  techniques find it  difficult to  produce a  good quality alignment within a reasonable amount of time. We align a significant number  of  ontology  pairs  from this  testbed  using  BCD-enhanced algorithms.   Our contributions represent  an important  step toward making a  significant class of  alignment techniques computationally feasible.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Speeding Up Iterative Ontology Alignment using Block-Coordinate Descent">
<meta name="citation_author" content="Thayasivam, U.">
<meta name="citation_author" content="Doshi, P.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="805">
<meta name="citation_lastpage" content="845">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4366/live-4366-8122-jair.pdf">

<cite>Y.  Zick, E.  Markakis and E.  Elkind (2014) "Arbitration and Stability in Cooperative Games with Overlapping Coalitions", Volume 50, pages 847-884</cite>
<p class="media"><a href="/media/4237/live-4237-8137-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4237/live-4237-8138-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4237'>doi:10.1613/jair.4237</a></p>
<p>Overlapping Coalition Formation (OCF) games, introduced by Chalkiadakis, Elkind, Markakis, Polukarov and Jennings in 2010, are cooperative games where players can simultaneously participate in several coalitions. Capturing the notion of stability in OCF games is a difficult task:deviating players may continue to contribute resources to joint projects with non-deviators, and the crucial question is what payoffs the deviators expect to receive from such projects. Chalkiadakis et al. introduce three stability concepts for OCF games---the conservative core, the refined core, and the optimistic core---that are based on different answers to this question. In this paper, we propose a unified framework for the study of stability in the OCF setting, which encompasses the stability concepts considered by Chalkiadakis et al. as well as a wide variety of alternative stability concepts.  Our approach is based on the notion of arbitration functions, which determine the payoff obtained by the deviators, given their deviation and the current allocation of resources. We provide a characterization of stable outcomes under arbitration. We then conduct an in-depth study of four types of arbitration functions, which correspond to four notions of the core; these include the three notions of the core considered by Chalkiadakis et al. Our results complement those of Chalkiadakis et al. and answer questions left open by their work. In particular, we show that OCF games with the conservative arbitration function are essentially equivalent to non-OCF games, by relating the conservative core of an OCF game to the core of a non-overlapping cooperative game, and use this result to obtain a strictly weaker sufficient condition for conservative core non-emptiness than the one given by Chalkiadakis et al.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Arbitration and Stability in Cooperative Games with Overlapping Coalitions">
<meta name="citation_author" content="Zick, Y.">
<meta name="citation_author" content="Markakis, E.">
<meta name="citation_author" content="Elkind, E.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="847">
<meta name="citation_lastpage" content="884">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4237/live-4237-8137-jair.pdf">

<cite>A.  Veit, Y.  Xu, R.  Zheng, N.  Chakraborty and K.  Sycara (2014) "Demand Side Energy Management via Multiagent Coordination in Consumer Cooperatives", Volume 50, pages 885-922</cite>
<p class="media"><a href="/media/4416/live-4416-8146-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4416/live-4416-8145-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4416'>doi:10.1613/jair.4416</a></p>
<p>A key challenge in creating a sustainable and energy-efficient society is to make consumer demand adaptive to the supply of energy, especially to the renewable supply. In this article, we propose a partially-centralized organization of consumers (or agents), namely, a consumer cooperative that purchases electricity from the market. In the cooperative, a central coordinator buys the electricity for the whole group.  The technical challenge is that consumers make their own demand decisions, based on their private demand constraints and preferences, which they do not share with the coordinator or other agents. We propose a novel multiagent coordination algorithm, to shape the energy demand of the cooperative. To coordinate individual consumers under incomplete information, the coordinator determines virtual price signals that it sends to the consumers to induce them to shift their demands when required. We prove that this algorithm converges to the central optimal solution and minimizes the electric energy cost of the cooperative.  Additionally, we present results on the time complexity of the iterative algorithm and its implications for agents' incentive compatibility. Furthermore, we perform simulations based on real world consumption data to (a) characterize the convergence properties of our algorithm and (b) understand the effect of differing demand characteristics of participants as well as of different price functions on the cost reduction. The results show that the convergence time scales linearly with the agent population size and length of the optimization horizon. Finally, we observe that as participants' flexibility of shifting their demands increases, cost reduction increases and that the cost reduction is not sensitive to variation in consumption patterns of the consumers.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Demand Side Energy Management via Multiagent Coordination in Consumer Cooperatives">
<meta name="citation_author" content="Veit, A.">
<meta name="citation_author" content="Xu, Y.">
<meta name="citation_author" content="Zheng, R.">
<meta name="citation_author" content="Chakraborty, N.">
<meta name="citation_author" content="Sycara, K.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="885">
<meta name="citation_lastpage" content="922">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4416/live-4416-8146-jair.pdf">

<cite>B.  Bonet and H.  Geffner (2014) "Belief Tracking for Planning with Sensing: Width, Complexity and Approximations", Volume 50, pages 923-970</cite>
<p class="media"><a href="/media/4475/live-4475-8150-jair.pdf">PDF</a>&nbsp;|&nbsp;<a href="/media/4475/live-4475-8149-jair.ps">PostScript</a>&nbsp;|&nbsp;<a href='http://dx.doi.org/10.1613/jair.4475'>doi:10.1613/jair.4475</a></p>
<p>We consider the problem of belief tracking in a planning setting where states are valuations over a set of variables that are partially observable, and beliefs stand for the sets of states that are possible. While the problem is intractable in the worst case, it has been recently shown that in deterministic conformant and contingent problems, belief tracking is exponential in a width parameter that is often bounded and small. In this work, we extend these results in two ways. First, we introduce a width notion that applies to non-deterministic problems as well, develop a factored belief tracking algorithm that is exponential in the problem width, and show how it applies to existing benchmarks. Second, we introduce a meaningful, powerful, and sound approximation scheme, beam tracking, that is exponential in a smaller parameter,  the problem causal width, and has much broader applicability. We illustrate the value of this algorithm over large instances of problems such as Battleship, Minesweeper, and Wumpus, where it yields state-of-the-art performance in real-time.</p>
<a href="/vol/vol50.html">Click here to return to Volume 50 contents list</a>
<meta name="citation_title" content="Belief Tracking for Planning with Sensing: Width, Complexity and Approximations">
<meta name="citation_author" content="Bonet, B.">
<meta name="citation_author" content="Geffner, H.">
<meta name="citation_publication_date" content="2014">
<meta name="citation_journal_title" content="Journal of Artificial Intelligence Research">
<meta name="citation_firstpage" content="923">
<meta name="citation_lastpage" content="970">
<meta name="citation_volume" content="50">
<meta name="citation_issn" content="1076 - 9757">
<meta name="citation_pdf_url" content="http://www.jair.org/media/4475/live-4475-8150-jair.pdf">
